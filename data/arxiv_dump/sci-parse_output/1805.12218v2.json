{
  "abstractText": "The study of genetic variants (GVs) can help find correlating population groups to identify cohorts that are predisposed to common diseases and explain differences in disease susceptibility and how patients react to drugs. Machine learning algorithms are increasingly being applied to identify interacting GVs to understand their complex phenotypic traits. Since the performance of a learning algorithm not only depends on the size and nature of the data but also on the quality of underlying representation, deep neural networks (DNNs) can learn non-linear mappings that allow transforming GVs data into more clustering and classification friendly representations than manual feature selection. In this paper, we proposed convolutional embedded networks (CEN) in which we combine two DNN architectures called convolutional embedded clustering (CEC) and convolutional autoencoder (CAE) classifier for clustering individuals and predicting geographic ethnicity based on GVs, respectively. We employed CAE-based representation learning on 95 million GVs from the \u20181000 genomes\u2019 (covering 2,504 individuals from 26 ethnic origins) and \u2018Simons genome diversity\u2019 (covering 279 individuals from 130 ethnic origins) projects. Quantitative and qualitative analyses with a focus on accuracy and scalability show that our approach outperforms state-of-the-art approaches such as VariantSpark and ADMIXTURE. In particular, CEC can cluster targeted population groups in 22 hours with an adjusted rand index (ARI) of 0.915, the normalized mutual information (NMI) of 0.92, and the clustering accuracy (ACC) of 89%. Contrarily, the CAE classifier can predict the geographic ethnicity of unknown samples with an F1 and Mathews correlation coefficient (MCC) score of 0.9004 and 0.8245, respectively. To provide interpretations of the predictions, we identify significant biomarkers using gradient boosted trees (GBT) and SHapley Additive exPlanations (SHAP). Overall, our approach is transparent and faster than the baseline methods, and scalable for 5% to 100% of the full human genome.",
  "authors": [
    {
      "affiliations": [],
      "name": "Md. Rezaul Karim"
    },
    {
      "affiliations": [],
      "name": "Michael Cochez"
    },
    {
      "affiliations": [],
      "name": "Achille Zappa"
    },
    {
      "affiliations": [],
      "name": "Ratnesh Sahay"
    },
    {
      "affiliations": [],
      "name": "Oya Beyan"
    },
    {
      "affiliations": [],
      "name": "Dietrich Rebholz-Schuhmann"
    },
    {
      "affiliations": [],
      "name": "Stefan Decker"
    }
  ],
  "id": "SP:56fc3825820de18b71b95261f98117bd5f471447",
  "references": [
    {
      "authors": [
        "M G."
      ],
      "title": "An integrated map of genetic variation from 1,092 human genomes",
      "venue": "Nature, vol. 491, no. 7422, p. 56, 2012.",
      "year": 2012
    },
    {
      "authors": [
        "L. Bomba",
        "K. Walter",
        "N. Soranzo"
      ],
      "title": "The impact of rare and lowfrequency genetic variants in common disease",
      "venue": "Genome biology, vol. 18, no. 1, p. 77, 2017.",
      "year": 2017
    },
    {
      "authors": [
        "H. Behravan",
        "J.M. Hartikainen",
        "M. Tengstr\u00f6m",
        "K. Pylk\u00e4s",
        "R. Winqvist",
        "V.-M. Kosma",
        "A. Mannermaa"
      ],
      "title": "Machine learning identifies interacting genetic variants contributing to breast cancer risk: A case study in finnish cases and controls",
      "venue": "Scientific reports, vol. 8, no. 1, p. 13149, 2018.",
      "year": 2018
    },
    {
      "authors": [
        "J. Byun",
        "Y. Han",
        "C.I. Amos"
      ],
      "title": "Ancestry inference using principal component analysis and spatial analysis: a distance-based analysis to account for population substructure",
      "venue": "BMC genomics, vol. 18, no. 1, p. 789, 2017.",
      "year": 2017
    },
    {
      "authors": [
        "M.P. Miller",
        "S. Kumar"
      ],
      "title": "Understanding human disease mutations through the use of interspecific genetic variation",
      "venue": "Human molecular genetics, vol. 10, no. 21, pp. 2319\u20132328, 2001.",
      "year": 2001
    },
    {
      "authors": [
        "Y. Laitman",
        "B.-J. Feng",
        "I.M. Zamir",
        "J.N. Weitzel",
        "E. Duncan",
        "A. Latif"
      ],
      "title": "Haplotype analysis of the 185delag brca1 mutation in ethnically diverse populations",
      "venue": "European Journal of Human Genetics, vol. 21, no. 2, p. 212, 2013.",
      "year": 2013
    },
    {
      "authors": [
        "B. Padhukasahasram"
      ],
      "title": "Inferring ancestry from population genomic data and its applications",
      "venue": "Frontiers in genetics, vol. 5, p. 204, 2014.",
      "year": 2014
    },
    {
      "authors": [
        "S. Sheehan",
        "Y.S. Song"
      ],
      "title": "Deep learning for population genetic inference",
      "venue": "PLoS computational biology, vol. 12, no. 3, 2016.",
      "year": 2016
    },
    {
      "authors": [
        "L.B. Jorde",
        "S.P. Wooding"
      ],
      "title": "Genetic variation, classification and race",
      "venue": "Nature genetics, vol. 36, pp. S28\u2013S33, 2004.",
      "year": 2004
    },
    {
      "authors": [
        "Z.H. Rosser",
        "T. Zerjal",
        "M.E. Hurles",
        "M. Adojaan",
        "D. Alavantic",
        "A. Amorim",
        "W. Amos",
        "M. Armenteros",
        "E. Arroyo",
        "G. Barbujani"
      ],
      "title": "Y-chromosomal diversity in europe is clinal and influenced primarily by geography, rather than by language",
      "venue": "The American Journal of Human Genetics, vol. 67, no. 6, pp. 1526\u20131543, 2000.",
      "year": 2000
    },
    {
      "authors": [
        "D.H. Alexander",
        "J. Novembre",
        "K. Lange"
      ],
      "title": "Fast model-based estimation of ancestry in unrelated individuals",
      "venue": "Genome research, vol. 19, no. 9, pp. 1655\u20131664, 2009.",
      "year": 2009
    },
    {
      "authors": [
        "X. Gao",
        "J. Starmer"
      ],
      "title": "Human population structure detection via multil-genotype clustering",
      "venue": "BMC genetics, vol. 8, no. 1, p. 34, 2007.",
      "year": 2007
    },
    {
      "authors": [
        "B. Padhukasahasram"
      ],
      "title": "Inferring ancestry from population genomic data and its applications",
      "venue": "Frontiers in genetics, vol. 5, p. 204, 2014.",
      "year": 2014
    },
    {
      "authors": [
        "J.M. Kidd",
        "S. Gravel",
        "S. Musharoff",
        "R. Chen"
      ],
      "title": "Population genetic inference from personal genome data: impact of ancestry and admixture on human genomic variation",
      "venue": "The American Journal of Human Genetics, vol. 91, no. 4, pp. 660\u2013671, 2012.",
      "year": 2012
    },
    {
      "authors": [
        "E.J. Parra",
        "A. Marcini",
        "J. Akey",
        "J. Martinson",
        "M.A. Batzer",
        "R. Cooper",
        "Forrester"
      ],
      "title": "Estimating african american admixture proportions by use of population-specific alleles",
      "venue": "The American Journal of Human Genetics, vol. 63, no. 6, pp. 1839\u20131851, 1998.",
      "year": 1839
    },
    {
      "authors": [
        "A.R. OBrien",
        "N.F. Saunders",
        "Y. Guo",
        "F.A. Buske",
        "R.J. Scott",
        "D.C. Bauer"
      ],
      "title": "Variantspark: population scale clustering of genotype information",
      "venue": "BMC genomics, vol. 16, no. 1, p. 1052, 2015.",
      "year": 2015
    },
    {
      "authors": [
        "C.C. Aggarwal",
        "C.K. Reddy"
      ],
      "title": "Data clustering: algorithms and applications",
      "venue": "CRC press,",
      "year": 2014
    },
    {
      "authors": [
        "M.R. Karim",
        "O. Beyan",
        "D. Rebholz-Schuhmann",
        "M. Cochez",
        "S. Decker"
      ],
      "title": "Deep learning-based clustering approaches for bioinformatics",
      "venue": "Briefings in Bioinformatics, pp. 1\u201319, 2010.",
      "year": 2010
    },
    {
      "authors": [
        "N. Jaques",
        "S. Taylor",
        "A. Sano",
        "R. Picard"
      ],
      "title": "Multimodal autoencoder: A deep learning approach to filling in missing sensor data and enabling better mood prediction",
      "venue": "2017 Seventh International Conference on Affective Computing and Intelligent Interaction (ACII). IEEE, 2017, pp. 202\u2013208.",
      "year": 2017
    },
    {
      "authors": [
        "E. Min",
        "X. Guo",
        "Q. Liu",
        "G. Zhang",
        "J. Cui",
        "J. Long"
      ],
      "title": "A survey of clustering with deep learning: From the perspective of network architecture",
      "venue": "IEEE Access, vol. 6, pp. 39 501\u201339 514, 2018.",
      "year": 2018
    },
    {
      "authors": [
        "M. Karim",
        "M. Cochez",
        "O. e. a. Beyan"
      ],
      "title": "OncoNetExplainer: Explainable predictions of cancer types based on gene expression data",
      "venue": "arXiv preprint arXiv:1909.04169, 2019.",
      "year": 1909
    },
    {
      "authors": [
        "M.E. Kaminski"
      ],
      "title": "The right to explanation, explained",
      "venue": "Berkeley Tech. LJ, vol. 34, p. 189, 2019.",
      "year": 2019
    },
    {
      "authors": [
        "E. Han",
        "P. Carbonetto",
        "R.E. Curtis",
        "Y. Wang",
        "J.M. Granka",
        "M.J. Barber"
      ],
      "title": "Clustering of 770,000 genomes reveals post-colonial population structure of north america",
      "venue": "Nature communications, vol. 8, p. 14238, 2017.",
      "year": 2017
    },
    {
      "authors": [
        "M. Lek",
        "K. Karczewski",
        "E. Minikel",
        "K. Samocha",
        "E. Banks",
        "e. a. Fennell"
      ],
      "title": "Analysis of protein-coding genetic variation in 60,706 humans",
      "venue": "BioRxiv, p. 030338, 2016.",
      "year": 2016
    },
    {
      "authors": [
        "M.R. Karim",
        "A. Zappa",
        "R. Sahay",
        "D. Rebholz-Schuhmann"
      ],
      "title": "A deep learning approach to genomics data for population scale clustering and ethnicity prediction",
      "venue": "pp. 1\u201315, 2017.",
      "year": 2017
    },
    {
      "authors": [
        "M.R. Karim",
        "A. Michel",
        "A. Zappa",
        "P. Baranov",
        "R. Sahay",
        "D. Rebholz-Schuhmann"
      ],
      "title": "Improving data workflow systems with cloud services and use of open data for bioinformatics research",
      "venue": "Briefings in Bioinformatics, p. bbx039, 2017.",
      "year": 2017
    },
    {
      "authors": [
        "J. Xie",
        "R. Girshick",
        "A. Farhadi"
      ],
      "title": "Unsupervised deep embedding for clustering analysis",
      "venue": "International conference on machine learning, 2016, pp. 478\u2013487.",
      "year": 2016
    },
    {
      "authors": [
        "M.R. Karim",
        "M. Cochez",
        "J.B. Jares",
        "M. Uddin",
        "O. Beyan",
        "S. Decker"
      ],
      "title": "Drug-drug interaction prediction based on knowledge graph embeddings and convolutional-lstm network",
      "venue": "Proceedings of the 10th ACM International Conference on Bioinformatics and Computational Biology, ser. BCB19. New York, NY, USA: ACM, 2019, p. 113123.",
      "year": 2019
    },
    {
      "authors": [
        "G.P. Consortium"
      ],
      "title": "A global reference for human genetic variation",
      "venue": "Nature, vol. 526, no. 7571, p. 68, 2015.",
      "year": 2015
    },
    {
      "authors": [
        "S. Belsare",
        "M. Levy-Sakin",
        "Y. Mostovoy",
        "S. Durinck",
        "S. Chaudhuri",
        "M. Xiao",
        "A.S. Peterson",
        "P.-Y. Kwok",
        "S. Seshagiri",
        "J.D. Wall"
      ],
      "title": "Evaluating the quality of the 1000 genomes project data",
      "venue": "BMC genomics, vol. 20, no. 1, pp. 1\u201314, 2019.",
      "year": 2019
    },
    {
      "authors": [
        "E. Birney",
        "N. Soranzo"
      ],
      "title": "Human genomics: end of start for population sequencing",
      "venue": "Nature, vol. 526, no. 7571, pp. 52\u201353, 2015.",
      "year": 2015
    },
    {
      "authors": [
        "G. Zaccone",
        "M.R. Karim"
      ],
      "title": "Deep learning with tensorflow second edition",
      "venue": "2018.",
      "year": 2018
    },
    {
      "authors": [
        "A. Chakravarti"
      ],
      "title": "Perspectives on human variation through the lens of diversity and race",
      "venue": "Cold Spring Harbor perspectives in biology, vol. 7, no. 9, p. a023358, 2015. IEEE/ACM TRANSACTIONS ON COMPUTATIONAL BIOLOGY AND BIOINFORMATICS, VOL. XXX, NO. XXX, MARCH 20XX 13",
      "year": 2015
    },
    {
      "authors": [
        "A.R. OBrien",
        "N.F. Saunders",
        "Y. Guo",
        "F.A. Buske",
        "R.J. Scott",
        "D.C. Bauer"
      ],
      "title": "Variantspark: population scale clustering of genotype information",
      "venue": "BMC genomics, vol. 16, no. 1, p. 1052, 2015.",
      "year": 2015
    },
    {
      "authors": [
        "E. Aljalbout",
        "V. Golkov",
        "Y. Siddiqui",
        "D. Cremers"
      ],
      "title": "Clustering with deep learning: Taxonomy and new methods",
      "venue": "arXiv preprint arXiv:1801.07648, 2018.",
      "year": 1801
    },
    {
      "authors": [
        "M.R. Karim",
        "S. Decker",
        "O. Beyan"
      ],
      "title": "A snapshot neural ensemble method for cancer prediction using copy number variations",
      "venue": "Neural Computing and Applications, vol. 2, pp. 21\u201345, 2019.",
      "year": 2019
    },
    {
      "authors": [
        "M. Alirezaie",
        "M. L\u00e4ngkvist",
        "M. Sioutis",
        "A. Loutfi"
      ],
      "title": "Semantic referee: A neural-symbolic framework for enhancing geospatial semantic segmentation",
      "venue": "arXiv preprint arXiv:1904.13196, 2019.",
      "year": 1904
    },
    {
      "authors": [
        "J.M. Joyce"
      ],
      "title": "Kullback-Leibler divergence",
      "venue": "International encyclopedia of statistical science, pp. 720\u2013722, 2011.",
      "year": 2011
    },
    {
      "authors": [
        "D.H. Alexander",
        "J. Novembre",
        "K. Lange"
      ],
      "title": "Unsupervised deep embedding for clustering analysis",
      "venue": "vol. 48, 2016.",
      "year": 2016
    },
    {
      "authors": [
        "L. v. d. Maaten",
        "G. Hinton"
      ],
      "title": "Visualizing data using t-SNE",
      "venue": "Journal of ML research, vol. 9, no. Nov, pp. 2579\u20132605, 2008.",
      "year": 2008
    },
    {
      "authors": [
        "P. Huang",
        "Y. Huang",
        "W. Wang",
        "L. Wang"
      ],
      "title": "Deep embedding network for clustering",
      "venue": "Pattern Recognition (ICPR), 2014 22nd International Conference on. IEEE, 2014, pp. 1532\u20131537.",
      "year": 2014
    },
    {
      "authors": [
        "O. Kilinc",
        "I. Uysal"
      ],
      "title": "Learning latent representations in neural networks for clustering through pseudo supervision and graphbased activity regularization",
      "venue": "arXiv:1802.03063, 2018.",
      "year": 1802
    },
    {
      "authors": [
        "M. Karim",
        "M. Cochez",
        "O.D. Beyan",
        "A. Zappa",
        "R. Sahay",
        "S. Decker",
        "D.-R. Schuhmann"
      ],
      "title": "Recurrent deep embedding networks for genotype clustering and ethnicity prediction",
      "venue": "arXiv preprint arXiv:1805.12218, 2018.",
      "year": 1805
    },
    {
      "authors": [
        "R. Tibshirani",
        "G. Walther",
        "T. Hastie"
      ],
      "title": "Estimating the number of clusters in a data set via the gap statistic",
      "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology), vol. 63, no. 2, pp. 411\u2013423, 2001.",
      "year": 2001
    },
    {
      "authors": [
        "A. Strehl",
        "J. Ghosh"
      ],
      "title": "Cluster ensembles\u2014a knowledge reuse framework for combining multiple partitions",
      "venue": "Journal of machine learning research, vol. 3, no. Dec, pp. 583\u2013617, 2002.",
      "year": 2002
    },
    {
      "authors": [
        "W.M. Rand"
      ],
      "title": "Objective criteria for the evaluation of clustering methods",
      "venue": "Journal of the American Statistical association, vol. 66, no. 336, pp. 846\u2013850, 1971.",
      "year": 1971
    },
    {
      "authors": [
        "H.W. Kuhn"
      ],
      "title": "The hungarian method for the assignment problem",
      "venue": "Naval research logistics, vol. 2, no. 1-2, pp. 83\u201397, 1955.",
      "year": 1955
    },
    {
      "authors": [
        "A. Rosenberg",
        "J. Hirschberg"
      ],
      "title": "V-measure: A conditional entropy-based external cluster evaluation measure",
      "venue": "Proceedings of the 2007 joint conference on empirical methods in natural language processing and computational natural language learning (EMNLP- CoNLL), 2007, pp. 410\u2013420.",
      "year": 2007
    },
    {
      "authors": [
        "S. Rhee",
        "S. Seo",
        "S. Kim"
      ],
      "title": "Hybrid approach of relation network and localized graph convolutional filtering for breast cancer subtype classification",
      "venue": "arXiv:1711.05859, 2017.",
      "year": 2017
    },
    {
      "authors": [
        "M.R. Karim",
        "G. Wicaksono",
        "I. G"
      ],
      "title": "Prognostically relevant subtypes and survival prediction for breast cancer based on multimodal genomics data",
      "venue": "IEEE Access, vol. 7, 2019.",
      "year": 2019
    },
    {
      "authors": [
        "S.M. Lundberg",
        "S.-I. Lee"
      ],
      "title": "A unified approach to interpreting model predictions",
      "venue": "Advances in Neural Information Processing Systems. Curran Associates, Inc., 2017, pp. 4765\u20134774.",
      "year": 2017
    },
    {
      "authors": [
        "R.B. Jain",
        "R.Y. Wang"
      ],
      "title": "Limitations of maximum likelihood estimation procedures when a majority of the observations are below the limit of detection",
      "venue": "Analytical chemistry, vol. 80, no. 12, pp. 4767\u20134772, 2008.",
      "year": 2008
    }
  ],
  "sections": [
    {
      "text": "Index Terms\u2014Population Genomics, Genotype Clustering, Bio-ancestry Inference, Deep Neural Networks, Representation Learning.\nF"
    },
    {
      "heading": "1 INTRODUCTION",
      "text": "Genetic variations (GVs) are structural variation in the DNA sequence in human genomes, which makes us all unique in terms of phenotype, e.g., genetic polymorphism is implicated in numerous diseases and constitute the majority of varying nucleotides among human genomes [1]. Genetic research has played a significant role in the discovery of new biological pathways underpinning complex human disease and the evaluation of new targets for therapeutic development [2]. In particular, biological understanding of the relationship between GVs can provide insights into the disease status of a patient (e.g., interacting GVs contributing to breast cancer risk) [3], which is the prerequisite to enable personalized treatment. Further benefits of studying GVs lies in the discovery and description of the genetic contribution to many human diseases based on their haplogroup and ethnicity [4]. Subsequently, finding similar population groups and identifying patients who are predisposed to\n\u2022 1Fraunhofer Institute for Applied Information Technology FIT, Germany\n\u2022 2RWTH Aachen University, Aachen, Germany\n\u2022 3Dept. of Computer Science, Vrije Universiteit Amsterdam, Netherlands\n\u2022 4Insight Centre for Data Analytics, NUI Galway, Ireland\n\u2022 5German National Library of Medicine, University of Cologne, Germany\ncommon or rare diseases at early stages is increasingly important in understanding the effects of biomarkers on the development of certain disease [5].\nDespite strides in characterizing human history from GVs data, progress in identifying genetic signatures of recent demography has been limited. Predicting haplogroup and ethnicity accurately (called bio-ancestry inferencing (BAI)1) is very challenging in which one of the most critical tasks is the analysis of genomic profiles to attribute individuals to specific ethnic populations or the interpretation of nucleotide haplotypes for diseases susceptibility [6]. Consequently, BAI is a frequently studied problem, with the main goal of identifying an individuals population of origin based on our knowledge of natural populations [7]. Accordingly, BAI has numerous applications in forensic analyses, genetic association studies, and personal genomics. Further, BAI is used as a checksum method to verify a samples integrity, e.g., case-control studies [4], where BAI is key to understand population stratification across the cohorts to help avoid probable spurious associations with even subtle ancestry differences [8]. Identification of ancestry is an important research challenge in which any direct assessment of disease-related GVs will yield more insights [9].\nSince the race of an individual depends on ancestry, grouping each into a cluster is expected to correlate with\n1Geographic origin of an individual based on genetic variants\nar X\niv :1\n80 5.\n12 21\n8v 2\n[ cs\n.L G\n] 1\n9 A\npr 2\n02 0\nthe traditional concepts of race. However, this correlation is not perfect since GVs occurs according to probabilistic principles, which often does not follow any continuous distribution across races but slightly overlaps across diverse populations, e.g., research [10] has exposed that population groups from Asia, Europe, Africa, and America can be separated based on their genomic data based on the fact that \u2018Y\u2019 chromosome lineage can be geographically localized, forming an evidence for clustering of human alleles. In this study, we try to understand: i) if individual genetic profiles can be used to attribute into specific populations, ii) if individual GVs are suitable for predicting it\u2019s ethnicity, iii) how individuals are distributed geographically, e.g, among population groups. However, grouping individuals or BAI based on GVs requires access to genomics data and efficient analytic methodologies to cope with millions of GVs from thousands of individuals [11]\u2013[14]. However, extracting significant features from a large-scale GVs is not only computationally expensive but also a critical bottleneck.\nPrevious approaches [15], [16] employed machine learning (ML) based approaches to answer these questions. In particular, ML-based clustering algorithms such as DBSCAN, OPTICS, Gaussian Mixture (GMM), Agglomerative clustering (AC), and K-means have long been used in literature to address issues with higher-dimensional input spaces. However, they are fundamentally limited to linear embedding [17]. Further, since structure of GVs data is different than numeric or categorical data, non-linear embedding is necessary before initializing the clustering operation. Hence, ML-based approaches failed to exploit non-linear relationships from high-dimensional GVs data and exhibit good accuracy at BAI and clustering tasks. In contrast, approaches based on neural networks (DNNs) can be more effective at RL and feature extraction [18]. In particular, a DNN architecture (e.g., autoencoder (AE)) with multiple hidden layers and non-linear activation functions, can capture more complex and higher-level features and contextual information from the input [19]. Further, non-linear mappings allows transforming input data into more clustering-friendly representations in which the data is mapped into a lower-dimensional feature space that helps fine-tune clustering [20].\nAlthough DNN have shown tremendous success to deal with such complex tasks, they are mostly perceived as \u2018black box\u2019 methods because of lack of understanding of their functionalities [21], which is a serious drawback. To recommend more accurate treatments and drug repositioning, interpretability is essential to provide insights on why and how a certain prediction is made by the algorithm outlining important biomarkers. Further, the \u2018right to explanation\u2019 of EU GDPR [22] gives patients the right to know why and how an algorithm made a diagnosis decision. Hence, DLbased systems have to be GDPR complaint.\nWe try to address the challenges and requirements in a scalable and efficient ways: first, we use Spark and ADAM for processing large-scale GVs to convert them into genotype objects. Then convolutional autoencoder (CAE) is employed representation learning on GVs data. Learned features are then used to: i) train the convolutional embedding clustering (CEC) for clustering individual to determine inter and intra-population groups, ii) train the CAE classifier to\npredict ethnicity of unknown samples. Finally, to provide interpretations of the predictions, we identify significant biomarkers using gradient boosted trees (GBT) and SHapley Additive exPlanations (SHAP). The rest of the paper is structured as follows: section 2 discusses related works and analyze their potential limitations. Section 3 chronicles our proposed approach in detail with materials and methods. Section 4 demonstrates some experiment results, discuss the findings, and highlights potential limitations of the study. Section 5 provides some explanations of the importance and relevance of the research reported and discussed some future works before concluding the paper."
    },
    {
      "heading": "2 RELATED WORK",
      "text": "Although 1000GP consortium has developed a global reference for human genetic variation for exome and genome sequencing and despite strides in characterizing human history from genetic polymorphism data, progress in identifying genetic signatures of recent demography has been limited [23]. Lek M. et al. [24], describe aggregation and analysis of high-quality protein-coding region and DNA sequence data for 60,706 individuals of diverse ancestries in which the objective metrics of pathogenicity are calculated for sequence variants against various classes of mutations. Their approach: i) identify as much as 3,230 genes with nearcomplete depletion of predicted protein-truncating variants, while 72% of these genes have no currently established human disease phenotype, ii) demonstrate that GVs can be used for efficient filtering of candidate disease-causing variants, which helps in the discovery of human \u2018knockout\u2019 variants in protein-coding genes.\nStudies on population structure clustering include [23], which identify fine-scale population structure clustering of 770,000 genomes in North America, which reveals postcolonial population structure. Another used approach is ADMIXTURE [15], which performs maximum likelihood estimation (MLE) of individual ancestries from multilocus SNP genotype data. However, this approach cannot cluster GVs comfortably giving an ARI of only 0.25. Further, ADMIXTURE requires a preprocessing step from VCF to PED format, which takes a significant amount of time. To address the shortcomings of ADMIXTURE, VariantSpark is proposed [16], which provides an interface from MLlib that offers a seamless genome-wide sampling of variants and provides a pipeline for visualizing results from the 1000GP and PGP. However, overall clustering accuracy is low, and VariantSpark does not provide support for classifying individuals based on genotypic information.\nResearch also focused on genomic inferring, and ethnicity prediction, e.g., literature [25] proposed approximate Bayesian computation (ABC), which is a likelihoodfree inference method based on simulating datasets and comparing their summary statistics. Although ABC\u2019s main advantages lie in its simplicity and ability to output a posterior distribution, it suffers from the \u2018curse of dimensionality\u2019 with decreasing accuracy and stability as the number of summary statistics grows [8]. Jinyoung B. et al [4], proposed a distance-based approach for BAI using principal component analysis and spatial analysis to assign individuals to population memberships.\nIn a previous work [26], we applied K-means for the population scale clustering and achieved better accuracy than ADMIXTURE and VariantSpark. For predicting ethnicity, we trained an MLP classifier, which achieved a stateof-the-art result with high confidence. However, two limitations remained: i) the feature extraction process based on SPARQL query and converting genotype data into Resource Description Format (RDF) [27] take non-trivial time for all the chromosomes. Excellent performance was obtained for the genotype dataset for a single chromosome due to a low number of latent variables, which shows inferior results for all the chromosomes because of the overfitting and lack of generalization while training MLP model."
    },
    {
      "heading": "3 MATERIALS AND METHODS",
      "text": "In this section, we describe materials and methods of our approach: first, we describe our feature engineering step we followed to prepare the training data consisting of GVs features and labels. Then we chronicle network constructions for clustering and classifying population groups. Finally, we describe the training process and hyper-parameter tuning."
    },
    {
      "heading": "3.1 Problem statement",
      "text": "Clustering individual\u2019s based on GVs is correlated with geographic ethnicity and bio-ancestry, where the main objective is grouping populations into clusters based on similarity, density, intervals, or particular statistical distributions measures of the data space [18]. Given GVs of n samples, X = {x1,x2, ...,xn}, where X \u2208 RD . We consider clustering individuals into k-categories (i.e. k super-population or sub-population groups), each represented by a centroid \u00b5j , j = 1, . . . , k. On the other hand, predicting the ethnicity of an individual xi is classifying a data point into a specific into sub-population groups based on its GVs.\nHowever, instead of clustering or classifying samples directly in the original data space X , we first transform the data with a nonlinear mapping f\u03b8 : X \u2192 Z , where \u03b8 are learnable parameters and Z \u2208 RK is the learned or embedded feature space, where K D. In our approach, to parametrize f\u03b8 , CAE architecture is employed due to it\u2019s function approximation properties and feature learning capabilities [28] by capturing local relationship values [29] in GVs with convolutional (conv) filters."
    },
    {
      "heading": "3.2 Datasets",
      "text": "Various genomics projects based on next Generation Sequencing technologies have emerged, including The Cancer Genome Atlas (TCGA)2, International Cancer Genome Consortium (ICGC)3, 1000 genomes project4 (1000GP), Human Genome Project (HGP)5, Simons Genome Diversity Project (SGDP)6, and Personal Genome Project(PGP)7. The HGP showed that significant genetic differences exist between individuals, whereas, inspired by HGP, 1000GP seeks\n2TCGA: https://cancergenome.nih.gov/ 3ICGC: https://dcc.icgc.org/ 41000GP: https://www.internationalgenome.org/ 5HGP: http://humangenes.org/ 6SGDP: http://reichdata.hms.harvard.edu/sgdp/ 7PGP: http://www.personalgenomes.org/\nto measure those differences to help biomedical researchers understand the roles of GVs in health and illness. In previous studies, the 1000GP [1], [30] serves as one of the prime sources to analyze genome-wide single nucleotide polymorphisms (SNPs) at scale for predicting individual\u2019s ancestry with regards to continental and regional origins [31]."
    },
    {
      "heading": "3.2.1 Data selection",
      "text": "Data used in this study from the 1000GP (phase 3) and SGDP act large catalog of human GVs, where the phase 3 of 1000GP provides GVs data of 2,504 individuals from 26 populations (i.e., ethnicity) in which samples are grouped into five super-population groups according to their predominant ancestry: Europe, Africa, America, and Asia in which each of the 26 populations has about 60-100 individuals [32].\nHowever, deletions and substitutions of less important variants (single nucleotide polymorphisms (SNPs), indels, and other structural variant classes) in quality control leaves a total of 88 million variants: 84.7 million were SNPs, 3.6 million short indels, and 60,000 structural variants) identified as high-quality haplotypes [1], [30], [33]. Each genotype comprises all 23 chromosomes and a separate panel file containing samples and population information. For multiallelic variants (e.g., Listing:1), each alternative allele frequency (AF) is calculated as the quotient of allele count and allele number (AN), where AF in the five super-population groups is calculated from the AN(range=[0,1]).\nHowever, one downside of 1000GP is that it\u2019s sequencing study focused on demographically large populations, which, unfortunately, tend to ignore smaller populations that are also important for understanding human diversity [34]. To include such smaller populations, GVs from the SGDP are used, which contains deep genome sequences of 279 individuals from 130 populations chosen to span much of human genetic, linguistic, and cultural variation, covering: 44 Africans, 22 Native Americans, 27 Central Asians or Siberians, 47 East Asians, 25 Oceanians, 39 South Asians, and 75 West Eurasians. The 1000GP8 and SGDP data9 are publicly available in variant call format (VCF). Additionally, population region for each sample is provided. 1 1 15211 rs78601809 T G 100 PASS AC\n=3050;AF=0.609026;AN=5008;NS =2504;DP=32245; EAS_AF=0.504; AMR_AF=0.6772;AFR_AF=0.5371; EUR_AF=0.7316;SAS_AF=0.6401; AA= t|||;VT=SNP\nListing 1: an example of multi allelic variants in 1000GP"
    },
    {
      "heading": "3.2.2 Population stratification",
      "text": "Since majority of the variants are SNPs and INDELs [8], computing likelihood of complex population genetic models is often infeasible from the multiple individuals. Further, population stratification is necessary to identify the presence of a systematic difference in AF between sub-populations in a population, possibly due to different ancestry. First, we\n8ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/release/20130502/ 9https://reichdata.hms.harvard.edu/pub/datasets/sgdp/\nprocess the panel file containing sample IDs of the individuals, population group, ethnicity, super population groups, and the gender info (table 2) and extract only the targeted population data which identify the population groups. Then we convert the GVs to common genotype object, followed by another round of filtering to extract data for the relevant individuals and super population groups only. Genotype objects are then converted into a sample variant object containing the following genotypic information:\n\u2022 Sample ID: to uniquely identify a sample \u2022 Variant ID: to uniquely identify a genetic variant \u2022 Alternate allele count: count of alternate alleles (AA)\nin which the sample differs from the reference genome.\nFurthermore, since ADMIXTUREs underlying statistical model does not take linkage disequilibrium (LD) into account, we remove variants with high LD and incomplete variants, assuming they are outliers [35]. Moreover, since 1000GP phase 3 contains overlapping and duplicate sites, we ignored duplicate sites in any analysis. The total number of sample (i.e., individual) is then determined, before grouping them using their variant IDs, and filtering out variants without support by the samples. Then we group variants by sample ID and sort them for each sample consistently using variant IDs, which gives us a sparse training data consisting of sample ID, variant ID, position ID, RS ID, and AA count, where a row represents an individual, a column represents a specific variant, and the \u201cRegion\u201d column signifies labels."
    },
    {
      "heading": "3.3 Network constructions and training",
      "text": "In our approach, we model two tasks into a single pipeline, which has three stages as shown in fig. 1: i) representation learning (RL) based on CAE, ii) population scale clustering using CEC in which we jointly optimize clustering and non-clustering losses, iii) ethnicity prediction using a CAE classifier. Non-clustering loss (NCL) is independent of the clustering algorithm and usually enforces a desired constraint on the learned model [36]. NCL also guarantees the learned representation to preserve spatial relationships between GVs so the original input can be reconstructed in the decoding phase [20], while the clustering loss is specific to the clustering method and clustering-friendliness of the learned representations [36]."
    },
    {
      "heading": "3.3.1 Construction of convolutional autoencoder",
      "text": "Inspired by literature [18], we learn the cluster and classification friendly representations of samples by employing CAE: first, to apply conv operations, we embed extracted GVs of each sample into a 2D image inspired by literature [37] in which each sample is reshaped from a 4,238x1 array into a 66 x 66 image by adding zero padding around the edges and normalized each image to [0,255]. Instead of manually engineered conv filters in a CNN, we constructed the CAE by adding conv and pooling layers. The CAE consists of an encoder that performs convolution and pooling operations and a decoder that performs unpooling and deconvolution (deconv) operations. From the given GVs,\nthe first conv layer in the encoder calculates jth feature map (FM) as follows [38]:\nhj = \u03c3 ( xi \u2217W jij + b j ) , (1)\nwhere xi is the input sample, W j ij is the j th filter from input channel i and filter j, bj is the bias for the jth filter, i.e., single bias per latent map10, \u03c3 is rectified linear unit (ReLu) activation function, and \u2217 denotes the conv operation. To obtain the translation-invariant representations, max-pooling is performed by downsampling conv layer\u2019s output and taking the maximum value in each m \u00d7 n non-overlapping sub-region [38]. In the decoding phase, unpooling and deconv operations are performed to preserve the positional-invariance information during the pooling operations. The deconv operation is performed to reconstruct xi as follows [38]:\nxi = \u03c3 ( oj \u2217W joj + c j ) , (2)\nwhere oj is jth FM and W joj is j th filter of unpooling layer o; j and cj are filter and bias of jth output layer. Hence, CAE learns optimal filters by minimizing the RL1, which is the distance measure dCAE between input xi and its corresponding reconstruction f(xi):\nLCAE = dCAE(xi, f(xi) = \u2211 i ||xi \u2212 f(xi)||2. (3)\nEventually, CAE-based RL results more abstract features that help to stabilize training and network converges faster, avoid corruption in feature space, and improve clustering quality [18]. The architecture of CAE consist of a 20-layer network in which batch normalization layer is used after every conv layer and the ReLu activation is used in every layer except for the last layer where softmax activation is used. In particular, the CAE part has the following structure:\n\u2022 Input layer: genetic variants of each sample reduced from 4,238 \u00d7 1 to 66 \u00d7 66 \u00d7 1 \u2022 Convolutional layer: of size 32 \u00d7 32 \u00d7 32 \u2022 Batch normalization layer: of size 32 \u00d7 32 \u00d7 32 \u2022 Convolutional layer: of size 16 \u00d7 16 \u00d7 64 \u2022 Batch normalization layer: of size 16 \u00d7 16 \u00d7 64 \u2022 Max-pooling layer: of size 2 \u00d7 2 \u2022 Convolutional layer: of size 8 \u00d7 8 \u00d7 128 \u2022 Batch normalization layer: of size 8 \u00d7 8 \u00d7 128 \u2022 Convolutional layer: of size 4 \u00d7 4 \u00d7 256 \u2022 Batch normalization layer: of size 4 \u00d7 4 \u00d7 256 \u2022 Max-pooling layer: of size 2 \u00d7 2 \u2022 Upsampling layer: of size 2 \u00d7 2 \u2022 Deconvolutional layer: of size 4 \u00d7 4 \u00d7 256 \u2022 Batch normalization layer: of size 4 \u00d7 4 \u00d7 256 \u2022 Deconvolutional layer: of size 8 \u00d7 8 \u00d7 128 \u2022 Batch normalization layer: of size 8 \u00d7 8 \u00d7 128 \u2022 Deconvolutional layer: of size 16 \u00d7 16 \u00d7 64 \u2022 Batch normalization layer: of size 16 \u00d7 16 \u00d7 64 \u2022 Upsampling layer: of size 2 \u00d7 2 \u2022 Deconvolutional layer: of size 32 \u00d7 32 \u00d7 32.\n10One bias per GV would introduce many degrees of freedom"
    },
    {
      "heading": "3.3.2 Construction and training of CEC network",
      "text": "Architecturally, CEC is an improved variant of DEC in which we employed CAE instead of vanilla AE and denoising AE. The CEC is trained in two phases: i) parameter initialization with a CAE (see section 3.3.1) and trained by optimizing the standard RL1, ii) parameter optimization by iterating between computing an auxiliary target distribution and minimizing the Kullback-Leibler divergence (KLD) [39] and cluster assignment hardening loss (CAHL) loss in which the cluster assignment is formulated, followed by the centroid updated with backpropagation.\nThe RL1 guarantees the learned representation to preserve important information (e.g., spatial relationships between features) so that the original input can be reconstructed [20]. Once the RL1 is optimized, latent features (LF) Z are extracted from the encoder, followed by normalizing them such that 1d ||zi|| 2 2 is approximately 1, where d is dimension of the feature space {zi \u2208 Z}. Next, from an initial estimate of the non-linear mapping f\u03b8 and centroids {\u00b5j \u2208 Z}Kj=1 (as trainable weights Z), we improve the clustering by alternating between two steps which we repeat until a convergence criterion is met [40]:\n\u2022 Step 1: soft assignment of Z to the cluster centroids. \u2022 Step 2: updating the mapping f\u03b8 and refining cluster\ncentroids by learning from initial assignments using an auxiliary target distribution.\nInitializing clustering on LF generates second type of loss called CAHL, which is specific to clustering method and clustering-friendliness of the learned representations [36]. Similar to literature [40], we consider normalized similarities between data points and centroids as the soft cluster assignments in which the Student t-distribution [41] is used as the kernel to measure the similarity between embedded point zj and centroid \u00b5j , where zi= f\u03b8 (xi) \u2208 Z corresponds to xi \u2208 X after embedding, \u03b1 is the degree of freedom, and qij is the probability of assigning sample i to cluster j [28].\nqij = (1 + ||zi \u2212 \u00b5j ||2/\u03b1)\u2212 \u03b1+1 2\u2211\nj\u2032(1 + ||zi \u2212 \u00b5j\u2032 ||2/\u03b1)\u2212 \u03b1+1 2\n(4)\nHowever, cross-validation of \u03b1 in the unsupervised setting is not a viable option. Moreover, learning \u03b1 is superfluous, similar to literature [40], we set \u03b1 to 1. In step 2, similarity between the distributions is evaluated using KLD w.r.t. by decreasing the distance between soft assignments (qij) and the auxiliary distribution (pij) as follows [28]:\nLKLD = KL(P ||Q) = \u2211 i \u2211 j pij log pij qij , (5)\nwhere qij \u2208 Q and pij \u2208 P are optimized through backpropagation. Minimizing this loss w.r.t. network parameters leads to smaller distances between the data points and their assigned cluster center for a better CQ, where the loss is computed by favoring a situation where points of a cluster are close to the mean of the cluster. Conversely, points that are close to the mean of another cluster will adversely affect the loss. However, since setting P is crucial to increase the CQ, similar to [40], the soft assignment qij is computed by\nraising auxiliary distribution pij to the second power and normalizing by frequency per cluster as follows:\npij = q2ij/fj\u2211 j\u2032 q 2 ij\u2032/fj\u2032 , (6)\nwhere fj = \u2211 i qij are soft cluster frequencies and P forces the assignments to have stricter probabilities between [0\u2013 1]. On the other hand, since the constraints enforced by the RL1 can be lost after training the network longer, using only clustering loss may lead to worse clustering results [36]. To tackle this issue and similar to literature [20], [42], [43], we performed joint training by setting \u03b1 such that the network training is affected by both clustering and non-clustering loss functions simultaneously in which combining them with a linear combination of individual loss is priory:\nL(\u03b4) = \u03c3LKLD(\u03b4) + (1\u2212 \u03c3)LCAE(\u03c3), (7)\nwhere LKLD(\u03b4) is the clustering loss, LCAE(\u03c3) is the nonclustering loss, and \u03c3 \u2208 [0, 1] is a constant hyperparameter to specify the weighting between both functions. We optimize L(\u03b4) using the first-order gradient-based AdaGrad with varying learning rates and different batch size, where gradients of L (w.r.t Z) for each data point zi and cluster centroid \u00b5j are computed as follows [40]:\n\u2202L \u2202zi = \u03b1+ 1 \u03b1 \u2211 j ( 1 + ||zi \u2212 \u00b5j ||2 \u03b1 )\u22121 (8) \u00d7(pij \u2212 qij)(zi \u2212 \u00b5j) \u2202L\n\u2202\u00b5j = \u2212\u03b1+ 1 \u03b1 \u2211 i ( 1 + ||zi \u2212 \u00b5j ||2 \u03b1 )\u22121 (9)\n\u00d7(pij \u2212 qij)(zi \u2212 \u00b5j).\nwhere the gradients \u2202L/\u2202zi are used in standard backpropagation to compute network\u2019s parameter gradient \u2202L/\u2202\u03b8. This iterative process continues until less than tol% of points change cluster assignment between two consecutive iterations for the cluster assignments [44]."
    },
    {
      "heading": "3.3.3 Training of CAE classifier",
      "text": "On the other hand, the CAE classifier has two module: autoencoder and classifier. As discussed in the previous section, after training the CAE, we remove the decoder components by making the first 20 layers trainable false, since the encoder part is already trained. 80% of the LF is used for the training the CAE classifier and 30% as the held-out test set. The training LF vector is fed into a flattening layer, followed by a dense, dropout, and Softmax layers (with output unit of 26 for 1000GP and 7 for SGDP) for the probability distribution over the classes.\nGaussian noise layer was also added followed by each conv, dense, and dropout (with a high probability value) to improve the model generalization by reducing overfitting. Moreover, to improve the classification results, we applied batch normalization and kept the adaptive rate. Weights are then updated using backpropagation, whereas the AdaGrad optimizer is used to optimize the categorical crossentropy loss between predicted (P) vs. true sub-population groups (T) in a random-search and 5-fold cross validation setting for finding optimal hyperparameters:\nE = \u2211 i,j Ti,j logPi,j + (1\u2212 Ti,j) log (1\u2212 Pi,j) (10)\nWhen the training is completed, the model is used to score against the test set to measure predicted population groups versus GVs producing in a multiclass setting."
    },
    {
      "heading": "3.4 Formulating performance metrics",
      "text": "We used three empirical measures: Elbow [45], generalizability G, and normalized mutual information (NMI) [46]. In Elbow, we calculate the cost using within-cluster sum of squares (WCSS) as a function of the number of clusters, K . Since Elbow performs better in a classical clustering setting [44], NMI is used for evaluating clustering results with different cluster numbers [28], which tells us the reduction in the entropy of class labels, computed as follows:\nNMI(y, c) = I(y, c)\n1 2 [H(y) +H(c)]\n, (11)\nwhere y signifies the ground-truth labels, c is the cluster assignment, I is the mutual information between y and c, and H(.) is the entropy. On the other hand, G is the ratio between training and validation loss [28], in which G is small when training loss is lower than the validation loss, an indication of high degree of overfitting.\nG = Ltrain\nLvalidation . (12)\nSince a good clustering accuracy also characterized by high intra-cluster similarity and low inter-cluster similarity for the data points. Accordingly, rand index (RI) is calculated based on the permutation model (PM) as follows [18]:\nRI = TP + TN\nTP + FP + FN + TN , (13)\nwhere TP, TN, FP, and FN signify true positive, true negative, false positive, and false-negative rates. RI has a value between 0 and 1, where 0 indicates the disagreement between two data clusters on any pair of points, and 1 signifies the perfect agreement (i.e., the same cluster). In our approach, normalized RI (ARI), which ranges between -1 (independent labeling) and 1 (perfect match) is used [47]. Further, to evaluate the CQ, unsupervised clustering accuracy (ACC) [28] metric is used, which takes a cluster assignment from a base clustering algorithm, assigns the ground truths, and computes the best match between them. From the ground ground-truth label yi and the cluster assignment ci, ACC is computed as follows:\nACC = max m\nn\u2211 i=1 1 { yi = m(ci) } n , (14)\nwhere m ranges overall possible one-to-one mappings between clusters and labels using Hungarian algorithm [48]. Further, since ground truths are available, homogeneity and completeness are formulated to desirable objectives for the cluster assignment using conditional entropy analysis [49]. While, the former signifies if each cluster contains only members of a single class, the latter tells us if all members of a given class are assigned to the same cluster."
    },
    {
      "heading": "4 EXPERIMENT RESULTS",
      "text": "We discuss the evaluation results, both quantitatively and qualitatively. Besides, a comparative analysis with state-ofthe-art approaches is provided."
    },
    {
      "heading": "4.1 Experiment setup",
      "text": "All the programs11 were written in Python and experimented on a computer having 32 cores, 256GB of RAM, and Debian 9.9, while the software stack consisted of Keras and scikit-learn with the TensorFlow backend. First, to achieve massive scalability while processing genotype data across all the chromosomes, we used ADAM12, while sparkling water13 is used to transform data between ADAM and Spark. Network training is then carried out on an Nvidia Titan Xp GPU with CUDA and cuDNN enabled.\nResults based on best hyperparameters produced through random search are reported empirically, where we verified whether the network converges to the optimal number of clusters by setting K = 2 and increasing it slowly. We also focused on investigating how the network training converged during the cluster assignments and updates by utilizing the Elbow method in which WCSS is calculated. Besides, other metrics such as ARI, NMI, ACC, completeness, and homogeneity. On the other hand, macro-averaged precision, recall, F1-score, and Matthias correlation coefficient (MCC) are reported in the multiclass setting."
    },
    {
      "heading": "4.2 Ethnicity prediction and inferencing analysis",
      "text": "We evaluated performance of the trained models in 3 folds: first, we extracted GVs of selected sub-population groups (i.e. \u2018ASW\u2019, \u2018CHB\u2019, \u2018CLM\u2019, \u2018FIN\u2019, \u2018GBR\u2019) from chromosome 22 data, which gives 494,328 alleles allowing 5 class and compare the actual labels to the same number of predicted ethnicity labels. This random sample selection provides a good classification accuracy (i.e., 92.86%), as shown in the confusion matrix in table 3. The full test set is then used to evaluate the model by measuring the prediction performance for sub-populations at the boundaries of the super population, giving precision, recall, F1, and MCC scores of 0.9025, 0.8983, 0.9004, and 0.8245, respectively.\n11https://github.com/rezacsedu/Convolutional-embedded-networks 12https://github.com/bigdatagenomics/adam 13https://www.h2o.ai/sparkling-water/\nHowever, since classes are imbalanced, accuracy gives a distorted estimation of the sub-populations. Hence, classspecific classification reports and MCC scores are reported in table 4. As shown, precision, recall, and f1 for the majority of sub-populations groups are high. In particular, the CAE classifier classifies JPT, KHV, CEU, TSI, FIN, GBR, IBS, LWK, ASW, ACB, MXL, PUR, CLM, PEL, GIH, PJL, BEB, STU, and ITU samples mostly correctly. However, for the CHB (Han Chinese in Beijing), CHS (Southern Han Chinese), and CDX (Chinese Dai in Xishuangbanna) populations, the CAE classifier made considerable misclassification, giving lower scores, probably because of similar GVs across those samples. Although African populations were mostly classified correctly, in the case of YRI (Yoruba in Nigeria), GWD (Gambian in Gambia), MSL (Mende in Sierra Leone), and ESN (Esan in Nigeria), a fairly high misclassification error is observed.\nEven though, CAE classifier confused between AFR, SAS, AMR, and EUR samples (as shown in fig. 4), this is still considerably low compared to literature [26]. The reason for the improvement is that all the variants with high LD and incomplete variants were removed in our preprocessing step, which has contributed towards non-corrupted latent features. The reason is simple with that minor factor, and we removed some impurities giving the network more quality features, which eventually helped in separating data points. On the other hand, class-specific MCC scores of the CAE\nclassifier suggests that predictions were strongly correlated with the ground truth, yielding a Pearson product-moment correlation coefficient higher than 0.70 for the majority subpopulation groups. Besides, the ROC curves in fig. 5 show consistent AUC scores across folds for both datasets, which signifies that the predictions by the CAE classifier in both cases are much better than random guessing.\nFinally, we trained the CAE classifier on the SGDP dataset for predicting unknown ethinicity in which similar data processing steps were followed. We experienced moderately lower predictive accuracy: out of 44 Africans, only 35 samples were predicted correctly, and the rest of the samples were predicted as South Asian. However, the CAE model shows high confidence at predicting Native Americans samples in which 20 samples out 22 were correctly predicted, and only 2 samples were predicted as West Eurasians; in the case of Central Asians or Siberians, 20 samples were correctly predicted, and 7 samples were predicted as West Eurasians, while out of 47 East Asians samples, 39 were correctly predicted, 6 samples were predicted as Africans, and 2 samples were predicted as South Asians.\nIn the case of Oceanians samples, 22 samples were correctly predicted out of 25, and only 3 samples were predicted as Native Americans. Besides, out of 39 South Asians samples, 29 were correctly predicted, and 10 samples were predicted as Africans, showing considerably high misclassification rates. Out of 75 West Eurasians samples, only 65 were correctly predicted, 8 were predicted as Central Asians or Siberians, and 2 were predicted as Native Americans. Overall, 211 samples out of 279 were correctly classified giving an F1-score of 0.83 in which CAE classifier was more confused between South Asian and Africans, Central Asians or Siberians and West Eurasians, and between East Asian and Africans, which is probably because GVs from these two groups are mostly mixed and share common alleles. A depth genomic analysis is further required to explain these."
    },
    {
      "heading": "4.3 Population scale cluster analysis",
      "text": "Clustering results are reported in table 5 with different metrics in which the AC algorithm performed the best\nclustering with optimal hyperparameters on CAE-based LF (highlighted in cyan): we observed an ARI, NMI, and ACC of 0.915, 0.927, and 0.896, respectively in which each cluster contains only members of a single class in 86.7% of the cases (homogeneity) and in 85.3% of the cases all members of a given class are assigned to the same cluster (completeness). The reason is that CAE learned LF that are more quality ones than raw GVs data, as shown in fig. 6, which eventually tends to better separability of data points.\nBesides, DBSCAN and K-means (on CAE-based on LF) also performed moderately well compared GMM and OPTICS algorithms in which DBSCAN turns out to be the second-best clustering algorithm. Contrarily, both OPTICS and GMM did not perform well, making the GMM the worst performer. On one hand, OPTICS is inherently better for sequences hence couldn\u2019t provide better separability of the data points. On the other hand, GMM doesn\u2019t have any uncertainty measure or probability w.r.t how much a data point is associated with a specific cluster.\nPredicted clusters by CEC can be observed in fig. 6d, where GVs of African (AFR), East Asian (EAS), and American (AMR) super-population groups are highly separated and grouped into distinct clusters. On the other hand, although European individuals can be mostly separated into persons of Finnish and non-Finnish ancestry [7], overall EUR is more mixed with AMR and consists predominantly of individuals, so didn\u2019t cluster well. This is probably the effect of several mixtures of GVs, e.g., migrational backgrounds and a potential reason for such low clustering accuracy. However, in cases where individuals were not clearly clustered due to diverse migrational backgrounds, the ancestry itself influences the treatment hence accurate\npopulation association may not be known for the patients even from the human leukocyte antigen (HLA) allele genotyping from SNP information [35]. Albeit, literature [35] observed an increase in cluster accuracy by removing individuals with a mixed background and operating at superpopulation level, and it was not the cases in the subpopulation level sine a recent research [24] has shown that the apparent separation between East Asian and other samples reflects a deficiency of Middle Eastern and Central Asian samples in the dataset.\nFurther, we investigate how well CEC converges to the optimal number of population groups: we started the clustering by setting K=2 (where applicable) and increased up to 35 and observe the clustering performance. We plotted NMI and G vs. several clusters and found a sharp drop of generalizability for K = 26 and 27, which means that 26 is the optimal number of clusters. To support this argument, the graph also shows that for 26, we observed the highest NMI of about 0.92. Subsequently, we utilize the Elbow method in which we calculated WCSS as a function of the number of clusters (i.e., K), for which we observed a drastic fall of WCSS when several clusters were around 25 and 26 for 1000GP and 7 and 8 for the SGDP.\nTo compare with VariationSpark, we further analyze the clustering of five super-population groups (i.e., \u2018EUR\u2019, \u2018AMR\u2019, \u2018AFR\u2019, \u2018EAS\u2019, \u2018SAS\u2019) for each to the label assigned by CEC. This experiment results in an ARI of 0.87, an ACC of 0.86, and an NMI of 0.88, showing higher confidence, at least in terms of ARI (albeit, this is still low compared to our overall ARI). Finally, we investigate which super-population group contains what percentage of human GVs in which CEC reveals an interesting statistics showing majority of the genetic variants were clustered into EUR (28.32%) and lowest into AMR (12.68%), while the distributions of samples from EAS, AFR, and SAS super-population groups were 22.25%, 18.65%, and 18.10%, respectively."
    },
    {
      "heading": "4.4 Qualitative study of the learned representations",
      "text": "Since GVs data are high-dimensional, learning the association between each feature was fairly considered. Inspired by literature [37], [50], [51] and to qualitatively study whether\n(a) Raw genetic variants (b) CAE latent FMs\n(c) Clustering raw genetic variants with agglomerative clustering (d) Agglomerative clustering based on CAE latent FMs\nFig. 6: t-SNE plots of different stages in clustering breast microscopy images\nthe learned representation can express the biological characteristics of the individuals, t-SNE of the CAE encoder\u2019s output, i.e., latent FM and raw GVs are plotted in fig. 6. From t-SNE plots, we can observe moderately high distinctive patterns between 4 super-population groups. However, not all these patterns clearly visible in the t-SNE plot of raw GVs, which signifies how CAE learned latent genetic properties better from the GVs profiles."
    },
    {
      "heading": "4.5 Shapley value-based explanations",
      "text": "Since CAE learns the representations are not easily interpretable, interpretable and disentangled representations are essential to provide insight into what features did the representations capture and what attributes of the samples are the clusters based on. To provide interpretations of the predictions, we identify significant GV biomarkers using GBT in which SHAP generates explanations. Shapley values are used to calculate the importance of a feature by comparing what a model predicts with and without a feature from all possible combinations of n features in the dataset S, where the prediction p of the model for a given feature i \u2208 S is generated w.r.t i, for which the Shapely value \u03c6 is calculated as follows [52]:\n\u03c6i(p) = \u2211\nS\u2286N/i\n|S|!(n\u2212 |S| \u2212 1)! n! (p(S \u222a i)\u2212 p(S)) (15)\nHowever, since the order in which a model sees features can affect the predictions, this computation is repeated in all possible orders to compare the features fairly [21]. The feature that does not modify the predicted value is expected to produce a Shapley value of 0. However, if two features contribute equally to the prediction, Shapley values should be the same [52]. The base value indicating the directions of the first prediction made by the GBT model is fig. 7 in which how much each feature is pushing model\u2019s output from the base value14 to the predicted output is shown. Features pushing the prediction higher are shown in red, those pushing to lower are in blue.\nIn fig. 8, top-15 common biomarkers are sorted by the sum of SHAP value magnitudes over all the samples and are ordered according to their importance, where the color represents the value of the feature from low to high, i.e., red represents high feature values, and blue represents low feature values. Overlapping points are jittered in the y-axis signifying the distribution of the Shapley values per feature,\n14The average model output over the training dataset passed\ni.e., delivery of the impact of each feature on the model output. However, since all effects describe the overall behavior of the model and are not necessarily causal, we provide additional annotations for these biomarkers in table 6. As seen, the majority of the variants are multi-allele SNPs."
    },
    {
      "heading": "4.6 Discussion and comparative analysis",
      "text": "Based on optimal K and other hyperparameters, CEC completes clustering in 22 hours with an ARI of 0.915, an NMI of 0.927 and ACC of 0.896 as shown in table 7, while VariationSpark requires 30h to finish the overall computation, leveraging an ARI of 0.82 only15.\n15VariationSpark and ADMIXTURE did not report NMI, ACC, homogeneity, and completeness\nADMIXTURE performs clustering based on the maximum likelihood estimation (MLE) of individual ancestries and multi-locus SNP genotypes. Overall processing time is considerably high (takes 35h), giving an ARI of only 0.25 [16]. Following are some potential reasons behind low clustering accuracy of VariationSpark and ADMIXTURE:\n\u2022 Being a K-means-based approach, VariationSpark has several limitations. K-means is based on the assumption that each cluster is equal-sized, where clusters to have hyper-sphere shapes [18], which is not the case for both 1000GP and SGDP datasets. Further, since K-means is sensitive to noise and outliers thus was probably trapped in a local optimum during the clustering operations. Nevertheless, we observed that the clustering results were slightly different for a different initial value of K since it\u2019s the only hyperparameter. \u2022 Being an MLE-based approach, ADMIXTURE is limited to accurately estimate population mean and standard deviation [53] in case of multi-locus SNP genotypes.\nOur study also investigated what percentage of the cases for each cluster contains members of a single class and what percentage of the cases for all members of a given class are assigned to the same cluster, giving 86.7% of homogeneity and 85.3% of the completeness of the clustering. Further, the in-memory caching mechanism of ADAM and Spark while processing VCF files made our processing pipeline 32% and 90% faster compared to VariantSpark and ADMIXTURE, respectively. Nevertheless, VariationSpark and ADMIXTURE are both black-box methods, whereas we explain the prediction made by models showing class and cluster discriminating features."
    },
    {
      "heading": "5 CONCLUSION AND OUTLOOK",
      "text": "In this paper, we implemented two powerful architecture called CEC and CAE classifiers for the clustering population and predicting genomic ancestry based on GVs of about 3,000 individuals from the 1000GP and SGDP. Our Spark and ADAM based data processing is particularly suitable for handling large-scale genomic data. Experiment results with a focus on accuracy and scalability show that our approach outperforms state-of-the-art approaches such as ADMIXTURE and VariantSpark. Our approach can perform clustering on VCF files from 2504 individuals consist of 84\nmillion GVs in just 22h, allowing faster clustering for wellcharacterized cohorts, where 20% of the genome is sufficient for the training. CEC can cluster the whole population by jointly optimized feature space with an ACC of 89%, which can be viewed as an unsupervised extension of semisupervised self-training. Similar to [28], CEC has linear complexity concerning several data points, which allowed us to scale to large datasets, whereas the CAE classifier can predict the ethnicity of unknown samples with an F1-score of 93%, which is consistent with all the genotypic dataset from 23 chromosomes, giving high-level of confidence.\nWe have seen that explaining the predictions with plots and charts are useful for exploration and discovery but interpreting them for the first time may be tricky, e.g., suppose, the CAE classifier predicts (or groups) a selected sample is of FIN population (or into EUR super-population group) in which models average response to the dataset is 0.6 and the model predicts that the selected sample\u2019s bioancestry with probability 0.75 by showing all the variables that have contributed to that prediction but still interpreting these for the first time may be tricky and need more human-interpretable decision rules in natural language. In the future, we intend to extend this work by providing: i) a more detailed analysis of intra-super-population groups and discuss the homogeneity and heterogeneity among different groups, ii) considering other datasets and factors like predicting population groups within larger geographic continents, iii) exploring if we can make share representations of the features out of both 1000 genomes and PGP datasets and cluster them simultaneously using CEC, iv) by generating decision rules to provide more human-interpretable explanations using neuro-symbolic reasoning."
    }
  ],
  "title": "Convolutional Embedded Networks for Population Scale Clustering and Bio-ancestry Inferencing",
  "year": 2020
}
