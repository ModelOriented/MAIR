{
  "abstractText": "Funding information This study was partially supported by Arnold Ventures and the Department of Computer Science at Duke University. In recent years, academics and investigative journalists have criticized certain commercial risk assessments for their blackbox nature and failure to satisfy competing notions of fairness. Since then, the field of interpretable machine learning has created simple yet effective algorithms, while the field of fair machine learning has proposed various mathematical definitions of fairness. However, studies from these fields are largely independent, despite the fact that many applications of machine learning to social issues require both fairness and interpretability. We explore the intersection by revisiting the recidivism prediction problem using state-of-the-art tools from interpretable machine learning, and assessing the models for performance, interpretability, and fairness. Unlike previous works, we compare against two existing risk assessments (COMPAS and the Arnold Public Safety Assessment) and train models that output probabilities rather than binary predictions. Wepresentmultiplemodels that beat these risk assessments in performance, and provide a fairness analysis of these models. Our results imply that machine learning models should be trained separately for separate locations, and updated over time.",
  "authors": [
    {
      "affiliations": [],
      "name": "Bin Han"
    },
    {
      "affiliations": [],
      "name": "Cynthia Rudin"
    },
    {
      "affiliations": [],
      "name": "WANG &HAN"
    }
  ],
  "id": "SP:1111c824b17681748e25de5c0ea2a61712ce3441",
  "references": [
    {
      "authors": [
        "R. Berk"
      ],
      "title": "An Impact Assessment of Machine Learning Risk Forecasts on Parole Board Decisions and Recidivism",
      "venue": "Experimental Criminology",
      "year": 2017
    },
    {
      "authors": [
        "K. Freeman"
      ],
      "title": "Algorithmic Injustice: How theWisconsin Supreme Court Failed to Protect Due Process",
      "venue": "Rights in State V. Loomis. North Carolina Journal of Law & Technology",
      "year": 2016
    },
    {
      "authors": [
        "C Rudin",
        "C Wang",
        "B. Coker"
      ],
      "title": "The age of secrecy and unfairness in recidivism prediction. arXiv:181100731 2019November;Accepted to Harvard Data Science Review",
      "year": 2019
    },
    {
      "authors": [
        "AW Flores",
        "CT Lowenkamp",
        "K. Bechtel"
      ],
      "title": "False Positives, False Negatives, and False Analyses: A Rejoinder to \u201cMachine Bias: There\u2019s Software Used Across the Country to Predict Future Criminals",
      "venue": "Federal probation 2016 September;80(2)",
      "year": 2016
    },
    {
      "authors": [
        "DieterichW",
        "C Mendoza",
        "T. Brennan"
      ],
      "title": "COMPAS Risk Scales: Demonstrating Accuracy Equity and Predictive Parity: Performance of the COMPAS Risk Scales in Broward County; 2016",
      "year": 2016
    },
    {
      "authors": [
        "C Barabas",
        "K Dinakar",
        "C. Doyle"
      ],
      "title": "The Problems With Risk Assessment Tools. The New York Times 2019 July;https: //www.nytimes.com/2019/07/17/opinion/pretrial-ai.html",
      "year": 2019
    },
    {
      "authors": [
        "C. O\u2019Neil"
      ],
      "title": "Weapons ofMath Destruction",
      "venue": "Crown Books;",
      "year": 2016
    },
    {
      "authors": [
        "J Zeng",
        "B Ustun",
        "C. Rudin"
      ],
      "title": "Interpretable classification models for recidivism prediction. Journal of the Royal Statistical Society: Series A (Statistics in Society) 2017;180(3):689\u2013722",
      "year": 2017
    },
    {
      "authors": [
        "E Angelino",
        "N Larus-Stone",
        "D Alabi",
        "M Seltzer",
        "C. Rudin"
      ],
      "title": "Certifiably optimal rule lists for categorical data",
      "venue": "Journal of Machine Learning Research",
      "year": 2018
    },
    {
      "authors": [
        "Y Lou",
        "R Caruana",
        "J Gehrke",
        "G. Hooker"
      ],
      "title": "Accurate intelligible models with pairwise interactions",
      "venue": "ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD);",
      "year": 2013
    },
    {
      "authors": [
        "Soares E",
        "Angelov PP"
      ],
      "title": "Fair-by-design explainable models for prediction of recidivism. ArXiv 2019;abs/1910.02043",
      "year": 2043
    },
    {
      "authors": [
        "A Gelb",
        "T Velazquez"
      ],
      "title": "Trust PC, of America US. The Changing State of Recidivism: Fewer People Going Back to Prison",
      "venue": "The PewCharitable Trusts",
      "year": 2018
    },
    {
      "authors": [
        "J Kim",
        "S Bushway",
        "H. Tsao"
      ],
      "title": "Identifying Classes of Explanation for Crime Drop: Period and Cohort Effects for New York State",
      "venue": "Journal of Quantitative Criminology 2016;32:357\u2013375. WANG &HAN ET AL",
      "year": 2016
    },
    {
      "authors": [
        "D Kehl",
        "P Guo",
        "S. Kessler"
      ],
      "title": "Algorithms in the Criminal Justice System: Assessing the Use of Risk Assessments in Sentencing",
      "year": 2017
    },
    {
      "authors": [
        "E Latessa",
        "P Smith",
        "R Lemke",
        "MakariosM",
        "C. Lowenkamp"
      ],
      "title": "Creation and Validation of the Ohio Risk Assessment System",
      "venue": "University of Cincinnati School of Criminal Justice Center for Criminal Justice Research;",
      "year": 2009
    },
    {
      "authors": [
        "R Hanson",
        "D. Thornton"
      ],
      "title": "Notes on the development of Static-2002. Ottawa, Ontario: Department of the Solicitor General of Canada 2003",
      "year": 2003
    },
    {
      "authors": [
        "Tollenaar N",
        "van der Heijden PGM"
      ],
      "title": "Which method predicts recidivism best?: a comparison of statistical, machine learning and data mining predictive models. Journal of the Royal Statistical Society: Series A (Statistics in Society) 2013;176(2):565\u2013584",
      "year": 2013
    },
    {
      "authors": [
        "P Howard",
        "B Francis",
        "K Soothill",
        "L. Humphreys"
      ],
      "title": "OGRS 3: The revised offender group reconviction scale",
      "venue": "Ministry of Justice;",
      "year": 2009
    },
    {
      "authors": [
        "RM Dawes",
        "D Faust",
        "PE. Meehl"
      ],
      "title": "Clinical versus actuarial judgment",
      "venue": "Science",
      "year": 1989
    },
    {
      "authors": [
        "Grove WM",
        "Meehl PE"
      ],
      "title": "Comparative efficiency of informal (subjective, impressionistic) and formal (mechanical, algorithmic) prediction procedures: The clinical\u2013statistical controversy",
      "venue": "Psychology, Public Policy,",
      "year": 1996
    },
    {
      "authors": [
        "Sherman LW"
      ],
      "title": "The power few: experimental criminology and the reduction of harm",
      "venue": "Journal of Experimental Criminology",
      "year": 2007
    },
    {
      "authors": [
        "N. James"
      ],
      "title": "Risk andNeeds Assessment in the Federal Prison System",
      "venue": "Congressional Research",
      "year": 2018
    },
    {
      "authors": [
        "J. Zweig"
      ],
      "title": "Extraordinary Conditions Of Release Under The Bail Reform Act",
      "venue": "Harvard Journal On Legislation",
      "year": 2010
    },
    {
      "authors": [
        "S Desmarais",
        "B Garrett",
        "C. Rudin"
      ],
      "title": "Risk Assessment Tools Are Not A Failed \u2019Minority Report",
      "year": 2019
    },
    {
      "authors": [
        "J Skeem",
        "Z Lin",
        "J Jung",
        "S. Goel"
      ],
      "title": "The limits of human predictions of recidivism",
      "venue": "Science Advances",
      "year": 2020
    },
    {
      "authors": [
        "B Garrett",
        "M. Stevenson"
      ],
      "title": "Open Risk Assessments",
      "venue": "Behavioral Science https://sites.law.duke.edu/ justsciencelab/2019/09/15/comment-on-pattern-by-brandon-l-garrett-megan-t-stevenson/,",
      "year": 2019
    },
    {
      "authors": [
        "J Roberts"
      ],
      "title": "vonHirschA. Previous Convictions at Sentening - Theoretical andApplied Perspective",
      "venue": "Bloomsbury Publishing;",
      "year": 2010
    },
    {
      "authors": [
        "Starr SB"
      ],
      "title": "The Risk Assessment Era: AnOverdue Debate",
      "venue": "Federal Sentencing Reporter",
      "year": 2015
    },
    {
      "authors": [
        "MA Neuilly",
        "KM Zgoba",
        "GE Tita",
        "SS. Lee"
      ],
      "title": "Predicting recidivism in homicide offenders using classification tree analysis",
      "venue": "Homicide studies",
      "year": 2011
    },
    {
      "authors": [
        "Friedman JH"
      ],
      "title": "Stochastic gradient boosting. Computational Statistics &amp; Data Analysis 2002;38(4):367\u2013378",
      "year": 2002
    },
    {
      "authors": [
        "Palocsay SW",
        "PingWang",
        "Brookshire RG"
      ],
      "title": "Predicting criminal recidivism using neural networks. Socio-Economic Planning Sciences 2000December;34:271\u2013284",
      "year": 2000
    },
    {
      "authors": [
        "RA Berk",
        "Y He",
        "SB. Sorenson"
      ],
      "title": "Developing a practical forecasting screener for domestic violence incidents",
      "venue": "Evaluation Review",
      "year": 2005
    },
    {
      "authors": [
        "S Goel",
        "JM Rao",
        "R. Shroff"
      ],
      "title": "Precinct or Prejudice? Understanding Racial Disparities in New York City\u2019s Stop-And-Frisk Policy. Institute ofMathematical Statistics 2016;10(1):365\u2013394",
      "year": 2016
    },
    {
      "authors": [
        "C. Rudin"
      ],
      "title": "Stop Explaining Black BoxMachine LearningModels for High Stakes Decisions and Use InterpretableModels Instead",
      "venue": "NatureMachine Intelligence",
      "year": 2019
    },
    {
      "authors": [
        "HardtM",
        "E Price",
        "N. Srebro"
      ],
      "title": "Equality of opportunity in supervised learning. In: Advances in neural information processing",
      "year": 2016
    },
    {
      "authors": [
        "A Agarwal",
        "A Beygelzimer",
        "Dud\u00edkM",
        "J Langford",
        "H. Wallach"
      ],
      "title": "A reductions approach to fair classification",
      "venue": "arXiv preprint",
      "year": 2018
    },
    {
      "authors": [
        "J Kleinberg",
        "S Mullainathan",
        "M. Raghavan"
      ],
      "title": "Inherent Trade-Offs in the Fair Determination of Risk Scores",
      "year": 2016
    },
    {
      "authors": [
        "PleissG",
        "RaghavanM",
        "WuF",
        "Kleinberg J",
        "WeinbergerK"
      ],
      "title": "On fairness and calibration",
      "venue": "Advances inNeural Information Processing Systems;",
      "year": 2017
    },
    {
      "authors": [
        "R. Binns"
      ],
      "title": "Fairness inMachine Learning: Lessons from Political Philosophy",
      "venue": "Journal ofMachine Learning Research",
      "year": 2018
    },
    {
      "authors": [
        "S Verma",
        "J. Rubin"
      ],
      "title": "Fairness Definitions Explained",
      "venue": "ACM/IEEE International Workshop on Software Fairness ACM;",
      "year": 2018
    },
    {
      "authors": [
        "R Berk",
        "H Heidari",
        "S Jabbari",
        "M Kearns",
        "A. Roth"
      ],
      "title": "Fairness in Criminal Justice Risk Assessments: The State of the Art",
      "venue": "Sociological Methods & Research",
      "year": 2017
    },
    {
      "authors": [
        "R. Berk"
      ],
      "title": "Accuracy and Fairness for Juvenile Justice Risk Assessments",
      "venue": "Journal of Empirical Legal Studies",
      "year": 2019
    },
    {
      "authors": [
        "Corbett-Davies S",
        "PiersonE",
        "FellerA",
        "Goel S",
        "HuqA"
      ],
      "title": "Algorithmicdecisionmaking and the cost of fairness",
      "venue": "InProceedings of the 23rd ACMSIGKDD International Conference on KnowledgeDiscovery andDataMining;",
      "year": 2017
    },
    {
      "authors": [
        "Barocas S",
        "Selbst AD"
      ],
      "title": "Big Data\u2019s Disparate Impact",
      "venue": "California Law Review",
      "year": 2016
    },
    {
      "authors": [
        "S Corbett-Davies",
        "S. Goel"
      ],
      "title": "The Measure and Mismeasure of Fairness: A Critical Review of Fair Machine Learning",
      "year": 2018
    },
    {
      "authors": [
        "V Vapnik",
        "A. Chervonenkis"
      ],
      "title": "A note on one class of perceptrons. Automation and Remote Control 1964;25",
      "year": 1964
    },
    {
      "authors": [
        "L Breiman",
        "J Friedman",
        "CJ Stone",
        "RA. Olshen"
      ],
      "title": "Classification and regression trees",
      "venue": "CRC press;",
      "year": 1984
    },
    {
      "authors": [
        "Freund Y",
        "Schapire RE"
      ],
      "title": "A decision-theoretic generalization of on-line learning and an application to boosting. Journal of computer and system sciences 1997;55(1):119\u2013139",
      "year": 1997
    },
    {
      "authors": [
        "T Chen",
        "C. Guestrin"
      ],
      "title": "Xgboost: A scalable tree boosting system",
      "venue": "Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and datamining;",
      "year": 2016
    },
    {
      "authors": [
        "B Ustun",
        "C. Rudin"
      ],
      "title": "Optimized Risk Scores",
      "venue": "Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining KDD \u201917,",
      "year": 2017
    },
    {
      "authors": [
        "StevensonMT",
        "Slobogin C"
      ],
      "title": "Algorithmic Risk Assessments and the Double-Edged Sword of Youth",
      "venue": "Washington University Law Review 2018;96(Vanderbilt Law Research Paper No",
      "year": 2018
    },
    {
      "authors": [
        "A Bindler",
        "R. Hjalmarsson"
      ],
      "title": "How punishment severity affects jury verdicts: Evidence from two natural experiments",
      "venue": "American Economic Journal: Economic Policy",
      "year": 2018
    },
    {
      "authors": [
        "Bushway SD",
        "Piehl AM"
      ],
      "title": "The inextricable link between age and criminal history in sentencing",
      "venue": "Crime & Delinquency",
      "year": 2007
    },
    {
      "authors": [
        "A. Mishra"
      ],
      "title": "Climate and Crime",
      "venue": "Global Journal of Science Frontier Research: H, Environment & Earth Science",
      "year": 2014
    },
    {
      "authors": [
        "J. Defronzo"
      ],
      "title": "Climate and Crime: Tests of an FBI Assumption. Environment and Behavior 1984;16",
      "year": 1984
    },
    {
      "authors": [
        "M Kleiman",
        "BJ Ostrom",
        "FL. Cheesman"
      ],
      "title": "Using risk assessment to inform sentencing decisions for nonviolent offenders in Virginia",
      "venue": "Crime &Delinquency",
      "year": 2007
    },
    {
      "authors": [
        "DworkC",
        "HardtM",
        "Pitassi T",
        "ReingoldO",
        "Zemel R"
      ],
      "title": "Fairness ThroughAwareness",
      "venue": "Proceedings of the 3rd Innovations in Theoretical Computer Science Conference ITCS \u201912,",
      "year": 2012
    },
    {
      "authors": [
        "A. Chouldechova"
      ],
      "title": "Fair prediction with disparate impact: A study of bias in recidivism prediction instruments. Big data 2017;5(2):153\u2013163",
      "year": 2017
    },
    {
      "authors": [
        "R Zemel",
        "Y Wu",
        "K Swersky",
        "T Pitassi",
        "C. Dwork"
      ],
      "title": "Learning fair representations",
      "venue": "In: International Conference onMachine Learning;",
      "year": 2013
    },
    {
      "authors": [
        "R Berk",
        "H Heidari",
        "S Jabbari",
        "JosephM",
        "M Kearns",
        "J Morgenstern"
      ],
      "title": "A convex framework for fair regression",
      "venue": "arXiv preprint",
      "year": 2017
    },
    {
      "authors": [
        "Agarwal A",
        "Dud\u00edkM",
        "WuZS"
      ],
      "title": "Fair Regression: QuantitativeDefinitions andReduction-basedAlgorithms",
      "venue": "arXiv preprint",
      "year": 2019
    },
    {
      "authors": [
        "P Cook",
        "J. Laub"
      ],
      "title": "After the Epidemic Recent Trends in YouthViolence in theUnited States",
      "venue": "Crime and Justice",
      "year": 2002
    },
    {
      "authors": [
        "B. Alfred"
      ],
      "title": "The Crime Drop in America: An Explanation of Some Recent Crime Trends",
      "venue": "Journal of Scandinavian Studies in Criminology and Crime Prevention",
      "year": 2006
    },
    {
      "authors": [
        "B Matthews",
        "J. Minton"
      ],
      "title": "Rethinking one of the criminology\u2019s \u2019brute facts\u2019: The age-crime curve and the crime drop in Scotland",
      "venue": "European Journal of Criminology",
      "year": 2017
    },
    {
      "authors": [
        "H. Hart"
      ],
      "title": "Predicting Parole Success",
      "venue": "Journal of Criminal Law and Criminology",
      "year": 1924
    },
    {
      "authors": [
        "H Lakkaraju",
        "C. Rudin"
      ],
      "title": "Learning Cost-Effective and Interpretable Treatment Regimes",
      "venue": "Proceedings of the 20th International Conference on Artificial Intelligence and Statistics,",
      "year": 2017
    },
    {
      "authors": [
        "BrennanT",
        "DieterichW",
        "Ehret B"
      ],
      "title": "Evaluating thePredictiveValidity of theCOMPASRisk andNeedsAssessment System",
      "venue": "Criminal Justice and Behavior",
      "year": 2009
    },
    {
      "authors": [
        "J Carollo",
        "J Hedlund",
        "HinesM"
      ],
      "title": "Expanded Validation of a Decision Aid for Pretrial Conditional Release",
      "year": 2007
    },
    {
      "authors": [
        "S Turner",
        "J Hess",
        "J. Jannetta"
      ],
      "title": "Development of the California Static Risk Assessment Instrument (CSRA)",
      "venue": "CEBCWorking Papers",
      "year": 2009
    },
    {
      "authors": [
        "Cadigan TP",
        "LowenkampCT"
      ],
      "title": "Implementing Risk Assessment in the Federal Pretrial Services System",
      "venue": "Federal Probation",
      "year": 2011
    },
    {
      "authors": [
        "PB Hoffman",
        "S. Adelberg"
      ],
      "title": "The Salient Factor Score: A Nontechnical Overview",
      "venue": "Fed Probation",
      "year": 1980
    },
    {
      "authors": [
        "NafekhM",
        "Motiuk LL"
      ],
      "title": "The Statistical Information onRecidivism, Revised1 (SIR-R1) Scale: APsychometric Examination",
      "venue": "Correctional Service of Canada. Research Branch;",
      "year": 2002
    },
    {
      "authors": [
        "Lazarsfeld PF"
      ],
      "title": "An Evaluation of the Pretrial Services Agency of the Vera Institute of Justice",
      "year": 1974
    },
    {
      "authors": [
        "Harris GT",
        "Rice ME"
      ],
      "title": "Violence Risk Appraisal Guide (VRAG)",
      "venue": "Cutler BL, editor. Encyclopedia of Psychology and Law SAGE Publications,",
      "year": 2008
    },
    {
      "authors": [
        "RE Fan",
        "KW Chang",
        "CJ Hsieh",
        "XR Wang",
        "CJ. Lin"
      ],
      "title": "LIBLINEAR: A Library for Large Linear Classification",
      "venue": "JMach Learn Res",
      "year": 2008
    },
    {
      "authors": [
        "B. Smith"
      ],
      "title": "Auditing DeepNeural Networks to Understand Recidivism Predictions",
      "venue": "PhD thesis, Haverford College;",
      "year": 2016
    },
    {
      "authors": [
        "B Ustun",
        "C. Rudin"
      ],
      "title": "Supersparse linear integermodels for optimizedmedical scoring systems",
      "venue": "Machine Learning 2015;p",
      "year": 2015
    }
  ],
  "sections": [
    {
      "text": "OR I G I NA L A RT I C L E"
    },
    {
      "heading": "In Pursuit of Interpretable, Fair and Accurate Machine Learning for Criminal Recidivism Prediction",
      "text": ""
    },
    {
      "heading": "CarolineWang \u2217 | Bin Han \u2217 | Bhrij Patel | Feroze Mohideen | Cynthia Rudin",
      "text": "Duke University, Durham, NC 27708, USA"
    },
    {
      "heading": "Correspondence",
      "text": "Bin Han, Department of Statistical Science, Duke University, Durham, NC 27708, USA Email: bin.han@duke.edu"
    },
    {
      "heading": "Funding information",
      "text": "This study was partially supported by Arnold Ventures and the Department of Computer Science at Duke University.\nIn recent years, academics and investigative journalists have criticized certain commercial risk assessments for their blackbox nature and failure to satisfy competing notions of fairness. Since then, the field of interpretable machine learning has created simple yet effective algorithms, while the field of fair machine learning has proposed various mathematical definitions of fairness. However, studies from these fields are largely independent, despite the fact that many applications of machine learning to social issues require both fairness and interpretability.\nWe explore the intersection by revisiting the recidivism prediction problem using state-of-the-art tools from interpretable machine learning, and assessing the models for performance, interpretability, and fairness. Unlike previous works, we compare against two existing risk assessments (COMPAS and the Arnold Public Safety Assessment) and train models that output probabilities rather than binary predictions. Wepresentmultiplemodels that beat these risk assessments in performance, and provide a fairness analysis of these models. Our results imply that machine learning models should be trained separately for separate locations, and updated over time.\n\u2217Equally contributing authors.\n1\nar X\niv :2\n00 5.\n04 17\n6v 1\n[ st\nat .M\nL ]\n8 M\nay 2\n02 0"
    },
    {
      "heading": "2 WANG &HAN ET AL.",
      "text": "K E YWORD S criminal recidivism, interpretability, fairness, COMPAS, machine learning\n| Abstract ofMain Results for Criminal Justice Practitioners Our goal is to study the predictive performance, interpretability, and fairness of machine learningmodels for pretrial recidivismprediction. Machine learningmethods are known for their ability to automatically generate high-performance models (that sometimes even surpass human performance) from data alone. However, many of the most common machine learning approaches produce \u201cblack-box\u201d models\u2014models that perform well, but are too complicated for humans to understand. \u201cInterpretable\u201d machine learning techniques seek to produce the best of both worlds: models that performaswell as black-box approaches, but also are understandable to humans. In this study, we generatemultiple black-box and interpretable machine learning models. We compare the predictive performance and fairness of the machine learningmodelswe generate, against twomodels that are currently used in the justice system to predict pretrial recidivism\u2014namely, the Risk of General Recidivism and Risk of Violent Recidivism scores from the COMPAS suite, and the NewCriminal Activity andNewViolent Criminal Activity scores from the Arnold Public Safety Assessment.\nWe first evaluate the predictive performance of all models, based on their ability to predict recidivism for six different types of crime (general, violent, drug, property, felony, and misdemeanor). Recidivism is defined as a new charge for which an individual is convictedwithin a specified time frame (which we specify as six months or two years). We consider each type of recidivism over the two time periods to control for time, rather than to consider predictions over an arbitrarily long or short pretrial period. Next, we examine whether amodel constructed using data from one region suffers in predictive performance when applied to predict recidivism in another region. Finally, we consider the latest fairness definitions created by themachine learning community. Using these definitions, we examine the behavior of the interpretable models, COMPAS, and the Arnold Public Safety Assessment, on race and gender subgroups.\nOur findings and contributions can be summarized as follows:\n\u2022 We contribute a set of interpretable machine learning models that can predict recidivism as well as black-box machine learningmethods and better than COMPAS or the Arnold Public Safety Assessment for the location they were designed for. Thesemodels are potentially useful in practice. Similar to the Arnold Public Safety Assessment, some of these interpretable models can bewritten down as a simple table that fits on one page of paper. Others can be displayed using a set of visualizations. \u2022 We find that recidivism predictionmodels that are constructed using data from one location do not tend to perform as well when they are used to predict recidivism in another location, leading us to conclude that models should be constructed on data from the location where they aremeant to be used, and updated periodically over time. \u2022 We reviewed the recent literature on algorithmic fairness, but most of the fairness criteria don\u2019t pertain to risk scores, they pertain only to yes/no classification decisions. Since we are interested in criminal justice risk scores in this work, the vastmajority of the algorithmic fairness criteria are not relevant. We chose to focus on the evaluation criteria that were relevant, namely calibration, balance for positive/negative class (BPC/BNC), and balanced group AUC (BG-AUC).We present an analysis of these fairness measures for two of the interpretable models (RiskSLIM and Explainable BoostingMachine) and the Arnold Public Safety Assessment (NewCriminal Activity score) on the two-year general recidivism outcome in Kentucky. We found that the fairness criteria were approximately met for both interpretablemodels for blacks/whites andmales/females (that is, themodels were fair according to these"
    },
    {
      "heading": "WANG &HAN ET AL. 3",
      "text": "criteria). However, the Arnold Public Safety Assessment\u2019s NewCriminal Activity score failed tomeet one fairness criterion (BPC/BNC). The results on fairness were not as consistent for the \u201cOther\u201d race category."
    },
    {
      "heading": "1 | INTRODUCTION",
      "text": "Predicting criminal recidivism using statistics has been the subject of almost a hundred years of research in criminal justice, psychology, and law. Today, actuarial risk assessments are inwideuse acrossmany countries, helping judgesmake life-changing decisions in pretrial release, sentencing, and probation. Risk assessments can help reduce costs, racial disparity, and incarceration rates\u2014and these benefits have already been realized in some jurisdictions [1]. However, some of themost widely used algorithms are secret, black-boxmodels created by corporations. As a result, individuals affected by these algorithms cannot know how these decisions weremade, or whether they weremade in error. These problems resulted in various lawsuits over the last decade, and came to the fore in 2016, when investigative journalists from the nonprofit organization ProPublica claimed that the COMPAS1 black-box recidivism predictionmodel was rife with racial bias [2, 3].\nThough ProPublica\u2019s findings were not validated [4, 5, 6], the COMPAS scandal demonstrated the issues with for-profit, secret algorithms making decisions in the justice system\u2014namely, possible violations of defendants\u2019 due process rights, difficulty in ensuring that the scoreswere calculated based on correct inputs, and the lack of independent fairness or performance guarantees. It highlighted theways that systemic bias in data can be propagated into the future, andwas symptomatic of growing public distrust in the algorithms that impact our daily lives [7, 8, 9].\nTo prevent errors, prevent due process violations, allow independent validation of models, and gain public trust, we must create transparent, interpretable and fair models. Fortunately, techniques for interpretable machine learning and theories of fairness have advanced considerably over the last few years. Multipleworks have demonstrated that publicly available interpretable machine learning algorithms can perform as well as black-box machine learning algorithms [10, 11, 12]. Moreover, high-dimensional data sets on criminal recidivism have become increasingly available. However, most machine learning papers treat recidivism prediction as a toy problem to test newmachine learning algorithms. They do not consider factors such as data quality or ease of computation of model predictions, which are paramount for creatingmodels that would be useful in practice. To our knowledge, there is only one prior work [13] that jointly considers interpretability, fairness, and predictive performance; however, it does not do so in a comprehensive way and focuses primarily on the design of a new algorithm.\nBeyond the problem of model optimization, various methodological questions remain with existing risk assessment systems. First, existing systems\u2014such asCOMPAS (CorrectionalOffenderManagement Profiling System forAlternative Sanctions) and LSI-R (Level of Service Inventory Revised)\u2014are often used across various states (or even countries) with only minor normalization [14, 15]. However, populations in different states can significantly differ because the data generation process is not the same, so applying the samemodel across states may not lead to the best possible performance. Second, empirical evidence indicates that the underlying probability distribution of recidivism has changed over time inmultiple locations [16]. For instance, a significant shift in the age distribution\u2014a key predictor in many recidivism predictionmodels\u2014has been observed in NewYork [17]. Thus, rather than using a static model with uneven performance across districts, a better solutionmight be to algorithmically generate models, so that they can be trained for specific locations and retrained if recidivism distributions shift over time.\nUsing modern tools of both interpretable and black-box machine learning, we revisit the recidivism prediction problem. We define recidivism as a new charge that an individual is convicted for within a certain time frame (sixmonths\n1COMPAS stands forCorrectionalOffenderManagement Profiling forAlternative Sanctions."
    },
    {
      "heading": "4 WANG &HAN ET AL.",
      "text": "or two years). We find that (1) black-boxmodels do not perform significantly better than interpretablemodels for any of the twelve recidivism problemswe consider. (2) Interpretable models generally perform better than existing actuarial risk assessments. (3)Models do not generalizewell across regions. (4) Only a small subset of themany proposed fairness definitions can be applied to regression problems and they vary across different models. We also note that existing techniques to enforce fairness generally require non-interpretable transformations, and therefore do not workwell with interpretable models.\nThis paper is structured as follows. Section 2 describes our contributions. Section 3 discusses the evolution of risk assessment in America, the current debate over risk assessments, and briefly reviews themachine learning literature on risk assessment. Section 4 describes the study\u2019s data sources. Section 5 discusses aspects of our methodology, including the prediction problems, problem setup, and the existing risk assessments we compare against. Section 6 presents the performance of baseline, non-interpretablemachine learningmethods, while Section 7 presents the performance of interpretable machine learning methods. Section 8 examines the generalization of recidivism predictionmodels across states. In Section 9, we describe the selection of fairness metrics and assess the fairness of the interpretable models. In Section 10, we discuss broader impacts and future lines of inquiry."
    },
    {
      "heading": "2 | CONTRIBUTION",
      "text": "Ourmain contribution is a set of interpretable, risk-calibrated linear models that perform better than existing actuarial risk assessments, and predict specific crime types. Other important aspects of our contribution are as follows:\n\u2022 We consider multiple types of recidivism (general, violent, drug, property, felony, and misdemeanor) at two time scales (six-month, two-year) for a total of 12 prediction problems. \u2022 Our analysis was conducted on two criminal history data sets (one fromBroward County, Florida, and the other from the state of Kentucky), which allowed us to understand variability in model performance across locations. We found that models do not generalize well between locations, and conclude that models should be trained on data from the location where they aremeant to be used. \u2022 Wediscuss how ourmodels satisfy fairness and interpretability criteria. To our knowledge, beyond the fields of fair and interpretablemachine learning, there are few paper that discuss both fairness and interpretability with the same attention as predictive performance. \u2022 The riskmodels trained as part of this study are interpretable, and could potentially be useful in practice after a careful, location-specific evaluation of their accuracy and fairness.\nSimilar to Zeng et al. [10], we usemachine learning techniques optimized for interpretability, and address multiple prediction problems. This work is an improvement over that of Zeng et al. [10] in the following ways. We use interpretable machine learning techniques to create risk scores representing probabilities of recidivism, rather thanmaking binary predictions (the tools we use had not been not invented at the time of Zeng et al. [10]\u2019s publication). We compare with COMPAS and the Arnold Public Safety Assessment (PSA), twomodels currently used in the justice system, whereas Zeng et al. [10] compared only with other machine learningmethods. We use data obtained at the pretrial stage rather than at prison-release. Sincemany jurisdictions utilize prediction instruments to determine pretrial release, this better aligns with the use cases of risk scores. Our data come from two locations, and includemore detailed information than in Zeng et al. [10], and aremore recent than 1994. Finally, models are assessed formultiple definitions of fairness (in addition to performance)."
    },
    {
      "heading": "WANG &HAN ET AL. 5",
      "text": ""
    },
    {
      "heading": "3 | BACKGROUND",
      "text": "Algorithmic risk assessment dates back to the early 1900s [18], and is used today at various stages of the criminal justice system, such as at pretrial, parole, probation, or even sentencing. In this work, we focus on forecasting recidivism at the pretrial stage. Though some states have implemented their own tools (Virginia, Pennsylvania, Kentucky), many utilize systems produced by companies, non-profits and other organizations [19]. These externally-produced risk assessments and some of the jurisdictions that utilize them include COMPAS (Florida, Michigan,Wisconsin,Wyoming, NewMexico), the Public Safety Assessment (New Jersey, Arizona, Kentucky,2 Phoenix, Chicago, Houston), LSI-R (Delaware, Colorado, Hawaii), and the Ohio Risk Assessment System [20, 21, 22]. The United States is not alone in using actuarial risk assessments. Canada uses the Static-2002 to assess risk of violent and sexual recidivism [23]; the Netherlands uses the Quickscan to assess static and dynamic risks of recidivism [24]; the U.K. uses theOffender Group Reconviction Scale to predict reoffense while on probation [25]."
    },
    {
      "heading": "3.1 | TheDebate over Risk Assessments",
      "text": "Since the inception of actuarial risk assessments, there has been debate over whether they should be used in the criminal justice system at all. Proponents claim that statistical models reduce overall violence levels and ensure the most efficient use of treatment and rehabilitative resources by helping judges identify the individuals that are truly dangerous. A large body of evidence appears to support this claim. Various studies have shown that statistical models aremore accurate than human experts [26, 27]. Others have shown that a small percentage of individuals commit the majority of crimes [28, 29, 30], indicating that correctly identifying dangerous individuals could lead to substantial decreases in violence levels. Proponents also claim that risk assessments are instrumental to reducing racial/economic disparity, allocating social services, and reducingmass incarceration [31]. In particular, some jurisdictions have adopted risk assessments at the pretrial stage to replace cash bail, which is widely viewed as biased against poor defendants [32, 33].\nIn practice, reducing overall violence levels, mass incarceration, and racial/economic disparity through actuarial risk assessment is complex. Critics have argued that as recidivism predictionmodels always rely on racially-biased features such as arrest records, actuarial risk assessment will only exacerbate racial and socioeconomic disparity, and should therefore be abolished [34, 35]. In a well-known incident, ProPublica claimed that COMPASwas biased against AfricanAmericans because there was a disparity in false positive rates and false negative rates between African-Americans and Caucasians [36]. Follow-up research showed that this bias was likely a property of the data generation process rather than the COMPASmodel, and that even a model that only relied on age showed a similar disparity in false positives and false negatives [4]. Actuarial risk assessment might be vulnerable to feature bias, but it is important to remember that other parts of the court system (such as bail and sentencing guidelines for judges) are not immune to feature bias either\u2014they also use criminal history and arrest records. Similarly, in one of the first large-scale empirical studies, Stevenson [37] showed that in Kentucky, the use of the Arnold PSA seemingly increased disparity betweenwhites and blacks at pretrial release. Because the risk scores were applied differently by judges in different counties, it seemed that white people benefitedmore than black people in terms of pretrial release numbers\u2014but within the same county, white and black defendants saw similar increases in release. Thus, rather than eliminating the use of risk scores, using them uniformly across counties may havemade risk assessments more fair across the state.\nOthers have argued that a fundamental flaw with risk assessments is that their simple labels obscure the true uncertainty behind their predictions [7]. This may be true for currently used risk assessments, but merely underscores 2Kentucky created and implemented their own tool in 2006 but transitioned to the Arnold PSA in 2013."
    },
    {
      "heading": "6 WANG &HAN ET AL.",
      "text": "the necessity for researchers to developmodels which do quantify uncertainty. While actuarial risk assessments are not perfect, wemust remember that in the absence of risk assessments, judges can only rely on their intuition\u2014and human intuition has been shown to be less reliable than statistical models [26, 27, 33, 38].\nAnother problem is that some of the most widely used risk prediction algorithms are for-profit and secret (e.g., COMPAS 3), yielding concerns over due process rights. In the 2017Wisconsin SupremeCourt case, Loomis v. Wisconsin, Loomis challenged the use of the proprietary risk prediction software, COMPAS, on the grounds that this violated his due process and equal protection rights [3]. Yet today, there are plenty of equally accurate, transparent risk prediction tools that publish their guidelines and full models. See Table 4 in the Appendix for examples. In this article, we compare against the Arnold PSA, an interpretable and publicly available tool which is used inmultiple jurisdictions.\nThere is also a general fear that the use of risk assessments could lead to situations similar to those depicted in the movie, \u201cMinority Report\u201d. InMinority Report, individuals were punished before they committed a crime based on oracles\u2019 visions of the future. However, one of themajor principles common to American criminal justice texts [40, 41] is that individuals should be punished based on the crimes they committed in the past. This illustrates why risk assessments have played only a minor role in sentencing. In reality, risk prediction tools are most heavily used in bail, parole, and social services decisions.\nRisk scores are no \u201cmagic bullet,\u201d but abolishing risk assessment without a useful alternative plan will not solve the problems above either. Reducing feature bias requires generations of community investment; jurisdictionsmust train judges on how to use risk scores; and communities must provide treatment resources for those deemed high risk. Risk assessments and other evidence-driven practices can be an important part of this solution. In themost recent revision of theModel Penal Code, the American Law Institute has supported giving people shorter prison terms or sending them to the community through the use of risk assessment tools [42, 43]. By providing simple and transparent risk scores, we hope tomitigate the possibility that risk assessments aremiscomputed, and enable judges and defendants to fully understand their scores."
    },
    {
      "heading": "3.2 | Black-box and InterpretableMachine Learning for Predicting Criminal Recidivism",
      "text": "There is an abundance of past research on usingmachine learningmethods to predict criminal recidivism. However, many of these studies utilize black-box, non-interpretablemodels, and only optimize for predictive performance. For instance, Neuilly et al. [44] used random forests to predict homicide offender recidivism. Other black-box models applied to this problem include stochastic gradient boosting [45] and neural networks [46].\nIn comparison, there is relatively little work using interpretable machine learning techniques to forecast recidivism. It is not even clear how interpretability should be defined in this domain4 . Berk et al. [47] used classical decision trees to build a simple screener for forecasting domestic violence for the Los Angeles Sheriff\u2019s Department. In 2016, Goel et al. [48] created a simple scoring system by rounding logistic regression coefficients, which helped address stop-and-frisk for the NewYork Police Department. Zeng et al. [10] was the first work usingmodernmachine learningmethods that globally optimized over the space of sparse linear integermodels to predict criminal recidivism. Despite the range of interpretable models that have been applied to the criminal recidivism problem, a common thread among these works is that simple, interpretablemodels can do just aswell as black-boxmodels, andbetter than humans. For instance, Angelino et al. [11] found that COMPAS shows no benefit in accuracy over very simplemachine learningmodels involving age\n3WhileCOMPAS\u2019 guidelines are published and validation studies havebeenperformed, the full formsof themodels are not available and someof the validation studies do not conform to standards of open science (i.e., the validation data is not publicly available [39], or the studies\u2019 authors are affiliated with the corporations that produced themodels. 4See Section 7 for a discussion of what we consider interpretable for the domain of criminal recidivism prediction"
    },
    {
      "heading": "WANG &HAN ET AL. 7",
      "text": "and criminal history. Skeem et al. [38] showed that algorithms outperformed humans on predicting criminal recidivism in three data sets, and demonstrated that the performance gapwas especially large when abundant risk factors were considered for risk prediction.\nThe approaches outlined above achieved interpretability through trainingmodelswith interpretable forms. Another major approach is post-hoc explainability, in which a simpler model provides insights into a black-boxmodel. However, post-hoc explanations are notoriously unreliable, or are not thorough enough to fully explain the black-boxmodel [49]. Additionally, there seems to be no clear benefit of black-boxmodels over inherently interpretablemodels in terms of prediction accuracy on the criminal recidivism problem [10, 24]. Thus, for a high-stakes problem such as predicting criminal recidivism, we choose not to utilize thesemethods.\nIn fact, there have been cases in criminal justice where post-hoc explanations led to incorrect conclusions and pervasivemisconceptions aboutwhat information someof themost common recidivismmodels use. The 2016COMPAS scandal\u2014where ProPublica reporters accused the proprietary COMPAS risk scores of an explicit dependence on race [36]\u2014was caused by a flawed, post-hoc explanation of a black-box model. In particular, ProPublica reasoned that if a post-hoc explanation of COMPAS depended linearly on race, then COMPAS depended on race (controlling for age and criminal history). However, as Rudin et al. [4] demonstrated, just because an explanation model depends on a variable does notmean that the black boxmodel depends on that variable. Thus, ProPublica\u2019s reasoning was incorrect. In particular, this analysis found that COMPAS does not seem to depend linearly on some of its input variables (age), and does not seem to depend on race after conditioning on age and criminal history variables. Criminologists have also criticized the ProPublica work for other reasons [5]. Despite the flaws in the ProPublica article, it is widely viewed as being a landmark paper on fairness in machine learning.\nA notable advantage of interpretable modelling for criminal justice is that some interpretable models allow a decision-maker to incorporate factors not in the database in a way that black-boxmodels cannot. For instance, scoring systems (linear models with integer coefficients) place all of themodel inputs onto the same scale: every input receives a number of points. The points of each factor in themodel provide clarity on how important each input is relative to the others."
    },
    {
      "heading": "3.3 | FairMachine Learning",
      "text": "Fairness is a crucial property of risk scores. As such, the recidivism prediction problem is a keymotivator for many of theseworks. However, recidivism prediction is rarely the primary focus of fairness papers. Many of these papers seek to make theoretical contributions by proposing definitions of fairness and creating algorithms to achieve these definitions, using recidivism prediction as a case study [50, 51]. Others have proven fairness impossibility theorems, showing when different fairness constraints cannot be achieved simultaneously. For instance, the two fairness definitions at the heart of the debate over COMPAS\u2019 fairness (calibration and balance for positive/negative class) cannot be achieved simultaneously in nontrivial cases 5 [52, 53]. These theorems show that many fairness definitions directly conflict, so there cannot be a single universal definition of fairness [52, 54, 55]. Moreover, there is often a trade-off between performance and fairness [56, 57, 58]. The emerging consensus is that any decision about the \u201cbest\u201d definition of fairness must rely heavily onmodel characteristics and domain-specific expertise.\nThe question of what should count as fair in criminal recidivism prediction can be answered by discussion among ethicists, judges, legislators, and stakeholders in the criminal justice system. Existing American anti-discrimination law provides a general legal framework for addressing this question. Under Title VII of the Civil Rights Act of 1964, there are two theories of liability: disparate impact and disparate treatment [59]. In this article, we use the definitions of fairness 5However, by placing relaxations on the conditions, the fairness definitions can be approximately satisfied simultaneously."
    },
    {
      "heading": "8 WANG &HAN ET AL.",
      "text": "from the field of fair machine learning, as they apply directly tomachine learningmodels and aremore specific than the general legal guidelines of disparate impact and treatment. Moreover, some of the definitions of fairness proposed by the field of fair machine learning community are inspired by these guidelines. See Corbett-Davies and Goel [60] for a detailed discussion of the relationship between algorithmic definitions of fairness and economic/legal definitions of discrimination."
    },
    {
      "heading": "4 | DATA",
      "text": "In this study, we used criminal history data sets from Broward County, Florida, and the state of Kentucky, allowing us to analyze howmodels perform across regions. The Broward County data set consists of publicly available criminal history and court data from Broward County, Florida. This data set consists of the full criminal history, probational history, and demographic data for the 11,757 individuals who received COMPAS scores at the pretrial stage from 2013-2014 (as released by ProPublica [36]). The probational history was computed from public criminal records released by the Broward Clerk\u2019s Office. Though the full data set includes 11,757 individuals, this analysis includes only the 1,954 for which we could also compute the PSA.We processed the Broward data using the samemethods as Rudin et al. [4]. From the processed data, we computed various features such as number of prior arrests, prior charges, prior felonies, prior misdemeanors, etc.\nThe Kentucky pretrial and criminal court data was provided by the Department of Shared Services, Research and Statistics in Kentucky. The data came from two systems: the Pretrial Services InformationManagement System (PRIM) and CourtNet. The PRIM data contain records regarding defendants, interviews, PRIM cases, bonds, etc., that were connected with the pretrial service interviews conducted between July 1, 2009 and June 30, 2018. The data from CourtNet provided further information about cases, charges, sentences, dispositions, etc. When constructing features from the Kentucky data set, we computed features that were as similar as possible to the Broward features (e.g., prior arrests, prior charges with different types of crimes, age at current charge) in order to comparemodels between the two regions. There are several features from Broward data which could not be computed from the Kentucky data, such as \u201cage at first offense\u201d and \u201cprior juvenile charges\u201d. A limitation of the Kentucky data set is that the policies governing risk assessments changed over the period when the data was gathered, possibly impacting the consistency of the data collection.\nA difference in the data processing between the two data sets is that when constructing prediction features and predictive labels, we considered non-convicted charges in the Broward data, but considered convicted charges in the Kentucky data. The reason for this choice is sample size. The processed Broward data contains only 1,954 records, and limiting the scope to convicted charges would yield only 1,297 records. The use of convicted versus non-convicted charges between the two regionsmight explain some discrepancies in the results in Section 8, wherewe discuss the generalization of recidivism predictionmodels between states. Note that manymodels currently implementedwithin the justice system rely on non-convicted charges (for instance, counts of prior arrests), but for the applications such as bail and parole, the use of non-convicted charges could be problematic\u2014it holds individuals accountable for crimes that theymay not have committed. Please refer to the Appendix (Section 11) for more details on the data processing and a full list of features for both data sets."
    },
    {
      "heading": "WANG &HAN ET AL. 9",
      "text": ""
    },
    {
      "heading": "5 | METHODOLOGY",
      "text": "Throughout our analysis, we compare with two tools that are currently used to predict recidivism in the U.S. justice system: COMPAS (Correctional OffenderManagement Profiling for Alternative Sanctions) and the Arnold PSA (Public Safety Assessment, created by Arnold Ventures6). Althoughwewould have liked to compare against more assessment tools, many of them use data that are not publicly available, or are owned by for-profit companies that do not release their models. For a detailed discussion of the other risk assessments we considered and the features weweremissing, please consult the Appendix (Section 11).\nMore specifically, we compared our models against the Arnold PSA\u2019s NewCriminal Activity (NCA) and NewViolent Criminal Activity (NVCA) scores on the general and violent recidivism problems, respectively. Note that the timeframes and labels for prediction are important here, and our choices distinguish this study frompastworks on recidivism prediction. Let us explain the time-frames next.\nIt is important that we chose fixed time-frames for prediction, in our case, two years or six months past the current charge dates. In reality, the scores are used to assess risks during the pretrial period. However, there is a huge amount of variation in pretrial periods, which can span a few days to a few years: the average pretrial time-span in Kentucky is 109 days, and could last upwards of 3-4 years. Since the pretrial period depends on the jurisdiction, we chose to fix time-spans (of six months and two years) so that themodels do not depend on the policy used for determining how long the pretrial period would be. That way, the risk calculations we produce dependmainly on the inherent characteristics of the individual, rather than the length of the pretrial period (potentially a characteristic of the jurisdiction). Also, this way, individuals with the same propensity to commit a new crimewithin six months (or two years depending onwhich risk score) are given identical risk scores, even if they have different expected time periods until their respective trials. The six-month time-span represents an approximate length of pretrial period. The two-year time-span providesmore balanced labels, since two years provides more time to commit crimes than six months. Additionally, our evaluation metric is AUC, which is a rank-statistic, and considers relative risk rather than absolute risk; that is, an individual who actually commits crimes within two years of their current charge date should be ranked higher than an individual who does not. The relative risk within the two-year time-frame is related to the relative risk for other (shorter or longer) time-frames, allowing thesemodels to potentially generalize to varying pretrial time-frames.\nAnother important aspect of our prediction problems is the definition of recidivism we chose. We predict the occurrence of a convicted chargewithin six months/two years for Kentucky. In other words, wewould like to predict whether someonewill be arrested, within six month or two years from their current charge, for another crime that they were later convicted for. This definition potentially alleviates a due process concern: if we instead include non-convicted charges, ourmodels might bemore likely to predict whowill be arrested thanwhowill be convicted, which is tied to policing practices. For Broward, wherewe did not have conviction information for later charges, we predicted any charge within six months/two years, which is the typical approach to recidivism prediction.\nIn Broward, we directly computed Arnold PSA scores, as the Arnold PSA is publicly available. The features used by the Arnold PSA are provided in Tables 16 & 17 in the Appendix. In Kentucky, we used the unscaled Arnold PSA scores that camewith the data set.7We compared against COMPAS\u2019 Risk of General Recidivism and Risk of Violent Recidivism risk scores on the two-year general and two-year violent prediction problems, respectively (bothmodels are designed to predict recidivism within two years). The COMPAS suite is proprietary, but COMPAS General and Violent scores were provided with the Broward County data set (we do not compare against COMPAS on the Kentucky data set). The COMPASGeneral and COMPASViolent scores appear to have been developed for a parole population 6previously named the Laura and John Arnold Foundation 7In Kentucky, Arnold PSA scores are reported to judges without scaling."
    },
    {
      "heading": "10 WANG &HAN ET AL.",
      "text": "[15], but have been applied for pretrial decisions in Broward. In this study, we consider the COMPAS scores for the outcomes they were actually applied for (pretrial decisions), rather than the outcomes they were developed for (parole decisions).\nIn Sections 6 and 7, we compare the performance of black-box and interpretable algorithms on the Broward and Kentucky data sets. We caution readers against comparing an algorithm\u2019s performance in Broward with its performance in Kentucky. An algorithm\u2019s differences in performance between the data sets could be attributed to the many differences between the two regions. For instance, the Broward data set is at the county level while the Kentucky data set is at the state level. As the Kentucky data is at the state level, it embeds diverse information about 120 counties (e.g., demographics, legislation, culture, local policing practices). Thus, in Sections 6 and 7, the comparisons between baseline models and interpretable models are conducted within each data set. In Section 8, we discus in detail the regional differences between Broward County and Kentucky, and present a set of experiments that illustratemodel performance gaps resulting from these regional differences."
    },
    {
      "heading": "5.1 | Prediction Labels",
      "text": "In addition to two-year general recidivism and two-year violent recidivism\u2014the two types of criminal recidivism considered by COMPAS and the PSA\u2014we computed recidivism prediction labels specific to various crime types, such as property, drug related recidivism and recidivismwith felony or misdemeanor level charges.8 Note that an individual could have multiple positive labels, indicating that the newly committed crime involves multiple charge types. We defined recidivism as a recorded charge within a certain time frame. Out of all the possible recidivism prediction tasks we considered, we selected the six most balanced: general, violent, drug, property, felony, and misdemeanor. To investigate the effect of temporal scale on predictive performance, we generated these six tasks using the timewindows two-years and six-months after the current charge date (or release date, if the individual went to prison for their current charge), for a total of twelve tasks. The summary of prediction tasks and the base rate of recidivism for each task is provided in Table 1.\n8For clarity, we apply the typewrite font to indicate the prediction tasks."
    },
    {
      "heading": "WANG &HAN ET AL. 11",
      "text": ""
    },
    {
      "heading": "5.2 | Problem Setup",
      "text": "Due to the binary nature of recidivism tasks, we approached these prediction problems as binary classification problems, but do not binarize the final predicted probabilities/scores of themachine learningmodels for the following reasons. First, existing risk scores are usually nonbinary. For instance, the Arnold PSA\u2019s unscaled NewCriminal Activity (NCA) score takes integer values from 0 through 13, while the COMPAS Risk of Recidivism and Risk of Violent Recidivism scores take on integer values from 1 through 10 [15, 20]. Second, we want to create more nuanced risk scores both by predicting highly-specific types of recidivism, (in addition to coarser categories like general recidivism), and by presenting non-binary scores which reflect a range of risk values.\nSince the predictions are nonbinary, we use Area Under the Curve (AUC) as our evaluationmetric. This decision also impacts the fairness metrics we assess, which we discuss in Section 9. We applied nested cross validation process to train themodels. Please refer to the Appendix (Section 11) for the details."
    },
    {
      "heading": "6 | BASELINE MACHINE LEARNING METHODS",
      "text": "To provide a basis of comparison for the interpretablemodels (presented in Section 7), we evaluated the performance of six common, non-interpretable machine learning methods in this section. We present the baseline prediction results for Broward and Kentucky in Tables 7 and 8 respectively. Baselinemodels and descriptions are provided below. The tuned hyperparameters and packages used for each problem are provided in the Appendix (Section 11).\n\u2022 `2 Penalized Logistic Regression: To prevent over-fitting, there is an `2 penalty term on the sum of squared coefficients in the loss function for logistic regression. Although this method produces linear models, we consider `2-penalized logistic regression to be non-interpretable because if the number of input features is large, there could be a large number of nonzero terms in themodel. \u2022 `1 Penalized Logistic Regression: To prevent over-fitting, there is an `1 penalty term on the sum of absolute values of coefficients in the loss function for logistic regression. This algorithm creates sparser models than `2 penalized logistic regression. Notice that the sparsity of the model depends on the magnitude of the penalty andmust be balancedwith consideration of prediction performance. In our experiments, `1 models with Broward data were sparse yet maintained good predictive performances. However, the best `1 models with Kentucky data still had toomany features, which made it difficult to interpret the results. Therefore, we classified `1-penalized logistic regression as a non-interpretable algorithm. \u2022 SVMwith a Linear Kernel [61]: An algorithm that outputs a hyperplane that separates two classes bymaximizing the sum ofmargins between the hyperplane and all points. Incorrectly classified points are penalized. Although SVMwith linear kernel yields a linear model, the concerns with `1 and `2 penalized logistic regressions apply here as well: the number of nonzero terms could be large, making it difficult to interpret themodel. \u2022 Random Forest [62]: An ensemblemethod that combines the predictions of multiple decision trees, each of which is trained on a bootstrap sample of the data. The implementation we use combines individual trees by averaging the probabilistic prediction of each tree. Random Forest is usually considered a black-box classifier because it is difficult to understand the individual contribution of each feature (which can be found inmany trees), and the joint relationship between features. \u2022 Boosted Decision Trees [63]: An ensemblemethodwhere a sequence of weak classifiers (decision trees) are fit to weighted versions of the data. Similar to random forest, boosted decision trees produce black-boxmodels because it is difficult to understand the joint relationships of the features. We use the XGBoost implementation [64]."
    },
    {
      "heading": "12 WANG &HAN ET AL.",
      "text": "Major Findings: We found that all baseline machine learning algorithms performed similarly across recidivism problems for the Kentucky data set. We also found that models performed better on the six-month prediction problems than on the two-year problems on Kentucky data, but not on Broward data. These findings will be discussed throughout the following subsections."
    },
    {
      "heading": "6.1 | Broward Baseline Results",
      "text": "Table 7 in the Appendix contains the performance of baseline algorithms on the Broward data; the results are visualized in Figure 1 (presented below). We noticed that no algorithm consistently performs better than the others. Simple linear models can even outperform black-boxmodels in some prediction problems. For instance, in the two-year prediction problems, `2-penalized logistic regression and LinearSVM tie in performance for the general recidivism prediction. XGBoost performs the best in violent and property prediction problems. `1-penalized logistic regression has the best performance in drug and felony prediction tasks, while `2-penalized logistic regression has the best performance in misdemeanor recidivism prediction. The largest performance gap is 5.1%, from property recidivism prediction.\nIn the six month prediction problems, we see the same phenomenon that no single model dominates the others in performance. Overall, the performance gaps across baseline models for the general, felony, and misdemeanor prediction tasks are small, while other prediction problems have larger gaps.\nF IGURE 1 Visualizations of Broward baseline results from Table 7 in the Appendix. Within each prediction problem, all algorithms performed similarly. No single algorithm consistently outperformed others."
    },
    {
      "heading": "6.2 | Kentucky Baseline Results",
      "text": "In Kentucky, complex and nonlinear baselines perform slightly better than linear models (see Table 8 in the Appendix and Figure 4, which is presented below), potentially due to the larger size of the Kentucky data set (1,956 records in Broward county versus 250K records in Kentucky). In particular, Random Forest and XGBoost uniformly perform slightly better than all the other algorithms on all prediction tasks, over both time periods we examined. XGBoost performs the best on all tasks. However, performance gaps, across all prediction problems and in both time frames, are very small. Thus, we conclude that all the baseline algorithms perform similarly over the Kentucky data set . One thing we noticed from the Kentucky results is that all algorithms perform slightly better on the six-month recidivism period"
    },
    {
      "heading": "WANG &HAN ET AL. 13",
      "text": "than on the two-year period.\nF IGURE 2 Visualizations of Kentucky baseline results from Table 8 in the Appendix. Random Forest and XGBoost consistently perform better than other models, but the results are similar across all models."
    },
    {
      "heading": "7 | INTERPRETABLE MACHINE LEARNING METHODS",
      "text": "For recidivism prediction, we considered several different types of interpretable machine learning methods with different levels of interpretability, ranging from scoring systems to decision trees, to additive models. Since the Burgess model in 1928 [65], recidivism risk assessments have traditionally been scoring systems, which are sparse linear models with positive integer coefficients. A scoring system can be visualized as a simple scoring table or set of figures. There have only recently been algorithms designed to optimally learn scoring systems directly from data, without manual feature selection or rounding. Scoring systems have several advantages: they allow an understanding of how variables act jointly to form the prediction; they are understandable by non-experts; risks can be computed without a calculator; and they are consistent with the form ofmodel that criminologists have built over the last century, where \u201cpoints\u201d are given to the individual, and the total points are transformed into a risk of recidivism. Further, outside information (such as risk factors that are not in any database) can be more easily incorporated into the risk score: it is much easier to determine howmany points to assign to a new factor if the points are integer-valued for the known risk factors (e.g., we could choose to subtract three points for drug treatment, to counteract four points of past drug-related arrests).\nWhile scoring systems appear to be the accepted standard for interpretability in the domain of criminal justice, imposing the constraints of linearity, sparsity, and integrality of coefficients could potentially be strong enough to reduce accuracy. Thus, we also consider modern algorithms that satisfy a subset of the conditions of interpretability (sparsity in features, ability to visualize/explain any variable interactions, linearity, integer coefficients). Specifically, we tested four interpretable machine learning algorithms: Classification and Regression Trees (CART), Explainable BoostingMachine (EBM), Additive Stumps, and RiskSLIM (Risk-Calibrated Supersparse Linear IntegerModels). Algorithm specifics are articulated below and the tested hyperparameters are provided in the Appendix (Section 11). We also tested two existing risk assessments\u2014the Arnold PSA and COMPAS\u2014and compared their performances to both baseline and interpretable machine learningmodels.\n\u2022 Classification and Regression Trees (CART) [62]: Amethod to create decision trees by continuously splitting input"
    },
    {
      "heading": "14 WANG &HAN ET AL.",
      "text": "features on certain values until a stopping criterion is satisfied. CART constructs binary trees using the feature and threshold that yields the largest information gain at each node. We constrain themaximum depth of the tree to ensure that it does not use toomany features. CARTmodels are nonlinear. They cannot bewritten as scoring systems, but can bewritten as logical models. \u2022 Explainable BoostingMachine (EBM) [12]: An algorithm that uses boosting to train Generalized AdditiveModels with a few interaction terms (GA2Ms). The contribution by each feature and feature interaction pair can be visualized. Themodels are interpretable andmodular, thus editable by experts. Themodels are generally not sparse, and cannot be written as scoring systems. \u2022 RiskSLIM [66]: An algorithm that generates sparse linear models with integer coefficients that have risk-calibrated probabilities. Themodels generated by RiskSLIM have form similar to that of models used in criminal justice over the last century. \u2022 Additive Stumps: An interpretable variation on `1-penalized logistic regression: for each feature, we generate multiple binary stumps (this pre-processing technique is discussed further in the next section), and apply `1- penalized logistic regression to these stumps. Ideally, the featureswill havemonotonically increasing (or decreasing) contributions to the estimated probability of recidivism. Models constructed using this method generally use fewer features than those constructed with vanilla `1-penalized logistic regression. Thesemodels are flexible and nonlinear. Thesemodels also cannot bewritten as scoring systems because they are not sparse in the number of nonlinearities. \u2022 Arnold PSA [20]: A widely-used, publicly available, interpretable risk assessment system that consists of three scores: New Criminal Activity (NCA), New Violent Criminal Activity (NVCA), and Failure to Appear (FTA). We compare against the NCA for the general recidivism problem, and against the NVCA for the violent recidivism problem, on both two-year and six-month time scales. The NCA has 7 factors, while the NVCA has 5 factors. \u2022 COMPAS [15]: A widely-used risk assessment system that consists of several scores, including the three that we study: Risk of General Recidivism (COMPASGeneral), Risk of Violent Recidivism (COMPASViolent), and Risk of Failure to Appear. We compare against the COMPASGeneral score for the two-year general recidivism problem, and compare against the COMPASViolent score for the two-year violent problem.\nMajor Findings: Overall, the best interpretable models performed approximately as well as the best black-boxmodels on both regions and both prediction time periods we considered."
    },
    {
      "heading": "7.1 | Pre-processing Features into Binary Stumps",
      "text": "Weperformed a data pre-processing technique for two of the interpretable machine learning algorithms: RiskSLIM and Additive Stumps. This technique consists of transforming all original features into binary stumps (dummy variables) using Equation 1. Pre-processing the features into stumps allows us to include nonlinear interactions between the features (e.g. age, criminal history) and labels. It also allows us to visualize each Additive Stumps model as a set of monotonically increasing (or decreasing) curves.\nFormally, stumps are binary indicators, which are created by splitting features at pre-specified thresholds. For a featureX (j ), and a set of threshold values K \u2208 \u00d2, we generate decreasing stumps S (j )\nk for all k \u2208 K as follows:\nS (j ) k =  1, forX (j ) \u2264 k 0, else\n(1)"
    },
    {
      "heading": "WANG &HAN ET AL. 15",
      "text": "We can generate increasing stumps analogously by substituting \u2265 for \u2264 in the definition above. The rationale behind the naming convention is as follows. Linear models constructed from increasing (respectively, decreasing) stumps have the nice property that if one sums the contribution from all stumps corresponding to a fixed original feature (i.e., f (X (j )) = \u2211\nk \u2208K ck S\n(j ) k for the featureX (j )), and the coefficients ck aremostly non-negative 9, the resulting function\nf (X (j )) is monotonically increasing (respectively decreasing), which is desirable for interpretability. More concretely, the \u201cage_at_current_charge\u201d feature ranges from 18 to 70 in our data. For all age-related features, we construct decreasing stumps for k = {18, 19, ..., , 60}. We chose decreasing stumps for age features because based on past studies [e.g., 4, 67] and criminological theory [16, 68, 69] , the probability of recidivism decreases with age. On the other hand, intuitively, the probability of recidivism should increase as criminal history increases. Thus, we construct increasing stumps for the remaining features (which relate to criminal history).\nTo select a collection of stumps for the RiskSLIM and Additive Stumpsmodel, we selected threshold values for all features by examining each feature visualization from EBM and choosing the threshold values that correspond to sharp drops in the predicted scores."
    },
    {
      "heading": "7.2 | Broward Prediction Results for InterpretableModels",
      "text": "Table 9 in the Appendix and Figure 3 show the results of interpretablemodels on the Broward data set. For all prediction problems in both two-year and six-month prediction periods that we examined, we observed that CART consistently performedworse than other algorithms. Additive Stumps and EBMperformed similarly on all the prediction tasks and outperformed other models, including the Arnold PSA and COMPAS, onmost of the prediction tasks.\nThe performances of the best interpretable models are very similar to that of the best baseline models\u2014this is true for each of the prediction problems we considered. The AUC gaps between the best interpretable models and best baselinemodels for all two-year prediction tasks range from 0.3% to 1.7% in absolute value, and range from 0.2% to 2.6% for six-month prediction tasks. The twomaximumprediction gaps, 1.7% and 2.6%, both come from drug recidivism prediction tasks. Prediction gaps from all other problems are smaller than 1%.\nF IGURE 3 Broward interpretable model results.\n9For decreasing (respectively increasing) stumps, if the coefficient for the largest (respectively smallest) stump is negative, the function f will still bemonotonic because the negative value will be subtracted from all values of the remaining stumps"
    },
    {
      "heading": "16 WANG &HAN ET AL.",
      "text": ""
    },
    {
      "heading": "7.3 | Kentucky Prediction Results for InterpretableModels",
      "text": "The Kentucky prediction results are provided in Table 10 in the Appendix, and visualized in Figure 4. For all prediction problems in both time frames we considered, CART, EBM, and Additive Stumps all had similar performances. RiskSLIM had relatively lower results compared to other interpretable models. All interpretable models performed better than the Arnold PSA, with the exception that the Arnold PSA performed slightly better (0.3%) than RiskSLIM on two-year general recidivism. Oncemore, we observed that the best interpretable models can perform approximately as well as the best black-boxmodels (XGBoost). For the two-year prediction tasks, the differences in performance between the best interpretable and the best black-boxmodels ranged from 0.7% to 0.9% in absolute value; for six-month problems, the difference ranged from 0.4% to 1.5%.\nF IGURE 4 Kentucky interpretable model results.\nSummary of Interpretable Models\u2019 Results: We found that the best interpretable models performed approximately as well as the best black-boxmodels, on both data sets and both time periods we considered, which is consistent with previous studies on other data sets [10]. The best interpretablemodels possess the advantage of being transparent and interpretable, allowing judges and defendants a better understanding of the predictions that themodel outputs."
    },
    {
      "heading": "7.4 | Tables and Visualizations of InterpretableModels",
      "text": "Each of the interpretable machine learningmethods producesmodels that can be visualized, either as a decision tree (CART), scoring table (RiskSLIM), or as a set of visualizations (EBM, Additive Stumps). In this section, we present these tables and visualizations for EBM, Additive Stumps and RiskSLIM, to give a clearer understanding of each model\u2019s interpretability. Here we used the two-year general recidivism prediction problem on Kentucky data as an example."
    },
    {
      "heading": "7.4.1 | EBMModels",
      "text": "The EBMpackage provides visualizations for each feature in the data set along with a bar chart of feature importance, both of which are displayed in an interactive dashboard. The dashboard allows users (potentially judges) to see the scores corresponding to each bar or line by hovering themouse over it. EBMmodels are not sparse in the number of features, so there could be visualizations for all features. Here, we show screenshots of the bar chart and visualizations"
    },
    {
      "heading": "WANG &HAN ET AL. 17",
      "text": "for the three most important features. EBM visualizations are similar to those from Additive Stumps, in that each feature\u2019s contribution to the score can be displayed separately. However, EBM scores do not tend to bemonotonically increasing or decreasing in each feature.\nF IGURE 5 Visualizations from EBMon two-year general recidivism. Top left: overall importance of each feature, ranked from themost important variable to least important. Remaining three: visualization for the contribution of the feature to the overall score (top) and histograms of feature values to show the distribution (bottom). Features contributions are visualized as bar charts if the feature takes binary value. The shaded grey area represents the confidence region. We see that as values get larger, there is more uncertainty in the predictions, whichmay be because we have fewer data points for such large feature values."
    },
    {
      "heading": "7.4.2 | Additive Stumps",
      "text": "Additive Stumpsmodels are constructed by thresholding the original features, such as age or criminal history, into binary stumps, followed by running `1-penalized logistic regression on the stumps. Choosing an appropriate regularization value for `1-penalized logistic regression can give us amodel that is sparse in the number of original features\u2014despite the fact that the regularization is directly on the stumps, not on the original features. For the Kentucky two-year general recidivism problem, the final model contains 28 stumps plus an intercept. These stumps are rooted under only 14 original features. Visualizations of the contributions for these 14 features are presented in Figure 6. Table 11, containing a scoring table that includes all 28 stumps plus an intercept, is provided in the Appendix (Section 11)."
    },
    {
      "heading": "18 WANG &HAN ET AL.",
      "text": "F IGURE 6 Visualizations of the total contribution for each of the original features in the Additive Stumpsmodel on two-year general recidivism. The contribution from each stump feature is the estimated coefficient from `1-penalized logistic regression."
    },
    {
      "heading": "7.4.3 | RiskSLIM",
      "text": "RiskSLIM produces scoring tables with coefficients optimized to be integers (\u201cpoints\u201d), whichmakes the predictions easier to calculate and interpret for users, such as judges. The total points are translated into probabilities using the logistic function provided at the top of the table. By examining a RiskSLIM model, users can easily identify which features contribute to the final score and by howmuch. We provide scoring tables in Table 2 for two-year general recidivism prediction on both Broward and Kentucky data sets. More tables are provided in the Appendix (Section 11).\nWe noticed that for each prediction problem, almost all five of the cross validation folds for the RiskSLIM algorithm yielded the same model on the (larger) Kentucky data set. In more detail, for Kentucky two-year drug and violent recidivism prediction problems, all five RiskSLIMmodels produced during cross validationwere identical. For the rest of the prediction labels, four out of five cross validationmodels were the same. For the six-month recidivism prediction problems, the misdemeanor prediction problem resulted in five identical RiskSLIMmodels, and the violent recidivism"
    },
    {
      "heading": "WANG &HAN ET AL. 19",
      "text": "prediction problem had four models that were the same. The fact that the Kentucky RiskSLIMmodels are often the same, despite being trained on different (albeit overlapping) subsets of data, suggests that they are robust to the exact subsample used for training."
    },
    {
      "heading": "8 | RECIDIVISM PREDICTION MODELS DO NOT GENERALIZE WELL ACROSS REGIONS",
      "text": "It is common practice for recidivism prediction systems to be applied across states, or even countries, with only minor tuning on local populations. Implicit in this practice is the assumption that models trained on data from one collection of locations will performwell when used in another collection of locations\u2014i.e., that models generalize across locations. For instance, the Arnold PSA, which was developed on 1.5 million cases from approximately 300 U.S. jurisdictions, has been adopted in the states of Arizona, Kentucky, New Jersey, andmany large cities including Chicago, Houston, Phoenix, etc. [20]. These systems have remained in place for years without any updates.\nHowever, based on our experimental results, we conjecture that different locations would benefit from specialized models that conform to the specific aspects of each location. For instance, let us briefly compare the state of Kentucky and Broward County in Florida. The demographics are completely different: Kentucky is not a diverse state (87.8% white, 7.8% black, and 4.4% other groups in 2019 [70]), whereas Broward County is more racially diverse (62.3%, white; 17.1%Hispanic or Latino; 12.2% black or African American; 5.07%Asian and other groups [71]. The geographies of the locations are drastically different as well: Kentucky is an interior state located in the Upland Southwith a humid subtropical climate, whereas Broward County is at the eastern edge of Florida with a tropical climate. Several studies have indicated an association between climate (temperature, humidity, and precipitation) and crime [72, 73, 74]. There aremany other factors that differ between the locations thatmight affect the generalization of the recidivism prediction models, such as different local prosecution practices, laws and theway they are administered, social service programs, local cultures, educational systems, and judges\u2019 views.\nBecausemodels tend to be used broadly across locations, in this section we aim to investigate howwell predictive models generalize between the two locations for which we have data. We trainedmodels on Kentucky and tested on Broward (and vice versa). We lookedmore closely at age, and examined how the joint probability distribution of age and recidivism differs between Broward and Kentucky. We focused on age because of its important relationship to recidivism [67, 69, 75].\nMajor Findings: Our analysis shows that models do not generalize well across regions, and the joint probability distribution of age and recidivism varies across states. Therefore, we suggest that different models be constructed in"
    },
    {
      "heading": "20 WANG &HAN ET AL.",
      "text": "different regions, and be updated periodically."
    },
    {
      "heading": "8.1 | Training onOne Region and Testing on theOther",
      "text": "In order to construct models on one region and test them on the other, we only used the shared features from both data sets. Nested cross validation was used to train both themodels that were trained in one region and tested in the other, and themodels that were trained and tested in the same region. More details about this procedure can be found in Section 11.7 in the Appendix. Table 12 and Table 14 in the Appendix respectively show the performance of models trained on Kentucky and tested on Broward, and models trained on Broward and tested on Kentucky. Table 13 and Table 15 respectively show the performance of models trained and tested on Broward, andmodels trained and tested on Kentucky.\nComparing Table 12 with Table 13, we observed that there is an overall decrease in model performance when models were trained in Kentucky and tested on Broward. For instance, for the two-year general recidivism problem, the performances drop between 3.5% to 6.0% on the baseline models. A similar pattern can be observed for the interpretable models. Conversely, when we trained models on Broward and tested on Kentucky, we observed even larger performance decreases from themodels trained and tested on only Kentucky (compare Table 14 to Table 15). For the two-year general prediction task, performance gaps from baselinemodels range between 5.1% and 8.6%, while the gaps range from 4.6% to 12.0% for interpretable models.\nThrough this experimentation, we concluded that for at least the twelve prediction problems in our setup, models do not generalize across states. This could be attributable to differences in the joint probability distribution of features and outcomes between locations. To understand the difference in these distributions more closely, we examine the age feature."
    },
    {
      "heading": "8.2 | Age-Recidivism Probability Distributions by Region",
      "text": "Age has traditionally been a highly predictive factor for recidivism [67, 69, 75]. Therefore, differences in the age distributions between two regions could significantly impact amodel\u2019s ability to generalize between regions.\nConsider the general recidivism problem as an example. In Kentucky, the probability of general recidivism for both six-month and two-year prediction periods peaks for individuals aged around the early to mid 30s and then decreases as age increases. In Broward County, the age distribution for the corresponding general recidivism problem is substantially different. From Figure 7, the probabilities seem to peak around ages 18-29, and then decrease after age 29. There are less data for higher ages, causing greater variance in the probabilities. For the violent recidivism problem, please refer to Figure 11 in the Appendix (Section 11).\nAdditionally, there is a large gap in the probability magnitudes between the two regions. For instance, the probabilities of general recidivism from the Broward data set can exceed 0.5, while the probabilities of general recidivism from Kentucky data are all less than 0.4. Thus, the populations of individuals from Broward and Kentucky who recidivate are different with respect to age.\nThis difference is directly manifested in the interpretable models presented in Section 7.4. We found that the selection of features differs between interpretable models trained on Broward and Kentucky data. For instance, referring to the simple RiskSLIMmodels listed in Section 11.8 in the Appendix, which show themost important features in each prediction problem, we noticed that with Broward data, almost all prediction problems contain at least one age feature, either \u201cage at current charge\u201d or \u201cage at first offense.\u201d This suggests that age is important in predicting recidivism across different problems trained on the Broward data. However, none of the RiskSLIM models trained\nWANG &HAN ET AL. 21\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\npr ob\nab ilit\ny\nlocation = KY\ntwo_year six_month\n18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 age_at_current_charge\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\npr ob\nab ilit\ny\nlocation = FL"
    },
    {
      "heading": "General Recidivism",
      "text": "F IGURE 7 Probability of recidivism v. age at current charge\u2014general recidivism\non the Kentucky data set use age features. Almost all the models use \u201cprior arrest\u201d features, reflecting the fact that Kentucky recidivism prediction problems rely more on prior criminal history information than on age."
    },
    {
      "heading": "9 | FAIRNESS",
      "text": "In this section, we conduct a technical discussion of a small fraction of the various fairness definitions that have emerged recently, and an evaluation of howwell the interpretablemodels satisfy themon theKentucky data set. Wefirst describe our rationale for selecting fairness definitions (calibration, balance for positive/negative class, and balanced group AUC). Next, we evaluate howwell the Arnold PSA, COMPAS, EBM (the best-performing interpretable models) and RiskSLIM (themost interpretable andmost constrainedmodels) satisfy these definitions on the two-year general recidivism and two-year violent recidivism problems in Kentucky. Finally, we discuss how current fairness-enforcement procedures interact with interpretability.\nMajor Findings: Empirically, we found no egregious violations of the three fairness definitions (group calibration, BPC/BNC, and BG-AUC) for both interpretable machine learning models we assessed (EBM and RiskSLIM) for the two-year general recidivism problem on the Kentucky data set. We found that the Arnold NCA raw score violated one of the fairness definitions (BPC/BNC). Overall, we observed a larger gap in fairness (for all three fairness measures we examined) between the largest and smallest sensitive groups, than between black andwhite sensitive groups. We also note that existing techniques to enforce fairness generally require non-interpretable transformations, and therefore do not work well with interpretable models."
    },
    {
      "heading": "22 WANG &HAN ET AL.",
      "text": ""
    },
    {
      "heading": "9.1 | Selection of Fairness Metrics: Calibration, Balance for Positive/Negative Class, Balanced Group AUC",
      "text": "As discussed in Section 5.2, we do not wish to consider binary risk scores in this study. This decision limits us to amuch smaller class of fairness definitions (e.g., statistical parity would not be relevant). Below, we summarize the definitions that apply to regression that we do not consider and the reasons why:\n\u2022 Fairness through unawareness states that amodel should not use any sensitive features [55]. However, if there are proxies for sensitive features present in the data set, themodel can still learn an association between a sensitive group and the outcome. Fairness through unawareness could be used if one decides that a proxy feature is permissible to use (e.g., if one decided that age could be used, despite its correlation with race), but we do not presume that this is what is desired for this application. Of course, if fairness through unawareness is desired, it is easy to construct models that satisfy this definition. \u2022 Individual fairness intuitively requires that \u201csimilar\u201d individuals are treated \u201csimilarly\u201d by themodel\u2014individuals with similar features should be given similar model scores. This type of fairness requires manually (and thus subjectively) defining a notion of similarity between individuals [76]. This type of subjective choice goes beyond the scope of this paper.\nOnce we limited ourselves to real-valued outcomes and eliminated the above definitions, only a few definitions remained. In a literature search for non-binary fairness definitions, we found the following: calibration, balance for positive class/balance for negative class (BPC/BNC) and balanced group AUC (BG-AUC).\nBelow,G denotes a (categorical) sensitive attribute such as race, and gi denotes one of the sensitive groups inG (e.g. African-American, Caucasian, and Hispanic, for the sensitive attribute of race).Y \u2208 {0, 1} denotes the ground-truth label (recidivism status) and S denotes the predicted score from amodel.\n\u2022 Calibration: We consider two notions of calibration. The first, group calibration, requires that for all predicted scores, the fraction of positive labels is the same across all groups. Mathematically, group calibration over the sensitive attributeG requires:\nP (Y = 1 |S = s,G = gi ) = P (Y = 1 |S = s,G = gj ), [i , j\nwhere s is the given value of a risk score.10 In practice, it is common to bin the score S if there aremany possible values. The second,monotonic calibration, requires that if s1 < s2, then P (Y = 1 |S = s1) < P (Y = 1 |S = s2). 11 These types of calibration are of particular concern to designers of current recidivism riskmodels. Group calibration means that a risk score holds the same \u201cmeaning\u201d for each race. Monotonic calibration means that if the score increases, the risk also increases. These notions are important because human decision-makers expect risk scores to have these intuitive properties (but not all algorithms produce calibratedmodels) [77]. \u2022 Balance for Positive Class (BPC) requires that for all individuals with a positive label, the expected values of the\n10In the case where scores are binary, group calibration is equivalent to requiring conditional use accuracy equality. 11We note that a real-valued score S between 0 and 1 is well-calibrated if P (Y = 1 |S = s) = s . Well-calibration says that the predicted probability of recidivism should be the same as the true probability of recidivism [55]. Although well-calibration is the definition of calibration that is standard in the statistics community, we consider monotonic-calibration here because any score that is monotonically-calibrated can be transformed to bewell-calibrated."
    },
    {
      "heading": "WANG &HAN ET AL. 23",
      "text": "predicted scores are the same across groups. Mathematically, a risk score S satisfies BPC if:\nE [S |Y = 1,G = gi ] = E [S |Y = 1,G = gj ], [i , j .\nSimilarly, a risk score S satisfiesBalance for Negative Class (BNC) if:\nE [S |Y = 0,G = gi ] = E [S |Y = 0,G = gj ], [i , j .\nBPC and BNC differ only in the labelY .12 Intuitively, BPCmeans that the average score for recidivists is the same in each group, while BNCmeans that the average score for non-recidivists is the same in each sensitive group. BPC/BNC is an intuitive notion of fairness, which says that it is permissible to give consistently higher (respectively lower) scores to individuals who truly belong to the positive (respectively negative) class. However, BPC/BNC limits the set of attributes where it is permissible to \u201cdiscriminate\u201d between individuals, to the labelY . Suppose the count of prior offenses is an important feature for a recidivism prediction model\u2014higher prior counts lead to higher scores. This is a reasonable model assumption because a higher prior count is correlated with higher recidivism rates. If on average, African-Americans have higher prior counts than Caucasians, the model will not satisfy BPC/BNC. For amodel to satisfy BPC/BNC, it must give the same average score to individuals from a certain race andwith a certain recidivism label, regardless of distributional differences in prior counts. Those who believe that prior counts and arrests are racially biased against African-Americansmight find this a desirable property of a fairness definition. On the other hand, thosewho find this undesirable can fix this by conditioning on the prior counts attribute as well. \u2022 Balanced Group AUC (BG-AUC) requires that the AUC of the risk score is the same for each sensitive group. This definition is our adaptation of overall accuracy equality [56], which asks that the score\u2019s accuracy is the same for each sensitive group. Our risk scores are not binary sowe do not assess accuracy in this work, but assessing the AUC for each group is the natural analog.\nSensitive attributes: The two sensitive attributes that are available in the Kentucky data sets are race and gender. In the Kentucky data set, all individuals are partitioned into Caucasian, African-American, Indian, Asian, and Other, but we group the Indian and Asian attributes into Other because there are very few individuals with these attributes. See Table 19 for the distribution of sensitive attributes in Kentucky. The Kentucky data set also partitions individuals into the genders Female and Male. To summarize,\nraces in Kentucky = {Caucasian, African-American, Other} sexes in Kentucky = {Female, Male}.\nWe remark that the binary versions of calibration and BPC/BNC definitions conflicted during the COMPAS scandal. Investigative journalists from ProPublica found that COMPAS had a higher false negative rate for Caucasians and a higher false positive rate for African-Americans.13 In response, however, Northpointe claimed that COMPAS scores were calibrated.\nKleinberg et al.\u2019s [52] impossibility theoremdemonstrated that the conflict between calibration andBPC/BNCholds in general. In particular, they show that if a model does not satisfy either of the two trivial cases\u2014amodel that always\n12In the binary case, BPC/BNC is equivalent to equalized odds [50], which requires that false positive rates and false negative rates are equal for each group. 13To determine false negative/positive rates, ProPublica binned COMPAS scores into binary scores,"
    },
    {
      "heading": "24 WANG &HAN ET AL.",
      "text": "makes perfect predictions, or a data set where the base rates of recidivism are equal for each sensitive group\u2014then the model cannot satisfy all three fairness definitions simultaneously. However, a relaxed version of the theorem states that, if either of the two conditions approximately hold (approximately perfect predictions, or approximately equal base rates), then the three fairness definitions can be approximately satisfied at the same time.\nFigure 10 in the Appendix shows that the base rates for all sensitive attributes under each prediction problem on the Kentucky data. We noticed that for the two-year general and violent recidivism problems, the base rates are similar to each other (less than 3% in differences) across gender and race categories (except for \u201cOther\u201d). Given the relaxed version of the impossibility theorem, together with the relaxed criteria of the 3% difference we considered, we expect that the three fairness definitions can all be approximately satisfied."
    },
    {
      "heading": "9.2 | Fairness Results",
      "text": "We assessedmodel fairness only on the Kentucky data because the Broward data has a limited sample size, potentially making the fairness results unreliable. (We attempted the evaluation on Broward data, but conditioning on race/gender and the true label/score in the Broward data led to subgroups that were too small, and therefore noisy results.) We compared the interpretablemodels, EBM and RiskSLIM, to the Arnold PSA on Kentucky. EBMhas the best performance onmost of the prediction problems on the Kentucky data set. RiskSLIM performs relatively worse, but is considerably simpler as there are nomore than five features in eachmodel, coefficients are integers, and themodel is linear.\nWe evaluated the two-year general and two-year violent problems, as they are the primary problems that the Arnold PSA is used for. For the two-year general problem, we evaluated the unscaled Arnold NewCriminal Activity (NCA) score; for the two-year violent problem,we assessed the unscaledArnoldNewViolent Criminal Activity (NVCA) score. Although Arnold Ventures provides a table to scale the Arnold scores, in Kentucky, judges are presented with the unscaled scores along with a categorization of the scores as low, medium, and high risk. Results for two-year general recidivism are presented directly in this section; results for two-year violent recidivism can be found in the Appendix.\nNote that each of the fairness conditions implicitly has a threshold parameter. For instance, the fairness condition BPC is strictly satisfied if themean scores betweenmultiple groups are \u201cequal\u201d to each other. However, whether two numbers are approximately equal is subjective and requires a threshold. So onemust always issue a disclaimer when stating that any of these fairness conditions are satisfied. Hencewe remark that subjective thresholds were used to determine whether fairness conditions were approximately satisfied in what follows.\n| Calibration As Figure 8 shows, the Arnold NCA raw score does not satisfy monotonic calibration for race or gender groups. The score approximately satisfies group calibration for race (excluding the \u201cOther\u201d group) for all score values except for 13, and approximately satisfies group calibration for gender for all score values less than 11. The reasonwhy higher Arnold NCA raw scores fail the calibration definitions may be that there are few individuals with higher scores in the data set , thus making the results less stable. Interestingly, we found that the scaled version of Arnold NCA fully satisfied monotonic and group calibration, but had slightly worse predictive performance. EBM and RiskSLIM both satisfy monotonic calibration and group calibration for all gender and race groups (excluding the \u201cOther\u201d group)."
    },
    {
      "heading": "WANG &HAN ET AL. 25",
      "text": "0.0 2.5 5.0 7.5 10.0 12.5 Arnold NCA Raw Score\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nP( Y\n= 1\n| S co\nre =\nsc or\ne,\nAt tr\n= at\ntr)"
    },
    {
      "heading": "Calib. of Arnold NCA Raw on general_two_year in Kentucky",
      "text": "All individuals African-American Caucasian\nOther female male\n(a) For the Arnold NCA raw score, none of the curves are monotonically increasing\u2014violating monotonic calibration. Calibration curves for the Caucasian and African-American race groups lie close to each other (except for the score 13), approximately satisfying group calibration. Curves for the male and female groups lie close to each other, but diverge for scores greater than 10, indicating that the score satisfies group calibration for gender for scores less than or equal to 10.\n0.0-0 .1 0.1-0 .2 0.2-0 .3 0.3-0 .4 0.4-0 .5 0.5-0 .6 0.6-0 .7 0.7-0 .8 0.8-0 .9 0.9-1 .0\nEBM Score\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nP( Y\n= 1\n| S co\nre =\nsc or\ne,\nAt tr\n= at\ntr)\nCalib. of EBM on general_two_year in Kentucky\n(b) For EBM, except for the \u201cOther\u201d group, the calibration curves are monotonically increasing and approximately equal to each other (satisfying monotonic calibration and group calibration).\n0.0-0 .1 0.1-0 .2 0.2-0 .3 0.3-0 .4 0.4-0 .5 0.5-0 .6 0.6-0 .7 0.7-0 .8 0.8-0 .9 0.9-1 .0\nRiskSLIM Score\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nP( Y\n= 1\n| S co\nre =\nsc or\ne,\nAt tr\n= at\ntr)\nCalib. of RiskSLIM on general_two_year in Kentucky\n(c) For RiskSLIM, the domain of the graph goes up to only 0.3 \u2212 0.4 (unlike the other graphs). The curves are monotonically increasing and overlap with each other with the \u201cOther\u201d category being lower than all the other ones. Thus, RiskSLIM is approximately group calibrated and monotonically calibrated.\nF IGURE 8 Calibration results for the Arnold NCA raw, EBM and RiskSLIM for two-year general recidivism on Kentucky."
    },
    {
      "heading": "26 WANG &HAN ET AL.",
      "text": "| Balance for Positive/Negative Class (BPC/BNC) Formodels that provide risk probabilities as output (namely EBMandRiskSLIMmodels), we apply a 3% rule to determine whether BPC and BNC conditions are satisfied. The unscaled Arnold NCA produces scores between 0 and 13 rather than probabilities, so we use a 0.4 difference threshold to determine whether BPC and BNC are satisfied. That is, if the difference in scores between the two groups is greater than the threshold, thenwe conclude that themodel violates BPC/BNC. All conclusions we present below exclude the \u201cOther\u201d race group, because of its small sample size.\nFigure 9(a) displays the BPC/BNC results for the Arnold NCA raw, and shows that Arnold NCA satisfies neither BNC nor BPC on gender or race groups. Figures 9(b) shows that EBM satisfies both BPC and BNC on race groups. EBM also satisfies BNC, but not BPC, on gender groups. Figure 9(c) displays the results for RiskSLIM, which satisfies both BPC and BNC on race and gender groups.\n| Balanced Group AUC (BG-AUC) Wedetermine whether themodels satisfy BG-AUC using a 3% rule. In Kentucky, AUC values are stable across sensitive attributes for all models, satisfying BG-AUC for both gender and race. The discrepancies in AUC between AfricanAmericans and Caucasians range from 0.3% (RiskSLIM) to 2.1% (Arnold NCA raw). The range gets smaller for gender groups, lying between 0.5% (Arnold NCA) to 1.3% (RiskSLIM). Hence, we found that the Arnold NCA raw, EBM and RiskSLIM all satisfy Balanced Group AUC for the race (excluding the \u201cOther\u201d group) and gender groups."
    },
    {
      "heading": "Kentucky",
      "text": "Summary of Fairness Results: For the two-year general recidivism problem on the Kentucky data set, we found no egregious violations of the three fairness definitions (group calibration, BPC/BNC, and BG-AUC) for either of the interpretable machine learningmodels we assessed (EBM and RiskSLIM), but we did find small violations. We found the Arnold NCA raw score violated one of the fairness definitions (BPC/BNC).\nIn more detail, we found that balanced group AUCwere approximately satisfied for all threemodels with respect to both gender and race groups (except for \u201cOther\u201d). With respect to calibration, both EBM and RiskSLIM satisfied monotonic and group calibration on both gender and race groups (except for the \u201cOther\u201d). Arnold PSA approximately satisfied group calibration on race (excluding risk score 13) and gender groups for scores less than 11. Additionally, EBM satisfied both BPC and BNC on race categories, while it only satisfied BNC on gender categories. RiskSLIM satisfied both definitions on both sensitive attributes. The Arnold NCA satisfied neither BPC nor BNC on race and gender groups.\nA caveat is that we limited the discussion of the race groups to Caucasians and African-Americans\u2014otherwise the \u201cOther\u201d groupwould have caused all models to fail all definitions of fairness (calibration curves for the \u201cOther\u201d group are significantly beneath curves for other groups, average predicted scores vary substantially from the other groups, and prediction AUC is significantly lower for the \u201cOther\u201d group). This may be because we have the least data for the \u201cOther\u201d\nWANG &HAN ET AL. 27\ngeneral_bnc general_bpc0.0\n2.5\n5.0\n7.5\n10.0\n12.5\nE( Ar\nno ld\n| At\ntr =\nat tr,\nY =\ni)\n3.74\n5.68\n2.95\n5.07\n1.83\n3.39 2.57\n4.57\n3.27\n5.41\nBPC/BNC for Arnold on general_two_year in Kentucky\nAfr-Am. Cauc. Other Race\nFemale Male\n(a) Differences in expected scores for African-Americans and Caucasians are greater than the threshold (0.4): 0.79 (race, negative class), 0.61 (race, positive class). Differences in expected scores for gender are also greater than the threshold: 0.7 (gender, negative class) and 0.84 (gender, positive class).\ngeneral_bnc general_bpc0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nE( EB\nM |\nAt tr\n= at\ntr, Y\n= i)\n0.09\n0.28\n0.08\n0.27\n0.03\n0.12 0.07\n0.24\n0.08\n0.28\nBPC/BNC for EBM on general_two_year in Kentucky\nAfr-Am. Cauc. Other Race\nFemale Male\n(b) Differences in expected scores for African-Americans and Caucasians are less than 0.03: 0.01 (race, negative class), 0.01 (race, positive class). Differences in expected scores for gender satisfy the threshold in negative class but not positive class: 0.01 (gender, negative class), 0.04 (gender, positive class).\ngeneral_bnc general_bpc0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nE( Ri\nsk SL\nIM |\nAt tr\n= at\ntr, Y\n= i)\n0.2\n0.35\n0.19\n0.35\n0.14\n0.22 0.19\n0.33\n0.19\n0.35\nBPC/BNC for RiskSLIM on general_two_year in Kentucky\nAfr-Am. Cauc. Other Race\nFemale Male\n(c) Difference inexpected scores forAfrican-Americans and Caucasians are less than .03: 0.01 (race, negative class), 0 (race, positive class). Differences in expected probabilities for the male and female groups are less than 0.03: 0.00 (gender, negative class), 0.02 (gender, positive class).\nF IGURE 9 Balance for Positive andNegative Class for the Arnold NCA raw score, EBM and RiskSLIM on the two-year general prediction problem in Kentucky. Red line indicates themaximum value output bymodels."
    },
    {
      "heading": "28 WANG &HAN ET AL.",
      "text": "race group, which is only 2.49% of the total sample. To ensure fairness, it is important that comparable amounts of data are gathered for each sensitive groupwhen possible. However, in non-diverse states such as Kentucky, theremay not be enough individuals in minority groups to create a large enough statistical sample."
    },
    {
      "heading": "9.3 | ADiscussion on the Interaction between Fairness and Interpretability",
      "text": "There are significant hurdles to using current fairness techniques with interpretable models. Moreover, the vast majority of the work on fairness has focused on the binary classification case. Thus, few definitions of fairness (let alone algorithms) work for problemswhere predictions are nonbinary.\nWe did not attempt to use fairness-enforcement techniques because many fairness techniques require a noninterpretable transformation (further discussedbelow). Once these transformations aremade, there is noway to correct them to produce an interpretable model afterwards. There are generally three approaches to fairness algorithms: preprocessing of features [78], altering the training loss function [79, 80], and post-processing of predictions [50, 51, 53]. The pre-processing steps are generally complicated transformations of the input features, which shreds the data\u2019s natural meaning. Similarly, post-processing approaches either transform the predictions in some way, performing \u201cfairness corrections\u201d [53] (which are non-interpretable), or require threshold selection, which is contrary to our goals of providing non-binary risk assessments [50]. The approaches tomodify training loss functions are themost promising, but model optimization for both fairness and interpretability constraints would require new algorithms and is beyond the scope of this work.\nIn problemswhere fairness is a significant concern, machine learning outputs are likely to be used as decision tools rather than decision-makers, so it is surprising that so little work has thoroughly examined fairness for regression or probability estimation."
    },
    {
      "heading": "10 | DISCUSSION AND FUTURE WORK",
      "text": "From this analysis, we conclude that the interpretablemodels can indeed performapproximately aswell as the black-box models in various recidivism prediction problems, and much can be gained in interpretability for small sacrifices in accuracy. On the Broward data set, we found that RiskSLIM, EBM, and Additive Stumps perform as well or better than the best black-boxmodels. On the Kentucky data set, we observed that EBM and Additive Stumps have extremely close performance to the best black-boxmodels\u2014Random Forest and XGBoost\u2014with average AUC differences around 1%, which is less than the uncertainty gap.\nWe observed that machine learning models for six-month outcomes generally outperform those for two-year outcomes (conditioning on the recidivism type). This may be because treatment/rehabilitation programs have a greater chance of taking effect over a two-year time span (as compared to the six-month time span), altering the probability of recidivism. Future work could investigate this hypothesis, or pose other hypotheses to explain this observation.\nWe also observed that machine learningmodels do not generalize well across states, perhaps due to differences in the feature distributions between regions\u2014in particular, we observed that the age distributions for Kentucky and Broward County are considerably different. Onemight easily imagine regional feature distributions shifting over time as well, which is supported by several studies [17, 81, 82, 83]. Even though these studies focused on disparate crime types, they consistently observed a drop in the rate of offending among younger people since the 1990s. Studies have explicitly shown that the distributions of age versus arrest rate has changed over time as well. For instance, Kim et al. [17] has reported that in the state of NewYork, themean age of the total arrested population increased by two years"
    },
    {
      "heading": "WANG &HAN ET AL. 29",
      "text": "between 1990 and 2010. They hypothesized that a decrease in arrests in younger people and an increase in arrests in older people together contributed to the increase in mean age.\nThere aremany reasons why data would change over time and over jurisdictions. Changing policies (e.g., the NYC stop and frisk program) could potentially alter whowould be arrested and for what types of crime. New cultural phenomena (e.g., in video games andmedia) could also influence people\u2019s behavior at a large scale. The above observations lead us to conclude that different recidivism predictionmodels could be constructed for different locations and should be periodically updated. Machine learningmodels are well-suited for efficient creation and updating of these kinds of models. A possible future line of work is to separate the Kentucky data at the jurisdiction level, and perform a causal analysis of the effects of different judicial and policing practices on the recidivism distribution.\nSimple, transparent models have been used for criminal justice applications for almost a century [65, 84]. They have the advantage that one can easily quantify the contributions of each feature to the predicted score. Judicial actors without much statistics background can understand these scores, and use them to help solve societal issues. Interpretable models are extremely valuable for current decision-making processes in criminal justice: they allow error-checking, help ensure due process, and allow judges to incorporate information outside the database into their decision-making process in a calibratedmanner.\nHowever, our work on interpretable risk prediction is only one step closer to what we view as the ultimate goal\u2014 placing recidivism prediction into the framework of formal decision analysis. Decision-making in the context of decision analysis involves theminimization of costs rather than risks. Towards this end, Lakkaraju and Rudin [85] considered several costs related to pretrial release decisions; these include the societal cost of releasing an individual whomight commit a crime before their trial, the cost of assigning an officer to an individual, and the cost to taxpayers of keeping an individual incarcerated. The importance of risk predictions vary between decision-making problems (release, parole, sentencing, etc.). In some cases, they play aminor role, yet in others, predictionsmay comprise the sole deciding factor. Because of this, it would be useful to have a cost-benefit analysis per decision that would help determine exactly when andwhere risk scores should participate.\nHence, an important and necessary direction for the future work would be to incorporate the framework of classical decision analysis into decision-making in the criminal justice system. Decision analysis tools would ideally allow practitioners to strike a balance between relevant considerations (e.g., future risks to society, costs of treatment programs to society, costs to families involved in the criminal justice system, costs to the individual, as well as more traditional modelling objectives such as fairness, interpretability, transparency, and predictive performance). While the full data measuring costs and risks to all stakeholders in the criminal justice process may never be available, it is important tomove in this direction, as this would bring us closer tomore consistent and informed decisionmaking."
    },
    {
      "heading": "ACKNOWLEDGEMENTS",
      "text": "We acknowledge partial funding fromArnold Ventures, the Duke Computer Science Undergraduate Research Fellows Program, the Lord Foundation of North Carolina and theDukeDepartment of Electrical and Computer Engineering. This report represents the findings of the authors and does not represent the views of any of the funding agencies. We thank the Broward County Sheriff\u2019s office and the Kentucky Department of Shared Services, Research and Statistics for their assistance and provision of data. Wewould also like to thank Daniel Sturtevant from the Kentucky Department of Shared Services, Research and Statistics for providing significant insight into the Kentucky data set, and Berk Ustun for his advice on training RiskSLIM. Finally, we thank BrandonGarrett fromDuke, Stuart Buck and Kristin Bechtel from Arnold Ventures, and Kathy Schiflett, ChristyMay, and Tara Blair fromKentucky Pretrial Services for their thoughtful comments on the article."
    },
    {
      "heading": "30 WANG &HAN ET AL.",
      "text": ""
    },
    {
      "heading": "CODE",
      "text": "Our code is here: https://github.com/BeanHam/interpretable-machine-learning"
    },
    {
      "heading": "WANG &HAN ET AL. 31",
      "text": "[18] Bureau of Justice Assistance; Bureau of Justice Assistance. History of Risk Assessment. Bureau of Justice Assistance 2020;https://psrac.bja.ojp.gov/basics/history.\n[19] Kehl D, Guo P, Kessler S. Algorithms in the Criminal Justice System: Assessing the Use of Risk Assessments in Sentencing. 2017 July;https://cyber.harvard.edu/publications/2017/07/Algorithms.\n[20] Public Safety Assessment. Risk Factors and Formulas. Laura and John Arnold Foundation 2019 9;(https://www. psapretrial.org/about/).\n[21] Latessa E, Smith P, Lemke R, MakariosM, Lowenkamp C. Creation and Validation of the Ohio Risk Assessment System. University of Cincinnati School of Criminal Justice Center for Criminal Justice Research; 2009.\n[22] Electronic Privacy Information Center. Algorithms in the Criminal Justice System. Electronic Privacy Information Center 2016 6;https://epic.org/algorithmic-transparency/crim-justice/.\n[23] Hanson R, Thornton D. Notes on the development of Static-2002. Ottawa, Ontario: Department of the Solicitor General of Canada 2003;.\n[24] Tollenaar N, van der Heijden PGM. Which method predicts recidivism best?: a comparison of statistical, machine learning and data mining predictive models. Journal of the Royal Statistical Society: Series A (Statistics in Society) 2013;176(2):565\u2013584.\n[25] Howard P, Francis B, Soothill K, Humphreys L. OGRS 3: The revised offender group reconviction scale. Ministry of Justice; 2009.\n[26] Dawes RM, Faust D,Meehl PE. Clinical versus actuarial judgment. Science 1989;243(4899):1668\u20131674. [27] Grove WM, Meehl PE. Comparative efficiency of informal (subjective, impressionistic) and formal (mechanical, algo-\nrithmic) prediction procedures: The clinical\u2013statistical controversy. Psychology, Public Policy, and Law 1996;2(2):293. [28] WolfgangME. Delinquency in a birth cohort. University of Chicago Press; 1987. [29] Sherman LW. The power few: experimental criminology and the reduction of harm. Journal of Experimental Criminol-\nogy 2007;3(4):299\u2013321. [30] MilgramA,MilgramA, editor,Why smart statistics are the key to fighting crime. Ted Talk; 2014. [31] James N. Risk andNeeds Assessment in the Federal Prison System. Congressional Research Service; 2018. [32] Zweig J. Extraordinary Conditions Of Release Under The Bail Reform Act. Harvard Journal On Legislation\n2010;47:555\u2013585. [33] Desmarais S, Garrett B, Rudin C. Risk Assessment Tools Are Not A Failed \u2019Minority Report\u2019. Law360 2019\nJuly;https://www.law360.com/access-to-justice/articles/1180373/risk-assessment-tools-are-not-a-failedminority-report- .\n[34] The Leadership Conference on Civil and Human Rights. The Use of Pretrial \"Risk Assessment\" Instrument: A Shared Statement of Civil Rights Concerns. 2018 August;http://civilrightsdocs.info/pdf/criminal-justice/PretrialRisk-Assessment-Full.pdf.\n[35] Pretrial Justice Institute. Updated Position on Pretrial Risk Assessment Tools. Pretrial Justice Institute 2020;https: //www.pretrial.org/wp-content/uploads/Risk-Statement-PJI-2020.pdf.\n[36] Angwin J, Larson J, Mattu S, Kirchner L. Machine Bias. ProPublica; 2016. [37] Stevenson M. Assessing Risk Assessment in Action. Minnesota Law Review 2018;http://www.minnesotalawreview.\norg/wp-content/uploads/2019/01/13Stevenson_MLR.pdf."
    },
    {
      "heading": "32 WANG &HAN ET AL.",
      "text": "[38] Skeem J, Lin Z, Jung J, Goel S. The limits of human predictions of recidivism. Science Advances 2020;6. [39] Garrett B, Stevenson M. Open Risk Assessments. Behavioral Science & Law 2020;https://sites.law.duke.edu/\njustsciencelab/2019/09/15/comment-on-pattern-by-brandon-l-garrett-megan-t-stevenson/, forthcoming. [40] Roberts J, vonHirschA. Previous Convictions at Sentening - Theoretical andApplied Perspective. Bloomsbury Publish-\ning; 2010. [41] Frase RS, Roberts J, Hester R, Mitchell KL, of Criminal Law RI, Justice C, editors, Robina Institute of Criminal Law and\nCriminal Justice, Criminal History Enhancements Sourcebook. Robina Institute of Criminal Law and Criminal Justice; 2015. https://robinainstitute.umn.edu/publications/criminal-history-enhancements-sourcebook.\n[42] Starr SB. The Risk Assessment Era: AnOverdue Debate. Federal Sentencing Reporter 2015 April;27:205\u2013206. [43] American Law Institute, Model Penal Code; 2017. https://www.ali.org/projects/show/sentencing/. [44] Neuilly MA, Zgoba KM, Tita GE, Lee SS. Predicting recidivism in homicide offenders using classification tree analysis.\nHomicide studies 2011;15(2):154\u2013176. [45] Friedman JH. Stochastic gradient boosting. Computational Statistics &amp; Data Analysis 2002;38(4):367\u2013378. [46] Palocsay SW, PingWang, Brookshire RG. Predicting criminal recidivism using neural networks. Socio-Economic Plan-\nning Sciences 2000December;34:271\u2013284. [47] Berk RA, He Y, Sorenson SB. Developing a practical forecasting screener for domestic violence incidents. Evaluation\nReview 2005;29(4):358\u2013383. [48] Goel S, Rao JM, Shroff R. Precinct or Prejudice? Understanding Racial Disparities in New York City\u2019s Stop-And-Frisk\nPolicy. Institute ofMathematical Statistics 2016;10(1):365\u2013394. [49] Rudin C. Stop Explaining Black BoxMachine LearningModels for High Stakes Decisions and Use InterpretableModels\nInstead. NatureMachine Intelligence 2019May;1:206\u2013215. [50] HardtM, Price E, Srebro N. Equality of opportunity in supervised learning. In: Advances in neural information process-\ning systems; 2016. p. 3315\u20133323. [51] Agarwal A, Beygelzimer A, Dud\u00edkM, Langford J,Wallach H. A reductions approach to fair classification. arXiv preprint\narXiv:180302453 2018;. [52] Kleinberg J, Mullainathan S, Raghavan M. Inherent Trade-Offs in the Fair Determination of Risk Scores.\narXiv:160905807 2016November;. [53] PleissG, RaghavanM,WuF,Kleinberg J,WeinbergerK. On fairness and calibration. In: Advances inNeural Information\nProcessing Systems; 2017. p. 5680\u20135689. [54] Binns R. Fairness inMachine Learning: Lessons from Political Philosophy. Journal ofMachine Learning Research 2018\nJanuary;81:1\u201311. [55] Verma S, Rubin J. Fairness Definitions Explained. In: ACM/IEEE International Workshop on Software Fairness ACM;\n2018. p. 1\u20137. [56] Berk R, Heidari H, Jabbari S, Kearns M, Roth A. Fairness in Criminal Justice Risk Assessments: The State of the Art.\nSociological Methods & Research 2017 03;. [57] Berk R. Accuracy and Fairness for Juvenile Justice Risk Assessments. Journal of Empirical Legal Studies 2019\nMarch;16(1):174\u2013194."
    },
    {
      "heading": "WANG &HAN ET AL. 33",
      "text": "[58] Corbett-Davies S, PiersonE, FellerA,Goel S,HuqA. Algorithmicdecisionmaking and the cost of fairness. In: InProceedings of the 23rd ACMSIGKDD International Conference on KnowledgeDiscovery andDataMining; 2017. p. 797\u2013806.\n[59] Barocas S, Selbst AD. Big Data\u2019s Disparate Impact. California Law Review 2016;104:671\u2013732. [60] Corbett-Davies S, Goel S. The Measure and Mismeasure of Fairness: A Critical Review of Fair Machine Learning.\narXiv:180800023v2 2018 August;. [61] Vapnik V, Chervonenkis A. A note on one class of perceptrons. Automation and Remote Control 1964;25. [62] Breiman L, Friedman J, Stone CJ, Olshen RA. Classification and regression trees. CRC press; 1984. [63] Freund Y, Schapire RE. A decision-theoretic generalization of on-line learning and an application to boosting. Journal\nof computer and system sciences 1997;55(1):119\u2013139. [64] Chen T, Guestrin C. Xgboost: A scalable tree boosting system. In: Proceedings of the 22nd acm sigkdd international\nconference on knowledge discovery and datamining; 2016. p. 785\u2013794. [65] Burgess EW, on Indeterminate-Sentence Law IC, Parole Springfield I, editors, Factors determining success or failure on\nparole. Illinois Committee on Indeterminate-Sentence Law and Parole Springfield, IL; 1928. [66] Ustun B, Rudin C. Optimized Risk Scores. In: Proceedings of the 23rd ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining KDD \u201917, New York, NY, USA: ACM; 2017. p. 1125\u20131134. http://doi.acm. org/10.1145/3097983.3098161.\n[67] StevensonMT, Slobogin C. Algorithmic Risk Assessments and the Double-Edged Sword of Youth. Washington University Law Review 2018;96(Vanderbilt Law Research Paper No. 18-36). http://dx.doi.org/10.2139/ssrn.3225350.\n[68] Bindler A, Hjalmarsson R. How punishment severity affects jury verdicts: Evidence from two natural experiments. American Economic Journal: Economic Policy 2018;10.\n[69] Bushway SD, Piehl AM. The inextricable link between age and criminal history in sentencing. Crime & Delinquency 2007;53(1):156\u2013183.\n[70] United States Census Bureau. QuickFacts: Kentucy; United States. 2019;https://www.census.gov/quickfacts/fact/ table/KY,US/PST045219.\n[71] United States Census Bureau. Hispanic or Latino Origin By Race 2011-2015 American Community Survey 5-Year Estimates. 2015;https://factfinder.census.gov/faces/tableservices/jsf/pages/productview.xhtml?pid=ACS_15_ 5YR_B03002&prodType=table.\n[72] Mishra A. Climate and Crime. Global Journal of Science Frontier Research: H, Environment & Earth Science 2014;14. [73] RansonM. Crime, weather, and climate change. Journal of Environmental Economics andManagement 2014;67. [74] Defronzo J. Climate and Crime: Tests of an FBI Assumption. Environment and Behavior 1984;16. [75] Kleiman M, Ostrom BJ, Cheesman FL. Using risk assessment to inform sentencing decisions for nonviolent offenders\nin Virginia. Crime &Delinquency 2007;53(1):106\u2013132. [76] DworkC, HardtM, Pitassi T, ReingoldO, Zemel R. Fairness ThroughAwareness. In: Proceedings of the 3rd Innovations\nin Theoretical Computer Science Conference ITCS \u201912, New York, NY, USA: ACM; 2012. p. 214\u2013226. http://doi.acm. org/10.1145/2090236.2090255.\n[77] Chouldechova A. Fair prediction with disparate impact: A study of bias in recidivism prediction instruments. Big data 2017;5(2):153\u2013163."
    },
    {
      "heading": "34 WANG &HAN ET AL.",
      "text": "[78] Zemel R,Wu Y, Swersky K, Pitassi T, Dwork C. Learning fair representations. In: International Conference onMachine Learning; 2013. p. 325\u2013333.\n[79] Berk R, Heidari H, Jabbari S, JosephM, Kearns M, Morgenstern J, et al. A convex framework for fair regression. arXiv preprint arXiv:170602409 2017;.\n[80] Agarwal A,Dud\u00edkM,WuZS. Fair Regression: QuantitativeDefinitions andReduction-basedAlgorithms. arXiv preprint arXiv:190512843 2019;.\n[81] Cook P, Laub J. After the Epidemic Recent Trends in YouthViolence in theUnited States. Crime and Justice 2002;29:1\u2013 37.\n[82] Alfred B. The Crime Drop in America: An Explanation of Some Recent Crime Trends. Journal of Scandinavian Studies in Criminology and Crime Prevention 2006;7:17\u201335.\n[83] Matthews B, Minton J. Rethinking one of the criminology\u2019s \u2019brute facts\u2019: The age-crime curve and the crime drop in Scotland. European Journal of Criminology 2017;15(3):296\u2013320.\n[84] Hart H. Predicting Parole Success. Journal of Criminal Law and Criminology 1924;14. [85] Lakkaraju H, Rudin C. Learning Cost-Effective and Interpretable Treatment Regimes. In: Singh A, Zhu J, editors. Pro-\nceedings of the 20th International Conference on Artificial Intelligence and Statistics, vol. 54 of Proceedings of Machine Learning Research Fort Lauderdale, FL, USA: PMLR; 2017. p. 166\u2013175. http://proceedings.mlr.press/v54/ lakkaraju17a.html.\n[86] BrennanT,DieterichW, Ehret B. Evaluating thePredictiveValidity of theCOMPASRisk andNeedsAssessment System. Criminal Justice and Behavior 2009 January;36(1):21\u201340.\n[87] Northpointe Inc . Measurement & Treatment Implications of COMPASCore Scales; 2009. [88] Carollo J, Hedlund J, HinesM. Expanded Validation of a Decision Aid for Pretrial Conditional Release; 2007. [89] The Colorado Pretrial Assessment Tool (CPAT): Administration, Scoring, and Reporting Manual. Colorado Asso-\nciation of Pretrial Services; 2015, https://university.pretrial.org/HigherLogic/System/DownloadDocumentFile. ashx?DocumentFileKey=47e978bb-3945-9591-7a4f-77755959c5f5.\n[90] Turner S, Hess J, Jannetta J. Development of the California Static Risk Assessment Instrument (CSRA). CEBCWorking Papers 2009;.\n[91] Cadigan TP, LowenkampCT. Implementing Risk Assessment in the Federal Pretrial Services System. Federal Probation 2011;75(2).\n[92] Hoffman PB, Adelberg S. The Salient Factor Score: A Nontechnical Overview. Fed Probation 1980;44:44. [93] NafekhM,Motiuk LL. The Statistical Information onRecidivism, Revised1 (SIR-R1) Scale: APsychometric Examination.\nCorrectional Service of Canada. Research Branch; 2002. [94] Orbis, Orbis, editor, Service Planning Instrument: An Innovative Assessment and Case Planning Tool. Orbis; 2014.\nhttps://orbispartners.com/wp-content/uploads/2014/07/SPIn-Brochure.pdf. [95] Lazarsfeld PF. An Evaluation of the Pretrial Services Agency of the Vera Institute of Justice. New York: Vera Institute\n1974;. [96] Harris GT, Rice ME. Violence Risk Appraisal Guide (VRAG). In: Cutler BL, editor. Encyclopedia of Psychology and Law\nSAGE Publications, Inc.; 2008. p. 848. https://sk.sagepub.com/reference/download/psychologylaw/n345.pdf."
    },
    {
      "heading": "WANG &HAN ET AL. 35",
      "text": "[97] Virginia Pretrial Risk Assessment Instrument - (VPRAI). Virginia Department of Criminal Justice Services; 2018, https://www.dcjs.virginia.gov/sites/dcjs.virginia.gov/files/publications/corrections/virginia-pretrialrisk-assessment-instrument-vprai_0.pdf.\n[98] Fan RE, Chang KW, Hsieh CJ,Wang XR, Lin CJ. LIBLINEAR: A Library for Large Linear Classification. JMach Learn Res 2008 Jun;9:1871\u20131874.\n[99] Smith B. Auditing DeepNeural Networks to Understand Recidivism Predictions. PhD thesis, Haverford College; 2016. [100] Ustun B, Rudin C. Supersparse linear integermodels for optimizedmedical scoring systems. Machine Learning 2015;p.\n1\u201343. http://dx.doi.org/10.1007/s10994-015-5528-6."
    },
    {
      "heading": "11 | APPENDIX",
      "text": ""
    },
    {
      "heading": "11.1 | Broward Data Processing",
      "text": "The Broward County data set consists of publicly available criminal history, court data and COMPAS scores from Broward County, Florida. The criminal history and demographic information were computed from raw data released by ProPublica [36]. The probational history was computed from public criminal records released by the Broward Clerk\u2019s Office.\nThe screening date is the date on which the COMPAS score was calculated. The features and labels were computed for an individual with respect to a particular screening date. For individuals who have multiple screening dates, we compute the features for each screening date, such that the set of events for calculating features for earlier screening dates is included in the set of events for later screening dates. On occasion, an individual will havemultiple COMPAS scores calculated on the same date. There appears to be no information distinguishing these scores other than their identification number, so we take the scores with the larger identification number.\nThe recidivism labels were computed for the timescales of six months and two years. Some individuals were sentenced to prison as a result of their offense(s). We used only observations for which we have six months/two years of data subsequent to the individual\u2019s release date.\nBelow, we describe details of the feature and label generation process.\n\u2022 Degree \u201c(0)\u201d charges seem to be veryminor offenses, so we exclude these charges. We infer whether a charge is a felony, misdemeanor, or traffic charge based off the charge degree. \u2022 Some of our features rely on classifying the type of each offense (e.g., whether or not it is a violent offense). We infer this from the statute number, most of which correspond to statute numbers from the Florida state crime code. \u2022 The raw Propublica data includes arrest data as well as charge data. Because the arrest data does not include the statute, which is necessary for us to determine offense type, we use the charge data to compute features that require the offense type. We use both charge and arrest data to predict recidivism. \u2022 For each person on each COMPAS screening date, we identify the offense\u2014which we call the current offense\u2014that most likely triggered the COMPAS screening. The current offense date is the date of themost recent charge that occurred on or before the COMPAS screening date. Any charge that occurred on the current offense date is part of the current offense. In some cases, there is no prior charge that occurred near the COMPAS screening date, suggesting charges may bemissing from the data set. For this reason we consider charges that occurred within 30 days of the screening date for computing the current offense. If there are no charges in this range, we say the current offense is missing. We exclude observations withmissing current offenses. We used some of the COMPAS"
    },
    {
      "heading": "36 WANG &HAN ET AL.",
      "text": "subscale items as features for our machine learningmodels. All such components of the COMPAS subscales that we compute are based on data that occurred prior to (not including) the current offense date. \u2022 The events/documents data includes a number of events (e.g., \u201cFile Affidavit Of Defense\u201d or \u201cFile Order Dismissing Appeal\u201d) related to each case, and thus to each person. To determine howmany prior offenses occurredwhile on probation, or if the current offense occurredwhile on probation, we define a list of event descriptions indicating that an individual was taken on or off probation. Unfortunately, there appear to bemissing events, as individuals often have consecutive \u201cOn\u201d or consecutive \u201cOff\u201d events (e.g., two \u201cOn\u201d events in a row, without an \u201cOff\u201d in between). In these cases, or if the first event is an \u201cOff\u201d event or the last event is an \u201cOn\u201d event, we define two thresholds, ton and tof f . If an offense occurred within ton days after an \u201cOn\u201d event or tof f days before an \u201cOff\u201d event, we count the offense as occurring while on probation. We set ton to 365 and tof f to 30. On the other hand, the \u201cnumber of times on probation\u201d feature is just the count of \u201cOn\u201d events and the \u201cnumber of times the probation was revoked\u201d feature is just the count of \u201cFile order of Revocation of Probation\u201d event descriptions (i.e., we do not infer missing probation events for these two features). \u2022 Current age is defined as the age in years, rounded down to the nearest integer, on the COMPAS screening date. \u2022 A juvenile charge is defined as an offense that occurred prior to the defendant\u2019s 18th birthday. \u2022 Labels and features were computed using charge data. \u2022 The final data set contains 1,954 records and 41 features."
    },
    {
      "heading": "11.2 | Kentucky Data Processing",
      "text": "The Kentucky pretrial and criminal court data was provided by the Department of Shared Services, Research and Statistics in Kentucky. The Pretrial Services InformationManagement System (PRIM) data contains records regarding defendants, interviews, PRIM cases, bonds etc., that are connectedwith the pretrial services\u2019 interviews conducted between July 1, 2009 and June 30, 2018. The caseswere restricted to havemisdemeanor, felony, and other level charges. The data from another system, CourtNet, provided further information about cases, charges, sentences, dispositions etc. for CourtNet casesmatched in the PRIM system. The Kentucky data can be accessed through a special data request to the Kentucky Department of Shared Services, Research and Statistics.\nCourtNet and PRIM data were processed separately and then combined together. We describe the details below:\n\u2022 For the CourtNet data, we filtered out cases with filing date prior to Jan. 1st, 1996, which were claimed to be less reliable records by the Kentucky Department of Shared Services, Research and Statistics (which provided the data). To investigate what types of crimes the individuals were involved in for each charge, such as drug, property, traffic-related crime, we used the Kentucky Uniform Crime Reporting Code (UOR Code), as well as detecting keywords in the UOR description. \u2022 From the PRIM system data, we extracted the probation, failure to appear, case pending, and violent charge information at the PRIM case level, as well as the Arnold PSA risk scores computed at the time of each pretrial services\u2019 interview. Since Kentucky did not use Arnold PSA until July 1st, 2013, we filtered out records before the this date. We omitted records without risk scores since wewant to compare the performance of the PSAwith other models. Only 33 records aremissing PSA scores; therefore we do not worry aboutmissing records impacting the results. Additionally, some cases in the PRIM system have \u201cindictment\u201d for the arrest type, along with an \u201coriginal\u201d arrest case ID, indicating that those cases were not new arrests. Wematched these cases with the records that correspond to the original arrests to avoid overcounting the number of prior arrests. Then we inner-joined the data from the two systems using person-id and prim-case-id."
    },
    {
      "heading": "WANG &HAN ET AL. 37",
      "text": "\u2022 For each individual, we used the date that is two years before the latest charge date in the Kentucky data, as a cutoff date. The data before the cutoff are used as criminal history information to compute features. The data after the cutoff are used to compute labels and check recidivism. In the data before the cutoff, the latest charge is treated as the current charge (i.e., the charge that would trigger a risk-assessment) for each individual. We compute features and construct labels using only convicted charges. However, the current charge can be either convicted or non-convicted. This ensures that our analysis includes all individuals that would receive a risk assessment, even if they were later found innocent of the current charge that triggered the risk assessment. It also ensures that criminal history features use only convicted charges, so that our risk assessments are not influenced by charges for crimes that the personmay not have committed. \u2022 In order to compute the labels, wemust ensure that there are at least two years of data following an individual\u2019s current charge date. For individuals who are sentenced to prison due to their current charge, we consider their release date instead of the current charge date. We omitted individuals for whom there were less than two years of data between their current charge date or release date, and the last date recorded in the data set. \u2022 To get the age at current charge information, we first calculated the date of birth (DOB) for each individual, using CourtNet case filing date and age at the CourtNet case filing date. Thenwe calculated \u201cage at current charge\u201d using the DOB and charge date (the charge date sometimes differs from the case filing date). Notice that there aremany errors in age records in the data. For instance, some people have age recorded over 150, which is certainly wrong but there is no way to correct it. To ensure the quality of our data, we limited the final current age feature to be inclusively between 18 and 70. This is also consistent with the range from Broward analysis. If the person was not sentenced to prison, we define current age as the age at current charge date. If the personwas sentenced to prison, we compute current age by adding the sentence time to the age at the current charge date. Note that this differs from the way risk scores are computed in practice\u2014usually risk scores are computed prior to the sentencing decision. This helps to handle distributional shift between the individuals with no prison sentence (for whom a 2-year evaluation can be handled directly) and the full population (some of whommay have been sentenced to prison and cannot commit a crime during their sentence). \u2022 We computed features using the data before the current charge date. The CourtNet data is organized by CourtNet cases, and each CourtNet case has charge level data. The PRIM data is organized by PRIM cases. Each CourtNet case can connect tomultiple PRIMcases.14 Therefore, to compute the criminal history information,wefirst grouped on PRIM case level to summarize the charge information. Next, we grouped on CourtNet case level to summarize PRIM case level information. Last, we grouped on the individual level to summarize the criminal histories. \u2022 On computing the ADE feature: The ADE featuremeans number of times the individual was assigned to alcohol and drug education classes. Note that by Kentucky state law, any individual convicted for a DUI is assigned to ADE classes. This does not indicate whether the individual successfully completed ADE classes. \u2022 We compute labels using the two years of data after the current charge date/release date. We constructed the general recidivism labels by checking whether a \u201cconvicted charge\u201d occurred within two years or six months from the current charge (or release date). Then, using the charge types of the convicted charge, other recidivism prediction labels were generated, such as drug or property-related recidivism. The final data set contains 250,778 records and 40 features. Note: there are degrees of experimenter freedom in some of these data processing choices; exploring all the possible choices here is left for future studies.\nTheArnold PSA features thatwere included in theKentucky data set (e.g., prior convictions, prior felony convictions 14This occurs because a newPRIMcase is loggedwhen anupdate occurs in the defendant\u2019s CourtNet case (for example, if the defendant fails to appear in court)."
    },
    {
      "heading": "38 WANG &HAN ET AL.",
      "text": "etc.) were computedbypretrial officerswhohadaccess to criminal history data fromboth inside andoutsideofKentucky. However, the Kentucky data set we received contained criminal history information fromwithin Kentucky only. Thus, the Arnold PSA features for Kentucky (which are included in our models as well) use both in-state and out-of-state information, but the remaining features (whichwe compute directly from theKentucky criminal history data) are limited to in-state criminal history.\nAdditionally, we were informed by Kentucky Pretrial Services team that the data set \u2019s sentencing information may not be reliable due to unmeasured confounding, including shock probation and early releases that would allow a prisoner to be released much earlier than the end date of the sentence. Because the sentence could be anywhere from zero days to the full length, we conducted a sensitivity analysis by excluding the sentence information in the data processing, which is equivalent to the assumption that no prison sentence was served. For that analysis, the current age of each individual was calculated to be the age at the current charge, and the prediction labels were generated from new charges within six months (or two years) from the current charge. The sensitivity analysis yielded predictive results that were almost exactly the same as the results in themain text, when the sentence information was used to determine age and prediction interval."
    },
    {
      "heading": "11.3 | WhyWeCompareOnly Against COMPAS and the PSA",
      "text": "The variables included in risk assessments are often categorized into static and dynamic factors. Static factors are defined as factors that cannot be reduced over time (e.g. criminal history, gender, and age-at-first-arrest). Dynamic factors are defined as variables that can change over time to decrease the risk of recidivism; they allow insight into whether a high-risk individual can lower their risk through rehabilitation, and sometimes improve prediction accuracy. Examples of dynamic factors include current age, treatment for substance abuse, andmental health status [19]. Dynamic factors are often included in risk-and-needs-assessments (RNAs), which in addition to identifying risk of recidivism, recommend interventions to practitioners (e.g., treatment programs, social services, diversion of individuals from jail).\nWith the exception of current age, our features all fall under the \u201cstatic\u201d classification. This renders us unable to compare against the risk assessment tools that use dynamic factors, whose formulas are public. The risk assessments that we examined are listed in Table 4. Since we have only criminal history and age variables, the only model we could compute from our data was the Arnold PSA.\nHowever, as we demonstrated in themain body of the paper, the fact that we do not possess dynamic factors is not necessarily harmful to the predictive performance of our models. The goal behind including dynamic factors in models is to improve prediction accuracy as well as be able to recommend interventions that reduce the probability of recidivism. While an admirable goal, the inclusion of dynamic factors does not come at zero cost andmay not actually produce performance gains for recidivism prediction. In Sections 6 and 7, we show that standard machine learning techniques (using only the static factors) and interpretable machine learning models (using only static factors) are able to outperform a criminal justice model that utilizes both static and dynamic factors (COMPAS). Furthermore, the inclusion of additional, unnecessary factors increases the risk of data entry errors, or exposes models to additional feature bias [60]. As Rudin et al. [4] reveals, data entry errors appear to be common in COMPAS score calculations and could lead to scores that are either too high or too low.\nAlthough the COMPAS suite is a proprietary (and thus black-box) risk-and-needs assessment, wewere still able to compare against its risk assessments thanks to the Florida\u2019s strong open-records laws. Created byNorthpointe (a subsidiary company of Equivant), COMPAS is a recidivism prediction suite which is used in criminal justice systems throughout the United States. It is comprised of three scores: Risk of General Recidivism, Risk of Violent Recidivism, and Risk of Failure to Appear. In this work, we examine the two risk scores relating to violent recidivism and general"
    },
    {
      "heading": "WANG &HAN ET AL. 39",
      "text": "recidivism. Each risk score is an integer from one to ten [86]. As COMPAS scores are proprietary instruments, the precise forms of its models are not publicly available. However, it is known that the COMPAS scores are computed from a subset of 137 input variables that include vocational/educational status, substance abuse, and probational history, in addition to the standard criminal history variables [86]. As such, we cannot directly compute these risk scores, and instead utilize the COMPAS scores released by ProPublica in the Broward County recidivism data set. We do not compare against COMPAS on the Kentucky data set, as our data set does not include COMPAS scores.\nThe PSAwas created by Arnold Ventures, and is a publicly available risk assessment tool. Similar to the COMPAS suite, it is comprised of three risk scores: Failure to Appear, NewCriminal Activity, and NewViolent Criminal Activity. Again, we compare against latter two scores. Both are additive integer models which take nine factors as input, relating to age, current charge, and criminal history. TheNewCriminal Activity model outputs a score from 1 to 6, while the New Violent Criminal Activity model outputs a binary score [20]. The PSA is an interpretable model."
    },
    {
      "heading": "LSI-CMI [14] X X X X X X",
      "text": ""
    },
    {
      "heading": "COMPAS [87] X X X X X X X",
      "text": ""
    },
    {
      "heading": "40 WANG &HAN ET AL.",
      "text": ""
    },
    {
      "heading": "11.4 | Hyperparameters | BaselineModels, CART, EBM",
      "text": "We applied nested cross validation to tune the hyperparameters. Please refer to Table 5 for parameter details.\n| Additive Stumps Stumps were created for each feature as detailed in Section 7.1. An additive model was created from the stumps using `1-penalized logistic regression, and nomore than 15 original features were involved in the additive models. But multiple stumps corresponding to each feature could be used in themodels. We chose to limit the size of themodel to 15 original features because then at most 15 plots would be generated to visualize the full model, which is a reasonable number of visualizations for users to digest.\nWe startedwith the smallest regularization parameter on `1 penalty that provides atmost 15 original features from themodel. This will be our lower bound for nested cross validation. From there, we perform nested cross validation over a grid of regularization parameters, all of which are greater than or equal to theminimum value of the regularization parameter found above. Please refer to Table 6 for more details.\n15The training procedure is slow for EBM, due to the size of Kentucky data, the nested cross validationwe applied, and the cross-validationwithin the algorithm to choose number of pairwise interactions. Therefore, we tested only one set of parameters, which gave reliable results."
    },
    {
      "heading": "WANG &HAN ET AL. 41",
      "text": "| RiskSLIM RiskSLIM is challenging to train, because it uses the CPLEX optimization software, which can be difficult to install and requires a license. Moreover, since RiskSLIM solves a very difficult mixed-integer nonlinear optimization problem, it can be slow to prove optimality, whichmakes it difficult to perform nested cross validation as nested cross validation requires many solutions of the optimization problem. A previous study [99] also noted similar problems with algorithms that use CPLEX (this study trained on SLIM [100], which is similar to the training process of RiskSLIM in that they both require CPLEX). Here we provide details of howwe trained RiskSLIM to help others use the algorithmmore efficiently.\n\u2022 We ran `1-penalized logistic regression on the stumps training data with a relatively large regularization parameter to obtain a small subset of features (that is, we used `1-penalized logistic regression for feature selection). Thenwe trained RiskSLIMusing nested cross validationwith this small subset of features. Themaximum run-time, maximum offset, and penalty value were set to 1,000 seconds, 100, and 1e-6 respectively. The coefficient range was set to [-5, 5], which would give us small coefficients that are easy to add/subtract. \u2022 If themodel converged to optimality (optimality gap less than 5%) within 1,000 seconds, we then ran `1-penalized logistic regression againwith a smaller regularization parameter to obtain a slightly larger subset of features towork with. We then trained RiskSLIMwith nested cross validation again on this larger subset of features. If RiskSLIM also generated an optimality gap less than 5%within 1,000 seconds and had better validation performance, we repeated this procedure. \u2022 Once either RiskSLIM could not converge to a 5% optimality gapwithin 1,000 seconds, or the validation performance did not improve by addingmore stumps, we stopped there, using the previously obtained RiskSLIMmodel as the final model. \u2022 This procedure generally stoppedwith between 12 and 20 stumps from `1-penalized logistic regression. Beyond this number of stumps, we did not observe improvements in performance in validation."
    },
    {
      "heading": "SixMonth",
      "text": ""
    },
    {
      "heading": "Two Year",
      "text": ""
    },
    {
      "heading": "BaselineModels",
      "text": ""
    },
    {
      "heading": "SixMonth",
      "text": ""
    },
    {
      "heading": "Two Year",
      "text": ""
    },
    {
      "heading": "BaselineModels",
      "text": ""
    },
    {
      "heading": "11.5 | Tables",
      "text": ""
    },
    {
      "heading": "42 WANG &HAN ET AL.",
      "text": ""
    },
    {
      "heading": "SixMonth",
      "text": ""
    },
    {
      "heading": "Two Year",
      "text": ""
    },
    {
      "heading": "SixMonth",
      "text": ""
    },
    {
      "heading": "Two Year",
      "text": ""
    },
    {
      "heading": "WANG &HAN ET AL. 43",
      "text": ""
    },
    {
      "heading": "21. ADE \u2265 1 0.1583 -...",
      "text": ""
    },
    {
      "heading": "44 WANG &HAN ET AL.",
      "text": ""
    },
    {
      "heading": "SixMonth",
      "text": ""
    },
    {
      "heading": "Two Year",
      "text": ""
    },
    {
      "heading": "SixMonth",
      "text": ""
    },
    {
      "heading": "Two Year",
      "text": ""
    },
    {
      "heading": "WANG &HAN ET AL. 45",
      "text": ""
    },
    {
      "heading": "SixMonth",
      "text": ""
    },
    {
      "heading": "Two Year",
      "text": ""
    },
    {
      "heading": "SixMonth",
      "text": ""
    },
    {
      "heading": "Two Year",
      "text": ""
    },
    {
      "heading": "46 WANG &HAN ET AL.",
      "text": ""
    },
    {
      "heading": "WANG &HAN ET AL. 47",
      "text": ""
    },
    {
      "heading": "Kentucky",
      "text": ""
    },
    {
      "heading": "Kentucky",
      "text": ""
    },
    {
      "heading": "48 WANG &HAN ET AL.",
      "text": ""
    },
    {
      "heading": "WANG &HAN ET AL. 49",
      "text": ""
    },
    {
      "heading": "11.6 | Figures",
      "text": "F IGURE 10 Base rates of all twelve types of recidivism on Kentucky data, conditioned (separately) on race and gender.\ndrug _six\n_mo nth drug _tw o_y ear\nfelo ny_\nsix_ mon\nth\nfelo ny_\ntwo _ye\nar\ngen eral\n_six _mo\nnth\ngen eral\n_tw o_y\near\nmis dem\nean or_s\nix_m onth\nmis dem\nean or_t\nwo_ yea\nr\nprop erty\n_six _mo\nnth\nprop erty\n_tw o_y\near\nviol ent_\nsix_ mon\nth\nviol ent_\ntwo _ye\nar\nPrediction Problem\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\nP( Y\n= 1\n| A ttr\n= a\nttr )\nCond. prob. of recidivism for all prediction problems on Kentucky\nAfrican-American Caucasian Other\nfemale male\nF IGURE 11 Probabilities of two-year and six-month violent recidivism, given the age at current charge.\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\npr ob\nab ilit\ny\nlocation = KY\ntwo_year six_month\n18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 age_at_current_charge\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\npr ob\nab ilit\ny\nlocation = FL\nViolent Recidivism"
    },
    {
      "heading": "50 WANG &HAN ET AL.",
      "text": "F IGURE 12 Calibration of the Arnold NVCARaw, EBM and RiskSLIM for two-year violent recidivism on Kentucky.\n0 2 4 6 Arnold NVCA Raw Score\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nP( Y\n= 1\n| S co\nre =\nsc or\ne,\nAt tr\n= at\ntr)\nCalib. of Arnold NVCA Raw on violent_two_year in Kentucky\nAll individuals African-American Caucasian\nOther female male\n(a) For the Arnold NVCA raw score, the curves satisfy monotonic calibration until the score value of 7, where the probabilities drop to 0. This may be because there are few individuals with an Arnold NVCA raw score equal to 7 in the data. The curves for African-Americans/Caucasians and males/females are close enough to satisfy group calibration (but we note that the African-American (respectively, male) curve is consistently higher than theCaucasian (respectively, female) curve), especially for larger rawNVCA scores.\n0.0-0 .1 0.1-0 .2 0.2-0 .3 0.3-0 .4 0.4-0 .5 0.5-0 .6 0.6-0 .7 0.7-0 .8 0.8-0 .9 0.9-1 .0\nEBM Score\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nP( Y\n= 1\n| S co\nre =\nsc or\ne,\nAt tr\n= at\ntr)\nCalib. of EBM on violent_two_year in Kentucky\nAll individuals African-American Caucasian\nOther female male\n(b) For EBM, the calibration curves for both gender and race groups are irregular, demonstrating that EBM satisfied neither groupcalibrationnormonotonic calibration, on raceand gender groups.\n0.0-0 .1 0.1-0 .2 0.2-0 .3 0.3-0 .4 0.4-0 .5 0.5-0 .6 0.6-0 .7 0.7-0 .8 0.8-0 .9 0.9-1 .0\nRiskSLIM Score\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nP( Y\n= 1\n| S co\nre =\nsc or\ne,\nAt tr\n= at\ntr)\nCalib. of RiskSLIM on violent_two_year in Kentucky\nAll individuals African-American Caucasian\nOther female male\n(c) For RiskSLIM, the curves are monotonically increasing and roughly overlap with each other. The calibration curve for African-Americans is slightly higher than for the Caucasian and the \u201cOther\u201d race groups. For the two gender groups, the curves are close to each other. We conclude that both race and gender approximately satisfy group calibration."
    },
    {
      "heading": "WANG &HAN ET AL. 51",
      "text": "F IGURE 13 Balance for Positive and Negative Class for the Arnold NVCARaw, EBM and RiskSLIM on the two-year violent prediction problem in Kentucky. Red line indicates themaximum value output bymodels.\nviolent_bnc violent_bpc0\n2\n4\n6\n8\nE( Ar\nno ld\n| At\ntr =\nat tr,\nY =\ni)\n1.86\n3.13\n1.57\n2.84\n1.15\n2.55\n1.35\n2.49\n1.73\n2.99\nBPC/BNC for Arnold on violent_two_year in Kentucky\nAfr-Am. Cauc. Other Race\nFemale Male\n(a) Differences in expected scores for African-Americans and Caucasians are greater than the threshold (0.2): 0.29 (race, negative class), 0.29 (race, positive class). Differences in expected scores for gender are also greater than the threshold: 0.38 (gender, negative class) and 0.50 (gender, positive class).\nviolent_bnc violent_bpc0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nE( EB\nM |\nAt tr\n= at\ntr, Y\n= i)\n0.01 0.03 0.0 0.020.0 0.010.0 0.010.0 0.03\nBPC/BNC for EBM on violent_two_year in Kentucky\nAfr-Am. Cauc. Other Race\nFemale Male\n(b) Differences in expected scores for African-Americans and Caucasians are less than 0.03: 0.01 (race, negative class), 0.01 (race, positive class). Differences in expected scores for gender also satisfy the threshold: 0.00 (gender, negative class) and 0.02 (gender, positive class).\nviolent_bnc violent_bpc0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nE( Ri\nsk SL\nIM |\nAt tr\n= at\ntr, Y\n= i)\n0.04 0.08 0.03 0.06 0.01 0.030.01 0.020.04 0.08\nBPC/BNC for RiskSLIM on violent_two_year in Kentucky\nAfr-Am. Cauc. Other Race\nFemale Male\n(c) Differences in expected scores for African-Americans and Caucasians are less than 0.03: 0.01 (race, negative class), 0.02 (race, positive class). Differences in expected scores for gender satisfy the threshold for the negative class, but not for thepositive class: 0.03 (gender, negative class) and 0.06 (gender, positive class)."
    },
    {
      "heading": "52 WANG &HAN ET AL.",
      "text": ""
    },
    {
      "heading": "11.7 | Nested Cross Validation Procedure",
      "text": "We applied five-fold nested cross validation to tune parameters. We split the entire data set into five equally-sized folds for the outer cross validation step. One fold was used as the holdout test set and the other four folds were used as the training set (call it \u201couter training set\u201d). The inner loop deals only with the outer training set ( 45 ths of the data). On this outer training set, we conducted five-fold cross validation and grid-searched hyperparameter values. After this point, each hyperparameter value had five validation results. We selected the parameter values with the highest average validation results and then trained themodel with this best set of parameters on the entire outer training set and tested it on the holdout test set.\nWe repeated the process above until each one of the original five folds was used as the holdout test set. Ultimately, we had five holdout test results, with which we were able to calculate the average and standard deviation of the performance.\nWe applied a variant of the nested cross validation procedure described above to perform the analysis discussed in Section 8\u2014wherewe trainedmodels on one region and tested on the other region. For instance, whenwe trained models on Broward and tested them on Kentucky, the Kentucky data was treated as the holdout test set. We split the Broward data into five folds and used four folds to do cross validation and constructed the final model using the best parameters. We then tested the final model on the entire Kentucky data set, as well as the holdout test set from Broward. We rotated the four folds and repeated the above process five times."
    },
    {
      "heading": "11.8 | RiskSLIM Tables",
      "text": ""
    },
    {
      "heading": "WANG &HAN ET AL. 53",
      "text": ""
    },
    {
      "heading": "54 WANG &HAN ET AL.",
      "text": ""
    },
    {
      "heading": "WANG &HAN ET AL. 55",
      "text": ""
    },
    {
      "heading": "56 WANG &HAN ET AL.",
      "text": ""
    },
    {
      "heading": "11.9 | Features",
      "text": ""
    },
    {
      "heading": "WANG &HAN ET AL. 57",
      "text": ""
    },
    {
      "heading": "58 WANG &HAN ET AL.",
      "text": ""
    }
  ],
  "title": "In Pursuit of Interpretable, Fair and Accurate Machine Learning for Criminal Recidivism Prediction",
  "year": 2020
}
