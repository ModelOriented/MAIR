{
  "abstractText": "This work proposes a novel general framework, in the context of eXplainable Artificial Intelligence (XAI), to construct explanations for the behaviour of Machine Learning (ML) models in terms of middle-level features. One can isolate two different ways to provide explanations in the context of XAI: low and middle-level explanations. Middle-level explanations have been introduced for alleviating some deficiencies of low-level explanations such as, in the context of image classification, the fact that human users are left with a significant interpretive burden: starting from low-level explanations, one has to identify properties of the overall input that are perceptually salient for the human visual system. However, a general approach to correctly evaluate the elements of middle-level explanations with respect ML model responses has never been proposed in the literature. We experimentally evaluate the proposed approach to explain the decisions made by an Imagenet pre-trained VGG16 model on STL-10 images and by a customised model trained on the JAFFE dataset, using two different computational definitions of middle-level features and compare it with two different XAI middle-level methods. The results show that our approach can be used successfully in different computational definitions of middle-level explanations.",
  "authors": [
    {
      "affiliations": [],
      "name": "Andrea Apicella"
    },
    {
      "affiliations": [],
      "name": "Salvatore Giugliano"
    },
    {
      "affiliations": [],
      "name": "Francesco Isgr\u00f2"
    },
    {
      "affiliations": [],
      "name": "Roberto Prevete"
    }
  ],
  "id": "SP:e856b61dad536a5fe3d41a9d5e88434ac0cb98ee",
  "references": [
    {
      "authors": [
        "A. Adadi",
        "M. Berrada"
      ],
      "title": "Peeking inside the black-box: A survey on explainable artificial intelligence (xai)",
      "venue": "IEEE Access 6, 52138\u201352160",
      "year": 2018
    },
    {
      "authors": [
        "A. Apicella",
        "F. Isgro",
        "R. Prevete",
        "A. Sorrentino",
        "G. Tamburrini"
      ],
      "title": "Explaining classification systems using sparse dictionaries",
      "venue": "Proceedings of the European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning, Special Session on Societal Issues in Machine Learning: When Learning from Data is Not Enough. Bruges, Belgium",
      "year": 2019
    },
    {
      "authors": [
        "A. Apicella",
        "F. Isgr\u00f2",
        "R. Prevete",
        "G. Tamburrini"
      ],
      "title": "Contrastive explanations to classification systems using sparse dictionaries",
      "venue": "International Conference on Image Analysis and Processing. pp. 207\u2013218. Springer, Cham",
      "year": 2019
    },
    {
      "authors": [
        "A. Apicella",
        "F. Isgr\u00f2",
        "R. Prevete",
        "G. Tamburrini"
      ],
      "title": "Middle-level features for the explanation of classification systems by sparse dictionary methods",
      "venue": "International Journal of Neural Systems 30(08), 2050040",
      "year": 2020
    },
    {
      "authors": [
        "A. Apicella",
        "F. Isgro",
        "R. Prevete",
        "G. Tamburrini",
        "A. Vietri"
      ],
      "title": "Sparse dictionaries for the explanation of classification systems",
      "venue": "PIE. p. 009. Rome, Italy",
      "year": 2019
    },
    {
      "authors": [
        "S. Bach",
        "A. Binder",
        "G. Montavon",
        "F. Klauschen",
        "K.R. M\u00fcller",
        "W. Samek"
      ],
      "title": "On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation",
      "venue": "PloS one 10(7)",
      "year": 2015
    },
    {
      "authors": [
        "R. Caccavale",
        "A. Finzi"
      ],
      "title": "Learning attentional regulations for structured tasks execution in robotic cognitive control",
      "venue": "Autonomous Robots 43(8), 2229\u20132243",
      "year": 2019
    },
    {
      "authors": [
        "A. Coates",
        "A. Ng",
        "H. Lee"
      ],
      "title": "An analysis of single-layer networks in unsupervised feature learning",
      "venue": "Proceedings of the fourteenth international conference on artificial intelligence and statistics. pp. 215\u2013223",
      "year": 2011
    },
    {
      "authors": [
        "M. Craven",
        "J.W. Shavlik"
      ],
      "title": "Extracting tree-structured representations of trained networks",
      "venue": "Advances in Neural Information Processing Systems. pp. 24\u201330. Denver, CO, USA",
      "year": 1996
    },
    {
      "authors": [
        "J. Devlin",
        "M.W. Chang",
        "K. Lee",
        "K. Toutanova"
      ],
      "title": "Bert: Pre-training of deep bidirectional transformers for language understanding",
      "year": 2018
    },
    {
      "authors": [
        "D. Erhan",
        "Y. Bengio",
        "Courville",
        ".",
        "P. Vincent"
      ],
      "title": "Visualizing higher-layer features of a deep network",
      "venue": "University of Montreal 1341(3), 1",
      "year": 2009
    },
    {
      "authors": [
        "L.H. Gilpin",
        "D. Bau",
        "B.Z. Yuan",
        "A. Bajwa",
        "M. Specter",
        "L. Kagal"
      ],
      "title": "Explaining explanations: An overview of interpretability of machine learning",
      "venue": "2018 IEEE 5th International Conference on data science and advanced analytics (DSAA). pp. 80\u201389. IEEE, Turin, Italy",
      "year": 2018
    },
    {
      "authors": [
        "R. Guidotti",
        "A. Monreale",
        "S. Ruggieri",
        "D. Pedreschi",
        "F. Turini",
        "F. Giannotti"
      ],
      "title": "Local rule-based explanations of black box decision systems",
      "venue": "CoRR abs/1805.10820",
      "year": 2018
    },
    {
      "authors": [
        "R. Guidotti",
        "A. Monreale",
        "S. Ruggieri",
        "F. Turini",
        "F. Giannotti",
        "D. Pedreschi"
      ],
      "title": "A survey of methods for explaining black box models",
      "venue": "ACM computing surveys (CSUR) 51(5), 93",
      "year": 2018
    },
    {
      "authors": [
        "R. Jenatton",
        "G. Obozinski",
        "F. Bach"
      ],
      "title": "Structured sparse principal component analysis",
      "venue": "Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics. pp. 366\u2013373",
      "year": 2010
    },
    {
      "authors": [
        "D.D. Lee",
        "H.S. Seung"
      ],
      "title": "Algorithms for non-negative matrix factorization",
      "venue": "Advances in neural information processing systems. pp. 556\u2013562. Vancouver, British Columbia, Canada",
      "year": 2001
    },
    {
      "authors": [
        "B. Letham",
        "C. Rudin",
        "T.H. McCormick",
        "D Madigan"
      ],
      "title": "Interpretable classifiers using rules and bayesian analysis: Building a better stroke prediction model",
      "venue": "The Annals of Applied Statistics 9(3), 1350\u20131371",
      "year": 2015
    },
    {
      "authors": [
        "X.H. Li",
        "C.C. Cao",
        "Y. Shi",
        "W. Bai",
        "H. Gao",
        "L. Qiu",
        "C. Wang",
        "Y. Gao",
        "S. Zhang",
        "X Xue"
      ],
      "title": "A survey of data-driven and knowledge-aware explainable ai",
      "venue": "IEEE Transactions on Knowledge and Data Engineering",
      "year": 2020
    },
    {
      "authors": [
        "M. Lyons",
        "S. Akamatsu",
        "M. Kamachi",
        "J. Gyoba"
      ],
      "title": "Coding facial expressions with gabor wavelets",
      "venue": "Proceedings, Third IEEE International Conference on Automatic Face and Gesture Recognition. pp. 200\u2013205. IEEE Computer Society",
      "year": 1998
    },
    {
      "authors": [
        "G. Montavon",
        "W. Samek",
        "K. M\u00fcller"
      ],
      "title": "Methods for interpreting and understanding deep neural networks",
      "venue": "Digital Signal Processing 73, 1\u201315",
      "year": 2018
    },
    {
      "authors": [
        "A. Nguyen",
        "A. Dosovitskiy",
        "J. Yosinski",
        "T. Brox",
        "J. Clune"
      ],
      "title": "Synthesizing the preferred inputs for neurons in neural networks via deep generator networks",
      "venue": "Lee, D.D., Sugiyama, M., Luxburg, U.V., Guyon, I., Garnett, R. (eds.) Advances in Neural Information Processing Systems 29, pp. 3387\u20133395. Curran Associates, Inc.",
      "year": 2016
    },
    {
      "authors": [
        "A. Nguyen",
        "J. Yosinski",
        "J. Clune"
      ],
      "title": "Understanding neural networks via feature visualization: A survey",
      "venue": "Samek, W., Montavon, G., Vedaldi, A., Hansen, L., Muller, K. (eds.) Explainable AI: Interpreting, Explaining and Visualizing Deep Learning, pp. 55\u201376. Springer",
      "year": 2019
    },
    {
      "authors": [
        "S.J. Oh",
        "B. Schiele",
        "M. Fritz"
      ],
      "title": "Towards reverse-engineering black-box neural networks",
      "venue": "Samek, W., Montavon, G., Vedaldi, A., Hansen, L., Muller, K. (eds.) Explainable AI: Interpreting, Explaining and Visualizing Deep Learning, pp. 121\u2013 144. Springer",
      "year": 2019
    },
    {
      "authors": [
        "O. Reyes",
        "S. Ventura"
      ],
      "title": "Performing multi-target regression via a parameter sharingbased deep network",
      "venue": "International Journal of Neural Systems",
      "year": 1950
    },
    {
      "authors": [
        "M.T. Ribeiro",
        "S. Singh",
        "C. Guestrin"
      ],
      "title": "why should i trust you?\u201d: Explaining the predictions of any classifier",
      "venue": "Proceedings of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. pp. 1135\u20131144. KDD \u201916, ACM",
      "year": 2016
    },
    {
      "authors": [
        "M.T. Ribeiro",
        "S. Singh",
        "C. Guestrin"
      ],
      "title": "Anchors: High-precision model-agnostic explanations",
      "venue": "Thirty-Second AAAI Conference on Artificial Intelligence. New Orleans, Louisiana, USA",
      "year": 2018
    },
    {
      "authors": [
        "C. Richter",
        "W. Vega-Brown",
        "N. Roy"
      ],
      "title": "Bayesian learning for safe high-speed navigation in unknown environments",
      "venue": "A., B., W., B. (eds.) Robotics Research, pp. 325\u2013341. Springer",
      "year": 2018
    },
    {
      "authors": [
        "W. Samek",
        "A. Binder",
        "G. Montavon",
        "S. Lapuschkin",
        "K.R. M\u00fcller"
      ],
      "title": "Evaluating the visualization of what a deep neural network has learned",
      "venue": "IEEE transactions on neural networks and learning systems 28(11), 2660\u20132673",
      "year": 2016
    },
    {
      "authors": [
        "K. Simonyan",
        "A. Zisserman"
      ],
      "title": "Very deep convolutional networks for large-scale image recognition",
      "venue": "International Conference on Learning Representations",
      "year": 2015
    },
    {
      "authors": [
        "J. Springenberg",
        "A. Dosovitskiy",
        "T. Brox",
        "M. Riedmiller"
      ],
      "title": "Striving for simplicity: The all convolutional net",
      "venue": "Proceedings of the Iternational Conference on Learning Representation (workshop track). San Diego, CA",
      "year": 2015
    },
    {
      "authors": [
        "G. Tessitore",
        "R. Prevete"
      ],
      "title": "Designing structured sparse dictionaries for sparse representation modeling",
      "venue": "Computer Recognition Systems 4, pp. 157\u2013166. Springer",
      "year": 2011
    },
    {
      "authors": [
        "G. Tessitore",
        "R. Prevete"
      ],
      "title": "Designing structured sparse dictionaries for sparse representation modeling",
      "venue": "Burduk, R., Kurzynski, M., Wozniak, M., Zolnierek, A. (eds.) Computer Recognition Systems 4, pp. 157\u2013166. Springer",
      "year": 2011
    },
    {
      "authors": [
        "A. Vedaldi",
        "S. Soatto"
      ],
      "title": "Quick shift and kernel methods for mode seeking",
      "venue": "European conference on computer vision. pp. 705\u2013718. Springer",
      "year": 2008
    },
    {
      "authors": [
        "Q. Zhang",
        "S. Zhu"
      ],
      "title": "Visual interpretability for deep learning: a survey",
      "venue": "Frontiers of Information Technology & Electronic Engineering 19(1), 27\u201339",
      "year": 2018
    }
  ],
  "sections": [
    {
      "text": "We experimentally evaluate the proposed approach to explain the decisions made by an Imagenet pre-trained VGG16 model on STL-10 images and by a customised model trained on the JAFFE dataset, using two different computational definitions of middle-level features and compare it with two different XAI middle-level methods. The results show that our approach can be used successfully in different computational definitions of middle-level explanations.\nKeywords: XAI \u00b7 Machine Learning \u00b7 Middle-level features."
    },
    {
      "heading": "1 Introduction",
      "text": "In the last years, Machine Learning (ML) approaches have been widely used to address several challenges in Artificial Intelligence (AI), such as image [30] and text classification [10] problems, multi-target regression [24] and robot navigation [27]. However, a large part of these approaches suffers from a pervasive lack of transparency also connected to the problem of explaining their behaviour in terms that ae easy to understand for human beings [17]. Indeed, it seems that the better ML systems become in terms of their performance, the harder it is to understand the underlying mechanisms and explain their behaviours [1]. For this reason, ML systems are often considered as black-box systems [1] insofar as their decisions are hard to interpret in terms of meaningful input features. Thus,\nar X\niv :2\n01 0.\n08 63\n9v 1\n[ cs\n.L G\n] 1\n6 O\nct 2\ngenerating explanations for ML system behaviours that are understandable to human beings is a central scientific and technological issue addressed by the rapidly growing AI research area of eXplainable Artificial Intelligence (XAI).\nThe literature counts various strategies to make ML systems - especially those endowed with Deep Neural Network (DNN) architectures [20] - interpretable and explainable [11,21]. XAI approaches to the explanation problem can be classified in several ways according to which properties are taken into account [1,14,34,22]. A key distinction is between low-level and middle-level input feature approaches. Low-level feature approaches to XAI attempt to explain the output of an ML system in terms of low-level features of the input such as pixels in case of image classification problems. One of the most successful methods for this type of approaches is the Layer-wise Relevance Propagation (LRP) [6], which associates a relevance value to each input element (to each pixel in the case of images) as an explanation of the ML model response. Thus, human users are left with a significant interpretive burden: starting from the relevance values of each input element (pixel), one has to identify properties of the overall input that are perceptually salient for the human visual system. A method which attemmpt to alleviate this drawback of low-level approaches to explanation have been proposed in [3,4], where explanations are provided in terms of middle-level properties (atoms) of the input which represent perceptually salient input parts.\nA popular method which is also based on middle-level properties of the input is LIME [25], which returns a set of image parts (superpixels), that could have driven the ML model to the given answer. This set of superpixels can be then considered as an explanation to the ML model response. To the best of our knowledge, these types of approaches can be classified as model-agnostic.\nModel-agnostic approaches correspond to XAI methods which are independent of the ML model to be explained [1], i.e., model-agnostic solutions are built relying only the relation between ML model inputs and outputs, without any consideration about the ML model internal state. Although this property ensures the applicability of these approaches to any ML model, on the other hand, how we will discuss more in details in Section 3, the explanations of the modelagnostic methods could not be fully related to the actual causal relationships between model\u2019s inputs and outputs which have contributed to the given model response. For instance, LIME returns an explanation inspecting the behaviour of the model in the neighbourhood of the input, but nothing ensures that, for that particular input instance, the answer of the classifier has a totally different explanation (for example, a particular on the background of the specific input image which the model has already seen during the training stage, making the model biased).\nIn this paper, we propose a new method, that we called Middle-Level Feature Relevance (MLFR), based on a variation of LRP that, instead of returning a relevance value for each input pixel, returns relevance values for a given set of middle-level features. This method can be applied whenever a) the input of a ML system can be encoded and decoded on the basis of middle-level features, and b) LRP can be applied on both the ML model and the decoder (see Section\n3 for further details). In this sense we consider MLFR as a a general framework insofar as it can be applied on several different computational definitions of middle-level features as we will discuss in Section 3. Notice that MLFR is not a model-agnostic approach, however it can be applied to a large class of ML models as well as LRP [6], for example feedforward neural networks architectures such as shallow network and deep networks.\nThis paper is structured as follows. Section 2 briefly reviews the related literature; Section 3 describes the proposed architecture; experiments and results are discussed in Section 4; the concluding Section 5 summarises the main results of the proposed explanation framework and outlines some future developments."
    },
    {
      "heading": "2 Related works",
      "text": "Many XAI methods have been proposed since explainability is now a sought for requirement for AI solution. The literature proposes several reviews trying to categorise/distinguish the existing methods [20,1,34,14] looking at different properties of the XAI methods. According to these categorisations, our method can be classified as a white-box and local XAI approach. White-box approaches require access to the internal structure of the ML model [1]. By contrast, blackbox, or model-agnostic, approaches provide explanation methods which are independent of the ML model [1], i.e., they need access only to the input-output relations of the ML model. Local approaches provide explanations for each given input, while the goal of global approaches is to produce an explanation for the whole behaviour of the ML system [20].\nMany model-agnostic approaches are based on proxy-models [9,7,23] or some type of maximisation of the ML model response with respect to the input, such as the Activation-Maximisation (AM) method [11]. Proxy models are models behaving similarly to the original model, but in a way that it is easier to explain [12]. Approaches based on AM method enables one to determine the input that makes the output of the ML model as close as possible to the model\u2019s initial response, for example, in case of classification problems, given Ck as the response of the ML model, one maximises the P (Ck|x) with respect to x satisfying some constraints on x. Notice that the explanations of the model-agnostic methods suffer from the lack of information about the actual input-output causal relationships which have contributed to the given ML model answers; thus these explanations may not be related to the specific ML model response to be explained [18].\nAnother critical distinction is based on the granularity level of the explanations. In fact, several XAI solutions provide explanations in terms of low-level input features. For instance, in image classification problems the output of an ML system is explained considering low-level features of the input image in terms of salience maps where to each pixel is associated a relevance value which quantifies the degree of importance of that pixel to cause the ML model response. Among the approaches of this type, Layer-wise Relevance Propagation (LRP) [6], is the most popular in the literature. LRP is a white-box approach, although\nit applies to many ML models such as deep networks. Notice that it is a general framework rather than a specific method insofar as it is defined as a set of constraints that an XAI algorithm should satisfy. Thus, different XAI algorithms with different explanations may be appropriate under these constraints [6]. For example, Deep-Taylor Decomposition [20] can be interpreted as a way of obtaining LRP.\nIn this type of approaches, human users are left with a significant interpretive burden: starting from the relevance values of each input element (pixel), one has to identify properties of the overall input that are perceptually salient for the human visual system. Thus, to alleviate this cognitive burden, an alternative model-agnostic method, called Explanation-Maximization (EM), was proposed in [2,3,4]. EM, which also applies in different areas, was instantiated in the context of image classification systems. EM obtains sets of perceptually salient middle-level properties of input images by applying sparse dictionary learning techniques and a variant of AM. These middle-level properties are used as building blocks for explanations of image classifications. However, this approach suffers from the typical shortcomings of the model-agnostic ones as regards the reliability of the explanations given, as previously discussed. Among other methods using middle-level input features to build explanations about ML model responses, LIME [25] can be considered the most popular in the literature. It is model-agnostic and based on a proxy-model: it explains the output of an ML system by observing its behaviour on perturbations of its input. The input is partitioned in a collection of components (super-pixel in the case of images); perturbed inputs are composed of specific superpositions of these components. Perturbed inputs and outputs are used to construct a local linear model which is used as a simplified proxy for the original ML system in the neighbourhood of the input. Thus, from the proxy, it is possible to infer an explanation of the original ML model response. However, the faithfulness of the proxy with respect to the original model remains an open issue [18]. Other methods, based on LIME, as in Ribeiro et al. [26] and Guidotti et al. [13], return explanations in terms of decision rules that are used as local conditions for decisions.\nThe method we propose in this paper differs from the works mentioned above in the following aspects, as it can be seen as a general framework to obtain middle-level explanations analysing the actual input-output relationship defined by the ML model. Thus, different definitions of middle-level input features with different resulting solutions may be possible under this general framework. The only constraints are that the input can be encoded and decoded based on the defined middle-level input features and that LRP can be applied on both the ML model to be explained and the input decoder."
    },
    {
      "heading": "3 Middle-Level Relevance",
      "text": "Given an ML model M which receives an input x \u2208 Rd and outputs y \u2208 Rc, let us suppose that x can be decomposed in a set of m middle-level features vi each one encoded by a value ui. More formally, we suppose that a decoder\nD : (V,u) \u2212\u2192 x \u2208 Rd exists. Where V = {vi}mi=1 is the set of v\u2019s middlelevel features and u \u2208 Rm encodes x in terms of the middle-level features. For example, in an image classification problem, a possible set of middle-level features can be the result of a segmentation algorithm on the input image x which produces a partition of x in m regions or partitions {Pi}mi=1. Each image\u2019s partition Pi can be represented by a vector v\ni \u2208 Rd such that their summation is equal to x, in this case the decoder is a linear combination of the vi with all the coefficients equal to 1, which represent the encoding of the image x on the basis of the m partitions (see Section 3.1).\nThen, if we can use LRP on both M and D, we can apply it on the model M and use the obtained relevance values to apply LRP on D thus getting a relevance value for each middle-level feature. In other words, we can stack D on the top of M , thus obtaining a new model DM which receives as input u and outputs y, and uses LRP propagation on DM from y to u. Let us take as an example of M a neural network composed of L layers. The LRP procedure computes a set of relevance values for any given layer l composed of kl neurons as the combination of the scores assigned to each neuron of l, representing the importance of each node for the network\u2019s output. The scores are computed by propagating the relevance values from the output layer to the input layer in a back-propagation fashion. Similarly, let us consider a shallow neural network composed of m input values ui, d output neurons with biases equal to 0, the identity as activation functions, and one hidden layer of weights W = V . This network can be seen as a decoder D where the weights associated with the connections going from each input value ui to all output neurons represent the middle-level feature vector v\ni. If we stack the shallow network/decoder D on the top of the L-layer model M , we obtain a new neural network model DM composed of L + 1 layers. Then, we can apply the LRP procedure on the whole DM model and obtain relevance values which designate what input\u2019s middle-level features have most contributed on the outcome yi (see Figure 1). In other words, we search for a relevance vector r \u2208 Rm which helps the user to know each middle-level feature of x how much has contributed to the ML model answer yi. Note that, this approach can be generalised to any decoder to which LRP applies. For instance, we can consider any dictionary learning approach, as for example [15,16,31] (see Section 3.2 for more details), where each input x can be decomposed as x = Vu + , V is a dictionary of middle-level features and is the reconstruction error vector. Also, in this case, we can notice that the decoder can be represented as a shallow neural network having the dictionary elements as weights and the biases in terms of the reconstruction error vector (see Section 3.2).\nIn the remainder of this section, we will describe two alternative ways (segmentation and dictionary learning) to obtain a decoder LRP method can be applied to, in more details. We experimentally tested our framework using both methods."
    },
    {
      "heading": "3.1 Decoder by super-pixel segmentation",
      "text": "Given an image x \u2208 Rd, we can obtain a partition of x composed of m elements Ph through any segmentation algorithm. We can associate to each element Ph a vector pvh \u2208 Rd such that pvhi = 1 if xi \u2208 Ph, otherwise pvhi = 0. Thus, each element Ph can be represented by the element-wise product between x and pv\nh, i.e., vh = pvh x, since this operation products selects all the pixels belonging to the element Ph.\nConsequently, we can decompose x as x = m\u2211\nh=1\nuhv h, with uh = 1. Then, the\ndecoder D is a linear combination of the vh with all the coefficients equal to 1,\nwhich represent the encoding of the image x on the basis of the m partition\u2019s elements.\nFollowing [25], in this paper we use the quickshift segmentation algorithm [33] where the elements of the partition are called super-pixels.\nWe assume that a possible explanation to the output of a given classifier can be obtained in terms of relevant super-pixels, where the relevance can be computed using an LRP-based procedure."
    },
    {
      "heading": "3.2 Decoder by Sparse Dictionary Learning methods",
      "text": "A sparse dictionary learning problem (see, for example, [32]) is a minimisation problem that one can formally describe as follows.\narg min U,V ||X \u2212 V U ||2F + \u03b31 k\u2211 i=1 \u2126V (vi)\ns.t. \u2200j,\u2126U (ui) < \u03b32\n(1)\nwhereX \u2208 Rd\u00d7n is composed of n experimental observations which are expressed as column vector xi \u2208 Rd, V is the dictionary, and the k columns vi of V are the dictionary elements or atoms, subject to some sparsity constraint possibly. Each column of X is approximated by a linear combination of the k columns of V , subjects to some sparsity constraint potentially. Thus, U \u2208 Rk\u00d7n is the matrix of the linear combination coefficients, i.e., the i-th column of U , ui, corresponds to the k coefficients of the linear combination of the k columns of V to approximate xi, the i-th column of X. \u2126V and \u2126U are some norms or quasi-norms that constrain or regularise the solutions of the minimisation problem, and \u03b31 \u2265 0 and \u03b32 \u2265 0 are parameters that control to what extent the dictionary and the coefficients are regularised.\nElements of a dictionary can be used to compute explanations of a ML model response in terms of middle-level input features [2,5,3,4].\nFor the experiments presented in this paper, we obtain the dictionaries from a specific sparse dictionary learning method based on SSPCA[15]. However, any dictionary learning/sparse coding method able to produce dictionaries that can be considered human-understandable can be used [2].\nGiven a dictionary V and an experimental observation x one can solve the minimisation problem as expressed in eq. 1 with respect to the coefficients only, finding a single column vector u. Consequently, x\u0303 = V u is an approximation of x with an error for each component equal to h = xh \u2212 x\u0303h. Then, the decoder D can be represented as a shallow neural network composed of just one weight layer W , k input values and d output neurons. Each output neuron j has the identity as activation function and the bias equal to j . The weights associated to the connection going from the j-th input value to all the output neurons correspond to j-th V \u2019s column. Consequently, given the decomposition of x as V u the decoder D receives u as input and outputs x."
    },
    {
      "heading": "4 Experimental assessment",
      "text": "In this section, we describe the experiments performed and show the results obtained. We show a set of explanations produced by our approach using two different experimental setups.\nThe former uses as middle-level features the super-pixel segmentation schema described in 3.1; the latter adopts the sparse dictionary approach described in 3.2. For the segmentation-based experiments, we used as classifier a VGG-16 [29] neural network trained on Imagenet, and as input images a subset of the STL10 dataset [8]. For the dictionary-based experiments, we use the JAFFE dataset [19] and a custom neural network trained from scratch with a final accuracy of the 94% on a test set. We chose to use a custom model because, to the best of our knowledge, there are no reference models for this particular dataset in the current literature. Notice that for this type of middle-level features we used a more simple dataset since dictionary learning methods on large datasets can be very expensive in terms of computational costs.\nWe compare the results obtained by the proposed method (MLFR) with two related methods proposed in the literature, LIME [25] and EM [2,4], and with a standard low-level feature method as LRP [6]. Notice that as we discussed in Section 2 the explanations returned by LIME and EM are based on features which can be considered of middle-level, but, differently from the MLFR approach, they are built in a black-box approach relying on a proxy model instead of the actual model in case of LIME, and in terms of dictionary elements by a variant of the activation-maximisation method, in case of EM. For the segmentation-based approach, we compared MLFR with LIME and LRP. For the dictionary-based approach, we compared our results with the ones produced by EM and LRP. The segments and the dictionaries are obtained respectively using Quickshift [33] (that is the same algorithm used by LIME to make the superpixel segmentation) and SSPCA [15].\nA visual comparison is not enough, and to give a quantitative evaluation of our results, we use the same strategy introduced in [28] and described in Section 4.2."
    },
    {
      "heading": "4.1 Qualitative results",
      "text": "Some results of the two proposed strategies are shown in Figure 2 for the superpixels-based approach and in Figure 3 for the dictionary-based approach. To make a comparison, we also report the explanations given by LIME and LRP methods for the superpixels-based approach and EM for the dictionary-based approach. For each input, we show the superposition of the three most relevant segments/atoms for LIME, EM and MLFR, and the heatmap produced by LRP.\nWith respect to LIME and EM, we can show that in several cases, the explanations produced by MLFR can be considered closer to what a human being expects from a classification system. For example, we expect that the output \u201chummingbird\u201d and \u201cindigo bunting\u201d (figure 2, first and second row, first column) is due mainly by the presence of the main components of a bird in the\nimage, neglecting non-relevant part as background sprigs. Similar considerations can be done for the \u201chartebeest\u201d and the \u201cgazelle\u201d (figure 2, fourth and fifth row, fifth column) and several other inputs shown in the figure. In particular, the \u201cbedington terrier\u201d input (figure 2, fifth row, fifth column) provides an interesting case due to the presence of several hypothetical relevant candidates (the several human being parts) which can lead the classifier toward different classification. The proposed method, in agreement with LRP, highlights that the dog face is one of the main discriminative parts behind the classifier\u2019s choice. The different results returned by LIME can be due to several factors, such as a sub-optimal training procedure of the proxy model or the inadequacy of the proxy model in representing the real one.\nSimilar consideration can be done for the results shown in figure 3 inherent the dictionary-based approach. We show inputs for several classes of the Jaffe dataset (SAD, SURPRISE, HAPPY, FEAR, ANGRY) and the results obtained respectively by the EM method and the proposed MLFR. It is possible to see how the proposed method highlights atoms which better characterise the faces, as we would expect by an emotion classifier. MLFR highlights details concerning facial expressions as the open mouth and the eyes for the inputs classified as \u201cSURPRISE\u201d or the smiling expression for the input classified for \u201cHAPPY\u201d (for example, on the figure 3 see the results of the inputs on the first, fourth and seventh columns of the 5th and 8th row). The results produced by EM method, instead, seems more confused and less clear and intuitive. As far as the EM method is not based on proxy models, it is again a black-box approach based only on the input/output relations of the classifier, so without any knowledge to the real inner state of the model. Furthermore, EM needs several hyperparameters to be set, which can affect the reliability of the results produced.\nNotice that for reasons of space we do not report the results obtained by LRP, since the qualitive comparison is similar to the one for the STL10 dataset."
    },
    {
      "heading": "4.2 Quantitative evaluation",
      "text": "In the previous section, we show the explanations obtained in terms of the most relevant middle-level features selected by MLFR compared against the ones selected by some related works proposed in the literature. However, all the consideration we made are based only on subjective evaluations, and an objective and quantitative evaluation of the explanation methods is still an open research problem.\nA possible quantitative evaluation framework was proposed in [28] with region flipping, a generalisation of the pixel-flipping measure proposed in [6]. In a nutshell, given an image classification to explain, regions of a given size are substituted iteratively, following the descending relevance order assigned to the central pixel (MoRF, Most Relevant First) by the explanation method. At each step, the difference between the original class score returned by the model and the score returned on the perturbed input is computed, generating a curve (MoRF curve). We expect that the better the explanation method is, the stronger the difference between the scores is. Repeating this process for several images and\naveraging between them, it is possible to obtain the Area Over the MoRF Perturbation Curve (AOPC):\nAOPC = 1\nL+ 1 < L\u2211 k=0 f(x(0))\u2212 f(x(k)) >p(x)\nwhere < \u00b7 >p(x) is the average over the dataset images, L is the number of regions and x(k) is the input at k\u2212th perturbation step. If the regions are well-ranked (so, relevant regions have a higher relevance), we expect that the resulting AOPC values are large, so we can infer that the largest the AOPC value is, the better the explanation method is. The original region-flipping method was originally defined for pixel-based heatmaps using regions of fixed size (9\u00d7 9 in [28]). However, it is easily adapted to our proposed method and LIME, considering that each middle-level feature is a single region. As a perturbation scheme, we adopt the same used in [28], changing each pixel in the region with a value sampled from the Uniform distribution. In figure 4a we plot the AOPC curve for LIME and our proposed method on the VGG16 model, showing that MLFR outperforms LIME in terms of AOPC curve, suggesting that the former, on average, gives a more reliably relevance score respect to the latter. We hypothesise that LIME, exploiting a proxy classifier which emulates the real one, may not capture the real \u201creasons\u201d behind the choices made by a classifier, so assigning scores to the features in a manner which not reflect the real inner state of the clas-\nsifier. Similar results are shown in figure 4b, where the results of the proposed approach are compared with the EM method. Again, in this case, the proposed method shows better results in terms of AOPC values, giving better reliability to the explanations produced."
    },
    {
      "heading": "5 Conclusion",
      "text": "In this work, we propose MLFR, a novel XAI method based on middle-level features. The proposed method generalises the well-known LRP method, initially proposed for low-level features (such as pixels for image domain), to middle-level features, returning data representations which can be interpreted by a human. We describe how the proposed method can be easily adapted to several classes of middle-level features. For instance, we show how two different middle-level input representations can be suitable for the proposed method, the former based on image segments directly obtained from the input to explain, the latter on a more general set of elements which can be constructed through some dictionary learning approach. However, nothing prevents to use other representations.\nTo evaluate the proposed method, we adapt the quantitative measure described in [28], proposed initially for pixelwise-based methods, to middle-level feature methods, and we make a comparison with others middle-level features approaches present in literature. The results of the experiments that we carried out are encouraging, both under the qualitative point of view, giving explanations that can be easily interpretable by the human being, and the quantitative point of view, giving performances in terms of AOPC curve which are comparable to other methods present in the current literature."
    }
  ],
  "title": "A general approach to compute the relevance of middle-level input features",
  "year": 2020
}
