{
  "abstractText": "Combating fake news needs a variety of defense methods. Although rumor detection and various linguistic analysis techniques are common methods to detect false content in social media, there are other feasible mitigation approaches that could be explored in the machine learning community. In this paper, we present open issues and opportunities in fake news research that need further attention. We first review different stages of the news life cycle in social media and discuss core vulnerability issues for news feed algorithms in propagating fake news content with three examples. We then discuss how complexity and unclarity of the fake news problem limits the advancements in this field. Lastly, we present research opportunities from interpretable machine learning to mitigate fake news problems with 1) interpretable fake news detection and 2) transparent news feed algorithms. We propose three dimensions of interpretability consisting of algorithmic interpretability, human interpretability, and the inclusion of supporting evidence that can benefit fake news mitigation methods in different ways.",
  "authors": [
    {
      "affiliations": [],
      "name": "Sina Mohseni"
    },
    {
      "affiliations": [],
      "name": "Eric D. Ragan"
    },
    {
      "affiliations": [],
      "name": "Xia Hu"
    }
  ],
  "id": "SP:87fe11ce8918304b7620cde498598270a3064e9b",
  "references": [
    {
      "authors": [
        "Darius Afchar",
        "Vincent Nozick",
        "Junichi Yamagishi",
        "Isao Echizen"
      ],
      "title": "Mesonet: a compact facial video forgery detection network",
      "venue": "2018 IEEE International Workshop on Information Forensics and Security (WIFS), pages 1\u20137. IEEE,",
      "year": 2018
    },
    {
      "authors": [
        "Sadia Afroz",
        "Michael Brennan",
        "Rachel Greenstadt. Detecting hoaxes"
      ],
      "title": "frauds",
      "venue": "and deception in writing style online. In Security and Privacy (SP), 2012 IEEE Symposium on, pages 461\u2013475. IEEE,",
      "year": 2012
    },
    {
      "authors": [
        "Hunt Allcott",
        "Matthew Gentzkow"
      ],
      "title": "Social media and fake news in the 2016 election",
      "venue": "Journal of Economic Perspectives, 31(2):211\u201336,",
      "year": 2017
    },
    {
      "authors": [
        "Bessi",
        "Ferrara",
        "2016] Alessandro Bessi",
        "Emilio Ferrara"
      ],
      "title": "Social bots distort the 2016 us presidential election online discussion",
      "year": 2016
    },
    {
      "authors": [
        "Aparna Bharati",
        "Daniel Moreira",
        "Joel Brogan",
        "Patricia Hale",
        "Kevin W Bowyer",
        "Patrick J Flynn",
        "Anderson Rocha",
        "Walter J Scheirer"
      ],
      "title": "Beyond pixels: Image provenance analysis leveraging metadata",
      "venue": "arXiv preprint arXiv:1807.03376,",
      "year": 2018
    },
    {
      "authors": [
        "Engin Bozdag. Bias in algorithmic filtering",
        "personalization"
      ],
      "title": "Ethics and information technology",
      "venue": "15(3):209\u2013227,",
      "year": 2013
    },
    {
      "authors": [
        "Carlos Castillo",
        "Marcelo Mendoza",
        "Barbara Poblete. Predicting information credibility in time-sensitive social media"
      ],
      "title": "Internet Research",
      "venue": "23(5):560\u2013 588,",
      "year": 2013
    },
    {
      "authors": [
        "Ekstrand et al",
        "2018] Michael D Ekstrand",
        "Mucun Tian",
        "Ion Madrazo Azpiazu",
        "Jennifer D Ekstrand",
        "Oghenemaro Anuyah",
        "David McNeill",
        "Maria Soledad Pera"
      ],
      "title": "All the cool kids, how do they fit in?: Popularity and demographic biases in recommender evaluation",
      "year": 2018
    },
    {
      "authors": [
        "Daniel Geschke",
        "Jan Lorenz",
        "Peter Holtz"
      ],
      "title": "The triple-filter bubble: Using agent-based modelling to test a meta-theoretical framework for the emergence of filter bubbles and echo chambers",
      "venue": "British Journal of Social Psychology,",
      "year": 2018
    },
    {
      "authors": [
        "David Gunning"
      ],
      "title": "Explainable artificial intelligence (xai)",
      "venue": "Defense Advanced Research Projects Agency (DARPA), nd Web,",
      "year": 2017
    },
    {
      "authors": [
        "Mario Haim",
        "Andreas Graefe",
        "HansBernd Brosius"
      ],
      "title": "Burst of the filter bubble? effects of personalization on the diversity of google news",
      "venue": "Digital Journalism, 6(3):330\u2013343,",
      "year": 2018
    },
    {
      "authors": [
        "Lei Hou",
        "Xue Pan",
        "Kecheng Liu. Balancing the popularity bias of object similarities for personalised recommendation"
      ],
      "title": "The European Physical Journal B",
      "venue": "91(3):47,",
      "year": 2018
    },
    {
      "authors": [
        "Zhiwei Jin",
        "Juan Cao",
        "Yongdong Zhang",
        "Jiebo Luo. News verification by exploiting conflicting social viewpoints in microblogs"
      ],
      "title": "In AAAI",
      "venue": "pages 2972\u2013 2978,",
      "year": 2016
    },
    {
      "authors": [
        "Thibaut Julliand",
        "Vincent Nozick",
        "Hugues Talbot. Image noise",
        "digital image forensics. In International Workshop on Digital Watermarking"
      ],
      "title": "pages 3\u201317",
      "venue": "Springer,",
      "year": 2015
    },
    {
      "authors": [
        "Antonis Kalogeropoulos",
        "Rasmus Kleis Nielsen. Social inequalities in news consumption. In FACTSHEET",
        "NEWS MEDIA DIGITAL MEDIA"
      ],
      "title": "pages 461\u2013475",
      "venue": "Reuters Institute for the Study of Journalism,",
      "year": 2018
    },
    {
      "authors": [
        "Kulesza et al",
        "2015] Todd Kulesza",
        "Margaret Burnett",
        "Weng-Keen Wong",
        "Simone Stumpf"
      ],
      "title": "Principles of explanatory debugging to personalize interactive machine learning",
      "venue": "In Proceedings of the 20th international conference on intelligent user interfaces,",
      "year": 2015
    },
    {
      "authors": [
        "Sejeong Kwon",
        "Meeyoung Cha",
        "Kyomin Jung",
        "Wei Chen",
        "Yajun Wang. Prominent features of rumor propagation in online social media"
      ],
      "title": "In 2013 IEEE 13th International Conference on Data Mining",
      "venue": "pages 1103\u20131108. IEEE,",
      "year": 2013
    },
    {
      "authors": [
        "Elisabeth Lex",
        "Mario Wagner",
        "Dominik Kowald"
      ],
      "title": "Mitigating confirmation bias on twitter by recommending opposing views",
      "venue": "arXiv preprint arXiv:1809.03901,",
      "year": 2018
    },
    {
      "authors": [
        "Jing Ma",
        "Wei Gao",
        "Prasenjit Mitra",
        "Sejeong Kwon",
        "Bernard J Jansen",
        "Kam-Fai Wong",
        "Meeyoung Cha. Detecting rumors from microblogs with recurrent neural networks"
      ],
      "title": "In IJCAI",
      "venue": "pages 3818\u20133824,",
      "year": 2016
    },
    {
      "authors": [
        "Sina Mohseni",
        "Niloofar Zarei",
        "Eric D Ragan"
      ],
      "title": "A survey of evaluation methods and measures for interpretable machine learning",
      "venue": "arXiv preprint arXiv:1811.11839,",
      "year": 2018
    },
    {
      "authors": [
        "Eli Pariser"
      ],
      "title": "The filter bubble: What the Internet is hiding from you",
      "venue": "Penguin UK,",
      "year": 2011
    },
    {
      "authors": [
        "Popat et al",
        "2018a] Kashyap Popat",
        "Subhabrata Mukherjee",
        "Jannik Str\u00f6tgen",
        "Gerhard Weikum"
      ],
      "title": "Credeye: A credibility lens for analyzing and explaining misinformation",
      "venue": "In Companion of the The Web Conference 2018 on The Web Conference",
      "year": 2018
    },
    {
      "authors": [
        "Kashyap Popat",
        "Subhabrata Mukherjee",
        "Andrew Yates",
        "Gerhard Weikum"
      ],
      "title": "Declare: Debunking fake news and false claims using evidence-aware deep learning",
      "venue": "arXiv preprint arXiv:1809.06416,",
      "year": 2018
    },
    {
      "authors": [
        "Martin Potthast",
        "Sebastian K\u00f6psel",
        "Benno Stein",
        "Matthias Hagen. Clickbait detection. In European Conference on Information Retrieval"
      ],
      "title": "pages 810\u2013817",
      "venue": "Springer,",
      "year": 2016
    },
    {
      "authors": [
        "Martin Potthast",
        "Johannes Kiesel",
        "Kevin Reinartz",
        "Janek Bevendorff",
        "Benno Stein"
      ],
      "title": "A stylometric inquiry into hyperpartisan and fake news",
      "venue": "arXiv preprint arXiv:1702.05638,",
      "year": 2017
    },
    {
      "authors": [
        "Walter Quattrociocchi",
        "Antonio Scala"
      ],
      "title": "and Cass R Sunstein",
      "venue": "Echo chambers on facebook.",
      "year": 2016
    },
    {
      "authors": [
        "Victoria L Rubin",
        "Tatiana Lukoianova. Truth",
        "deception at the rhetorical structure level"
      ],
      "title": "Journal of the Association for Information Science and Technology",
      "venue": "66(5):905\u2013917,",
      "year": 2015
    },
    {
      "authors": [
        "Victoria Rubin",
        "Niall Conroy",
        "Yimin Chen",
        "Sarah Cornwell"
      ],
      "title": "Fake news or truth? using satirical cues to detect potentially misleading news",
      "venue": "Proceedings of the Second Workshop on Computational Approaches to Deception Detection, pages 7\u201317,",
      "year": 2016
    },
    {
      "authors": [
        "Karishma Sharma",
        "Feng Qian",
        "He Jiang",
        "Natali Ruchansky",
        "Ming Zhang",
        "Yan Liu"
      ],
      "title": "Combating fake news: A survey on identification and mitigation techniques",
      "venue": "arXiv preprint arXiv:1901.06437,",
      "year": 2019
    },
    {
      "authors": [
        "Kai Shu",
        "Amy Sliva",
        "Suhang Wang",
        "Jiliang Tang",
        "Huan Liu"
      ],
      "title": "Fake news detection on social media: A data mining perspective",
      "venue": "ACM SIGKDD Explorations Newsletter, 19(1):22\u201336,",
      "year": 2017
    },
    {
      "authors": [
        "Kai Shu",
        "Suhang Wang",
        "Huan Liu"
      ],
      "title": "Exploiting tri-relationship for fake news detection",
      "venue": "arXiv preprint arXiv:1712.07709,",
      "year": 2017
    },
    {
      "authors": [
        "Kai Shu",
        "Deepak Mahudeswaran",
        "Suhang Wang",
        "Dongwon Lee",
        "Huan Liu"
      ],
      "title": "Fakenewsnet: A data repository with news content",
      "venue": "social context and dynamic information for studying fake news on social media. arXiv preprint arXiv:1809.01286,",
      "year": 2018
    },
    {
      "authors": [
        "Nikita Spirin",
        "Jiawei Han"
      ],
      "title": "Survey on web spam detection: principles and algorithms",
      "venue": "ACM SIGKDD explorations newsletter, 13(2):50\u201364,",
      "year": 2012
    },
    {
      "authors": [
        "Eugenio Tacchini",
        "Gabriele Ballarin",
        "Marco L Della Vedova",
        "Stefano Moret",
        "Luca de Alfaro"
      ],
      "title": "Some like it hoax: Automated fake news detection in social networks",
      "venue": "arXiv preprint arXiv:1704.07506,",
      "year": 2017
    },
    {
      "authors": [
        "Valcarce et al",
        "2018] Daniel Valcarce",
        "Alejandro Bellog\u0131\u0301n",
        "Javier Parapar",
        "Pablo Castells"
      ],
      "title": "On the robustness and discriminative power of information retrieval metrics for top-n recommendation",
      "venue": "In Proceedings of the 12th ACM Conference on Recommender Systems,",
      "year": 2018
    },
    {
      "authors": [
        "Andreas Vlachos",
        "Sebastian Riedel"
      ],
      "title": "Fact checking: Task definition and dataset construction",
      "venue": "Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science, pages 18\u201322,",
      "year": 2014
    },
    {
      "authors": [
        "Yang et al",
        "2019] Fan Yang",
        "Shiva K. Pentyala",
        "Sina Mohseni",
        "Mengnan Du",
        "Hao Yuan",
        "Rhema Linder",
        "Eric D. Ragan",
        "Shuiwang Ji",
        "Xia (Ben) Hu"
      ],
      "title": "Xfake: Explainable fake news detector with visualizations",
      "venue": "In Companion of the The Web Conference",
      "year": 2019
    },
    {
      "authors": [
        "Robert B Zajonc"
      ],
      "title": "Mere exposure: A gateway to the subliminal",
      "venue": "Current directions in psychological science, 10(6):224\u2013228,",
      "year": 2001
    },
    {
      "authors": [
        "Matthew D Zeiler",
        "Rob Fergus. Visualizing",
        "understanding convolutional networks. In European conference on computer vision"
      ],
      "title": "pages 818\u2013833",
      "venue": "Springer,",
      "year": 2014
    },
    {
      "authors": [
        "Xinyi Zhou",
        "Reza Zafarani"
      ],
      "title": "Fake news: A survey of research",
      "venue": "detection methods, and opportunities. arXiv preprint arXiv:1812.00315,",
      "year": 2018
    },
    {
      "authors": [
        "Arkaitz Zubiaga",
        "Ahmet Aker",
        "Kalina Bontcheva",
        "Maria Liakata",
        "Rob Procter"
      ],
      "title": "Detection and resolution of rumours in social media: A survey",
      "venue": "ACM Computing Surveys (CSUR), 51(2):32,",
      "year": 2018
    }
  ],
  "sections": [
    {
      "heading": "1 Introduction",
      "text": "Fake news has many faces. Allcott and Gentzkow [2017] define fake news as an article that is intentionally created and verifiably false, however, it may hide behind deceptive writing or behind an innocent headline. In practice, every time we close a door to prevent the propagation of fake news, it enters from another. Fake news does not only spread due to intentional and malicious dissemination, but also thrives simply because it is an easy sell and commonly has a very large audience through social media. Although social media was primarily designed to benefit people with information sharing, nowadays it also contains many forms of misinformation and disinformation. The complex nature of news veracity analysis and multi-modal information sources contribute to the fake news identification problem. Even though many fake news related concepts have been well studied in machine learning research, the grand challenge of fake news is yet to\nbe overcome. The importance of analyzing information truthfulness is greater than ever with the increasing popularity and impact of social media among people.\nBesides fake news detection methods as a primary solution, news-feed algorithms (identical to news recommenders in most cases) are also vulnerable to data misuse and attacks that could result in the spread of fake news. The popularity of social media has resulted in large, continuously updating collections of user data. Alongside users\u2019 activity in sharing information to socialize and express their opinions in a virtual social environment, machine learning algorithms constantly process user activity and content for personalized content and targeted advertisement on large scale. However, with the growing amount of user data in social media, the implications of personalized data for the dissemination and consumption of news has caught the attention of many, especially given evidence of the influence of malicious social media accounts on the spread of fake news to bias users during the 2016 US election [Bessi and Ferrara, 2016]. Studies show the targeted distribution of erroneous or misleading \u201cfake news\u201d may have resulted in large-scale manipulation of users\u2019 news feeds as part of the intense competition for attention in the digital media space [Kalogeropoulos and Nielsen, 2018].\nInterpretable machine learning algorithms are gaining increasing attention as people are interacting more with machine learning products in day-to-day life [Gunning, 2017]. Interpretable algorithms in social media applications can assist users in identifying biased algorithms by explaining why a certain recommendation or decision is made. Transparency is an essential need since biased algorithms at the decisionmaking and information-distribution levels may cause unintentional discrimination that could result in the loss of opportunities or social stigmatization at a large scale. Lack of explanation of how the content is selected for the user may result in unaware users who think they have access to all available information rather than only a small subset of news. For example, research shows personalized news feed algorithms are not immune to bias [Bozdag, 2013] and can even cause intellectual isolation [Pariser, 2011] over time.\nIn this paper, we present open issues and opportunities in fake news research that would benefit from further attention from the machine learning community. Rather than focusing only on fake news detection methods, we also present research opportunities to mitigate the circulation of false con-\nar X\niv :1\n90 4.\n03 01\n6v 1\n[ cs\n.S I]\n4 A\npr 2\n01 9\ntent in social media. We first review current fake news detection and mitigation methods at different stages of the news life cycle: creation of news, distribution of news, and consumption of news. Next, we open the discussion on open issues in fake news research. We review how complexity and unclarity of the fake news detection problem limit the advancements in this field. For a broader investigation, we also discuss the vulnerability of news feed algorithms in propagating fake news content, affecting news diversity, and hindering credibility. At last, we present interpretability as a solution for improving fake news methods through 1) interpretable fake news detection and 2) transparent news feed algorithms. We propose three dimensions of interpretability consisting of algorithmic interpretability, human interpretability, and the inclusion of supporting evidence that can benefit fake news mitigation methods in different ways."
    },
    {
      "heading": "2 News Life Cycle in Social Media",
      "text": "Different fake news surveys (e.g., [Shu et al., 2017a], [Zhou and Zafarani, 2018], [Sharma et al., 2019] and [Zubiaga et al., 2018]) provide comprehensive reviews of fake news definitions, data mining methods, training data sets, and recognition metrics for fake news research. However, these works lack to acknowledge an important missing piece in the current fake news mitigation research. In this section, we emphasize on a research gap in studying effects of news recommendation algorithms on the spread of fake content and crediting unreliable sources (see Figure 1). In order to open the discussion toward research limitations and opportunities, we briefly review current fake content detection approaches during three main stages of news life cycle in social media."
    },
    {
      "heading": "2.1 Creation of News",
      "text": "The first stage is to detect fake content at the news creation step which traditionally is done with human review through expert review and crowdsourcing techniques at the early stages. Experts review the truthfulness of the news by evidence and determine whether claims are accurate or false (partially or entirely). Fact checking is a knowledgebased approach usually done by fact-checking organization (e.g., Politifact.com and Snopes.com) to judging the veracity of news piece with external references [Vlachos and Riedel, 2014]. However, expert-review fact-checking methods are time-consuming, expensive, and not scalable for stopping the spread of fake content in social media.\nMachine learning methods also analyze falsified context (not limited to news or social media) using various types of data mining and machine learning techniques. One approach, for instance, is to use linguistic features to analyze writing styles to detect possible false content [Afroz et al., 2012]. For example, recognizing deception-oriented [Rubin and Lukoianova, 2015] and hyper-partisan content [Potthast et al., 2017] can be used as a basis for detecting intentionally falsified information. Also, various spam detection [Spirin and Han, 2012] and satire news detection [Rubin et al., 2016] methods. Another approach is to use clickbait detection algorithms [Potthast et al., 2016] to analyze inconsistency between headlines and content of the news for possible fake news detection.\nAdditionally, fact-checking is not limited to the correctness of textual content\u2014images and videos may be evaluated or used as evidence as well. For the cases of forged images and videos, researchers use deep learning methods (e.g., [Julliand et al., 2015; Afchar et al., 2018]) to detect falsified contents. New methods like provenance analysis have also been utilized for content validation via generating provenance graph of images as the same content is shared and modified over time [Bharati et al., 2018]."
    },
    {
      "heading": "2.2 Distribution of News",
      "text": "The next stage of news life in social media is the distribution of content via search engines and news feed algorithms. Although distribution of news can be an effective stage to combat fake news distribution, machine learning research communities paid little attention to the importance of news search engines, news recommender algorithms, and daily news feeds in propagating fake news. For example, echo chambers (discusses in section 3.2) are from social media vulnerability points that create and propagate false information. Multiple sources of evidence show personalized news feed algorithms can have drastic effects on news diversity and the creation of echo chambers in social media. In a recent study by Geschke et al. [2018] presented an agent-based simulation of different information filtering scenarios that may contribute to social fragmentation of users into distinct echo chambers. Their observation of agent-based modeling found that social media filters can boost social polarization and lessen the interconnections of social media echo chambers. In the next sections, we further discuss the accountability of news feed algorithms for propagating fake content by creating echo chambers and\npropose interpretability as a potential solution for this problem."
    },
    {
      "heading": "2.3 Consumption of News",
      "text": "The final stage of analyzing fake news is to process social media users\u2019 stance, analyze news propagation patterns, and estimate news source credibility to find possible false content in social media. Research on social media data mining show dominant results in detecting fake content and account, however, utilizing social media data means waiting until the fake content is already exposed to the users. This shows a trade-off between leveraging rich social data for fake news detection and waiting until a group of users is exposed to fake content.\nIn addition to analyzing the news by their content, other social media information such as source credibility [Castillo et al., 2013], users\u2019 stance [Jin et al., 2016], and news temporal spreading pattern [Kwon et al., 2013] have been used to assess the veracity of the news. Such social features can be applied to user groups to evaluate the credibility of specific news pieces by considering the stance of a group of users for the news topics [Tacchini et al., 2017]. Similarly, rumor detection methods aim to detect a track of posts discussing a specific topic [Ma et al., 2016].\nTo increase fake news detection accuracy and model generalizability, training on multi-source and multi-modal datasets are also studied. For example, Shu et al. [2017b] explored the correlation between news publisher bias, user stance, and user engagement together in their Tri-Relationship fake news detection framework. In a following work, Shu et al. [2018] proposed a training dataset to include news content and social context along with dynamic information of news.\nAlthough most aforementioned data mining methods do not perform a direct fake news detection, these methods can leverage both social and textual feature to identify suspicious news pieces for human review."
    },
    {
      "heading": "3 Open Issues in Fake News Research",
      "text": "In the previous sections, we discussed current methods to detect and mitigate fake content at different stages of news life in social media. Now we review two main issues in the fake news research that put a limit on the current state of the art systems."
    },
    {
      "heading": "3.1 The (Un)clarity in Problem Characterization",
      "text": "Problem characterization is the very first step in problem solving process. Reviewing literature from multiple disciplines such a social science, psychology and machine learning shows various definitions for fake news and its related phenomena. Although each represents a type of inaccurate information and news (i.e., information from current events), one can find at least seven different concepts related to the falsified information and news including hoax, fake news, rumors, deceptive news, spams, click baits, forged images, and videos, etc. Although different in shape and purpose, these concepts share the same nature of inaccurate information: either in form of misinformation or disinformation. In this section, we briefly review different fake news related phenomena and their relationship to the fake news problem."
    },
    {
      "heading": "Incomplete Problems",
      "text": "Reviewing fake news related literature indicates a misinterpretation between false news detection and fake news recognition methods. Although machine learning researchers actively study different methods to detect the existence and circulation of various types of misinformation and disinformation in social media, yet the fake news gets a narrow definition today. Allcott and Gentzkow [2017] define fake news as an article that is intentionally created and verifiably false. Here we review examples of misinformation and disinformation and compare their differences with fake news. Various types of misinformation in forms of inaccurate posts (e.g., inaccurate scientific facts), rumor (e.g., inaccurate reporting of an event) and news (e.g., political and economic news) represent incomplete statements and events. Also, disinformation is purposely created for financial and political gain (e.g., fake news and rumors), advertisement (e.g., click baits), entertainment (e.g., satire news and online memes), fame (e.g., forged photos and videos) and many other purposes. Therefore in many cases, fake news could be a subcategory of disinformation and detecting fake news is not always equal to detecting any rumor or deceptive report.\nHowever,, in reality, fake news may still spread with rumor-like patterns in social media or may be embedded in a click-bait political advertisement. This means, although various detection techniques could be effective in detecting the\nfake news (as well as other types of inaccurate information), this is not necessarily equal to fake news in its narrow definition. The fake news problem clarification tells us detecting fake-new-related concept \u2013 although might seem the similar \u2013 is not the same as detecting fake news."
    },
    {
      "heading": "Incomplete Solutions",
      "text": "After meta-reviewing recent fake news papers and related surveys (e.g., [Shu et al., 2017a], [Zhou and Zafarani, 2018], [Sharma et al., 2019], [Spirin and Han, 2012] and [Zubiaga et al., 2018]) to analyze different perspectives of fake news solutions, we present a 2D categorization of different misinformation and disinformation types and current machine learning detection methods in Table 1. This table shows although an ultimate solution for fake news detection needs a knowledge-based fact-checking approach, there are similarities between fake news (in its narrow definition) detection methods to other related concepts. This similarity is due to the fact that fake news still has an information nature and needs a way through social network and conventional news channels to find its audience.\nThese similarities open the door for leveraging other techniques (e.g., style based and social context based approaches) in detecting fake news content. However, these solutions are considered incomplete for the problem of fake news if they are not verified by supporting facts and justifications. Therefore, knowledge-based fact checking approaches (either performed by human or machine learning) become dominant solutions for fake news recognition. This suggests an extra fact-checking step after the detection step. The extra recognition step is similar to rumor debunking confirmation in early detection method [Ma et al., 2016]. The necessity of factchecking step in fake news recognition problem is because fake news detection needs to be verifiable by either supporting facts, justifications, or explanations. The fake news solution clarification tells us fake news detection without performing the fact checking would present an incomplete solution."
    },
    {
      "heading": "3.2 The (Un)accountability of News Feed Algorithms",
      "text": "Nowadays, personalized news feed algorithms on social media provide content to users based on users\u2019 profiles, interests, social media network, and other past click behavior. Although these algorithms have a key role in the news life cycle, there is not enough research on designing news feed algorithms robust to fake news propagation.\nIn the following, we review open issues in personalized news feed and search algorithms that have a positive effect on propagation and believe of fake news."
    },
    {
      "heading": "Echo Chambers in Social Media",
      "text": "Echo chamber in social media describes a phenomenon where homogeneous views are reinforced by communication inside closed social media groups. Echo chambers have been previously studied in relation to creating polarized opinions and shaping a false sense of credibility for users whose frequent news sources are through social media [Zajonc, 2001]. This false sense of credibility holds users in a vulnerable position\nof accepting biased and fake news content. In similar circumstances, although news feed algorithms are meant to provide content related to users interest, these machine learning algorithms can adjust a user\u2019s news feed with a certain perception of reality and trigger user confirmation bias to over-trust unreliable sources [Quattrociocchi et al., 2016]. In studying methods to overcome echo chambers in social media, Lex et al. [2018] presented a content-based news recommendation that can increase exposure of the opposite view to users in order to mitigate the echo chamber effect in social media. In another work, Hou et al. [2018] demonstrated methods to balance popularity bias in network-based recommendation systems and to significantly improve the system\u2019s news diversity."
    },
    {
      "heading": "Filter Bubbles in Search Engines",
      "text": "Filter bubble is another term to describe the negative effects of personalized news search engines [Pariser, 2011]. Filter bubbles represent a state of information isolation where users are only exposed to a certain perspective of information brought to them by personalized search engines. The lack of exposure to diverse viewpoints creates a filter bubble for individuals and increase the chance of accepting fake content, accrediting unreliable sources, and further distributing fake content. In the context of news search engines, Haim et al. [2018] observed negative results in an exploratory analysis of personalized news search engine effect on news diversity. One approach to encounter unwanted negative effects of these algorithms is to define new measures and standards for sensitive data and products recommendation systems. For better measuring information diversity in recommender systems, Ekstrand et al. [2018] and Valcarce et al. [2018] proposed new evaluation measures to account for other considerations like users\u2019 popularity bias or content diversity. The importance of new measures for news recommendation algorithms comes from considering the diversity of content across different user groups.\nThe importance of news feed algorithms is in their ability to increase information and opinion diversity in social media. News diversity in social media is another approach to combat fake news by eliminating echo chambers and biased search engines."
    },
    {
      "heading": "4 Opportunities in Combating Fake News",
      "text": "Considering the recent attention in designing end-to-end interpretable fake news detection systems (e.g., [Yang et al., 2019] and [Popat et al., 2018a]), the necessity of interpretability, and variety of social media data modality; it is essential to take new interpretable design approaches to improve model robustness. In this section, we discuss two research opportunities for fake new mitigation focused on fake news detection methods, news diversity metrics, news feed algorithms, and their evaluation measures."
    },
    {
      "heading": "4.1 Interpretable Fake News Detection",
      "text": "While interpretable machine learning can help model designer for debugging and model validation [Zeiler and Fergus, 2014], machine learning explanations may also help end-users understand and trust the machine learning systems. However, appropriate explanation type for machine learning\nexperts is different from novice end-users and media experts. Figure 2 shows three dimensions of interpretability that serve different purposes in intelligent fake news mitigation systems. In the following, we discuss how these three dimensions can improve fake news detection research.\nAlgorithmic Interpretability Interpretability of algorithms is usually defined as a degree of human understandability of a algorithms decision-making process. We introduce the Algorithmic Interpretability term as a degree of machine learning experts ability to visualize model parameters and inspect model behavior. Algorithmic interpretability helps in machine learning designers to debug and tune model parameters for more accurate and reliable models. Machine learning system transparency also leads to evaluate model robustness (for reliability and safety) and model fairness (not being biased). Researchers implemented various interpretability methods to explain individual input instances (e.g., feature importance) and visualizing the whole model (e.g., model internal weights). Fake news detection algorithms can also benefit from algorithmic interpretability and transparency to verify model fairness toward different speakers and topics.\nHuman Interpretability Although beneficial for experts to understand the model behavior, algorithmic interpretability methods are not necessarily useful and even understandable for machine learning product end-users. We introduce Human Interpretability as a product of machine learning interpretability methods that can significantly benefit fake news detection systems. Human interpretability provides decision-making transparency to endusers with understandable explanations of \u201chow the system works\u201d and \u201cwhy this decision is made\u201d. These explanations may help the user to better understand model prediction details, increase user trust on machine decisions, and therefore increase human-machine performance on the tasks. Humancomputer interaction (HCI) research shows machine learning explanations increase the overall performance of endusers by increasing user knowledge and reliance on the system [Kulesza et al., 2015]. Moreover, human interpretability opens new evaluation methods and measures for the interpretability method. Different human-subject studies in HCI research evaluate interpretability measures such as user understanding, mental model, explanation usefulness, trust, and task performance [Mohseni et al., 2018]."
    },
    {
      "heading": "Supporting Evidence",
      "text": "In the context of fact-checking and fake news detection, providing supporting evidence is an essential element to justify the decision made by experts and organizations. Supporting evidence are accurate information which have relationship to the news event and can verify the veracity of the news. Providing supporting evidence from the training data or the knowledge graph can provide extensive why explanation for each instance. Related to this, new research by Yang et al. [2019] and [2018b] propose explainable fake news detection methods that assist end-users to identify news credibility with supporting evidence selected from a set of verified news. We suggest designing interpretable algorithm that\ncan explain their decisions by providing supporting examples from training data can help users better understand the system and trust the detection results."
    },
    {
      "heading": "4.2 Interpretable News Feed Algorithms",
      "text": "News feed algorithms have an important role in content and news distribution in social media (see Figure 1). In this section, we discuss two different machine learning interpretability approaches that can benefit the mitigation of fake news propagation in social media."
    },
    {
      "heading": "Interpretability for Robust News Feed",
      "text": "Another fake news mitigation approach is to embed news veracity checking tools in news feed systems to provide explanations about the news content veracity to the user. The news feed algorithm can perform high-level veracity analysis (e.g., source credibility, deceptive language flag, click bait flag) on social media posts to assist the users with data-driven veracity check results. Although interpretable news feed is not detection or debunking solution, in real scenarios, mitigation solutions could be more advantageous than automatically debunking of the news.\nThe advantages of fake news resistant news feed are in two folds. First, veracity checking of content in the distribution step benefits both news consumers and content providers in contrast to auto content removal from the network. For the user side, providing veracity analysis does not limit the user access to information (in contrast to debunking news) and yet help the users to decide by providing interpretable explanations. For the content provider side, veracity analysis of misinformation such as satire news and click baits (for entertainment and advertisement purposes) does not automatically remove the content and may still cause damage to the news and content providers. Second, by looking at news life cycle stages in Figure 1, veracity checking of the news on the distribution step is the faster method compared to other socialcontext-dependent methods such as rumor detection.\nWe argue that the effectiveness of fake news mitigation algorithms will further extend if being able to provide useful veracity checking explanations for the end user. Such explanations designed for news feed and search algorithms could benefit both expert journalists and non-expert news consumers."
    },
    {
      "heading": "Interpretability for User Awareness",
      "text": "Algorithmic transparency is another way that social media can benefit from interpretable machine learning to mitigate the distribution of fake content. News feed algorithms in social media process sensitive user data to generate a user\u2019s reading list and do not give options for the user to choose among content. In these circumstances, the lack of explanation of \u201chow the content is selected?\u201d for the user may bias the user toward what the algorithm is providing in the news feed and not the entire reality.\nWhile transparency is not a silver bullet to stop fake news propagation in social media, it helps the user to understand how news feed algorithms work to use them appropriately. For example, \u201cmodel visualization of user preference\u201d (e.g., in form of user model) and \u201cnews attributes that contribute to the news recommendation\u201d (e.g., features weights ) will help users to understand how the content selection algorithm is working. Other explanations such as \u201cwhat kind of soft filters are applied\u201d on users feed also would benefit on the purpose of user awareness. News feed transparency through model explanation could also reduce the occurrence of large scale adverse situations such as propagation of biased and fake news in social media.\nWe suggest adding model and instance explanations to news feed algorithms would serve as a major leap toward accountable news feed systems. Such transparency could defeat fake new by increasing user awareness and eliminating situations such as echo chambers and filter bubbles."
    },
    {
      "heading": "5 Conclusion",
      "text": "We reviewed the opportunities and limitations in the current research on fake news detection methods and the transparency of news feed algorithms. After reviewing current fake news detection methods along three main stages of news life in social media, we discuss two open issues in machine learning fake news research. In the first discussion, we clarified the issues in fake news detection problem characterization, Then we emphasized the vulnerability of news feed algorithms in propagating fake news. In the last section, we presented interpretability with its three dimensions as a potential solution and direction for fake news research that can benefit both machine learning experts and social media endusers. We also introduced interpretable news feed systems as an effective fake news elimination solution in real scenarios compared to the current detection and debunking solutions."
    }
  ],
  "title": "Open Issues in Combating Fake News: Interpretability as an Opportunity",
  "year": 2019
}
