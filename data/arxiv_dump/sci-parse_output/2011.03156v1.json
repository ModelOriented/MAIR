{
  "abstractText": "In this article, we introduce a fairness interpretability framework for measuring and explaining bias in classification and regression models at the level of a distribution. In our work, motivated by the ideas of Dwork et al. (2012), we measure the model bias across sub-population distributions using the Wasserstein metric. The transport theory characterization of the Wasserstein metric allows us to take into account the sign of the bias across the model distribution which in turn yields the decomposition of the model bias into positive and negative components. To understand how predictors contribute to the model bias, we introduce and theoretically characterize bias predictor attributions called bias explanations. We also provide the formulation for the bias explanations that take into account the impact of missing values. In addition, motivated by the works of Strumbelj and Kononenko (2014) and Lundberg and Lee (2017) we construct additive bias explanations by employing cooperative game theory.",
  "authors": [
    {
      "affiliations": [],
      "name": "Alexey Miroshnikov"
    },
    {
      "affiliations": [],
      "name": "Konstandinos Kotsiopoulos"
    },
    {
      "affiliations": [],
      "name": "Ryan Franks"
    },
    {
      "affiliations": [],
      "name": "Arjun Ravi Kannan"
    }
  ],
  "id": "SP:d44eff5c3ff443fe4d775e56d561badf092e6f8b",
  "references": [
    {
      "authors": [
        "S. Barocas",
        "M. Hardt",
        "A. Narayanan"
      ],
      "title": "Fairness and Machine Learning: Limitations and Opportunities",
      "year": 1965
    },
    {
      "authors": [
        "NJ"
      ],
      "title": "E",
      "venue": "del Barrio, H. Inouzhe, C. Matr\u00e1n, On approximate validation of models: a Kolmogorov\u2013Smirnov-",
      "year": 2006
    },
    {
      "authors": [
        "P. Hall"
      ],
      "title": "On the art and science of machine learning explanations",
      "venue": "arXiv preprint arxiv:1810.02909,",
      "year": 2018
    },
    {
      "authors": [
        "M. Hardt",
        "E. Price",
        "N. Srebro"
      ],
      "title": "Equality of opportunity in supervised learning",
      "venue": "In Advances in Neural Information Processing Systems,",
      "year": 2015
    },
    {
      "authors": [
        "R. Heller",
        "Y. Heller",
        "M. Gorfine"
      ],
      "title": "A consistent multivariate test of association based on ranks of distances",
      "year": 2013
    },
    {
      "authors": [
        "R. Heller",
        "Y. Heller",
        "S. Kaufman",
        "B. Brill",
        "M. Gorfine"
      ],
      "title": "Consistent distribution-freek-sample and independence tests for univariate random variables",
      "venue": "Journal of Machine Learning Research,",
      "year": 2016
    },
    {
      "authors": [
        "H. Jiang",
        "O. Nachum"
      ],
      "title": "Identifying and Correcting Label Bias in Machine Learning",
      "venue": "Proceedings of the 23-rd International Conference on Artificial Intelligence and Statistics (AISTATS),",
      "year": 2020
    },
    {
      "authors": [
        "F. Kamiran",
        "T. Calders",
        "M. Pechenizkiy"
      ],
      "title": "Discrimination aware decision tree learning",
      "venue": "Proc. of the 10-th IEEE Intern. Conf. on Data Mining. pp",
      "year": 2010
    },
    {
      "authors": [
        "F. Kamiran",
        "T. Calders"
      ],
      "title": "Classifying without discriminating, 2009 2nd International Conference on Computer, Control and Communication, Karachi, pp",
      "venue": "doi: 10.1109/IC4.2009.4909197,",
      "year": 2009
    },
    {
      "authors": [
        "T. Kamishima",
        "S. Akaho",
        "H. Asoh",
        "J. Sakuma"
      ],
      "title": "Fairness-Aware Classifier with Prejudice Remover Regularizer, Proceedings of the European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECMLPKDD)",
      "venue": "Part II,",
      "year": 2012
    },
    {
      "authors": [
        "Y.L. Koralov"
      ],
      "title": "Sinay, Theory of probability and random processes",
      "year": 1998
    },
    {
      "authors": [
        "K. Kotsiopoulos",
        "A. Miroshnikov",
        "A. Ravi Kannan"
      ],
      "title": "On mutual information coalition-based explainers for machine learning interpretability",
      "venue": "in preparation,",
      "year": 2020
    },
    {
      "authors": [
        "M.S. Kovalev",
        "L.V. Utkin"
      ],
      "title": "A robust algorithm for explaining unreliable machine learning survival models using the Kolmogorov\u2013Smirnov bounds",
      "venue": "Neural Networks,",
      "year": 2020
    },
    {
      "authors": [
        "S. Lipovetsky",
        "M. Conklin"
      ],
      "title": "Analysis of regression in game theory approach",
      "venue": "Appl. Stochastic Models Bus. Ind.,",
      "year": 2001
    },
    {
      "authors": [
        "S.M. Lundberg",
        "G.G. Erion",
        "S.-I. Lee"
      ],
      "title": "Consistent individualized feature attribution for tree ensembles",
      "venue": "arXiv preprint arxiv:1802.03888,",
      "year": 2019
    },
    {
      "authors": [
        "S.M. Lundberg",
        "S.-I. Lee"
      ],
      "title": "A unified approach to interpreting model predictions",
      "venue": "31st Conference on Neural Information Processing Systems,",
      "year": 2017
    },
    {
      "authors": [
        "A. Miroshnikov",
        "K. Kotsiopoulos",
        "R. Franks",
        "A. Ravi Kannan"
      ],
      "title": "Bias mitigation of ML models without access to the protected attribute",
      "venue": "In preparation,",
      "year": 2020
    },
    {
      "authors": [
        "G. Owen"
      ],
      "title": "Values of games with a priori unions",
      "venue": "Essays in Mathematical Economics and Game Theory (R. Henn and O. Moeschlin,",
      "year": 1977
    },
    {
      "authors": [
        "G. Owen"
      ],
      "title": "Modification of the Banzhaf-Coleman index for games with a priori unions. In: Power, Voting and Voting Power (M.J",
      "year": 1982
    },
    {
      "authors": [
        "E. \u0160trumbelj",
        "I. Kononenko"
      ],
      "title": "Explaining prediction models and individual predictions with feature",
      "year": 1986
    },
    {
      "authors": [
        "C. Villani"
      ],
      "title": "Topics in Optimal Transportation",
      "venue": "American Mathematical Society,",
      "year": 2003
    },
    {
      "authors": [
        "B. Woodworth",
        "S. Gunasekar",
        "M.I. Ohannessian",
        "N. Srebro"
      ],
      "title": "Learning nondiscriminatory predictors",
      "year": 2004
    }
  ],
  "sections": [
    {
      "text": "Keywords: Optimal transport theory, Wasserstress distance, ML interpretability, ML fairness, cooperative game"
    },
    {
      "heading": "1 Introduction",
      "text": "Contemporary machine learning (ML) techniques surpass traditional statistical methods in terms of their higher predictive power and their capability of processing a larger number of attributes. However, these novel ML algorithms generate models that have a complex structure which makes it difficult for their outputs to be interpreted with high precision. Another important issue is that a highly accurate predictive model might lack fairness by generating outputs that may result in discriminatory outcomes for protected subgroups. Thus, it is imperative to design predictive systems that are not only accurate but also achieve the desired fairness level.\nWhen used in certain contexts, predictive models, and strategies that rely on such models, are subject to laws and regulations that ensure fairness. For instance, a hiring process in the United States (US) must comply with the Equal Employment Opportunity Act (EEOA, 1972). Similarly, financial institutions (FI) in the US that are in the business of extending credit to applicants are subject to the Equal Credit Opportunity Act (ECOA), the Fair Housing Act (FHA), and other fair lending laws; for details see (ECOA, 1974, FHA, 1974). These laws often specify protected attributes that FIs must consider when maintaining fairness in lending decisions.\nExamples of protected attributes include race, gender, age, ethnicity, national origin, marital status, and others. Under the ECOA, for example, it is unlawful for a creditor to discriminate against an applicant for a loan on the basis of race, gender or age. Even though direct usage of protected attributes in building a model is often prohibited by law (e.g. overt discrimination), some otherwise benign attributes can serve as \u201cproxies\u201d because they may share dependencies with a protected attribute. For this reason, it is crucial for data scientists to conduct a fairness review of their trained models in consultation with compliance professionals in order to evaluate the predictive modeling system for potential unfairness.\n\u2217Emerging Capabilities Research Group, Discover Financial Services Inc., Riverwoods, IL \u2020co-first author and corresponding author, alexeymiroshnikov@discover.com \u2021co-first author, kostaskotsiopoulos@discover.com \u00a7ryanfranks@discover.com \u00b6arjunravikannan@discover.com\nar X\niv :2\n01 1.\n03 15\n6v 1\n[ cs\n.L G\n] 6\nN ov\n2 02\nAt an algorithmic level, the bias can be viewed as an ability to differentiate between two subpopulations at the level of data or outcomes; this point of view is taken in the seminal work of Dwork et al. (2012) that introduces the concept of the bias at the level of data distribution in the context of randomized classifiers. If bias (regardless of its definition) is present in data when training an ML model, the ability to differentiate between subgroups might potentially lead to discriminatory outcomes. For this reason, the model bias can be viewed as a measure of unfairness and hence its measurement is central to the model fairness assessment.\nThere is a comprehensive body of research on ML fairness that discusses bias measurements and innovative bias mitigation methodologies. Some of the notable works on this topic are Kamiran et al. (2009) on classification schemes for learning unbiased models by modifying the biased data sets, Kamiran et al. (2010) on decision-tree learners with non-discrimination constraints, Dwork et al. (2012) on fair classification metrics, including individual fairness criterion, and algorithms maximizing performance with fairness constraints, Kamishima et al. (2012) on regularization approaches for discriminative probabilistic models, Zemel et al. (2013) on fairness algorithms with fairness constraints, Feldman et al. (2015) on removing disparate impact, in the sense of statistical parity, in classifiers by making data sets unbiased, Hardt et al. (2015) on classifier fairness criteria, such as equalized odds and equal opportunity, and post-processing techniques removing discrimination, Woodworth et al. (2017) on nearly-optimal learning predictors with equalized odds fairness constraint, Zhang et al. (2018) on mitigating biases with adversaries, Jiang (2020) on the bias correction techniques via re-weighting data, etc.\nThe bias mitigation techniques in the aforementioned articles, most of which focus on fairness in classification, rely on access to the protected attribute, which presents an issue in the financial industry setting for reasons explained below. A typical setup in the ML literature related to classification fairness is as follows. Given the data (X,G, Y ), where X \u2208 Xn are predictors, G \u2208 {0, 1} is a protected attribute and Y \u2208 {0, 1} is a binary output variable, the objective is to construct a non-discriminative (or fair) model subject to a certain fairness criterion.\nThere are two typical routes for the construction of such a model. The first route is to construct a fair classification score f\u0303(X;G) or a fair classifier Y\u0303 (X;G) by either transforming or re-weighting the predictors X in accordance with G, or minimizing an appropriate loss function with a non-discriminative constraint based on G; see, for instance, Feldman et al. (2015), Jiang (2020) for the former, and Zemel et al. (2013), Woodworth et al. (2017), Zhang et al. (2018), for the latter. Here, the dependence of f\u0303 on G may be indirect, for example, originating from the constraint in the minimization algorithm. The second route is to consider the trained model f(X), which is possibly discriminative, and then design a fair\nscore f\u0303(X;G) or a fair classifier Y\u0303 (X;G), using a post-processing corrective technique that utilizes the information on the joint distribution (f(X), G). This type of method is appealing because the algorithm neither requires the knowledge of G in training nor involves model re-training; see Hardt et al. (2015).\nIn the financial industry setting, however, the bias mitigation methodologies that require explicit consideration of protected class status in training or production are not acceptable because ECOA prohibits the use of protected class status when making a lending decision. Furthermore, FIs are explicitly legally prohibited from collecting information on some protected attributes. For these reasons many of those bias mitigation techniques described in the fairness literature are simply infeasible for FIs; for details see Dickerson et al. (2020), Barocas et al. (2018).\nAnother issue, pertinent specifically to classification models, is the choice of the bias measurement metric. Specifically, the main focus in the ML fairness literature is on the measurement of the bias at the level of the classifier Yt(X) = 1{f(X)>t}. Given a favorable outcome Y = 1, the bias measurements are often based on fairness criteria such as statistical parity, which reads P(Yt = 1|G = 0) = P(Yt = 1|G = 1), or alternative criteria such as equalized odds and equal opportunity, which read P(Yt = 1|G = 0, Y = k) = P(Yt = 1|G = 1, Y = k) for k \u2208 {0, 1} and k = 1, respectively; see Feldman et al. (2015), Hardt et al. (2015). Similarly, the mitigation procedures in the classification literature often focus on the construction of an optimal (possibly randomized) classifier that maximizes the utility subject to the classifier fairness constraint; see for instance Hardt et al. (2015), Woodworth et al. (2017), Kamiran and Calders (2020).\nFor the financial industry, however, the above approaches are less relevant, and sometimes infeasible, for the following reasons. In the model selection stage and fairness assessment stage, there is often no pre-determined classifier threshold. Data scientists select the classification model f(X) based on the overall performance across all thresholds (for instance, AUC) and the same is true for compliance\ndepartment (CD) professionals who often assess fairness at the level of the whole classification score1. The main reason for that is that the strategies and decision-making procedures in FIs may rely on the whole classification score, or its distribution, not a single classifier with a fixed threshold.\nIn light of the aforementioned reasons, and given FIs legal constraints, it is crucial to be able measure the bias at the level of the model, and incorporate model-based fairness metrics in the design of the bias mitigation techniques. One acceptabl form of fairness assessment and accompanying bias mitigation procedures in FIs could be the following:\n(S1) Given a model f(X), perform a fairness assessment by measuring the bias across the subpopulation distributions f(X)|G = k, k \u2208 {0, 1}.\n(S2) If the model bias exceeds a certain threshold, determine the main drivers for the bias, that is, determine the list of predictors Xi1 , Xi2 , . . . , Xir contributing the most to that bias.\n(S3) Mitigate the bias by constructing a post-processed model f\u0303(X; f) utilizing the information on the most biased predictors {i1, i2, . . . , ir} and without the direct use of the protected attribute G.\nIn this article, addressing steps (S1) and (S2), we develop an interpretability framework for measuring and explaining bias in ML models. The main objective of the methodology is a) to introduce an appropriate metric to measure the bias at the level of the model distribution, called model bias; and b) to introduce and theoretically characterize contributions of predictors X1, X2, . . . , Xn to that bias, called bias explanations, and investigate their properties. The post-processing methods (S3) are investigated in the companion paper Miroshnikov et al. (2020b). In what follows, we provide a summary of the key ideas and main results.\nModel setup. We consider the joint distribution (X,G, Y ), where X = (X1, X2, . . . , Xn) \u2208 Xn are predictors, G \u2208 {0, 1} is the protected attribute, with the non-protected class G = 0, and Y is either a response variable with values in R (not necessarily a continuous random variable) or binary one with values in {0, 1}. We denote a trained model by f(X) = E\u0302[Y |X], assumed to be trained on (X,Y ) without access to G. We assume that there is a predetermined favorable model direction, denoted by \u2191 and \u2193; if the favorable direction is \u2191 then the relationship f(x) > f(y) favors the input x, and if it is \u2193 the input y. In the case of binary Y \u2208 {0, 1}, the favorable direction \u2191 is equivalent to Y = 1 being a favorable outcome, and \u2193 to Y = 0. While the main text focuses on the case of a binary protected attribute G to simplify the exposition, the framework and all of the results in the article have a natural extension to multi-valued protected attribute G \u2208 {0, 1, . . . ,K \u2212 1} and multi-valued regressors (see Appendix)."
    },
    {
      "heading": "Key components of the framework.",
      "text": "\u2022 In the spirit of Dwork et al. (2012), we define the model bias by\nBiasW1(f |G) = DW1(f(X)|G = 0, f(X)|G = 1)\nwhere DW1 is the Wasserstein metric, also known as the Earth Mover distance, which measures the distance between two probability distributions. We say that the model f is fair up to the W1-based bias if BiasW1(f |G) \u2264 .\n\u2022 The metric DW1 is connected with the concept of optimal mass transport. It measures the minimal cost of transporting one distribution into another; see Villani (2003), Santambrogio (2015). In the transport context, we can monitor the flow direction and measure the transport efforts in the favorable and non-favorable directions. In particular, we introduce the model bias decomposition,\nBiasW1(f |G) = Bias + W1 (f |G) + Bias\u2212W1(f |G),\nin which Bias+W1(f |G), called positive model bias, measures the transport effort for moving points of the unprotected subpopulation distribution f(X)|G = 0 in the non-favorable direction and Bias\u2212W1(f |G), called negative model bias, in the favorable one. Measuring the two flows allows us to take into account the sign of the bias and get a more informative perspective on its origin.\n\u2022 We establish the connection of the model bias with that of classifiers. We show that the model bias can be viewed as the integrated statistical parity bias for classifiers Yt = 1{f>t}, where integration is performed across thresholds t \u2208 R. Similar relationships are established for positive and\n1Compliance departments have access to the protected attribute G or its proxies for compliance purposes only.\nnegative model biases; see Theorem 3.3 and Theorem 3.4. Furthemore, we show how to construct a model bias metric that is consistent with any classifier fairness criterion based on group parity; see Appendix D.\n\u2022 To understand how predictors contribute to the model bias, we introduce and theoretically characterize bias predictor attributions called bias explanations. For the construction, we make use of contemporary ML interpretability methods. A generic interpreter, or explainer, typically has the form Ei(X; f) and makes an attempt to measure the contribution of the predictor Xi to the model value f(X); see, for instance, PDP (Friedman, 2001), SHAP (Lundberg and Lee, 2017), LIME (Ribeiro et al., 2016). Given an explainer Ei(X; f), we measure the bias explanation of the predictor Xi by computing the cost of transporting the distribution of Ei(X; f)|G = 0 to that of Ei(X; f)|G = 1:\n\u03b2i(f |G) = DW1 ( Ei(X; f)|G = 0, Ei(X; f)|G = 1).\nSimilarly to the model bias, the transport theory gives rise to the positive model bias explanation \u03b2+i and negative model bias explanation \u03b2 \u2212 i that satisfy \u03b2i = \u03b2 + i + \u03b2 \u2212 i , as well as the net model bias explanation \u03b2neti = \u03b2 + i \u2212 \u03b2 \u2212 i . The collection of explanations {(\u03b2i, \u03b2 + i , \u03b2 \u2212 i , \u03b2 net i )}ni=1 yields the Bias Explanation Plot which displays the distribution of the biases across the predictors.\n\u2022 In many applications, training data sets may contain observations where values of certain predictors are missing. Contemporary ML algorithms can handle missing values by treating these cases as a separate category for each predictor: \u2018na\u2019 (not available). This enables ML models to perform predictions even when missing values are given as input, which implies that their presence in a data set could impact the distribution of the model and consequently could impact the bias. To understand this impact we consider models that operate on the domain that allows for both numerical values as well as missing values \u2018na\u2019 . In particular, we show that the bias explanations \u03b2\u00b1i are decomposed into the sum of two signed values \u03b2 na\u00b1 i and \u03b2 num\u00b1 i , respectively, one of which\nis fully characterized by the missing value event {Xi = na}; see Lemma 4.4. \u2022 The bias explanations are in general not additive, even if the explanations are. To construct\nadditive bias explanations we employ a cooperative game theory approach motivated by the ideas of Shapley (1953), Strumbelj and Kononenko (2014), Lundberg and Lee (2017). We design a set function, called bias game, by setting\nvbias(S) = DW1 ( ES(X; f)|G = 0, ES(X; f)|G = 1 ) , S \u2282 {1, 2, . . . , n}\nwhere ES(X; f) is an explainer of the group predictor XS . We then define bias explanations to be Shapley values \u03d5i[v bias] of the game vbias, in which case, BiasW1(f |G) = \u2211 i \u03d5i[v\nbias]. Similar approach is applied to construct additive positive and negative bias explanations.\n\u2022 In the presence of strong dependencies in predictors, certain explainers may produce inconsistent attributions, in the sense discussed in Sundararajan and Najmi (2019), Janzing et al. (2019), Chen et al. (2020), which potentially could lead to inconsistent bias explanations. To resolve this issue, one approach is to partition the set of predictors into groups that form weakly independent unions such that within each union the predictors share strong dependencies (thus, forming coalitions by dependencies) and then construct a corresponding coalition-based explainer; for details see Aas et al. (2020), Kotsiopoulos et al. (2020). In our work, we use a similar approach to construct coalitionbased bias explanations. Given a partition P = {S1, S2, . . . , Sn} of predictors by dependencies, we construct a quotient game vP,bias obtained by restriction of vbias to unions Sj and then define the bias explanation of the coalition XSj to be the Shapley value \u03c6j [v\nbias,P ] of the game vbias,P ; for details see Owen (1977). Similar remarks apply for positive and negative bias explanations.\nStructure of the paper. In Section 2, we introduce the requisite notation and fairness criteria for classifiers. In Section 3, we introduce model-based fairness metrics. We also introduce positive, negative, and net model biases and establish their connection with the classifier bias. In Section 4, we provide a theoretical characterization of the bias explanations and discuss their properties, as well as assess the impact of missing data on the bias. In Appendix A, we discuss the properties of Wasserstein metrics. In Appendix B, we provide proofs related to the model bias and bias explanations. In Appendix E, we discuss grouped predictors and their bias explanations. In Appendix C, we extend the framework to the case where the protected attribute has multiple values. In Appendix D we discuss the model bias metrics consistent with generic classifier fairness criteria based on group parity."
    },
    {
      "heading": "2 Preliminaries",
      "text": ""
    },
    {
      "heading": "2.1 Notation and hypotheses",
      "text": "We consider the joint distribution (X,G, Y ), where X = (X1, X2, . . . , Xn) \u2208 Xn are the predictors, G \u2208 {0, 1, . . . ,K \u2212 1} is the protected attribute and Y is either a response variable with values in R (not necessarily a continuous random variable) or binary one with values in {0, 1}. We encode the nonprotected class as G = 0. We assume that all random variables are defined on the common probability space (\u2126,P,F), where \u2126 is a sample space, P a probability measure, and F a \u03c3-algebra of sets.\nThe true model and a trained one, which is assumed to be trained without access to G, are denoted by\nf(X) = E[Y |X] and f\u0302(X) = E\u0302[Y |X],\nrespectively. In the case of binary Y they read f(X) = P(Y = 1|X) and f\u0302(X) = P\u0302(Y = 1|X). We denote a classifier based on the trained model by\nY\u0302t = Y\u0302t(X; f\u0302) = 1{f\u0302(X)>t}, t \u2208 R.\nThe subpopulation cumulative distribution function (CDF) of f\u0302(X)|G = k is denoted by\nFk(t) = Ff\u0302 |G=k(t) = P(f\u0302(X) \u2264 t|G = k)\nand the corresponding generalized inverse (or quantile function) F [\u22121] k is defined by:\nF [\u22121] k (p) = F [\u22121] f\u0302 |G=k (p) = inf x\u2208R\n{ p \u2264 Fk(x) } . (2.1)\nWe assume that there is a predetermined favorable model direction, denoted by either \u2191 or \u2193. If the favorable direction is \u2191 then the relationship f(x) > f(z) favors the input x, and if it is \u2193 the input z. The sign of the favorable direction of f\u0302 is denoted by \u03c2f\u0302 and satisfies\n\u03c2f\u0302 =\n{ 1, if the favorable direction of f\u0302 is \u2191\n\u22121, if the favorable direction of f\u0302 is \u2193 .\nIn the case of binary Y , the favorable direction \u2191 is equivalent to Y = 1 being a favorable outcome, and \u2193 to Y = 0; see Section 2.4.\nIn what follows we first develop the framework in the context of the binary protected attribute G \u2208 {0, 1} and then extend it to the case of the multi-label protected attribute G \u2208 {0, 1, 2, . . . ,K \u2212 1}; see Appendix C."
    },
    {
      "heading": "2.2 Fairness criteria for classifiers",
      "text": "When undesired biases concerning demographic groups (or protected attributes) are in the training data, well-trained models will reflect those biases. There have been numerous articles devoted to ML systems that lead to fair decisions. In these works, various measurements for fairness have been suggested. In what follows, we describe several well-known definitions which help measure fairness of classifiers.\nDefinition 2.1 (Feldman et al. (2015)). Suppose that Y is binary with values in {0, 1} and Y = 1 is the favorable outcome. Let Y\u0302 be a classifier.\n\u2022 The classifier Y\u0302 satisfies statistical parity if\nP(Y\u0302 = 1|G = 0) = P(Y\u0302 = 1|G = 1).\n\u2022 The data set (X,G, Y ) is fair if for any classifier Y\u0302 trained on (X,Y )\nBER(Y\u0302 , G) > 1\n2 , BER := balanced error rate.\nThe statistical parity requires that the proportions of people in the favorable class Y\u0302 = 1 within each group G = k, k \u2208 {0, 1} are the same. The data set being fair means that given the predictor X it is impossible to construct a classifier that allows one to guess G better than a naive classifier.\nDefinition 2.2 (Hardt et al. (2015)). Let Y and Y\u0302 be as in Definition 2.1.\n\u2022 The classifier Y\u0302 satisfies equalized odds if\nP(Y\u0302 = 1|Y = y,G = 0) = P(Y\u0302 = 1|Y = y,G = 1), y \u2208 {0, 1}\n\u2022 The classifier Y\u0302 satisfies equal opportunity if\nP(Y\u0302 = 1|Y = 1, G = 0) = P(Y\u0302 = 1|Y = 1, G = 1).\nThe notions of equalized odds and equal opportunity are presented in Hardt et al. (2015). The equalized odds constraint requires the classifier to make the same misclassification error rates for each class of the protected attribute G and the label Y . Equal opportunity constraint requires the misclassification rates to be the same for each class G = k only for the individual labeled as Y = 1.\nClassifiers can be randomized, that is, a classifier can be given as a mapping\n\u03bd(x) : Rp \u2192 P({0, 1})\nwhere \u03bd(x) is a probability measure on \u2126 = {0, 1}. In the context of randomized classifiers, the authors of Dwork et al. (2012) define individual fairness via the Lipschitz property for the mapping x\u2192 \u03bd(x). Definition 2.3 (Dwork et al. (2012)). Let \u03bdx be a probability measure on {0, 1} that represents a randomized classifier that places individuals with predictor values X = x into one of the classes {0, 1}. Let D be a metric on the space of measures defined on \u2126 = {0, 1} and metric d on Rp. The mapping x\u2192 \u03bdx satisfies (D, d)-Lipschitz property if\nD(\u03bdx1 , \u03bdx2) \u2264 d(x1, x2).\nLipschitz property encodes the concept of individual fairness, which requires that similar people are treated similarly; see Dwork et al. (2012).\nTo understand the intuition behind the individual fairness, consider a randomized classifier\nRp 3 x\u2192 \u03bdx \u2208 P({0, 1}), and set h(x) := \u03bdx(1). (2.2)\nNote that the probability measure \u03bdx can be viewed as a pushforward probability measure of the random variable Y such that\nY |X = x \u223c Bernoulli(h(x)), h(x) = P(Y = 1|X = x). (2.3)\nLet D = DTV be the total variation distance between two probability measures and d be the scaled Euclidean distance in Rp, that is, d(x1, x2) = L\u2016x1 \u2212 x2\u20162 ; see Royden and Fitzpatrick (2010). Then the individual fairness property reads:\nDTV (\u03bdx1 , \u03bdx2) = 1\n2 \u2211 a\u2208{0,1} |\u03bdx1(a)\u2212 \u03bdx2(a)| = |h(x1)\u2212 h(x2)| \u2264 L\u2016x1 \u2212 x2\u20162 (2.4)\nfor all x1, x2 \u2208 Rp. Thus, in the context of DTV metric, the individual fairness property is analogous to the global Lipschitz property for the score function h(x).\nIt turns out that the classifier fairness criteria specified above are incompatible with one another; see Dwork et al. (2012), Feldman et al. (2015), Hardt et al. (2015). In this article, we develop an interpretability framework for the model bias at the level of a distribution that is consistent with the statistical parity fairness criterion. We should point out however that the framework can be naturally adapted to be consistent with the definition of equalized odds and equal opportunity; see Appendix D."
    },
    {
      "heading": "2.3 Group classifier fairness example",
      "text": "There are numerous reasons why a trained classifier may lead to unfair outcomes. To illustrate, we provide an instructive example that shows how predictors and labels, as well as their relationship with the protected attribute, affect classifier fairness.\nConsider a data set (X,Y,G) where the predictor X depends on G \u2208 {0, 1}, Y \u2208 {0, 1} is binary, with favorable outcome Y = 0, and the classification score f depends explicitly on X only:\nX \u223c N(\u00b5\u2212 a \u00b7G,\u221a\u00b5), \u00b5 = 5, a = 1 Y \u223c Bernoulli(f(X)), f(X) = P(Y = 1|X) = logistic(\u00b5\u2212X).\n(M1)\nThe data set is constructed in such a way that the proportions of Y = 0|G = k in the two groups are different: P(Y = 0|G = 0) = 0.5, P(Y = 0|G = 1) = 0.36. The predictor X serves as a good proxy for G. Though the true score f(X) does not depend explicitly on G, a classifier trained on X will learn that the higher the value of X the more likely it is that Y = 0. Using the logistic regression model f\u0302 and the classifier Y\u0302 1\n2 = 1{f\u0302> 1 2 } we obtain\nP(Y\u0302 1 2 = 0|G = 0) = 0.5, P(Y\u0302 1 2 = 0|G = 1) = 0.33 P(Y\u0302 1 2 = 0|Y = 1, G = 0) = 0.21, P(Y\u0302 1 2 = 0|Y = 1, G = 1) = 0.13 P(Y\u0302 1 2 = 0|Y = 0, G = 0) = 0.79, P(Y\u0302 1 2 = 0|Y = 0, G = 1) = 0.69.\nThe classifier Y\u0302 1 2 does not satisfy neither the statistical parity, nor the equal opportunity, nor the equalized odds criterion. In fact, for any threshold t \u2208 R the classifier Y\u0302t does not satisfy any of the aforementioned fairness criteria; see Figure 1b."
    },
    {
      "heading": "2.4 Classifier bias based on statistical parity",
      "text": "In this section we provide a definition for classifier bias based on the statistical parity fairness criterion and establish some basic properties of the classifier bias. In what follows, we suppress the symbol \u0302 , using it only when it is necessary to differentiate between the true model and the trained one. The same rule applies to classifiers.\nDefinition 2.4. Let f(X) be a model, G \u2208 {0, 1} protected attribute, G = 0 non-protected class, and \u03c2f the sign of the favorable direction of f . Let Fk be the CDF function of f(X)|G = k. Let t \u2208 R.\n\u2022 The signed classifier (or statistical parity) bias for a threshold t \u2208 R is defined by\nb\u0303ias C t (f |G) = ( P(Yt = 1{\u03c2f=1}|G = 0)\u2212 P(Yt = 1{\u03c2f=1}|G = 1) ) \u00b7 \u03c2f\n= ( F1(t)\u2212 F0(t) ) \u00b7 \u03c2f .\nWe say that Yt favors the non-protected class G = 0 if the signed bias is positive. Respectively, Yt favors the protected class G = 1 if the signed bias is negative.\n\u2022 The classifier bias at t \u2208 R is defined by\nbiasCt (Z|G) = |b\u0303ias C t (Z|G)|.\nRemark 2.1. Suppose that Y \u2208 {0, 1} is binary and that the favorable direction is \u2191, which implies that 1{\u03c2f=1} = 1. Then Yt favors the non-protected class G = 0 if and only if there is a larger proportion of individuals from class G = 0 for which Yt = 1 compared to the class G = 1. This, from statistical parity perspective, describes the outcome Y = 1 as favorable. Similar remarks apply to the case when the favorable direction is \u2193. Thus in the case of binary Y the favorable direction is \u2191 (\u2193) is equivalent to the favorable outcome Y = 1 (Y = 0).\nRemark 2.2. The bias definitions provided in this section can be extended to the case when the protected attribute G contains several classes, G \u2208 {0, 1, 2, . . . ,K \u2212 1}, as well as to other fairness criteria such as equal opportunity and equalized odds; see Appendix C and Appendix D."
    },
    {
      "heading": "2.5 Quantile bias and geometric parity",
      "text": "Given a model f and a threshold t \u2208 R, the classifier bias based on statistical parity measures the difference in population sizes corresponding to groups G = {0, 1} for which Yt = 0. This measurement however does not take into account the geometry of the score distribution, that is, the score values themselves.\nFor example, when measuring the bias in incomes among \u2018females\u2019 and \u2018males\u2019 one can view the difference of expected incomes in the two groups as \u2018bias\u2019. Alternatively, one can measure an income bias by evaluating the absolute difference of the \u2018female\u2019 median income and \u2018male\u2019 median income, which is often done in various social studies. This motivates us to take into account the geometry of the score distribution when defining bias. For this reason, we propose the notion of the quantile bias which operates on the domain of the score rather than the sample space.\nDefinition 2.5. Let f(X) be a model, G \u2208 {0, 1} the protected attribute, G = 0 the non-protected class, and \u03c2f the sign of the favorable direction of f . Let F [\u22121] k be the quantile function of f(X)|G = k. Let p \u2208 [0, 1]. \u2022 The signed p-th quantile is defined by\nb\u0303ias Q p (f |G) = ( F [\u22121] 0 (p)\u2212 F [\u22121] 1 (p) ) \u00b7 \u03c2f\n\u2022 The p-th quantile bias is defined by\nbiasQp (Z|G) = |b\u0303ias Q p (Z|G)|.\nAs a counterpart to statistical parity, we also introduce quantile (geometric) parity.\nDefinition 2.6 (geometric parity). Let f be a model, G \u2208 {0, 1} the protected attribute, and G = 0 the non-protected class.\n\u2022 We say that the model f satisfies p-th quantile (or geometric) parity if\nbiasQp (f |G) = 0.\n\u2022 Let t \u2208 R. The classifier Yt satisfies quantile (or geometric) parity if\nbiasQp0(f |G) = 0, p0 = F [\u22121] 0 (t).\nRemark 2.3. Given a score f , the quantile bias measures the difference between subpopulation quantile values. For a given threshold t, the p0-quantile signed bias, with p0 = F [\u22121] 0 (t), measures by how much the corresponding score values of the protected class G = 1 differ from that of G = 0 or equivalently by how much the threshold for the protected group should be shifted to achieve the quantile parity (and in some cases statistical parity) between the two populations; see Figure 2a for the illustration.\nLemma 2.1. Let f be a model, G \u2208 {0, 1} the protected attribute, and G = 0 the unprotected class. Suppose that t0 \u2208 R is a point at which the CDFs F0 and F1 are continuous and strictly increasing. Then Yt0 satisfies statistical parity if and only if it satisfies geometric parity.\nProof. The result follows from Definition 2.4, Definition 2.5, and the fact that F0 and F1 are locally invertible at t0."
    },
    {
      "heading": "3 Model bias metric",
      "text": "In this section, we introduce an appropriate metric to measure the model bias at the level of its distribution and then investigate its properties by establishing the connection with classifier and quantile fairness criteria.\nDefinition 3.1 (D-model bias). Let X \u2208 Rn be predictors, f(X) be a model, and G \u2208 {0, 1} the protected attribute. Let D(\u00b7, \u00b7) be a metric on the space of probability measures Pm(R), with m \u2265 0. The D-based model bias is defined as the distance between the subpopulation distributions of the model:\nBiasD(f |G) := D(Pf |G=0, Pf |G=1), (3.1)\nwhere Pf |G=k is the pushforward probability measure of f(X)|G = k, provided E[|f(X)|m] <\u221e. We say that the model f is fair up to the D-based model bias if BiasD(f |G) \u2264 ."
    },
    {
      "heading": "3.1 Wasserstein and Kolmogorov-Smirnov distances",
      "text": "To determine an appropriate metric D to be used in (3.1) is not a trivial task. The choice depends on the context in which the model bias is measured. We argue that it is desirable for the metric to have the following properties:\n(P1) It should be continuous with respect to the change in the geometry of the distribution.\n(P2) It should be non-invariant with respect to monotone transformations of the distributions.\nThe property (P1) makes sure that the metric keeps track of changes in the geometry. For instance, suppose an \u201cincome\u201d of the group {G = 0} is x0 and that of {G = 1} is x1. A metric that measures income inequality should be able to sense the distance between x0 and x0 + \u03b5. That is, having two delta measures \u03b4x0 and \u03b4x0+\u03b5 the metric must ensure that as \u03b5\u2192 0 the distance D(\u03b4x0 , \u03b4x0+\u03b5) approaches zero. The property (P1) also makes sure that slight changes in the subpopulation distributions lead to a slight change in bias measurements, which is important for stability with respect to changes in the dataset X.\nThe property (P2) makes sure that the metric is non-invariant with respect to monotone transformations. That is, given two random variables X0 and X1 and a continuous, strictly increasing transformation T : R \u2192 R, one would expect the change in distance between T (X0) and T (X1) whenever T is not the identity map. For example, if T (x) = \u03b1 \u00b7 x, we would expect the distance between T (X0) = \u03b1X0 and T (X1) = \u03b1X1 depend continuously on \u03b1.\nIn what follows we consider two potential candidates for the distances: the Wasserstein distance DWq and Kolmogorov-Smirnov distance DKS , a popular metric in the machine learning community; for example, see Hardt et al. (2015), del Barrio et al. (2019), Kovalev and Utkin (2020).\nTo introduce these metrics and investigate their properties we switch our focus to probability measures; recall that any random variable Z gives rise to the pushforward probability measure PZ(A) =\nP(Z \u2208 A) on R, and the reverse is true, for any \u00b5 \u2208 P(R) with the CDF F\u00b5(a) = \u00b5((\u2212\u221e, a]) there is a random variable Z such that PZ = \u00b5. Similar remarks apply for random vectors; see Shiryaev (1980). Given T : Rk \u2192 Rm and \u00b5 \u2208P(Rk), we denote by T#\u00b5 a measure such that T#\u00b5(B) = \u00b5 ( T\u22121(B)).\nThe Wasserstein distance Wq is connected with the concept of optimal mass transport. Given two probability measures \u00b51, \u00b52 \u2208Pq(R) with finite q-th moment and the cost function c(x1, x2) = |x1\u2212x2|q, the Wasserstein distance DWq is defined by\nDWq (\u00b51, \u00b52) := T 1/q\n|x1\u2212x2|q (\u00b51, \u00b52)\nwhere\nT|x1\u2212x2|q (\u00b51, \u00b52) = inf \u03b3\u2208P(R2) {\u222b R2 |x1 \u2212 x2|q d\u03b3(x1, x2), with marginals \u00b51, \u00b52 } is the minimal cost of transporting the distribution \u00b51 into \u00b52, and vice versa in view of the symmetry of the cost function. A joint probability measure \u03b3 \u2208P(R2) with marginals \u00b51 and \u00b52 is called a transport plan. It specifies how each point x1 from supp(\u00b51) gets distributed in the course of the transportation; specifically, the transport of x1 is described by the conditional probability measure \u03b3x2|x1 .\nIt can be shown that the Wasserstein metric for probability measures on R can be expressed in terms of the quantile functions\nDWq (\u00b51, \u00b52) = (\u222b 1 0 |F [\u22121]\u00b51 (p)\u2212 F [\u22121] \u00b52 (p)| q dp )1/q , (3.2)\nwhich makes the computation feasible; see Proposition A.2 or Santambrogio (2015, p. 66). The Kolmogorov-Smirnov metric estimates the largest difference between the CDFs:\nDKS(\u00b51, \u00b52) = sup t\u2208R |\u00b51((\u2212\u221e, t])\u2212 \u00b52((\u2212\u221e, t])| = sup t\u2208R |F\u00b51(t)\u2212 F\u00b52(t)|. (3.3)\nTo get an understanding of the behavior of these two distances consider two delta measures located at x0 and x0 + \u03b5, respectively. By definition of the two metrics it follows that\nDWq (\u03b4x0 , \u03b4x0+\u03b5) = \u03b5, DKS(\u03b4x0 , \u03b4x0+\u03b5) = 1.\nThus DWq is continuous with respect to a shift of a point mass, while DKS is not; see Figure 3a. Furthermore, for any two random variables X0 and X1 and \u03b1 > 0\nDWq (P\u03b1X0 , P\u03b1X1) = \u03b1DWq (PX0 , PX1), DKS(P\u03b1X0 , P\u03b1X1) = DKS(PX0 , PX1),\nwhich implies that even a multiplicative transformation T (x) = \u03b1x affects the Wasserstein distance but not KS one; see Figure 3b.\nClearly, the two metrics are fundamentally different in how they assess the distance. To get a better understanding of this difference, we provide a theoretical characterization of some of their properties.\nDefinition 3.2 (geometric continuity). Let D(\u00b7, \u00b7) be a metric on Pk(Rn), with k \u2265 0. We say that D is continuous with respect to the geometry of the distribution if for any \u00b5 \u2208Pk(Rn)\nlim \u21920+ D(\u00b5, T\u03b5#\u00b5) = 0,\nfor any family {T\u03b5}\u03b5>0 of continuously differentiable maps from Rn to Rn that satisfy (i) det\u2207T\u03b5 > 0.\n(ii) The family {T\u03b5 \u2212 I}\u03b5 has a common compact support. (iii) T\u03b5 \u2192 I uniformly on Rn as \u03b5\u2192 0, where I is the identity map. Definition 3.3 (invariance). Let D(\u00b7, \u00b7) a metric on Pk(Rn). Let T : Rn \u2192 Rn be a map such that T#\u00b5 \u2208Pk(Rn) for every \u00b5 \u2208Pk(Rn). We say that D is invariant under the transformation T if\nD(\u00b51, \u00b52) = D ( T#\u00b51, T#\u00b52 ) .\nTheorem 3.1. The distances DWq and DKS satisfy:\n(a) DWq on Pq(R) is continuous with respect to the geometry of the distribution. (b) DKS on P(R) is not continuous with respect to the geometry of the distribution.\nProof. See Appendix A.3.\nThe next theorem investigates invariance of the Wasserstein and KS distances.\nTheorem 3.2. Let T : R \u2192 R be a continuous, strictly increasing map, which is not the identity map. The distances DWq and DKS satisfy:\n(a) DWq is non-invariant under T , provided T#\u00b5 \u2208Pq(R) for any \u00b5 \u2208Pq(R). (b) DKS is invariant under T .\nProof. See Appendix A.3.\nMetric choice. Theorem 3.1 and Theorem 3.2 demonstrate that the Wasserstein metric relies on the geometry of the distribution. In particular, the distance is affected in a continuous way by the change in the geometry of the distribution. This, in turn, provides the desired sensitivity of the the Wasserstein metric with respect to slight changes in the dataset distribution, including shifts, which is relevant for ML models with ragged CDFs, which makes the Wasserstein metric a perfect candidate for the model bias measurement. In contrast, the KS distance relies purely on statistical properties of the distribution; it is conservative and it is not affected by continuous monotonic transformations of underlying distributions. Thus, the KS metric fails to satisfy either of the practically desired properties (P1) and (P2) and we find it less suitable for bias measurement at the level of the model. For this reason, in what follows, we choose to work with the Wasserstein distance.\nOrder preserving optimal transport plan. We now provide several useful properties of the Wasserstein metric, which we will be employing in the following sections.\nGiven two probability measures \u00b51, \u00b52 \u2208 Pq(R), it can be shown that the joint probability measure \u03c0\u2217 \u2208P(R2) with the CDF\nF\u03c0\u2217(a, b) = min(F\u00b51(a), F\u00b52(b)) (3.4)\nis an optimal transport plan for transporting \u00b51 into \u00b52 with the cost function c(x1, x2) = |x1\u2212x2|q, and thus,\nDqWq (\u00b51, \u00b52) = T|x1\u2212x2|q (\u00b51, \u00b52) = \u222b R2 |x1 \u2212 x2|qd\u03c0\u2217(x1, x2). (3.5)\nMost importantly, \u03c0\u2217 is the only monotone (order preserving) transport plan, in the sense that\n(x1, x2), (x \u2032 1, x \u2032 2) \u2208 supp(\u03c0\u2217), x1 < x\u20321 \u21d2 x2 \u2264 x\u20322.\nIn a special case, when \u00b51 is atomless, the transport plan \u03c0 \u2217 is determined by the monotone map\nT \u2217 = F [\u22121]\u00b52 \u25e6 F\u00b51 , (3.6)\ncalled an optimal transport map. Specifically, each point x1 of the distribution \u00b51 is transported to the point x2 = T \u2217(x1). Thus, \u00b52 = T \u2217 #\u00b51, and the conditional probability measure \u03c0 \u2217 x2|x1 = \u03b4T\u2217(x1) for x1 \u2208 supp(\u00b51); see Figure 4a. In this case, (3.5) reads\nDqWq (\u00b51, \u00b52) = T|x1\u2212x2|q (\u00b51, \u00b52) = \u222b R |x1 \u2212 T \u2217(x1)|qd\u00b51(x1) = E[|X1 \u2212 T \u2217(X1)|q], PX1 = \u00b51.\nFor details, see Theorem A.1 and Proposition A.2, or Santambrogio (2015). In a general case, under the transport plan \u03c0\u2217, points x1 \u2208 supp(\u00b51) for which \u00b51({x1}) = 0 are transported as a whole, while the \u201catoms\u201d, points x1 for which \u00b51({x1}) > 0, are allowed to be split or spread along R; see Figure 4b that illustrates the transport flow under \u03c0\u2217 in the general case.\nTo compute the portion of the transport cost used for moving points of \u00b51 to the right of left, it is sufficient to restrict the attention to the regions x1 < x2 and x1 > x2, respectiely. Lemma 3.1. Let \u00b51, \u00b52 \u2208Pq(R). Under the monotone plan \u03c0\u2217 the transport efforts to the left and right for the cost function c(x1, x2) = |x1 \u2212 x2|q are given by:\nT \u2192\u2190 |x1\u2212x2|q (\u00b51, \u00b52) = \u222b {\u00b1(x2\u2212x1)>0} |x1 \u2212 x2|qd\u03c0\u2217(x1, x2)\n= \u222b{ \u00b1(F [\u22121]\u00b52 (p)\u2212F [\u22121] \u00b51 (p))>0 } |F [\u22121]\u00b51 (p)\u2212 F [\u22121]\u00b52 (p)|qdp. (3.7)\nHence, the Wasserstein distance Wq can be expressed as\nDWq (\u00b51, \u00b52) = ( T \u2190|x1\u2212x2|q (\u00b51, \u00b52) + T \u2192 |x1\u2212x2|q (\u00b51, \u00b52) )1/q . (3.8)\nFurthermore, if \u00b51 is atomless, (3.7) reads\nT \u2192\u2190 |x1\u2212x2|q (\u00b51, \u00b52) = \u222b{ \u00b1(T\u2217(x1)\u2212x1)>0\n} |x1 \u2212 T \u2217(x1)|q d\u00b51(x1), T \u2217 = F [\u22121]\u00b52 \u25e6 F\u00b51 (3.9) Proof. See Proposition A.2.\n3.2 W1-based model bias and its components\nFor q = 1 the Wasserstein distance DW1 is known as the Earth Mover distance. Since the distance is symmetric, we have\nDW1(Pf |G=0, Pf |G=1) = DW1(Pf |G=1, Pf |G=0).\nThus, in the context of transport theory, BiasW1(f |G) is precisely the cost of transporting the distribution of f |G = 0 into that of f |G = 1, or equivalently the cost of transporting the distribution of f |G = 1 into that of f |G = 0.\nIt turns out that the W1 based model bias formulation is consistent with both statistical parity fairness criterion as well as quantile parity criterion, which is shown by the following theorem.\nTheorem 3.3. Let f be a model and G \u2208 {0, 1} the protected attribute. DW1 based model bias satisfies\nBiasW1(f |G) = \u222b 1\n0\n|F [\u22121]0 (p)\u2212 F [\u22121] 1 (p)|dp = \u222b R |F0(t)\u2212 F1(t)|dt\n= \u222b 1 0 biasQp (f |G) dp = \u222b R biasCt (f |G) dt.\n(3.10)\nProof. Seep Appendix B.1.1, or alternatively Santambrogio (2015).\nPositive and negative model bias. According to Lemma 3.1, the cost of transporting a distribution is the sum of transport effort to the left and the transport effort to the right. This motivates us to define the positive bias as the transport effort for moving the particles of f |G = 0 in the non-favorable direction and the negative bias as the transport effort in the favorable one; equivalently the latter is the transport effort for moving the particles of f |G = 1 into the favorable direction and the former is the transport effort into the non-favorable one.\nMotivated by Lemma 3.1 we define positive and negative model biases as follows:\nDefinition 3.4. Let f be a model, G \u2208 {0, 1} the protected attribute, {G = 0} the non-protected class, and \u03c2f the sign of the favorable direction of f .\n\u2022 The positive and negative W1 based model biases are defined by Bias\u00b1W1(f |G) = \u222b P\u00b1 \u00b1(F\u221210 (p)\u2212 F \u22121 1 (p)) \u00b7 \u03c2f dp (3.11)\nwhere P\u00b1 = { p \u2208 (0, 1) : \u00b1b\u0303ias Q p (f |G) = \u00b1(F\u221210 (p)\u2212 F \u22121 1 (p)) \u00b7 \u03c2f > 0 } . (3.12)\nIn this case, the model bias is disaggregated as follows:\nBiasW1(f |G) = Bias + W1 (f |G) + Bias\u2212W1(f |G). (3.13)\n\u2022 The net model bias is defined by\nBnetW1 (f |G) = B + W1 (f |G)\u2212 B\u2212W1(f |G).\nWe next establish that the positive and negative W1 model biases can be expressed in terms of quantile and classifier biases:\nTheorem 3.4. Let f,G and \u03c2f be as in Definition 3.4. The positive and negative W1-based model biases satisfy Bias\u00b1W1(f |G) = \u222b P\u00b1 biasQp (f |G) dp = \u222b T\u00b1 biasCt (f |G) dt (3.14)\nwhere T\u00b1 = { t \u2208 R : \u00b1b\u0303ias C t (f |G) = \u00b1(F1(t)\u2212 F0(t)) \u00b7 \u03c2f > 0 } . (3.15)"
    },
    {
      "heading": "The net bias satisfies",
      "text": "BiasnetW1 (f |G) = \u222b 1\n0\nb\u0303ias Q p (f |G)dp = \u222b R b\u0303ias C t (f |G) dt\n= ( E[f(X)|G = 0]\u2212 E[f(X)|G = 1] ) \u00b7 \u03c2f\n(3.16)\nProof. See Appendix B.1.2.\nRemark 3.1. In the context of classification, Theorem 3.14 states that the positive W1 based model bias is the integrated classifier bias over the set of thresholds t \u2208 T+ where the classifiers Yt = 1{f(X>t} favor the non-protected class G = 0. Similar remark holds for the negative model.\nExample. To understand the statement of Theorem 3.4 consider the following classification risk model (\u03c2f = \u22121) with a predictor whose variance depends on the attribute G:\nX \u223c N(\u00b5, (1 +G)\u221a\u00b5), \u00b5 = 5 Y \u223c Bernoulli(f(X)), f(X) = P(Y = 1|X) = \u03c3(\u00b5\u2212X).\n(M2)\nwhich leads to the presence of both positive and negative bias components in the score distribution. Figure 5 depicts the subpopulation score CDFs of the trained GBM classifier and illustrates the fact that the integrated positive quantile and classifier biases yield the positive model bias, and a similar relationship holds for the negative model bias.\nRemark 3.2. The remarkable properties (3.10) and (3.14) of W1 based bias allow one to use thresholds and quantiles interchangeably to compute the model bias which is meaningful in the context of classification problems. For this reason, we will make the DW1 distance our primary metric utilized in the bias explanation engine."
    },
    {
      "heading": "4 Bias explanations",
      "text": "Whenever the model bias is significant, it is crucial to quantify the contribution of each predictor to the model bias. To do this we design a bias explainer framework that combines the Wasserstein based bias measurement methodology with model interpretability methodologies.\nWhile the bias explainer we developed below is agnostic to the choice of a model explainer, we will review several well-known interpretability methods that will help to demonstrate how the bias explainer works in practice."
    },
    {
      "heading": "4.1 Model interpretability",
      "text": "The objective of a model interpretor, or explainer, is to explain, or simply quantify, the contribution of a predictor to the model value. Several methods of interpreting ML model outputs have been designed and used over the years. Some notable ones are Partial Dependence Plots (PDP) (Friedman, 2001) and SHAP values (Lundberg and Lee, 2017).\nPartial Dependence Function. Partial Dependence Plots are the graphs of what is known as the Partial Dependence Function. In this article we will use the shorthand PDP to refer to both when the context is clear. PDP marginalizes out the variables whose impacts to the output are not of interest, providing a quantity that serves as an overall average impact of the values of the remaining features.\nLet X = (X1, X2, . . . , Xn) be predictors, XS with S \u2286 {1, 2, . . . , n} a subvector of X, and \u2212S the complement set. Given an ML model f(X), the partial dependence function f\u0304S , or PDPS , of f on XS is given by\nPDPS(X; f) = f\u0304S(X) := E[f(XS , X\u2212S)]|xS=XS \u2248 1\nN N\u2211 j=1 f(XS , X (j) \u2212S). (4.1)\nIf S = {i}, we write f\u0304i and the complement subvector to Xi is denoted by X\u2212i. We need to note that f\u0304S depends explicitly only on the subvector XS . However, we adopt the above notation for the partial dependence function and write it as a function of the entire vector of predictors X which will be needed later when working with generic explainers.\nShapley Additive Explanations. In its original form the Shapley values appear in the context of cooperative games; see Shapley (1953), Young (1985). A cooperative game with n players is a superadditive set function v that acts on N = {1, 2, . . . , n} and satisfies v(\u2205) = 0. Shapley was interested in determining the contribution by each player to the game value v(N). It turns out that under certain symmetry assumptions the contributions are unique and they are called Shapley values; furthermore, the super-additivity assumption can in principle be dropped (uniqueness and existence still hold).\nIt is shown in Shapley (1953) that there exists a unique collection of values {\u03d5i}ni=1 satisfying the axioms of symmetry, efficiency, and law of aggregation, ((A1)-(A3) in Shapley (1953)), it is given by\n\u03d5i[v] = \u2211 S\u2286N \u03b3n(s)[v(S)\u2212 v(S\\{i})], \u03b3n(s) = (s\u2212 1)!(n\u2212 s)! n! , s = |S|, n = |N |. (4.2)\nThe values provide a disaggregation of the value v(N) of the game into n parts that represent a contribution to the worth by each player:\nn\u2211 i=1 \u03d5i[v] = v(N). (4.3)\nThe explanation techniques explored in Strumbelj and Kononenko (2014) and Lundberg and Lee (2017) utilize cooperative game theory to compute the contribution of each predictor to the model value. In particular, given a model f , Lundberg and Lee (2017) consider the games\nvCE(S; f) = E[f |XS ], vPDP (S; f) = PDPS(X; f) (4.4)\nwith vCE(\u2205; f) = vPDP (\u2205; f) = E[f(X)].\nThe values \u03d5i[v CE ] computed by (4.2) are called SHAPs. In what follows we will refer to \u03d5i[v PDP ] also as SHAP values and write\n\u03d5i[v PDP ] = SHAPi(X; f, v PDP ) and \u03d5i[v CE ] = SHAPi(X; f, v CE).\nThe games defined in (4.4) are not cooperative since they do not satisfy the condition v(\u2205) = 0. However, the values provide a useful description for predictors, specifically \u03d5i represents the contribution of the feature Xi to the deviation of the model prediction from the average prediction. By setting \u03d50 = E[f ] and extending N to N0 = N \u222a {0}, the SHAP values satisfy the additivity property:\nn\u2211 i=0 SHAPi(X; f, v CE) = n\u2211 i=0 SHAPi(X; f, v PDP ) = f(X).\nIn general, SHAPs are computationally intensive to evaluate due to the different combinations of predictors that need to be considered; in addition, computing \u03c6i[v\nCE ] is challenging when the predictor\u2019s dimension is large in light of the curse of dimensionality; see Hastie et al. (2016). Lundberg et al. (2019) created a fast method called TreeSHAP to evaluate \u03c6[vCE ] but it can only be applied to ML algorithms that incorporate tree-based techniques; in addition, the algorithm approximates E[f |XS ] \u2248 PDPS(X; f) which produces the approximates of \u03d5i[v PDP ]. To understand the difference between \u03d5i[v CE ] and \u03d5i[v\nPDP ] see Janzing et al. (2019), Sundararajan and Najmi (2019), Chen et al. (2020), Kotsiopoulos et al. (2020)."
    },
    {
      "heading": "4.2 Bias explanations of predictors",
      "text": "In this section, given a model, we define the bias explanation (or contribution) of each predictor. An extension to groups of predictors maybe found in Appendix E.\nIn what follows we will be using the following notation. Given predictors X = (X1, X2, . . . , Xn) and a model f , a generic single feature explainer of f that quantifies the attribution of each predictor Xi to the model value f(X) is denoted by\nE(X; f) = (E1(X; f), E2(X; f), . . . , En(X; f)). (4.5)\nDefinition 4.1. Let X = (X1, X2, . . . , Xn) be predictors, f a model, G \u2208 {0, 1} the protected attribute, G = 0 the non-protected class, and \u03c2f the sign of the favorable direction of f . Let E(X; f) be an explainer of f that satisfies E [ |E(X; f)| ] <\u221e.\n\u2022 The bias explanation of the predictor Xi is defined by \u03b2i(f |G;Ei) = DW1(Ei(X; f)|G = 0, Ei(X; f)|G = 1) = \u222b 1\n0\n|F [\u22121]Ei|G=0 \u2212 F [\u22121] Ei|G=1 | dp.\n\u2022 The positive bias and negative bias explanations of the predictor Xi are defined by \u03b2\u00b1i (f |G;Ei) = \u222b Pi\u00b1 (F [\u22121] Ei|G=0 \u2212 F [\u22121]Ei|G=1) \u00b7 \u03c2f dp\nwhere Pi\u00b1 = {p \u2208 [0, 1] : \u00b1(F [\u22121]Ei|G=0 \u2212 F [\u22121] Ei|G=1\n) \u00b7 \u03c2f > 0}. In this case the Xi bias explanation is disaggregated as follows:\n\u03b2i(f |G;Ei) = \u03b2+i (f |G;Ei) + \u03b2 \u2212 i (f |G;Ei).\n\u2022 The Xi net bias explanation is defined by\n\u03b2neti (f |G;Ei) = \u03b2+i (f |G;Ei)\u2212 \u03b2 \u2212 i (f |G;Ei).\n\u2022 The classifier (or statistical parity) bias of the explainer Ei for a threshold t \u2208 R is defined by\nb\u0303ias C t (Ei|G) = ( FEi|G=1(t)\u2212 FEi|G=0(t) ) \u00b7 \u03c2f .\nLemma 4.1. Let X, f , G, Ei(X; f), and \u03c2f be as in the definition 4.1. Then \u03b2net(f |G;Ei) = ( E[Ei(X; f\u0302)|G = 0]\u2212 E[Ei(X; f\u0302)|G = 1] ) \u00b7 \u03c2f . (4.6)\nProof. The result follows directly from Theorem 3.4 by setting \u03c2Ei = \u03c2f .\nThe explainer Ei that appears in Definition 4.1 is a generic one. In the examples that follow we chose to work with explainers based on PDP or SHAPs. For these explainers, the bias explanations always lie in the unit interval.\nLemma 4.2. Let f be a classification score and G \u2208 {0, 1} the protected attribute. Let the explainer Ei be either PDPi(f), SHAPi(f, v CE), or SHAPi(f, v PDP ). Then \u03b2i, \u03b2 \u2212 i , \u03b2 + i \u2208 [0, 1]. Proof. The lemma follows from the fact that f \u2208 [0, 1] and the definition of explainer values. Intuition. For a given model f and the explainer Ei the explanation \u03b2i quantifies the DW1 distance between the distributions of the explainer Ei|G = 0 and Ei|G = 1, that is, this value is an assessment of the bias introduced by the predictorXi. The value \u03b2i is the area between the corresponding subpopulation explainer CDFs FEi|G=k, k \u2208 {0, 1}, similar to the area depicted in Figure 5. The value \u03b2 + i represents the bias across quantiles of the explainer Ei for which the predictor Xi favors the unprotected class G = 0 and \u03b2\u2212i represents the bias across quantiles for which Xi favors the protected class G = 1. The \u03b2 net i assesses the net contribution across different quantiles and represents an explanation that allows one to assess whether on average the predictor Xi favors class G = 0 or class G = 1; see Lemma 4.1.\nIn what follows we consider several simple examples to get more intuition behind the bias explanation values as well as discuss their additivity or the lack thereof. To avoid complex notation we suppress the dependence of the bias explanations on f and G.\nDefinition 4.2. Let f , X, G, and Ei be as in Definition 4.1.\n\u2022 We say that Ei strictly favors non-protected class G = 0 if \u03b2\u2212i (f |G;Ei) = 0. \u2022 We say that Ei strictly disfavors protected class G = 0 if \u03b2+i (f |G;Ei) = 0. \u2022 We say that Xi has mixed bias explanations if \u03b2+i (f |G;Ei) and \u03b2 \u2212 i (f |G;Ei) are both nonzero.\nOffsetting. Since each predictor may favor one class or the other, the predictors may offset each other in terms of the bias contributions to the model bias. To understand the offsetting effect consider a binary classification risk model (\u03c2f = \u22121) with two predictors:\nX1 \u223c N(\u00b5+G, 1), X2 \u223c N(\u00b5\u2212G, 1) Y \u223c Bernoulli(f(X)), f(X) = P(Y = 1|X) = logistic(2\u00b5\u2212X1 \u2212X2)\n(M3)\nwhere \u00b5 = 5, and {Xi|G = k}i,k are independent and P(G = 0) = P(G = 1). We next train logistic regression score f\u0302(X), with \u03c2f\u0302 = \u22121, and choose the explainer to be Ei = PDPi. By construction the explanation E1 of the predictor X1 strictly favors class G = 0, while that of X2 strictly favors class G = 1. Moreover,\n\u03b21(f\u0302 |G;E1) = \u03b2+1 (f\u0302 |G;E1) = \u03b2 \u2212 2 (f |G;E2) = \u03b21(f\u0302 |G;E2) \u2248 0.17.\nCombining the two predictors at the model level leads to bias offsetting. By construction the resulting model bias is BiasW1(f |G) = 0. Figure 6 displays the CDFs for the trained score subpopulations f\u0302 |G = k and the corresponding explainers Ei|G = k, which illustrates the offsetting phenomena numerically.\nAnother important point we need to make is that the equality \u03b2neti = 0 does not in general imply that the predictor Xi has no affect on the model bias. This is a consequence of (4.6). Moreover, predictors with mixed bias might amplify the model bias as well as offset it. To understand how mixed bias predictors interact at the level of the model bias consider the following risk classification model (\u03c2f = \u22121).\nX1 \u223c N(\u00b5, 1 +G), X2 \u223c N(\u00b5, 1 +G) Y \u223c Bernoulli(f(X)), f(X) = P(Y = 1|X) = logistic(2\u00b5\u2212X1 \u2212X2).\n(M4)\nwhere \u00b5 = 5, and {Xi|G = k}i,k are independent and P(G = 0) = P(G = 1). As before we train a logistic regression score f\u0302 , with \u03c2f\u0302 = \u22121, and choose Ei = PDPi. By construction, the true classification score f satisfies \u03b2neti (f |G) = 0 for each predictor Xi. Furthermore, the CDFs of explainers satisfy\n(FEi(f)|G=0(t)\u2212 FEi(f)|G=1(t)) \u00b7 sgn(t\u2212 0.5) > 0\nfor any threshold t 6= 0.5. Combining the two predictors at the level of the model leads to amplifying the positive and negative model biases and hence the model bias itself. Figure 7 displays the CDFs for the trained score subpopulations f\u0302 |G = k and the corresponding explainers Ei(f\u0302)|G = k. The numerics illustrates that as long as the regions for positive and negative bias of mixed predictors agree the mixed bias predictors, when combined, will increase the model bias.\nIf the regions of positive and negative bias for two predictors do not agree, then offsetting will happen. To see this, let us modify the above example as follows:\nX1 \u223c N(\u00b5, 2\u2212G), X2 \u223c N(\u00b5, 1 +G) Y \u223c Bernoulli(f(X)), f(X) = P(Y = 1|X) = logistic(2\u00b5\u2212X1 \u2212X2).\n(M5)\nBy construction, \u03b2neti (f |G) = 0 for each predictor. However, the region of thresholds where the explainer E1(f) favors class G = 0 coincides with the region where E2(f) favors class G = 1, and the same holds for the two complimentary regions. This leads to bias offsetting so that BiasW1(f |G) = 0. The numerical results for this example are displayed in Figure 7.\nBias Explanation Plots. Given a machine learning model f , predictors X \u2208 Rn, protected attribute G, and the explainers Ei, the corresponding bias explanations{\n(\u03b2i, \u03b2 + i , \u03b2 \u2212 i , \u03b2 net i )(f |G;Ei) }n i=1\nare sorted according to any desired entry in the 4-tuple and then displayed in that order. This plot is called Bias Explanation Plot (BEP).\nTo showcase how BEP works, consider a classification risk model with five predictors (\u03c2f = \u22121):\n\u00b5 = 5, a = 1 20 (10,\u22124, 16, 1,\u22123) X1 \u223c N(\u00b5\u2212 a1(1\u2212G), 0.5 +G), X2 \u223c N(\u00b5\u2212 a2(1\u2212G), 1) X3 \u223c N(\u00b5\u2212 a3(1\u2212G), 1), X4 \u223c N(\u00b5\u2212 a4(1\u2212G), 1\u2212 0.5G) X5 \u223c N(\u00b5\u2212 a5(1\u2212G), 1\u2212 0.75G) Y \u223c Bernoulli(f(X)), f(X) = P(Y = 1|X) = logistic( \u2211 iXi \u2212 24.5).\n(M6)\nwhere {Xi|G = k}i,k are independent and P(G = 0) = P(G = 1). We next generate 20, 000 samples from the distribution (X,Y ) and train a reularized XGBoost model which produces the score f\u0302 . Figure 9 displays the CDFs of the subpopulation scores f\u0302 |G = k and those of the explainers Ei = SHAPi(f\u0302 , vPDP ).\nThe numerically computed model bias and its disaggregation are given by\n(BiasW1 ,Bias + W1 ,Bias\u2212W1 ,Bias net W1 )(f\u0302 |G) = (0.1533, 0.1533, 0, 0.1533)\nThe bias explanations are then computed as the Earth Mover distance, and its disaggregation, between the distributions of subpopulation explainers Ei(f\u0302)|G = k. The bias explanations are given by\n(\u03b21, \u03b2 + 1 , \u03b2 \u2212 1 , \u03b2 net 1 ) = (0.0860, 0.0799, 0.0061, 0.0738) (\u03b22, \u03b2 + 2 , \u03b2 \u2212 2 , \u03b2 net 2 ) = (0.0328, 0, 0.0328,\u22120.0328) (\u03b23, \u03b2 + 3 , \u03b2 \u2212 3 , \u03b2 net 3 ) = (0.1100, 0.1100, 0, 0.1100) (\u03b24, \u03b2 + 4 , \u03b2 \u2212 4 , \u03b2 net 4 ) = (0.0289, 0.0169, 0.0119, 0.0050) (\u03b25, \u03b2 + 5 , \u03b2 \u2212 5 , \u03b2 net 5 ) = (0.0584, 0.0127, 0.0457,\u22120.0330)\nFigure 10 displays the above bias explanations in the increasing order by total bias and positive bias as well as ranked net bias.\nRelationship with model bias. The positive and negative bias explanations provide an informative way to determine the main drivers for positive and negative bias among predictors, which can be done by ranking the bias attributions. However, though informative the positive and negative bias explanations are not additive. That is, in general\nBias\u00b1W1(f\u0302 |G) 6= n\u2211 i=1 \u03b2\u00b1i (f\u0302 |G;Ei). (4.7)\nThe main reasons for lack of additivity is the fact that predictors interact at the level of the model bias in a non-trivial way (in view of the offsetting across classifier thresholds); see Example 7.\nFor additive models with independent predictors however one can establish the following result.\nLemma 4.3. Let X = (X1, X2, . . . , Xn) be independent predictors. Suppose that f(X) is an additive model such that f(X) = f1(X1) + f2(X2) + \u00b7 \u00b7 \u00b7+ fn(Xn). Let an explainer Ei be either PDPi, SHAPi(v PDP ), or SHAPi(v CE). Let {\u03b2i, \u03b2+i , \u03b2 \u2212 i , \u03b2 net i }i be the bias explanations of f . Then\nBiasnetW1 (f |G) = Bias + W1 (f |G)\u2212 Bias\u2212W1(f |G) = n\u2211 i=1 ( \u03b2+i \u2212 \u03b2 \u2212 i ) = n\u2211 i=1 \u03b2neti . (4.8)\nProof. see Appendix B.1.3.\nRemark 4.1. Let f be as in Lemma 4.3. Suppose f strictly favors either class G = 0 or class G = 1, that is, BiasW1(f |G) = (1\u2212 \u03b4) \u00b7 Bias+W1(f |G) + \u03b4 \u00b7 Bias \u2212 W1\n(f |G) where \u03b4 \u2208 {0, 1}, and suppose the same is true for predictors, that is, \u03b2i = (1\u2212 \u03b4i) \u00b7 \u03b2+i + \u03b4i \u00b7 \u03b2 \u2212 i , where \u03b4i \u2208 {0, 1}. Then\n(\u22121)\u03b4 \u00b7 BiasW1(f |G) = n\u2211 i=1 (\u22121)\u03b4i \u00b7 \u03b2i. (4.9)\nIn Section 4.4 we propose an approach based on a cooperative game theory motivated by Lundberg and Lee (2017) that leads to additive bias explanations."
    },
    {
      "heading": "4.3 Impact of missing data on bias",
      "text": "In real world examples, trained models often rely on data-sets containing many predictors. However, for a given observation, determining the value of each predictor may not be feasible. When this happens, modelers have a choice: they may eliminate the missing values (i.e. by imputing the mean predictor value or creating a predictive model) or leave them as missing. In the latter case, the trained model learns how to make predictions on missing values and the data-set it employs may have them. For categorical predictors, this has the effect of adding a new category: \u2018na\u2019 (not available). For numeric predictors, this has the effect of making the predictor mixed \u2013 containing both a numeric and categorical domain.\nBecause model bias depends only on the distributions of the model score for observations in the unprotected and protected classes, the existence of missing values does not impede the calculation of model bias or the use of the bias explainer. However the model score is a function of predictors. For this reason, they do impact the distribution of the scores and consequently could impact the bias. This raises interpretability concerns.\nFor example, when trying to explain how \u2018income\u2019 causes bias in predicted default probability among \u2018females\u2019 and \u2018males\u2019, bias may emerge from two sources: differences in the true distributions of income between \u2018females\u2019 and \u2018males\u2019 and differences in how income data can be gathered between \u2018females\u2019 and \u2018males\u2019. For instance, when optional questions are involved in data collection, different classes of people may have different rates of response. Distinguishing between these sources of bias could allow modelers to make more targeted interventions focused on either mitigating bias with respect to the predictor\u2019s distribution itself or mitigating bias with respect to data collection. For this reason, the ability to\ndisaggregate bias into a contribution from the distribution of missing values and the contribution from the distribution of the predictor is desirable.\nTo understand how missing values contribute to the model bias we will consider a classification score that operates on the domain that allows for both numerical values as well as missing values \u2018na\u2019. In particular, we show that the positive and negative bias explanations disaggregate into two parts, one of which is fully characterized by the missing value event {Xi = na} event.\nDefinition 4.3. Let X = (X1, X2, . . . , Xn) \u2208 ( R \u222a {na} )n be predictors. Let f be a model, G the protected attribute, and Ei(X; f) the explainer of the predictor Xi. Define\nF nai,k(t) = P(Ei \u2264 t|G = k,Xi = na), pnai,k = P(Xi = na|G = k) F numi,k (t) = P(Ei \u2264 t|G = k,Xi \u2208 R), pnumi,k = P(Xi \u2208 R|G = k).\nLemma 4.4. Let X,G, f and Ei be as in Definition 4.3, and let G = 0 be the non-protected class. Let \u03b2+i (f |G), \u03b2 \u2212 i (f |G) be the positive and negative bias explanations of the predictor Xi as in Definition 4.1. Then \u03b2\u00b1i (f |G) = \u03b2 na\u00b1 i (f |G) + \u03b2 num\u00b1 i (f |G) (4.10)\nwhere\n\u03b2na\u00b1i (f |G) = \u00b1 ( pnai,1 \u2212 pnai,0 ) \u03bb { (\u2212\u221e, Ei(na)] \u2229 Ti\u00b1) } \u00b7 \u03c2f\n\u03b2num\u00b1i (f |G) = \u00b1 \u222b Ti\u00b1 ( F numi,1 (t)p num i,1 \u2212 F numi,0 (t)pnumi,0 ) \u00b7 \u03c2f dt.\n(4.11)\nHere \u03bb denotes the Lebesgue measure and the sets Ti\u00b1 = {t : \u00b1b\u0303ias C t (Ei|G) > 0}.\nProof. See Appendix B.2.\nTo illustrate how the proportions of missing values in the protected attribute classes affect the above decompositions, we consider the model (M6) and generate 10, 000 samples in such a way that some samples of the predictor X4 are missing. In this example we chose\nP(X4 = na|G = 0) = 0.05, P(X4 = na|G = 0) = 0.25.\nWe then train a regularized XGBoost model with 150 trees. For the predictor X4 the explainer E4 satisfies E4(na) = 0.5 and its CDFs are given in Figure 11a. Figure 11b illustrates the decomposition of the signed classifier bias for the explainer E4 into the classifier bias of the explainer that comes from missing values and the one that comes from numerical values according to Lemma B.3."
    },
    {
      "heading": "4.4 Additive Shapley-bias explanations",
      "text": "The bias explanations introduced in Section 4.2 do not satisfy additivity property. To achieve additivity one can consider an alternative approach for computing bias explanations. This approach is based on cooperative game theory and was explored in numerous works in the area of machine learning interpretability (Lundberg and Lee, 2017, Strumbelj and Kononenko, 2014, Lipovetsky and Conklin, 2001). In the spirit of Lundberg and Lee (2017), we define a cooperative game in which the players are predictors and the outcome is their bias contributions and then compute corresponding Shapley values.\nGroup explainers. Let X = (X1, X2, . . . , Xn) be predictors and f a model. A generic group explainer of f is denoted by ES(X; f), S \u2282 {1, 2, . . . , n}. (4.12) We assume that ES quantifies the attribution of each predictor XS with S \u2282 {1, 2, . . . , n} to the model value f(X) and satisfies\nE\u2205(X; f) = E[f(X)], E{1,2,...,n}(X; f) = f(X). (4.13)\nRelatively straightforward group explainers can be constructed using PDPs and SHAPs. In particular, for a nonempty S \u2282 {1, 2, . . . , n} one can set a group explainer as\nPDPS(X; f) or \u03c6S [v] = SHAPS(X; f, v) = \u2211 i\u2208S SHAPi(X; f, v) with v \u2208 {vCE , vPDP }. (4.14)\nDefinition 4.4. Let X,G, f , and \u03c2f be as in Definition 4.1. Let ES(X; f) be a group explainer of f .\n\u2022 Cooperative bias-game vbias associated with f and G is defined by\nvbias(S) = DW1(ES(X; f)|G = 0, ES(X; f)|G = 1), S \u2282 {1, 2, . . . , n}.\nIn the transport context, the value vbias(S) is the minimal cost of transporting the distribution of ES |G = 0 to that of ES |G = 1, and vice versa. \u2022 Positive bias-game vbias+ and negative bias-game vbias\u2212 are defined as follows. Suppose one transports optimally the distribution ES |G = 0 to that of ES |G = 1. Then \u25e6 vbias+(S) is the transport effort for moving points of ES |G = 0 in the non-favorable direction. \u25e6 vbias\u2212(S) is the transport effort for moving points of ES |G = 0 in the favorable direction.\nThe above values are specified in Lemma 3.1 for q = 1.\n\u2022 Net bias-game is defined by vbias,net = vbias+ \u2212 vbias+.\n\u2022 The Xi Shapley-bias explanations are defined by\n\u03d5biasi (f |G) = \u03d5i[vbias], \u03d5bias\u00b1i (f |G) = \u03d5i[v bias\u00b1], \u03d5bias,neti (f |G) = \u03d5i[v bias,net] (4.15)\nwhere {\u03d5i[\u00b7]}Ni=1 stand for Shapley values defined in (4.2).\nLemma 4.5. The Shapley bias-explanations defined in (4.15) satisfy\nBiasW1(f |G) = n\u2211 i=1 \u03d5biasi , Bias \u00b1 W1 (|G) = n\u2211 i=1 \u03d5bias\u00b1i , Bias net W1 (f |G) = n\u2211 i=1 \u03d5bias,neti\nand, thus, \u03d5[vbias] = \u03d5[vbias+] + \u03d5[vbias\u2212]\n\u03d5[vbias,net] = \u03d5[vbias\u2212]\u2212 \u03d5[vbias\u2212].\nProof. The result follows from Shapley (1953) and the properties of the W1 based model bias.\nExample. Applying the above methodology to f\u0302 and G of the model (M6) we compute the Shapley-bias explanations of predictors Xi, i \u2208 {1, 2, . . . , 5} displayed in Figure 12. The group explainer used in this example for the construction of the bias-games is ES = SHAPS defined in (4.12).\nRemark 4.2. Unlike the regular bias explanations which by construction are always non-negative, the Shapley-bias explanations are signed, that is, they can be both positive and negative.\nRemark 4.3. The Shapley-bias explanations are computationally expensive. What helps to alleviate the issue is grouping predictors by dependencies (or forming coalitions by dependencies) and then constructing a quotient bias-game; this approach is motivated by the ideas presented in Owen (1977), Lorenzo-Freire (2017), Aas et al. (2020), Kotsiopoulos et al. (2020) and discussed in Appendix E."
    },
    {
      "heading": "5 Conclusion",
      "text": "In this paper, we presented a novel bias interpretability framework for measuring and explaining bias in classification and regression models at the level of a distribution that utilizes the Wasserstein metric and the theory of optimal mass transport. We introduced and theoretically characterized bias predictor attributions to the model bias and provided the formulation for the bias attributions that take into account the impact of missing values. In addition, we constructed additive bias explanations utilizing cooperative game theory. To our knowledge, bias interpretability methods at the level of a distribution have not been addressed in the literature before.\nAt a higher level, the model bias is a non-trivial superposition of predictor bias attributions. The bias explanations we introduced determine the contribution of a given predictor to the model bias. However, any two or more predictors will interact in the context of the bias explanations. For example, if one predictor favor non-protected class and the other favors protected it might be possible that when two predictors utilized by the model the total effect on model bias could be zero. This phenomenon opens up numerous avenues for future research to investigate the interactions of predictors across subpopulation distributions in the context of bias explanations. This is where ML interpretability techniques can come into play and aid with the study of predictor interactions in the model bias.\nTo make bias explanations additive we utilized cooperative game theory which lead to additive Shapley-bias explanations. These explanations rely on the Shapley formula, which makes them computationally expensive. The intractability of such calculations can be mitigated by grouping predictors by dependencies and then computing the Shapley bias attributions for each group (via a quotient game) reduces the dimensionality. However, if the number of groups is large the issue of computational intensity remains. Thus, a possible research direction is to investigate methods that allow for approximation of the additive bias explanations and their fast computations.\nIn this paper, we formulated the methodology that computes the model bias and quantifies the contribution of predictors to that bias. However, the real application of the bias explanation methodology lies in bias mitigation, which will be useful in regulatory settings such as the financial industry, that utilizes the information about the main drivers of the model bias. This will be investigated in our upcoming paper. The framework is generic and in principle can be applied to a wide range of predictive ML systems. For instance, it might be insightful to understand the predictor attributions to probabilistic differences of populations studied in physics, biology, medicine, economics, etc."
    },
    {
      "heading": "Acknowledgment",
      "text": "The authors would like to thank Steve Dickerson (CAO, Decision Management at Discover Financial Services (DFS)), Raghu Kulkarni (VP, Data Science at DFS) and Melanie Wiwczaroski (Sr. Director, Enterprise Fair Banking at DFS) for formulation of the problem as well as helpful business and compliance insights. We also thank Patrick Haggerty (Director & Senior Counsel at DFS) and Kate Prochaska (Sr. Counsel & Director, Regulatory Policy at DFS) for their helpful comments relevant to regulatory issues that arise in the financial industry. We also would like to thank professors Markos Katsoulakis and Robin Young from the University of Massachusetts Amherst, and professor Matthias Steinru\u0308cken from the University of Chicago for their valuable comments and suggestions that aided us in writing this article."
    },
    {
      "heading": "Appendix",
      "text": ""
    },
    {
      "heading": "A Optimal transport and the Wasserstein distance",
      "text": ""
    },
    {
      "heading": "A.1 Kantorovich transport problem",
      "text": "To formulate the transport problem we need to introduce the following notation. Let B(Rk) denote the \u03c3-algebra of Borel sets. The space of all Borel probability measures on Rk is denoted by P(Rk). The space of probability measure with finite q-th moment is denoted by\nPq(Rk) = {\u00b5 \u2208P(Rk) : \u222b Rk |x|qd\u00b5(x) <\u221e}.\nDefinition A.1 (push-forward).\n(a) Let P be a probability measure on a measurable space (\u2126,F). Let X \u2208 Rp be a random vector defined on \u2126. The push-forward probability distribution of P by X is defined by\nPX(A) := P ( {\u03c9 \u2208 \u2126 : X(\u03c9) \u2208 A} ) .\n(b) Let \u00b5 \u2208 P(Rk) and T : Rk \u2192 Rm be Borel measurable, the pushforward of \u00b5 by T , which we denote by T#\u00b5 is the measure that satisfies\n(T#\u00b5)(B) = \u00b5 ( T\u22121(B) ) , B \u2282 B(Rk).\n(c) Given measure \u00b5 = \u00b5(dx1, dx2, ..., dxk) \u2208 P(Rk) we denote its marginals onto the direction xj by (\u03c0xj )#\u00b5 and the cumulative distribution function by\nF\u00b5(a1, a2, . . . , ak) = \u00b5((\u2212\u221e, a1]\u00d7 (\u2212\u221e, a2] . . . , (\u2212\u221e, ak])\nProposition A.1 (change of variable). Let T : Rk \u2192 Rm be Borel measurable map and \u00b5 \u2208 P(R). Let g \u2208 L1(Rm, T#\u00b5). Then \u222b\nRm g(y)T#\u00b5(dy) = \u222b Rk g(T (x))\u00b5(dx).\nProof. See Shiryaev (1980, p. 196).\nDefinition A.2 (Kantorovich problem on R). Let \u00b51, \u00b52 \u2208 P(R) and c(x1, x2) \u2265 0 be a cost function. Consider the problem\ninf \u03b3\u2208\u03a0(\u00b51,\u00b52) {\u222b R2 c(x1, x2)\u03b3(dx1, dx2) } =: Tc(\u00b51, \u00b52)\nwhere \u03a0(\u00b51, \u00b52) = {\u03b3 \u2208 P(R2) : (\u03c0xj )#\u03b3 = \u00b5j} denotes the set of transport plans between \u00b51 and \u00b52, and Tc(\u00b51, \u00b52) denotes the minimal cost of transporting \u00b51 into \u00b52.\nThe following theorem contains facts established in the texts such as Shorack and Wellner (1986), Villani (2003), Santambrogio (2015).\nTheorem A.1. Let \u00b51, \u00b52 \u2208P(R). Let c(x1, x2) = h(x\u2212 y) \u2265 0 with h convex and let\n\u03c0\u2217 := (F\u22121\u00b51 , F \u22121 \u00b52 )#\u03bb|[0,1] \u2208P(R 2) (A.1)\nwhere \u03bb|[0,1] denotes the Lebesgue measure restricted to [0, 1]. Suppose that Tc(\u00b51, \u00b52) <\u221e. Then\n(a) \u03c0\u2217 \u2208 \u03a0(\u00b51, \u00b52) and F\u03c0\u2217 = min(F (a), F (b)). (b) \u03c0\u2217 is an optimal transport plan that is\nTc(\u00b51, \u00b52) = \u222b R2 h(x1 \u2212 x2) d\u03c0\u2217(x1, x2).\n(c) \u03c0\u2217 is the only monotone transport plan, that is, it is the only plan that satisfies the property\n(x1, x2), (x \u2032 1, x \u2032 2) \u2208 supp(\u03c0\u2217) \u2282 R2 x1 < x\u20321 \u21d2 x2 \u2264 x\u20322.\n(d) If h is strictly convex then \u03c0\u2217 is the only optimal transport plan.\n(e) If \u00b51 is atomless, then \u03c0 \u2217 is determined by the monotone map T \u2217 = F [\u22121] \u00b52 \u25e6 F\u00b51 , called an optimal\ntransport map. Specifically, \u00b52 = T \u2217 #\u00b51 and hence \u03c0 \u2217 = (I, T \u2217)#\u00b51, where I is the identity map. Consequently,\u222b\nR2 h(x1 \u2212 x2) d\u03c0\u2217(x1, x2) = \u222b R h(x1 \u2212 T \u2217(x1))d\u00b51(x1) = E[X1 \u2212 T \u2217(X1)], \u00b52 = PX1 ."
    },
    {
      "heading": "A.2 Wasserstein distance",
      "text": "Definition A.3. Let q \u2265 1. The Wasserstein distance Wq on Pq(R) is defined by\nWq(\u00b51, \u00b52) := T 1/q |x1\u2212x2|q (\u00b51, \u00b52), \u00b51, \u00b52 \u2208Pq(R)\nwhere\nT|x1\u2212x2|q (\u00b51, \u00b52) = inf \u03b3\u2208P(R2) {\u222b R2 |x1 \u2212 x2|qd\u03b3, \u03b3 \u2208 \u03a0(\u00b51, \u00b52) } .\nThe distance is always finite as \u222b R |x|\nqd\u00b5i <\u221e. Proposition A.2. Suppose that \u00b51, \u00b52 \u2208Pq(R), q \u2208 [1,\u221e). Let \u03c0\u2217 be defined by (A.1). Then\nDqWq (\u00b51, \u00b52) = T|x1\u2212x2|q (\u00b51, \u00b52) = \u222b R2 |x1 \u2212 x2|qd\u03c0\u2217(x1, x2) = \u222b 1 0 |F [\u22121]\u00b51 (p)\u2212 F [\u22121] \u00b52 (p)| qdp <\u221e."
    },
    {
      "heading": "Furthermore, for any Borel set B \u2282 R2\u222b",
      "text": "A |x1 \u2212 x2|qd\u03c0\u2217(x1, x2) = \u222b {p\u2208(0,1): (F [\u22121]\u00b51 (p),F [\u22121] \u00b52 )(p))\u2208B} |F [\u22121]\u00b51 (p)\u2212 F [\u22121] \u00b52 (p)| qdp.\nProof. The result follows Proposition A.1 and properties (a) and (b) of Theorem A.1.\nA.3 Geometric continuity and invariance theorems\nLemma A.1. Let \u00b5 \u2208 (R). Let T : R\u2192 R be continuous, strictly increasing map. Then\nFT#\u00b5(z) = F\u00b5 \u25e6 T \u22121(z), z \u2208 R F [\u22121] T#\u00b5 (p) = T \u25e6 F [\u22121]\u00b5 (p), p \u2208 (0, 1).\nProof. First, observe that\nFT#\u00b5(t) = \u00b5({x : T (x) \u2264 t}) = \u00b5({x : x \u2264 T \u22121(t)}) = F\u00b5 \u25e6 T\u22121(t).\nNext, observe that from the definition of the generalized inverse it follows that\nF [\u22121]\u00b5 (p) \u2264 t \u21d4 p \u2264 F\u00b5(t).\nThen by above for p \u2208 (0, 1), we have\nF [\u22121] T#\u00b5 (p) = inf z\n{ p \u2264 FT#\u00b5(z) } = inf\nz\n{ p \u2264 F\u00b5 \u25e6 T\u22121(z) } = inf\nz\n{ T \u25e6 F [\u22121]\u00b5 (p) \u2264 z } .\nThus, F [\u22121] T#\u00b5 (p0) \u2265 T \u25e6 F [\u22121]\u00b5 (p0). Next, observe that\nF [\u22121]\u00b5 (p) = inf x\n{ p \u2264 F\u00b5(x) } = inf\nx\n{ F [\u22121]\u00b5 (p) \u2264 x } and hence by above\nT \u25e6 F [\u22121]\u00b5 (p) = T (\ninf x\n{ F [\u22121]\u00b5 (p) \u2264 x }) = inf\nx\n{ T \u25e6 F [\u22121]\u00b5 (p) \u2264 x } = F [\u22121] T#\u00b5 (p).\nCombining the above results proves the lemma."
    },
    {
      "heading": "A.3.1 Proof of Theorem 3.1",
      "text": "Proof. Let q \u2208 [1,\u221e). Let T be a family of maps from R to R as in Definition 3.2. Take \u00b5 \u2208 Pq(R). Since T\u03b5 \u2212 I has compact support, there is a bounded B \u2282 R such that T\u03b5(x) = x for all x \u2208 Bc. Thus,\u222b\nR |x|qdT\u03b5#\u00b5(x) = \u222b R |T\u03b5(x)|qd\u00b5(x) = \u222b B |T\u03b5(x)|qd\u00b5(x) + \u222b Bc |x|qd\u00b5(x) <\u221e\nand hence T\u03b5#\u00b5 \u2208Pq(R). Next, consider a probability measure \u03c0 = (I, T\u03b5)#\u00b5. By construction, its marginals are \u00b5 and T\u03b5#\u00b5 and hence \u03c0 is a transport plan. Then, Lemma A.1 and the definition of the distance DWq imply\nDqWq (\u00b5\u03b5, T\u03b5#\u00b5) \u2264 \u222b R2 |x1 \u2212 x2|d\u03c0(x1, x2) = \u222b R |x1 \u2212 T\u03b5(x1)|d\u00b5(x1).\nSending \u03b5 \u2192 0 in the above inequality, and using the assumption that I \u2212 T\u03b5 \u2192 0 uniformly in R, we conclude that DqWq (\u00b5, T\u03b5#\u00b5)\u2192 0. This proves the statement (a).\nNext, we let \u00b5 = \u03b4x0 . Let T\u03b5 = I + \u03b5\u03d5, where \u03d5 \u2208 C10 (R) is a nonnegative function that satisfies \u03d5(x0) = 1 and |\u03d5\u2032| < 1. Then,\nlim \u03b5\u21920 DKS(\u03b4x0 , T\u03b5#\u03b4x0) = lim \u03b5\u21920 DKS(\u03b4x0 , \u03b4x0+\u03b5) = 1.\nThis proves the statement (b)."
    },
    {
      "heading": "A.3.2 Proof of Theorem 3.2",
      "text": "Proof. Let T : R \u2192 R be continuous and strictly increasing. Let q \u2208 [1,\u221e). Suppose that DWq on Pq(R) is invariant under T . Then, using Lemma A.1, we obtain\nDqWq (T#\u00b51, T#\u00b52) = \u222b 1 0 |T \u25e6 F [\u22121]\u00b51 (p)\u2212 T \u25e6 F [\u22121] \u00b52 | qdp\n= \u222b 1 0 |F [\u22121]\u00b51 (p)\u2212 F [\u22121] \u00b52 | qdp = DqWq (\u00b51, \u00b52)\n(A.2)\nLet \u00b51 = \u03b4a and \u00b52 = \u03b4b for a < b. Then, the above equality reads\n(T (b)\u2212 T (a))q = (b\u2212 a)q, \u2200a, b.\nHence, T is the identity map. This proves the statement (a). Since T is strictly increasing, T (R) is connected. Hence T\u22121 is well defined on T (R). Then, using Lemma A.1, for any s \u2208 T (R) we have\n|FT#\u00b51(s)\u2212 FT#\u00b52(s)| = |F\u00b51(T \u22121(s))\u2212 F\u00b52(T \u22121(s))| \u2264 \u2016F\u00b51 \u2212 F\u00b52\u2016L\u221e(R).\nFor any s \u2208 (T (R))c the above difference is zero because FT#\u00b51 and FT#\u00b52 are either both zero at s or both equal to one. Similarly, using the fact that T is increasing, we have for any t \u2208 R\n|F\u00b51(t)\u2212 F\u00b52(t)| = |FT#\u00b51(T (t))\u2212 FT#\u00b52(T (t))| \u2264 \u2016FT#\u00b51 \u2212 FT#\u00b52\u2016L\u221e(R).\nCombining the above inequalities gives the statement (b)."
    },
    {
      "heading": "B Bias properties theorems",
      "text": ""
    },
    {
      "heading": "B.1 Model bias properties",
      "text": "Lemma B.1. Let X be a random variable with E|X| <\u221e. Let X+ = max(0, X), X\u2212 = max(0,\u2212X).Then\nE[X] = E[X+]\u2212 E[X\u2212], E[X+] = \u222b \u221e\n0 (1\u2212 F (t)) dt, E[X\u2212] = \u222b 0 \u2212\u221e F (t)dt (B.1)\nwhere F is the CDF of X.\nProof. Note that |X(\u03c9)| \u2265 X+(\u03c9), X\u2212(\u03c9) \u2265 0 and hence E[X+] and E[X\u2212] are finite. Recalling that X = X+ \u2212X\u2212, we obtain (B.1)1.\nNext, by definition of the expectation, we have \u221e > E[X+] = \u222b\n\u2126\nX+(\u03c9)P(d\u03c9) = \u222b\n\u2126 (\u222b R 1{0\u2264x\u2264X+(\u03c9)}dx ) P(d\u03c9)\n= \u222b R 1{0\u2264x} (\u222b \u2126 1{x\u2264X+(\u03c9)}P(d\u03c9) ) dx = \u222b \u221e 0 (1\u2212 F (x)) dx\nwhere we applied the Tonelli\u2019s theorem to exchange the order of integration. This proves (B.1)2. The proof for (B.1)3 is similar.\nLemma B.2. Let \u03bb denote the Lebesgue measure on R. Let f, g be \u03bb-measurable functions such that g \u2264 f .\n(i) Suppose that f \u2212 g \u2208 L1(R). Then\n\u03bb2 ({ (x, y) : g(x) < y < f(x) }) = \u222b R (f \u2212 g) d\u03bb = \u03bb2 ({ (x, y) : g(x) \u2264 y \u2264 f(x) }) <\u221e. (B.2)\n(ii) Suppose that \u03bb2 ({ (x, y) : g(x) < y < f(x) }) <\u221e. Then f \u2212 g \u2208 L1(R) and (B.2) holds.\nProof. Suppose that f \u2212 g \u2208 L1(R). Since f and g are measurable, the set {(x, y) : g(x) < y < f(x) }\nis measurable with respect to the product measure \u03bb2 = \u03bb\u2297 \u03bb. Then by the Tonelli\u2019s theorem we obtain\n\u221e > \u222b R (f(x)\u2212 g(x)) d\u03bb(x) = \u222b R (\u222b R 1{y:g(x)<y<f(x)} d\u03bb(y) ) d\u03bb(x)\n= \u222b R2 1{(x,y):g(x)<y<f(x)} d(\u03bb\u2297 \u03bb) = \u03bb2 ( {(x, y) : g(x) < y < f(x)} ) ,\nwhich proves the first equality in (B.2). The second equality (B.2) is proved similarly. This gives (i). Suppose that \u03bb2 ( {(x, y) : g(x) < y < f(x)} ) < \u221e. Following the calculations above in the reverse order we conclude that f \u2212 g \u2208 L1(R) and hence (B.2) holds. This proves (ii).\nTheorem B.1. Let X0, X1 be random variables with E|Xi| < \u221e, i \u2208 {0, 1}. Let Fi denote the CDF of Xi, F [\u22121] i denote the generalized inverse of Fi, and let\nT0 = {t \u2208 R : F1(t) < F0(t)}, T1 = {t \u2208 R : F0(t) < F1(t)} P0 = {p \u2208 (0, 1) : F [\u22121]1 (p) < F [\u22121] 0 (p)}, P1 = {p \u2208 (0, 1) : F [\u22121] 0 (p) < F [\u22121] 1 (p)}.\n(B.3)"
    },
    {
      "heading": "Then \u222b",
      "text": "T0 F0(t)\u2212 F1(t) dt = \u222b P1 F [\u22121] 1 (p)\u2212 F [\u22121] 0 (p) dp <\u221e\u222b\nT1 F1(t)\u2212 F0(t) dt = \u222b P0 F [\u22121] 0 (p)\u2212 F [\u22121] 1 (p) dp <\u221e.\n(B.4)\nProof. Define the set A0 = {(p, t) \u2208 (0, 1)\u00d7 R : F1(t) < p \u2264 F0(t)}.\nNote (p, t) \u2208 A0 implies t \u2208 T0. Hence, applying Lemma B.2, we obtain\n\u03bb2(A0) = \u222b T0 F0(t)\u2212 F1(t) dp <\u221e\nwhere the finiteness of the right hand side follows from the fact that E|Xi| <\u221e and Lemma B.1. Observe next that the definition of the generalized inverse implies that\nF [\u22121] i (p) \u2264 t \u21d4 p \u2264 Fi(t), F [\u22121] i (p) > t \u21d4 p > Fi(t) (B.5)\nand hence A0 = {(p, t) \u2208 (0, 1)\u00d7 R : F [\u22121]0 (p) \u2264 t < F [\u22121] 1 (p)}. Note by above (p, t) \u2208 A0 implies that p \u2208 P1. Hence, Lemma B.2 imply\n\u03bb2(A0) = \u222b P1 F [\u22121] 1 (p)\u2212 F [\u22121] 0 (p) dp\nand this proves (B.4)1. The proof of (B.4)2 is similar."
    },
    {
      "heading": "B.1.1 Proof of Theorem 3.3",
      "text": "Proof. Since E|f(X)| <\u221e, we have E[|f(X)|G = k|] <\u221e for k \u2208 {0, 1}. Then by Theorem B.4 we have\nDW1 ( f(X)|G = 0, f(X)|G = 1 ) = \u222b 1 0 |F [\u22121]f |G=0(p)\u2212 F [\u22121] f |G=1(p)| dp\n= \u2211\ni={0,1}\n\u222b Pi |F [\u22121]f |G=0(p)\u2212 F [\u22121] f |G=1(p)| dp\n= \u2211\ni={0,1}\n\u222b Ti |Ff |G=0(t)\u2212 Ff |G=1(t)| dt\n= \u222b R |Ff |G=0(t)\u2212 Ff |G=1(t)| dt < \u221e.\nwhere Pi, Ti are defined in (B.3). Hence the result follows from Definition 2.4, Definition 2.5 and the above equality."
    },
    {
      "heading": "B.1.2 Proof of Theorem 3.4",
      "text": "Proof. Suppose first that favorable direction is \u2191. Since E|f(X)| <\u221e, we have E[|f(X)||G = k] <\u221e for k \u2208 {0, 1}. Then by Theorem B.4\nBias+ ( f |G ) = \u222b P+ F [\u22121] f |G=0(p)\u2212 F [\u22121] f(X)|G=1(p) dt = \u222b T + Ff |G=1(t)\u2212 Ff |G=0(t) dt <\u221e.\nHence (3.14)1 follows from Definition 2.4, Definition 2.5, and the above equality. The proof for (3.14)2 is similar.\nNext, by (3.14) and Lemma B.1 we have\nBiasnet(f |G) = Bias+(f |G)\u2212 Bias\u2212(f |G)\n= \u222b T + ( Ff |G=1(t)\u2212 Ff |G=0(t) ) dt\u2212 \u222b T\u2212 ( Ff |G=0(t)\u2212 Ff |G=1(t) ) dt\n= \u222b 0 \u2212\u221e ( Ff |G=1(t)\u2212 Ff |G=0(t) ) dt+ \u222b \u221e 0 ( (1\u2212 Ff |G=0(t))\u2212 (1\u2212 Ff |G=1(t)) ) dt = E[f(X)|G = 0]\u2212 E[f(X)|G = 1].\nThis proves (3.16). If the favorable direction is \u2193, the proof of (3.14) and (3.16) is similar."
    },
    {
      "heading": "B.1.3 Proof of Lemma 4.3",
      "text": "Proof. Suppose that Ei(X; f) = PDPi(X; f). Then, in view of the additivity of f , we have\nPDPi(X; f) = fi(Xi)\u2212 E[fi(Xi)] + E[f(X)]\nand hence by Lemma 4.1 we have \u03b2neti (f |G;PDPi) = ( E[fi(Xi)|G = 0]\u2212 E[fi(Xi)|G = 1] ) \u00b7 \u03c2f .\nSumming up the net bias explanations gives\u2211 i \u03b2neti (f |G;PDPi) = \u2211 i ( E[fi(Xi)|G = 0]\u2212 E[fi(Xi)|G = 1] ) \u00b7 \u03c2f\n= ( E[f(X)|G = 0]\u2212 E[f(X)|G = 1] ) \u00b7 \u03c2f = BiasnetW1 (f |G).\n(B.6)\nSuppose that Ei(X; f) = SHAPi(X; f, v PDP ). Since {Xi}ni=1 are independent and f is additive,\nSHAPi(X; f, v PDP ) = SHAPi(X; f, v CE) = fi(Xi)\u2212 E[fi(Xi)] = PDPi(X; f) + E[f(X)].\nSince a shift in the distribution does not affect the bias, the bias explanation based on SHAPi coincide with that of PDPi. This together with (B.6) proves the lemma.\nB.2 Missing values bias explanations.\nLemma B.3. Let X, f and Ei be as in Definition 4.3. Then for each t \u2208 R\nb\u0303ias C t (Ei|G) = b\u0303ias C,na t (Ei|G) + b\u0303ias C,num t (Ei|G) (B.7)\nwhere\nb\u0303ias C,na t (Ei|G) = 1{Ei(na)\u2264t} ( pnai,1 \u2212 pnai,0 ) \u00b7 \u03c2f\nb\u0303ias C,num t (Ei|G) = ( F numi,1 (t)p num i,1 \u2212 F numi,0 (t)pnumi,0 ) \u00b7 \u03c2f .\n(B.8)\nProof. Suppose that \u2191 is the favorable direction. Then Definition 2.4 implies\nb\u0303ias C\nt (Ei|G) = Fi,1(t)\u2212 Fi,0(t) = P(Ei \u2264 t|G = 1, Xi = na)P(Xi = na|G = 1) \u2212 P(Ei \u2264 t|G = 0, Xi = na)P(Xi = na|G = 0) + P(Ei \u2264 t|G = 1, Xi \u2208 R)P(Xi \u2208 R|G = 1) \u2212 P(Ei \u2264 t|G = 0, Xi \u2208 R)P(Xi \u2208 R|G = 0).\n(B.9)\nObserve next that the CDF F nai,k can be expressed as\nF nai,k(t) = P(Ei \u2264 t|Xi = na, G = k) = P(Ei(na) \u2264 t) = 1{Ei(na)\u2264t}.\nCombining the above results proves the lemma."
    },
    {
      "heading": "Proof of Lemma 4.4.",
      "text": "Proof. By Theorem 3.4, Lemma B.3, and Definition 4.1 we have\n\u03b2\u00b1i = B \u00b1 W1 (Ei|G) = \u00b1 \u222b Ti\u00b1 ( b\u0303ias C,na t (Ei|G) + b\u0303ias C,num t (Ei|G) ) dt.\nThen the above relation and (B.8) prove the lemma."
    },
    {
      "heading": "C Extension to protected attributes with multiple classes.",
      "text": "Let X = (X1, . . . , Xn) be predictors, Y a response variable, and f(X) a model. Suppose that the protected attribute G \u2208 {0, 1, . . . ,K \u2212 1} and that G = 0 is the non-protected class. Let D is a metric on the space of probability distributions on R. In that case, Definition 3.1 of D-based model bias can be extended naturally to the case of multiple protected attributes as follows:\nBiasD(f |G \u2208 {0, 1, . . . ,K \u2212 1) := \u2211 k wkBiasD(f |G \u2208 {0, k}), wi \u2265 0. (C.1)\nWhen D = W1 the model bias reads: BiasW1(f |G \u2208 {0, 1, . . . ,K \u2212 1) = \u2211 k wkBiasW1(f |G \u2208 {0, k})\n= \u2211 k wk \u222b 1 0 |F [\u22121]0 (p)\u2212 F [\u22121] k (p)| dp\n(C.2)\nwhile the positive and negative model biases are defined by Bias\u00b1W1(f |G \u2208 {0, 1, . . . ,K \u2212 1) := \u2211 k wkBias \u00b1 W1 (f |G \u2208 {0, k})\n= wk \u222b P0k\u00b1 \u00b1 ( F [\u22121] 0 (p)\u2212 F [\u22121] k (p) ) \u00b7 \u03c2f dp\n(C.3)\nwhere P0k\u00b1 = {p \u2208 [0, 1] : \u00b1 ( F [\u22121] 0 (p)\u2212 F [\u22121] k (p) ) \u00b7 \u03c2f > 0}. (C.4)"
    },
    {
      "heading": "D Bias consistency with generic group-based parity",
      "text": "Let X,Y,G, f be as in Appendix C. Let \u2126 be a sample space, and let A = {A1, A2, . . . , AM} be a collection of disjoint subsets of \u2126. Define\nAkm = {G = k} \u2229Am, k \u2208 {0, 1, . . . ,K \u2212 1},m \u2208 {1, . . . ,M}.\nLet Yt = 1{f(X)>t}. A generic A group-based parity condition then reads\nP(Yt = 1{\u03c2f=1}|Akm) = P(Yt = 1{\u03c2f=1}|A0m), k \u2208 {1, . . . ,K \u2212 1}, m \u2208 {1, . . . ,M}. (D.1)\nDefine the following W1-based model bias BiasW1,A(f |G) := \u2211 k,m wkmDW1(f |A0m, f |Akm), wkm \u2265 0. (D.2)\nThe above bias metric is consistent with the generic parity criterion (D.1) in the following sense:\nBiasW1,A(f |G) = \u2211 k,m wkm \u222b 1 0 |F [\u22121]f |A0m \u2212 F [\u22121] f |Akm | dt\n= \u2211 k,m wkm \u222b R |P(Yt = 1{\u03c2f=1}|Akm)\u2212 P(Yt = 1{\u03c2f=1}|A0m)| dt.\n(D.3)\nwhere the second equality follows from Theorem B.1. The corresponding positive and negative model biases in this case are defined by\nBias\u00b1W1,A(f |G) := \u2211 k,m wkmBias \u00b1 W1 (f |{A0m, Akm})\n= \u2211 k,m wkm \u222b P0m,km\u00b1 \u00b1 ( F [\u22121] f |A0m(p)\u2212 F [\u22121] f |Akm (p) ) \u00b7 \u03c2f dp\n(D.4)\nP0m,km\u00b1 = {p \u2208 [0, 1] : \u00b1 ( F\n[\u22121] f |A0m(p)\u2212 F [\u22121] Akm\n(p) ) \u00b7 \u03c2f > 0}. (D.5)\nExample. Suppose that the favorable direction is \u2191. Suppose that G \u2208 {0, 1} and that the response variable Y \u2208 {0, 1}. Let A = {{Y = 0}, {Y = 1}}. In that case, the group-based parity condition (D.1) reads P(Yt = 1|G = 0, Y = m) = P(Yt = 1|G = 1, Y = m), m = 0, 1, which is the equalized odds criterion; Hardt et al. (2015)."
    },
    {
      "heading": "E Coalition-based additive bias explanations",
      "text": "The interpretability tools such as PDPs or SHAPs are at the heart of the bias explainer developed in the main text. It is known that dependencies in predictors as well as their interactions may cause explainers to produce inconsistent attributions; see, for example, Goldstein et al. (2015), Hastie et al. (2016), Sundararajan and Najmi (2019), Janzing et al. (2019), Chen et al. (2020). As a consequence, the use of inconsistent explainers may cause the bias explainer to quantify the bias incorrectly. It turns out that constructing appropriately designed group predictors may help to resolve the issues of onconsistent interpretations and as a consequence help with the proper quantification of the bias explanations; see Aas et al. (2020), Kotsiopoulos et al. (2020).\nThe works Aas et al. (2020) and Kotsiopoulos et al. (2020) show that forming unions of predictors by dependencies (or forming coalitions by dependencies) and then constructing corresponding coalitionbased explainers alleviates the issues with inconcistent attributions caused by intricate dependencies.\nGrouping techniques used in Kotsiopoulos et al. (2020) are based on hierarchical variable clustering that utilizes the estimates of Maximal Information Coefficient\u2217 (MIC\u2217), a regularized version of the mutual information. MIC\u2217 is a state of the art methodology developed in Reshef et al. (2016) that estimates the dependencies in predictors and which outperforms techniques based on PCA or distance correlation developed in Szekely et al. (2007).\nOnce dependency-based coalitions are formed, the main challenge is to properly define a coalitionbased explainer. The article Kotsiopoulos et al. (2020) provides the details of various constructions and discusses subtle issues that arise in the process; the work shows that, mutual information-based grouping leads to consistent model explanations that are both true to the data and true to the model in the sense discussed in the works of Sundararajan and Najmi (2019), Janzing et al. (2019), and Chen et al. (2020).\nMotivated by the ideas in Kotsiopoulos et al. (2020) we design additive bias explanations of coalitionbased predictors, where coalitions are formed by dependencies, as follows.\nLet X = (X1, X2, . . . , Xn) be predictors and f a model. Let Sj = {ij1, i j 2, . . . , i j kj } be disjoint sets\nthat partition the set of predictor\u2019s indexes,\n{1, 2, . . . , n} = r\u22c3 j=1 Sj , P = {S1, S2, . . . , Sr},\nso that XS1 , XS2 , . . . , XSr form weakly independent unions (coalitions) such that within each coalition the predictors share significant amount of mutual information; groupings satisfying the above criteria can be constructed using the approaches in Aas et al. (2020) and Kotsiopoulos et al. (2020). Each union XSj may be viewed as a single predictor, called a coalition-based predictor (or a coalition).\nTo quantify the bias explanation of each coalition XSj using a cooperative game theory approach, it is necessary to view each coalition as a single player. This leads a quotient bias-game\nv\u0304bias,P(U) = vbias ( \u22c3 j\u2208U Sj ) , (E.1)\nobtained by restricting the bias-game vbias introduced in Definition 4.4 to the sets S1, S2, . . . , Sr. The additive Shapley-bias explanations of coalitions XS1 , XS2 , . . . , XSn are then defined as Shapley values of the quotient bias-game:\n\u03d5\u0304bias,Pj = \u03d5j [v\u0304 bias,P ], j \u2208 {1, 2, . . . , r}. (E.2)\nWe note that the bias explanations \u03d5\u0304j [v\u0304 bias,P ] and \u03d5Sj [v bias] defined in (4.14) differ; for details, see Owen (1977), Lorenzo-Freire (2017). Similar construction is used to compute positive and negative bias explanations \u03d5\u0304bias+,Pj and \u03d5\u0304 bias\u2212,P j , respectively, of coalitions XS1 , XS2 , . . . , XSk ."
    }
  ],
  "title": "Wasserstein-based fairness interpretability framework for machine learning models",
  "year": 2020
}
