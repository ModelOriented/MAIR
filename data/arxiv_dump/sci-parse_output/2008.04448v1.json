{
  "abstractText": "With the advent of Industry 4.0, Data Science and Explainable Artificial Intelligence (XAI) has received considerable intrest in recent literature. However, the entry threshold into XAI, in terms of computer coding and the requisite mathematical apparatus, is really high. For fault diagnosis of steel plates, this work reports on a methodology of incorporating XAI based insights into the Data Science process of development of high precision classifier. Using Synthetic Minority Oversampling Technique (SMOTE) and notion of medoids, insights from XAI tools viz. Ceteris Peribus profiles, Partial Dependence and Breakdown profiles have been harvested. Additionally, insights in the form of IF-THEN rules have also been extracted from an optimized Random Forest and Association Rule Mining. Incorporating all the insights into a single ensamble classifier, a 10 fold cross validated performance of 94% has been achieved. In sum total, this work makes three main contributions viz.: methodogly based upon utilization of medoids and SMOTE, of gleaning insights and incorporating into model development process. Secondly the insights themselves are contribution, as they benefit the human experts of steel manufacturing industry, and thirdly a high precision fault diagnosis classifier has been developed.",
  "authors": [
    {
      "affiliations": [],
      "name": "Athar Kharal"
    }
  ],
  "id": "SP:6ffb6d7e9d7288b5b7c61fb84a71b1d5822f5698",
  "references": [
    {
      "authors": [
        "Baniecki",
        "Hubert",
        "Przemyslaw Biecek"
      ],
      "title": "The Grammar of Interactive Explanatory Model Analysis.",
      "venue": "[Cs, Stat],",
      "year": 2020
    },
    {
      "authors": [
        "Barredo Arrieta",
        "Alejandro",
        "Natalia Daz-Rodrguez",
        "Javier Del Ser",
        "Adrien Bennetot",
        "Siham Tabik",
        "Alberto Barbado",
        "Salvador Garcia"
      ],
      "title": "Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges Toward Responsible AI.",
      "venue": "Information Fusion",
      "year": 2020
    },
    {
      "authors": [
        "N.V. Chawla",
        "K.W. Bowyer",
        "L.O. Hall",
        "W.P. Kegelmeyer"
      ],
      "title": "SMOTE: Synthetic Minority overSampling Technique.",
      "venue": "Journal of Artificial Intelligence Research",
      "year": 2002
    },
    {
      "authors": [
        "Christina B. Azodi",
        "Jiliang Tang",
        "Shin-Han Shiu"
      ],
      "title": "Opening the Black Box: Interpretable Machine Learning for Geneticists.",
      "year": 2020
    },
    {
      "authors": [
        "Consoli",
        "Sergio",
        "Valentina Presutti",
        "Diego Reforgiato Recupero",
        "Andrea G. Nuzzolese",
        "Silvio Peroni",
        "Misael Mongiovi",
        "Aldo Gangemi"
      ],
      "title": "Producing Linked Data for Smart Cities: The Case of Catania.",
      "venue": "Big Data Research",
      "year": 2017
    },
    {
      "authors": [
        "Deng",
        "Houtao"
      ],
      "title": "Interpreting Tree Ensembles with",
      "year": 2019
    },
    {
      "authors": [
        "E.L. Russell"
      ],
      "title": "L.H.Chiang, and R.D.Braatz",
      "venue": "\u201cFault",
      "year": 2000
    },
    {
      "authors": [
        "81\u201393. Friedman",
        "Jerome H"
      ],
      "title": "Greedy Function Approxima",
      "year": 2001
    },
    {
      "authors": [
        "01212. Kang",
        "Seokho"
      ],
      "title": "Model Validation Failure in Class",
      "year": 2020
    },
    {
      "authors": [
        "Kiani"
      ],
      "title": "Quality Control and Classification",
      "year": 2018
    },
    {
      "authors": [
        "Jochen Garcke"
      ],
      "title": "Explainable Machine Learning",
      "year": 2020
    }
  ],
  "sections": [
    {
      "text": "Index Terms\u2014Automated Manufacturing; Predictive Maintenance; Explainable Artificial Intelligence; Insight Harvesting; Fault Diagnosis\nI. INTRODUCTION\nData Science carries two senses of meaning namely \u2018Science of Data\u2019 and \u2018Science by Data\u2019. In the sense of Science of Data, new methods, techniques and insights for handling, manipulating and predictive modeling of data are developed and extended. In the second sense i.e. \u2018Science by Data\u2019, data of a specific scientific domain is examined and knowledge is extracted from it for the betterment and enhancement of the original scientific domain. It is this second sense namely doing \u2018Science by Data\u2019, that this paper owes the spirit and motivation.\nInsight, the structural understanding of phenomena, is the essence of human intelligence. Usualy, insight is distilled from the fusion of information from various sources e.g. past experiences, analysis techniques and methods (Roscher et al. 2020). There is no reason to not to expect insights from Artificial Intelligence (AI) as well, which more appropriately may (in our view, must) be called Machine Intelligence (MI). AI has shown great strides in recent times. Passing\nthrough the stages of Symbolic AI, Computational AI, it has now reached to another milestone namely, Explainable AI (XAI) or more appropriately Explainable Machine Learning (XML). Explainability has remained a much sought after feature of AI systems right from the era of Expert Systems hype of AI, mainly because even the best human experts needed assistance/explanation from the machine to reach the right decisions. Non-availability of explainability has also remained one of the major barriers in a wide-scale practical adoption of AI. Improved understanding of the inner working of a phenomenon and its machine representation i.e. ML model leads to the correction of its weaknesses and flaws. Understanding of one subsytem, obtained from XAI, may also be transferred to another system thus \u2018democratizing\u2019 the invaluable expertise in a cummulative manner. Black box models, though considerably successful in prediction tasks, but at the same time they present a totally opaque face when it comes to explanation and insight. In recent time the confluence of Data Science (as Science of Data) with AI techniques has achieved great deal of success to peep into even a high precision predictive models of black-box genre. Clearly, it is an area located at the crossroads of data-fusion and informationfusion.\nThe understanding, structural relations and the relational insight achieved by data is just as good as the data itself. It cannot guarantee the causal scientific relationship of the external world. Nevertheless, an explainable AI model offers a viable hypothesis to begin with (cf. Section 3 of (Roscher et al. 2020)). The journey of scientific enquiry, at least may be begun in most of the cases in the right direction. Furthermore, explainability also ensures that only meaningful variables are allowed to take part in final modelling tasks thus apporaching the ideal of a truly parsimonious model as well. Christina et. al. (Christina B. Azodi, Jiliang Tang, and Shin-Han Shiu 2020) have highlighted the same point more aptly: \u201cML interpretation strategies mostly do not identify causal relationships between input features and labels. Instead, interpretations should be used to generate hypotheses of cause-effect relations that can be tested experimentally.\u201d\nIndustrial plant monitoring focuses upon optimization of resources and thus attempts to achieve minimal error costs,\nar X\niv :2\n00 8.\n04 44\n8v 1\n[ cs\n.A I]\n1 0\nA ug\n2 02\nimproved quality of production and safety of workplace. A timely, precise and indicative of the root-cause detections of defects or faults is fundamental to such monitoring. Compliance management also neciassitates the explainablity of the machine learning models. That is why due to heavy cost of production, in general all industry, and in particular steel production industry has shown great intrest and effort to reduce the hugely expensive faults of production. Fault diagnostics in conjuction with fault isolation and root-cause analysis also contributes to precautionary maintenance or the prognostics of the production system (Soylemezoglu, Jagannathan, and Saygin 2011). Due to cost of faults and the ushering of Industry 4.0, Internet of Things (IoT) and Cyber Physical System (CPS), interest in data analytics based solutions has increased to a remarkable degree.\nFault diagnosis within an industrial production setup is further important on two more counts. Obviously, one for maintaing the quality of production. This requires fast and precise, either human experts (Gabriel Schwartz and Ko Nishino 2020) or preferably intelligent digital solutions like machine and statistical learning based predictive models. As the human expertise becomes more and more scarce and costly, the automated digital solutions become even more relevant and indemand. Second and more important utility of fault diagnosis is its use to identify the exact causes, variables and factors of production which are responsible for a particular fault. This usage of fault diagnosis, in fact, demands developing the insight into the production system through its various variables. It is exactly here that the present work is located. It aims to benefit the quality production of steel plates at industrial scale by enhancing the human insight of the fault producing factors and causes.\nSteel plates fault diangosis has been studied by various researchers. In (Nkonyana et al. 2019), authors employed various models viz. ANN (Kharal and Saleem 2012), SVM and Random Forests. Random Forest achieved the highest accuracy of 77.80% while SVM followed by 73.60%. So far the highest accuracy of prediction has been reported by Tian in (Tian, Fu, and Wu 2015). This SVM implementation reported a performance of 80.7443%. However, authors also noted that other sample balancing techniques may bring better result than the one used. In the settings of smart cities, the fault reporting and diagnostics using linked data has also been studied in (Consoli et al. 2017). Authors of (E.L. Russell, L.H.Chiang, and R.D.Braatz 2000) have applied the Canonical Variate Statistic, residual space PCA, DPCA, and CVA to the Tennessee Eastman process simulator of a wide variety of faults occurring in a chemical plant. Kazemi et.al. (Kazemi, Hajian, and Kiani 2018) calculate the positions (ranking) of each classifier compared to the five datasets and calculate the average ranking, and showed the quality of the proposed metaclassifier to be the best.\nThis paper is organized as follows: It comprises two main parts. Section II describes various details of machine learning experiments e.g. Class balancing, optimization and learner comparisons. Section III focuses upon extraction of insights\nfrom the trained machine learning model. It extracts insights from XAI tools, namely Feature Importance (III-A1), Ceteris Peribus Profiles (III-A2), Partial Dependence Profiles (III-A3) and Breakdown Profiles (III-A4). Section III-B and III-C present the insights extracted from optimized random forest and association rules mining, respectively. Section IV describes how all the obtained insights may be incorporated into an ensamble learner. Finally Section V presents the discussion and conclusions. A graphical summary is presented in the following flow diagram:\nGraphical Summary: Balancing of data has been followed by experimenting with 9 different classifier algorithms thus resulting into 36 different machine learning experiments. Optimized best performer, XAI and Association Rule Mining has been employed to develop an ensamble with high accuracy."
    },
    {
      "heading": "II. MACHINE LEARNING DETAILS",
      "text": "To correctly classify the type of surface defects of steel plates during industrial production, Semeion of Italy collected a data of 1941 faulty steel plates and recorded 27 observable characteristics/features of each of the faulty plate. This data is publically available through UCI (Semeion, n.d.). Data has 34 columns of which 27 are the observable variables/features and 7 binary columns each showing presence or absence of each of the seven types of faults. The last seven columns are one hot encoded classes, i.e. if the plate fault is classified as (say) \u201cStains\u201d there will be a 1 in that column and 0\u2019s in the other columns. However for the machine learning experiment requirements, following two changes have been made in present work: 1. Merged two factor variables viz. \u2018TypeOfSteel A300\u2019 and\u2019TypeOfSteel A400\u2019 as a single variable named \u2018TypeOfSteel\u2019, 2. Merged the 7 fault indicator variables into one variable showing all the 7 faults. This new target variable has been named as \u2018Fault\u2019. Variable names of the dataset after the abovementioned two changes are given in Table I:\nAll the variables in dataset are numeric except \u2018TypeOfSteel\u2019 and \u2018Fault\u2019 which are categorical. \u2018TypeOfSteel\u2019 indicates type to be either A300 or A400. \u2018Fault\u2019 is the target variable containing 7 labels of fault names (cf. Figure 2).\n\u2018Type Of Steel\u2019 has important implications for the possible Faults. Intrestingly Fault distribution for both kinds of steels i.e. A300 and A400 is highly imbalanced e.g. only one each instance of K Scratch and Stains is found in A300 type of steel. Also Dirtiness appears to be rare for this type of steel. On the other side Z Scratch is relatively a rare event for A400 type. Details may be further seen in Table II. Medoids are important notion used in this work. Details shall be given latter, for the time being it is roughly defined as a prototypical observation (row of data) for a given category (say) \u2018A300\u2019. Medoids of A300 and A400 types of steel have been given in Figure 1."
    },
    {
      "heading": "A. Class Balancing",
      "text": "As this is an imbalanced multiclass problem class balancing has rightly been pointed by (Tian, Fu, and Wu 2015) to be a promising direction of improvement. Counts and percentages for each class are shown in the bar plot of Figure 2:\nAs most of the machine learning algorithms carry a tacit assumption of labels of a classification dataset being equally distributed, therefore for an imbalanced problem, like the\npresent one incorporate bias towards the majority class (Pan et al. 2020). Even if the size of dataset is large enough to provide for considerable cases of each class, the problem of validation of the generlization capability of the trained model becomes difficult. The situation even worsens for an extreme rarity condition (Kang 2020) alongwith the inhenerent randomness of data.\nVarious solutions and techniques have been developed to address this imbalance problem (Santiago Egea Gomez et al. 2019). Out of these three main ones have been chosen namely, undersampling, oversampling and SMOTE. Weheras the names of undersampling and oversampling render their internal working clear, technique of SMOTE (Synthetic Minority\nOversampling Technique) needs some explanation. SMOTE was originally introduced in (Chawla et al. 2002) and it is basically an oversampling method but before oversampling it creats, or technically saying \u2018synthesizes\u2019, a new instance of the minority class which is mathematically on the straight line between a pair of minority instance and one of its K nearest neighbors. This procedure is replicated for a predefined number of times.\nBy its very design SMOTE is implemented for a binary classification settings. However this work extended its computation for a multiclass settings using turn by turn slection of max-class and each one class from the rest of smaller classes and then applying SMOTE upon each of the binary-class splits of the original data. Using all the 3 strategies upon the original dataset in total, yielded four datasets. Characteristics of each of these datsets are given in Table III:\nVarious machine learning algorithms have been applied to the problem of fault diagnostics of steel plates as noted in Introduction of this paper. Almost all of these methods were though aimed at achieving higher accuracy rather than gaining insight of the fault producing mechanism, as is the case here. To make an exhaustive and definitive search, present work employed 9 machine learning algorithms on each of the four datasets of Table III, thus resulting to 36 machine learning experiments in total. However such a large number of comparisions requires a level-field for all algorithms. Therefore a common-to-all 10 fold cross validation scheme was provided to each of the 36 ML experiments. Hardware is an HP Laptop with Intel R\u00a9 Core i7-7500U CPU @ 2.70Ghz. Final results of the comparison of 9 learners over the 4 datsets are given in Table IV."
    },
    {
      "heading": "B. Optimization",
      "text": "Random Forest performed best on both the simple oversampling and SMOTE datasets. Considering that Oversampling\nis basically multiple copies of the minority classes and thus do not expose the randomness and internal structure of the variation of different types of faults and this work has a higher priority for extracting human-useful insights over the precision of predictions hence SMOTE data has been chosen. Hyperparameters of the Random Forest have also been optimized, with following values of the final Random Forest (henceforth RF) model; Number of trees: 186, Mtry: 5, Target node size: 1, Variable importance using: permutation, Splitrule: gini, OOB prediction error: 7.37%. Confusion matrix for the optimized RF model is given as Table V, with \u2018True\u2019 in rows and \u2018Predicted\u2019 in Columns:\nIt is notable here for later use that Bumps and Common Other seem to have something common in them. An \u2018overlap\u2019 while labeling the faults may not be ruled out here. Error percentage for each type of fault as calculated from the Confusion Matrix is also shown in Table VI.\nIII. INSIGHT HARVESTING\nThis work primarily intends to harvest insight using which it achieves higher predictive precision. Insight is the accurate and deep (in the sense of structural) understanding of a system which is rooted into external real world. This in itself remains a subject of philosophical analysis whether explanation equates to an insight. This question becomes even more pronounced in view of the consideration that explaination is about the internal working whereas insight is somewhat externalized and related to real world causal relations. By its very nature insight is obtained by infusion of various pieces of information obtained from different sources and attempts. In the same vein this work gleans various pieces of information namely XAI, Optimized Random Forest based extracted rules and Association Rule Mining (henceforth ARM). Following point made in (Christina B. Azodi, Jiliang Tang, and Shin-Han Shiu 2020) is very much\nin point here: \u201cJust like there is no one universally best ML algorithm, there will not likely be one ML interpretation strategy that works best on all data or for all questions. Rather, the interpretation strategy should be tailored to what one wants to learn from the ML model and confidence in the interpretation will come when multiple approaches tell the same story.\u201d"
    },
    {
      "heading": "A. Explainable Artificial Intelligence",
      "text": "XAI is an umbrella term for a host of tools and techniques ranging from Mean Loss based variable importance and Model Specific tools to Model Agnostic techniques. The taxanomies introduced in (Barredo Arrieta et al. 2020) and (Baniecki and Biecek 2020) are very beneficial to the intrested reader. XAI provides for domain expert learning from ML, human requirement of automatic decisions, compliance of regulations and passing of security audits of the trained predictive models. Industrial Informatics has also flourished in recent times by using XAI (De Silva et al. 2020). Specifically present work resorts to Mean Loss based feature importance, Ceteris Peribus Profiles, Partial Dependence Profiles and Breakdown Profiles for harvesting insights into the trained model.\n1) Feature Importance: Feature importance is to be kept in view throughout as a number of techniques in XAI are available for almsot all variables and thus produce unmanageable heap of information. Knowing the most significant as well as the non-colinear variables helps to reduce the cognitive load for ML interpretations. Non-colinearity is also a strong requirement in XAI tools like PDP. Hence computation of feature importance is the natural first step. Besides the feature importances obtained from the optimized Random Forest, a number of other measures may be computed and combined to benchmark the features for subsequent use (Figure 3). Short description of each computed measure is given as: - CMIM: Minimal Conditional Mutual Information - DT: Decision Tree - Performance: Predictive Performance - Ranger: RF Optimized using permutation strategy - RF simple: Random Forest simple - Mean Loss: model agnostic XAI technique\nTable VII shows the importance ranks of each feature (with 1 having largest mean loss). This importance rank shall be used subsequently to figure out their relative places within the Breakdown plots of each fault-medoid.\nMost of the XAI tools work best only for the least correlated features with no or minimum colinearity. Therefore correlation coefficient of all features has been calculated for filtering of features and five features namely, Sum of Luminosity, Pixels Areas, X Perimeter, X Minimum, Y Minimum are having correlation more than 0.90. It is noted here for subsequent usage in XAI tools where needed e.g. in Ceteris Peribus profiles and Breakdown plots.\n2) Ceteris Paribus for Fault Medoids: A model which is otherwise opaque, may easily be peeped into if effect of each explanatory variable is examined separately. A further improvement is to examine one variable while keeping all others at some constant value, so to say, average value. Ceteris Peribus (CP) profile is the tool for this kind of analysis.\nCeteris Peribus is latin meaning \u201call others being constant\u201d. CP investigates the local curvature of the response surface of the black-box model (Goldstein et al. 2015). From the insight viewpoint it provides an indication how, in actual world, a variable may be affecting the fault creation. In literature CP technique has also been named as \u2018what-if\u2019 analysis or Individual Conditional Expectations (ICE).\nCP may be rigorously defined as follows: Let D stand for a dataset with n rows and p columns. Here p stands for the number of variables while n stands for the number of observations. As local methods operate on a single observation, let x\u2217 \u2208 R stand for an observation of interest. Let f : X \u2192 R denote for the model of interest, where X = Rp is the pdimensional input space. Then Ceteris Paribus is the profile g (z) for variable xi and observation x is defined as:\ngx\u2217(z) = f(x \u2217|xi = z)\nAs CP is a local technique i.e. based upon a single instance (row) of data. Thus any row in data may be chosen, however it is more suitable for the current scenario to choose to represent each Fault type by its most representative instance. In cluster analysis an instance with minimum dissimilarity (Yu et al. 2018) within a cluster is said to be its medoid. This notion is akin to taht of \u2018centroid\u2019 in computation of clusters. While various methods of medoid calculation are available we choose the fastest one i.e. medians of all numeric variables and mode (most frequent level) of a categorical variable. It is important to note that medoid here have been calculated from the original imbalanced dataset so that they remain \u2018typical\u2019 and no randomness creeps in. So to say, for this work a medoid refers to an object within a Fault-set for which average dissimilarity between it and all the other members of the cluster is minimal. Other such works demonstrating the usefulness of typical prototypes for XAI may be found in (Gurumoorthy et al. 2019) and the refernces therein. Typical fault profiles (medoids) used herein are given as Table VIII ordered by Variable Importance ranks as given in Table VII:\nOnce each numeric variable is scaled, these medoids may also be visualized as radar plots (Figure 4). A closer examination of the radar patterns reveals following insights:\nInsights 1. Bumps and Common Other are having striking similarity of shape with each other. 2. Z Scratch & K Scratch are very distinct and their mutual defferetiation is easier. 3. A typical Dirtiness case (i.e. medoid) is spread almost all over the 26 features. 4. Except K Scratch all other typical fault cases have presence on the \u2018Minimum Of Luminosity\u2019 feature. Visually it may be see as a tail in the radar plot. This renders this variable important for classification.\nFigure 5 provides Ceteris Peribus (CP) profiles of only 6 important variables because for each of the seven medoids\nand 6 variables these turns out to be 42 CP-plots. The real utility of CP profile shines once the insights, general observing of the similarities, regularities, pecularities and/or differences, are noted and separated for later use by human experts or by the machine algorithms. Therefore for the variables not shown here, just the gleaned insights are listed below:\nInsights 1. Bumps and Common Other almost always mirror each other in CP profile of all variables except X Maximum. This indicates a kind of \u2018overlap\u2019 while labeling has been originally done. 2. For thinner plates (with thickness less than 100) maximum probablity is for Bumps, which is understandabale. 3. As the density rugs provide indication of support (by number of available observations), therefore e.g. Steel Plate Thickness is \u2018intresting\u2019 for values less than 100. Beyond that point curves are flat and hence unintresring. Moreover beyond 100 supporting observations are scarce thus weakening the confidence of prediction.\nBumps\nCommon_Other\nDirtinessK_Scratch Pastry tains Z_Scratch\nBumps\nCommon_Other\nDirtinessK_Scratch Pastry StainsZ_Scratch\nBumps\nCommon_Other\nDirtinessK_Scratch PastryStains\nZ_Scratch\nBumps\nCommon_Other\nDirtinessK_ScratchPastry Stains\nZ_Scratch\nBumps\nCommon_Other\nDirtinessK_Scratch\nPastry\nStains\nZ_Scratch\nBumps\nCommon_Other\nDirtinessK_Scratch PastrySta s\nZ_Scratch\nLogOfAreas Minimum_of_Luminosity X_Maximum\nSteel_Plate_Thickness Length_of_Conveyer Orientation_Index\n1 2 3 4 5 0 50 100 150 200 0 500 1000 1500\n100 200 3001200 1400 1600 1800\u22121.0 \u22120.5 0.0 0.5 1.0 0.0\n0.2\n0.4\n0.6\n0.0\n0.2\n0.4\n0.6\npr ed\nic tio\nn\nCeteris Peribus Profiles\nFig. 5. Ceteris Peribus Profiles: Although CP is essentially a local (i.e. limited to the single observation of intrest) technique, however here the localness syndrom has been balanced by the use of Fault Medoids. Density rugs on the horizontal axis provide an indication of strength of available observations equivalently strength of evidence for the CP patterns.\n3) Partial Dependence Plots: Partial Dependence (PD) Plot describes how certain set of variables affect an average prediction. It is important to differentiate between Ceteris Peribus (CP) and PD. CP is localized to a certain instance, and in present work this happens to be medoid of a fault. PD is non-local in its character as it talks about a general \u2018average\u2019 prediction, albiet the set of variables being chosen by the user. PD shows how the prediction \u2018partially\u2019 depends upon the few variables of intrest. PD also describes the kind of relationaship e.g. linear, cuvilinear and step etc. More elaborately, a PDP examines a variable of intrest at a specific range. At each value within the range, the predictive model predicts for all cases (rows), and the prediction is then averaged out. Therefore a relation depicted by PDP is useful only once the features are\nnon-colinear. Technically, a PD may be written (Friedman 2001) as:\nf\u0302xs (xS) = ExC [ f\u0302xs (xS , xC) ] = \u222b f\u0302xs (xS , xC) dP (xC)\nwhere xS are the features for which the partial dependence function should be plotted and xC are the other features used in the machine learning model f\u0302 .\nThe partial function f\u0302xs is estimated by calculating averages in the training data, also known as Monte Carlo method:\nf\u0302xs (xS) = 1\nn n\u2211 i=1 f\u0302xs ( xS , x (i) C ) This work, in the manner of (R. Berk and J. Bleich 2013), demonstrates the benefit of using optimized Random Forest and its PDPs to precisely indicate the predictor-response relationships under imbalanced classification settings which are quite common in industrial problems. As humans can percieve only a few number of variables at any given time therefore in Figure 6, 7 and 8 target features for PD plots are chosen to be small in number and those also from the most important variables/features.\nBumps\nCommon_Other\nDirtiness\nK_ScratchPastry\nStains\nZ_Scratch\nBumps\nCommon_Other\nDirtiness\nK_Scratch\nPastryStains\nZ_Scratch\nBumps\nCommon_Other\nDirtiness\nK_Scratch\nPastryStains\nZ_Scratch\nSteel_Plate_Thickness Length_of_Conveyer Orientation_Index\n100 200 3001200 1400 1600 1800\u22121.0 \u22120.5 0.0 0.5 1.0\n0.1\n0.2\n0.3\nav er\nag e\npr ed\nic tio\nn\nPDP profiles (1)\nFig. 6. Partial Dependence Plots - 1: Dependence of an average prediction upon different variables. Contribution of Steel Plate Thickness to CommonOther type of fault is noteworthy.\n4) Breakdown Profiles: Medoids of each fault provides an important opportunity to figure out a hypothesis of how different variables affect the model-prediction of the specific medoid. Thus indicating the significance of different variables for different types of predictions. This is a kind of variable attribution exercise which attributes contribution of each individual variable to the present result. Technically known as Breakdown (BD) Profiles, there full plots showing each of the 27 variables is too space consuimg. Therefore only one BD plot is shown in Figure 9. One easily finds the most important contributors from a visual analysis of this plot. The variables in red color drag the prediction \u2018backward\u2019 and positive contributors push it forward and hence the net prediction result (in deep blue) is obtained.\nFor rest of the Fault-medoids only top-10 bigger contributors are listed alongwith their positive or negative contributions (using the +/- sign). For saving space we present the top ten features from the Breakdown plot of each fault-medoid in the following:\nBumps Empty Index +0.095, Square Index +0.068, Luminosity Index +0.056, TypeOfSteel +0.047, Orientation Index +0.046, Length of Conveyer +0.045, Edges X Index +0.042, X Perimeter +0.038, Minimum of Luminosity +0.038, Maximum of Luminosity +0.03\nCommon Other Length of Conveyer +0.085, Steel Plate Thickness +0.052, X Minimum +0.033, Luminosity Index -0.027, Minimum of Luminosity -0.027, Orientation Index +0.027,\nFig. 9. Breakdown Plot for CommonOther medoid. Breakdown portrays the contribution (positive/negative) of each predictive feature towards the final prediction of a certain observations, which, in this case is a medoid.\nEdges X Index -0.025, Edges Y Index +0.021, X Maximum +0.018, TypeOfSteel +0.012\nDirtiness Y Maximum +0.132, Y Minimum +0.121, Edges X Index +0.043, Edges Index +0.037, X Maximum +0.027, Maximum of Luminosity +0.024, TypeOfSteel -0.023, Minimum of Luminosity +0.02, Luminosity Index +0.016, Outside X Index +0.016\nK Scratch Log Y Index +0.109, X Minimum +0.07, Sum of Luminosity +0.058, X Perimeter +0.05, TypeOfSteel -0.05, Pixels Areas +0.05, Steel Plate Thickness +0.045, X Maximum +0.034, LogOfAreas +0.033, SigmoidOfAreas +0.032\nPastry Edges Y Index +0.091, Empty Index +0.059, Orientation Index +0.054, Outside X Index +0.051, Y Perimeter +0.046, Length of Conveyer +0.044, X Perimeter +0.044, Luminosity Index +0.039, Minimum of Luminosity +0.036, Edges X Index +0.033\nStains Length of Conveyer +0.1, Steel Plate Thickness +0.089, LogOfAreas +0.075, Pixels Areas +0.074, Sum of Luminosity +0.065, SigmoidOfAreas +0.049, TypeOfSteel -0.048, Y Perimeter +0.045, Log Y Index +0.029, X Maximum +0.026\nZ Scratch Length of Conveyer -0.101, TypeOfSteel +0.014, Empty Index -0.01, X Maximum -0.008, X Minimum -0.007, Outside X Index -0.007, Steel Plate Thickness +0.006, Luminosity Index -0.006, Log Y Index -0.005, Log X Index -0.004"
    },
    {
      "heading": "B. Random Forest Rules",
      "text": "In search of greater explanability and interpretability a natural target is to search under the hood of a relatively high precision predictive model as is the case with presnt dataset and Random Forest (RF) model developed and optimized earlier. Use of variable selection for a minimal and relevent set of conditions from the association rules has been enunciated by (Deng et al. 2014). Therefore exposing of the internal structure of RF model has been carried out in line with the works (Deng et al. 2014) and (Deng 2019).\nRule based models are very good at interpretability across a wide area of applications. Such models base upon a simplified uderstanding of their internal workings. However one of the major problems with rule generation is the large number of rules besides their coverage and specificity. In simpler terms coverage is the amount and specificity is the length of a rule. Usage of these measures of rule quality is fundamentally dictated by the intention of the user. For the sake of explainability a smaller number of rules with as much coverage as possible is a pre-requisite. A rule having very lengthy antecedent or consequent becomes very difficult to understand and interpret. Therefore transparency and explainability of a model is directly proportional to the coverage and specificity of the rule-base.\nIn literature a large number of research works may be found regarding production of rules but considerably fewer works are seen for filtering and pruning strategies of the rulebase. This work proposes a novel approcah for model building using rules mining: discovering a large number of rules and then focusingin the rulebase using apriori expertise e.g. semantic analysis and the quantitative filtering on the basis of various quality measures of rules.\nThe rules produced by machine are of course mechanistic and may be \u2018humanized\u2019 by re-writing them in near humanlanguage."
    },
    {
      "heading": "C. Association Rules",
      "text": "Association rule mining is now a well established sub-area of machine learning with the additional advantage of being fully transparent from the explainability view point (Grabot 2020).\nComments on Rules 1. Length of RF-3 is too prohibitive. 2. Rules for K Scratch and Stains have highest counts and confidence, however at the same time they have smallest lengths. Thus making these rules very attractve insights for human use."
    },
    {
      "heading": "IV. FINAL MODEL BUILDING",
      "text": "Insights alongwith the extracted rules from RF and associations mining may be incorporated into a data frame for computing implementation. An example may be put here for explaining the rule-classifier ensamble as used in this work: Suppose following 5 rules have been decided to be included in the ensamble: 1. IF V1 is AV 1, V3 is CV 3 THEN R1 2. IF V2 is AV 3, V3 is CV 3 THEN R2 3. IF V1 is BV 1, V1 is AV 1, V3 is AV 3 THEN R3 4. IF V2 is BV 3, V3 is CV 3 THEN R4 5. IF V1 is CV 1, V2 is BV 2, V3 is BV 3 THEN R5\nIn present case AV 1 etc are the semi-open intervals appearing in rules listings of Table X. Next, these may be re-written as the following table (a data frame):\nV1 V2 V3 Result AV1 \u2212 CV3 R1 \u2212 AV2 CV3 R2 BV1 AV2 AV3 R3 \u2212 BV3 CV3 R4 CV1 BV2 BV3 R5\nThe Directed Acyclic Graph for development of the customized model based upon XAI insights combined with\nrules from RF and Association Rules Mining (ARM) is shown in Figure (10):\nPerformance of the developed model is given as:\nSame Confusion matrix for our developed classifier may be visualized as follows:"
    },
    {
      "heading": "V. DISCUSSION AND CONCLUSIONS",
      "text": "This work makes three contributions viz: insights, methodology and a high precision classifier. First of all it obtains valuable-for-humans insights using XAI, secondly it introduces a methodology to incorporate insights from three areas to build a high precision fault classifier. This high precision classifier is the third contribution.\nUse of Synthetic Minority Oversampling Technique to balance the multi-class imbalance and the use of Medoids for taking full advantage of the local model agnostic XAI tools like Ceteris Peribus profiles and Breakdown plots are also important methodological innovations of this work.\nThe considerably improved performance of the classifier, in our view, is the result of few basic factors of this research. Idea of using various class balancing techniques for the problem of steel plate faults dataset was originally proposed by (Tian, Fu, and Wu 2015) in their conclusion of the paper as a future direction. However, this they proposed while ignoring all the 673 cases of Common Other type of faults.\nIn literature one finds a considerable number of sythetic data generation and class balancing techniques (Laopez et al. 2013).\nA suitable choice of prototypical cases (Gurumoorthy et al. 2019) have been demonstrated to be useful for the purpose of explainability. Our present work makes a suitable choice of the representative, equivalently central and prototypical, cases as medoids. This choice of medoids allows for peeking into the global behaviour of the trained model while using a localized technique, for example, Ceteris Peribus.\nRepresenting collection of rules as a data frame has been in practice for various practical use cases e.g. illustrative example in (Deng 2019). Present work makes some changes for its own usage to cobble various If-Then rules obtained from XAI Insights, Random Forest and Association Mining.\nInsights help to do Science (and Engineering, of course) \u2018by\u2019 Data. The insights obtained in this work, at the very first place, are themselves directly useful for the production engineers. These insights provide for sharpening of the skill of humans to detect a fault and its relevant factors/variables.\nUsually XAI is used to develop white-box models for human consumption. However in this work XAI has been used in a less trodden manner i.e. getting insights from XAI and incorporating them back into rule-based ensemble development. These insights have been re-entered into the loop\nof model building which has clearly benefited the new model by improving its performance by almost 7%.\nThis approach has also benefited from the wholistic view developed by Baniecki et. al. (Baniecki and Biecek 2020). Techniques labeled as 1-4 were applied from Figure 2 (reproduced below) of Baniecki paper.\nCertain areas remained unaddressed as well. There are other than SMOTE techinques available for oversampling and balancing of data (Laopez et al. 2013). These techniques should also be checked for even higher accuracy of the developed classifier. Another approach may be to develop a surrogate model for obtaining even more transparancy. Although in view of an Optimized Random Forest and its extracted rules this appears not too useful, but the quality measures of RF rules indicates a possible direction of improvement."
    }
  ],
  "title": "Explainable Artificial Intelligence Based Fault Diagnosis and Insight Harvesting for Steel Plates Manufacturing",
  "year": 2020
}
