{
  "abstractText": "The \u2018Clever Hans\u2019 effect occurs when the learned model produces correct predictions based on the \u2018wrong\u2019 features. This effect which undermines the generalization capability of an ML model and goes undetected by standard validation techniques has been frequently observed for supervised learning where the training algorithm leverages spurious correlations in the data. The question whether Clever Hans also occurs in unsupervised learning, and in which form, has received so far almost no attention. Therefore, this paper will contribute an explainable AI (XAI) procedure that can highlight the relevant features used by popular anomaly detection models of different type. Our analysis reveals that the Clever Hans effect is widespread in anomaly detection and occurs in many (unexpected) forms. Interestingly, the observed Clever Hans effects are in this case not so much due to the data, but due to the anomaly detection models themselves whose structure makes them unable to detect the truly relevant features, even though vast amounts of data points are available. Overall, our work contributes a warning against an unrestrained use of existing anomaly detection models in practical applications, but it also points at a possible way out of the Clever Hans dilemma, specifically, by allowing multiple anomaly models to mutually cancel their individual structural weaknesses to jointly produce a better and more trustworthy anomaly detector.",
  "authors": [
    {
      "affiliations": [],
      "name": "Jacob Kauffmann"
    },
    {
      "affiliations": [],
      "name": "Lukas Ruff"
    },
    {
      "affiliations": [],
      "name": "Gr\u00e9goire Montavon"
    },
    {
      "affiliations": [],
      "name": "Klaus-Robert M\u00fcller"
    }
  ],
  "id": "SP:a32a9eb398472f2d81497532fa8f6647b96267bf",
  "references": [
    {
      "authors": [
        "S. Akcay",
        "A.A. Abarghouei",
        "T.P. Breckon"
      ],
      "title": "GANomaly: Semi-supervised anomaly detection via adversarial training",
      "venue": "ACCV (3), volume 11363 of Lecture Notes in Computer Science, pages 622\u2013637. Springer",
      "year": 2018
    },
    {
      "authors": [
        "S. Bach",
        "A. Binder",
        "G. Montavon",
        "F. Klauschen",
        "K.-R. M\u00fcller",
        "W. Samek"
      ],
      "title": "On pixelwise explanations for non-linear classifier decisions by layer-wise relevance propagation",
      "venue": "PLOS ONE, 10(7):e0130140,",
      "year": 2015
    },
    {
      "authors": [
        "P. Bergmann",
        "M. Fauser",
        "D. Sattlegger",
        "C. Steger"
      ],
      "title": "MVTec AD - A comprehensive real-world dataset for unsupervised anomaly detection",
      "venue": "CVPR, pages 9592\u20139600. Computer Vision Foundation / IEEE",
      "year": 2019
    },
    {
      "authors": [
        "K.S. Beyer",
        "J. Goldstein",
        "R. Ramakrishnan",
        "U. Shaft"
      ],
      "title": "When is \u201cnearest neighbor\u201d meaningful? In C",
      "venue": "Beeri and P. Buneman, editors, ICDT, volume 1540 of Lecture Notes in Computer Science, pages 217\u2013235. Springer",
      "year": 1999
    },
    {
      "authors": [
        "C.S. Calude",
        "G. Longo"
      ],
      "title": "The deluge of spurious correlations in big data",
      "venue": "Foundations of Science,",
      "year": 2016
    },
    {
      "authors": [
        "V. Chandola",
        "A. Banerjee",
        "V. Kumar"
      ],
      "title": "Anomaly detection: A survey",
      "venue": "ACM computing surveys (CSUR), 41(3):15",
      "year": 2009
    },
    {
      "authors": [
        "G. Cohen",
        "S. Afshar",
        "J. Tapson",
        "A. Van Schaik"
      ],
      "title": "EMNIST: Extending MNIST to handwritten letters",
      "venue": "IJCNN, pages 2921\u20132926",
      "year": 2017
    },
    {
      "authors": [
        "Y. Cong",
        "J. Yuan",
        "J. Liu"
      ],
      "title": "Sparse reconstruction cost for abnormal event detection",
      "venue": "CVPR, pages 3449\u20133456. IEEE Computer Society",
      "year": 2011
    },
    {
      "authors": [
        "M. Goldstein",
        "S. Uchida"
      ],
      "title": "A comparative evaluation of unsupervised anomaly detection algorithms for multivariate data",
      "venue": "PLOS ONE,",
      "year": 2016
    },
    {
      "authors": [
        "N. G\u00f6rnitz",
        "M. Kloft",
        "K. Rieck",
        "U. Brefeld"
      ],
      "title": "Toward supervised anomaly detection",
      "venue": "Journal of Artificial Intelligence Research, 46:235\u2013262",
      "year": 2013
    },
    {
      "authors": [
        "S. Hawkins",
        "H. He",
        "G. Williams",
        "R. Baxter"
      ],
      "title": "Outlier detection using replicator neural networks",
      "venue": "DaWaK, volume 2454, pages 170\u2013180",
      "year": 2002
    },
    {
      "authors": [
        "H. Hoffmann"
      ],
      "title": "Kernel PCA for novelty detection",
      "venue": "Pattern Recognition, 40(3):863\u2013874",
      "year": 2007
    },
    {
      "authors": [
        "C. Huang",
        "J. Cao",
        "F. Ye",
        "M. Li",
        "Y. Zhang",
        "C. Lu"
      ],
      "title": "Inverse-transform autoencoder for anomaly detection",
      "venue": "CoRR, abs/1911.10676",
      "year": 2019
    },
    {
      "authors": [
        "J. Kauffmann",
        "K.-R. M\u00fcller",
        "G. Montavon"
      ],
      "title": "Towards explaining anomalies: A deep Taylor decomposition of one-class models",
      "venue": "Pattern Recognition, 101:107198",
      "year": 2020
    },
    {
      "authors": [
        "D.P. Kingma",
        "J. Ba"
      ],
      "title": "Adam: A method for stochastic optimization",
      "venue": "ICLR",
      "year": 2015
    },
    {
      "authors": [
        "M. Kowal",
        "S. Ananieva",
        "T. Th\u00fcm"
      ],
      "title": "Explaining anomalies in feature models",
      "venue": "GPCE, pages 132\u2013143. ACM",
      "year": 2016
    },
    {
      "authors": [
        "S. Lapuschkin",
        "S. W\u00e4ldchen",
        "A. Binder",
        "G. Montavon",
        "W. Samek",
        "K.-R. M\u00fcller"
      ],
      "title": "Unmasking Clever Hans predictors and assessing what machines really learn",
      "venue": "Nature Communications, 10(1):1096",
      "year": 2019
    },
    {
      "authors": [
        "A. Lazarevic",
        "V. Kumar"
      ],
      "title": "Feature bagging for outlier detection",
      "venue": "KDD, pages 157\u2013166. ACM",
      "year": 2005
    },
    {
      "authors": [
        "B. Micenkov\u00e1",
        "R.T. Ng",
        "X. Dang",
        "I. Assent"
      ],
      "title": "Explaining outliers by subspace separability",
      "venue": "ICDM, pages 518\u2013527. IEEE Computer Society",
      "year": 2013
    },
    {
      "authors": [
        "G. Montavon",
        "S. Lapuschkin",
        "A. Binder",
        "W. Samek",
        "K.-R. M\u00fcller"
      ],
      "title": "Explaining nonlinear classification decisions with deep Taylor decomposition",
      "venue": "Pattern Recognition, 65:211\u2013222",
      "year": 2017
    },
    {
      "authors": [
        "G. Montavon",
        "A. Binder",
        "S. Lapuschkin",
        "W. Samek",
        "K.-R. M\u00fcller"
      ],
      "title": "Layer-wise relevance propagation: An overview",
      "venue": "Explainable AI, volume 11700 of Lecture Notes in Computer Science, pages 193\u2013209. Springer",
      "year": 2019
    },
    {
      "authors": [
        "N. Mu",
        "J. Gilmer"
      ],
      "title": "MNIST-C: A robustness benchmark for computer vision",
      "venue": "CoRR, abs/1906.02337",
      "year": 2019
    },
    {
      "authors": [
        "H.V. Nguyen",
        "H.H. Ang",
        "V. Gopalkrishnan"
      ],
      "title": "Mining outliers with ensemble of heterogeneous detectors on random subspaces",
      "venue": "DASFAA (1), volume 5981 of Lecture Notes in Computer Science, pages 368\u2013383. Springer",
      "year": 2010
    },
    {
      "authors": [
        "E. Parzen"
      ],
      "title": "On estimation of a probability density function and mode",
      "venue": "The Annals of Mathematical Statistics,",
      "year": 1962
    },
    {
      "authors": [
        "S. Pidhorskyi",
        "R. Almohsen",
        "G. Doretto"
      ],
      "title": "Generative probabilistic novelty detection with adversarial autoencoders",
      "venue": "NeurIPS, pages 6822\u20136833",
      "year": 2018
    },
    {
      "authors": [
        "M.A. Pimentel",
        "D.A. Clifton",
        "L. Clifton",
        "L. Tarassenko"
      ],
      "title": "A review of novelty detection",
      "venue": "Signal Processing, 99:215\u2013249",
      "year": 2014
    },
    {
      "authors": [
        "M.T. Ribeiro",
        "S. Singh",
        "C. Guestrin"
      ],
      "title": "why should I trust you?\u201d: Explaining the predictions of any classifier",
      "venue": "KDD, pages 1135\u20131144. ACM",
      "year": 2016
    },
    {
      "authors": [
        "M. Rosenblatt"
      ],
      "title": "Remarks on some nonparametric estimates of a density function",
      "venue": "The Annals of Mathematical Statistics,",
      "year": 1956
    },
    {
      "authors": [
        "L. Ruff",
        "N. G\u00f6rnitz",
        "L. Deecke",
        "S.A. Siddiqui",
        "R.A. Vandermeulen",
        "A. Binder",
        "E. M\u00fcller",
        "M. Kloft"
      ],
      "title": "Deep one-class classification",
      "venue": "ICML, volume 80 of Proceedings of Machine Learning Research, pages 4390\u20134399. PMLR",
      "year": 2018
    },
    {
      "authors": [
        "L. Ruff",
        "R.A. Vandermeulen",
        "N. G\u00f6rnitz",
        "A. Binder",
        "E. M\u00fcller",
        "K.-R. M\u00fcller",
        "M. Kloft"
      ],
      "title": "Deep semi-supervised anomaly detection",
      "venue": "ICLR",
      "year": 2020
    },
    {
      "authors": [
        "T. Schlegl",
        "P. Seeb\u00f6ck",
        "S.M. Waldstein",
        "U. Schmidt-Erfurth",
        "G. Langs"
      ],
      "title": "Unsupervised anomaly detection with generative adversarial networks to guide marker discovery",
      "venue": "International Conference on Information Processing in Medical Imaging, pages 146\u2013157",
      "year": 2017
    },
    {
      "authors": [
        "B. Sch\u00f6lkopf",
        "J.C. Platt",
        "J. Shawe-Taylor",
        "A.J. Smola",
        "R.C. Williamson"
      ],
      "title": "Estimating the support of a high-dimensional distribution",
      "venue": "Neural Computation, 13(7):1443\u20131471",
      "year": 2001
    },
    {
      "authors": [
        "K. Simonyan",
        "A. Zisserman"
      ],
      "title": "Very deep convolutional networks for large-scale image recognition",
      "venue": "ICLR",
      "year": 2015
    },
    {
      "authors": [
        "B.L. Sturm"
      ],
      "title": "A simple method to determine if a music information retrieval system is a \u201chorse",
      "venue": "IEEE Transactions on Multimedia, 16(6):1636\u20131644",
      "year": 2014
    },
    {
      "authors": [
        "D.M.J. Tax",
        "R.P.W. Duin"
      ],
      "title": "Support vector data description",
      "venue": "Machine Learning, 54(1):45\u201366",
      "year": 2004
    },
    {
      "authors": [
        "A. Zimek",
        "E. Schubert",
        "H. Kriegel"
      ],
      "title": "A survey on unsupervised outlier detection in high-dimensional numerical data",
      "venue": "Statistical Analysis and Data Mining, 5",
      "year": 2012
    }
  ],
  "sections": [
    {
      "heading": "1 Introduction",
      "text": "Anomaly detection [6, 26] is a common machine learning problem for which numerous approaches have been proposed. They can be roughly organized into density-based [24, 25], reconstruction-based [11, 12, 8], and boundary-based approaches [32, 35, 31, 29]. Recently, there has been an effort to develop benchmarks and procedures to systematically assess and compare the performance of different anomaly detection models [9, 3]. Because it is hard to conceive a representative set of anomalies that would test for all particular ways in which a point can be anomalous, common testing procedures (based on a score and some set of labeled outliers) can strongly under/overestimate the performance of a given trained anomaly detection model.\nIn this paper, we propose to further validate an anomaly detector by inspecting its decision structure, in particular, to ensure that the model has not unintentionally implemented a \u2018Clever Hans\u2019 strategy [17]. \u2018Clever Hans\u2019 refers to a famous horse that was believed to be able to perform arithmetic calculations asked by his trainer. Later studies have revealed, that the horse was basing its consistently correct predictions not on performing the actual mathematical calculation but on watching unintended gestures of the trainer or some other human. Similar Clever Hans effects have been observed in areas such as information retrieval [34] or supervised learning [17]. In the latter case, this typically arises from letting the learning model exploit spurious correlations in data. These spurious correlations have been\nar X\niv :2\n00 6.\n10 60\n9v 1\n[ cs\n.L G\n] 1\n8 Ju\nn 20\npredicted to become increasingly severe as datasets become larger [5], thus, making the problem increasingly relevant. To the best of our knowledge, however, this interesting effect has not yet been studied in unsupervised models such as anomaly detection.\nTo analyze the Clever Hans effect in the context of anomaly detection, we need to endow the anomaly detection models with explainable AI (XAI) capabilities, so that they are able to provide explanations to their own prediction, specifically, identifying input features that have contributed the most to the anomaly score. Various methods have been proposed for explaining anomaly models [19, 16, 14], although so far, explanation techniques do not homogeneously and reliably apply to all anomaly detection models. Also, there is a need to systematically detect Clever Hans effects from those explanations, something that has so far only been done for supervised classifiers [17].\nIn this paper, we address the questions above by contributing in the following distinct ways:\n1. We propose to embed common anomaly detection models in a three-layer neural network architecture consisting of (i) feature extraction, (ii) distance computation and (iii) pooling. This common representation of anomaly models allows us to systematically explain their predictions in a way that is comparable across models.\n2. We demonstrate in the context of anomaly detection that \u2018Clever Hans\u2019 effects can also be found in large amounts and in various forms. For this, we make use of the MNIST-C [22] and MVTec [3] corpora which come with ground-truth anomalies and pixel-wise annotations.\n3. We analyze on a more abstract level how the Clever Hans effect systematically arises in different anomaly detection models. Unlike in supervised learning, where this effect is mainly the result of exploiting spurious correlations in the data, Clever Hans is found here to be intrinsic to the structure of the anomaly detection model.\n4. Observing that each model has structural weaknesses that lead to Clever Hans strategies, we investigate a bagging approach, which we find to produce improvements compared to the individual models.\nOverall, through the lens of XAI, our paper brings attention to the limitations of standard anomaly detection models, in particular, their exposure to the Clever Hans effect. While it underlines further motivation and necessity for improving current anomaly detectors, it also shows that the Clever Hans effect can be reduced via a simple bagging technique."
    },
    {
      "heading": "2 Towards Explainable Anomaly Detection",
      "text": "In the following, we introduce a common XAI framework that is applicable to a broad range of anomaly detection models by first embedding them into a layered neural network architecture, and then using the resulting layered architecture to support a common backpropagation procedure for extracting explanations. Anomaly detection models can be roughly divided into three categories:\nDensity-Based Models Density-based anomaly detection consists of first learning a probability model on inlier data, and then measure outlierness for a new data point x based on the model probability score, e.g. o(x) = \u2212 log p\u03b8(x). For the common kernel density estimation models [28, 24] (KDE), the outlier score can be rewritten up to a constant factor and additive term as a soft min-pooling over distances\no(x) = min\u03b3j (\u2016x\u2212 xj\u20162). (1)\nWhen the kernel is Gaussian, the min-pooling is given as min\u03b3j (hj) = \u2212\u03b3\u22121 log \u2211 j exp(\u2212\u03b3 hj), i.e. an inverted log-sum-exp pooling with stiffness \u03b3. When the kernel is t-Student, the\nmin-pooling takes the form of a harmonic mean [14]. A similar min-pooling formulation can be obtained for other types of density models such as Gaussian mixture models.\nReconstruction-Based Models Reconstruction is another paradigm for anomaly detection [11, 12, 8], where the outlier score is given as the divergence between the original data and its reconstruction, for example given by an autoencoder x 7\u2192 r(x). Typically, the outlier score is given by the squared reconstruction distance:\no(x) = \u2016r(x)\u2212 x\u20162.\nWhen the autoencoder output is not deterministic and produces instead a conditional distribution p\u03b8(x | r(x)) at the output, outlierness can be measured as the negative logprobability, and when the conditional distribution is Gaussian, it becomes equivalent to the squared distance.\nBoundary-Based Models Boundary-based models learn an envelope that contains the inliers. Data points outside the envelope are then predicted to be outliers. The envelope or decision boundary can be built in input space, e.g. as in the one-class SVM [32] or Support Vector Data Description [35], but also in feature space, e.g. in deep one-class classification [29]. A deep one-class model typically builds a spherical envelope in feature space by defining the outlier score\no(x) = \u2016\u03c6(x)\u20162\nwhere the feature map is trained to minimize this quantity for inliers, subject to some regularization constraint. Interestingly, boundary-based models are not restricted to unsupervised data and can instead leverage supervised outlier data to refine the decision boundary [10, 1, 30].\nWe have briefly reviewed three families of methods for outlier detection: density-based, reconstruction-based, and boundary-based. Although these methods may appear to be different in all technical aspects, we find that outlier scores produced by each of them can in fact be conceptually embedded in the same three-layer architecture: (1) Feature extraction: Features that are considered to be relevant for the task of anomaly detection are extracted or made more salient in the feature map representation. (This step is only present in the deep one-class model). (2) Distances: Distances to some template(s) are then produced. Templates are either the data distribution itself for the KDE model, the autoencoder reconstruction for the reconstruction-based model, or the origin in feature space for the deep one-class model. (3) Pooling: Produced distances are then pooled using some common pooling function (this step is only present in the KDE model). The way each model fits in this three-layer architecture is shown in Figure 1.\nThis architecture lets us build an explanation technique that homogeneously applies to all three models. Our approach to explanation is based on the framework of \u2018Deep Taylor Decomposition\u2019 [20, 14], where a Taylor expansion is applied at each layer to identify the most contributing terms in the lower layer. Overall, the method leads to an LRP-type [2] backward propagation procedure from the output score to the input features. Propagation in the different layers is detailed below.\nPooling Layer Let o((sk)k) be some function performing the pooling. We identify a \u2018deactivation\u2019 line in the space of pooled elements on which the pooling function is linear. We then select the root point (s\u0303k)k on that line, i.e. where the output of the pooling becomes zero. Finally, the attribution is given by the elements of a first-order Taylor expansion at the root point, i.e. Rk = [\u2207o(s\u0303)]k \u00b7 (sk \u2212 s\u0303k). To further propagate to the lower layers, we can approximate the attribution as a homogeneous function of the pooled activations, i.e. R\u0302k = cksk.\nDistance Layer When attribution hits the distance layer, the output must be further redistributed to different dimensions entering in the distance computation. Here, we express the score we have obtained in the layer above in terms of the distance inputs: R\u0302k(a) = ck \u2211 j(ajk \u2212\u00b5jk)2. The variable \u00b5jk is either constant (KDE and Deep) or is treated as such (Autoencoder). A second-order Taylor expansion at the origin gives diagonal second-order terms: Rjk = ck \u00b7 (ajk \u2212 \u00b5jk)2.\nFeatures Layer To further propagate the decomposition throughout the network, we need \u00b5 = 0 (i.e. the outlier model is centered at the origin), which lets us express the terms from the layer above as Rjk = ck\u03c6jk(x)2. This quantity can be propagated to the input features using standard LRP [2].\nFurther explanations and justification for these propagation steps are given in Appendix A of the Supplement. Finally, when compared to other approaches, especially sampling-based approaches, which do not rely on the underlying model structure [19, 27], the propagationbased approach we have developed here is faster (it requires only a single pass in the model) and also does not require access to the input distribution."
    },
    {
      "heading": "3 Unmasking Clever Hans Effects in Anomaly Models",
      "text": "Having equipped anomaly detection models with a systematic way of explaining their predictions in terms of input features, we will now look for possible Clever Hans effects in these anomaly models, especially when the latter are applied to real data.\nOur experiments are performed on two recent datasets: MNIST-C [22], and MVTec [3]. A peculiarity of these two datasets which makes them ideal testbeds is that they either come with the data generation process (from which ground-truth explanations of anomalies can be built) or with ground-truth pixel-wise annotations of the anomaly patterns.\nFor each dataset, we train three models: A kernel density estimator (KDE), an autoencoder (Auto), and a deep one-class model (Deep). For the KDE model, we use a Gaussian kernel where we select the scale such that the likelihood of an inlier validation set is maximized. For the autoencoder on MNIST-C, we use a LeNet-type encoder that has two convolutional layers with max-pooling followed by two fully connected layers that map to an encoding of 64 dimensions. We construct the decoder symmetrically where we replace convolution and max-pooling with deconvolution and upsampling respectively. For MVTec, we use an encoder-decoder architecture as presented in [13] which maps to a bottleneck of 512 dimensions. Both, the encoder and decoder here consist of four blocks having two 3\u00d73 convolutional layers followed by max-pooling or upsampling respectively. We train the autoencoders using the Adam optimizer [15] such that the reconstruction error of an inlier validation set is minimized. The Deep One-Class model is built on top of the feature extractor of a CNN. For MNIST-C, we train a classifier CNN on the \u2018letters\u2019 subset of the EMNIST dataset [7]. The feature extractor consists of three convolutional layers that are interleaved with `2 pooling layers, leading to a 640 dimensional feature space. For MVTec, we start with a standard pretrained VGG-16 network [33] and we cut off the top classification layer, which results in a 4096 dimensional feature space. For both models, the outlier score is defined to be the squared norm of the feature vector after a linear whitening transformation that is regularized such that the outlier detection ROC score on a validation set with some outliers is maximized.\nOn the MNIST-C dataset, the autoencoder delivers the best results, however, there are exceptions to this, for example, scale, rotation, or translations are better detected by the deep one-class model. On the MVTec dataset, the deep one-class model performs best on most classes, which can be explained by the more abstract level and multiscale properties present in this dataset. The question we ask is whether the results are truly reliable or whether some of the accuracies are compromised due to the Clever Hans effect.\nHere, we leverage the fact that for the two considered datasets, MNIST-C and MVTec, ground-truth pixel-wise annotations are given and can therefore be confronted with the pixel-wise explanations of the model predictions. We can define a score measuring the mismatch between the detection accuracy (measured as the area under the ROC curve) and the explanation accuracy (measured as the cosine similarity between the ground-truth and the pixel-wise explanation). Pixel-wise explanations are passed through a rectification function so that the cosine similarity is always greater or equal to zero. We define the Clever Hans score as the difference:\nClever Hans score = detection accuracy \u2212 explanation accuracy\nThe Clever Hans score is a number which ranges from \u22121 to 1 (or \u2212100 and 100 if expressed on a percentage scale). The closer to 100, the more the anomaly detector has fooled the standard validation procedure by exploiting the wrong input features. In particular, we will look for anomaly detection models and tasks with highest Clever Hans scores to highlight the widespread presence of this effect in anomaly detection. Figure 3 shows the top-3 classes with highest Clever Hans scores for each model and dataset.\nHighest Clever Hans scores are obtained for the MVTec dataset which is also a more difficult and high-dimensional anomaly detection problem. It is also notable that different classes appear in the top-3 for the different models, suggesting that the models are affected by the problem in different ways. To shed light on the diverse Clever Hans effects, we look at single instances from classes in the top-3. Examples for each model and dataset are given in Figure 4.\nWe observe for example on MNIST-C, that the KDE model, although correctly identifying the anomalous dotted pattern, also highlights the whole digit region. The same occurs for the wood class on the MVTec dataset, where the high-frequency wood stripes appear as anomalous and completely dominate the small local perforations on the wood that are the true sources of anomaly.\nSimilar Clever Hans effects can be observed for the autoencoder, in particular, for the MNIST-C canny edges transformation. Here, although the whole interior of the digit has turned from white to black, the autoencoder completely ignores that change of color and only highlights the contour of the digit. On MVTec, a large contamination is present at the center of the bottle (photographed from above), however, the autoencoder bases its anomaly prediction on very fine elements on the outer part of the bottle.\nClever Hans effects can also be observed for the deep one-class model. The MNIST-C stripe transformation makes the whole border of the image turn from black to white, however, the deep one-class model bases its decision only on the edges of the added stripes and the interaction between these edges and the digit. On the MVTec data, the decision is mostly based on looking at the transition between the zipper tissue and the white background, rather than attending the true source of anomaly which is the zipper opening.\nIn all the examples above, the anomaly model has produced high outlier scores, but these high scores were produced systematically for the \u2018wrong\u2019 reasons. These Clever Hans strategies potentially undermine the generalization capability of the models, even for the classes with the highest measured anomaly detection accuracy."
    },
    {
      "heading": "4 Understanding Why Anomaly Models are Clever Hanses",
      "text": "In practice, it is not feasible to inspect the explanation of every new prediction. We thus need to attain more systematic insights into what conceptually and theoretically causes these Clever Hans strategies. Because the observed effects manifest themselves in various ways for the different models, we hypothesize that they are inherent to the structure of the anomaly detection models, rather than a petty effect of the training data. In the following, we give explanations for why the Clever Hans effect arises systematically in these models, and connect them to the effects observed in Fig. 4.\nKernel Density Estimation: We propose an explanation of the Clever Hans effect in KDE, based on the concentration of distances phenomenon occurring in high-dimensional spaces [36]. In such spaces, distances between different pairs of data points (assumed to be sampled randomly) become increasingly similar, i.e. their deviation from the mean converges to zero. By rewriting the KDE model of Eq. (1) as\no(x) = d\u0304\u2212 \u03b3\u22121 log \u2211N j=1 exp(\u2212\u03b3(\u2016x\u2212 xj\u20162 \u2212 d\u0304))\nwhere d\u0304 indicates the average distance to the inliers, and making use of the linear approximations exp(t) = 1 + t and log(N + t) = log(N) + t/N at t = 0, the KDE outlier score can be approximated as\no(x) \u2248 \u2016x\u2212 x\u0304\u20162 \u2212 log(N)/\u03b3 + cst.\nwhere x\u0304 denotes the mean of the inliers (cf. Appendix B in the Supplement for a derivation). This result suggests that in effect, KDE implements a difference-to-the-mean anomaly detection strategy in the input space. This strategy is also revealed by the heatmaps in Fig. 4 where the difference-to-the-mean component (the digit area and the wood stripes) explains a greater fraction of the KDE outlier score than to the true anomalies (the dotted line and the wood perforations).\nAutoencoder Reconstruction: For this model, the Clever Hans effect finds its source in data points x whose reconstruction r(x) lies far away from the true data distribution. Hence, the difference x\u2212 r(x) which supports the construction of the outlier score, is mainly supported by features that do not connect in any meaningful way to the input distribution and are therefore irrelevant for explaining anomaly. This weakness is also revealed by the heatmap explanations in Fig. 4. The MNIST-C canny-edge transformation brings the data point in a completely different region of the input space, where the reconstruction-based explanation focuses on edges rather than focusing on the global transformation of the digit from white to black. A similar effect is observed on the MVTec data for the class bottle, where the large contamination artefact at the center of the image can be more easily reconstructed than some non-anomalous patterns at the border of the bottle, therefore, misidentifying again the true source of outlierness.\nDeep One-Class: Deep models measure outlierness as the distance in some feature space different from the input space o(x) = \u2016\u03c6(x)\u2212\u03c6(x\u2032)\u20162. Typically, the feature map incorporates feature weightings, activation units, and pooling steps that produce distortions on the local input geometry. While such distortions are desirable in supervised learning to build invariance and gain statistical efficiency, it almost surely also compresses a few of the many components in which an anomaly could occur. This phenomenon can be clearly seen in the explanations of Fig. 4: For the MNIST-C example, the outer border of the explanation is zero although the stripe outlier pattern extends to the very border of the image. Similarly, on the MVTec example, the deep network attends the border of the zipper fabric much more strongly than the center area where the true anomaly can be found.\nThe weaknesses of these different anomaly detection techniques and the Clever Hans strategies they induce are summarized as a cartoon in Fig. 5. The KDE model builds\na hypersphere centered at the data mean. The autoencoder builds some reconstruction manifold (here from top to bottom) and the outlier score is given as the distance to that manifold. Finally, the deep model learns a separating surface between the inliers and outliers, whose orientation is strongly affected by the learned feature map.\nWhile in this cartoon example all the data is correctly predicted, the explanations, here depicted as an orthogonal projection on the decision boundary, are strongly influenced by the structure of the model. Clever Hans predictions are highlighted in red and correspond to examples whose explanation deviates significantly from the ground-truth explanation.\nTo summarize, we have argued that flaws on the decision structure revealed by our explanation technique are mostly the consequence of model limitations and biases. This finding substantially differs from the study of Clever Hans in the context of supervised learning where it was found that such effect was rather induced by spurious correlations in the data [17]."
    },
    {
      "heading": "5 Improving Prediction with a Bag of Clever Hanses",
      "text": "Motivated by the structural weaknesses of individual anomaly detection models, we discuss a simple improvement consisting of bagging these individual detectors to arrive at a better and more robust prediction strategy. Several works on bagging anomaly detectors have delivered encouraging results [18, 23]. The bagged model we consider here computes\noBag(x) = 1\n3\n( oKDE(x) + oAuto(x) + oDeep(x) ) ,\nwhere the outlier scores of each individual model have been standardized over the training examples of the given class to have mean 0 and variance 1. The bagging can be understood as a soft max-pooling over outlier scores. It effectively implements a disjunction (i.e. logical \u2018OR\u2019) of all outlier areas of the individual models. A geometrical motivation for this strategy can be found in Figure 5, where the resulting area would closely match the ground-truth outlier area shown on the left.\nWe test the bagging approach on the MVTec dataset. Using the bagged outlier score obtained from averaging the standardized scores of individual models, we again compute the ROC detection score for each class. Because the bagged model simply adds an average pooling layer, it still fits in the 3-layer feature / distance / pooling architecture presented in Section 2, and we can therefore also compute the explanations and the Clever Hans score. ROC and Clever Hans scores averaged over all classes are plotted for each model in Figure 6 (left).\nLooking only at the ROC score, the bagged model appears to be outperformed by the deep model. However, a closer inspection reveals that the higher measured detection performance of the deep model comes along with stronger Clever Hans effects. Taking both factors into consideration, the bagged model ranks first among all four models. Figure 6 (right) shows\nexplanations produced by each model on the same images as in Fig. 4. We observe that although relatively far from the ground-truth, explanations for the bagged model comprise a broader range of feature types compared to individual models, hence, pixels used for the decision more frequently overlap with the truly anomalous pixels. This wider support for the detection task should consequently translate into a better generalizing model."
    },
    {
      "heading": "6 Conclusion",
      "text": "The \u2018Clever Hans\u2019 effect has been often observed in the context of supervised classifiers. In this work, we demonstrated that this effect also occurs in unsupervised learning, specifically, in anomaly detection. Through a newly contributed XAI procedure highlighting relevant features for each anomaly model and a way of quantifying the Clever Hans effect based on matching pixel-wise ground-truth annotations, our work has revealed the widespread occurrence of Clever Hans phenomena in anomaly detection models, additionally exhibiting a wealth of forms such effect can assume in practice. Furthermore, our analysis has revealed that the Clever Hans effect can be mainly attributed to the structure of the anomaly detection models rather than the data itself.\u2014In fact, the same could be said about the original \u2018Clever Hans\u2019 horse when faced with arithmetic calculations: Even if the horse would have seen many mathematical formulas along with their correct answer (including cases where the trainer was not there!), the horse would still be unable to learn the proper problem representation as he is structurally unable to do so. He would therefore invariably continue to predict as a \u2018Clever Hans\u2019.\nInterestingly, every anomaly detection model in our study exhibits Clever Hanses reasoning at least in some cases, and each of them does so in its own particular way. While our work warns against an unreflected use of anomaly detection models in practice, especially for safety-critical tasks, it also sheds a more optimistic note on the problem, specifically, we have demonstrated that a simple bagging approach combining the various Clever Hans models can reduce the Clever Hans effect and lead to sensibly improved results. Future work will aim to go beyond simple bagging to develop new and structurally less rigid models in order to avoid Clever Hans strategies and further improve generalization performance."
    },
    {
      "heading": "Acknowledgments",
      "text": "This work was funded by the German Federal Ministry of Education and Research (BMBF) as BIFOLD: Berlin Institute for the Foundations of Learning and Data (ref. 01IS18025A and ref. 01IS18037A) and ALICE III (01IS18049B), as well as by the German Research\nFoundation (DFG) as Math+: Berlin Mathematics Research Center (EXC 2046/1, project-ID: 390685689). This work was partly supported by the Institute for Information & Communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (No. 2017-0-00451, No. 2017-0-01779)."
    },
    {
      "heading": "A Details on LRP-Based Anomaly Explanation",
      "text": "In the following, we give some background on the layer-wise relevance propagation (LRP) [2] approach to explanation, and more specifically, details and justification for the procedure we use in the paper to explain anomalies. LRP is an explanation technique that leverages the internal structure of neural networks to ease the process of explanation. LRP operates by performing a purposely designed backward pass in the neural network from the output to the input. The backward pass can be derived from the framework of deep Taylor decomposition (DTD) [20]. The LRP method was later on extended to other models, e.g. kernel-based anomaly detection [14], where one first needs to perform a preliminary \u2018neuralization\u2019 step which transforms the model into an equivalent neural network. In the present work, we have generalized the approach to a broader family of anomaly detection models. The neural network equivalents of considered models are shown in Table 1.\nFigure 7 illustrates the LRP approach. The derivation of the backward pass with DTD is based on a relevance function Rk(a) and its Taylor expansion gives the messages Rj\u2190k to propagate backwards.\nBecause the function Rk(a) can be complex, it is typically substituted by a relevance model R\u0302k(a) = ak(a) \u00b7 ck with ck constant. Thus, in the following, we discuss for each layer type encountered in our anomaly detection models:\n1. For the analyzed relevance model, how to choose an appropriate root point a\u0303 at which to perform the Taylor expansion so that meaningful messages Rj\u2190k can be extracted.\n2. Whether the redistributed messages Rj\u2190k (or their aggregation Rj = \u2211 k Rj\u2190k) are\nstructured in a way that an appropriate relevance model can be built to iterate the propagation procedure one layer below.\nWe start the discussion with the pooling layers, and then continue with the distance layer and finally the feature layers of the deep network."
    },
    {
      "heading": "A.1 Propagation in Average Pooling Layers",
      "text": "Average pooling layers are defined as:\nak(a) = 1\nN N\u2211 j=1 aj\nwhere N is the number of neurons in the pool. We assume that the relevance Rk attributed to the output of the pool is appropriately described by the relevance model:\nR\u0302k(a) = ak(a) \u00b7 ck\n= ( 1 N N\u2211 j=1 aj ) \u00b7 ck\nwith ck constant. The relevance model can be expanded at the root point a\u0303 = 0, which gives us the first-order terms:\nRj\u2190k = ck N \u00b7 aj\ndefining how much of the relevance in Rk must be redistributed on the neurons of the layer below.\nAs a second step, we would like to verify that the lower-layer relevances are structured in a way that they support a suitable model for propagation in the layer below. Here, we note that\nRj\u2190k = aj \u00b7 ck/N\ufe38 \ufe37\ufe37 \ufe38 cj\nwhere cj can again be treated as constant."
    },
    {
      "heading": "A.2 Propagation in Negative Log-Sum-Exp Layers",
      "text": "We consider a negative log-sum-exp pooling\nak(a) = smin j\n\u03b3{aj}\nwhere we have used the notation smin\u03b3j {aj} = \u2212\u03b3\u22121 log \u2211N j=1 exp(\u2212\u03b3 aj), and where \u03b3 is the stiffness parameter. Like for average pooling, we assume the relevance to redistribute can be modeled as\nR\u0302k(a) = ak(a) \u00b7 ck\nwith ck constant. A first-order Taylor expansion at the root point a\u0303 = a \u2212 ak1 gives the first-order terms [14]:\nRj\u2190k = sargmin \u03b3 j {aj}Rk,\nwhere we have used the notation sargmin\u03b3j {aj} = exp(\u2212\u03b3 \u00b7 aj)/ \u2211 j\u2032 exp(\u2212\u03b3 \u00b7 aj\u2032). This expression can be further developed to let appear aj as a factor:\nRj\u2190k = aj sargmin \u03b3 j {aj} ck\ufe38 \ufe37\ufe37 \ufe38 pk + smin\u03b3j\u2032{aj\u2032 \u2212 aj} sargmin \u03b3 j {aj} ck\ufe38 \ufe37\ufe37 \ufe38 \u03b8k .\nWhen \u03b3 \u2192 \u221e (and assuming that elements in the pool have different values), the term pk becomes locally constant and the \u03b8k converges to zero (a proof is given in [14]). This limit result gives support for treating the two terms as constant and zero respectively when propagating to the lower layers, and therefore, a suitable relevance model can be built for the layer below."
    },
    {
      "heading": "A.3 Propagation in Squared Distance Layers",
      "text": "Distance layers considered in the main text have the form\nak(a) = \u2016a\u2212 \u00b5k\u20162\nAgain, we assume we can build a relevance model of the type R\u0302k(a) = ak(a) \u00b7 ck with ck constant, and we also treat \u00b5k to be constant. Here, the function is quadratic, hence, decomposition can only be achieved by performing a second-order Taylor expansion. Choosing the root point a\u0303 = \u00b5k yields the second-order terms [14]:\nRj\u2190k = ck (aj \u2212 \u00b5jk)2\nWhen each input feature contributes to several distances (e.g. the distances to the multiple training points in KDE), relevance scores can be aggregated as: Rj = \u2211 k Rj\u2190k. When there is only a single distance and this distance is w.r.t. the origin (as for the deep model considered in the main text), the equation above reduces to Rj \u2190 cka2j . In the case of the deep model, the relevance score needs to be further propagated through the multiple layers of features."
    },
    {
      "heading": "A.4 Transition from Distance to Feature Layers",
      "text": "The last layer of features in the deep model is a linear whitening layer, whose output neurons can be simply written as:\nak(a) = \u2211 j ajwjk\nRelevance coming from the distance layer above has the form Rk(a) = (ak(a))2ck with ck approximately locally constant. Hence, combining the two equations, we can build the relevance model\nR\u0302k(a) = (\u2211 j ajwjk )2 ck\nwith ck constant. However, because of the squaring operation, it is difficult to extract simple messages Rj\u2190k to redistribute to the layer below. Instead, we observe that the relevance model can be decomposed into an infinite sum of piecewise linear relevance models:\nR\u0302k\u03c4 (a) = max(0, \u2211 j ajwjk sign(ak)\u2212 \u03c4) \u00b7 2\u2206\u03c4 \u00b7 ck\nwith small intervals \u2206\u03c4 . These relevance models jointly sum to the original relevance model (i.e. \u2211 \u03c4\u2208{0,\u2206\u03c4,2\u2206\u03c4,... } R\u0302k\u03c4 (a) \u2248 R\u0302k(a)). For all of these models, there is a root point on the interval [0,a], and choosing a\u0303 very close to the root point but still on the activated domain gives the directional redistribution:\nRj\u2190k\u03c4 = ajwjk\u2211 j\u2032 aj\u2032wj\u2032k Rk\u03c4\nFinally, summing over \u03c4 gives:\nRj\u2190k = ajwjk\u2211 j\u2032 aj\u2032wj\u2032k Rk\nAs a last step, we need to verify that the scores Rj\u2190k, or more precisely, the aggregated score Rj = \u2211 k Rj\u2190k support an appropriate relevance model for the layer below. We observe that the relevance can be restructured as:\nRj = aj \u00b7 (\u2211\nk\nwjk\n(\u2211 j\u2032 aj\u2032wj\u2032k )2\u2211 j\u2032 aj\u2032wj\u2032k ck ) = aj \u00b7\n(\u2211 k wjk (\u2211 j\u2032 aj\u2032wj\u2032k ) ck ) = ajcj\nwhere we have aj appearing as a linear term, and cj a term that depends on aj but only through a nested sum involving many other activations, hence diluting the dependency."
    },
    {
      "heading": "A.5 Propagation in Feature Layers",
      "text": "Deep neural networks used in this work are composed of a succession of Linear/ReLU layers of the type\nak(a) = max ( 0, \u2211 j ajwjk ) Again, we assume the model is given as a multiple of the output activation, i.e.\nR\u0302k(a) = ak(a) \u00b7 ck = max ( 0, \u2211 j ajwjk ) \u00b7 ck.\nThe function is linear with a on the activated domain. A root point can be found on the segment\na\u0303 \u2208 {a\u2212 t \u00b7 a (1 + \u03b3 \u00b7 1wk 0) | t \u2208 R}\nwhere \u03b3 is a hyper-parameter between 0 and \u221e, and 1{\u00b7} is an indicator function applied element-wise. Performing a Taylor expansion at this root point, more exactly, very close to the root point but still in the activated domain, gives the first-order terms [21]:\nRj\u2190k = aj(wjk + \u03b3w + jk)\u2211\nj\u2032 aj(wj\u2032k + \u03b3w + j\u2032k)\nRk.\nThe higher the parameter \u03b3, the more preference is given to the positive contributions to support the explanation. Empirically, this also leads to explanations that are more robust to the high nonlinearity [21].\nFinally, we need to verify that a suitable relevance model can be built for the layer below. We observe that relevance in that layer can be written as\nRj = aj \u00b7 (\u2211\nk\n(wjk + \u03b3w + jk)\nmax(0, \u2211 j ajwjk)\u2211\nj\u2032 aj\u2032(wj\u2032k + \u03b3w + j\u2032k)\nck ) = ajcj\nwhere similarly to Section A.4, we can use the nested sums argument to justify the treatment of cj as constant in the relevance model of the layer below. For further details on how to propagate relevance in the various layers of a deep neural network, see [21]."
    },
    {
      "heading": "B Behavior of KDE for High-Dimensional Data",
      "text": "High-dimensional input spaces are subject to the effect of concentration of distances, where distances between different randomly sampled vectors become similar [4, 36]. Here, we show that under such effect, the KDE model for outlierness becomes approximately a simple distance-to-the-mean function, i.e.\no(x) \u221d \u2016x\u2212 \u00b5\u20162 + const.\nTo show the above, we decompose the distances as:\n\u2016x\u2212 xj\u20162 = d\u0304+ (\u2016x\u2212 xj\u20162 \u2212 d\u0304)\ni.e. a mean distance (over all data points (xj)Nj=1), and a deviation from the mean, which according to the concentration of distances effect becomes small. Taking the KDE outlier\nfunction, and making use of the approximations exp(t) = 1+t and log(N+t) = log(N)+t/N , which are valid when t is close to 0, we start from the KDE model and arrive at the stated reduction.\n\u03b3\u22121o(x) = \u2212 1 \u03b3 log \u2211 j exp(\u2212\u03b3\u2016x\u2212 xj\u20162)\n= d\u0304\u2212 1 \u03b3 log \u2211 j exp(\u2212\u03b3(\u2016x\u2212 xj\u20162 \u2212 d\u0304))\n\u2248 d\u0304\u2212 1 \u03b3 log \u2211 j (1\u2212 \u03b3(\u2016x\u2212 xj\u20162 \u2212 d\u0304))\n= d\u0304\u2212 1 \u03b3 log(N \u2212 \u03b3 \u2211 j (\u2016x\u2212 xj\u20162 \u2212 d\u0304))\n\u2248 d\u0304\u2212 1 \u03b3\n[ log(N)\u2212 1 N \u03b3 \u2211 j (\u2016x\u2212 xj\u20162 \u2212 d\u0304)) ]\n= \u2212 1 \u03b3 log(N) + 1 N \u2211 j \u2016x\u2212 xj\u20162\n= \u2212 1 \u03b3 log(N) + \u2016x\u2212 \u00b5\u20162 \u2212 \u2016\u00b5\u20162 + 1 N \u2211 j \u2016xj\u20162\n= \u2016x\u2212 \u00b5\u20162 + const."
    }
  ],
  "title": "The Clever Hans Effect in Anomaly Detection",
  "year": 2020
}
