{
  "abstractText": "The lack of interpretability is an inevitable problem when using neural network models in real applications. In this paper, a new explainable neural network called GAMI-Net, based on generalized additive models with structured interactions, is proposed to pursue a good balance between prediction accuracy and model interpretability. The GAMI-Net is a disentangled feedforward network with multiple additive subnetworks, where each subnetwork is designed for capturing either one main effect or one pairwise interaction effect. It takes into account three kinds of interpretability constraints, including a) sparsity constraint for selecting the most significant effects for parsimonious representations; b) heredity constraint such that a pairwise interaction could only be included when at least one of its parent effects exists; and c) marginal clarity constraint, in order to make the main and pairwise interaction effects mutually distinguishable. For model estimation, we develop an adaptive training algorithm that firstly fits the main effects to the responses, then fits the structured pairwise interactions to the residuals. Numerical experiments on both synthetic functions and real-world datasets show that the proposed explainable GAMI-Net enjoys superior interpretability while maintaining competitive prediction accuracy in comparison to the explainable boosting machine and other benchmark machine learning models.",
  "authors": [
    {
      "affiliations": [],
      "name": "Zebin Yang"
    },
    {
      "affiliations": [],
      "name": "Aijun Zhang"
    },
    {
      "affiliations": [],
      "name": "Agus Sudjianto"
    }
  ],
  "id": "SP:4edb2b90e46ccfee94511c03bd3a1bd59ab50af8",
  "references": [
    {
      "authors": [
        "Jacob Bien",
        "Jonathan Taylor",
        "Robert Tibshirani"
      ],
      "title": "A lasso for hierarchical interactions",
      "venue": "Annals of statistics,",
      "year": 2013
    },
    {
      "authors": [
        "Rich Caruana",
        "Yin Lou",
        "Johannes Gehrke",
        "Paul Koch",
        "Marc Sturm",
        "Noemie Elhadad"
      ],
      "title": "Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission",
      "venue": "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
      "year": 2015
    },
    {
      "authors": [
        "Hugh Chipman"
      ],
      "title": "Bayesian variable selection with related predictors",
      "venue": "Canadian Journal of Statistics,",
      "year": 1996
    },
    {
      "authors": [
        "Nam Hee Choi",
        "William Li",
        "Ji Zhu"
      ],
      "title": "Variable selection with the strong heredity constraint and its oracle property",
      "venue": "Journal of the American Statistical Association,",
      "year": 2010
    },
    {
      "authors": [
        "Mengnan Du",
        "Ninghao Liu",
        "Xia Hu"
      ],
      "title": "Techniques for interpretable machine learning",
      "venue": "arXiv preprint arXiv:1808.00033,",
      "year": 2018
    },
    {
      "authors": [
        "Jerome H Friedman"
      ],
      "title": "Greedy function approximation: a gradient boosting machine",
      "venue": "Annals of statistics,",
      "year": 2001
    },
    {
      "authors": [
        "Jerome H Friedman",
        "Werner Stuetzle"
      ],
      "title": "Projection pursuit regression",
      "venue": "J. Am. Stat. Assoc.,",
      "year": 1981
    },
    {
      "authors": [
        "Jerome H Friedman",
        "Bogdan E Popescu"
      ],
      "title": "Predictive learning via rule ensembles",
      "venue": "The Annals of Applied Statistics,",
      "year": 2008
    },
    {
      "authors": [
        "Leilani H Gilpin",
        "David Bau",
        "Ben Z Yuan",
        "Ayesha Bajwa",
        "Michael Specter",
        "Lalana Kagal"
      ],
      "title": "Explaining explanations: An overview of interpretability of machine learning",
      "venue": "IEEE 5th International Conference on data science and advanced analytics (DSAA),",
      "year": 2018
    },
    {
      "authors": [
        "Trevor Hastie",
        "Robert Tibshirani"
      ],
      "title": "Generalized Additive Models",
      "year": 1990
    },
    {
      "authors": [
        "Trevor Hastie",
        "Robert Tibshirani",
        "Jerome Friedman"
      ],
      "title": "The Elements of Statistical Learning: Data Mining, Inference, and Prediction",
      "year": 2009
    },
    {
      "authors": [
        "Giles Hooker"
      ],
      "title": "Discovering additive structure in black box functions",
      "venue": "In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining,",
      "year": 2004
    },
    {
      "authors": [
        "Kurt Hornik"
      ],
      "title": "Approximation capabilities of multilayer feedforward networks",
      "venue": "Neural networks,",
      "year": 1991
    },
    {
      "authors": [
        "Jeng-Neng Hwang",
        "Shyh-Rong Lay",
        "Martin Maechler",
        "R Douglas Martin",
        "Jim Schimert"
      ],
      "title": "Regression modeling in back-propagation and projection pursuit learning",
      "venue": "IEEE Trans. Neural Netw.,",
      "year": 1994
    },
    {
      "authors": [
        "V Roshan Joseph"
      ],
      "title": "A bayesian approach to the design and analysis of fractionated experiments",
      "year": 2006
    },
    {
      "authors": [
        "Diederik P Kingma",
        "Jimmy Ba"
      ],
      "title": "Adam: A method for stochastic optimization",
      "year": 2014
    },
    {
      "authors": [
        "Yin Lou",
        "Rich Caruana",
        "Johannes Gehrke"
      ],
      "title": "Intelligible models for classification and regression",
      "venue": "In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining,",
      "year": 2012
    },
    {
      "authors": [
        "Yin Lou",
        "Rich Caruana",
        "Johannes Gehrke",
        "Giles Hooker"
      ],
      "title": "Accurate intelligible models with pairwise interactions",
      "venue": "In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining,",
      "year": 2013
    },
    {
      "authors": [
        "Scott M Lundberg",
        "Su-In Lee"
      ],
      "title": "A unified approach to interpreting model predictions",
      "venue": "In Advances in Neural Information Processing Systems,",
      "year": 2017
    },
    {
      "authors": [
        "Scott M Lundberg",
        "Gabriel Erion",
        "Hugh Chen",
        "Alex DeGrave",
        "Jordan M Prutkin",
        "Bala Nair",
        "Ronit Katz",
        "Jonathan Himmelfarb",
        "Nisha Bansal",
        "Su-In Lee"
      ],
      "title": "From local explanations to global understanding with explainable ai for trees",
      "venue": "Nature machine intelligence,",
      "year": 2020
    },
    {
      "authors": [
        "Christoph Molnar"
      ],
      "title": "Interpretable machine learning: A guide for making black box models explainable",
      "venue": "E-book at\u00a1 https://christophm. github. io/interpretable-ml-book/\u00bf, version dated,",
      "year": 2018
    },
    {
      "authors": [
        "W James Murdoch",
        "Chandan Singh",
        "Karl Kumbier",
        "Reza Abbasi-Asl",
        "Bin Yu"
      ],
      "title": "Interpretable machine learning: definitions, methods, and applications",
      "venue": "PNAS (accepted),",
      "year": 2019
    },
    {
      "authors": [
        "John A Nelder"
      ],
      "title": "The selection of terms in response-surface models how strong is the weakheredity principle",
      "venue": "The American Statistician,",
      "year": 1998
    },
    {
      "authors": [
        "Harsha Nori",
        "Samuel Jenkins",
        "Paul Koch",
        "Rich Caruana"
      ],
      "title": "Interpretml: A unified framework for machine learning interpretability",
      "venue": "arXiv preprint arXiv:1909.09223,",
      "year": 2019
    },
    {
      "authors": [
        "Pradeep Ravikumar",
        "John Lafferty",
        "Han Liu",
        "Larry Wasserman"
      ],
      "title": "Sparse additive models",
      "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),",
      "year": 2009
    },
    {
      "authors": [
        "Marco Tulio Ribeiro",
        "Sameer Singh",
        "Carlos Guestrin"
      ],
      "title": "Why should i trust you?: Explaining the predictions of any classifier",
      "venue": "In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining,",
      "year": 2016
    },
    {
      "authors": [
        "Wojciech Samek",
        "Alexander Binder",
        "Gr\u00e9goire Montavon",
        "Sebastian Lapuschkin",
        "KlausRobert M\u00fcller"
      ],
      "title": "Evaluating the visualization of what a deep neural network has learned",
      "venue": "IEEE transactions on neural networks and learning systems,",
      "year": 2016
    },
    {
      "authors": [
        "Andrew M Saxe",
        "James L McClelland",
        "Surya Ganguli"
      ],
      "title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks",
      "venue": "arXiv preprint arXiv:1312.6120,",
      "year": 2013
    },
    {
      "authors": [
        "Daria Sorokina",
        "Rich Caruana",
        "Mirek Riedewald"
      ],
      "title": "Additive groves of regression trees",
      "venue": "In European Conference on Machine Learning,",
      "year": 2007
    },
    {
      "authors": [
        "Jiawei Su",
        "Danilo Vasconcellos Vargas",
        "Kouichi Sakurai"
      ],
      "title": "One pixel attack for fooling deep neural networks",
      "venue": "IEEE Transactions on Evolutionary Computation,",
      "year": 2019
    },
    {
      "authors": [
        "Sanli Tang",
        "Xiaolin Huang",
        "Mingjian Chen",
        "Chengjin Sun",
        "Jie Yang"
      ],
      "title": "Adversarial attack type i: Cheat classifiers by significant changes",
      "venue": "IEEE transactions on pattern analysis and machine intelligence,",
      "year": 2019
    },
    {
      "authors": [
        "Michael Tsang",
        "Dehua Cheng",
        "Yan Liu"
      ],
      "title": "Detecting statistical interactions from neural network weights",
      "venue": "In International Conference on Learning Representations,",
      "year": 2018
    },
    {
      "authors": [
        "Joel Vaughan",
        "Agus Sudjianto",
        "Erind Brahimi",
        "Jie Chen",
        "Vijayan N Nair"
      ],
      "title": "Explainable neural networks based on additive index models",
      "venue": "The RMA Journal,",
      "year": 2018
    },
    {
      "authors": [
        "Zebin Yang",
        "Aijun Zhang",
        "Agus Sudjianto"
      ],
      "title": "Enhancing explainability of neural networks through architecture constraints",
      "venue": "arXiv preprint arXiv:1901.03838,",
      "year": 2019
    },
    {
      "authors": [
        "Ming Yuan",
        "V Roshan Joseph",
        "Hui Zou"
      ],
      "title": "Structured variable selection and estimation",
      "venue": "The Annals of Applied Statistics,",
      "year": 2009
    },
    {
      "authors": [
        "Xiaoyong Yuan",
        "Pan He",
        "Qile Zhu",
        "Xiaolin Li"
      ],
      "title": "Adversarial examples: Attacks and defenses for deep learning",
      "venue": "IEEE transactions on neural networks and learning systems,",
      "year": 2019
    }
  ],
  "sections": [
    {
      "text": "The lack of interpretability is an inevitable problem when using neural network models in real applications. In this paper, a new explainable neural network called GAMI-Net, based on generalized additive models with structured interactions, is proposed to pursue a good balance between prediction accuracy and model interpretability. The GAMI-Net is a disentangled feedforward network with multiple additive subnetworks, where each subnetwork is designed for capturing either one main effect or one pairwise interaction effect. It takes into account three kinds of interpretability constraints, including a) sparsity constraint for selecting the most significant effects for parsimonious representations; b) heredity constraint such that a pairwise interaction could only be included when at least one of its parent effects exists; and c) marginal clarity constraint, in order to make the main and pairwise interaction effects mutually distinguishable. For model estimation, we develop an adaptive training algorithm that firstly fits the main effects to the responses, then fits the structured pairwise interactions to the residuals. Numerical experiments on both synthetic functions and real-world datasets show that the proposed explainable GAMI-Net enjoys superior interpretability while maintaining competitive prediction accuracy in comparison to the explainable boosting machine and other benchmark machine learning models.\nKeywords: Explainable neural network, generalized additive model, pairwise interaction, model interpretability.\nar X\niv :2\n00 3.\n07 13\n2v 1\n[ st\nat .M"
    },
    {
      "heading": "1 Introduction",
      "text": "Deep learning is one of the leading techniques in artificial intelligence (AI). Despite its great success, a fundamental and unsolved problem is that the working mechanism of deep neural networks is hardly understandable. Without sufficient interpretability, it would be dangerous to apply these AI systems in real-life applications. A well-trained deep neural network is known to have an accurate predictive performance on data at hand. However, the model may perform abnormally as the data is slightly changed, as its inner decision-making process is unknown. Some recent examples can be referred to as the adversarial attacks, where a convolutional neural network can be easily fooled by its attackers (Yuan et al., 2019, Tang et al., 2019, Su et al., 2019).\nInterpretable machine learning is an emerging research topic that tries to solve the aforementioned problem and open up the black-box of complicated machine learning algorithms (Du et al., 2018, Molnar et al., 2018, Gilpin et al., 2018, Murdoch et al., 2019). Two categories of interpretability are generally investigated, i.e., post-hoc interpretability and intrinsic interpretability. In the post-hoc analysis, a fitted model is interpreted using external tools. Examples of this category include the partial dependence plot (PDP; Friedman, 2001), local interpretable model-agnostic explanations (LIME; Ribeiro et al., 2016), SHapley Additive exPlanations (SHAP; Lundberg and Lee, 2017, Lundberg et al., 2020) and heatmap visualization of deep neural networks (Samek et al., 2016). In contrast, intrinsic interpretability aims at making the model intrinsically interpretable. A lot of statistical models belong to this category, e.g., the generalized linear model, decision tree and na\u0308\u0131ve Bayes classifier. In this paper, we limit our focus on the second type of interpretability.\nThe generalized additive index model (GAIM) is such an intrinsically interpretable model when proper constraints are imposed. It was first proposed by Friedman and Stuetzle (1981) in the name of projection pursuit regression. The GAIM is shown to have close connections with feedforward neural networks (Hwang et al., 1994), which has universal approximation capability as the number of hidden nodes is sufficiently large (Hastie et al., 2009). The functional relationship between predictors x \u2208 Rp and the response y is represented by\ng(E(y|x)) = \u00b5+ M\u2211 i=1 hi(w T i x), (1)\nwhere M is the number of additive components, \u00b5 is the intercept, wi \u2208 Rp are the projection indices for each additive component i = 1, . . . ,M , and hi are the corresponding nonparametric ridge functions. The GAIM has been reformulated using neural network architecture, which is called explainable neural network (xNN; Vaughan et al., 2018). In xNN,\na fully-connected multi-layer perceptron is disentangled into many additive and independent subnetworks; each subnetwork represents a shape function which can be easily visualized and interpreted. Recently, the interpretability of xNN is further enhanced by inducing sparsity, orthogonality and smoothness constraints, see details in (Yang et al., 2019).\nDespite each of the additive component in GAIM is simple, it is still hard to interpret the projected data zi = w T i x if without adequate domain knowledge. Such a problem can be solved by reducing the GAIM to the generalized additive model (GAM; Hastie and Tibshirani, 1990), where each predictor is directly modeled without projection. In practice, the GAM is more preferable regarding interpretability while it also has sacrificed prediction performance as feature interactions are not modeled.\nIn this paper, a novel explainable neural network is proposed by enhancing the GAM with pairwise interactions, and we call it GAMI-Net. Each main effect / pairwise interaction is modeled by a fully-connected subnetwork consisting of one / two input nodes, multiple hidden layers, and one output node. These independent subnetworks are then additively combined to form the final output. The subnetwork-represented main effects and pairwise interactions can be easily interpreted, which is a key factor for interpretable machine learning. Moreover, three additional interpretability constraints are considered, including,\n\u2022 Sparsity. Model parsimony is an essential factor for an interpretable model. In GAMI-Net, only non-trivial main effects and pairwise interactions are included.\n\u2022 Heredity. Heredity principle is introduced to make the model structurally interpretable. That is, we ignore pairwise interactions whose parent effects are pruned.\n\u2022 Marginal Clarity. The marginal clarity constraint is employed to avoid potential confusion between the main effects and pairwise interactions.\nAn adaptive training algorithm is introduced for model estimation. In the first stage, the main effect subnetworks are trained and pruned (subject to sparsity consideration). In the second stage, all the main effects subnetworks are fixed and pairwise interactions are fitted (subject to both heredity and sparsity constraints). Moreover, the marginal clarity constraint is achieved by adaptive calibration during training.\nNote the GAMI-Net formulation is also used in the explainable boosting machine (EBM; Lou\net al., 2012, 2013, Caruana et al., 2015), and a detailed comparison between GAMI-Net and EBM is provided in Section 4. Numerical experiments on both synthetic functions and real-world datasets are conducted. The results reveal that GAMI-Net has superior interpretability performance as compared to EBM, with easily interpretable main effects and\npairwise interactions. Moreover, it is shown GAMI-Net is competitive regarding predictive performance, which makes it a promising tool for interpretable machine learning.\nThis paper is organized as follows. Section 2 presents the proposed GAMI-Net model. Two synthetic functions and several real-world datasets are used to test the GAMI-Net performance in Section 4. Finally, Section 5 concludes the paper and remarks on future work."
    },
    {
      "heading": "2 GAMI-Net Methodology",
      "text": "This section introduces the proposed GAMI-Net architecture, the imposed interpretability considerations, and several computation aspects."
    },
    {
      "heading": "2.1 Network Architecture",
      "text": "In GAMI-Net, a complex functional relationship is formulated via its lower-order representations, including both main effects and pairwise interactions, as follows,\ng(E(y|x)) = \u00b5+ \u2211 i\u2208S1 hi(xi) + \u2211 (i,j)\u2208S2 fij(xi, xj), (2)\nwhere S1, S2 are the set of active main effects and pairwise interactions, respectively, subject to sparsity constraint. Accordingly, the network architecture that formulates (2) is presented in Fig. 1. It consists of two modules, i.e., the main effects module and the pairwise interactions module. Each main effects subnetwork consists of one input node, multiple hidden layers and one output node (for capturing hi(xi)); while the architecture of interaction subnetworks (for capturing fij(xi, xj)) is slightly different, as there exist two nodes in the input layer.\nThe main effect subnetwork fits a curve, while the interaction subnetwork approximates a surface. According to the universal approximation theorem (Hornik, 1991), a single-hiddenlayer feedforward neural network can be a universal approximator with a suitable choice of hidden nodes. Such subnetworks are flexible enough to capture any form of functions upon proper network configuration (Hornik, 1991). For categorical variables, the multiple hidden layers can be changed to many bias nodes, where each node captures the intercept effect of a corresponding dummy variable.\nThe architecture is also designed to represent the three interpretability constraints. Each of the subnetwork output is processed according to the marginal clarity constraint, and then\nlinearly combined to get the final output (plus an intercept node). Also, the output layer weights are represented using dashed lines, due to the sparsity and heredity constraints. Only the most important effects can be selected, while the negligible ones are removed from the model. See the following section for the details of the imposed interpretability constraints."
    },
    {
      "heading": "2.2 Interpretability Consideration",
      "text": "When modeling main and higher-order effects, three rules are considered to be essential, i.e., hierarchical, sparsity and heredity principles. The first hierarchical principle states that lower-order effects are generally more important than higher-order effects, and therefore, only main effects and pairwise interactions are considered.\nFor pursuing model interpretability, GAMI-Net is developed by considering the above\nthree principles. In specific, only main effects and pairwise interactions are captured. Even Higher-order interactions may help improve the prediction performance, they are not included in GAMI-Net. Sparsity and heredity constraints are introduced to pursue better interpretability. Furthermore, to make different effects distinguishable, marginal clarity constraint is employed. And here we elaborate on why and how the three interpretability constraints (sparsity, heredity, and marginal clarity) are imposed."
    },
    {
      "heading": "2.2.1 Sparsity Constraint",
      "text": "The sparsity principle is a commonly used rule which is also an essential building block for model interpretability. In practice, only a few important effects are significant and the else trivial effects should be pruned. As the number of predictors increases, the estimated model in (2) may contain many negligible effects that are hard to interpret. Therefore, it is critical to introduce the sparsity constraint to induce model parsimony.\nThe contribution of each main effect and pairwise interaction can be quantified by the variation it explains, i.e., ||hi||2. In this paper, we select the top-s1 main effects, such that S1 is the sorted indices of the top-ranked main effects (in descending order). As in the sparse GAM (Ravikumar et al., 2009), the unimportant main effect functions will be enforced to zero and only a few active main effects will be included in the final model.\nThe sparsity of pairwise interactions can also be controlled by selecting the top-s2 pairwise interactions according to ||fij||2. Similarly, we use S2 to denote the indices of the top-ranked pairwise interactions (also in descending order)."
    },
    {
      "heading": "2.2.2 Heredity Constraint",
      "text": "It is possible that an interaction component being selected while its parent effects are not. This is due to the lack of heredity and will add difficulty for model interpretation. The heredity constraint enforces the hierarchical structure between main effects and interactions (Nelder, 1998). It implies that an interaction component is less likely to be important if none of its parent effects are active. There exist two kinds of heredity constraints, i.e., the weak heredity and the strong heredity. The former requires that a pairwise interaction can only be included when at least one of its parent effects is active:\n||fij||2 6= 0\u21d2 ||hi||2 + ||hj||2 6= 0. (3)\nIn contrast, the strong heredity principle requires that both of its parent effects are active:\n||fij||2 6= 0\u21d2 ||hi||2 \u00d7 ||hj||2 6= 0. (4)\nBy introducing the heredity constraint, a model will become structurally understandable with enhanced interpretability (Chipman, 1996, Joseph, 2006). The heredity constraint has been widely used in variable selection literature, for instance, the hierarchical lasso (Bien et al., 2013), which introduced hierarchy restriction to lasso for selecting important main effects and interactions; more related literature can be referred to Choi et al. (2010) and Yuan et al. (2009). In this paper, we simply consider the weak heredity constraint."
    },
    {
      "heading": "2.2.3 Marginal Clarity",
      "text": "Without any constraints, the main effects can be easily absorbed by its child interactions and vice versa. For instance, a main effect and corresponding child pairwise interactions can be mutually absorbed, which leads to multiple representations. The existence of multiple representations will make the model estimates unstable and lead to further confusion. Therefore, we introduce the marginal clarity constraint to make the model more identifiable. For each main effect, its overall mean is enforced to zero. Meanwhile, the marginal means of each pairwise interaction are also constrained to zero, i.e.,\nf iij(xi) = \u222b fij (xi, xj) dxj = 0,\nf jij(xj) = \u222b fij (xi, xj) dxi = 0.\n(5)\nThe above integrations can be empirically approximated using a large number of evenly distributed grid points over the domain, and we will discuss it in the next section."
    },
    {
      "heading": "2.3 Computational Aspects",
      "text": "An adaptive training algorithm is introduced to sequentially estimate the main effects and pairwise interactions in GAMI-Net, which can be depicted in the following two stages."
    },
    {
      "heading": "2.3.1 Training Main Effects",
      "text": "In the first step, all the main effect subnetworks are simultaneously estimated while the pairwise interactions are frozen to zero. The trainable parameters in the network are updated by mini-batch gradient descent with adaptive learning rates determined by the Adam optimizer (Kingma and Ba, 2014), which is easily scalable to very big datasets. For each main effect subnetwork, a large number of equally spaced grid points {ak}Kik=1,min {xi} \u2264 a1 < . . . < aKi \u2264 max {xi} within the feature space are scattered to approximate the overall mean of each main effect subnetwork, i.e., 1\nKi\n\u2211Ki k=1 h\u0302i(ak) for each i = 1, 2, . . . , p. These means are\nthen used for centering the corresponding subnetwork outputs, to satisfy the marginal clarity constraint. The symbol Ki denotes the number of grid points. When the corresponding variable xi is continuous, Ki can be set to a relatively large integer; while as it is categorical variable, the value of Ki should be set to the number of unique classes in variable xi.\nThe training step will stop as the maximum training epochs are reached or certain early stopping criterion is met. All the fitted main effect subnetworks are then pruned according to the sparsity constraint, and the negligible effects are accordingly removed. Empirically, we can rank all the subnetworks according to their functional variation ||hi||2, and select the top-ranked subnetworks according to the sparsity constraint defined in. Finally, all the remaining subnetworks are re-trained to reduce bias."
    },
    {
      "heading": "2.3.2 Training Interactions",
      "text": "As the main effects are all captured, the subnetworks of pairwise interactions are then fitted. However, as there exist C2p possible pairwise interactions, it will be extremely time-consuming if all of them are used. To reduce the computation complexity, a filtering procedure is additionally introduced to remove the trivial pairwise interactions which are less likely to be significant. There exist many interaction detection methods in literature, for instance, the ANOVA test, additive Grove (Sorokina et al., 2007), RuleFit (Friedman et al., 2008), hierarchical lasso (Bien et al., 2013), and the neural network-based interaction detection (Tsang et al., 2018).\nIn addition to the interaction detection algorithms, some of the pairwise interactions should not be included in the final model due to the heredity constraint. In this paper, we incorporate the heredity constraint with the interaction detection algorithm in (Lou et al., 2013) where the pairwise interactions can be efficiently ranked via shallow tree-like models. As shown in Algorithm 1, the modified method select the top-N pairwise interactions which can largely reduce computation burden.\nThe prediction residuals in stage 1 are used to fit the selected pairwise interactions via the Adam optimizer. To satisfy the marginal clarity constraint, many equally spaced 2-D grid points are generated as the input of each interaction subnetwork, and the marginal means are then calculated for centering the output of each pairwise interaction subnetwork\nf\u0302 iij(ai) \u2248 1\nKj Kj\u2211 k=1 f\u0302ij(ai, bk),\nf\u0302 jij(bj) \u2248 1\nKi Ki\u2211 k=1 f\u0302ij(ak, bj),\n(6)\nAlgorithm 1 Pairwise Interaction Filtering Require: Training data, S1 (Active main effects) and N (Maximum Number of pairwise\ninteractions).\n1: Calculate the prediction residual of main effects. 2: for Each j 6= i, i \u2208 S1 or j \u2208 S1 do 3: Evaluate the strength of interaction (i, j) by building shallow tree-like model. 4: a) Find one cut ci on xi and greedily search two cuts on xj that are above and below\nci;\n5: b) Find one cut cj on xj and greedily search two cuts on xi that are above and below\ncj;\n6: Set the strength to the minimal error w.r.t the residual. 7: end for 8: Rank all the evaluated pairwise interactions and obtain the best N pairwise interactions.\nAlgorithm 2 GAMI-Net Training Algorithm Require: Training data, s1, s2 (Sparsity), N (Maximum Number of pairwise interactions).\n1: Train all the main effect subnetworks. 2: Rank and select the top-s1 fitted main effect subnetworks according to their magnitudes. 3: Fine tune the selected main effect subnetworks. 4: Filter the pairwise interaction via Algorithm 1. 5: Train the pairwise interaction subnetworks. 6: Rank and select the top-s2 pairwise interaction subnetworks according to their magni-\ntudes.\n7: Fine tune the selected interaction subnetworks.\nwhere {ak}Kik=1 and {bk} Kj k=1 are ordered grid points within the feature spaces of xi and xj, respectively. According to the sparsity constraint, the trivial pairwise interaction effects will be pruned with the top-s2 pairwise interactions left. Finally, the selected interactions will be fine-tuned for some epochs using the Adam optimizer. In summary, the training algorithm of GAMI-Net is presented in Algorithm 2."
    },
    {
      "heading": "2.4 Hyperparameters",
      "text": "The GAMI-Net is configured with the following empirical settings. The maximal number of pairwise interactions is set to N = 20. For each main effect subnetwork and interaction\nsubnetwork, the hidden layer structure is configured to [20, 10] with hyperbolic tangent nodes. To ensure marginal clarity, the number of grid size is set to 41 for all numerical variables or the number of unique classes for each categorical variable.\nAll the network connecting weights are initialized with the Gaussian Orthogonal initializer (Saxe et al., 2013). The initial learning rate of Adam optimizer is set to 0.001. The training epochs for main effects and pairwise interactions are both set to 2000 and the corresponding fine-tuning epochs are fixed to 50. The mini-batch sample size is determined according to the sample size of different datasets. A 20% validation set is split for early stopping, and the early stopping threshold is set to 100 epochs.\nThe sparsity hyperparameters (s1, s2) are important for the performance of GAMI-Net. To avoid the additional computational burden of hyperparameter tuning, we use the validation loss to greedily select the optimal (s1, s2). We start with an empty model. The most important main effect (with the highest IR) is first added and we evaluate its validation loss. Next, the second important main effect is sequentially added, followed by all the other main effects. The optimal can be set to the one that minimizes the validation loss.\nAlternatively, to pursue a parsimonious model, we choose a smaller s1 with a slight sacrifice on the validation performance. A similar procedure can be applied to determining s2 of pairwise interaction. In practice, the tolerance is set to 1% of the minimal validation error. Note such a procedure is performing a greedy search. When the computational burden is not a major concern, we may jointly optimize (s1, s2) by testing all the possible combinations."
    },
    {
      "heading": "3 Interpretability of GAMI-Net",
      "text": "The proposed GAMI-Net is intrinsically interpretable regarding the following aspects."
    },
    {
      "heading": "3.1 Importance Ratio (IR)",
      "text": "As the GAMI-Net is estimated, we can inspect the contribution of each independent variable to the overall prediction. The IR of each main effect can be quantitatively measured by\nIR(i) = ||hi||2/T, (7)\nwhere T = \u2211 i\u2208S1 ||hi||2 + \u2211\n(i,j)\u2208S2 ||fij||2. Similarly, the IR of pairwise interactions can be measured by\nIR(i, j) = ||fij||2/T. (8)\nIt can be seen that the sum of all IR equals to one. In practice, we can sort the variables according to the IR in descending order, and the ones with large IR are more important."
    },
    {
      "heading": "3.2 Global Interpretation",
      "text": "In addition to measuring the importance of each estimated effect, we can further inspect the relationship between one/two independent variables and the response via visualizing the fitted shape function. Unlike the post-hoc diagnostic tool PDP (Friedman, 2001), such relationships can be directly obtained from GAMI-Net and there is no need to worry about correlated predictors. For example, the univariate plot for each predictor reveals the inputoutput relationship, which can be linear, convex, monotonic or other nonlinear forms. The bivariate heatmap for each pairwise interaction can similarly show the joint effects of two predictors. For categorical variables, the main effects can be visualized using a bar chart; and the corresponding interactions can be accordingly visualized."
    },
    {
      "heading": "3.3 Local Interpretation",
      "text": "In addition to the global interpretation, the GAMI-Net can also be locally explained, leading to a transparent decision-making system. Given a sample x, the model not only outputs the final decision but also how it is obtained. The additive components of fi(xi) (for each i \u2208 S1) and hij(xi, xj) (for each (i, j) \u2208 S2) can be provided. Such information can be quite useful for understanding the decision procedure."
    },
    {
      "heading": "3.4 Discussion",
      "text": "The proposed GAMI-Net is closely related to the explainable boosting machine (EBM; Lou et al., 2013) as both of them are based on (2). The EBM has received much attention and it is publicly available in the python package \u201cinterpret\u201d by Microsoft. In EBM, each main effect / pairwise interaction is estimated via gradient boosted shallow trees, which is modified from the standard gradient boosting model (Friedman, 2001). In practice, the EBM is shown to have a strong approximation ability as compared to black-box models like random forest and neural networks, while the fitted model is interpretable in terms of the univariate (main effects) and bivariate (interactions) plots.\nThe GAMI-Net makes a lot of non-trivial improvements over the EBM. First of all, as tree-based models are used in EBM, the estimated shape functions are all piece-wise constant. It is likely to observe unexpected jumps in the fitted model that is hard to explain.\nWhen there exist outliers or noise, such a problem may become worse. Second, the EBM does not consider further constraints for enhanced interpretability. In contrast, the proposed GAMI-Net is likely to be more interpretable as several constraints are imposed. For instance, without marginal clarity constraint, the main effect and its corresponding child pairwise interactions may be mixed and mutually absorbed, which leads to the identification problem. The main differences between GAMI-Net and EBM are summarized in Table 1. Note the GAMI-Net fitted shape functions for numerical variables can be both continuous (for continuous activations) or piecewise constant (for discrete activations), while we only focus on the continuous version which tends to have better interpretability."
    },
    {
      "heading": "4 Numerical Experiments",
      "text": "In this section, the proposed GAMI-Net is tested on two synthetic functions and several real-world datasets."
    },
    {
      "heading": "4.1 Experimental Setup",
      "text": "Two versions of EBM are tested, including EBM with pairwise interactions and EBM-GAM without pairwise interactions. Several classic benchmarks are also introduced, including the generalized linear models (GLM), multi-layer perceptron (MLP) and random forest (RF). In specific, the least absolute shrinkage and selection operator (Lasso) is used for regression and logistic regression (LogR) is employed for classification. All the compared models are grouped into 2 categories, i.e., intrinsically interpretable models (GAMI-Net, EBM, EBMGAM, and GLM) and black-box models (MLP and RF).\nEach dataset is split into training, validation and test sets. In EBM and EBM-GAM, the interactions numbers are set to 20 and 0, respectively. All the other hyperparameters of EBM and EBM-GAM are set to the default values. For both Lasso and Logistic regression (with `1 shrinkage), the regularization strength is tuned within {10\u22122, 10\u22121, 100, 101, 102}. We use a two-hidden-layer MLP with [40, 20] hyperbolic tangent nodes. The RF is built with 100 base trees, and the maximum tree depth is selected from {3, 4, 5, 6, 7, 8}. All the hyperparameters are selected based on the prediction performance of a 20% hold-out validation set. For each dataset, we evaluate the test set root mean squares error (RMSE) for regression tasks and the area under the curve (AUC) for binary classification tasks.\nAll the experiments are implemented using the Python environment on a server with multiple Intel Xeon 2.60G CPUs. The proposed GAM-Net is implemented using the TensorFlow 2.0 platform. The source code has been packaged in a Python package \u201cgaminet\u201d, which can be found in the link https://github.com/ZebinYang/gaminet. The EBM is implemented based on the interpret package (Nori et al., 2019). All the other benchmark models are implemented using the Scikit-learn package."
    },
    {
      "heading": "4.2 Simulation Study",
      "text": "We consider two synthetic functions in the regression setting. Each of them has ten independent variables that are generated from the uniform distribution, and the response variables are calculated via complicated nonlinear transformations of the predictors plus a noise term generated from the standard normal distribution. For each scenario, we consider four sample sizes, i.e., n = {500, 1000, 2000, 5000}. Scenario 1. In this case, both main effects and pairwise interactions are included, as follows,\ny =8 ( x1 \u2212 1\n2\n)2 + 1\n10 e(\u22128x2+4) + 3 sin (2\u03c0x3x4)+\n5e\u22122(2x5\u22121) 2\u2212 1 2 [15x6+12(2x5\u22121)2\u221213] 2 + \u03b5.\n(9)\nIt can be seen that Scenario 1 exactly follows the model formulation defined in (2). All the ten variables are uniformly distributed in [0, 1], and the last four variables actually have no influence on the response variable.\nScenario 2. The second case is a widely used example (Hooker, 2004). In addition to main effects and pairwise interactions, higher-order interactions are considered in this\nscenario, as follows,\ny =\u03c0x1x2 \u221a\n2x3 \u2212 sin\u22121 (x4) + log (x3 + x5)\u2212 x9 x10 \u221a x7 x8 \u2212 x2x7 + \u03b5,\n(10)\nwhere x4, x5, x8, x10 are uniformly distributed in [0.6, 1] and all the other variables are uniformly distributed in [0, 1].\nTo illustrate the training procedure of GAMI-Net, Fig. 2 presents its training and validation loss of Scenario 1. There exist three arrows in each figure, which corresponds to the operations described in Algorithm 2. It can be observed that the losses jump significantly as pairwise interactions are added to the network, which shows the superiority of GAMI over GAM. The two pruning procedures also contribute a lot to the prediction performance. Although the training losses increase, the corresponding validation loss decrease after pruning. This phenomenon indicates that the pruning procedure can help prevent overfitting and the deleted effects are more likely to be noises.\nIn addition, the validation loss for determining optimal (s1, s2) is visualized in Fig. 3. The x-axis denotes the number of included main effects / pairwise interactions. In particular, zero represents the model when only the intercept is included. The results show that s1 = 6 and s2 = 2, which is exactly the same as the actual function.\nFig. 4 draws the ground truth, the global interpretation of GAMI-Net and EBM of Scenario 1 with n = 5000. Note in the original formulation of Scenario 1, it his assumed to have 2 active main effects {x1, x2} and 2 active interaction effects {(x3, x4), (x5, x6)}. We can equivalently separate marginal effects from the interactions so that the active main effects also include x3 \u2212 x6. The main effects are always first presented and followed by pairwise interactions. The same kind of effect is ranked in the descending order of IR (in brackets). It can be observed that all the six main effects and two pairwise interactions are successfully captured by GAMI-Net, which is close to that of the ground truth.\nSince EBM does not have a pruning procedure, the final model includes 10 main effects and 20 pairwise interactions. To make a valid comparison, we also draw its first 6 main effects and first 2 pairwise interactions. The EBM can also approximately capture the shape of these important effects. However, due to the use of gradient boosting trees, the estimated shape functions are all piecewise constant and the existence of sudden jumps makes it hard to interpret. Second, we also calculate IR for each effect in EBM using the same method as in GAMI-Net. The result of EBM is shown to have a larger bias as compared to the actual model. For example, the interaction x5, x6 is underestimated and the overall IR captured by these active effects are smaller than 80%. That means the noisy effects take more than 10% of the contribution.\nTable 2 reports the averaged test set RMSE and standard deviation (10 repetitions) of different models on Scenarios 1 to 2, respectively. For each dataset, the best interpretable and black-box models are both highlighted in bold. It can be observed that the GAMI-Net outperforms all the compared models including both interpretable and black-box models. In scenario 1, it is not surprising that GAMI-Net outperforms the black-box models including MLP and RF since its settings satisfy the model assumption of GAMI-Net. In scenario 2, even the true function has higher-order interactions, GAMI-Net still finds a good approximation."
    },
    {
      "heading": "4.3 Real Data Applications",
      "text": "We further consider 8 regression and 8 binary classification real-world datasets. Most of the datasets are obtained from the UCI machine learning repository except for the California\nHousing dataset, which is originated from StatLib. The datasets come from different domains, e.g., medical care, finance, etc. The sample sizes range from 306 (Haberman-survival) to 45211 (Bank) and the number of features also varies from 3 (Titanic) to 30 (Breast-cancerwisc-diag). The detailed information of each dataset is presented in Tables 3 - 3, where the corresponding test set performance of each compared method is report.\nGenerally speaking, the GAMI-Net shows comparative predictive performance to that of EBM and other benchmark models. The GAMI-Net is more likely to have better prediction performance when the actual shape functions are continuous and smooth. In contrast, the EBM will outperform as the shape functions are piece-wise constant. In practice, it is hard to say which one will perform better. Two datasets are picked to demonstrate the interpretability of GAMI-Net."
    },
    {
      "heading": "4.3.1 Bike Sharing Dataset",
      "text": "The Bike Sharing dataset is a regression dataset (https://archive.ics.uci.edu/ml/datasets/ bike+sharing+dataset). Each sample records the basic environmental information including 8 categorical variables, e.g., the season and the weather situation; and 4 numerical variables, e.g., the temperature and wind speed. The target is to predict the hourly count of rental bikes in the Capital bike share system between 2011 and 2012.\nThe global interpretation of the Bike Sharing dataset is shown in Fig. 5a. In addition to the univariate plots and bivariate heatmaps, the density of corresponding variables are also provided. In total, 10 (out of 12) main effects and 19 (out of 20) pairwise interactions are shown to have a significant influence on the target. Due to the limit of page size, we only present the top 8 main effects and top 4 pairwise interaction. The most important variable is \u201chr\u201d (hour, ranges from 0 to 23) with IR equals to 15.7%. It can be observed that there exist two peaks of bike sharing around 8 AM and 5 PM, which corresponds to the rush hour in a day. The second important variable is call \u201catemp\u201d (feeling temperature in Celsius divided to 50), and low feeling temperature (below 20 Celsius) is a negative factor for bike sharing. The categorical variable \u201cyr\u201d (year, 0 denotes 2011 and 1 means 2012) is the third important one, and the results mean there exists an increasing trend of bike sharing over time.\nRegarding pairwise interactions, the \u201chr vs. weekday\u201d is shown to be relatively important. As the main effects are removed due to the marginal clarity constraint, the estimated pairwise interactions can be individually illustrated. For instance, the rush hour has a negative influence on weekends. Finally, the local interpretability of GAMI-Net is demonstrated in Fig. 6a, which shows the prediction diagnosis of a data point."
    },
    {
      "heading": "4.3.2 Bank Marketing Dataset",
      "text": "This dataset is typically used in a binary classification setting (https://archive.ics. uci.edu/ml/datasets/Bank+Marketing). The dataset has 9 categorical variables and 7 numerical variables, denote a client\u2019s age, education, job, and other related information. The goal is to predict whether a client will subscribe the term deposit.\nSimilarly, we draw the top 8 main effects and top 4 pairwise interaction of the Bank Marketing dataset in Fig. 5b, which is a subset of the selected 13 (out of 16) main effects and 14 (out of 20) pairwise interactions. The top three important variables are \u201cduration\u201d (last contact duration, in seconds), \u201cpoutcome\u201d (outcome of the previous marketing campaign) and \u201cmonth\u201d (last contact month of year). The two most significant pairwise interactions are \u201ccontact vs. month\u201d and \u201cday vs. month\u201d, which are the statistics of the last contact."
    },
    {
      "heading": "5 Conclusion",
      "text": "In this paper, an intrinsically explainable neural network called GAMI-Net is proposed. It approximates any complex functional relationship using subnetwork-represented main effects and pairwise interactions, which can be easily interpreted using continuous 1-D line plots and\n2-D heatmaps. Further constraints are considered to enhanced interpretability, include the heredity constraint (to enforce structurally understandable estimates), the sparsity constraint (for model parsimony) and the marginal clarity constraint (to avoid effects mixing problem). The experimental results show that the proposed model has a close predictive performance to the classic black-box machine learning models like random forest and multi-layer perceptron. Meanwhile, the estimated GAMI-Net is highly interpretable as compared to its counterpart model EBM.\nSome future works are worthy of further investigation. For example, shape constraints can be induced for each component, e.g., monotonic increasing/decreasing, convex, or concave, according to the prior experience or domain knowledge. In addition to the main effects and pairwise interactions, higher-order interactions can also be considered for better approximation performance."
    }
  ],
  "title": "GAMI-Net: An Explainable Neural Network based on Generalized Additive Models with Structured Interactions",
  "year": 2020
}
