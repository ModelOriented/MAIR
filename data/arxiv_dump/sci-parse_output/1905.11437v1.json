{
  "abstractText": "This survey samples from the ever-growing family of adaptive resonance theory (ART) neural network models used to perform the three primary machine learning modalities, namely, unsupervised, supervised and reinforcement learning. It comprises a representative list from classic to modern ART models, thereby painting a general picture of the architectures developed by researchers over the past 30 years. The learning dynamics of these ART models are briefly described, and their distinctive characteristics such as code representation, long-term memory and corresponding geometric interpretation are discussed. Useful engineering properties of ART (speed, configurability, explainability, parallelization and hardware implementation) are examined along with current challenges. Finally, a compilation of online software libraries is provided. It is expected that this overview will be helpful to new and seasoned ART researchers.",
  "authors": [
    {
      "affiliations": [],
      "name": "Leonardo Enzo Brito da Silvaa"
    },
    {
      "affiliations": [],
      "name": "Islam Elnabarawy"
    },
    {
      "affiliations": [],
      "name": "Donald C. Wunsch"
    }
  ],
  "id": "SP:d12c2484e4d7c58ab33df71019e454ef48976efc",
  "references": [
    {
      "authors": [
        "G.P. Amis",
        "G.A. Carpenter"
      ],
      "title": "Default ARTMAP 2",
      "venue": "In Proc. IEEE International Joint Conference on Neural Networks (IJCNN) (pp. 777\u2013782)",
      "year": 2007
    },
    {
      "authors": [
        "D.G. Amorim",
        "M.F. Delgado",
        "S.B. Ameneiro",
        "R.R. Amorim"
      ],
      "title": "Evolu\u00e7\u00e3o das Redes ART e suas Funcionalidades",
      "venue": "Revista OPARA,",
      "year": 2011
    },
    {
      "authors": [
        "G.C. Anagnostopoulos",
        "M. Georgiopoulos"
      ],
      "title": "Ellipsoid ART and ARTMAP for incremental clustering and classification",
      "venue": "In Proc. IEEE International Joint Conference on Neural Networks (IJCNN) (pp. 1221\u20131226)",
      "year": 2001
    },
    {
      "authors": [
        "G.C. Anagnostopoulos",
        "M. Georgiopoulos"
      ],
      "title": "Ellipsoid ART and ARTMAP for incremental unsupervised and supervised learning. In Aerospace/Defense Sensing, Simulation, and Controls (pp. 293\u2013 304)",
      "venue": "International Society for Optics and Photonics",
      "year": 2001
    },
    {
      "authors": [
        "G.C. Anagnostopoulos",
        "M. Georgiopoulos"
      ],
      "title": "Category regions as new geometrical concepts in FuzzyART and Fuzzy-ARTMAP",
      "venue": "Neural Networks,",
      "year": 2002
    },
    {
      "authors": [
        "G.C. Anagnostopoulos",
        "M. Georgiopoulos"
      ],
      "title": "Putting the Utility of Match Tracking in Fuzzy ARTMAP Training to the Test",
      "year": 2003
    },
    {
      "authors": [
        "G.C. Anagnostopoulos",
        "M. Georgiopulos"
      ],
      "title": "Hypersphere ART and ARTMAP for unsupervised and supervised, incremental learning",
      "venue": "In Proc. IEEE International Joint Conference on Neural Networks (IJCNN) (pp. 59\u201364)",
      "year": 2000
    },
    {
      "authors": [
        "R. Andonie"
      ],
      "title": "A Converse H-theorem for Inductive Processes",
      "venue": "Comput. Artif. Intell., 9 , 161\u2013167.",
      "year": 1990
    },
    {
      "authors": [
        "R. Andonie",
        "L. Sasu"
      ],
      "title": "A Fuzzy ARTMAP Probability Estimator with Relevance Factor",
      "venue": "In Proc. of the 11th European Symposium on Artificial Neural Networks (ESANN) (pp. 367\u2013372)",
      "year": 2003
    },
    {
      "authors": [
        "R. Andonie",
        "L. Sasu"
      ],
      "title": "Fuzzy ARTMAP with input relevances",
      "venue": "IEEE Transactions on Neural Networks,",
      "year": 2006
    },
    {
      "authors": [
        "R. Andonie",
        "L. Sasu",
        "V. Beiu"
      ],
      "title": "A Modified Fuzzy ARTMAP Architecture for Incremental Learning",
      "year": 2003
    },
    {
      "authors": [
        "R. NCI) (pp. 124\u2013129). Andonie",
        "L. Sasu",
        "V. Beiu"
      ],
      "title": "Fuzzy ARTMAP with relevance factor",
      "venue": "In Proc. IEEE International",
      "year": 2003
    },
    {
      "authors": [
        "Y.R. Asfour",
        "G.A. Carpenter",
        "S. Grossberg",
        "G.W. Lesher"
      ],
      "title": "Fusion ARTMAP: an adaptive",
      "year": 1993
    },
    {
      "authors": [
        "L.J. 026. Bain",
        "M. Engelhardt"
      ],
      "title": "Introduction to Probability and Mathematical Statistics",
      "year": 1992
    },
    {
      "authors": [
        "Brooks/Cole",
        "G. Cengage Learning. Bartfai"
      ],
      "title": "Hierarchical clustering with ART neural networks",
      "year": 1994
    },
    {
      "authors": [
        "R. White"
      ],
      "title": "Adaptive Resonance Theory-based Modular Networks for Incremental",
      "year": 1997
    },
    {
      "authors": [
        "J.C. Bezdek",
        "R.J. Hathaway"
      ],
      "title": "VAT: a tool for visual assessment of (cluster) tendency",
      "year": 2002
    },
    {
      "authors": [
        "S. Esener"
      ],
      "title": "Optoelectronic Fuzzy ARTMAP processor. Optical Computing",
      "year": 1995
    },
    {
      "authors": [
        "N. Brannon",
        "J. Seiffertt",
        "T. Draelos",
        "D.C. Wunsch II"
      ],
      "title": "Coordinated machine learning and decision",
      "year": 2009
    },
    {
      "authors": [
        "L.E. Brito da Silva",
        "I. Elnabarawy",
        "D.C. Wunsch II"
      ],
      "title": "Distributed dual vigilance fuzzy adaptive resonance theory learns online, retrieves arbitrarily-shaped clusters, and mitigates order dependence",
      "year": 2018
    },
    {
      "authors": [
        "L.E. Brito da Silva",
        "I. Elnabarawy",
        "D.C. Wunsch II"
      ],
      "title": "Dual vigilance fuzzy adaptive resonance theory",
      "venue": "Neural Networks,",
      "year": 2019
    },
    {
      "authors": [
        "L.E. Brito da Silva",
        "D.C. Wunsch II"
      ],
      "title": "Validity Index-based Vigilance Test in Adaptive Resonance Theory Neural Networks",
      "venue": "In Proc. IEEE Symposium Series on Computational Intelligence (SSCI) (pp",
      "year": 2017
    },
    {
      "authors": [
        "L.E. Brito da Silva",
        "D.C. Wunsch II"
      ],
      "title": "A study on exploiting VAT to mitigate ordering effects in Fuzzy ART",
      "venue": "In Proc. IEEE International Joint Conference on Neural Networks (IJCNN) (pp. 2351\u20132358)",
      "year": 2018
    },
    {
      "authors": [
        "T. Cacoullos"
      ],
      "title": "Estimation of a multivariate density",
      "venue": "Annals of the Institute of Statistical Mathematics,",
      "year": 1966
    },
    {
      "authors": [
        "G.A. Carpenter"
      ],
      "title": "A distributed outstar network for spatial pattern learning",
      "venue": "Neural Networks, 7 , 159 \u2013 168. doi:10.1016/0893-6080(94)90064-7.",
      "year": 1994
    },
    {
      "authors": [
        "G.A. Carpenter"
      ],
      "title": "Distributed activation, search, and learning by ART and ARTMAP neural networks",
      "venue": "Proc. International Conference on Neural Networks (ICNN) (pp. 244\u2013249).",
      "year": 1996
    },
    {
      "authors": [
        "G.A. Carpenter"
      ],
      "title": "Distributed ART networks for learning, recognition, and prediction",
      "venue": "Proc. World Congress on Neural Networks (WCNN) (pp. 333 \u2013 344).",
      "year": 1996
    },
    {
      "authors": [
        "G.A. Carpenter"
      ],
      "title": "Distributed Learning, Recognition, and Prediction by ART and ARTMAP Neural Networks",
      "venue": "Neural Networks, 10 , 1473 \u2013 1494. doi:10.1016/S0893-6080(97)00004-X.",
      "year": 1997
    },
    {
      "authors": [
        "G.A. Carpenter"
      ],
      "title": "Default ARTMAP",
      "venue": "Proc. IEEE International Joint Conference on Neural Networks (IJCNN) (pp. 1396\u20131401). volume 2. doi:10.1109/IJCNN.2003.1223900.",
      "year": 2003
    },
    {
      "authors": [
        "G.A. Carpenter",
        "S.C. Gaddam"
      ],
      "title": "Biased ART: A neural architecture that shifts attention toward previously disregarded features following an incorrect prediction",
      "venue": "Neural Networks,",
      "year": 2010
    },
    {
      "authors": [
        "G.A. Carpenter",
        "M.N. Gjaja"
      ],
      "title": "Fuzzy ART Choice Functions",
      "venue": "Proc. World Congress on Neural Networks (WCNN),",
      "year": 1994
    },
    {
      "authors": [
        "G.A. Carpenter",
        "S. Grossberg"
      ],
      "title": "A massively parallel architecture for a self-organizing neural pattern recognition machine. Computer Vision, Graphics, and Image Processing",
      "year": 1987
    },
    {
      "authors": [
        "G.A. Carpenter",
        "S. Grossberg"
      ],
      "title": "1987b). ART 2: self-organization of stable category recognition codes for analog input patterns",
      "venue": "Appl. Opt.,",
      "year": 1987
    },
    {
      "authors": [
        "G.A. Carpenter",
        "S. Grossberg"
      ],
      "title": "ART 3: Hierarchical search using chemical transmitters in selforganizing pattern recognition architectures",
      "venue": "Neural Networks,",
      "year": 1990
    },
    {
      "authors": [
        "G.A. Carpenter",
        "S. Grossberg",
        "N. Markuzon",
        "J.H. Reynolds",
        "D.B. Rosen"
      ],
      "title": "Fuzzy ARTMAP: A neural network architecture for incremental supervised learning of analog multidimensional maps",
      "venue": "IEEE Transactions on Neural Networks,",
      "year": 1992
    },
    {
      "authors": [
        "G.A. Carpenter",
        "S. Grossberg",
        "J.H. Reynolds"
      ],
      "title": "ARTMAP: Supervised real-time learning and classification of nonstationary data by a self-organizing neural network",
      "venue": "Neural Networks,",
      "year": 1991
    },
    {
      "authors": [
        "G.A. Carpenter",
        "S. Grossberg",
        "J.H. Reynolds"
      ],
      "title": "A fuzzy ARTMAP nonparametric probability estimator for nonstationary pattern recognition problems",
      "venue": "IEEE Transactions on Neural Networks,",
      "year": 1995
    },
    {
      "authors": [
        "G.A. Carpenter",
        "S. Grossberg",
        "D.B. Rosen"
      ],
      "title": "ART 2-A: An adaptive resonance algorithm for rapid category learning and recognition",
      "venue": "Neural Networks,",
      "year": 1991
    },
    {
      "authors": [
        "G.A. Carpenter",
        "S. Grossberg",
        "D.B. Rosen"
      ],
      "title": "Fuzzy ART: Fast stable learning and categorization of analog patterns by an adaptive resonance system",
      "venue": "Neural Networks,",
      "year": 1991
    },
    {
      "authors": [
        "G.A. Carpenter",
        "N. Markuzon"
      ],
      "title": "ARTMAP-IC and medical diagnosis: Instance counting and inconsistent cases",
      "venue": "Neural Networks,",
      "year": 1998
    },
    {
      "authors": [
        "G.A. Carpenter",
        "B.L. Milenova",
        "B.W. Noeske"
      ],
      "title": "Distributed ARTMAP: a neural network for fast distributed supervised learning",
      "venue": "Neural Networks,",
      "year": 1998
    },
    {
      "authors": [
        "G.A. Carpenter",
        "W.D. Ross"
      ],
      "title": "ART-EMAP: A neural network architecture for object recognition by evidence accumulation",
      "venue": "IEEE Transactions on Neural Networks,",
      "year": 1995
    },
    {
      "authors": [
        "G.A. Carpenter",
        "Tan",
        "A.-H"
      ],
      "title": "Rule extraction: From neural architecture to symbolic representation",
      "venue": "Connection Science,",
      "year": 1995
    },
    {
      "authors": [
        "T.P. Caudell"
      ],
      "title": "Hybrid optoelectronic adaptive resonance theory neural processor, ART1",
      "venue": "Appl. Opt.,",
      "year": 1992
    },
    {
      "authors": [
        "W.H. Chin",
        "C.K. Loo",
        "M. Seera",
        "N. Kubota",
        "Y. Toda"
      ],
      "title": "Multi-channel Bayesian Adaptive Resonance Associate Memory for on-line topological map building",
      "venue": "Applied Soft Computing ,",
      "year": 2016
    },
    {
      "authors": [
        "A.R. da Silva",
        "L.F.W. Goes"
      ],
      "title": "HearthBot: An Autonomous Agent Based on Fuzzy ART Adaptive Neural Networks for the Digital Collectible Card Game HearthStone",
      "venue": "IEEE Transactions on Games,",
      "year": 2018
    },
    {
      "authors": [
        "I. Dagher",
        "M. Georgiopoulos",
        "G.L. Heileman",
        "G. Bebis"
      ],
      "title": "Ordered fuzzy ARTMAP: a fuzzy ARTMAP algorithm with a fixed order of pattern presentation",
      "venue": "In Proc. IEEE International Joint Conference on Neural Networks (IJCNN) (pp. 1717\u20131722)",
      "year": 1998
    },
    {
      "authors": [
        "I. Dagher",
        "M. Georgiopoulos",
        "G.L. Heileman",
        "G. Bebis"
      ],
      "title": "An ordering algorithm for pattern presentation in fuzzy ARTMAP that tends to improve generalization performance",
      "venue": "IEEE Transactions on Neural Networks,",
      "year": 1999
    },
    {
      "authors": [
        "N. DeClaris",
        "Su",
        "M.-C"
      ],
      "title": "A novel class of neural networks with quadratic junctions",
      "venue": "In Proc. IEEE International Conference on Systems, Man, and Cybernetics (pp. 1557\u20131562)",
      "year": 1991
    },
    {
      "authors": [
        "N. DeClaris",
        "Su",
        "M.-C"
      ],
      "title": "Introduction to the theory and applications of neural networks with quadratic junctions",
      "venue": "In Proc. IEEE International Conference on Systems, Man, and Cybernetics (pp. 1320\u20131325)",
      "year": 1992
    },
    {
      "authors": [
        "Du",
        "K.-L."
      ],
      "title": "Clustering: A neural network approach",
      "venue": "Neural Networks, 23 , 89 \u2013 107. doi:10.1016/j. neunet.2009.08.007.",
      "year": 2010
    },
    {
      "authors": [
        "A.E. Eiben",
        "J.E. Smith"
      ],
      "title": "Introduction to Evolutionary Computing",
      "year": 2015
    },
    {
      "authors": [
        "I. Elnabarawy",
        "D.R. Tauritz",
        "D.C. Wunsch II"
      ],
      "title": "Evolutionary Computation for the Automated Design of Category Functions for Fuzzy ART: An Initial Exploration",
      "venue": "In Proc. Genetic and Evolutionary Computation Conference Companion (GECCO)",
      "year": 2017
    },
    {
      "authors": [
        "I. Elnabarawy",
        "D.C. Wunsch II",
        "A.M. Abdelbar"
      ],
      "title": "Biclustering ARTMAP Collaborative Filtering Recommender System",
      "venue": "In Proc. IEEE International Joint Conference on Neural Networks (IJCNN) (pp. 2986\u20132991)",
      "year": 2016
    },
    {
      "authors": [
        "S. Furao",
        "O. Hasegawa"
      ],
      "title": "An incremental network for on-line unsupervised classification and topology learning",
      "venue": "Neural Networks,",
      "year": 2006
    },
    {
      "authors": [
        "M. Georgiopoulos",
        "H. Fernlund",
        "G. Bebis",
        "G.L. Heileman"
      ],
      "title": "Order of Search in Fuzzy ART and Fuzzy ARTMAP: Effect of the Choice Parameter",
      "venue": "Neural Networks,",
      "year": 1996
    },
    {
      "authors": [
        "E. Gomez-Sanchez",
        "Y.A. Dimitriadis",
        "J.M. Cano-Izquierdo",
        "J. Lopez-Coronado"
      ],
      "title": "Safe\u03bcARTMAP: a new solution for reducing category proliferation in fuzzy ARTMAP",
      "venue": "In Proc. International Joint Conference on Neural Networks (IJCNN) (pp. 1197\u20131202)",
      "year": 2001
    },
    {
      "authors": [
        "E. Gomez-Sanchez",
        "Y.A. Dimitriadis",
        "J.M. Cano-Izquierdo",
        "J. Lopez-Coronado"
      ],
      "title": "\u03bcARTMAP: use of mutual information for category reduction in Fuzzy ARTMAP",
      "venue": "IEEE Transactions on Neural Networks,",
      "year": 2002
    },
    {
      "authors": [
        "E. Granger",
        "Y. Savaria",
        "P. Lavoie",
        "Cantin",
        "M.-A"
      ],
      "title": "A comparison of self-organizing neural networks for fast clustering of radar pulses",
      "venue": "Signal Processing ,",
      "year": 1998
    },
    {
      "authors": [
        "S. Grossberg"
      ],
      "title": "A prediction theory for some nonlinear functional-differential equations i",
      "venue": "learning of lists. Journal of Mathematical Analysis and Applications, 21 , 643 \u2013 694. doi:10.1016/0022-247X(68) 90269-2.",
      "year": 1968
    },
    {
      "authors": [
        "S. Grossberg"
      ],
      "title": "Some networks that can learn, remember, and reproduce any number of complicated space-time patterns, i",
      "venue": "Journal of Mathematics and Mechanics, 19 , 53\u201391.",
      "year": 1969
    },
    {
      "authors": [
        "S. Grossberg"
      ],
      "title": "Neural expectation: cerebellar and retinal analogs of cells fired by learnable or unlearned pattern classes",
      "venue": "Kybernetik , 10 , 49\u201357. doi:10.1007/BF00288784.",
      "year": 1972
    },
    {
      "authors": [
        "S. Grossberg"
      ],
      "title": "Adaptive pattern classification and universal recoding: I",
      "venue": "Parallel development and coding of neural feature detectors. Biological Cybernetics, 23 , 121\u2013134. doi:10.1007/BF00344744.",
      "year": 1976
    },
    {
      "authors": [
        "S. Grossberg"
      ],
      "title": "Adaptive pattern classification and universal recoding: II",
      "venue": "Feedback, expectation, olfaction, illusions. Biological Cybernetics, 23 , 187\u2013202. doi:10.1007/BF00340335.",
      "year": 1976
    },
    {
      "authors": [
        "S. Grossberg"
      ],
      "title": "How does a brain build a cognitive code? Psychological Review , 87 , 1\u201351",
      "venue": "doi:10. 1037/0033-295X.87.1.1.",
      "year": 1980
    },
    {
      "authors": [
        "S. Grossberg"
      ],
      "title": "Adaptive Resonance Theory: how a brain learns to consciously attend, learn, and recognize a changing world",
      "venue": "Neural networks, 37 , 1\u201347. doi:10.1016/j.neunet.2012.09.017.",
      "year": 2013
    },
    {
      "authors": [
        "M.J. Healy",
        "T.P. Caudell"
      ],
      "title": "Guaranteed two-pass convergence for supervised and inferential learning",
      "venue": "IEEE Transactions on Neural Networks,",
      "year": 1998
    },
    {
      "authors": [
        "M.J. Healy",
        "T.P. Caudell"
      ],
      "title": "Ontologies and Worlds in Category Theory: Implications for",
      "venue": "Neural Systems. Axiomathes,",
      "year": 2006
    },
    {
      "authors": [
        "M.J. Healy",
        "T.P. Caudell",
        "S.D.G. Smith"
      ],
      "title": "A neural architecture for pattern sequence verification through inferencing",
      "venue": "IEEE Transactions on Neural Networks,",
      "year": 1993
    },
    {
      "authors": [
        "C.S. Ho",
        "J.J. Liou",
        "M. Georgiopoulos",
        "G.L. Heileman",
        "C. Christodoulou"
      ],
      "title": "Analogue circuit design and implementation of an adaptive resonance theory (ART) neural network architecture",
      "venue": "International Journal of Electronics,",
      "year": 1994
    },
    {
      "authors": [
        "H. Isawa",
        "H. Matsushita",
        "Y. Nishio"
      ],
      "title": "Fuzzy Adaptive Resonance Theory Combining Overlapped Category in consideration of connections",
      "venue": "In Proc. IEEE International Joint Conference on Neural Networks (IJCNN) (pp. 3595\u20133600)",
      "year": 2008
    },
    {
      "authors": [
        "H. Isawa",
        "H. Matsushita",
        "Y. Nishio"
      ],
      "title": "Improved Fuzzy Adaptive Resonance Theory Combining Overlapped Category in Consideration of Connections",
      "venue": "In IEEE Workshop on Nonlinear Circuit Networks (NCN) (pp. 8\u201311)",
      "year": 2008
    },
    {
      "authors": [
        "H. Isawa",
        "H. Matsushita",
        "Y. Nishio"
      ],
      "title": "Fuzzy ART Combining Overlapped Categories Using Variable Vigilance Parameters",
      "venue": "In Proc. International Workshop on Nonlinear Circuits and Signal Processing (NCSP) (pp. 661\u2013664)",
      "year": 2009
    },
    {
      "authors": [
        "H. Isawa",
        "M. Tomita",
        "H. Matsushita",
        "Y. Nishio"
      ],
      "title": "Fuzzy Adaptive Resonance Theory with Group Learning and its Applications",
      "venue": "In Proc. International Symposium on Nonlinear Theory and its Applications (NOLTA) (pp. 292\u2013295)",
      "year": 2007
    },
    {
      "authors": [
        "S. Ishihara",
        "K. Hatamoto",
        "M. Nagamachi",
        "Y. Matsubara"
      ],
      "title": "ART1.5SSS for Kansei engineering expert system",
      "venue": "In Proc. International Conference on Neural Networks (IJCNN) (pp. 2512\u20132515)",
      "year": 1993
    },
    {
      "authors": [
        "S. Ishihara",
        "K. Ishihara",
        "M. Nagamachi",
        "Y. Matsubara"
      ],
      "title": "arboART: ART based hierarchical clustering and its application to questionnaire data analysis",
      "venue": "In Proc. IEEE International Conference on Neural Networks (ICNN) (pp. 532\u2013537)",
      "year": 1995
    },
    {
      "authors": [
        "J.M.C. Izquierdo",
        "M. Almonacid",
        "M. Pinzolas",
        "J. Ibarrola"
      ],
      "title": "dFasArt: Dynamic neural processing in FasArt model",
      "venue": "Neural Networks,",
      "year": 2009
    },
    {
      "authors": [
        "J.M.C. Izquierdo",
        "Y.A. Dimitriadis",
        "M. Ara\u00fazo",
        "J.L. Coronado"
      ],
      "title": "FasArt: A New Neuro-Fuzzy Architecture for Incremental Learning in System Identification",
      "venue": "In IFAC Proceedings Volumes (pp",
      "year": 1996
    },
    {
      "authors": [
        "J.M.C. Izquierdo",
        "Y.A. Dimitriadis",
        "J.L. Coronado"
      ],
      "title": "FasBack: matching-error based learning for automatic generation of fuzzy logic systems",
      "venue": "In Proc. International Fuzzy Systems Conference (pp. 1561\u20131566)",
      "year": 1997
    },
    {
      "authors": [
        "J.M.C. Izquierdo",
        "Y.A. Dimitriadis",
        "E.G. S\u00e1nchez",
        "J.L. Coronado"
      ],
      "title": "Learning from noisy information in FasArt and FasBack neuro-fuzzy systems",
      "venue": "Neural Networks,",
      "year": 2001
    },
    {
      "authors": [
        "L.C. Jain",
        "M. Seera",
        "C.P. Lim",
        "P. Balasubramaniam"
      ],
      "title": "A review of online learning in supervised neural networks",
      "venue": "Neural Computing and Applications,",
      "year": 2014
    },
    {
      "authors": [
        "T. Kasuba"
      ],
      "title": "Simplified Fuzzy ARTMAP",
      "venue": "AI Expert , 8 , 18\u201325.",
      "year": 1993
    },
    {
      "authors": [
        "J. Kennedy",
        "R. Eberhart"
      ],
      "title": "Particle swarm optimization",
      "venue": "In Proc. International Conference on Neural Networks (ICNN) (pp. 1942\u20131948)",
      "year": 1995
    },
    {
      "authors": [
        "S. Kim"
      ],
      "title": "Novel approaches to clustering , biclustering algorithms based on adaptive resonance theory and intelligent control",
      "venue": "Ph.D. thesis Missouri University of Science and Technology.",
      "year": 2016
    },
    {
      "authors": [
        "S. Kim",
        "D.C. Wunsch II"
      ],
      "title": "A GPU based Parallel Hierarchical Fuzzy ART clustering",
      "venue": "In Proc. IEEE International Joint Conference on Neural Networks (IJCNN) (pp. 2778\u20132782)",
      "year": 2011
    },
    {
      "authors": [
        "D.E. Knuth"
      ],
      "title": "Backus Normal Form vs",
      "venue": "Backus Naur Form. Communications of the ACM , 7 , 735\u2013736. doi:10.1145/355588.365140.",
      "year": 1964
    },
    {
      "authors": [
        "V. Koltchinskii"
      ],
      "title": "Rademacher penalties and structural risk minimization",
      "venue": "IEEE Transactions on Information Theory ,",
      "year": 2001
    },
    {
      "authors": [
        "D. Lam",
        "M. Wei",
        "D.C. Wunsch II"
      ],
      "title": "Clustering Data of Mixed Categorical and Numerical Type With Unsupervised Feature Learning",
      "venue": "IEEE Access,",
      "year": 2015
    },
    {
      "authors": [
        "P. Lavoie"
      ],
      "title": "Choosing a choice function: granting new capabilities to ART",
      "venue": "Proc. IEEE International Joint Conference on Neural Networks (IJCNN) (pp. 1988\u20131993). volume 3. doi:10.1109/IJCNN.1999. 832689.",
      "year": 1999
    },
    {
      "authors": [
        "D.S. Levine",
        "P.A. Penz"
      ],
      "title": "ART 1.5\u2013A simplified adaptive resonance network for classifying low-dimensional analog data",
      "venue": "In Proc. of International Conference on Neural Networks (IJCNN) (pp. 639\u2013642)",
      "year": 1990
    },
    {
      "authors": [
        "C.P. Lim",
        "R.F. Harrison"
      ],
      "title": "An Incremental Adaptive Network for On-line Supervised Learning and Probability Estimation",
      "venue": "Neural Networks,",
      "year": 1997
    },
    {
      "authors": [
        "C.P. Lim",
        "R.F. Harrison"
      ],
      "title": "Modified Fuzzy ARTMAP Approaches Bayes Optimal Classification Rates: An Empirical Demonstration",
      "venue": "Neural Networks,",
      "year": 1997
    },
    {
      "authors": [
        "C.P. Lim",
        "R.F. Harrison"
      ],
      "title": "ART-Based Autonomous Learning Systems: Part I \u2014 Architectures and Algorithms",
      "venue": "Innovations in ART Neural Networks (pp. 133\u2013166). Heidelberg: Physica-Verlag HD. doi:10.1007/978-3-7908-1857-4_6",
      "year": 2000
    },
    {
      "authors": [
        "C.P. Lim",
        "R.F. Harrison"
      ],
      "title": "ART-Based Autonomous Learning Systems: Part II \u2014 Applications",
      "venue": "Innovations in ART Neural Networks (pp. 167\u2013188). Heidelberg: Physica-Verlag HD. doi:10.1007/978-3-7908-1857-4_7",
      "year": 2000
    },
    {
      "authors": [
        "E. Lughofer"
      ],
      "title": "Extensions of vector quantization for incremental clustering",
      "venue": "Pattern Recognition, 41 , 995 \u2013 1011. doi:10.1016/j.patcog.2007.07.019.",
      "year": 2008
    },
    {
      "authors": [
        "J.B. MacQueen"
      ],
      "title": "Some Methods for Classification and Analysis of MultiVariate Observations",
      "venue": "L. M. L. Cam, & J. Neyman (Eds.), Proc. of the fifth Berkeley Symposium on Mathematical Statistics and Probability (pp. 281\u2013297). University of California Press volume 1.",
      "year": 1967
    },
    {
      "authors": [
        "S. Majeed",
        "A. Gupta",
        "D. Raj",
        "Rhee",
        "F.C.-H"
      ],
      "title": "Uncertain fuzzy self-organization based clustering: interval type-2 fuzzy approach to adaptive resonance theory",
      "venue": "Information Sciences,",
      "year": 2018
    },
    {
      "authors": [
        "S. Marriott",
        "R.F. Harrison"
      ],
      "title": "A modified fuzzy ARTMAP architecture for the approximation of noisy mappings",
      "venue": "Neural Networks,",
      "year": 1995
    },
    {
      "authors": [
        "T. Martinetz",
        "K. Schulten"
      ],
      "title": "Topology representing networks",
      "venue": "Neural Networks,",
      "year": 1994
    },
    {
      "authors": [
        "T.M. Martinetz",
        "K.J. Shulten"
      ],
      "title": "A \u201cNeural-Gas\u201d Network Learns Topologies",
      "venue": "Artificial Neural Networks (pp. 397\u2013402)",
      "year": 1991
    },
    {
      "authors": [
        "M. Mart\u0301\u0131nez-Zarzuela",
        "F.J. D\u0131\u0301az Pernas",
        "J.F. D\u0131\u0301ez Higuera",
        "M.A. Rod\u0155\u0131guez"
      ],
      "title": "Fuzzy ART Neural Network Parallel Computing on the GPU",
      "venue": "Computational and Ambient Intelligence (pp. 463\u2013470)",
      "year": 2007
    },
    {
      "authors": [
        "M. Mart\u0301\u0131nez-Zarzuela",
        "F.J. D\u0131\u0301az-Pernas",
        "A.T. de Pablos",
        "F. Perozo-Rond\u00f3n",
        "M. Ant\u00f3n-Rod\u0155\u0131guez",
        "D. Gonz\u00e1lez-Ortega"
      ],
      "title": "Fuzzy ARTMAP Based Neural Networks on the GPU for High-Performance Pattern Recognition",
      "venue": "New Challenges on Bioinspired Applications (pp. 343\u2013352)",
      "year": 2011
    },
    {
      "authors": [
        "M. Mart\u0301\u0131nez-Zarzuela",
        "F.J.D. Pernas",
        "A.T. de Pablos",
        "M.A. Rod\u0155\u0131guez",
        "J.F.D. Higuera",
        "D.B. Giralda",
        "D.G. Ortega"
      ],
      "title": "Adaptative Resonance Theory Fuzzy Networks Parallel Computation Using CUDA",
      "venue": "Bio-Inspired Systems: Computational and Ambient Intelligence (pp. 149\u2013156)",
      "year": 2009
    },
    {
      "authors": [
        "L. Massey"
      ],
      "title": "Discovery of hierarchical thematic structure in text collections with adaptive resonance theory",
      "venue": "Neural Computing and Applications, 18 , 261\u2013273. doi:10.1007/s00521-008-0178-2.",
      "year": 2009
    },
    {
      "authors": [
        "L. Meng",
        "A.H. Tan"
      ],
      "title": "Heterogeneous Learning of Visual and Textual Features for Social Web Image Co-Clustering",
      "venue": "Technical Report School of Computer Engineering,",
      "year": 2012
    },
    {
      "authors": [
        "L. Meng",
        "Tan",
        "A.-H",
        "D. Wunsch II"
      ],
      "title": "Vigilance adaptation in adaptive resonance theory",
      "venue": "In Proc. IEEE International Joint Conference on Neural Networks (IJCNN) (pp. 1\u20137)",
      "year": 2013
    },
    {
      "authors": [
        "L. Meng",
        "Tan",
        "A.-H",
        "D.C. Wunsch II"
      ],
      "title": "Adaptive scaling of cluster boundaries for large-scale social media data clustering",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems,",
      "year": 2016
    },
    {
      "authors": [
        "L. Meng",
        "A.H. Tan",
        "D. Xu"
      ],
      "title": "Semi-Supervised Heterogeneous Fusion for Multimedia Data CoClustering",
      "venue": "IEEE Transactions on Knowledge and Data Engineering ,",
      "year": 2014
    },
    {
      "authors": [
        "R.J. Meuth"
      ],
      "title": "Meta-Learning Computational Intelligence Architectures",
      "venue": "Ph.D. thesis Missouri University of Science and Technology.",
      "year": 2009
    },
    {
      "authors": [
        "B. Moore"
      ],
      "title": "Art 1 and pattern clustering",
      "venue": "Proceedings of the 1988 connectionist models summer school (pp. 174\u2013185). Morgan Kaufmann Publishers San Mateo, CA.",
      "year": 1989
    },
    {
      "authors": [
        "P. Nooralishahi",
        "C.K. Loo",
        "M. Seera"
      ],
      "title": "Semi-supervised topo-Bayesian ARTMAP for noisy data",
      "venue": "Applied Soft Computing ,",
      "year": 2018
    },
    {
      "authors": [
        "T.H. Oong",
        "N.A.M. Isa"
      ],
      "title": "Feature-Based Ordering Algorithm for Data Presentation of Fuzzy ARTMAP Ensembles",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems,",
      "year": 2014
    },
    {
      "authors": [
        "R. Palaniappan",
        "C. Eswaran"
      ],
      "title": "Using genetic algorithm to select the presentation order of training patterns that improves simplified fuzzy ARTMAP classification performance",
      "venue": "Applied Soft Computing ,",
      "year": 2009
    },
    {
      "authors": [
        "G.I.S. Palmero",
        "Y.A. Dimitriadis",
        "J.M.C. Izquierdo",
        "E.G. S\u00e1nchez",
        "E.P. Hern\u00e1ndez"
      ],
      "title": "ARTBased Model Set for Pattern Recognition: FasArt Family",
      "venue": "World Scientific",
      "year": 2000
    },
    {
      "authors": [
        "E. Parrado-Hern\u00e1ndez",
        "E. G\u00f3mez-S\u00e1nchez",
        "Y.A. Dimitriadis"
      ],
      "title": "Study of distributed learning as a solution to category proliferation in Fuzzy ARTMAP based neural systems",
      "venue": "Neural Networks,",
      "year": 2003
    },
    {
      "authors": [
        "E. Parzen"
      ],
      "title": "On Estimation of a Probability Density Function and Mode",
      "venue": "The Annals of Mathematical Statistics, 33 , 1065\u20131076.",
      "year": 1962
    },
    {
      "authors": [
        "M.E. Raijmakers",
        "P.C. Molenaar"
      ],
      "title": "Exact ART: A Complete Implementation of an ART Network",
      "venue": "Neural Networks,",
      "year": 1997
    },
    {
      "authors": [
        "K. RamaKrishna",
        "V.A. Ramam",
        "R.S. Rao"
      ],
      "title": "Mathematical Neural Network (MaNN) Models Part III: ART and ARTMAP in OMNI METRICS",
      "venue": "Journal of Applicable Chemistry ,",
      "year": 2014
    },
    {
      "authors": [
        "G.A. Rummery",
        "M. Niranjan"
      ],
      "title": "On-line Q-learning using connectionist systems",
      "venue": "Technical Report CUED/F-INFENG/TR",
      "year": 1994
    },
    {
      "authors": [
        "E.G. Sanchez",
        "Y.A. Dimitriadis",
        "J.M. Cano-Izquierdo",
        "J.L. Coronado"
      ],
      "title": "MicroARTMAP: use of mutual information for category reduction in fuzzy ARTMAP",
      "venue": "In Proc. IEEE International Joint Conference on Neural Networks (IJCNN) (pp. 47\u201352)",
      "year": 2000
    },
    {
      "authors": [
        "L.M. Sasu",
        "R. Andonie"
      ],
      "title": "Function Approximation with ARTMAP Architectures",
      "venue": "International Journal of Computers, Communications & Control ,",
      "year": 2012
    },
    {
      "authors": [
        "L.M. Sasu",
        "R. Andonie"
      ],
      "title": "Bayesian ARTMAP for regression",
      "venue": "Neural Networks,",
      "year": 2013
    },
    {
      "authors": [
        "R.E. Schapire"
      ],
      "title": "The strength of weak learnability",
      "venue": "Machine Learning , 5 , 197\u2013227. doi:10.1007/ BF00116037.",
      "year": 1990
    },
    {
      "authors": [
        "J. Seiffertt",
        "D.C. Wunsch II"
      ],
      "title": "Unified Computational Intelligence for Complex Systems volume 6 of Evolutionary Learning and Optimization",
      "year": 2010
    },
    {
      "authors": [
        "T. Serrano-Gotarredona",
        "B. Linares-Barranco"
      ],
      "title": "A Modified ART 1 Algorithm more Suitable for VLSI Implementations",
      "venue": "Neural Networks,",
      "year": 1996
    },
    {
      "authors": [
        "T. Serrano-Gotarredona",
        "B. Linares-Barranco",
        "A.G. Andreou"
      ],
      "title": "Adaptive Resonance Theory Microchips: Circuit Design Techniques",
      "year": 1998
    },
    {
      "authors": [
        "P.K. Simpson"
      ],
      "title": "Fuzzy min-max neural networks",
      "venue": "i. classification. IEEE Transactions on Neural Networks, 3 , 776\u2013786. doi:10.1109/72.159066.",
      "year": 1992
    },
    {
      "authors": [
        "P.K. Simpson"
      ],
      "title": "Fuzzy min-max neural networks - part 2: Clustering",
      "venue": "IEEE Transactions on Fuzzy Systems, 1 , 32\u2013. doi:10.1109/TFUZZ.1993.390282.",
      "year": 1993
    },
    {
      "authors": [
        "C. Smith",
        "D.C. Wunsch II"
      ],
      "title": "Particle Swarm Optimization in an adaptive resonance framework",
      "venue": "In Proc. IEEE International Joint Conference on Neural Networks (IJCNN) (pp. 1\u20134)",
      "year": 2015
    },
    {
      "authors": [
        "D.F. Specht"
      ],
      "title": "Probabilistic neural networks",
      "venue": "Neural Networks, 3 , 109 \u2013 118. doi:10.1016/ 0893-6080(90)90049-Q.",
      "year": 1990
    },
    {
      "authors": [
        "D.F. Specht"
      ],
      "title": "A general regression neural network",
      "venue": "IEEE Transactions on Neural Networks, 2 , 568\u2013576. doi:10.1109/72.97934.",
      "year": 1991
    },
    {
      "authors": [
        "N. Srinivasa"
      ],
      "title": "Learning and generalization of noisy mappings using a modified probart neural network",
      "venue": "IEEE Transactions on Signal Processing , 45 , 2533\u20132550. doi:10.1109/78.640717.",
      "year": 1997
    },
    {
      "authors": [
        "Su",
        "M.-C",
        "N. DeClaris",
        "Liu",
        "T.-K"
      ],
      "title": "Application of neural networks in cluster analysis",
      "venue": "In Proc. IEEE International Conference on Systems, Man, and Cybernetics (pp. 1\u20136)",
      "year": 1997
    },
    {
      "authors": [
        "Su",
        "M.-C",
        "Liu",
        "T.-K"
      ],
      "title": "Application of neural networks using quadratic junctions in cluster analysis",
      "venue": "Neurocomputing ,",
      "year": 2001
    },
    {
      "authors": [
        "Su",
        "M.-C",
        "Liu",
        "Y.-C"
      ],
      "title": "A hierarchical approach to ART-like clustering algorithm",
      "venue": "In Proc. IEEE International Joint Conference on Neural Networks (IJCNN) (pp. 788\u2013793)",
      "year": 2002
    },
    {
      "authors": [
        "Su",
        "M.-C",
        "Liu",
        "Y.-C"
      ],
      "title": "A new approach to clustering data with arbitrary shapes",
      "venue": "Pattern Recognition,",
      "year": 2005
    },
    {
      "authors": [
        "R.S. Sutton",
        "A.G. Barto"
      ],
      "title": "Introduction to Reinforcement Learning",
      "year": 2018
    },
    {
      "authors": [
        "Tan",
        "A.-H."
      ],
      "title": "Adaptive Resonance Associative Map",
      "venue": "Neural Networks, 8 , 437 \u2013 446. doi:10.1016/ 0893-6080(94)00092-Z.",
      "year": 1995
    },
    {
      "authors": [
        "Tan",
        "A.-H"
      ],
      "title": "Cascade ARTMAP: integrating neural computation and symbolic knowledge processing",
      "venue": "IEEE Transactions on Neural Networks,",
      "year": 1997
    },
    {
      "authors": [
        "Tan",
        "A.-H."
      ],
      "title": "FALCON: a fusion architecture for learning, cognition, and navigation",
      "venue": "Proc. IEEE International Joint Conference on Neural Networks (IJCNN) (pp. 3297\u20133302). volume 4. doi:10.1109/ IJCNN.2004.1381208.",
      "year": 2004
    },
    {
      "authors": [
        "Tan",
        "A.-H."
      ],
      "title": "Self-organizing Neural Architecture for Reinforcement Learning",
      "venue": "J. Wang, Z. Yi, J. M. Zurada, B.-L. Lu, & H. Yin (Eds.), Advances in Neural Networks - ISNN 2006 (pp. 470\u2013475). Berlin, Heidelberg: Springer Berlin Heidelberg. doi:10.1007/11759966_70.",
      "year": 2006
    },
    {
      "authors": [
        "Tan",
        "A.-H",
        "G.A. Carpenter",
        "S. Grossberg"
      ],
      "title": "Intelligence Through Interaction: Towards a Unified Theory for Learning",
      "venue": "Advances in Neural Networks \u2013 ISNN 2007 (pp. 1094\u20131103)",
      "year": 2007
    },
    {
      "authors": [
        "Tan",
        "A.-H",
        "N. Lu",
        "D. Xiao"
      ],
      "title": "Integrating Temporal Difference Methods and Self-Organizing Neural Networks for Reinforcement Learning With Delayed Evaluative Feedback",
      "venue": "IEEE Transactions on Neural Networks,",
      "year": 2008
    },
    {
      "authors": [
        "Tang",
        "X.-l",
        "M. Han"
      ],
      "title": "Semi-supervised Bayesian ARTMAP",
      "venue": "Applied Intelligence,",
      "year": 2010
    },
    {
      "authors": [
        "J.T. Tou",
        "R.C. Gonzalez"
      ],
      "title": "Pattern recognition principles. Addison-Wesley",
      "year": 1974
    },
    {
      "authors": [
        "S.W. Tsay",
        "R.W. Newcomb"
      ],
      "title": "VLSI implementation of ART1 memories",
      "venue": "IEEE Transactions on Neural Networks,",
      "year": 1991
    },
    {
      "authors": [
        "M. Tscherepanow"
      ],
      "title": "TopoART: A Topology Learning Hierarchical ART Network",
      "venue": "K. Diamantaras, W. Duch, & L. S. Iliadis (Eds.), Artificial Neural Networks \u2013 ICANN 2010 (pp. 157\u2013167). Berlin, Heidelberg: Springer Berlin Heidelberg. doi:10.1007/978-3-642-15825-4_21.",
      "year": 2010
    },
    {
      "authors": [
        "M. Tscherepanow"
      ],
      "title": "An Extended TopoART Network for the Stable On-line Learning of Regression Functions",
      "venue": "B.-L. Lu, L. Zhang, & J. Kwok (Eds.), International Conference on Neural Information Processing (ICONIP) (pp. 562\u2013571). Berlin, Heidelberg: Springer Berlin Heidelberg. doi:10.1007/978-3-642-24958-7_65.",
      "year": 2011
    },
    {
      "authors": [
        "M. Tscherepanow"
      ],
      "title": "Incremental On-line Clustering with a Topology-Learning Hierarchical ART Neural Network Using Hyperspherical Categories",
      "venue": "P. Perner (Ed.), Proc. Industrial Conference on Data Mining (ICDM) (pp. 22\u201334). ibai-publishing.",
      "year": 2012
    },
    {
      "authors": [
        "M. Tscherepanow",
        "M. Kortkamp",
        "M. Kammer"
      ],
      "title": "A hierarchical ART network for the stable incremental learning of topological structures and associations from noisy data",
      "venue": "Neural Networks,",
      "year": 2011
    },
    {
      "authors": [
        "M. Tscherepanow",
        "S. K\u00fchnel",
        "S. Riechers"
      ],
      "title": "Episodic Clustering of Data Streams Using a TopologyLearning Neural Network",
      "venue": "Proceedings of the ECAI Workshop on Active and Incremental Learning (AIL) (pp. 24\u201329)",
      "year": 2012
    },
    {
      "authors": [
        "M. Tscherepanow",
        "S. Riechers"
      ],
      "title": "An Incremental On-line Classifier for Imbalanced, Incomplete, and Noisy Data",
      "venue": "Proceedings of the ECAI Workshop on Active and Incremental Learning (AIL) (pp. 18\u201323)",
      "year": 2012
    },
    {
      "authors": [
        "Vakil-Baghmisheh",
        "M.-T",
        "N. Pave\u0161i\u0107"
      ],
      "title": "A Fast Simplified Fuzzy ARTMAP Network",
      "venue": "Neural Processing Letters,",
      "year": 2003
    },
    {
      "authors": [
        "M. Versace",
        "R.T. Kozma",
        "D.C. Wunsch"
      ],
      "title": "Adaptive Resonance Theory Design in Mixed Memristive-Fuzzy Hardware",
      "venue": "Advances in Neuromorphic Memristor Science and Applications (pp. 133\u2013153)",
      "year": 2012
    },
    {
      "authors": [
        "S.J. Verzi",
        "G.L. Heileman",
        "M. Georgiopoulos"
      ],
      "title": "Boosted ARTMAP: Modifications to fuzzy ARTMAP motivated by boosting theory",
      "venue": "Neural Networks,",
      "year": 2006
    },
    {
      "authors": [
        "S.J. Verzi",
        "G.L. Heileman",
        "M. Georgiopoulos",
        "G. Anagnostopoulos"
      ],
      "title": "Off-line structural risk minimization and BARTMAP-S",
      "venue": "In Proc. IEEE International Joint Conference on Neural Networks (IJCNN) (pp. 2533\u20132538)",
      "year": 2002
    },
    {
      "authors": [
        "S.J. Verzi",
        "G.L. Heileman",
        "M. Georgiopoulos",
        "G.C. Anagnostopoulos"
      ],
      "title": "Universal approximation with Fuzzy ART and Fuzzy ARTMAP",
      "venue": "In Proc. IEEE International Joint Conference on Neural Networks (IJCNN) (pp. 1987\u20131992)",
      "year": 2003
    },
    {
      "authors": [
        "S.J. Verzi",
        "G.L. Heileman",
        "M. Georgiopoulus",
        "M.J. Healy"
      ],
      "title": "Rademacher penalization applied to fuzzy ARTMAP and boosted ARTMAP",
      "venue": "In Proc. IEEE International Joint Conference on Neural Networks (IJCNN) (pp. 1191\u20131196)",
      "year": 2001
    },
    {
      "authors": [
        "B. Vigdor",
        "B. Lerner"
      ],
      "title": "The Bayesian ARTMAP",
      "venue": "IEEE Transactions on Neural Networks,",
      "year": 2007
    },
    {
      "authors": [
        "D. Wang",
        "B. Subagdja",
        "Tan",
        "A.-H",
        "Ng",
        "G.-W"
      ],
      "title": "Creating human-like autonomous players in real-time first person shooter computer games",
      "venue": "In Proc. Twenty-First Innovative Applications of Artificial Intelligence Conference (pp",
      "year": 2009
    },
    {
      "authors": [
        "D. Wang",
        "A. Tan"
      ],
      "title": "Creating Autonomous Adaptive Agents in a Real-Time First-Person Shooter Computer Game",
      "venue": "IEEE Transactions on Computational Intelligence and AI in Games,",
      "year": 2015
    },
    {
      "authors": [
        "J.R. Williamson"
      ],
      "title": "Gaussian ARTMAP: A Neural Network for Fast Incremental Learning of Noisy Multidimensional Maps",
      "venue": "Neural Networks, 9 , 881 \u2013 897. doi:10.1016/0893-6080(95)00115-8.",
      "year": 1996
    },
    {
      "authors": [
        "II D.C. Wunsch"
      ],
      "title": "An optoelectronic learning machine: invention, experimentation, analysis of first hardware implementation of the ART 1 neural network",
      "venue": "Ph.D. thesis University of Washington.",
      "year": 1991
    },
    {
      "authors": [
        "II D.C. Wunsch"
      ],
      "title": "ART properties of interest in engineering applications",
      "venue": "Proc. International Joint Conference on Neural Networks (IJCNN) (pp. 3380\u20133383). doi:10.1109/IJCNN.2009.5179094.",
      "year": 2009
    },
    {
      "authors": [
        "D.C. Wunsch II",
        "T.P. Caudell",
        "C.D. Capps",
        "R.J. Marks",
        "R.A. Falk"
      ],
      "title": "An optoelectronic implementation of the adaptive resonance neural network",
      "venue": "IEEE Transactions on Neural Networks,",
      "year": 1993
    },
    {
      "authors": [
        "R. Xu",
        "D.C. Wunsch II"
      ],
      "title": "BARTMAP: A viable structure for biclustering",
      "venue": "Neural Networks,",
      "year": 2011
    },
    {
      "authors": [
        "R. Xu",
        "D.C. Wunsch II",
        "S. Kim"
      ],
      "title": "Methods and systems for biclustering algorithm. U.S. Patent 9,043,326",
      "venue": "Filed January",
      "year": 2012
    },
    {
      "authors": [
        "K.S. Yap",
        "C.P. Lim",
        "I.Z. Abidin"
      ],
      "title": "A Hybrid ART-GRNN Online Learning Neural Network With a \u03b5-Insensitive Loss Function",
      "venue": "IEEE Transactions on Neural Networks,",
      "year": 2008
    },
    {
      "authors": [
        "K.S. Yap",
        "C.P. Lim",
        "M.T. Au"
      ],
      "title": "Improved GART Neural Network Model for Pattern Classification",
      "year": 2011
    },
    {
      "authors": [
        "M. MED.2009.5164629. Yava\u015f",
        "F.N. Alpaslan"
      ],
      "title": "Hierarchical behavior categorization using correlation based adaptive",
      "year": 2012
    }
  ],
  "sections": [
    {
      "text": "This survey samples from the ever-growing family of adaptive resonance theory (ART) neural network models used to perform the three primary machine learning modalities, namely, unsupervised, supervised and reinforcement learning. It comprises a representative list from classic to modern ART models, thereby painting a general picture of the architectures developed by researchers over the past 30 years. The learning dynamics of these ART models are briefly described, and their distinctive characteristics such as code representation, long-term memory and corresponding geometric interpretation are discussed. Useful engineering properties of ART (speed, configurability, explainability, parallelization and hardware implementation) are examined along with current challenges. Finally, a compilation of online software libraries is provided. It is expected that this overview will be helpful to new and seasoned ART researchers.\nKeywords: Adaptive Resonance Theory, Neural Networks, Clustering, Unsupervised Learning, Classification, Regression, Reinforcement Learning, Survey, Explainable.\nContents"
    },
    {
      "heading": "1 Introduction 3",
      "text": ""
    },
    {
      "heading": "2 ART models for unsupervised learning 4",
      "text": "2.1 Elementary architectures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n2.1.1 ART 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 2.1.2 ART 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 2.1.3 Fuzzy ART . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 2.1.4 Fuzzy Min-Max . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 2.1.5 Distributed ART . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 2.1.6 Gaussian ART . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 2.1.7 Hypersphere ART . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 2.1.8 Ellipsoid ART . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 2.1.9 Quadratic neuron ART . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 2.1.10 Bayesian ART . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 2.1.11 Grammatical ART . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 2.1.12 Validity index-based vigilance fuzzy ART . . . . . . . . . . . . . . . . . . . . . . . . . 15 2.1.13 Dual vigilance fuzzy ART . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n2.2 Topological architectures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 2.2.1 Fuzzy ART-GL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 2.2.2 TopoART . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 2.3 Hierarchical architectures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 2.3.1 ARTtree . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n\u2217Corresponding author Email address: leonardoenzo@ieee.org (Leonardo Enzo Brito da Silva)\nPreprint submitted to arXiv.org May 29, 2019\nar X\niv :1\n90 5.\n11 43\n7v 1\n[ cs\n.N E\n] 4\nM ay\n2 01\n9\n2.3.2 Self-consistent modular ART . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 2.3.3 ArboART . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 2.3.4 Joining hierarchical ART . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 2.3.5 Hierarchical ART with splitting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 2.3.6 Distributed dual vigilance fuzzy ART . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n2.4 Biclustering and data fusion architectures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 2.4.1 Fusion ART . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 2.4.2 Biclustering ARTMAP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 2.4.3 Generalized heterogeneous fusion ART . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 2.4.4 Hierarchical Biclustering ARTMAP . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 2.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24"
    },
    {
      "heading": "3 ART models for supervised learning 24",
      "text": "3.1 Architectures for classification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\n3.1.1 ARTMAP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 3.1.2 Fuzzy ARTMAP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 3.1.3 Fuzzy Min-Max . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 3.1.4 Fusion ARTMAP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 3.1.5 LAPART . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 3.1.6 ART-EMAP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 3.1.7 Adaptive resonance associative map . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 3.1.8 Gaussian ARTMAP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 3.1.9 Probabilistic fuzzy ARTMAP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 3.1.10 ARTMAP-IC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 3.1.11 Distributed ARTMAP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35 3.1.12 Hypersphere ARTMAP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36 3.1.13 Ellipsoid ARTMAP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36 3.1.14 \u00b5ARTMAP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36 3.1.15 Default ARTMAPs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 3.1.16 Boosted ARTMAP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 3.1.17 Fuzzy ARTMAP with input relevances . . . . . . . . . . . . . . . . . . . . . . . . . . . 40 3.1.18 Bayesian ARTMAP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41 3.1.19 Generalized ART . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42 3.1.20 Self-supervised ARTMAP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 3.1.21 Biased ARTMAP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 3.1.22 TopoART-C . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\n3.2 Architectures for regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 3.2.1 PROBART . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45 3.2.2 FasArt and FasBack . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 3.2.3 Fuzzy ARTMAP with input relevances . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 3.2.4 Generalized ART . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 3.2.5 TopoART-R . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 3.2.6 Bayesian ARTMAP for regression . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48 3.3 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48"
    },
    {
      "heading": "4 ART models for reinforcement learning 48",
      "text": "4.1 Reactive FALCON . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48 4.2 Temporal difference FALCON . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 4.3 Unified ART . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50 4.4 Extended unified ART . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51"
    },
    {
      "heading": "5 Advantages of ART 52",
      "text": "5.1 Speed . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52 5.2 Configurability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52 5.3 Explainability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52 5.4 Parallelization and hardware implementation . . . . . . . . . . . . . . . . . . . . . . . . . . . 53"
    },
    {
      "heading": "6 ART challenges and open problems 53",
      "text": "6.1 Input order dependency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 6.2 Vigilance parameter adaptation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 6.3 New metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 6.4 Distributed representations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54 6.5 Dichotomy of match- and error-based learning . . . . . . . . . . . . . . . . . . . . . . . . . . . 54"
    },
    {
      "heading": "7 Code repositories 54",
      "text": ""
    },
    {
      "heading": "8 Conclusions 55",
      "text": ""
    },
    {
      "heading": "1. Introduction",
      "text": "Adaptive Resonance Theory (ART) (Grossberg, 1976a,b, 1980, 2013) is a biologically-plausible theory of how a brain learns to consciously attend, learn and recognize patterns in a constantly changing environment. The theory states that resonance regulates learning in neural networks with feedback (recurrence). Thus, it is more than a neural network architecture, or even a family of architectures. However, it has inspired many neural network architectures that have very attractive properties for applications in science and engineering, such as being fast and stable incremental learners with relatively small memory requirements and straightforward algorithms (Wunsch II, 2009). In this context, fast learning refers to the ability of the neurons\u2019 weight vectors to converge to their asymptotic values directly with each input sample presentation. These, and other properties, make ART networks attractive to many researchers and practitioners, as they have been used successfully in a variety of science and engineering applications.\nART addresses the problem of stability vs. plasticity (Carpenter & Grossberg, 1987a; Grossberg, 1980). Plasticity refers the ability of a learning algorithm to adapt and learn new patterns. In many such learning systems plasticity can lead to instability, a situation in which learning new knowledge leads to the loss or corruption of previously-learned knowledge, also known as catastrophic forgetting. Stability, on the other hand, is defined by the condition that no prototype vector can take on a previous value after it has changed, and that an infinite presentation of inputs results in forming a finite number of clusters (Moore, 1989; Xu & Wunsch II, 2009). ART addresses this stability-plasticity dilemma by introducing the ability to learn arbitrary input patterns in a fast and stable self-organizing fashion without suffering from catastrophic forgetting.\nThere have been some previous studies with similar objectives of surveying the ART neural network literature (Amorim et al., 2011; Du, 2010; Jain et al., 2014; RamaKrishna et al., 2014). This survey expands on those works, compiling a broad and informative sampling of ART neural network architectures from the ever-growing machine learning literature. It captures a representative set of examples of various ART architectures in the unsupervised, supervised and reinforcement learning modalities, as well as some models that cross these boundaries and/or combine multiple learning modalities. The overarching goal of this survey is to provide researchers with an accessible coverage of these models, with a focus on their motivations, interpretations for engineering applications and a discussion of open problems for consideration. It is not meant as a comparative assessment of these models but rather a roadmap to assess options.\nThe remainder of this paper is organized as follows. Section 2 presents a sampling of unsupervised learning (UL) ART models, divided into elementary, topological, hierarchical, biclustering and data fusion architectures. Section 3 discusses supervised learning (SL) ART models for both classification and regression. Reinforcement learning (RL) ART models are discussed in Section 4. Sections 5 and 6 discuss some of the useful properties of ART architectures and open problems in this field, respectively. Section 7 provides links to some repositories of ART neural network code, and Section 8 concludes the paper."
    },
    {
      "heading": "2. ART models for unsupervised learning",
      "text": ""
    },
    {
      "heading": "2.1. Elementary architectures",
      "text": "At their core, the elementary ART models are predominantly used for unsupervised learning applications. However, they also lay the foundation to build complex ART-based systems capable of performing all three machine learning modalities (Secs. 2, 3, and 4). This section describes the main characteristics of ART family members in terms of their code representation, long-term memory unit, system dynamics (which encompasses activation, match, resonance and learning) and user-defined parameters. For clarity, Table 1 summarizes the common notation used in the following subsections.\nAn elementary ART neural network model (Fig. 1) usually consists of two fully connected layers as well as a system responsible for its decision-making capabilities:\n\u2022 Feature representation field F1: this is the input layer. In feedforward mode, the output y(F1) of this layer, or short-term memory (STM), simply propagates the input samples x \u2208 Rd to the F2 layer via the bottom-up long-term memory units (LTMs) \u03b8bu. In feedback mode, the F1 layer works as a comparator, in which x and the F2\u2019s expectation (in the form of a top-down LTM \u03b8\ntd) are compared and the outcome y(F1) is sent to the orienting subsystem. Hence, F1 is also known as comparison layer.\n\u2022 Category representation field F2: this layer yields the network output y(F2) (STM). It is also known as recognition or competitive layer. Neurons, prototypes, categories and templates will be used interchangeably when referring to the F2 nodes. The LTM associated with a category j is \u03b8j = {\u03b8buj ,\u03b8tdj }, j = 1, ..., N . Note that not all elementary ART models discussed in this survey have independent bottom-up and top-down LTM parts; however, \u03b8 is always used to indicate the LTM (or set of adaptive parameters) of a given category.\n\u2022 Orienting subsystem: this is a system that regulates both the search and learning mechanisms by inhibiting or allowing categories to resonate.\nNote that some ART models represent pre-processing procedures of the input samples by another layer preceding F1, namely the Input field F0. In this survey, it is assumed that the inputs to an ART network have already gone through the required transformations, and thus this layer is omitted from the discussion.\nART models are competitive, self-organizing, dynamic and modular networks. When a sample x is presented, a winner-takes-all (WTA) competition takes place over its categories at the output layer F2. Then, the neuron J that optimizes that model\u2019s activation function across the nodes is chosen, e.g., the neuron that maximizes some similarity measure T to the presented sample\nJ = arg max j (Tj). (1)\nA category represents a hypothesis. Therefore, a hypothesis test cycle, commonly referred to as a vigilance test, is performed by the orienting subsystem to determine the adequacy of the selected category, i.e., the winner category must satisfy a match criterion (or several match criteria). If the confidence on such a hypothesis is larger than the minimum threshold (namely, the vigilance parameter \u03c1), the neural network enters in a resonance state and learning (i.e., adaptation of the long-term memory (LTM) units) is allowed. Otherwise, category J is inhibited, the next highest ranked category is selected, and the search resumes. If no category satisfies the required resonance condition(s), then a new one is created to encode the presented input sample. This ability to reject a hypothesis/category via a two-way similarity measure, i.e. permissive clustering (Seiffertt & Wunsch II, 2010), makes ART stand out from other methods, such as k-means (MacQueen, 1967). A vigilance region (V R) for a given network category j can be defined in the data space as\nV Rj = {x : Mj(x) satisfies the resonance constraint}, (2)\nwhere Mj is the match function, which yields the confidence on hypothesis j. In other words, it is the region in the input space containing the set of all points such that the resonance criteria is met. Therefore satisfying (or not) the vigilance test for sample x can be modeled using\n1V Rj (x) = { 1, if x \u2208 V Rj 0, otherwise , (3)\nwhere 1{\u00b7} is the indicator function. The resonance constraint in Eq. (2) is depends on the vigilance parameter \u03c1, which regulates the granularity of the network as ART maps samples to categories. Particularly, lower vigilance encourages generalization (Vigdor & Lerner, 2007). Selecting the vigilance parameter is a difficult task in clustering problems. Concretely, the problem of choosing the number of clusters is traded for the problem of choosing the vigilance value.\nDistinct ART models feature specific LTM units, activation and match functions, vigilance criteria and learning laws. Algorithm 1 summarizes the dynamics of an elementary ART model.\nAlgorithm 1: Elementary ART algorithm.\nInput : x, {\u03b1,\u03b2,\u03b3,\u03c1,\u03bb} (parameters). Output: y(F2).\n/* Notation C: set of ART nodes. \u039b: subset of highly active nodes (\u039b \u2286 C). \u03b8: LTM unit. \u03b1: activation function parameter(s). \u03b2: learning function parameter(s). \u03b3: match function parameter(s). \u03c1: vigilance parameter(s). \u03bb: initialization parameter(s). fT (\u00b7): activation function. fM (\u00b7): match function. fL(\u00b7): learning function. fV (\u00b7): vigilance function (e.g., fV =\n\u2227 k 1kV RJ (x)).\nfN (\u00b7): initialization function. */ 1 Present input sample: x \u2208X. 2 Compute activation function(s): Tj = fT (x,\u03b8j ,\u03b1), \u2200j \u2208 C. 3 Perform WTA competition: J = arg max\nj\u2208\u039b (Tj).\n4 Compute match function(s): MkJ = f k M (x,\u03b8J ,\u03b3), \u2200k, k \u2265 1. 5 Perform vigilance test(s): VJ = fV (1 1 V RJ\n(x), ...,1kV RJ (x)). 6 if VJ is TRUE then 7 Update category J : \u03b8newJ = fL(x,\u03b8 old J ,\u03b2). 8 else 9 Deactivate category J : \u039b\u2190 \u039b\u2212 {J}.\n10 if \u039b 6= {\u2205} then 11 Go to step 3."
    },
    {
      "heading": "12 else",
      "text": "13 Set J = |C|+ 1. 14 Create new category: C \u2190 C \u222a {J}. 15 Initialize new category: \u03b8newJ = fN (x,\u03bb).\n16 Set output: y (F2) j =\n{ 1, if j = J\n0, otherwise ."
    },
    {
      "heading": "17 Go to step 1.",
      "text": ""
    },
    {
      "heading": "2.1.1. ART 1",
      "text": "The ART 1 neural network (Carpenter & Grossberg, 1987a) was the seminal implementation of the theory championed by Grossberg used for engineering applications. It relies on crisp set theoretic operators to cluster binary input samples using a similarity measure based on Hamming distance (Serrano-Gotarredona et al., 1998).\nLTM. ART 1 categories are parameterized with bottom-up and top-down adaptive weight vectors \u03b8 = {wbu,wtd}.\nActivation. When a sample x is presented to ART 1, the activation function of each category j is computed as\nTj = \u2016x \u2229wbuj \u20161 . = \u3008wbuj ,x\u3009 = d\u2211 i=1 xiw bu ji , (4)\nwhere x is a binary input, \u2229 is a binary logic AND, wbu is the bottom-up weight vector, \u2016 \u00b7 \u20161 is the L1 norm, and \u3008\u00b7, \u00b7\u3009 is an inner product.\nWhen a given node J is selected via the WTA competition, the output of the F2 activity (short-term\nmemory - STM) becomes\ny (F2) j =\n{ 1, if j = J\n0, otherwise , (5)\nmoreover, the F1 activity (short-term memory - STM) is defined as\ny(F1) =\n{ x, if F2 is inactive\nx \u2229wtdJ , otherwise . (6)\nNote that the WTA competition always include one uncommitted node, which is is guaranteed to satisfy the vigilance criterion following Eq. (7).\nMatch and resonance. The highest activated node J is tested for resonance using\nMJ = \u2016y(F1)\u20161 \u2016x\u20161 = \u2016x \u2229wtdJ \u20161 \u2016x\u20161 , (7)\nwhere V RJ = {x : MJ(x) \u2265 \u03c1} and \u03c1 \u2208 [0, 1]. The vigilance criterion checks if 1V RJ (x) is true, and, in the affirmative case, the category is allowed to learn.\nLearning. When the system enters a resonant state, learning is ensued as\nwtdJ (new) = x \u2229wtdJ (old), (8)\nwbuJ (new) = L\nL\u2212 1 + \u2016wtdJ (new)\u20161 wtdJ (new), (9)\nwhere L > 1 is a user-defined parameter (larger values of L bias the selection of uncommitted nodes over committed ones). Note that the bottom-up weight vectors are normalized versions of their top-down counterparts. If an uncommitted node is selected to learn sample x, then another one is created and initialized as\nwtd = ~1, (10)\nwbu = L\nL\u2212 1 + d wtd. (11)\nART 1 features the following appealing properties thoroughly discussed in (Serrano-Gotarredona et al., 1998): \u201cvigilance or variable coarseness, self-scaling, self-stabilization in a small number of iterations, online learning, capturing rate events, direct assess to familiar input patterns, direct assess to subset and superset patterns, biasing the network to form new categories.\u201d"
    },
    {
      "heading": "2.1.2. ART 2",
      "text": "ART 2 (Carpenter & Grossberg, 1987b) and 2-A (Carpenter et al., 1991b) represent the initial effort toward extending ART 1 (Sec. 2.1.1) applications to real valued data. They were largely supplanted by Fuzzy ART (Sec. 2.1.3) which has since become one of the most widely used and referenced foundational building block for ART networks. This was followed by other architectures such as the ART 3 (Carpenter & Grossberg, 1990) hierarchical architecture, Exact ART (Raijmakers & Molenaar, 1997) (which is a complete ART network based on ART 2) and Correlation-based ART (Yavas\u0327 & Alpaslan, 2009) along with its hierarchical variant (Yavas\u0327 & Alpaslan, 2012) which use correlation analysis methods for category matching. Particularly, the ART 2-A (Carpenter et al., 1991b) architecture was developed following ART 2 with the same properties and a much faster speed.\nLTM. The internal category representation in ART 2-A consists of an adaptive scaled weight vector \u03b8 = {w}.\nActivation. The activation function of each category j in response to a normalized input sample x is computed as\nTj =\n{ \u03b1 \u2211 i xi, if j is uncommitted\nxwj , if j is committed , (12)\nwhere \u03b1 \u2264 1\u221a d is the choice parameter.\nMatch and resonance. The category with the highest activation value is chosen via winner-takes-all selection. Its match function is computed as\nMJ = TJ , (13)\nand the vigilance test is performed to determine whether resonance occurs using the following: MJ \u2265 \u03c1, where 0 \u2264 \u03c1 \u2264 1 is the vigilance threshold.\nIf the winning category passes the vigilance test, resonance occurs, and the category is allowed to learn this input pattern. If the category fails the vigilance test, a reset signal is triggered for this category, and the category with the next highest activation is selected for the same process.\nLearning. When resonance occurs, the weights of the winning category are updated as\nwJ(new) =\n{ x, if J is uncommitted\n\u03b2x+ (1\u2212 \u03b2)wJ(old), if j is committed , (14)\nwhere 0 < \u03b2 \u2264 1 is the learning rate."
    },
    {
      "heading": "2.1.3. Fuzzy ART",
      "text": "Fuzzy ART (FA) (Carpenter et al., 1991c) is arguably the most widely used ART model. It extends the capabilities of ART 1 (Sec. 2.1.1) to process real-valued data by incorporating fuzzy set theoretic operators (Zadeh, 1965). Typically, samples are pre-processed by applying complement coding (Carpenter et al., 1992, 1991a). This transformation doubles the original input dimension while imposing a constant norm (x\u2190 [x, ~1\u2212 x]):\n\u2016x\u20161 = 2d\u2211 i=1 xi = d\u2211 i=1 xi + d\u2211 i=1 (1\u2212 xi) = d. (15)\nThis process encodes the degree of presence and absence of each data feature. The augmented input vector prevents a category proliferation type due to weight erosion (Carpenter, 1997).\nLTM. Each category LTM unit is a weight vector \u03b8 = {w}. If complement coding is employed, then w = [u,vc], and the geometric interpretation of a category is a hyperrectangle (or hyperbox), in the data space, with lower left corner u and upper right corner vc representing features ranges (minimum and maximum data statistics).\nActivation. The activation function of a category j is defined as (Weber law)\nTj = \u2016x \u2227wj\u20161 \u03b1+ \u2016wj\u20161 , (16)\nwhere \u2227 is a component-wise fuzzy AND/intersection (minimum), \u03b1 > 0 is the choice parameter which is related to the system\u2019s complexity (it can be seen as a regularization parameter that penalizes large weights). Its role has been thoroughly investigated in (Georgiopoulos et al., 1996). The activation function measures the degree to which x is a fuzzy subset of wj and is biased towards smaller categories. The F1 activity is defined as\ny(F1) =\n{ x, if F2 is inactive\nx \u2227wJ , otherwise . (17)\nMatch and resonance. When the winner node J is selected, the F2 activity is\ny (F2) j =\n{ 1, if j = J\n0, otherwise . (18)\nand a hypothesis testing cycle is conducted using\nMJ = \u2016y(F1)\u20161 \u2016x\u20161 = \u2016x \u2227wJ\u20161 \u2016x\u20161 , (19)\nwhere V RJ = {x : MJ(x) \u2265 \u03c1} and \u03c1 \u2208 [0, 1] is the vigilance parameter. The vigilance criterion checks if 1V RJ (x) is true, and, in the affirmative case, the category is allowed to learn. An uncommitted category will always satisfy the match criterion. Fuzzy ART vigilance regions are hyperoctagons and thoroughly discussed in (Anagnostopoulos & Georgiopoulos, 2002; Meng et al., 2016; Verzi et al., 2006). The match function ensures that if learning takes place, the updated category will not exceed the maximum allowed size. Specifically, category j\u2019s size is measured as\nRj = \u2016vj \u2212 uj\u20161 = d\u2211 i=1 [(1\u2212 wj,d+i)\u2212 wj,i] = d\u2212 \u2016wj\u20161, (20)\nwhere, considering the complement coded inputs, \u2212d \u2264 Rj \u2264 d (for an uncommitted category: Rj = \u2212d). Particularly, the match function measures the size of the category if it is allowed to learn the presented sample. Thus, the vigilance criterion imposes an upper bound to the category size defined by the vigilance parameter (\u03c1)\nRJ \u2295 x = d\u2212 \u2016x \u2227wj\u20161 \u2264 d(1\u2212 \u03c1), (21)\nwhere RJ \u2295x represents the smallest hyperrectangle capable of enclosing both RJ and the presented sample x.\nLearning. If the vigilance test fails, then the winner category is inhibited, and the search continues until another one is found or created. When the vigilance criterion in met by category J , it adapts using\nwJ(new) = (1\u2212 \u03b2)wJ(old) + \u03b2(x \u2227wJ(old)), (22)\nwhere \u03b2 \u2208 (0, 1] is the learning parameter. If an uncommitted node is recruited to learn sample x, then another one is created and initialized as w = ~1. According to Eq. (22), the norm of a weight vector is monotonically non-increasing during learning since categories can only expand (Vigdor & Lerner, 2007)."
    },
    {
      "heading": "2.1.4. Fuzzy Min-Max",
      "text": "The Fuzzy Min-Max neural network (Simpson, 1993) is an unsupervised learning network that uses fuzzy set theory to build clusters using a hyperbox representation discovered via the fuzzy min-max learning algorithm. Each category in Fuzzy Min-Max is represented explicitly as a hyperbox, with the minimum and maximum points of the hyperbox as well as a value for the membership function that measures the degree to which each input pattern falls within this category. The category hyperboxes are adjusted to fit each input sample using a contraction and expansion algorithm that expands the hyperbox of the winning category to fit the input sample and then contracts any other hyperboxes that are found to overlap with the new hyperbox boundaries."
    },
    {
      "heading": "2.1.5. Distributed ART",
      "text": "The distributed ART (dART) (Carpenter, 1996a,b, 1997) features distributed code representation for activation, match and learning processes to improve noise robustness and memory compression in a system that features fast and stable learning. Particularly, in WTA mode, distributed ART reduces in functionality to fuzzy ART (Sec. 2.1.3).\nLTM. The distributed ART LTM units consist of bottom-up (\u03c4 bu) and top-down (\u03c4 td) adaptive thresholds (\u03b8 = {\u03c4 bu, \u03c4 td}), which are initialized as small random values and ~0, respectively. When employing complement coding, the geometric interpretation of a category j is a family of hyperrectangles nested by the activation levels y (F2) j \u2208 [0, 1]. The edges of hyperrectangle Rj(y (F2) j ) are defined, for each input dimension\ni, as the bounded interval [ [y\n(F2) j \u2212 \u03c4 buj,i ]+, 1\u2212 [y (F2) j \u2212 \u03c4 buj,d+i]+\n] \u2014 where [\u03be]+ = max(0, \u03be) is a rectifier op-\nerator. Note that the Rj size decreases as y (F2) j increases. Particularly, setting y (F2) j = 1 yields the smallest hyperrectangle R(1), and the substitution wj = (1\u2212 \u03c4 bu) corresponds to fuzzy ART\u2019s LTM. Activation. The activation function can be defined as a choice-by-difference (Carpenter & Gjaja, 1994) (Tj \u2208 [0, d]) variant\nTj = \u2016[x \u2227 (1\u2212 \u03c4 buj )\u2212\u2206j ]+\u20161 + (1\u2212 \u03b1)\u2016[\u03c4 bu j \u2212 \u03b4j ]+\u20161 , 0 < \u03b1 < 1, (23)\nor a Weber law (Carpenter & Grossberg, 1987a) (Tj \u2208 [0, 1]) variant\nTj = \u2016[x \u2227 (1\u2212 \u03c4 buj )\u2212\u2206j ]+\u20161 \u03b1+ d\u2212 \u2016[\u03c4 buj \u2212 \u03b4j ]+\u20161 , \u03b1 > 0, (24)\nwhere [\u03be]+ is a component-wise rectifier operator (i.e., [\u03bek] + = max(0, \u03bek) for each component k of vector \u03be), and \u2206 and \u03b4 are the medium-term memory (MTM) depletion parameters. After the nodes\u2019 activations are computed, the F2 activity can be obtained by employing the increased-gradient content-addressable-memory (IG CAM) rule:\ny (F2) j =  (Tj) p\u2211 \u03bb\u2208\u039b (T\u03bb)p , if j \u2208 \u039b\n0, otherwise\n, (25)\nsuch that \u2016y(F2)\u20161 = 1 and p > 0. The subset \u039b consists of the nodes such that TJ \u2265 Tj for J \u2208 \u039b and j /\u2208 \u039b. Examples are the Q-max rule (see Sec. 3.1.10) or greater than average activations (i.e., \u039b = {j : Tj \u2265 Tavg}, Tavg = 1/N \u2211N j=1 Tj). Note that the power law f(\u03b6) = \u03b6\np converges to WTA when p\u2192 +\u221e. Match and Resonance. The distributed ART\u2019s match function is defined as\nM = \u2016y(F1)\u20161 \u2016x\u20161 , (26)\nwhere the F1 activity is given by y(F1) = x \u2227 \u03c3, (27)\nand\n\u03c3i = N\u2211 j=1 [y (F2) j \u2212 \u03c4 td ji ] + , \u03c3i \u2208 [0, 1]. (28)\nResonance occurs if 1V R(x) = 1, where V R = {x : M(x) \u2265 \u03c1} and \u03c1 \u2208 [0, 1]. Otherwise, the MTM depletion parameters are updated as\n\u2206ji(new) = \u2206ji(old) \u2228 (xi \u2227 [yj \u2212 \u03c4 buji ]+), (29)\n\u03b4ji(new) = \u03b4ji(old) \u2228 (yj \u2227 \u03c4 buji ), (30)\nand the distributed dynamics continue by recomputing Eqs. (25) through (26). Note that the depletion parameters \u2206 and \u03b4 are (re)set to ~0 at the beginning of every input sample presentation.\nLearning. When the system enters a resonant state, distributed learning takes place according to the nodes\u2019 activation levels. Specifically, the top-down adaptive thresholds are updated using the distributed outstar learning law (Carpenter, 1994):\n\u03c4 tdji (new) = \u03c4 td ji (old) + \u03b2\n[\u03c3i \u2212 xi]+\n\u03c3i\n[ y\n(F2) j \u2212 \u03c4 td ji (old)\n]+ , (31)\nwhereas the bottom-up adaptive thresholds are updated using the distributed instar learning law (Carpenter, 1997):\n\u03c4 buji (new) = \u03c4 bu ji (old) + \u03b2\n[ y\n(F2) j \u2212 \u03c4 bu ji (old)\u2212 xi\n]+ , (32)\nwhere \u03b2 \u2208 [0, 1] is the learning rate. The adaptive thresholds\u2019 components, \u2208 [0, 1], start near zero and monotonically increase during the learning process. After learning takes place, the depletion parameters \u2206 and \u03b4 are both reset to their initial values (~0). In WTA mode, the distributed instar and outstar learning laws become the instar (Grossberg, 1972) and outstar (Grossberg, 1968, 1969) laws, respectively, and thus distributed ART reduces to fuzzy ART (Sec. 2.1.3)."
    },
    {
      "heading": "2.1.6. Gaussian ART",
      "text": "Gaussian ART (Williamson, 1996) was developed to reduce category proliferation in noisy environments and to provide a more efficient category LTM unit.\nLTM. Each category j is a Gaussian distribution composed by mean \u00b5j \u2208 Rd, standard deviation \u03c3j \u2208 Rd and instance counting nj (i.e., the number of samples encoded by category j used to compute its a priori probability). Therefore, a category is geometrically interpreted as a hyperellipse in the data space.\nActivation. Gaussian ART is rooted in Bayes\u2019 decision theory, and as such its activation function is defined as:\nTj = p\u0302(cj |x) = p\u0302(x|cj)p\u0302(cj)\np\u0302(x) , (33)\nwhere the likelihood is estimated as\np\u0302(x|cj) = exp\n[ \u22121\n2 (\u00b5j \u2212 x)T \u03a3\u22121j (\u00b5j \u2212 x) ] \u221a\n(2\u03c0) d det(\u03a3j) , (34)\nand the prior as\np\u0302(cj) = nj N\u2211 i=1 ni . (35)\nNote that the evidence p\u0302(x) is neglected in the computations (since it is equal for all categories cj), and feature independence is assumed, i.e., \u03a3j is a diagonal matrix (\u03a3j = diag(\u03c3 2 j,1, ..., \u03c3 2 j,d)). Therefore, since it assumes uncorrelated features, it cannot capture covarying data. A category J is then chosen following the maximum a posteriori (MAP) criterion:\nJ = arg max j (Tj) = arg max j\n[p\u0302(cj |x)] . (36)\nMatch and Resonance. The match function is defined as a normalized version of p\u0302(x|cj):\nMJ = exp\n[ \u22121\n2 (\u00b5J \u2212 x)T \u03a3\u22121J (\u00b5J \u2212 x)\n] , (37)\nwhich is then compared to the vigilance parameter threshold \u03c1 \u2208 (0, 1]. Note that in the original Gaussian ART paper (Williamson, 1996), a log discriminant is used to reduce the computational burden in both the activation (Eq. (33)) and match (Eq. (37)) functions.\nLearning. When the vigilance criterion is met, learning is ensued for the resonating category J as\nnJ(new) = nJ(old) + 1, (38)\n\u00b5\u0302J(new) =\n( 1\u2212 1\nnJ(new)\n) \u00b5\u0302J(old) +\n1\nnJ(new) x, (39)\n\u03c32J,i(new) =\n( 1\u2212 1\nnJ(new)\n) \u03c32J,i(old) +\n1\nnJ(new) (\u00b5J,i(new)\u2212 xi)2 . (40)\nIf a new category is created, then it is initialized with nN+1 = 1, \u00b5N+1 = x, and \u03a3N+1 = \u03c3 2 initI (isotropic). The initial standard deviation \u03c3init in Gaussian ART directly affects the number of categories created."
    },
    {
      "heading": "2.1.7. Hypersphere ART",
      "text": "The Hypersphere ART (HA) (Anagnostopoulos & Georgiopulos, 2000) architecture was designed as a successor for Fuzzy ART (Section 2.1.3) that inherits its advantageous qualities while utilizing fewer categories and having a more efficient internal knowledge representation.\nLTM. Each category is represented as \u03b8 = {R,m}, where mj \u2208 Rd and Rj \u2208 R are the centroid and radius, respectively. Since it does not require complement coding of input samples, it uses d+ 1 memory per category, which is a smaller memory requirement than fuzzy ART, which uses 2d memory to represent the hyperrectangular categories. Naturally, categories are hyperspheres in the data space.\nActivation. The category activation function Tj for each F2 category j is calculated as:\nTj = R\u0304\u2212max(Rj , ||x\u2212mj ||2)\nR\u0304\u2212Rj + \u03b1 , (41)\nwhere || \u00b7 ||2 is the L2 (Euclidean) norm, \u03b1 \u2208 (0,\u221e) is the choice parameter and R\u0304 \u2208 [Rmax,\u221e) is the radial extend parameter which controls the maximum possible category size achieved during training. The lower-bound Rmax is defined as:\nRmax = 1\n2 max i,j ||xi \u2212 xj ||2 (42)\nMatch and resonance. The winning category J is selected using WTA competition, and the match function is computed as\nMJ = 1\u2212 max(Rj , ||x\u2212mj ||2)\nR\u0304 , (43)\nwhere the vigilance criterion is MJ \u2265 \u03c1. Learning. If the winning category satisfies the vigilance test, then resonance occurs, and the radius RJ and centroid mJ of the winning node are updated as follows:\nRnewJ = R old J +\n\u03b2\n2\n[ max ( RoldJ , ||x\u2212moldJ ||2 ) \u2212RoldJ ] , (44)\nmnewJ = m old J +\n\u03b2\n2\n( x\u2212moldJ ) [ 1\u2212 min ( RoldJ , ||x\u2212moldJ ||2 ) ||x\u2212moldJ ||2 ] , (45)\nwhere \u03b2 \u2208 (0, 1] is the learning rate parameter. If the winning category fails the vigilance test, it is reset, and the process is repeated. Eventually, either a category succeeds or a new one is created with its radius and centroid initialized as RJ = 0 and mJ = x, respectively."
    },
    {
      "heading": "2.1.8. Ellipsoid ART",
      "text": "Ellipsoid ART (EA) (Anagnostopoulos & Georgiopoulos, 2001a,b) is a generalization of hypersphere ART that uses hyperellipses instead of hyperspheres to represent the categories. These require 2d+1 memory and are subjected to two distinct constraints during training: (1) maintain a constant ratio between the lengths of their major and minor axes, and (2) maintain a fixed direction of their major axis once it is set. These restrictions, however, can pose some limitations to the categories discovered by ellipsoid ART depending on the order in which the input samples are presented.\nLTM. A category j in ellipsoid ART is described by its parameters \u03b8j = {mj ,dj , Rj}, where mj is the centroid of the category\u2019s hyperellipses, dj is the direction of the category\u2019s major axis and Rj is the category\u2019s radius (or half the length of its major axis).\nActivation. The distance between an input sample and a category j is calculated as:\ndis(x,mj) =  1 \u00b5 \u221a ||x\u2212mj ||22 \u2212 (1\u2212 \u00b52) [ dTj (x\u2212mj) ]2 if dj 6= 0\n||x\u2212mj ||2 if dj = 0 , (46)\nwhere || \u00b7 ||2 is the L2 (Euclidean) vector norm and \u00b5 \u2208 (0, 1] is a user-specified parameter that defines the ratio between a category\u2019s major and minor axes. The category activation function Tj for each category j is then calculated as:\nTj = R\u0304\u2212Rj \u2212max {Rj , dis(x,mj)}\nR\u0304\u2212 2Rj + \u03b1 , (47)\nwhere \u03b1 \u2208 (0,+\u221e) is the choice parameter, and R\u0304 \u2265 1\u00b5 maxp,q \u2016xp \u2212 xq\u20162 is a user-specified parameter. Match and resonance. The match function of the winning category J selected using winner-takes-all is given by\nMJ = 1\u2212 RJ + max {RJ , dis(x,mJ)||}\nR\u0304 , (48)\nwhere \u03c1 \u2208 (0, 1] is the vigilance parameter. Learning. If the winning category J satisfies MJ \u2265 \u03c1, then resonance occurs, and it is updated as follows:\nRnewJ = R old J +\n\u03b2\n2\n[ max { RoldJ , dis(x,m old J ) } \u2212RoldJ ] , (49)\nmnewJ = m old J +\n\u03b2\n2\n( x\u2212moldJ ) [ 1\u2212 min { RoldJ , dis(x,m old J ) }\ndis(x,moldJ )\n] , (50)\ndj = x(2) \u2212mJ ||x(2) \u2212mJ ||2 , (51)\nwhere \u03b2 \u2208 (0, 1] is the learning rate, and x(2) represents the second input sample to be encoded by this category. When a new category is created, its major axis direction dJ is initially set to the zero vector ~0, and then Eq. (51) is used to update it when the second pattern is committed to the category. The hyperellipse\u2019s major axis direction stays fixed after that.\nIf the winning category fails the vigilance check, then it is inhibited, and the entire process is repeated until a winner category satisfies the resonance criterion. If no existing category succeeds, then a new category is created with its weights initialized with RJ = 0, mJ = x, and dJ = ~0."
    },
    {
      "heading": "2.1.9. Quadratic neuron ART",
      "text": "The quadratic neuron ART model (Su & Liu, 2002, 2005) was developed in the context of a multiprototype-based clustering framework that integrates dynamic prototype generation and hierarchical agglomerative clustering to retrieve arbitrarily-shaped data structures.\nLTM. A category j is a quadratic neuron (DeClaris & Su, 1991, 1992; Su et al., 1997; Su & Liu, 2001) parameterized by \u03b8j = {sj ,Wj , bj}, where sj , Wj = [w(j)k,i ]d\u00d7d, and bj are the adaptable LTMs. Particularly, these neurons are hyperellipsoid structures in the multidimensional data space.\nActivation. The activation of a quadratic neuron j is given by Tj = exp ( \u2212s2j\u2016zj \u2212 bj\u2016 2 2 ) , (52)\nwhere zj is a linear transformation of the input x\nzj = Wjx. (53)\nMatch and resonance. After the winning node J is selected using WTA competition, the system will enter a resonant state if node J \u2019s response is larger than or equal to the vigilance parameter \u03c1, i.e., if MJ \u2265 \u03c1, where the match function is equal to the activation function (Eq. (52)).\nLearning. If the vigilance criterion is satisfied for node J , then its parameters p \u2208 {sj ,Wj , bj} are adapted using gradient ascent\np(new) = p(old) + \u03b7 \u2202TJ\n\u2202p(old) , (54)\nwhere \u03b7 is the learning rate. Specifically, bJ,i(new) = bJ,i(old) + \u03b7b [ 2s2JTJ (zJ,i \u2212 bJ,i) ] , (55)\nw (J) k,i (new) = w (J) k,i (old) + \u03b7w [ \u22122s2JTJ (zJ,k \u2212 bJ,k)xi ] , (56)\nsJ(new) = sJ(old) + \u03b7s ( \u22122sJTJ\u2016zJ \u2212 bJ\u201622 ) , (57)\nwhere \u03b7b, \u03b7w and \u03b7s are the learning rates. Otherwise, a new category is created and initialized with bN+1 = x, WN+1 = Id\u00d7d, and sN+1 = sinit, where sinit \u2208 R is a user-defined parameter."
    },
    {
      "heading": "2.1.10. Bayesian ART",
      "text": "LTM. Bayesian ART (BA) (Vigdor & Lerner, 2007) is another architecture using multidimensional Gaussian distributions to parameterize the categories: \u03b8 = {N (\u00b5,\u03a3), p}, where \u00b5, \u03a3 and p are the mean, covariance matrix, and prior probability, respectively. The latter parameter is computed using the number of samples n learned by a category.\nActivation. Like Gaussian ART (Sec. 2.1.6), Bayesian ART also integrates Bayes decision theory in its framework. Thus, its activation function is given by the posterior probability of category j:\nTj = p\u0302(cj |x) = p\u0302(x|cj)p\u0302(cj) N\u2211 l=1 p\u0302(x|cl)p\u0302(cl) , (58)\nwhere p\u0302(x|cj) is the same as Eq. (34) but uses a full covariance matrix (instead of diagonal), and p\u0302(cj) is the estimated prior probability of category j as in Eq. (35).\nMatch and Resonance. After the WTA competition is performed and the winner category J is selected using the maximum a posteriori probability (MAP) criterion (Eq. (36)), the match function is computed as\nMJ = det(\u03a3J), (59)\nsuch that the vigilance criterion is designed to limit category J \u2019s hyper-volume. The vigilance test is defined as MJ \u2264 \u03c1, where \u03c1 represents the maximum allowed hyper-volume.\nLearning. If the selected category resonates (i.e., the match criterion is satisfied), then learning occurs. The sample count and means are updated using Eq. (38) and Eq. (39), respectively. The covariance matrix is updated as:\n\u03a3\u0302J(new) =\n( nJ(old)\nnJ(new)\n) \u03a3\u0302J(old) +\n1\nnJ(new) (x\u2212 \u00b5\u0302J(new))(x\u2212 \u00b5\u0302J(new))T I, (60)\nwhich corresponds to the sequential maximum-likelihood estimation of parameters for a multidimensional Gaussian distribution (Vigdor & Lerner, 2007). The Hadamard product is used when a diagonal covariance matrix is desired. Otherwise, a new category is created with nN+1 = 1, \u00b5N+1 = x, and \u03a3N+1 = \u03a3init. Naturally, the initial covariance matrix should satisfy the vigilance constraint (i.e., \u03a3init = \u03c3 2 initI, where \u03c32init \u03c11/d). In this ART model, categories can both grow and shrink."
    },
    {
      "heading": "2.1.11. Grammatical ART",
      "text": "The Grammatical ART (GramART) architecture (Meuth, 2009) represents a specialized version of ART designed to work with variable-length input patterns which are used to encode grammatical structure. It builds templates while adhering to a Backus-Naur form grammatical structure (Knuth, 1964).\nLTM. To allow for comparisons between variable-length input patterns, GramART uses a generalized tree representation to encode its internal categories. Each node in the tree for a category contains an array representing the distribution of the different possible grammatical symbols at that node.\nActivation. The activation function for a category j is defined as a parallel to Fuzzy ART\u2019s activation function (Sec. 2.1.3), but GramART defines its own operator for calculating the intersection between a category and an input pattern. A tree in GramART is defined as an ordered pair (N,R) where N is a set of nodes and R is a set of binary relations that describe the structure of the tree. For nodes x and y:\nR(x, y) =\n{ 0, if y is not a successor of x\n> 0, if y is a successor of x , (61)\nThe activation of a category j in GramART is given by\nTj = |x \u2229wj | \u2016wj\u2016 , (62)\nwhere the intersection operator |x \u2229wj | is defined as:\n|x \u2229wj | = r\u2211 i=0 wj [i, xi], (63)\nand wj [i, xi] represents each of the values stored in wj corresponding to the symbols present in the input pattern x. The tree norm operator \u2016wj\u2016 is defined as the number of nodes in the tree.\nMatch and resonance. The category with the highest activation value is chosen using winner-takes-all selection, and the following vigilance criterion is checked to determine whether the input pattern resonates with this category:\nMJ = |x \u2229wJ | \u2016x\u2016 > \u03c1. (64)\nIf this vigilance criterion is satisfied, resonance occurs and the category is allowed to learn this input pattern. Otherwise, it is reset, and the category with the next best activation is checked.\nLearning. When resonance occurs, the weight of the winning category is updated using the following learning rule:\nwj [i] = wj [i] \u2217N + \u03b4j\nN + 1 , (65)\nwhere\n\u03b4j =\n{ 1, if xi = j\n0, otherwise . (66)\nThe weights are updated recursively down the grammar tree, and they reflect the probability of a tree symbol occurring in the node representing this particular category."
    },
    {
      "heading": "2.1.12. Validity index-based vigilance fuzzy ART",
      "text": "The validity index-based vigilance fuzzy ART (Brito da Silva & Wunsch II, 2017) endows fuzzy ART with a second vigilance criterion based on cluster validity indices (Xu & Wunsch II, 2009). The usage of this immediate reinforcement signal alleviates input order dependency and allows for a more a robust hyper-parameterization.\nLTM. This is a fuzzy ART-based architecture. Therefore, categories are hyperrectangles as described in Sec. 2.1.3.\nActivation. The validity index-based vigilance fuzzy ART activation function is equal to fuzzy ART\u2019s and thus, is computed using Eq. (16) in Sec. 2.1.3.\nMatch and Resonance. After a winner J is selected, the first match function (M1J) is identical to fuzzy ART\u2019s (Eq. (19) in Sec. 2.1.3), whereas the second (M2J) is defined as\nM2J = \u2206f = f(\u2126\u0302)\u2212 f(\u2126), (67)\nwhich represents the penalty (or reward) incurred by assigning sample x to category J and thereby changing the current clustering state of the data set from \u2126 to \u2126\u0302 (if there is no change in assignment, then M2J = 0). The function f(\u2126) corresponds to a cluster validity index value given a partition \u2126 = {\u03c91, ..., \u03c9k} of\ndisjointed clusters \u03c9i (defined by categories i), where k\u22c3 i=1 \u03c9i = X. The second vigilance region is then V R2J = {x : M2J(x) \u2265 \u03c12}, and \u03c12 \u2208 R. The vigilance criterion checks if 1V RJ (x) = 1. In the affirmative case, the category is allowed to learn. Note that the discussion so far implies the maximization of a cluster validity index; naturally, when minimization is sought, the inequality in the definition of V R2J should be reversed. This is a greedy algorithm that selects the best clustering assignment based on immediate feedback. Naturally, performance is biased toward the data structures favored by the selected cluster validity index.\nLearning. If both vigilances are satisfied, then learning is ensued. Otherwise, the search resumes or a new category is created. The learning rules are identical to fuzzy ART\u2019s (Sec. 2.1.3). Note that the validity index-based vigilance fuzzy ART model learns in offline mode, given that the entire data is used for the computation of Eq. (67)."
    },
    {
      "heading": "2.1.13. Dual vigilance fuzzy ART",
      "text": "The dual vigilance fuzzy ART (DVFA) (Brito da Silva et al., 2019) seeks retrieve arbitrarily shaped clusters with low parameterization requirements via a single fuzzy ART module. This is accomplished by augmenting fuzzy ART with two vigilance parameters, namely, the upper bound (\u03c1UB \u2208 [0, 1]) and lower bound (0 \u2264 \u03c1LB \u2264 \u03c1UB \u2264 1), representing quantization and cluster similarity, respectively.\nLTM. The categories of the dual vigilance fuzzy ART are hyperrectangles. Activation. The activation function of the dual vigilance fuzzy ART is the same as fuzzy ART\u2019s (Eq. (16) in Sec. 2.1.3). Match and resonance. When a category J is chosen by the WTA competition, it is subjected to a dual vigilance mechanism. The first match function (M1J) uses \u03c1UB in Eq. (19), whereas the second (M 2 J) is conducted using a more relaxed constraint; i.e., it uses \u03c1LB in Eq. (19). Learning. If the first vigilance criterion is satisfied, then learning proceeds as in fuzzy ART (Eq. (22)). Otherwise, the second test is performed, and, if satisfied, a new category is created and mapped to the same cluster as the category undergoing the dual vigilance tests via a mapping matrix Mmap = [mrow,col]N\u00d7K (where N is the number of categories and K is the number of clusters). Alternately, if both tests fail, then the search continues with the next highest ranked category; if there are none left, then a new node is created and the matrix Mmap expands:\nmr,c =  1, if row = N + 1 and col = K + 1\n0, if row = N + 1 and col 6= K + 1 0, if row 6= N + 1 and col = K + 1 mr,c, if row 6= N + 1 and col 6= K + 1 . (68)\nThe associations between categories and clusters are permanent in this incremental many-to-one mapping (multi-prototype representation of clusters), and they enable the data structures of arbitrary geometries to be detected by dual vigilance fuzzy ART\u2019s simple design."
    },
    {
      "heading": "2.2. Topological architectures",
      "text": "The ART models discussed in this section are designed to enable multi-category representation of clusters, thus capturing the data topology more faithfully. Generally, they are used to cluster data in which arbitrarilyshaped structures are expected (multi-prototype clustering methods)."
    },
    {
      "heading": "2.2.1. Fuzzy ART-GL",
      "text": "Fuzzy ART with group learning (fuzzy ART-GL) model (Isawa et al., 2007) augments fuzzy ART (Sec. 2.1.3) with topology learning (inspired by neural-gas (Martinetz & Schulten, 1994; Martinetz & Shulten, 1991)) to retrieve clusters with arbitrary shapes. The code representation, LTMs and dynamics of fuzzy ART remain the same. However, when a sample is presented, a connection between the first and second resonating categories (if they both exist) is created by setting the corresponding entry of an adjacency matrix to one. This model also possesses an age matrix, which tracks the duration of such connections and whose dynamics are as follows: the entry related to the first and second current resonating categories is refreshed (i.e., set to zero) following a sample presentation, whereas all other entries related to the first resonating category are incremented by one. Connections with an age value above a certain threshold expire, i.e., they are pruned (note that the threshold varies deterministically over time). This procedure allows this model to dynamically create and remove connections between categories during learning (co-occurrence of resonating categories, thus following a Hebbian approach). Clusters are defined by groups of connected categories.\nThe fuzzy ART combining overlapped category in consideration of connections (C-fuzzy ART) variant (Isawa et al., 2008a) was developed to mitigate category proliferation, which is accomplished by merging the first resonant category with another connecting and overlapping category. Another variant introduced in (Isawa et al., 2008b, 2009) augments the latter model with individual and adaptive vigilance parameters to further reduce category proliferation."
    },
    {
      "heading": "2.2.2. TopoART",
      "text": "Fuzzy topoART (Tscherepanow, 2010) is a model that combines fuzzy ART (Sec. 2.1.3) and topology learning (inspired by self-organizing incremental neural networks (Furao & Hasegawa, 2006)). Specifically, it features the same representation, activation/match functions, vigilance test and search/learning mechanisms as fuzzy ART, while integrating noise robustness and topology-based learning.\nBriefly, the topoART model consists of two fuzzy ART-based modules (topoARTs A and B) that cluster, in parallel, the data in two hierarchical levels, while sharing the same complement coded inputs. Each\ncategory is endowed with an instance counting feature n (i.e., sample count), such that every \u03c4 learning cycles (i.e., iterations) categories that encoded less than a minimum number of samples \u03c6 are dynamically removed. Once this threshold is reached, \u201ccandidate\u201d categories become \u201cpermanent\u201d categories, which can no longer be deleted. In this setup, module A serves as a noise filtering mechanism for module B. The propagation of a sample to module B depends on which type of module A\u2019s category was activated. Specifically, a sample is fed to module B if and only if the corresponding module A\u2019s resonant category is \u201cpermanent\u201d; therefore, module B will only focus on certain regions of the data space. Note that no additional information is passed from module A to B, and both can form clusters independently.\nRegarding the hierarchical structure, the vigilance parameters of modules A and B are related by\n\u03c1b = 1\n2 (\u03c1a + 1) , (69)\nsuch that module B\u2019s maximum category size is 50% smaller than module A\u2019s (\u03c1a and \u03c1b are module A\u2019s and B\u2019s vigilance parameters, respectively), which implies that module B has a higher granularity (\u03c1b \u2265 \u03c1a) and thus yields a finer partition of the data set.\nTopoART employs competitive and cooperative learning: not only the winner category J1 but also the second winner J2 is allowed to learn (naturally, both need to satisfy the vigilance criteria). The learning rates are set as \u03b2J2 < \u03b2J1 = 1, such that the second winner partially learns to encode the presented sample. If the first and second winner both exist, then they are linked to establish a topological structure. These lateral connections are permanent, unless categories are removed via the noise thresholding procedure. Clusters are formed by the connected categories, thus better reflecting the data distribution and enabling the discovery of arbitrarily-shaped data structures (topoART is a graph-based multi-prototype clustering method).\nFinally, in prediction mode, the following activation function, which is independent of category size, is used:\nTj = 1\u2212 \u2016(x \u2227wj)\u2212wj\u20161\n\u2016x\u20161 , (70)\nthe vigilance test is neglected, and only \u201cpermanent\u201d nodes are allowed to be activated. A number of topoART variants have been developed in the literature, e.g., the hypersphere topoART (Tscherepanow, 2012), which replaces fuzzy ART modules with hypersphere ARTs (Sec. 2.1.7); the episodic topoART (Tscherepanow et al., 2012) which incorporates temporal information (i.e., time variable and thus the order of input presentation) to build a spatio-temporal mapping throughout the learning process and generate \u201cepisode-like\u201d clusters; and the topoART-AM (Tscherepanow et al., 2011), which builds hierarchical hetero-associative memories via a recall mechanism."
    },
    {
      "heading": "2.3. Hierarchical architectures",
      "text": "Elementary ART modules have been used as building blocks to construct both bottom-up (agglomerative) and top-down (divisive) hierarchical architectures. Typically, these follow one of two designs (Massey, 2009): (i) cascade (series connection) of ART modules in which the output of a preceding ART layer is used as the input of the succeeding one, or (ii) parallel ART modules enforcing different vigilance criteria while having a common input layer."
    },
    {
      "heading": "2.3.1. ARTtree",
      "text": "The ARTtree (Wunsch II et al., 1993) is a way of building a hierarchy of ART neural modules in which an input sample is sent simultaneously to every module in every level of the tree. Each node in the ART tree hierarchy is connected to one of its parent\u2019s F2 categories, and each of the F2 categories in this node is connected to one of its children. The nodes in each layer of the tree hierarchy share a common vigilance value, and the vigilance typically increases further down the tree such that tiers of the tree that have more nodes are associated with higher vigilance values.\nWhen an input sample is presented to the ARTtree hierarchy, all the ART nodes can be allowed to perform their match and activation functions, but only the node connected to its parent\u2019s winning F2 category is allowed to resonate with and learn this pattern. Therefore, resonance only cascades down a single path in the ARTtree, and no other nodes outside that path are allowed to learn this sample. This can effectively allow ART to perform a type of varying-k-means clustering (Wunsch II et al., 1993).\nThe highly parallel nature of ARTtree lends itself well to hardware-based implementations, such as optoelectronic implementations (Wunsch II et al., 1993) and massively parallel implementations via general purpose Graphics Processing Unit (GPU) acceleration (Kim & Wunsch II, 2011). The study presented in (Kim & Wunsch II, 2011) performed this task using NVIDIA CUDA GPU hardware and an implementation of ARTtree that uses fuzzy ART units in the tree nodes. The results reported in the study show a massive speed boost for deep trees when compared to the CPU in terms of computing time, while smaller trees performed worse on the GPU due to the high data transfer penalties between the CPU and GPU memory."
    },
    {
      "heading": "2.3.2. Self-consistent modular ART",
      "text": "The self-consistent modular ART (SMART) (Bartfai, 1994) is a modular architecture designed to perform hierarchical divisive clustering (i.e., to represent different levels of data granularity in a top-down approach). It builds a self-consistent hierarchical structure via self-organization and uses ART 1 (Sec. 2.1.1) as elementary units. In this architecture, a number of ART modules operate in parallel with different vigilance parameter values, while receiving the same input samples and connecting in a manner that makes the hierarchical cluster representation self-consistent. These connections are such that many-to-one mapping of specific to general categories is learned across such modules. Specifically, the hierarchy is explicitly represented via associative links between modules.\nConcretely, a two-level SMART architecture can be implemented using an ARTMAP (Sec. 3.1.1) in autoassociative mode; i.e., ARTMAP is used in an unsupervised manner by presenting the same input sample to both modules A and B with different vigilance parameters and forcing a hierarchical structure by making \u03c1A > \u03c1B , such that module B enforces its categorization (an internal supervision) on module A."
    },
    {
      "heading": "2.3.3. ArboART",
      "text": "ArboART (Ishihara et al., 1995) is an agglomerative hierarchical clustering method based on ART. More specifically, it uses ART 1.5-SSS (small sample size) (Ishihara et al., 1993) (variant of ART 1.5 (Levine & Penz, 1990), which in turn is a variation of ART 2 (Carpenter & Grossberg, 1987b)), as a building block. Briefly, prototypes of one ART are the inputs to another ART with looser vigilance (similarity constraint). Therefore, prototypes obtained from a lower level (bottom part of the dendrogram) are fed to the next ART layer. ART modules on higher layers have decreasingly lower vigilance values, i.e., the similarity constraint is less strict. This enables the construction of a tree (hierarchical graph structure). One of the advantages over traditional hierarchical methods is that it does not require a full recomputation when a new sample is added, only partial recomputations are needed in ART (inside the specific clusters). ArboART uses several layers of ART as well as one pass learning. Concretely, it makes super-clusters of previous clusters in a hierarchical way, thereby making a generalization of categories in the process."
    },
    {
      "heading": "2.3.4. Joining hierarchical ART",
      "text": "The joining hierarchical ART (HART-J) (Bartfai, 1996) is a hierarchical agglomerative clustering method (bottom-up approach) that uses ART 1 modules (Sec. 2.1.1) as building blocks and follows a cascade design. Specifically, each layer of this multi-layer model corresponds to an ART 1 network that clusters the prototypes generated by the preceding layer. The input of layer l is given by:\nxl = x1 \u2229 ( l\u22121\u22c2 i=1 wi,J ) , l = {1, ..., L}, (71)\nwhere L is the number of layers, x1 is equal to the input sample x, and wi,J is the resonant neuron J of layer i. Interestingly, it is not imperative to reduce the vigilance values at higher layers to generate the hierarchy: the \u201ceffective\u201d vigilance level of layer l is given by:\n\u03c1\u0302l = l\u220f j=1 \u03c1j . (72)\nwhich decreases even if the vigilance increases with l given that \u03c1l \u2208 [0, 1] \u2200l. This fact is used to derive an upper bound for the maximum number of layers Lmax. If all vigilance values are equal to \u03c1, then\nLmax = bn+ 1e, where n is the minimum integer that satisfies\nn > \u2212 logK log \u03c1 , (73)\nassuming that \u2016xl\u20161 = constant = K. Naturally, succeeding networks can learn at most the number of prototypes from the previous layer. Learning can occur in sequential (waiting for stabilization before the next layer starts learning) or parallel (learning occurs in each layer in each presentation of inputs) modes. The former generates fewer categories but the training time, measured in number of epochs, is much smaller using the parallel approach.\nHART-J is compared to SMART in (Bartfai, 1995). Contrary to SMART, HART-J has no associative connection or feedback between hierarchical layers as a mechanism to enforce self-consistency. The constraint causing lower layers to have larger vigilance values than the higher layers guarantees consistency. In HART-J, the hierarchies \u201cemerge\u201d since there are no explicit links. It is reported that SMART builds a less compact model (larger number of categories) due to categorization forced by its internal feedback mechanism, whereas HART-J builds a simpler and more compact network."
    },
    {
      "heading": "2.3.5. Hierarchical ART with splitting",
      "text": "The hierarchical ART with splitting (HART-S) (Bartfai & White, 1997b) consists of a cascade of ART 1 (Sec. 2.1.1) modules that performs incremental hierarchical divisive clustering (successive splitting in a top-down approach). A fuzzy HART-S (Bartfai & White, 1997a) variant uses a cascade of fuzzy ARTs, where each module clusters the difference between the input and the weight vector of the resonant category belonging the preceding layer. Specifically, the input to layer l + 1 (l = {1, ..., L}, where L is the maximum number of layers) is given by:\nxl+1 = x1 \u2227wcl,J , (74)\nwhich recursively corresponds to\nxl+1 = x1 \u2227 ( l\u2227 i=1 wci ) , (75)\nwhere x1 = x is the data sample and w c l,J is the complement of the weight vector associated with the resonant neuron J of layer l. The hierarchy is explicitly represented by links between parent and children categories in a tree-like structure. These adaptive associative connections between consecutive modules ensure that only children of the preceding parent module can be activated. In its most general case, the fuzzy ART modules in each layer have their own set of parameters. Particularly, Fuzzy HART-S uses two global parameters: a resolution parameter to control the depth of the hierarchical tree (i.e., if |xk| < S, then there is no more splitting, where S is the maximum size of root/global input x1) and a feature threshold parameter to control the propagation of features throughout the layers.\nStrategies to prune and rebuild prototypes to improve HART-S in terms of network complexity (measured by the number of categories) are presented in (Bartfai & White, 1998). During learning, the former strategy removes small clusters (and all their children if applicable) based on a cluster size threshold (percentage of the total number of samples), and the latter changes the components of a prototype weight vector to better reflect the samples associated with them."
    },
    {
      "heading": "2.3.6. Distributed dual vigilance fuzzy ART",
      "text": "The distributed dual vigilance fuzzy ART (DDVFA) (Brito da Silva et al., 2018) is a dual vigilancebased ART model designed to improve memory compression and perform several ART-based hierarchical agglomerative clustering (HAC) methods online. It consists of a global ART module whose F2 nodes are local fuzzy ARTs: the global module is used for decision making while the local module builds multi-prototype representations of clusters (many-to-one mappings).\nThe activation of a global ART F2 node i (T g i ) is a function of the activations of the k F2 nodes of its\ncorresponding local fuzzy ART module: T gi = f ( T i1 , ... , T i j , ... , T i k ) , (76)\nwhere T ij is the activation function of the F2 node j of the local fuzzy ART module i, which uses a higher order activation function defined as\nT ij = ( \u2016x \u2227wij\u20161 \u03b1+ \u2016wij\u20161 )\u03b3 , (77)\nand \u03b3 \u2265 1 is a power parameter whose role is akin to a kernel width. Similarly, the match function of a global ART F2 node i (M g i ) is defined as\nMgi = g ( M i1 , ... , M i j , ... , M i k ) , (78)\nwhere M ij is the match function of the F2 node j of the local fuzzy ART module i, which uses the following normalized higher order match function\nM ij = ( \u2016wij\u20161 \u2016x\u20161 )\u03b3\u2217 T ij , (79)\nwhere 0 \u2264 \u03b3\u2217 \u2264 \u03b3 is the reference kernel width with respect to which the match function is normalized. Both functions f(\u00b7) and g(\u00b7) are based on HAC methods, as listed in Table 2.\nThe DDVFA features a dual vigilance mechanism: when a sample x is presented and the F2 node I of the global ART is the winner, then V RI = {x : MgJ (x) \u2265 \u03c1LB} and \u03c1LB \u2208 [0, 1]. The vigilance criterion checks if 1V RgI (x) is true. If not, the search continues, or a new local fuzzy ART module is created. If so, the corresponding local fuzzy ART module is allowed to learn. The local Fuzzy ART module imposes a stricter constraint for its winner nodes: V RIJ = {x : M IJ (x) \u2265 \u03c1UB} and 0 \u2264 \u03c1LB \u2264 \u03c1UB \u2264 1. Again, the vigilance criterion checks if 1V RIJ (x) is true, and, if so, the category is allowed to learn. Otherwise, the search resumes or a new node is created following the standard ART dynamics.\nWhen input order cannot be addressed via an offline pre-processing strategy (Sec. 6.1), then DDVFA should be used in conjunction with a Merge ART module to mitigate input order dependency in online learning applications. This module is connected to DDVFA in series, i.e., in a cascade design. The inputs to Merge ART are fuzzy ART modules with all their corresponding categories. Like DDVFA, Merge ART\u2019s F2 nodes are also fuzzy ART modules. When a DDVFA\u2019s fuzzy ART node l is fed to Merge ART, an activation matrix Tk,l = [ti,j ]R\u00d7C (where R and C are the number of categories in Merge ART node k and DDVFA node l, respectively) is computed as\nti,j = ( \u2016wlj \u2227wki \u20161 \u03b1+ \u2016wki \u20161 )\u03b3 , (80)\nwhere wlj is the weight vector of category j of DDVFA local fuzzy ART module l, and w k i is the weight vector of category i of Merge ART module k. The actual activation of Merge ART node k uses matrix Tk,l and follows one of the HAC forms as listed in Table 3. Assuming Merge ART\u2019s F2 node k is the winner, its match matrix Mk,l = [mi,j ]R\u00d7C is computed as\nmi,j = ( \u2016wki \u20161 \u2016wlj\u20161 )\u03b3\u2217 ti,j , (81)\nwhere the actual match of Merge ART node k uses matrix Mk,l and also uses one of the formulations listed in Table 3. If the vigilance constraint is satisfied (i.e., Mk \u2265 \u03c1LB), then ARTK(new)\u2190 ARTK(old) \u222aARTl, i.e., the weights of both ART modules are concatenated. To further reduce model complexity, the final step of Merge ART consists of feeding the weight vectors of each ART module to an independent fuzzy ART parameterized with \u03c1 = \u03c1UB , \u03b3 and \u03b3\n\u2217. Note that the Merge ART module can be run once or until convergence, where the latter is defined as no change in the Merge ART nodes between two consecutive iterations."
    },
    {
      "heading": "2.4. Biclustering and data fusion architectures",
      "text": ""
    },
    {
      "heading": "2.4.1. Fusion ART",
      "text": "Fusion ART (Tan et al., 2007) extends ART capabilities by augmenting it with multiple and independent F1 layers (or input channels/field), all of which are connected to a shared F2 layer. This model is then capable of learning mappings across multiple channels simultaneously.\na,b ki is the number of F2 nodes in local fuzzy ART module i. b pj = nij ngi , where nij is the number of samples encoded by category j of local fuzzy ART module i, and\nngi = \u2211 j nij . c wc is a centroid, whose l component is computed as wc,l = min j (wj,l) for l = {1, ..., 2d}.\na pi = nki nk and pj = nlj nl , where nik is the number of samples encoded by category i of Merge ART node k,\nand nk = \u2211 i nki . The variables n l j and nl refer to DDVFA node l and are defined similarly. b wkc and w l c are the centroids representing all categories of ART (2) k and ART (1) l , respectively. Their components are given by wkc,n = min j ( wkj,n ) and wlc,n = min j ( wlj,n ) , where n = {1, ..., 2d}.\nActivation. The activation function of a category j is a weighted sum of the activation functions of each input field\nTj = K\u2211 k=1 \u03b3k \u2016xk \u2227wkj \u20161 \u03b1k + \u2016wkj \u20161 , (82)\nwhere xk is the complement coded input to the kth F1 layer (F k 1 or channel k), and \u03b3 k \u2208 [0, 1] and \u03b1k \u2208 (0,\u221e) are the contribution and choice parameters of F k1 , respectively. The variable K is the total number of input channels such that x = [x1, ...,xk, ...,xK ] and category j\u2019s LTM is wj = [w 1 j , ...,w k j , ...w K j ].\nMatch and resonance. When category J is selected by the WTA competition, one match function is computed for each channel\nMkJ = \u2016y(Fk1 )\u20161 \u2016xk\u20161 = \u2016xk \u2227wkJ\u20161 \u2016xk\u20161 , (83)\nwhere V RkJ = {x : MkJ (x) \u2265 \u03c1k}, and \u03c1k \u2208 [0, 1] is F k1 \u2019s vigilance parameter. The vigilance test must be satisfied for all input fields simultaneously. Otherwise, a mismatch triggers a category reset and a match tracking procedure takes place. Particularly, the global vigilance criterion is satisfied if all channels meet\ntheir individual vigilance criteria, i.e., if K\u2227 k=1 1kV RJ (x) = 1. If this condition is not satisfied, fusion ART\u2019s match tracking mechanism simultaneously raises all vigilance parameters until a mismatch is triggered in one of the channels. The search then continues until a resonant category is found or created. Then, learning takes place as\nwkJ(new) = (1\u2212 \u03b2k)wkJ(old) + \u03b2k(xk \u2227wkJ(old)), \u2200k, (84)\nwhere \u03b2k \u2208 (0, 1] is the learning parameter of layer F k1 . When a new input is presented, \u03c1k = \u03c1\u0304k, where \u03c1\u0304k is the baseline vigilance of layer F k1 . Additionally, if an input to a channel is not present, then it is set to ~1 to enable the prediction/recovery of missing values.\nNotably, fusion ART generalizes some other ART models, i.e., by appropriately designing fusion ART, it can reduce to different ART models and perform distinct machine learning modalities: (i) 1 channel (samples) fusion ART reduces to ART (Carpenter et al., 1991c) (Sec. 2.1.3) and performs match-based unsupervised learning, (ii) 2 channels (samples and class labels) fusion ART reduces to adaptive resonance associative map - ARAM (Tan, 1995) (Sec. 3.1.7) and performs association-based supervised learning and (iii) 3 channels (states, actions and rewards) fusion ART reduces to fusion architecture for learning, cognition, and navigation - FALCON (Tan, 2004) (Secs. 4.1 and 4.2) and performs reinforcement learning. Additionally, fusion ART can perform instruction-based learning by rule-based knowledge integration (generation of IF-THEN rules mapping antecedents and consequents from one channel to another, and rule insertion capability)."
    },
    {
      "heading": "2.4.2. Biclustering ARTMAP",
      "text": "Biclustering ARTMAP (BARTMAP) (Xu & Wunsch II, 2011; Xu et al., 2012) is based on fuzzy ARTMAP (Carpenter et al., 1992) (Sec. 3.1.2) and was designed to find correlation-based subspace clustering. It uses two Fuzzy ART modules (ARTa and ARTb) connected through a regulatory inter-ART module to achieve a biclustering of the data matrix on both the input space (rows) and the feature space (columns). The ARTb module is used to cluster the feature vectors and create a set of feature clusters. Then, the samples are presented to the ARTa module while using the inter-ART module to integrate the clustering results on both the feature and input spaces and create biclusters that capture the local relations between the inputs and features. Note that BARTMAP learns in offline mode. This architecture was shown to perform fast and stable biclustering of gene expression data (Xu & Wunsch II, 2011) and later modified to build a collaborative filtering recommendation system (Elnabarawy et al., 2016).\nThe BARTMAP algorithm begins by presenting all the feature vectors to ARTb (which is a standard fuzzy ART module), using it to build clusters of the feature vectors. Next, it begins presenting the input vectors to ARTa and allows it to build clusters in the input space. If ARTa places an input in a previously committed category, the inter-ART module then computes the similarity between the new sample and the samples in the existing cluster, but only within each feature cluster from ARTb, thereby testing the correlation between the new sample and each of the existing biclusters. If any of the biclusters passes a user-defined correlation threshold \u03b7, the cluster is updated with the new sample. However, if none of the current biclusters passes, the ARTa vigilance threshold is temporarily increased (match tracking mechanism, see Sec. 3.1.1), and the sample is presented again to find a new cluster. If no suitable cluster is found that also satisfies the correlation threshold, the ARTa vigilance will eventually be increased enough to force the creation of a new cluster.\nConsider the data matrix X = [xi,j ]N\u00d7d, encompassing N samples in a d-dimensional feature space.\nAfter ARTb detects Nb clusters of features, the k th input to ARTa becomes xk = [x cb1 k , ...,x cbi k , ...,x cbNb k ] \u2208 Rd, where x cbi k comprises the subset of components of xk associated with the i\nth feature cluster identified by ARTb (c b i ). The similarity between the input sample xk and an ARTa cluster c a j with n a j samples, across an\nARTb feature cluster c b i with n b i features, is defined using the average Pearson correlation coefficient (Bain & Engelhardt, 1992) as follows:\nr\u0304caj ,cbi (xk) = 1\nnaj naj\u2211 l=1,xl\u2208caj rcaj ,cbi (x cbi k ,x cbi l ), (85)\nwhere\nrcaj ,cbi (x cbi k ,x cbi l ) =\nnbi\u2211 t=1 (x cbi k,t \u2212 x\u0304 cbi k )(x cbi l,t \u2212 x\u0304 cbi l )\u221a\nnbi\u2211 t=1 (x cbi k,t \u2212 x\u0304 cbi k ) 2 \u221a nbi\u2211 t=1 (x cbi l,t \u2212 x\u0304 cbi l ) 2 . (86)\nHere, x cbi m,t refers to the value for sample xm at feature t within the ARTb cluster c b i (m = k, l). Similarly,\nx\u0304 cbi m denotes the average value of xm across all the features in ARTb\u2019s cluster c b i :\nx\u0304 cbi m = 1\nnbi nbi\u2211 t=1 x cbi m,t. (87)"
    },
    {
      "heading": "2.4.3. Generalized heterogeneous fusion ART",
      "text": "The generalized heterogeneous fusion ART (GHF-ART) (Meng et al., 2014) is a model designed to perform co-clustering of heterogeneous data (i.e., mixed data types). It extends the heterogeneous fusion ART (HF-ART) (Meng & Tan, 2012), which is a two-channels fusion ART-based model, to a multiple channel architecture. The distinctive characteristic of the generalized heterogeneous fusion ART is that its learning functions vary according to each data type, i.e., when a winner node J satisfies the vigilance criterion, different channels are adapted following different learning functions fkL(\u00b7). For instance, if the input xk corresponds to a visual feature from image data or a text feature from a document, then the corresponding weight vector is updated following Eq. (84). Alternately, if xk is a feature from data meta-information, then the weight vector of the corresponding channel k is adapted using the recursive mean formula\nwkJ(new) =\n( 1\u2212 1\nnJ(new)\n) wkJ(old) +\n1\nnJ(new) xk, (88)\nnJ(new) = nJ(old) + 1, (89)\nwhere nJ corresponds to the number of samples encoded by node J . Another key characteristic of the generalized heterogeneous fusion ART is the adaptive channel weighting: the contribution parameters are initially uniformly initialized, and then, during learning, undergo self-adaptation using\n\u03b3k(new) = Rk K\u2211 k=1 Rk , \u2200k, (90)\nwhere\nRk = exp \u2212 1 N N\u2211 j=1 Dkj  , (91)\nDkj =\n1 nj nj\u2211 l=1 \u2016wkj \u2212 xkl \u20161\n\u2016wkj \u20161 . (92)\nThe variable R is a robustness measure used to estimate the discriminative power of each channel given the intra-cluster scatter. In practice, performing the offline computations in Eq. (92) can be expensive.\nTherefore, since only DkJ needs to be updated after the presentation of each sample, then \u03b3 k(new) can be estimated incrementally. Particularly, when there is a resonant committed node J , if xk is a meta-information feature, then\nDkJ(new) = nJ(old)\nnJ(new)\u2016wkJ(new)\u20161( \u2016wkJ(old)\u20161D k J(old)\u2212 \u2016wkJ(new)\u2212 nJ(old)\nnJ(new) wkJ(old)\u20161 +\n1\nnJ(old) \u2016wkJ(new)\u2212 xk\u20161\n) , (93)\notherwise,\nDkJ(new) = nJ(old)\nnJ(new)\u2016wkJ(new)\u20161( \u2016wkJ(old)\u20161D k J(old)\u2212 \u2016wkJ(old)\u2212wkJ(new)\u20161 +\n1\nnJ(old) \u2016wkJ(new)\u2212 xk\u20161\n) .\n(94)\nIf a new category is created, regardless of xk type, the contribution parameters are updated via a proportionality change\n\u03b3k(new) =\n( Rk ) N N+1\nK\u2211 k=1 (Rk) N N+1 , \u2200k, (95)\nwhere N is the number of categories. Note that the generalized heterogeneous fusion ART can also include prior knowledge by appropriate initialization of the network."
    },
    {
      "heading": "2.4.4. Hierarchical Biclustering ARTMAP",
      "text": "Hierarchical Biclustering ARTMAP (H-BARTMAP) (Kim, 2016) uses BARTMAP (2.4.2) iteratively to obtain a hierarchy of biclusters. The algorithm begins by running BARTMAP on the complement coded data with low vigilance values, which produces a relatively small number of larger-sized biclusters. In the following step, H-BARTMAP uses a bicluster matching threshold and a correlation fitness function to build and evaluate the biclusters at the current level. After that, the BARTMAP algorithm is used again on each of the resulting clusters with increased vigilance and correlation thresholds. These are adjusted by small values that are a function of the number of samples as well as the number of features and average correlation in each bicluster. The H-BARTMAP algorithm repeats those two steps recursively for a specified number of times. Then, the best layer in the recursive tree that optimizes the desired cluster validity index (Xu & Wunsch II, 2009) or any other user-specified criteria is chosen."
    },
    {
      "heading": "2.5. Summary",
      "text": "Table 4 summarizes the nature of the category representations of the ART elementary models described in the previous subsections, during activation, match and learning stages. Particularly, it lists if winner-takes-all (WTA) or distributed (D) coding is employed by these networks."
    },
    {
      "heading": "3. ART models for supervised learning",
      "text": ""
    },
    {
      "heading": "3.1. Architectures for classification",
      "text": "ART models used for supervised learning applications typically follow an ARTMAP architecture (Fig. 2), which consists of two elementary ART units (ARTa and ARTb) interconnected by an associative learning network, namely the map field, that performs multidimensional mappings between categories of both such units, as well as allowing for associative recalls when the input to one of the ART modules is missing. Notably, ARTMAP models usually inherit the properties of their elementary ART building blocks. This section describes the main characteristics of members of the supervised ART family in terms of their map field LTM units, dynamics (which encompasses activation, match, resonance criterion and learning) and user-defined parameters. For clarity, Table 5 summarizes the notation used in the following subsections.\nWhen an ARTMAP architecture is used for pattern recognition or classification tasks, typically ARTa clusters data samples while ARTb clusters class labels in parallel. Therefore, while ART maps samples to categories, an ARTMAP architecture goes one step further and maps categories to classes. During training, ARTa is subjected to a certain level of agreement with ARTb\u2019s activity, given that the latter is encodes the target labels. This is performed by a second vigilance test that uses ARTb\u2019s supervisory signal (i.e., response) to trigger a mismatch or allow learning given an incorrect or correct prediction, respectively. Specifically, when ARTa\u2019s prediction is disproven by ARTb\u2019s, then the map field triggers a match tracking mechanism in which ARTa\u2019s resonating category is inhibited, the baseline vigilance is temporarily changed and the search process restarts, causing ARTa to select another category. Therefore, the map field is a critic, i.e., its purpose is to assess the quality of the mapping between both ART modules and also the necessity of adding a new node based on a supervised signal. Specifically, by engaging the match tracking mechanism, ARTMAP trades generalization for specificity to decrease training error. Often, ARTb is omitted and an Nb-dimensional vector of labels is used in its place (since ARTb\u2019s vigilance parameter would typically be set to 1, which would correspond to the number of categories being equal to the number of classes). Moreover, ARTa\u2019s baseline vigilance parameter, which controls the granularity of the input space, is usually set to small values since this correlates with improved generalization capabilities and a higher level of compression, i.e., network complexity. Algorithm 2 summarizes the dynamics of an elementary ARTMAP model."
    },
    {
      "heading": "3.1.1. ARTMAP",
      "text": "The first adaptive resonance theory supervised predictive mapping (predictive ART or ARTMAP) model (Carpenter et al., 1991a) consists of two binary ART 1 modules (Section 2.1.1), ARTa and ARTb, connected via an inter-ART associative memory, namely the map field Fab. The latter performs multidimensional mappings between the binary input samples clustered by modules A and B. Moreover, when the input of a module is missing, it can be recalled by such associative memory. The map field LTM is represented by a matrix W ab = [wabij ]Na\u00d7Nb such that w ab ij = 1 if there is an association between category i of ARTa and category j of ARTb and zero otherwise, and Na and Nb are the number of nodes in ARTa and ARTb, respectively. The matrix W ab is initialized as 1 (i.e., the row vector wab1 = ~1). The bottom-up and top-down weight vectors of both ART 1\u2019s are initialized as described in Section 2.1.1.\nAlgorithm 2: Elementary ARTMAP algorithm.\nInput : {xa,xb}, {ARTa and ARTb parameters}, {\u03b2ab,\u03b3ab,\u03c1ab,\u03bbab} (map field parameters). Output: y(F ab) (map field activity).\n/* Notation Cl: set of ARTl nodes (l = a, b). \u03b8ab: map field LTM unit. \u03b2ab: map field learning function parameter(s). \u03b3ab: map field match function parameter(s). \u03c1ab: map field vigilance parameter(s). \u03bbab: map field initialization parameter(s). fabM (\u00b7): map field match function. fabL (\u00b7): map field learning function. fabV (\u00b7): map field vigilance function. fabN (\u00b7): map field initialization function. fabI (\u00b7): map field inference function. */ /* Training */\n1 Present input xb \u2208Xb to ARTb. 2 Perform the dynamics of ARTb and find its resonating category K (Alg. 1). 3 Present input xa \u2208Xa to ARTa. 4 Perform the dynamics of ARTa and find its resonating category J (Alg. 1). 5 Compute the map field match function: MabJ = f ab M (J,K,\u03b8 ab,\u03b3ab). 6 Perform the map field vigilance test: VJ = f ab V = 1V Rab\nJ (xa).\n7 if VJ is TRUE then 8 Update ARTa\u2019s and ARTb\u2019s categories J and K (Alg. 1). 9 if ARTa OR ARTb created a new node then\n10 \u03b8ab|Ca|+1 = f ab N (J,K,\u03bbab). 11 else 12 Update the map field: \u03b8abJ (new) = f ab L (x,\u03b8 ab J (old),\u03b2ab)."
    },
    {
      "heading": "13 else",
      "text": "14 Inhibit ARTa\u2019s category J for x\na."
    },
    {
      "heading": "15 Go to step 3.",
      "text": ""
    },
    {
      "heading": "16 Go to step 1.",
      "text": "/* Inference */ 17 Present input xa \u2208Xa to ARTa. 18 Perform the dynamics of ARTa (Alg. 1). 19 Compute the degree of association to each ARTb node k according to ARTa\u2019s activity(s): \u03c3k = f ab I (y Fa2 ,\u03b8ab).\n20 Set output: y (Fab) j = 1, if j = arg maxk (\u03c3k)0, otherwise .\nTraining. The map field Fab activity is defined as\ny(F ab) =  y(F b 2 ) \u2229wabJ , if both ARTs are active (training) wabJ , if only ARTa is active (prediction) y(F b 2 ), if only ARTb is active\n~0, otherwise\n. (96)\nwhere wabJ = (wJ1, ..., wJNb) is the J th row of W ab, which is associated with ARTa\u2019s resonant category J .\nAfter resonant nodes for both ART modules have been selected following the presentation of a sample\npair (xa,xb), the map field match function is computed as\nMabJ = \u2016y(Fab)\u20161 \u2016y(F b2 )\u20161 = \u2016y(F b2 ) \u2229wabJ \u20161 \u2016y(F b2 )\u20161 , (97)\nwhere the vigilance test is satisfied if MabJ \u2265 \u03c1ab. During training, if ARTa\u2019s prediction is correct (i.e., confirmed by ARTb\u2019s supervised signal feedback), all three modules learn. Otherwise, a match tracking mechanism (MT+) is engaged, such that ARTa\u2019s vigilance parameter is temporarily raised by an amount small enough to inhibit the resonant category\n\u03c1a = M a J + , 0 < 1, (98)\nand the search process restarts. Either another resonant category is found or a new one is created, and the vigilance returns to its baseline value (\u03c1a = \u03c1\u0304a) upon the presentation of a new input pair. Complement coding is usually employed to avoid cases in which ARTa\u2019s vigilance is raised to a value greater than one.\nNow consider that the resonant categories of ARTa and ARTb are J and K, respectively. When the map field vigilance test is satisfied (MabJ \u2265 \u03c1ab), then ARTa and ARTb are updated as described in Sec. 2.1.1, and the map field weight vector associated with category J is updated as\nwabJk(new) = y (F b2 ) \u2229wabJ (old) =\n{ 1, if k = K\n0, otherwise (99)\nsuch that it becomes permanently associated with ARTb\u2019s category K. Note that the F a 1, F a 2 and F ab layers may be viewed as input, hidden and output layers, respectively.\nInference. In prediction mode, it is sufficient to track the map field\u2019s weight vector wabJ and set it as the systems\u2019 output, i.e., when an ARTa\u2019s resonant category J is found, the predicted class K is obtained as\nK = arg max k (\u03c3k) , (100)\nwhere\n\u03c3k = Na\u2211 j=1 wabjky (Fa2 ) j . (101)\nA simplified ARTMAP version, namely the simple ARTMAP (Serrano-Gotarredona et al., 1998), replaces\nARTb (and thus its F b 2 activity y (F b2 )) with a binary vector yb indicating the class membership of the input sample xa (i.e., ybk = 1 if x a belongs to class k, and ybi = 0 \u2200i 6= k)."
    },
    {
      "heading": "3.1.2. Fuzzy ARTMAP",
      "text": "Fuzzy ARTMAP (FAM) (Carpenter et al., 1992) is to ARTMAP what fuzzy ART is to ART 1: it extends the capabilities of ARTMAP to enable the processing of real-valued data by replacing logical with fuzzy AND intersection. Thus, fuzzy ARTMAP also consists of two fuzzy ART modules, ARTa and ARTb, connected by a map field Fab that maps the categories of one ART to another via a matrix of weights W ab, as described in Sec. 3.1.1.\nTraining. The map field Fab activity is defined as\ny(F ab) =  y(F b 2 ) \u2227wabJ , if both ARTs are active (training) wabJ , if only ARTa is active (prediction) y(F b 2 ), if only ARTb is active\n~0, otherwise\n. (102)\nDuring training, ARTa and ARTb perform their dynamics (Section 2.1.3) simultaneously and independently, with their respective inputs, until both establish resonant nodes J and K, respectively. Then, the\nmap field computes its activity vector using these two pieces of information, as defined in Eq. (102). Next, a second (map field) vigilance test is performed to assess the mapping correctness using\nMabJ = \u2016y(Fab)\u20161 \u2016y(F b2 )\u20161 = \u2016y(F b2 ) \u2227wabJ \u20161 \u2016y(F b2 )\u20161 , (103)\nand, if it satisfies MabJ \u2265 \u03c1ab (\u03c1ab \u2208 [0, 1]), then learning takes place. Otherwise, in response to a mismatch, the match tracking mechanism (M+) is triggered: the current resonating category J is inhibited (lateral reset), ARTa\u2019s vigilance parameter is raised by a small constant (Eq. (98)) and the search continues with the remaining nodes until a resonant category that satisfies both \u03c1a and \u03c1ab is either found or created. Finally, \u03c1a is reset to its baseline value \u03c1a = \u03c1\u0304a for the presentation of the following sample. However, the study in (Anagnostopoulos & Georgiopoulos, 2003) indicates that not using match tracking (MT+) reduces the computational burden and model complexity while improving generalization capabilities (Andonie & Sasu, 2006).\nIn both fuzzy ART modules learning is ensued as described in Section 2.1.3, whereas in the map field, its weight vectors are updated such that a permanent association is made between the active nodes of ARTa and ARTb\nwabJk(new) = y (F b2 ) \u2227wabJ (old) =\n{ 1, if k = K\n0, otherwise . (104)\nNote that uncommitted nodes participate in the WTA competition. They are initialized as ~1, and the ARTa\u2019s ones are mapped to all ARTb nodes. A slow-learning mode was introduced in (Carpenter et al., 1995):\nwabJ (new) = (1\u2212 \u03b2ab)wabJ (old) + \u03b2ab [ y(F b 2 ) \u2227wabJ (old) ] , (105)\nwhere \u03b2ab is the map field\u2019s learning rate, and the conditional probability p(c b K |caJ) can be estimated nonparametrically as\np\u0302(cbK |caJ) = wabJK Nb\u2211 i=1 wabJi . (106)\nInference. In testing mode only ARTa is active. Its output is used to make a prediction and concretely retrieve the labels from ARTb via the F\nab\u2019s weight matrix (Eqs. (100) and (101)). Note that training, prediction/inference and learning are all WTA (based on a single category).\nThe simplified fuzzy ARTMAP (SFAM) (Kasuba, 1993) is a simplification of the original fuzzy ARTMAP specifically devised for classification tasks, in which, like simple ARTMAP in Sec. 3.1.1, ARTb is replaced by vectors indicating the class labels. Another simplified design is discussed in (Vakil-Baghmisheh & Paves\u030cic\u0301, 2003)."
    },
    {
      "heading": "3.1.3. Fuzzy Min-Max",
      "text": "Fuzzy Min-Max (Simpson, 1992) is a supervised learning neural network classifier that uses fuzzy sets for its internal categories, like its clustering counterpart (Sec. 2.1.4). It is composed of three layers of neurons: an input layer FA, a layer of hyperbox nodes FB and a layer of class nodes FC. The hyperbox fuzzy sets are adjusted using an expansion-and-contraction-based fuzzy min-max classification learning algorithm that adjusts the fuzzy associations between the inputs and classes. It accomplishes that by identifying which hyperbox to expand for each input and expanding it correspondingly. Then, it identifies any resulting overlap between hyperboxes of different classes and minimally adjusts these hyperboxes to eliminate the overlap."
    },
    {
      "heading": "3.1.4. Fusion ARTMAP",
      "text": "Fusion ARTMAP (Asfour et al., 1993) is a modular neural network model designed to classify data originating from multiple sources (i.e., to perform sensor fusion). It generalizes fuzzy ARTMAP (Sec. 3.1.2) by incorporating multiple ART modules, one for each sensor. The outputs of these local ART modules are fed to a fuzzy ARTMAP, specifically, to the latter\u2019s ARTa module, since ARTb receives the class labels.\nAnother key feature of fusion ARTMAP is the parallel match tracking. Following an incorrect prediction, the vigilance parameter of each ART module is raised (individual ARTs and fuzzy ARTMAP\u2019s ARTa)\n\u03c1k = \u03c1\u0304k + \u2206\u03c1, \u2200k, (107)\n\u2206\u03c1 = (MnJ \u2212 \u03c1\u0304n) + , (108)\nn = arg min k\n( MkJ ) , (109)\nwhere \u03c1k and \u03c1\u0304k are the vigilance and baseline vigilance of ART module k, respectively. Each ART module can have its own baseline vigilance parameter, or the entire fusion ARTMAP system can have a single common baseline vigilance. The variable MkJ is the match function value of ART module k\u2019s category J . Note that ART module n yielded the smallest match value and is therefore deemed the least predictive.\nThe vigilance values of the local ART modules and fuzzy ARTMAP\u2019s ARTa are increased by the same value, which is enough to promote a mismatch in ART module n. Therefore, the latter is forced to promote a new search, while the other modules maintain their output. This procedure enables credit assignment to specific modules instead of uniformly blaming all modules regardless of their predictive power. Fusion ARTMAP improves memory compression (compared to single-ART module systems that concatenate all sensor data into a single large vector) given the sharing of the local ART\u2019s weight vectors across fuzzy ARTMAP.\nThe generalized symmetric fusion ARTMAP (Asfour et al., 1993) replaces fuzzy ARTMAP with a global ART module that receives the outputs of all local ART modules and is responsible for the decision-making process. This model can handle multiple input sensors and multiple supervised inputs. In cases consisting of only one supervised input, the functionality is reduced to fusion ARTMAP."
    },
    {
      "heading": "3.1.5. LAPART",
      "text": "The LAPART 1 (Healy et al., 1993) and LAPART 2 (Healy & Caudell, 1998) neural networks are two ART-based logic inference and supervised learning architectures. The LAPART 1 architecture uses two ART 1 networks A and B to learn logic inference and association, wherein if network A assigns its input sample to a category, that results in network B assigning its input to the corresponding category. It then uses the learned inference associations between the two networks to test hypotheses and classification decisions. The LAPART 2 algorithm uses the same architecture but introduces a lateral reset procedure and builds a rule extraction network that was shown to converge in two passes through the training data."
    },
    {
      "heading": "3.1.6. ART-EMAP",
      "text": "Adaptive resonance theory with spatial and temporal evidence integration (ART-EMAP) (Carpenter & Ross, 1995) augments fuzzy ARTMAP with a number of features to deal with noisy or ambiguous data: distributed representation during inference, integration of spatial-time information, extension of the map field into a multiple field EMAP module and a fine-tuning unsupervised learning stage (rehearsal).\nTraining. ART-EMAP training is identical to fuzzy ARTMAP\u2019s (Sec. 3.1.2). Inference. ART-EMAP introduces two contrast enhancement procedures for distributed activation: the\nnormalized power rule defined as\ny (Fa2 ) j =\n(T aj ) p\nNa\u2211 i=1 (T ai ) p , p > 1, (110)\nand the threshold rule\ny (Fa2 ) j = [T aj \u2212 T ]+ Na\u2211 i=1 [T ai \u2212 T ]+ (111)\nwhere T is a threshold parameter, and [\u03be]+ = max{0, \u03be} is a rectifier operation. The activity of the first map field F ab1 is then defined as\ny(F ab 1 ) = Sab (112)\nwhere\nSabk = Na\u2211 j=1 wabjky (Fa2 ) j , (113)\nA class is predicted using such distributed representation via the second map field activity F ab2\ny (Fab2 ) k =\n{ 1, if k = K\n0, otherwise , (114)\nwhere K = arg max\nk\n[ y\n(Fab1 ) k\n] . (115)\nTo address ambiguity (i.e., categories with similar activation values), the F ab2 activity can be redefined as:\ny (Fab2 ) k =\n{ 1, if y\n(Fab1 ) k > (DC)y (Fab1 ) j \u2200j 6= k\n0, otherwise (116)\nwhere DC \u2265 1 is a decision criterion. While y(F ab 2 )\nk = ~0, the system waits for another input (i.e., data\nsamples from the same and yet unknown class) until the inequality in Eq. (116) is satisfied. Moreover, the power rule can also be applied to the F ab1 activity\ny (Fab1 ) k =\n(Sabk ) q\nNb\u2211 i=1 (Sabi ) q , q > 1, (117)\nwhere the q is the power parameter. To handle noisy environments, ART-EMAP uses a map evidence accumulation field F abE that combines information from multiple F ab1 activities over time:\nT abk (new) = T ab k (old) + y (Fab1 ) k , (118)\nwhere T abk is the evidence accumulating MTM. It is initialized as zero (T ab = ~0) and reset once the DC is satisfied. The F ab2 activity can then be redefined as\ny (Fab2 ) k = { 1, if Tk > (DC)Tj \u2200j 6= k 0, otherwise , (119)\nwhere improved accuracy correlates with larger DC values and a greater number of samples (Carpenter & Ross, 1995).\nFinally, to learn from the samples used to disambiguate prediction, an unsupervised learning stage (\u201crehearsal\u201d) takes place. In this fine-tuning stage, the LTMs of ARTa, ARTb and the map field maintain their values, whereas another set of weights from Fa2 to F ab E is adapted when such samples are re-presented to the system."
    },
    {
      "heading": "3.1.7. Adaptive resonance associative map",
      "text": "The fuzzy adaptive resonance associative map (ARAM) (Tan, 1995) extends ART autoassociative to heteroassociative mappings by connecting two ARTs (A and B) via a common category representation field F2.\nLTM. Fuzzy ARAM has two F1 layers connected to a single F2 layer whose LTM unit is \u03b8 = {w = [wa,wb]}. Activation. When normalized and complement coded inputs (x = [xa,xb]) are presented, the activation\nfunction is computed as\nTj = \u03b3 |xa \u2227waj | \u03b1a + |waj | + (1\u2212 \u03b3) |xb \u2227wbj | \u03b1b + |wbj | , (120)\nwhere \u03b3 \u2208 [0, 1] is the contribution parameter. Note that there is an independent set of parameters for each module: choice parameters \u03b1m > 0, learning parameters \u03b2m \u2208 [0, 1] and vigilance parameters \u03c1m \u2208 [0, 1], where m \u2208 {a, b}.\nMatch and resonance. Consider that node J has been selected via a WTA competition. F1 and F2 activities are defined as:\ny (Fm1 ) j =\n{ xm, if Fm2 is inactive\nxm \u2227wmJ , otherwise , (121)\nwhere m \u2208 {a, b}, and\ny (F2) j =\n{ 1, if j = J\n0, otherwise . (122)\nThe match functions are computed for node J as\nMmJ = \u2016y(Fm1 )\u20161 \u2016xm\u20161 = \u2016xm \u2227wmJ \u20161 \u2016xm\u20161 , (123)\nand resonance occurs if MmJ \u2265 \u03c1m for both m \u2208 {a, b} simultaneously. Thus, V RJ = {[xa,xb] : MaJ (xa) \u2265 \u03c1a and M bJ(x\nb) \u2265 \u03c1b}. In this case, learning is ensued such that the weights wmJ are updated using fuzzy ART\u2019s learning rule (Eq. (22) in Sec. 2.1.3). Otherwise, a match tracking mechanism temporarily raises the baseline \u03c1\u0304a (which is reset at the start of each sample presentation) as in fuzzy ARTMAP (Sec. 3.1.2), and the search for another resonant category continues. If an uncommitted category is recruited, then another one is initialized as wm = ~1. Specifically, when such dynamics take place and \u03b3 = 1, fuzzy ARAM is functionally equivalent to fuzzy ARTMAP (Tan, 1995)."
    },
    {
      "heading": "3.1.8. Gaussian ARTMAP",
      "text": "The Gaussian ARTMAP (GAM) (Williamson, 1996) is a discriminative model (Vigdor & Lerner, 2007) that uses Gaussian ART elementary units (Sec. 2.1.6) as building blocks.\nTraining. Training follows the standard ARTMAP dynamics (Sec. 3.1.1), where the match tracking mechanism is triggered following a predictive error.\nInference. During testing mode, predictions are made considering the total probability of each classes,\ni.e., by using Eqs. (100) and (101) with y (Fa2 ) j = T a j (Eq. (33))."
    },
    {
      "heading": "3.1.9. Probabilistic fuzzy ARTMAP",
      "text": "The probabilistic fuzzy ARTMAP (PFAM) (Lim & Harrison, 1997a, 2000a) combines fuzzy ARTMAP\u2019s code compression ability (Sec. 3.1.2) with the probability density function estimation of probabilistic neural networks (PNN) (Specht, 1990) in a hybrid system: during training, a fuzzy ARTMAP variant is used to generate prototypes in a supervised manner, whereas during inference, the PNN uses Bayes decision theory to make predictions.\nTraining. Training is similar to fuzzy ARTMAP, except for the following:\n1. Map field dynamics: the activity of Fab used to compute the match function (Eq. (103) in Sec. 3.1.2) is defined as\ny(F ab) = y(F b 2 ) \u2227 w\nab J\n\u2016wabJ \u20161 , (124)\nand when learning is ensued, W ab is updated using\nwabJ (new) = w ab J (old) + y\n(Fab); (125)\n2. If the match tracking mechanism is engaged, then the condition\n0 \u2264 \u03c1a \u2264 min (1,MaJ + ) , 0 < 1, (126)\nis enforced to enable identical categories to be associated with different classes (Lim & Harrison, 1997b);\n3. Centroids \u00b5aj are embedded in ARTa (i.e., the LTM unit is \u03b8 = {w,\u00b5}). These are initialized as \u00b5aj = ~0 and recursively estimated using\n\u00b5aj (new) = \u00b5 a j (old) +\n1\n\u2016wabJ \u20161\n( xa \u2212 \u00b5aj (old) ) , (127)\nwhere xa is complement coded for fuzzy ARTMAP categories w but not for the centroids \u00b5.\nInference. Prediction is accomplished using the maximum a posteriori (MAP) or minimum-risk estimate:\np\u0302(cbk|xa) = p\u0302(xa|cbk)p\u0302(cbk)l(cjk), (128)\nwhere l(cjk) represents the cost of selecting c b k when in fact the true class is c b j . The prior probability estimate of a given class k is given by the ratio of the number of samples encoded by ARTa\u2019s prototypes that are mapped to class k to the total number of samples presented to PFAM:\np\u0302(cbk) =\nNa\u2211 j=1 wabjk\nNb\u2211 k=1 Na\u2211 j=1 wabjk , (129)\nand p(xa|cbk) is estimated using the Parzen-window method (Cacoullos, 1966; Parzen, 1962) with isotopic Gaussians kernels (\u03a3j = \u03c3 2 j I)\np\u0302(xa|cbk) = Na\u2211 j=1 1cbk (\u00b5aj )\nNa\u2211 i=1 1cbk (\u00b5ai ) e\n( \u2212 \u2016xa\u2212\u00b5aj \u2016 2 2\n2\u03c32 j\n)\n(2\u03c0) d 2 \u03c3dj\n, (130)\nwhere\n1cbk (\u00b5aj ) = { 1, if \u00b5aj \u2208 cbk 0, otherwise . (131)\nThe kernels used for the realization of the Parzen-window density estimation have heteroscedastic components, which are computed as\n\u03c3j = 1\nr min i \u2016\u00b5aj \u2212 \u00b5ai \u20162, (132)\nor determined using the k-nearest neighbors method (Duda et al., 2000)\n\u03c3j = 1\nk k\u2211 i=1 \u2016\u00b5aj \u2212 \u00b5ai \u2016, 1 \u2264 k \u2264 Na \u2212 1, (133)\nwhere r is a user-defined overlapping parameter, and \u00b5aj and \u00b5 a i belong to different classes in Eqs. (132) and (133)."
    },
    {
      "heading": "3.1.10. ARTMAP-IC",
      "text": "The ARTMAP-IC model (Carpenter & Markuzon, 1998) is a fuzzy ARTMAP variant whose key characteristics are (i) a new match tracking mechanism (MT-) to reduce model complexity and (ii) the inclusion of instance counting (via a new counting field F3) for probabilistic distributed prediction.\nARTMAP-IC replaces ARTb with a vector y b encoding the classes of the classification problem, such\nthat, for a given input xa presented to ARTa,\nybi = { 1, if xa \u2208 class i 0, otherwise , (134)\nThe activity of the counting field F3 (located in-between ARTa and F ab) is defined as\ny (F3) j =  y (Fa2 ) j , training cjy (Fa2 ) j\nNa\u2211 i=1 ciy (Fa2 ) i , prediction , (135)\nwhere the instance counting weight cj records the number of samples that are encoded by category j, i.e., the number of times it is activated. The map field Fab activity can then be defined as\ny(F ab) = { yb \u2227U , training U , prediction\n(136)\nwhere the kth component of the map field\u2019s input is\nUk = Na\u2211 j=1 wabjky (F3) j , k = 1, ..., Nb, (137)\nand here Nb represents the number of classes. Training. During training, the match function is defined as\nMabJ = \u2016yb \u2227U\u20161 \u2016yb\u20161 = \u2016yb \u2227wabJ \u20161, (138)\nsince U = wabJ (because y (Fa2 ) = y(F3)) and \u2016yb\u20161 = 1. If the vigilance criterion is not satisfied (MabJ < \u03c1ab), then the new match tracking mechanism (MT-) is engaged such that ARTa\u2019s vigilance is set to\n\u03c1a(new) = M a J + , \u2264 0 and \u2016 \u2016 small, (139)\nand the search proceeds as with fuzzy ARTMAP. Otherwise, if learning is ensued, then fuzzy ARTa and the map field weight vectors learn as described in Secs. 2.1.3 and 3.1.2, respectively. The instance counting is updated as\ncj(new) = cj(old) + y (Fa2 ) j , (140)\nwhere cj \u2019s are initialized as 0. Inference. During testing, no search occurs, and ARTMAP-IC uses the Q-max rule to distribute Fa2 activity via the following contrast enhancement procedure:\ny (Fa2 ) j =  Tj\u2211 \u03bb\u2208\u039b T\u03bb , if j \u2208 \u039b\n0, otherwise\n, (141)\nwhere \u039b is the set formed by the Q categories with the largest activation values (Q is a user-defined parameter). This is similar to k-nearest neighbors (Duda et al., 2000) where Q assumes the role of k (Carpenter & Markuzon, 1998). Setting Q = 1 leads to WTA mode.\nFinally, the probability of class k is then computed as\n\u03c3k = Uk Nb\u2211 l=1 Ul =\n\u2211 j\u2208\u039b wabjkcjTj\nNb\u2211 l=1 \u2211 j\u2208\u039b wabjl cjTj . (142)"
    },
    {
      "heading": "3.1.11. Distributed ARTMAP",
      "text": "Distributed ARTMAP (dARTMAP) (Carpenter et al., 1998) was developed to improve supervised ART models regarding model compactness and noise robustness (i.e., reduce category proliferation) while performing fast and stable learning via distributed representation. It features distributed activation, match and learning functions. Notably, distributed ARTMAP generalizes the following supervised ART models (Carpenter, 2003): \u201cdARTMAP \u2283 ARTMAP-IC \u2283 default ARTMAP \u2283 fuzzy ARTMAP\u201d, where \u2283 is used to indicate containment considering this ARTMAP\u2019s ecosystem.\nIn case of classification problems, distributed ARTMAP uses distributed ART (Sec. 2.1.5) as a building block for ARTa, while replacing ARTb with a binary vector indicating the input\u2019s class membership (Eq. (134) in Sec. 3.1.10). The distributed ARTMAP uses an increased-gradient content-addressable memory (IG CAM) rule for contrast enhancement. A CAM rule defines a function that yields the steady state values of the network\u2019s STM when an input sample is presented. Particularly, distributed ARTMAP\u2019s CAM rule defines a power function that is controlled by a parameter p. The latter has a role akin to the variance in Gaussian kernels, and, as it tends to infinity, the network converges to WTA.\nTraining. During training, the distributed ARTMAP alternates between distributed and WTA modes. Like ARTMAP-IC (Sec. 3.1.10), distributed ARTMAP features a counting field Fa3 (for instance counting purposes) which is cascaded to Fa2 and employs the MT- match tracking search algorithm. Briefly, the distributed representation undergoes the unsupervised (Eqs. (26) to (28)) and supervised vigilance (i.e., prediction assessment) tests, and if one of them fails the system switches to WTA mode and its corresponding dynamics are carried out (in which nodes can be added incrementally). Otherwise, distributed mode dynamics take place.\nParticularly, the distributed ARTMAP uses the distributed choice-by-difference activation function (Eq. (23) in Sec. 2.1.5 disregarding the depletion parameters)\nTj = 2d\u2211 i=1 [ xai \u2227 (1\u2212 \u03c4 bui ) ] + (1\u2212 \u03b1) 2d\u2211 i=1 \u03c4 bui , \u03b1 \u2208 (0, 1), (143)\nand, after these are computed, the following subsets of highly active nodes are considered:\n1. \u039b = {j : Tj \u2265 Tu} 2. \u039b\u2032 = {j : Tj = (2\u2212 \u03b1)d}\nwhere Tu is the activation function of an uncommitted node (\u03c4 bu = \u03c4 td = ~0). The IG CAM rule specifies the following functions for the steady-state activities of distributed ARTMAP\u2019s modes\n\u2022 Distributed mode\n\u2013 If \u039b\u2032 6= {\u2205}, then\ny (Fa2 ) j =  1 |\u039b\u2032| , \u2200j \u2208 \u039b\u2032\n0, otherwise , (144)\nwhere | \u00b7 | represents the cardinality of a set. \u2013 If \u039b\u2032 = {\u2205} and \u039b 6= {\u2205}, then\ny (Fa2 ) j =  1 1 + \u2211 \u03bb\u2208\u039b,\u03bb6=j [ (2\u2212 \u03b1)d\u2212 Tj (2\u2212 \u03b1)d\u2212 T\u03bb ]p , \u2200j \u2208 \u039b 0, otherwise\n(145)\nwhere p \u2208 (0,\u221e) is the power parameter. The ARTa\u2019s counting field F3 activity is then defined as\ny (Fa3 ) j =\ncjy (Fa2 ) j\nC\u2211 \u03bb=1 c\u03bby (Fa2 ) \u03bb , (146)\nwhere C is the number of ARTa\u2019s committed nodes, and cj is the instance counting of node j (if uncommitted, then cj = 0). The signal used in the ARTa\u2019s match function is then\n\u03c3i = C\u2211 j=1 [ y (Fa3 ) j \u2212 \u03c4 td j,i ]+ , i = 1, ..., 2d. (147)\n\u2022 WTA mode\n\u2013 If \u039b 6= {\u2205}, then the winner node is J = arg max j\u2208\u039b (Tj). \u2013 If \u039b = {\u2205}, then the uncommitted node is recruited to learn the presented input sample.\nThe ARTa\u2019s counting field F3 activity is then\ny (Fa3 ) j = y (Fa2 ) j =\n{ 1, if j = J\n0, otherwise , (148)\nand the signal used in the ARTa\u2019s match function is \u03c3i = ( 1\u2212 \u03c4 tdJ,i ) , i = 1, ..., 2d. (149)\nIf the vigilance test of ARTa is not satisfied (Eqs. (26) to (28)) in Sec. 2.1.5), then distributed ARTMAP reverts to WTA mode, and the search continues until a resonant node is either found or created. Finally, the output class is then estimated using Eqs. (100) and (101) with y (Fa3 ) j in place of y (F2) j . If the prediction is incorrect, then match tracking is engaged using the MT- algorithm (Sec. 3.1.10). Otherwise, ARTa adapts using the distributed ART learning laws described in Sec. 2.1.5 (the top-down thresholds components are updated using y (Fa3 ) j in place of y (F2) j in Eq. (31)), and the instance countings are updated using Eq. (140) in Sec. 3.1.10. Note that if the distributed ARTMAP system enters a resonant state while in distributed mode, then, prior to learning, a credit assignment stage takes place in which the nodes permanently associated with the wrong class are inhibited, the Fa2 activity is re-normalized (i.e., \u2016y(F a 2 )\u20161 = 1) and the F a 3 activity and the signal \u03c3 are recomputed using Eqs. (146) and (147), respectively. Inference. To make a prediction for a new sample x, distributed ARTMAP operates similarly to the training phase but always in distributed mode and with search and learning disabled (i.e., in feedforward mode)."
    },
    {
      "heading": "3.1.12. Hypersphere ARTMAP",
      "text": "Hypersphere ARTMAP (HAM) (Anagnostopoulos & Georgiopulos, 2000) closely follows the operation of fuzzy ARTMAP (Sec. 3.1.2) but instead uses hypersphere ART (Sec. 2.1.7) modules for ARTa and ARTb. ARTb is responsible for clustering the classes (x b), ARTa does the data samples (x a) and the inter-ART maps the ARTa categories to the ARTb categories regulated by the match tracking procedure."
    },
    {
      "heading": "3.1.13. Ellipsoid ARTMAP",
      "text": "Similar to hypersphere ARTMAP, ellipsoid ARTMAP (EAM) (Anagnostopoulos & Georgiopoulos, 2001a,b) uses ellipsoid ART (Sec. 2.1.8) for both its ARTa and ARTb modules while closely following the fuzzy ARTMAP\u2019s operation (Sec. 3.1.2)."
    },
    {
      "heading": "3.1.14. \u00b5ARTMAP",
      "text": "The \u00b5ARTMAP model (Gomez-Sanchez et al., 2002; Sanchez et al., 2000) is a fuzzy ARTMAP variant developed to reduce the type of category proliferation due to overlapping classes, consequently improving generalization capability. This is accomplished by regulating the conditional entropy between the input (ARTa) and output (ARTb) spaces\nH(ARTb|ARTa) = Na\u2211 j=1 hj , (150)\nwhere hj is the contribution of ARTa\u2019s node j to the total entropy:\nhj = \u2212p\u0302(caj ) Nb\u2211 k=1 p\u0302(cbk|caj ) log2 p\u0302(cbk|caj ), (151)\nand the probabilities are estimated using the map field\u2019s LTM unit, whose dynamics are similar to PROBART\u2019s (Sec. 3.2.1). This process indirectly controls the training error, which is relaxed to address overfitting.\nTraining. Training is divided into two phases, and the first one is performed online. Assuming the resonant categories of ARTa and ARTb are J and K, respectively, the map field vigilance test is defined using Eq. (151):\nMabJ = hJ , (152)\nwhere\np(cbk|caj ) =  y (Fab) k \u2016y(Fab)\u20161 , if j = J\nwabjk \u2016wabj \u20161 , otherwise\n, (153)\np(caj ) = \n\u2016y(Fab)\u20161\n\u2016y(Fab)\u20161 + Na\u2211\ni=1,i6=J \u2016wabi \u20161\n, if j = J\n\u2016wabj \u20161\n\u2016y(Fab)\u20161 + Na\u2211\ni=1,i6=J \u2016wabi \u20161\n, otherwise\n. (154)\nNote, however, that if J is an uncommitted node, then\np(cbk|caJ) =\n{ 1, if k = K\n0, otherwise , (155)\nwhich implies hJ = 0. The value of hJ measures the homogeneity of ARTb nodes (i.e., classes) associated with ARTa\u2019s category J . If M ab J \u2264 hmax, where hmax is a user-defined parameter, then the map field vigilance is satisfied and learning is ensued as in PROBART (Eq. (209)). Otherwise, ARTa\u2019s node J is inhibited, and the search continues without changing ARTa\u2019s vigilance parameter. Note that hmax = 0 implies mapping to a single class, whereas hmax > 0 allows mapping to different classes (i.e., non-zero training error).\nNext, an offline training phase is performed to measure the overlap between categories. In this second training phase no learning is permitted within the ART modules. Probabilities are re-estimated using\np(cbk|caj ) = vabjk \u2016vabj \u20161 , (156)\np(caj ) = \u2016vabj \u20161 Na\u2211 i=1 \u2016vabi \u20161 , (157)\nwhere a temporary map field co-occurrence matrix V ab is updated in a unsupervised manner, i.e., without match tracking (Initialization: V ab = 0). The total entropy H is computed using Eq. (150), and if H > Hmax, where Hmax is a user-defined parameter, then the mapping is considered too entropic. ARTa\u2019s category M with the largest contribution hM is removed, and the baseline vigilance \u03c1\u0304a is increased for all new uncommitted categories as\n\u03c1\u0304a = \u2016waM\u20161 \u2016xa\u20161 + , (158)\nthus adaptively tuning individual vigilance parameters of ARTa\u2019s categories. The samples that were associated with node M are re-presented and the learning process resumes. This entire process is repeated until H \u2264 Hmax, in which the training stops. Notably, if hmax, Hmax \u2265 log2Nb then \u00b5ARTMAP behaves similarly to PROBART, whereas if hmax = 0 and Hmax \u2265 log2Nb, then \u00b5ARTMAP behaves similarly to fuzzy ARTMAP.\nInference. Predictions are made using Eqs. (100) and (101), i.e., the class output K is estimated as the one that has the largest frequency of association with ARTa\u2019s resonant category J .\nUnder certain conditions, \u00b5ARTMAP creates large categories that lead to considerable overlaps and decrease the system\u2019s performance. The safe-\u00b5ARTMAP (Gomez-Sanchez et al., 2001) variant is a generalization of \u00b5ARTMAP that adds another vigilance criterion to mediate learning. Specifically, to avoid the formation of large hyperrectangles that enclose far apart samples belonging to the same class, besides passing both the ARTa and the map field vigilance tests, an ARTa category also needs to undergo a distance criterion defined as\nM\u2206wJ = \u2016waJ\u20161 \u2212 \u2016waJ \u2227 xa\u20161\n\u2016xa\u20161 . (159)\nOnly if this third vigilance test is also satisfied (M\u2206wJ \u2264 \u03b4, 0 < \u03b4 < 1 \u2212 \u03c1), then learning takes place. This imposes a restriction on the instantaneous change of a category size, which is upper bounded by \u2016xa\u20161\u03b4. Particularly, safe-\u00b5ARTMAP reduces to \u00b5ARTMAP when \u03b4 = 1 (which effectively implies the absence of a constraint)."
    },
    {
      "heading": "3.1.15. Default ARTMAPs",
      "text": "The default ARTMAP 1 model (Carpenter, 2003) is characterized by the usage of a distributed representation to perform continuously-valued predictions, as opposed to binary and fuzzy ARTMAP models (Secs. 3.1.1 and 3.1.2), which use WTA code representation.\nTraining. Default ARTMAP 1\u2019s training is akin to fuzzy ARTMAP\u2019s, except for (i) the absence of ARTb (default ARTMAP 1 is a simplified architecture), (ii) its ARTa module employs the choice-by-difference activation function defined as (Carpenter & Gjaja, 1994)\nTj = \u2016x \u2227waj \u20161 + (1\u2212 \u03b1)(d\u2212 \u2016w a j \u20161), \u03b1 \u2208 (0, 1), (160)\nand (iii) the match tracking algorithm, which is MT- search (Carpenter & Markuzon, 1998). Inference. As opposed to fuzzy ARTMAP, default ARTMAP 1 uses a distributed representation for inference, where two subsets of highly active neurons are selected as:\n1. \u039b = {\u03bb = 1, ..., Na : T\u03bb > \u03b1d} 2. \u039b\u2032 = {\u03bb = 1, ..., Na : T\u03bb = d (i.e., w\u03bb = xa)}\nNext, the IG CAM rule is applied:\n\u2022 If \u039b\u2032 6= {\u2205}, then\nyj =  1 |\u039b\u2032| , \u2200j \u2208 \u039b\u2032\n0, otherwise , (161)\nwhere | \u00b7 | represents the cardinality of a set.\n\u2022 If \u039b\u2032 = {\u2205}, then\nyj =  [ 1 d\u2212 Tj ]p \u2211 \u03bb\u2208\u039b [ 1 d\u2212 T\u03bb ]p , \u2200j \u2208 \u039b 0, otherwise . (162)\nFinally, the predictions for each class are obtained using Eqs. (100) and (101) in Sec. 3.1.1. In a WTA system, such as fuzzy ARTMAP, after learning a sample, an immediate re-presentation is guaranteed to yield a correct prediction, i.e., it passes the \u201cnext-input-test\u201d. However, the default ARTMAP\n1 WTA prediction during training might not be the same as the distributed one. To overcome this problem, the default ARTMAP 2 model (Amis & Carpenter, 2007) introduces the \u201cdistributed-next-input-test\u201d during training, to assure that a correct prediction would also be performed under a distributed representation. Briefly, in order to anticipate an error, after learning from a sample in a WTA mode, the prediction is verified again using a distributed representation. If the distributed prediction is correct, then learning resumes by returning to WTA mode and presenting the next sample. Otherwise, the match tracking mechanism is engaged, the system reverts to WTA mode, the resonant category is inhibited and the network restarts the search to learn more from that sample."
    },
    {
      "heading": "3.1.16. Boosted ARTMAP",
      "text": "Boosted ARTMAP (Verzi et al., 1998) is a variant of fuzzy ARTMAP (Sec. 3.1.2) closely related to PROBART (Marriott & Harrison, 1995) (Sec. 3.2.1). It is inspired by Boosting theory (Schapire, 1990) and was developed to improve the fuzzy ARTMAP\u2019s generalization capability (since it is prone to overfitting the training data) and to create less complex networks (i.e., to reduce the type of category proliferation caused by overlapping classes). These are addressed by regulating the training error, which is allowed to be non-zero. Particularly, boosted ARTMAP\u2019s ARTa and ARTb modules are boosted ART models (which are identical to fuzzy ART, except that the categories are endowed with individual vigilance parameters), and its map field dynamics are equal to PROBARTs\u2019.\nTraining. Boosted ARTMAP learning is offline. After a first pass through the data, the error of ARTa\u2019s category j is estimated as\n\u03b5j = pjej = \u2016wabj \u20161 \u2212max k\n( wabjk ) Na\u2211 m=1 Nb\u2211 n=1 wabmn , (163)\nwhere\npj = p ( x selects caj ) = p(caj ) =\n\u2016wabj \u20161 Na\u2211 m=1 Nb\u2211 n=1 wabmn , (164)\nej = p ( c\u2217 not predicted by caj ) = 1\u2212 max k\n( wabjk ) \u2016wabj \u20161 , (165)\nand the total error is given by\n\u03b5T = Na\u2211 j=1 \u03b5j =\nNa\u2211 j=1 [ \u2016wabj \u20161 \u2212max k ( wabjk )] Na\u2211 m=1 Nb\u2211 n=1 wabmn , (166)\nwhere c\u2217 is the true class. Then, the vigilance parameters of ARTa\u2019s nodes are raised by a user-defined parameter \u03b4: \u03c1\u03bb(new) = \u03c1\u03bb(old) + \u03b4, \u03bb \u2208 \u039b, (167) where \u039b = {\u03bb : \u03b5\u03bb > \u03b5max}, i.e., \u039b is the subset of nodes \u03bb with contributions \u03b5\u03bb to the total error \u03b5T larger than the desired error \u03b5max. If \u039b = {\u2205} but the total error \u03b5T is above the desired error \u03b5max (i.e., if \u03b5T > \u03b5max), then the vigilances of all nodes j with the largest contribution \u03b5j are increased following Eq. (167). Note that when new nodes are added to the system, their initial vigilance parameter is set to a relaxed baseline value \u03c1\u0304.\nInference. In prediction mode, when a sample is presented, the corresponding class label is obtained using the map field weight vector associated with ARTa\u2019s resonant category J\nK = arg max k\n[ wabJk ] . (168)\nAs discussed in (Gomez-Sanchez et al., 2002), due to the lack of a match tracking mechanism, this version of boosted ARTMAP cannot handle \u201cpopulated exceptions\u201d, i.e., when samples from one class surrounds\nanother and it is necessary to create a category inside another category. The second version of boosted ARTMAP (Verzi et al., 2006) augments its predecessor with a match tracking mechanism to regulate the training error, whose map field dynamics are discussed next.\nTraining. During learning, when a sample pair is presented and ARTa\u2019s and ARTb\u2019s resonant nodes are J and K, respectively, the map field match function is given by\nMabJ = (1\u2212 e\u2032J) \u2016yF b2 \u2227wab\n\u2032\nJ \u20161 \u2016yF b2 \u20161 , (169)\nand resonance occurs if the winning category satisfies MJ > (1\u2212 )\u03c1ab, where \u2208 [0, 1] is the error tolerance parameter that binds the training error. The map field then learns as in PROBART (Eq. (209)). Otherwise, the match tracking mechanism is engaged. The temporary variables e\u2032J and w ab \u2032\nJ in Eq. (169) are computed as if category J were allowed to learn:\nwab \u2032\nJl = 1, if l = arg maxk ( wabJk ) d0 + e, otherwise , (170)\ne\u2032J = 1\u2212 max k\n( wab \u2032\u2032\nJk ) \u2016wab \u2032\u2032 J \u20161 , (171)\nwab \u2032\u2032\nJl =\n{ wabJl + 1, if l = K\nwabJl , otherwise , (172)\nwhere d\u00b7e is the ceiling function. If node J is uncommitted, then wab \u2032\nJ = ~1 and e \u2032 J = 0 (no mismatch will\ntake place). Inference. Predictions are made using Eq. (168). Note that boosted ART generalizes fuzzy ART, and boosted ARTMAP reduces in functionality to fuzzy ARTMAP by setting \u03b5d = 0 and \u03c1 ab > 0.5 and to PROBART by setting \u03b5d = 1. Note that boosted ARTMAP performs empirical risk minimization, however, variants of boosted ARTMAP, such as (Verzi et al., 2006; Verzi et al., 2002, 2001), perform structural risk minimization and use Rademacher penalization (Koltchinskii, 2001)."
    },
    {
      "heading": "3.1.17. Fuzzy ARTMAP with input relevances",
      "text": "The fuzzy ARTMAP with input relevances (FAMR) model (Andonie & Sasu, 2003; Andonie & Sasu, 2006; Andonie et al., 2003) is a fuzzy ARTMAP variant that modifies the map field dynamics, while maintaining the remaining dynamics of fuzzy ARTMAP. Thus, the incremental and non-parametric estimation of posterior probabilities based on the map field is augmented to reflect the degree of importance of incoming samples, especially when these are arriving from multiple heterogeneous sources corrupted by different noise levels.\nTraining. Particularly, a sample arriving at time t > 0 has a relevance factor qt \u2208 (0,\u221e). It is a user-defined or computed parameter, e.g., samples may be ranked based on their source noise level or have their relevance factors made proportional to its importance. Assuming the resonant categories of ARTa and ARTb are J and K, respectively, then the map field recursive update equations are based on the stochastic approximation procedure (Andonie, 1990):\nwabjk(new) =  wabjk(old), j 6= J (1\u2212At)wabjk(old) +At, j = J, k = K (1\u2212At)wabjk(old), j = J, k 6= K , (173)\nwhere At =\nqt QJ(new) , (174)\nQJ(new) = QJ(old) + qt, (175)\nand Q = [Q1...QNa ]. Thus, an entry w ab i,j of the map field matrix W ab is an estimate of p(cbk|cak). If a new category K is created in ARTb, then the map field weights w ab jk are adapted as:\nwabjk(new) =  q0 Nb(new)Qj , \u2200j, k = K wabjk(old)\u2212 wabjK(new)\nNb(new)\u2212 1 , \u2200j, k 6= K\n, (176)\nwhere Nb(new) = Nb(old) + 1, is the new number of nodes in ARTb. If a new category is created in ARTa (J = Na + 1), then QJ is set as q0 \u2265 0 (initial relevance parameter) and wabJk = 1/Nb, \u2200k. Finally, the map field\u2019s vigilance test is redefined as\nMabJ = Nbw ab JK , (177)\nsuch that MabJ \u2265 \u03c1ab must be satisfied for resonance to occur. Inference. Predictions are made similarly to fuzzy ARTMAP (Sec. 3.1.2)."
    },
    {
      "heading": "3.1.18. Bayesian ARTMAP",
      "text": "Bayesian ARTMAP (BAM) (Vigdor & Lerner, 2007) is a generative model based on Bayes\u2019 decision theory (Vigdor & Lerner, 2007) that uses Bayesian ART modules (Sec. 2.1.10) as building blocks and represents class density by Gaussian mixtures. Moreover, the posterior probabilities in Bayes\u2019 theorem are estimated within and between ART modules.\nTraining. During training, the map field LTM unit is a matrix of association frequency (sample count) W ab = N = [nkj ]Nb\u00d7Na that is used to estimate the ARTa and ARTb joint probability distribution\np\u0302(cbk, c a j ) = nkj Nb\u2211 i=1 Na\u2211 l=1 nil , (178)\nsuch that soft and hard mappings between ART modules are possible, i.e., a deterministic many-to-one mapping or a probabilistic many-to-many mapping based on p\u0302(cbk, c a j ). The match tracking mechanism is triggered by the system if the match function value for ARTa\u2019s resonant category J\nMabJ = p\u0302(c b k|caJ) = nk,J Nb\u2211 i=1 ni,J , (179)\ndoes not satisfy MabJ \u2265 \u03c1ab, where \u03c1ab represents the minimum class posterior probability threshold. Note that setting \u03c1ab = 1 enforces a hard many-to-one mapping, and Bayesian ARTMAP reduces to Gaussian ARTMAP during inference. In case of a mismatch, ARTa\u2019s vigilance is temporarily changed to\n\u03c1a = M a J \u2212 \u03b4, 0 \u2264 \u03b4 MaJ , (180)\nwhere MaJ is computed using Eq. (59). The search continues until another resonant node is found or a new one is created. When learning is finally ensued, the matrix N entry nKJ (class K and ARTa\u2019s resonant node J association) is updated as\nnKJ(new) = nKJ(old) + 1. (181)\nInference. During testing, the class of an unseen sample is predicted using\nK = arg max k\n( p\u0302(cbk|xa) ) , (182)\nwhere\np\u0302(cbk|xa) =\nNa\u2211 j=1 p\u0302(cbk|caj )p\u0302(xa|caj )p\u0302(caj )\nNb\u2211 i=1 Na\u2211 l=1 p\u0302(cbi |cal )p\u0302(xa|cal )p\u0302(cal ) , (183)\np\u0302(caj ) =\nNb\u2211 k=1 nkj\nNa\u2211 l=1 Nb\u2211 k=1 nkl , (184)\np\u0302(cbk|caj ) = nkj Nb\u2211 i=1 nij . (185)\nBayesian ARTMAP variants have been developed for various tasks, such as semi-supervised learning (Nooralishahi et al., 2018; Tang & Han, 2010) and associative memory (Chin et al., 2016)."
    },
    {
      "heading": "3.1.19. Generalized ART",
      "text": "The Generalized ART (GART) (Yap et al., 2008) is a hybrid model that combines a Gaussian ARTMAP (Williamson, 1996) (Sec. 2.1.6) variant to cluster samples in the input space and a generalized regression neural network (GRNN) (Specht, 1991) to perform prediction. In this model, the mapping is one-to-one (bijective) and thus Na = Nb = N .\nTraining. Like Gaussian and Bayesian ARTs (Secs. 2.1.6 and 2.1.10, respectively), the two modified Gaussian ART modules A and B use Bayes\u2019 theorem to compute their activation functions (posterior probability as in Eq. (33)), where the prior p\u0302(caj ) is estimated using Eq. (35). Again, the evidence p\u0302(x\na) is the same for all categories and thus does not influence the WTA competition. The conditional probability estimate p\u0302(xa|\u03b8aj ) is given by\np\u0302(xa|\u03b8aj ) \u221d exp [ \u22121\n2 \u03bb(\u03b4aj (x a))\n] , (186)\nwhere \u03bb(\u03b4aj ) is defined an \u03b5-insensitive loss function to handle outliers and noisy data\n\u03bb(\u03b4aj ) = { 0, if \u03b4aj \u2264 \u03b5a \u03b4aj \u2212 \u03b5a, otherwise , (187)\n\u03b5a \u2265 0 is a user-defined parameter (if \u03b5 = 0, then Eq. (187) reduces to the Laplacian loss function), and\n\u03b4aj (x a) = d\u2211 i=1 \u2223\u2223\u2223\u00b5aji \u2212 xi \u03c3aji \u2223\u2223\u2223, (188) the parameters \u00b5aj , \u03c3 a j and n a j correspond to the centroid, standard deviation and sample count of ARTa\u2019s category j. When ARTa\u2019s BMU is selected via WTA, the following match functions are computed\nMaJ = p\u0302(x a|caj ), (189)\nM bJ = p\u0302(x b|cbj), (190)\nwhere the systems enters a resonant state if MmJ \u2265 \u03c1m, \u03c1m \u2208 [0, 1], m \u2208 {a, b}, i.e., if both vigilance tests are simultaneously satisfied. If learning is ensued, then\nnaJ(new) = n a J(old) + 1, (191)\n\u00b5aJ(new) =\n[ 1\u2212 1\nnaJ(new)\n] \u00b5aJ(old) +\n1\nnaJ(new) xa, (192)\n\u03c3aJ(new) =\n[ 1\u2212 1\nnaJ(new)\n] \u03c3aJ(old) +\n1\nnaJ(new)\n\u2223\u2223\u2223\u00b5aJ(new)\u2212 xa\u2223\u2223\u2223. (193)\nwhere the standard deviation update is based on the Laplacian distribution. For a newly created category, naJ(new) = 1, \u00b5 a J = x a, \u03c3aJ = \u03b3a, \u03c3 a J = \u03c3 2 init\n~1 (user-defined initial standard deviation). Similar dynamics hold for ARTb, and for both modules N = N + 1.\nInference. A prediction for an unseen sample x is made using\nf(xa) =\nN\u2211 j=1 p\u0302(caj |xa) \u03c3bj \u00b5bj\nN\u2211 j=1 p\u0302(caj |xa) \u03c3bj , f(xa) \u2208 R1. (194)\nThe enhanced GART (EGART) (Yap et al., 2010) adds network pruning and rule extraction strategies to the Generalized ART model. Moreover, p\u0302(xa|caj ) is formally defined as the Laplacian likelihood function\np\u0302(xa|caj ) = 1 2d d\u220f i=1 \u03c3aji exp\n[ \u2212\nd\u2211 i=1 1 \u03c3aij \u2223\u2223\u2223\u00b5aij \u2212 xai \u2223\u2223\u2223 ] , (195)\nand, like Gaussian ART, ARTa\u2019s match function is a normalized version of Eq. (195)\nMaJ = p\u0302(x a|caj ) = exp\n[ \u2212\nd\u2211 i=1 1 \u03c3aij \u2223\u2223\u2223\u00b5aij \u2212 xai \u2223\u2223\u2223 ] , (196)\nwhere for resonance to occur in ARTa, M a J \u2265 \u03c1a must be satisfied. The match tracking mechanism compares M bJ to \u03c1b\nM bJ = p\u0302(x a|caj ) = exp\n[ \u2212\nd\u2211 i=1 1 \u03c3aij \u2223\u2223\u2223\u00b5aij \u2212 xai \u2223\u2223\u2223 ] , (197)\nand if it is not satisfied, then the match tracking mechanism temporarily raises \u03c1a, inhibits the current winner category J and resumes the search. The learning and prediction mechanisms are the same as Generalized ART.\nThe improved GART (IGART) (Yap et al., 2011) builds upon the enhanced GART by incorporating an ordering algorithm (Dagher et al., 1999) to determine the order of input presentation as well as providing multivariate prediction f(xa) \u2208 RL when in inference mode:\nfl(x a) =\nN\u2211 j=1 p\u0302(caj |xa) \u03c3bjl \u00b5bjl\nN\u2211 j=1 p\u0302(caj |xa) \u03c3bjl , l \u2208 {1, ..., L}. (198)"
    },
    {
      "heading": "3.1.20. Self-supervised ARTMAP",
      "text": "The self-supervised ARTMAP (SSARTMAP) (Amis & Carpenter, 2010) is a model designed for selfsupervised learning applications. This machine learning modality consists of a supervised learning phase, in which only certain data features are specified, followed by an unsupervised phase, in which all the data features are specified. Similar to fuzzy ARTMAP (Sec. 3.1.2), this model\u2019s LTM is defined by \u03b8 = {w = [u,vc]}, whose geometric interpretation are hyperrectangles in the data space. An artifact of this learning modality is the \u201cundercommitted\u201d categories, defined by the presence of \u201cundercommitted\u201d features (i.e., \u2203i : ui > vi).\nTraining. During the first phase, where supervised learning takes place for a pre-defined number of epochs, only d\u0304 features are presented to the network. That is, a sample x carries information only with respect to a subset of features. The latter are complement coded, whereas the unspecified features are set to 1\u2019s:\nxi =  xi, if i = 1, ..., d\u0304\n1\u2212 xi, if i = d+ 1, ..., d+ d\u0304 1, otherwise , (199)\nsuch that \u2016x\u20161 = 2d\u2212 d\u0304 and d\u0304 \u2264 d. Then, an activation function based on choice-by-difference (Carpenter & Gjaja, 1994) is computed for each category j:\nTj = (2d\u2212 \u2016x\u20161)\u2212\n( \u2016wj\u20161 \u2212 \u2016x \u2227waj \u20161 ) 1\u2212 \u03b3\u03c6j \u2212 \u03b1 (d\u2212 \u2016wj\u20161) , (200)\nwhere 0 < \u03b1 < 1 is the choice parameter, 0 < \u03b3 < 1\u2212 \u03b1 is the undercommittement factor, and 0 \u2264 \u03c6j \u2264 1 is the degree of undercommittement of category j, defined as\n\u03c6j = 1\nd d\u2211 i=1 [uj,i \u2212 vj,i]+ = 1 d d\u2211 i=1 [wj,i \u2212 (1\u2212 wj,d+i)]+ , (201)\nwhere [\u00b7]+ is a rectifier operator. After the activation functions are computed, a subset of highly active categories is formed: \u039b = {j : Tj \u2265 Tu = \u03b1d}, where Tu is the activation function of an uncommitted category (initialized as w = ~1). If \u039b = {\u2205}, then an uncommitted category is recruited and permanently mapped to the class label paired with the current input sample. Otherwise, the mapping of the resonant committed category J is assessed. If it is correct, then learning is ensued as\nwJ(new) = wJ(old)\u2212 \u03b21 [wJ(old)\u2212 x]+ , (202)\nwhere [\u00b7]+ is a component-wise rectifier operator and \u03b21 \u2208 (0, 1] is the learning parameter of this first training phase. If the prediction is incorrect, then the match tracking mechanism (user-defined MT+ or MT-, see Sec. 3.1.10) inhibits the resonant neuron, slightly changes the baseline vigilance parameter \u03c1\u0304 and restarts the search.\nDuring the second phase, unsupervised learning takes place for another pre-defined number of epochs. As opposed to the previous phase, all the data features are presented (i.e., x = [x, ~1\u2212 x]), and distributed representation is employed. Additionally, the network runs in slow learning mode, and no mismatches occur (the vigilance parameter is set to zero). Particularly, if \u039b = {\u2205}, then no learning takes place. Next, the activation functions are computed using Eq. (200). The distributed activity y(F2) of layer F2 is established using the IG CAM rule described in Sec. 3.1.15 (Eqs. (161) and (162)). All weight vectors are thus updated using the distributed instar learning law\nwj(new) = wj(old)\u2212 \u03b22 [ yj~1\u2212 ( ~1\u2212wj(old) ) \u2212 x ]+ , (203)\nwhere j \u2208 \u039b, and \u03b22 \u2208 [0, 1] is the learning parameter of the second training phase. Inference. In inference mode, the self-supervised ARTMAP dynamics are identical to the unsupervised training stage, except that no learning takes place. Predictions are made using Eqs. (100) and (101) in Sec. 3.1.1."
    },
    {
      "heading": "3.1.21. Biased ARTMAP",
      "text": "Biased ARTMAP (bARTMAP) (Carpenter & Gaddam, 2010) augments fuzzy ARTMAP with a featural biasing mechanism to handle ordering effects that arise in fast online learning mode. Said mechanism temporarily alters the network\u2019s focus among the input sample features following a predictive error.\nTraining. During training, the choice-by-difference activation function (Eq. (160)) is used to find the winner category J , whose match function is computed as\nMJ = \u2016y\u0303(F1)\u20161 \u2016x\u0303\u20161 , (204)\nx\u0303 = [x\u2212 e]+ , (205)\ny\u0303(F1) = [ y(F1) \u2212 e ]+ , (206)\nwhere [\u00b7]+ is a component-wise rectifier operator, x\u0303 is the biased complement coded input vector, y\u0303(F1) is the biased F1 activity and e \u2208 R2d is the bias vector, which is set to ~0 at the beginning of each input\npresentation (such that x\u0303 = x and y\u0303(F1) = y(F1)). If the category J successfully passes the vigilance test (i.e., if it satisfies MJ \u2265 \u03c1) and is mapped to the correct class, then the learning dynamics are identical to fuzzy ART\u2019s (Eq. (22) in Sec. 2.1.3). Alternately, if the prediction based on the resonant category is incorrect, then the bias vector is updated using Eq. (207),\nei(new) =  ei(old), if \u03bb [[ y (F1) i \u2212 ei(old) ]+ \u2212 \u2016y(F1)\u20161 2d ] \u2264 0 ei(old), if ei(old) \u2265 \u03bb [[ y (F1) i \u2212 ei(old) ]+ \u2212 \u2016y(F1)\u20161 2d ] > 0[ y (F1) i \u2212 \u2016y(F1)\u20161 2d ] 1 + \u03bb\u22121 , if y (F1) i > ei(old) and \u03bb [[ y (F1) i \u2212 ei(old) ] \u2212 \u2016y(F1)\u20161 2d ] > ei(old) , \u03bb \u2265 0, (207) the match tracking algorithm alters the vigilance parameter value (MT-, Sec. 3.1.10) and the search resumes. The bias strength parameter \u03bb in Eq. (207) can be selected by cross-validation procedures (note that setting \u03bb = 0 implies an unbiased model, i.e., fuzzy ARTMAP).\nInference. In prediction mode, biased ARTMAP behaves identically to fuzzy ARTMAP (Sec. 3.1.2)."
    },
    {
      "heading": "3.1.22. TopoART-C",
      "text": "TopoART-C (Tscherepanow & Riechers, 2012) is an incremental classifier based on fuzzy topoART (Sec. 2.2.2). In this architecture, each topoART module (A and B) is augmented with a classification layer F3 that is connected to the category layer F2. Additionally, module B is endowed with a mask layer F0 preceding its feature layer F1 to handle incomplete data.\nTraining. During training, the vigilance tests are layered: the first is unsupervised and equal to fuzzy ART\u2019s (Sec. 2.1.3), while the second is supervised and determines whether a correct class prediction was made. These must be simultaneously satisfied for the system to enter a resonant state and learn.\nInference. Prediction is made using topoART B, since topoART A is only used to filter noise and is therefore disregarded. Specifically, such a prediction depends on whether or not an unknown sample is completely enclosed by at least one category (which implies alternative activation function (Eq. (70)) equal to 1). In the affirmative case, the system predicts the class associated with the smallest node (measured using Eq. (20)). In the negative case, the system makes a prediction based on a subset of highly active categories. Note that if the sample has missing values, then only non-missing attributes are used in the computations."
    },
    {
      "heading": "3.2. Architectures for regression",
      "text": "The supervised ART models described so far have been primarily used for classification purposes. Although, in theory, all ARTMAP variants may be used to perform regression tasks (Sasu & Andonie, 2013). For instance, fuzzy ARTMAP was shown to be a universal function approximator in (Verzi et al., 2003). This section reviews architectures developed specifically for incremental function approximation/interpolation. An experimental comparative study on some of these ART-based regression models can be found in (Sasu & Andonie, 2012)."
    },
    {
      "heading": "3.2.1. PROBART",
      "text": "The PROBART model (Marriott & Harrison, 1995) is a fuzzy ARTMAP variant designed to approximate noisy continuous mappings. It has a distinct map field dynamic, whose activity is given by\ny(F ab) =  wabJ + y (F b2 ), if both ARTs are active wabJ , if only ARTa is active y(F b 2 ), if only ARTb is active\n~0, otherwise\n. (208)\nThis change turns the map field\u2019s weight matrix W ab into a frequency counter for the co-occurrence of resonant categories in both ART modules (i.e., it records the number of associations between nodes of ARTa and ARTb), thereby storing probabilistic information. Note that, in this model it is initialized as W\nab = 0. Training. PROBART does not possess a match tracking mechanism, since it is adequate for classification tasks (Marriott & Harrison, 1995) and rule extraction (Carpenter & Tan, 1995) but not for regression (Srinivasa, 1997). Moreover, it directly affects the probability estimation process. Therefore, ARTa\u2019s vigilance remains fixed. When learning is ensued, Fab weights are updated as\nwabJ (new) = w ab J (old) + y\n(Fab), (209)\nconsidering that ARTa\u2019s and ARTb\u2019s resonating nodes are J and K, respectively. Inference. The lth component of the prediction f\u0302(xa), when ARTa\u2019s resonating category is J , is computed as\nf\u0302l(x a) =\n1\n\u2016wabJ \u20161 Nb\u2211 k=1 wabJkw b kl = Nb\u2211 k=1 pJkw b kl, (210)\nwhere pJk = p\u0302(c b k|caJ) = wabJk \u2016wabJ \u20161 , wabJ is the J th row of W ab, \u2016wabJ \u20161 is the total number of samples associated with ARTa\u2019s node J across all ARTb nodes, w ab Jn is the number of co-activations of ARTa\u2019s node J and ARTb\u2019s node n, l \u2208 {1, ..., db} and db is the original non-complement coded dimension (number of features) of ARTb\u2019s input samples. The prediction is thus an average weighted by the conditional probabilities. Note that, to perform accurate mappings, PROBART requires large ARTa vigilance parameter values, consequently generating a large number of categories (Gomez-Sanchez et al., 2002).\nPROBART\u2019s generalization capability is limited by its WTA prediction, which is addressed by Modified PROBART (Srinivasa, 1997) via distributed prediction. The training process is identical for both models; the difference lies in the inference mode. Each feature l of the prediction f\u0302 \u2032(xa) is computed as\nf\u0302 \u2032l (x a) =\n\u2211 m\u2208S\nMm\u03b3mf\u0302m,l(x a)\u2211 m\u2208S Mm\u03b3m , (211)\nwhere S is the set of ARTa\u2019s resonant nodes for input xa (i.e., Mm \u2265 \u03c1a, Mm is the match function value of ARTa\u2019s neuron m), f\u0302m,l(x\na) is ARTa\u2019s neuron m prediction for feature l computed from Eq (210) and \u03b3m is ARTa\u2019s neuron m\u2019s frequency of winning. Concretely, the prediction is an average weighted by ARTa\u2019s nodes\u2019 match function values and instance countings. The size of the set S considered for distributed prediction is defined for each component l using a heuristic that minimizes the root mean squared error over the entire training set."
    },
    {
      "heading": "3.2.2. FasArt and FasBack",
      "text": "FasArt (Izquierdo et al., 1996, 2001) is a neuro-fuzzy system that reinterprets fuzzy ARTMAP (Sec. 3.1.2) as a fuzzy logic system by defining categories as decomposable fuzzy sets in their data spaces (universes).\nTraining. The training dynamics are identical to fuzzy ARTMAP\u2019s (ARTa, ARTb, and the map field), with the exception that the activation function, now also regarded as a fuzzy membership function, is defined as\nTj = d\u220f i=1 Tj,i, (212)\nwhere Tj,i is a triangular fuzzy membership function\nTj,i =  [ \u03b3(xi \u2212 wj,i) + 1 \u03b3(cj,i \u2212 wj,i) + 1 ]+ , if xi \u2264 cj,i[\n\u03b3(1\u2212 xi \u2212 wj,d+i) + 1 \u03b3(1\u2212 cj,i \u2212 wj,d+i) + 1\n]+ , if xi > cj,i , (213)\nthe parameter \u03b3 is the fuzzification rate that controls the width of the fuzzy set support (and consequently the generalization capabilities) and cj is the centroid associated with category j. The fuzzy support associated\ncategory j is thus defined by wj , cj and \u03b3. The weight vector wJ of a resonant category J is updated using fuzzy ART\u2019s learning dynamics (Eq. (22) in Sec. 2.1.3), whereas the centroid is updated using\ncJ(new) = (1\u2212 \u03b2c)cJ(old) + \u03b2cx, (214)\nwhere \u03b2c \u2208 (0, 1] is the centroid\u2019s learning parameter. This learning dynamic is the same for both ART modules. However, it should be noted that note that the LTMs of ARTa are also subjected to the constraint of making a correct prediction.\nInference. The prediction of each feature m is obtained using the following defuzzification procedure (average of fuzzy set centroids):\nf\u0302m(x a) =\nNb\u2211 k=1 Nb\u2211 j=1 cbk,mw ab j,kT a j\nNb\u2211 k=1 Nb\u2211 j=1 wabj,kT a j , (215)\nwhere T aj is the activation of ARTa\u2019s category j, c b k,m is the m th component of ARTb\u2019s centroid c b k associated with category k and wabj,k is the {j, k} entry of the map field matrix W ab. Note that FasArt is a universal function approximator (Izquierdo et al., 2001).\nFor fine-tuning purposes, particularly to improve performance and network compactness (i.e., to reduce category proliferation), FasBack (Izquierdo et al., 1997; Izquierdo et al., 2001) enhances FasArt with errorbased learning by using the gradient descent optimization method to adapt some of its parameters\np(new) = p(old)\u2212 \u03b7 \u2202E \u2202p(old) , (216)\nwhere p \u2208 {caj , cbk, wabi,j}, \u03b7 is the learning rate, E is error to be minimized\nE = 1 2 \u2016f\u0302(xa)\u2212 d\u201622, (217)\nand f\u0302(xa) and d are the system\u2019s prediction and the desired response, respectively. Note that two learning cycles are performed: a match-based one followed by an error-based one.\nFasArt has spawned many variants including recurrent (Palmero et al., 2000), distributed (ParradoHerna\u0301ndez et al., 2003) and dynamic (Izquierdo et al., 2009) models."
    },
    {
      "heading": "3.2.3. Fuzzy ARTMAP with input relevances",
      "text": "The fuzzy ARTMAP with input relevances (FAMR) (Andonie & Sasu, 2006; Andonie et al., 2003), when used for regression applications, makes predictions similarly to PROBART (Eq. (210) in Sec. 3.2.1). Particularly, PROBART is said to be a special case of FAMR with its parameters set to q0 = 0, qt = q \u2208 (0,\u221e) (constant) and \u03c1ab = 0."
    },
    {
      "heading": "3.2.4. Generalized ART",
      "text": "The generalized ART and its variants (Sec. 3.1.19) can be used for both classification and regression problems, for instance, by setting \u03c1b = 1 for the former and \u03c1b = \u03c1a for the latter (Yap et al., 2008)."
    },
    {
      "heading": "3.2.5. TopoART-R",
      "text": "TopoART-R (Tscherepanow, 2011) is a variant of fuzzy topoART (Sec. 2.2.2) designed for regression purposes. In this model, topoART module B is endowed with an input control layer F0 preceding its feature layer F1 to process samples with missing attributes (i.e., make predictions).\nTraining. TopoART-R training is similar to topoART (Sec. 2.2.2); however, it does not perform topological learning. Particularly, the complement coded independent and dependent variables are concatenated as a single input vector to be presented to the network. During the vigilance test stage, two match functions are independently computed for the dependent and independent variables.\nInference. Similar to topoART-C (Sec. 3.1.22), during testing, module A is disregarded, the activation function used is given by Eq. (70) in Sec. 2.2.2 and the prediction strategy depends on whether or not the\ninput sample is fully enclosed by at least one \u201cpartial\u201d category (i.e., a hyperrectangle in the multidimensional space formed by the non-missing attributes of the presented sample, from which a prediction is sought). In the affirmative case, a \u201ctemporary\u201d category is created from the intersection of these \u201cpartial\u201d categories. Then, a prediction is the center of the interval defined by the upper and lower bound components of the \u201ctemporary\u201d category that correspond to a given missing attribute (dependent and independent variables are treated as missing and non-missing, respectively). In the negative case, the \u201ctemporary\u201d category is created as a weighted average of a subset of highly active nodes, and then the prediction is carried out as previously described."
    },
    {
      "heading": "3.2.6. Bayesian ARTMAP for regression",
      "text": "The Bayesian ARTMAP for regression (BAR) (Sasu & Andonie, 2013) uses two Bayesian ART modules to perform clustering on both the input and the output spaces. All the dynamics of Bayesian ARTMAP discussed in Section 3.1.18 hold, except for the for the prediction (i.e., the function approximation) which is given by:\nf\u0302(xa) = Nb\u2211 k=1 p\u0302(cbk|xa)\u00b5bk, (218)\nwhere p\u0302(cbk|xa) is computed as described in Section 3.1.18. The Bayesian ARTMAP for regression was shown to be a universal function approximator (Sasu & Andonie, 2013)."
    },
    {
      "heading": "3.3. Summary",
      "text": "Table 6 summarizes the architectures discussed in terms of their training, inference/testing and the map field\u2019s mapping characteristics. Particularly, it lists if winner-takes-all (WTA) or distributed (D) coding is employed by these networks and whether the learned mapping is many-to-one (ARTa 7\u2192 ARTb, surjective) or many-to-many (many-to-one and one-to-many)."
    },
    {
      "heading": "4. ART models for reinforcement learning",
      "text": "The ART models described in the following subsections are used to perform reinforcement learning in which agents learn in real-time, incrementally and continuously by interacting with a complex and dynamic environment. ART-based reinforcement learning systems have found growing applications, for instance, in the computer games (da Silva & Goes, 2018; Wang et al., 2009; Wang & Tan, 2015) and situation awareness (Brannon et al., 2006, 2009) domains."
    },
    {
      "heading": "4.1. Reactive FALCON",
      "text": "The reactive fusion architecture for learning, cognition, and navigation (R-FALCON) (Tan, 2004) is a fusion ART-based model (Sec. 2.4.1) that possesses three channels (or F1 layers), viz., the sensory field (Fs1), the motor field (F a 1) and the feedback field (F r 1), which are used to learn mappings across states (s = [s1, ..., sn], where sj \u2208 [0, 1],\u2200j), actions (a = [a1, ..., am], ai \u2208 [0, 1],\u2200i), and rewards (r \u2208 [0, 1]), respectively. The general sense-act-learn dynamics of R-FALCON are described next.\nPrediction. Consider an agent currently at a state s. The inputs to R-FALCON\u2019s Fs1, F a 1 and F r 1 layers are set to xs = s, xa = ~1 and xr = [1, 0], respectively. Note that the feedback field is modeled using xr = [r, 1\u2212 r]. A node J is then selected via a WTA competition (node J maximizes Eq. (82) in Sec. 2.4.1). This setting of xr used for prediction biases selection towards maximal rewards.\nAction selection policy. The activity of layer Fa1, given by\ny(F a 1 ) = xa \u2227waJ = waJ , (219)\nis used to select the action I as I = arg max\n1\u2264i\u2264m\n( y\n(Fa1 ) i\n) . (220)\nThe agent performs the selected action I and then enters a new state s\u2032. Learning. Learning is ensued similarly to fusion ART (Sec. 2.4.1) using the appropriate F1 layers\u2019\ninputs, which depend on the feedback received from performing the selected action:\nR-FALCON suffers from category proliferation, so it must undergo pruning heuristics to enhance interpretability and scalability. Moreover, it can only effectively handle problems with immediate rewards."
    },
    {
      "heading": "4.2. Temporal difference FALCON",
      "text": "The temporal difference fusion architecture for learning, cognition, and navigation (TD-FALCON) (Tan, 2006; Tan et al., 2008) is a fusion ART-based model developed to effectively handle not only problems with immediate rewards but also problems with delayed rewards. This is accomplished by integrating the temporal difference methods (Sutton & Barto, 2018) of Q-learning (Watkins & Dayan, 1992) and state-action-rewardstate-action (SARSA) (Rummery & Niranjan, 1994) in the learning framework. Therefore, TD-FALCON is a value iteration method that learns action policies and value functions for state-action pairs via temporal difference learning. Briefly, the TD-FALCON dynamics are as follows.\nPrediction. For a given state s, the value function of all actions in the set of actions is predicted by setting the inputs to TD-FALCON\u2019s Fs1, F a 1, and F r 1 to x\ns = s, xa = a and xr = ~1, respectively. The action vector a is such that aI = 1 and ai = 0 for i 6= I, when taking action I. A node J is then selected via a WTA competition (node J maximizes Eq. (82) in Sec. 2.4.1) for each action.\nAction selection policy. The Fr1 layer activities, given by\ny(F r 1 ) = xr \u2227wrJ = wrJ , (221)\nare then used to compute the Q-values\nQ(s,a) = y\n(F r1 ) 1\nm\u2211 i=1 y (F r1 ) i . (222)\nAn action is then chosen using either a decay -greedy or a softmax policy, in order to address the exploration-exploitation trade-off. The agent is now in a new state s\u2032.\nLearning. Finally the system acts, receives a feedback from the environment and learns using the state (xs = s), action (xa = a), and reward (xr = [Q(s,a), 1\u2212Q(s,a)]) triad. The value function used in xr is estimated using\nQ(s,a) = Q(s,a) + \u2206Q(s,a) (223)\nwhere \u2206Q(s,a) = \u03b1eTD, (224)\neTD is the temporal difference error and \u03b1 is the learning rate. Particularly, the TD error for Q-learning (off-policy) is\neTD = r + \u03b3max a\u2032\nQ(s\u2032,a\u2032)\u2212Q(s,a), (225)\nwhile the TD error for SARSA (on-policy) is\neTD = r + \u03b3Q(s \u2032,a\u2032)\u2212Q(s,a), (226)\nwhere r is the immediate feedback and \u03b3 \u2208 [0, 1] is the discount factor. Additionally, TD-FALCON incorporates self-scaling (Q-values \u2208 [0, 1]) by using\n\u2206Q(s,a) = \u03b1eTD (1\u2212Q(s,a)) . (227)\nTD-FALCON trades faster learning for a less compact network (category proliferation), compared gradientbased reinforcement learning approaches, in which the training process is considerably slower but have a smaller network complexity or memory footprint (i.e., less neurons). One of the limitations of TD-FALCON is the bounded Q-values in the range [0, 1], which restricts the classes of problems that it can tackle."
    },
    {
      "heading": "4.3. Unified ART",
      "text": "The unified ART (Seiffertt & Wunsch II, 2010) is an ART model designed for mixed-modality learning, so that it seamlessly switches among the canonical machine learning modalities (UL, SL and RL). An important characteristic of this integration is the weight sharing between modalities. It uses a Markov Decision Process and Q-learning framework, and it has found application, for instance, in the field of situation awareness (Brannon et al., 2006, 2009).\nBriefly, the unified ART consists of a fuzzy ART module (Sec. 2.1.3) and a controller. The latter is represented by a matrix V = [vij ]N\u00d7m, whose entries vij estimate value functions, where N and m are the number of categories and available actions, respectively.\nPrediction. Upon presentation of an input s, the fuzzy ART dynamics are performed. If an uncommitted category is selected, then the controller\u2019s matrix V need to be expanded accordingly.\nAction selection policy. After the output activity y(F2) of layer F2 is established, it is used to select an action I such that\nI = arg max 1\u2264i\u2264m (ai) . (228)\nwhere a = y(F2) TV = [a1...am]. (229)\nThe output activity is binary and defined using Eq. (18) in Sec. 2.1.3 when in WTA mode. Alternately, to reduce category proliferation, the output activity can be defined in the distributed mode by setting y (F2) j = Tj , where the activation functions are computed using Eq. (16).\nLearning. After undertaking the selected action, the environment transitions to the next state s\u2032, and learning proceeds according to the type of signal received. Assuming WTA mode with resonant node J :\n\u2022 Supervised signal: this signal has the highest priority. If the correct action was selected, then the controller learns as\nvJ,i =\n{ vmax, if i = I\n0, otherwise , (230)\nwhere vmax is the maximum allowable value. Otherwise, a mismatch triggers a search for a new resonant neuron within the fuzzy ART module.\n\u2022 Reinforcement signal: In case of a reward, the controller learns as\nvJ,I = vJ,I + \u03b1r, (231)\nwhere \u03b1 is a learning rate. Conversely, a penalty causes a mismatch in the fuzzy ART module, which then initiates a search for a new resonant node. The controller still learns using Eq. (231).\n\u2022 Unsupervised signal: this scenario corresponds to the absence of a signal. No learning takes place in the controller.\nNote that, for all signal types, when a resonant neuron is found within the fuzzy ART module, it is adapted according to the fast learning mode described in Sec. 2.1.3."
    },
    {
      "heading": "4.4. Extended unified ART",
      "text": "The extended unified ART (Seiffertt & Wunsch II, 2010) is another fuzzy ART-based model designed to perform mixed-modality learning, which is accomplished via layered, modality-dependent, vigilance tests. These multiple vigilance criteria must be simultaneously satisfied for the ART system to enter a resonant state and ensue learning. Particularly, this model encodes the states in fuzzy ART\u2019s weight matrixW = [wi,j ]N\u00d7n, and the value functions of the state-action pairs in both the critic\u2019s matrix V = [vi,j ]N\u00d7m and the actor\u2019s matrix U = [ui,j ]N\u00d7m (whose role is akin to ARTMAP\u2019s map field matrix W\nab (Sec. 3.1.1)), where N is the number of categories, n is the dimension of the state space and m is the number of available actions. Uncommitted nodes are initialized by augmenting W with a row equal to ~1, while U and V are expanded with row vectors containing small random values.\nPrediction. Upon arriving at a state s, the highest active node J is found following fuzzy ART\u2019s dynamics (Sec. 2.1.3) using the choice-by-difference activation function (Eq. (160) in Sec. 3.1.15).\nAction selection policy. An action is selected using\nI = arg max 1\u2264i\u2264m (uJ,i) , (232)\nwhere uJ is the J th row of U .\nLearning. After performing the chosen action, the environment evolves to the next state s\u2032 following its dynamics; vigilance tests and learning are then ensued in consonance with the type of signal feedback from the environment. Particularly, in unsupervised learning mode, the extended unified ART learning dynamics are akin to fuzzy ART\u2019s, where there exists only a single match function MULJ (Eq. (19)) and a corresponding unsupervised vigilance test and parameter \u03c1UL. In this learning mode, neither the actor nor the critic are updated. In reinforcement learning mode, besides the unsupervised vigilance test, a reinforcement vigilance test is performed, where the match functionMRLJ is equal to the temporal difference error (Sec. 4.2) computed using V ; if satisfied (MRLJ > \u03c1RL, where \u03c1RL \u2265 0 is the reinforcement learning vigilance parameter), then the actor is updated as\nuJ,I = min (uJ,I + \u03b1r, umax) , (233)\nwhere umax is the upper bound for any entry of U , and the critic is updated using Eq. (231). If the RL test is not satisfied, a mismatch occurs, and a new search is triggered for the next highest ranking category. This process is repeated until a category satisfies the UL vigilance test while also being associated with an action (Eq. (232)) that is different from the one taken at s (i.e., i 6= I), or a new category is created. Finally, supervised learning mode adds a second match function MSLJ on top of the unsupervised one. The\nformer is akin to default ARTMAP\u2019s (Sec. 3.1.15) and assesses if the action taken was the correct one. In the affirmative case, only the actor is updated,\nuJ,i =\n{ umax, if i = I\n0, otherwise , (234)\nwhereas in the negative case, a match tracking procedure (MT-) (Carpenter & Markuzon, 1998) slightly decreases fuzzy ART\u2019s baseline vigilance parameter during this input presentation cycle, and the search restarts. Note that in all learning modes, when a category is allowed to learn, it does so by following fuzzy ART\u2019s learning dynamics (Sec. 2.1.3)."
    },
    {
      "heading": "5. Advantages of ART",
      "text": ""
    },
    {
      "heading": "5.1. Speed",
      "text": "One of the main advantages of ART neural network architectures is the speed with which they can process data and the relatively small number of epochs they typically require to converge. This is combined with the fact that they can be operated entirely in an online mode, which makes them very effective when working with streaming data or datasets that are too large to fit entirely in memory.\nParticularly, the ART 1 (Sec. 2.1.1) and fuzzy ART (Sec. 2.1.3) neural networks only require an amount of work linear in the number N of samples in the dataset per epoch, and the amount of work performed for each input sample presentation is similarly linear in the number of features d in the dataset, and the number of category templates k, that this sample is compared against. This leads to a running time complexity of O(Ndk), which means that the running time will grow linearly with the growth of any of these variables when the remaining variables are constant. In the absolute worst case, when each sample is put in its own category, this running time degrades to O(N2d) since k = N in this case; although this situation is uncommon. The same running time complexity analysis applies to other ART neural architectures that faithfully follow the same learning algorithm. A thorough discussion of fuzzy ART computational complexity analysis was presented in (Granger et al., 1998), and summarized in other studies such as (Majeed et al., 2018; Meng et al., 2016, 2014)."
    },
    {
      "heading": "5.2. Configurability",
      "text": "Another one of ART\u2019s main advantages is its ease of configurability (Wunsch II, 2009). For many unsupervised learning ART neural architectures, the most influential parameter is the vigilance value \u03c1, which controls when resonance occurs between an input sample and a category and subsequently whether this category would be allowed to learn the sample or not. In this way, the ART architectures do not require the choice of the number of clusters when used as clustering algorithms, unlike many other clustering algorithms. Meanwhile, the choice of which ART architecture to use and the choice of a reasonable vigilance value can allow the discovery of many useful clusters without needing to tweak many sensitive parameter values."
    },
    {
      "heading": "5.3. Explainability",
      "text": "The way that ART builds well-behaved templates representing the categories it learns from the data is another one of its core strengths (Wunsch II, 2009). After sufficient learning has taken place, these templates can provide the ability to interpret the results of the neural network learning (Carpenter & Tan, 1995; Healy & Caudell, 2006; Tan, 1997) and to visualize the boundaries of each discovered category or clusters. This property is an invaluable one, since many other types of neural networks can only be used as a black-box component that cannot be explained or interpreted."
    },
    {
      "heading": "5.4. Parallelization and hardware implementation",
      "text": "Another major strength of ART neural networks is their potential for massive parallelism and hardware implementation (Wunsch II, 2009). Notably, early contributions include optoelectronics (Blume & Esener, 1995; Caudell, 1992; Wunsch II, 1991; Wunsch II et al., 1993), analog (Ho et al., 1994) and VLSI (SerranoGotarredona & Linares-Barranco, 1996; Serrano-Gotarredona et al., 1998; Tsay & Newcomb, 1991) systems and, more recently, an implementation in memristive hardware (Versace et al., 2012). Although ART networks are incremental learners, and thus suffer from ordering effects (Sec. 6.1), the calculation of the match and activation function for each category can easily be done in parallel. Thus, ART models lend themselves well to GPU implementations, e.g., fuzzy ART in (Mart\u0301\u0131nez-Zarzuela et al., 2007, 2009), fuzzy ARTMAP in (Mart\u0301\u0131nez-Zarzuela et al., 2011) and ARTtree in (Kim & Wunsch II, 2011). This offers the opportunity for lower cost, energy consumption and memory footprint than other neural networks\u2019 hardware while maintaining online learning capabilities."
    },
    {
      "heading": "6. ART challenges and open problems",
      "text": ""
    },
    {
      "heading": "6.1. Input order dependency",
      "text": "An important problem faced by all agglomerative clustering or incremental learning algorithms, including ART, is order-dependence of data presentation. This is especially true in fast online learning mode. Many approaches have been developed to mitigate such ordering effects, and they mostly consist of suitable preand post-processing strategies (c.f. (Brito da Silva & Wunsch II, 2018) and the references cited within). Particularly, for supervised ART models, these strategies include Max-Min clustering (Tou & Gonzalez, 1974) in (Dagher et al., 1998, 1999); genetic algorithms (Eiben & Smith, 2015) in (Baek et al., 2014; Palaniappan & Eswaran, 2009); uncorrelated feature-based ordering in (Oong & Isa, 2014); featural biasing in (Carpenter & Gaddam, 2010); and voting strategies in (Amis & Carpenter, 2007, 2010; Carpenter, 2003; Carpenter et al., 1992; Carpenter & Markuzon, 1998; Lim & Harrison, 2000a,b; Williamson, 1996). In regards to unsupervised ART models, examples of strategies are split, merge and delete operations in (Lughofer, 2008); merging heuristics in (Isawa et al., 2008a,b, 2009); cluster validity index-based vigilance tests in (Brito da Silva & Wunsch II, 2017); and exploiting the ordering properties of visual assessment of cluster tendency (VAT) (Bezdek, 2017; Bezdek & Hathaway, 2002) in (Brito da Silva & Wunsch II, 2018). The presentation order of inputs still remains an open problem (even if there is meaningful temporal information embedded in the order of sample presentation (e.g., a time series) and it is much more pronounced when presentation is done in a random order), thus requiring further investigation."
    },
    {
      "heading": "6.2. Vigilance parameter adaptation",
      "text": "The vigilance is the single most important parameter in any ART model. Selecting suitable values is critical to the network performance and complexity, especially in clustering applications. However, it is often set empirically in an ad hoc manner. In unsupervised learning mode, vigilance adaptation has been addressed in fuzzy ART through the activation maximization, confliction minimization and hybrid integration rules (Meng et al., 2013, 2016); the combination with particle swarm optimization (Kennedy & Eberhart, 1995) and cluster validity indices (Xu & Wunsch II, 2009) in (Smith & Wunsch II, 2015); defining the vigilance as a function of the category size (Isawa et al., 2008b, 2009); or modeling it as a fuzzy membership function (Majeed et al., 2018). Despite these contributions, setting the vigilance parameter still remains a challenging task worthy of further exploration, particularly in the online learning mode."
    },
    {
      "heading": "6.3. New metrics",
      "text": "Another challenging area in the development of ART neural networks is the use of new metrics and representations that would allow ART to more robustly solve some domain-specific problems (Wunsch II, 2009), such as grammar inference and natural language processing (Meuth, 2009). Some cases require customized neural network designs, such as when the data structure is neither binary nor continuous-valued vectors or when the data has many categorical attributes with large sets of possible values for each attribute. (Notably, mixed-type data is addressed in (Lam et al., 2015) in the context of unsupervised feature extraction). In such general cases, it would be highly desirable to have ART models that can deal with this data in its native\nform without requiring transformations while still maintaining the desirable properties that hold for many existing ART models.\nDifferent activation functions can endow ART-based systems with new and improved capabilities to tailor the function according to the application. Approaches discussed in (Lavoie, 1999) include making the activation function a function of additional parameters (e.g., vigilance and time), defining individual activation functions for each category and dynamically varying the parameters between epochs without resetting the weights. All these modifications do not change the dynamics of the standard model; although changing the activation function implies changing the search order among the categories. Additionally, there have been some attempts at combining ART with evolutionary computing approaches and hyper-heuristics to achieve this goal (c.f. (Elnabarawy et al., 2017) and the references cited within), but there remain many challenges and opportunities to be addressed in this area."
    },
    {
      "heading": "6.4. Distributed representations",
      "text": "The winner-take-all category selection process used in the majority of ART architectures can sometimes lead to category proliferation and is one of the limiting factors of ART\u2019s capacity for mapping complex relations (Parrado-Herna\u0301ndez et al., 2003; Wunsch II, 2009). Extending the capabilities of many ART architectures toward distributed representations would lead to greater representational power for these architectures and allow them to encode more complex templates. However, the challenging aspect of this process is to maintain the desirable speed and stability of those ART systems in the presence of this distributed representation. There are examples of architectures that use distributed representations (see Tables 4 and 6), especially in supervised learning, however there are still many issues to be investigated."
    },
    {
      "heading": "6.5. Dichotomy of match- and error-based learning",
      "text": "In (Wunsch II, 2009) the conjecture is made that the dichotomy of match-based learning (i.e., Hebbian learning and ART) and error-based learning (i.e., using backpropagation in feed-forward neural networks such as deep learning architectures) is likely a false one. This still lacks a definitive resolution. Some contributions combined the use of match-based and error-based learning such as (Izquierdo et al., 2001; Su & Liu, 2002, 2005) by using gradient methods to optimize some of the ART parameters. However, the problem of building a system that can do both match- and error-based learning like animals appear to be capable of remains a more complex and interesting challenge, but it holds great promise for much more stable and effective machine learning. In biology, there are clear examples of learning that can happen quickly under the right circumstances, implying match-based learning, as well as incrementally improving through supervised or reinforcement learning in a way that implies error-based learning. The ability to master both types of learning and resolve this conjecture is believed to be a gateway to building machine learning systems that are fast and stable, possessing the ability for life-long learning and being resilient in the face of unpredictable changes in the environment."
    },
    {
      "heading": "7. Code repositories",
      "text": "A list of publicly available online source code/repositories is provided below:\n\u2022 github.com/ACIL-Group\n\u2022 techlab.bu.edu/main/article/software\n\u2022 ntu.edu.sg/home/asahtan/downloads.htm\n\u2022 http://www2.imse-cnm.csic.es/~bernabe\n\u2022 ee.bgu.ac.il/~boaz/software.html\n\u2022 libtopoart.eu"
    },
    {
      "heading": "8. Conclusions",
      "text": "This survey presents an overview of ART models used to perform unsupervised learning (a.k.a. clustering), classification, regression and reinforcement learning tasks. It provides a description for each model focusing on the motivation behind their designs, their dynamics, as well as key characteristics such as their code representation and long-term memory. Advantages of ART are discussed as well as open problems. Although mature, the field has room to grow and is still full of opportunities."
    },
    {
      "heading": "Acknowledgment",
      "text": "This research was sponsored by the Missouri University of Science and Technology Mary K. Finley Endowment and Intelligent Systems Center; the Coordenac\u0327a\u0303o de Aperfeic\u0327oamento de Pessoal de N\u0131\u0301vel Superior - Brazil (CAPES) - Finance code BEX 13494/13-9; the Army Research Laboratory (ARL) and the Lifelong Learning Machines program from DARPA/MTO, and it was accomplished under Cooperative Agreement Number W911NF-18-2-0260. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Laboratory or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation herein."
    }
  ],
  "title": "A Survey of Adaptive Resonance Theory Neural Network Models for Engineering Applications",
  "year": 2019
}
