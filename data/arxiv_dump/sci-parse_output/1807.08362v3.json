{
  "abstractText": "We propose definitions of fairness in machine learning and artificial intelligence systems that are informed by the framework of intersectionality, a critical lens arising from the Humanities literature which analyzes how interlocking systems of power and oppression affect individuals along overlapping dimensions including gender, race, sexual orientation, class, and disability. We show that our criteria behave sensibly for any subset of the set of protected attributes, and we prove economic, privacy, and generalization guarantees. We provide a learning algorithm which respects our intersectional fairness criteria. Case studies on census data and the COMPAS criminal recidivism dataset demonstrate the utility of our methods.",
  "authors": [
    {
      "affiliations": [],
      "name": "James R. Foulds"
    },
    {
      "affiliations": [],
      "name": "Rashidul Islam"
    },
    {
      "affiliations": [],
      "name": "Kamrun Naher Keya"
    },
    {
      "affiliations": [],
      "name": "Shimei Pan"
    }
  ],
  "id": "SP:1f962b51ce78d2d6f12bac447d55399f80bfb814",
  "references": [
    {
      "authors": [
        "M. Alexander"
      ],
      "title": "The new Jim Crow: Mass incarceration in the age of colorblindness",
      "venue": "T. N. P.",
      "year": 2012
    },
    {
      "authors": [
        "J. Angwin",
        "J. Larson",
        "S. Mattu",
        "L. Kirchner"
      ],
      "title": "Machine bias: There\u2019s software used across the country to predict future criminals",
      "venue": "and it\u2019s biased against blacks. ProPublica, May, 23",
      "year": 2016
    },
    {
      "authors": [
        "S. Barocas",
        "A.D. Selbst"
      ],
      "title": "Big data\u2019s disparate impact",
      "venue": "Cal. L. Rev., 104:671",
      "year": 2016
    },
    {
      "authors": [
        "R. Berk",
        "H. Heidari",
        "S. Jabbari",
        "M. Joseph",
        "M. Kearns",
        "J. Morgenstern",
        "S. Neel",
        "A. Roth"
      ],
      "title": "A convex framework for fair regression",
      "venue": "FAT/ML Workshop",
      "year": 2017
    },
    {
      "authors": [
        "R. Berk",
        "H. Heidari",
        "S. Jabbari",
        "M. Kearns",
        "A. Roth"
      ],
      "title": "Fairness in criminal justice risk assessments: The state of the art",
      "venue": "Sociological Methods and Research, 1050:28",
      "year": 2018
    },
    {
      "authors": [
        "A. Beutel",
        "J. Chen",
        "Z. Zhao",
        "E.H. Chi"
      ],
      "title": "Data decisions and theoretical implications when adversarially learning fair representations",
      "venue": "FAT/ML Workshop",
      "year": 2017
    },
    {
      "authors": [
        "T. Bolukbasi",
        "K.-W. Chang",
        "J.Y. Zou",
        "V. Saligrama",
        "A.T. Kalai"
      ],
      "title": "Man is to computer programmer as woman is to homemaker? Debiasing word embeddings",
      "venue": "Advances in NeurIPS",
      "year": 2016
    },
    {
      "authors": [
        "J. Buolamwini",
        "T. Gebru"
      ],
      "title": "Gender shades: Intersectional accuracy disparities in commercial gender classification",
      "venue": "FAT*, pages 77\u201391",
      "year": 2018
    },
    {
      "authors": [
        "A. Campolo",
        "M. Sanfilippo",
        "M. Whittaker"
      ],
      "title": "A",
      "venue": "Selbst K. Crawford, and S. Barocas. AI Now 2017 Symposium Report. AI Now",
      "year": 2017
    },
    {
      "authors": [
        "C.R. Charig",
        "D.R. Webb",
        "S.R. Payne",
        "J.E. Wickham"
      ],
      "title": "Comparison of treatment of renal calculi by open surgery",
      "venue": "percutaneous nephrolithotomy, and extracorporeal shockwave lithotripsy. British Medical Journal (BMJ) (Clin Res Ed), 292(6524):879\u2013882",
      "year": 1986
    },
    {
      "authors": [
        "P.H. Collins"
      ],
      "title": "Black feminist thought: Knowledge, consciousness, and the politics of empowerment (2nd ed.)",
      "year": 2002
    },
    {
      "authors": [
        "K. Crenshaw"
      ],
      "title": "Demarginalizing the intersection of race and sex: A black feminist critique of antidiscrimination doctrine",
      "venue": "feminist theory and antiracist politics. U. Chi. Legal F., pages 139\u2013167",
      "year": 1989
    },
    {
      "authors": [
        "J. Dastin"
      ],
      "title": "Amazon scraps secret AI recruiting tool that showed bias against women",
      "venue": "Reuters",
      "year": 2018
    },
    {
      "authors": [
        "A.Y. Davis"
      ],
      "title": "Are prisons obsolete",
      "venue": "Seven Stories Press,",
      "year": 2011
    },
    {
      "authors": [
        "Michele Donini",
        "Luca Oneto",
        "Shai Ben-David",
        "John S Shawe-Taylor",
        "Massimiliano Pontil"
      ],
      "title": "Empirical risk minimization under fairness constraints",
      "venue": "In Advances in Neural Information Processing Systems,",
      "year": 2018
    },
    {
      "authors": [
        "C. Dwork",
        "M. Hardt",
        "T. Pitassi",
        "O. Reingold",
        "R. Zemel"
      ],
      "title": "Fairness through awareness",
      "venue": "Proceedings of ITCS, pages 214\u2013226. ACM",
      "year": 2012
    },
    {
      "authors": [
        "C. Dwork",
        "F. McSherry",
        "K. Nissim",
        "A. Smith"
      ],
      "title": "Calibrating noise to sensitivity in private data analysis",
      "venue": "Th. of Cryptography, pages 265\u2013284",
      "year": 2006
    },
    {
      "authors": [
        "C. Dwork",
        "A. Roth"
      ],
      "title": "The algorithmic foundations of differential privacy",
      "venue": "Theoretical Computer Science, 9(3-4):211\u2013407",
      "year": 2013
    },
    {
      "authors": [
        "J.M. Grant",
        "L. Mottet",
        "J.E. Tanis",
        "J. Harrison",
        "J. Herman",
        "M. Keisling"
      ],
      "title": "Injustice at every turn: A report of the national transgender discrimination survey",
      "venue": "National Center for Transgender Equality",
      "year": 2011
    },
    {
      "authors": [
        "M. Hardt",
        "E. Price",
        "N. Srebro"
      ],
      "title": "et al",
      "venue": "Equality of opportunity in supervised learning. In Advances in NeurIPS, pages 3315\u20133323",
      "year": 2016
    },
    {
      "authors": [
        "U. Hebert-Johnson",
        "M. Kim",
        "O. Reingold",
        "G. Rothblum"
      ],
      "title": "Multicalibration: Calibration for the (Computationally-identifiable) masses",
      "venue": "Proceedings of the 35th ICML, PMLR",
      "year": 2018
    },
    {
      "authors": [
        "b. hooks"
      ],
      "title": "Ain\u2019t I a Woman: Black Women and Feminism",
      "year": 1981
    },
    {
      "authors": [
        "Matthew Jagielski",
        "Michael Kearns",
        "Jieming Mao",
        "Alina Oprea",
        "Aaron Roth",
        "Saeed Sharifi-Malvajerdi",
        "Jonathan Ullman"
      ],
      "title": "Differentially private fair learning",
      "venue": "arXiv preprint arXiv:1812.02696,",
      "year": 2018
    },
    {
      "authors": [
        "S.A. Julious",
        "M.A. Mullee"
      ],
      "title": "Confounding and simpson\u2019s paradox",
      "venue": "British Medical Journal (BMJ), 309(6967):1480\u20131481",
      "year": 1994
    },
    {
      "authors": [
        "M. Kearns",
        "S. Neel",
        "A. Roth",
        "Z.S. Wu"
      ],
      "title": "Preventing fairness gerrymandering: Auditing and learning for subgroup fairness",
      "venue": "J. Dy and A. Krause, editors, Proc. of ICML, PMLR 80, pages 2569\u20132577",
      "year": 2018
    },
    {
      "authors": [
        "Os Keyes",
        "Jevan Hutson",
        "Meredith Durbin"
      ],
      "title": "A mulching proposal: Analysing and improving an algorithmic system for turning the elderly into high-nutrient slurry",
      "venue": "In Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems, page alt06",
      "year": 2019
    },
    {
      "authors": [
        "D. Kifer",
        "A. Machanavajjhala"
      ],
      "title": "Pufferfish: A framework for mathematical privacy definitions",
      "venue": "TODS, 39(1):3",
      "year": 2014
    },
    {
      "authors": [
        "R. Kohavi"
      ],
      "title": "Scaling up the accuracy of naive-Bayes classifiers: a decision-tree hybrid",
      "venue": "Proceedings of SIGKDD, pages 202\u2013207",
      "year": 1996
    },
    {
      "authors": [
        "M.J. Kusner",
        "J. Loftus",
        "C. Russell",
        "R. Silva"
      ],
      "title": "Counterfactual fairness",
      "venue": "NeurIPS",
      "year": 2017
    },
    {
      "authors": [
        "A. Lorde"
      ],
      "title": "Age",
      "venue": "race, class, and sex: Women redefining difference. In Sister Outsider, pages 114\u2013124. Ten Speed Press",
      "year": 1984
    },
    {
      "authors": [
        "Max O Lorenz"
      ],
      "title": "Methods of measuring the concentration of wealth",
      "venue": "Publications of the American statistical association,",
      "year": 1905
    },
    {
      "authors": [
        "S. Mitchell",
        "E. Potash",
        "S. Barocas"
      ],
      "title": "Prediction-based decisions and fairness: A catalogue of choices",
      "venue": "assumptions, and definitions. arXiv preprint arXiv:1811.07867",
      "year": 2018
    },
    {
      "authors": [
        "C. Munoz",
        "M. Smith",
        "D.J. Patil"
      ],
      "title": "Big data: A report on algorithmic systems",
      "venue": "opportunity, and civil rights. Exec. Office of the President",
      "year": 2016
    },
    {
      "authors": [
        "S.U. Noble"
      ],
      "title": "Algorithms of Oppression: How Search Engines Reinforce Racism",
      "venue": "NYU Press",
      "year": 2018
    },
    {
      "authors": [
        "G. Pleiss",
        "M. Raghavan",
        "F. Wu",
        "J. Kleinberg",
        "K.Q. Weinberger"
      ],
      "title": "On fairness and calibration",
      "venue": "Advances in NeurIPS, pages 5684\u20135693",
      "year": 2017
    },
    {
      "authors": [
        "P.L. Roth",
        "P. Bobko",
        "F.S. Switzer III"
      ],
      "title": "Modeling the behavior of the 4/5ths rule for determining adverse impact: Reasons for caution",
      "venue": "Journal of Applied Psychology, 91(3):507",
      "year": 2006
    },
    {
      "authors": [
        "C. Simoiu",
        "S. Corbett-Davies",
        "S. Goel"
      ],
      "title": "et al",
      "venue": "The problem of inframarginality in outcome tests for discrimination. The Annals of Applied Statistics, 11(3):1193\u20131216",
      "year": 2017
    },
    {
      "authors": [
        "C. Verschelden"
      ],
      "title": "Bandwidth Recovery: Helping Students Reclaim Cognitive Resources Lost to Poverty",
      "venue": "Racism, and Social Marginalization. Stylus",
      "year": 2017
    },
    {
      "authors": [
        "J. Wald",
        "D.J. Losen"
      ],
      "title": "Defining and redirecting a school-to-prison pipeline",
      "venue": "New directions for youth development, 2003(99):9\u201315",
      "year": 2003
    }
  ],
  "sections": [
    {
      "text": "I. INTRODUCTION\nThe increasing impact of artificial intelligence and machine learning technologies on many facets of life, from commonplace movie recommendations to consequential criminal justice sentencing decisions, has prompted concerns that these systems may behave in an unfair or discriminatory manner [3], [35], [36]. A number of studies have subsequently demonstrated that bias and fairness issues in AI are both harmful and pervasive [2], [7], [8]. The AI community has responded by developing a broad array of mathematical formulations of fairness and learning algorithms which aim to satisfy them [4], [17], [22], [43]. Fairness, however, is not a purely technical construct, having social, political, philosophical and legal facets [9]. At this juncture, the necessity has become clear for interdisciplinary analyses of fairness in AI and its relationship to society, to civil rights, and to the social goals which are to be achieved by mathematical fairness definitions, which have not always been made explicit [34].\nIn particular, it is important to connect fairness and bias in algorithms to the broader context of fairness and bias in society, which has long been the concern of civil rights and feminist scholars and activists [28], [36]. In this work, we address the specific challenges of fairness in AI that are motivated by intersectionality, an analytical lens from the third-wave feminist movement which emphasizes that civil rights and feminism should be considered simultaneously rather than separately [13]. We propose intersectional AI fairness criteria and perform a comprehensive, interdisciplinary analysis of their relation to the concerns of diverse fields including the humanities, law, privacy, economics, and statistical machine learning. Our contributions include:\nThis work was performed under the following financial assistance award: 60NANB18D227 from U.S. Department of Commerce, National Institute of Standards and Technology.\n1) A critical analysis of the consequences of intersectionality in the particular context of fairness for AI, 2) Three novel fairness metrics: differential fairness (DF) which aims to uphold intersectional fairness for AI and machine learning systems, DF bias amplification, a slightly more politically conservative fairness definition which measures the bias specifically introduced by an algorithm, and differential fairness with confounders which can alter outcome distributions (DFC), 3) Proofs of the desirable intersectionality, privacy, economic, and generalization properties of our metrics, 4) A learning algorithm which enforces our criteria, and 5) Case studies on census and criminal recidivism data\nwhich demonstrate our methods\u2019 practicality and their benefits versus the subgroup fairness criterion of [27].\nII. INTERSECTIONALITY AND FAIRNESS IN AI\nWe begin with an introduction to intersectionality and an analysis of its relationship to fairness in an artificial intelligence and machine learning context. Intersectionality is a lens for examining societal unfairness which originally arose from the observation that sexism and racism have intertwined effects, in that the harm done to Black women by these two phenomena is more than the sum of the parts [13], [40]. The notion of intersectionality was later extended to include overlapping injustices along more general axes [11]. In its general form, intersectionality emphasizes that systems of oppression built into society lead to systematic disadvantages along intersecting dimensions, which include not only gender, but also race, nationality, sexual orientation, disability status, and socioeconomic class [11]\u2013[13], [24], [32], [40]. These systems are interlocking in their effects on individuals at each intersection of the affected dimensions.\nThe term intersectionality was introduced by Kimberle\u0301 Crenshaw in the 1980\u2019s [13] and popularized in the 1990\u2019s, e.g. by Patricia Hill Collins [11], although the ideas are much older [12], [40]. In the context of machine learning and fairness, intersectionality was recently considered by [8], who studied the impact of the intersection of gender and skin color on computer vision performance, and by [23], [27], who aimed to protect certain subgroups in order to prevent \u201cfairness gerrymandering.\u201d From a humanities perspective, [36] critiqued the behavior of the Google search engine with an intersectional lens, by examining the search results for terms relating to women, people of color, and their intersections, e.g. \u201cBlack girls.\u201d\nar X\niv :1\n80 7.\n08 36\n2v 3\n[ cs\n.L G\n] 1\n0 Se\np 20\n19\n(a) Inframarginality (b) Intersectionality (c) Inframarginality (d) Intersectionality (Causal Assumption) (Causal Assumption) (Ideal World) (Ideal World)\nIntersectionality has implications for AI fairness beyond the use of multiple protected attributes. Many fairness definitions aim (implicitly or otherwise) to uphold the principle of inframarginality, which states that differences between protected groups in the distributions of \u201cmerit\u201d or \u201crisk\u201d (e.g. the probability of carrying contraband at a policy stop) should be taken into account when determining whether bias has occurred [39]. A closely related argument is that parity of outcomes between groups is at odds with accuracy [17], [22]. Intersectionality theory provides a counterpoint: these differences in risk/merit, while acknowledged, are frequently due to systemic structural disadvantages such as racism, sexism, inter-generational poverty, the school-to-prison pipeline, mass incarceration, and the prison-industrial complex [12], [13], [15], [24], [42]. Systems of oppression can lead individuals to perform below their potential, for instance by reducing available cognitive bandwidth [41], or by increasing the probability of incarceration [1], [15]. In short, the infra-marginality principle makes the implicit assumption that society is a fair, level playing field, and thus differences in \u201cmerit\u201d or \u201crisk\u201d between groups in data and predictive algorithms are often to be considered legitimate. In contrast, intersectionality theory posits that these distributions of merit and risk are often influenced by unfair societal processes (see Figure 1).\nAs an example of a scenario affected by unfair processes, consider the task of predicting prospective students\u2019 academic performance for use in college admissions decisions. As discussed in detail by [41], and references therein, individuals belonging to marginalized and non-majority groups are disproportionately impacted by challenges of poverty and\nracism (in its structural, overt, and covert forms), including chronic stress, access to healthcare, under-treatment of mental illness, micro-aggressions, stereotype threat, disidentification with academics, and belongingness uncertainty. Similarly, LGBT and especially transgender, non-binary, and gender non-conforming students disproportionately suffer bullying, discrimination, self-harm, and the burden of concealing their identities. These challenges are often further magnified at the intersection of affected groups. A survey of 6,450 transgender and gender non-conforming individuals found that the most serious discrimination was experienced by people of color, especially Black respondents [21]. Verschelden explains the impact of these challenges as a tax on the \u201ccognitive bandwidth\u201d of non-majority students, which in turn affects their academic performance. She states that the evidence is clear\n\u201c...that racism (and classism, homophobia, etc.) has made people physically, mentally, and spiritually ill and dampened their chance at a fair shot at higher education (and at life and living).\u201d\nA classifier trained to predict students\u2019 academic performance from historical data hence aims to emulate outcomes that were substantially affected by unfair factors [3]. An accurate predictor for a student\u2019s GPA may therefore not correspond to a fair decision-making procedure [5]. We can resolve this apparent conflict if we are careful to distinguish between the statistical problem of classification, and the economic problem of the assignment of outcomes (e.g. admission decisions) to individuals based on classification. Viewing the classifier\u2019s task as a policy question, it becomes clear that high accuracy need not be the primary goal of the system, especially when we\nconsider that \u201caccuracy\u201d is measured on unfair data.1\nIn Figure 1 we summarize the causal assumptions regarding society and data, and the idealized \u201cperfect world\u201d scenarios implicit in the two approaches to fairness. Inframarginality (a) emphasizes that the distribution over relevant attributes X varies across protected groups A, which leads to potential differences in so-called \u201cmerit\u201d or \u201crisk\u201d between groups, typically presumed to correspond to latent ability and thus \u201cdeservedness\u201d of outcomes Y [39]. Intersectionality (b) emphasizes that we must also account for systems of oppression which lead to (dis)advantage at the intersection of multiple protected groups, impacting all aspects of the system including the ability of individuals to succeed (\u201cmerit\u201d) to their potential, had they not been impacted by (dis)advantage [13]. In the ideal world that an algorithmic (or other) intervention aims to achieve, inframarginality-based fairness desires that individual \u201cmerit\u201d is the sole determiner of outcomes (c) [22], [39], which can lead to disparity between groups [17]. In ideal intersectional fairness (d), since ability to succeed is affected by unfair processes, it is desired that this unfairness is corrected and individuals achieve their true potential [41]. Assuming potential does not substantially differ across protected groups, this implies that parity between groups is typically desirable.2\nIn light of the above, we argue that an intersectional definition of fairness in AI should satisfy the following criteria:\nA Multiple protected attributes should be considered. B All of the intersecting values of the protected attributes,\ne.g. Black women, should be protected by the definition. C We should still also ensure that protection is provided on\nindividual protected attribute values, e.g. women. D The definition should protect minority groups, who are\noften particularly affected by discrimination in society. E The definition should ensure that systematic differences\nbetween the protected groups, assumed to be due to structural oppression, are rectified, rather than codified.\nThese desiderata do not uniquely specify a fairness definition, but they provide a set of guidelines to which legal, political, and contextual considerations can then be applied to determine an appropriate fairness measure for a particular task."
    },
    {
      "heading": "III. EXISTING FAIRNESS DEFINITIONS",
      "text": "We now consider existing fairness definitions and their relation to the aforementioned criteria (see the Appendix for further discussion of related work). Relevant fairness definitions aim to detect and prevent discriminatory (or other) bias with respect to a set of protected attributes, such as gender, race, and disability status. Given criterion A, we focus on multi-attribute definitions. The two dominant multi-attribute\n1Amazon recently abandoned a classifier for job candidate selection which was found to be gender biased [14]. We speculate that this was likely due to similar issues.\n2Disparity could still be desirable if there are legitimate confounders which depend on protected groups, e.g. choice of department that individuals apply to in college admissions. We address this scenario in Section VII.\napproaches in the literature are subgroup fairness [27] and multicalibration [23].\nWe adapt the notation of [29] to all definitions in this paper. Suppose M(x) is a (possibly randomized) mechanism which takes an instance x \u2208 \u03c7 and produces an outcome y for the corresponding individual, S1, . . . , Sp are discrete-valued protected attributes, A = S1 \u00d7 S2 \u00d7 . . . \u00d7 Sp, and \u03b8 is the distribution which generates x. For example, the mechanism M(x) could be a deep learning model for a lending decision, A could be the applicant\u2019s possible gender and race, and \u03b8 the joint distribution of credit scores and protected attributes. The protected attributes are included in the attribute vector x, although M(x) is free to disregard them (e.g. if this is disallowed). The setting is illustrated in Figure 3.\nDefinition III.1. (Statistical Parity Subgroup Fairness [27]) Let G be a collection of protected group indicators g : A \u2192 {0, 1}, where g(s) = 1 designates that an individual with protected attributes s is in group g. Assume that the classification mechanism M(x) is binary, i.e. y \u2208 {0, 1}.\nThen M(x) is \u03b3-statistical parity subgroup fair with respect to \u03b8 and G if for every g \u2208 G,\n|PM,\u03b8(M(x) = 1)\u2212 PM,\u03b8(M(x) = 1|g(s) = 1)| \u00d7 P\u03b8(g(s) = 1) \u2264 \u03b3 . (1)\nNote that \u03b3 \u2208 [0, 1], smaller is better. The first term penalizes a difference between the probability of the positive class label for group g, and the population average of this probability. The term P\u03b8(g(s) = 1) weights the penalty by the size of group g as a proportion of the population. Statistical parity subgroup fairness (SF) is a multi-attribute definition satisfying criterion A. To satisfy B and C, G can be all intersectional subgroups (e.g. Black women) and top-level groups (e.g. men). The first term in Equation 1, which encourages similar outcomes between groups, enforces criterion E.\nFrom an intersectional perspective, one concern with SF is that it does not satisfy criterion D, the protection of minority groups. The term P\u03b8(g(s) = 1) weights the \u201cper-group\n(un)fairness\u201d for each group g, i.e. Equation 1 applied to g alone, by its proportion of the population, thereby specifically downweighting the consideration of minorities. In Figure 2, we show an example where varying the size of a minority group P (minority) drastically alters \u03b3-subgroup fairness, which finds that a rather extreme scenario is more acceptable when the minority group is small. Our proposed criterion, -DF (introduced in Section IV), is constant in P (minority).\nFigure 4 reports \u201cper-group\u201d \u03b3\u2019s on the UCI Adult census dataset, i.e. Equation 1 applied separately to each group, empirically seen have an increasing relationship with P (group). The final \u03b3-SF is determined by the worst case of the pergroup \u03b3\u2019s. A small minority group thereby will most likely not directly affect \u03b3-SF, since the downweighting makes it unlikely to be the \u201cmost unfair\u201d group.\nKearns et al. [27] justify the use of the P\u03b8(g(s) = 1) term via statistical considerations, as it is useful to prove generalization guarantees to extrapolate from empirical estimates of \u03b3 (see Section VIII-D). From a different ethical perspective, total utilitarianism, increasing the utility (i.e. reducing unfairness)\nfor a large group of individuals at the expense of smaller groups could also be justified by the increase in the total utility of the population. The problem with total utilitarianism, of course, is that it admits a scenario where many people possess low utility. We do not intend to dismiss SF as a valid notion of fairness. Our claim here, rather, is simply that due to its treatment of minority groups, SF does not fully encapsulate the principles of fairness advocated by intersectional feminist scholars and activists [11], [13], [24], [32], [40].\nOther candidate multi-attribute fairness definitions include false positive subgroup fairness [27] and multicalibration [23]. These definitions are similar to SF, but they concern false-positive rates and calibration of prediction probabilities, respectively. Since they focus on reliability of estimation rather than allocation of outcomes, they do not directly address criterion E, and so are weaker definitions from a civil rights/feminist perspective. This does not preclude their use for intersectional fairness scenarios in which harms are caused by incorrect predictions, rather than unfair outcome assignments; indeed, this is the type of approach [8] take for studying intersectional fairness in computer vision applications. Nevertheless, we will not consider them further here."
    },
    {
      "heading": "IV. DIFFERENTIAL FAIRNESS (DF) MEASURE",
      "text": "We now introduce our proposed fairness measures which satisfy our intersectionality criteria from Section III. Note that there are multiple conceivable fairness definitions which satisfy these criteria. For example, SF could be adapted to address criterion D by simply dropping the P\u03b8(g(s) = 1) term, at the loss of its associated generalization guarantees. We instead select an alternative formulation, which is similar to this approach in spirit, but which has additional beneficial properties from a societal perspective regarding the law, privacy, and economics, as we shall discuss below. Our formalism has a particularly elegant intersectionality property, in that Criterion C (protecting higher-level groups) follows automatically from Criterion B (protecting intersectional subgroups).\nWe motivate our criteria from a legal perspective. Consider the 80% rule, established in the Code of Federal Regulations\n[20] as a guideline for establishing disparate impact in violation of anti-discrimination laws such as Title VII of the Civil Rights Act of 1964. The 80% rule states that there is legal evidence of adverse impact if the ratio of probabilities of a particular favorable outcome, taken between a disadvantaged and an advantaged group, is less than 0.8:\nP (M(x) = 1|group A)/P (M(x) = 1|group B) < 0.8 . (2)\nOur first proposed criterion, which we call differential fairness (DF), extends the 80% rule to protect multi-dimensional intersectional categories, with respect to multiple output values. We similarly restrict ratios of outcome probabilities between groups, but instead of using a predetermined fairness threshold at 80%, we measure fairness on a sliding scale that can be interpreted similarly to that of differential privacy, a definition of privacy for data-driven algorithms [18]. Differential fairness measures the fairness cost of mechanism M(x) with a parameter .\nDefinition IV.1. A mechanism M(x) is -differentially fair (DF) with respect to (A,\u0398) if for all \u03b8 \u2208 \u0398 with x \u223c \u03b8, and y \u2208 Range(M),\ne\u2212 \u2264 PM,\u03b8(M(x) = y|si, \u03b8) PM,\u03b8(M(x) = y|sj , \u03b8) \u2264 e , (3)\nfor all (si, sj) \u2208 A\u00d7A where P (si|\u03b8) > 0, P (sj |\u03b8) > 0.\nIn Equation 3, si, sj \u2208 A are tuples of all protected attribute values, e.g. gender, race, and nationality, and \u0398 is a set of distributions \u03b8 which could plausibly generate each instance x.3 For example, \u0398 could be the set of Gaussian distributions over credit scores per value of the protected attributes, with mean and standard deviation in a certain range.\nThis is an intuitive intersectional definition of fairness: regardless of the combination of protected attributes, the probabilities of the outcomes will be similar, as measured by the\n3The possibility of multiple \u03b8 \u2208 \u0398 is valuable from a privacy perspective, where \u0398 is the set of possible beliefs that an adversary may have about the data, and is motivated by the work of [29]. Continuous protected attributes are also possible, in which case sums are replaced by integrals in our proofs.\nratios versus other possible values of those variables, for small values of . For example, the probability of being given a loan would be similar regardless of a protected group\u2019s intersecting combination of gender, race, and nationality, marginalizing over the remaining attributes in x. If the probabilities are always equal, then = 0, otherwise > 0. We have arrived at our criterion based on the 80% rule, but it can also be derived as a special case of pufferfish [29], a generalization of differential privacy [19] which uses a variation of Equation 3 to hide the values of an arbitrary set of secrets.\nDefinition IV.2. A mechanism M(x) is -pufferfish private [29] in a framework (S,Q,\u0398) if for all \u03b8 \u2208 \u0398 with x \u223c \u03b8, for all secret pairs (si, sj) \u2208 Q and y \u2208 Range(M),\ne\u2212 \u2264 PM,\u03b8(M(x) = y|si, \u03b8) PM,\u03b8(M(x) = y|sj , \u03b8) \u2264 e , (4)\nwhen si and sj are such that P (si|\u03b8) > 0, P (sj |\u03b8) > 0.\nDifferential fairness adapts pufferfish to the task of defining algorithmic fairness, by selecting a set of protected attributes as the secrets, and ensuring that the values of these attributes are indistinguishable. Thus, differential fairness provides a closely related privacy guarantee to differential privacy.\nIf PM,\u03b8 is unknown, it can be estimated using the empirical distribution, or via a probabilistic model of the data. Assuming discrete outcomes, PData(y|s) = Ny,sNs , where Ny,s and Ns are empirical counts of their subscripted values in the dataset D. Empirical differential fairness (EDF) corresponds to verifying that for any y, si, sj , we have\ne\u2212 \u2264 Ny,si Nsi Nsj Ny,sj \u2264 e , (5)\nAlternatively, if we estimate -DF via the posterior predictive distribution of a Dirichlet-multinomial model, the criterion for any y, si, sj becomes\ne\u2212 \u2264 Ny,si + \u03b1 Nsi + |Y|\u03b1 Nsj + |Y|\u03b1 Ny,sj + \u03b1 \u2264 e , (6)\nwhere scalar \u03b1 is each entry of the parameter of a symmetric Dirichlet prior with concentration parameter |Y|\u03b1, Y = Range(M). We refer to this as smoothed EDF.\nNote that EDF and smoothed EDF methods can sometimes be unstable in extreme cases when nearly all instances are assigned to the same class. To address this issue, instead of using empirical hard counts per group Ny,s, we can also use soft counts for (smoothed) EDF, based on a probabilistic classifier\u2019s predicted P (y|x), as follows:\ne\u2212 \u2264 \u2211\nx\u2208D:A=si P (y|x) + \u03b1 Nsi + |Y|\u03b1 Nsj + |Y|\u03b1\u2211 x\u2208D:A=sj P (y|x) + \u03b1 \u2264 e .\n(7)"
    },
    {
      "heading": "V. DF BIAS AMPLIFICATION MEASURE",
      "text": "We can adapt DF to measure fairness in data, i.e. outcomes assigned by a black-box algorithm or social process, by using (a model of) the data\u2019s generative process as the mechanism.\nDefinition V.1. A labeled dataset D = {(x1, y1), . . . , (xN , yN )} is -differentially fair (DF) in A with respect to model PModel(x, y) if mechanism M(x) = y \u223c PModel(y|x) is -differentially fair with respect to (A, {PModel(x)}), for PModel trained on the dataset.\nSimilarly to differential privacy, differences 2\u2212 1 between two mechanisms M2(x) and M1(x) are meaningful (for fixed A and \u0398, and for tightly computed minimum values of ), and measure the additional \u201cfairness cost\u201d of using one mechanism instead of the other. When 1 is the differential fairness of a labeled dataset and 2 is the differential fairness of a classifier measured on the same dataset, 2\u2212 1 is a measure of the extent to which the classifier increases the unfairness over the original data, a phenomenon that [43] refer to as bias amplification.\nDefinition V.2. A mechanism M(x) satisfies ( 2\u2212 1)-DF bias amplification with respect to (A,\u0398, D,M) if it is 2-DF and D is a labeled dataset which is 1-DF with respect to model M.\nPolitically speaking, -DF is a relatively progressive notion of fairness which we have motivated based on intersectionality (disparities in societal outcomes are largely due to systems of oppression), and which is reminiscent of demographic parity [17]. On the other hand, ( 2\u2212 1)-DF bias amplification is a more politically conservative fairness metric which does not seek to correct unfairness in the original dataset (i.e. it relaxes criterion E), in line with the principle of infra-marginality (a system is biased only if disparities in its behavior are worse than those in society) [39]. Informally, 2-DF and ( 2 \u2212 1)- DF bias amplification represent \u201cupper and lower bounds\u201d on the unfairness of the system in the case where the relative effect of structural oppression on outcomes is unknown.\nVI. ILLUSTRATIVE WORKED EXAMPLES\nA simple worked example of differential fairness is given in Figure 5. In the example, given an applicant\u2019s score x on a standardized test, the mechanism M(x) = x \u2265 t approves\nthe hiring of a job applicant if their test score x \u2265 t, with t = 10.5. The scores are distributed according to \u03b8, which corresponds to the following process. The applicant\u2019s protected group is 1 or 2 with probability 0.5. Test scores for group 1 are normally distributed N(x;\u00b51 = 10, \u03c3 = 1), and for group 2 are distributed N(x;\u00b52 = 12, \u03c3 = 1). In the figure, the group-conditional densities are plotted on the top, along with the threshold for the hiring outcome being yes (i.e. M(x) = 1). Shaded areas indicate the probability of a yes hiring decision for each group (overlap in purple). On the bottom, the calculations show that M(x) is -differentially fair for = 2.337. This means that the probability ratios are bounded within the range (e\u2212 , e ) = (0.0966, 10.35), i.e. one group has around 10 times the probability of some particular hiring outcome than the other (y = no). Under the presumption that the two groups are roughly equally capable of performing the job overall, this is clearly unsatisfactory in terms of fairness.\nThe intersectional setting, in which there are multiple protected variables, is specifically addressed by differential fairness, by considering the probabilities of outcomes for each intersection of the set of protected variables. We illustrate this setting with an example on admissions of prospective students to a particular University X. In the scenario, the protected attributes are gender and race, and the mechanism is the admissions process, with a binary outcome. Our data, shown in Table I, is adapted from a real-world scenario involving treatments for kidney stones, often used to demonstrate Simpson\u2019s paradox [10], [26]. Here, the \u201cparadox\u201d is that for race 1, individuals of gender A are more likely to be admitted than those of gender B, and for race 2, those of gender A are also more likely to be admitted than those of gender B, yet counter-intuitively, gender B is more likely to be admitted overall.\nSince the admissions process is a black box, we model it using Equation 5, empirical differential fairness (EDF). By calculating the log probability ratios of (Gender,Race) pairs from Table I, as well as for the pairs of probabilities for the declined admission outcome (1 \u2212 P (admit)), and plugging them into Equation 5, we see that the mechanism is = 1.511- DF with A = Gender \u00d7 Race. By calculating using the admission probabilities in the Overall row (Gender) and the Overall column (Race), we find that = 0.2329 for A = Gender, and = 0.8667 for A = Race. We will prove\nin Theorem VIII.1 that with A = Gender\u00d7Race is an upper bound on -DF for A = Gender and for A = Race. Thus, even with a \u201cSimpson\u2019s reversal\u201d differential (un)fairness will not increase after summing out a protected attribute."
    },
    {
      "heading": "VII. DEALING WITH CONFOUNDER VARIABLES",
      "text": "As we have seen, differential fairness can be used to measure the inequity between the outcome probabilities for the protected groups and their intersections at different levels of measurement granularity, although it does not determine whether the inequities were due to systemic factors and/or discrimination. In the case study above, a confounding variable which could explain the Simpson\u2019s reversal is the decision of the prospective student on whether to apply to University X. The -DF criterion is appropriate when the differences are believed to be due to systems of oppression, as posited by intersectionality theory, and such confounder variables are not present. With confounders, parity in outcomes between intersectional protected groups, which -DF rewards, may no longer be desirable (see Figure 6). We propose an alternative fairness definition for when known confounders are present.\nDefinition VII.1. Let \u03b8 \u2208 \u0398 be distributions over (x, c), where c \u2208 C are confounder variables. A mechanism M(x) is -differentially fair with confounders (DFC) with respect to (A,\u0398, C), if for all c \u2208 C, M(x) is -DF with respect to (A,\u0398|c), where \u0398|c = {P (x|\u03b8, c)|\u03b8 \u2208 \u0398}.\nIn the university admissions case, Definition VII.1 penalizes disparity in admissions at the department level, and the most unfair department determines the overall unfairness -DFC.\nTheorem VII.1. Let M be an -DFC mechanism in (A,\u0398, C), Then M is -differentially fair in (A,\u0398).\nFrom Theorem VII.1, if we protect differential fairness per department, we obtain differential fairness and its corresponding theoretical economic and privacy guarantees in the University\u2019s overall admissions, bounded by the of the most unfair department, even in the case of a Simpson\u2019s reversal. A proof is given in the Appendix. If confounder variables are latent, we can attempt to infer them probabilistically in order to\napply DFC. Alternatively, ( 2\u2212 1)-DF bias amplification can still be used to study the impact of an algorithm on fairness."
    },
    {
      "heading": "VIII. PROPERTIES OF DIFFERENTIAL FAIRNESS",
      "text": "We now discuss the theoretical properties of our definitions."
    },
    {
      "heading": "A. Differential Fairness and Intersectionality",
      "text": "Differential fairness explicitly encodes protection of intersectional groups (criterion B). For DF, we prove that this automatically implies fairness for each of the protected attributes individually (criterion C), and indeed, any subset of the protected attributes. For example, if a loan approval mechanism M(x) is -DF in A = gender \u00d7 race \u00d7 nationality, it is also -DF in, e.g., A = gender by itself, or A = gender \u00d7 nationality. In other words, by ensuring fairness at the intersection of gender, race, and nationality under our criterion, we also ensure the same degree of fairness between genders overall, and between gender/nationality pairs overall, and so on. In the above, is a worst case, and DF may also hold for lower values of .\nLemma VIII.1. (Proof given in the Appendix.) The -DF criterion can be rewritten as: for any \u03b8 \u2208 \u0398, y \u2208 Range(M),\nlog max s\u2208A:P (s|\u03b8)>0\nPM,\u03b8(M(x) = y|s, \u03b8)\n\u2212 log min s\u2208A:P (s|\u03b8)>0 PM,\u03b8(M(x) = y|s, \u03b8) \u2264 . (8)\nTheorem VIII.1. (Intersectionality Property) Let M be an -differentially fair mechanism in (A,\u0398), A = S1\u00d7S2\u00d7 . . .\u00d7 Sp, and let D = Sa \u00d7 . . . \u00d7 Sk be the Cartesian product of a nonempty proper subset of the protected attributes included in A. Then M is -differentially fair in (D,\u0398).\nProof. Define E = S1 \u00d7 . . . \u00d7 Sa\u22121 \u00d7 Sa+1 . . . \u00d7 Sk\u22121 \u00d7 Sk+1 \u00d7 . . . \u00d7 Sp, the Cartesian product of the protected attributes included in A but not in D. Then for any \u03b8 \u2208 \u0398, y \u2208 Range(M),\nlog max s\u2208D:P (s|\u03b8)>0\nPM,\u03b8(M(x) = y|D = s, \u03b8)\n= log max s\u2208D:P (s|\u03b8)>0 \u2211 e\u2208E PM,\u03b8(M(x) = y|E = e, s, \u03b8)P\u03b8(E = e|s, \u03b8)\n\u2264 log max s\u2208D:P (s|\u03b8)>0 \u2211 e\u2208E\nmax e\u2032\u2208E:P\u03b8(E=e\u2032|s,\u03b8)>0( PM,\u03b8(M(x) = y|E = e\u2032, s, \u03b8) ) \u00d7 P\u03b8(E = e|s, \u03b8)\n= log max s\u2208D:P (s|\u03b8)>0 max e\u2032\u2208E:P\u03b8(E=e\u2032|s,\u03b8)>0\nPM,\u03b8(M(x) = y|E = e\u2032, s, \u03b8)\n= log max s\u2032\u2208A:P (s\u2032|\u03b8)>0\nPM,\u03b8(M(x) = y|s\u2032, \u03b8)\nBy a similar argument, log mins\u2208D:P (s|\u03b8)>0 PM,\u03b8(M(x) = y|D = s, \u03b8) \u2265 log mins\u2032\u2208A:P (s\u2032|\u03b8)>0 PM,\u03b8(M(x) = y|s\u2032, \u03b8).\n0 5 10 15\nNumber of protected attributes\n0\n5\n10 15 N u m b e r o f g ro u p s\n10 6\nAll groups and subgroups Bottom-level intersectional groups only\nFig. 7. The number of groups and intersectional subgroups to protect when varying the number of protected attributes, with 2 values per protected attribute.\nApplying Lemma VIII.1, we hence bound in (D,\u0398) as\nlog max s\u2208D:P (s|\u03b8)>0\nPM,\u03b8(M(x) = y|D = s, \u03b8)\n\u2212 log min s\u2208D:P (s|\u03b8)>0 PM,\u03b8(M(x) = y|D = s, \u03b8)\n\u2264 log max s\u2032\u2208A:P (s\u2032|\u03b8)>0 PM,\u03b8(M(x) = y|s\u2032, \u03b8)\n\u2212 log min s\u2032\u2208A:P (s\u2032|\u03b8)>0 PM,\u03b8(M(x) = y|s\u2032, \u03b8) \u2264 . (9)\nThis property is philosophically concordant with intersectionality, which emphasizes empathy with all overlapping marginalized groups. However, its benefits are mainly practical: in principle, one could protect all higher-level groups in SF by specifying \u2211p j=1 ( p j ) Kj binary indicator protected groups, where K is the number of values per protected attribute. This quickly becomes computationally and statistically infeasible. For example, Figure 7 counts the number of protected groups that must be explicitly considered under the two intersectional fairness definitions, in order to respect the intersectional fairness criteria B and C. The intersectionality property (Theorem VIII.1) implies that when the the bottom-level intersectional groups are protected (blue curve), differential fairness will automatically protect all higher-level groups and subgroups (red curve). Since subgroup fairness does not have this property, all of the groups and subgroups (red curve) must be protected explicitly with their own group indicators g(s). Although the number of bottom-level groups grows exponentially in the number of protected attributes, the total number of groups grows much faster, at the combinatorial rate of \u2211p j=1 ( p j ) Kj ."
    },
    {
      "heading": "B. Privacy Interpretation",
      "text": "The differential fairness definition, and the resulting level of fairness obtained at any particular measured fairness parameter , can be interpreted by viewing the definition through the lens\nof privacy. Differential fairness ensures that given the outcome, an untrusted vendor/adversary can learn very little about the protected attributes of the individual, relative to their prior beliefs, assuming their prior beliefs are in \u0398:\ne\u2212 P (si|\u03b8) P (sj |\u03b8) \u2264 P (si|M(x) = y, \u03b8) P (sj |M(x) = y, \u03b8) \u2264 e P (si|\u03b8) P (sj |\u03b8) . (10)\nE.g., if a loan is given to an individual, an adversary\u2019s Bayesian posterior beliefs about their race and gender will not be substantially changed. Thus, the adversary will be unable to infer that \u201cthis individual was given a loan, so they are probably white and male.\u201d Our definition thereby provides fairness guarantees when the user of M(x) is untrusted, cf. [17], by preventing subsequent discrimination, e.g. in retaliation to a fairness correction. Although DF is a population-level definition, it provides a privacy guarantee for individuals. The privacy guarantee only holds if \u03b8 \u2208 \u0398, which may not always be the case. Regardless, the value of may typically be interpreted as a privacy guarantee against a \u201creasonable adversary.\u201d The privacy guarantee is inherited from pufferfish, a general privacy framework which DF instantiates [29]."
    },
    {
      "heading": "C. Economic Guarantees",
      "text": "We also show that differential fairness provides economic guarantees. An -differentially fair mechanism admits a disparity in expected utility of as much as a factor of exp( ) \u2248 1+ (for small values of ) between pairs of protected groups with si \u2208 A, sj \u2208 A, for any utility function that could be chosen. E.g., consider a loan approval process, where the utility of being given a loan is 1, and being denied is 0. Suppose the approval process is ln(3)-differentially fair. The process could then be three times as likely to award a loan to white men as to white women, and thus award white men three times the expected utility as white women. The proof follows the case of differential privacy [19]. Let u(y) : Range(M(x))\u2192 R\u22650 be a utility function. Then:\nEPM,\u03b8 [ u(y)|si ] = \u222b PM,\u03b8(y|si)u(y)dy (11)\n\u2264 \u222b e PM,\u03b8(y|sj)u(y)dy = e EPM,\u03b8 [ u(y)|sj ] .\nSimilarly, for ( 2 \u2212 1)-DF bias amplification, M(x) admits at most an exp( 2 \u2212 1) \u2248 1 + 2 \u2212 1 (for small values of 2 \u2212 1) multiplicative increase in the disparity of expected utility between pairs of protected intersections of groups with si \u2208 A, sj \u2208 A, relative to the data generating process M."
    },
    {
      "heading": "D. Generalization Guarantees",
      "text": "In order to ensure that an algorithm is truly fair, it is important that the fairness properties obtained on a dataset will extend to the underlying population. Kearns et al. [27] proved that empirical estimates of the quantities per group which determine subgroup fairness, PM,\u03b8(y = 1|g(s) = 1)P\u03b8(g(s) = 1), will be similar to their true values, with enough data relative to the VC dimension of the classification model\u2019s concept class H. We state their result below.\nTheorem VIII.2. [27]\u2019s Theorem 2.11 (SP Uniform Convergence). Fix a class of functions H and a class of group indicators G. For any distribution P , let S \u223c Pm be a dataset consisting of m examples (xi, yi) sampled i.i.d. from P . Then for any 0 < \u03b4 < 1, with probability 1 \u2212 \u03b4, for every h \u2208 H and g \u2208 G, we have:\n|P (y = 1|g(s) = 1, h)P (g(s) = 1) \u2212 PS(y = 1|g(s) = 1, h)PS(g(s) = 1)|\n\u2264 O\u0303 (\u221a (VCDIM(H) + VCDIM(G)) logm+ log(1/\u03b4)\nm\n) .\n(12)\nHere, O\u0303 hides logarithmic factors, and PS is the empirical distribution from the S samples. It is natural to ask whether a similar result holds for differential fairness. As [27] note, the SF definition was chosen for statistical reasons, revealed in the above equation: the P\u03b8(g(s) = 1) term in SF arises naturally in their generalization bound. For DF, we specifically avoid this term due to its impact on minority groups, and must instead bound PM,\u03b8(y|s) per group s. For this case, we prove the following generalization guarantee.\nTheorem VIII.3. Fix a class of functions H, which without loss of generality aim to discriminate the outcome y = 1 from any other value, denoted here as y = 0. For any conditional distribution P (y,x|s) given a group s, let S \u223c Pm be a\ndataset consisting of m examples (xi, yi) sampled i.i.d. from P (y,x|s). Then for any 0 < \u03b4 < 1, with probability 1\u2212 \u03b4, for every h \u2208 H, we have:\n|P (y = 1|s, h)\u2212 PS(y = 1|s, h)| \u2264 O\u0303 (\u221aVCDIM(H) logm+ log(1/\u03b4)\nm\n) . (13)\nProof. Let g(s\u2032) = 1 when s\u2032 = s and 0 otherwise, and let G = {g(s\u2032)}. We see that G has a VC-dimension of 0. The result follows directly by applying Theorem VIII.2 ( [27]\u2019s Theorem 2.11) to H and G, and considering the bound for the distributions P over (x, y) where P (g(s\u2032) = 1) = 1.\nWhile SF has generalization bounds which depend on the overall number of data points, DF\u2019s generalization guarantee requires that we obtain a reasonable number of data points for each intersectional group in order to accurately estimate -DF. This difference, the price of removing the minority-biasing term, should be interpreted in the context of the differing goals of our work and [27], who aimed to prevent fairness gerrymandering by protecting every conceivable subgroup that could be targeted by an adversary.\nIn contrast, our goal is to uphold intersectionality, which simply aims to enact a more nuanced understanding of unfairness than with a single protected dimension such as gender or race. In practice, consideration of 2 or 3 intersecting protected dimensions already improves the nuance of assessment.\nSufficient data per intersectional group can often be readily obtained in such cases, e.g. [8] studied the intersection of gender and skin color on fairness. Similarly, [27] focus on the challenge of auditing subgroup fairness when the subgroups cannot easily be enumerated, which is important in the fairness gerrymandering setting. In contrast, in our intended applications of preserving intersectional fairness the number of intersectional groups is often only around 22 \u2013 25."
    },
    {
      "heading": "IX. LEARNING ALGORITHM",
      "text": "In this section we introduce a simple, practical learning algorithm for differentially fair classifiers (DF-Classifiers). Our algorithm uses the fairness cost as a regularizer to balance the trade-off between fairness and accuracy. We minimize, with respect to the classifier MW(x)\u2019s parameters W, a loss function LX(W) plus a penalty on unfairness which is weighted by a tuning parameter \u03bb > 0. We train fair neural networks using gradient descent (GD) on our objective via backpropagation and automatic differentiation. The learning objective for training data X becomes:\nmin W [LX(W) + \u03bbRX( )] (14)\nwhere RX( ) = max(0, MW(x) \u2212 1) represents the fairness penalty term, and MW(x) is the for MW(x). To make the objective differentiable, MW(x) is measured using soft counts (Equation 7). If 1 is 0, this penalizes -DF, and if 1 is the data\u2019s , this penalizes bias amplification. Optimizing for bias amplification will also improve -DF, up to the 1 threshold. In practice, we found that a warm start optimizing LX(W) only for several \u201cburn-in\u201d iterations improves convergence. For large datasets, stochastic gradient descent (SGD) can be used instead of batch GD. In this case, we recommend that MW(x) be estimated on a development set D, as minibatch estimates may be unstable in the intersectional data regime."
    },
    {
      "heading": "X. EXPERIMENTS",
      "text": "We performed all experiments on two datasets: the Adult 1994 U.S. census income data from the UCI repository [30] (protected attributes: race, gender, USA vs non-USA nationality), and the COMPAS dataset regarding a system that is used to predict criminal recidivism [2] (protected attributes: race and gender).4"
    },
    {
      "heading": "A. Fair Learning Algorithm",
      "text": "The goals of our experiments were to demonstrate the practicality of our DF-Classifier method in learning an intersectionally fair classifier, and to compare its behavior to a learned subgroup fair SF-Classifier and a typical classifier (without the fairness penalty term of Equation 14), especially with regards to minorities. Instead of [27]\u2019s algorithm, we trained the SF-Classifier using the same GD+backpropagation approach, replacing with \u03b3 in Equation 14, i.e. RX(\u03b3) = max(0, \u03b3MW(x)\u2212 \u03b31). This simplifies and speeds up learning to handle deep neural networks.\n4Predicted income, used for consequential decisions like housing approval, may result in digital redlining [3].\nAll classifiers were trained on a common neural network architecture via adaptive gradient descent optimization (Adam) with learning rate = 0.01 using pyTorch. The configuration of the neural network was 3 hidden layers, 16 neurons in each layer, \u201crelu\u201d and \u201csigmoid\u201d activations for the hidden and output layers, respectively. We trained for 500 iterations, disabling the fairness penalties for the first 50 \u201cburn-in\u201d iterations. We chose \u03bb as 0.1 and 1.0 for DF-Classifier and SF-Classifier, respectively, as a best trade-off value via grid search over the randomly held out 20% development sets.\nWe learned fair classifiers in several settings: 1) we set the target thresholds to perfect fairness, 1=0.0 and \u03b31=0.0 for DF-Classifier and SF-Classifier, respectively, and 2) to penalize bias amplification by the algorithm, by setting the thresholds to 1= data and \u03b31=\u03b3data for DF-Classifier and SF-Classifier, respectively. Finally, to protect the 80%-rule we set 1=\u2212 log 0.8 = 0.2231 for DF-Classifier only. Since there is no straightforward way to enforce the 80%-rule for SFClassifier, it was not considered in this analysis.\nTables II and III compare the classifiers on the Adult and COMPAS datasets, respectively. Both DF-Classifier and SFClassifier were able to substantially improve their fairness metrics over the typical classifier, with modest costs in accuracy, F1 score, and ROC AUC, and the trade-off varied roughly monotonically in the target value 1 or \u03b31. Based on soft count estimation (Equation 7), the DF-Classifier with 1 = 0 improved from = 1.646 to = 0.428 on Adult with a loss of 2.8 percentage points of accuracy. On COMPAS, it improved from = 0.773 to = 0.180, corresponding to a worst-case difference in utility between groups of a factor of e \u2248 1.2, with a loss of just 1.4 percentage points of accuracy. When trained to prevent bias amplification, the fairness metrics were improved with little (COMPAS) to no (Adult) reduction in accuracy. While SF-Classifier typically had slightly higher accuracy under the same settings, DFClassifier often greatly improved \u03b3-SF as well, while SFClassifier enjoyed only modest improvements in -DF. The conclusions were similar with \u201chard count\u201d smoothed EDF estimates (Equation 6), but the metrics\u2019 estimates were higher.\nAn important goal of this work was to consider the impact of the fairness methods on minority groups. In Figure 8, we report the \u201cper-group unfairness,\u201d defined as Equations 1 and 3 with one group held fixed, versus the group\u2019s probability (i.e. size) on the COMPAS dataset. Both methods improve their corresponding per-group unfairness measures over the typical classifier. On the other hand, similarly to Figure 4, the \u03b3-SF metric only assigns high per-group unfairness values to large groups in its measurement, so minority groups are not able to influence the overall \u03b3-SF unfairness. This was not the case for -DF metric, where groups of various sizes had similarly high per-group values. Furthermore, the DF-Classifier improved the per-group fairness under both metrics for groups of all sizes, while the SF-classifier did not improve the per-group \u03b3-SF for small groups. Our overall conclusion is that the DF-Classifier is able to achieve intersectionally fair classification with minor loss in\nperformance, while providing greater protection to minority groups than when enforcing subgroup fairness.\nB. Inequity of Fairness Measures\nWe have seen that the \u03b3-SF metric downweights the consideration of minorities (cf. Figures 4 and 8). In this experiment, we quantify the resulting inequity of fairness consideration using the Gini coefficient [33], a commonly used measure of statistical dispersion which is often used to represent the inequity of income. The Gini coefficient (G) of a fairness metric F is calculated as\nG = 1\n2\u00b5 n\u2211 i=1 n\u2211 j=1 P (si)P (sj)|Fsi \u2212 Fsj | , (15)\nwhere \u00b5 = \u2211n i=1 FsiP (si) and P (si) is the fraction of population belonging to the ith intersectional group, while Fsi represents the fairness measure (i.e. per-group or \u03b3) of that group. For a fixed algorithm and data distribution, a fairness metric with a smaller Gini coefficient distributes its (un)fairness consideration more equitably across the population, which is typically desirable in the sense that the entire population has a voice in the determination of (un)fairness.\nTable IV shows a comparison of G values for the -DF and \u03b3-SF metrics on the Adult and COMPAS datasets. Both fairness metrics are measured for the labeled dataset (i.e. Data) as well as for a logistic regression (LR) classifier (i.e. LR) trained on the same dataset. In all the experiments, the G value for -DF is much lower compared to \u03b3-SF\u2019s G value. Thus, -DF was observed to provide a more equitable distribution of its per-group fairness measurements, presumably due to its more inclusive treatment of minority groups."
    },
    {
      "heading": "C. Evaluation of Intersectionality Property",
      "text": "In our final experiment (Table V), we studied the ability of \u03b3-SF to preserve the intersectionality property shown for -DF in Theorem VIII.1, by measuring fairness with different sets\nof protected attributes. The property is violated if removing a protected attribute increases the metric. As expected, -DF obeyed the intersectionality property, but \u03b3-SF violated it as \u03b3 for gender > \u03b3 for race \u00d7 gender (COMPAS), and \u03b3 for gender > \u03b3 for gender \u00d7 nationality (Adult)."
    },
    {
      "heading": "XI. CONCLUSION",
      "text": "We introduced three AI fairness definitions which satisfy intersectional fairness desiderata, differential fairness and its bias amplification and confounder-aware counterparts, and proved their attractive properties regarding the law, privacy, economics, and statistical learning, along with a learning algorithm to enforce them. With extensive experiments across two datasets, we have shown that our criteria can be practically attained, and they behave more equitably with regard to minority groups than subgroup fairness. In future work, we plan to investigate the impact of data sparsity on the measurement and enforcement of fairness in the intersectional multi-attribute regime."
    },
    {
      "heading": "ACKNOWLEDGMENT",
      "text": "We thank Rosie Kar for valuable advice and feedback regarding intersectional feminism."
    },
    {
      "heading": "A. Proof of Lemma VIII.1",
      "text": "Proof. The definition of -differential fairness is, for any \u03b8 \u2208 \u0398, y \u2208 Range(M), (si, sj) \u2208 A \u00d7 A where P (si|\u03b8) > 0, P (sj |\u03b8) > 0,\ne\u2212 \u2264 PM,\u03b8(M(x) = y|si, \u03b8) PM,\u03b8(M(x) = y|sj , \u03b8) \u2264 e . (16)\nTaking the log, we can rewrite this as:\n\u2212 \u2264 logPM,\u03b8(M(x) = y|si, \u03b8) \u2212 logPM,\u03b8(M(x) = y|sj , \u03b8) \u2264 . (17)\nThe two inequalities can be simplified to:\n| logPM,\u03b8(M(x) = y|si, \u03b8)\u2212 logPM,\u03b8(M(x) = y|sj , \u03b8)| \u2264 . (18)\nFor any fixed \u03b8 and y, we can bound the left hand side by plugging in the worst case over (si, sj),\n| logPM,\u03b8(M(x) = y|si, \u03b8)\u2212 logPM,\u03b8(M(x) = y|sj , \u03b8)| \u2264 log max\ns:P (s|\u03b8)>0 PM,\u03b8(M(x) = y|s, \u03b8)\n\u2212 log min s:P (s|\u03b8)>0 PM,\u03b8(M(x) = y|s, \u03b8) . (19)\nPlugging in this bound, which is achievable and hence is tight, the criterion is then equivalent to:\nlog max s:P (s|\u03b8)>0\nPM,\u03b8(M(x) = y|s, \u03b8)\n\u2212 log min s:P (s|\u03b8)>0 PM,\u03b8(M(x) = y|s, \u03b8) \u2264 . (20)"
    },
    {
      "heading": "B. Proof of Theorem VII.1",
      "text": "Proof. Let \u03b8 \u2208 \u0398, y \u2208 Range(M), c \u2208 C, and (si, sj) \u2208 A\u00d7A where P (si|\u03b8) > 0 and P (sj |\u03b8) > 0. We have:\nPM,\u03b8(M(x) = y|si, \u03b8) PM,\u03b8(M(x) = y|sj , \u03b8)\n= \u2211 c\u2208C PM,\u03b8(M(x) = y|si, c, \u03b8)PM,\u03b8(c|si, \u03b8)\u2211 c\u2208C PM,\u03b8(M(x) = y|sj , c, \u03b8)PM,\u03b8(c|sj , \u03b8)\n=\n\u2211 c\u2208C\nPM,\u03b8(M(x)=y|si,c,\u03b8) PM,\u03b8(M(x)=y|sj ,c,\u03b8)P\u03b8(c|si, \u03b8)\u2211\nc\u2208C PM,\u03b8(M(x)=y|sj ,c,\u03b8) PM,\u03b8(M(x)=y|sj ,c,\u03b8)P\u03b8(c|sj , \u03b8)\n= \u2211 c\u2208C PM,\u03b8(M(x) = y|si, c, \u03b8) PM,\u03b8(M(x) = y|sj , c, \u03b8) P\u03b8(c|si, \u03b8)\n\u2264 \u2211 c\u2208C e P\u03b8(c|si, \u03b8) = e . (21)\nReversing si and sj and taking the reciprocal shows the other inequality."
    },
    {
      "heading": "C. Related Work",
      "text": "This section discusses relationships with other concepts in fairness, privacy, and in the treatment of subsets of protected groups.\n1) Fairness Definitions: An overview of fairness research can be found in [5]. We briefly describe several of the most influential mathematical definitions of fairness below, and discuss their relationships to our proposed differential fairness criterion.\nThe 80% rule: Our criterion is related to the 80% rule, a.k.a. the four-fifths rule, a guideline for identifying unintentional discrimination in a legal setting which identifies disparate impact in cases where P (y = 1|s1)/P (y = 1|s2) \u2264 0.8, for a favourable outcome y = 1, disadvantaged group s1, and best performing group s2 [20]. This corresponds to testing that \u2265 \u2212 log 0.8 = 0.2231, in a version of Equation 3 where only the outcome y = 1 is considered.\nDemographic Parity: [17] defined (and criticized) the fairness notion of demographic parity, a.k.a. statistical parity, which requires that P (y|si) = P (y|sj) for any outcome y and pairs of protected attribute values si, sj (here assumed to be a single attribute). This can be relaxed, e.g. by requiring the total variation distance between the distributions to be less than . Differential fairness is closely related as it also aims to match probabilities of outcomes, but measures differences using ratios, and allows for multiple protected attributes. The criticisms of [17] are mainly related to ways in which subgroups of the protected groups can be treated differently while maintaining demographic parity, which they call \u201csubset targeting,\u201d and which [27] term \u201cfairness gerrymandering.\u201d Differential fairness explicitly protects the intersection of multiple protected attributes, which can be used to mitigate some of these abuses.\nEqualized Odds: To address some of the limitations with demographic parity, [22] propose to instead ensure that a classifier has equal error rates for each protected group. This fairness definition, called equalized odds, can loosely be understood as a notion of \u201cdemographic parity for error rates instead of outcomes.\u201d Unlike demographic parity, equalized odds rewards accurate classification, and penalizes systems only performing well on the majority group. However, theoretical work has shown that equalized odds is typically incompatible with correctly calibrated probability estimates [37]. It is also a relatively weak notion of fairness from a civil rights perspective compared to demographic parity, as it does not ensure that outcomes are distributed equitably. Hardt et al. also propose a variant definition called equality of opportunity, which relaxes equalized odds to only apply to a \u201cdeserving\u201d outcome. It is straightforward to extend differential fairness to a definition analogous to equalized odds, although we leave the exploration of this for future work. A more recent algorithm for enforcing equalized odds and equality of opportunity for kernel methods was proposed by [16].\nIndividual Fairness (\u201cFairness Through Awareness\u201d): The individual fairness definition, due to [17], mathematically enforces the principle that similar individuals should\nget similar outcomes under a classification algorithm. An advantage of this approach is that it preserves the privacy of the individuals, which can be important when the user of the classifications (the vendor), e.g. a banking corporation, cannot be trusted to act in a fair manner. However, this is difficult to implement in practice as one must define \u201csimilar\u201d in a fair way. The individual fairness property also does not necessarily generalize beyond training set. In this work, we take inspiration from Dwork et al.\u2019s untrusted vendor scenario, and the use of a privacy-preserving fairness definition to address it.\nCounterfactual Fairness: [31] propose a causal definition of fairness. Under their counterfactual fairness definition, changing protected attributes A, while holding things which are not causally dependent on A constant, will not change the predicted distribution of outcomes. While theoretically appealing, there are difficulties in implementing this in practice. First, it requires an accurate causal model at the fine-grained individual level, while even obtaining a correct populationlevel causal model is generally very difficult. To implement it, we must solve a challenging causal inference problem over unobserved variables, which generally requires approximate inference algorithms. (In the case of differential fairness, we advocate the use of Bayesian models which typically require approximate inference as well, although empirical distributions can be used if sufficient data is available.) Finally, to achieve counterfactual fairness, the predictions (usually) cannot make direct use of any descendant of A in the causal model. This generally precludes using any of the observed features as inputs.\nThreshold Tests: [39] address infra-marginality by modeling risk probabilities for different subsets (i.e. attribute values) within each protected category, and requiring algorithms to threshold these probabilities at the same points when determining outcomes. In contrast, based on intersectionality theory, our proposed differential fairness criterion specifies protected categories whose intersecting subsets should be treated equally, regardless of differences in risk across the subsets. Our definition is appropriate when the differences in risk are due to structural systems of oppression, i.e. the risk probabilities themselves are impacted by an unfair process. We also provide a bias amplification version of our metric, following [43], which is more in line with the infra-marginality perspective.\n2) Privacy Definitions: Differential Privacy: Our work on fairness is inspired by differential privacy, the gold-standard notion of privacy for data-driven algorithms [19]. Essentially, differential privacy is a promise: if an individual contributes their data to a dataset, their resulting utility, due to algorithms applied to that dataset, will not be substantially affected. The privacy guarantee is obtained via the use of randomized algorithms, typically by adding sufficient noise, e.g. from the Laplace distribution, in order to obfuscate the impact of any one data point on the algorithms\u2019 outputs.\nDefinition A.1. M(x) is -differentially private if\nP (M(x) \u2208 S) P (M(x\u2032) \u2208 S) \u2264 e\nfor all outcomes S, and pairs of databases x, x\u2032 differing in a single element.\nSimilarly to differential privacy, our proposed differential fairness definition bounds ratios of probabilities of outcomes resulting from a mechanism. However, there are several important differences. When bounding these ratios, differential fairness considers different values of a set of protected attributes, rather than databases that differ in a single element. It posits a specified set of possible distributions which may generate the data, while differential privacy implicitly assumes that the data are independent [29]. Finally, since differential fairness considers randomness in data as well as in the mechanism, it can be satisfied with a deterministic mechanism, while differential privacy can only be satisfied with a randomized mechanism.\n3) Other Related Work: Fairness and Intersectionality: Of particular relevance to this work, fairness in an intersectional setting has been considered by [8] in a computer vision context, and by [27] and [23], who aim to protect certain subgroups by preventing \u201cfairness gerrymandering.\u201d\nFairness and Uncertainty: Bayesian modeling of fairness has been performed by [39] in the context of stop-and-frisk policing, and by [31], who use Bayesian inference on a causal model. As an alternative to the Bayesian methodology, adversarial methods are another strategy for managing uncertainty in a fairness context, e.g. [6] apply this approach to the setting of ensuring fairness given a limited number of observations in which demographic information is available. [38] study various hypothesis testing methods for the 80% rule in the small data regime.\nFairness and Privacy: The work of [25] also addresses untrusted vendors, focusing on differentially private fair learning algorithms (with respect to protected attributes) which obtain obtain fairness under a different criterion. In contrast, differential fairness ensures that the behavior of the final algorithm, rather than the learning process for the algorithm, preserves the privacy of the individuals\u2019 protected attributes."
    }
  ],
  "title": "An Intersectional Definition of Fairness",
  "year": 2019
}
