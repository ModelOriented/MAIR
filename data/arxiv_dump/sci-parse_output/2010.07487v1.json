{
  "abstractText": "Trust is a central component of the interaction between people and AI, in that \u2019incorrect\u2019 levels of trust may cause misuse, abuse or disuse of the technology. But what, precisely, is the nature of trust in AI? What are the prerequisites and goals of the cognitive mechanism of trust, and how can we cause these prerequisites and goals, or assess whether they are being satisfied in a given interaction? This work aims to answer these questions. We discuss a model of trust inspired by, but not identical to, sociology\u2019s interpersonal trust (i.e., trust between people). This model rests on two key properties of the vulnerability of the user and the ability to anticipate the impact of the AI model\u2019s decisions. We incorporate a formalization of \u2018contractual trust\u2019, such that trust between a user and an AI is trust that some implicit or explicit contract will hold, and a formalization of \u2018trustworthiness\u2019 (which detaches from the notion of trustworthiness in sociology), and with it concepts of \u2019warranted\u2019 and \u2018unwarranted\u2019 trust. We then present the possible causes of warranted trust as intrinsic reasoning and extrinsic behavior, and discuss how to design trustworthy AI, how to evaluate whether trust has manifested, and whether it is warranted. Finally, we elucidate the connection between trust and XAI using our formalization.",
  "authors": [
    {
      "affiliations": [],
      "name": "Alon Jacovi"
    },
    {
      "affiliations": [],
      "name": "Ana Marasovi\u0107"
    },
    {
      "affiliations": [],
      "name": "Tim Miller"
    }
  ],
  "id": "SP:208838816eefb19239c47905ebbb90e7c5afa9ed",
  "references": [
    {
      "authors": [
        "David Alvarez Melis",
        "Tommi Jaakkola"
      ],
      "title": "Towards Robust Interpretability with Self-Explaining Neural Networks",
      "venue": "In Advances in Neural Information Processing Systems 31,",
      "year": 2018
    },
    {
      "authors": [
        "Matthew Arnold",
        "Rachel K.E. Bellamy",
        "Michael Hind",
        "Stephanie Houde",
        "Sameep Mehta",
        "Aleksandra Mojsilovic",
        "Ravi Nair",
        "Karthikeyan Natesan Ramamurthy",
        "Alexandra Olteanu",
        "David Piorkowski",
        "Darrell Reimer",
        "John T. Richards",
        "Jason Tsay",
        "Kush R. Varshney"
      ],
      "title": "FactSheets: Increasing trust in AI services through supplier\u2019s declarations of conformity",
      "venue": "IBM J. Res. Dev",
      "year": 2019
    },
    {
      "authors": [
        "Pepa Atanasova",
        "Jakob Grue Simonsen",
        "Christina Lioma",
        "Isabelle Augenstein"
      ],
      "title": "Generating Fact Checking Explanations",
      "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics,",
      "year": 2020
    },
    {
      "authors": [
        "Jeremy Barnes",
        "Lilja \u00d8vrelid",
        "Erik Velldal"
      ],
      "title": "Sentiment Analysis Is Not Solved! Assessing and Probing Sentiment Classification",
      "venue": "In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP. Association for Computational Linguistics,",
      "year": 2019
    },
    {
      "authors": [
        "Emily M. Bender",
        "Batya Friedman"
      ],
      "title": "Data Statements for Natural Language Processing: Toward Mitigating System",
      "venue": "Bias and Enabling Better Science. Trans. Assoc. Comput. Linguistics",
      "year": 2018
    },
    {
      "authors": [
        "Samuel R. Bowman",
        "Gabor Angeli",
        "Christopher Potts",
        "Christopher D. Manning"
      ],
      "title": "A large annotated corpus for learning natural language inference",
      "venue": "In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics,",
      "year": 2015
    },
    {
      "authors": [
        "Diogo V Carvalho",
        "Eduardo M Pereira",
        "Jaime S Cardoso"
      ],
      "title": "Machine learning interpretability: A survey on methods and metrics",
      "venue": "Electronics 8,",
      "year": 2019
    },
    {
      "authors": [
        "Chaofan Chen",
        "Kangcheng Lin",
        "Cynthia Rudin",
        "Yaron Shaposhnik",
        "SijiaWang",
        "Tong Wang"
      ],
      "title": "An Interpretable Model with Globally Consistent Explanations for Credit Risk",
      "year": 2018
    },
    {
      "authors": [
        "F. Dalvi",
        "Nadir Durrani",
        "Hassan Sajjad",
        "Yonatan Belinkov",
        "A. Bau",
        "James R. Glass"
      ],
      "title": "What Is One Grain of Sand in the Desert? Analyzing Individual Neurons in Deep NLP Models",
      "year": 2019
    },
    {
      "authors": [
        "Arun Das",
        "Paul Rad"
      ],
      "title": "Opportunities and Challenges in Explainable Artificial Intelligence (XAI): A Survey",
      "year": 2020
    },
    {
      "authors": [
        "Jay DeYoung",
        "Sarthak Jain",
        "Nazneen Fatema Rajani",
        "Eric Lehman",
        "Caiming Xiong",
        "Richard Socher",
        "Byron C. Wallace"
      ],
      "title": "ERASER: A Benchmark to Evaluate Rationalized NLP Models",
      "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics,",
      "year": 2020
    },
    {
      "authors": [
        "Jesse Dodge",
        "Suchin Gururangan",
        "Dallas Card",
        "Roy Schwartz",
        "Noah A. Smith"
      ],
      "title": "Show Your Work: Improved Reporting of Experimental Results",
      "venue": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Association",
      "year": 2019
    },
    {
      "authors": [
        "Finale Doshi-Velez",
        "Been Kim"
      ],
      "title": "Towards a rigorous science of interpretable machine learning",
      "venue": "arXiv preprint arXiv:1702.08608",
      "year": 2017
    },
    {
      "authors": [
        "Allyson Ettinger",
        "Ahmed Elgohary",
        "Philip Resnik"
      ],
      "title": "Probing for semantic evidence of composition by means of simple classification tasks. In Proceedings of the 1st Workshop on Evaluating Vector-Space Representations for NLP. Association for Computational Linguistics, Berlin, Germany, 134\u2013139",
      "year": 2016
    },
    {
      "authors": [
        "Matt Gardner",
        "Yoav Artzi",
        "Victoria Basmova",
        "Jonathan Berant",
        "Ben Bogin",
        "Sihao Chen",
        "Pradeep Dasigi",
        "Dheeru Dua",
        "Yanai Elazar",
        "Ananth Gottumukkala",
        "Nitish Gupta",
        "Hanna Hajishirzi",
        "Gabriel Ilharco",
        "Daniel Khashabi",
        "Kevin Lin",
        "Jiangming Liu",
        "Nelson F. Liu",
        "Phoebe Mulcaire",
        "Qiang Ning",
        "Sameer Singh",
        "Noah A. Smith",
        "Sanjay Subramanian",
        "Reut Tsarfaty",
        "Eric Wallace",
        "Ally Zhang",
        "Ben Zhou"
      ],
      "title": "Evaluating NLP Models via Contrast Sets",
      "venue": "CoRR abs/2004.02709 (2020)",
      "year": 2020
    },
    {
      "authors": [
        "Timnit Gebru",
        "Jamie Morgenstern",
        "Briana Vecchione",
        "Jennifer Wortman Vaughan",
        "HannaM.Wallach",
        "Hal Daum\u00e9",
        "Kate Crawford"
      ],
      "title": "Datasheets for Datasets",
      "venue": "In Proceedings of the 5th Workshop on Fairness, Accountability, and Transparency in Machine Learning",
      "year": 2018
    },
    {
      "authors": [
        "Amirata Ghorbani",
        "James Wexler",
        "James Y. Zou",
        "Been Kim"
      ],
      "title": "Towards Automatic Concept-based Explanations",
      "venue": "In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems",
      "year": 2019
    },
    {
      "authors": [
        "Ian J. Goodfellow",
        "Jonathon Shlens",
        "Christian Szegedy"
      ],
      "title": "Explaining and Harnessing Adversarial Examples",
      "venue": "In 3rd International Conference on Learning Representations,",
      "year": 2015
    },
    {
      "authors": [
        "Yash Goyal",
        "Ziyan Wu",
        "Jan Ernst",
        "Dhruv Batra",
        "Devi Parikh",
        "Stefan Lee"
      ],
      "title": "Counterfactual Visual Explanations (Proceedings of Machine",
      "venue": "Learning Research,",
      "year": 2019
    },
    {
      "authors": [
        "Riccardo Guidotti",
        "Anna Monreale",
        "Salvatore Ruggieri",
        "Franco Turini",
        "Fosca Giannotti",
        "Dino Pedreschi"
      ],
      "title": "A Survey of Methods for Explaining Black Box Models",
      "venue": "ACM Comput. Surv",
      "year": 2018
    },
    {
      "authors": [
        "Suchin Gururangan",
        "Swabha Swayamdipta",
        "Omer Levy",
        "Roy Schwartz",
        "Samuel Bowman",
        "Noah A. Smith"
      ],
      "title": "Annotation Artifacts in Natural Language Inference Data. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",
      "venue": "Association for Computational Linguistics,",
      "year": 2018
    },
    {
      "authors": [
        "Peter Hase",
        "Mohit Bansal"
      ],
      "title": "Evaluating Explainable AI: Which Algorithmic Explanations Help Users Predict Model Behavior",
      "venue": "CoRR abs/2005.01831 (2020)",
      "year": 2020
    },
    {
      "authors": [
        "Katherine Hawley"
      ],
      "title": "Trust, distrust and commitment",
      "venue": "Nou\u0302s 48,",
      "year": 2014
    },
    {
      "authors": [
        "Lisa Anne Hendricks",
        "Ronghang Hu",
        "Trevor Darrell",
        "Zeynep Akata"
      ],
      "title": "Grounding Visual Explanations",
      "venue": "In ECCV",
      "year": 2018
    },
    {
      "authors": [
        "Robert RHoffman"
      ],
      "title": "A taxonomy of emergent trusting in the human\u2013machine relationship. Cognitive systems engineering: The future for a changing world",
      "year": 2017
    },
    {
      "authors": [
        "Gert Jan Hofstede"
      ],
      "title": "Intrinsic and Enforceable Trust: A Research Agenda",
      "venue": "European Association of Agricultural Economists,",
      "year": 2006
    },
    {
      "authors": [
        "Alon Jacovi",
        "Yoav Goldberg"
      ],
      "title": "Aligning Faithful Interpretations with their Social Attribution",
      "venue": "CoRR abs/2006.01067",
      "year": 2020
    },
    {
      "authors": [
        "Alon Jacovi",
        "Yoav Goldberg"
      ],
      "title": "Towards Faithfully Interpretable NLP Systems: How Should We Define and Evaluate Faithfulness",
      "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics,",
      "year": 2020
    },
    {
      "authors": [
        "Harmanpreet Kaur",
        "Harsha Nori",
        "Samuel Jenkins",
        "Rich Caruana",
        "Hanna M. Wallach",
        "Jennifer Wortman Vaughan"
      ],
      "title": "Interpreting Interpretability: Understanding Data Scientists",
      "venue": "Use of Interpretability Tools for Machine Learning",
      "year": 2019
    },
    {
      "authors": [
        "Divyansh Kaushik",
        "Eduard H. Hovy",
        "Zachary Chase Lipton"
      ],
      "title": "Learning The Difference That Makes A Difference With Counterfactually-Augmented Data",
      "venue": "In 8th International Conference on Learning Representations,",
      "year": 2020
    },
    {
      "authors": [
        "Daniel Khashabi",
        "Snigdha Chaturvedi",
        "Michael Roth",
        "Shyam Upadhyay",
        "Dan Roth"
      ],
      "title": "Looking Beyond the Surface: A Challenge Set for Reading Comprehension over Multiple Sentences",
      "year": 2018
    },
    {
      "authors": [
        "Been Kim",
        "Martin Wattenberg",
        "Justin Gilmer",
        "Carrie Cai",
        "James Wexler",
        "Fernanda Viegas",
        "Rory Sayres"
      ],
      "title": "Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)",
      "year": 2017
    },
    {
      "authors": [
        "Svetlana Kiritchenko",
        "Saif Mohammad"
      ],
      "title": "Examining Gender and Race Bias in Two Hundred Sentiment Analysis Systems",
      "venue": "In Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics, *SEM@NAACL-HLT 2018,",
      "year": 2018
    },
    {
      "authors": [
        "J. Kleinberg",
        "S. Mullainathan"
      ],
      "title": "Simplicity Creates Inequity: Implications for Fairness, Stereotypes, and Interpretability",
      "venue": "Proceedings of the 2019 ACM Conference on Economics and Computation",
      "year": 2019
    },
    {
      "authors": [
        "Pang Wei Koh",
        "Percy Liang"
      ],
      "title": "Understanding Black-box Predictions via Influence Functions (Proceedings of Machine",
      "venue": "Learning Research,",
      "year": 2017
    },
    {
      "authors": [
        "Hana Kopecka",
        "Jose M Such"
      ],
      "title": "Explainable AI for Cultural Minds. https://sites.google.com/view/dexahai-at-ecai2020/home Workshop on Dialogue, Explanation and Argumentation for Human-Agent Interaction",
      "venue": "DEXAHAI ; Conference",
      "year": 2020
    },
    {
      "authors": [
        "Olga Kovaleva",
        "Alexey Romanov",
        "Anna Rogers",
        "Anna Rumshisky"
      ],
      "title": "Revealing the Dark Secrets of BERT",
      "venue": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Association",
      "year": 2019
    },
    {
      "authors": [
        "Johannes Kunkel",
        "Tim Donkers",
        "Lisa Michael",
        "Catalin-Mihai Barbu",
        "J\u00fcrgen Ziegler"
      ],
      "title": "Let Me Explain: Impact of Personal and Impersonal Explanations on Trust in Recommender Systems",
      "venue": "In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems (Glasgow, Scotland Uk) (CHI \u201919)",
      "year": 2019
    },
    {
      "authors": [
        "David Lazer",
        "Ryan Kennedy",
        "Gary King",
        "Alessandro Vespignani"
      ],
      "title": "The parable of Google Flu: traps in big data analysis",
      "venue": "Science 343,",
      "year": 2014
    },
    {
      "authors": [
        "John D Lee",
        "Katrina A See"
      ],
      "title": "Trust in automation: Designing for appropriate reliance",
      "venue": "Human factors 46,",
      "year": 2004
    },
    {
      "authors": [
        "J David Lewis",
        "Andrew Weigert"
      ],
      "title": "Trust as a social reality",
      "venue": "Social forces 63,",
      "year": 1985
    },
    {
      "authors": [
        "Sarah Lichtenstein",
        "Baruch Fischhoff",
        "Lawrence D Phillips"
      ],
      "title": "Calibration of probabilities: The state of the art",
      "venue": "In Decision making and change in human affairs",
      "year": 1977
    },
    {
      "authors": [
        "Zachary C. Lipton"
      ],
      "title": "The mythos of model interpretability",
      "venue": "Commun. ACM 61,",
      "year": 2018
    },
    {
      "authors": [
        "Yi-Ju Lu",
        "Cheng-Te Li"
      ],
      "title": "GCAN: Graph-aware Co-Attention Networks for Explainable Fake News Detection on Social Media",
      "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics,",
      "year": 2020
    },
    {
      "authors": [
        "Michael A. Madaio",
        "Luke Stark",
        "Jennifer Wortman Vaughan",
        "H. Wallach"
      ],
      "title": "Co-Designing Checklists to Understand Organizational Challenges and Opportunities around Fairness in AI",
      "venue": "Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems",
      "year": 2020
    },
    {
      "authors": [
        "Roger C Mayer",
        "James H Davis",
        "F David Schoorman"
      ],
      "title": "An integrative model of organizational trust",
      "venue": "Academy of management review 20,",
      "year": 1995
    },
    {
      "authors": [
        "Tim Miller"
      ],
      "title": "Contrastive Explanation: A Structural-Model Approach",
      "venue": "CoRR abs/1811.03163",
      "year": 2018
    },
    {
      "authors": [
        "Tim Miller"
      ],
      "title": "Explanation in Artificial Intelligence: Insights from the Social Sciences",
      "venue": "Artificial Intelligence",
      "year": 2019
    },
    {
      "authors": [
        "B. Misztal"
      ],
      "title": "Trust in Modern Societies: The Search for the Bases of Social Order",
      "year": 1996
    },
    {
      "authors": [
        "Margaret Mitchell",
        "SimoneWu",
        "Andrew Zaldivar",
        "Parker Barnes",
        "Lucy Vasserman",
        "Ben Hutchinson",
        "Elena Spitzer",
        "Inioluwa Deborah Raji",
        "Timnit Gebru"
      ],
      "title": "2019. Model Cards for Model Reporting",
      "venue": "In Proceedings of the Conference on Fairness, Accountability, and Transparency,",
      "year": 2019
    },
    {
      "authors": [
        "Raja Parasuraman",
        "Victor Riley"
      ],
      "title": "Humans and Automation: Use, Misuse, Disuse, Abuse",
      "venue": "Human Factors 39,",
      "year": 1997
    },
    {
      "authors": [
        "Joelle Pineau"
      ],
      "title": "The Machine Learning Reproducibility Checklist. https: //www.cs.mcgill.ca/~jpineau/ReproducibilityChecklist.pdf",
      "year": 2020
    },
    {
      "authors": [
        "Shauli Ravfogel",
        "Yanai Elazar",
        "Hila Gonen",
        "Michael Twiton",
        "Yoav Goldberg"
      ],
      "title": "Null It Out: Guarding Protected Attributes by Iterative Nullspace Projection",
      "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics,",
      "year": 2020
    },
    {
      "authors": [
        "Marco Tulio Ribeiro",
        "Tongshuang Wu",
        "Carlos Guestrin",
        "Sameer Singh"
      ],
      "title": "Beyond Accuracy: Behavioral Testing of NLP Models with CheckList",
      "venue": "In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics,",
      "year": 2020
    },
    {
      "authors": [
        "Mireia Ribera",
        "\u00c0gata Lapedriza"
      ],
      "title": "Can we do better explanations? A proposal of user-centered explainable AI",
      "venue": "In IUI Workshops",
      "year": 2019
    },
    {
      "authors": [
        "Philipp Schmidt",
        "Felix Bie\u00dfmann"
      ],
      "title": "Quantifying Interpretability and Trust in Machine Learning Systems",
      "venue": "CoRR abs/1901.08558 (2019)",
      "year": 2019
    },
    {
      "authors": [
        "K. Simonyan",
        "A. Vedaldi",
        "Andrew Zisserman"
      ],
      "title": "Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps",
      "venue": "In 2nd International Conference on Learning Representations ICLR, Workshop Track Proceedings",
      "year": 2014
    },
    {
      "authors": [
        "Alison Smith-Renner",
        "Ron Fan",
        "Melissa Birchfield",
        "Tongshuang Wu",
        "Jordan Boyd- Graber",
        "Daniel S. Weld",
        "Leah Findlater"
      ],
      "title": "No Explainability without Accountability: An Empirical Study of Explanations and Feedback in Interactive ML",
      "venue": "In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems (Honolulu, HI,",
      "year": 2020
    },
    {
      "authors": [
        "Robert C. Solomon"
      ],
      "title": "Creating Trust",
      "venue": "Business Ethics Quarterly 8,",
      "year": 1998
    },
    {
      "authors": [
        "Gabriel Stanovsky",
        "Noah A. Smith",
        "Luke Zettlemoyer"
      ],
      "title": "Evaluating Gender Bias in Machine Translation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, Florence, Italy, 1679\u20131684",
      "year": 2019
    },
    {
      "authors": [
        "Tony Sun",
        "Andrew Gaut",
        "Shirlyn Tang",
        "Yuxin Huang",
        "Mai ElSherief",
        "Jieyu Zhao",
        "Diba Mirza",
        "Elizabeth Belding",
        "Kai-Wei Chang",
        "William Yang Wang"
      ],
      "title": "Mitigating Gender Bias in Natural Language Processing: Literature Review. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, Florence, Italy, 1630\u20131640",
      "year": 2019
    },
    {
      "authors": [
        "Jonathan Tallant"
      ],
      "title": "Commitment in Cases of Trust and Distrust",
      "venue": "Thought: A Journal of Philosophy 6,",
      "year": 2017
    },
    {
      "authors": [
        "Jonathan Tallant",
        "Donatella Donati"
      ],
      "title": "Trust: from the Philosophical to the Commercial",
      "venue": "Philosophy of Management 19,",
      "year": 2020
    },
    {
      "authors": [
        "Alon Talmor",
        "Yanai Elazar",
        "Yoav Goldberg",
        "Jonathan Berant"
      ],
      "title": "oLMpics - On what Language Model Pre-training Captures",
      "venue": "CoRR abs/1912.13283 (2019)",
      "year": 2019
    },
    {
      "authors": [
        "Alon Talmor",
        "Jonathan Herzig",
        "Nicholas Lourie",
        "Jonathan Berant"
      ],
      "title": "CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge",
      "year": 2019
    },
    {
      "authors": [
        "Erico Tjoa",
        "Cuntai Guan"
      ],
      "title": "A Survey on Explainable Artificial Intelligence (XAI): Towards Medical XAI",
      "venue": "CoRR abs/1907.07374 (2019)",
      "year": 2019
    },
    {
      "authors": [
        "Ehsan Toreini",
        "Mhairi Aitken",
        "Kovila Coopamootoo",
        "Karen Elliott",
        "Carlos Gonzalez Zelaya",
        "Aad van Moorsel"
      ],
      "title": "The Relationship between Trust in AI and Trustworthy Machine Learning Technologies. In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency (Barcelona, Spain) (FAT* \u201920)",
      "venue": "Association for Computing Machinery,",
      "year": 2020
    },
    {
      "authors": [
        "Suresh Venkatasubramanian"
      ],
      "title": "Algorithmic Fairness: Measures, Methods and Representations. In Proceedings of the 38th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems (Amsterdam, Netherlands) (PODS \u201919)",
      "venue": "Association for Computing Machinery,",
      "year": 2019
    },
    {
      "authors": [
        "Sarah Myers West",
        "Meredith Whittaker",
        "Kate Crawford"
      ],
      "title": "Discriminating systems: Gender, race and power in AI",
      "year": 2019
    },
    {
      "authors": [
        "Stephen Wright"
      ],
      "title": "Trust and Trustworthiness",
      "venue": "Philosophia 38,",
      "year": 2010
    },
    {
      "authors": [
        "Feiyu Xu",
        "Hans Uszkoreit",
        "Yangzhou Du",
        "Wei Fan",
        "Dongyan Zhao",
        "Jun Zhu"
      ],
      "title": "Explainable AI: A Brief Survey on History, Research Areas, Approaches and Challenges",
      "year": 2019
    }
  ],
  "sections": [
    {
      "text": "CCS CONCEPTS \u2022 Human-centered computing \u2192 HCI theory, concepts and models; \u2022 Applied computing \u2192 Sociology; Psychology; \u2022 Social and professional topics \u2192 Computing / technology policy; \u2022 Computing methodologies \u2192 Artificial intelligence; Machine learning.\nKEYWORDS artificial intelligence, trust, trustworthiness, machine learning, risk, vulnerability, sociology, explainability, interpretability"
    },
    {
      "heading": "1 INTRODUCTION",
      "text": "With the rise of opaque and poorly-understood machine learning models in the field of AI, trust is often cited as a key desirable property of the interaction between any user and the AI [8, 11, 71, 76]. The recent rapid growth in the field of explainable AI (XAI) is also, in part, motivated by the need to maintain trust between the human user and the AI [31, 35, 46, 52, 59, 72]. By designing AI that users can and will trust to interact with, the AI can be safely implemented in society.\nHowever, literature seldom discusses specific models of trust between humans and AI. What, precisely, are the prerequisites for trust between a user and AI? For what goals does the cognitive mechanism of trust exist? How can we design AI which facilitates these prerequisites and goals? And how can we assess whether the prerequisites exist, and whether the purpose behind the trust has been achieved?\nIn this work, we are interested in formalizing the \u2018trust\u2019 transaction between the user and AI, and using this formalization to further our understanding of the requirements behind AI which can be integrated in society. We consider \u2018artificial intelligence\u2019 to be any automation which is attributed with intent by the user [social attribution, 52], i.e., anthropomorphized with a human-like reasoning process. For our purpose, we consider the user to be an individual person, rather than an organization, though aspects of the work are applicable to the latter as well.\nThere are many vague and unclear aspects of trust which are difficult to formalize with the tools available to us in literature on AI and Human-Computer Interaction (HCI). For this reason, we first discuss how interpersonal trust is defined in sociology, and derive a basic, yet functional, definition of trust between a human and an AI, based on the prerequisites and goals behind the trustor developing trust in the AI (Section 2). Specifically, the trustor must be vulnerable to the AI\u2019s actions, and the trustor\u2019s goal in developing trust is to anticipate the impact of the AI model\u2019s decisions.\nHowever, the above definition is incomplete: though the goal is anticipating \u2018intended\u2019 behavior, what can we say about when and whether this goal is achieved? We develop the definition further by answering two questions: (1) what is the AI being trusted with (i.e., what is \u2018intended\u2019)? and (2) what differentiates trust which achieves this goal, and trust which does not? Section 3 answers (1) via a notion of contractual trust, and Section 4 answers (2) via notions\nar X\niv :2\n01 0.\n07 48\n7v 1\n[ cs\n.A I]\n1 5\nO ct\n2 02\nof warranted and unwarranted trust. In Section 5 we complete the definition of Human-AI trust with a formal summary of the above.\nWith these definitions, we are now equipped to discuss the causes of trust in the AI (specifically, warranted trust in a particular contract), and how we should pursue the development of AI that will be trusted. In Section 6, we answer the question: what are the mechanisms by which an AI model gains the trust of a person? Namely, we define and formalize notions of intrinsic trust, which is based on the AI\u2019s observable reasoning process, and extrinsic trust which is based on the AI\u2019s external behavior.\nBoth intrinsic and extrinsic trust are deeply related to XAI. As mentioned, XAI literature frequently notes trust as a principal motivation in the development of explanations and interpretations in AI, but seldom elucidates the precise connection between the methods and the goal. In Section 7, we unravel this \u2018goal\u2019 of XAI \u2014to facilitate trust\u2014by utilizing our formulation thus far.\nIn Section 8 we pivot to the question of evaluating trust, by discussing the evaluation of the vulnerability in the interaction and the evaluation of the ability to anticipate. Finally, in Section 9 we expand on other aspects of interpersonal trust and general humanmachine trust (i.e., automation which is not attributed with intent), how they relate to our notion of Human-AI trust, and possible future extensions of our formalization.\nContributions. Weprovide a formal perspective of Human-AI trust which is rooted in, but nevertheless not the same as, sociology\u2019s perspective of interpersonal trust. We use this formalization to inform notions of the causes behind Human-AI trust, the connection between trust andXAI, and the evaluation of trust.We hope that this work enables a principled approach to developing AI that should, and will, be trusted in practice.\nNote on the organization of the work. The following sections provide an informal description of trust in AI via a narrative, in the interest of accessibility (\u00a72,3,4). We provide formal, concise definitions of our taxonomy after completing the relevant explanations (\u00a75). Additionally, for coherency we bypass some nuanced reasoning behind our choice of formalization, which we make available at the end of the work (\u00a79)."
    },
    {
      "heading": "2 A BASIC DEFINITION OF TRUST",
      "text": "To understand human trust in AI (Human-AI trust), a useful place to start is to examine research in philosophy, psychology, and sociology of how people trust each other (interpersonal trust). In this section, we present a primitive (and incomplete, as we will show) definition of trust that will serve as a basis for the rest of the work.\nDefinition (interpersonal trust). A common basic definition of trust regards it as a directional transaction between two parties: if A believes that B will act in A\u2019s best interest, and accepts vulnerability to B\u2019s actions, then A trusts B [49]. The goal of trust is to \u201cmake social life predictable [by anticipating the impact of behavior], and make it easier to collaborate between people\u201d [53].1\n1This definition of trust is considered overly simplistic in sociology. In Section 9 we discuss aspects of more elaborate formalizations of interpersonal trust, and whether they are relevant to Human-AI trust.\nNoteworthy in this definition, and key to defining Human-AI trust, are the notions of anticipation and vulnerability. In particular, interpersonal trust exists to mitigate uncertainty and risk of collaboration by enabling the trustor\u2019s ability to anticipate the trustee\u2014where \u2018anticipating\u2019 refers to a belief that the trustee will act in the trustor\u2019s best interests. We maintain that Human-AI trust exists for the same purpose, as a sub-case of trust in automation, following Hoffman [28]: trust is an attempt to anticipate the impact of behavior under risk. Based on this, we conclude:\nRisk is a prerequisite to the existence of Human-AI trust. We refer to risk as a disadvantageous or otherwise undesirable event to the trustor (that is a result of interactingwith the trustee), which can possibly\u2014but not certainly\u2014occur [24]. Therefore, \u201cto act in A\u2019s best interest\u201d is to avoid the unfavorable event. Admitting vulnerability means that the trustor perceives both of the following: (1) that the event is undesirable; and (2) that it is possible. Ideally, the existence of trust can only be verified after verifying the existence of risk, i.e., by proving that both conditions hold.\nFor example, AI-produced credit scoring [9] represents a risk to the loan officer: a wrong decision carries a risk (among others) that the applicant defaults in the future. The loss event must be undesirable to the user (the loan officer), who must understand that the decision (credit score) could theoretically be incorrect, and that it is not certainly incorrect, for trust to manifest. Similarly, from the side of the applicants (if they have a choice as to whether to use the AI), the associated risk is to be denied (or to be charged a higher interest rate) on a loan that they deserve, and trust manifests if they believe that the AI will work in their interest (the risk will not occur).\nDistrust manifests in attempt to mitigate the risk. The notion of distrust is important, as it is the mechanism by which the user attempts to avoid the unfavorable outcome. We adapt Tallant\u2019s definition of distrust: A distrusts B if A does not accept vulnerability to B\u2019s actions, because A believes that B may not act in A\u2019s best interest [67]. Importantly, distrust is not equivalent to the absence of trust [50], as the former includes some belief, where the latter is lack of belief\u2014or in other words, distrust is trust in the negative scenario. For the remainder of this paper, we focus our analysis on trust, as the link to distrust is straightforward.\nThe ability to anticipate is a goal, but not necessarily a symptom, of Human-AI trust. The ability or inability of the user to anticipate the behavior of an AI model in the presence of uncertainty or risk, is not indicative of the existence or absence of trust. We illustrate this in \u00a74. We stress that anticipating intended behavior is the user\u2019s goal in developing trust, but not necessarily the AI developer\u2019s goal."
    },
    {
      "heading": "3 CONTRACTUAL TRUST",
      "text": "The above notion of anticipating ability is incomplete. If the goal of trust is to enable the trustor\u2019s ability to anticipate, what does the human trustor anticipate in the AI model\u2019s behavior? And what is the role of the \u2018anticipated behavior\u2019 in the definition of Human-AI trust?"
    },
    {
      "heading": "3.1 Trust in Model Correctness",
      "text": "XAI research commonly refers to the trust that the model is correct [e.g., 22, 46, 60]. What does this mean, exactly?\nTo illustrate this question, consider some binary classification task, and suppose we have a baseline that is completely random by design, and a trained model that achieves the performance of the random baseline (i.e., 50% accuracy in this case).2 Since the trained model performs poorly, a simple conclusion to draw is that we cannot trust this model to be correct. But is this true?\nSuppose now that the trainedmodel with random baseline performance does not behave randomly. Instead, it is biased in a specific manner, and this bias can be revealed with an interpretation or explanation of the model behavior. This explanation reveals to the user that on some types of samples, the model\u2014which maintains random baseline performance\u2014is more likely to be correct than for others.3 As an illustrative example, consider a credit-scoring AI model which is more likely to be correct for certain sub-populations.\nThe performance of the second model did not change, yet we can say that now, with the added explanation, a trustor may have more trust that the model is correct (on specific instances). What has changed? The addition of the explanation enabled the model to be more predictable, such that the user can now better anticipate whether the model\u2019s decision is correct or not for given inputs (e.g. by looking at whether the individual is part of a certain subpopulation), compared to the model without any explanation. Note that this is merely refers to one \u2018instance\u2019 of anticipation; it refers to anticipating a particular attribute of the AI\u2019s decision (correctness), whereas in the previous definition (\u00a72), it refers to general behavior.\nWe arrive at a more nuanced and accurate view of what \u201ctrust in model correctness\u201d refers to: it is in fact not trust in the general performance ability of the model, but that the patterns that distinguish the model\u2019s correct and incorrect cases are available to the user."
    },
    {
      "heading": "3.2 The General Case: Trust in a Contract",
      "text": "The above example of model correctness is merely an instance of what Hawley [26], Tallant [67] refer to as trust with commitment or contractual trust. Contractual trust is a model of trust in which a trustor has a belief that the trustee will stick to a specific contract.4\nIn this work, we contend that all Human-AI trust is contractual,5 and that regardless of what the contract is in a particular interaction, to discuss Human-AI trust, the contract must be explicit.\nGenerally, the contract may refer to any functionality which is deemed useful, even if it is not concrete performance at the end-task that the model was trained for. Therefore, model correctness is only one instance of contractual trust. For example, a model trained to classify medical samples into classes can reveal strong correlations between attributes for one of those classes, giving leads to research on causation between them, even if the model was not useful for the original classification task [42, 46]. 2Assume that the performance evaluation is representative of real usage for now, although this is an important factor that we will discuss in Section 6.2. 3E.g., calibrated probabilities [45], where the classification probabilities of a model are calibrated with some measure of its uncertainty, can produce this effect. 4To our knowledge Hawley [26] is the first to formalize trust as \u201ctrust with commitment [= contract].\u201d Tallant [67] expands on their work with terminology of contractual trust. 5Although they do not refer to contractual trust in their work, Hoffman [28] provide support to formalize trust in automation (beyond AI) as multi-dimensional (which we interpret as multi-contractual), rather than a binary variable or sliding scale.\nContracts and contexts. The idea of context is important in trust: people can trust something in one context but not another [28]. For example, amodel trained to classifymedical samples into classes can perform strongly for samples that are similar to those in its training set, but poorly on those where some features were infrequent, even though the \u2018contract\u2019 appears the same. Therefore, contractual trust can be stated as being conditioned on context. For readability in the rest of this paper, we omit context from the discussion, but implicitly, we consider the contract to be conditioned on, and thus include, the context of the interaction.\nWhat are useful contracts? The European Commission has outlined detailed guidelines onwhat should be required fromAImodels for them to be trustworthy (see Table 1, col. 1\u20132).6 Each of these requirements can be used to specify a useful contract.\nAnother area of research that is relevant for defining contracts is the work that proposes standardized documentations to communicate the performance characteristics of trained AI models. The examples of such documentations are: data statements [6], datasheets for datasets [18], model cards [54], reproducibility checklists [56], fairness checklists [48], and factsheets [2].\nWe illustrate the connection between these documentations and the European requirements in Table 1. For example, if transparency is the stated contract then all of the mentioned documentations could be used to specify information that AI developers need to provide such that they can evaluate and increase users\u2019 trust in transparency of an AI system.\nExplanation and analysis types depend on the contract. We argue that \u201cbroad trust\u201d is built on many contracts, each involving many factors and requiring different evaluation methods. For example, the models\u2019 efficiency in terms of the number of individual neurons responsible for a prediction is relevant for sustainability, but likely not for, e.g., ensuring accessibility.\nWe have previously illustrated that the addition of explanation of the model\u2019s behavior can increase users\u2019 trust based on one contract (\u00a73.1). Just as different evaluation methods are needed for different types of contractual trust, so are different types of explanations. In Table 1, we outline different established types of explanatory methods and analyses that could be suitable for increasing different types of contractual trust derived from the European requirements.\nConclusions. The formalization of contracts allows us to clarify the goal of anticipation in Human-AI trust: contracts specify the behavior to be anticipated, and to trust the AI is to believe that a set of contracts will be upheld.\nSpecific contracts have been outlined and explored in the past when discussing the integration of AI models in society. We advocate for adoption of the taxonomy of contracts in Human-AI trust, for three reasons: (1) it has, though recent, precedence in sociology; (2) it opens a general view of trust as a multi-dimensional transaction, for which all relevant dimensions should be explored before integration in society; and importantly, (3) the term implies an obligation by the AI developer to carry out a prior agreement.\n6The guidelines are available at https://ec.europa.eu/digital-single-market/en/news/ ethics-guidelines-trustworthy-ai."
    },
    {
      "heading": "4 TRUSTWORTHY AI",
      "text": "Trust, as mentioned, aims to enable the ability to anticipate intended behavior through the belief that a contract will be upheld. Further, as mentioned in Section 2, the ability to anticipate does not necessarily manifest with the existence of trust; it is possible for a user to trust a model despite their inability to anticipate its behavior. In other words, the belief exists, but may or may not be followed by the desired behavior. What differentiates trust that \u2018succeeds\u2019 at this goal from trust that does not?\nLet us separate the two cases from the perspective of AI: given that the user trusts the AI model, anticipation depends on whether the model is able to carry out its contract. This perspective distinguishes \u201ctrust\u201d (an attitude of the trustor) from being \u201ctrustworthy\u201d (a property of the trustee) [64, 75], and we say that an AI model is trustworthy to some contract if it is capable of maintaining this contract.\nTrust and trustworthiness are entirely disentangled: pursuing one does not entail pursuing the other, and trustworthiness is not a prerequisite for trust, in that trust can exist in a model which\nis not trustworthy, and a trustworthy model does not necessarily gain trust. We say that the trust is warranted if it is the result of trustworthiness, and otherwise it is unwarranted [50]. Warranted trust is sometimes referred to as trust which is calibrated with trustworthiness [43]. In other words, trust is the cognitive mechanism to give the \u2018illusion\u2019 of anticipating intended behavior, which becomes reality when the trust is warranted, and the trustor feels \u201cbetrayed\u201d when the illusion is broken.\nFor example, consider a user interacting with an AI model via some visual interface (GUI), and the user trusts the AI to make a correct prediction on some task. There is a correlative, but not causal, relationship between high-quality GUI and trustworthy AI models due to a shared variable of high budget in the development of the AI. If the cause of the user\u2019s trust is the model GUI, then manipulation of the model\u2019s ability to make good predictions will not affect this trust, and thus it is unwarranted. If the cause of the trust is the model\u2019s performance ability (due to its higher budget), then theoretically manipulating this performance level will affect the level of trust. This example is illustrated in Figure 1.\nFormally, we define warranted Human-AI trust via a causal (interventionist) relationship with trustworthiness: incurred Human-AI trust is warranted if the trustworthiness of the model can be theoretically manipulated to affect the incurred trust. Note that by this definition, it is possible for a trustworthy model to incur unwarranted Human-AI trust\u2014in this case, the trust will not be betrayed, even though it is unwarranted.\nWhen XAI literature refers to trust, we assume that it is referring to trust which is warranted.7 Therefore, we contend that when pursuingHuman-AI trust, unwarranted trust should be explicitly evaluated against, and avoided or otherwise minimized. See [55] for analysis on the dangers of human usage (or avoidance) of automation when trust and trustworthiness are misaligned, on axes of disuse, misuse and abuse. Specifically, trust exceeding trustworthiness leads to misuse, while trustworthiness exceeding trust leads to disuse.\nFinally, the notion of warranted distrust is similar to that of warranted trust, and easily derived from it: we say that the distrust is warranted if it is sourced in the non-trustworthiness of the AI, i.e., the lack of capability of the AI to maintain the contract, and otherwise, it is unwarranted. It stands to ethics that if the AI is incapable of maintaining some relevant contract, it is a desired outcome (desired by the AI developer) that the user develop warranted distrust in that contract, which will be beneficial in applying the AI to some scenario in spite of its flaws.\n7Not all researchers and organizations may be interested in the distinction between warranted and unwarranted trust, unless required by regulation. We contend that this is an ethical issue. In this work, we assume that it is a core motivation of the field to remain ethical and not implement or prioritize unwarranted trust."
    },
    {
      "heading": "5 DEFINING HUMAN-AI TRUST",
      "text": "This section serves as a formal definition of the taxonomy thus far. Note that we consider AI as an automation which is attributed with human-like intelligence by the human interacting with it.8\nTrustworthy AI. An AI is trustworthy to contract C if it is capable of maintaining the contract.\nHuman-AI trust. If H (human) perceives that M (AI model) is trustworthy to contract C, and accepts vulnerability to M\u2019s actions, then H trusts M contractually to C. The objective of H in trusting M is to anticipate that M will maintain C in the presence of uncertainty, and consequently, trust does not exist if H does not perceive risk.\nPreviously, we note that a user\u2019s \u201canticipation\u201d, defined as the user\u2019s belief that the AI will work \u2018as intended\u2019, is a key aspect of interpersonal trust that we will use to define Human-AI trust. Indeed, \u201canticipation\u201d happens when a user believes that the AI is capable of maintaining the contract, i.e., that the AI is trustworthy to the contract, which is a prerequisite of Human-AI trust.\nWarranted and unwarranted Human-AI trust. H\u2019s trust in M (to C) is warranted if it is caused by trustworthiness in M. This holds if it is theoretically possible to manipulate M\u2019s capability to maintain C, such that H\u2019s trust in M will change. Otherwise, H\u2019s trust in M is unwarranted.\nHuman-AI distrust. If H (human) perceives that M (AI) is not trustworthy to contract C, and therefore does not accept vulnerability to M\u2019s actions, then H distrusts M contractually to C. We say that it is warranted distrust if the distrust is caused by the nontrustworthiness of M."
    },
    {
      "heading": "6 CAUSES OF TRUST",
      "text": "The next natural question to ask is on the cause behind trust in an AI model. As established earlier, we say that trustworthiness is a prerequisite to warranted trust. What causes a model to be trustworthy? And what enables trustworthy models to incur trust?\nWe divide causes of warranted trust into two types: intrinsic and extrinsic.9"
    },
    {
      "heading": "6.1 Intrinsic Trust",
      "text": "A model is more trustworthy when the observable decision process of the model matches user priors on what this process should be. This is equivalent to, for example, a doctor which is consideredmore trustworthy because they are citing various respectable studies to justify their claims.\nExplanation in AI aims to explain the decision process of the AI to the user, such that they can understand why and how the decision was made. However, the process of explaining does not, in itself, enable intrinsic trust. Only when (1) the user successfully comprehends the true reasoning process of the model, and (2) the reasoning process of the model matches the user\u2019s priors of agreeable reasoning, intrinsic trust is gained.\n8We consider AI as an automation attributed with intent, and thus, Human-AI trust is a sub-case of human-machine trust [28, 55]. Trust in automation which is not anthropomorphized can be considered as reliance (\u00a79.1). 9The distinction, and choice of terminology, are ours. Note that this is unrelated to \u2018intrinsic or enforceable\u2019 trust as coined by Hofstede [29].\nFor example, a decision tree is a model whose inner workings can be well-understood by the user (if it is sufficiently small). However, e.g., for a task involving complex expert knowledge, a layman user will not be able to gain intrinsic trust in the model regardless of how \u2018simple\u2019 and interpretable the model is. If the user has no prior on what behavior is trustworthy for the given task, intrinsic trust will not be gained, even if the AI is easy to understand.\nFurther of note is the granularity of interpretability that will facilitate intrinsic trust. The issue of enumerating relevant priors to the user through explanation is not trivial: it is possible to convey information about the reasoning process which is accurate to the model and easy to comprehend, but nevertheless irrelevant to the priors that the user cares to know about. In the same way, it may be possible to derive an explanation of the reasoning process at very coarse granularity and remain descriptive of the relevant priors.\nIn the example of the doctor citing respectable studies, it is not necessary for the user to be able to understand those studies\u2014 merely that they are respectable, and that their claims match the claims of the doctor. As another example, \u201cthe model should not make any decision based on the number of commas in the text\u201d can be such a prior, if the user truly perceives \u201cthe number of commas\u201d as irrelevant to the decision. Generally, we regard any prior as relevant so long as it is useful to upholding the contract in question. For example, the absence of heuristical behavior (such as detecting the number of commas) can be such a prior towards a contract of model correctness.\nIntrinsic trust can be increased in a disciplined manner by formalizing the priors behind trustworthy or non-suspicious behavior, and incorporating behavior which upholds those priors. The documentations outlined in Table 1 (\u00a73) could be useful for that."
    },
    {
      "heading": "6.2 Extrinsic Trust",
      "text": "It is additionally possible for a model to become trustworthy not through explanation, but through behavior: in this case, the source\nof trust is not the decision process of the model, but the evaluation methodology or the evaluation data. This is equivalent to a doctor who is considered more trustworthy because they have a long history of making correct diagnoses; or because they graduated from a prestigious institute that is considered to have rigorous student evaluation. The trust comes from observing symptoms of a trustworthy model, rather than the model\u2019s inner working.\nExtrinsic trust can be seen as the inverse of intrinsic trust: where aligned priors cause a trustworthy model, a trustworthy model causes convincing external symptoms. Since both share a causal relationship with a trustworthy model, they are both utilized by people as indicators of trustworthiness and thus incur warranted trust. This perspective is outlined in Figure 2.\nFor AI models, extrinsic trust is, in essence, trust in the evaluation scheme. To increase extrinsic trust is a matter of justifying that a model can generalize to unseen instances based on expected behavior of the model on other unseen instances.\nExtrinsic trust is gained by two independent requirements: (1) when themodel is trustworthy, and (2)when the evaluation scheme is trustworthy. To show that some evaluation scheme is \u2018trustworthy\u2019 is to justify that the distribution of instances during evaluation matches the distribution of the true unseen instances that will require trust (in a specific contract) by the user in the future\u2014or in other words, guarantee that it is only possible for the AI to pass the evaluation if it is capable of maintaining the contract.10\nThree main methods of evaluation towards extrinsic trust:\n(1) By proxy. Expert (human) opinion on AI reasoning or behavior can enable non-experts to gain extrinsic trust in the AI. Note that the expert does not necessarily gain trust at this point, because the expert may or may not be vulnerable to the AI\u2019s decisions, as the interaction between the expert and the AI is made under different\n10E.g., an evaluation scheme that verifies whether the AI does not discriminate against a sub-population may be different from an evaluation scheme that verifies general performance ability.\nterms than that between the AI and the user. Also of a note is that what exactly constitutes a trustworthy expert for this purpose is a question of interpersonal trust, and not Human-AI trust.\n(2) Post-deployment data. Most simply, the examples that the model sees during production are the most trustworthy representatives of general behavior evaluation, notwithstanding issues of distribution shift over time. Such examples may or may not have gold supervision, and performance may be measured by some weaker signal. Although the distribution of these examples is realistic, it is not controllable, and thus there is little control on the specification of contracts to be evaluated.\n(3) Test sets. Sets of examples, distributed in some specific way, for which gold labels are available. Test sets are generally seen as imperfect approximators of post-deployment data, as their distribution is not representative of true unseen data [12] and thus cannot imply overall generalization.\nHowever, this perspective only takes into account one type of contract: high general post-deployment performance. The distribution of the test examples is controllable, such that they can specialize in specific contracts, such as specific phenomena, robustness and adversarial robustness, fairness and ethics, privacy, and so on. For example, by showing that a model performs equally for two large (and convincingly distributed) collections of people of different races, a user may deem the model more trustworthy to the contract: \u201cI trust you to not discriminate based on race.\u201d Previous work has used specialized test sets to verify particular contracts [5, 36, 69], or outlined methodologies for constructing such evaluation [17, 33, 58, 63].\nHow can we verify whether an evaluation scheme (in particular, test sets and deployment data) is trustworthy? Using data for validation assumes the following: (1) that the data accurately represents the underlying mechanism it comes from (e.g., the label is correct, the distribution is representative for the contract at the time of data collection); (2) that the underlying mechanism is negligibly affected by distribution shift over time; and (3) that the evaluation metrics represent the contract\u2014i.e., that a positive result implies the capability to maintain the contract, and the inverse. The degree to which these assumptions hold affects the validity of the evaluation scheme.\nNotably, point (3) is affected by the \u2018accurate\u2019 formalism of contracts. For example, there are multiple formal measures of fairness such as individual fairness, demographic parity and equalized error rates [73]. However we cannot say that each one of these measures completely encapsulates the social requirement of fairness: each measure formalizes a different aspect of fairness, and there cannot be a solution that satisfies all of them."
    },
    {
      "heading": "7 EXPLAINABILITY AND TRUST",
      "text": "As mentioned, the following is a common claim of XAI literature:\nXAI for Trust (common): A key motivation of XAI and interpretability is to increase the trust of users in the AI.\nHowever, the above claim is difficult to probe without a definition of trust. Following the formalization in this work, we can now elucidate this claim with details on the true nature of this motivation:\nXAI for Trust (extended): A key motivation of XAI and interpretability is to (1) increase the trustworthiness of the AI, (2) increase the trust of the user in a trustworthy AI, or (3) increase the distrust of the user in a non-trustworthy AI, all corresponding to a stated contract, so that the user develops warranted trust or distrust in that contract.\nLet us clarify this claim by unraveling it:"
    },
    {
      "heading": "A key motivation of XAI and interpretability is ...",
      "text": "... to (1) increase the trustworthiness of the AI ... AI is said to be trustworthy to a contract if it is capable of maintaining the contract. Then XAI is a method of creating a capability by revealing the relevant signals in the AI reasoning process (as in the example of the random baseline-like model, \u00a73.1). An AI model that hides these signals would be less trustworthy as they fail to uphold some contracts.\n... (2) increase the trust of the user in a trustworthy AI ... The goal of developing trust, from the user\u2019s perspective, is to enable the ability to anticipate behavior in the presence of risk. Then XAI is a method of allowing the user easier access to the signals that enable this anticipation.\n... or (3) increase the distrust of the user in a non-trustworthy AI ...\nSimilarly, the user\u2019s goal in distrust, in the presence of risk, is to anticipate when desired behavior will not happen. This is the inverse of (2), therefore XAI is a method of enabling distrust.\n... all corresponding to a stated contract ... The above is only relevant with respect to specific contracts that dictate what precisely is anticipated. Therefore, the contract must be explicitly recognized by the XAI methodology, so that it reveals information which is relevant to create or reveal the capability of the AI to maintain the contract.\n... so that the user develops warranted trust or distrust in that contract.\nFor the user to achieve their goal of anticipation, their trust should be warranted, and XAI verifies this by revealing the intrinsic or extrinsic sources (causes) of the trust."
    },
    {
      "heading": "8 EVALUATING TRUST",
      "text": "The last question that remains for us to discuss is on the evaluation of trust. What should evaluation of trust satisfy? Do current methods of evaluating trust fulfill these requirements?"
    },
    {
      "heading": "8.1 Vulnerability in Trust and Trustworthiness",
      "text": "The distinction of evaluation between trust and trustworthiness stems from their distinction on vulnerability: trustworthiness does not require the trustor to accept risk to manifest, and trust does. For instance, the evaluation of the accuracy of AI models may be used to increase external trust in the model for others by asserting trustworthiness or by providing evidence for trustworthiness, but they are not on their own evaluating any notion of trust, as there is no risk involved.\nVulnerability in Trust. By definition, the question of trust does not exist when the user does not assume a risk in trusting the AI. The user must depend on the AI on some level for risk to manifest: for example, when the user is already confident in the answer and does not actually depend on the AI\u2014as is the case for many machine learning datasets today\u2014there is no unfavorable event that directly results from the AI\u2019s actions. Therefore, experiments which simply ask the users whether they trust themodel for some trivial task evaluate neither trust nor trustworthiness.\nWe illustrate this point with the ERASER benchmark [13] as a case-study (Figure 3). ERASER is a benchmark for natural language processing interpretability, consists of multiple datasets. For the purpose of this illustration, we set very simple measures of vulnerability and required effort for humans to solve associated tasks. Foremost, we assume the task domain (what the AI is trained on) is also the use-case (the real interaction setup), e.g., the user must provide the label for a problem instance, and is advised by an AI model trained to do the same. We heuristically measure required human effort using the length of the input (average number of tokens). The longer the text is, the more time-consuming and laborious is the task to people, and consequently people are more dependent on AI. We set the \u201ceffort boundary\u201d around 774 tokens\u2014a few of minutes of reading\u2014and the \u201cvulnerability boundary\u201d right from natural language inference [7], commonsense QA [70], and multi-sentence reading comprehension [34], as these tasks are designed only to test models\u2019 capabilities in capturing particular linguistic phenomena, and, to the best of our knowledge, there is no undesirable event from interacting with them in the base scenario.\nThe question of trust can be considered meaningful for tasks above the \u201ceffort boundary\u201d and right from the \u201cvulnerability boundary\u201d, and notably, only two to three ERASER tasks can be placed in those areas (e.g., evidence inference).11\nWe advocate that only use-cases that can be attributed with both considerable required human effort and vulnerability, are used to target, evaluate and discuss trust. Although in our ERASER example we treat the task domains as use-cases, comprehensive discussion in this area must develop the use-case explicitly. The use-case may not necessarily be an exact replica of the AI\u2019s task domain, and thus, the question of vulnerability would depend entirely on the use-case; additionally, we note that the measure of difficulty is not solely constrained to how time-consuming the task is, as even \u2018easy\u2019 tasks may require trust if the stakes are high.\nVulnerability in Trustworthiness. Whether a model is intrinsically or extrinsically trustworthy is unrelated to the existence of vulnerability in the user. For example, a domain expert can verify the intrinsic trustworthiness of a model by verifying whether the model performs the expected reasoning steps to arrive at its decision, despite the expert not necessarily assuming any vulnerability in doing this inspection, the same way that a class teacher evaluates their students.\n11Given a scientific article describing a randomized controlled trials, evidence inference is the task of inferring whether a given intervention is reported to either significantly increase, significantly decrease, or have no significant effect on a specified outcome, as compared to a comparator of interest."
    },
    {
      "heading": "8.2 Warranted and Unwarranted Trust",
      "text": "Of course, it is impossible to differentiate between warranted and unwarranted trust simply by evaluating whether the user trusts the model. In this area, Kaur et al. [32] show a synthetic experimental setup to evaluate unwarranted trust, and conclude that even data scientists are susceptible to develop unwarranted trust, despite some mathematical understanding of the models. Smith-Renner et al. [62] show similar conclusions on unwarranted trust in a different experimental setup. However, this area is underdeveloped.\nOur discussion of warranted trust in Sections 4 and 5 suggests a possible evaluation based on manipulationist causality\u2014i.e., that if the trust is warranted, the level of trust can be manipulated by manipulating the cause of trustworthiness. This gives rise to the following evaluation protocol:\n(1) Measure the level of trust in an interaction. (2) Manipulate the real trustworthiness of the model (e.g., by\nhandicapping it in some way; by improving its predictions; or even by replacing the model with an oracle). (3) Measure the level of trust after the manipulation. The amount of change due to the manipulation indicates the level of warranted trust."
    },
    {
      "heading": "8.3 Evaluating \u2018Anticipation\u2019",
      "text": "While not an evaluation of trust, there are methods of evaluating the ability of users to successfully anticipate the AI\u2019s behavior, an important aspect of Human-AI trust, via simulatability [15, 25]\u2014the ability of the user to simulate the outcome of the AI on an instance level. Thus, simulatability can serve as one proxy signal to assess whether the goal of trust has been achieved, though we note that it does not concretely verify the existence of trust\u2014on the contrary, it relies on a prior assumption that trust exists. Additionally, since simulatability is performed on an instance level, it does not clarify contracts that are not behaviorally observable on an instance level (e.g., contracts that deal with privacy concerns, or with global properties of the predictions over a large sample set)."
    },
    {
      "heading": "9 DISCUSSION",
      "text": "The basis of our formalization of Human-AI trust is the basic definition of interpersonal trust in sociology (\u00a72). In this section, we first discuss additional aspects of interpersonal trust and humanmachine trust (trust in automation), and their relevance to our definitions (\u00a75). We then present two directions for extension of our formalization inspired by other factors of interpersonal trust."
    },
    {
      "heading": "9.1 Other Elements of Interpersonal Trust and Trust in Automation",
      "text": "Trust vs Reliance. Baier [4] introduces the term reliance for trust in inanimate objects, such as trusting a stool to hold your weight. We may feel betrayed when our trust fails us, but this is not true for reliance because we did not attribute intent to the trustee [68]. Despite the fact that AI models are tools, people anthropomorphize them and assign intent to them [30, 52]. This makes us believe that reliance is not suitable for AI models, and for this reason we aim to define Human-AI trust instead of reliance. This positions Human-AI trust as a distinct sub-case of human-machine trust, or trust in\nautomation, where otherwise the automation may not be attributed with intent (in which case, it is reliance).\nWarranted and JustifiedTrust in Social Science. Our definition of warranted Human-AI trust (\u00a75) is trust which is caused by trustworthiness (to some contract). This definition is only applicable to Human-AI trust, and is not strictly relevant in sociology.\nSociology elects to define trustworthiness and justified trust by the effect, rather than the cause: in interpersonal trust, the trustee is trustworthy, and the trust in them was justified, if the trust was not betrayed. Two natural questions emerge: (1) why does sociology define interpersonal trust in this way? (2) and why do we not adopt this definition for Human-AI trust?\nThe capability of the trustee to maintain the trust, and their intent to do so, are both prerequisites to interpersonal trust, but are not necessarily sufficient. This is due to the complex nature of human interaction, with respect to elements of chance and outside factors (e.g., the difference between innocent mistake and negligence, \u2018chains\u2019 of trust, and so on). This makes the assignment of blame difficult or ill-defined: e.g., if the trustee was fully capable of maintaining their trust, and intended to do so, but contracted a common cold which ultimately prevented them from achieving their goal\u2014it is difficult to define them as trustworthy or otherwise.\nHuman-AI trust does not share these limitations, as (1) the AI (by definition as an automation) does not possess real intent; and (2) the AI\u2019s capabilities are well-defined. As a result, we diverge from sociology definitions on trustworthiness and justified trust, and adopt a stricter causal definition of warranted trust.\nCan Trust Become Warranted? In this work, we discuss warranted trust as a \u2018state\u2019 that trust can become, on a binary. This is equivalent to the notion of trust becoming \u2018calibrated\u2019 as discussed by Lee and See [43]. However, this is not realistic since trust can increase or decrease freely\u2014on a sliding scale. But is the sliding scale, in itself, sufficient to describe trust? More specifically, Hoffman [28] argues that trust cannot be positioned on a binary, or even a sliding scale, as trust is multi-dimensional. In his own words:\nIn my own relation to my word processing software, I am positive that it will perform well in the crafting of simple documents, but I am simultaneously confident it will crash when the document gets long, or when it has multiple high-resolution images. And every time that there is a software upgrade, the trusting of many of the functions becomes tentative and skeptical. [Therefore,] trust is not a single state.\nWe argue that Hoffman\u2019s view is implicitly informed by a notion of contractual trust. In other words, while general trust cannot be described by a single state (or sliding scale) of warranted or unwarranted, trust in a contract can, since it is \u2018calibrated\u2019 by the capability of the model to maintain the contract, where contracts are dimensions."
    },
    {
      "heading": "9.2 Future Extensions of the Formalization",
      "text": "Trust in the AI Developer. Literature in sociology often considers aspects of larger social constructs (beyond a single transaction of two people), specifically relationships and communities and the propagation of trust in them. A common theme in such models\nis the incentives of both parties (e.g., trust in family vs trust in a business partner).\nThis notion is, of course, relevant to Human-AI trust. For example, since there is a close relationship between workplaces with discriminatory practices and discriminatory AI tools [74], it is likely that those who are discriminated against have more incentive to trust AI tools produced by teams that represent them. By recognizing the incentives of the developer, the user may gain trust in the AI.\nAI as an automation does not embody intent. Formally, the intent of the AI developer manifests in the capability of the model to maintain specific contracts, rather than adopting some anthropomorphized notion of intent. Therefore, we make two claims on the nature of this trust.\nTrust in the AImodel based on trust in the AI developer is an instance of interpersonal trust by proxy, and not HumanAI trust. Therefore, studies of the influence of the trustee\u2019s incentives on Human-AI trust, should build on the existing research in sociology that investigates the influence of relationships and communities on interpersonal trust [44]. Consequently, the question of whether this trust is warranted or unwarranted is ill-defined. Our definition of warranted trust\u2014as trust which is caused by trustworthiness\u2014is a definition that does not apply to interpersonal trust (as mentioned in \u00a79.1).\nTo conclude, while trust in the AI developer could possibly influence the user-AI interaction, and should therefore be studied and modeled, it is strictly not part of our model of Human-AI trust. We therefore position this topic as future work towards a more nuanced model of trust in the interaction between people and AI.\nPersonal Attributes of the Trustor. As mentioned, in this work we consider each interaction as a \u2018clean slate\u2019 transaction of trust. Future work in this area may incorporate elements of the personal attributes of the trustor into the model, such as personality, sociocultural background [39], prior existence of trust or distrust, or the restoration of trust which has been betrayed."
    },
    {
      "heading": "10 CONCLUSION",
      "text": "While Trust is a central component in the interaction between a user and the AI, current references to trust in the XAI literature are vague and under-specified. In this work, we provide several working definitions for trust-related terms, and answer the questions of what is necessary to allow trust to occur, and what is necessary for the goal of trust (anticipation of desired contracts) to be achieved. The goal of this formalization is to allow a disciplined method of developing trustworthy AI models that will incur trust in practice. We discuss intrinsic reasoning and extrinsic behavior as causes of warranted trust to be pursued in designing a trustworthy model. This is directly connected to XAI, which can provide the framework to verify whether a model is trustworthy or to create trustworthiness in the model. We further note that the question of trust in this context hinges on a notion of vulnerability, which is not satisfied by many evaluation methods currently used in XAI.\nTakeaways. We collect the various conclusions in this work into guidelines that should inform the design of AI which is both trustworthy and trusted.\n(1) The assessment of risk is necessary prior to the assessment of trust.When deciding whether an AI requires trust, or when evaluating trust, verify the existence of vulnerability of the user in the actions of the AI, by: (1) verifying that the user considers some of the actions of the AI as unfavorable to them; and (2) verifying that the user considers both the favorable and unfavorable outcomes as realistic. (2) AI developers should be explicit about the contracts that their models maintain. AI developers should use the right affordances to make the relevant contracts explicit. This can help to avoid situations where a contract is implicitly assumed by a user, despite not being upheld by the AI (unwarranted trust). (3) Successful anticipation, while the goal of trust, is not indicative of warranted trust. The trust can be unwarranted if it is not sourced in trustworthiness, in which case, the anticipation may depend on a different variable which does not exist in other situations (such as the quality of the AI user interface). Therefore, though simulatability methods are useful and valuable as methods of assessing this property, it is dangerous to rely on them solely. (4) Trust is only ethically desirable if it is warranted. Unwarranted trust is not guaranteed to accomplish its goal, since it is not sourced by trustworthiness. This can cause issues of abuse, disuse or misuse of the AI. While unwarranted trust may be in the interest of some parties, AI research should make efforts to both diagnose and avoid unwarranted trust by, among other things, identifying relevant contracts and assessing trustworthiness. (5) Distrust is not strictly undesirable if it is warranted. Just as trustworthiness and warranted trust must both manifest for the AI to be useful in practice, so too must warranted distrust follow non-trustworthiness for contracts which are relevant to the application. Completely trustworthy AI, for all relevant contracts, may be prohibitively difficult to achieve\u2014in which case warranted distrust is the mechanism that will allow the imperfect AI to be useful. (6) Explanation seems to be uniquely positioned for Human-AI trust as amethod for causing intrinsic trust for general users. Other causes of trust, such as empirical evaluation and authority, are extrinsic. This may help to explain the recent interest in XAI. (7) Methods of evaluating whether trust is warranted are underdeveloped, and require future work. It is important to verify whether the trust of the user in an AI is warranted. However, there is currently little work on achieving this goal, and this questioned is positioned to be central for future research on Human-AI trust.\nWe hope that this work informs AI research in the following ways: (1) by encouraging more accurate discussion of trust in AI, through a more transparent definition of trust which includes the notions of contractual and warranted trust; (2) by encouraging claims on trust and on methods of causing or evaluating trust to be founded on, and distinguish between, the notions of intrinsic and extrinsic trust; (3) by requiring to explicitly recognize and verify risk in user studies that make claims on trust."
    }
  ],
  "title": "Formalizing Trust in Artificial Intelligence:  Prerequisites, Causes and Goals of Human Trust in AI",
  "year": 2020
}
