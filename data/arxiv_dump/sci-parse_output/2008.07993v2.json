{
  "abstractText": "Predictive business process monitoring (PBPM) is a class of techniques designed to predict behaviour, such as next activities, in running traces. PBPM techniques aim to improve process performance by providing predictions to process analysts, supporting them in their decision making. However, the PBPM techniques\u2019 limited predictive quality was considered as the essential obstacle for establishing such techniques in practice. With the use of deep neural networks (DNNs), the techniques\u2019 predictive quality could be improved for tasks like the next activity prediction. While DNNs achieve a promising predictive quality, they still lack comprehensibility due to their hierarchical approach of learning representations. Nevertheless, process analysts need to comprehend the cause of a prediction to identify intervention mechanisms that might affect the decision making to secure process performance. In this paper, we propose XNAP, the first explainable, DNN-based PBPM technique for the next activity prediction. XNAP integrates a layer-wise relevance propagation method from the field of explainable artificial intelligence to make predictions of a long short-term memory DNN explainable by providing relevance values for activities. We show the benefit of our approach through two real-life event logs.",
  "authors": [
    {
      "affiliations": [],
      "name": "Sven Weinzierl"
    },
    {
      "affiliations": [],
      "name": "Sandra Zilker"
    },
    {
      "affiliations": [],
      "name": "Jens Brunk"
    },
    {
      "affiliations": [],
      "name": "Kate Revoredo"
    },
    {
      "affiliations": [],
      "name": "Martin Matzner"
    },
    {
      "affiliations": [],
      "name": "J\u00f6rg Becker"
    }
  ],
  "id": "SP:2016b04f0ad7b6dc83ec68eedcb329abedb97131",
  "references": [
    {
      "authors": [
        "L. Arras",
        "J. Arjona-Medina",
        "M. Widrich",
        "G. Montavon",
        "M. Gillhofer",
        "K.R. M\u00fcller",
        "S. Hochreiter",
        "W. Samek"
      ],
      "title": "Explaining and interpreting LSTMs",
      "venue": "Explainable AI: Interpreting, explaining and visualizing deep learning, pp. 211\u2013238. Springer",
      "year": 2019
    },
    {
      "authors": [
        "L. Arras",
        "G. Montavon",
        "K.R. M\u00fcller",
        "W. Samek"
      ],
      "title": "Explaining recurrent neural network predictions in sentiment analysis",
      "venue": "Proceedings of the 8th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis. pp. 159\u2013168. ACL",
      "year": 2017
    },
    {
      "authors": [
        "S. Bach",
        "A. Binder",
        "G. Montavon",
        "F. Klauschen",
        "K.R. M\u00fcller",
        "W. Samek"
      ],
      "title": "On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation",
      "venue": "PloS one 10(7), e0130140",
      "year": 2015
    },
    {
      "authors": [
        "A. Barredo Arrieta",
        "N. D\u0131\u0301az-Rod\u0155\u0131guez",
        "J. Del Ser",
        "A. Bennetot",
        "S. Tabik",
        "A. Barbado",
        "S. Garcia",
        "S. Gil-Lopez",
        "D. Molina",
        "R. Benjamins",
        "R. Chatila",
        "F. Herrera"
      ],
      "title": "Explainable artificial intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI",
      "venue": "Information Fusion 58, 82\u2013115",
      "year": 2020
    },
    {
      "authors": [
        "D. Breuker",
        "M. Matzner",
        "P. Delfmann",
        "J. Becker"
      ],
      "title": "Comprehensible predictive models for business processes",
      "venue": "MIS Quarterly 40(4), 1009\u20131034",
      "year": 2016
    },
    {
      "authors": [
        "C. Di Francescomarino",
        "C. Ghidini",
        "F. Maggi",
        "F. Milani"
      ],
      "title": "Predictive process monitoring methods: Which one suits me best? In: Proceedings of the 16th International Conference on Business Process Management",
      "venue": "pp. 462\u2013479. Springer",
      "year": 2018
    },
    {
      "authors": [
        "C. Di Francescomarino",
        "C. Ghidini",
        "F. Maggi",
        "G. Petrucci",
        "A. Yeshchenko"
      ],
      "title": "An eye into the future: Leveraging a-priori knowledge in predictive business process monitoring",
      "venue": "Proceedings of the 15th International Conference on Business Process Management. pp. 252\u2013268. Springer",
      "year": 2017
    },
    {
      "authors": [
        "M. Du",
        "N. Liu",
        "X. Hu"
      ],
      "title": "Techniques for interpretable machine learning",
      "venue": "Communications of the ACM 63(1), 68\u201377",
      "year": 2019
    },
    {
      "authors": [
        "J. Evermann",
        "J.R. Rehse",
        "P. Fettke"
      ],
      "title": "Predicting process behaviour using deep learning",
      "venue": "Decision Support Systems 100, 129\u2013140",
      "year": 2017
    },
    {
      "authors": [
        "D. Gunning"
      ],
      "title": "Explainable artificial intelligence (XAI)",
      "venue": "Defense Advanced Research Projects Agency 2",
      "year": 2017
    },
    {
      "authors": [
        "S. Hochreiter",
        "J. Schmidhuber"
      ],
      "title": "Long short-term memory",
      "venue": "Neural Computation 9(8), 1735\u20131780",
      "year": 1997
    },
    {
      "authors": [
        "N.S. Keskar",
        "D. Mudigere",
        "J. Nocedal",
        "M. Smelyanskiy",
        "P.T.P. Tang"
      ],
      "title": "On largebatch training for deep learning: Generalization gap and sharp minima",
      "venue": "Proceedings of the 5th International Conference on Learning Representations. pp. 1\u201316. openreview.net",
      "year": 2017
    },
    {
      "authors": [
        "Y. LeCun",
        "Y. Bengio",
        "G. Hinton"
      ],
      "title": "Deep learning",
      "venue": "Nature 521(7553), 436",
      "year": 2015
    },
    {
      "authors": [
        "F. Maggi",
        "C. Di Francescomarino",
        "M. Dumas",
        "C. Ghidini"
      ],
      "title": "Predictive monitoring of business processes",
      "venue": "Proceedings of the 26th International Conference on Advanced Information Systems Engineering. pp. 457\u2013472. Springer",
      "year": 2014
    },
    {
      "authors": [
        "A. M\u00e1rquez-Chamorro",
        "M. Resinas",
        "A. Ruiz-Cort\u00e1s"
      ],
      "title": "Predictive monitoring of business processes: a survey",
      "venue": "Transactions on Services Computing pp. 1\u201318",
      "year": 2017
    },
    {
      "authors": [
        "N. Mehdiyev",
        "P. Fettke"
      ],
      "title": "Prescriptive process analytics with deep learning and explainable artificial intelligence",
      "venue": "Proceedings of the 28th European Conference on Information Systems. AISeL",
      "year": 2020
    },
    {
      "authors": [
        "I. Nunes",
        "D. Jannach"
      ],
      "title": "A systematic review and taxonomy of explanations in decision support and recommender systems",
      "venue": "User Modeling and User-Adapted Interaction 27(3-5), 393\u2013444",
      "year": 2017
    },
    {
      "authors": [
        "J.R. Rehse",
        "N. Mehdiyev",
        "P. Fettke"
      ],
      "title": "Towards explainable process predictions for industry 4.0 in the DFKI-Smart-Lego-Factory. K\u00fcnstliche Intelligenz",
      "year": 2019
    },
    {
      "authors": [
        "M.T. Ribeiro",
        "S. Singh",
        "C. Guestrin"
      ],
      "title": "Why should I trust you?\u201d Explaining the predictions of any classifier",
      "venue": "Proceedings of the 22nd International Conference on Knowledge Discovery and Data Mining. pp. 1135\u20131144",
      "year": 2016
    },
    {
      "authors": [
        "B. Schwegmann",
        "M. Matzner",
        "C. Janiesch"
      ],
      "title": "precep: Facilitating predictive eventdriven process analytics",
      "venue": "Proceedings of the 8th International Conference on Design Science Research in Information Systems. pp. 448\u2013455. Springer",
      "year": 2013
    },
    {
      "authors": [
        "A. Senderovich",
        "C. Di Francescomarino",
        "C. Ghidini",
        "K. Jorbina",
        "F.M. Maggi"
      ],
      "title": "Intra and inter-case features in predictive process monitoring: A tale of two dimensions",
      "venue": "Proceedings of the 15th International Conference on Business Process Management. pp. 306\u2013323. Springer",
      "year": 2017
    },
    {
      "authors": [
        "R. Sindhgatta",
        "C. Ouyang",
        "C. Moreira",
        "Y. Liao"
      ],
      "title": "Interpreting predictive process monitoring benchmarks",
      "venue": "arXiv:1912.10558",
      "year": 2019
    },
    {
      "authors": [
        "F. Taymouri",
        "M. La Rosa",
        "S. Erfani",
        "Z.D. Bozorgi",
        "I. Verenich"
      ],
      "title": "Predictive business process monitoring via generative adversarial nets: The case of next event prediction",
      "venue": "arXiv:2003.11268",
      "year": 2020
    },
    {
      "authors": [
        "I. Verenich",
        "M. Dumas",
        "M. La Rosa",
        "H. Nguyen"
      ],
      "title": "Predicting process performance: A white-box approach based on process models",
      "venue": "Journal of Software: Evolution and Process 31(6), e2170",
      "year": 2019
    },
    {
      "authors": [
        "S. Weinzierl",
        "S. Zilker",
        "J. Brunk",
        "K. Revoredo",
        "A. Nguyen",
        "M. Matzner",
        "J. Becker",
        "B. Eskofier"
      ],
      "title": "An empirical comparison of deep-neural-network architectures for next activity prediction using context-enriched process event logs",
      "venue": "arXiv:2005.01194",
      "year": 2020
    },
    {
      "authors": [
        "S. Weinzierl",
        "K.C. Revoredo",
        "M. Matzner"
      ],
      "title": "Predictive business process monitoring with context information from documents",
      "venue": "Proceedings of the 27th European Conference on Information Systems. pp. 1\u201310. AISeL",
      "year": 2019
    }
  ],
  "sections": [
    {
      "text": "Keywords: Predictive business process monitoring, explainable artificial intelligence, layer-wise relevance propagation, deep neural networks."
    },
    {
      "heading": "1 Introduction",
      "text": "Predictive business process monitoring (PBPM) [14] emerged in the field of business process management (BPM) to improve the performance of operational business processes [5,20]. PBPM is a class of techniques designed to predict behaviour, such as next activities, in running traces. PBPM techniques aim to\nar X\niv :2\n00 8.\n07 99\n3v 2\n[ cs\n.A I]\nimprove process performance by providing predictions to process analysts, supporting them in their decision making. Predictions may reveal inefficiencies, risks and mistakes in traces supporting process analysts on their decisions to mitigate the issues [7]. Typically, PBPM techniques use predictive models, that are extracted from historical event log data. Most of the current techniques apply \u201ctraditional\u201d machine-learning (ML) algorithms to learn models, which produce predictions with a higher predictive quality [6]. The PBPM techniques\u2019 limited predictive quality was considered as the essential obstacle for establishing such techniques in practice [26]. Therefore, a plethora of works has proposed approaches to further increase predictive quality [22]. By using deep neural networks (DNNs), the techniques\u2019 predictive quality was improved for tasks like the next activity prediction [9].\nIn practice, a process analyst\u2019s choice to use a PBPM technique does not only depend on a PBPM technique\u2019s predictive quality. Ma\u0301rquez-Chamorro et al. [15] state that the explainability of a PBPM technique\u2019s predictions is also an important factor for using such a technique in practice. By providing an explanation of a prediction, the process analyst\u2019s confidence in a PBPM technique improves and the process analyst may adopt the PBPM technique [17]. However, DNNs learn multiple representations to find the intricate structure in data, and therefore the cause of a prediction is difficult to retrieve [13]. Due to the lack of explainability, a process analysts cannot identify intervention mechanisms that might affect the decision making to secure the process performance. To address this issue, explainable artificial intelligence (XAI) has developed as a sub-field of artificial intelligence. XAI is a class of ML techniques that aims to enable humans to understand, trust and manage the advanced artificial \u201cdecision-supporters\u201d by producing more explainable models, while maintaining a high level of predictive quality [10]. For instance, in a loan application process, the prediction of the next activity \u201cDecline application\u201d (cf. (a) in Fig. 1) produced by a model trained with a DNN can be insufficient for a process analyst to decide if this is a normal behaviour or some intervention is required to avoid an unnecessary refusal of the application. In contrast, the prediction with explanation (cf. (b) in Fig. 1) informs the process analyst that some important details are missing for approving the application because the activity \u201cAdd details\u201d has a high relevance on the prediction of the next activity \u201cDecline application\u201d.\nIn this paper, we propose the explainable PBPM technique XNAP. XNAP integrates a layer-wise relevance propagation (LRP) method from XAI to make next activity predictions of a long short-term memory (LSTM) DNN explainable\nby providing relevance values for each activity in the course of a running trace. To the best of the authors\u2019 knowledge, this work proposes the first approach to make LSTM-based next activity predictions explainable.\nThe paper is structured as follows. Sec. 2 introduces the required background. In Sec. 3, we present related work on explainable PBPM and reveal the research gap. Sec. 4 introduces the design of XNAP. In Sec. 5, the benefits of XNAP are demonstrated based on two real-life event logs. In Sec. 6 we provide a summary and point to future research directions.\n2 Background\n2.1 Preliminaries4\nDefinition 1 (Vector, Matrix, Tensor). A vector x = (x1, x2, . . . , xn) T\nis an array of numbers, in which the ith number is identified by xi. If each number of vector x lies in R and the vector x contains n numbers, then the vector x lies in Rn\u00d71, and the vector x\u2019s dimension is n\u00d71. A matrix M = ( x(1),x(2), . . . ,x(n)\n) is a two-dimensional array of numbers, where M \u2208 Rn\u00d7d. A tensor T is an ndimensional array of numbers. If n = 3, then T is a tensor of the third order with T = ( M(1),M(2), . . . ,M(n) ) , where T \u2208 Rn\u00d7b\u00d7u.\nDefinition 2 (Event, Trace, Event Log). An event is a tuple (c, a, t) where c is the case id, a is the activity (event type) and t is the timestamp. A trace is a non-empty sequence \u03c3 = \u2329 e1, . . . , e|\u03c3| \u232a of events such that \u2200i, j \u2208 {1, . . . , |\u03c3|}\nei.c = ej .c. An event log L is a set { \u03c31, . . . , \u03c3|L| } of traces. A trace can also be considered as a sequence of vectors, in which a vector contains all or a part of the information relating to an event, e.g. an event\u2019s activity. Formally, \u03c3 =\u2329 x(1),x(2), . . . ,x(t) \u232a , where x(i) \u2208 Rn\u00d71 is a vector, and the superscript indicates the time-order upon which the events happened.\nDefinition 3 (Prefix and label). Given a trace \u03c3 = \u2329 e1, . . . , ek, . . . , e|\u03c3| \u232a , a prefix of length k, that is a non-empty sequence, is defined as f (k) p (\u03c3) = \u3008e1, . . . , ek\u3009, with 0 < k < |\u03c3c| and a label (i.e. next activity) for a prefix of length k is defined as f\n(k) l (\u03c3) = \u3008ek+1\u3009. The above definition also holds for an\ninput trace representing a sequence of vectors. For example, the tuple of all possible prefixes and the tuple of all possible labels for \u03c3 = \u3008x(1),x(2),x(3)\u3009 are \u3008\u3008x(1)\u3009, \u3008x(1),x(2)\u3009\u3009 and \u3008x(2),x(3)\u3009."
    },
    {
      "heading": "2.2 Layer-wise Relevance Propagation for LSTMs",
      "text": "LRP is a technique to explain predictions of DNNs in terms of input variables [3]. For a given input sequence \u03c3 = \u3008x(1),x(2),x(3)\u3009, a trained DNN model Mc and a calculated prediction o = Mc(\u03c3), LRP reverse-propagates the prediction o through the DNN model Mc to assign a relevance value to each input variable\n4Note definitions are inspired by the work of Taymouri et al. [23].\nof \u03c3 [1]. A relevance value indicates to which extent an input variable contributes to the prediction. NoteMc is a DNN model, and c is a target class for which we want to perform LRP. In this paper, Mc is an LSTM model, i.e. a DNN model with an LSTM [11] layer as a hidden layer. The architecture of the \u201cvanilla\u201d LSTM (layer) is common in the PBPM literature for the task of predicting next activities [25]. For instance, an explanation of it can be found in the work of Evermann et al. [9].\nTo calculate the relevance values of the input variables, LRP performs two computational steps. First, it sets the relevance of an output layer neuron corresponding to the target class of interest c to the value o =Mc(\u03c3). It ignores the other output layer neurons and equivalently sets their relevance to zero. Second, it computes a relevance value for each intermediate lower-layer neuron depending on the neural connection type. A DNN\u2019s layer can be described by one or more neural connections. In turns, the LRP procedure can be described layerby-layer for different types of layers included in a DNN. Depending on the type of a neural connection, LRP defines heuristic propagation rules for attributing the relevance to lower-layer neurons given the relevance values of the upper-layer neurons [3].\nIn case of recurrent neural network layers, such as LSTM [11] layers, there are two types of neural connections: many-to-one weighted linear connections, and two-to-one multiplicative interactions [2]. Therefore, we restrict the definition of the LRP procedure to these types of connections. For weighted connections, let zj be an upper-layer neuron. Its value in the forward pass is computed as zj = \u2211 i zi \u00b7wij + bj , while zi are the lower-layer neurons, and wij as well as bj are the connection weights and biases. Given each relevance Rj of the upperlayer neurons zj , LRP computes the relevance Ri of the lower-layer neurons zi. Initially, Rj = Mc(\u03c3) is set. The relevance distribution onto lower-layer neurons comprises two steps. First, by computing relevance messages Ri\u2190j going from upper-layer neurons zj to lower-layer neurons zi. The messages Ri\u2190j are computed as a fraction of the relevance Rj accordingly to the following rule:\nRi\u2190j = zi \u00b7wij + \u00b7sign(zj)+\u03b4\u00b7bjN\nzj + \u00b7 sign(zj) \u00b7Rj . (1)\nN is the total number of lower-layer neurons connected to zj , is a stabiliser (small positive number, e.g. 0.001) and sign(zj) = (1zj\u22650 \u2212 1zj<0) is the sign of zj . Second, by summing up incoming messages for each lower-layer neuron zi to obtain relevance Ri. Ri is computed as \u2211 j Ri\u2190j . If the multiplicative factor \u03b4 is set to 1.0, the total relevance of all neurons in the same layer is conserved. If it is set to 0.0, the total relevance is absorbed by the biases.\nFor two-to-one multiplicative interactions between lower-layer neurons, let zj be an upper-layer neuron. Its value in the forward pass is computed as the multiplication of two lower-layer neuron values zg and zs, i.e. zj = zg \u00b7zs. In such multiplicative interactions, there is always one of two lower-layer neurons that represents a gate with a value range [0, 1] as the output of a sigmoid activation function. This neuron is called gate zg, whereas the remaining one is the source\nzs. Given such a configuration, and denoting by Rj the relevance of the upperlayer neuron zj , the relevance can be redistributed onto lower-layer neurons by: Rg = 0 and Rs = Rj . With this reallocation rule, the gate neuron already decides in the forward pass how much of the information contained in the source neuron should be retained to make the overall classification decision."
    },
    {
      "heading": "3 Related Work on Explainable PBPM",
      "text": "In the past, PBPM research has mainly focus on improving the predictive quality of PBPM approaches to foster the transfer of these approaches into practice. In contrast, the PBPM approaches\u2019 explainability was scarcely discussed although it can be equally important since missing explainability might limit the PBPM approaches\u2019 applicability [15]. In the context of ML, XAI has already been considered in different approaches [4]. However, PBPM research has just recently started to focus on XAI. Researchers differentiate between two types of explainability. First, ante-hoc explainability provides transparency on different levels of the model itself; thus they are referred to as transparent models. This can be the complete model, single components or learning algorithms. Second, post-hoc explainability can be provided in the form of visualisations after the model was trained since they are extracted from the trained model [8].\nConcerning ante-hoc explainability in PBPM, multiple approaches have been proposed for different prediction tasks. For example, Maggi et al. [14] propose a decision-tree-based, Breuker et al. [5] a probabilistic-based, Rehse et al. [18] a rule-based and Senderovic et al. [21] a regression-based approach.\nIn terms of post-hoc explainability, research has focused on model-agnostic approaches. These are techniques that can be added to any model in order to extract information from the prediction procedure [4]. In contrast, model-specific explanations are methods designed for certain models since they examine the internal model structures and parameters [8]. Fig. 2 depicts an overview of approaches for post-hoc explainability in PBPM. Verenich et al. [24] propose a two-step decomposition-based approach. Their goal is to predict the remaining time. First, they predict on an activity-level the remaining time. Next, these predictions are aggregated on a process-instance-level using flow analysis techniques. Sindhgatta et al. [22] provide both global and local explainability for XGBoost, this is for outcome and remaining time predictions. Global explanations are on a prediction-model-level. Therefore, the authors implemented permutation feature importance. On the contrary, Local explanations are on a trace-level, i.e. they describe the predictions regarding a trace. For this, the authors apply LIME [19]. This method perturbs the input, observes how predictions change and based on that, tries to provide explainability. Mehdiyev and Fettke [16] present an approach to make DNN-based process outcome predictions explainable. Thereby, they generate partial dependence plots (PDP) to provide causal explanations. Rehse et al. [18] create global and local explanations for outcome predictions. Based on a DL architecture with LSTM layers, they apply a connection weight approach to calculate the importance of features and therefore provide global\nexplainability. For local explanations, the authors determine the contribution to the prediction outcome via learned rules and individual features.\nIn comparison to those approaches, LRP is not part of the training phase and presumes a learned model. LRP peaks into the model to calculate relevance backwards from the prediction to the input. Thus, through the use of LRP, we contribute by providing the first model-specific post-hoc explanations of LSTMbased next activity predictions."
    },
    {
      "heading": "4 XNAP: Explainable Next Activity Prediction",
      "text": "XNAP is composed of an offline and an online component. In the offline component, a predictive model is learned from a historical event log by applying a Bi-LSTM DNN. In the online component, the learned model is used for producing next activity predictions in running traces. Given the next activity predictions and the learned predictive model, LRP determines relevance values for each activity of running traces."
    },
    {
      "heading": "4.1 Offline Component: Learning a Bi-LSTM model",
      "text": "The offline component receives as input an event log, pre-processes it, and outputs a Bi-LSTM model learned based on the pre-processed event log.\nPre-processing: The offline component\u2019s pre-processing step transforms an event log L into a data tensor X and a label matrix Y (i.e. next activities). The procedure comprises four steps. First, we transform an event log L into a matrix S \u2208 RE\u00d7U . E is the event log\u2019s size |L|, whereas U is the number of an event tuple\u2019s elements. Note that we add an activity to the end of each sequence to predict their end. Second, we onehot-encode the string values of the activity attribute in S because a Bi-LSTM requires a numerical input for calculating forward and backward propagations. After this step, we get the matrix S \u2208 RE\u00d7H , where H is the number of different activity values in the event log L. Third, we create prefixes and next activity labels. Thereby, a tuple of prefixes R is created from S \u2208 RE\u00d7H by applying the function fp, whereas a tuple of labels K is created from S \u2208 RE\u00d7H through the function fl. Lastly, we construct a third-order data tensor X \u2208 R|R|\u00d7M\u00d7H based on the prefix tuple R as well as a\nlabel matrix Y \u2208 R|K|\u00d7H based on the label tuple K, where M is the longest trace in the event log L, i.e. |max\u03c3(L)|. The remaining space for a sequence \u03c3c \u2208 X is padded with zeros, if |\u03c3c| < |max\u03c3(L)|.\nModel learning: XNAP learns a Bi-LSTM modelM that maps the prefixes onto the next activity labels based on the data tensor X and label matrix Y from the previous step. We use the Bi-LSTM architecture, an extension of \u201cvanilla\u201d LSTMs since Bi-LSTMs are forward and backward LSTMs that can exploit control-flow information from two directions of sequences. XNAP\u2019s Bi-LSTM architecture comprises an input layer, a hidden layer, and an output layer. The input layer receives the data tensor and transfers it to the hidden layer. The hidden layer is a Bi-LSTM layer with a dimensionality of 100, i.e. the Bi-LSTM\u2019s cell internal elements have a size of 100. We assign the activation function tanh to the Bi-LSTM\u2019s cell output. To prevent overfitting, we perform a random dropout of 20% of input units along with their connections. The model connects the BiLSTM\u2019s cell output to the neurons of a dense output layer. Its number of neurons corresponds to the number of the next activity classes. For learning weights and biases of the Bi-LSTM architecture, we apply the Nadam optimisation algorithm with a categorical cross-entropy loss and default values for parameters. Note that the loss is calculated based on the Bi-LSTM\u2019s prediction and the next activity ground truth label stored in the label matrix Y. Additionally, we set the batch size to 128. Following Keskar et al. [12], gradients are updated after each 128th trace of the data tensor X. Larger batch sizes tend to sharp minima and impair generalisation. The number of epochs (learning iterations) is set to 100, to ensure convergence of the loss function."
    },
    {
      "heading": "4.2 Online Component: Producing predictions with explanations",
      "text": "The online component receives as input a running trace, performs a pre-processing, creates a next activity prediction and concludes with the creation of a relevance value for each activity of the running trace regarding the prediction. The prediction is obtained by using the learned Bi-LSTM model from the offline component. Given the prediction, LRP determines the activity relevances by backwards passing the learned Bi-LSTM model.\nPre-processing: The online component\u2019s pre-processing step transforms a running trace \u03c3r into a data tensor and a label matrix, as already described in the offline component\u2019s pre-processing step. Note that we terminate the online phase if |\u03c3r| is \u2264 1 since, for such traces, there is insufficient data to base prediction and relevance creation upon. Further, we assume that we have already observed all possible activities as well as the longest trace in the offline component. Thus, matrix S and tensor X lay in R1\u00d7H and R1\u00d7M\u00d7H . In the offline component, next activity labels are not known and based on the data tensor Xr for a running trace \u03c3r a next activity is predicted.\nPrediction creation: Given the data tensor Xr from the previous step, the trained Bi-LSTM model M from the offline component returns a probability distribution p1\u00d7H , containing the probability values of all activities. We retrieve the prediction p from p through argmax(p[j]), with 1 \u2264 j \u2264 H.\nRelevance creation: Lastly, we provide explainability of the prediction p by applying LRP. For a next activity prediction p, LRP determines a relevance value for each activity in the course of a running trace \u03c3r towards it by decomposing the prediction, from the output layer to the input layer, backwards through the model. Note the prediction p was created in the previous step based on all activities of the running trace \u03c3r. In doing that, we apply the LRP approach proposed by Arras et al. [2] that is designed for LSTMs. As mentioned in Sec. 2, a layer of a DNN can be described by one or more neural connections. Depending on the layer\u2019s type, LRP defines rules for attributing the relevance to lower-layer neurons given the relevance values of the upper-layer neurons. After backwards passing the model by considering conversation rules of different layers, LRP returns a relevance value for each onehot-encoded input activity of the data tensor X. Finally, to visualise the relevance values, e.g. by a heatmap, positive relevance values are rescaled to the range [0.5, 1.0] and negative ones to the range [0.0, 0.5]."
    },
    {
      "heading": "5 Results",
      "text": ""
    },
    {
      "heading": "5.1 Event logs",
      "text": "We demonstrate the benefit of XNAP with two real-life event logs that are detailed in Table 1.\nFirst, we use the helpdesk event log5. It contains data of a ticketing management process form a software company. Second, we make use of the bpi2019 event log6. It was provided by a coatings and paint company and depicts an order handling process. Here, we only consider sequences of max. 250 events and extract a 10%-sample of the remaining sequences to lower computation effort."
    },
    {
      "heading": "5.2 Experimental Setup",
      "text": "LRP is a model-specific method that requires a trained model for calculating activity relevances to explain predictions. Therefore, we report the predictive quality of the trained models, and then demonstrate the activity relevances.\nPredictive quality: To improve model generalisation, we randomly shuffle the traces of each event log. For that, we perform a process-instance-based sampling to consider process-instance-affiliation of event log entries. This is important since LSTMs map sequences depending on the temporal order of their\n5https://data.mendeley.com/datasets/39bp3vv62t/1. 6https://data.4tu.nl/repository/uuid:a7ce5c55-03a7-4583-b855-98b86e1a2b07.\nelements. Afterwards, for each event log, we perform a ten-fold cross-validation. Thereby, in every iteration, an event log\u2019s traces are split alternately into a 90%-training and 10%-test set. Additionally, we use 10% of the training set as a validation set. While we train the models with the remaining training set, we use the validation set to avoid overfitting by applying early stopping after ten epochs. Consequently, the model with the lowest validation loss is selected for testing. To measure predictive quality, we calculate the average weighted Accuracy (overall correctness of a model) and average weighted F1-Score (harmonic mean of Precision and Recall).\nExplainability: To demonstrate the explainability of XNAP\u2019s LRP, we pick the Bi-LSTM model with the highest F1-Score value and randomly select two traces from all traces of each event log. One of these traces has a size of five; the other one has a size of eight. We use traces of different sizes to investigate our approach\u2019s robustness.\nTechnical details: We conducted all experiments on a workstation with 12 CPU cores, 128 GB RAM and a single GPU NVIDIA Quadro RXT6000. We implemented the experiments in Python 3.7 with the DL library Keras7 2.2.4 and the TensorFlow8 1.14.1 backend. The source code can be found on Github9."
    },
    {
      "heading": "5.3 Predictive Quality",
      "text": "The Bi-LSTM model of XNAP predicts the next most likely activities for the helpdesk event log with an average (Avg) Accuracy and F1-Score of 84% and 79.8% (cf. Table 2). For the bpi2019 event log, the model achieves an Avg Accuracy and F1-Score of 75.5% and 72.7%. For each event log, the standard deviation (SD) of the Accuracy and F1-Score values is between 1.0% and 1.5%."
    },
    {
      "heading": "5.4 Explainability",
      "text": "We show the activity relevance values of XNAP\u2019s LRP on the example of two traces per event log (cf. Fig. 3). The time steps (columns) represent the activities that are used as input. For each trace, we predict the next activity for different prefix lengths (rows). We start with a minimum of three and make one next activity prediction until the maximum length of the trace is reached (five and eight in our examples). The data-given ground truth is listed in the last column. We\n7https://keras.io. 8https://www.tensorflow.org. 9https://github.com/fau-is/xnap.\nuse a heatmap to indicate the relevance of the input activities to the prediction of the same row. For example, in the traces (a) and (b), the activity \u201cResolve\nticket\u201d (C) has a high relevance on predicting the next activity \u201cEnd (E)\u201d. With that, a process analyst knows that the trace will end since the ticket was resolved. Another example is in the traces (c) and (d), where the activity \u201cRecord Invoice Receipt (C)\u201d has a high relevance on predicting the next activity \u201cClear Invoice (D)\u201d. Thus, a process analyst knows that the invoice can be cleared in the next step because the invoice receipt was recorded."
    },
    {
      "heading": "6 Conclusion",
      "text": "Given the fact that DNNs achieve a promising predictive quality at the expense of explainability and based on our identified research gap, we argue that there is a crucial need for making LSTM-based next activity predictions explainable. We introduced XNAP, an explainable PBPM technique, that integrates an LRP method from the field of XAI to make a BI-LSTM\u2019s next activity prediction explainable by providing a relevance value for each activity in the course of a running trace. We demonstrated the benefits of XNAP with two event logs. By analysing the results, we made three main observations. First, LRP is a modelspecific XAI method; thus, the quality of the relevance scores depend strongly on the model\u2019s predictive quality. Second, XNAP performs better for traces with a smaller size and a higher number of different activities. Third, XNAP computes the relevance values of activities in very few seconds. In contrast, model-agnostic approaches, e.g. PDP [16], need more computation time.\nIn future work, we plan to validate our observations with further event logs. Additionally, we will conduct an empirical study to evaluate the usefulness of\nXNAP. We also plan on hosting a workshop with process analysts to better understand how a prediction\u2019s explainability contributes to the adoption of a PBPM system. Moreover, we plan to adapt the propagation rules of XNAP\u2019s LRP also to determine relevance values of context attributes. Another avenue for future research is to compare the explanation capability of a model-specific method like LRP to a model-agnostic method like LIME for, e.g. the DNNbased next activity prediction. Finally, XNAP\u2019s explanations, which are rather simple, might not capture an LSTM model\u2019s complexity. Therefore, future research should investigate new types of explanations that better represent this high complexity."
    },
    {
      "heading": "Acknowledgments",
      "text": "This project is funded by the German Federal Ministry of Education and Research (BMBF) within the framework programme Software Campus under the number 01IS17045. The fourth author received a grand from O\u0308sterreichische Akademie der Wissenschaften."
    }
  ],
  "title": "XNAP: Making LSTM-based Next Activity Predictions Explainable by Using LRP",
  "year": 2020
}
