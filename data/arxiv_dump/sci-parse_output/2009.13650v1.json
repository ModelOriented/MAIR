{
  "abstractText": "Deep learning has produced big advances in artificial intelligence, but trained neural networks often reflect and amplify bias in their training data, and thus produce unfair predictions. We propose a novel measure of individual fairness, called prediction sensitivity, that approximates the extent to which a particular prediction is dependent on a protected attribute. We show how to compute prediction sensitivity using standard automatic differentiation capabilities present in modern deep learning frameworks, and present preliminary empirical results suggesting that prediction sensitivity may be effective for measuring bias in individual predictions.",
  "authors": [
    {
      "affiliations": [],
      "name": "Krystal Maughan"
    },
    {
      "affiliations": [],
      "name": "Joseph P. Near"
    }
  ],
  "id": "SP:7202d048dd7e494096c7189e9d81d7132865cee7",
  "references": [
    {
      "authors": [
        "Philip Adler",
        "Casey Falk",
        "Sorelle A Friedler",
        "Tionney Nix",
        "Gabriel Rybeck",
        "Carlos Scheidegger",
        "Brandon Smith",
        "Suresh Venkatasubramanian"
      ],
      "title": "Auditing blackbox models for indirect influence",
      "venue": "Knowledge and Information Systems,",
      "year": 2018
    },
    {
      "authors": [
        "Alekh Agarwal",
        "Alina Beygelzimer",
        "Miroslav Dud\u00edk",
        "John Langford",
        "Hanna Wallach"
      ],
      "title": "A reductions approach to fair classification",
      "venue": "arXiv preprint arXiv:1803.02453,",
      "year": 2018
    },
    {
      "authors": [
        "Solon Barocas",
        "Moritz Hardt",
        "Arvind Narayanan"
      ],
      "title": "Fairness and Machine Learning",
      "venue": "fairmlbook.org,",
      "year": 2019
    },
    {
      "authors": [
        "At\u0131l\u0131m G\u00fcnes Baydin",
        "Barak A Pearlmutter",
        "Alexey Andreyevich Radul",
        "Jeffrey Mark Siskind"
      ],
      "title": "Automatic differentiation in machine learning: a survey",
      "venue": "The Journal of Machine Learning Research,",
      "year": 2017
    },
    {
      "authors": [
        "Rachel KE Bellamy",
        "Kuntal Dey",
        "Michael Hind",
        "Samuel C Hoffman",
        "Stephanie Houde",
        "Kalapriya Kannan",
        "Pranay Lohia",
        "Jacquelyn Martino",
        "Sameep Mehta",
        "Aleksandra Mojsilovic"
      ],
      "title": "Ai fairness 360: An extensible toolkit for detecting, understanding, and mitigating unwanted algorithmic bias",
      "venue": "arXiv preprint arXiv:1810.01943,",
      "year": 2018
    },
    {
      "authors": [
        "Alex Beutel",
        "Jilin Chen",
        "Zhe Zhao",
        "Ed H Chi"
      ],
      "title": "Data decisions and theoretical implications when adversarially learning fair representations",
      "venue": "arXiv preprint arXiv:1707.00075,",
      "year": 2017
    },
    {
      "authors": [
        "Toon Calders",
        "Faisal Kamiran",
        "Mykola Pechenizkiy"
      ],
      "title": "Building classifiers with independency constraints",
      "venue": "IEEE International Conference on Data Mining Workshops,",
      "year": 2009
    },
    {
      "authors": [
        "Flavio Calmon",
        "Dennis Wei",
        "Bhanukiran Vinzamuri",
        "Karthikeyan Natesan Ramamurthy",
        "Kush R Varshney"
      ],
      "title": "Optimized pre-processing for discrimination prevention",
      "venue": "In Advances in Neural Information Processing Systems,",
      "year": 2017
    },
    {
      "authors": [
        "L Elisa Celis",
        "Lingxiao Huang",
        "Vijay Keswani",
        "Nisheeth K Vishnoi"
      ],
      "title": "Classification with fairness constraints: A meta-algorithm with provable guarantees",
      "venue": "In Proceedings of the Conference on Fairness, Accountability, and Transparency,",
      "year": 2019
    },
    {
      "authors": [
        "L Elisa Celis",
        "Vijay Keswani"
      ],
      "title": "Improved adversarial learning for fair classification",
      "venue": "arXiv preprint arXiv:1901.10443,",
      "year": 2019
    },
    {
      "authors": [
        "Cynthia Dwork",
        "Moritz Hardt",
        "Toniann Pitassi",
        "Omer Reingold",
        "Richard Zemel"
      ],
      "title": "Fairness through awareness",
      "venue": "In Proceedings of the 3rd innovations in theoretical computer science conference,",
      "year": 2012
    },
    {
      "authors": [
        "Cynthia Dwork",
        "Frank McSherry",
        "Kobbi Nissim",
        "Adam Smith"
      ],
      "title": "Calibrating noise to sensitivity in private data analysis",
      "venue": "In Theory of cryptography conference,",
      "year": 2006
    },
    {
      "authors": [
        "Michael Feldman"
      ],
      "title": "Computational fairness: Preventing machine-learned discrimination",
      "year": 2015
    },
    {
      "authors": [
        "Michael Feldman",
        "Sorelle A Friedler",
        "JohnMoeller",
        "Carlos Scheidegger",
        "Suresh Venkatasubramanian"
      ],
      "title": "Certifying and removing disparate impact",
      "venue": "In proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining,",
      "year": 2015
    },
    {
      "authors": [
        "Pratik Gajane",
        "Mykola Pechenizkiy"
      ],
      "title": "On formalizing fairness in prediction with machine learning",
      "venue": "arXiv preprint arXiv:1710.03184,",
      "year": 2017
    },
    {
      "authors": [
        "Amanda Gentzel",
        "Dan Garant",
        "David Jensen"
      ],
      "title": "The case for evaluating causal models using interventional measures and empirical data",
      "venue": "In Advances in Neural Information Processing Systems,",
      "year": 2019
    },
    {
      "authors": [
        "Moritz Hardt",
        "Eric Price",
        "Nati Srebro"
      ],
      "title": "Equality of opportunity in supervised learning",
      "venue": "In Advances in neural information processing systems,",
      "year": 2016
    },
    {
      "authors": [
        "Rebecca Heilweil"
      ],
      "title": "Artificial intelligence will help determine if you get your next job. https://www.vox.com/recode/2019/12/12/20993665/artificial-intelligence-aijob-screen",
      "year": 2099
    },
    {
      "authors": [
        "Miguel A Hern\u00e1n",
        "John Hsu",
        "Brian Healy"
      ],
      "title": "A second chance to get causal inference right: a classification of data science",
      "venue": "tasks. Chance,",
      "year": 2019
    },
    {
      "authors": [
        "Kashmir Hill"
      ],
      "title": "Wrongfully accused by an algorithm. https://www.nytimes.com/ 2020/06/24/technology/facial-recognition-arrest.html",
      "year": 2020
    },
    {
      "authors": [
        "Mark Johanson"
      ],
      "title": "How a $0.04 metro fare price hike sparked massive unrest in chile",
      "year": 2019
    },
    {
      "authors": [
        "Michael Kearns",
        "Seth Neel",
        "Aaron Roth",
        "Zhiwei Steven Wu"
      ],
      "title": "An empirical study of rich subgroup fairness for machine learning",
      "venue": "In Proceedings of the Conference on Fairness, Accountability, and Transparency,",
      "year": 2019
    },
    {
      "authors": [
        "Christos Louizos",
        "Kevin Swersky",
        "Yujia Li",
        "Max Welling",
        "Richard Zemel"
      ],
      "title": "The variational fair autoencoder",
      "venue": "arXiv preprint arXiv:1511.00830,",
      "year": 2015
    },
    {
      "authors": [
        "Kristian Lum",
        "James Johndrow"
      ],
      "title": "A statistical framework for fair predictive algorithms",
      "venue": "arXiv preprint arXiv:1610.08077,",
      "year": 2016
    },
    {
      "authors": [
        "Masoud Mansoury",
        "Himan Abdollahpouri",
        "Mykola Pechenizkiy",
        "Bamshad Mobasher",
        "Robin Burke"
      ],
      "title": "Fairmatch: A graph-based approach for improving aggregate diversity in recommender systems",
      "venue": "arXiv preprint arXiv:2005.01148,",
      "year": 2020
    },
    {
      "authors": [
        "Ninareh Mehrabi",
        "Fred Morstatter",
        "Nripsuta Saxena",
        "Kristina Lerman",
        "Aram Galstyan"
      ],
      "title": "A survey on bias and fairness in machine learning, 2019",
      "year": 2019
    },
    {
      "authors": [
        "Robinson Meyer"
      ],
      "title": "Everything we know about facebook\u2019s secret mood manipulation experiment. https://www.theatlantic.com/technology/archive/ 2014/06/everything-we-know-about-facebooks-secret-mood-manipulationexperiment/373648",
      "year": 2014
    },
    {
      "authors": [
        "Chris Russell",
        "Matt J Kusner",
        "Joshua Loftus",
        "Ricardo Silva"
      ],
      "title": "When worlds collide: integrating different counterfactual assumptions in fairness",
      "venue": "In Advances in Neural Information Processing Systems,",
      "year": 2017
    },
    {
      "authors": [
        "Shreya Shankar",
        "Yoni Halpern",
        "Eric Breck",
        "James Atwood",
        "Jimbo Wilson",
        "D Sculley"
      ],
      "title": "No classification without representation: Assessing geodiversity issues in open data sets for the developing world",
      "venue": "arXiv preprint arXiv:1711.08536,",
      "year": 2017
    },
    {
      "authors": [
        "Sahil Verma",
        "Julia Rubin"
      ],
      "title": "Fairness definitions explained",
      "venue": "IEEE/ACM International Workshop on Software Fairness (FairWare),",
      "year": 2018
    },
    {
      "authors": [
        "Christina Wadsworth",
        "Francesca Vera",
        "Chris Piech"
      ],
      "title": "Achieving fairness through adversarial learning: an application to recidivism prediction",
      "venue": "arXiv preprint arXiv:1807.00199,",
      "year": 2018
    },
    {
      "authors": [
        "Ali Winston",
        "Ingrid Burrington"
      ],
      "title": "A pioneer in predictive policing is starting a troubling new project. https://www.theverge.com/2018/4/26/17285058/ predictive-policing-predpol-pentagon-ai-racial-bias",
      "year": 2018
    },
    {
      "authors": [
        "Blake Woodworth",
        "Suriya Gunasekar",
        "Mesrob I Ohannessian",
        "Nathan Srebro"
      ],
      "title": "Learning non-discriminatory predictors",
      "venue": "arXiv preprint arXiv:1702.06081,",
      "year": 2017
    },
    {
      "authors": [
        "Muhammad Bilal Zafar",
        "Isabel Valera",
        "Manuel Gomez Rodriguez",
        "Krishna P Gummadi"
      ],
      "title": "Fairness beyond disparate treatment & disparate impact: Learning classification without disparate mistreatment",
      "venue": "In Proceedings of the 26th International Conference on World Wide Web,",
      "year": 2017
    },
    {
      "authors": [
        "Muhammad Bilal Zafar",
        "Isabel Valera",
        "Manuel Gomez Rodriguez",
        "Krishna P Gummadi"
      ],
      "title": "Fairness constraints: Mechanisms for fair classification",
      "venue": "arXiv preprint arXiv:1507.05259,",
      "year": 2015
    },
    {
      "authors": [
        "Rich Zemel",
        "Yu Wu",
        "Kevin Swersky",
        "Toni Pitassi",
        "Cynthia Dwork"
      ],
      "title": "Learning fair representations",
      "venue": "In International Conference on Machine Learning,",
      "year": 2013
    },
    {
      "authors": [
        "Brian Hu Zhang",
        "Blake Lemoine",
        "Margaret Mitchell"
      ],
      "title": "Mitigating unwanted biases with adversarial learning",
      "venue": "In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society,",
      "year": 2018
    },
    {
      "authors": [
        "Chiyuan Zhang",
        "Samy Bengio",
        "Moritz Hardt",
        "Benjamin Recht",
        "Oriol Vinyals"
      ],
      "title": "Understanding deep learning requires rethinking generalization",
      "venue": "arXiv preprint arXiv:1611.03530,",
      "year": 2016
    }
  ],
  "sections": [
    {
      "heading": "1 INTRODUCTION",
      "text": "In a recent New York Times article, a facial recognition algorithm incorrectly identified the wrong individual given a blurry image of a shoplifter [23]. The facial recognition system was found to perform noticeably worse when used to identify non-white demographics. A lack of diversity in the data set used to train the system was said to have contributed to the bias in the algorithm. In this particular case, this bias resulted in the wrongful arrest of a man for a crime of which he was innocent. Today, automated Artificial Intelligence systems are pervasive: they are used to automate the processes of hiring candidates [12, 21], firing candidates [26] and in recommending consumer products based on our predicted emotions and those of our peer groups [31]. An error or erroneous prediction can therefore lead to the loss of a job, the loss of credit or economic and legal implications that can perpetuate cycles of vulnerability and poverty. On a larger scale, when we look at the total life-cycle of automated artificial intelligence systems, these automated systems will be adopted in some capacity by countries with less economic and political stability. Erroneous individual and group predictions can therefore lead to larger societal issues such as civic unrest and an increased mistrust of governments and larger institutions by citizens of those nations [24]. Unfortunately, numerous examples suggest that automated systems inherit and even amplify the biases already present in society [36]. The goal of fair AI is to better understand the origins of these biases and their impacts on society, and to develop mitigations and recommendations to improve the situation. Defining Fairness. Fairness is defined as the degree to which judgments can be considered to discriminate against a particular individual or a group [30]. This acknowledges an ideal in which members of all groups are regarded as equal and no one group has dominance or influence over another.\nHowever, Artificial Intelligence tends to reflect and amplify the unfairness we see in society, resulting in automated systems that demonstrate bias in favor of a particular individual or group. Historical events that perpetuated power dynamics (such as colonialism or imperialism) and disrupted the cultural and social economies\nof marginalized people have directly shaped today\u2019s society. We have quantified progress or proof of advocacy for fairness by measuring incremental improvements on these egregious systems of inequity and the devaluation of marginalized groups. Since data is a proxy for this history, unfairness in Artificial Intelligence systems therefore becomes an inevitable automation of these biases.\nMeasuring fairness turns out to be extremely challenging. Numerous formal metrics have been developed to measure fairness in machine learning systems [4, 13, 17, 20, 34, 39], but no single metric completely captures our intuitive notions of fairness. Most metrics measure group fairness: they compare the rate of positive outcomes for privileged and unprivileged groups of individuals. As a result, a model that scores highly on a group fairness metric may still make blatantly unfair predictions for particular individuals as long as outcomes are similar on average. Measuring individual fairness [13] remains a challenging problem. Fairness in Deep Learning. In this paper, we study the problem of fairness in the setting of deep learning. A number of approaches have been proposed to improve group fairness in deep neural networks [7, 11, 33, 35, 41, 42], many ofwhich yield large improvements in group fairness metrics without significantly affecting accuracy. Prediction Sensitivity: a measure of individual fairness. In this paper, we propose a new approach designed to help understand fairness properties of individual predictions. Our approach measures the extent to which a prediction is based on the value of a protected attribute which encodes an individual\u2019s membership in either a privileged or unprivileged group.\nWe call this measurement the prediction sensitivity, due to its similarity to the idea of sensitivity in differential privacy [13, 14]. We describe how to compute prediction sensitivity efficiently in standard deep learning frameworks via a novel application of automatic differentiation. Finally, we present preliminary empirical results from comparing prediction sensitivity to standard metrics for group fairness, suggesting that prediction sensitivity may be effective for understanding bias in trained models.\nPrediction sensitivity is intended as a first step towards understanding individual fairness in deep neural networks. Like all formal metrics for fairness, there are vital aspects of societal bias that are not captured by the definition. An important example of this for prediction sensitivity is the problem of redundant encodings [13]: features in the data which are not marked as protected attributes, but which are correlatedwith the protected attribute. Prediction sensitivity measures (only) the effect of the protected attribute on the model\u2019s prediction, and ignores any effect that correlated attributes might have. As a result, it may be possible to train models with low prediction sensitivity (indicating fair predictions) that nevertheless make unfair predictions by relying on redundant encodings. This and other limitations of prediction sensitivity (detailed in Section 7) are important topics for future work.\nar X\niv :2\n00 9.\n13 65\n0v 1\n[ cs\n.A I]\n2 8\nSe p\n20 20"
    },
    {
      "heading": "2 CHALLENGES OF DEFINING FAIRNESS",
      "text": "The issue of fairness in Artificial Intelligence models is particularly complex because of the manner in which we create these models. Models are typically trained and learn from data but the process is a black box which is inscrutable and difficult to reproduce. There have been methods which attempt to address the inscrutability of models by measuring causality via identification of potential outcomes. One method used for inferring the causal reasoning of data is the potential outcomes framework or the Rubin causal model (Johansson et al., 2016) [18]. This method identifies the possibilities of potential outcomes of a model by looking at individual features and all possible outcomes. Causal inference makes unit-level causal effects difficult to observe, giving rise to the fundamental problem of causal inference. Feature level analysis also has trade-offs. Whenever features are individually assessed, phenomena such as Simpson\u2019s Paradox can become problematic and obscure issues which are attributable to bias [22]. In understanding the complexity of black box models, we have made efforts to simplify or decomplexify models to reason about why they make certain predictions. Types of Bias. Bias can be defined as a systematic error or judgment. Bias often manifests based on assumptions that are proven to be untrue. These issues of bias can take place due to the data collection process, the data codification process, the feature selection and modelling process and the model prediction process. Human beings have inherent biases. These biases are aggregated and embedded within our data collection and analysis processes. One example of human bias is group attribution bias, whereby we judge a person as part of a monolithic group rather than as an individual,and project those tendencies of the generalized group to all individuals within that group. Implicit biases are an anecdotal extension of our experiences onto the larger group. These experiences are encoded into the data that we collect, the features we use, and the algorithmic predictions of our data. An example of bias that manifests in the data collection process is selection bias, whereby the data collected is not representative of the real world. An example of selection bias includes ethical issues in data collection that involve remuneration for giving access to one\u2019s data. In scenarios where remuneration is given, care must be taken so that the most economically disadvantaged persons are not giving away their data solely for the promise of remuneration rather than because they choose to do so. Participation bias (also called non-responsive bias) is another issue with data collection. This means that there are gaps in the data because of non-participation by certain groups of people. An example of participation bias might be a lack of persons willing to take part in a survey because the specificity of details in the survey may encourage doxing and retaliation by another group. Another bias is that of seeing cause in correlation or spuriousness, where a feature is identified as associated with a variable when there is no intrinsic correlation. This bias affects the model prediction process, resulting in wrongful predictions. Group Fairness versus Individual Fairness. Fairness is often identified by one\u2019s relationship to a particular group, where the assumption is that no specific individual is regarded as more unequal than another within the group. This is typically referred to as Equality. More broadly speaking, everyone is treated the same regardless of the specificity of their characteristics; that is, in spite of these\ncharacteristics. Allocation is therefore based on the needs of the group rather than the needs of a specific individual. In another definition, each individual\u2019s characteristics are individually assessed, and fairness is assessed by these characteristics. Each individual is judged fairly based on their specific needs and requirements, and allocations take the needs of the individual into account. This is typically called Equity. Measuring Fairness. A number of formal fairness metrics have been developed to measure fairness [13, 17, 20, 34, 39]; Barocas et al. [4] provide a comprehensive overview. These metrics are typically used to measure fairness by comparing the rates of positive predictions for members of the privileged and unprivileged group. They are typically defined in terms of a protected attribute, a feature in the data that indicates each individual\u2019s membership status in either the privileged or unprivileged group. Two specific metrics we will use in this paper are statistical parity [13] (also called demographic parity) and disparate impact [16]. Both are measures of group fairness."
    },
    {
      "heading": "3 BACKGROUND",
      "text": "Deep Learning. In this paper, we focus on machine learning models represented by artificial neural networks [19]. A model F is parameterized by a set of weights \u03b8 which are optimized during training; we write F (\u03b8 ,x) to represent a prediction made by the trained model on an example x . Deep learning models are typically trained by optimizing a loss function L using a ground-truth label y for each example. We write the loss for an example x ,y as L(F (\u03b8 ,x),y). During training, the loss is used to update the weights \u03b8 so that loss is reduced during the next training epoch. Automatic Differentiation.Automatic differentiation [5] is a computational method used to evaluate the derivative of a function efficiently. Automatic differentiation is normally used for computing the gradient of the loss with respect to the model\u2019s weights:\n\u2207\u03b8 (L(F (\u03b8 ,x),y))\nThis gradient is a vector containing the partial derivative of the model\u2019s output with respect to each of the weights. Automatic differentiation systems in modern deep learning frameworks are specifically designed to efficiently compute gradients for functions with many inputs, and they are usually used to calculate gradients during training. Fairness in Machine Learning. The bulk of previous work on fairness in machine learning attempts to improve group fairness at training time, often by the introduction of new kinds of regularization [2, 3, 7\u201311, 15, 20, 27, 28, 32, 33, 35, 37\u201341]. Many of these approaches are suitable for deep learning, and have been empirically validated using the metrics described above.\nExisting approaches focus on notions of group fairness, and are validated using metrics for group fairness. As a result, they can sometimes produce models that give blatantly unfair predictions for specific individuals, even though they score well on group fairness metrics."
    },
    {
      "heading": "4 PREDICTION SENSITIVITY",
      "text": "We propose prediction sensitivity, which quantifies the extent to which a prediction depends on the protected attribute. We hypothesize that models which rely heavily on the value of the protected attribute are likely to make different predictions for members and non-members of the advantaged group. Prediction sensitivity may therefore be a useful measure of individual fairness for each prediction made by the model.\nThe name \u201cprediction sensitivity\u201d comes from the connection to the notion of function sensitivity used in differential privacy [14]. Prediction sensitivity can be viewed as an approximation of function sensitivity for neighboring inputs that differ in their protected attribute. However, prediction sensitivity is not an upper bound on function sensitivity, as discussed later in this section. Prediction sensitivity is also related to the individual metric proposed by Dwork et al. [13], but prediction sensitivity can be efficiently computed for artificial neural networks. Formal Definition. Formally, we assume the existence of a neural network architecture F such that for a vector of trained weights \u03b8 and feature vector x , we can make a prediction y\u0302 as follows:\ny\u0302 = F (\u03b8 ,x) For such a model F , we define the prediction sensitivity with\nrespect to attribute a \u2208 x as the partial derivative: \u2202 \u2202a F (\u03b8 ,x)\nComputing Prediction Sensitivity. Automatic differentiation libraries are commonly used to compute gradients of the loss during training; the same libraries can be used to efficiently compute prediction sensitivity. Given a loss function L and a training example x ,y, for current weights \u03b8 , the training process might compute the gradient: \u2207\u03b8 (L(F (\u03b8 ,x),y)) Here, we write \u2207\u03b8 to denote the gradient with respect to each weight in \u03b8 . Thus the gradient contains partial derivatives of the loss with respect to the weights. Deep learning frameworks like TensorFlow are designed to compute gradients like these via automatic differentiation, but they are also capable of computing other types of gradients.\nTo compute prediction sensitivity for a feature vector x , we want to obtain the partial derivative of the prediction with respect to one feature. Given a trained model consisting of F and \u03b8 , we can compute prediction sensitivity for the feature a as:\n|\u2207a (F (\u03b8 ,x))| This value can be efficiently computed using existing deep learning frameworks.We have implemented this approach in TensorFlow, and used it to obtain the empirical results in Section 6. Properties of Prediction Sensitivity. Prediction sensitivity intends to capture the extent to which predictions depend on the protected status of an individual, but it is not always a perfect measurement of this dependence, and care must be taken in applying it. First, a model may predict positive outcomes for the advantaged group at a higher rate without relying on the protected attribute, and prediction sensitivity will not be useful in this case. Section 6 contains our preliminary empirical results suggesting that biased\nmodels often do learn to rely on the protected attribute in making predictions.\nSecond, prediction sensitivity measures sensitivity to changes in the protected attribute at one particular point, and small changes to one or more features could potentially increase or decrease prediction sensitivity significantly (in fact, the amount of this change is potentially unbounded). In most applications, we must assume that prediction sensitivity is fairly smooth\u2014i.e. that small changes in features do not produce large changes in prediction sensitivity\u2014in order for the measure to be useful. We hypothesize that most neural networks result in reasonably smooth prediction sensitivities; we discuss future work on this topic in Section 7.\nThird, the scale of prediction sensitivity depends on the model\u2019s architecture and the trained weights, in addition to the features of the input example. As a result, it is not possible to compare prediction sensitivities across trained models. A prediction sensitivity of 0.1 may be relatively low for one model, and indicate a prediction that is minimally dependent on the protected attribute (e.g. the prediction sensitivity for the other features may be 10.0); the same prediction sensitivity may be very high for another model. We discuss the impact of this fact in Section 7."
    },
    {
      "heading": "5 APPLYING PREDICTION SENSITIVITY",
      "text": "Prediction sensitivity is designed to be a measure of individual fairness, but there are several different ways it could be applied to improve fairness in AI systems. Fairness Monitor. One obvious opportunity for applying prediction sensitivity is as a way to measure the fairness of individual predictions made by a deployed model, and to signal an alarm for predictions that are not fair. A fairness monitor can be implemented by deciding on a prediction sensitivity threshold, calculating the prediction sensitivity for each prediction made, and signaling an alarm for each prediction whose sensitivity is above the threshold. We present a preliminary empirical evaluation of the potential for this approach to improve the fairness of predictions in Section 6.\nThe fairness monitor idea also has several unresolved challenges. First, it cannot improve predictions\u2014only raise an alarm for bad ones\u2014and it is still unclear how the feedback given by such an alarm could be used to improve the model. Second, it may be challenging to decide on a threshold in practice, especially because we expect that the appropriate setting for the threshold will depend on the training data, model architecture, and training algorithm used. Third, it may be difficult in practice to validate that a particular choice of threshold is the right one\u2014the fairness monitor may result in both false negatives and false positives, and both classes of error may be difficult to identify. Tracking Statistics.When a useful threshold is difficult to identify, prediction sensitivity could still be useful in measuring statistics about individual fairness of the predictions made by the deployed model. For example, the distribution of prediction sensitivities over many predictions might reveal low sensitivity for most predictions, with a few outliers\u2014suggesting that these outliers should be investigated with the goal of improving the model. It might also reveal\nchanges over time as the data used in making predictions starts to differ from the training data used to build the model. Evaluating Models. We hypothesize that prediction sensitivity might also be useful for evaluating a trained model empirically to understand how it makes predictions. For example, the distribution of prediction sensitivities over a testing set might represent a useful measure for fairness comparable to metrics like statistical parity and disparate impact. Unlike those metrics, however, prediction sensitivity is a measure of individual fairness, and may therefore reveal additional information that metrics for group fairness cannot (e.g. for models that usually make fair predictions, but are blatantly unfair for some outliers). Our preliminary empirical results in Section 6 provide some support for this idea."
    },
    {
      "heading": "6 EMPIRICAL EVALUATION",
      "text": "To explore the feasibility of using prediction sensitivity to understand the fairness of a model\u2019s predictions, we performed an empirical evaluation using a simple neural network trained on the Adult dataset [1]. The bias in this dataset is well-documented [6]: models trained without mitigations demonstrate bias against unprivileged groups on both gender and race.\nUsing TensorFlow, We trained a simple neural network consisting of two fully-connected hidden layers to perform the binary classification task of the Adult dataset. We implemented two commonly-used metrics for group fairness: statistical parity [13] and disparate impact [16]. When evaluated on the gender attribute, our model (trained without any mitigations for fairness) yielded a statistical parity value of -0.04 and a disparate impact value of 0.53. Both of these values indicate bias against the unprivileged group. Distribution of Prediction Sensitivity over Features. Figure 1 summarizes, using box-and-whisker plots, the distribution of prediction sensitivities from the model we trained for 14 features in the Adult dataset\u2019s test set. Features like education and occupation, which we might expect to have a large influence on income, indeed tend to have high prediction sensitivity. The gender and race features, which indicate an individual\u2019s membership in privileged or unprivileged groups, also have significant prediction sensitivity\u2014 contributing to the model\u2019s bias. Surprisingly, the relationship feature has the highest average prediction sensitivity\u2014perhaps because this feature acts as a proxy for gender. This is an important issue for future work, discussed more in Section 7. Effect of the Fairness Monitor. To measure how effective prediction sensitivity would be in building the fairness monitor described in Section 5, we evaluated this approach on the test set of the Adult dataset using two metrics for group fairness. For each example in the test set, we used our trained model to make a prediction, and measured the prediction sensitivity with respect to the \u201cgender\u201d feature. If the prediction sensitivity exceeded a threshold, we discarded the prediction (as a proxy for sounding the alarm described in Section 5); if not, we kept the prediction. Then, we evaluated only the \u201ckept\u201d predictions using a metric for group fairness (statistical parity or disparate impact). We repeated this experiment for various values of the threshold to determine the effect of the threshold\u2019s value on the group fairness metrics.\nThe results appear in Figures 2 (for statistical parity) and 3 (for disparate impact). In both cases, we see that the group fairness\nmetric improves as the threshold for prediction sensitivity goes down. This effect shows that the individual measure of fairness provided by prediction sensitivity may be well-aligned with the group-level measures of fairness already in common use, suggesting that prediction sensitivity may indeed be useful in measuring the fairness of individual predictions made by a model."
    },
    {
      "heading": "7 DISCUSSION & FUTUREWORK",
      "text": "Limitations of Preliminary Results. Our preliminary results suggest that prediction sensitivity may be a useful measure of individual fairness, but more study is needed to understand it fully. In particular, it is challenging even to define the meaning of false negatives and false positives in the setting of a fairness monitor\u2014doing so would require a \u201cground truth\u201d about the fairness of a particular prediction, which is impossible to formalize. Our experiments also do not measure the affect of the fairness monitor on accuracy, since discarding predictions is not realistic during deployment anyway. We expect that in any intervention based on prediction sensitivity, a tradeoff will exist between fairness and accuracy, and we plan to investigate this further in the future. Finally, our experiments examined only a single dataset and trained model; we plan to extend our evaluation to additional datasets and a variety of models in future work. Smoothness of Prediction Sensitivity. As mentioned earlier, prediction sensitivity measures the effect of changes in the protected attribute on the model\u2019s prediction at a particular point, but does not say anything about prediction sensitivity at nearby points. For example, prediction sensitivity may be very low for a particular example, but a slightly modified example may result in very high prediction sensitivity. As a result, it may be impossible to set a reasonable threshold for an application like a fairness monitor that avoids blatant false negatives; this effect also means that it is not possible to compare prediction sensitivity values across trained models. In future work, we plan to develop an automated way to analyze the smoothness of prediction sensitivity for a particular trained model. Correlated Attributes. Prediction sensitivity does not address the possibility of non-protected attributes that are correlated with the protected attribute (also called a redundant encoding by Dwork et al. [13]). The model may use such an attribute to make its prediction, and ignore the protected attribute; in this case, the model may result in unfair outcomes for the unprivileged group, even when its prediction sensitivity with respect to the protected attribute is low. This possibility is a direct result of the way prediction sensitivity is defined: it measures one way the model could arrive at an unfair prediction (i.e. by relying on the value of the protected attribute), but ignores other ways that the same outcome could occur (e.g. by relying on a correlated attribute).\nOur empirical results suggest that models which make unfair predictions (as measured by group fairness metrics like disparate impact) tend to rely on the protected attribute to do so. However, these results are for a single model trained on a single dataset; further experiments on additional models and datasets are needed. In addition, interventions during training to mitigate bias may have a large effect on the connection between group fairness and prediction sensitivity; we plan to study these as well."
    },
    {
      "heading": "8 RELATEDWORK",
      "text": "As the problem of bias in machine learning becomes more apparent, an increasing amount of research has been devoted to the topic. Mehrabi et al. [30] provide a survey, including broad definitions of bias and fairness use in the field of Artificial Intelligence, and how these definitions lay the groundwork for empirical measures of fairness in the field today. The book by Barocas et al. [4] gives an excellent overview of the field of algorithmic fairness more generally.\nThe bulk of previous work on fairness in machine learning attempts to improve fairness at training time, often by the introduction of new kinds of regularization [3, 8, 10, 32, 37\u201339]. Recent work in this area for deep learning (much of it leveraging adversarial learning) has demonstrated impressive empirical results, showing that the tradeoff between fairness and accuracy can often be successfully navigated in practice [7, 11, 33, 35, 41]. Other approaches\nare based on pre-processing, and do not require changes to the training process [2, 9, 27, 28, 40]. These approaches are suitable for deep learning, but typically do not provide the same level of accuracy as modifications to the training process. Finally, several approaches based on post-processing have been proposed [15, 20].\nExtensive work has also explored how to measure fairness, and why obtaining fairness in Artificial Intelligence is difficult. A number of different metrics have been proposed [4, 13, 17, 20, 34, 39], most of them designed to ensure group fairness. Other metrics apply to specific cases; for example, Mansoury et al. [29] describe the importance of aggregate diversity and propose a methods for improving aggregate diversity in recommender systems. Individual fairness has also been studied [13], and has been extended to approaches for cross-cultural fairness that identifies and measures fairness based on subgroups [25]. Zhang et al. [42] shows how\ncurrent methods of generalizing neural networks are faulty, and suggests a link between generalization and fairness."
    },
    {
      "heading": "9 CONCLUSION",
      "text": "We have proposed prediction sensitivity, a measure of individual fairness in deep neural networks that may help us understand sources of bias in automated systems based on Artificial Intelligence. We showed how to compute prediction sensitivity efficiently using existing deep learning frameworks, and we presented preliminary empirical results suggesting that prediction sensitivity may be a useful metric in measuring bias. We believe that prediction sensitivity is an important first step towards better understanding individual fairness, and we have identified a number of important areas for future study in that direction."
    },
    {
      "heading": "ACKNOWLEDGEMENTS",
      "text": "We thank David Darais and Kristin Mills for their contributions to the development of this work, and the Mechanism Design for Social Good reviewers for their helpful comments. This research was supported in part by an Amazon Research Award."
    }
  ],
  "title": "Towards a Measure of Individual Fairness for Deep Learning",
  "year": 2020
}
