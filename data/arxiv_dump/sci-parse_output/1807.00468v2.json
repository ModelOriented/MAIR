{
  "abstractText": "Fairness is a critical trait in decision making. As machine-learning models are increasingly being used in sensitive application domains (e.g. education and employment) for decision making, it is crucial that the decisions computed by such models are free of unintended bias. But how can we automatically validate the fairness of arbitrary machine-learning models? For a given machine-learning model and a set of sensitive input parameters, our Aeqitas approach automatically discovers discriminatory inputs that highlight fairness violation. At the core of Aeqitas are three novel strategies to employ probabilistic search over the input space with the objective of uncovering fairness violation. Our Aeqitas approach leverages inherent robustness property in common machine-learning models to design and implement scalable test generation methodologies. An appealing feature of our generated test inputs is that they can be systematically added to the training set of the underlying model and improve its fairness. To this end, we design a fully automated module that guarantees to improve the fairness of the model. We implemented Aeqitas and we have evaluated it on six stateof-the-art classifiers. Our subjects also include a classifier that was designed with fairness in mind. We show that Aeqitas effectively generates inputs to uncover fairness violation in all the subject classifiers and systematically improves the fairness of respective models using the generated test inputs. In our evaluation, Aeqitas generates up to 70% discriminatory inputs (w.r.t. the total number of inputs generated) and leverages these inputs to improve the fairness up to 94%.",
  "authors": [
    {
      "affiliations": [],
      "name": "Sakshi Udeshi"
    },
    {
      "affiliations": [],
      "name": "Pryanshu Arora"
    },
    {
      "affiliations": [],
      "name": "Sudipta Chattopadhyay"
    }
  ],
  "id": "SP:5eaade33ebd53c0336d8abd60a273e50f9778709",
  "references": [
    {
      "authors": [
        "Cynthia Dwork",
        "Moritz Hardt",
        "Toniann Pitassi",
        "Omer Reingold",
        "Richard S. Zemel"
      ],
      "title": "Fairness through awareness",
      "venue": "In Innovations in Theoretical Computer Science",
      "year": 2012
    },
    {
      "authors": [
        "Alhussein Fawzi",
        "Omar Fawzi",
        "Pascal Frossard"
      ],
      "title": "Analysis of classifiers\u2019 robustness to adversarial perturbations",
      "venue": "Machine Learning,",
      "year": 2018
    },
    {
      "authors": [
        "Michael Feldman",
        "Sorelle A. Friedler",
        "John Moeller",
        "Carlos Scheidegger",
        "Suresh Venkatasubramanian"
      ],
      "title": "Certifying and removing disparate impact",
      "venue": "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data",
      "year": 2015
    },
    {
      "authors": [
        "Sainyam Galhotra",
        "Yuriy Brun",
        "Alexandra Meliou"
      ],
      "title": "Fairness testing: testing software for discrimination",
      "venue": "In Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering,",
      "year": 2017
    },
    {
      "authors": [
        "Gabriel Goh",
        "Andrew Cotter",
        "Maya R. Gupta",
        "Michael P. Friedlander"
      ],
      "title": "Satisfying real-world goals with dataset constraints",
      "venue": "In Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems",
      "year": 2016
    },
    {
      "authors": [
        "Kathrin Grosse",
        "Nicolas Papernot",
        "Praveen Manoharan",
        "Michael Backes",
        "Patrick D. McDaniel"
      ],
      "title": "Adversarial examples for malware detection",
      "venue": "In Computer Security - ESORICS 2017 - 22nd European Symposium on Research in Computer Security, Oslo, Norway,",
      "year": 2017
    },
    {
      "authors": [
        "Alon Y. Halevy",
        "Peter Norvig",
        "Fernando Pereira"
      ],
      "title": "The unreasonable effectiveness of data",
      "venue": "IEEE Intelligent Systems,",
      "year": 2009
    },
    {
      "authors": [
        "Toshihiro Kamishima",
        "Shotaro Akaho",
        "Hideki Asoh",
        "Jun Sakuma"
      ],
      "title": "Fairnessaware classifier with prejudice remover regularizer",
      "venue": "European Conference, ECML PKDD",
      "year": 2012
    },
    {
      "authors": [
        "Louisa Lam",
        "Ching Y. Suen"
      ],
      "title": "Application of majority voting to pattern recognition: an analysis of its behavior and performance",
      "venue": "IEEE Trans. Systems, Man, and Cybernetics, Part A,",
      "year": 1997
    },
    {
      "authors": [
        "Phil McMinn"
      ],
      "title": "Search-based software test data generation: a survey",
      "venue": "Softw. Test., Verif. Reliab.,",
      "year": 2004
    },
    {
      "authors": [
        "Nicolas Papernot",
        "Patrick D. McDaniel",
        "Ian J. Goodfellow",
        "Somesh Jha",
        "Z. Berkay Celik",
        "Ananthram Swami"
      ],
      "title": "Practical black-box attacks against machine learning",
      "venue": "In Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security, AsiaCCS 2017,",
      "year": 2017
    },
    {
      "authors": [
        "Nicolas Papernot",
        "Patrick D. McDaniel",
        "Ananthram Swami",
        "Richard E. Harang"
      ],
      "title": "Crafting adversarial input sequences for recurrent neural networks",
      "venue": "In 2016 IEEE Military Communications Conference,",
      "year": 2016
    },
    {
      "authors": [
        "Dino Pedreshi",
        "Salvatore Ruggieri",
        "Franco Turini"
      ],
      "title": "Discrimination-aware data mining",
      "venue": "In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining,",
      "year": 2008
    },
    {
      "authors": [
        "Kexin Pei",
        "Yinzhi Cao",
        "Junfeng Yang",
        "Suman Jana"
      ],
      "title": "Deepxplore: Automated whitebox testing of deep learning systems",
      "venue": "In Proceedings of the 26th Symposium on Operating Systems Principles,",
      "year": 2017
    },
    {
      "authors": [
        "Yuchi Tian",
        "Kexin Pei",
        "Suman Jana",
        "Baishakhi Ray"
      ],
      "title": "Deeptest: Automated testing of deep-neural-network-driven autonomous cars",
      "venue": "CoRR, abs/1708.08559,",
      "year": 2017
    },
    {
      "authors": [
        "Matthew Wicker",
        "Xiaowei Huang",
        "Marta Kwiatkowska"
      ],
      "title": "Feature-guided black-box safety testing of deep neural networks. In Tools and Algorithms for the Construction and Analysis of Systems - 24th International Conference, TACAS 2018",
      "venue": "Held as Part of the European Joint Conferences on Theory and Practice of Software,",
      "year": 2018
    },
    {
      "authors": [
        "Muhammad Bilal Zafar",
        "Isabel Valera",
        "Manuel Gomez-Rodriguez",
        "Krishna P. Gummadi"
      ],
      "title": "Fairness constraints: Mechanisms for fair classification",
      "venue": "In Proceedings of the 20th International Conference on Artificial Intelligence and Statistics,",
      "year": 2017
    }
  ],
  "sections": [
    {
      "text": "We implemented Aeqitas and we have evaluated it on six stateof-the-art classifiers. Our subjects also include a classifier that was designed with fairness in mind. We show that Aeqitas effectively generates inputs to uncover fairness violation in all the subject classifiers and systematically improves the fairness of respective models using the generated test inputs. In our evaluation, Aeqitas generates up to 70% discriminatory inputs (w.r.t. the total number of inputs generated) and leverages these inputs to improve the fairness up to 94%.\nCCS CONCEPTS \u2022 Software and its engineering \u2192 Software testing and debugging;\nKEYWORDS Software Fairness, Directed Testing, Machine Learning ACM Reference Format: Sakshi Udeshi, Pryanshu Arora, and Sudipta Chattopadhyay. 2018. Automated Directed Fairness Testing. In Proceedings of the 2018 33rd ACM/IEEE International Conference on Automated Software Engineering (ASE \u201918), September 3\u20137, 2018, Montpellier, France. ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/3238147.3238165\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. ASE \u201918, September 3\u20137, 2018, Montpellier, France \u00a9 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-5937-5/18/09. . . $15.00 https://doi.org/10.1145/3238147.3238165\n1"
    },
    {
      "heading": "1 INTRODUCTION",
      "text": "Nondiscrimination is one of the most critical factors for social protection and equal human rights. The basic idea behind nondiscrimination is to eliminate any societal bias based on sensitive attributes, such as race, gender or religion. For example, it is not uncommon to discover the declaration of following nondiscrimination policy in universities [12]:\n\u201cThe University is committed to a policy of equal opportunity for all persons and does not discriminate on the basis of race, color, national origin, age, marital status, sex, sexual orientation, gender identity, gender expression, disability, religion, height, weight, or veteran status in employment, educational programs and activities, and admissions\"\nDue to the massive progress in machine learning in the last few decades, its application has now escalated over a variety of sensitive domains, including education and employment. The key insight is to primarily automate decision making via machine-learning models. On the flip side, such models may introduce unintended societal bias due to the presence of bias in their training dataset. This, in turn, violates the non-discrimination policy that the respective organization or the nation is intended to fight for. The validation of machine-learning models, to check for possible discrimination, is therefore critically important.\nIn this paper, we are concerned about the case that any two individuals who are similar with respect to a job at hand should also be treated in a similar fashion during decision making. Thus, we focus towards individual fairness, as it is critical for eliminating societal bias and aim to check for discrimination that might violate individual fairness [2]. The precise nature of such discrimination depends on the machine-learning model and its input features. Consequently, given a machine-learning model and the input features of the model, it is possible to systematically explore the input space and discover inputs that induce discrimination. We call such inputs discriminatory inputs. The primary objective of this paper is\nar X\niv :1\n80 7.\n00 46\n8v 2\n[ cs\n.L G\n] 3\n1 Ju\nl 2 01\n8\nto design scalable techniques that facilitate rapid discovery of discriminatory inputs. In particular, given a machine-learning model and a set of discriminatory input features (e.g. race, religion, etc.), our Aeqitas approach automatically discovers inputs to clearly highlight the discriminatory nature of the model under test.\nAs an example, consider the decision boundary of a classifier shown in Figure 1. Assume the two points A and B that differ only in being GenderA or GenderB . Despite being vastly similar, except in the gender aspect, the model classifies the points A and B differently. If we consider that such a classifier is used to predict the level of salary, then it certainly introduces unintended societal bias based on gender. Such unfair social biases not only affect the decisions of today but also might amplify it for future generations. The reason behind the discrimination (i.e. unfairness), as shown between points A and B, can be due to outdated training data that unintentionally introduces bias in certain attributes of the classifier model, e.g., gender in Figure 1. Using our Aeqitas approach, we automatically discover the existence of inputs similar to A and B with high probabilities. These inputs, then, are used to systematically retrain the model and reduce its unfairness.\nThe reason Aeqitas works is due to its directed strategy for test generation. In particular, Aeqitas exploits the inherent robustness property of common machine learning models for systematically directing test generation. As a result of this robustness property, the models should exhibit low variation in their output(s) with small perturbations in their input(s). For example, consider the points A1 and A2 which are in the neighbourhood of the point A. Since the point A exhibits discriminatory nature, it is likely that both points A1 and A2 will be discriminatory, as reflected via the presence of points B1 and B2, respectively. In our Aeqitas approach, we first randomly sample the input space to discover the presence of discriminatory inputs (e.g. point A in Figure 1). Then, we search the neighbourhood of these inputs, as discovered during the random sampling, to find the presence of more inputs (e.g. points A1 and A2 in Figure 1) of the same nature.\nAn appealing feature of Aeqitas is that it leverages the generated test inputs and systematically retrains the machine-learning model under test to reduce its unfairness. The retraining module is completely automatic and it therefore acts as a significant aid to the software engineers to improve the (individual) fairness of machine-learning models. The directed test generation and automated retraining set Aeqitas apart from the state-of-the-art in fairness testing [5]. While existing work [5] also considers test generation, such tests were generated randomly. If the discriminatory inputs are located only in specialized locations of the input space, then random test generators are unlikely to be effective in finding individuals discriminated by the corresponding model. To this end, Aeqitas empirically validates that a directed test generation, to uncover the discriminatory input regions, is indeed more desirable than random test generation. Moreover, Aeqitas provides statistical evidence that if it fails to discover any discriminatory input, then the machine-learning model under test is fair with high probability.\nThe remainder of the paper is organized as follows. After providing an overview of Aeqitas (Section 2), we make the following contributions:\n(1) We present Aeqitas, a novel approach to systematically generate discriminatory test inputs and uncover the fairness violation in machine-learning models. To this end, we propose three different strategies with varying levels of complexity (Section 4). (2) We present a fully automated technique to leverage the generated discriminatory inputs and systematically retrain the machine-learning models to improve its fairness (Section 4). (3) We provide an implementation of Aeqitas based on python. Our implementation and all experimental data are publicly available (Section 5). (4) We evaluate our Aeqitas approach with six state-of-theart classifiers including a classifier that was designed with fairness in mind. Our evaluation reveals that Aeqitas is effective in generating discriminatory inputs and improving the fairness of the classifiers under test. In particular, Aeqitas generated up to 70% discriminatory inputs (w.r.t. the total number of inputs generated) and improved the fairness up to 94% (Section 5).\nAfter discussing the related work (Section 6), we outline different threats to validity (Section 7) before conclusion and consequences (Section 8)."
    },
    {
      "heading": "2 BACKGROUND",
      "text": "In this section, we will discuss the critical importance of fairness testing and outline the key insight behind our approach.\nImportance of fairness The usage of machine learning is increasingly being observed in areas that are under the purview of antidiscrimination laws. In particular, application domains such as law enforcement, credit, education and employment can all benefit from machine learning. Hence, it is crucial that decisions influenced by any machine-learning model are free of any unnecessary bias.\nAs an example, consider a machine-learning model that predicts the income levels of a person. It is possible that such a model was trained on a dataset, which, in turn was unfairly biased to a certain gender or a certain race. As a result, for all equivalent characteristics, barring the gender or race, the credit worthiness of a person will be predicted differently by this model. If financial institutions used such a model to determine the credit worthiness of an individual, then individuals might be disqualified only on the basis of their gender or race. Such a discrimination is certainly undesirable, as it reinforces and amplifies the unfair biases that we, as a society are continuously fighting against.\nFairness in Aeqitas Aeqitas aims to discover the violation of individual fairness [2] in machine-learning models. This means, Aeqitas aims to find instances of pair of inputs I and I \u2032 that are classified differently despite being vastly similar. The similarity between inputs I and I \u2032 is based on a set of potentially discriminatory input parameters (see Definition 4.1). Detecting the violation of individual fairness is challenging. This is because inputs that are prone to the violation of individual fairness might be located only in specific regions of the input space of a model. Consequently, specialized and directed techniques are required to rapidly locate these input regions. This is the primary motivation behind the development of Aeqitas. For the rest of the paper, we will simply\nuse the term fairness (instead of individual fairness) in the light of our Aeqitas approach (see Definition 4.1). Towards fair machine-learning models A naive approach to design fair machine-learning models is to ignore certain sensitive attributes such as race, color, religion, gender, disability, or family status. It is natural to assume that if such attributes are held back from decision making, then the respective model will not discriminate. Unfortunately, such an approach of accomplishing fairness through blindness fails. This is because of the presence of redundant encoding in the training dataset [15]. Due to the redundant encoding, it is frequently possible to predict the unknown (sensitive) attributes from other seemingly innocuous features. For example, consider certain ethnic groups in a city that are geographically bound to certain areas. In such cases, even if a machine-learning model in a financial institute does not use ethnicity as a parameter to decide credit worthiness, it is possible to guess ethnicity from geographic locations, which indeed might be a parameter for the model. Therefore, it is critical to systematically test a machine-learning model to validate its fairness property. Why fairness testing is different In contrast to classic software testing, testing machine-learning models face additional challenges. Typically, these models are deployed in contexts where the formal specification of the software functionality is difficult to develop. In fact, such models are designed to learn from existing data because of the challenges in creating a mathematical definition of the desired software properties. Moreover, an erroneous software behaviour can be rectified by retraining the machine-learning models. However, for classic software, a software bug is typically fixed via modifying the responsible code. State-of-the-art in fairness testing The state-of-the-art in systematic testing of software fairness is still at its infancy. In contrast to existing work [5], Aeqitas focuses on directed test generation strategy. As evidenced by our evaluation, this is crucial to locate specific input regions that violate individual fairness. To illustrate our objective, consider a machine-learning model f and its inputs I and I \u2032. I differs from I \u2032 only in being assigned a different value in a potentially discriminatory input parameter. For example, if gender is the potentially discriminatory input parameter, then I will be different from I \u2032 only in being GenderA or GenderB . We are interested to discover inputs I or I \u2032, where the difference in outputs of the model, captured via | f (I ) \u2212 f (I \u2032)|, is beyond a pre-determined threshold. We call such inputs I or I \u2032 to be discriminatory inputs for the model f . It is important to note that the discrimination threshold and the potentially discriminatory input parameters are supplied by the users of our tool. In the preceding example, the potentially discriminatory input parameter, i.e., gender can be specified by the user. Similarly, users can also fine tune the value at which | f (I ) \u2212 f (I \u2032)| is considered to be discriminatory. Robustness in machine learning Robustness is a notion that says that the output of a machine-learning model is not dramatically affected by small changes to its input [3]. Assume a model f , let i be the input to f and \u03b4 be a small value. If f is robust, then f (i) \u2248 f (i + \u03b4 ). Nevertheless, existing techniques provide evidence to find inputs that violate this robustness property. Such inputs are called adversarial inputs [14] [7] [13]. However, adversarial inputs generally cover only a small fraction of the entire input space. This\nis evident by the fact that adversarial inputs need to be crafted using very specialized techniques. Additionally, Aeqitas is designed to avoid these adversarial input regions by systematically directing the test generators. Intuitively, Aeqitas achieves this by reducing the probability to explore an input region when tested inputs from the region did not exhibit discriminatory nature (see Algorithm 2 for details). Consequently, if adversarial or non-robust input regions do not exhibit discriminatory nature, such regions will eventually be explored only with very low probability."
    },
    {
      "heading": "3 APPROACH AT A GLANCE",
      "text": "We propose, design and evaluate three schemes, with varying levels of complexities, to systematically uncover software fairness problems. The crucial components of our approach are outlined below.\nGlobal search In the first step of all our proposed schemes, we uniformly sample the inputs and record the discriminatory inputs that we find. In the light of uniformly sampling the input space, we can guarantee, with very high probability, to discover a discriminatory input, if such an input exists. For instance, Figure 2(a) highlights the probability of finding a discriminatory input in an input space with only 1% discriminatory inputs. Therefore, if discriminatory inputs exist, the first step of our proposed schemes guarantee to find at least one such input with high probabilities.\nLocal search The second step of our proposed schemes share the following hypothesis: If there exists a discriminatory input I \u2208 I, where I captures the input domain, then there exist more discriminatory inputs in the input space closer to I . The input domain I can be considered as the cartesian product of the domain of n input parameters, say P1, P2, . . . , Pn . We assume Ik captures the domain of input parameter Pk . Therefore, I = I1 \u00d7 I2 \u00d7 . . . \u00d7 In . An input parameter p \u2208 \u22c3ni=1 Pi can be potentially discriminatory if the output of the machine-learning model should not be biased towards specific values in Ip . Without loss of generality, we assume a subset of parameters Pdisc \u2286 \u22c3n i=1 Pi to be potentially discriminatory. For an input I \u2208 I, we use Ik to capture the value of parameter Pk within input I . Based on this notion, we explore the following methods to realize our hypothesis. Our methods differ on how we systematically explore the neighbourhood of a discriminatory input I (d ). I (d ), in turn, was discovered in the first step of Aeqitas.\n(1) First a parameter p \u2208 \u22c3ni=1 Pi \\ Pdisc is randomly chosen. Then a small perturbation (i.e. change) \u03b4 is added to I (d )p . Typically \u03b4 \u2208 {\u22121,+1} as we consider integer and real-valued input parameters in our evaluation.\n(2) In the second method, we assign probabilities on how to perturb a chosen parameter. A specific parameter p \u2208 \u22c3ni=1 Pi \\ Pdisc is still chosen uniformly at random. However, if a given perturbation \u03b4 of I (d )p consistently yields discriminatory inputs, then the perturbation \u03b4 is employed with higher probability. Since \u03b4 typically belongs to a small set of values, such a strategy works efficiently in practice. (3) The third method augments the second method by refining probabilities to perturb an input parameter. Concretely, if\nOur proposed methodologies are fully automated, they do not require the source code of the models and work efficiently in practice for state-of-the-art classifiers.\nFigure 3 illustrates Aeqitas approach when I and I \u2032 were discovered in the first step. Then, the second step explored the neighbourhood of I by adding small changes \u03b4 to an input parameter. Estimation of discriminatory inputs An appealing feature of Aeqitas is that we can estimate the percentage of discriminatory inputs in I. To this end, we leverage the law of large numbers (LLN) in probability theory. In particular, we generate K inputs uniformly at random and check whether they can lead to discriminatory inputs. Assume that K \u2032 \u2264 K inputs turn out to be discriminatory. We compute the ratio K \u2032K over a large number of trials. According to LLN, the average of these ratios closely approximates the actual percentage of discriminatory inputs in I. Figure 2(b) highlights such convergence after only 400 trials when K was chosen to be 1000. Why Aeqitas works? The reason Aeqitas works is because of the robustness property of common machine-learning models. In particular, if we perturb the input to a model by some small \u03b4 , then the output is not expected to change dramatically. As we expect the machine-learning models under test to be relatively robust, we can leverage their inherent robustness property to systematically generate test inputs that exhibit similar characteristics. In our Aeqitas approach, we focus on the discriminatory nature of a given input. We aim to discover more discriminatory inputs in the\nproximity of an already discovered discriminatory input leveraging the robustness property.\nHow Aeqitas can be used to improve software fairness? We have designed a fully automated module that leverages on the discriminatory inputs generated by Aeqitas and retrains the machine-learning model under test. We empirically show that such a strategy provides useful capabilities to a developer. Specifically, our Aeqitas approach automatically improves the fairness of machine-learning models via retraining. For instance, in certain decision tree classifiers, our Aeqitas approach reduced the fraction of discriminatory inputs up to 94%."
    },
    {
      "heading": "4 DETAILED APPROACH",
      "text": "In this section, we discuss our Aeqitas approach in detail. To this end, we will use the notations captured in Table 1.\nOur approach revolves around discovering discriminatory inputs via systematic perturbation. We introduce the notion of discriminatory inputs and perturbation formally before delving into the algorithmic details of our approach.\nDefinition 4.1. (Discriminatory Input and fairness) Let f be a classifier under test, \u03b3 be the pre-determined discrimination threshold (e.g. chosen by the user), and I \u2208 I. Assume I \u2032 \u2208 I such that there exists a non-empty set Q \u2286 Pdisc and for all q \u2208 Q , Iq , I \u2032 q and for all p \u2208 P \\Q , Ip = I \u2032p . If | f (I ) \u2212 f (I \u2032)| > \u03b3 , then I is called a discriminatory input of the classifier f and is an instance that manifests the violation of (individual) fairness in f .\n1\nDefinition 4.2. (Perturbation) We define perturbation \u0434 as a function \u0434 : I \u00d7 (P \\ Pdisc ) \u00d7 \u0393 \u2192 I where \u0393 = {\u22121,+1} captures the set of directions to perturb an input parameter. If I \u2032 = \u0434(I ,p,\u03b4 ) where I \u2208 I, p \u2208 P \\ Pdisc and \u03b4 \u2208 \u0393, then I \u2032p = Ip + \u03b4 and for all q \u2208 P \\ {p}, we have I \u2032q = Iq .\nIt is worthwhile to mention that the set of directions to perturb an input parameter, i.e. \u0393 can easily be extended with more possibilities to perturb. Besides, it can also be customized with respect to different input parameters. However, for the sake of brevity, we will stick with the simplified version stated in Definition Theorem 4.2.\nAn overview of our overall approach appears in Figure 4. The main contribution of this paper is an automated test generator to discover fairness violation. This involves two stages: 1) global search (GLOBAL_EXP) and 2) local search (LOCAL_EXP) over the input domain I. Optionally, the generated test inputs can be leveraged to retrain the model under test and improve fairness.\nIn the following, we will describe the crucial components of our Aeqitas approach, as shown in Figure 4."
    },
    {
      "heading": "4.1 Global Search",
      "text": "The motivation behind our global search (cf. procedure global_exp in Algorithm 1) is to discover some points in I that can be used to drive our local search algorithm. To this end, we first select an input I randomly from the input domain. Input I , then, is used to generate a set of inputs that cover all possible values of sensitive parameters Pdisc \u2286 P . This leads to a set of inputs I(d ). We note that the set of sensitive parameters (e.g. race, religion, gender) Pdisc typically has a small size. Therefore, despite the exhaustive nature of generating I(d ), this is practically feasible. Finally, we discover the discriminatory inputs (cf. Definition 4.1) within I(d ) and use the resulting discriminatory input set for further exploration during our local search over I."
    },
    {
      "heading": "4.2 Local Search",
      "text": "In this test generation phase, we take the inputs generated by our global search (i.e. disc_inputs) and then search in the neighbourhood of disc_inputs to discover other inputs with similar characteristics (cf. procedure local_exp in Algorithm 2). Our search strategy is motivated from the robustness property inherent in\nAlgorithm 1 Global Search 1: procedure global_exp(P , Pdisc ) 2: disc_inps \u2190 \u03d5 3: \u25b7 N is the number of trials in global search 4: for i in (0, N) do 5: Select an input I \u2208 I at random 6: \u25b7 I(d ) extends I with all possible values of Pdisc 7: I(d ) \u2190 {I \u2032 | \u2200p \u2208 P \\ Pdisc . Ip = I \u2032p } 8: if (\u2203I, I \u2032 \u2208 I(d ), |f (I ) \u2212 f (I \u2032) | > \u03b3 ) then 9: disc_inps \u2190 disc_inps \u222a {I } 10: end if 11: end for 12: return disc_inps 13: end procedure\nAlgorithm 2 Local Search 1: procedure local_exp(disc_inps , P , Pdisc , \u2206v , \u2206pr ) 2: Test \u2190 \u03d5 3: Let P \u2032 = P \\ Pdisc 4: Let \u03c3pr [p] = 1|P \u2032 | for all p \u2208 P \u2032\n5: Let \u03c3v [p] = 0.5 for all p \u2208 P \u2032 6: for I \u2208 disc_inps do 7: \u25b7 N is the number of trials in local search 8: for i in (0, N) do 9: Select p \u2208 P \u2032 with probability \u03c3pr [p]\n10: Select \u03b4 = \u22121 with probability \u03c3v [p] 11: \u25b7 Note that I is modified as a side-effect of modifying Ip 12: Ip \u2190 Ip + \u03b4 13: \u25b7 I(d ) extends I with all values of Pdisc 14: I(d ) \u2190 {I \u2032 | \u2200p \u2208 P \\ Pdisc . Ip = I \u2032p } 15: if (\u2203I, I \u2032 \u2208 I(d ), |f (I ) \u2212 f (I \u2032) | > \u03b3 ) then 16: \u25b7 Add the perturbed input I 17: Test \u2190 Test \u222a {I } 18: end if 19: update_prob(I , p , Test, \u03b4 , \u2206v , \u2206pr ) 20: end for 21: end for 22: return Test 23: end procedure\ncommon machine-learning models. According to the notion of robustness, the neighbourhood of an input should produce similar output. Therefore, it becomes logical to search the neighbourhood of disc_inputs , as these are the discriminatory inputs and their neighbourhood are likely to be discriminatory for robust models.\nTo search the neighbourhood of disc_inputs , Aeqitas perturbs an input I \u2208 disc_inputs by changing the value of some parameter p \u2208 P \\ Pdisc (i.e. Ip ). The value of the parameter p is perturbed by \u03b4 \u2208 {\u22121,+1}. We note that as a side-effect of changing Ip , input I is automatically modified. This modified version of I is further perturbed in subsequent iterations of the inner loop in Algorithm 2. Our Aeqitas approach chooses a parameter p \u2208 P \\ Pdisc with probability \u03c3pr [p] (cf. Algorithm 2). For all p \u2208 P \\ Pdisc , initially \u03c3pr [p] was assigned to 1|P\\Pdisc | . Once p is chosen its value is perturbed by \u03b4 = \u22121 with probability \u03c3v [p] and by \u03b4 = +1 with probability 1 \u2212 \u03c3v [p]. \u03c3v [p] is initialized to 0.5 for all parameters in p \u2208 P \\ Pdisc .\nAlgorithm 3 Aeqitas semi-directed update probability 1: procedure update_prob(I , p , Test, \u03b4 , \u2206v , \u2206pr ) 2: if (I \u2208 Test \u2227 \u03b4 = \u22121) \u2228 (I < Test \u2227 \u03b4 = +1) then 3: \u03c3v [p] \u2190 min(\u03c3v [p] + \u2206v , 1) 4: end if 5: if (I < Test \u2227 \u03b4 = \u22121) \u2228 (I \u2208 Test \u2227 \u03b4 = +1) then 6: \u03c3v [p] \u2190 max(\u03c3v [p] \u2212 \u2206v , 0) 7: end if 8: end procedure\nAlgorithm 4 Aeqitas fully-directed update probability 1: procedure update_prob(I , p , Test, \u03b4 , \u2206v , \u2206pr ) 2: if (I \u2208 Test \u2227 \u03b4 = \u22121) \u2228 (I < Test \u2227 \u03b4 = +1) then 3: \u03c3v [p] \u2190 min(\u03c3v [p] + \u2206v , 1) 4: end if 5: if (I < Test \u2227 \u03b4 = \u22121) \u2228 (I \u2208 Test \u2227 \u03b4 = +1) then 6: \u03c3v [p] \u2190 max(\u03c3v [p] \u2212 \u2206v , 0) 7: end if 8: if I \u2208 Test then 9: \u03c3pr [p] \u2190 \u03c3pr [p] + \u2206pr 10: \u03c3pr [p] \u2190 \u03c3pr [p]\u2211\nx\u2208P\\Pdisc \u03c3pr [x ] for all p \u2208 P \\ Pdisc\n11: end if 12: end procedure\nAeqitas employs three different strategies, namely Aeqitas random, Aeqitas semi-directed and Aeqitas fully-directed, to update the probabilities in \u03c3pr and \u03c3v . This is to direct the test generation process with a focus on discovering discriminatory inputs. In the following, we will outline the different strategies implemented within Aeqitas.\nAequitas random. Aeqitas random does not update the initial probabilities assigned to \u03c3pr and \u03c3v . This results in \u03b4 (i.e. perturbation value) andp (i.e. the parameter to perturb) both being chosen randomly. Intuitively, Aeqitas random explores inputs around the neighbourhood of disc_inputs (i.e. set of discriminatory inputs discovered via global search) uniformly at random. Nevertheless, Aeqitas random empirically outperforms a purely random search over the input space. This is because it still performs a random search in a constrained input region \u2013 specifically, the input region that already contains discriminatory inputs.\nAequitas semi-directed. Aeqitas semi-directed drives the test generation by systematically updating \u03c3v , i.e., the probabilities to perturb the value of an input parameter by \u03b4 = \u22121 (cf. Algorithm 3). The parameter p, to perturb, is still chosen randomly. Initially, we choose \u03b4 \u2208 {\u22121,+1} where the probability that \u03b4 = \u22121 is \u03c3v [p] and the probability that \u03b4 = +1 is 1\u2212\u03c3v [p]. If the perturbed input is discriminatory (cf. Definition 4.1), then we increase the probability associated with \u03c3v [p] by a pre-determined offset \u2206v . Otherwise, \u03c3v [p] is reduced by the same offset \u2206v . Intuitively, the updates to probabilities in \u03c3v prioritise a direction \u03b4 \u2208 {\u22121,+1} when the respective direction results in discriminatory inputs.\nAequitas fully-directed. Aeqitas fully-directed extends Aeqitas semi-directed by systematically updating the probabilities to choose a parameter for perturbation. To this end, we update\nprobabilities in \u03c3pr during the test generation process (cf. Algorithm 4). Assume we pick a parameter p \u2208 P \\ Pdisc to perturb. Initially, we have \u03c3pr [p] = 1|P\\Pdisc | . If the perturbation of the given parameter p by \u03b4 results in a discriminatory input, then we add a pre-determined offset \u2206pr to \u03c3pr [p]. To reflect this change in probability, we normalize \u03c3pr [p\u2032] to\n\u03c3pr [p\u2032]\u2211 x\u2208P\\Pdisc \u03c3pr [x ]\nfor every p\u2032 \u2208 P \\ Pdisc . Intuitively, the updates to probabilities in \u03c3pr prioritize a parameter when perturbing the respective parameter results in discriminatory inputs."
    },
    {
      "heading": "4.3 Estimation using LLN",
      "text": "An attractive feature of Aeqitas is that we can estimate the percentage of discriminatory inputs in I for any given model. We leverage the Law of Large Numbers (LLN) from probability theory to accomplish this. Let \u039b be an experiment. In this experiment, we generatem inputs uniformly at random. These are independent and identically distributed (IID) samples I1, I2 . . . Im . We execute these inputs and count the number of inputs that are discriminatory in nature. Letm\u2032 be the number of inputs that are discriminatory. \u039b then outputs the percentagem = m\n\u2032\u00d7100 m .\n\u039b is conducted K times. In each instance of the experiment, we collect the outcomem1,m2 . . .mK . Let M = K\u22121 \u2211K i=1mi . According to LLN, the average of the results, i.e. M , obtained from a large number of trials, should be close to the expected value, and it will tend to become closer as more trials are performed. This implies as,\nK \u2192\u221e M \u2192 M\u2217\nwhereM\u2217 is the true percentage of the discriminatory inputs present in I for the machine-learning model under test. This phenomenon was observed in our experiments. Figure 2(b) shows that the M converges only after 400 trials (i.e. K = 400)."
    },
    {
      "heading": "4.4 Improving Model Fairness",
      "text": "It has been observed that generated test inputs showing the violation of desired-properties in machine-learning models can be leveraged for improving the respective properties. This was accomplished via augmenting the training dataset with the generated test inputs and retraining the model [17].\nHence, we intend to evaluate the usefulness of our generated test inputs to improve the model fairness via retraining. To this end, Aeqitas has a completely automated module that guarantees reduction of the percentage of discriminatory inputs in I. We achieve this by systematically adding portions of generated discriminatory inputs to the training dataset.\nAssume Test be the set of discriminatory inputs generated by Aeqitas. Aeqitas is effective in generating discriminatory inputs and the size of the set Test is usually large. A naive approach to retrain the model will be to add all generated discriminatory inputs to the training dataset. Such an approach is likely to fail to improve the fairness of the model. This is because the generated test inputs are targeted towards finding discrimination and are unlikely to follow the true distribution of the training data. Therefore, blindly adding all the test inputs to the training set will bias its distribution towards the distribution of our generated test inputs. To solve this\nAlgorithm 5 Retraining 1: procedure Retraining(f , Test, training_data) 2: N \u2190\u221e 3: fcur \u2190 f 4: for i in (2, N) do 5: pi \u2190 a random real number between (2i\u22122, 2i\u22121) 6: if pi > 100 then 7: Exit the loop 8: end if 9: k \u2190 len(training_data) 10: naddn \u2190 pi \u00b7k 100 11: TDaddn \u2190 randomly selected naddn inputs from T est 12: TDnew \u2190 training_data \u222a TDaddn 13: fnew \u2190 model trained using TDnew 14: \u25b7 Estimate the number of discriminatory inputs (section 4.3) 15: faircur \u2190 LLN_Fairness_Estimation (fcur ) 16: fairnew \u2190 LLN_Fairness_Estimation (fnew ) 17: if (faircur > fairnew ) then 18: fcur \u2190 fnew 19: else 20: Exit the loop 21: end if 22: end for 23: return fcur 24: end procedure\nchallenge, it is important that only portions of discriminatory inputs from Test are added to the training dataset.\nLet pi be the percentage, with respect to the size of the training data, that we choose at any given iteration i . If size of training data is M , then we select pi \u00b7M100 discriminatory inputs from Test at random and add these discriminatory inputs to the training dataset. For i \u2208 [2,N ], we set pi randomly in a range between [2i\u22122, 2i\u22121]. The intuition behind this is to find an efficient mechanism to systematically add inputs from Test to the training dataset and to approximate the optimal reduction in discriminatory inputs. We terminate the process when adding inputs from Test to the training dataset does not decrease the estimated fraction of discriminatory inputs in I. The currently trained model (i.e. fcur in Algorithm 5) is then taken as the improved model with better (individual) fairness score. In this way, we can guarantee that our retraining process always terminates with a reduction in discriminatory inputs.\nOur retraining strategy is designed to be fast without sacrificing the fairness significantly. Our main objective is to demonstrate that Aeqitas generated test inputs can indeed be used by the developers to improve the individual fairness of their models. The amount of added test inputs (generated by Aeqitas) is chosen from exponentially increasing intervals (i.e. the interval [2i\u22122, 2i\u22121] in Algorithm 5). Such a strategy is taken to quickly scope the sensitivity of the model with respect to the generated test data. Moreover, by choosing a random number pi in the interval, we try not to overshoot the value of pi by a large margin that causes the optimal reduction of discriminatory inputs in I. As a result, our proposed retraining strategy maintains a balance between improving model fairness and the efficiency of retraining.\nIt is well known that adding more data to a machine-learning algorithm is likely to lead to increased accuracy [8]. A relevant\nchallenge here is attributed to the labeling of the generated test data. There exists a number of effective strategies to tackle this problem. One such strategy is finding the label via a simple majority of a number of classifiers [10]. Majority voting has been shown to be very effective for a wide range of problems [16] and we believe it should be readily applicable in our context of improving fairness as well. Nevertheless, test data labeling is an orthogonal problem in the domain of machine learning and we consider it to be beyond the scope of the problem targeted by Aeqitas."
    },
    {
      "heading": "4.5 Termination",
      "text": "Aeqitas can be configured to have various termination conditions depending on the particular use case of the developer. In particular, Aeqitas can be terminated with the following possible conditions:\n(1) Aeqitas can terminate after it has generated a user specified number of discriminatory inputs from I. This feature can be used when a certain number of discriminatory inputs need to be generated for testing, evaluation or retraining of the model. (2) Aeqitas can also terminate within a given time bound. This is useful to quickly check if the model exhibits discrimination for a particular set of sensitive parameters.\nIn our evaluation, we used both the termination criteria to evaluate the effectiveness and efficiency of Aeqitas."
    },
    {
      "heading": "5 RESULTS",
      "text": "Experimental setup. We evaluate Aeqitas across a wide variety of classifiers, including a classifier which was designed to be fair. Some salient features of these classifiers are outlined in Table 2. In particular, Fair SVM (cf. Table 2) was specifically designed with fairness in mind [19]. The rest of the classifiers under test are the standard implementations found in Python\u2019s Scikit-learn machine learning library. These classifiers are used in a wide variety of applications by machine-learning engineers across the world.\nOther than Fair SVM [19], we have used Scikit-learn\u2019s Support Vector Machines (SVM), Multi Layer Perceptron (MLPC), Random Forest and Decision Tree implementations for our experiments. We also evaluate an Ensemble Voting Classifier (Ensemble), in which we take the combination of two classifier predictions. The classifiers we use are Random Forest and Decision Tree estimators (cf. Table 2).\nAll classifiers listed in Table 2 are used for predicting the income. These classifiers are trained with the data obtained from the US census [1]. The size of this training data set is around 32,000. We train all the six classifiers on this training data. The objective is to classify whether the income of an individual is above $50,000 (captured via classifier output \u201c+1\") or below (captured via classifier output \u201c-1\"). For all the classifiers, set of discriminatory parameters,\ni.e. Pdisc is the gender of an individual. The threshold value for identifying a discriminatory input is set to zero. This means, if I differs from I \u2032 only in being GenderA or GenderB , then I or I \u2032 are discriminatory inputs of a classifier f when | f (I ) \u2212 f (I \u2032)| \u2265 0. In our experiments we set the perturbation \u03b4 \u2208 {\u22121,+1} and both \u2206v and \u2206pr as 0.001. These are user defined variables that guide our Aeqitas approach. In particular, these variables are used to systematically refine the probabilities to choose an input parameter to perturb and to choose a perturbation value \u03b4 (cf. Section 4).\nWe implement Aeqitas in Python, as it is a popular choice of language for the development of machine-learning models and related applications. The implementation is around 600 lines of python code. All our experiments were performed on an Intel i7 processor having 64GB of RAM and running Ubuntu 16.04.\nKey results. We use three different test generation methodologies, namely Aeqitas random, Aeqitas semi-directed and Aeqitas fully-directed. These methodologies differ with respect to the increasing levels of sophistication in systematically searching the input space (cf. Section 4.2). In particular, Aeqitas fullydirected involves the highest level of sophistication in searching the input space. As expected, Aeqitas fully-directed consistently outperforms the Aeqitas random and Aeqitas semi-directed, as observed from Figure 5. However, Aeqitas fully-directed and Aeqitas semi-directed demand more computational resources per unit time than Aeqitas random. As a result, Aeqitas random is more appropriate to use, as compared to the rest of our approaches, for testing with limited computational resources per unit time. The test subject used in Figure 5 was the Fair SVM (cf. Table 2).\nTo illustrate the power of our Aeqitas approach over the stateof-the-art fairness testing [5], we also compare our approaches with the state-of-the-art, which, in turn is captured via \u201cRandom\" in Figure 5. It is evident that even the least powerful technique implemented within our Aeqitas approach (i.e. Aeqitas random) significantly outperforms the state-of-the-art. In our evaluation, we discovered that Aeqitas is more effective than the state-of-the-art random testing by a factor of 9.6 on average and up to a factor of 20.4. We measured the effectiveness via the number of discriminatory inputs generated by a test generation technique. Aeqitas also provides capabilities to automatically retrain a machine-learning model with the objective to reduce the number of discriminatory inputs. To this end, Aeqitas reduced the number of discriminatory inputs by 43.2% on average with a maximum reduction of 94.36%.\nRQ1: How effective is Aeqitas in finding discriminatory inputs?\nWe evaluate the capability of Aeqitas in effectively generating discriminatory inputs. For all the subject classifiers, we measure the effectiveness of our test algorithms via the number of discriminatory inputs generated with respect to the number of total inputs generated.\nA purely random approach is not effective in generating discriminatory inputs. As observed from Figure 5, the number of discriminatory inputs generated by such an approach does not increase rapidly over the number of inputs generated. This is expected, as a purely random approach does not incorporate any systematic strategy to discover inputs violating fairness. The ineffectiveness of random testing persists across all the subject classifiers, as observed in Table 3.\nAs observed from Table 3, all test generation approaches implemented within Aeqitas outperform a purely random approach. In particular, the rate at which our Aeqitas approach generates discriminatory inputs is significantly higher than a purely random approach. As a result, Aeqitas provides scalable and effective technique for machine learning engineers who aim to rapidly discover fairness issues in their models. Aeqitas random, Aeqitas semi-directed and Aeqitas fully-directed involve increasing level of sophistication in directing the test input generation. As a result, Aeqitas fully-directed approach performs the best among all our test generators. In particular, Aeqitas semi-directed is on an average 46.7% and up to 64.9% better than Aeqitas random. Finally, Aeqitas full-directed is on an average 29.5% and up to 56.56% better than Aeqitas semi-directed.\nBy design, Aeqitas does not generate any false positives. This means that any discriminatory input generated by Aeqitas are indeed discriminatory to the model under test, subject to the chosen threshold of discrimination.\nFinding: Aeqitas fully-directed approach outperform a purely random approach up to a factor of 20.4 in terms of the number of discriminatory inputs generated. It also performs up to 56.7% better than Aeqitas semi-directed, which, in turn performs up to 64.9% better than Aeqitas random, our least sophisticated approach.\nRQ2: How efficient is Aeqitas in finding discriminatory inputs?\nTable 4 summarizes how much time each of the methods takes to generate 10,000 discriminatory inputs. On an average Aeqitas random performs 64.42% faster than the state of the art. The improvement in Aeqitas fully-directed is even more profound. On an average, Aeqitas fully-directed is 83.27% faster than the state of the art, with a maximum improvement of 96.62% in the case of Multi Layer Perceptron.\nIt is important to note that the reported time in Table 4 includes both the time needed for test generation and for test execution.\nHence, the reported time is highly dependent on the execution time of the model under test.\nClassifier estimated % of disc inputin I (95% confidence interval) %Impr %Inps added\nbefore retraining\nafter retraining\nFair SVM 3.86 (3.76, 3.95) 2.89 (2.64, 3.14) 25.15 15.6 SVM 0.33 (0.14, 0.51) 0.12 (0.09, 0.14) 63.54 26.9\nMLPC 0.39 (0.36, 0.42) 0.28 (0.27, 0.29) 30.12 23.7 Random Forest 8.84 (8.78, 8.91) 6.68 (6.35, 7.01) 24.48 32.4 Decision Tree 0.48 (0.45, 0.51) 0.027 (0.026, 0.028) 94.36 10.6\nEnsemble 7.73 (7.14, 8.32) 6.06 (5.64, 6.48) 21.58 28.3\nAeqitas has a completely automated module which guarantees a decrease in the percentage of discriminatory inputs in I. The discriminatory inputs, as discovered by Aeqitas, were systematically added to the training dataset (cf. Section 4.4). The results of retraining the classifiers appear in Table 5. In general, retraining the classifiers is not significantly time consuming. In particular, each classifier was retrained within an hour. For some classifiers, such as the SVM, our retraining scheme only took a few minutes.\nWe leverage the law of large numbers (LLN) from statistical theory to estimate the percentage of discriminatory inputs in I (cf. Section 4.3). In particular, we randomly sample a large number of inputs from I and compute the ratio of discriminatory inputs to the total inputs sampled. This experiment is repeated a large number of times and the average of the computed ratio is used as the estimate for the percentage of discriminatory inputs in I. We note from statistical theory that as the number of experiment is repeated a large number of times, the average of the computed ratio should be close to the expected fraction of discriminatory inputs in I. We also compute the 95% confidence interval estimate for the\npercentage of discriminatory inputs in I. It is useful to note that these intervals are fairly tight and that adds to the confidence we have in our point estimates as well.\nAs observed from Table 5, Aeqitas is effective in reducing the percentage of discriminatory inputs in I for all the classifiers under test. Specifically, we observe an average improvement of 43.2%, in terms of reducing the discriminatory inputs. Using our retraining module, we added an average of only 7463 datapoints (22.92% of the original training data) to achieve the result obtained in Table 5.\nFinding: Retraining using Aeqitas lowers the discrimination percentage in I by an average of 43.2% and up to 94.36%."
    },
    {
      "heading": "6 RELATEDWORK",
      "text": "In this section, we review the related literature and position our work on fairness testing. Fair Machine Learning Models The machine learning research community have turned their attention on designing classifiers that avoid discrimination [2, 4, 6, 9, 19]. These works primarily focus on the theoretical aspects of classifier models to achieve fairness in the classification process. Such a goal is either achieved by preprocessing training data or by modifying existing classifiers to limit discrimination. Our work is complementary to the approaches that aim to design fair machine-learning models. We introduce an efficient way to search the input domain of classifiers whose goal is to achieve fairness in decision making. We wish to provide a mechanism for these classifiers to quickly evaluate their fairness properties and help improve their fairness in decision making via retraining, if necessary. Fairness Testing From the software engineering point of view, the research on validating the fairness of machine-learning models is still at its infancy. A recent work [5] along this line of research defines software fairness and discrimination, including a causalitybased approach to algorithmic fairness. However, in contrast to our Aeqitas approach, the focus of this work is more on defining fairness and tests were generated in random [5]. In particular, Aeqitas can be used as a directed test generation module to uncover discriminatory inputs and discovery of these inputs is essential to understand individual fairness [2] of a machine-learning model. In addition to this and unlike existing approach [5], Aeqitas provides a module to automatically retrain the machine-learning models and reduce discrimination in the decisions made by these models.\nTesting and Verification of Machine Learning models DeepXplore [16] is a whitebox differential testing algorithm for systematically finding inputs that can trigger inconsistencies between multiple deep neural networks (DNNs). The neuron coverage was used as a systematic metric for measuring how much of the internal logic of a DNNs have been tested. More recently, DeepTest [17] leverages metamorphic relations to identify erroneous behaviors in a DNN. The usage of metamorphic relations somewhat solves the limitation of differential testing, especially to lift the requirement of having multiple DNNs implementing the same functionality. Finally, a feature-guided black-box approach is proposed recently to validate the safety of deep neural networks [18]. This work uses their method to evaluate the robustness of neural networks in safety-critical applications such as traffic sign recognition.\nThe objective of these works, as explained in the preceding paragraph, is largely to evaluate the robustness property of a given machine-learning model. In contrast, we are interested in the fairness property, which is fundamentally different from robustness. Therefore, validating fairness requires special attention along the line of systematic test generation. Search based testing Search-based testing has a long and varied history. The most common techniques are hill climbing, simulated annealing and genetic algorithms [11]. These have been applied extensively to test applications that largely fall in the class of deterministic software systems. Aeqitas is the first instance in our knowledge that employs a novel search algorithm to test the fairness of machine-learning systems. We believe that we can port Aeqitas for the usage in a much wider machine-learning context."
    },
    {
      "heading": "7 THREATS TO VALIDITY",
      "text": "The effectiveness and efficiency of Aeqitas critically depends on the following factors: Robustness: Our Aeqitas approach is based on the hypothesis that the machine-learning models under test exhibit robustness. This is a reasonable assumption, as we expect the models under test to be deployed in production settings. As evidenced by our evaluation, Aeqitas approach, which is based on the aforementioned hypothesis, was effective to localize the search in the vicinity of discriminatory input regions for state-of-the-art models. Training data and access tomodel: Aeqitas needs access to the training data and the training mechanism of the machine-learning model to be able to evaluate and retrain the model. Without access to the training data, Aeqitas will not be able to successfully improve the fairness of the model. This is because Aeqitas is used to generate test inputs that violate fairness and augment the original training set to improve the model under test. The generated test inputs, however, is not sufficient to train a machine-learning model from scratch. Input Structure: Aeqitas works on real-valued inputs. Aeqitas, in its current form, does not handle image, sound or video inputs. This, however, does not diminish the applicability of Aeqitas. Numerous real-world applications still use only real-valued data for prediction. These include applications in finance, security, social welfare, education, healthcare and human resources. Examples of applications include income prediction, crime prediction, disease prediction, job short-listing and college short-listing, among others.\nFor models that take inputs such as images and videos, we need to incorporate additional techniques for automatically generating valid input data. However, we believe that the core idea behind our Aeqitas approach, namely the global and the local search employed over the input space, will still remain valid. Probability change parameter: The users of Aeqitas will have to experiment and carefully choose \u2206v and \u2206pr values which change the probabilities of choosing p (i.e. the input parameter to perturb) and \u03b4 (i.e. the perturbation value). If \u2206v (respectively, \u2206pr ) is too high, then an overshoot might occur and a certain discriminatory input region may never be explored. If \u2206v (respectively, \u2206pr ) is too low, then the effectiveness of Aeqitas semi-directed and Aeqitas fully-directed would be very similar to Aeqitas random. In our experiments, we evaluated with a few \u2206v and \u2206pr values before our results stabilized. Limited discriminatory input features: We evaluate Aeqitas with discriminatory input feature gender. Hence, we cannot conclude the effectiveness of Aeqitas for other potentially discriminatory input features. However, the mechanism behind Aeqitas is generic and allows extensive evaluation for other discriminatory input features in a future extension of the tool."
    },
    {
      "heading": "8 CONCLUSION",
      "text": "In this paper, we propose Aeqitas\u2013 a fully automated and directed test generation strategy to rapidly generate discriminatory inputs in machine-learning models. The key insight behind Aeqitas is to exploit the robustness property of common machine learning models and use it to systematically direct the test generation process. Aeqitas provides statistical evidence on the number of discriminatory inputs in a model under test. Moreover, Aeqitas incorporates strategies to systematically leverage the generated test inputs to improve the fairness of the model. We evaluate Aeqitas with state-of-the-art classifiers and demonstrate that Aeqitas is effective in generating discriminatory test inputs as well as improving the fairness of machine-learning models. At its current state, however, Aeqitas does not have the capability to localize the cause of discrimination in a model. Further work is required to isolate the cause of discrimination in the model.\nAeqitas provides capabilities to lift the state-of-the-art in testing machine-learning models. We envision to extend our Aeqitas approach beyond fairness testing and for machine-learning models taking complex inputs including images and videos. We hope that the central idea behind our Aeqitas approach would influence the rigorous software engineering principles and help validate machinelearning applications used in sensitive domains. For reproducibility and advancing the state of research, we have made our tool and all experimental data publicly available:\nhttps://github.com/sakshiudeshi/Aequitas"
    },
    {
      "heading": "ACKNOWLEDGMENT",
      "text": "The authors would like to thank Chundong Wang and the anonymous reviewers for their insightful comments. The first author is supported by the President\u2019s Graduate Fellowship funded by the Ministry of Education, Singapore."
    }
  ],
  "title": "Automated Directed Fairness Testing",
  "year": 2018
}
