{
  "abstractText": "Background / introduction. Vector symbolic architectures (VSA) are a viable approach for the hyperdimensional representation of symbolic data, such as documents, syntactic structures, or semantic frames. Methods. We present a rigorous mathematical framework for the representation of phrase structure trees and parse trees of context-free grammars (CFG) in Fock space, i.e. infinite-dimensional Hilbert space as being used in quantum field theory. We define a novel normal form for CFG by means of term algebras. Using a recently developed software toolbox, called FockBox, we construct Fock space representations for \u2217Corresponding author Peter beim Graben Bernstein Center for Computational Neuroscience, Berlin, Germany Peter beim Graben \u00b7Markus Huber \u00b7Werner Meyer \u00b7 Ronald R\u00f6mer \u00b7Matthias Wolff Department of Communication Engineering Brandenburgische Technische Universit\u00e4t (BTU) Cottbus\u2013Senftenberg Platz der Deutschen Einheit 1 D \u2013 03046 Cottbus E-mail: peter.beimgraben@b-tu.de ar X iv :2 00 3. 05 17 1v 2 [ cs .C L ] 2 5 Se p 20 20 2 beim Graben et al. the trees built up by a CFG left-corner (LC) parser. Results. We prove a universal representation theorem for CFG term algebras in Fock space and illustrate our findings through a low-dimensional principal component projection of the LC parser states. Conclusions. Our approach could leverage the development of VSA for explainable artificial intelligence (XAI) by means of hyperdimensional deep neural computation. It could be of significance for the improvement of cognitive user interfaces and other applications of VSA in machine learning.",
  "authors": [
    {
      "affiliations": [],
      "name": "Peter beim Graben"
    },
    {
      "affiliations": [],
      "name": "Markus Huber"
    },
    {
      "affiliations": [],
      "name": "Werner Meyer"
    },
    {
      "affiliations": [],
      "name": "Ronald R\u00f6mer"
    },
    {
      "affiliations": [],
      "name": "Matthias Wolff"
    }
  ],
  "id": "SP:df3d1e788a3141187ad28422f5b651580f8d5aa0",
  "references": [
    {
      "authors": [
        "C.E. Shannon"
      ],
      "title": "Computers and automata",
      "venue": "Proceedings of the Institute of Radio Engineering, 41(10):1234 \u2013 1241,",
      "year": 1953
    },
    {
      "authors": [
        "S. Young"
      ],
      "title": "Cognitive user interfaces",
      "venue": "IEEE Signal Processing Magazine, 27(3):128 \u2013140,",
      "year": 2010
    },
    {
      "authors": [
        "S. Haykin"
      ],
      "title": "Cognitive Dynamic Systems",
      "venue": "Cambridge University Press,",
      "year": 2012
    },
    {
      "authors": [
        "R. R\u00f6mer"
      ],
      "title": "P",
      "venue": "beim Graben, M. Huber, M. Wolff, G. Wirsching, and I. Schmitt. Behavioral control of cognitive agents using database semantics and minimalist grammars. In Proceedingsd of the 10th IEEE International Conference on Cognitive Infocommunications (CogInfoCom), pages 73 \u2013 78,",
      "year": 2019
    },
    {
      "authors": [
        "L. Karttunen"
      ],
      "title": "Features and values",
      "venue": "In Proceedings of the 10th International Conference on Computational Linguistics, pages 28 \u2013 33, Stroudsburg (PA),",
      "year": 1984
    },
    {
      "authors": [
        "B.F. Skinner"
      ],
      "title": "Verbal Behavior",
      "venue": "Martino Publishing, Mansfield Centre (CT), 2015. 1st Edition",
      "year": 1957
    },
    {
      "authors": [
        "K. Steinbuch",
        "E. Schmitt"
      ],
      "title": "Adaptive systems using learning matrices",
      "venue": "In H. L. Oestericicher and D. R. Moore, editors, Biocybernetics in Avionics, pages 751 \u2013 768. Gordon and Breach, New York,",
      "year": 1967
    },
    {
      "authors": [
        "Y. LeCun",
        "Y. Bengio",
        "G. Hinton"
      ],
      "title": "Deep learning",
      "venue": "Nature, 521(7553):436 \u2013 444,",
      "year": 2015
    },
    {
      "authors": [
        "J. Schmidhuber"
      ],
      "title": "Deep learning in neural networks: An overview",
      "venue": "Neural Networks, 61: 85 \u2013 117,",
      "year": 2015
    },
    {
      "authors": [
        "P. Smolensky"
      ],
      "title": "Tensor product variable binding and the representation of symbolic structures in connectionist systems",
      "venue": "Artificial Intelligence, 46(1-2):159 \u2013 216,",
      "year": 1990
    },
    {
      "authors": [
        "E. Mizraji"
      ],
      "title": "Context-dependent associations in linear distributed memories",
      "venue": "Bulletin of Mathematical Biology, 51(2):195 \u2013 205,",
      "year": 1989
    },
    {
      "authors": [
        "T.A. Plate"
      ],
      "title": "Holographic reduced representations",
      "venue": "IEEE Transactions on Neural Networks, 6(3):623 \u2013 641,",
      "year": 1995
    },
    {
      "authors": [
        "P. beim Graben",
        "R. Potthast"
      ],
      "title": "Inverse problems in dynamic cognitive modeling",
      "venue": "Chaos, 19(1):015103,",
      "year": 2009
    },
    {
      "authors": [
        "P. Kanerva"
      ],
      "title": "Hyperdimensional computing: An introduction to computing in distributed representation with high-dimensional random vectors",
      "venue": "Cognitive Computation, 1(2): 139 \u2013 159,",
      "year": 2009
    },
    {
      "authors": [
        "R.W. Gayler"
      ],
      "title": "Vector symbolic architectures are a viable alternative for Jackendoff\u2019s challenges",
      "venue": "Behavioral and Brain Sciences, 29:78 \u2013 79, 2",
      "year": 2006
    },
    {
      "authors": [
        "S.D. Levy",
        "R. Gayler"
      ],
      "title": "Vector Symbolic Architectures: A new building material for artificial general intelligence",
      "venue": "In Proceedings of the Conference on Artificial General Intelligence, pages 414 \u2013 418,",
      "year": 2008
    },
    {
      "authors": [
        "Y. Bengio",
        "A. Courville",
        "P. Vincent"
      ],
      "title": "Representation learning: A review and new perspectives",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, 35(8): 1798 \u2013 1828,",
      "year": 2013
    },
    {
      "authors": [
        "M.N. Jones",
        "D.J.K. Mewhort"
      ],
      "title": "Representing word meaning and order information in a composite holographic lexicon",
      "venue": "Psychological Review, 114(1):1 \u2013 37,",
      "year": 2007
    },
    {
      "authors": [
        "I. Schmitt",
        "G. Wirsching",
        "M. Wolff"
      ],
      "title": "Quantum-based modelling of database states",
      "venue": "In D. Aerts, A. Khrennikov, M. Melucci, and T. Bourama, editors, Quantum-Like Models for Information Retrieval and Decision-Making, STEAM-H: Science, Technology, Engineering, Agriculture, Mathematics & Health, pages 115 \u2013 127. Springer, Cham, 26 beim Graben et al.",
      "year": 2019
    },
    {
      "authors": [
        "G. Recchia",
        "M. Sahlgren",
        "P. Kanerva",
        "M.N. Jones"
      ],
      "title": "Encoding sequential information in semantic space models: Comparing holographic reduced representation and random permutation",
      "venue": "Computational Intelligence and Neuroscience, 2015:58,",
      "year": 2015
    },
    {
      "authors": [
        "B. Emruli",
        "R.W. Gayler",
        "F. Sandin"
      ],
      "title": "Analogical mapping and inference with binary spatter codes and sparse distributed memory",
      "venue": "In Proceedings of the International Joint Conference on Neural Networks (IJCNN), pages 1 \u2013 8,",
      "year": 2013
    },
    {
      "authors": [
        "D. Widdows",
        "T. Cohen"
      ],
      "title": "Reasoning with vectors: A continuous model for fast robust inference",
      "venue": "Logic Journal of the IGPL, 23(2):141 \u2013 173, 11",
      "year": 2014
    },
    {
      "authors": [
        "E. Mizraji"
      ],
      "title": "Vector logic allows counterfactual virtualization by the square root of NOT",
      "venue": "Logic Journal of the IGPL, 07",
      "year": 2020
    },
    {
      "authors": [
        "D. Kleyko",
        "E. Osipov",
        "R.W. Gayler"
      ],
      "title": "Recognizing permuted words with vector symbolic architectures: A Cambridge test for machines",
      "venue": "Procedia Computer Science, 88:169 \u2013 175,",
      "year": 2016
    },
    {
      "authors": [
        "V.I. Gritsenko",
        "D.A. Rachkovskij",
        "A.A. Frolov",
        "R. Gayler",
        "D. Kleyko",
        "E. Osipov"
      ],
      "title": "Neural distributed autoassociative memories : A survey",
      "venue": "Cybernetics and Computer Engineering Journal, 188(2):5 \u2013 35,",
      "year": 2017
    },
    {
      "authors": [
        "E. Mizraji",
        "A. Pomi",
        "J. Lin"
      ],
      "title": "Improving neural models of language with inputoutput tensor contexts",
      "venue": "In A. Karpov, O. Jokisch, and R. Potapova, editors, Speech and Computer, pages 430 \u2013 440, Cham,",
      "year": 2018
    },
    {
      "authors": [
        "M. Wolff",
        "M. Huber",
        "G. Wirsching",
        "R. R\u00f6mer"
      ],
      "title": "P",
      "venue": "beim Graben, and I. Schmitt. Towards a quantum mechanical model of the inner stage of cognitive agents. In Proceedings of the 9th IEEE International Conference on Cognitive Infocommunications (CogInfoCom), pages 000147 \u2013 000152,",
      "year": 2018
    },
    {
      "authors": [
        "G.S. Carmantini"
      ],
      "title": "P",
      "venue": "beim Graben, M. Desroches, and S. Rodrigues. A modular architecture for transparent computation in recurrent neural networks. Neural Networks, 85: 85 \u2013 105,",
      "year": 2017
    },
    {
      "authors": [
        "J.E. Hopcroft",
        "J.D. Ullman"
      ],
      "title": "Introduction to Automata Theory, Languages, and Computation",
      "venue": "Addison\u2013Wesley, Menlo Park, California,",
      "year": 1979
    },
    {
      "authors": [
        "D.W. Otter",
        "J.R. Medina",
        "J.K. Kalita"
      ],
      "title": "A survey of the usages of deep learning for natural language processing",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems, pages 1 \u2013 21,",
      "year": 2020
    },
    {
      "authors": [
        "Y. Goldberg"
      ],
      "title": "Neural network methods for natural language processing, volume 10 of Synthesis Lectures on Human Language Technologies",
      "venue": "Morgan & Claypool, Williston,",
      "year": 2017
    },
    {
      "authors": [
        "S. Minaee",
        "N. Kalchbrenner",
        "E. Cambria",
        "N. Nikzad",
        "M. Chenaghlu",
        "J. Gao"
      ],
      "title": "Deep learning based text classification: A comprehensive review",
      "venue": "arXiv:2004.03705 [cs.CL],",
      "year": 2020
    },
    {
      "authors": [
        "C.-H. Chen",
        "V. Honavar"
      ],
      "title": "A neural network architecture for syntax analysis",
      "venue": "IEEE Transactions on Neural Networks, 10:91 \u2013 114,",
      "year": 1999
    },
    {
      "authors": [
        "J.B. Pollack"
      ],
      "title": "The induction of dynamical recognizers",
      "venue": "Machine Learning, 7:227 \u2013 252,",
      "year": 1991
    },
    {
      "authors": [
        "H.T. Siegelmann",
        "E.D. Sontag"
      ],
      "title": "On the computational power of neural nets",
      "venue": "Journal of Computer and System Sciences, 50(1):132 \u2013 150,",
      "year": 1995
    },
    {
      "authors": [
        "R. Socher",
        "C.D. Manning",
        "A.Y. Ng"
      ],
      "title": "Learning continuous phrase representations and syntactic parsing with recursive neural networks",
      "venue": "In Proceedings of the NIPS 2010 Deep Learning And Unsupervised Feature Learning Workshop, volume 2010, pages 1 \u2013 9,",
      "year": 2010
    },
    {
      "authors": [
        "S. Hochreiter",
        "J. Schmidhuber"
      ],
      "title": "Long short-term memory",
      "venue": "Neural Computation, 9 (8):1735 \u2013 1780,",
      "year": 1997
    },
    {
      "authors": [
        "D. Hupkes",
        "V. Dankers",
        "M. Mul"
      ],
      "title": "and E",
      "venue": "Bruni. Compositionality decomposed: How do neural networks generalise? Journal of Artificial Intelligence Research, 67:757 \u2013 795,",
      "year": 2020
    },
    {
      "authors": [
        "Y.N. Dauphin",
        "A. Fan",
        "M. Auli",
        "D. Grangier"
      ],
      "title": "Language modeling with gated convolutional networks",
      "venue": "arXiv:1612.08083 [cs.CL],",
      "year": 2016
    },
    {
      "authors": [
        "M.K. Patrick",
        "A.F. Adekoya",
        "A.A. Mighty",
        "B.Y. Edward"
      ],
      "title": "Capsule networks \u2014 a survey",
      "venue": "Journal of King Saud University,",
      "year": 2019
    },
    {
      "authors": [
        "M. Yang",
        "W. Zhao",
        "L. Chen",
        "Q. Qu",
        "Z. Zhao",
        "Y. Shen"
      ],
      "title": "Investigating the transferring capability of capsule networks for text classification",
      "venue": "Neural Networks, 118:247 \u2013 261,",
      "year": 2019
    },
    {
      "authors": [
        "H. Palangi",
        "P. Smolensky",
        "X. He",
        "L. Deng"
      ],
      "title": "Deep learning of grammaticallyinterpretable representations through question-answering",
      "venue": "arXiv:1705.08432,",
      "year": 2017
    },
    {
      "authors": [
        "H. Palangi",
        "P. Smolensky",
        "X. He",
        "L. Deng"
      ],
      "title": "Question-answering with grammaticallyinterpretable representations",
      "venue": "In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence (AAAI-18),",
      "year": 2018
    },
    {
      "authors": [
        "S. Tang",
        "P. Smolensky"
      ],
      "title": "and V",
      "venue": "R. de Sa. A simple recurrent unit with reduced tensor product representations. In Proceedings of ICLR 2020,",
      "year": 2019
    },
    {
      "authors": [
        "G. Marcus"
      ],
      "title": "The next decade in AI: Four steps towards robust artificial intelligence",
      "venue": "arXiv:2002.06177 [cs.AI],",
      "year": 2020
    },
    {
      "authors": [
        "D. Doran",
        "S. Schulz",
        "T.R. Besold"
      ],
      "title": "What does explainable AI really mean? A new conceptualization of perspectives",
      "venue": "arXiv:1710.00794 [cs.AI],",
      "year": 2017
    },
    {
      "authors": [
        "G. Montavon",
        "W. Samek",
        "K.-R. M\u00fcller"
      ],
      "title": "Methods for interpreting and understanding deep neural networks",
      "venue": "Digital Signal Processing, 73:1 \u2013 15,",
      "year": 2018
    },
    {
      "authors": [
        "V. Fock"
      ],
      "title": "Konfigurationsraum und zweite Quantelung",
      "venue": "Zeitschrift f\u00fcr Physik, 75(9):622 \u2013 647,",
      "year": 1932
    },
    {
      "authors": [
        "D. Aerts"
      ],
      "title": "Quantum structure in cognition",
      "venue": "Journal of Mathematical Psychology, 53(5): 314 \u2013 348,",
      "year": 2009
    },
    {
      "authors": [
        "M. Wolff",
        "G. Wirsching",
        "M. Huber"
      ],
      "title": "P",
      "venue": "beim Graben, R. R\u00f6mer, and I. Schmitt. A Fock space toolbox and some applications in computational cognition. In A. Karpov, O. Jokisch, and R. Potapova, editors, Speech and Computer, pages 757 \u2013 767, Cham,",
      "year": 2018
    },
    {
      "authors": [
        "M. Kracht"
      ],
      "title": "The Mathematics of Language",
      "venue": "Number 63 in Studies in Generative Grammar. Mouton de Gruyter, Berlin,",
      "year": 2003
    },
    {
      "authors": [
        "J.T. Hale"
      ],
      "title": "What a rational parser would do",
      "venue": "Cognitive Science, 35(3):399 \u2013 443,",
      "year": 2011
    },
    {
      "authors": [
        "P. Wegner"
      ],
      "title": "Interactive foundations of computing",
      "venue": "Theoretical Computer Science, 192: 315 \u2013 351,",
      "year": 1998
    },
    {
      "authors": [
        "P. G\u00e4rdenfors"
      ],
      "title": "Knowledge in Flux",
      "venue": "Modeling the Dynamics of Epistemic States. MIT Press, Cambridge (MA),",
      "year": 1988
    },
    {
      "authors": [
        "J. Groenendijk",
        "M. Stokhof"
      ],
      "title": "Dynamic predicate logic",
      "venue": "Linguistics and Philosophy, 14(1):39 \u2013 100,",
      "year": 1991
    },
    {
      "authors": [
        "M. Kracht"
      ],
      "title": "Dynamic semantics",
      "venue": "Linguistische Berichte, Sonderheft X:217 \u2013 241,",
      "year": 2002
    },
    {
      "authors": [
        "P. Smolensky"
      ],
      "title": "Harmony in linguistic cognition",
      "venue": "Cognitive Science, 30:779 \u2013 801,",
      "year": 2006
    },
    {
      "authors": [
        "P. Kanerva"
      ],
      "title": "The binary spatter code for encoding concepts at many levels",
      "venue": "In M. Marinaro and P. Morasso, editors, Proceedings of International Conference on Artificial Neural Networks (ICANN 1994), volume 1, pages 226 \u2013 229, London,",
      "year": 1994
    },
    {
      "authors": [
        "P.A.M. Dirac"
      ],
      "title": "A new notation for quantum mechanics",
      "venue": "Mathematical Proceedings of the Cambridge Philosophical Society, 35(3):416 \u2013 418,",
      "year": 1939
    },
    {
      "authors": [
        "P. Smolensky"
      ],
      "title": "Symbolic functions from neural computation",
      "venue": "Philosophical Transactions of the Royal Society London, A 370(1971):3543 \u2013 3569,",
      "year": 2012
    },
    {
      "authors": [
        "M. Huber",
        "M. Wolff",
        "W. Meyer",
        "O. Jokisch",
        "K. Nowack"
      ],
      "title": "Some design aspects of a cognitive user interface",
      "venue": "Online Journal of Applied Knowledge Management, 6(1):15 \u2013 29,",
      "year": 2018
    },
    {
      "authors": [
        "M. Minsky"
      ],
      "title": "A framework for representing knowledge",
      "venue": "Technical Report AIM-306, M.I.T., Cambridge (MA),",
      "year": 1974
    },
    {
      "authors": [
        "J.F. Allen"
      ],
      "title": "Natural language processing",
      "venue": "In Encyclopedia of Computer Science, pages 1218 \u2013 1222. Wiley, Chichester (UK),",
      "year": 2003
    },
    {
      "authors": [
        "G. Tur",
        "D. Hakkani-T\u00fcr",
        "L. Heck",
        "S. Parthasarathy"
      ],
      "title": "Sentence simplification for spoken language understanding",
      "venue": "In Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5628 \u2013 5631,",
      "year": 2011
    },
    {
      "authors": [
        "G. Mesnil",
        "Y. Dauphin",
        "K. Yao",
        "Y. Bengio",
        "L. Deng",
        "D. Hakkani-Tur",
        "X. He",
        "L. Heck",
        "G. Tur",
        "D. Yu",
        "G. Zweig"
      ],
      "title": "Using recurrent neural networks for slot filling in spoken language understanding",
      "venue": "IEEE Transactions on Audio, Speech and Language Processing, 23(3):530 \u2013 539,",
      "year": 2015
    },
    {
      "authors": [
        "J. Allen"
      ],
      "title": "Dialogue as collaborative problem solving",
      "venue": "In Proceedings of Interspeech Conference, page 833,",
      "year": 2017
    },
    {
      "authors": [
        "J.F. Allen",
        "O. Bahkshandeh"
      ],
      "title": "W",
      "venue": "de Beaumont, L. Galescu, and C. M. Teng. Effective broad-coverage deep parsing. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence,",
      "year": 2018
    },
    {
      "authors": [
        "R.F. Por"
      ],
      "title": "and T",
      "venue": "van Gelder, editors. Mind as Motion: Explorations in the Dynamics of Cognition. MIT Press, Cambridge (MA),",
      "year": 1995
    }
  ],
  "sections": [
    {
      "text": "proach for the hyperdimensional representation of symbolic data, such as documents, syntactic structures, or semantic frames. Methods. We present a rigorous mathematical framework for the representation of phrase structure trees and parse trees of context-free grammars (CFG) in Fock space, i.e. infinite-dimensional Hilbert space as being used in quantum field theory. We define a novel normal form for CFG by means of term algebras. Using a recently developed software toolbox, called FockBox, we construct Fock space representations for \u2217Corresponding author Peter beim Graben Bernstein Center for Computational Neuroscience, Berlin, Germany\nPeter beim Graben \u00b7Markus Huber \u00b7Werner Meyer \u00b7 Ronald Ro\u0308mer \u00b7Matthias Wolff Department of Communication Engineering Brandenburgische Technische Universita\u0308t (BTU) Cottbus\u2013Senftenberg Platz der Deutschen Einheit 1 D \u2013 03046 Cottbus E-mail: peter.beimgraben@b-tu.de ar X iv :2\n00 3.\n05 17\n1v 2\n[ cs\n.C L\n] 2\n5 Se\nthe trees built up by a CFG left-corner (LC) parser. Results. We prove a universal representation theorem for CFG term algebras in Fock space and illustrate our findings through a low-dimensional principal component projection of the LC parser states. Conclusions. Our approach could leverage the development of VSA for explainable artificial intelligence (XAI) by means of hyperdimensional deep neural computation. It could be of significance for the improvement of cognitive user interfaces and other applications of VSA in machine learning.\nKeywords Geometric cognition, formal grammars, language processing, vector symbolic architectures, Fock space, explainable artificial intelligence (XAI)"
    },
    {
      "heading": "1 Introduction",
      "text": "Claude E. Shannon, the pioneer of information theory, presented in 1952 a \u201cmaze-solving machine\u201d as one of the first proper technical cognitive systems [1].1 It comprises a maze in form of a rectangular board partitioned into discrete cells that are partially separated by removable walls, and a magnetized \u201cmouse\u201d (nicknamed \u201cTheseus\u201d, after the ancient Greek hero) as a cognitive agent. The mouse possesses as an actuator a motorized electromagnet beneath the maze board. The magnet pulls the mouse through the maze. Sensation and memory are implemented by a circuit of relays, switching their states after encounters with a wall. In this way, Shannon technically realized a simple, non-hierarchic perception-action cycle (PAC) [2], quite similar to the more sophisticated version depicted in Fig. 1 as a viable generalization of a cybernetic feedback loop.\n1 See also Shannon\u2019s instructive video demonstration at https://www.youtube.com/watch?v=\nvPKkXibQXGA.\nIn general, PAC form the core of a cognitive dynamic system [2, 3]. They describe the interaction of a cognitive agent with a dynamically changing world as shown in Fig. 1. The agent is equipped with sensors for the perception of its current state in the environment and with actuators allowing for active state changes. A central control prescribes goals and strategies for problem solving that could be trained by either trial-and-error learning as in Shannon\u2019s construction, or, more generally, by reinforcement learning [3].\nIn Shannon\u2019s mouse-maze system, the motor (the actuator) pulls the mouse along a path until it bumps into a wall which is registered by a sensor. This perception is stored by switching a relay, subsequently avoiding the corresponding action. The behavior control prescribes a certain maze cell where the agent may find a \u201cpiece of cheese\u201d as a goal. When the goal is eventually reached, no further action is necessary. In a first run, the mouse follows an irregular path according to a trial-and-error strategy, while building up a memory trace in the relay array. In every further run, the successfully learned path is pursued at once. However, when the operator modifies the arrangement of walls, the previously learned path\nbecomes useless and the agent has to learn from the very beginning. Therefore, Shannon [1, p. 1238] concludes:\nThe maze-solver may be said to exhibit at a very primitive level the abilities to (1) solve problems by trial and error, (2) repeat the solutions without the errors, (3) add and correlate new information to a partial solution, (4) forget a solution when it is no longer applicable.\nIn Shannon\u2019s original approach, the mouse learns by trial-and-error whenever it bumps into a wall. More sophisticated cognitive dynamic systems should be able to draw logical inferences and to communicate either with each other or with an external operator, respectively [4]. This requires higher levels of mental representations such as formal logics and grammars. Consider, e.g., the operator\u2019s utterance:\nthe mouse ate cheese (1)\n(note that symbols will be set in typewriter font in order to abstract from their conventional meaning in the first place). In the PAC described in Fig. 1, the acoustic signal has firstly to be analyzed in order to obtain a phonetic string representation. For understanding its meaning, the agent has secondly to process the utterance grammatically through syntactic parsing. Finally, the syntactic representation, e.g. in form of a phrase structure tree, must be interpreted as a semantic representation which the agent can ultimately understand [5]. Depending upon such understanding, the agent can draw logical inferences and derive the appropriate behavior for controlling the actuators. In case of verbal behavior [6], the agent therefore computes an appropriate response, first as a semantic representation, that is articulated into a syntactic and phonetic form and finally synthesized as an acoustic signal. In any\ncase, high-level representations are symbolic and their processing is rule-driven, in contrast to low-level sensation and actuation where physical signals are essentially continuous.\nOriginally, Shannon used an array of relays as the agent\u2019s memory. This has later been termed the \u201clearning matrix\u201d by Steinbuch and Schmitt [7]. Learning matrices and vector symbolic architectures (VSA) provide viable interfaces between hierarchically organized symbolic data structures such as phrase structure trees or semantic representations and continuous state space approaches as required for deep neural networks (DNN) [8, 9]. Beginning with seminal studies by Smolensky [10] and Mizraji [11], and later pursued by Plate [12], beim Graben and Potthast [13], and Kanerva [14] among many others, those architectures have been dubbed VSA by Gayler [15] (cf. also [16]).\nIn a VSA, symbols and variables are represented as filler and role vectors of some underlying linear embedding spaces [17, 18], respectively. When a symbol is assigned to a variable, the corresponding filler vector is bound to the corresponding role vector. Different filler-role bindings can be bundled together to form a data structure [16], such as a list, a frame, or a table of a relational data base [19]. Those structures can be recursively bound to other fillers and further bundled together to yield arbitrarily complex data structures [13].\nVSA have recently been employed for semantic spaces [18, 20], logical inferences [21, 22, 23], data base queries [19, 24], and autoassociative memories [25, 26]. Wolff et al. [27] developed a VSA model for cognitive representations and their induction in Shannon\u2019s mouse-maze system. In the present study, we focus on the dashed region in Fig. 1, by elaborating earlier approaches for VSA language processors [13, 28]. Specifically, we discuss vector space representations of context-free grammars (CFG) and push-down automata [29], as used in current speech and language technologies [17, 30, 31, 32].\nDeploying neural networks in language technology became increasingly important in recent time. Beginning with hard-wired recurrent neural architectures [33, 34, 35, 28], the advent of deep learning algorithms lead to state-of-the-art language processing through recursive neural networks (RNN, [36]), through long-short-term memory networks (LSTM, [37, 38]), and through convolutional neural networks (CNN, [8, 39]), with their most recent improvements, capsule networks [40, 41]; for a survey consult [17, 30, 31, 32]. Particularly interesting are latest attempts of Smolensky and collaborators to merge VSA and DNN into tensor product recurrent networks (TPRN, [42, 43, 44]) which are able to directly learn fillerrole bindings by end-to-end training under a special quantization regularization constraint.\nDespite these impressive achievements, DNN are intrinsic black-box models, propagating input patterns through their hidden layers toward the associated output patterns. The hidden layers may have several hundred-thousands up to some billions synaptic weight parameters that are trained by regularized gradient climbing algorithms. After training, the network develops a hidden representation of the input features and the computational rules to transform them into output. Yet these representations are completely opaque and nobody can explain how the input is mapped onto the output [8].\nTherefore, according to Marcus [45], the next-generation AI, must be explainable, robust and trustworthy. Creating explainable AI (XAI) [46] is an important challenge for current research [47]. For this aim, it is mandatory not only to develop new algorithms and networks architectures, such as TPRN [42, 43, 44], e.g., but also conceptual understanding of their formal structures. To this end, we present rigorous proofs for vector space representations of context-free grammars (CFG) and push-down automata. We suggest a novel normal form for CFG, allowing to express CFG parse trees as terms over a symbolic term algebra. Rule-based derivations over that algebra are then represented as transformation matrices in Fock space\n[48, 49]. Our approach could lead to the development of new machine learning algorithms for training neural networks as rule-based symbol processors. In contrast to black-box DNN, our method is essentially transparent and hence explainable and trustworthy."
    },
    {
      "heading": "2 Methods",
      "text": "We start from a symbolic, rule-based system that can be described in terms of formal grammar and automata theory. Specifically, we chose context-free grammars (CFG) and pushdown automata as their processors here [29]. In the second step, we reformulate these languages through term algebras and their processing through partial functions over term algebras. We introduce a novel normal form for CFG, called term normal form, and prove that any CFG in Chomsky normal form can be transformed into term normal form. Finally, we introduce a vector symbolic architecture by assigning basis vectors of a high-dimensional linear space to the respective symbols and their roles in a phrase structure tree. We suggest a recursive function for mapping CFG phrase structure trees onto representation vectors in Fock space and prove a representation theorem for the partial rule-based processing functions. Finally, we present a software toolbox, FockBox for handling Fock space VSA representations [50].\n2.1 Context-free Grammars\nConsider again the simple sentence (1) as a motivating example. According to linguistic theory, sentences such as (1) exhibit a hierarchical structure, indicating a logical subjectpredicate relationship. In (1) \u201cthe mouse\u201d appears as subject and the phrase \u201cate cheese\u201d\nas the predicate, which is further organized into a transitive verb \u201cate\u201d and its direct object \u201ccheese\u201d. The hierarchical structure of sentence (1) can therefore be either expressed through regular brackets, as in (2)\n[[[the] [mouse]] [ate [cheese]]] , (2)\nor, likewise as a phrase structure tree as in Fig. 2\nIn Fig. 2 every internal node of the tree denotes a syntactic category: S stands for \u201csentence\u201d, NP for \u201cnoun phrase\u201d, the sentence\u2019s subject, VP for \u201cverbal phrase\u201d, the predicate, D for \u201cdeterminer\u201d, N for \u201cnoun\u201d, and V for \u201cverb\u201d.\nThe phrase structure tree Fig. 2 immediately gives rise to a context-free grammar (CFG)\nby interpreting every branch as a rewriting rule in Chomsky normal form [51, 29]\nS\u2192 NP VP (3a)\nNP\u2192 D N (3b)\nVP\u2192 V N (3c)\nD\u2192 the (3d)\nN\u2192 mouse (3e)\nV\u2192 ate (3f)\nN\u2192 cheese (3g)\nwhere one distinguishes between syntactical rules (3a \u2013 3c) and lexical rules (3d \u2013 3g), respectively. More abstractly, a CFG is given as a quadruple G = (T,N,S,R), such that in our example T = {the,mouse,ate,cheese} is the set of words or terminal symbols, N = {S,NP,VP,D,N,V} is the set of categories or nonterminal symbols, S \u2208 N is the distinguished start symbol, and R\u2282 N\u00d7 (N\u222aT )\u2217 is a set of rules. A rule r = (A,\u03b3) \u2208 R is usually written as a production r : A\u2192 \u03b3 where A \u2208 N denotes a category and \u03b3 \u2208 (N\u222aT )\u2217 a finite string of terminals or categories of length n = |\u03b3|.\nContext-free grammars can be processed by push-down automata [29]. Regarding psycholinguistic plausibilty, the left-corner (LC) parser is particularly relevant because inputdriven bottom-up and expectation-driven top-down processes are tightly intermingled with each other [52]. An LC parser possesses, such as any other push-down automaton, two\nmemory tapes: firstly a working memory, called stack, operating in a last-in-first-out (LIFO) fashion, and an input tape storing the sentence to be processed.\nIn the most simple cases, when a given CFG does not contain ambiguities (as in (3a \u2013 3g) for our example (1)), an LC parser can work deterministically. The LC parsing algorithm operates in four different modes: i) if nothing else is possible and if the input tape is not empty, the first word of the input is shifted into the stack; ii) if the first symbol in the stack is the left corner of a syntactic rule, the first stack symbol is rewritten by a predicted category (indicated by square brackets in Tab. 1) followed by the left-hand side of the rule (project); iii) if a category in the stack was correctly predicted, the matching symbols are removed from the stack (complete); iv) if the input tape is empty and the stack only contains the start symbol of the grammar, the automaton moves into the accepting state; otherwise, syntactic language processing had failed. Applying the LC algorithm to our example CFG leads to the symbolic process shown in Tab. 1.\nThe left-corner parser shown in Tab. 1 essentially operates autonomously in modes project, complete and accept, but interactively in shift mode. Thus, we can significantly simplify the parsing process through a mapping from one intermediary automaton configuration to another one that is mediated by the interactively shifted input word [53]. Expressing the configurations as temporary phrase structure trees yields then the symbolic computation in Fig. 3.\nAccording to our previous definitions, the states of the processor are the automaton configurations in Tab. 1 or the temporary phrase structures trees in Fig. 3, that are both interpretable in terms of LC parsing and language processing for an informed expert observer. Moreover, the processing steps in the last column of Tab. 1 and also the interactive mappings Fig. 3 are understandable and thereby explainable by the observer. In principle, one could augment the left-corner parser with a \u201creasoning engine\u201d [46] that translates the formal language used in those symbolic representations into everyday language. The result would be something like the (syntactic) \u201cmeaning\u201d JwK of a word w that can be regarded as the operator mapping a tree in Fig. 3 to its successor. This interactive interpretation of meaning is well-known in dynamic semantics [54, 55, 56]. Therefore, symbolic AI is straightforwardly interpretable and explainable [46].\n2.2 Algebraic Description\nIn order to prepare the construction of a vector symbolic architecture (VSA) [10, 11, 12, 13, 14, 15] in the next step, we need an algebraically more sophisticated description. This is\nprovided by the concept of a term algebra [51]. A term algebra is defined over a signature \u03a3 = (F, rank) where F is a finite set of function symbols and rank : F \u2192 N0 is an arity function, assigning to each symbol f \u2208 F an integer indicating the number of arguments that f has to take.\nTo apply this idea to a CFG, we introduce a new kind of grammar normal form that we call term normal form in the following. A CFG G = (T,N,S,R) is said to be in term normal form when for every category A \u2208 N holds: if A is expanded into n \u2208 N rules, r1 : A\u2192 \u03b31 to rn : A\u2192 \u03b3n, then |\u03b31|= . . .= |\u03b3n|.\nIt can be easily demonstrated that every CFG can be transformed into a weakly equivalent CFG in term normal form, where weak equivalence means that two different grammars derive the same context-free language. A proof is presented in Appendix 6.1.\nObviously, the rules (3a \u2013 3c) of our example above are already in term normal form, simply because they are not ambiguous. Thus, we define a term algebra by regarding the set of variables V = N\u222aT as signature with arity function rank : V \u2192N0 such that i) rank(a) = 0 for all a \u2208 T , i.e. terminals are nullary symbols and hence constants; ii) rank(A) = |\u03b3| for categories A \u2208 N, that are expanded through rules A\u2192 \u03b3 . Moreover, when G is given in Chomsky normal form, for all categories A \u2208 N appearing exclusively in lexical rules rank(A) = 1, i.e. lexical categories (D, N, V) are unary functions. Whereas, rank(A) = 2 for all categories A \u2208 N that appear exclusively in syntactic rules, which are hence binary functions.\nFor a general CFG G in term normal form, we define the term algebra T(G) inductively: i) every terminal symbol a \u2208 T is a term, a \u2208 T(G). ii) Let A \u2208 N be a category with rank(A) = k and let t0, . . . , tk\u22121 \u2208 T(G) be terms, then A(t0, . . . , tk\u22121) \u2208 T(G) is a term. Ad-\nditionally, we want to describe LC phrase structure trees as well. To this end, we extend the signature by the predicted categories P = {[N], [VP]}, that are interpreted as constants with rank(C) = 0 for C \u2208 P. The enlarged term algebra is denoted by TLC(G). We also allow for /0 \u2208 TLC(G).\nIn the LC term algebra TLC(G), we encode the tree of step 1 in Fig. 3 (beginning with\nthe empty tree t0 = /0 in step 0) as term\nt1 = NP(D(the),[N]) (4)\nbecause rank(NP) = 2, rank(D) = 1, and rank(the) = rank([N]) = 0. Likewise we obtain\nt2 = S(NP(D(the),N(mouse)),[VP]) (5)\nas the term representation of the succeeding step 2 in Fig. 3.\nNext, we define several partial functions over TLC(G) as follows [10, 57].\ncat(A(t0, . . . , tk)) = A (6a)\nexi(A(t0, . . . , tk)) = ti (6b)\nconsk(A, t0, . . . , tk) = A(t0, . . . , tk) . (6c)\nHere, the function cat : TLC(G) \u2192 N yields the category, i.e. the function symbol A of the term A(t0, . . . , tk) \u2208 TLC(G). The functions exi : TLC(G)\u2192 TLC(G) for term extraction and consk : N\u00d7TLC(G)k+1\u2192 TLC(G) as term constructor are defined only partially, when A(t0, . . . , tk)\u2208Dom(exi), if k = rank(A)\u22121 and i< k, as well as (A, t0, . . . , tk)\u2208Dom(consk), if k = rank(A)\u22121.\nBy means of the term transformations (6a \u2013 6c) we can express the action of an incrementally and interactively shifted word a \u2208 T through a term operator JaK : TLC(G)\u2192\nTLC(G). For the transition from, e.g., LC tree 1 to LC tree 2 in Fig. 3 we obtain\nJmouseK(t1) = cons2(S,cons2(cat(t1),ex0(t1),N(mouse)),[VP]) = t2 . (7)\nTherefore, the (syntactic) meaning of the word \u201cmouse\u201d is its impact on the symbolic term algebra.\n2.3 Vector Symbolic Architectures\nIn vector-symbolic architectures (VSA) [10, 11, 12, 13, 14, 15] hierarchically organized complex data structures are represented as vectors in high dimensional linear spaces. The composition of these structures is achieved by two basic operations: binding and bundling. While bundling is commonly implemented as vector superposition, i.e. addition, different VSA realize binding in particular ways: originally through tensor products [10, 11], through circular convolution in reduced holographic representations (HRR) [12], through XOR spatter code [58] or through Hadamard products [16]. While HRR, spatter code, Hadamard products or a combination of tensor products with nonlinear compression [57] are lossy representations that require a clean-up module (usually an attractor neural network, cf. [14]), tensor product representations of basis vectors are faithful, thereby allowing interpretable and explainable VSA [46].\nComing back to our linguistic example, we construct a homomorphism \u03c8 : TLC(G)\u222a N \u2192F from the term algebra unified with its categories N to a vector space F in such a way, that the structure of the transformations (6a \u2013 6c) is preserved. The resulting images \u03c8(t) for terms t \u2208TLC(G) become vector space operators, i.e. essentially matrices acting on F .\nAgain, we proceed inductively. First we map the symbols in TLC(G)\u222aN onto vectors. To each atomic symbol s \u2208 T \u222aN \u222aP we assign a so-called filler basis vector |s\u3009= \u03c8(s) \u2208F , calling the subspace VF = span(\u03c8(T \u222aN \u222aP)) the filler space. Its dimension n = dimVF corresponds to the number of atomic symbols in T \u222aN\u222aP, which is n = 13 in our example.\nLet further m = max({|\u03b3| |(A\u2192 \u03b3) \u2208 R}) be the length of the largest production of grammar G. Then, we define m+1 so-called role vectors |i\u3009, spanning the role space VR = span({|i\u3009 |0 \u2264 i \u2264 m}). Note that we employ the so-called Dirac notation from quantum mechanics that allows a coordinate-free and hence representation-independent description here [59]. Then, the role |0\u3009 denotes the 1st daughter node, |1\u3009 the 2nd daugther and so on, until the last daughter |m\u22121\u3009. The remaining role |m\u3009 bounds the mother node in the phrase structure trees of grammar G. In our example, because G has Chomsky normal form, we have m = 2 = dimVR\u22121 such that there are three roles for positions in a binary branching tree: left daughter |0\u3009, right daughter |1\u3009, and mother |2\u3009. For binary trees, we also use a more intuitive symbolic notation: left daughter |/\u3009, right daughter |\\\u3009, and mother |\u2227\u3009.\nLet A(t0, . . . , tk) \u2208 TLC(G) be a term. Then, we define the tensor product representation\nof A(t0, . . . , tk) \u2208 TLC(G) in vector space F recursively as follows\n\u03c8(A(t0, . . . , tk)) = |A\u3009\u2297 |m\u3009\u2295\u03c8(t0)\u2297|0\u3009\u2295 \u00b7 \u00b7 \u00b7\u2295\u03c8(tk)\u2297|m\u22121\u3009 . (8)\nAs a shorthand notation, we suggest the Dirac expression\n|A(t0, . . . , tk)\u3009= |A\u3009\u2297 |m\u3009\u2295 |t0\u3009\u2297 |0\u3009\u2295 \u00b7 \u00b7 \u00b7\u2295 |tk\u3009\u2297 |m\u22121\u3009 . (9)\nHere the symbol \u201c\u2297\u201d refers to the (Kronecker) tensor product, mapping two vectors onto another vector, in contrast to the dyadic (outer) tensor product, which yields a matrix,\nhence being a vector space operator. In addition, \u201c\u2295\u201d denotes the (outer) direct sum that is mandatory for the superposition of vectors from spaces with different dimensionality.\nObviously, the (in principle) infinite recursion of the mapping \u03c8 leads to an infinite-\ndimensional representation space\nF = \u221e\u2295\np=0\n( VF \u2297V \u2297 p\nR\n) \u2295VR , (10)\nthat is known as Fock space from quantum field theory [13, 48, 49, 60].\nIn quantum field theory, there is a distinguished state |0\u3009 6= 0, the vacuum state, spanning a one-dimensional subspace, the vacuum sector that is isomorphic to the underlying number field. According to (10), this sector is contained in the subspace spanned by filler and role spaces, VF \u2295VR. Therefore, we could represent the empty tree in Fig. 3 by an arbitrary role; a suitable choice is the mother role \u03c8( /0) = |m\u3009 \u223c= |0\u3009, hence symbolizing the vacuum state.\nUsing the tensor product representation (8), we can recursively compute the images of\nour example terms above. For (4) we obtain\n|t1\u3009= |NP(D(the),[N])\u3009= |NP\u3009\u2297 |2\u3009\u2295 |D(the)\u3009\u2297 |0\u3009\u2295 |[N]\u3009\u2297 |1\u3009=\n|NP\u3009\u2297 |2\u3009\u2295 (|D\u3009\u2297 |2\u3009\u2295 |the\u3009\u2297 |0\u3009)\u2297|0\u3009\u2295 |[N]\u3009\u2297 |1\u3009=\n|NP\u3009\u2297 |2\u3009\u2295 |D\u3009\u2297 |2\u3009\u2297 |0\u3009\u2295 |the\u3009\u2297 |0\u3009\u2297 |0\u3009\u2295 |[N]\u3009\u2297 |1\u3009=\n|NP2\u3009\u2295 |D20\u3009\u2295 |the00\u3009\u2295 |[N]1\u3009=\n|NP\u2227\u3009\u2295|D\u2227/\u3009\u2295 |the//\u3009\u2295 |[N]\\\u3009 , (11)\nwhere we used the compressed Dirac notation |a\u3009\u2297|b\u3009= |ab\u3009 in the last steps. The last line is easily interpretable in terms of phrase structure: It simply states that NP occupies the root of the tree, D appears as its immediate left daughter, the is the left daughter\u2019s left daughter\nand a leave, and finally [N] is a leave bound to the right daughter of the root. Note that the Dirac kets have to be interpreted from the right to the left (reading the arabic manner). The vector |t1\u3009 belongs to a Fock subspace of dimension\nq = n mp+1\u22121\nm\u22121 +m (12)\nwhere n = dim(VF), m = dim(VR) and p the embedding depth in the phrase structure tree step 1 of Fig. 3. This leads to q1 = 172 for |t1\u3009.\nSimilarly, we get for (5)\n|t2\u3009= |S(NP(D(the),N(mouse)),[VP])\u3009=\n|S\u3009\u2297 |2\u3009\u2295 |NP(D(the),N(mouse))\u3009\u2297 |0\u3009\u2295 |[VP]\u3009\u2297 |1\u3009=\n|S\u3009\u2297 |2\u3009\u2295 (|NP\u3009\u2297 |2\u3009\u2295 |D(the)\u3009\u2297 |0\u3009\u2295 |N(mouse)\u3009\u2297 |1\u3009)\u2297|0\u3009\u2295 |[VP]\u3009\u2297 |1\u3009=\n|S\u3009\u2297 |2\u3009\u2295 |NP\u3009\u2297 |2\u3009\u2297 |0\u3009\u2295 |D(the)\u3009\u2297 |0\u3009\u2297 |0\u3009\u2295 |N(mouse)\u3009\u2297 |1\u3009\u2297 |0\u3009\u2295 |[VP]\u3009\u2297 |1\u3009=\n|S\u3009\u2297 |2\u3009\u2295 |NP\u3009\u2297 |2\u3009\u2297 |0\u3009\u2295 (|D\u3009\u2297 |2\u3009\u2295 |the\u3009\u2297 |0\u3009)\u2297|0\u3009\u2297 |0\u3009\u2295\n(|N\u3009\u2297 |2\u3009\u2295 |mouse\u3009\u2297 |0\u3009)\u2297|1\u3009\u2297 |0\u3009\u2295 |[VP]\u3009\u2297 |1\u3009=\n|S\u3009\u2297 |2\u3009\u2295 |NP\u3009\u2297 |2\u3009\u2297 |0\u3009\u2295 |D\u3009\u2297 |2\u3009\u2297 |0\u3009\u2297 |0\u3009\u2295 |the\u3009\u2297 |0\u3009\u2297 |0\u3009\u2297 |0\u3009\u2295\n|N\u3009\u2297 |2\u3009\u2297 |1\u3009\u2297 |0\u3009\u2295 |mouse\u3009\u2297 |0\u3009\u2297 |1\u3009\u2297 |0\u3009\u2295 |[VP]\u3009\u2297 |1\u3009=\n|S2\u3009\u2295 |NP20\u3009\u2295 |D200\u3009\u2295 |the000\u3009\u2295 |N210\u3009\u2295 |mouse010\u3009\u2295 |[VP]1\u3009=\n|S\u2227\u3009\u2295|NP\u2227/\u3009\u2295 |D\u2227//\u3009\u2295 |the///\u3009\u2295 |N\u2227\\/\u3009\u2295 |mouse/\\/\u3009\u2295 |[VP]\\\u3009 , (13)\nwhere we have again utilized the more intuitive branching notation in the last line which can be straightforwardly interpreted in terms of tree addresses as depicted in Fig. 3 (step 2). Computing the dimension of the respective Fock subspace according to (12) yields q2 = 523 for |t2\u3009.\nIn Fock space, the interactive and incremental action of a word a\u2208 T is then represented\nas a matrix operator JaK\u03c8 : F \u2192F . For the transition from (4) to (5) we obtain\nJmouseK\u03c8 |t1\u3009= JmouseK\u03c8(|NP\u2227\u3009\u2295|D\u2227/\u3009\u2295 |the//\u3009\u2295 |[N]\\\u3009) =\n|S\u2227\u3009\u2295|NP\u2227/\u3009\u2295 |D\u2227//\u3009\u2295 |the///\u3009\u2295 |N\u2227\\/\u3009\u2295 |mouse/\\/\u3009\u2295 |[VP]\\\u3009= |t2\u3009 . (14)\nIn order to prove \u03c8 a homomorphism, we define the following linear maps on F .\ncat(|u\u3009) = (1\u2297\u3008m|)|u\u3009 (15a)\nexi(|u\u3009) = (1\u2297\u3008i|)|u\u3009 (15b)\nconsk(|a\u3009, |u0\u3009, . . . , |uk\u3009) = |a\u3009\u2297 |m\u3009\u2295 |u0\u3009\u2297 |0\u3009\u2295 \u00b7 \u00b7 \u00b7\u2295 |uk\u3009\u2297 |k\u3009 , (15c)\nhere, 1 denotes the unit operator (i.e. the unit matrix) and the Dirac \u201cbra\u201d vectors \u3008k| are linear forms from the dual role space V \u2217R that are adjoined to the role \u201cket\u201d vectors |k\u3009 such that \u3008i|k\u3009= \u03b4ik with Kronecker\u2019s \u03b4ik = 0(1) for i 6= k(i = k).\nBy means of these homomorphisms we compute the meaning of \u201cmouse\u201d as Fock space\noperator through\nJmouseK\u03c8 |t1\u3009= cons2(|S\u3009,cons2(cat(|t1\u3009),ex0(|t1\u3009), |N(mouse)\u3009), |[VP]\u3009) = |t2\u3009 . (16)\nInserting (15a \u2013 15c) yields\nJmouseK\u03c8 |t1\u3009= cons2(|S\u3009,cons2((1\u2297\u30082|)|t1\u3009,(1\u2297\u30080|)|t1\u3009, |N(mouse)\u3009), |[VP]\u3009) =\ncons2(|S\u3009,(1\u2297\u30082|)|t1\u3009\u2297 |2\u3009\u2295 (1\u2297\u30080|)|t1\u3009\u2297 |0\u3009\u2295 |N(mouse)\u3009\u2297 |1\u3009, |[VP]\u3009) =\n|S\u3009\u2297|2\u3009\u2295((1\u2297\u30082|)|t1\u3009\u2297|2\u3009\u2295(1\u2297\u30080|)|t1\u3009\u2297|0\u3009\u2295|N(mouse)\u3009\u2297|1\u3009)\u2297|0\u3009\u2295|[VP]\u3009)\u2297|1\u3009=\n|S\u3009\u2297|2\u3009\u2295((1\u2297\u30082|)|t1\u3009\u2297|2\u3009\u2295(1\u2297\u30080|)|t1\u3009\u2297|0\u3009\u2295(|N\u3009\u2297|2\u3009\u2295|mouse\u3009\u2297|0\u3009)\u2297|1\u3009)\u2297|0\u3009\u2295\n|[VP]\u3009)\u2297|1\u3009= |t2\u3009 , (17)\nwhere we have expanded |N(mouse)\u3009 as in (13) above. Note that the meaning of \u201cmouse\u201d crucially depends on the given state |t1\u3009 subjected to the operator JmouseK\u03c8 , making meaning highly contextual. This is an important feature of dynamic semantics as well [54, 55, 56]."
    },
    {
      "heading": "3 Results",
      "text": "The main result of this study is a Fock space representation theorem for vector symbolic architectures of context-free grammars that follows directly from the definitions (15a \u2013 15c) and is proven in Appendix 6.2.\nThe tensor product representation \u03c8 : TLC(G)\u222aN \u2192F is a homomorphism with re-\nspect to the term transformations (6a \u2013 6c). It holds\ncat(|A(t0, . . . , tk)\u3009) = |cat(A(t0, . . . , tk))\u3009 (18a)\nexi(|A(t0, . . . , tk)\u3009) = |exi(A(t0, . . . , tk))\u3009 (18b)\nconsk(|A\u3009, |t0\u3009, . . . , |tk\u3009) = |consk(A, t0, . . . , tk)\u3009 . (18c)\nFor the particular example discussed above, we obtain the Fock space trajectory in\nTab. 2.\nMoreover, we present the complete Fock space LC parse generated by FockBox which is a MATLAB toolbox provided by Wolff et al. [50] as its three-dimensional projection after principal component analysis (PCA [50]) in Fig. 4 as illustration."
    },
    {
      "heading": "4 Discussion",
      "text": "In this article we developed a representation theory for context-free grammars and pushdown automata in Fock space as a vector symbolic architecture (VSA). We presented rigorous proofs for the representations of suitable term algebras. To this end, we suggested a novel normal form for CFG allowing to express CFG parse trees as terms over a symbolic term algebra. Rule-based derivations over that algebra are then represented as transformation matrices in Fock space.\nMotivated by a seminal study of Shannon [1] on cognitive dynamic systems [3], our work could be of significance for levering research on cognitive user interfaces (CUI) [2,\n61]. Such systems are subject of ambitious current research. Instead of using keyboards and displays as input-output interfaces, users pronounce requests or instructions to a device as spoken language and listen to its uttered responses. To this aim, state-of-the-art language technology scans the acoustically analyzed speech signal for relevant keywords that are subsequently inserted into semantic frames [62] to interpret the user\u2019s intent. This slot filling procedure [63, 64, 65] is based on large language corpora that are evaluated by machine learning methods, such as deep learning of neural networks [8, 9, 65]. The necessity to overcome traditional slot filling techniques by proper semantic analyses technologies has already been emphasized by Allen [66]. His research group trains semantic parsers from large language data bases such as WordNet or VerbNet that are constrained by hand-crafted expert knowledge and semantic ontologies [63, 67].\nAnother road toward realistic CUI systems is the development of utterance-meaning transducers (UMT) that map syntactic representations obtained from the speech signal onto semantic representations in terms of feature value relations (FVR) [5, 61]. This is achieved through a perception action cycle, comprising the three components: perception, action and behavior control. The perception module transforms the input from the signal layer to the semantic symbolic layer, the module for behavior control solves decision problems based on semantic information and computes appropriate actions. Finally, the action module executes the result by producing acoustic feedback. Behavior control can flexibly adapt to user\u2019s demands through reinforcement learning.\nFor the implementation of rule-based symbolic computations in cognitive dynamic systems, such as neural networks, VSA provide a viable approach. Our results contribute a formally sound basis for this kind of future research and engineering. In contrast to cur-\nrent black-box approaches, our method is essentially transparent and hence explainable and trustworthy [45, 46]."
    },
    {
      "heading": "5 Conclusion",
      "text": "We reformulated context-free grammars (CFG) through term algebras and their processing through push-down automata by partial functions over term algebras. We introduced a novel normal form for CFG, called term normal form, and proved that any CFG in Chomsky normal form can be transformed into term normal form. Finally, we introduced a vector symbolic architecture (VSA) by assigning basis vectors of a high-dimensional linear space to the respective symbols and their roles in a phrase structure tree. We suggested a recursive function for mapping CFG phrase structure trees onto representation vectors in Fock space and proved a representation theorem for the partial rule-based processing functions. We illustrated our findings by an interactive left-corner parser and used FockBox, a freely accessible MATLAB toolbox, for the generation and visualization of Fock space VSA. Our approach directly encodes symbolic, rule-based knowledge into the hyperdimensional computing framework of VSA and can thereby supply substantial insights into the future development of explainable artifical intelligence (XAI).\nCompliance with Ethical Standards\nEthical approval: This article does not contain any studies with human participants or animals performed by any of the authors.\nConflict of interest: The authors declare that they have no conflict of interest."
    },
    {
      "heading": "6 Appendix",
      "text": "6.1 Proof of term normal form\nDefinition 1 (Context-free grammar) A context-free grammar (CFG) is a quadruple G = (T,N,S,R) with a set of terminals T , a set of nonterminals N, the start symbol S \u2208 N and a set of rules R \u2286 N\u00d7 (N \u222a T )\u2217. A rule r = (A,\u03b3) \u2208 R is usually written as a production r : A\u2192 \u03b3 .\nDefinition 2 (Chomsky normal form) According to [29] a CFG G = (T,N,S,R) is said to be in Chomsky normal form iff every production r \u2208 R is one of\nA \u2192 BC (19a)\nA \u2192 a (19b)\nS \u2192 \u03b5 (19c)\nwith A \u2208 N, B,C \u2208 N \\{S} and a \u2208 T .\nIt is a known fact, that for every CFG G there is an equivalent CFG G\u2032 in Chomsky normal form [29]. It is also known that if G does not produce the empty string \u2014 absence of production (19c) \u2014 then there is an equivalent CFG G\u2032 in Chomsky reduced form [29].\nDefinition 3 (Chomsky reduced form) A CFG G = (T,N,S,R) is said to be in Chomsky reduced form iff every production r \u2208 R is one of\nA \u2192 BC (20a)\nA \u2192 a (20b)\nwith A,B,C \u2208 N and a \u2208 T .\nBy utilizing some of the construction steps for establishing Chomsky normal form from\n[29] we deduce\nCorollary 1 For every CFG G in Chomsky reduced form there is an equivalent CFG G\u2032 in Chomsky normal form without a rule corresponding to production (19c).\nProof Let G be a CFG in Chomsky reduced form. Clearly G does not produce the empty string. The only difference to Chomsky normal form is the allowed presence of the start symbol S on the right-hand side of rules in R. By introducing a new start symbol S0 and inserting rules {(S0,\u03b3) | \u2203(S,\u03b3) \u2208 R} we eliminate this presence and obtain an equivalent CFG in Chomsky normal form without a production of form (19c). ut\nDefinition 4 (Term normal form) A CFG G = (T,N,S,R) is said to be in term normal form iff R\u2286 N\u00d7 (N\u222aT )+ and for every two rules r = (A,\u03b3) \u2208 R and r\u2032 = (A\u2032,\u03b3 \u2032) \u2208 R\nA = A\u2032 =\u21d2 |\u03b3|= |\u03b3 \u2032|\nholds.\nWe state and proof by construction:\nTheorem 1 For every CFG G = (T,N,S,R) not producing the empty string there is an equivalent CFG G\u2032 in term normal form.\nProof Let G = (T,N,S,R) be a CFG not producing the empty string. Let G\u2032 = (T,N\u2032,S,R\u2032) be the equivalent CFG in Chomsky reduced form and D\u2286 N\u2032 be the set of all nonterminals from G\u2032 which have productions of both forms (20a) and (20b).\nWe establish term normal form by applying the following transformations to G\u2032:\n1. For every nonterminal A \u2208 D let R\u2032\u2032A = {(A,BC) \u2208 R\u2032 | B,C \u2208 N\u2032} be the rules corre-\nsponding to productions of form (20a) and R\u2032A = {(A,a) \u2208 R\u2032 | a \u2208 T} be the rules corresponding to productions of form (20b). We add (a) new nonterminals A\u2032\u2032 and A\u2032, (b) a new rule (A\u2032\u2032,BC) for every rule (A,BC) \u2208 R\u2032\u2032A and (c) a new rule (A\u2032,a) for every rule (A,a) \u2208 R\u2032A. Finally, we remove all rules R\u2032\u2032A\u222aR\u2032A from R\u2032. 2. For every nonterminal A \u2208 D let LA = {(X ,AY ) \u2208 R\u2032 | X ,Y \u2208 N\u2032} be the set of rules\nwhere A appears at first position on the right-hand side. For every rule (X ,AY ) \u2208 LA we add (a) a new rule (X ,A\u2032\u2032Y ) and (b) a new rule (X ,A\u2032Y ). Finally, we remove all rules LA from R\u2032. 3. For every nonterminal A \u2208 D let RA = {(X ,Y A) \u2208 R\u2032 | X ,Y \u2208 N\u2032} be the set of rules\nwhere A appears at second position on the right-hand side. For every rule (X ,Y A) \u2208 RA we add (a) a new rule (X ,Y A\u2032\u2032) and (b) a new rule (X ,Y A\u2032). Finally, we remove all rules RA from R\u2032.\n4. If S \u2208 D then we add\n(a) a new start symbol S0, (b) a new rule (S0,S\u2032) and (c) a new rule (S0,S\u2032\u2032).\n5. Finally, we remove D from N\u2032. ut\nWe immediately deduce\nCorollary 2 For every CFG G only producing strings of either exactly length 1 or at least length 2 there is an equivalent CFG G\u2032 in term normal form which is also in Chomsky normal form.\nProof We handle the two cases separately.\nCase 1 Let G be a CFG producing strings of exactly length 1. Since G does not produce the empty string there is an equivalent CFG G\u2032 in Chomsky reduced form where every rule is of form (20b) and the only nonterminal being the start symbol. Obviously, G\u2032 is in Chomsky normal form and also in term normal form.\nCase 2 Let G be a CFG producing strings of at least length 2. Since G does not produce the empty string there is an equivalent CFG in Chomsky reduced form and from corollary 1 follows that there is an equivalent CFG in Chomsky normal form. Applying the construction from theorem 1 to this CFG leads to a CFG G\u2032 in term normal formal. Since G does not produce strings of length 1 step 4 is omitted by the construction and G\u2032 stays in Chomsky normal form. ut\nWe also state the opposite direction.\nCorollary 3 Every CFG G for which an equivalent CFG G\u2032 in Chomsky normal form exists which is also in term normal form, produces either only strings of length 1 or at least of length 2.\nProof Let G = (T,N,S,R) be a CFG in Chomsky normal form and term normal form at the same time. Clearly, G does not produce the empty string. Let R|S \u2286 R be the set of rules with the start symbols S on the left side. Since G is in term normal form we have to consider the following two cases.\nCase 1 Let (S,\u03b3) \u2208 R be a rule where \u03b3 \u2208 T . Then every rule in the set R|S has to be of the same form. It follows that G only produces strings of length 1.\nCase 2 Let (S,AB) \u2208 R be a rule with A,B \u2208 N. Then every rule in the set R|S has to be of the same form. It follows that strings produced by G have to be at least of length 2. ut\nWe instantly deduce\nTheorem 2 Those CFGs for which a Chomsky normal form in term normal exists are exactly the CFGs producing either only strings of length 1 or strings with at least length 2.\nwhich follows directly from corollaries 2 and 3.\n6.2 Proof of representation theorem\nThe proof of the Fock space representation theorem for vector symbolic architectures follows from direct calculation using the definition of the tensor product representation (9).\nProof\ncat(|A(t0, . . . , tk)\u3009) = (1\u2297\u3008m|)|A(t0, . . . , tk)\u3009=\n(1\u2297\u3008m|)(|A\u3009\u2297 |m\u3009\u2295 |t0\u3009\u2297 |0\u3009\u2295 \u00b7 \u00b7 \u00b7\u2295 |tk\u3009\u2297 |k\u3009) = |A\u3009= |cat(A(t0, . . . , tk))\u3009 ,\nexi(|A(t0, . . . , tk)\u3009) = (1\u2297\u3008i|)|A(t0, . . . , tk)\u3009=\n(1\u2297\u3008i|)(|A\u3009\u2297 |m\u3009\u2295 |t0\u3009\u2297 |0\u3009\u2295 \u00b7 \u00b7 \u00b7\u2295 |tk\u3009\u2297 |k\u3009) = |ti\u3009= |exi(A(t0, . . . , tk))\u3009 ,\nconsk(|A\u3009, |t0\u3009, . . . , |tk\u3009) = |A\u3009\u2297 |m\u3009\u2295 |t0\u3009\u2297 |0\u3009\u2295 \u00b7 \u00b7 \u00b7\u2295 |tk\u3009\u2297 |k\u3009=\n|A(t0, . . . , tk)\u3009= |consk(A, t0, . . . , tk)\u3009\nut"
    }
  ],
  "title": "Vector symbolic architectures for context-free grammars",
  "year": 2020
}
