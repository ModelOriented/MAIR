{
  "abstractText": "Recent rapid advances in Artificial Intelligence (AI) and Machine Learning have raised many questions about the regulatory and governance mechanisms for autonomous machines. Many commentators, scholars, and policy-makers now call for ensuring that algorithms governing our lives are transparent, fair, and accountable. Here, I propose a conceptual framework for the regulation of AI and algorithmic systems. I argue that we need tools to program, debug and maintain an algorithmic social contract, a pact between various human stakeholders, mediated by machines. To achieve this, we can adapt the concept of human-in-the-loop (HITL) from the fields of modeling and simulation, and interactive machine learning. In particular, I propose an agenda I call societyin-the-loop (SITL), which combines the HITL control paradigm with mechanisms for negotiating the values of various stakeholders affected by AI systems, and monitoring compliance with the agreement. In short, \u2018SITL = HITL + Social Contract.\u2019",
  "authors": [
    {
      "affiliations": [],
      "name": "Iyad Rahwan"
    }
  ],
  "id": "SP:69c1d27722de73178e1ec71ab1caf5b3074d8ce4",
  "references": [
    {
      "authors": [
        "H. Aldewereld",
        "V. Dignum",
        "Y. hua Tan"
      ],
      "title": "Design for values in software development",
      "venue": "Handbook of Ethics,",
      "year": 2014
    },
    {
      "authors": [
        "S. Amershi",
        "M. Cakmak",
        "W.B. Knox",
        "T. Kulesza"
      ],
      "title": "Power to the people: The role of humans in interactive machine learning",
      "year": 2014
    },
    {
      "authors": [
        "K.J. Arrow"
      ],
      "title": "Social choice and individual values, volume 12. Yale university",
      "year": 2012
    },
    {
      "authors": [
        "E. Bakshy",
        "S. Messing",
        "L.A. Adamic"
      ],
      "title": "Exposure to ideologically diverse news and opinion on facebook",
      "year": 2015
    },
    {
      "authors": [
        "D. Baldassarri",
        "G. Grossman"
      ],
      "title": "Centralized sanctioning and legitimate authority promote cooperation in humans",
      "venue": "Proceedings of the National Academy of Sciences,",
      "year": 2011
    },
    {
      "authors": [
        "R. Berk",
        "H. Heidari",
        "S. Jabbari",
        "M. Kearns",
        "A. Roth"
      ],
      "title": "Fairness in criminal justice risk assessments: The state of the art",
      "venue": "arXiv preprint arXiv:1703.09207",
      "year": 2017
    },
    {
      "authors": [
        "K. Binmore"
      ],
      "title": "Natural justice",
      "year": 2005
    },
    {
      "authors": [
        "D. Boyd",
        "K. Crawford"
      ],
      "title": "Critical questions for big data: Provocations for a cultural, technological, and scholarly phenomenon",
      "year": 2012
    },
    {
      "authors": [
        "E. Bozdag"
      ],
      "title": "Bias in algorithmic filtering and personalization",
      "venue": "Ethics and information technology,",
      "year": 2013
    },
    {
      "authors": [
        "M. Cakmak",
        "C. Chao",
        "A.L. Thomaz"
      ],
      "title": "Designing interactions for robot active learners",
      "venue": "IEEE Transactions on Autonomous Mental Development,",
      "year": 2010
    },
    {
      "authors": [
        "A. Caliskan",
        "J.J. Bryson",
        "A. Narayanan"
      ],
      "title": "Semantics derived automatically from language corpora contain human-like biases",
      "year": 2017
    },
    {
      "authors": [
        "P. Callejo",
        "R. Cuevas",
        "A. Cuevas",
        "M. Kotila"
      ],
      "title": "Independent auditing of online display advertising campaigns",
      "venue": "In Proceedings of the 15th ACM Workshop on Hot Topics in Networks (HotNets),",
      "year": 2016
    },
    {
      "authors": [
        "C. Castelfranchi"
      ],
      "title": "Artificial liars: Why computers will (necessarily) deceive us and each other",
      "venue": "Ethics and Information Technology,",
      "year": 2000
    },
    {
      "authors": [
        "Y. Chen",
        "J.K. Lai",
        "D.C. Parkes",
        "A.D. Procaccia"
      ],
      "title": "Truth, justice, and cake cutting",
      "venue": "Games and Economic Behavior,",
      "year": 2013
    },
    {
      "authors": [
        "D.K. Citron",
        "F.A. Pasquale"
      ],
      "title": "The scored society: due process for automated predictions",
      "venue": "Washington Law Review,",
      "year": 2014
    },
    {
      "authors": [
        "V. Conitzer",
        "M. Brill",
        "R. Freeman"
      ],
      "title": "Crowdsourcing societal tradeoffs",
      "venue": "In Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems,",
      "year": 2015
    },
    {
      "authors": [
        "J.W. Crandall",
        "M.A. Goodrich"
      ],
      "title": "Experiments in adjustable autonomy",
      "venue": "In Systems, Man, and Cybernetics,",
      "year": 2001
    },
    {
      "authors": [
        "T. Cuzzillo"
      ],
      "title": "Real-world active learning: Applications and strategies for human-in-the-loop machine learning",
      "venue": "Technical report, O?Reilly",
      "year": 2015
    },
    {
      "authors": [
        "M. Delvaux"
      ],
      "title": "Motion for a European Parliament resolution: with recommendations to the commission on civil law rules on robotics",
      "venue": "Technical Report (2015/2103(INL)),",
      "year": 2016
    },
    {
      "authors": [
        "N. Diakopoulos"
      ],
      "title": "Algorithmic accountability: Journalistic investigation of computational power structures",
      "venue": "Digital Journalism,",
      "year": 2015
    },
    {
      "authors": [
        "K. Dinakar",
        "J. Chen",
        "H. Lieberman",
        "R. Picard",
        "R. Filbin"
      ],
      "title": "Mixed-initiative real-time topic modeling & visualization for crisis counseling",
      "venue": "In Proceedings of the 20th International Conference on Intelligent User Interfaces,",
      "year": 2015
    },
    {
      "authors": [
        "A. Etzioni",
        "O. Etzioni"
      ],
      "title": "AI assisted ethics",
      "venue": "Ethics and Information Technology,",
      "year": 2016
    },
    {
      "authors": [
        "F. Fukuyama"
      ],
      "title": "The origins of political order: from prehuman times to the French Revolution",
      "venue": "Profile books",
      "year": 2011
    },
    {
      "authors": [
        "G. Gates",
        "J. Ewing",
        "K. Russell",
        "D. Watkins"
      ],
      "title": "How Volkswagen\u2019s \u2018defeat devices",
      "year": 2015
    },
    {
      "authors": [
        "D. Gauthier"
      ],
      "title": "Morals by agreement",
      "year": 1986
    },
    {
      "authors": [
        "\u00d6. G\u00fcrerk",
        "B. Irlenbusch",
        "B. Rockenbach"
      ],
      "title": "The competitive advantage of sanctioning institutions",
      "year": 2006
    },
    {
      "authors": [
        "W.D. Hamilton"
      ],
      "title": "The evolution of altruistic behavior",
      "venue": "American naturalist,",
      "year": 1963
    },
    {
      "authors": [
        "W. Haviland",
        "H. Prins",
        "B. McBride",
        "D. Walrath"
      ],
      "title": "Cultural anthropology: the human challenge",
      "venue": "Cengage Learning",
      "year": 2013
    },
    {
      "authors": [
        "J. Henrich"
      ],
      "title": "Cultural group selection, coevolutionary processes and large-scale cooperation",
      "venue": "Journal of Economic Behavior & Organization,",
      "year": 2004
    },
    {
      "authors": [
        "A. Hern"
      ],
      "title": "partnership on artificial intelligence",
      "year": 2016
    },
    {
      "authors": [
        "E. Horvitz"
      ],
      "title": "Principles of mixed-initiative user interfaces",
      "venue": "In Proceedings of the SIGCHI conference on Human Factors in Computing Systems,",
      "year": 1999
    },
    {
      "authors": [
        "M. Johnson",
        "J.M. Bradshaw",
        "P.J. Feltovich",
        "C.M. Jonker",
        "M.B. Van Riemsdijk",
        "M. Sierhuis"
      ],
      "title": "Coactive design: Designing support for interdependence in joint activity",
      "venue": "Journal of Human-Robot Interaction,",
      "year": 2014
    },
    {
      "authors": [
        "J. Kleinberg",
        "S. Mullainathan",
        "M. Raghavan"
      ],
      "title": "Inherent trade-offs in the fair determination of risk scores. arXiv preprint arXiv:1609.05807",
      "year": 2016
    },
    {
      "authors": [
        "D. Leben"
      ],
      "title": "A rawlsian algorithm for autonomous vehicles",
      "venue": "Ethics and Information Technology,",
      "year": 2017
    },
    {
      "authors": [
        "B. Letham",
        "C. Rudin",
        "T.H. McCormick",
        "D Madigan"
      ],
      "title": "Interpretable classifiers using rules and bayesian analysis: Building a better stroke prediction model",
      "year": 2015
    },
    {
      "authors": [
        "S. Levy"
      ],
      "title": "The AI revolution is on. Wired",
      "year": 2010
    },
    {
      "authors": [
        "W. Lippmann"
      ],
      "title": "The phantom public",
      "venue": "Transaction Publishers",
      "year": 1927
    },
    {
      "authors": [
        "M.L. Littman"
      ],
      "title": "Reinforcement learning improves behaviour from evaluative",
      "venue": "feedback. Nature,",
      "year": 2015
    },
    {
      "authors": [
        "B. Liu"
      ],
      "title": "Sentiment analysis and opinion mining",
      "venue": "Synthesis lectures on human language technologies,",
      "year": 2012
    },
    {
      "authors": [
        "J. Markoff"
      ],
      "title": "Machines of loving grace. Ecco",
      "year": 2015
    },
    {
      "authors": [
        "H. Moulin",
        "F. Brandt",
        "V. Conitzer",
        "U. Endriss",
        "J. Lang",
        "A.D. Procaccia"
      ],
      "title": "Handbook of Computational Social Choice",
      "year": 2016
    },
    {
      "authors": [
        "M. Nowak",
        "R. Highfield"
      ],
      "title": "Supercooperators: Altruism, evolution, and why we need each other to succeed",
      "year": 2011
    },
    {
      "authors": [
        "C. O\u2019Neil"
      ],
      "title": "Weapons of math destruction: How big data increases inequality and threatens democracy. Crown Publishing Group (NY)",
      "year": 2016
    },
    {
      "authors": [
        "T. O\u2019Reilly"
      ],
      "title": "Open data and algorithmic regulation",
      "year": 2016
    },
    {
      "authors": [
        "L. Orseau",
        "S. Armstrong"
      ],
      "title": "Safely interruptible agents",
      "venue": "In Uncertainty in Artificial Intelligence: 32nd Conference (UAI)",
      "year": 2016
    },
    {
      "authors": [
        "E. Pariser"
      ],
      "title": "The filter bubble: What the Internet is hiding from you",
      "venue": "Penguin UK",
      "year": 2011
    },
    {
      "authors": [
        "F. Pasquale"
      ],
      "title": "The black box society: The secret algorithms that control money and information",
      "year": 2015
    },
    {
      "authors": [
        "A.S. Pentland"
      ],
      "title": "The data-driven society",
      "venue": "Scientific American,",
      "year": 2013
    },
    {
      "authors": [
        "A.C. Pigou"
      ],
      "title": "The economics of welfare",
      "year": 2192
    },
    {
      "authors": [
        "J. Rawls"
      ],
      "title": "A theory of justice",
      "year": 1971
    },
    {
      "authors": [
        "P.J. Richerson",
        "R. Boyd"
      ],
      "title": "Not by genes alone",
      "year": 2005
    },
    {
      "authors": [
        "O. Russakovsky",
        "J. Deng",
        "H. Su",
        "J. Krause",
        "S. Satheesh",
        "S. Ma",
        "Z. Huang",
        "A. Karpathy",
        "A. Khosla",
        "M Bernstein"
      ],
      "title": "Imagenet large scale visual recognition challenge",
      "venue": "International Journal of Computer Vision,",
      "year": 2015
    },
    {
      "authors": [
        "B. Scott"
      ],
      "title": "Visions of a techno-leviathan: The politics of the bitcoin blockchain",
      "venue": "E-International Relations",
      "year": 2014
    },
    {
      "authors": [
        "T.B. Sheridan"
      ],
      "title": "Telerobotics, automation, and human supervisory control",
      "year": 1992
    },
    {
      "authors": [
        "T.B. Sheridan"
      ],
      "title": "Supervisory control. Handbook of Human Factors and Ergonomics",
      "venue": "Third Edition,",
      "year": 2006
    },
    {
      "authors": [
        "K. Sigmund",
        "H. De Silva",
        "A. Traulsen",
        "C. Hauert"
      ],
      "title": "Social learning promotes institutions for governing the commons",
      "year": 2010
    },
    {
      "authors": [
        "B. Skyrms"
      ],
      "title": "Evolution of the social contract",
      "year": 2014
    },
    {
      "authors": [
        "L. Sweeney"
      ],
      "title": "Discrimination in online ad delivery",
      "year": 2013
    },
    {
      "authors": [
        "M. Tambe",
        "P. Scerri",
        "D.V. Pynadath"
      ],
      "title": "Adjustable autonomy for the real world",
      "venue": "Journal of Artificial Intelligence Research,",
      "year": 2002
    },
    {
      "authors": [
        "A.L. Thomaz",
        "C. Breazeal"
      ],
      "title": "Teachable robots: Understanding human teaching behavior to build more effective robot learners",
      "venue": "Artificial Intelligence,",
      "year": 2008
    },
    {
      "authors": [
        "R.L. Trivers"
      ],
      "title": "The evolution of reciprocal altruism",
      "venue": "Quarterly review of biology,",
      "year": 1971
    },
    {
      "authors": [
        "Z. Tufekci"
      ],
      "title": "Algorithmic harms beyond facebook and google: Emergent challenges of computational agency",
      "venue": "J. on Telecomm. & High Tech. L.,",
      "year": 2015
    },
    {
      "authors": [
        "P. Turchin"
      ],
      "title": "Ultrasociety: How 10,000 Years of War Made Humans the Greatest Cooperators on Earth",
      "venue": "Beresta Books",
      "year": 2015
    },
    {
      "authors": [
        "J. Valentino-DeVries",
        "J. Singer-Vine",
        "A. Soltani"
      ],
      "title": "Websites vary prices, deals based on users",
      "year": 2012
    },
    {
      "authors": [
        "I. Van de Poel"
      ],
      "title": "Translating values into design requirements",
      "venue": "In Philosophy and engineering: Reflections on practice, principles and process,",
      "year": 2013
    },
    {
      "authors": [
        "H.P. Young"
      ],
      "title": "Individual strategy and social structure: An evolutionary theory of institutions",
      "year": 2001
    }
  ],
  "sections": [
    {
      "heading": "1 Introduction",
      "text": "\u201cArt goes yet further, imitating that Rationall and most excellent worke of Nature, Man. For by Art is created that great LEVIATHAN called a COMMON-WEALTH, or STATE, (in latine CIVITAS) which is but an Artificiall Man\u201d Thomas Hobbes (1651). Leviathan\nDespite the initial promise of Artificial Intelligence, a long \u2018AI Winter\u2019 ensued in the 1980s and 1990s, as problems of automated reasoning proved much harder\n?An earlier version of this article was published under the same title on medium.com, on August 12, 2016.\nae-mail: irahwan@mit.edu\nthan initially anticipated [48]. But recent years have seen rapid theoretical and practical advances in many areas of AI. Prominent examples include machines learning their own representations of the world via Deep Neural Network architectures [41], Reinforcement Learning from evaluative feedback [45], and economic reasoning in markets and other multi-agent systems [58]. The result is an accelerating proliferation of AI technologies in everyday life [43].\nThese advances are yielding substantial societal benefits, ranging from more efficient supply chain management, to better matchmaking in peer-to-peer markets and online dating apps, to more reliable medical diagnosis and drug discovery [71].\nBut AI advances have also raised many questions about the regulatory and governance mechanisms for autonomous machines and complex algorithmic systems. Some commentators are concerned that algorithmic systems are not accountable because they are black boxes whose inner workings are not transparent to all stakeholders [59]. Others raised concern over people unwittingly living in filter bubbles created by news recommendation algorithms [11, 57]. Others argue that data-driven decision-support systems can perpetuate injustice, because they can also be biased either in their design, or by picking up human biases in their training data [13, 76]. Furthermore, algorithms can create feedback loops that reinforce inequality [10], for example in the use of AI in predictive policing or creditworthiness prediction, making it difficult for individuals to escape the vicious cycle of poverty [54].\nIn response to these alarms, various academic and governmental entities have started thinking seriously about AI governance. Recently, the United States White House National Science and Technology Council Committee on Technology released a report with recom-\nar X\niv :1\n70 7.\n07 23\n2v 2\n[ cs\n.C Y\n] 2\n6 Ju\nl 2 01\nmendations ranging from eliminating bias from data, to regulating autonomous vehicles, to introducing ethical training to computer science curricula [51]. The European Union, which has enacted many personal data privacy regulations, will soon vote on a proposal to grant robots legal status in order to hold them accountable, and to produce a code of ethical conduct for their design [21]. The Institute of Electrical and Electronics Engineers recently published a vision on \u2018Ethically Aligned Design\u2019 [37]. Industry leaders have also taken the initiative to create a \u2018Partnership on AI\u2019 to establish best practices for AI systems and to educate the public about AI [34].\nMy goal is this paper is to introduce a conceptual framework for thinking about the regulation of AI and data-driven systems. I argue that we need a new kind of social contract: an algorithmic social contract, that is a contract between various stakeholders, mediated by machines. To achieve this, we need to adopt a societyin-the-loop (SITL) framework in thinking about AI systems, which adapts the concept of human-in-the-loop (HITL), from the fields of supervisory control and interactive machine learning, but extends it to oversight conducted by society as a whole."
    },
    {
      "heading": "2 Human-in-the-Loop",
      "text": "In a human-in-the-loop (HITL) system, a human operator is a crucial component of an automated control process, handling challenging tasks of supervision, exception control, optimization and maintenance (Figure 1). The notion has been studied for decades within the field of supervisory control [2, 68]. Sheridan defined human supervisory control as a process by which \u201cone or more human operators are intermittently programming and continually receiving information from a computer that itself closes an autonomous control loop through artificial effectors to the controlled process or task environment\u201d [67].\nThese ideas then made their way into the field of Human-Computer Interaction (HCI). Scientists began working on mixed-initiative user interfaces, in which the autonomous system can make intelligent decisions about when and how to engage the human [36].\nRecently, a number of articles have been written about the importance of applying HITL thinking to Artificial Intelligence (AI) and machine learning (ML) systems. A simple form of HITL ML is the use of human workers to label data for training machine learning algorithms. This has produced invaluable benchmarks that spurred major advances in computer vision, for example [65].\nAnother example of HITL ML is interactive machine learning, which can help machines learn faster or more effectively by integrating feedback interactively from users [3, 20]. This type of HITL ML has been going on for a while. For example, many computer applications learn from your behavior in order to improve their ability to serve you better (e.g. by predicting the next word you are going to type). Similarly, when you mark an email as \u2018spam\u2019 in an online email service, you are one of many humans in the loop of a complex machine learning algorithm (specifically an active learning system), helping it in its continuous quest to improve email classification as spam or non-spam.\nMore sophisticated examples of HITL ML are now emerging, in which the human-in-the-loop has more explicit knowledge of the state of the system. For instance, in a crisis counseling system, a machine learning system classifies messages sent by callers, and provides visualizations to a human counselor in real-time [23]. Thus, the human and the machine learning system work in tandem to deliver effective counseling.\nScalable Cooperation Iyad Rahwan\nHITL thinking has also been applied successfully to human-robot interaction (HRI) [12]. This includes dynamically adapting the degree of autonomy given to robots [19, 73], interactively teaching reinforcement learning robots to adopt particular behaviors [74], and designing flexible human-robot teams [38].\nThere is another role of the HITL paradigm, which is closer to the problems discussed in the present article. HITL is not only a means to improve AI systems\u2019 accuracy in classification or to speed up the convergence of a reinforcement learning robot. Rather, HITL can also be a powerful tool for regulating the behavior of AI systems. For instance, many scholars now advocate for expert oversight, by a human operator, over the behavior of \u2018killer robots\u2019 or credit scoring algorithms [17].\nThe presence of a human fulfills two major functions in a HITL AI system:\n1. The human can identify misbehavior by an otherwise autonomous system, and take corrective action. For instance, a credit scoring system may misclassify an adult as ineligible for credit, due to an error in data entry in their age\u2013something a human may spot from the applicant\u2019s photograph. Similarly, a computer vision system on a weaponized drone may mis-identify a civilian as a combatant, and the human operator\u2013it is hoped\u2013would ensure that such cases are identified, and override the system. Some work is underway to ensure AI cannot learn to disable their own kill-switch [56]. 2. The human can be involved in order to provide an accountable entity in case the system misbehaves. If a fully autonomous system causes harm to human beings, having a human in the loop provides trust that somebody would bare the consequence of such mistakes, and thus have incentive to minimize those mistakes. This person may be a human within a tight control loop (e.g. an operator of a drone) or a much slower loop (e.g. programmers in a multi-year development cycle of an autonomous vehicle). Until we find a way to punish algorithms for harm to humans, it is hard to think of any other alternative.\nWhile HITL is a useful interaction paradigm for building AI systems that are subject to oversight, I believe it does not sufficiently emphasize the role of society as a whole in such oversight. HITL suggests that once we put a human expert, or group of experts, within the loop of an AI system, the problem of regulation is solved. But as I shall discuss in the following section, this may not always be the case."
    },
    {
      "heading": "3 Society-in-the-Loop",
      "text": "What happens when an AI system does not serve a narrow, well-defined function, but a broad function with wide societal implications? Consider an AI algorithm that controls millions of self-driving cars; or a set of news filtering algorithms that influence the political beliefs and preferences of millions of citizens; or algorithms that mediate the allocation of resources and labor in an entire economy. What is the HITL equivalent of these algorithms? This is where we make the qualitative shift from HITL to society in the loop (SITL).\nWhile HITL AI is about embedding the judgment of individual humans or groups in the optimization of AI systems with narrow impact, SITL is about embedding the values of society, as a whole, in the algorith-\nmic governance of societal outcomes that have broad implications. In other words, SITL becomes relevant when the scope of both the input and the output of AI systems is very broad. But one might ask, why should this be any different?\nThe move from HITL to SITL raises a fundamentally different problem: how to balance the competing interests of different stakeholders, including the interests of those who govern through algorithms? This is, traditionally, a problem of defining a social contract [70]. To put it in the most skeletal form, we can say:\nSITL = HITL + Social Contract\nTo elaborate on this simple equation, we need to take a short detour into political philosophy."
    },
    {
      "heading": "4 Detour: The Social Contract",
      "text": "Humans are the ultimate cooperative species [53]. Cultural anthropologists trace the evolution of political systems of governance from decentralized bands and tribes, to increasingly centralized chiefdoms, sovereign states and empires [31].\nOver time, humans reached the limits of old cooperative institutions such as kin selection\u2013helping others who share their genes [30], and reciprocal altruism\u2013helping others who would later help them back [75]. These old mechanisms cannot scale adequately to larger groups. In the face of inter-group competition, evolutionary pressure favored the emergence, and spread, of more complex social institutions to coordinate people\u2019s behaviors [77, 80]. For example, centralized sanctioning power is able to prevent higher-order free-riding\u2013following cooperative norms, but not contributing to their enforcement\u2013that undermine cooperation in larger groups [6, 29, 69].\nThe founders of social contract theory, going back to Thomas Hobbes\u2019 landmark book, Leviathan [35], posit that centralized government is legitimate precisely because it enables industrious people to cooperate via third-party enforcement of contracts among strangers (see Figure 2). Some of these contracts are explicit, such as marriage contracts or commercial transactions. Other aspects of the social contract are implicit, being embedded in social norms that govern every day life. In both cases, the contract embodies mutual consent to the government\u2019s legitimate use of force\u2013or people\u2019s use of social pressure\u2013 to guard people\u2019s rights and punish violators [8, 70].\nHobbes gave his Leviathan, the sovereign, enormous power. Subsequently, the social contract undertook many stages of evolution, thanks to enlightenment thinkers like John Locke [47], Jean-Jacques Rousseau\n[64], all the way to John Rawls [62] David Gauthier [28] and Brian Skyrms [70] in modern times. These thinkers refined our conception of how the social contract emerges in the first place, as well as the ways in which we can keep it from collapsing.\nModern political institutions, including the modern state, are a product of these evolutionary mechanisms of political development, which combine institutional innovation with learning. As Fukuyama puts it, \u201c[s]ocieties are not trapped by their pasts and freely borrow ideas and institutions from each other\u201d [26].\nThe result of this evolutionary process is a social contract that can provide the efficiency and stability of sovereign states, but which also ensures the sovereign implements the general will [64] of the people, and is held in some way accountable for violations of fundamental rights. In the same manner, \u201c[n]ew algorithmic decisionmakers are sovereign over important aspects of individual lives\u201d Thus, lack of accountability and due process for algorithmic decisions risks \u201cpaving the way to a new feudal order\u201d [17]."
    },
    {
      "heading": "5 The Algorithmic Social Contract",
      "text": "The SITL paradigm that I advocate is more akin to the interaction between a government and a governed citizenry, than the interaction between a drone and its operator. Similar to the role of due process and accountability in the traditional social contract, SITL can be conceived as an attempt to embed the general will into an algorithmic social contract.\nBy using the social contract metaphor, the SITL paradigm emphasizes an important distinction from the traditional HITL paradigm (Figure 3). In a HITL system, a human controller ensures that the AI system fulfills uncontested and common goals on behalf of societal stakeholders\u2013e.g. ensuring a plane lands safely, or improving food quality inspection. In addition, in the SITL domain, society must agree on two aspects:\n1. Society must resolve tradeoffs between the different values that AI systems can strive towards\u2013e.g. tradeoffs between security and privacy, or the tradeoffs between different notions of fairness [7, 39].\n2. Society must agree on which stakeholders would reap which benefits and pay which costs\u2013e.g. how improvements in safety made possible by driverless cars are to be distributed between passengers and pedestrians, or which degree of collateral damage, if any, is acceptable in autonomous warfare.\nIn human-based government, citizens use various channels\u2014e.g. democratic voting, opinion polls, civil society institutions, the media\u2014to articulate their expectations to the government. Meanwhile, the government, through its bureaucracy and various branches undertakes the function of governing, and is ultimately evaluated by the citizenry. And while citizens are not involved in the details [44], they are the arbiters among all of these institutions, and have the power to replace their key actors.\nModern societies are (in theory) SITL human-based governance machines. Some of those machines are better programmed, and have better \u2018user interfaces\u2019 than others. Similarly, as more governance functions get en-\ncoded into AI algorithms, we need to create channels between human values and governance algorithms.\nTo implement SITL, we need to know what types of behaviors people expect from AI, and to enable policymakers and the public to articulate these expectations (goals, ethics, norms, social contract) to machines. To close the loop, we also need new metrics and methods to evaluate AI behavior against quantifiable human values. In other words: we need to build new tools to program, debug, and monitor the algorithmic social contract between humans and algorithms\u2013that is, algorithms that are effective sovereigns over important aspects of social and economic life, whether or not they are actually operated by governments. This requires both government regulation and industry standards that represent the expectations of the public, with corresponding oversight."
    },
    {
      "heading": "6 The SITL Gap",
      "text": "Why are we not there yet? There has been a flurry of thoughtful treaties on the social and legal challenges posed by the opaque algorithms that permeate and govern our lives. While these seminal writings help illuminate many of the challenges, they fall short on comprehensive solutions.\n6.1 Articulating Societal Values\nOne barrier to implementing SITL is the cultural divide between engineering on one hand, and the humanities on the other (see Figure 4). Thoughtful legislators, legal scholars, media theorists, and ethicists are very skilled at revealing moral hazards, and identifying ways in which moral principles and constitutional rights may be violated [15]. But they are not always able to articulate those expectations in ways that engineers and designers can operationalize.\nScalable Cooperation Iyad Rahwan\n6.2 Quantifying Externalities & Negotiating Tradeoffs\nAlgorithms can generate what economists refer to as negative externalities\u2014costs incurred by third parties not involved in the decision [61]. For example, if autonomous vehicle algorithms over-prioritize the safety of passengers\u2014who own them or pay to use them\u2014 they may disproportionately increase the risk borne by\npedestrians. Quantifying these kinds of externalities is not always straightforward, especially when they occur as a consequence of long, indirect causal chains, or as a result of machine code that is opaque to humans.\nOnce we have quantified externalities, we need to negotiate the tradeoffs they embody. If certain ways to increase pedestrian safety in autonomous vehicles imply reduction in passenger safety, which tradeoffs are acceptable?\nHuman experts already implement tradeoffs as they design policies and products. For example, reducing the speed limit on a road reduces the utility for drivers who want to get home quickly, while increasing the overall safety of drivers and pedestrians. It is possible to completely eliminate accidents\u2014by reducing the speed limit to zero and banning cars\u2014but this would also eliminate the utility of driving, and regulators attempt to strike a balance that society is comfortable with through a constant learning process.\nQuantifying tradeoffs in any complex system, with many interacting parts, is always difficult. In complex economic systems, there are often unintended consequences of design choices. As AI becomes an integral part of such systems, the problem of quantifying those tradeoffs becomes even harder. For example, subtle algorithm design choices in autonomous vehicles may lead to a particular tradeoff between risks to passengers and risks to pedestrians. Identifying, let alone negotiating those tradeoffs, may be much harder than setting a speed limit\u2013if only due to the greater degrees of freedom when making design choices. This may be further complicated by the fact that algorithms learn from their experience, which may lead to shifts in the tradeoffs being made, going beyond what the programmers intended.\n6.3 Verifying Compliance with Societal Values\nComputer scientists and engineers are not always able to quantify the behaviors of their systems such that they can be easily understood by ethicists and legal theorists. This makes it more difficult to scrutinize the behavior of algorithms against set expectations. Even simple notions such as \u2018fairness\u2019 can be formalized in many different ways mathematically or in computer code [7].\nAn important component of Figure 4 is that both human values and AI are ongoing constant co-evolution. Thus, the evolution of technical capability can dramatically (and even irreversibly) alter what society considers acceptable\u2014think of how privacy norms have changed because of the utility provided by smart phones and the Internet."
    },
    {
      "heading": "7 Bridging the Gap",
      "text": "There are many efforts underway to bridge the societyin-the-loop gap. Below is an incomplete list of efforts that I believe are relevant, and a discussion of their merits and limitations.\n7.1 Articulating Values: Design, Crowdsourcing & Sentiment Analysis\nIn the broader context of technology design, various value-sensitive design methodologies have been proposed [25], which can be applied to software development [1, 79]. These approaches may prove helpful in the design of AI systems.\nSome AI scientists propose to use of crowdsourcing [18] to identify societal tradeoffs in a programmable way. There are some efforts to collect data about people\u2019s preferences over values implemented in AI algorithms, such as those that control driverless cars. Using methods from the field of moral psychology, one can identify potential moral hazards due to the incentives of different users of the road [9]. For example, my coauthors and I have developed a public-facing survey tool that elicits the public\u2019s moral expectations from autonomous cars faced with ethical dilemmas [49]. We have collected over 30 million decisions to date. Findings from this data can help regulators and car makers understand some of the psychological barriers to the wide adoption of autonomous vehicles.\nIn many domains, it may be possible to measure societal values directly from observational data, without having to run explicit polling campaigns or build dedicated crowdsourcing platforms [46]. For example, automated sentiment analysis on social media discourse can quantify people\u2019s reaction to different moral violations committed by AI systems. While these approaches have their limitations, they can help gauge the evolution of public attitudes, and their readiness to accept new social pacts through machines.\n7.2 Negotiation: Social Choice & Contractarianism\nThe field of computational social choice [4, 50] explores the aggregation of societal preferences and fair allocation of resources. Because these aggregation mechanisms can be implemented algorithmically, they provide a potential solution to the problem of negotiating tradeoffs of different stakeholders [16, 52, 58].\nAn alternative approach to the negotiation of values is to use normative and meta-ethical tools from social contract theory to identify enforceable outcomes\nthat rational actors would be willing to opt into. For instance, Leben recently proposed an algorithm that allows autonomous vehicles to resolve dilemmas of unavoidable harm using Rawls\u2019 Contractarianism [40]. In particular, Leben proposes to program cars to make decisions that rational actors would take if they were in a hypothetical \u2018original position\u2019 behind a \u2018veil of ignorance.\u2019 This veil would, for example, conceal whether the person is a passenger or a pedestrian in a given accident, leading them to choose the maximin solution\u2013that is, the decision that minimizes how bad the worse-case outcome is.\n7.3 Compliance: People Watching Algorithms\nAn important function for ensuring accountability is the ability to scrutinize the behavior of those in power, through mechanisms of trasparency. In the context of algorithms, this does not mean having access to computer source code, as intuitive as this notion might seem.\nReading the source code of a modern machine learning algorithm tells us little about its behavior, because it is often through the interaction between algorithms and data that things like discrimination emerge. Transparency must, therefore, be about the external behavior of algorithms. Indeed, this is how we regulate the behavior of humans\u2013not by looking into their brain\u2019s neural circuitry, but by observing their behavior and judging it against certain standards of conduct. Of course, this observation can benefit from the ability of the algorithm to give human-interpretable explanations of their decisions [42].\nThe new journalistic practice of algorithmic accountability reporting provides a framework for scrutiny of algorithmic decisions that is purely behavioral [22]. As an example, Sweeney has demonstrated that Web searches for names common among African Americans cause online advertising algorithms to serve ads suggestive of an arrest record, which can harm the individual being searched [72]. Investigative journalism has also revealed evidence of price discrimination based on users\u2019 information, sparking a debate about the appropriateness of this practice [78].\nWe might also envision a role for professional algorithm auditors, people who interrogate algorithms to ensure compliance with pre-set standards. This interrogation may utilize real or synthetic datasets designed to identify whether an algorithm violates certain requirements. For instance, an algorithm auditor may provide sample job applications to identify if a job matching algorithm is discriminating between candidates based on irrelevant factors. Or an autonomous vehicle algorithm auditor may provide simulated traffic scenarios to en-\nsure the vehicle is not disproportionately increasing the risk to pedestrians or cyclists in favor of passengers.\nOne weakness of auditing in a simulated environment\u2013 using computer simulation or fake data\u2013is the potential for adversarial behavior: the algorithm being audited may attempt to trick the algorithm doing the auditing. This is similar to \u2018defeat devices,\u2019 a term used to describe software or hardware features that interfere with or disables car emissions controls under real world driving conditions, even if the vehicle passes formal emissions testing [27]. In a similar fashion, an autonomous vehicle control algorithm may detect that it is being tested in a virtual environment\u2013e.g. by noticing that the distribution of scenarios is skewed towards ethical dilemmas\u2013and behave differently under such testing conditions.\nThe possibility of this generalized \u2018defeat device\u2019 subversion necessitates continuous monitoring and auditing in real-world conditions, not just simulated conditions at certification time. Such continuous monitoring may benefit from automation, as I discuss in the next section.\n7.4 Compliance: Algorithms Watching Algorithms\nRecently, Amitai and Oren Etzioni proposed a new class of algorithms, called oversight programs, whose function is to \u201cmonitor, audit, and hold operational AI programs accountable\u201d [24]. Note the emphasis on \u2018operational,\u2019 suggesting that these oversight programs are aligned with the point I made earlier about the futility of source code inspection as the only means for regulation.\nOversight algorithms, thus, perform a similar function to today\u2019s spam filtering algorithms. But their scope is much wider, as they investigate suspicious behavior by rogue AI algorithms maliciously violating human values. For example, a new class of browser plug-ins is allowing independent, data-driven auditing of the information provided by online advertising platforms to advertisers [14]. This has revealed issues in the transparency and accuracy of the current algorithmicallymediated online advertising ecosystem.\nOne can imagine an algorithm that conducts realtime quantification of the amount of bias caused by a news filtering algorithm\u2013akin to Facebook\u2019s recent study [5]\u2013and raising an alarm if bias increases beyond a certain threshold.\n7.5 The Limits of Public Engagement\nIt is worth highlighting the limits of crowdsourcing of societal values in general, and when it comes to\nAI in particular. One of the most influential figures in 20th century journalism, Walter Lippman, warned of over-reliance on public opinion when it comes to policy matters that require significant expertise. In Lippman\u2019s words, \u201cPublic opinion is not a rational force.... It does not reason, investigate, invent, persuade, bargain or settle\u201d [44]. This is because it is impossible for a lay person to be fully informed about all facets of every policy question: even an expert practitioner or regulator in one field\u2013say medicine\u2013cannot be sufficiently informed to weigh in on policy matters in another field\u2013say monetary policy. The role of public opinion, Lippman contends, is to check the use of sovereign force, based on assessments made digestible to them by disagreeing experts, pundits and journalists.\nThere is a lot of merit in Lippman\u2019s argument. But he misses a second important role that the public plays: that of shaping moral values and norms. Experts alone cannot dictate what societal values should be. They can influence those values by providing relevant facts, such as the importance of physical exercise in promoting health, or the importance of recycling in the preservation of the environment. But ultimately, norms are shaped through the interaction of various social and evolutionary forces [33, 63]. And these values must influence the metrics against which the performance of experts\u2013or AI algorithms\u2013is measured."
    },
    {
      "heading": "8 Discussion",
      "text": "The ideas outlined in this article are not entirely new, and many have been discussed in the context of digital democracy [32] and the data-driven society [60]. Tim O\u2019Reilly recently coined the term algorithmic regulation to describe data-driven governance [55]. To O\u2019Reilly, successful algorithmic regulation must satisfy the following properties (quoted verbatim):\n1. A deep understanding of the desired outcome 2. Real-time measurement to determine if that out-\ncome is being achieved 3. Algorithms (i.e. a set of rules) that make adjust-\nments based on new data 4. Periodic, deeper analysis of whether the algorithms\nthemselves are correct and performing as expected.\nI agree with O\u2019Reilly\u2019s characterization. From my perspective, the identification and negotiation of desired outcomes are non-trivial problems. And ensuring that algorithms are performing as expected is not just a technical challenge, but also a social one. This is what makes the social contract framework helpful.\nNote that SITL operates at different time-scales than HITL. It looks more like public feedback on regula-\ntions and legislations, than feedback on frequent microlevel decisions. Nevertheless, I believe there is value in ensuring we pay attention to all component of \u2018the loop\u2019 using an explicit framework. This will be increasingly important as the time between diagnosis and policy adjustment becomes shorter, thanks to progress in data science and machine learning.\nI attempted to synthesize various concerns and solutions put forward by many scholars who are thinking about the regulation of algorithmic systems that govern social and economic life. I organized these discussions within two paradigms that have a long history: the human-in-the-loop paradigm from the fields of computer science and supervisory control, and the \u2018social contract\u2019 paradigm from political philosophy. The result can be summarized by a call-to-arms that defines the challenge ahead:\nto build institutions and tools that put the societyin-the-loop of algorithmic systems, and allows us to program, debug, and monitor the algorithmic social contract between humans and governance algorithms.\nThe Age of Enlightenment marked humanity\u2019s transition towards the modern social contract, in which political legitimacy no longer emanates from the divine authority of kings, but from the mutual agreement among free citizens to appoint a sovereign. We spent centuries taming Hobbes\u2019s Leviathan, the all-powerful sovereign [35]. We must now create and tame the new TechnoLeviathan."
    },
    {
      "heading": "Acknowledgement",
      "text": "I am grateful for financial support from the Ethics & Governance of Artificial Intelligence Fund, as well as support from the Siegel Family Endowment.\nI am endebted to Joi Ito, Suelette Dreyfus, Cesar Hidalgo, Alex \u2018Sandy\u2019 Pentland, Tenzin Priyadarshi and Mark Staples for conversations and comments that helped shape this article. I\u2019m grateful to Brett Scott for allowing me to appropriate the term \u2018Techno-Leviathan\u2019 which he originally presented in the context of Cryptocurrency [66]. I thank Deb Roy for introducing me to Walter Lippman\u2019s \u2018The Phantom Public\u2019 and for constantly challenging my thinking. I thank Danny Hillis for pointing to the co-evolution of technology and societal values. I thank James Guszcza for suggesting the term \u2018algorithm auditors\u2019 and for other helpful comments."
    }
  ],
  "title": "Society-in-the-Loop: Programming the Algorithmic Social Contract",
  "year": 2018
}
