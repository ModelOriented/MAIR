{
  "abstractText": "Development of interpretable machine learning models for clinical healthcare applications has the potential of changing the way we understand, treat, and ultimately cure, diseases and disorders in many areas of medicine. These models can serve not only as sources of predictions and estimates, but also as discovery tools for clinicians and researchers to reveal new knowledge from the data. High dimensionality of patient information (e.g., phenotype, genotype, and medical history), lack of objective measurements, and the heterogeneity in patient populations often create significant challenges in developing interpretable machine learning models for clinical psychiatry in practice. In this paper we take a step towards the development of such interpretable models. First, by developing a novel categorical rule mining method based on Multivariate Correspondence Analysis (MCA) capable of handling datasets with large numbers of features, and second, by applying this method to build transdiagnostic Bayesian Rule List models to screen for psychiatric disorders using the Consortium for Neuropsychiatric Phenomics dataset. We show that our method is not only at least 100 times faster than state-of-the-art rule mining techniques for datasets with 50 features, but also provides interpretability and comparable prediction accuracy across several benchmark datasets.",
  "authors": [
    {
      "affiliations": [],
      "name": "Qingzhu Gao"
    },
    {
      "affiliations": [],
      "name": "Humberto Gonzalez"
    },
    {
      "affiliations": [],
      "name": "Parvez Ahammad"
    }
  ],
  "id": "SP:5a5d9398bb52741aba397c017ea52f6b74c21301",
  "references": [
    {
      "authors": [
        "R. Agrawal",
        "R. Srikant"
      ],
      "title": "Fast algorithms for mining association rules in large databases",
      "venue": "Proceedings of the 20th International Conference on Very Large Data Bases. pp. 487\u2013499",
      "year": 1994
    },
    {
      "authors": [
        "M. Anthimopoulos",
        "S. Christodoulidis",
        "L. Ebner",
        "A. Christe",
        "S. Mougiakakou"
      ],
      "title": "Lung pattern classification for interstitial lung diseases using a deep convolutional neural network",
      "venue": "IEEE Transactions on Medical Imaging 35(5), 1207\u20131216",
      "year": 2016
    },
    {
      "authors": [
        "A.L. Beam",
        "I.S. Kohane"
      ],
      "title": "Big data and machine learning in health care",
      "venue": "JAMA 319(13), 1317\u20131318",
      "year": 2018
    },
    {
      "authors": [
        "C. Borgelt"
      ],
      "title": "Frequent item set mining",
      "venue": "Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery 2(6), 437\u2013456",
      "year": 2012
    },
    {
      "authors": [
        "S.P. Brooks",
        "A. Gelman"
      ],
      "title": "General methods for monitoring convergence of iterative simulations",
      "venue": "Journal of Computational and Graphical Statistics 7(4), 434\u2013455",
      "year": 1998
    },
    {
      "authors": [
        "A. Campolo",
        "M. Sanfilippo",
        "M. Whittaker",
        "K. Crawford"
      ],
      "title": "AI Now 2017 report",
      "venue": "AI Now Institute at New York University",
      "year": 2017
    },
    {
      "authors": [
        "J. Cohen"
      ],
      "title": "A coefficient of agreement for nominal scales",
      "venue": "Educational and Psychological Measurement 20(1), 37\u201346",
      "year": 1960
    },
    {
      "authors": [
        "D. Dheeru",
        "E. Karra Taniskidou"
      ],
      "title": "UCI machine learning repository (2017), http: //archive.ics.uci.edu/ml",
      "year": 2017
    },
    {
      "authors": [
        "T.G. Dietterich"
      ],
      "title": "An experimental comparison of three methods for constructing ensembles of decision trees: Bagging, boosting, and randomization",
      "venue": "Machine Learning 40(2), 139\u2013157",
      "year": 2000
    },
    {
      "authors": [
        "A. Gelman",
        "D.B. Rubin"
      ],
      "title": "Inference from iterative simulation using multiple sequences",
      "venue": "Statistical Science 7(4), 457\u2013472",
      "year": 1992
    },
    {
      "authors": [
        "L.H. Gilpin",
        "D. Bau",
        "B.Z. Yuan",
        "A. Bajwa",
        "M. Specter",
        "L. Kagal"
      ],
      "title": "Explaining explanations: An approach to evaluating interpretability of machine learning",
      "venue": "ArXiv Preprints",
      "year": 2018
    },
    {
      "authors": [
        "M.J. Greenacre",
        "J. Blasius"
      ],
      "title": "Multiple correspondence analysis and related methods",
      "venue": "Chapman & Hall/CRC",
      "year": 2006
    },
    {
      "authors": [
        "D. Gunning"
      ],
      "title": "DARPA explainable artificial intelligence (XAI) (2017), https:// www.darpa.mil/program/explainable-artificial-intelligence",
      "year": 2017
    },
    {
      "authors": [
        "J. Han",
        "J. Pei",
        "Y. Yin"
      ],
      "title": "Mining frequent patterns without candidate generation",
      "venue": "ACM SIGMOD Record 29(2), 1\u201312",
      "year": 2000
    },
    {
      "authors": [
        "P. Hendricks"
      ],
      "title": "titanic: Titanic Passenger Survival Data Set (2015), https://github. com/paulhendricks/titanic, R package version",
      "year": 2015
    },
    {
      "authors": [
        "Y. LeCun",
        "Y. Bengio",
        "G. Hinton"
      ],
      "title": "Deep learning",
      "venue": "Nature 521, 436\u2013444",
      "year": 2015
    },
    {
      "authors": [
        "B. Letham",
        "C. Rudin",
        "T.H. McCormick",
        "D. Madigan"
      ],
      "title": "Interpretable classifiers using rules and bayesian analysis: Building a better stroke prediction model",
      "venue": "The Annals of Applied Statistics 9(3), 1350\u20131371",
      "year": 2015
    },
    {
      "authors": [
        "W. Li",
        "J. Han",
        "J. Pei"
      ],
      "title": "CMAR: Accurate and efficient classification based on multiple class-association rules",
      "venue": "Proceedings of the 2001 IEEE International Conference on Data Mining. pp. 369\u2013376",
      "year": 2001
    },
    {
      "authors": [
        "Z.C. Lipton"
      ],
      "title": "The mythos of model interpretability",
      "venue": "ACM Queue 16(3)",
      "year": 2018
    },
    {
      "authors": [
        "B. Liu",
        "W. Hsu",
        "Y. Ma"
      ],
      "title": "Integrating classification and association rule mining",
      "venue": "Proceedings of the Fourth International Conference on Knowledge Discovery and Data Mining. pp. 80\u201386",
      "year": 1998
    },
    {
      "authors": [
        "M. Lo\u00e8ve"
      ],
      "title": "Probability Theory I",
      "venue": "Springer",
      "year": 1977
    },
    {
      "authors": [
        "F. Pedregosa",
        "G. Varoquaux",
        "A. Gramfort",
        "V. Michel",
        "B. Thirion",
        "O. Grisel",
        "M. Blondel",
        "P. Prettenhofer",
        "R. Weiss",
        "V. Dubourg",
        "J. Vanderplas",
        "A. Passos",
        "D. Cournapeau",
        "M. Brucher",
        "M. Perrot",
        "E. Duchesnay"
      ],
      "title": "Scikit-learn: Machine learning in Python",
      "venue": "Journal of Machine Learning Research 12, 2825\u20132830",
      "year": 2011
    },
    {
      "authors": [
        "R.A. Poldrack",
        "E. Congdon",
        "W. Triplett",
        "K.J. Gorgolewski",
        "K.H. Karlsgodt",
        "J.A. Mumford",
        "F.W. Sabb",
        "N.B. Freimer",
        "E.D. London",
        "T.D. Cannon",
        "R.M. Bilder"
      ],
      "title": "A phenome-wide examination of neural and cognitive function",
      "venue": "Scientific Data 3, 160110",
      "year": 2016
    },
    {
      "authors": [
        "C. Rudin",
        "B. Letham",
        "D. Madigan"
      ],
      "title": "Learning theory analysis for association rules and sequential event prediction",
      "venue": "Journal of Machine Learning Research 14, 3441\u2013 3492",
      "year": 2013
    },
    {
      "authors": [
        "G. Valdes",
        "J.M. Luna",
        "E. Eaton",
        "C. II",
        "L.H. Ungar",
        "T.D. Solberg"
      ],
      "title": "MediBoost: a patient stratification tool for interpretable decision making in the era of precision medicine",
      "venue": "Scientific Reports 6, 37854",
      "year": 2016
    },
    {
      "authors": [
        "J. Wyatt",
        "D. Spiegelhalter"
      ],
      "title": "Field trials of medical decision-aids: potential problems and solutions",
      "venue": "Proceedings of the Annual Symposium on Computer Application in Medical Care. pp. 3\u20137",
      "year": 1991
    },
    {
      "authors": [
        "X. Yin",
        "J. Han"
      ],
      "title": "CPAR: Classification based on predictive association rules",
      "venue": "Proceedings of the 2003 SIAM International Conference on Data Mining. pp. 331\u2013 335",
      "year": 2003
    },
    {
      "authors": [
        "Q. Zhu",
        "L. Lin",
        "M.L. Shyu",
        "S.C. Chen"
      ],
      "title": "Feature selection using correlation and reliability based scoring metric for video semantic detection",
      "venue": "Proceedings of the IEEE 4th International Conference on Semantic Computing. pp. 462\u2013469",
      "year": 2010
    }
  ],
  "sections": [
    {
      "heading": "1 Introduction",
      "text": "The use of novel Artificial Intelligence (AI) tools to derive insights from clinical psychiatry datasets has consistently increased in recent years [3], generating highly predictive models for heterogeneous datasets. While high predictability is indeed a desirable result, the healthcare community requires that the AI models are also interpretable, so that experts can learn new insights from these models, or even better, so that experts can improve the performance of the models by tuning the data-driven models. We take a practical approach towards solving this problem, by developing a new rule mining method for wide categorical datasets, and by applying our mining method to build interpretable transdiagnostic screening tools for psychiatric disorders - aiming to capture underlying commonalities among these disorders.\nStarting from early clinical decision support systems (CDSS) [26], the interpretation that clinicians obtain from data-driven models was identified as a\nar X\niv :1\n81 0.\n11 55\n8v 2\n[ cs\n.L G\n] 1\n6 D\nec 2\ncritical element in their practical deployment. A report by the AI Now Institute remarks as the top recommendation in their 2017 report that core government agencies, including those responsible for healthcare, \u201cshould no longer use black box AI and algorithmic systems\u201d [6]. The Explainable Artificial Intelligence (XAI) program at DARPA has as one of its goals to \u201cenable human users to understand, appropriately trust, and effectively manage the emerging generation of artificially intelligent partners\u201d [13]. In contrast, popular machine learning methods such as artificial neural networks [16] and ensemble models [9] are known for their elusive readout. For example, while artificial neural network applications exist for tumor detection in CT scans [2], it is virtually impossible for a person to understand the rational behind such a mathematical abstraction.\nInterpretability is often loosely defined as understanding not only what a model emitted, but also why it did [11]. As explained in [19], rule-based decision models offer desirable interpretation properties such as trust, transparent simulatability, and post-hoc text explanations. Recent efforts towards interpretable machine learning models in healthcare can be found in the literature, such as the development of a boosting method to create decision trees as the combination of single decision nodes [25]. Bayesian Rule List (BRL) [24,17] mixes the interpretability of sequenced logical rules for categorical datasets, together with the inference power of Bayesian statistics. Compared to decision trees, BRL rule lists take the form of a hierarchical series of if-then-else statements where model emissions are correspond to the successful association to a given rule. BRL results in models that are inspired, and therefore similar, to standard human-built decision-making algorithms.\nWhile BRL is by itself an interesting model to try on clinical psychiatry datasets, it relies on the existence of an initial set of rules from which the actual rule lists are built, which is similar to the approach taken by other associative classification methods [20,27,18]. Frequent pattern mining has been a standard tool to build such initial set of rules, with methods like Apriori [1] and FPGrowth [14] being commonly used to extract rules from categorical datasets. However, frequent pattern mining methods do not scale well for wide datasets, i.e., datasets where the total number of categorical features is much larger than the number of samples, commonly denoted as p n. Most clinical healthcare datasets are wide and thus require new mining methods to enable the use of BRL in this research area.\nIn this paper we propose a new rule mining technique that is not based on the frequency in which certain categories simultaneously appear. Instead, we use Multiple Correspondence Analysis (MCA) [12], a particular application of correspondence analysis to categorical datasets, to establish a similarity score between different associative rules. We show that our new MCA-miner method is significantly faster than commonly used frequent pattern mining methods, and that it scales well to wide datasets. Moreover, we show that MCA-miner performs equally well as other miners when used together with BRL. Finally, we use MCAminer and BRL to analyze a dataset designed for the transdiagnostic study\nof psychiatric disorders, building interpretable predictors to support clinician screening tasks."
    },
    {
      "heading": "2 Problem Description and Definitions",
      "text": "We begin by introducing definitions used throughout this paper. An attribute, denoted a, is a categorical property of each data sample, which can take a discrete and finite number of values denoted as |a|. A literal is a Boolean statement checking if an attribute takes a given value, e.g., given an attribute a with categorical values {c1, c2} we can define the following literals: a is c1, and a is c2. Given a collection of attributes {ai}pi=1, a data sample is a list of categorical values, one per attribute. A rule, denoted r, is a collection of literals, with length |r|, which is used to produce Boolean evaluations of data samples as follows: a rule evaluates to True whenever all the literals are also True, and evaluates to False otherwise.\nIn this paper we consider the problem of efficiently building rule lists, which are evaluated sequentially until one rule is satisfied, for datasets with a large total number of categories among all attributes (i.e., \u2211p i=1|ai|), a common situation among datasets related to health care or pharmacology. Given n data samples, we represent a dataset as a matrix X with dimensions n \u00d7 p, where Xi,j is the category assigned to the i-th sample for the j-th attribute. We also consider a categorical label for each data sample, collectively represented as a vector Y with length n. We denote the number of label categories by `, where ` \u2265 2. If ` = 2, we are solving a standard binary classification problem. If, instead, ` > 2 then we solve a multi-class classification problem.\nBayesian Rule Lists (BRL) is a framework proposed by Rudin et al. [24,17] to build interpretable classifiers. Although BRL is a significant step forward in the development of XAI methods, searching over the configuration space of all possible rules containing all possible combinations of literals obtained from a given dataset is simply infeasible. Letham et al. [17] offer a good compromise solution to this problem, where first a set of rules is mined from a dataset, and then BRL searches over the configuration space of combinations of the prescribed set of rules using a custom-built MCMC algorithm. While efficient rule mining methods are available in the literature, we show in Sec. 5 that such methods fail to execute on datasets with a large total number of categories, due to either unacceptably long computation time or prohibitively high memory usage.\nIn this paper we build upon the method in [17] developing two improvements. First, we propose a novel rule mining algorithm based on Multiple Correspondence Analysis that is both computational and memory efficient, enabling us to apply BRL on datasets with a large total number of categories. Our MCA-based rule mining algorithm is explained in detail in Sec. 3. Second, we parallelized the MCMC search method in BRL by executing individual Markov chains in separate CPU cores of a computer. Moreover, we periodically check the convergence of the multiple chains using the generalized Gelman & Rubin convergence criteria [5,10], thus stopping the execution once the convergence criteria is met.\nAs shown in Sec. 5.2, our implementation is significantly faster than the original single-core version, enabling the study of more datasets with longer rules or a large number of features."
    },
    {
      "heading": "3 MCA-based Rule Mining",
      "text": "Multiple Correspondence Analysis (MCA) [12] is a method that applies the power of Correspondence Analysis (CA) to categorical datasets. For the purpose of this paper it is important to note that MCA is the application of CA to the indicator matrix of all categories in the set of attributes, thus generating principal vectors projecting each of those categories into a euclidean space. We use these principal vectors to build an efficient heuristic merit function over the set of all available rules given the categories in a dataset."
    },
    {
      "heading": "3.1 Rule Score Calculation",
      "text": "First, we compute the MCA principal vectors of the extended data matrix concatenating X and Y , defined as Z = [ X Y ] with dimensions n\u00d7(p+1). Let us de-\nnote the MCA principal vectors associated each categorical value by {vj} \u2211 i|ai| j=1 , where {ai}pi=1 is the set of attributes in the dataset X. Also, let us denote the MCA principal vectors associated to label categories by {\u03c9k}`k=1.\nSince each category can be mapped to a literal statement, as explained in Sec. 2, these principal vectors serve as a heuristic to evaluate the quality of a given literal to predict a label [28]. Therefore, we define the score between each vj and each \u03c9k by \u03c1j,k = cos](vj , \u03c9k) = \u3008vj ,\u03c9k\u3009\n\u2016vj\u20162 \u2016\u03c9k\u20162 . Note that in the context of\nrandom variables, \u03c1i,k is equivalent to the correlation between vj and \u03c9k [21]. We compute the score between a rule r and label category k, denoted \u00b5k(r), as the average among the scores between the literals in r and the same label category: \u00b5k(r) = 1 |r| \u2211 l\u2208r \u03c1l,k. Finally, we search the configuration space of rules r built using the combinations of all available literals in a dataset such that |r| \u2264 rmax, and identify those with highest scores for each label category. These top rules are the output of our miner, and are passed over to the BRL method as the set of rules from which rule lists will be built.\nThe pseudocode for our rule mining algorithm is shown in Fig. 1, where we parallelized the loop iterating over label categories in line 3."
    },
    {
      "heading": "3.2 Rule Prunning",
      "text": "Since the number of rules generated by all combinations of all available literals up to length rmax is excessively large even for modest values of rmax, our miner includes two conditions under which we efficiently eliminate rules from consideration.\nFirst, similar to the approach in FP-Growth [14] and other popular miners, we eliminate rules whose support over each label category is smaller than a userdefined threshold smin. Recall that the support of a rule r for label category k,\ndenoted suppk(r), is the fraction of data samples that the rule evaluates to True among the total number of data samples associated to a given label. Given a rule r, note that once a rule r fails to pass our minimum support test, we stop considering all rules longer than r that also contain the all the literals in r since their support is necessarily smaller.\nSecond, we eliminate rules whose score is smaller than a user-defined threshold \u00b5min. Suppose that we want to build a new rule r\u0302 by taking a rule r and adding a literal l. In that case, given a category k the score of this rule must satisfy \u00b5k(r\u0302) = 1 |r|+1 ( |r|\u00b5k(r)+\u03c1l,k ) \u2265 \u00b5min. Let \u03c1k = maxl \u03c1l,k be the largest score among all available literals, then we can predict that at least one extension of r will have a score greater than \u00b5min if \u00b5k(r) \u2265 1|r| ( (|r|+ 1)\u00b5min\u2212 \u03c1k ) = mk(|r|). Given the maximum number of rules to be mined per label M , we recompute \u00b5min as we iterate combining literals to build new rules. As \u00b5min increases due to better candidate rules becoming available, the rule-acceptance bound mk becomes more restrictive, resulting in less rules being considered and therefore in a faster overall mining."
    },
    {
      "heading": "4 Benchmark Experiments",
      "text": "We benchmark the performance and computational efficiency of our MCA-miner against the \u201cTitanic\u201d dataset [15], as well as the following 5 datasets available in the UCI Machine Learning Repository [8]: \u201cAdult,\u201d \u201cAutism Screening Adult\u201d (ASD), \u201cBreast Cancer Wisconsin (Diagnostic)\u201d (Cancer), \u201cHeart Disease\u201d (Heart), and \u201cHIV-1 protease cleavage\u201d (HIV ). These datasets represent a\nTable 1: Performance evaluation of FP-Growth against MCA-miner when used with BRL on benchmark datasets. ttrain is the full training wall time in seconds.\nDataset n p\n\u2211p\ni=1|ai| FP-Growth + BRL MCA-miner + BRL\nAccuracy ROC-AUC ttrain Accuracy ROC-AUC ttrain\nAdult 45,222 14 111 0.81 0.85 512 0.81 0.85 115 ASD 248 21 89 0.87 0.90 198 0.87 0.90 16\nCancer 569 32 150 0.92 0.97 168 0.92 0.94 22\nHeart 303 13 49 0.82 0.86 117 0.82 0.86 15\nHIV 5,840 8 160 0.87 0.88 449 0.87 0.88 36 Titanic 2,201 3 8 0.79 0.76 118 0.79 0.75 10\nwide variety of real-world experiments and observations, thus allowing us to fairly compare our improvements against the original BRL implementation using the FP-Growth miner. All 6 benchmark datasets correspond to binary classification tasks. We conduct the experiments using the same set up in each of the benchmarks, namely quantizing all continuous attributes into either 2 or 3 categories, while keeping the original categories of all other variables. We train and test each model using 5-fold cross-validations, reporting the average accuracy and Area Under the ROC Curve (ROC-AUC) as model performance measurements.\nTable 1 presents the empirical results comparing both implementations. To guarantee a fair comparison between both implementations we fixed the parameters rmax = 2 and smin = 0.3 for both methods, and we set \u00b5min = 0.5, and M = 70 for MCA-miner. Our multi-core implementations for both MCA-miner and BRL were executed on 6 parallel processes, and only stopped when the Gelman & Rubin parameter [5] satisfied R\u0302 \u2264 1.05. We ran all the experiments using a single AWS EC2 c5.18xlarge instance with 72 cores. It is clear from our experiments that our MCA-miner matches the performance of FP-Growth in each case, while significantly reducing the computation time required to mine rules and train BRL models."
    },
    {
      "heading": "5 Screening Tools for Clinical Psychiatry",
      "text": "The Consortium for Neuropsychiatric Phenomics (CNP) [23] is a project aimed at understanding shared and distinct neurobiological characteristics among multiple diagnostically distinct patient populations. Four groups of subjects are included in the study: healthy controls (HC, n = 130), Schizophrenia patients (SCHZ, n = 50), Bipolar Disorder patients (BD, n = 49), and Attention Deficit and Hyperactivity Disorder patients (ADHD, n = 43). The total number of subjects in the dataset is n = 272. Our goal in analyzing the CNP dataset is to develop interpretable screening tools to identify the diagnosis of these three psychiatric disorders in patients, as well as finding transdiagnostic tools that identify the commonalities among these disorders."
    },
    {
      "heading": "5.1 CNP Self-Reported Instruments Dataset",
      "text": "Among other data modalities, the CNP study includes responses to p = 578 individual questions per subject [23], belonging to 13 self-report clinical questionnaires with a total of \u2211p i=1|ai| = 1350 categories. The 13 questionnaires are: \u201cAdult ADHD Self-Report Screener\u201d (ASRS ), \u201cBarratt Impulsiveness Scale\u201d (Barratt), \u201cChapman Perceptual Aberration Scale\u201d (ChapPer), \u201cChapman Social Anhedonia Scale\u201d (ChapSoc), \u201cChapman Physical Anhedonia Scale\u201d (ChapPhy), \u201cDickman Function and Dysfunctional Impulsivity Inventory\u201d (Dickman), \u201cEysenck\u2019s Impusivity Inventory\u201d (Eysenck), \u201cGolden & Meehl\u2019s 7 MMPI Items Selected by Taxonomic Method\u201d (Golden), \u201cHypomanic Personality Scale\u201d (Hypomanic), \u201cHopkins Symptom Check List\u201d (Hopkins), \u201cMultidimensional Personality Questionnaire \u2013 Control Subscale\u201d (MPQ), \u201cTemperament and Character Inventory\u201d (TCI ), and \u201cScale for Traits that Increase Risk for Bipolar II Disorder\u201d (BipolarII ).\nThe details about these questionnaires are beyond the scope of this paper, and due to space constraints we abbreviate the individual questions using the name in parenthesis in the list above together with the question number. Depending on the particular clinical questionnaire, each question results in a binary answer (i.e., True or False) or a rating integer (e.g., from 1 to 5). We used each possible answer as a literal attribute, resulting in a range from 2 to 5 categories per attribute."
    },
    {
      "heading": "5.2 Performance Benchmark",
      "text": "This is a challenging dataset for most rule learning algorithms since it is wide, with more features than samples since \u2211p i=1|ai| p n. Indeed, just generating all rules with 3 literals from this dataset results in approximately 23 million rules. Fig. 2a compares the wall execution time of our MCA-miner against three\npopular associative mining methods: FP-Growth, Apriori, and Carpenter, all using the implementation in the PyFIM package [4] and the same set of CNP features. While the associative mining methods are reasonably efficient on datasets with few features, for datasets with roughly 100 features they result in out-ofmemory errors or impractically long executions (longer than 12 hours) even on large-scale compute-optimized AWS EC2 instances. In comparison, MCA-miner empirically exhibits a grow rate compatible with datasets much larger than CNP. It is worth noting that while FP-Growth is shown as the fastest associative mining method in [4], its scaling behavior vs. the number of attributes is practically the same as Apriori in our experiments.\nIn addition to the increased performance due to MCA-miner, we also improved the implementation of the BRL training MCMC algorithm by running parallel Markov chains simultaneously in different CPU cores, as explained in Sec. 2. Fig. 2b shows the BRL training time comparison for the same rule set between our multi-core implementation against the original single-core implementation reported in [17]. Also, Fig. 2c shows that the multi-core implementation convergence time scales linearly with the number of Markov chains, with tsingle-core \u2248 12 Nchains tmulti-core."
    },
    {
      "heading": "5.3 Interpretable Classifiers",
      "text": "In the interest of building the best possible screening tool for the psychiatric disorders present in the CNP dataset, we build three different classifiers. First, we build a binary transdiagnostic classifier to separate HC from the set of Patients, defined as the union of SCHZ, BD, and ADHD subjects. Second, we build a multi-class classifier to separate all four original categorical labels available in the dataset. Finally, we evaluate the performance of the multi-class classifier as a transdiagnostic tool by repeating the binary classification task and comparing the results. All validations were performed using 5-fold cross-validation. In addition to using Accuracy and ROC-AUC as performance metrics as in Sec. 4, we also report the Cohen\u2019s \u03ba coefficient [7], which ranges between -1 (complete misclassification) to 1 (perfect classification), as another indication for the effect size of our classifier since it is compatible with both binary and multi-class classifiers and commonly used in the healthcare literature.\nBinary transdiagnostic classifier The rule list was generated using all the available samples, namely 130 HC vs. 142 Patients, and is shown in Fig. 3. A description of the questions in Fig. 3 is shown in Table 3. Note that most subjects are classified with a high probability in the top two rules, which is useful in situations where fast clinical screening is required. The confusion matrix for this classifier is show in Fig. 5a.\nWe also benchmark the performance of our method against other commonly used machine learning algorithms compatible with categorical data, using their Scikit-learn [22] implementations and default parameters. As shown in Table 2, our method has comparable effect size, if not better, than the state of the art.\nMulti-class classifier Fig. 4 shows the rule list obtained using the all 4 labels in the CNP dataset. We sub-sampled the dataset to balance out each label, resulting in n = 43 subjects for each of the four classes, with a total of n = 172 samples. Our classifier has an accuracy of 0.57 and Cohen\u2019s \u03ba of 0.38, and Fig. 5b shows the resulting confusion matrix. The questions present in the rule list are detailed in Table 3.\nWhile the accuracy of the rule list as a multi-class classifier is not perfect, it is worth noting how just 7 questions out of a total of 578 are enough to produce a relatively balanced output among the rules. Also note that, even though each of the 13 questionnaires in the dataset has been thoroughly tested in the literature as clinical instruments to detect and evaluate different traits and behaviors, the 7 questions picked by our rule list do not favor any of the questionnaires in particular. This is an indication that classifiers are better obtained from different sources of data, and likely improve their performance as other modalities, such as mobile digital inputs, are included in the dataset.\nBinary classification using multi-class rule list We replace the ADHD, BD, and SCHZ labels with Patients to evaluate the performance of the multi-class classifier as a binary transdiagnostic classifier. Using the cross-validated multiclass models, we compute their performance as binary classifiers obtaining an accuracy of 0.77, ROC-AUC of 0.8, and Cohen\u2019s \u03ba of 0.54. The confusion matrix is shown in Fig. 5c. These values are on par with those in Table 2, showing that our method does not decrease performance by adding more categorical labels.\nNote that while the original binary classifier is highly accurate identifying HC subjects, the multi-class classifier with binary emission is better at identifying Patient subjects, opening the door to new techniques capable of fusing the best properties of these different rule lists."
    },
    {
      "heading": "6 DISCUSSION",
      "text": "We formulated a novel MCA-based rule mining method, with excellent scaling properties against the number of categorical attributes, and presented a new implementation of the BRL algorithm using multi-core parallelization. We also studied the CNP dataset for psychiatric disorders using our new method, resulting in rule-based interpretable classifiers capable of screening patients from self-reported questionnaire data. Our results not only show the viability of building interpretable models for state-of-the-art clinical psychiatry datasets, but also highlight the scalability of these models to larger datasets to understand the interactions and differences between these disorders. We are actively exploring avenues for improving recruitment and reducing screening rejections in clinical trials."
    }
  ],
  "title": "MCA-based Rule Mining Enables Interpretable Inference in Clinical Psychiatry",
  "year": 2018
}
