{
  "abstractText": "Decision tree learning is a widely used approach in machine learning, favoured in applications that require concise and interpretable models. Heuristic methods are traditionally used to quickly produce models with reasonably high accuracy. A commonly criticised point, however, is that the resulting trees may not necessarily be the best representation of the data in terms of accuracy, size, and other considerations such as fairness. In recent years, this motivated the development of optimal classification tree algorithms that globally optimise the decision tree in contrast to heuristic methods that perform a sequence of locally optimal decisions. We follow this line of work and provide a novel algorithm for learning optimal classification trees based on dynamic programming and search. Our algorithm supports constraints on the depth of the tree and number of nodes and we argue it can be extended with other requirements. The success of our approach is attributed to a series of specialised techniques that exploit properties unique to classification trees. Whereas algorithms for optimal classification trees have traditionally been plagued by high runtimes and limited scalability, we show in a detailed experimental study that our approach uses only a fraction of the time required by the state-of-the-art and can handle datasets with tens of thousands of instances, providing several orders of magnitude improvements and notably contributing towards the practical realisation of optimal decision trees. c \u00a92020 Demirovi\u0107, Lukina, Hebrard, Chan, Bailey, Leckie, Ramamohanarao, and Stuckey. Demirovi\u0107, Lukina, Hebrard, Chan, Bailey, Leckie, Ramamohanarao, and Stuckey",
  "authors": [
    {
      "affiliations": [],
      "name": "Emir Demirovi\u0107"
    },
    {
      "affiliations": [],
      "name": "Emmanuel Hebrard"
    },
    {
      "affiliations": [],
      "name": "Jeffrey Chan"
    },
    {
      "affiliations": [],
      "name": "Kotagiri Ramamohanarao"
    },
    {
      "affiliations": [],
      "name": "Peter J. Stuckey"
    }
  ],
  "id": "SP:a562bac64d32aa5f7af68743cd7cc46402625055",
  "references": [
    {
      "authors": [
        "Sina Aghaei",
        "Mohammad Javad Azizi",
        "Phebe Vayanos"
      ],
      "title": "Learning optimal and fair decision trees for non-discriminative decision-making",
      "venue": "In Proceedings of AAAI-19,",
      "year": 2019
    },
    {
      "authors": [
        "Ga\u00ebl Aglin",
        "Siegfried Nijssen",
        "Pierre Schaus"
      ],
      "title": "Learning optimal decision trees using caching branch-and-bound search",
      "venue": "In Proceedings of AAAA-20,",
      "year": 2020
    },
    {
      "authors": [
        "Pranav Ashok",
        "Mathias Jackermeier",
        "Pushpak Jagtap",
        "Jan K\u0159et\u0301\u0131nsk\u1ef3",
        "Maximilian Weininger",
        "Majid Zamani"
      ],
      "title": "dtcontrol: decision tree learning algorithms for controller representation",
      "venue": "arXiv preprint arXiv:2002.04991,",
      "year": 2020
    },
    {
      "authors": [
        "Florent Avellaneda"
      ],
      "title": "Efficient inference of optimal decision trees",
      "venue": "In Proceedings of AAAI-20,",
      "year": 2020
    },
    {
      "authors": [
        "Osbert Bastani",
        "Yewen Pu",
        "Armando Solar-Lezama"
      ],
      "title": "Verifiable reinforcement learning via policy extraction",
      "venue": "In Advances in Neural Information Processing Systems,",
      "year": 2018
    },
    {
      "authors": [
        "Dimitris Bertsimas",
        "Jack Dunn"
      ],
      "title": "Optimal classification trees",
      "venue": "Machine Learning,",
      "year": 2017
    },
    {
      "authors": [
        "Dimitris Bertsimas",
        "Romy Shioda"
      ],
      "title": "Classification and regression via integer optimization",
      "venue": "Operations Research,",
      "year": 2007
    },
    {
      "authors": [
        "Leo Breiman",
        "JH Friedman",
        "RA Olshen",
        "CJ Stone"
      ],
      "title": "Classification and regression trees",
      "venue": "Cole Statistics/Probability Series,",
      "year": 1984
    },
    {
      "authors": [
        "Usama M. Fayyad",
        "Keki B. Irani"
      ],
      "title": "Multi-interval discretization of continuous-valued attributes for classification learning",
      "venue": "In Ruzena Bajcsy, editor, Proceedings of IJCAI\u201993,",
      "year": 1993
    },
    {
      "authors": [
        "Michael R Garey"
      ],
      "title": "Optimal binary identification procedures",
      "venue": "SIAM Journal on Applied Mathematics,",
      "year": 1972
    },
    {
      "authors": [
        "Xiyang Hu",
        "Cynthia Rudin",
        "Margo Seltzer"
      ],
      "title": "Optimal sparse decision trees",
      "venue": "In Advances in Neural Information Processing Systems,",
      "year": 2019
    },
    {
      "authors": [
        "HyunJi Kim"
      ],
      "title": "Package discretization in cran-r",
      "venue": "https://CRAN.R-project.org/package=discretization,",
      "year": 2015
    },
    {
      "authors": [
        "Hyafil Laurent",
        "Ronald L Rivest"
      ],
      "title": "Constructing optimal binary decision trees is NPcomplete",
      "venue": "Information Processing Letters,",
      "year": 1976
    },
    {
      "authors": [
        "Nina Narodytska",
        "Alexey Ignatiev",
        "Filipe Pereira",
        "Joao Marques-Silva"
      ],
      "title": "Learning optimal decision trees with SAT",
      "venue": "In Proceedings of IJCAI-18,",
      "year": 2018
    },
    {
      "authors": [
        "Siegfried Nijssen",
        "Elisa Fromont"
      ],
      "title": "Mining optimal decision trees from itemset lattices",
      "venue": "In Proceedings of SIGKDD-07,",
      "year": 2007
    },
    {
      "authors": [
        "F. Pedregosa",
        "G. Varoquaux",
        "A. Gramfort",
        "V. Michel",
        "B. Thirion",
        "O. Grisel",
        "M. Blondel",
        "P. Prettenhofer",
        "R. Weiss",
        "V. Dubourg",
        "J. Vanderplas",
        "A. Passos",
        "D. Cournapeau",
        "M. Brucher",
        "M. Perrot",
        "E. Duchesnay"
      ],
      "title": "Scikit-learn: Machine learning in Python",
      "venue": "Journal of Machine Learning Research,",
      "year": 2011
    },
    {
      "authors": [
        "Ross Quinlan"
      ],
      "title": "Programs for Machine Learning",
      "year": 1993
    },
    {
      "authors": [
        "H\u00e9lene Verhaeghe",
        "Siegfried Nijssen",
        "Gilles Pesant",
        "Claude-Guy Quimper",
        "Pierre Schaus"
      ],
      "title": "Learning optimal decision trees using constraint programming",
      "venue": "In Proceedings of CP-19,",
      "year": 2019
    },
    {
      "authors": [
        "Sicco Verwer",
        "Yingqian Zhang"
      ],
      "title": "Learning decision trees with flexible constraints and objectives using integer optimization",
      "venue": "In Proceedings of CPAIOR-17,",
      "year": 2017
    },
    {
      "authors": [
        "Sicco Verwer",
        "Yingqian Zhang"
      ],
      "title": "Learning optimal classification trees using a binary linear program formulation",
      "venue": "In Proceedings of AAAI-19,",
      "year": 2019
    }
  ],
  "sections": [
    {
      "text": "ar X\niv :2\nc\u00a92020 Demirovic\u0301, Lukina, Hebrard, Chan, Bailey, Leckie, Ramamohanarao, and Stuckey.\nKeywords: Decision trees, optimality, constraints, search, dynamic programming"
    },
    {
      "heading": "1. Introduction",
      "text": "Decision trees are traditionally built using heuristic methods, such as CART (Breiman et al. (1984)), and can produce high-quality trees in low computational time. A commonly criticised point, however, is that heuristically constructed decision trees may not necessarily be the best representation of the data in terms of accuracy, size, or other considerations such as fairness.\nAn alternative is to construct optimal decision trees, i.e., the best possible decision tree according to a given metric. The idea of computing optimal decision trees dates back to approximately the 1970s when constructing optimal decision trees was proven to be NPhard by Laurent and Rivest (1976). As emphasised by Bertsimas and Dunn (2017), while optimal decision trees have always been desirable, the authors of the CART algorithm (Breiman et al. (1984)) found that such trees were computationally infeasible given the resources of the time, and hence heuristic algorithms were the only option.\nOptimal decision trees are enticing for several reasons. It has been observed that a more accurate representation of the data offers better generalisation on unseen data (Bertsimas and Dunn (2017); Verwer and Zhang (2017, 2019)). This has been reiterated in our experiments as well. Optimal decision trees allow incorporating additional constraints that may be difficult to support in a heuristic algorithm. This is particularly important in socially-sensitive contexts, where special measures need to be taken to ensure fairness in machine learning. Otherwise, obtained models may implicitly or explicitly perpetuate discrimination and biases, reducing social welfare (Aghaei et al. (2019)). In some applications, the goal is to optimise the size of the decision tree representing a given controller to save memory for embedded devices (Ashok et al. (2020)). Decision trees, in particular those of small size, are desirable for formal methods when verifying properties of trained controllers, as opposed to more complex machine learning models (Bastani et al. (2018)). In recent years, there has been growing interest in explainable artificial intelligence. The basic premise is that machine learning models, apart from high accuracy, must also be able to explain their decisions to a (non-expert) human. This is necessary to increase human trust and reliability of machine learning in complex scenarios that are conventionally handled by humans. Optimal decision trees of small size naturally fit within the scope of explainable AI, as their reduced size is more convenient for human interpretation.\nLearning problems are defined as mathematical programs: an objective function is posed possibly together with a set of constraints. An advantage of optimal decision tree algorithms over heuristic approaches is that they adhere precisely to the given specification. This allows a clear analysis and assessment of the suitability of the particular mathematical formulation for a given application. In contrast, in heuristic methods there is a discrepancy between the target learning problem and the goals of the heuristic algorithm. In more detail, heuristic methods for decision trees do not necessarily directly optimise according to the learning problem, but rather locally optimise a sequence of subproblems with respect to a surrogate metric. While this has shown to produce reasonably accurate models quickly, it may be difficult to make conclusive statements on the learning problem definition, as the heuristic approach may not faithfully follow the desired metrics. For example, a specification might\nbe deemed suboptimal not due to a flaw in the definition, but rather because of the inability of the heuristic algorithm to optimise according to the specification.\nDespite the appeal of optimal algorithms for decision trees, heuristic methods are historically the dominant approach due to computational reasons. As both algorithmic techniques and hardware advanced, optimal decision trees have become within practical reach and attracted growing interest from the research community. In particular, there has been a surge of successful methods in the past few years. These approaches use generic optimisation methods, namely integer programming (Bertsimas and Dunn (2017); Verwer and Zhang (2017, 2019); Aghaei et al. (2019)), constraint programming (Verhaeghe et al. (2019)), and SAT (Narodytska et al. (2018)), and algorithms tailored to the decision tree problem (Nijssen and Fromont (2007); Hu et al. (2019); Aglin et al. (2020)). The methods DL8 (Nijssen and Fromont (2007)) and DL8.5 (Aglin et al. (2020)) are of particular interest as they can be seen as a starting point for our work. The DL8.5 approach has been shown to be highly effective, outperforming the other approaches, and is a demonstration that specialised methods may have an advantage over generic optimisation.\nOur Contribution. While previous works use highly related ideas, the presentation and terminology may differ substantially. In this work, we unify and generalise successful concepts from the literature by viewing the problem through the lens of a conventional algorithmic framework, namely dynamic programming and search. We introduce novel algorithmic techniques that reduce computation time by orders of magnitude when compared to the state-of-the-art. This notably contributes towards the practical application of optimal classification trees, which was traditionally plagued by high runtimes. We conduct an experimental study on a wide range of benchmarks from the literature to show the effectiveness of our approach and its components, and reiterate that optimal decision trees lead to better generalisation in terms of out-of-sample accuracy. Our framework supports constraints on the depth of the tree and the number of nodes, and we argue it is flexible and may be extended with other requirements. In more detail, the contributions are as follows:\n\u2022 MurTree (Section 4), a framework for computing optimal classification trees, i.e., decision trees that minimise the number of misclassifications. The framework allows constraints on the depth and the number of nodes of the decision tree. The node constraint is not considered in all works on optimal decision trees, notably it is not supported by the previously fastest algorithm, DL8.5. Additional objective functions and constraints may be added that admit a dynamic programming formulation (Section 4.9).\n\u2022 A clear high-level view of the framework using conventional algorithmic principles, namely dynamic programming and search, that unifies and generalises ideas from the literature (Section 4.1).\n\u2022 A specialised algorithm for computing the optimal classification tree of depth two, which serves as the backbone of our framework (Section 4.3). It uses a frequency counting method to avoid explicitly referring to the dataset. This substantially reduces the runtime of computing optimal trees which, when combined with an incremental technique that takes into account previous computations, provides orders of magnitude speed-ups.\n\u2022 A novel similarity-based lower bound on the number of misclassifications for an optimal decision tree. The bound is effective in determining that portions of the search space cannot contain better decision trees than currently found during the search, which allows the algorithm to prune parts of the search space without needing further inspection, providing additional speed-ups. The bound is derived by examining previously computed subtrees and computing the number of misclassifications that must hold in the new search space (Section 4.5).\n\u2022 We incorporate the constraint on the number of nodes in the tree, extend the caching technique to take into account both the depth and number of nodes constraint (Section 4.6), refine the lower bounding technique on the number of misclassifications from DL8.5 (Aglin et al. (2020)) to produce stronger bounds (4.6.1), and provide an incremental solving option to allow reusing computations when solving a series of increasingly large decision trees (Section 4.6.3), e.g., as encountered in hyper-parameter tuning. Further improvements include a dynamic post-order node exploration strategy (Section 4.7) that leads to consistent improvements over a conventional post-order search.\n\u2022 We provide a detailed experimental study to analyse the effectiveness of our individual techniques and scalability of our approach, evaluate our approach with respect to the state-of-the-art optimal classification tree algorithms, and compare against heuristic decision tree and random forest algorithms on out-of-sample accuracy (Section 5). The experimental results show that our approach provides highly accurate trees and exhibits speed-ups of (several) orders of magnitude when compared to the state-ofthe-art.\nThe rest of the paper is organised as follows. In the next section, we introduce the notions and definitions used throughout the paper. In Section 3, we review the state-ofthe-art for optimal decision trees. Our main contribution is given in Section 4, where we describe our MurTree framework. In Section 5, we conduct a series of empirical evaluations of our approach and conclude in Section 6."
    },
    {
      "heading": "2. Preliminaries",
      "text": "A feature is a variable that encodes information about an object. We speak of binary fbinary, categorical fcategorical, and continuous features fcontinuous depending on their domain, i.e., fbinary \u2208 {0, 1}, finteger \u2208 N, and fcontinuous \u2208 R. A feature vector is a vector of features. An instance is a pair that consists of a feature vector and a value representing the class. A class can take continuous or discrete values. A dataset, or simply data, is a set of instances. While features within a vector may have different domains, the i-th feature of each feature vector of the dataset shares the same domain. The assumption is that the features describe certain characteristics about the objects, and the i-th feature of each feature vector refers to the same characteristic of interest.\nThe process of learning seeks to compute a learning function that performs classification, i.e., maps feature vectors to classes. The target learning function is restricted to a particular form, e.g., the form of a decision tree (see further), and the goal is to compute a function\nthat minimises or maximises the target metric for a given dataset. If the domain of the classes of the dataset is discrete, we speak of a classification problem, and otherwise of a regression problem for continuous classes.\nDecision trees are binary trees from computer science. We call leaf and non-leaf nodes classification and predicate nodes, respectively. Each predicate node is given a predicate that maps feature vectors to a Boolean value, i.e., {0, 1}. The left and right edges of a predicate node are associated with the values zero and one, respectively. Each classification node is assigned a fixed class.\nA decision tree is a learning function that performs classification according to the following recursive procedure. Given a feature vector, it starts by considering the root node. If the considered node is a classification node, its class determines the class of the feature vector and the procedure terminates. Otherwise, the node is a predicate node, and the left child node will be considered next if the predicate of the node evaluates to zero, and otherwise the right child node is selected. The process recurses until a class is determined.\nThe (feature) depth of a decision tree is the maximum number of feature nodes any instance may encounter during classification. The size of a decision tree is the number of feature nodes. It follows that the maximum size of a decision tree with depth d is 2d \u2212 1. We note that in the literature, in some cases, the size is defined as the total number of nodes in the tree. These definitions are equivalent and can be used interchangeably, as a tree with n predicate nodes has n+ 1 classification nodes.\nIn practice, the predicates take a special form. For single-variate or axis-aligned decision trees, which are the focus of this work, predicates only consider a single feature and typically test whether it exceeds a threshold value. We refer to these nodes are feature nodes, as the predicate depends solely on one feature. Furthermore, the predicates are chosen based on the dataset. Generalisations of decision trees are straight-forward: multi-variate versions use predicates that operate on more than one feature, and predicates can be substituted by functions whose co-domains are of size n, in which case the decision tree is an n-ary tree with an analogous definition. These generalisations are mentioned for completeness and are not further discussed.\nWe use special notation for binary datasets, where the domain of features and classes is Boolean. Given a feature vector fv, we write fi \u2208 fv and fi \u2208 fv if the i-th feature has value one and zero, respectively. The value one indicates the feature is present in the feature vector, and otherwise it is not present. Features fi and fi are referred to as positive and negative features, respectively. We limit the predicates to only output the value of a particular feature in the feature vector and simply write fi and fi for the predicates. The binary dataset D is partitioned into a positive and negative class of instances based on the classes, i.e., D = D+ \u222a D\u2212. We consider the partitions as sets of feature vectors since their class is clear from context, and write D(f) as the set of instances from D that contain feature f , and analogously for multiple features, e.g., D(f1, f2) are the set of instances that contain both f1 and f2. The misclassification score of a decision tree on data is the number of instances for which classification produces the incorrect class considering the data as ground truth."
    },
    {
      "heading": "3. Literature Review",
      "text": "Historically the most popular techniques for decision tree learning were based on heuristics due to their effectiveness and scalability. Examples of these algorithms include CART, originally proposed by Breiman et al. (1984), and C4.5 by Quinlan (1993). These algorithms start with a single node, and iteratively expand the tree based on metrics such as information gain and Gini coefficients, and possibly post-process the obtained decision trees to prune branches in an effort to reduce overfitting. While there is a vast literature on heuristic algorithms for decision trees, in this work we are primarily concerned with optimal decision trees, and hence direct further discussion to such settings.\nBertsimas and Shioda (2007) presented a mixed-integer programming approach for optimal decisions that worked well on smaller datasets. Mixed-integer programming formulations with better performance were given by Bertsimas and Dunn (2017) and Verwer and Zhang (2017). These methods encode the optimal decision tree by fixing the tree depth in advance, creating variables to represent the predicates for each node, and adding constraints to enforce the decision tree structure. These approaches were later improved by BinOPT (Verwer and Zhang (2019)), a binary linear programming formulation, that took advantage of implicitly binarising data to reduce the number of variables and constraints required to encode the problem. Aghaei et al. (2019) used a mixed-integer programming formulation for optimal decision trees that supported fairness metrics. The authors argued that using machine learning in socially sensitive contexts may perpetuate discrimination if no special measures are taken into account. They propose fairness metrics and incorporate them in a mixed-integer programming formulation.\nAn encoding of decision trees using propositional logic (SAT) has been devised by Narodytska et al. (2018). In this line of work, the aim is to construct the smallest tree in terms of the total number of nodes that perfectly describes the given dataset, i.e., leads to zero misclassifications on the training data. An initial perfect decision tree is constructed using a heuristic method, after which a series of SAT-solver calls are made, each time posing the problem of computing a perfect tree with one less node. The SAT approach of Avellaneda (2020) simplifies the encoding by fixing the depth of the tree and employs an incremental approach where instances are gradually added to the formulation rather than being considered completely from the start.\nNijssen and Fromont (2007) introduced a framework named DL8 for optimal decision trees that could support a wide range of constraints. They took advantage that the left and right subtree of a given node can be optimised independently, introduced a caching technique to save subtrees computed during the algorithm in order to reuse them at a later stage, and combined these with ideas from the pattern mining literature to compute optimal decision trees. DL8 laid an important foundation for optimal decision tree algorithms that follow.\nVerhaeghe et al. (2019) approached the optimal classification tree problem by minimising the misclassifications using constraint programming. The independence of the left and right subtrees from Nijssen and Fromont (2007) was captured in an AND-OR search framework. Upper bounding on the number of misclassifications was used to prune parts of the search space and their algorithm incorporated an itemset mining technique to speed-\nup the computation of instances per node and used a caching technique similar to DL8 (Nijssen and Fromont (2007)),\nHu et al. (2019) presented an algorithm that computes the optimal decision tree by considering a balance between misclassifications and number of nodes. They apply exhaustive search, caching, and lower bounding of the misclassifications based on the cost of adding a new node to the decision tree. Compared to other recent optimal decision tree algorithms, the method relies on the number of nodes playing an important role in the metric of optimality and a limited number of binary features, e.g., the authors experimented with datasets with up to twelve binary features.\nAglin et al. (2020) developed DL8.5 by combining and refining the ideas from DL8 and the constraint programming approach. Their main addition was an upper bounding technique, which limited the upper misclassification value of a child node once the optimal subtree was computed for its sibling, and a lowering bound technique, where the algorithm stored information not only about computed optimal subtrees but also pruned subtrees to provide a lower bound on the misclassifications of a subtree. This led to an algorithm that outperformed previous approaches by a notable margin.\nExploiting properties specific to the decision tree learning problem proved to be valuable in improving algorithmic performance in previous work. In particular, search and pruning techniques, caching computation for later reuse, and the techniques that take advantage of the decision tree structure all lead to notable gains in performance. These are the main reasons for the success of specialised methods over generic frameworks, such as integer programming and SAT. As there is a significant overlap of ideas and techniques used in related work, we discuss these in more detail in Section 4.1 when presenting the high-level view of our framework.\nLastly, we refer the readers to a curated list of decision tree papers by Benedek Rozemberczki: https://github.com/benedekrozemberczki/awesome-decision-tree-papers."
    },
    {
      "heading": "4. MurTree: Our Framework for Optimal Classification Trees",
      "text": "Our framework computes optimal classification trees by exhaustive search. The search space is exponentially large, but special measures are taken to efficiently iterate through solutions, exploit the overlap between solutions, and avoid computing suboptimal decision trees.\nWe give the main idea of the algorithm, then provide the full pseudo code, and follow up with individual subsections where we present each individual technique in greater detail.\nFor the sake of clarity, the remaining text focusses on optimal classification trees that minimise the number of misclassified instances for binary datasets and binary classification. Extending the framework for general settings, such as continuous and categorical data, is discussed in Section 4.9."
    },
    {
      "heading": "4.1 High-Level Idea",
      "text": "We note two important properties of decision trees:\nProperty 1 (Independence) Given a dataset D, a feature node partitions the dataset D into its left and right subtree, such that Dleft \u2229 Dright = \u2205 and D = Dleft \u222a Dright.\nProperty 2 (Overlap) Given a classification node, a set of features encountered on the path from the root node to the classification node, and an instance, the order in which the features are used to evaluate the instance does not change the classification result.\nBoth properties follow directly from the definition of decision trees and are emphasised as they play a major role in designing decision tree algorithms. Property 1 allows computing the misclassification score of the tree as the sum of the misclassification scores of its left and right subtree, and as will be discussed, once a feature node is selected, the left and right subtrees can be optimised independently of each other. Property 2 shows there is an overlap between decision trees that share the same features, which is taken advantage of by caching techniques (see Section 4.6 for more details).\nThe dynamic programming formulation of optimal classification trees given in Eq. 1 provides a high-level summary of our framework. The input parameters consist of a binary dataset D with features F , an upper bound on depth d, and an upper bound on the number of feature nodes n. The output is the minimum number of misclassifications possible on the data given the input decision tree characteristics. The key observations are given by Properties 1 and 2. The two observations, independence and overlap, when combined reveal the dynamic programming structure of decision trees.\nT (D, d, n) =\n\n  \n  \nT (D, d, 2d \u2212 1) n > 2d \u2212 1 min{|D+|, |D\u2212|} n = 0 min{T (D(f), d \u2212 1, n \u2212 i\u2212 1) general case\n+ T (D(f), d\u2212 1, i) : f \u2208 F , i \u2208 [0, n \u2212 1]}\n(1)\nThe first case in Eq. 1 places a natural limit on the number of feature nodes given the depth. The second case defines the misclassification score for classification nodes. The general case states that computing the optimal misclassification score amounts to examining all possible feature splits and ways to distribute the feature node count to the left and right child of the root node. For each combination of a selected feature and node count distribution to its children, the optimal misclassification is computed recursively as the sum of the optimal misclassifications of its children. The formulation is exponential in the depth, feature node limit, and number of features, but with special care, as presented in the subsequent sections, it is possible to compute practically relevant optimal classification trees within a reasonable time.\nEq. 1 serves as the core foundation of our framework. In contrast to related work, we take advantage of the structure of decision trees to allow imposing a limit on the number of nodes as presented in Eq. 1. Previous approaches either place no constraint on the number of nodes apart from the depth (Nijssen and Fromont (2007); Aglin et al. (2020)), limit the number of nodes by penalising the objective function for each node in the tree (Hu et al. (2019)), or allow constraints on the number of nodes but do not make use of decision tree properties (Bertsimas and Dunn (2017); Narodytska et al. (2018); Verwer and Zhang (2017, 2019); Avellaneda (2020)). The last point is particularly important as the ability to exploit optimal decision tree properties is essential for achieving the best performance.\nSimpler and/or modified forms of Eq. 1 were used in some previous work under different terminology. The AND-OR search method (Verhaeghe et al. (2019)), pattern mining\napproach (Nijssen and Fromont (2007); Aglin et al. (2020)), and the search by Hu et al. (2019) use the independence property of the left and right subtree (Property 1). Those approaches save computed optimal subtrees (Property 2), which corresponds to memoisation as an integral part of dynamic programming (Section 4.6). Framing the problem as a dynamic program dates from the 1970s (e.g., Garey (1972)), but the description in works afterwards deviated as new techniques were introduced. We present the problem back in its original dynamic programming format and together with our node limitation addition, unite and generalise previous approaches using conventional algorithmic notation.\nA key component of our framework is a specialised algorithm for computing decision trees of depth at most two. It takes advantage of the specific decision tree structure by performing a precomputation on the data, which allows it to compute the optimal decision tree without explicitly referring to the data. This offers a significantly lower computational complexity compared to the generic case of Eq. 1, but is applicable in practice only to decision trees of depth two. Thus, rather than following Eq. 1 until the base case, we stop the recursion once a tree of depth two is required and invoke the specialised method.\nA defining characteristic of search algorithms are pruning techniques, which detect areas of the search that may be discarded without losing optimality. In the case of decision trees, subtrees may be pruned based on the lower or upper bound of the number of misclassifications of the given subtrees. If the bound shows that the misclassifications of a currently considered subtree will result in a high value, the subtree can be pruned, effectively reducing the search space. The challenge when designing bounding techniques is to find the correct balance between pruning power and the computational time required by the technique.\nWe introduce a novel similarity-based lower bounding technique (Section 4.5) that derives a bound based on the similarity of the previously considered subtrees. We use our lower bounding method in combination with the previous lower bounding approach introduced in DL8.5 (Aglin et al. (2020)), which we describe in the following text. Given a parent node, once the optimal subtree is computed for one of the children, an upper bound can be posed on the other child subtree based on the best decision tree known for the parent node and the number of misclassifications of the optimal child subtree. If a subtree fails to produce a solution within the posed upper bound, the upper bound is effectively a lower bound that can be used once the same subtree is encountered again in the search. Our algorithms uses a refinement of the described lower bound, which additionally takes into account all lower bounds of the children of the parent node (Section 4.6.1). Hu et al. (2019) uses a bound for an objective function that balances the accuracy (misclassifications) and number of nodes in the tree. If \u03b1 is the penalty in terms of misclassifications for adding a node to the decision tree, then \u03b1 also serves as a lower bound for each subtree (otherwise it is not worth introducing a node). We do not incorporate this last bound explicitly in our framework, but instead compute trees with such objective functions by solving a series of (overlapping) trees that optimise only the misclassification score (Section 4.9.3).\nThe remaining part of the paper describes our techniques in more detail."
    },
    {
      "heading": "4.2 Main Loop of the Framework",
      "text": "Algorithm 1 summarises our framework. As discussed in the previous section, it can be seen as an instantiation of Eq. 1 with additional techniques to speed-up the computation.\nThe algorithm takes as input a dataset D consisting of positive D+ and negative D\u2212 instances, the maximum depth and size (number of feature nodes) of the decision tree, and an upper bound that represents a limit on the number of misclassifications before the tree is considered infeasible. The output is an optimal classification tree respecting the input constraints on the depth, size, and upper bound, or a flag indicating that no such tree exists, i.e., the problem is infeasible. The latter occurs as a result of recursive calls (see further), which pose an upper bound that is necessary to ensure the decision tree has a lower misclassification value than the best tree found so far in the search.\nThe upper bound is initially set to the misclassification score of a single classification node for the data and is updated throughout the execution. Note that at the start of the algorithm a tighter upper bound could be computed by using a heuristic algorithm.\nThe base case of Eq. 1 is initially tested and a classification node is returned if no feature nodes are allowed. In addition, subtrees that are at their lower boundmisclassification values are already optimal and are returned immediately.\nAfter the initial tests, the algorithm attempts to prune the current tree based on two lower bounds: our novel similarity-based approach (Section 4.5), and our generalisation of the cache-based lower bounding introduced in DL8.5 (Aglin et al. (2020)) but now extended to take into account the number of nodes in the tree.\nAssuming pruning did not take place, if the current subtree has already been computed as part of a previous recursive call, the solution is retrieved from the cache and the current call terminates. Caching subtrees for trees where the depth is constrained dates from DL8 (Nijssen and Fromont (2007)). In our work, the algorithm caches with respect to the depth and number of node constraints.\nA key aspect of our framework is that trees of depth at most two are computed using a specialised procedure (Section 4.3). It solves the optimal decision tree problem in a sense as a unit operation and ignores the upper bound. The result is stored in the cache for future computation regardless of the feasibility of tree with respect to the upper bound, but the upper bound determines if the obtained tree is considered feasible.\nIf none of the above criteria is met, the algorithm reaches the general case from Eq. 1, where the search space is exhaustively explored through a series of overlapping recursions (Algorithm 2).\nRecall that the size of tree, i.e., the number of feature nodes, is given as input. One node is allocated as the root, and the remaining node budget is split among its children. For each feature, the algorithm considers all possible combinations of distributing the remaining node budget to its left and right subtrees. Note that determining the maximum size of thfe left subtree immediately fixes the maximum size of the right subset, and that special care needs to be taken to not allocate a size to a subtree that is greater than it may support with respect to its depth.\nFor a chosen tree configuration (the feature of the subtree root and the size of its subtrees), the algorithm determines which subtree to recurse on first. Previous work in DL8 and DL8.5 fixed the order by exploring the left before the right subtree. In our framework, we introduce a dynamic strategy that prioritises the subtree with the largest gap between its lower and upper bound (Section 4.7). The intuition is that this subtree is more probable to have a higher misclassification score, which in turn increases the likelihood of pruning the other sibling.\nThe algorithm then solves the subtrees in the chosen order. If the first subtree is infeasible, this implies that the lower bound of the subtree is one greater than the given upper bound. The information is stored in the cache in case the bound is needed in one of the other recursive calls. This bound was introduced in DL8.5 Aglin et al. (2020) and we provide a further refinement by into account pairwise sum of the lower bounds of the children (Section 4.6.1). Recall that the misclassification score of the root is the sum of the misclassifications of its children, and therefore the second subtree can be discarded if its sibling already led to an infeasible tree.\nIf both recursive calls successfully terminated, the obtained decision tree is recorded as the best tree found so far and the solution is stored in the cache. In our framework, as soon as a new globally optimal decision tree is encountered, it is identified as such. This leads to fully anytime behaviour, i.e., the execution can be stopped at any given point in time to return its current best solution. In the previous work of DL8.5, for instance, a globally improving solution was only detected at the root node of the complete decision tree.\nOnce all the recursive calls have been completed, the search space of the subtree has been exhaustively explored. The cache is updated with respect to the best locally found subtree: either the subtree is stored in the cache as an optimal subtree, or its lower bound is updated in case no feasible subtree was found. In this manner, all possible decision trees are explored and a tree with minimum misclassification score is returned.\nThe dynamic programming aspect can be seen as the method divides the main problem into smaller overlapping subproblems, owing to Properties 1 and 2. Search is used to prune the search space, saving computation time, and drives the algorithm towards the specialised algorithm, which efficiently computes optimal subtrees of depth at most two. The last point is a key component in reducing the overall runtime compared to previous approaches, as discussed in Section 4.3.\nLastly, we note two points not included in the pseudo-code for simplicity. Note the following definition and proposition:\nDefinition 1 (Degenerate Decision Trees) A decision tree is degenerate if it contains at least one classification node that does not classify any training instance.\nProposition 2 (Pruning Degenerate Trees) Given a degenerate decision tree with n feature nodes and misclassification score s on the training data, there exists at least one other decision tree with n\u2032 < n feature nodes and misclassification score s\u2032 \u2264 s.\nDegenerate trees may occur during the algorithm when splitting on a nondiscriminative feature, such that one subtree contains no training instances, i.e., |D| = 0. Due to Proposition 2, we deem these trees infeasible and prune them as soon as they are detected.\nThe second point is that the initial best subtree is set to a classification node if allowed by the upper bound, rather than an infeasible tree as given in Algorithm 1, which may trigger a global update of the best solution.\nThis concludes the description of the main loop of our framework. Before proceeding with detailing each component of our algorithm, we reiterate the differences between our approach and DL8.5 (Aglin et al. (2020)) in light of the technical description given above.\nComparison with DL8.5 (Aglin et al. (2020)). By virtue of taking into account the structure of decision trees, Algorithm 1 shares a similar layout as in DL8.5, but there are notable\ndifferences that result in orders of magnitude speed-ups. The differences can be summarised as follows: 1) we allow constraining the size of tree in addition to the depth, which is important in obtaining the smallest optimal decision, e.g., to improve interpretability or learn trees that generalise on unseen instances (Section 5.4), 2) our specialised algorithm (Section 4.3) is substantially more efficient at computing trees with depth two when compared to the general algorithm in Algorithm 1 or DL8.5, 3) we propose a new lower bound based on the similiarity with previously computed subtrees to further prune the search space (Section 4.5) and refine the previous lower bound (4.6.1), 4) our cache policy (Section 4.6) is extended to support the size of the tree constraint and allows for incremental solving, allowing reusing computation when solving trees with increasing depth and size, e.g., during hyper-parameter tuning, 5) we dynamically determine which subtree to explore first based on pruning potential (Section 4.7), rather than use a static stategy, and 6) our framework immediately updates the best global solution as soon as it is computed rather than only at the root node."
    },
    {
      "heading": "4.3 Specialised Algorithm for Trees of Depth Two",
      "text": "An essential part of our framework is a specialised method for computing optimal decision trees of depth two. The procedure is repeatedly called in our framework, i.e., each time a tree of at most depth two needs to be optimally solved. In the following, we present an algorithm that achieves lower complexity than the general algorithm (Eq. 1 and Prop. 3) when considering trees with depth two.\nPrior to presenting our specialised algorithm, we discuss the complexity of computing decision trees of depth two using Eq. 1 as the baseline.\nProposition 3 Computing the optimal classification tree of depth two using Eq. 1 can be done in O(|D| \u00b7 |F|2) time.\nAssume that splitting the data based on a feature node is done in O(|D|) time. Eq. 1 considers |F| splits for root and for each feature performs 2 \u00b7 |F| splits for its children. This results in 2 \u00b7 |F|2 splits and an overall runtime of O(|D| \u00b7 |F|2), proving Proposition 3. In practice, partitioning the dataset based on a feature can be sped-up using bitvector operations and caching subproblems (Aglin et al. (2020); Verhaeghe et al. (2019); Hu et al. (2019)), but the complexity remains as this only impacts the hidden constant in the big-O.\nIn the following, we present an algorithm with lower complexity and additional practical improvements which, when combined, reduce the runtime of computing the optimal classification tree of depth two by orders of magnitudes."
    },
    {
      "heading": "4.4 Algorithm Description",
      "text": "Algorithm 3 provides a summary. The input is a dataset D and the output is the optimal classification tree of depth two with three feature nodes that minimises the number of misclassified instances.\nThe specialised procedure computes the optimal decision tree in two phases. In the first step, it computes frequency counts for each pair of features, i.e., the number of instances in which both features are present. In the second step, it exploits the frequency counts to efficiently enumerate decision trees without needing to explicitly refer to the data. This\nAlgorithm 1: MurTree(D, depth, size, UB), a dynamic programming and search algorithm for computing optimal classification trees\ninput: Dataset D = D+ \u222a D\u2212, size \u2208 N, depth \u2208 N, upper bound on the misclassifications UB \u2208 N output: Optimal classification tree within the input size and depth that minimises the misclassification score on D\n1 begin // Base case (Eq. 1): no feature nodes are possible or the node is already at\nits lower bound\n2 if depth = 0 \u2228 size = 0 \u2228 LB(D, depth, size) = classification node(D) then 3 return classification node(D)\n// Prune using similiarity-based lower bounding (Section 4.5) if possible\n4 if SimiliarityLowerBound(D, depth, size) > UB then 5 return infeasible\n// Prune using cache-based pruning (Section 4.6.2) if possible\n6 if CachedLowerBound(D, depth, size) > UB then 7 return infeasible\n// Use cached subtrees if possible (Section 4.7)\n8 if subtree (D, depth, size) has already been computed then 9 return GetCachedSubtree(D, size, depth)\n// Use the specialised algorithm from Section 4.3 if possible\n10 if depth \u2264 2 then 11 best subtree\u2190 SpecialisedAlgorithm(D, size, depth)\n// Store the subtree regardless of the UB since it is optimal (Section 4.6.1)\n12 UpdateCache(D, depth, size, best subtree) 13 if score(best subtree) > UB then 14 return infeasible 15 else 16 return best subtree\n// General case (Eq. 1): exhaustively search using Algorithm 2\n17 best tree\u2190MurTree.GeneralCase(D, depth, size, UB) 18 return best tree\nAlgorithm 2: MurTree.GeneralCase(D, depth, size, UB), the general case of Eq. 1 in Algorithm 1\n1 begin // General case (Eq. 1): exhaustively explore the search space 2 best tree\u2190 infeasible // RLB is the lower bound using Eq. 15 3 RLB \u2190\u221e 4 for feature f \u2208 F do 5 max size subtree\u2190 min{2(depth\u22121) \u2212 1, size \u2212 1} 6 min size subtree\u2190 (size\u2212 1\u2212max size subtree) 7 for left size \u2208 [min size subtree,max size subtree] do 8 right size\u2190 (size\u2212 1\u2212 left size)\n// Dynamic post-order: process left subtree first (Section 4.7)\n9 if LowerBound(D(f), depth\u2212 1, left size) < LowerBound(D(f), depth\u2212 1, right size) then\n10 left subtree\u2190MurTree(D(f), left size, depth \u2212 1, UB) // No need to compute the other subtree if this one failed 11 if left subtree is infeasible then 12 RLB \u2190 min{RLB,LowerBound(D(f), depth \u2212 1, left size) + LowerBound(D(f), depth\u2212 1, right size)} 13 continue\n14 right size\u2190 (size\u2212 1\u2212 left size) 15 right subtree\u2190\nMurTree(D(f), right size, depth \u2212 1, UB \u2212 score(left subtree)) // If both children are feasible, update the globally and locally best\nsolution, the cache (Section 4.6), and the upper bound\n16 if right subtree is feasible then 17 best subtree\u2190 DecisionTree(f, left subtree, right subtree) 18 UpdateGlobalSolution(best subtree, depth, size) 19 UpdateCache(D, depth, size, best subtree)\nUB \u2190 score(best subtree)\u2212 1\n20 else 21 RLB \u2190 min{RLB,LowerBound(D(f), depth \u2212 1, left size) + LowerBound(D(f), depth\u2212 1, right size)}\n22 else // Dynamic post-order: process right subtree first (Section 4.7) 23 Process right subtree analogously as above // Cache the optimal solution or record the lower bound (Section 4.6.1) 24 UpdateCache(D, depth, size, UB,RLB, best subtree) 25 return best tree\nprovides a substantial speed-up compared to iterating through features and splitting data as given in the dynamic programming formulation (Eq. 1) for decision trees of depth two. We now discuss each phase in more detail and present a technique to incrementally compute the frequency counts.\nAlgorithm 3: Specialised algorithm for computing optimal classification trees of depth two with three nodes\ninput: Binary dataset D = D+ \u222a D\u2212 output: Optimal classification tree of depth two with three feature nodes that minimises the misclassification score on D\n1 begin 2 \u2200fi : FQ +(fi)\u2190 0 \u2227 FQ \u2212(fi)\u2190 0 3 \u2200fi, fj, i < j : FQ +(fi, fj)\u2190 0 \u2227 FQ\n\u2212(fi, fj)\u2190 0 /* Step 1: construct the frequency counter of positive features */\n4 for fv \u2208 D+ do 5 for fi \u2208 fv do 6 increment FQ+(fi) 7 for fj \u2208 fv s.t. i < j do 8 increment FQ+(fi, fj) 9 FQ\u2212 is computed as above using D\u2212\n/* Step 2: construct the optimal decision tree based on the frequency counters FQ+ and FQ\u2212 */\n/* Compute the best left and right subtrees for each feature */\n10 for fi \u2208 F do 11 for fj \u2208 F s.t. i 6= j do 12 CS(fi, fj)\u2190 min{FQ +(fi, fj), FQ \u2212(fi, fj)} 13 CS(fi, fj)\u2190 min{FQ +(fi, fj), FQ\n\u2212(fi, fj)} /* Compute branch with fi as root and fj as left child */\n14 MSleft(fi, fj)\u2190 CS(fi, fj) + CS(fi, fj) 15 if BestLeftSubtree(fi).misclassification > MSleft(fi, fj) then 16 BestLeftSubtree(fi).misclassification\u2190MSleft(fi, fj) 17 BestLeftSubtree(fi).feature\u2190 fj 18 The best right subtree with fi as the root and fj as the right child is\ncomputed analogously as above /* Compute the best tree by taking the feature with the minimum sum of\nmisclassification of its children */\n19 best tree\u2190 argminfi\u2208F{BestLeftSubtree(fi).misclassification + BestRightSubtree(fi).misclassification} 20 return best tree"
    },
    {
      "heading": "4.4.1 Phase One: Frequency counting (Algorithm 3, Lines 2-9)",
      "text": "Let FQ+(fi) and FQ +(fi, fj) denote the frequency counts in the positive instances for a single feature and a pair of features, respectively. The functions FQ\u2212(fi) and FQ \u2212(fi, fj) are defined analogously for the negative instances.\nA key observation is that based on FQ(fi) and FQ(fi, fj), we may compute FQ(fi), FQ(fi, fj), FQ(fi, fj), and FQ(fi, fj). This is done as follows:\nFQ+(fi) = |D +| \u2212 FQ+(fi) (2)\nFQ+(fi, fj) = FQ +(fi)\u2212 FQ +(fi, fj) (3)\nFQ+(fi, fj) = FQ +(fj)\u2212 FQ +(fi, fj) (4)\nFQ+(fi, fj) = |D +| \u2212 FQ+(fi)\u2212 FQ +(fj)\u2212 FQ +(fi, fj) (5)\nThe equations make use of the fact that the features are binary. For example, Eq. 2 states that if the total number of positive instances is |D+| and we computed the frequency count FQ+(fi), then the frequency count FQ\n+(fi) is the number of instances in which fi does not appear, i.e., the difference between |D+| and FQ+(fi). Similar reasoning is applied to the other equations and computing the frequency count FQ\u2212 is analogous.\nThe following proposition summarises the runtime of computing FQ+(fi, fj).\nProposition 4 (Computational Complexity of Phase One) Let m+ denote the maximum number of features in any single positive instance. Frequency counts FQ+(fi, fj) can be computed in O(|D+| \u00b7m2+) time with O(F 2) memory.\nAn efficient way of computing the frequency counts is to represent the feature vector as a sparse vector, and iterate through each instance in the dataset and increase a counter for each individual feature and each pair of features. This leads to the proposed complexity result. The additional memory is required to store the frequency counters, allowing to query a frequency count as a constant time operation. Note that the pairwise frequency count is symmetric, i.e., FQ+(fi, fj) = FQ\n+(fj, fi), which requires only to consider fi and fj in the frequency count for i < j. This results in a smaller hidden constant in the big-O notation."
    },
    {
      "heading": "4.4.2 Phase Two: Optimal tree computation (Algorithm 3, Lines 10-19)",
      "text": "Recall that a classification node is assigned the positive class if the number of positive instances exceeds the number of negative instances, otherwise the node class is negative. Let CS(fi, fj) be the classification score for a classification node with all instances of D containing both features fi and fj. The classification score is then computed as follows.\nCS(fi, fj) = min { FQ+(fi, fj), FQ \u2212(fi, fj) }\n(6)\nGiven a decision tree with depth two, a root node with feature froot, a left and right child node with features fleft and fright, we may compute the misclassification score in\nconstant time assuming the frequency counts are available. Let MSleft and MSright denote the misclassification scores of the left or right subtree. The computations are as follows.\nMSleft(froot, fleft) = CS(froot, fleft) +CS(froot, fleft) (7)\nMSright(froot, fright) = CS(froot, fright) + CS(froot, fright) (8)\nThe total misclassification score of the tree is the sum of misclassifications of its children. As the number of misclassification can be computed solely based on the frequency counts, we may conclude the computational complexity.\nProposition 5 (Computational Complexity of Phase Two) Given the frequency counts FQ+ and FQ\u2212, the optimal subtree tree can be computed in O(|F |2) time with O(|F |) memory.\nIt follows from Property 1 that given a root node with feature froot, the left and right subtrees can be optimised independently. Therefore, it is sufficient to compute for each feature its best left and right subtrees, and take the feature with the minimum sum of its child misclassifications. To compute the best left and right feature for each feature, the algorithm maintains information about the best left and right child for each feature found so far, leading to the memory requirement from Proposition 5. The best features are initially arbitrarily chosen. Recall that from Property 1 it follows that the left and right subtree can be optimised independently:\nmin fleft,fright\u2208F MS(froot, fleft, fright) = min fleft\u2208F MSleft(froot, fleft)+ min fright\u2208F MSright(froot, fright)\nTherefore, rather than considering triplets of features (froot, fleft, fright), it iterates through each pair of features (froot, fchild), computes the misclassification values of the left subtree using Eq. 7, updates the best left child for feature froot, and performs the same procedure for the right child. After iterating through all pairs of features, the best left and right subtree is known for each feature, leading to the proposed complexity. The optimal decision tree can then be computed by finding the feature with minimum misclassification cost of its combined left and right misclassification.\nAfter discussing each individual phase, we may conclude the overall complexity:\nProposition 6 (Computational Complexity of Depth-2 Decision Trees) Let m be the upper limit on the number of features in any single positive and negative instance. The number of operations required to computing an optimal decision tree is O(|D| \u00b7m2+ |F|2) using O(F2) auxiliary memory.\nThe result follows by combining Propositions 4 and 5. The obtained runtime is substantially lower at the expense of using additional memory compared to the dynamic programming formulation (Eq. 1) outlined in Proposition 3. Note that instances with binary features are naturally sparse. If the majority of instances contain more than half of the features, then as a preprocessing step all feature values may be inverted to achieve sparsity\nwithout loss of generality. The advantage of our approach is exemplified with lower sparsity ratios, i.e., small m values.\nThere are several additional points to note, which are not shown in Algorithm 3 to keep the pseudo-code succinct.\nThe above discussion assumed the feature node limit was set to three. The algorithm can be modified for the case of two feature nodes, keeping the same complexity, while in the case with only one feature node the pairwise computations are no longer necessary leading to O(|D| \u00b7m+ |F |) complexity. Similarly, the algorithm is implemented to lexicographically minimise the misclassification score and then the size of the tree.\nTo improve the performance in practice, the algorithm iterates through pairs of features (fi, fj) such that i < j. After updating the current best left and right subtree feature using fi as the root and fj as the child, the same computation is done using fj as the root and fi as the child. Compared to the pseudo-code in Algorithm 3, this cuts the number of iterations by half, but each iteration does twice as much work, which results in a speed-up in practice. Moreover, rather than computing the best tree in a separate loop after computing the best left and right subtrees for each feature, this is done on the fly by keeping track of the best subtree encountered so far during the algorithm.\nSpecialised algorithm for decision trees of depth three. We considered computing decision trees with depth three using a similar idea. Even though this results in a better bigO complexity for trees of depth three, albeit requiring O(F3) memory, our preliminary results did not indicate practical benefits. Including additional low-level optimisation might improve the results, but for the time being we leave this as an open question."
    },
    {
      "heading": "4.4.3 Incremental Computation",
      "text": "The specialised method for computing decision trees of depth two is repeatedly called in the framework. For each call, the algorithm is given a different dataset that is a result of applying a split in one of the nodes in the tree. The key observation is that datasets which differ only in a small number of instances result in similar frequency counts. The idea is to exploit this by only updating the necessary difference rather than recomputing the frequency counts from scratch.\nThe key point is to view the previous dataset Dold and the new dataset Dnew in terms of their intersection and differences.\nObservation 1 Given two datasets Dnew and Dold, let their difference be denoted as Din = Dnew \\ Dold and Dout = Dold \\ Dnew and their intersection as Dsame = Dnew \u2229 Dold. We may express the datasets as Dnew = Din \u222a Dsame and Dold = Dout \u222a Dsame\nWe first note that set operations can be done efficiently for datasets.\nProposition 7 (Computational Complexity of Set Operations on Datasets) Given a dataset D and two of its subsets Dnew \u2286 D and Dold \u2286 D, the sets Din = Dnew \u2212 Dold and Dout = Dold \u2212Dnew can be computed in O(|Dnew|+ |Dold|) time using O(|D|) memory.\nThe above can be realised by associating each instance of the original dataset D with a unique ID and afterwards using direct hashing to query in constant time the presence of an\ninstance in a dataset. Once the differences have been computed, the frequency counts may be updated incrementally.\nProposition 8 (Computational Complexity of Incremental Frequency Computation) Let m denote the maximum number of features in any considered instance. Given the frequency counts FQold of a previous dataset Dold, a new dataset Dnew, and their differences Din and Dout, the frequency counts FQnew of the new dataset Dnew can be computed in O((|Din|+ |Dout|) \u00b7m 2) time.\nTo show the complexity, note the difference between FQold and FQnew.\nObservation 2 Let K(FQ) denote the set of instances used to compute the frequency counts FQ. It follows that K(FQold) = Dout \u222a Dsame and K(FQnew) = Din \u222a Dsame.\nConsider taking FQold and applying a series of operations to reach the new frequency counts FQnew. The complexity result of Proposition 8 follows from the previous observations and the following:\nObservation 3 The frequency counts FQold already capture the counts for instances Dsame\nObservation 4 The frequency counts FQold need to be incremented using instances Din\nObservation 5 The frequency counts FQold need to be decremented using instances Dout\nUsing the incremental update procedure is sensible only if the number of updates required is small compared to recomputing from scratch. Therefore, in our framework, in each call to compute a decision tree of depth two, the algorithm incurs an overhead (Proposition 7) to compute the differences between the old and new dataset. It proceeds with the incremental computation if |Din \u222a Dout| < |Dnew|, and otherwise computes from scratch.\nNote that the overhead is negligible compared to the overall complexity of computing the optimal tree of depth two (Proposition 6), but the benefits can be significant if the difference is small. As shown in the experimental section, this is frequently the case in practice, as two successive features considered for splitting are unlikely to lead to vastly different splits."
    },
    {
      "heading": "4.5 Similiarity-Based Lower Bounding",
      "text": "We present a novel lower bounding technique that does not rely on the algorithm having previously searched a specific path, as opposed to the cache-based lower bound introduced in the later sections. Given a dataset Dnew for a node, the method aims to derive a lower bound by taking into account a previously computed optimal decision tree using the dataset Dold. It infers the bound by considering the difference in the number of instances between the previous dataset Dold and the current dataset Dnew. The bound is used to prune portions of the search space that are guaranteed to not contain a better solution than the best decision tree encountered so far in the search.\nAssume that for both datasets, the depth and the number of allowed feature nodes requirements are identical. As in the previous section, we define the sets Din = Dnew \\Dold, Dout = Dold \\ Dnew, and Dsame = Dnew \u2229 Dold.\nGiven the limits on the depth d and number of features nodes n, a dataset Dnew, and a dataset Dold with T (Dold, d, n) as the misclassification score of the optimal decision tree of Dold (recall Eq. 1), we define the similarity-based lower bound,\nLB(Dnew,Dold, d, n) = T (Dold, d, n)\u2212 |Dout|, (9)\nwhich is a lower bound for the number of misclassifications of the optimal decision tree for the dataset Dnew of a tree of depth d with n feature nodes, i.e.,\nProposition 9 LB(Dnew,Dold, d, n) \u2264 T (Dnew, d, n).\nAs a result, subtrees with a lower bound greater than its upper bound are immediately pruned, effectively speeding up the search. To show that Proposition 9 is indeed a lower bound, let T (D) = T (D, d, n), note that removing Dout from Dold may reduce the misclassification cost by at most |Dout|:\nT (Dold)\u2212 T (Dold \\ Dout) = T (Dold)\u2212 T (Dsame) \u2264 |Dout|. (10)\nT (Dold)\u2212 |Dout| \u2264 T (Dsame). (11)\nAdding instances to Dsame cannot decrease the misclassification score T (Dsame):\nT (Dnew) = T (Dsame \u222a Din) \u2265 T (Dsame) (12)\nCombining Eq. 11 and 12 we arrive at:\nT (Dold)\u2212 |Dout| \u2264 T (Dnew) (13)\nLB(Dnew,Dold, d, n) \u2264 T (Dnew) (14)\nwhich shows the derivation of Proposition 9. As shown in the experimental results (Section 5.2.1), the use of the similarity-based lower bound reduces the runtime for all datasets, with only a few exceptions, and in some cases the obtained reduction is an order of magnitude."
    },
    {
      "heading": "4.6 Caching of Optimal Subtrees (Memoisation)",
      "text": "As is common in dynamic programming algorithms, a caching or memoisation table is maintained to avoid recomputing subproblems. In our case, information about computed optimal subtrees is stored. This is used to retrieve a subtree that has already been computed when needed, provide lower bounds, and reconstruct the optimal decision tree at the end of the algorithm. Caching has been used in previous works (Nijssen and Fromont (2007); Aglin et al. (2020); Verhaeghe et al. (2019); Hu et al. (2019)), and here we extend it to support constraints on the number of nodes and incremental solving.\nThe key observation is that given a path from the root to any given node, each permutation of the feature nodes on the path results in the same dataset for the node furthest from the root, e.g., D(fi)(fj) = D(fj)(fi). This allows representing a path as a set of features,\ne.g., {fi, fj}. Each time an optimal subtree is computed during search, its path and root node are stored in the cache. If no subtree could be computed within the specified upper bound, a lower bound is derived based on the information collected during the search and the given upper bound. The bound is stored in the cache for reuse later on. Caching and lower bounds derived in this manner have been used in DL8.5 Aglin et al. (2020) to compute optimal decision trees with a given depth.\nWe generalise caching to support being used when the number of nodes is also a constraint, in addition to the depth, and allow incremental solving, i.e., the scenario when progressively large trees in terms of depth and size are computed in succession, e.g., during hyper-parameter tuning. Furthermore, we strengthen the bound introduced in DL8.5 (Aglin et al. (2020)) by using information obtained during the search.\nRecall that only the root node is stored for a given path. When necessary, the complete subtree may be reconstructed as a series of queries to the cache, where each time a single node is retrieved, as introduced in DL8 (Nijssen and Fromont (2007)). In our framework, there is an exception to the mentioned tree reconstruction procedure. After solving a tree of depth two, none of its children are stored in the cache. During the algorithm these are not necessary, but the children are needed when reconstructing the best decision tree found at the end. In this case, the required child nodes are recomputed using Algorithm 3. The computational overhead is negligible compared to the overall execution time, but this avoids storing an exponential number of paths (recall that the number of paths increases exponentially with the depth) which do not serve a purpose other than the final reconstruction."
    },
    {
      "heading": "4.6.1 Storing Subtrees and Lower Bounds in the Cache",
      "text": "Each node is associated with a path, represented as a set of features. Our cache maps a path to a list of cache entries, where each entry is composed of an optimal assignment, a lower bound, and the size limit. The depth is not explicitly stored as it can be derived based on the maximum depth and the number of features in the path. It is possible to store a lower bound without the optimal assignment, but note that the optimal assignment is the tightest lower bound. Initially, the cache is empty, and the lists and their entries are created dynamically during search as needed. There are two types of scenarios that prompt a cache storage.\nScenario one: node has been exhaustively searched and a solution has been found within the upper bound. In this situation, the computed subtree is optimal and the corresponding entry is stored using its root node assignment as the optimal assignment, the lower bound is set to the misclassification score, and the feature node limit is the limit that was assigned to the node.\nIn case the algorithm determines that the lowest classification score may be achieved using fewer nodes than imposed by the node limit, we may use the following proposition to create additional cache entries:\nProposition 10 Let T (D, d, n) be the misclassification score of the optimal decision tree for the dataset D with depth limit d and node limit n. If there exists an n\u2032 < n such that T (D, d, n\u2032) = T (D, d, n), then T (D, d, i) = T (D, d, n) for i \u2208 [n\u2032, n].\nDuring the algorithm, a given path can only be exhaustively explored once, as the next time the path is encountered its corresponding solution is retrieved from the cache (Section 4.6.2).\nScenario two: a node has been exhaustively explored, but no solution has been found within the upper bound. It follows that the optimal subtree corresponding to the path has at least as many misclassifications as the upper bound incremented by one. This is the lower bounding reasoning introduced in DL8.5 (Aglin et al. (2020)).\nIn this work, we propose a stronger lower bound. Let LB(D, d, n) be the lower bound for the number of misclassifications of an optimal decision tree for the dataset D with n nodes and depth d, i.e., T (D, d, n) \u2265 LB(D, d, n). We introduce the following refined lower bound RLB:\nRLB(D, d, n) = min{LB(D(f , d\u22121, s1)+LB(D(f, d\u22121, s2) | f \u2208 F \u2227 s1+s2 = n\u22121} (15)\nThe bound RLB considers all possible assignments of features and sizes to the root and its children, and selects the minimum sum of the lower bounds of its children. It follows that no decision tree may have a misclassification score lower than RLB. We combine RLB with the upper bound to obtain a lower bound for the case where no decision tree with less than the specified upper bound UB could be found:\nT (D, d, n) \u2265 max{RLB(D, d, n), UB + 1}. (16)\nThe proposed bound generalises the bound from DL8.5 (Aglin et al. (2020)), which only considers the second expression on the right-hand side of Eq. 16 to derive a lower bound when no tree could be found within the given upper bound. In our experiments, we observed the strengthened bound provides a speed-up by a factor of at most two on several datasets.\nOnce the bound has been computed, it is recorded in a cache entry for the path, along with the size limit of the node, and the optimal assignment is set to null."
    },
    {
      "heading": "4.6.2 Retrieving Subtrees and Lower Bounds from the Cache",
      "text": "When a new child node is created, the algorithm searches through the cache to detect if the optimal solution is already present. Ideally, the cache entry of the path matches the size limit imposed on the node, but a lower bound for the current tree may be inferred from the bounds of the larger tree, formally summarised in the following proposition.\nProposition 11 Given the dataset D and depth bound d and the maximum number of feature nodes n, a bound for a larger tree is a bound for the current tree, i.e., \u2200n\u2032 \u2265 n, d\u2032 \u2265 d : LB(D, d\u2032, n\u2032) \u2264 LB(D, d, n).\nWhen retrieving a lower bound and no lower bound has been stored for the currently queried decision tree, Proposition 11 allows inferring a lower bound from larger trees that may be stored in the cache. Note that the lower bounds are nonincreasing with size, i.e.,\nLB(D, d, n) \u2264 LB(D, d, n + 1) (17)\nThe tightest bound is returned when retrieving the lower bound. Assuming no bound with the prescribed number of nodes is stored, the bound of the smallest tree that is greater than the target size is returned should such an entry exist. If there are no applicable entries in the cache, the trivial lower bound of zero is returned. For example, given a dataset D(f1, f2) with the node limit set to five, if there is no subtree for the given size in the cache but there is an entry when the node limit was set to six and seven, then the lower bound using six nodes is the tightest valid lower bound available for the tree with five nodes."
    },
    {
      "heading": "4.6.3 Incremental Solving",
      "text": "We label incremental solving as the process of querying the algorithm to compute progressively larger decision trees. For example, once the algorithm has computed an optimal decision tree for a given depth and size, a user may be interested in a tree with more nodes to understand if the additional nodes would lead to a meaningful decrease in the misclassification score, or as part of hyper-parameter tuning.\nThe cache naturally supports these types of queries when the depth is fixed and the size is increased, since the problem of computing a larger tree includes smaller trees as its subproblems, and all cache entries remain valid. In a sense, the framework incorporates incremental solving throughout its execution. However, not all entries can be kept once the global depth is increased.\nWhen search and caching is performed, the results learned are (implicitly) only correct with respect to the maximum depth as the depth is not explicitly stored. Nevertheless, a portion of the cache entries remains valid when the depth is increased. Observe that for certain size values, increasing the depth is redundant and does not increase the search space. Intuitively, the limited tree size does not allow benefiting from a larger depth. Formally, given size s and depth d, if s \u2264 d, then the set of all possible decision trees of size s with depth d is equivalent to the set of decision trees of size s with depth d\u2032, where d\u2032 \u2265 d. As a result, when incrementally computing a globally optimal decision tree of depth d\u2032 and the cache of the computation of a tree of depth d is available with d\u2032 > d, we keep all cache entries such that |P |+ size limit \u2264 d, where P is the corresponding path of the entry, and discard the rest."
    },
    {
      "heading": "4.7 Node Selection Strategy",
      "text": "Given a feature for a node and the size allocation for its children, the algorithm decides on which child node to recurse on first. Our search strategy is a variant of post-order traversal, labelled dynamic post-order, which dynamically decides which child node to visit first. The idea is to prioritise the child node that has the heuristically-determined largest potential to improve the current decision tree, which in turn leads to a higher chance to prune to the other sibling. The potential is computed as the gap between the upper and lower bound of the child. Note that the upper bound of the parent is used as the upper bound of its children. The lower bound for a child is retrieved from the cache. This provided consistent improvements in runtime (Section 5.2.3) when compared to the strategy used in DL8.5 (Aglin et al. (2020)), which always visits the left subtree before the right subtree."
    },
    {
      "heading": "4.8 Feature and Size Selection",
      "text": "For a given node, each possible tree configuration (a feature and the size of its children) is considered one at a time, unless the node is pruned or the optimal solution is retrieved from the cache (see Subsection 4.6). The order in which tree configurations are explored may have an impact on performance, as evidenced in most search algorithms in general.\nIn our experiments, the simplest variants performed the best: features are selected in the order as given in the dataset, and the size is distributed by considering increasingly larger right subtrees as described in Algorithm 1. Alternative options are possible, such as ordering the features according to their corresponding Gini coefficient as done in heuristic algorithms, but none of these alternatives lead to sizable benefits over the simplest strategies. This is discussed in more detail in the experimental section (Section 5.2.3)."
    },
    {
      "heading": "4.9 Extensions",
      "text": "To ease the presentation, we discussed our framework in the context of binary classification on binary datasets. In this section, we discuss extensions to general settings."
    },
    {
      "heading": "4.9.1 General Data",
      "text": "The input to our framework is a dataset containing binary features. Datasets with continuous and/or categorical features are binarised. In our work, this done using a supervised discretisation algorithm based on the Minimum Description Length Principle (MDLP) by Fayyad and Irani (1993), effectively converting each feature into a categorical feature based on the statistical significance of the feature values for the class, and then using a one-hot encoding to binarise the features. We use the MDLP implementation available in the R programming package (Kim (2015)).\nNote that (univariate) decision tree algorithms implicitly binarise the dataset during learning. Each feature node is assigned a predicate that evaluates whether a given feature value meets a threshold, which can be seen as a binary feature. When binarising the dataset, the possible decisions are decided upfront rather than during execution."
    },
    {
      "heading": "4.9.2 Multi-Classification",
      "text": "To extend the algorithm for multi-classification, the key step is to generalise Algorithm 3 to compute frequency counters for each class. Equations analogous to Equations 2\u20148 are devised to compute the misclassification scores. Since classes partition the data, the complexity results remain valid for multi-classification."
    },
    {
      "heading": "4.9.3 Additional Constraints and Objective Functions",
      "text": "Our framework may be modified to support constraints and objective functions that can be expressed in the dynamic programming formulation (Eq. 1) and solved using a (variant) of the specialised algorithm (Section 4.3).\nFurthermore, objective functions that have a dependency on the number of nodes can also be handled. An alternate objective for optimal decision trees is\nmisclassifications+ \u03b1\u00d7 nodes (18)\nwhich balances the size of the decision tree against the misclassifications. This objective was used in the original CART paper (Breiman et al. (1984)) and discussed in some of the other optimal decision tree works (Bertsimas and Dunn (2017); Hu et al. (2019)). While we could extend our solution to directly work with the sparse objective, we implemented a nonintrusive modification. We compute a series of decision trees minimising the misclassification score using k nodes, where k \u2208 [0,max num nodes], and then evaluate each solution with respect to the objective and select the best. Our framework supports incremental solving, allowing cached subtrees from computing a decision tree with k\u2032 nodes to be reused when considering k\u2032 + 1 nodes. Another way of viewing the process is to consider it as a hyperparameter tuning procedure where the resulting trees are evaluated with respect to the new objective. A similar procedure was used by Bertsimas and Dunn (2017) to tune their integer programming approach.\nWe discussed multi-classification and the above objective function as special cases. Additional examples may include imposing a minimum number of instances per node, different linear penalties for misclassifying classes rather than treating each misclassification equally, and adding fairness objectives and constraints as in the work of Aghaei et al. (2019)."
    },
    {
      "heading": "5. Computational Study",
      "text": "The goal of this section is to compare the performance of our method with the state-of-theart and empirically evaluate the scalability of our algorithm and the benefits of incremental computation and lower bounding. With this in mind, we designed three major themes to investigate, each addressing a unique set of questions: variations and scalability of our approach, effectiveness compared to the state-of-the-art optimal classification tree algorithms, and out-of-sample accuracy as compared to heuristically-obtained decision trees and random forests."
    },
    {
      "heading": "5.1 Datasets and Computational Environment",
      "text": "We use publicly available datasets used in previous works (Bertsimas and Dunn (2017); Verwer and Zhang (2019); Narodytska et al. (2018); Aglin et al. (2020); Hu et al. (2019)), most of which are available from the UCI and CP4IM repositories. The datasets include 85 classification problems with a mixture of binary, categorical, and continuous features. Datasets with categorical and/or continuous features are converted into binary datasets as discussed in Section 4.9.1. Datasets with missing values were excluded from experimentation. We note that some benchmarks appeared in previous works under different binarisation techniques or simplifications, e.g., multi-classification turned into binary-classification using a \u2018one-versus-all\u2019 scheme or a subset of the features were removed. For these cases, we include the different versions as separate datasets. The binarised datasets together with the binarisation script will be readily available soon (please contact the first author in the meantime).\nExperiments were run on an Intel i-7-7700HQ CPU with 32 GB of RAM running one algorithm at a time using one processor. The timeout was set to ten minutes except for the hyper-parameter tuning where no limit was enforced. In the following, we dedicate a separate subsection to each of the three major experimental topics."
    },
    {
      "heading": "5.2 Variations of Our Algorithm and Scalability",
      "text": "The aim of this subsection is to investigate variations of our approach, determine the effectiveness of the introduced incremental computation and lower bounding techniques, analyse scalability, and impact of the feature and node selection strategies.\nIn the first part, we consider the algorithm without incremental or similarity-based lower bound computation, and observe the effect of adding these techniques to the runtime. In the second part, we discuss the impact of the number of instances, depth, and features with respect to the runtime. In the third part, we fix the algorithm parameters to their default settings and vary either the feature or node selection strategy.\nWe note that the default setting of our algorithm uses all techniques presented in the paper and the in-order feature selection and dynamic post-order node selection strategy."
    },
    {
      "heading": "5.2.1 Part One: Incremental and Lower Bound Computation",
      "text": "In Table 1, we show the effect of incrementally computing the frequency counters (Section 4.4.3) rather than doing it from scratch in each iteration, and combining it with our similarity-based lower-bound (Section 4.5).\nThe trend across all benchmarks is uniform, as each addition to the algorithm improves the runtime considerably. Incremental computation is useful as the splits considering two features might only differ in a small number of instances. Thus performing minor changes to the previously computed frequency counters is more favourable than recomputing from scratch. For similar reasons, the lower bound works well as it manages to identify cases where computing a subtree is unnecessary unless it deviates enough from the previously computed subtree.\nAn important observation that contributes to these positive points is that computing the difference of two sets of instances can be done in linear time with respect to the number of instances, which is comparatively inexpensive when considering the time spent on computing optimal subtrees of depth two using the specialised algorithm, but can save a significant amount of computation."
    },
    {
      "heading": "5.2.2 Part Two: Scalability",
      "text": "We investigate the sensitivity of our algorithm with respect to the number of instances and maximum depth. In Table 2, results are shown when our algorithm is run to compute trees of depth \u2208 [4, 5] on datasets where instances are duplicated k \u2208 [1, 2, 4] times. We note that trees with depth three are omitted as these are computed within seconds. The results indicate a linear dependency with the number of instances for the majority of the datasets. As most of the computational time is spent in repeatedly solving optimal subtrees of depth two (Section 4.4), the finding is consistent with the theoretical complexity (Proposition 6). This is a notable improvement over generic optimisation approaches, such as integer programming or SAT. The latter may exhibit an exponential runtime dependency on the number of instances as new binary variables are introduced for each instance, and typically do not consider datasets with more than a thousand instances.\nNote that the experiments regarding the scalability with respect to the number of instances are merely indicative. In practice, however, introducing more instances might implicitly increase or decrease the number of binary features in the discretisation and have\nan effect on shaping the structure of the dataset, both of which may impact positively or negatively the running time. The results do show that the bottleneck of the approach is not necessarily in the number of instances.\nIn contrast to the number of instances, the depth consistently has a large impact on the running time. The number of possible decision trees grows exponentially as the depth increases, which is reflected in the computational experiments. For example, our approach computes depth-three trees within seconds, but the runtimes go up notably for depth four and five. In previous works, depth four has been seen as the benchmark value, but with\nMurTree such trees are computable within a reasonable time, and in many cases greater depths are also possible.\nApart from the depth, another important factor is the number of binary features, which additionally dictates the number of possible decision trees necessary to explore to find the optimal tree. As the ability of our techniques to prune and reduce computational time depends on the structure of the dataset, it is difficult to artificially increase the number of features and show the dependency. For example, duplicating features would not lead to conclusive statements on the impact of the number of features on runtime, as our lower bounding mechanism would trivially prune these features. We instead refer to the computational complexity of our algorithm from Proposition 6 and the number of possible decision trees as an indicative measure of the influence of the number of binary features and sparsity of the feature vectors on the runtime. Note that some of the instances contain a high number of features, e.g., australian-un has 1163 binary features, but it is still within practical reach.\nOverall, we conclude that our approach scales reasonably well for the tested datasets with depth four, having computed the optimal decision tree for the majority of the instances within seconds, and all within ten minutes. Deeper optimal decision trees of depth five or greater remain a challenge."
    },
    {
      "heading": "5.2.3 Part Three: Feature and Node Selection Strategies",
      "text": "In Tables 3, we vary the feature and node selection strategies as presented in Section 4.8 for decision trees of depth four with fifteen decision nodes.\nThe best performing feature selection strategy is the in-order variant, which selects features in the order given in the dataset. The benefit is that there is no additional overhead introduced and it orders the features in a manner that the incremental computation and similarity-based lower bounding can exploit, as two neighbouring features in an instance tend to be similar, which is a result of binarisation. In contrast, using the Gini measure for feature ordering can be beneficial in finding a good decision tree early in the search, but it carries an overhead for each node which does not pay off in proving optimality.\nThe dynamic post-order strategy provides consistent improvement over the fixed postorder strategy used by DL8.5 (Aglin et al. (2020)). Given a node, once one of its children have been exhaustively explored, tighter upper bounds can be imposed on the other sibling. As this is the main mechanism for pruning, it is important to direct the algorithm to exhaust a child node as soon as possible. Heuristically speaking, solving the subtree with a greater potential for misclassification can yield tighter constraints on the other sibling once it has been exhaustively explored, resulting in more frequent pruning.\nTo summarise this section, the experimental results have confirmed the efficiency of our incremental frequency computation and similarity-based lower bounding approach. Each of the techniques provides a reduction in terms of runtime. Our approach scales (roughly) linearly with respect to the number of instances, and the depth of the tree has a large influence on the runtime, i.e., decision trees of depth three and four are typically computed within seconds or minutes, respectively, but trees of depth five are notably more challenging as they may timeout after ten minutes, depending on the dataset. Increasing the number of binary features increases the expected runtime, but this is difficult to measure it depends\non the effectiveness of the pruning techniques for the dataset at hand. Lastly, we found that inspecting features in the order as given in the dataset was more effective than ordering features according to their corresponding Gini coefficients, and our dynamic node selection strategy offered consistent improvements over a static strategy."
    },
    {
      "heading": "5.3 Comparison Against State-Of-The-Art Optimal Decision Tree Algorithms",
      "text": "Amongst the optimal decision tree methods discussed in Section 3, we consider DL8.5 by Aglin et al. (2020) as the main competing method. The rationale is that DL8.5 has been shown to largely outperform the other techniques based on generic optimisation modelling, such as integer programming (Verwer and Zhang (2019); Bertsimas and Dunn (2017)) and constraint programming (Verhaeghe et al. (2019)), when minimising the misclassification score given constraints on the depth of the tree. There are two other approaches worth mentioning as a direct comparison was not considered before in the literature.\nThe SAT method by Narodytska et al. (2018) takes a different approach: rather than directly minimising the misclassifications given a fixed depth, it attempts to construct the smallest decision in terms of the total number of nodes that perfectly fits the data, i.e., trees that have a misclassification score of zero. As finding the zero-misclassification tree using the complete dataset was computationally infeasible for SAT, and also prone to overfitting, the authors proposed to subsample datasets by selecting 5-20% of the instances. While this setting has its merits, it diverges from the goals of our paper. Furthermore, we found that our algorithm computes the perfect decision tree within seconds on exactly the same subsampled data used in the SAT paper and as can be seen in tables, we can directly optimise with the complete datasets. The reason for the discrepancy in runtime between our approach and SAT is that we provide a highly specialised procedure that exploits classification tree properties, e.g., Properties 1 and 2. The same reasoning holds when comparing to other generic (optimisation) frameworks such as integer programming. For these reasons, we decided not to further discuss the SAT method.\nHu et al. (2019) introduced an algorithm for minimising the weighted sum of the number of misclassifications and number of nodes (Eq. 18). We may support this objective as part of hyper-parameter tuning (see Section 4.9.3). We found that our framework computes optimal trees with the specified objective within seconds for the benchmarks used by Hu et al. (2019). We are also able to optimise with solely the misclassification criteria, whereas their algorithm relies on the sparsity weight \u03b1 having a significant impact on the optimality criteria as the main pruning technique is a bound based on \u03b1, e.g., unless \u03b1 is sufficiently high, the approach does not terminate within a reasonable time. Lastly, their method was designed for up to twelve binary features, while in our experiments, the datasets may have up to one thousand binary features. Therefore, we do not do further comparisons as their approach times out on the majority of the datasets. The runtimes given in the hyper-parameter tuning section provide an insight into the computation time taken by our method to compute the optimal tree with the sparse objective (Eq. 18). We note that better runtimes could be obtained through modifying our algorithm to directly support the objective since additional pruning can be done."
    },
    {
      "heading": "5.3.1 Comparison with DL8.5 by Aglin et al. (2020)",
      "text": "The aim of this section is to assess the effectiveness of our MurTree approach with respect to DL8.5, the state-of-the-art method for optimal decision trees. In machine learning, it is standard practice to compare learning algorithms on out-of-sample accuracy. In our case, we evaluate the algorithms solely based on runtime. This is motivated by the fact that both algorithms are directly optimising the number of misclassifications. Therefore,\nit makes sense to use runtime as the metric to assess the ability of the approaches in exhaustively exploring the search space to compute the provably optimal classification tree. A lower runtime indicates a more effective approach. We note that there may be several optimal decision trees with the same minimum misclassification score, but we do not discuss these in more detail as neither approach discriminates between them, i.e., trees are only evaluated based on their misclassification score. Analysing the particular differences among optimal decision trees is out of the scope of this work. Such an evaluation is tied to a broader question of designing a more appropriate objective function for decision trees to allow better generalisation. As out-of-sample accuracy is an important question for any machine learning algorithm, we reserved the next section to evaluate the quality of optimal decision trees in their current form as out-of-sample classifiers. We now proceed with the comparison with DL8.5.\nWe set the maximum depth to four for both methods. Since DL8.5 does not support constraining the number of nodes, the maximum number of feature nodes is set to fifteen for our method to ensure a fair comparison. We provide the complete dataset to both algorithms without dividing into the training and test set. Ten minutes is allocated for each dataset.\nThe runtimes, given in Table 4, show that our method is orders of magnitude faster than DL8.5. This is a significant result, as DL8.5 has been previously shown to outperform other techniques for optimal classification trees based on integer and constraint programming by a large margin. This further illustrates the advantage of designing and specialising domainspecific optimisation algorithms compared to using off-the-shelf tools. Both DL8.5 and our MurTree approach exploit the dynamic programming structure of decision trees, but our method employs additional techniques to further take advantage of the properties of decision trees. The reduced runtime contributes greatly towards the application of optimal classification tree methods in practice, especially when tuning is involved (see further for different tuning settings)."
    },
    {
      "heading": "5.4 Comparison Against Conventional Algorithms on Out-Of-Sample Accuracy",
      "text": "In this section, we analyse the suitability of our optimal decision trees as out-of-sample classifiers. The aim is to demonstrate that more accurate trees of limited size lead to better generalisations than what is offered by heuristic approaches. Note that the restricted size of optimal decision trees plays the role of a regulariser to avoid overfitting. The main comparison is done against an optimised implementation of CART (Breiman et al. (1984)), a widely used decision tree learning algorithm. For illustrative purposes, we also make a comparison with random forests, as a related method that typically improves accuracy over standard decision tree algorithms at the expense of being less interpretable. As will be discussed, our experiments further confirm similar empirical findings (Verwer and Zhang (2019); Bertsimas and Dunn (2017)).\nWe perform grid-search hyper-parameter tuning for each method using stratified fivefold cross-validation, and each method is tuned and evaluated on exactly the same folds. We use the framework provided by the sklearn (Pedregosa et al. (2011)) Python package for\nmachine learning. The main criteria for evaluating algorithms in this section is the accuracy of the test folds, while the training accuracy is given for completeness."
    },
    {
      "heading": "5.4.1 Comparison Against CART",
      "text": "We considered three tuning settings for our method to analyse the effect of restricting tuning options. This is mostly important for datasets for which computing the optimal decision tree is computationally expensive, and consequently limiting parameter values may be beneficial. The three settings are as follows:\n1. MT-A: Fully exploit available parameters of our algorithm until depth four, i.e., depth \u2208 1, 2, 3, 4 and feature node count \u2208 {depth, depth + 1, ..., 2depth \u2212 1}.\n2. MT-R: We compute a reduced set of parameters by considering the depth and feature node count of the best decision tree produced by CART. The heuristically obtained tree provides an upper bound on the allowed parameter values for tuning, i.e., given a CART tree with depth d and number of feature nodes s, we tune with depth \u2208 1, ..., d and feature node count \u2208 {depth, ..., s}.\n3. MT-F: Only a single parameter is set based on the CART tree. We fix the parameter values to match those produced by the best decision tree computed using CART. Note that this is not necessarily a hyper-tuning approach, but rather a method for selecting parameter values when computational time is limited.\nDue to the number of considered datasets, we divided the tables of results into two parts: the upper (Table 5) and lower half (Table 6). The training and test accuracy is provided for the best tuned version of the methods, averaged across the five folds. The total time to perform tuning and retraining is given. CART was tuned using depth \u2208 [1, 2, 3, 4].\nOur algorithm achieves consistently equal or better testing accuracy on the datasets regardless of the tuning strategy, with a few exceptions where the difference is marginal. This demonstrates that globally optimising trees captures the main features of the datasets more faithfully rather than locally optimising the tree in stages as in CART, even when not considering all possible parameter settings. The results are the best when all parameters are selected for tuning, and we observe a degradation as we limit the parameter options, but still remaining higher than CART in terms of accuracy.\nThe disadvantage of our algorithm when compared to CART is the computational time. While CART computes the trees in negligible time, the runtime to tune our algorithm may vary. In many cases, the runtime is under half a second, making it comparable to CART, but for some datasets the runtime may be in the range of hundreds or thousands of seconds. Nevertheless, the runtimes remain reasonably short and may be manageable for offline learning applications. For cases where training time is limited, our third most restricted parameter setting can serve as a possible heuristic for selecting the appropriate parameters for our method based on the characteristics of the tree produced by CART. If time is not critical, then naturally tuning with all parameters is the best option."
    },
    {
      "heading": "5.4.2 Comparison Against Random Forests",
      "text": "The main comparison is done against decision tree algorithms, but for completeness we compare optimal decision trees with tuned random forests using the same sklearn Python package. A forest of trees is typically more accurate than a single decision tree, but the resulting model is less concise and more difficult for human interpretation. The forests were tuned by varying the number of trees in the forest from [10, 50, 100], selecting the maximum depth from [no limit, 1, 2, 4], and considering a subset of the features at each step to evaluate with respect to [|F|, 12 |F|, \u221a |F|, log2(|F|)], where F is the set of features.\nWe show the results in Tables 5 and 6. Random forests tend to outperform CART in terms of accuracy, but not on every dataset. However, optimal decision trees provide higher accuracy on almost all datasets despite the simplicity of the their resulting model. This highlights that fully optimising a method may be more beneficial than using more complex but less optimised models. The runtime to tune random forests differs among the datasets, but it is clear that it is no longer negligible as for CART. Our approach can achieve lower or comparative training times when compared to random forests, although for several datasets the runtimes may be considerably higher."
    },
    {
      "heading": "6. Conclusion",
      "text": "We presented MurTree, a framework for computing optimal decision trees, i.e., decision trees that achieve the best representation of the data in terms of minimising the number of\nmisclassifications. The framework is based on dynamic programming and search. Our novel techniques exploit decision tree properties to provide orders of magnitude speed-ups when compared to the state-of-the-art. The conducted experimental study shows that optimal decision trees are highly desirable as their out-of-sample accuracy is greater than decision trees and random forests obtained using conventional learning algorithms, while providing concise and interpretable models. Traditional heuristic algorithms are typically faster, but we show that for the majority of the datasets tested our approach can compute optimal trees within seconds or minutes.\nConsidering novel metrics for evaluating optimality to improve the ability to generalise better on unseen data may be a direction for future work. In particular, pruning techniques from heuristic approaches, that are typically applied as a post-processing step, may be incorporated directly in the optimal decision tree objective. Analysing the effect of supervised discretisation algorithms for binarising the datasets may lead to additional insights. Lastly, our efforts were mainly focussed on trees of depth at most five, and extending our techniques to handle much deeper trees, such as depth ten, would be of interest for particular applications."
    }
  ],
  "title": "MurTree: Optimal Classification Trees via Dynamic Programming and Search",
  "year": 2020
}
