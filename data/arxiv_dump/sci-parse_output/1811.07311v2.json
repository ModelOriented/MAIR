{
  "abstractText": "As machine learning algorithms continue to improve, there is an increasing need for explaining why a model produces a certain prediction for a certain input. In recent years, several methods for model interpretability have been developed, aiming to provide explanation of which subset regions of the model input is the main reason for the model prediction. In parallel, a significant research community effort is occurring in recent years for developing adversarial example generation methods for fooling models, while not altering the true label of the input,as it would have been classified by a human annotator. In this paper, we bridge the gap between adversarial example generation and model interpretability, and introduce a modification to the adversarial example generation process which encourages better interpretability. We analyze the proposed method on a public medical imaging dataset, both quantitatively and qualitatively, and show that it significantly outperforms the leading known alternative method. Our suggested method is simple to implement, and can be easily plugged into most common adversarial example generation frameworks. Additionally, we propose an explanation quality metric -APE \u201cAdversarial Perturbative Explanation\u201d, which measures how well an explanation describes model decisions.",
  "authors": [
    {
      "affiliations": [],
      "name": "Yoel Shoshan"
    },
    {
      "affiliations": [],
      "name": "Vadim Ratner"
    }
  ],
  "id": "SP:02b5d296c0828e52fec0a8f7b96aba521626c1ac",
  "references": [
    {
      "authors": [
        "C.-H. Chang",
        "E. Creager",
        "A. Goldenberg",
        "D. Duvenaud"
      ],
      "title": "Interpreting neural network classifications with variational dropout saliency maps",
      "venue": "In Proc. NIPS, 2017",
      "year": 2017
    },
    {
      "authors": [
        "P. Dabkowski",
        "Y. Gal"
      ],
      "title": "Real time image saliency for black box classifiers",
      "venue": "In Advances in Neural Information Processing Systems,",
      "year": 2017
    },
    {
      "authors": [
        "R.C. Fong",
        "A. Vedaldi"
      ],
      "title": "Interpretable explanations of black boxes by meaningful perturbation",
      "venue": "arXiv preprint arXiv:1704.03296,",
      "year": 2017
    },
    {
      "authors": [
        "I.J. Goodfellow",
        "J. Shlens",
        "C. Szegedy"
      ],
      "title": "Explaining and harnessing adversarial examples",
      "venue": "arxiv, 2018",
      "year": 2018
    },
    {
      "authors": [
        "M. Heath",
        "K. Bowyer",
        "D. Kopans",
        "R. Moore",
        "W.P. Kegelmeyer"
      ],
      "title": "The digital database for screening mammography",
      "venue": "In Proceedings of the 5th international workshop on digital mammography,",
      "year": 2000
    },
    {
      "authors": [
        "D.P. Kingma",
        "J. Ba"
      ],
      "title": "Adam: A method for stochastic optimization",
      "venue": "arXiv preprint arXiv:1412.6980,",
      "year": 2014
    },
    {
      "authors": [
        "A. Kurakin",
        "I. Goodfellow",
        "S. Bengio"
      ],
      "title": "Adversarial examples in the physical world",
      "venue": "arXiv preprint arXiv:1607.02533,",
      "year": 2016
    },
    {
      "authors": [
        "A. Modas",
        "S.-M. Moosavi-Dezfooli",
        "P. Frossard"
      ],
      "title": "Sparsefool: a few pixels make a big difference",
      "venue": "arXiv preprint arXiv:1811.02248,",
      "year": 2018
    },
    {
      "authors": [
        "R. Rey-de Castro",
        "H. Rabitz"
      ],
      "title": "Targeted nonlinear adversarial perturbations in images and videos",
      "venue": "arXiv preprint arXiv:1809.00958,",
      "year": 2018
    },
    {
      "authors": [
        "H. Robbins",
        "S. Monro"
      ],
      "title": "A stochastic approximation method",
      "venue": "Ann. Math. Statist., 22(3):400\u2013407,",
      "year": 1951
    },
    {
      "authors": [
        "K. Simonyan",
        "A. Vedaldi",
        "A. Zisserman"
      ],
      "title": "Deep inside convolutional networks: Visualising image classification models and saliency maps",
      "venue": "arXiv preprint arXiv:1312.6034,",
      "year": 2013
    },
    {
      "authors": [
        "C. Szegedy",
        "V. Vanhoucke",
        "S. Ioffe",
        "J. Shlens",
        "Z. Wojna"
      ],
      "title": "Rethinking the inception architecture for computer vision",
      "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
      "year": 2016
    },
    {
      "authors": [
        "M.D. Zeiler"
      ],
      "title": "Adadelta: an adaptive learning rate method",
      "venue": "arXiv preprint arXiv:1212.5701,",
      "year": 2012
    },
    {
      "authors": [
        "C. Zhang",
        "Z. Yang",
        "Z. Ye"
      ],
      "title": "Detecting adversarial perturbations with saliency",
      "venue": "arXiv preprint arXiv:1803.08773,",
      "year": 2018
    }
  ],
  "sections": [
    {
      "heading": "1. Introduction",
      "text": "As machine learning algorithms continue to improve, there is an increasing need for explaining why a given model produces a certain prediction. The benefits of such explanation fall roughly into two categories: explaining the end result for the user, and analysis of the network by the researcher. An example of the former is providing localized information on medical images classification to understand which areas in a given image made a model to classify an image as malignant. An example of the latter is that by seeing what caused decisions in a model, one can understand what needs to be improved in the algorithm.\nIn recent years, several methods for detecting saliency\nhave been developed, aiming to provide explanation of which subset regions of the model input are the main reason for the model prediction. Part of the methods [?, 3, 1]focus on introducing perturbations into the input of a model and then analyzing the modified model prediction.\nIn parallel, a significant effort is being put in recent years into developing adversarial examples generation methods for fooling models, usually aiming to keep the \u201ctrue label\u201d of the input, as will be classified by a human reader ([8, 4]). The goal of such adversarial attacks is to identify unwanted behavior of a network and exploit it.\nIn this paper, we bridge the gap between the domains of network explanation and adversarial examples. We introduce a modification to the adversarial example generation process which aims to maximize interpretability. Similarly to the domain of adversarial attacks, we change the input in a way that affects the output of a model. Unlike previous works on model explanation, our method does not require any reference \u201cdeletion\u201d image and does not require training an additional NN model. Additionally, our method provides explanation in the full resolution of the original input.\nWe analyze the proposed method on a public medical imaging dataset, both quantitatively and qualitatively, and show that it significantly outperforms the leading known alternative method.\nThis modification is simple to implement, and can be easily plugged in into most common adversarial example generation frameworks. The resulting method is also applicable to non-image-related tasks.\nThe paper is organized as follows: Section 2 reviews related work, in section 3 we introduce a new metric that we use to measure performance, section 4 describes the proposed method, section 5 presents experimental data, section 6 discusses key differences from existing methods, and section 7 discusses and concludes this work."
    },
    {
      "heading": "2. Related Work",
      "text": "When analyzing a model (e.g. neural network) based classifier, one of the questions that arise is given an input and a classification results, which parts of the input most af-\n1\nar X\niv :1\n81 1.\n07 31\n1v 2\n[ cs\n.L G\n] 2\n1 N\nov 2\n01 8\nfect the result. This gave rise to several methods of deriving saliency maps. In particular, several gradient-based techniques were proposed, such as [11], computing the gradient of the classification loss with respect to the image, and using the gradient magnitude as a measure of saliency. However, this resulted in highly irregular saliency maps. This issue was addressed by Fong and Vedaldi [3]. There, the authors define perturbations of the input as a cross-fade between the original image and an image that represents deletion of data. Regularization terms were introduced when deriving the cross-fade \u201cmask\u201d, which was then used as a saliency map. Chang et al.[1] suggest an alternative method in which a generative model (Variational Auto-Encoder) attempts to \u201cdelete\u201d region by inpainting. Dabkowski and Gal [2] achieve real-time performance by training a network to directly generate saliency masks from images, with the training data still dependent on mask and reference image. Rey-de-Castro and Rabitz [9] suggest adding a perturbation generator network, a differentiable neural network component that learns to distort the input image.\nAn important relevant field of study, which is gaining popularity in recent years is adversarial attacks. [4]. There, the original image is perturbed to modify a classifier prediction, in a way that will not be noticed by a human reader. Usually, small changes of the input are acceptable, as long as the \u201ctrue label\u201d (the label that a human will assign to this input) remains unchanged."
    },
    {
      "heading": "3. Proposed Metric",
      "text": ""
    },
    {
      "heading": "3.1. Saliency",
      "text": "Defining image saliency in the context of a neural network is a non trivial matter. In non formal terms, given a model, an input, and a model prediction based on the given input, the saliency map should explain how different parts of the input influenced the prediction. There is strong motivation to find saliency generation methods and metrics. A meaningful saliency map should help us see through the \u201cblack box\u201d of the model. It may help us discover situations when a classifier model got the right answers but due to the wrong reasons. For example, in the famous story in which a classifier was trained to discover camouflaged tanks, and performed very well when looking only at classification results. When the results were analyzed, it was discovered that all of the camouflaged tanks images were taken on cloudy days, while the tank-less images were taken on sunny days. A proper saliency method would clearly have revealed this bias, and show that the classifier didn\u2019t really learn to detect the tanks at all. An additional example is explaining a medical imaging classifier decision that helps to deliver more localized information on the decision, which may help in providing more information to the human reader, increase trust of the model and even lead to dis-\ncovery of new features undiscovered by humans until now."
    },
    {
      "heading": "3.2. Proposed APE metric - \u201cAdversarial Perturbative Explanation\u201d",
      "text": "Ground truth (GT) based metrics, by themselves, do not represent well explanation performance. Firstly, we are explaining a model prediction behavior that may be flawed to begin with. In such case it is desirable that the explanation will reveal the weakness of the model. Secondly, there may be evidence outside the annotated GT object that may legitimately influence a model decision.\nTherefore, to be able to quantitatively compare between explanation methods, we formulate the APE (Adversarial Perturbative Explanation) metrics.\nPerturbation based explanation describes regions that affect the decision of a model, given a modified (perturbed) input. When creating a formal metric, we considered two main aspects. Requirement 1: In the spirit of Ockham\u2019s razor, we want the explanation to be as \u201csimple\u201d as possible. Requirement 2: Additionally, it should suppress (or alternatively, maintain) class evidence, depending whether a SDR (smallest destroying region) or a SSR (smallest sufficient region) [2], respectively, is required. We define two metrics corresponding to SDR and SSR, APED and APES respectively.\nNote: when deriving this metric we were inspired by Fong and Vedaldi [3] loss formulation, with the key difference that we use L0 directly and not an approximation of it in one of the terms.\n3.2.1 APED\nLet us consider a model M , an input I , a perturbed input I\u0303 , and class index i. Firstly, we define I\u0302 to be the clipped version of I\u0303 , constraining it to remain within valid input value range. Let binarization be defined as:\nB (f) (x) = { 1 f (x) 6= 0 0 f (x) = 0\n(1)\nwe propose a saliency metric composed of the following terms:\n1. Classification term:\nEc\n( I\u0302 ) = M ( I\u0302 ) i\n(2)\nM(I\u0302)ibeing the prediction of the model given input I\u0302 w.r.t. class index i. This term expresses the \u201cdestructiveness\u201d of the saliency region w.r.t. class i.\n2. Sparsity term:\nEs\n( I, I\u0302 )\n= L0\n( I \u2212 I\u0302 ) (3)\n3. Smoothness term:\nEr\n( I, I\u0302 ) = TV ( B ( I \u2212 I\u0302 )) (4)\nThe overall proposed saliency metric is defined as follows:\n1\nN L0\n( I \u2212 I\u0302 ) + 1\nN TV\n( B ( I \u2212 I\u0302 )) +M ( I\u0302 ) i (5)\nN = number of elements on I Classification term Eq.2 addresses requirement 1. Sparsity and smoothness terms Eqs. 3, 4(combined) address requirement 2.\n3.2.2 APES\nSimilarly to the SDR version, we define I\u0302 to be the clipped version of I\u0303 , constraining it to remain within valid input value range.\n1. Classification term:\nEc\n( I\u0302 ) = \u2223\u2223\u2223M (I\u0302)\ni \u2212M(I)i \u2223\u2223\u2223 (6) M(I\u0302)ibeing the prediction of the model given input I\u0302 w.r.t. class index i. This term expresses how well the original model classification is preserved.\n2. Sparsity term:\nEs\n( I, I\u0302 )\n= L0\n( 1\u2212B ( I \u2212 I\u0302 )) (7)\n3. Smoothness term:\nEr\n( I, I\u0302 ) = TV ( 1\u2212B ( I \u2212 I\u0302 )) (8)\nThe overall proposed saliency metric is defined as follows:\n1\nN L0\n( 1\u2212B ( I \u2212 I\u0302 )) + 1\nN TV\n( 1\u2212B ( I \u2212 I\u0302 )) +\u2223\u2223\u2223M (I\u0302)\ni \u2212M(I)i \u2223\u2223\u2223 . (9) N = number of elements on I\n3.2.3 APE terms weights\nIn some settings, it may prove useful to define coefficients that provide different weights to each term.\nWe formalize such term weighting in eq. 10, \u03b1sp, \u03b1sm and \u03b1cl being the sparsity, smoothness and classification weight respectively. We formalize it for APED, but it can be symmetrically formalized for APESwhich we omit for brevity.\n\u03b1sp 1\nN L0\n( I \u2212 I\u0302 ) +\u03b1sm 1\nN TV\n( B ( I \u2212 I\u0302 )) +\u03b1clM ( I\u0302 ) i\n(10) In this paper, when measuring APE based performance (table 1), we only use \u03b1sp = 1, \u03b1sm = 1, \u03b1cl = 1 (eq. 10) as we feel that this is appropriate for the examined domain and task. However, it is possible that in a different domain or task, different metric coefficients will be more suitable. For example, if in the examined domain or task, it is known in advance that objects are relatively big, \u03b1sm can be increased and possibly \u03b1sp can be reduced. Such modification will strongly favor smooth connected components explanations and discourage too sparse explanations."
    },
    {
      "heading": "4. Proposed Method",
      "text": "We focus only on optimizing w.r.t. APED , as we believe that in certain domains, such as medical imaging, SDR is preferred, since it explores less drastic modifications to the input and allows the model to consider larger context.\nLet I\u0302 be the perturbed input. Since support function is not continuously differentiable, we approximate binarization Eq. 1 by the following:\nB ( I \u2212 I\u0302 ) (x) h S (x) \u2261\n2 1 + exp ( \u2212\u03b3 (\u2223\u2223\u2223I (x)\u2212 I\u0302 (x)\u2223\u2223\u2223\u2212 \u03b5)) \u2212 1. (11)\nThe result approximates a smoothed step function which receives negative values below \u03b5 (we found \u03b3 = 30, \u03b5 = 0.01 to achieve good results).\nInspired by Kurakin et al.[7] we formulate an optimization problem which aims to find I\u0302 that minimizes the proposed APED metric 5.\nPhase 1:We initialize I\u0302 to I . Then, we iteratively modify I\u0302 using the gradient of the loss w.r.t. the input, while keeping model M frozen.\nAny gradient based optimizer may be used, including, for example, SGD [10], Adam [6], AdaDelta [13]. For brevity we describe the SGD update step, while in practice we use Adam [6] optimizer..\n4I\u0302 = \u2212\u03b4 \u2202E \u2202I\u0302k\n(12)\nThe loss E is a smooth version of the saliency metric:\nE ( I, I\u0302 ) = M ( I\u0302 ) i + \u03b1sum (S) + \u03b2TV (S) (13)\nThe first term reduces classification value of class i. The second term approximates the size of support of P , with\ntwo important differences. Firstly, as was mentioned previously, it is smooth w.r.t. S. Secondly, very small values of the perturbation result in negative values of S, decreasing the overall value of the second member. This should encourage close-to-zero perturbations over most of the image. This will also be useful later when we derive the saliency mask. The third term encourages smoothness of S, preferring continues regions of non-zero values over scattered individual elements (pixels in the case of images).\nOn each iteration, after computing the update step Eq. 12we constrain I\u0302 to remain in the original applicable values range which I is sampled from, by clipping its values. After either completing a defined number of iterations, or a when reaching convergence, we derive the saliency mask by thresholding S at zero:\nsal (x) = { 1 S (x) \u2265 0 0 S (x) < 0\n(14)\nWe found that this is sufficient for the purpose of explanation, however, zeroing out some regions of P may increase the classification term, increasing the overall APEDscore. We therefore introduce phase 2, which finds the smallest achievable classification probability (for class i), given that perturbations are only allowed within the mask derived in 14\nPhase 2: Small (below \u03b5) perturbations of the input may also affect classification (and classification term), and eliminating those perturbations in Eq. 14 may increase overall loss. In order to guarantee that classification loss remain small for the derived saliency mask, we introduce the second phase of the algorithm. The purpose of this stage is to make sure that we minimize the classification term while allowing perturbations only within the mask derived in Eq. 14. For this purpose we find a perturbed image, similarly to Eq. 12, that is non-zero only inside the mask, starting with I\u030c = I ,:\n4I\u030c (x) = \u2212\u03b4 \u2202E \u2202I\u030ck sal (x) (15)\nThis time, we only minimize the classification term.\nE ( I\u030c ) = M ( I\u030c ) i\n(16)\nOn this second phase we drop both sparsity and smoothness terms to allow non-regulated changes to occur within the mask regions, compensating for ommiting the out-ofmask changes."
    },
    {
      "heading": "5. Experiments",
      "text": "We compare our method with Fong and Vedaldi ([3]) both qualitatively and quantitatively. Quantitative comparison is based on two metrics. One is the metric discussed in section 3 (APED). The other measures a ground truth\n(GT) match by counting the portion of the explanation mask CCs (Connected Components) that intersect with the tested object. The proposed method performs significantly better performance w.r.t. both metrics (Table 1).\nTo evaluate our method we have selected the DDSM dataset [5], a digital database for screening mammography. The main reasons are a. The dataset is publicly available; b. The images are high resolution averaging at around 6000x4000 pixels; c. The objects are of varying sizes ranging between pixel sized micro-calcifications to large tumors; d. Malignancy classification is a hard and non-trival task, representing an interesting setting to explore explainability."
    },
    {
      "heading": "5.1. Model and experiment setting",
      "text": "For the purpose of the experiment we train a single inception-resnet-v3[12] model on per-image classification task of malignant vs. non-malignant images. The model architecture is modified to accept single-channel inputs (grayscale). Feature extraction layers up to 2 layers after \u201cmixed 6a\u201d layer are kept, followed by a fully connected layer of size 256, and finally a fully connected layer of size 2, followed by a softmax operation over the two classes \u201cmalignant\u201d vs. \u201cnon-malignant\u201d. It is important to note that, during training, the model is never exposed to localization information, and is trained w.r.t. the overall image malignancy label (the presence of at least a single malignant finding). This results in \u02dc8M trainable parameters. Standard batch-normalization was used.\nWe randomly split the dataset into 80% train-set (\u02dc8000 images) and 20% validation-set (\u02dc2000 images), making sure that no patient appears on both train and validation sets. The model is tested on the validation set on which it achieves a \u02dc0.8 rocauc in the mentioned classification task. The qualitative and quantitative (table 1) results are calculated on the validation set.\nWe explore several setups (Table 1) of both methods w.r.t. L0approximation, Total Variation (TV) and tv\u03b3 (Total Variation \u03b3) [3]. Masks for Fong and Vedaldi [3] are thresholded with T value which provides the bestAPEDscore for the entire validation set, scanned at steps of 1e-6. Masks for our proposed method are calculated by thresholding with 0 (eq. 14) \u03b3 = 30, \u03b5 = 0.01 (eq. 11). We do not scan for additional \u03b3and values as our initial \u201cguess\u201d worked well."
    },
    {
      "heading": "5.2. GT localization metric",
      "text": "DDSM dataset contains localized GT (ground truth) information delineating malignant lesions. Since it is important to know if the perturbation method managed to \u201cfool\u201d the model in nonsensical ways, especially in the context of adversarial example generation, we examine how well the explanation masks correlate with the actual findings in the images. For this purpose, we take each generated mask, and\nmeasure which percentage of its CCs (connected components) have non zero intersection with GT lesions/objects. See table 1."
    },
    {
      "heading": "5.3. Results",
      "text": "As demonstrated in Table 1, our method achieves best performance on both APED and GT localization metrics. Furthermore, we achieve top results on each separate APEDcomponent. Figure 1 1 shows 4 representative example images, demonstrating GT malignant lesions, our proposed method explanation mask and an alternative method explanation mask. Figure 2 2shows how the adversarial perturbation images look like for the compared methods, zoomed in at the malignant lesion (the object that the model should search for). It can be seen how Fong and Vedaldi [3] either darkens or blurs regions localized at the lesion area, depending on the reference deletion image, while our proposed method generates localized adversarial \u201cpatches\u201d. It can also be seen that our method generates significantly less localized false positive mask locations when compared with the GT, which explains why our proposed method got, quantitatively, better GT localization performance results. (\u201cGT localization results\u201d column in table 1)."
    },
    {
      "heading": "5.4. Other methods",
      "text": "In addition to Fong and Vedaldi [3] we tried to get performance results for Rey-de-Castro and Rabitz [9]. However, no matter what hyper parameters we tried, we could not make it generate meaningful explanations. We either got an almost full or completely empty mask. We believe that it\u2019s due to the relatively small capacity of the perturbation generator architecture and its lack of ability to capture complex objects due to a too local context. We believe that it can be seen, on Rey-de-Castro and Rabitz [9] visualizations, where it appears clear that the perturbation generator, due to lack of capacity, resorted into distorting most edges in the image, many times regardless of correlation with the relevant GT object. This can be seen, for example, in figure 3 in [9] where non-cat-related edges are highlighted in addition to the cat-related edges. However, on medical images the images are rich with edges, and the vast majority of them are irrelevant to potential malignancy. We tried to increase the model capacity by increasing the number of layers, and the number of input/output channels, excluding the first and the last layer which consist of a single output channel, but it did not help the model converge. We tried layers numbers ranging between 1 to 30, and convolution filters number ranging from 1 to 128 on each layer. It is possible that a larger model and/or a different architecture for the perturbation generator will be able to converge, however, we did not manage to find it."
    },
    {
      "heading": "6. Difference from existing methods",
      "text": "In this section we will compare the proposed method with some of the state-of-the-art methods in the field. In [3], Fong and Vedaldi describe a loss function which is similar to ours (namely, classification, smoothness and sparsity approximation). However there are several main differences between the methods:\n1. The requirement to provide a \u201cdeletion\u201d image. For example, a blurred version of the input. It is not always clear what is the best way to delete information, especially in domains outside natural images. Our proposed method does not require providing any reference \u201cdeletion\u201d image. A dark or blurry region in medical imaging setting does not always equal a lack of evidence or objects. Additionally, since in practice in their described method, an element-wise cross-fade is performed between the original input and the reference \u201cdeletion\u201d image, the possible pixel values are limited. As an extreme example, if both the input and the reference \u201cdeletion\u201d image have the same value, their method will not be able to provide explanation that perturb this element at all. Furthermore, non visual features like, for example, clinical data records, or stocks market values, may have complex relations between them, and it is not clear what the reference \u201cdeletion\u201d values should be.\n2. In some settings it is better to provide explanations in the original input full resolution. An example is calcifications (accumulation of calcium salts in a body tissue) as seen on xray, which may be as small as a single pixel, but provide valuable medical evidence. Our proposed method converges well and does not seem to overfit, unlike what Fong and Vedaldi describe.\nIn [1], Chang et al. introduce a different way to suppress the information within a region of an image. They train a generative model (Variational Auto-Encoder) to impute information inside a mask. Training a generative model (such as GAN or VAE) is a non trivial matter, and our proposed method does not require it.\nRey-de-Castro and Rabitz in [9] propose to train an additional model, to generate image perturbations. While there are several similarities to our proposed methods, there are key differences:\n1. The method requires an additional perturbation generator model. Firstly, choosing an appropriate architecture may prove difficult. On one hand, a too simplistic model may not be sufficient to capture the desired behavior, due to low capacity and/or lack of large enough context. On the other hand, a complex model may be difficult to train. In contrast, our proposed\nmethod does not require constructing any additional model architecture, as we perturb the values of the input directly. It\u2019s worth noting that our modification of the input is non-linear, as it originates from gradients propagation through a non-linear function (the original model).\n2. The lack of total variation in the proposed loss function does not encourage \u201csimple\u201c explanations.\n3. Rey-de-Castro and Rabitz optimize for L1, while we optimize to minimize a closer approximation of L0 (eq. 11).\nAdditionally, as we describe in section 5.4 it proved difficult, in practice, to make the perturbation generator provide meaningful explanations that go beyond perturbing most edges in the image, regardless of their relation to any object."
    },
    {
      "heading": "7. Conclusions",
      "text": "In this paper we presented a new method for explaining the decision of a given model on a specified input. The method is based on adversarial examples generation, which, when constrained to simple changes, is shown to provide well-localized meaningful explanations. We test the method on a public dataset of breast mammogram images, and show that it significantly outperforms the current state-of-the art in the field, both quantitatively, using heuristic metrics and ground-truth-based comparison, and qualitatively.\nWhile the analyzed network is never exposed to localization information, the proposed explanation method also extracts meaningful local cues. Extending this functionality within the framework of weakly-supervised segmentation is part of currently ongoing work."
    }
  ],
  "title": "Regularized adversarial examples for model interpretability",
  "year": 2018
}
