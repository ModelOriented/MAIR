{
  "abstractText": "Explaining the behaviors of deep neural networks, usually considered as black boxes, is critical especially when they are now being adopted over diverse aspects of human life. Taking the advantages of interpretable machine learning (interpretable ML), this paper proposes a novel tool called Catastrophic Forgetting Dissector (or CFD) to explain catastrophic forgetting in continual learning settings. We also introduce a new method called Critical Freezing based on the observations of our tool. Experiments on ResNet articulate how catastrophic forgetting happens, particularly showing which components of this famous network are forgetting. Our new continual learning algorithm defeats various recent techniques by a significant margin, proving the capability of the investigation. Critical freezing not only attacks catastrophic forgetting but also exposes explainability.",
  "authors": [
    {
      "affiliations": [],
      "name": "Giang Nguyen"
    },
    {
      "affiliations": [],
      "name": "Shuan Chen"
    },
    {
      "affiliations": [],
      "name": "Tae Joon Jun"
    },
    {
      "affiliations": [],
      "name": "Daeyoung Kim"
    }
  ],
  "id": "SP:571754090add3576aea532b93b7b3bbd87e5405b",
  "references": [
    {
      "authors": [
        "Z. Che",
        "S. Purushotham",
        "R. Khemani",
        "Y. Liu"
      ],
      "title": "Interpretable deep models for icu outcome prediction",
      "venue": "AMIA Annual Symposium Proceedings. vol. 2016, p. 371. American Medical Informatics Association",
      "year": 2016
    },
    {
      "authors": [
        "P. Dabkowski",
        "Y. Gal"
      ],
      "title": "Real time image saliency for black box classifiers",
      "venue": "Advances in Neural Information Processing Systems. pp. 6967\u20136976",
      "year": 2017
    },
    {
      "authors": [
        "I.J. Goodfellow",
        "M. Mirza",
        "D. Xiao",
        "A. Courville",
        "Y. Bengio"
      ],
      "title": "An empirical investigation of catastrophic forgetting in gradient-based neural networks",
      "venue": "arXiv preprint arXiv:1312.6211",
      "year": 2013
    },
    {
      "authors": [
        "S. Hou",
        "X. Pan",
        "C. Change Loy",
        "Z. Wang",
        "D. Lin"
      ],
      "title": "Lifelong learning via progressive distillation and retrospection",
      "venue": "Proceedings of the European Conference on Computer Vision (ECCV). pp. 437\u2013452",
      "year": 2018
    },
    {
      "authors": [
        "R. Kemker",
        "M. McClure",
        "A. Abitino",
        "T.L. Hayes",
        "C. Kanan"
      ],
      "title": "Measuring catastrophic forgetting in neural networks",
      "venue": "Thirty-second AAAI conference on artificial intelligence",
      "year": 2018
    },
    {
      "authors": [
        "J. Kirkpatrick",
        "R. Pascanu",
        "N. Rabinowitz",
        "J. Veness",
        "G. Desjardins",
        "A.A. Rusu",
        "K. Milan",
        "J. Quan",
        "T. Ramalho",
        "A Grabska-Barwinska"
      ],
      "title": "Overcoming catastrophic forgetting in neural networks",
      "venue": "Proceedings of the national academy of sciences 114(13), 3521\u20133526",
      "year": 2017
    },
    {
      "authors": [
        "Z. Li",
        "D. Hoiem"
      ],
      "title": "Learning without forgetting",
      "venue": "IEEE transactions on pattern analysis and machine intelligence 40(12), 2935\u20132947",
      "year": 2017
    },
    {
      "authors": [
        "T.Y. Lin",
        "M. Maire",
        "S. Belongie",
        "J. Hays",
        "P. Perona",
        "D. Ramanan",
        "P. Doll\u00e1r",
        "C.L. Zitnick"
      ],
      "title": "Microsoft coco: Common objects in context",
      "venue": "European conference on computer vision. pp. 740\u2013755. Springer",
      "year": 2014
    },
    {
      "authors": [
        "C.X. Ling",
        "T. Bohn"
      ],
      "title": "A unified framework for lifelong learning in deep neural networks",
      "venue": "arXiv preprint arXiv:1911.09704",
      "year": 2019
    },
    {
      "authors": [
        "U. Michieli",
        "P. Zanuttigh"
      ],
      "title": "Incremental learning techniques for semantic segmentation",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision Workshops. pp. 0\u20130",
      "year": 2019
    },
    {
      "authors": [
        "C.V. Nguyen",
        "A. Achille",
        "M. Lam",
        "T. Hassner",
        "V. Mahadevan",
        "S. Soatto"
      ],
      "title": "Toward understanding catastrophic forgetting in continual learning",
      "venue": "arXiv preprint arXiv:1908.01091",
      "year": 2019
    },
    {
      "authors": [
        "G. Nguyen",
        "T.J. Jun",
        "T. Tran",
        "D. Kim"
      ],
      "title": "Contcap: A comprehensive framework for continual image captioning",
      "venue": "arXiv preprint arXiv:1909.08745",
      "year": 2019
    },
    {
      "authors": [
        "K. Shmelkov",
        "C. Schmid",
        "K. Alahari"
      ],
      "title": "Incremental learning of object detectors without catastrophic forgetting",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision. pp. 3400\u20133409",
      "year": 2017
    },
    {
      "authors": [
        "K. Simonyan",
        "A. Vedaldi",
        "A. Zisserman"
      ],
      "title": "Deep inside convolutional networks: Visualising image classification models and saliency maps",
      "venue": "arXiv preprint arXiv:1312.6034",
      "year": 2013
    },
    {
      "authors": [
        "O. Tasar",
        "Y. Tarabalka",
        "P. Alliez"
      ],
      "title": "Incremental learning for semantic segmentation of large-scale remote sensing data",
      "venue": "IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing 12(9), 3524\u20133537",
      "year": 2019
    },
    {
      "authors": [
        "J. Yosinski",
        "J. Clune",
        "A. Nguyen",
        "T. Fuchs",
        "H. Lipson"
      ],
      "title": "Understanding neural networks through deep visualization",
      "venue": "arXiv preprint arXiv:1506.06579",
      "year": 2015
    },
    {
      "authors": [
        "M.D. Zeiler",
        "R. Fergus"
      ],
      "title": "Visualizing and understanding convolutional networks",
      "venue": "European conference on computer vision. pp. 818\u2013833. Springer",
      "year": 2014
    },
    {
      "authors": [
        "F. Zenke",
        "B. Poole",
        "S. Ganguli"
      ],
      "title": "Continual learning through synaptic intelligence",
      "venue": "Proceedings of the 34th International Conference on Machine Learning-Volume 70. pp. 3987\u20133995. JMLR. org",
      "year": 2017
    },
    {
      "authors": [
        "M. Zhai",
        "L. Chen",
        "F. Tung",
        "J. He",
        "M. Nawhal",
        "G. Mori"
      ],
      "title": "Lifelong gan: Continual learning for conditional image generation",
      "venue": "Proceedings of the IEEE International Conference on Computer Vision. pp. 2759\u20132768",
      "year": 2019
    },
    {
      "authors": [
        "L.M. Zintgraf",
        "T.S. Cohen",
        "T. Adel",
        "M. Welling"
      ],
      "title": "Visualizing deep neural network decisions: Prediction difference analysis",
      "venue": "arXiv preprint arXiv:1702.04595",
      "year": 2017
    }
  ],
  "sections": [
    {
      "text": "Keywords: Explainable AI \u00b7 Explainable Deep Learning \u00b7 Catastrophic Forgetting \u00b7 Continual Learning."
    },
    {
      "heading": "1 Introduction",
      "text": "Regarding human evolution, life-long learning has been considered as one of the most crucial abilities, helping us develop more complicated skills throughout the lifetime. The idea of this learning strategy is hence deployed extensively by the deep learning community. Life-long learning (or continual learning) enables machine learning models to perceive new knowledge while simultaneously exposing backward-forward transfer, non-forgetting, or few-show learning [9]. While the aforementioned properties are the ultimate goals for life-long learning systems, catastrophic forgetting or semantic drift naturally occurs in deep neural networks in life-long learning settings because they are vastly optimized upon gradient descent algorithm [3].\nCatastrophic forgetting is defined as when we use a trained model on a given domain to address a new task, due to adapting to the new data samples, the model forgets what it learned before on the old domain. As shown in Fig. 1, if we use fine-tuning for continual learning, the knowledge acquired from the previous task will be eradicated and the inference becomes irrelevant. In fact,\nar X\niv :2\n00 5.\n01 00\n4v 2\n[ cs\n.L G\n] 8\nO ct\nthe description of the cat photo is shifted from \u201ca cat laying on a bed next to a blanket\u201d to \u201ca person is standing in an orange room\u201d.\nAlthough catastrophic forgetting is tough and undesirable, research to understand this problem is rare amongst the deep learning community. The interest in understanding or measuring catastrophic forgetting does not commensurate with the number of research to deal with this problem. Kemker et al [5] develop new metrics to help compare continual learning techniques fairly and directly. Nguyen et al [11] study which properties cause the hardness for the learning process. By modeling the chosen properties using task space, they can estimate how much a model forgets in a sequential learning scenario, shedding light on factors affecting the error rate on a task sequence. However, they can not show us what is being forgotten or which components are forgetting inside the model, but revealing what properties of tasks trigger catastrophic forgetting. By comparison, our work focuses on understanding which components of a network are volatile corresponding to a given sequence of tasks to articulate catastrophic forgetting.\nMore specifically, this research introduces a novel approach to elaborate catastrophic forgetting by visualizing hidden layers in class-incremental learning (considered as the hardest scenario in continual learning). In this learning paradigm, the use of previous data is prohibited and instances of the incoming tasks are unseen. We develop a tool named Catastrophic Forgetting Dissector (or CFD) which automates the dissection of catastrophic forgetting, exactly pointing out which components, in a model, are causing the forgetting. We formally adopt Intersection over Union (IoU), a popular evaluation metric in detection and segmentation tasks which essentially computes the overlapping ratio between two frames, to measure the forgetting degree of deep neural networks in this paper. The degree of forgetting is objectively measured after each class is added, thus giving us an intuition of how forgetting happens on a given part of the network.\nA work from Kemker et al [5] conducts experiments on state-of-the-art continual learning techniques that address catastrophic forgetting. It is demonstrated that although the algorithms work, but only on weak constrains and unfair baselines, thus the forgetting problem is not fully solved and witnessed yet. They insist on the infeasibility of using toy datasets, such as MNIST [6] or CIFAR [18] in continual learning experiments. As a result, this work motivates us to choose Split MS-COCO [12] to comprehensively measure the forgetting on deep neural networks. From the results of the dissection, we try to infer the plastic components in the network to preserve the accumulated knowledge. Critical freezing protects these components by simply keeping weights unchanged while training the new network.\nThere are three main contributions in this work: 1) We propose a novel and pioneering method to analyze catastrophic forgetting in continual learning. 2) We introduce a new approach to mitigate catastrophic forgetting based on the findings. 3) Our extensive experiments demonstrate the efficacy of critical freezing and suggest recondite understanding about catastrophic forgetting."
    },
    {
      "heading": "2 Related Work",
      "text": "Feature Visualization Contemporary interpretability methods bring us advantages to understand the decision-making process of deep neural networks, ranging from visualizing saliency maps [14][2] to transforming models into humanfriendly structures [1]. In [17], deconvnets allow us to recognize which features are expected by a specific part of a network or what properties of image excite a chosen neuron the most. In stark contrast to feeding an input image to diagnose, Yosinski et al [16] attempt to generate an image which maximizes the activation of a given neuron by gradient descent algorithm. Visualizing the activation of a neuron or a layer in networks helps us categorize the specific role of each block, layer, or even a node. It has been proved that the earlier layers extract local features, such as edges or colors; while deeper layers are responsible for detecting globally distinctive characteristics. Prediction Difference Analysis (PDA) [20], even more specifically, highlights pixels that support or counteract a certain class, indicating which features are positive or negative to a prediction. However, these tools only provide the computer vision, leaving the conclusion for users. This manual process can not ensure the quality of the observation when we may have hundreds or even thousands of attribution maps. CFD automatically detects the forgetting components in a network, entirely leveraging the generated activation difference maps from PDA [20].\nCatastrophic Forgetting Many works have managed to address the forgetting problem in generative models [19], object detection [13], semantic segmentation [15], or captioning [12]. Besides, fine-tuning is considered as a baseline in [12][13][15][10] to consider the superiority of the proposed techniques. Freezing either a few specific layers or a major part of a network is proposed in [12][10], which reveals that just simply keeping some parameters unchanged can greatly\nhelp networks become robust against catastrophic forgetting. Learning without Forgetting (LwF) [7] utilizes old models to generate pseudo data, which helps the new model reach a shared low-error region of problems. Also leveraging the old networks, knowledge distillation approaches [10][4] are formally recognized to facilitate better generalization in life-long learning via a teacher-student learning strategy. Nevertheless, algorithms are inclined to rely on external factors (e.g., input or rehearsal data, objective functions [7][10]) while ignoring the question of why catastrophic forgetting internally happens. In this research, our algorithm is derived from catastrophic forgetting exploration."
    },
    {
      "heading": "3 Approach",
      "text": "Although research from [11] shows an interest in understanding catastrophic forgetting, they focus on how task properties influence the hardness of sequential learning. Hence, they are explaining based on the input data. CFD approaches the problem from an alternative perspective, trying to explain how forgetting happens over time based on the computer vision of models. In comparison, the ultimate goal of this tool is to figure out the most plastic conv layers or blocks in a network. Plasticity means a low degree of stiffness or being easy to change. Although a variety of continual learning techniques have been proposed to alleviate catastrophic forgetting, none of them takes advantage of the findings from Interpretable ML. Critical freezing is built on the top of CFD\u2019s investigation to provide an interpretable and effective approach to deal with catastrophic forgetting. In the learning process, the optimal state of the old model is employed to initialize the new network. This way mimics the working mechanism of the human brain."
    },
    {
      "heading": "3.1 Catastrophic Forgetting Dissector - CFD",
      "text": "To dissect the model, we visualize the activation difference maps of hidden layers with respect to a specific unit (likely in the last layer) to understand the forgetting effect. We initially hypothesized that different features of the objects could be captured and visualized by particular feature maps in different layers. By looking into each response map in one conv block, we realize diverse features, such as eyes, face shape, car wheels, or background are isolatedly recognized by different channels, which avers our hypothesis. Unfortunately, considering each feature map manually by human eyes to study the forgetting is inefficient. To solve this issue, we compare the activation difference maps of feature maps with the ground truth segmentation to choose just one representative map in each conv block when the model has been trained on the first task. When new tasks arrive, the compare activation difference maps of two old and new models, in the same conv block, to measure the forgetting degree. In general, we do not seek for the answer that what features are being forgotten, but which conv blocks are forgetting.\nThe visualizations of the tool and the ground truth segmentation are simultaneously employed to infer the forgetting blocks. We assume that the semantic segmentation label of MS-COCO dataset [8] is what human eyes perceive. Next, we compare this segmentation with the computer vision of the model, particularly concentrating on positive evidence for a prediction to see how supportive features are disregarded.\nThe IoU value between the segmentation and evidence is calculated as shown in Fig. 2 (a). On the right side of Fig. 2 (a), we have an input image of a train, red dots advocate the fact that the output should be \u201ctrain\u201d while blue ones contradict this prediction.\nHaving the m-th activation difference map (FM) in the l-th block of a model M and the ground truth segmentation GT, the IoU is computed as:\nIoUM,GT (l,m) = FM(l,m) \u2229GT FM(l,m) \u222aGT\n(1)\nTo select the activation difference map having the largest overlap with the ground truth in l-th conv block, the representative map (RM) with the best IoU is RMM,GT :\nRMM,GT (l) = argmaxm(IoUM,GT (l,m)) (2)\nTo understand how the computer vision changes over the training process, we compare the RMs in the new model with the RMs of the old model shown in Fig. 2 (b). We compute IoUMO,GT by (1) then achieve RMMO,GT from (2) (RMMO,GT is the representative map of the old model). The forgetting effect of each trained model is measured by the IoUs between RMMO,GT and activation\nAlgorithm 1 CFD\n1: Input: Sample set S, segmentation ground truth GT , old model MO, new model MN , number of blocks K 2: Output: Forgetting conv block F 3: i = 0 4: L = \u2205 5: repeat 6: I = S[i] 7: IoUs = \u2205 8: FM = PDA(I) 9: for j = 1 to K do\n10: RMMO,GT \u2190 FM with highest IoUMO,GT 11: RMMN ,MO \u2190 FM with highest IoUMN ,MO 12: Append(IoUs,max(IoUMN ,MO )) 13: end for 14: b\u2190 blocks with the highest drop in IoUs 15: Append( L, b) 16: i = i+ 1 17: until i = size(S) 18: F \u2190 Most frequent block in L\ndifference maps of MN (MN is the new model):\nIoUMN ,MO (l,m) = FM(m) \u2229RMMO,GT FM(m) \u222aRMMO,GT\n(3)\nSimilar to the method of finding out the best map fitting with ground truth, the map representing the best memory of the original activation difference map is denoted as RMMN ,MO in (4). RMMN ,MO is determined as:\nRMMN ,MO (l) = argmaxm(IoUMN ,MO (l,m)) (4)\nIn the same block of both the old and new model, the role of a filter can be adjusted. For instance, the 50th filter in the 2nd block of the old model detects the eyes, but the same filter in the same block of the new model may consider the face. Hence, we should not make a comparison based on the index of a filter.\nThe workflow of CFD is given by Algorithm 1. The sample set S is particularized in Section 4, GT is the segmentation ground-truth from MS-COCO dataset, MO and MN are the old and new model for comparison respectively, and K is the number of the conv blocks in the network (K = 5 with ResNets). L is a list containing the most forgetting conv block with respect to all the images in S. By inputting an image I from S, we get the visualization of maps (FM) over MO and MN by PDA. However, we need to pick the representative activation difference map (RM) amongst thousands of maps in a conv block.\nTo choose the representative map RM of the jth conv block in a model, we define representative map RM to be the map having the largest overlap with the RM of the previous model (RMprev in short) for the same j th block.\nParticularly, the RMprev of MO is the ground truth because MO is the starting model, and RMMO,GT is the map at j\nth block of MO. Likewise, RMprev of MN at j\nth block is RMMO,GT , and we obtain RMMN ,MO by comparing maps of MN and RMMO,GT . The IoU values between RMMO,GT and RMMN ,MO are calculated at each conv block and appended to a list IoUs. After calculating IoU drops through the ResNet blocks and denote the block giving the highest drop as b, we can put b as the block where the most substantial forgetting happens into a list L, tested on the input image I. Finally, we generalize on all images of S to return the most forgetting component F.\nIn the juxtaposition of the old and new model, the IoUs are visually drawn to provide a bird-eye view of the forgetting trend shown in Fig. 2 (c). We may argue that the conv block having the lowest IoU (block 5) is the victim of catastrophic forgetting. However, this assumption is not asserted because of the error accumulation in deep neural networks. The computer vision of deeper blocks is directly attributed to earlier conv blocks. Once forgetting occurs in the first conv block, it will be propagated throughout the entire network.\nWe propose to leverage IoU slopes to find the weakest block b. At a point, if the IoU drops significantly compared to the previous value, it should be the sign of catastrophic forgetting. In Fig. 2 (c), a plummet of the IoU value is seen between block 2 and block 3 (0.642 to 0.388). The first thought appearing in our mind was that the 3rd is forgetting most catastrophically. It is true but we need to regard the fact that the worst map in block 3 is a result of block 2 and block 1. This finding is the foundation for our technique to prevent catastrophic forgetting."
    },
    {
      "heading": "3.2 Critical Freezing",
      "text": "Fine-tuning techniques play a pivotal role in training deep networks if data distribution evolves. Regarding a pre-trained model, the feature extractor which captures global information is carefully protected in adaptation. The output layer may be superseded, or the learning rate should be tweaked to a tiny number. Another dominant approach is to freeze the weights of the early layers. They are all effective yet ambiguous because we can not ensure freezing which layers will give the best result. Using the investigation from CFD, we freeze the precursors of the most plastic conv block in a deep neural network. If a network has K conv blocks, and we find the F th convolutional block broken, then we try to freeze earlier blocks than the F th block. If updating the fragile components are necessary, a learning rate on those blocks should be thoroughly calibrated. The procedure of critical freezing is shown in Algorithm 2. The objective function is the standard cross-entropy loss for image captioning, V is the size of the vocabulary, Y ik is the ground truth, and Y\u0302 i k is the prediction."
    },
    {
      "heading": "4 Experiments",
      "text": "We use a dataset called Split MS-COCO from [12] to reproduce catastrophic forgetting in image captioning task with incremental learning schemes. The dataset\nAlgorithm 2 Critical Freezing\n1: Input: Sample set S, segmentation ground truth GT , old model MO, new model MN , number of blocks K 2: Output: optimal state \u03b8\u2217 3: MN \u2190 MO // initialize new model by old parameters 4: F \u2190 CFD(S, GT , MO, MN , K) 5: for i = 1 to F \u2212 1 do 6: grad(MN [i], False) // freeze the block i th 7: end for 8: \u03b8\u2217 \u2190 argmin\n\u03b8\u0302\n(\u2212 \u2211V i=1 Y i k log Y\u0302 i k )\ncontains over 47k images for training and over 23k images for validation and testing. Regarding the incremental learning setup, a new class is introduced at each time step. Initially, we train with 19 classes to acquire the base model, followed by adding 5 classes sequentially. The captioning model is divided into an encoder and a decoder, in which the encoder is the ResNet-50, and the decoder includes an embedding layer, a single-layer LSTM, and a fully-connected layer producing a word at a time step. As CFD works on a single image, running multiple times on different and diverse input images is needed, helping us to generalize the observation of forgetting.\nInput Block 1 Block 2 Block 3 Block 4 Block 5\nWe choose a sample set S (bicycle, car, motorcycle, airplane, bus, train, bird, cat, dog, horse, sheep, and cow) from 19 trained classes. The results of CFD reinforce the fact that the 3rd conv block of ResNet is the most plastic component\nof this famous conv net given the learning sequence. To evaluate critical freezing, we perform fine-tuning and various schemes of freezing. In fine-tuning, the old model initializes the new model, and training is done by minimizing the loss on the new task. As the network contains two parts, encoder and decoder, we freeze them separately to specify the best freezing strategy. In addition, we choose various combinations of blocks to be frozen besides a famous baseline in continual learning experiments called LwF [7]. Two knowledge distillation techniques from [10] are also taken into comparison. The traditional scores for image captioning are considered in evaluating the superiority of critical freezing over the baselines. BLEU4 and ROUGEL are essentially word-overlap based metrics, while CIDEr and SPICE are more trustworthy because they give more weights on significant terms, such as verbs or nouns. Therefore, we will give discussion only on CIDEr score for conciseness.\nAfter adding a new class, we obtain M20, and Mn is the model when a total number of n classes are witnessed. In Fig. 3, the visualized results show that the first and second blocks of ResNet-50 can overall capture the outline of objects. Computer vision turns to represent more detailed features from the objects and other background features to determine the class of the input image in the deeper blocks. Also, the IoUs of different blocks in models, comparing with ground truth, are calculated by (1) and (2). The results reveal that although different models show the performance of the classification inconsistently, the IoUMn,GT (n > 19) values are roughly similar at all the blocks, which implies that no matter the how good the performance is, the level of matching between maps of each model and the human vision is preserved (Fig. 4 (a)).\nTo measure how much the forgetting occurs in the ResNet, we compute IoUs by (3) and (4). It is clearly shown in Fig. 4 (b) that the IoUs between M20 and M19 are roughly equal to the corresponding figure for M24 and M19 in every block. For M20, the IoUM20,M19 is always around 0.8 at the first block, showing that a marginal forgetting happens here. The IoUM20,M19 starts to drop along\nthe blocks because the later maps are constructed by the previous maps. The forgetting effect persists and does not show which block is forgetting the most.\nFor M24, the first block of the model still gets a high IoU comparing with M19 and the values decrease from the second block. Unlike the constantly decreasing trend seen in M20, the decreasing rate of IoUM24,M19 fluctuates through the blocks and a severe drop at block 3 is observed in every testing input, suggesting that the forgetting effect might happen the most in this block. Iterating this procedure on all images of the set S reinforces that the worst forgetting happens at block 3.\nWhile two naive approaches of freezing in [12] are also implemented, we devise critical freezing based on findings, which only freezes critical conv blocks. As shown in Table. 1, precisely freezing helps to learn on both the new and old tasks much more effectively. Our freezing scheme outperforms the other approaches on new tasks by a large margin (26.1 CIDEr) while achieving comparable performance with LwF [7] on past tasks (9.5 CIDEr) although LwF [7] is far more complicated. Knowledge distillation on intermediate feature space (KD1) and output layer (KD2) [10] claims 20.8 and 18.6 CIDEr respectively. We argue that the frozen blocks contain global information derived from past tasks, which is valuable and should be accumulated during lifetime rather than changing. Finetuning optimizes the loss on the new task without any guidance; as a result, the model may not fall into the low-error regions of tasks. We try to freeze each conv block of the ResNet to fortify the hypothesis that properly freezing is really better than ambiguous freezing in fine-tuning schemes. Hence, critical freezing exerts a promising influence on fine-tuning techniques in deep learning."
    },
    {
      "heading": "5 Conclusion and Future Work",
      "text": "As the presence of catastrophic forgetting hinders the life-long learning, understanding how this phenomenon happens in computer vision is imperative. We introduce CFD to grasp catastrophic forgetting. The investigation of our tool unearths the mystical question about catastrophic forgetting.\nFrom knowing where the forgetting issue is coming from, a new technique has been proposed focusing on plastic components of a model to moderate the information loss. The experiments illustrate the superiority of critical freezing over various freezing schemes and existing techniques. To the best of our knowledge, no work has been done for mitigating catastrophic forgetting under the supervision of Interpretable ML. By knowing which regions are needed to be kept intact, not only could the performance on the old task be largely improved, but the new task is also more conquerable. Ultimately, critical freezing could benefit a variety of fine-tuning schemes and continual learning approaches.\nThere are future works following our paper. Scaling this work for other tasks and deep networks can better validate the proposed continual learning algorithm\u2019s feasibility. We believe that our work results are just a starting point for a new direction to address catastrophic forgetting by interpretable methods completely. The experiments on conv blocks are coarse, which could be further refined by using conv layers, which will likely give much better results. We are choosing PDA [20] because the maps are clear; however, we can also adopt other advanced methods, such as using attribution or saliency to analyze the forgetting problem better."
    }
  ],
  "title": "Explaining How Deep Neural Networks Forget by Deep Visualization",
  "year": 2020
}
