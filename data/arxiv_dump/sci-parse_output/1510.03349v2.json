{
  "abstractText": "The leaderboard in machine learning competitions is a tool to show the performance of various participants and to compare them. However, the leaderboard quickly becomes no longer accurate, due to hack or overfitting. This article gives two pieces of advice to prevent easy hack or overfitting. By following these advice, we reach the conclusion that something like the Ladder leaderboard introduced in [1] is inevitable. With this understanding, we naturally simplify Ladder by eliminating its redundant computation and explain how to choose the parameter and interpret it. We also prove that the sample complexity is cubic to the desired precision of the leaderboard.",
  "authors": [
    {
      "affiliations": [],
      "name": "ZHENG Wenjie"
    }
  ],
  "id": "SP:2fe460afe83add5edd8c44a92ebb747b074f6324",
  "references": [
    {
      "authors": [
        "Avrim Blum",
        "Moritz Hardt"
      ],
      "title": "The ladder: A reliable leaderboard for machine learning competitions",
      "venue": "In Proceedings of The 32nd International Conference on Machine Learning,",
      "year": 2015
    },
    {
      "authors": [
        "Marcus Hardt",
        "Jonathan Ullman"
      ],
      "title": "Preventing false discovery in interactive data analysis is hard",
      "venue": "In Foundations of Computer Science (FOCS),",
      "year": 2014
    },
    {
      "authors": [
        "Thomas Steinke",
        "Jonathan Ullman"
      ],
      "title": "Interactive fingerprinting codes and the hardness of preventing false discovery",
      "venue": "arXiv preprint arXiv:1410.1228,",
      "year": 2014
    },
    {
      "authors": [
        "Olivier Bousquet",
        "Andr\u00e9 Elisseeff"
      ],
      "title": "Stability and generalization",
      "venue": "The Journal of Machine Learning Research,",
      "year": 2002
    },
    {
      "authors": [
        "Olivier Bousquet",
        "St\u00e9phane Boucheron",
        "G\u00e1bor Lugosi"
      ],
      "title": "Introduction to statistical learning theory",
      "venue": "In Advanced lectures on machine learning,",
      "year": 2004
    },
    {
      "authors": [
        "Luc Devroye",
        "L\u00e1szl\u00f3 Gy\u00f6rfi",
        "G\u00e1bor Lugosi"
      ],
      "title": "A probabilistic theory of pattern recognition, volume 31",
      "venue": "Springer Science & Business Media,",
      "year": 2013
    },
    {
      "authors": [
        "Vladimir Vapnik"
      ],
      "title": "The nature of statistical learning theory",
      "venue": "Springer Science & Business Media,",
      "year": 2013
    }
  ],
  "sections": [
    {
      "heading": "1 Introduction",
      "text": "Machine learning competitions have been a popular platform for young students to practice their knowledge, for scientists to apply their expertise, and for industries to solve their data mining problems. For instance, the Internet streaming media company Netflix held the Netflix Prize competition in 2006, to find a better program to predict user preferences. Kaggle, an online platform, hosts regularly competitions since 2010. These competitions are usually prediction problems. The participant is given the independent variable X, and then he is required to predict the dependent variable Y . Usually, the host divides his data into three data sets: training, validation and test. The training dataset is fully available: every participant (having a competition account) can download it and observe its Y as well as its X. This allows them to build their models. The validation data set is partially available to participants: they can only observe its X. This dataset is used to construct the so-called leaderboard. The participants submit their prediction of Y to the host, and the host calculates their scores and ranks, and show them on the leaderboard, so that every participant could know his chance to win the competition. The test dataset is private. They are only used once at the final day to determine who is the final winner. Usually, the winner gets a reward. The reason that the final result is determined by the reserved test set instead of the validation set is because the validation set could be hacked. Since the participant could submit his prediction over and over during the life of the competition, he has much chance to improve his model\u2019s performance on the validation set, either by overfitting or hacking. In consequence, by the final day, the score he gets on the validation set may have been much higher than his model deserves. This is why it is frequently observed that the final winner of the competition is not the \u201cwinner\u201d on the leaderboard. Although the leaderboard has no effect on the decision of the final winner, it could be quite annoying if it cannot truly reflect the performance of each participant. Firstly, such a leaderboard allures inexperienced participants to overfit the validation set. Secondly, it encourages certain participants to hack the validation set in order to get a fake temporary honor or to disturb the order of the competition. Thirdly, it is not a good experience to see one\u2019s non-overfitting model rank below someone hacking the validation set. It could be said that during the whole life of the competition, the participants compete around the leaderboard. In view of this, some researchers tried to build an accurate leaderboard by preserving the accuracy of the estimator of the loss function. This could be hard since the participant can modify their model\nar X\niv :1\n51 0.\n03 34\n9v 2\n[ st\nat .M\nL ]\n7 J\nun 2\n01 7\nadaptively according to the feedback they get from the leaderboard. [2, 3] suggest that maintaining accurate estimates on a sequence of many adaptively chosen classifiers may be computationally intractable. In light of this, [1] proposes the Ladder mechanism to restrain the feedback that the participant could get from the leaderboard. The idea is that the participant gets a score if and only if this score is higher than the best among the past by some margin. By restraining the feedback, the participants have less information to adapt their models, and thus less chance to hack the leaderboard by overfitting the validation set. However, [1] fails to point out whether this kind of mechanism is necessary: does there exist any other mechanism that achieves the same or better effect? If we have to use Ladder, what is its strength and shortcoming? How well can we hack the leaderboard? Is is true that the first leader is better than the second? If so, then by how much? There are two parameters in Ladder: margin \u03b7 and precision \u03b7; what is their relationship? Does the heuristic way to choose \u03b7 provided in the paper have any theoretic guarantee? If the participant holds many accounts, is Ladder still effective? In our paper, we answer these questions. A better understanding of the leaderboard will be achieved during the reading of this article. First, we show that traditional leaderboard is easy to hack. In consequence, something like Ladder mechanism is necessary. Then, we perform another hack to show that a leaderboard cannot be arbitrarily accurate (Section 2). Afterwards, we recognize the essence inside the Ladder mechanism and thus simplify it (Section 3). Finally, we generalize the Theorem 3.1 in [1] to take into consideration of the fact that each participant may be allowed to possess multiple accounts. And we slightly improve the upper bound as well (by eliminating the dependence on n in the logarithmic factor). Our result shows that, while using Ladder mechanism, the leaderboard needs O\u0303(M \u22123) samples to control the error within , where M is the number of accounts.1 This result suggests that Ladder is relatively robust to number of submissions, but may still be vulnerable to number of accounts (Section 4). In this article, we study the binary classification competition, but our result can be straightforwardly generalized to other kinds of competitions. The highlight of this article is that we are not advertising any \u201cmagic\u201d algorithm; we are just pursuing a better understanding of the leaderboard. We depart from some basic property (robust against hacks and overfittings) that a leaderboard should satisfy in order to protect its accuracy and fairness. We reach the conclusion that something like Ladder is inevitable. This understanding further allows us to eliminate the redundant computation in the origin Ladder, and to only retain its essence. We also give an upper bound. But we do not stop there. We interpret this upper bound and use it as a tool to understand the advantages as well as limitation of Ladder when used in practice."
    },
    {
      "heading": "2 Leaderboard failure",
      "text": "In this sections, we show some examples where the leaderboard is hackable if it releases certain information. With these examples, we know at least what to avoid when building a leaderboard."
    },
    {
      "heading": "2.1 Full-information leaderboard is hackable",
      "text": "In this subsection, we show that if a leaderboard shows the score of each submission, this leaderboard is easy to hack. Supposing that the validation set contains n different data points S = {(x1, y1), (x2, y2), . . . , (xn, yn)}, where yi \u2208 {0, 1} for each i. The participant is expected to build a function f = f(x), so that f(x) is a good estimator of y. Let the score be the accuracy of this estimator, which is defined as score(f) = 1n \u2211n i=1 1{yi=f(xi)} on the validation set. Every time the participant submits his y\u0302i = f(xi) to the host, the host shows the score of f to the participant via the leaderboard. We show that this kind of leaderboard is easy to hack.\n1O\u0303() stands for omitting the logarithm term.\nTo hack this leaderboard, we perform a boosting attack.2 The idea is that if we have many independent submissions, whose accuracy are only a little higher than 0.5, then we can combine them via majority vote policy to construct a submission whose accuracy is much higher than 0.5. In detail, we randomly pick a vector u \u2208 {0, 1}n. If its accuracy is higher than 0.5, then we keep v = u, and otherwise v = 1n \u2212 u. Having got m such vectors v1, v2, . . . , vm, we construct the submission y\u0302m = (y\u0302m1 , y\u0302 m 2 , . . . , y\u0302 m n ) T , where y\u0302mi equals to 1, if 1 m \u2211m j=1 v j i > 0.5, and equals to 0 otherwise. Figure 1 shows the result of this attack on a leaderboard of 1000 samples. We see that within 103 submissions, the hacker\u2019s score climbs from 0.5 to 0.8 on the leaderboard. Therefore, in order to protect the leaderboard from the boosting attack, we cannot release information each time there is a submission. In consequence, we adopt the idea that the leaderboard gives the participant feedback only when his score is higher than the highest in the past."
    },
    {
      "heading": "2.2 High-precision leaderboard is hackable",
      "text": "Normally, a leaderboard shows two things \u2013 score and rank. In this subsection, we show that if a leaderboard precisely reflects the ranks, then this leaderboard is easy to hack. For this, we consider a minimal leaderboard, which shows nothing other than the ranks. In other words, the participants are not able to observe the scores. Furthermore, inheriting the argument from the previous subsection, the leaderboard uses the highest score that a participant has ever achieved to compute the rank. In other words, a participant knows nothing even if he beats his old scores unless his new score is higher enough to beat another participant, whose rank was higher than his. This leaderboard displays really little information. However, even with so little information displayed, it is still hackable. For this, we perform a brute-force enumeration attack. Precisely, the hacker signs up two accounts A and B. At first, he uses A to submit a random guess u1, and he gets a rank a1 for A. Then, he flips one component in this submission (say, u11 from 0 to 1, or from 1 to 0), and uses B to submit it as u2. He will get a rank b1 for B, which is different than a1. Let us assume that b1 is higher than a1(otherwise, we just switch the name of account A and B). Then, he flips an unchanged\n2Actually this is not really a boosting technique, but we use the same terminology as in [1] here.\ncomponent (say u22) and switches to A to submit it as u3. The score u3 yields is either higher or lower than u2. If it is lower than u2 (must be equal to u1 in this case), then he sees no change on A\u2019s rank. In this case, he repeats this step by flipping another component (say u23) until it is higher than u2 or all components have been flipped once. If it is higher than u2, since the leaderboard precisely reflects the rank, it must move A\u2019s rank from a1to a2, which is higher than b1. Once again, the hacker switches to the account B and repeats the process . . . He gets a1 \u227a b1 \u227a a2 \u227a b2 \u227a a3 \u227a \u00b7 \u00b7 \u00b7. Since the score is bounded by 1, and each score increment is constant, the hacker finally gets the score 1, which also means getting all answers right and ranking the highest on the leaderboard. With this simple attack, we show that as long as the leaderboard precisely reflects the rank, a hacker equipped with two accounts can achieve arbitrarily high score. Therefore, a leaderboard should never reveal precise ranks. In other words, there are cases where two participants with different scores (this difference will not be observable) see them ranked together on the leaderboard."
    },
    {
      "heading": "3 Simplified Ladder leaderboard",
      "text": "In the previous section, we learned that a leaderboard should avoid some pitfalls. In this section, we show that the Ladder leaderboard [1] successfully avoids them. We first introduce Ladder and simplify it, and then demonstrate its robustness against boosting and enumeration attacks. Throughout this paper, we use the following notation.\nNotation. [x]\u03b7 denotes the number x rounded to the nearest integer multiple of \u03b7; bxc\u03b7 to the nearest not higher than x; dxe\u03b7 to the nearest not lower than x. \u03b7 is a number in (0, 1], and is usually among the values 0.1, 0.01, etc.. When \u03b7 is missing, it is consider to be 1 in convention. log x denotes the binary logarithm. The idea of Ladder is simple. The leaderboard only shows the best score that a participant has ever achieved, and updates it only when a record-breaking score is higher than it by some margin \u03b7 (Algorithm 1). Notice that there is a parameter \u03b7 in this algorithm. To overcome this inconvenience, [1] also suggests a parameter-free Ladder leaderboard. However, this parameter-free version is hackable (Figure 1). The method is to make a first submission containing half 1 and half 0. Then switch the place of a 1 and a 0 in each submission. In this paper, we give a simplified Ladder version (Algorithm 2) as well as a way to choose the optimal value of \u03b7. Comparing these two algorithms, the only tiny difference is the condition in Step 3 \u2013 we drop the margin \u03b7. This raises naturally the question whether this modification breaks the algorithm? No. In fact, the margin \u03b7 is already captured in the assignment Rt \u2190 [ht]\u03b7 by he precision \u03b7. Since Rt\u22121 is always an integer multiple of \u03b7, Rt can be greater than Rt\u22121 if and only if ht is higher than Rt\u22121 by a margin of \u03b72 . The simplification does not stop here. Indeed, Step 3 can be rephrased\nas Rt = max ( Rt\u22121, [ht]\u03b7 ) . So the idea of the simplified Ladder leaderboard is just to authentically display the best score achieved by each participant so far, but with a certain level of precision \u03b7.\nAlgorithm 1 Original Ladder [1] Assign initial score R0 \u2190 \u2212\u221e. for round t = 1, 2, . . . do\n1. Receive submission ut 2. ht \u2190score of ut 3. If ht > Rt\u22121 + \u03b7 then Rt \u2190 [ht]\u03b7 else Rt \u2190 Rt\u22121 4. Show Rt on the leaderboard\nend for Note: [x]\u03b7 denotes the number x rounded to the nearest integer multiple of \u03b7.\nAlgorithm 2 Simplified Ladder Assign initial score R0 \u2190 \u2212\u221e. for round t = 1, 2, . . . do\n1. Receive submission ut 2. ht \u2190score of ut 3. If ht > Rt\u22121 then Rt \u2190 [ht]\u03b7 else Rt \u2190 Rt\u22121 4. Show Rt on the leaderboard\nend for Note: [x]\u03b7 denotes the number x rounded to the nearest integer multiple of \u03b7.\nThis understanding is very helpful. On the one hand, it simplifies the implementation of the Ladder leaderboard. The practitioners have less chance to make an error (e.g., by accidentally dropping the precision \u03b7 or configure a smaller one). On the other hand, it greatly simplifies the analysis. \u03b7 here is no longer an algorithmic parameter. It is instead the precision which the leaderboard offers. After the presentation of Ladder algorithm, now let us try to answer this question: does Ladder avoid the pitfalls mentioned above? Yes. On the one hand, it only reveals the highest score so far. On the other hand, it does not give arbitrarily precise information \u2013 a participant yielding 0.644 is not distinguishable from another participant yielding 0.636 when \u03b7 = 0.01. But there still remain questions that whether Ladder is hackable. Is it robust against all attacks besides the above mentioned ones? The answer is yes, provided that the attacker does not possess many accounts. And the robustness is proportional to the cubic root of the size of the validation set. If we want the precision \u03b7 to be 0.1, we should have 103 samples; if \u03b7 = 0.01, we will need 106 samples. This will be proved in the next section. Here again our understanding of Ladder contributes. If \u03b7 is too large, which means that the precision is low, the participants will not be distinguishable \u2013 they are clustered on the leaderboard. Such a leaderboard is not informative. If \u03b7 is too small, which means the precision is too high and the leaderboard reveals too much information, there will not be enough samples to maintain the authenticity of the leaderboard \u2013 the leaderboard is easy to hack or overfit. Such a leaderboard is false. This trade-off is the central topic of the next section. In the rest of this section, we present some results about some attacks on Ladder. Figure 1 shows that Ladder is robust against the boosting attack. Figure 2 shows some brute-force enumeration attacks on Ladder. In these examples, to defend against the attacks, Ladder uses less samples than necessary, for the brute-force is not very efficient, since it does not make use of all information available on the leaderboard."
    },
    {
      "heading": "4 Sample complexity",
      "text": "In this section, we present the sample complexity of the Ladder leaderboard. We show that \u03b7 is not only the display precision of the leaderboard, but also the optimal value of \u03b7 is the lowest leaderboard error possible. This optimal value is \u03b7\u2217 = O\u0303 ( 3 \u221a M n ) , where M is the number of accounts and n is the number of samples in the validation set. Suppose that our data (X,Y ) lie in some space \u2126\u00d7{0, 1}. They follow a distribution D on this space. Validation set S = {(x1, y1), . . . , (xn, yn)} are samples drawn i.i.d. from this distribution. A classifier of this problem is represented by the function f : \u2126\u2192 {0, 1}. The accuracy of this classifier is defined as\nRD(f) := Pr(f(X) = Y ).\nIts accuracy on the validation set is defined as\nRS(f) := 1\nn n\u2211 i=1 I(f(xi) = yi),\nwhere I(\u00b7) is the indicator function. RS(f) can be seen as an estimator of RD(f), whose error could be measured by the quantity |RS(f)\u2212 RD(f)|. Normally, this error should be small [4, 5, 6, 7]. However, due to the overfitting or hack by repeated and adaptive submissions, this error could grow larger and larger. As this error grows, the leaderboard is no longer a qualified index of the performance of participants. The scores it displays no longer reflect the true accuracy of the models, and the ranks it shows do not truly imply that one participant\u2019s model is better than another\u2019s. This is why the traditional leaderboard fails. Since RS(f) is no longer a good estimator of RD(f), one may ask whether there exist other estimators. [2, 3] show that no computationally efficient estimator can achieve error o(1) on more than n2+o(1) adaptively chosen functions in a traditional leaderboard. Therefore, [1] as well as this article tries another approach: the Ladder leaderboard. In Ladder, the leaderboard only displays the best ever score that an account has achieved Rt := max1\u2264i\u2264tRS(fi), where fi is the function which characterizes the i-th submission associated with a given account. Thus, at the moment t, the error of the score displayed on the leaderboard could be measured by the quantity |Rt\u2212max1\u2264i\u2264tRD(fi)|. Across the time, the leaderboard error of R1, . . . , Rk of a single account is measured with\nlberr(R1, . . . , Rk) := max 1\u2264t\u2264k \u2223\u2223\u2223\u2223Rt \u2212 max1\u2264i\u2264tRD(fi) \u2223\u2223\u2223\u2223 .\nA small leaderboard error means that the score displayed on the leaderboard is close to the best score the account in question gets on the underlying true distribution. [1] gives an upper bound to the leaderboard error. However, it does not take into consideration the fact that a participant may possess multiple accounts, in which case their reasoning breaks. In this paper, we take that into consideration and our upper bound is slightly tighter than theirs (logarithmic factor) when degenerated to one-person-one-account case. If a participant has multiple accounts, he can then switch among different accounts to submit successive submissions. His submission thus does not depend only on the history of the current account, but also on the histories of other accounts. Denote\nFt = { fmi : 1 \u2264 i \u2264 km, 1 \u2264 m \u2264M, M\u2211 m=1 km = t }\nas the submission history right after the moment t, where fmi signifies using account m to submit account m\u2019s i-th submission, and km is the subtotal submissions associated with account m. Denote\nRt = { Rmi : 1 \u2264 i \u2264 km, 1 \u2264 m \u2264M, M\u2211 m=1 km = t } as the feedback (score) history associated with Ft.\nTheorem. Given a competition with a validation set of n samples, where the Ladder leaderboard employs a display precision of \u03b7 , and each participant can possess at most M accounts. For any set of k (adaptively chosen) classifiers Fk submitted by a participant, his scores Rk displayed on the leaderboard satisfy\nPr { max\n1\u2264t\u2264km,1\u2264m\u2264M \u2223\u2223\u2223\u2223Rmt \u2212 max1\u2264i\u2264tRD(fmi ) \u2223\u2223\u2223\u2223 > \u03b7} \u2264 exp(\u2212\u03b72n2 + ( M \u03b7 + 1 ) log 2k + 1 ) . (1)\nIn particular, for some \u03b7 = O ( 3 \u221a M log k n ) , Ladder achieves with high probability: for any m =\n1, . . . ,M ,\nlberr(Rm1 , . . . , R m km) \u2264 O\n( 3 \u221a M log k\nn\n) .\nHere, we successfully thrust the number of submissions k into the logarithmic factor. Thus, the leaderboard error is no longer sensitive to the number of submissions. A participant can submit as many times as he wishes (if he is able of course). However, we notice that the number of accounts M is still outside of the logarithmic factor, which means that the leaderboard error grows quickly when M grows. Although it is only an upper bound, which is less persuasive than a lower bound, we can provide a counter example to illustrate the effect of multi-account. Consider the extreme case where the participant submits each submission with a brand new account every time (i.e., M = k), the leaderboard error grows quickly with the submissions. Indeed, Ladder shows no difference from the traditional leaderboard in this case. The theorem\u2019s aim is to prove a small leaderboard error. But why does it matter? What is the relation to the robustness of a leaderboard? The consequence of a small leaderboard error means that a participant\u2019s score sticks to his score on the ground truth. It is not likely that he could climb up on the leaderboard either by overfitting or by hack. If he marks a leap on the leaderboard, chances are that he really improved his prediction model. Therefore, the theorem can be interpreted as: when the display precision of ladder is set to the optimal\nvalue \u03b7\u2217 = O ( 3 \u221a M log k n ) , a participant who gets a score s (an integer multiple of \u03b7\u2217) has large chance that his true score on the ground truth is within [s\u2212 \u03b7\u2217, s+ \u03b7\u2217]. If two participants A and B get the score sA and sB respectively, where sA \u2212 sB > 2\u03b7\u2217, chances are that A really outperforms B. Particularly, a hacker has little chance to get a score higher than 12 + \u03b7\n\u2217, since he learns nothing and his true score should be the same as the random guess. Naturally, we would like \u03b7\u2217 small. This depends on n. We see that to achieve a small \u03b7\u2217 = , we will need O ( M log k 3 ) samples."
    },
    {
      "heading": "5 Proof of Theorem",
      "text": "From the state-of-art literature [4, 5, 6, 7], we already have\nPr { max 1\u2264t\u2264k |RS(ht)\u2212RD(ht)| > \u03b5 } \u2264 2k exp(\u22122\u03b52n), (2)\nfor a series of functions h1, . . . , hk which are independent of the validation set S. This inequality is quite close to our destination. However, because of the sequential and adaptive nature of our problem, ht+1 is a function of RS(h1), . . . , RS(ht), which means that it is not independent of S. Thus, we cannot apply the above inequality directly. The technique employed in this proof is to eliminate the dependence by enumerating all possible realizations.\nProof. Suppose that the participant has an algorithm A to decide the next function which characterizes the next submission, and which account to submit with, in using the past history: ht+1 = A(Rt), where ht+1 will become one member of the fmi in Ft+1. A can be either deterministic or random provided that it does not depend on S. ht+1 is dependent on S, however,\ng := A { rmi : 1 \u2264 i \u2264 km, 1 \u2264 m \u2264M, M\u2211 m=1 km = t }\nis not, where rmi is one realization of Rmi . In consequence, we can apply (2) to g. The remaining issue is to count how many different g\u2019s we could have within k submissions. To this end, we use a compression algorithm to encode every possible g. First of all, to specify at which submission this g is submitted, we need dlog ke bits. Then, each g can have history coming from M different accounts. For each single account, we calculate the bits needed. Since the history within an account is a monotone increasing series, which should be multiples of 1/\u03b7 and be in interval [0, 1], it can only take value in d1/\u03b7e numbers and jump at most b1/\u03b7c steps. Now we calculate the number of bits required to encode each jump. Here, we use a trick, which allows us to get rid of the n inside the logarithm in [1] \u2013 we only code the place where there is a jump regardless of jumping height. If the jump height is 1/\u03b7, then we code this place once. If the jump height is s/\u03b7, then we code it s times. To code this place once, we need at most dlog ke bits. Thus, the total bits demanded is\ndlog ke+M \u00d7 \u230a 1\n\u03b7\n\u230b \u00d7 dlog ke \u2264 (M\n\u03b7 + 1) log 2k.\nThen, we can apply (2) to the g\u2019s in setting = \u03b7/2:\nPr { max g |RS(g)\u2212RD(g)| > \u03b7 2 } \u2264 2\u00d7 2( M \u03b7 +1) log 2k exp ( \u2212\u03b7 2n 2 ) .\nThe left side equals exactly\nPr { max h\u2208Fk |RS(h)\u2212RD(h)| > \u03b7 2 } ,\nwhile the right side is bounded by\nexp ( \u2212\u03b7 2n\n2 +\n( M\n\u03b7 + 1\n) log 2k + 1 ) ,\nwhich is exactly the right side of (1). Conditioned on the event { maxh\u2208Fk |RS(h)\u2212RD(h)| \u2264 \u03b7 2 } , we have\nmax 1\u2264t\u2264km,1\u2264m\u2264M \u2223\u2223\u2223\u2223max1\u2264i\u2264tRS(fmi )\u2212 max1\u2264i\u2264tRD(fmi ) \u2223\u2223\u2223\u2223 \u2264 \u03b72 .\nAnd since we have \u2223\u2223\u2223\u2223Rmt \u2212 max1\u2264i\u2264tRS(fmi ) \u2223\u2223\u2223\u2223 \u2264 \u03b72\nbecause of the rounding error, by the triangular inequality, we get\nmax 1\u2264t\u2264km,1\u2264m\u2264M \u2223\u2223\u2223\u2223Rmt \u2212 max1\u2264i\u2264tRD(fmi ) \u2223\u2223\u2223\u2223 \u2264 \u03b7,\nwhich is exactly what we want on the left side of (1)."
    },
    {
      "heading": "6 Discussion",
      "text": "Ladder is vulnerable if the number of accounts each participant can hold is unlimited. It is possible to launch a boosting attack on Ladder leaderboard by using a brand-new account for each submission (Figure (1)). Given limited number of accounts (e.g. one account for each participant), Ladder is robust against the number of submissions. However, compared with the quadratic sample complexity of general statistical / machine learning tasks, the cubic sample complexity of the Ladder leaderboard may still remain a bit too expensive. For a mere 0.01 leaderboard error, the validation set has to have 106 samples. This is not possible in most competitions. Even if it is, we may still have questions such as why not put these samples into the training dataset so as to enable the participants to use more complex models. This makes the loss of Ladder more than its gain. On the other hand, we do not know whether this cubic root upper bound is tight. We have yet found an efficient attack algorithm that could achieve this upper bound, nor have we discovered a tight lower bound. That is to say, Ladder may actually work better than we expected here. In practice, the competition hosts employ as well other measures, such as limiting the number of submissions per day, disqualifying participants secretly signing up other accounts etc., to strengthen the accuracy of the leaderboard. These measures could be combined with Ladder so as to provide a more accurate leaderboard than the traditional one."
    }
  ],
  "title": "Toward a Better Understanding of Leaderboard",
  "year": 2018
}
