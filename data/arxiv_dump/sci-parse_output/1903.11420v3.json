{
  "abstractText": "Explainable Artificial Intelligence (XAI)has received a great deal of attention recently. Explainability is being presented as a remedy for the distrust of complex and opaque models. Model agnostic methods such as LIME, SHAP, or Break Down promise instance-level interpretability for any complex machine learning model. But how faithful are these additive explanations? Can we rely on additive explanations for non-additive models? In this paper, we (1) examine the behavior of the most popular instance-level explanations under the presence of interactions, (2) introduce a new method that detects interactions for instance-level explanations, (3) perform a large scale benchmark to see how frequently additive explanations may be misleading.",
  "authors": [
    {
      "affiliations": [],
      "name": "A PREPRINT"
    },
    {
      "affiliations": [],
      "name": "Alicja Gosiewska"
    },
    {
      "affiliations": [],
      "name": "Przemyslaw Biecek"
    }
  ],
  "id": "SP:2004903aef904980a7dcd376630d465dc2ec94c8",
  "references": [
    {
      "authors": [
        "H. Alemzadeh",
        "J. Raman",
        "N. Leveson",
        "Z. Kalbarczyk",
        "R.K. Iyer"
      ],
      "title": "Adverse events in robotic surgery: A retrospective study of 14 years of fda data",
      "venue": "PLOS ONE, 11(4):1\u201320,",
      "year": 2016
    },
    {
      "authors": [
        "D. Alvarez-Melis",
        "T. Jaakkola"
      ],
      "title": "A causal framework for explaining the predictions of black-box sequence-to-sequence models",
      "venue": "In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,",
      "year": 2017
    },
    {
      "authors": [
        "D. Alvarez-Melis",
        "T.S. Jaakkola"
      ],
      "title": "On the Robustness of Interpretability Methods",
      "venue": "arXiv e-prints, art",
      "year": 2018
    },
    {
      "authors": [
        "S. Bach",
        "A. Binder",
        "G. Montavon",
        "F. Klauschen",
        "K.-R. M\u00fcller",
        "W. Samek"
      ],
      "title": "On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation",
      "venue": "PLOS ONE, 10(7):1\u201346,",
      "year": 2015
    },
    {
      "authors": [
        "J.C. Banzhaf"
      ],
      "title": "Weighted voting doesn\u201dt work: a mathematical analysis",
      "year": 1965
    },
    {
      "authors": [
        "P. Biecek"
      ],
      "title": "DALEX: Explainers for Complex Predictive Models in R",
      "venue": "Journal of Machine Learning Research,",
      "year": 2018
    },
    {
      "authors": [
        "B. Bischl",
        "G. Casalicchio",
        "M. Feurer",
        "F. Hutter",
        "M. Lang",
        "R.G. Mantovani",
        "J.N. van Rijn",
        "J. Vanschoren"
      ],
      "title": "OpenML benchmarking suites and the OpenML100",
      "year": 2017
    },
    {
      "authors": [
        "A. Datta",
        "S. Sen",
        "Y. Zick"
      ],
      "title": "Algorithmic transparency via quantitative input influence: Theory and experiments with learning systems",
      "venue": "In 2016 IEEE Symposium on Security and Privacy (SP),",
      "year": 2016
    },
    {
      "authors": [
        "L. Edwards",
        "M. Veale"
      ],
      "title": "Enslaving the algorithm: From a \u201cright to an explanation\u201d to a \u201cright to better decisions\u201d",
      "venue": "IEEE Security Privacy,",
      "year": 2018
    },
    {
      "authors": [
        "N. Gill",
        "P. Hall"
      ],
      "title": "An Introduction to Machine Learning Interpretability",
      "venue": "O\u2019Reilly Media, Incorporated,",
      "year": 2018
    },
    {
      "authors": [
        "T.W. Gillespie"
      ],
      "title": "Understanding waterfall plots",
      "venue": "Journal of the advanced practitioner in oncology,",
      "year": 2012
    },
    {
      "authors": [
        "R. Guidotti",
        "S. Ruggieri"
      ],
      "title": "On The Stability of Interpretable Models",
      "venue": "arXiv e-prints, art",
      "year": 2018
    },
    {
      "authors": [
        "A. Jacovi",
        "O. Sar Shalom",
        "Y. Goldberg"
      ],
      "title": "Understanding convolutional neural networks for text classification",
      "venue": "pages 56\u201365,",
      "year": 2018
    },
    {
      "authors": [
        "M. Kuzba",
        "E. Baranowska",
        "P. Biecek"
      ],
      "title": "pyCeterisParibus: explaining Machine Learning models with Ceteris Paribus Profiles in Python",
      "venue": "Journal of Open Source Software,",
      "year": 2019
    },
    {
      "authors": [
        "H. Lakkaraju",
        "O. Bastani"
      ],
      "title": "how do i fool you?\u201d: Manipulating user trust via misleading black box explanations",
      "venue": "In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society,",
      "year": 2020
    },
    {
      "authors": [
        "O. Loyola-Gonz\u00e1lez"
      ],
      "title": "Black-box vs. white-box: Understanding their advantages and weaknesses from a practical point of view",
      "venue": "IEEE Access,",
      "year": 2019
    },
    {
      "authors": [
        "S.M. Lundberg",
        "S.-I. Lee"
      ],
      "title": "A unified approach to interpreting model predictions",
      "venue": "Advances in Neural Information Processing Systems",
      "year": 2017
    },
    {
      "authors": [
        "S.M. Lundberg",
        "G.G. Erion",
        "S.-I. Lee"
      ],
      "title": "Consistent Individualized Feature Attribution for Tree Ensembles",
      "venue": "arXiv e-prints, art",
      "year": 2018
    },
    {
      "authors": [
        "Marco Tulio Ribeiro",
        "Sameer Singh",
        "Carlos Guestrin"
      ],
      "title": "Anchors: High-precision model-agnostic explanations",
      "venue": "In AAAI Conference on Artificial Intelligence,",
      "year": 2018
    },
    {
      "authors": [
        "M. McGough"
      ],
      "title": "How bad is Sacramento\u2019s air, exactly? Google results appear at odds with reality, some say",
      "venue": "https: //www.sacbee.com/news/california/fires/article216227775.html,",
      "year": 2018
    },
    {
      "authors": [
        "C. Molnar"
      ],
      "title": "Interpretable Machine Learning",
      "venue": "https://christophm.github.io/interpretable-ml-book/",
      "year": 2019
    },
    {
      "authors": [
        "C. Molnar",
        "B. Bischl",
        "G. Casalicchio"
      ],
      "title": "iml: An r package for interpretable machine learning",
      "venue": "JOSS, 3(26):786,",
      "year": 2018
    },
    {
      "authors": [
        "C. O\u2019Neil"
      ],
      "title": "Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy",
      "venue": "Crown Publishing Group, 2016",
      "year": 2016
    },
    {
      "authors": [
        "T.L. Pedersen",
        "M. Benesty"
      ],
      "title": "lime: Local Interpretable Model-Agnostic Explanations, 2019",
      "venue": "URL https://CRAN. R-project.org/package=lime. R package version",
      "year": 2019
    },
    {
      "authors": [
        "M.T. Ribeiro",
        "S. Singh",
        "C. Guestrin"
      ],
      "title": "Why Should I Trust You?\": Explaining the Predictions of Any Classifier",
      "venue": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data",
      "year": 2016
    },
    {
      "authors": [
        "C. Rudin"
      ],
      "title": "Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead",
      "venue": "Nature Machine Intelligence,",
      "year": 2019
    },
    {
      "authors": [
        "R.R. Selvaraju",
        "M. Cogswell",
        "A. Das",
        "R. Vedantam",
        "D. Parikh",
        "D. Batra"
      ],
      "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
      "venue": "In The IEEE International Conference on Computer Vision (ICCV),",
      "year": 2017
    },
    {
      "authors": [
        "A. Shrikumar",
        "P. Greenside",
        "A. Kundaje"
      ],
      "title": "Learning Important Features Through Propagating Activation Differences",
      "venue": "arXiv e-prints, art",
      "year": 2017
    },
    {
      "authors": [
        "K. Simonyan",
        "A. Vedaldi",
        "A. Zisserman"
      ],
      "title": "Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps",
      "venue": "arXiv e-prints, art",
      "year": 2013
    },
    {
      "authors": [
        "M. Staniak",
        "P. Biecek"
      ],
      "title": "Explanations of Model Predictions with live and breakDown Packages",
      "venue": "The R Journal,",
      "year": 2018
    },
    {
      "authors": [
        "M. Staniak",
        "P. Biecek"
      ],
      "title": "LIME-based Explanations With Interpretable Inputs Based on Ceteris Paribus Profiles, 2019",
      "venue": "URL https://modeloriented.github.io/localModel/. R package version",
      "year": 2019
    },
    {
      "authors": [
        "J. Vanschoren",
        "J.N. van Rijn",
        "B. Bischl",
        "L. Torgo"
      ],
      "title": "Openml: Networked science in machine learning",
      "venue": "SIGKDD Explorations,",
      "year": 2013
    },
    {
      "authors": [
        "S. Wachter",
        "B. Mittelstadt",
        "C. Russell"
      ],
      "title": "Counterfactual Explanations without Opening the Black Box: Automated Decisions and the GDPR",
      "venue": "arXiv e-prints, art",
      "year": 2017
    },
    {
      "authors": [
        "R. Wexler"
      ],
      "title": "When a Computer Program Keeps You in Jail",
      "venue": "https://www.nytimes.com/2017/06/13/opinion/ how-computers-are-harming-criminal-justice.html,",
      "year": 2017
    },
    {
      "authors": [
        "C.-K. Yeh",
        "C.-Y. Hsieh",
        "A. Sai Suggala",
        "D. Inouye",
        "P. Ravikumar"
      ],
      "title": "On the (In)fidelity and Sensitivity for Explanations",
      "venue": "arXiv e-prints, art",
      "year": 2019
    }
  ],
  "sections": [
    {
      "heading": "1 Introduction",
      "text": "Predictive models are used in almost every aspect of our lives, in school, at work, in hospitals, police stations, and even dating services. They are useful, yet, at the same time can be a serious threat. Models that make unexplainable predictions may be harmful (O\u2019Neil, 2016). The list of such cases range from accidents while using surgical robots that cause minor injuries to patients (Alemzadeh et al., 2016) to problems with automated criminal justice technologies (Wexler, 2017) or incorrect predictions of air quality (McGough, 2018).\nThe need for higher transparency and explainability of models has been a hot topic in the last year, both in the Machine Learning community (Gill and Hall, 2018) as well as in the legal community that coined the phrase \u201eRight to Explanation\u201d in the discussion around General Data Protection Regulation (Wachter et al., 2017; Edwards and Veale, 2018). Since predictive models affect our lives so greatly, we should have the right to know what drives their predictions.\nIn recent years, several methods for model explanations have been developed. New techniques were proposed for image data (Simonyan et al., 2013; Ribeiro et al., 2016; Selvaraju et al., 2017), text data (Marco Tulio Ribeiro and Sameer Singh and Carlos Guestrin, 2018; Jacovi et al., 2018; Alvarez-Melis and Jaakkola, 2017), and tabular data (Molnar, 2019; Biecek, 2018). The main idea behind local explanations is to create an understandable representation of the local behavior of an underlying model. Yet, as predictive models are complex and good explanations should be simple, there is always a trade-off between fidelity and readability of explanations. Sparse explanations will be only approximations and simplifications of the underlying model, and the simpler explanation, the more we lose in the fidelity. This causes an increasing number of voices to avoid using explanations in high-stakes decisions (Rudin, 2019; Lakkaraju and Bastani, 2020; Loyola-Gonz\u00e1lez, 2019). That is why it is important to assess not only the accuracy of a model but also assess the accuracy of such explanations (Alvarez-Melis and Jaakkola, 2018).\nIn this article, we focus only on tabular data which are the most frequent in real-world applications. One of the most known local explanations for tabular data, are SHapley Additive exPlanations (SHAP) (Lundberg and Lee, 2017), Local Interpretable Model-agnostic Explanations (LIME) (Ribeiro et al., 2016), and Break\nar X\niv :1\n90 3.\n11 42\n0v 3\n[ cs\n.L G\n] 8\nDown (Staniak and Biecek, 2018). Such tools are widely adopted, but little is said about the quality of their explanations (Guidotti and Ruggieri, 2018; Yeh et al., 2019).\nThe idea behind LIME is to fit a locally-weighted interpretable linear model in the neighborhood of a particular observation. Numerical and categorical features are converted into binary vectors for their interpretable representations. Such an interpretable representation may be a binary vector indicating the presence or absence of a word in the text classification task or super-pixel in the image classification task. For tabular data and continuous features, quantile-based discretization is performed. A linear model is then fitted on simplified binary variables sampled around the instance of interest. Therefore, the coefficients of this model can be considered as variable effects.\nThere are several modifications of the LIME (Ribeiro et al., 2016) approach, for example live (Staniak and Biecek, 2018) and two R implementations of lime (Pedersen and Benesty, 2019; Molnar et al., 2018). The live method is aimed at regression problems and tabular data. There are two main differences between live and LIME. In live, similar instances around original observation are generated by perturbing one feature at a time and original variables are used as interpretable inputs. Another variant of LIME is localModel (Staniak and Biecek, 2019). In this method, local sampling is based on decision trees and Ceteris Paribus Profiles (Kuzba et al., 2019). Categorical variables are dichotomized as in the splits of a decision tree, which models the marginal relationship between the feature and response. Numerical variables are transformed into a binary via discretization of Ceteris Paribus Profiles for observation under consideration. On the contrary to other approaches, localModel creates interpretable features based not only on the distribution of underlying data but also on a model. The potential problems with LIME-based approaches is the variety of possible definitions of the similarity instances, especially for tabular data. Further on, the problem is the choice of the distribution from which similar observations should be drawn and finally the instability of the explanations (Molnar, 2019).\nThe SHapley Additive exPlanations (SHAP) (Lundberg and Lee, 2017) are a unification of several methods, among them: LIME, DeepLIFT (Shrikumar et al., 2017), and layer-wise relevance propagation (Bach et al., 2015). SHAP is based on Shapley values, a technique used in game theory. In this method, we calculate the contribution of variable (coalition of players) as an average of contributions of each possible ordering of variables. Another coalition-based measure of variable influence is Quantitative Input Influence (QII) (Datta et al., 2016). The contributions are calculated based on Shapley values or Banzhaf index (Banzhaf, 1965). The Banzhaf index measures the power of a player by the fraction of all votes that the player can alter by changing their decision.\nAnother local method is Break Down (Staniak and Biecek, 2018). The main idea of Break Down is to generate orderspecific explanations of features\u2019 contributions. It is important to consider ordering for two reasons. For non-additive models the order of features in an explanation matters, this means that an interpretation of the model-reasoning depends on the order in which the explanation is read. An example of different interpretations is presented in Section 2.4. Setting a proper order helps to increase the understanding of prediction. Human perception usually associates the prediction with only a few variables. Therefore, it is important to highlight only the most important features and set insignificant variables at the end of the explanation.\nIn the Break Down method, contributions of variables are calculated sequentially. The effects of consecutive variables depend on the change of expected model prediction while all previous variables are fixed. Contributions of features are presented in the form of waterfall plots. This form of visualization is appreciated and is widely used to present results in oncology clinical trials (Gillespie, 2012). Waterfall plots facilitate interpretation of an explanation in the form of a scenario, in which the prediction comes from successive contributions of variables.\nThe key issue of local explanations, such as SHAP and LIME, is that they show additive local representations, while complex models are usually non-additive. Therefore, current methods often do not include all nuances of a model, such as interactions, and therefore turn out to be too imprecise. Thus, we need to find approaches that are more accurate to explain the underlying model. One of the possible ways of solving this problem is to take into account the interactions between features.\nContributions in this article are the following:\n1. In Section 2, we point out three main problems with additive explanations, such as inconsistency, uncertainty, and infidelity. We identify the reasons behind these issues, such as ignoring interactions. We introduce a visual representation of additive explanation uncertainty.\n2. In Section 3, we introduce the novel iBreakDown method to capture local interactions and generate nonadditive explanations with interactions visualized by waterfall plots. We prove that the Shapley value is an average over Break Down contributions for all possible ordering of variables.\n3. In Section 4, we performed a large scale benchmark to show that non-additive local relationships between features are frequent.\n4. We have developed R and Python libraries with the implementation of the iBreakDown algorithm and supplementary visual explanations."
    },
    {
      "heading": "2 What is wrong with additive explanations?",
      "text": "In this section, we present the state-of-the-art methods for additive explanations with example of the toy data set Titanic1. We show inconsistency in their results, describe a method to assess their uncertainty and clear the idea of local interaction in a model that may be the reason for infidelity."
    },
    {
      "heading": "2.1 A toy example",
      "text": "For the purpose of the example, we have trained a random forest model to predict whether a passenger survived or not and used different additive methods to explain the model\u2019s predictions for the same passenger.\nGraphical presentation of a LIME explanation is presented in Figure 1. Results of SHAP for the Titanic data set generated with the Python library are presented in Figure 2. Two Break Down explanations are presented in Figure 3. Contributions of variables differ between scenarios because each scenario relies on a different order of variables. The Break Down contributions are calculated by consecutively adding variables, one by one. Therefore, the order of variables is important. For an additive model, regardless of the order, the contribution values are equal in each scenario. Changes in values suggest that the model is non-additive, thus, there is an interaction between variables."
    },
    {
      "heading": "2.2 Inconsistency of additive explanations",
      "text": "The common approaches to local explanations consider the effect of each variable separately. However, when interactions occur in the model, relationships between variables should be also taken into account. Omitting influence of interactions causes a loss of a part of the information about the effects of the variables, but adds undesired randomness in the evaluation of these effects.\nThe values of feature importance LIME and contributions for SHAP and Break Down are summarized in Table 1. The size of effects differs between methods, and there are even differences in the judgment of whether the impact is positive or negative. It is not clear which explanation should be considered the most reliable.\nLIME approximates the underlying model with a linear model, while SHAP averages across all possible combinations of variable contributions. Break Down calculate contributions based on the specified order of variables. For additive models, the results of LIME, SHAP, and Break Down would be similar. The interaction of variables may be the cause of\n1https://www.kaggle.com/c/titanic\ndifferences between explanations. What is more, in Figure 3, we see that values of contributions even differ for other orders of variables. The differences between Break Down scenarios also leads to the conclusion that the reason for inconsistency can be the interaction between variables. Visualization of different variable orders in the Break Down method rendered it possible to identify the source of differences in LIME and SHAP predictions, and thus better-explained model prediction. However, interactions are not included in any of these three methods, thus we should not rely on these explanations.\nDetecting interactions would reduce the uncertainty and would increase the fidelity of explanations. One approach to capturing interactions may be analyzing different orders of features in the Break Down algorithm. However, comparing many scenarios is highly ineffective. As the number of variables increases, the number of cases to review increases factorially. The solution to this problem is iBreakDown, a local explanation method that captures interactions. We introduce iBreakDown in Section 3."
    },
    {
      "heading": "2.3 Uncertainty of additive explanations",
      "text": "When generating an explanation for a model, it is important to know how much it can by relied upon. Therefore, the uncertainty of the explanation should also be assessed. We propose a methodology for assessing the uncertainty of Break Down explanations. The idea is to use bootstrapping to generate a sample of different explanations and measure the stability of contribution values.\nIn this setup, we have one fixed underlying model and one baseline explanation of this model. The first step is to generate m random samples of variable orders. Next, we generate a Break Down explanation concerning each sampled variable order. As a result, we obtain m new explanations. The procedure of computing the uncertainty of explanations is presented in Algorithm 1.\nThe example summary plot of bootstrapping explanations is presented in Figure 4. Uncertainty is realized as a variation of contribution values between explanations. Error bars show the range of contribution values for explanations generated on different variable orders. Widths of error bars indicate the uncertainty of variables\u2019 contributions. The wider the bar, the less certain contribution is.\nWe impose randomness of explanations by forcing different variable orders while the model and explained instance are fixed. Each order of variables is a bootstrap sample. The whole variability is the result of the uncertainty of the explanation. What is more, as the SHAP method is average over all Break Down scenarios and SHAP is the unification of different explanations, bars show also the uncertainty of SHAP.\nAlgorithm 1 Explanation level uncertainty 1: Input: Xn\u00d7p - data; f - model; x\u2217 - new observation 2: for k in {1, 2, ...,K} do 3: sample pathk of features as random permutation 4: Calculate explanations [\u2206\u2217,k1 , ...,\u2206 \u2217,k p ] of model f , observation x\n\u2217, data set X , and pathk 5: A matrix of contributions \u2206\u2217,ji 6: Shapley additive contribution for feature i is an average of vector [\u2206\u2217,1i , ...,\u2206 \u2217,K i ] 7: Explanation level uncertainty for feature i is interquartile range of vector [\u2206\u2217,1i , ...,\u2206 \u2217,K i ]\nSince Break Down is an additive method of explanation, the high variability of contribution, realized by wide error bars, is related to the occurrence of interaction. To reduce the uncertainty of explanation, the interaction should be taken into account."
    },
    {
      "heading": "2.4 Infidelity of additive explanations",
      "text": "Now, we will broaden the example for Titanic data and explain the interaction in the underlying model. We also showing an iBreakDown explanation.\nIn our example, the training data set consists of 4 variables.\n\u2022 Survival - binary variable indicates whether passenger survived, 1 for survival and 0 for death. \u2022 Age - numerical variable, age in years. \u2022 Sex - binary variable, 0 for male and 1 for female. \u2022 PClass - categorical variable, ticket class, 1, 2, or 3.\nWe explain the model\u2019s prediction for a 2-year-old boy traveling in second class. The model predicts survival with a probability of 0.964. We would like to explain this probability and understand which factors drive this prediction. In Figure 3, we showed two Break Down explanations. Each of them may be interpreted differently.\nScenario 1: The passenger is a boy, and this feature alone decreases the chances of survival by 19 percentage points. He was travelling in the second class, which also lower survival probability by 6.6 percentage points. Yet, he is very young, which increases the odds by 81.2 percentage points . The reasoning behind such an explanation on this level is that most passengers in second class are adults, therefore a child from the second class has high chances of survival.\nScenario 2: The passenger is a boy, and this feature alone decreases survival probability by 19 percentage points . However, he is very young, therefore the odds are higher (by 26.6 percentage points ) than for adult men. The explanation in the last step says that he traveled in second class, which make the odds of survival even more higher (by 48 percentage points ). The interpretation of this explanation is that most children in third class and being a child in second class should increase the chances of survival.\nNote that the effect of the second class is negative in explanations for scenario 1 but positive in explanations for scenario 2. Two interpretations of the above scenarios imply the existence of an interaction between age and ticket class. The algorithm introduced in the previous section founds this interaction. The corresponding explanation is presented in Figure 5.\nScenario 3 (with interactions): The passenger is a boy in second class, which increases the chance of survival by 53.5 percentage points because the effect of age depends on the passenger class.\nThe inclusion of the interaction in the explanation produced different reasoning, that better reflects the way the underlying model predicts the probability of survival."
    },
    {
      "heading": "3 How to explain interactions",
      "text": "If the uncertainty of model explanations is linked with the presence of interactions, then we have to include interactions to model explanations. This way we will have more stable and reliable explanations. In this section, we introduce a novel methodology for the identification of interactions in instance-level explanations. The algorithm works in a similar vein with SHAP or Break Down but is not restricted to additive effects. The intuition is the following:\n1. Calculate a single-step additive contribution for each feature. 2. Calculate a single-step contribution for every pair of features. Subtract additive contribution to assess the\ninteraction specific contribution. 3. Order interaction effects and additive effects in a list that is used to determine sequential contributions.\nThis simple intuition may be generalized into higher order interactions.\nLet f : X\u2192 R be a predictive model and x\u2217 \u2208 X be an observation to explain. For the sake of simplicity, we consider a univariate model output, more suited for classification or regression, but every step can be easily generalized into multiclass classification or multivariate regression.\nFor a feature xi we may define a single-step contribution.\n\u2206i = scorei(f, x \u2217) = E[f(x)|xi = x\u2217i ]\u2212 E[f(x)]. (1)\nExpected model prediction E[f(x)] is sometimes called baseline or intercept and may be denoted as \u2206\u2205. Expected value E[f(x)|xi = x\u2217i ] corresponds to an average prediction of a model f if feature xi is fixed on x\u2217i coordinate from the observation to explain x\u2217. \u2206i measures a naive single-step local variable importance. It indicates how much the average prediction of model f changes if feature xi is set on x\u2217i .\nAlgorithm 2 is a procedure for the calculation of \u2206i, i.e. single-step contributions for each feature.\nFor a pair of variables xi, xj we introduce a single-step contribution as\n\u2206ij = scorei,j(f, x \u2217) = E[f(x)|xi = x\u2217i , xj = x\u2217j ]\u2212 E[f(x)]. (2)\nSimilarly, we introduce a corresponding interaction specific contribution as\n\u2206Iij = E[f(x)|xi = x\u2217i , xj = x\u2217j ]\u2212 E[f(x)|xi = x\u2217i ]\u2212 E[f(x)|xj = x\u2217j ] + E[f(x)]. (3)\nIt is an equivalent to\n\u2206Iij = E[f(x)|xi = x\u2217i , xj = x\u2217j ]\u2212 scorei(f, x\u2217)\u2212 scorej(f, x\u2217)\u2212 E[f(x)] = \u2206ij \u2212\u2206i \u2212\u2206j .\n(4)\nA value of E[f(x)|xi = x\u2217i , xj = x\u2217j ] is an average model output if feature xi and xj are fixed on x\u2217i and x\u2217j respectively. \u2206Iij is the difference between collective effect of variables xi and xj denoted as \u2206ij and their additive effects \u2206i and \u2206j . Therefore, \u2206Iij measures the importance of local lack-of-additiveness (aka. interaction) between features i and j. For additive models \u2206Iij is small for any i, j.\nAlgorithm 3 is a procedure for calculation of \u2206ij and \u2206Iij , i.e. single-step contributions and interactions for each pair.\nCalculating \u2206i for each variable is Step 1, computing \u2206Iij for each pair of variables is Step 2. Note that contributions \u2206i do not sum to the final model prediction. We only use them to determine the order of features in which the instance shall be explained.\nWe need to provide one more symbol, that corresponds to the added contribution of feature xi to the set of features with indexes from set J .\nDefinition 1 Break Down contribution of the feature xi to the set of features with indexes from J can be formulated as:\n\u2206i|J = E[f(X)|xJ\u222a{i} = x\u2217J\u222a{i}]\u2212E[f(X)|xJ = x \u2217 J ] = \u2206J\u222a{i} \u2212\u2206J . (5)\nDefinition 2 Break Down contribution of a pair of features xi and xj to the set of features with set of indexes from J can be formulated as:\n\u2206ij|J = E[f(X)|xJ\u222a{i,j} = x\u2217J\u222a{i,j}]\u2212E[f(X)|xJ = x \u2217 J ] = \u2206J\u222a{i,j} \u2212\u2206J . (6)\nOnce the order of single-step importance is determined based on \u2206i and \u2206Iij scores, the final explanation is the attribution to the sequence of \u2206i|J scores. These contributions for all p features sum up to the model predictions because\n\u22061,2...p = f(x \u2217)\u2212 E[f(X)].\nAlgorithm 4 applies consecutive conditioning to ordered variables. It consists of setting a path due to the calculated effects, then calculating contributions. The time complexity of each of the two steps of the procedure is O(p) where p is the number of explanatory variables.\nThe introduced method takes into account the interactions between variables. A large difference between the sum of consecutive effects of features and the effect of a pair of features indicates interaction. The algorithm can be generalized to interactions between any number of variables.\nThere is a similar idea of calculating differences between the sum of independent effects of variables and joint effect to calculate SHAP interaction values (Lundberg et al., 2018). However, their approach is based on averaging contributions over all possible ordering of features. Such an approach makes it hard to assess the uncertainty or stability of the explanation.\nAlgorithm 2 Single-step contributions of features 1: Input: Xn\u00d7p - data; f - model; x\u2217 - new observation 2: Calculate average model response 3: \u2206\u2205 = mean(f(X)) 4: for i in {1, 2, ...p} do 5: Calculate contribution of the i-th feature 6: avg_yhat = mean(f(Xxi=x\u2217i )) 7: \u2206i = avg_yhat\u2212\u2206\u2205 8: [\u22061, ...,\u2206p] contains contributions of features\nAlgorithm 3 Single-step contributions of pairs of features 1: Input: Xn\u00d7p - data; f - model; x\u2217 - new observation; \u2206i - vector of single-step contributions. 2: for i in {1, 2, ...p} do 3: for j in {1, 2, ...p}/{i} do 4: Calculate contribution of pair i,j. 5: avg_yhat = mean(f(Xxi=x\u2217i ,xj=x\u2217j )) 6: \u2206ij = avg_yhat\u2212\u2206\u2205 7: \u2206Iij = \u2206ij \u2212\u2206i \u2212\u2206j 8: \u2206I contains a matrix with interaction contributions for pairs of features (\u2206Iij).\nAlgorithm 4 Sequential explanations 1: Input: Xn\u00d7p - data; f - model; x\u2217 - new observation; [\u22061, ...,\u2206p] - vector of single-step feature contributions;\n\u2206I - table of single-step feature interactions (\u2206Iij); 2: Calculate \u2206\u2217 which is a sorted union of \u2206i and \u2206Iij ordered by absolute values of elements. 3: features - a table of features and pairs in order corresponding to \u2206\u2217. 4: open = {1, 2, ..., p} 5: for candidates in features do 6: if candidates in open then 7: path = append(path, candidates) 8: open = setdiff(open, candidates) 9: yhat = mean(f(X|x\u00acopen = x\u2217\u00acopen))\n10: avg_yhats = append(avg_yhats, yhat) 11: Explanation order is determined in the path vector. 12: history = \u2205 13: for k in {1, 2, ...length(path)} do 14: I is a single variable or pair of variables 15: I = path[k] 16: history = history \u222a I 17: attribution[i] = \u2206I|history = \u2206I\u222ahistory \u2212\u2206history = avg_yhats[k]\u2212 avg_yhats[k \u2212 1] 18: Explanations are in the attribution vector."
    },
    {
      "heading": "3.1 Break Down as a SHAP scenario",
      "text": "Definition 3 The Shapley value (Molnar, 2019) \u03c6 of the feature j, model f , and observation x\u2217 is defined as\n\u03c6j(x \u2217, f) = \u2211 S\u2282{1,...,p}\\{j} |S|!(p\u2212 |S| \u2212 1)! p! [fS\u222aj(xS\u222aj)\u2212 fS(xS)], (7)\nwhere xi is an i-th variable of observation x\u2217, p is the number of variables, |S| denotes the size of subset S of model features indexes, and fS(xS) is the expected value of function f conditional on feature values in set S, fS(xS) = E[f(X)|xS = x\u2217S ].\nAccording to the Definition 1, fS(xS) = E[f(X)|xS = x\u2217S ] = \u2206S and \u2206S\u222a{j} \u2212\u2206S = \u2206j|S .\nLet \u03c0(S) be the set of permutations of the set S and \u03c1(j, v) be a subset of elements from v \u2208 \u03c0({1, ..., p}), preceding j-th element.\nRemark 1 Shapley value \u03c6j(x\u2217, f) is an average over Break Down contributions \u2206j|\u03c1(j,v) for all possible ordering of variables.\nProof\nShapley value for variable j is\n\u03c6j(x \u2217, f) = \u2211\nS\u2282{1,...,p}\\{j}\n|S|!(p\u2212 |S| \u2212 1)! p! [fS\u222a{j}(xS\u222a{j})\u2212 fS(xS)] =\n\u2211 S\u2282{1,...,p}\\{j} |S|!(p\u2212 |S| \u2212 1)! p! [\u2206S\u222a{j} \u2212\u2206S ] =\n1\np! \u2211 S\u2282{1,...,p}\\{j} |S|!(p\u2212 |S| \u2212 1)!\u2206j|S ,\n(8)\nwhere \u2206j|S is Break Down contribution of variable xj to the set of features S. Let us note that |\u03c0(S)| = |S|! and \u2206j|S is the same for any ordering over S. Therefore |S|!\u2206j|S = \u2211\n\u03a0\u2208\u03c0(S) \u2206j|\u03a0, where \u03a0 is an ordered set. Similarly (p\u2212 |S| \u2212 1)!\u2206j|S = \u2211 \u03a0\u2208\u03c0({1,...,p}\\(S\u222a{j})) \u2206j|\u03a0. From this and Equation 8 we have\n\u03c6j(x \u2217, f) =\n1\np! \u2211 S\u2282{1,...,p}\\{j} \u2211 \u03a01\u2208\u03c0(S) |p\u2212 S \u2212 1|!\u2206j|\u03a01 =\n1\np! \u2211 S\u2282{1,...,p}\\{j} \u2211 \u03a01\u2208\u03c0(S) \u2211 \u03a02\u2208\u03c0({1,...,p}\\(S\u222a{j})) \u2206j|\u03a01,\u03a02 ,\n(9)\nwhere \u2206j|\u03a01,\u03a02 is a Break Down contribution when the order of variables preceding xj is the order in \u03a01, and the order of variables following xj is the order in \u03a02,\nx\u2022, .................., x\u2022\ufe38 \ufe37\ufe37 \ufe38 \u03a01\u2208\u03c0(S) , xj , x\u2022, .................., x\u2022\ufe38 \ufe37\ufe37 \ufe38 \u03a02\u2208\u03c0({1,...,p}\\(S\u222a{j})) . (10)\nLet us note that the last part of the equation sum over all possible orderings of p features. For given variable xj and given set S, the Break Down contribution \u2206j|S is the same for any order of variables in S, therefore \u2200\u03a0,\u03a0\u0304\u2208\u03c0(S)\u2206j|\u03a0 = \u2206j|\u03a0\u0304. The same holds for orders of variables following xj .\n\u03c1(v) is a set of variables preceding xj in the order of variables v \u2208 \u03c0({x1, ..., xp}), then from Equation 9 we have\n\u03c6j(x \u2217, f) =\n1\np! \u2211 v\u2208\u03c0({1,...,p}) \u2206j|\u03c1(v). (11)\nWe sum Break Down contributions \u2206j|\u03c1(v) over all possible p! orderings of variables. Therefore, Shap value \u03c6j(x\u2217, f) is an average over all possible orderings of Break Down contributions."
    },
    {
      "heading": "4 How frequent are interactions in machine learning models?",
      "text": "We have applied the iBreakDown method on dozens of classification data sets. The experiment aimed to justify the need to include interactions in local explanations. We address the following questions: (1) Are the additive explanation methods reliable enough? (2) Are the interactions useful for local explanations?"
    },
    {
      "heading": "4.1 Setup of the benchmark on OpenML",
      "text": "We have performed experiment on 28 data sets from OpenML100 (Bischl et al., 2017) collection of data sets. We have selected data sets for binary classification that do not contain missing values and consist of less than 100 features. For each data set, we have fitted random forest and three gradient boosting machines (GBM) models with a maximum depth of trees equal 1, 2, and 3. A depth of 1 implies an additive model, a depth of 2 implies a model with up to two-way interactions, and a depth of 3 implies a model with up to three-way interactions. The performance of models for all\nof the data sets is presented in Figure 6. AUC was calculated for the first train and test split defined in each OpenML task. For the benchmark, we took 50 observations from each data set (28) and for each model (4), then we calculated iBreakDown explanations of these observations. That gave us in total 50 \u2217 28 \u2217 4 = 5600 explanations."
    },
    {
      "heading": "4.2 Results",
      "text": "The results of the experiment are in Tables 3 and 2. For each data set and each model, the table contains the number of interaction occurrences. For example, for task 3 and model GBM 2, 49 explanations do not contain interactions and 1 explanation with 1 interaction. For GBM models with interactions and random forest, interactions were identified in most of the tasks. There is a clear correlation that the more complex interactions included in the model, the more local interactions were detected by iBreakDown.\nTask 3493 deserves particular focus since the AUC of the models trained on this task was significantly different (see Figure 6). Including interaction in the models increased performance, therefore we expected to identify interactions in explanations. Indeed, explanations of models with interactions contain at least one interaction. Figure 7 consists of models\u2019 explanations for the same observation. An additive GBM model does not have any interaction in the Break Down path, while more complex models include interactions in their paths. The detection of different interactions is due to the fact that the models could learn different relationships between the variables.\nAccording to the results in Table 3, the iBreakDown method detected local interactions, although the models under consideration were additive. We find the reason for this in the correlations in the data that are reflected in the detection of interaction. Taking into account the overall results and the above example, we can answer the questions stated at the beginning of this section.\n(1) Are the additive methods reliable enough? About 71% of the explanations from the benchmark consist of interactions. Therefore, the usage of additive explanations would strongly simplify the explanations, which would make them less accurate. As a result, there would be considerable uncertainty in these explanations.\n(2) Are the interactions useful for local explanations? Nuances of the model can be better identified by including local interactions in the explanations, which both increase trust in the predictions and reduce uncertainty. The experiment shows that interactions were detected for many observations. The iBreakDown method allows us to better explain models\u2019 predictions for these instances."
    },
    {
      "heading": "5 Conclusions",
      "text": "This article examined the behaviour of the common local additive explanations such as SHAP, LIME, and Break Down. For the same random forest model, each method generated inconsistent explanations, sometimes even with opposite signs. As we demonstrated, some of the uncertainty and infidelity of the explanations is linked with the lack of additives in the model, which cannot be grasped by the additive explanations. Simple explanations may omit some important parts of model behavior, therefore we introduced the procedure to measure and visualize this type of the uncertainty.\nTo solve the problems with uncertainty and infidelity of explanations, we introduced the new iBreakDown method, which identifies local interactions and generates not-only-additive explanations. The theoretical backbone of this algorithm is similar to the SHAP and Break Down methods, yet, in contrast to them we also considered pairwise interactions. It should be noted that, for simple linear models, interactions may be included directly in model terms, yet we do not have that control in the case of more complex models. Such models grab interactions due to their elastic structure and we showed how such interactions can be identified and presented.\nFinally, we applied the iBreakDown on several data sets and showed that in the majority of the explanations our method detected local interactions, therefore additive explanations were not reliable enough.\nFor tabular data, most of the local explanation methods are additive. Applying them to non-additive models increases the uncertainty of such explanations. Tools in the area of Interpretable Machine Learning are developed to explain complex black-box models. We cannot assume that such complex models will be additive, instead we should expect, identify and handle interactions in these models. A solution to handling interactions and explaining uncertainty linked with feature contributions is the iBreakDown algorithm."
    },
    {
      "heading": "5.1 Future work",
      "text": "The iBreakDown method identifies interactions and measures their contributions. However, the main effects of variables and interaction between them are currently presented as a single value. It would be desirable to separate the main effects and the contribution of an interaction and present deeper visual clues that help to understand the role of interaction.\nThe presented approach for handling the explanation level of uncertainty also needs further examination. The inclusion of interactions in the explanation improves its certainty, yet at the same time, explanations may become more difficult to understand than the additive representations. It is a field for extensive cognitive studies of visual presentation of explanations."
    },
    {
      "heading": "5.2 Software",
      "text": "The Break Down with the interactions algorithm and plots are implemented and available as open source R package iBreakDown 2 and Python library piBreakDown 3. R package iBreakDown also provides interactive versions of plots implemented in D3.js JavaScript library and diagnostic plots for Break Down explanations.\nThe code that generates examples included in this article and performs experiments can be found in the GitHub repository: 4.\n2https://github.com/ModelOriented/iBreakDown 3https://github.com/ModelOriented/piBreakDown"
    },
    {
      "heading": "6 Acknowledgements",
      "text": "We would like to acknowledge and thank Hubert Baniecki for his valuable contribution to the development of the iBreakDown package, especially the interactive plots. We would like to thank Mateusz Staniak, Anna Gierlak, Katarzyna Kobylin\u0301ska, Anna Kozak, and Katarzyna Woznica for the valuable discussions.\nThis work was supported by the Polish National Science Centre under Opus Grant number 2017/27/B/ST6/01307 and 2016/21/B/ST6/02176."
    }
  ],
  "title": "DO NOT TRUST ADDITIVE EXPLANATIONS",
  "year": 2020
}
