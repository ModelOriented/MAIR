{
  "abstractText": "Much of the focus in machine learning research is placed in cr eating new architectures and optimization methods, but the overall loss fun ctio is seldom questioned. This paper interprets machine learning from a multi -objective optimization perspective, showing the limitations of the default li near combination of loss functions over a data set and introducing the hypervolume in dicator as an alternative. It is shown that the gradient of the hypervolume is defin ed by a self-adjusting weighted mean of the individual loss gradients, making it si milar to the gradient of a weighted mean loss but without requiring the weights to b e defined a priori. This enables an inner boosting-like behavior, where the cur rent model is used to automatically place higher weights on samples with higher l osses but without requiring the use of multiple models. Results on a denoising au toencoder show that the new formulation is able to achieve better mean loss than t e direct optimization of the mean loss, providing evidence to the conjecture t hat self-adjusting the weights creates a smoother loss surface.",
  "authors": [
    {
      "affiliations": [],
      "name": "Conrado S. Miranda"
    }
  ],
  "id": "SP:9f80dc3eb89b0142b607ae5bff50faf5280e1d26",
  "references": [
    {
      "authors": [
        "N.D. Koller"
      ],
      "title": "Friedman.Probabilistic Graphical Models: Principles and Techniques",
      "venue": "MIT press,",
      "year": 2009
    },
    {
      "authors": [
        "Y. Bengio"
      ],
      "title": "Learning deep architectures for AI",
      "venue": "Foundations and trends R \u00a9 in Machine Learning,",
      "year": 2009
    },
    {
      "authors": [
        "K.P. Bennett",
        "E. Parrado-Hern\u00e1ndez"
      ],
      "title": "The interplay of optimization and machine learning research",
      "venue": "The Journal of Machine Learning Research,",
      "year": 2006
    },
    {
      "authors": [
        "M. C"
      ],
      "title": "Bishop.Pattern Recognition and Machine Learning",
      "year": 2006
    },
    {
      "authors": [
        "A. Banerjee",
        "S. Merugu",
        "I. S Dhillon",
        "J. Ghosh"
      ],
      "title": "Cluste ring with Bregman divergences",
      "venue": "The Journal of Machine Learning Research,",
      "year": 2005
    },
    {
      "authors": [
        "P.M. Long",
        "R.A. Servedio"
      ],
      "title": "Random classification noise def ats all convex potential boosters",
      "venue": "Machine Learning,",
      "year": 2010
    },
    {
      "authors": [
        "K. Deb"
      ],
      "title": "Multi-objective optimization. InSearch methodologies, pages 403\u2013449",
      "year": 2014
    },
    {
      "authors": [
        "S.P. Boyd",
        "L. Vandenberghe"
      ],
      "title": "Convex Optimization",
      "year": 2004
    },
    {
      "authors": [
        "E. Zitzler",
        "D. Brockhoff",
        "L. Thiele"
      ],
      "title": "The hypervolume indicator revisited: On the design of Paretocompliant indicators via weighted integration",
      "venue": "In Evolutionary multi-criterion optimization,",
      "year": 2007
    },
    {
      "authors": [
        "E. Zitzler",
        "L. Thiele",
        "M. Laumanns",
        "C.M. Fonseca",
        "V.G. Da Fonseca"
      ],
      "title": "Performance Assessment of Multiobjective Optimizers: An Analysis and Review",
      "venue": "Evolutionary Computation, IEEE Transactions on,",
      "year": 2003
    },
    {
      "authors": [
        "N. Beume",
        "C.M. Fonseca",
        "M. L\u00f3pez-Ib\u00e1\u00f1ez",
        "L. Paquete",
        "an d J. Vahrenhold"
      ],
      "title": "On the complexity of computing the hypervolume indicator",
      "venue": "Evolutionary Computation, IEEE Transactions on,",
      "year": 2009
    },
    {
      "authors": [
        "A. Auger",
        "J. Bader",
        "D. Brockhoff",
        "E. Zitzler"
      ],
      "title": "Theory of the hypervolume indicator: optimal \u03bcdistributions and the choice of the reference point",
      "venue": "In Proceedings of the tenth ACM SIGEVO workshop on Foundations of genetic algorithms,",
      "year": 2009
    },
    {
      "authors": [
        "R.E. Schapire"
      ],
      "title": "The strength of weak learnability",
      "venue": "Machine learning,",
      "year": 1990
    },
    {
      "authors": [
        "P. Vincent",
        "H. Larochelle",
        "Y. Bengio",
        "P.-A. Manzago l"
      ],
      "title": "Extracting and Composing Robust Features with Denoising Autoencoders",
      "venue": "In International Conference on Machine Learning,",
      "year": 2008
    }
  ],
  "sections": [
    {
      "text": "ar X\niv :1\n50 6.\n01 11\n3v 2\n[s ta\nt.M L]\nMuch of the focus in machine learning research is placed in creating new architectures and optimization methods, but the overall loss functio is seldom questioned. This paper interprets machine learning from a multi-objective optimization perspective, showing the limitations of the default linear combination of loss functions over a data set and introducing the hypervolume indicator as an alternative. It is shown that the gradient of the hypervolume is defined by a self-adjusting weighted mean of the individual loss gradients, making it similar to the gradient of a weighted mean loss but without requiring the weights to be defined a priori. This enables an inner boosting-like behavior, where the current model is used to automatically place higher weights on samples with higher losses but without requiring the use of multiple models. Results on a denoising autoencoder show that the new formulation is able to achieve better mean loss than te direct optimization of the mean loss, providing evidence to the conjecture that self-adjusting the weights creates a smoother loss surface."
    },
    {
      "heading": "1 Introduction",
      "text": "Many machine learning algorithms, including neural networks, can be divided into three parts: the model, which is used to describe or approximate the structure present in the training data set; the loss function, which defines how well an instance of the modelfits the samples; and the optimization method, which adjusts the model\u2019s parameters to improve therror expressed by the loss function. Obviously these three parts are related, and the generalization capability of the obtained solution depends on the individual merit of each one of the three parts, nd also on their interplay.\nMost of current research in machine learning focuses on creating new models [1, 2], for the different applications and data types, and new optimization methods [3], which may allow faster convergence, more robustness, and a better chance to escape from poor local minima. However, focus is rarely attributed to the cost functions used in different applications.\nMany cost functions come from statistical models [4], such as the quadratic error or cross-entropy. These cost functions, in their linear formulations, usually define convex loss functions [4, 5]. Moreover, when building the statistical model of a sample set, frquently the total cost becomes the sum of losses for each sample. Although this methodology is sound, it can be problematic in real-world applications involving more complex models.\nThe first problem, discussed more extensively in Sec. 2, is that linear combination of losses can reduce the number of solutions achievable by the optimization algorithm. The second problem is that this cost may not reflect the behavior expected by the user.\nConsider, for instance, a classification problem with samplesx1 andx2 and two choices of parameters\u03b81 and\u03b82. For\u03b81, the classifier has confidences99% and49% for the correct classes ofx1 and x2, which means thatx2 is classified incorrectly. For\u03b82, they are classified correctly with confidences90% and53%, respectively. Most machine learning users would prefer using \u03b82, becausex1 has a high-confidence correct prediction in both cases whilex2 is classified incorrectly if\u03b81 is used. Nonetheless, the mean cross-entropy loss, which is the standard cost for classification problems, places higher cost to\u03b82 than to\u03b81, so the optimizer will choose\u03b81.\nAlthough this toy example may not be representative of all the trade-offs involved, it clearly reflects the conflict between what the user wants and what the cost function expresses. One possible solution to this problem is the use of boosting techniques to increasethe weight of incorrectly classified samples and training multiple classifiers, as discussed in Sec. 3, but this requires multiple models and may not be robust to noise [6]. Moreover, this solution still restricts the solutions achievable by the optimization algorithm due to the linear combination oflosses.\nThis paper presents a solution to this problem by evaluatingthe model-fitting problem in a multiobjective optimization perspective, of which the standardlinear combination of losses is a particular case, and uses another method for transforming the problem into a single-objective optimization, which allows the same standard optimization algorithms to be used. It will be shown that the gradient of the new objective automatically provides higher weightsfor samples with higher losses, like boosting methods, but uses a single model, with the current parameters working as the previous model in boosting. It is important to highlight that, although this paper focuses on optimization by gradient, the method proposed also works for hill-climbingo discrete problems.\nThe author\u2019s conjecture that automatically placing more learning pressure on samples with high losses may improve the overall generalization, because it may remove small bumps in the error surface. Therefore, the multi-objective formulation allows better minima to be reached. Experimental results presented in this paper provide evidence for this conje ture.\nThe paper is organized as follows. Section 2 provides an overview of multi-objective optimization, properly characterizing the maximization of the hypervolume as a performance indicator for the learning system, given the data set and the loss function. Section 3 deals with a gradient-based approach to maximize the hypervolume, and shows that the gradient operator promotes the selfadjustment of the weights associated with each loss functio. Sections 4 and 5 describe the experiments performed and the results obtained, respectively. Finally, Section 6 provides the concluding remarks and future research directions."
    },
    {
      "heading": "2 Multi-objective optimization",
      "text": "Multi-objective optimization is a generalization of the traditional single-objective optimization, where the problem is composed of multiple objective functions fi : X \u2192 Yi, i \u2208 {1, . . . , N}, whereYi \u2286 R [7]. Using the standard notation, the problem can be described by:\nmin x\u2208X (f1(x), f2(x), . . . , fN(x)),\nwhereX is the decision space and includes all constraints of the optimization.\nIf some of the objectives have the same minima, then the redundant objectives can be ignored during optimization. However, if their minima are different, for examplef1(x) = x2 andf2(x) = (x\u22121)2, then there is not a single optimal point, but a set of different trade-offs between the objectives. A solution that establishes an optimal trade-off, that is, iti impossible to reduce one of the objectives without increasing another, is said to be efficient. The set of fficient solutions is called the Pareto set and its counterpart in the objective spaceY = Y1 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7 YN is called the Pareto frontier.\nA common approach in optimization used to deal with multi-objective problems is to combine the objectives linearly [7, 8], so that the problem becomes\nmin x\u2208X\nN\u2211\ni=1\nwifi(x), (1)\nwhere the weightwi \u2208 R+ represents the importance given to objectivei, i \u2208 {1, . . . , N}.\nAlthough the optimal solution of the linearly combined problem is guaranteed to be efficient, it is only possible to achieve any efficient solution when the Pareto f ontier is convex [8]. This means that more desired solutions may not be achievable by performing aline r combination of the objectives.\nAs an illustrative example, consider the problem whereX = [0, 1] and the Pareto frontier is almost linear, withf1(x) \u2248 x andf2(x) \u2248 1\u2212x, but slightly concave. Any combination of weightsw1 and w2 will only be able to providex = 0 or x = 1, depending on the specific weights, but a solution close to 0.5 might be more aligned with the trade-offs expected by the user."
    },
    {
      "heading": "2.1 Hypervolume indicator",
      "text": "Since the linear combination of objectives is not going to work properly on non-convex Pareto frontiers, it is desirable to investigate other forms of transforming the multi-objective problem into a single-objective one, which allows the standard optimization tools to be used.\nOne common approach in the multi-objective literature is toresort to the hypervolume indicator [9], which is frequently used to analyze a set of candidate solutions [10] and can be expensive in such cases [11]. However for a single solution, its logarithm canbe written as:\nlogHz(x) =\nN\u2211\ni=1\nlog(zi \u2212 fi(x)), (2)\nwherez \u2208 RN is called the Nadir point andzi > fi(x), \u2200x \u2208 X . The problem then becomes maximizing the hypervolume over the domain, and this optimization is able to achieve a larger number of efficient points, without requiring convexity of the Pareto frontier [12].\nAmong the many properties of the hypervolume, two must be highlighted in this paper. The first is that the hypervolume is monotonic in the objectives, which means that any improvement in any objective causes the hypervolume to increase, which in turnis aligned with the loss minimization. The maximum of the single-solution hypervolume is a point inthe Pareto set, which means that the solution is efficient.\nThe second property is that, like the linear combination, italso maintains some shape information from the objectives. If the objectives are convex, then their linear combination is convex and the hypervolume is concave, since\u2212fi(x) is concave and the logarithm of a concave function is concave.\nTherefore, the hypervolume has the same number of parameters as the linear combination, one for each objective, and also maintains optimality, but may be abl to achieve solutions more aligned with the user\u2019s expectations.\nConsider the previous example withf1(x) \u2248 x andf2(x) \u2248 1\u2212 x. If z1 = z2, which indicates that both objectives are equally important a priori, then the hypervolume maximization will indeed find a solution close tox = 0.5. So the parameterz defines prior inverse importances that are closer to what one would expect from weights: if they are equal, the solver should try to provide a balanced relevance to the objectives.\nTherefore, the hypervolume maximization with a single model tri s to find a good solution to all objectives at the same time, achieving the best compromise po sible given the prior inverse importancesz. Note that, contrary to the weightsw in the linear combination, the valuesz actually reflect the user\u2019s intent for the solution found in the optimization."
    },
    {
      "heading": "2.2 Loss minimization",
      "text": "A standard objective in machine learning is minimization ofs me loss functionl : X \u00d7 \u0398 \u2192 R over a given data setX = {x1, . . . , xN} \u2282 X . Note that this notation includes both supervised and unsupervised learning, as the spaceX can include both the samples and their targets.\nUsing the multi-objective notation, the loss minimizationproblem is described as:\nmin \u03b8\u2208\u0398 (l(x1, \u03b8), . . . , l(xN , \u03b8)).\nJust like in other areas of optimization, the usual approachto solve these problems in machine learning is the use of linear combination of the objectives,as shown in Eq. (1). Common examples\nof this method are using the mean loss as the objective to be minimized and defining a regularization weight [4], where the regularization characterizes other objectives.\nHowever, as discussed in Sec. 2, this approach limits the number of solutions that can be obtained, which motivates the use of the hypervolume indicator. Sincethe objectives differ only in the samples used for the loss function and considering that all samples have equal importance1, the Nadir point z can have the same value for all objectives so that the solution found is balanced in the losses. This value is given by the parameter\u00b5, so thatzi = \u00b5, \u2200i. Then the problem becomes\nmax \u03b8\u2208\u0398 logH\u00b5(\u03b8),\nwhere\nlogH\u00b5(\u03b8) = N\u2211\ni=1\nlog(\u00b5\u2212 l(xi, \u03b8)), (3)\nwhich, as stated in Sec. 2.1, maintains the convexity information of the loss function but has a single parameter\u00b5 to be chosen along the iterative steps of the optimization.\nAs will be shown in Sec. 3, smaller values of\u00b5 place learning pressure on samples with large losses, improving the worst-case scenario, while large values of\u00b5 approximates learning to the uniform mean loss. And, of course, as learning progresses, the relative hardness of each sample may vary significantly."
    },
    {
      "heading": "3 Gradient as the operator for self-adjusting the weights",
      "text": "As exemplified in Sec. 1, the standard metric of average errorcan be problematic by providing excessive confidence to some samples while preventing less-confident samples to improve. A solution to this problem is to use different weights, as in Eq. (1), with higher weights for harder samples instead of using the same weight for all samples.\nHowever, one may not know a priori which samples are harder, which prevents the straightforward use of the weighted mean. One classical solution to this problem in classification is the use of boosting techniques [13], where a set of classifiers are learnt instead of a single one and they are merged into an ensemble. When training a new classifier, the errors of the previous ones on each sample are taken into account to increase the weights of incorrectly labeled samples, placing more pressure on the new classifier to predict these samples\u2019 labels correctly.\nThe requirement of multiple models, to form the boosting ensemble, can be hard to train with very large models and may be too slow to be used in real problems, since multiple models have to be evaluated. Therefore, a single model that is able to self-adjust the weights so that higher weights are going to be assigned to currently harder samples is desired.\nTaking the gradient of the losses hypervolume\u2019s logarithm,shown in Eq. (3), one finds\n\u2202 logH\u00b5 \u2202\u03b8\n= \u2212 N\u2211\ni=1\n1\n\u00b5\u2212 l(xi, \u03b8)\n\u2202l(xi, \u03b8)\n\u2202\u03b8 = \u2212\nN\u2211\ni=1\nwi \u2202l(xi, \u03b8)\n\u2202\u03b8 , wi =\n1\n\u00b5\u2212 l(xi, \u03b8) , (4)\nwhich is similar to a weighted mean loss cost with weightswi, i \u2208 {1, . . . , N}, replacing the minimization of the loss by the maximization of the hypervolume. However, the weightsw are determined automatically as a function of\u00b5, so that they do not have to be defined a priori and can change during the learning algorithm\u2019s execution. Moreover, higher cost implies higher weight, that is,\nl(xi, \u03b8) > l(xj , \u03b8) \u21d2 wi > wj , i, j \u2208 {1, . . . , N} (5)\nwhich is expected, as more pressure must be placed on sampleswith higher losses.\nTherefore, hypervolume maximization using gradient adjusts the parameters in the same direction as weighted mean loss minimization with appropriate weights. Furthermore, as\u00b5 approaches maxi l(xi, \u03b8), the worst case becomes the main influence in the gradient andthe problem becomes\n1This is the same motivation for using the uniform mean loss. If prior importance is available, it can be used to define the value ofz, like it would be used to definew in the weighted mean loss.\nsimilar to minimizing the maximum loss, while as\u00b5 approaches infinity, the problem becomes similar to minimizing the uniform mean loss. Thus a single parameter is able to control the problem\u2019s placement between these two extremes.\nMoreover, the hypervolume\u2019s logarithm, show in Eq. (2), is ju t a sum of the different objectives, like the linear combination shown in Eq. (1). This allows thesame optimization methods to be used for maximizing the hypervolume, which includes methods that deal with large data sets and scalability issues, such as the stochastic gradient descent with mini-batches."
    },
    {
      "heading": "4 Experiment",
      "text": "To evaluate the performance of maximizing the hypervolume,experiments were carried out using a denoising autoencoder [14], which is a neural network with ahidden layer that tries to reconstruct the input at the output, given a corrupted version of the input. The purpose is to reconstruct the digits in the MNIST data set, with 50000 training samples and 10000 validation and test samples each, as usual with this data set. Since the uniform mean loss is the usual objective for this kind of problem, it is used as a baseline, establishing a reference for comparison with the new proposed formulation, and as the indicator for comparison between the baseline andthe hypervolume method.\nIn order to be able to use the same learning rate for the hypervolume and mean loss problems, and since the mean loss already has normalized weights, the parameter\u2019s adjustment step was divided by the sum of the weightswi, i \u2208 {1, . . . , N}, computed using the hypervolume gradient in order to normalize the weights.\nThe hypervolume maximization depends on the definition of the parameter\u00b5, as discussed in Sec. 3. Since it must be larger than the objectives when the gradientis computed, the parameter\u00b5 used in the experiments is defined by:\n\u00b5(t) = max i l(xi, \u03b8 (t)) + \u01eb(t), \u01eb(t) = \u01eb(0) + \u03bat, \u01eb(0), \u03ba \u2265 0, (6)\nwheret is the epoch number,\u03b8(t) are the parameters at epocht, \u01eb(0) is the initial slack constant, and\u03ba is a user-chosen constant that can decrease the learning pressure over bad examples, as the number of epochs increase. For the stochastic gradient using mi i-batches,\u03b8(t) corresponds to the parameters at that point, but\u01eb(t) is only increased after the full epoch is completed.\nThis way of defining the Nadir point removes the requirement of ch osing an absolutely good value by allowing a value relative to the current losses to be determined, making its choice easier. The parameters were arbitrarily chosen as\u01eb(0) = 1 and\u03ba = 1 for the experiments, so that the problem becomes more similar to the mean loss problem as the learningprogresses, since the mean loss is used as the performance indicator.\nThe optimization was performed using gradient descent withlearning rate0.1 and mini-batch size of 500 samples for100 epochs, with the cross-entropy as the loss function. A totalof 50 runs with different initializations of the random number generator we e used.\nSalt-and-pepper noise was added to the training images withvarying probabilityp, but with equal probabilities of black or white corruption, and500 hidden units were used in the denoising autoencoder. Notice that the number of inputs, and consequently the number of outputs, is784, since the digits are represented by28 \u00d7 28 images. In order to evaluate the performance, the mean and maximum losses over the training, validation, and test setsw re computed without corruption of the inputs."
    },
    {
      "heading": "5 Results",
      "text": "Figure 1 shows the mean loss over the iterations for the training, validation, and test sets of the MNIST data set for different corruption probabilitiesp. The plotted values are the difference between the standard mean loss minimization, used as a baseline, and the mean loss when maximizing the hypervolume. Positive values mean that the hypervolumemaximization provides lower mean losses, which corresponds to better results. The same initial conditions and noises were used for the baseline and the proposed method, so that the difference in performance in a single run is caused by the difference of objectives and is not caused by different ra dom numbers.\nFrom Fig. 1, it is clear that the hypervolume method providesb tter results than the baseline for all corruption levels, indicating a better fit of the parameters. Moreover, the difference becomes larger as the noise level increases, indicating that the proposed method is more robust and guides to better generalization.\nIt is important to highlight that the hypervolume method achieves better performance than the baseline in a metric different from its optimization objective.This indicates that the higher weight imposed on samples with high loss improves the overall generalization, which is aligned with the conjecture proposed in Sec. 1.\nAnother effect noted in Fig. 1 is that the noise level influences the overall behavior of the difference between the methods. Forp \u2264 0.3, the hypervolume method always achieves better results faster than the baseline and most of the times forp = 0.4. As the number of iterations increase, the difference between the two methods decreases forp \u2264 0.3, and increases forp = 0.4. Notice also\nthat the difference in performance is more favorable to the hypervolume method when the noise level gets higher. If this occurred only for the training set, it would indicate an overfitting problem. But since it occurs on all sets, it means that the hypervolumemethod is able to cope better with higher noise levels, increasing the generalization.\nFigure 2 shows the maximum loss over the different parts of the data set as the learning advances. For all parts of the data set, in some cases the worst-case loss isincreased by the hypervolume method, but the median shows that improvement happens in most cases,with possibly large improvements, as shown by the upper bounds. The increase in the worst-case losin the training data set is explained by the fact that this performance is measured on the noiseless data, while the training happens on the noisy data.\nTable 1 shows the resulting mean losses for both methods on the test set. The point of evaluation was chosen as the one where the noiseless validation set presented the smallest mean loss. Clearly the hypervolume method provides better results than the commonly used mean loss for all corruption levels, and the differences are significant at less than 0.1%using the paired t-test."
    },
    {
      "heading": "6 Conclusion",
      "text": "This paper presented the problem of minimizing the loss functio over a data set from a multiobjective optimization perspective. It discussed the issue of using linear combination of loss functions as the target of minimization in machine learning, proposing the use of another metric, the hypervolume indicator, as the target to be optimized. This metric preserves optimality conditions of the losses and may achieve trade-offs between the losses moraligned to the user\u2019s expectations.\nIt is also shown that the gradient of this metric is equivalent to the gradient of a weighted mean loss, but without requiring the weights to be determined a priori.Instead, the weights are determined automatically by the gradient and possesses a boosting-like behavior, with the losses for the current set of parameters establishing the weights\u2019 values.\nExperiments with the MNIST data set and denoising autoencoders show that the new proposed objective achieves better mean loss over the training, validation, and test data. This increase in performance occurs even though the performance indicator is different from the objective being optimized, which could cause the problem to converge to an optimal solution to the problem but with low performance in the mean loss metric. This indicatesthat the hypervolume maximization is similar to the mean loss minimization, but may be able to achieve better minima due to the change in the loss landscape, as conjectured in Sec. 1.\nResults also show that the maximum loss is almost always reduced in the training, validation, and test sets, with a slight reduction in some cases and large improvements on others. Moreover, by using the validation set to find the best set of parameters, the hypervolume method achieved statistically significant smaller mean loss in the test set, indicating that i was able to generalize better.\nFuture research directions involve analyzing the effect ofincluding regularization terms, which usually are added to the linear combination of loss functions, as new objectives. The use of this method for larger problems and models and in other learning settings, such as multi-task learning, where separating tasks in different objectives is natural, should also be pursued."
    },
    {
      "heading": "Acknowledgments",
      "text": "The authors would like to thanks CNPq for the financial support."
    }
  ],
  "title": "Multi-Objective Optimization for Self-Adjusting Weighted Gradient in Machine Learning Tasks",
  "year": 2018
}
