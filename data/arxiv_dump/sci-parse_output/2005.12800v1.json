{
  "abstractText": "Principles of analogical reasoning have recently been applied in the context of machine learning, for example to develop new methods for classification and preference learning. In this paper, we argue that, while analogical reasoning is certainly useful for constructing new learning algorithms with high predictive accuracy, is is arguably not less interesting from an interpretability and explainability point of view. More specifically, we take the view that an analogy-based approach is a viable alternative to existing approaches in the realm of explainable AI and interpretable machine learning, and that analogy-based explanations of the predictions produced by a machine learning algorithm can complement similarity-based explanations in a meaningful way. To corroborate these claims, we outline the basic idea of an analogy-based explanation and illustrate its potential usefulness by means of some examples.",
  "authors": [
    {
      "affiliations": [],
      "name": "Eyke H\u00fcllermeier"
    }
  ],
  "id": "SP:5fce498dccff44d87f55228b084090e5002152b2",
  "references": [
    {
      "authors": [
        "M. Ahmadi Fahandar",
        "E. H\u00fcllermeier"
      ],
      "title": "Feature selection for analogy-based learning to rank",
      "venue": "DS 2019, 22nd International Conference on Discovery Science. pp. 279\u2013289. No. 11828 in LNAI, Springer, Split, Croatia",
      "year": 2019
    },
    {
      "authors": [
        "M. Ahmadi Fahandar",
        "E. H\u00fcllermeier"
      ],
      "title": "Learning to rank based on analogical reasoning",
      "venue": "Proceedings AAAI\u2013 2018, 32th AAAI Conference on Artificial Intelligence. pp. 2951\u20132958. New Orleans, Louisiana, USA",
      "year": 2018
    },
    {
      "authors": [
        "R. Andrews",
        "J. Diederich",
        "A. Tickle"
      ],
      "title": "Survey and critique of techniques for extracting rules from trained artificial neural networks",
      "venue": "Knowledge-Based Systems 8(6), 373\u2013389",
      "year": 1995
    },
    {
      "authors": [
        "G. Beliakov",
        "S. James"
      ],
      "title": "Citation-based journal ranks: the use of fuzzy measures",
      "venue": "Fuzzy Sets and Systems",
      "year": 2010
    },
    {
      "authors": [
        "M. Bounhas",
        "M. Pirlot",
        "H. Prade"
      ],
      "title": "Predicting preferences by means of analogical proportions",
      "venue": "Case-Based Reasoning Research and Development. pp. 515\u2013531. Springer International Publishing, Cham",
      "year": 2018
    },
    {
      "authors": [
        "M. Bounhas",
        "H. Prade",
        "G. Richard"
      ],
      "title": "Analogy-based classifiers for nominal or numerical data",
      "venue": "International Journal of Approximate Reasoning 91, 36\u201355",
      "year": 2017
    },
    {
      "authors": [
        "W. Cheng",
        "E. H\u00fcllermeier"
      ],
      "title": "Learning similarity functions from qualitative feedback",
      "venue": "Bergmann, R., Althoff, K. (eds.) Proceedings ECCBR\u20132008, 9th European Conference on Case-Based Reasoning. pp. 120\u2013134. Trier, Germany",
      "year": 2008
    },
    {
      "authors": [
        "T. Cover",
        "P. Hart"
      ],
      "title": "Nearest neighbor pattern classification",
      "venue": "IEEE Transactions on Information Theory IT-13, 21\u201327",
      "year": 1967
    },
    {
      "authors": [
        "D. Dubois",
        "H. Prade",
        "G. Richard"
      ],
      "title": "Multiple-valued extensions of analogical proportions",
      "venue": "Fuzzy Sets and Systems 292, 193\u2013202",
      "year": 2016
    },
    {
      "authors": [
        "J. F\u00fcrnkranz",
        "E. H\u00fcllermeier"
      ],
      "title": "Preference Learning",
      "venue": "Springer-Verlag",
      "year": 2011
    },
    {
      "authors": [
        "D. Gentner"
      ],
      "title": "The mechanisms of analogical reasoning",
      "venue": "Vosniadou, S., Ortony, A. (eds.) Similarity and Analogical Reasoning, pp. 197\u2013241. Cambridge University Press",
      "year": 1989
    },
    {
      "authors": [
        "R. Goodman",
        "S. Flaxman"
      ],
      "title": "European Union regulations on algorithmic decision-making and a \u201cright to explanation",
      "venue": "AI Magazine 38(3), 1\u20139",
      "year": 2017
    },
    {
      "authors": [
        "I. Guyon",
        "A. Elisseeff"
      ],
      "title": "An introduction to variable and feature selection",
      "venue": "Journal of Machine Learning Research 3, 1157\u20131182",
      "year": 2003
    },
    {
      "authors": [
        "S.M. Lundberg",
        "S.I. Lee"
      ],
      "title": "A unified approach to interpreting model predictions",
      "venue": "Proc. NeurIPS, Advances in Neural Information Processing Systems. pp. 4765\u20134774",
      "year": 2017
    },
    {
      "authors": [
        "L. Miclet",
        "H. Prade"
      ],
      "title": "Handling analogical proportions in classical logic and fuzzy logics settings",
      "venue": "Proceedings ECSQARU, 10th European Conference on Symbolic and Quantitative Approaches to Reasoning with Uncertainty. pp. 638\u2013650. Springer Berlin Heidelberg, Berlin, Heidelberg",
      "year": 2009
    },
    {
      "authors": [
        "C. Molnar"
      ],
      "title": "Interpretable Machine Learning: A Guide for Making Black Box Models Explainable (2018)",
      "year": 2018
    },
    {
      "authors": [
        "M.T. Ribeiro",
        "S. Singh",
        "C. Guestrin"
      ],
      "title": "Why should I trust you?\u201d Explaining the predictions of any classifier",
      "venue": "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. pp. 1135\u20131144",
      "year": 2016
    },
    {
      "authors": [
        "W. Samek",
        "G. Montavon",
        "A. Vedaldi",
        "L. Hansen",
        "Mller",
        "K.R. (eds."
      ],
      "title": "Explainable AI: Interpreting, Explaining and Visualizing Deep Learning",
      "venue": "Springer",
      "year": 2019
    },
    {
      "authors": [
        "A. Van Looveren",
        "J. Klaise"
      ],
      "title": "Interpretable counterfactual explanations guided by prototypes",
      "venue": "CoRR abs/1907.02584",
      "year": 1907
    }
  ],
  "sections": [
    {
      "heading": "1 Introduction",
      "text": "Over the past couple of years, the idea of explainability and related notions such as transparency and interpretability have received increasing attention in artificial intelligence (AI) in general and machine learning (ML) in particular. This is mainly due to the ever growing number of real-world applications of AI technology and the increasing level of autonomy of algorithms taking decisions on behalf of people, and hence of the social responsibility of computer scientists developing these algorithms. Meanwhile, algorithmic decision making has a strong societal impact, which has led to the quest for understanding such decisions, or even to claiming a \u201cright to explanation\u201d [12]. Explainability is closely connected to other properties characterizing a \u201cresponsible\u201d or \u201ctrusthworthy\u201d AI/ML, such as fairness, safety, robustness, responsibility, and accountability, among others.\nMachine learning models, or, more specifically, the predictors induced by a machine learning algorithm on the basis of suitable training data, are not immediately understandable most of the time. This is especially true for the most \u201cfashionable\u201d class of ML algorithms these days, namely deep neural networks. On the contrary, a neural network is a typical example of what is called a \u201cblack-box\u201d model in the literature: It takes inputs and produces associated outputs, often with high predictive accuracy, but the way in which the inputs and outputs are related to each other, and the latter are produced on the basis of the former, is very intransparent, as it involves possibly millions of mathematical operations and nonlinear transformations conducted by the network (in an\n? Draft of an article in Proc. MDAI 2020, 17th Int. Conf. on Modeling Decisions for Artificial Intelligence.\nar X\niv :2\n00 5.\n12 80\n0v 1\n[ cs\n.L G\n] 2\n3 M\nay 2\nattempt to simulate the neural activity of a human brain). A lack of transparency and interpretability is arguably less problematic for other ML methodology with a stronger \u201cwhite-box\u201d character, most notably symbol-oriented approaches such as rules and decision trees. Yet, even for such methods, interpretability is far from being guaranteed, especially because accurate models often require a certain size and complexity. For example, even if a decision tree might be interpretable in principle, a tree with hundreds of nodes will hardly be understandable by anyone.\nThe lack of transparency of contemporary ML methodology has triggered research that is aimed at improving the interpretability of ML algorithms, models, and predictions. In this regard, various approaches have been put forward, ranging from \u201cinterpretability by design\u201d, i.e., learning models with in-built interpretability, to model-agnostic explanations \u2014 a brief overview will be given in the next section. In this paper, we propose to add principles of analogical reasoning [11] as another alternative to this repertoire. Such an approach is especially motivated by so-called example-based explanations, which refer to the notion of similarity. Molnar [16] describes the blueprint of such explanations as follows: \u201cThing B is similar to thing A and A caused Y, so I predict that B will cause Y as well.\u201d In a machine learning context, the \u201cthings\u201d are data entities (instances), and the causes are predictions. In (binary) classification, for example, the above pattern might be used to explain the learner\u2019s prediction for a query instance: A belongs to the positive class, and B is similar to A, hence B is likely to be positive, too. Obviously, this type of explanation is intimately connected to the nearest neighbor estimation principle [8].\nNow, while similarity establishes a relationship between pairs of objects (i.e. tuples), an analogy involves four such objects (i.e. quadruples). The basic regularity assumption underlying analogical reasoning is as follows: Given objects A, B, C, D, if A relates to B as C relates to D, then this \u201crelatedness\u201d also applies to the properties caused by these objects (for example, the predictions produced by an ML model). Several authors have recently elaborated on the idea of using analogical reasoning for the purpose of (supervised) machine learning [6,2,5], though without raising the issue of interpretability. Here, we will argue that analogy-based explanations can complement similarity-based explanations in a meaningful way.\nThe remainder of the paper is organized as follows. In the next section, we give a brief overview of different approaches to interpretable machine learning. In Section 3, we provide some background on analogy-based learning \u2014 to this end, we recall the basics of a concrete method that was recently introduced in [2]. In Section 4, we elaborate on the idea on analogy-based explanations in machine learning, specifically focusing on classification and preference learning."
    },
    {
      "heading": "2 Interpretable Machine Learning",
      "text": "In the realm of interpretable machine learning, two broad approaches are often distinguished. The first is to learn models that are inherently interpretable, i.e., models with in-built transparency that are interpretable by design. Several classical ML methods are put into this category, most notably symbol-oriented approaches like decision trees, but also methods that induce \u201csimple\u201d (typically linear) mathematical models, such as logistic regression. The second approach is to extract interpretable information from presumably intransparent \u201cblack-box\u201d models. Within this category, two subcategories can be further distinguished.\nIn the first subcategory, global approximations of the entire black-box model are produced by training more transparent \u201cwhite-box\u201d models as a surrogate. This can be done, for example, by using the black-box model as a teacher, i.e., to produce training data for the white-box model [3]. In the second subcategory, which is specifically relevant for this paper, the idea is to extract interpretable local information, which only pertains to a restricted region in the instance space, or perhaps only to a single instance. In other words, the idea is to approximate a black-box model only locally instead of globally, which, of course, can be accomplished more easily, especially by simple models. Prominent examples of this approach are LIME [17] and SHAP [14]. These approaches are qualified as model agnostic, because they use the underlying model only as a black-box that is queried for the purpose of data generation.\nIn addition to generic, universally applicable methods of this kind, there are various methods for extracting useful information that are specifically tailored to certain model classes, most notably deep neural networks [18]. Such methods seek to provide some basic understanding of how such a network connects inputs with outputs. To this end, various techniques for making a network more transparent have been proposed, many of them based on the visualization of neural activities.\nInterestingly, the focus in interpretable machine learning has been very much on classification so far, while other ML problems have been considered much less. In particular, there is very little work on interpretable preference learning and ranking [10]. As will be argued later on, the idea of analogy-based explanation appears to be especially appealing from this point of view."
    },
    {
      "heading": "3 Analogy-Based Learning",
      "text": "In this section, we briefly recall the basic ideas of an analogy-based learning algorithm that was recently introduced in [2]. This will set the stage for our discussion of analogy-based explanation in the next section, and provide a basis for understanding the main arguments put forward there.\nThe mentioned approach proceeds from the standard setting of supervised learning, in which data objects (instances) are described in terms of feature vectors x = (x1, . . . , xd) \u2208 X \u2286 Rd. The authors are mainly interested in the problem of ranking, i.e., in learning a ranking function \u03c1 that accepts any (query) subset Q = {x1, . . . ,xn} \u2286 X of instances as input. As output, the function produces a ranking in the form of a total order of the instances, which can be represented by a permutation \u03c0, with \u03c0(i) the rank of instance xi. Yet, the algorithmic principles underlying this approach can also be used for the purpose of classification. In the following, to ease explanation, we shall nevertheless stick to the case of ranking."
    },
    {
      "heading": "3.1 Analogical Proportions",
      "text": "The approach essentially builds on the following inference pattern: If object a relates to object b as c relates to d, and knowing that a is preferred to b, we (hypothetically) infer that c is preferred to d. This principle is formalized using the concept of analogical proportion [15]. For every quadruple of objects a, b, c,d, the latter provides a numerical degree to which these objects are in analogical relation to each other. To this end, such a degree is first determined for each attribute value (feature) separately, and these degrees are then combined into an overall degree of analogy.\nMore specifically, for four values a, b, c, d from an attribute domain X, the quadruple (a, b, c, d) is said to be in analogical proportion, denoted by a : b :: c : d, if \u201ca relates to b as c relates to d\u201d, or formally:\nE ( R(a, b),R(c, d) ) , (1)\nwhere the relation E denotes the \u201cas\u201d part of the informal description. R can be instantiated in different ways, depending on the underlying domain X:\n\u2013 In the case of Boolean variables, where X = {0, 1}, there are 24 = 16 instantiations of the pattern a : b :: c : d, of which only the following 6 satisfy a set of axioms required to hold for analogical proportions:\na b c d 0 0 0 0 0 0 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1 1 1 1\nThis formalization captures the idea that a differs from b (in the sense of being \u201cequally true\u201d, \u201cmore true\u201d, or \u201cless true\u201d, if the values 0 and 1 are interpreted as truth degrees) exactly as c differs from d, and vice versa. \u2013 In the numerical case, assuming all attributes to be normalized to the unit interval [0, 1], the concept of analogical proportion can be extended on the basis of generalized logical operators [6,9]. In this case, the analogical proportion will become a matter of degree, i.e., a quadruple (a, b, c, d) can be in analogical proportion to some degree between 0 and 1. An example of such a proportion, withR being the arithmetic difference, i.e.,R(a, b) = a\u2212 b, is the following:\nv(a, b, c, d) = { 1\u2212 |(a\u2212 b)\u2212 (c\u2212 d)|, if sign(a\u2212 b) = sign(c\u2212 d) 0, otherwise.\n(2)\nNote that this formalization indeed generalizes the Boolean case (where a, b, c, d \u2208 {0, 1}). Another example is geometric proportionsR(a, b) = a/b.\nTo extend analogical proportions from individual values to complete feature vectors, the individual degrees of proportion can be combined using any suitable aggregation function, for example the arithmetic mean:\nv(a, b, c,d) = 1\nd d\u2211 i=1 v(ai, bi, ci, di) ."
    },
    {
      "heading": "3.2 Analogical Prediction",
      "text": "The basic idea of analogy-based learning is to leverage analogical proportions for the purpose of analogical transfer, that is, to transfer information about the target of prediction. In the case of preference learning, the target could be the preference relation between two objects c and d, i.e.,\nwhether c d or d c. Likewise, in the case of classification, the target could be the class label of a query object d (cf. Fig. 1 for an illustration).\nIn the context of preference learning, the authors in [2] realize analogical transfer in the style of the k-nearest neighbor approach: Given a query pair (c,d), they search for the tuples (ai, bi) in the training data producing the k highest analogies {ai : bi :: c : d}ki=1. Since the preferences between ai and bi are given as part of the training data, each of these analogies suggests either c d or d c by virtue of analogical transfer, i.e., each of them provides a vote in favor of the first or the second case. Eventually, the preference with the higher number of votes is adopted, or the distribution of votes is turned into an estimate of the probability of the two cases.\nObviously, a very similar principle could be invoked in the case of (binary) classification. Here, given a query instance d, one would search for triplets (ai, bi, ci) in the training data forming strong analogies ai : bi :: ci : d, and again invoke the principle of analogical transfer to conjecture about the class label of d. Each analogy will suggest the positive or the negative class, and the corresponding votes could then be aggregated in one way or the other."
    },
    {
      "heading": "3.3 Feature Selection",
      "text": "Obviously, the feature representation of objects will have a strong effect on whether, or to what extent, the analogy assumption applies to a specific problem, and hence influence the success of the analogical inference principle. Therefore, prior to applying analogical reasoning methods, it could make sense to find an embedding of objects in a suitable space, so that the assumption of the above inference pattern holds true in that space. This is comparable, for example, to embedding objects in Rd in such a way that the nearest neighbor rule with Euclidean distance yields good predictions in a classification task.\nIn [1], the authors address the problem of feature selection [13] in analogical inference, which can be seen as a specific type of embedding, namely a projection of the data from the original feature space to a subspace. By ignoring irrelevant or noisy features and restricting to the most relevant dimensions, feature selection can often improve the performance of learning methods. Moreover, feature selection is also important from an explainability point of view, because the representation of data objects in terms of meaningful features is a basic prerequisite for the interpretability of a\nmachine learning model operating on this representation. In this regard, feature selection is also more appropriate than general feature embedding techniques. The latter typically produce new features in the form of (nonlinear) combinations of the original features, which lose semantic meaning and are therefore difficult to interpret."
    },
    {
      "heading": "4 Analogy-based Explanation",
      "text": "To motivate an analogy-based explanation of predictions produced by an ML algorithm, let us again consider the idea of similarity-based explanation as a starting point. As we shall argue, the former can complement the latter in a meaningful way, especially because it refers to a different type of \u201cknowledge transfer\u201d. As before, we distinguish between two exemplary prediction tasks, namely classification and ranking. This distinction is arguably important, mainly for the following reason: In classification, a property (class membership) is assigned to a single object x, whereas in ranking, a property (preference) is ascribed to a pair of objects (c,d). Moreover, in the case of ranking, the property is in fact a relation, namely a binary preference relation. Thus, since analogybased inference essentially deals with \u201crelatedness\u201d, ranking and preference learning lends itself to analogy-based explanation quite naturally, perhaps even more so than classification.\nFor the purpose of illustration, we make use of a data set that classifies 172 scientific journals in the field of pure mathematics into quality categories A\u2217, A, B, C [4]. Each journal is moreover scored in terms of 5 criteria, namely\n\u2013 cites: the total number of citations per year; \u2013 IF: the well-known impact factor (average number of citations per article within two years\nafter publication); \u2013 II: the immediacy index measures how topical the articles published in a journal are (cites to\narticles in current calendar year divided by the number of articles published in that year); \u2013 articles: the total number of articles published; \u2013 half-line: cited half-life (median age of articles cited).\nIn a machine learning context, a classification task may consist of predicting the category of a journal, using the scores on the criteria as features. Likewise, a ranking task may consist of predicting preferences between journals, or predicting an entire ranking of several journals."
    },
    {
      "heading": "4.1 Explaining Class Predictions",
      "text": "Similarity-based explanations typically \u201cjustify\u201d a prediction by referring to local (nearest neighbor) information in the vicinity of a query instance x. In the simplest case, the nearest neighbor of x is retrieved from the training data, and its class label is provided as a justification (cf. Fig. 2): \u201cThere is a case x\u2032 that is similar to x, and which belongs to class y, so x is likely to belong to y as well.\u201d For example, there is another journal with similar scores on the different criteria, and which is ranked in category A, which explains why this journal is also put into this category.\nA slightly more general approach is to retrieve, not only the single nearest neighbor but the k nearest neighbors, and to provide information about the distribution of classes among this set of\nexamples. Information of that kind is obviously useful, as it conveys an idea of the confidence and reliability of a prediction. If many neighbors are all from the same class, this will of course increase the trust in a prediction. If, on the other side, the distribution of classes in the neighborhood is mixed, the prediction might be considered as uncertain, and hence the explanation as less convincing.\nSimilarity- or example-based explanations of this kind are very natural and suggest themselves if a nearest neighbor approach is used by the learner to make predictions. It should be mentioned, however, that similarity-based explanations can also be given if predictions are produced by another type of model (like the discriminative model indicated by the dashed decision boundary in Fig. 2). In this case, the nearest neighbor approach serves as a kind of surrogate model. This could be justified by the fact that most machine learning methods do indeed obey the regularity assumption underlying similarity-based inference. For example, a discriminant function is more likely to assign two similar objects to the same class than to separate them, although such cases do necessarily exist as well.\nObviously, a key prerequisite of the meaningfulness of similarity-based explanations is a meaningful notion of similarity, formalized in terms of an underlying similarity or distance function. This assumption is far from trivial, and typically not satisfied by \u201cdefault\u201d metrics like the Euclidean distance. Instead, a meaningful measure of distance needs to properly modulate the influence of individual features, because not all features might be of equal importance. Besides, such a measure should be able to capture interactions between features, which might not be considered independently of each other. For example, depending on the value of one feature, another feature might be considered more or less important (or perhaps completely ignored, as it does not apply any more). In other words, a meaningful measure of similarity may require a complex aggregation of the similarities of individual features [7]. Needless to say, this may hamper the usefulness of similarity-based explanations: If a user cannot understand why two cases are deemed similar, she will hardly accept a similar case as an explanation.\nAnother issue of the similarity-based approach, which brings us to the analogy-based alternative, is related to the difficulty of interpreting a degree of similarity or distance. Often, these degrees are\nnot normalized (especially in the case of distance), and therefore difficult to judge: How similar is similar? What minimal degree of similarity should be expected in an explanation? For example, when explaining the categorization of a journal as B by pointing to a journal with similar properties, which is also rated as B, one may wonder whether the difference between them is really so small, or not perhaps big enough to justify a categorization as A.\nAn analogy-based approach might be especially interesting in this regard, as it explicitly refers to this distance, or, more generally, the relatedness between data entities. In the above example, an analogy-based explanation given to the manager of a journal rated as B instead of A might be of the following kind: \u201cThere are three other journals, two rated A (a and c) and one rated B (b). The relationship between a and b is very much the same as the relationship between c and your journal, and b was also rated B.\u201d For example, a and b may have the same characteristics, except that b has 100 more articles published per year. The manager might now be more content and accept the decision more easily, because she understands that 100 articles more or less can make the difference.\nA concrete example for the journal data is shown in Fig. 3. Here, an analogy is found for a journal with (normalized) scores of 0.03, 0.06, 0.08, 0.04, 1 on the five criteria. To explain why this journal is only put in category C but not in B, three other journals a, b, c are found, a from category A, and b and c from category B, so that a : b :: c : d (the degree of analogical proportion is \u2248 0.98). Note that the score profiles of a and c resp. b and d are not very similar themselves, which is not surprising, because a and c are from different categories. Still, the four journals form an analogy, in the sense that an A-journal relates to a B-journal as a B-journal relates to a C-journal.\nNote that this type of explanation is somewhat related to the idea of explaining with counterfactuals [19], although the cases forming an analogy are of course factual and not counterfactual. Nevertheless, an analogy-based explanation may give an idea of what changes of features might be required to achieve a change in the classification. On the other side, an analogy can of course also explain why a certain difference is not enough. For example, explaining the rating as B to a journal d by pointing to another journal c might not convince the journal manager, because she feels that, despite the similarity, her journal is still a bit better in most of the criteria. Finding an analogy a : b :: c : d, with journals a and b also categorized as B, may then be helpful and convince her that the difference is not significant enough.\nOf course, just like the nearest neighbor approach, analogy-based explanations are not restricted to a single analogy. Instead, several analogies, perhaps sorted by their strength (degree of analogical proportion), could be extracted from the training data. In this regard, another potential advantage of an analogy-based compared to a similarity-based approach should be mentioned: While both approaches are local in the sense of giving an explanation for a specific query instance, i.e., local with respect to the explanandum, the similarity-based approach is also local with respect to the explanans, as it can only refer to cases in the vicinity of the query \u2014 which may or may not exist. The analogy-based approach, on the other side, is not restricted in this sense, as the explanans is not necessarily local. Instead, a triplet a, b, c forming an analogy with a query d can be distributed in any way, and hence offers many more possibilities for constructing an explanation (see also Fig. 4). Besides, one should note that there are much more triplets than potential nearest neighbors (the former scales cubicly with the size of the training data, the latter only linearly).\nLast but not least, let us mention that analogy-based explanations, just like similarity-based explanations, are in principle not limited to analogical learning, but could also be used in a modelagnostic way, and as a surrogate for other models."
    },
    {
      "heading": "4.2 Explaining Preference Predictions",
      "text": "In the case of classification, training data is given in the form of examples of the form (x, y), where x is an element from the instance space X and y a class label. In the case of preference learning, we assume training data in the form of pairwise preferences a b, and the task is to infer the preferential relationship for a new object pair (c,d) \u2208 X \u00d7 X given as a query (or a ranking of more than two objects, in which case the prediction of pairwise preferences could be an intermediate step). How could one explain a prediction c d? The similarity-based approach amounts to finding a \u201csimilar\u201d preference a b in the training data. It is not immediately clear, however, what similarity of preferences is actually supposed\nto mean, even if a similarity measure on X is given. A natural solution would be a conjunctive combination: a preference a b is similar to c d if both a is similar to c and b is similar to d. This requirement might be quite strong, so that finding similar preferences gets difficult.\nThe analogy-based approach can be seen as a relaxation. Instead of requiring the objects to more or less coincide, they are only supposed to stand in a similar relationship to each other, i.e.,R(a, b) \u223c R(c,d). The relationshipR can mean similarity, but does not necessarily need to do so (as shown by the definition ofR in terms of arithmetic or geometric proportions). The explanation of preferences in terms of analogy appears to be quite natural. For the purpose of illustration, consider again our example: Why did the learner predict a preference c d, i.e., that journal c is ranked higher (evaluated better than) journal d? To give an explanation, one could find a preference a b between journals in the training data, so that (a, b) is in analogical proportion to (c,d). In the case of arithmetic proportions, this means that the feature values (ratings of criteria) of a deviate from the feature values of d in much the same way as those of c deviate from those of d, and this deviation will then serve as an explanation of the preference."
    },
    {
      "heading": "5 Conclusion and Future Work",
      "text": "In this paper, we presented some preliminary ideas on leveraging the principle of analogy for the purpose of explanation in machine learning. This is essentially motivated by the recent interest in analogy-based approaches to ML problems, such as classification and preference learning, though hitherto without explicitly addressing the notion of interpretability. In particular, we tried to highlight the potential of an analogy-based approach to complement similarity-based (example-based) explanation in a reasonable way.\nNeedless to say, our discussion is just a first step, and the evidence we presented in favor of analogy-based explanations is more of an anecdotal nature. In future work, the basic ideas put forward need to be worked out in detail, and indeed, there are many open questions to be addressed. For example, which analogy in a data set is best suited for explaining a specific prediction? In the case of analogy, the answer appears to be less straightforward than in the case of similarity, where more similarity is simply better than less. Also, going beyond the retrieval of a single analogy for explanation, how should one assemble a good composition of analogies? Last but not least, it would of course also be important to evaluate the usefulness of analogy-based explanation in a more systematic way, ideally in a user study involving human domain experts."
    }
  ],
  "title": "Towards Analogy-Based Explanations in Machine Learning",
  "year": 2020
}
