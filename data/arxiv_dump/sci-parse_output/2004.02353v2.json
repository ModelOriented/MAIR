{
  "abstractText": "While machine learning techniques have been successfully applied in several fields, the black-box nature of the models presents challenges for interpreting and explaining the results. We develop a new framework called Adaptive Explainable Neural Networks (AxNN) for achieving the dual goals of good predictive performance and model interpretability. For predictive performance, we build a structured neural network made up of ensembles of generalized additive model networks and additive index models (through explainable neural networks) using a two-stage process. This can be done using either a boosting or a stacking ensemble. For interpretability, we show how to decompose the results of AxNN into main effects and higher-order interaction effects. The computations are inherited from Google\u2019s open source tool AdaNet and can be efficiently accelerated by training with distributed computing. The results are illustrated on simulated and real datasets.",
  "authors": [
    {
      "affiliations": [],
      "name": "Jie Chen"
    },
    {
      "affiliations": [],
      "name": "Joel Vaughan"
    },
    {
      "affiliations": [],
      "name": "Vijayan N. Nair"
    },
    {
      "affiliations": [],
      "name": "Agus Sudjianto"
    }
  ],
  "id": "SP:c8942b705d455943005cd38b9d1bb20ab185bfc3",
  "references": [
    {
      "authors": [
        "Abadi",
        "Mart\u0131\u0144",
        "Paul Barham",
        "Jianmin Chen",
        "Zhifeng Chen",
        "Andy Davis",
        "Jeffrey Dean",
        "Matthieu Devin"
      ],
      "title": "Tensorflow: A system for large-scale machine learning.",
      "venue": "12th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI}",
      "year": 2016
    },
    {
      "authors": [
        "Adam-Bourdarios",
        "Claire",
        "Cowan",
        "Glen",
        "Germain",
        "Cecile",
        "Guyon",
        "Isabelle",
        "Kegl",
        "Balazs",
        "Rousseau",
        "David"
      ],
      "title": "Learning to discover: the higgs boson machine learning challenge.\" URL http://higgsml",
      "venue": "lal. in2p3. fr/documentation",
      "year": 2014
    },
    {
      "authors": [
        "Bien",
        "Jacob",
        "Taylor",
        "Jonathan",
        "Tibshirani",
        "Robert"
      ],
      "title": "A lasso for hierarchical interactions.\" Annals of statistics (NIH Public Access",
      "venue": "Proceedings of the 21th ACM SIGKDD international conference on knowledge",
      "year": 2013
    },
    {
      "authors": [
        "Cortes",
        "Corinna",
        "Mehryar Mohri",
        "Umar Syed"
      ],
      "title": "Adanet: Adaptive structural learning of artificial neural networks.",
      "venue": "Proceedings of the 34th International Conference on Machine Learning-Volume",
      "year": 2014
    },
    {
      "authors": [
        "Dodge",
        "Yadolah",
        "Daniel} Commenges"
      ],
      "title": "The Oxford dictionary of statistical terms",
      "year": 2006
    },
    {
      "authors": [
        "Feurer",
        "Matthias",
        "Aaron Klein",
        "Katharina Eggensperger",
        "Jost Springenberg",
        "Manuel Blum",
        "Frank Hutter"
      ],
      "title": "Efficient and robust automated machine learning.\" Advances in neural information processing systems",
      "venue": "Progress in Artificial Intelligence (Springer)",
      "year": 2015
    },
    {
      "authors": [
        "Friedman",
        "Jerome H",
        "Werner Stuetzle"
      ],
      "title": "Projection pursuit regression.",
      "venue": "Journal of the American statistical Association",
      "year": 1981
    },
    {
      "authors": [
        "Goldstein",
        "Alex",
        "Adam Kapelner",
        "Justin Bleich",
        "Emil Pitkin"
      ],
      "title": "Peeking inside the black box: Visualizing statistical learning with plots of individual conditional expectation.",
      "venue": "Journal of Computational and Graphical Statistics (Taylor & Francis)",
      "year": 2015
    },
    {
      "authors": [
        "Hooker",
        "Giles"
      ],
      "title": "Discovering additive structure in black box functions.\" Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining",
      "venue": "Journal of Computational and Graphical Statistics (Taylor & Francis)",
      "year": 2004
    },
    {
      "authors": [
        "Hu",
        "Linwei",
        "Jie Chen",
        "Vijayan N. Nair",
        "Agus Sudjianto"
      ],
      "title": "Locally interpretable models and effects based on supervised partitioning (LIME-SUP).\" arXiv preprint",
      "year": 2018
    },
    {
      "authors": [
        "Lou",
        "Yin",
        "Rich Caruana",
        "Johannes Gehrke",
        "Giles Hooker"
      ],
      "title": "Accurate intelligible models with pairwise interactions.",
      "venue": "Workshop on Automatic Machine Learning",
      "year": 2013
    },
    {
      "authors": [
        "Purushotham",
        "Sanjay",
        "Min",
        "Martin Renqiang",
        "Kuo",
        "C-C Jay",
        "Ostroff",
        "Rachel"
      ],
      "title": "Factorized sparse learning models with interpretable high order feature",
      "venue": "Journal on Uncertainty Quantification (SIAM)",
      "year": 2014
    },
    {
      "authors": [
        "Sobol",
        "Ilya M"
      ],
      "title": "Sensitivity estimates for nonlinear mathematical models.\" Mathematical modelling and computational experiments",
      "year": 1993
    },
    {
      "authors": [
        "Tibshirani",
        "Robert"
      ],
      "title": "Regression shrinkage and selection via the lasso.\" Journal of the Royal Statistical Society: Series B (Methodological) (Wiley Online Library",
      "year": 1996
    },
    {
      "authors": [
        "Tsang",
        "Michael",
        "Hanpeng Liu",
        "Sanjay Purushotham",
        "Pavankumar Murali",
        "Yan Liu"
      ],
      "title": "2018. \"Neural interaction transparency (nit): Disentangling learned interactions for improved interpretability.",
      "venue": "Advances in Neural Information Processing Systems",
      "year": 2018
    },
    {
      "authors": [
        "Vaughan",
        "Joel",
        "Agus Sudjianto",
        "Erind Brahimi",
        "Jie Chen",
        "Vijayan N. Nair"
      ],
      "title": "Explainable Neural Networks based on Additive Index Models.",
      "year": 2018
    }
  ],
  "sections": [
    {
      "text": "While machine learning techniques have been successfully applied in several fields, the black-box\nnature of the models presents challenges for interpreting and explaining the results. We develop a new framework called Adaptive Explainable Neural Networks (AxNN) for achieving the dual goals of good predictive performance and model interpretability. For predictive performance, we build a structured neural network made up of ensembles of generalized additive model networks and additive index models (through explainable neural networks) using a two-stage process. This can be done using either a boosting or a stacking ensemble. For interpretability, we show how to decompose the results of AxNN into main effects and higher-order interaction effects. The computations are inherited from Google\u2019s open source tool AdaNet and can be efficiently accelerated by training with distributed computing. The results are illustrated on simulated and real datasets.\nKeywords: Additive index models, Boosting, Generalized additive models, Interpretable machine learning, Main effects and Interactions, Stacking\nAbbreviations used in the paper:\n AIM: Additive Index Model  AxNN: Adaptive explainable neural network  FFNN: Feedforward neural networks  GAM: Generalized additive model  GAMnet: Generalized additive model network  ML: Machine learning  NN: Neural network  RF: Random forest  XGB: XgBoost  xNN: Explainable Neural Network\n1 email: Jie.Chen@wellsfargo.com\n2 The views expressed in this paper are the personal views of the authors and do not necessarily reflect the\nviews of Wells Fargo Bank, N.A., its parent company, affiliates, and subsidiaries."
    },
    {
      "heading": "1 Introduction",
      "text": "Machine learning (ML) algorithms provide significant advantages of over traditional data analysis\nmethods for several reasons: i) availability of scalable algorithms to handle large datasets; ii) ability to automate much of the routine data analysis and modeling tasks; and iii) develop flexible models with excellent predictive power. However, the techniques are complex and the results are not easy to understand, interpret, and explain. This poses a serious problem in a number of application areas, especially in regulated industries like banking.\nThere has been extensive research to develop diagnostics that can be used to do post-hoc\ninterpretation and explanation of complex ML algorithms. Early work on diagnostic tools for studying input-output relationships include partial dependence plots (J. H. Friedman 2001) and individual conditional expectation plots by (Goldstein, et al. 2015). More recent work includes accumulated local effect plots (Apley 2016) and accumulated total derivative effect plots (Liu, et al. 2018).\nThere are also attempts to explicitly build in explainability into the architecture of ML algorithms.\nGA2M, a fast algorithm based on special tree structures, was introduced in (Lou, et al. 2013, Caruana, et al. 2015) to include pairwise interactions and thereby extend generalized additive models (GAMs). (Vaughan, et al. 2018, Yang, Zhang and Sudjianto 2019) developed a structured neural network (called explainable neural network or xNN) based on additive index models or AIMs. (Tsang, Liu, et al. 2018) proposed neural interaction transparency to disentangle shared-learning across different interactions via a special neural network structure.\nThis paper proposes a new approach aimed at simultaneously achieving good predictive\nperformance and model interpretability. The technique is based on a two-stage framework that called `adaptive explainable neural network\u2019 (AxNN). In the first stage, an ensemble with base learners of generalized additive model networks (GAMnet) is used to capture the main effects. In the second stage, an ensemble of explainable neural networks (xNN), that are incremental to the first stage, is used to adaptively fit additive index models (AIMs). The differences between the two stages can be interpreted as interaction effects, allowing for direct interpretation of the fitted model. Our flexible implementation allows model-fitting through boosting or stacking. Both of them have similar predictive performance.\nAxNN does not require extensive tuning of the neural networks. This is a major advantage as\nhyper-parameter optimization can be computationally intensive and has been the subject of considerable focus in the ML literature. Many AutoML approaches have been developed to avoid manual tuning by automatic learning and optimization. Examples include auto-sklearn (Feurer, et al. 2015), auto-pytorch (Mendoza, et al. 2016), and AdaNet (Weill, et al. 2019). Among them, AdaNet is especially relevant for our work. It is an adaptive algorithm for learning a neural architecture. Our computational engine for AxNN is built using Google\u2019s AdaNet implementation, and it inherits the benefits of efficient neural network architecture search by adaptively learning via multiple subnetwork candidates. See Section 2.2 for more discussion.\nThe remaining part of this paper is organized as follows. Section 2 gives an overview of related\nwork. Section 3 discusses our AxNN formulation, describes the model-fitting algorithms for stacking and boosting ensembles as well as the ridge decomposition to identify main and interaction effects. Sections 4 and 5 demonstrate the usefulness of the results on synthetic and real datasets. The paper concludes with remarks in Section 6."
    },
    {
      "heading": "2 Review of related work",
      "text": "2.1 Explainable neural network (xNN)\nExplainable neural network (xNN), proposed in (Vaughan, et al. 2018), is based on the additive\nmultiple index model (AIM):\n\ud835\udc53(\ud835\udc99) = \ud835\udc541(\ud835\udf371 \ud835\udc47 \ud835\udc99) + \ud835\udc542(\ud835\udf372 \ud835\udc47 \ud835\udc99) + \u2026 + \ud835\udc54\ud835\udc3e (\ud835\udf37\ud835\udc3e \ud835\udc47 \ud835\udc99) (1)\nwhere \ud835\udc54\ud835\udc58 (. ), \ud835\udc58 = 1, \u2026 , \ud835\udc3e are often referred as ridge functions, \ud835\udefd\ud835\udc58 as projection indices, \ud835\udc99 is a \ud835\udc43-dimensional covariate. The \ud835\udc3e = 1 case is called a single index model (Dudeja and Hsu 2018). There are many ways of estimating AIMs, and the earliest methodology was called projection pursuit (Friedman and Stuetzle 1981). See (Ruan and Yuan 2010) for penalized least-squares estimation and (Yang, et al. 2017) for the use of Stein\u2019s method.\nThe xNN approach in (Vaughan, et al. 2018) uses a structured neural network (NN) and provides\na direct approach for model-fitting via gradient-based training methods. Figure 1 illustrates the architecture with three structural components: (i) the projection layer (first hidden layer) \ud835\udefd\ud835\udc58 \ud835\udc47\ud835\udc65 with linear activation functions (ii) subnetwork \ud835\udc54\ud835\udc58 (\u22c5) that is a fully connected, multi-layer neural network with nonlinear activation functions (e.g., RELU), and (iii) the combination layer \ud835\udc53(\ud835\udc65) that computes a weighted sum of the output of ridge functions.\nThe xNN-based formulation in (Vaughan, et al. 2018) is computationally fast since it uses available\nefficient algorithms for NNs. It is trained using the same mini\u2013batch gradient\u2013based methods and is easy to fit on large datasets. Further, one can take advantage of advances in modern computing such as GPUs. However, it requires careful hyper-parameter tuning which is often computationally intensive.\nGeneralized additive model network (GAMnet) are special cases of xNN used to estimate the\nfollowing GAM structure using NNs:\n\ud835\udc53(\ud835\udc99) = \ud835\udc541(\ud835\udc651) + \ud835\udc542(\ud835\udc652) + \u2026 + \ud835\udc54\ud835\udc43 (\ud835\udc65\ud835\udc43). (2)\nIn this case, each ridge function in Figure 1 has only a one-dimensional input and captures just the main effect of the corresponding predictor.\n2.2 AdaNet\nAs noted earlier, the computational foundation of our AxNN approach relies on the AdaNet\nalgorithm (Cortes, Gonzalvo, et al. 2017). AdaNet aims to directly minimize the DeepBoost\ngeneralization bound (Cortes, Mohri and Syed 2014) when applied to neural networks. It does this by growing an NN as an ensemble of subnetworks that minimizes the objective\n\ud835\udc39(\ud835\udc64) = 1\n\ud835\udc41 \u2211 \u03a6(\u2211 \ud835\udc64\ud835\udc57 \ud835\udc3d \ud835\udc57=1 \u210e\ud835\udc57 (\ud835\udc99\ud835\udc8a) \ud835\udc41 \ud835\udc56=1 , \ud835\udc66\ud835\udc56 ) + \u2211 (\ud835\udf06\ud835\udc5f(\u210e\ud835\udc57 ) + \ud835\udefd) \ud835\udc3d \ud835\udc57=1 |\ud835\udc64\ud835\udc57 |, (3)\nWhere \ud835\udc41 is number of sample size, \u03a6 is the loss function, \ud835\udc3d is the number of iterations, \ud835\udc64\ud835\udc57 is the\nweight for each base learner, \u210e\ud835\udc57 is the base learner, \ud835\udc99\ud835\udc56 = (\ud835\udc65\ud835\udc56,1 , \u2026 , \ud835\udc65\ud835\udc56,\ud835\udc43) \ud835\udc47 is the set of predictors,\n\ud835\udc5f(\u210e\ud835\udc57 ) is a complexity measure based on Rademacher complexity approximation, \ud835\udf06 and \ud835\udefd are tunable hyperparameters, and the second summation in (3) is the regularization term for network complexity.\nAdaNet grows the ensemble of NNs adaptively. At each iteration, it measures the ensemble loss\nfor multiple candidates and selects the best one to move onto for the next iteration. For subsequent iterations, the structure of the previous subnetworks is frozen, and only newly added subnetworks are trained. AdaNet is capable of adding subnetworks of different depths and widths to create a diverse ensemble, and trades off performance improvement with the number of parameters.\nCurrent applications of AdaNet are based on feedforward NNs (FFFNs), convolution or recurrent\nNNs. The results from the latter two can be difficult to interpret, so we restrict attention to FFNNs. Our AxNN inherits its performance and tuning advantages from AdaNet. So, AxNN is computationally efficient and scalable, and the computations can be easily accelerated with distributed CPU, GPU, or even TPU hardware.\n3 Adaptive explainable neural networks (AxNNs)\n3.1 Formulation\nThere are many approaches in the literature for formulating and fitting a statistical model in terms\nof interpretable components. In particular, the notions of main effects and higher-order interactions have been around since at least the 1920s when two-way ANOVA was introduced. These concepts have been extended over the years to more complex models with many different definitions (see for example, Friedman & Popescu, 2008; Sorokina, et al. 2008; Dodge and Commenges 2006; and Tsang, Cheng and Liu 2017). There are also a variety of approaches to estimating these components (Sobol 1993; Tibshirani 1996; Hooker 2004; Hooker 2007; Friedman & Popescu,2008; Bien 2013; and Purushotham 2014).\nBefore going further, it is worth reiterating that it is rarely possible to reconstruct an underlying\nmodel form exactly based on data. The fitted predictive model is an approximation and the form will depend on the particular architecture used, whether it is simple parametric models, semi-parametric models such as splines, or non-parametric machine learning algorithms such as random forest, gradiaent boosting, or neural networks. Our AxNN approach is no different.\nThe formulation and underlying architecture of AxNN is described below. Let\n\u210e(\ud835\udc38(\ud835\udc4c|\ud835\udc65)) = \ud835\udc53(\ud835\udc99) = \ud835\udc53(\ud835\udc651 , \u2026 , \ud835\udc65\ud835\udc43)\ndenote the overall model that captures the effects of all \ud835\udc43 predictors, conditional on a fixed set of values the predictors. Here, \u210e(\u22c5) is a link function such as the logit for binary responses. Further, let\n\ud835\udc54\ud835\udc5d (\ud835\udc65\ud835\udc5d) be the main effect that represents the effect of \ud835\udc65\ud835\udc5d, averaged over all the other predictors, for \ud835\udc5d = 1, \u2026 , \ud835\udc43. Define the difference:\n\ud835\udc3c(\ud835\udc99) = \ud835\udc53(\ud835\udc99) \u2212 [\ud835\udc541(\ud835\udc651) + \u22ef + \ud835\udc54\ud835\udc43(\ud835\udc65\ud835\udc43)]. (4)\nAxNN consists of: a) first fitting the main effects using GAMnet; and ii) then using xNN to fit an AIM to capture the remaining structure in \ud835\udc3c(\ud835\udc99) in equation (4).\nThe paper further shows how the fitted results can be decomposed into main effects and higher-\norder interaction effects. It also develops diagnostics that can be used to visualize the input-output relationships, similar to partial dependence plots (PDPs). However, unlike the PDPs which are posthoc diagnostic tools, the main and interaction effects from AxNN are obtained directly from decomposing the ridge functions of the AxNN algorithm. One side benefit is that they do not suffer from the extrapolation concerns in the presence of correlated predictors discussed in (Liu, et al. 2018). We also provide an importance measure for ranking the significance of all the detected main and interaction effects.\nSince there is some ambiguity in the literature, we note that the term main effect is used here to\ndenote the effect associated with an individual predictor obtained by projecting the original model onto the space spanned by GAMs. To make this concrete, consider following simple model with independent predictors. Let\n\ud835\udc53(\ud835\udc99) = \ud835\udefd0 + \ud835\udefd1\ud835\udc651 + \ud835\udefd2\ud835\udc652 + \ud835\udefd12\ud835\udc651 2\ud835\udc652 2 .\nFurther, let \ud835\udc50\ud835\udc56 = \ud835\udc38(\ud835\udc4b\ud835\udc56 2), \ud835\udc56 = 1, 2, where the expectation is taken with respect to the distribution of the predictors. Then, fitting a GAM will estimate the main effects (\ud835\udefd1\ud835\udc651 + \ud835\udefd12\ud835\udc502\ud835\udc651 2), and (\ud835\udefd2\ud835\udc652 + \ud835\udefd12\ud835\udc501\ud835\udc652 2) respectively for the two predictors. These are quadratic while the quadratic terms appear only in the interaction in the model above. Further, the residual interaction term is \ud835\udc3c(\ud835\udc99) = \ud835\udefd12(\ud835\udc651 2 \u2212 \ud835\udc501)(\ud835\udc652 2 \u2212 \ud835\udc502). Note that it satisfies the usual condition that its marginal expectation with respect to the distribution of each predictor is zero. In this simple case, the original model can be reconstructed exactly by algebraic re-arrangement of the main effects and interactions.\nBut this will not be the case in most realistic situations. Consider, for example, the function\n\ud835\udc53(\ud835\udc99) = log(\ud835\udc651 + \ud835\udc652) with \ud835\udc4e < \ud835\udc65\ud835\udc56 < \ud835\udc4f, \ud835\udc56 = 1, 2. For suitable values of (\ud835\udc4e, \ud835\udc4f), the function log(\ud835\udc651 + \ud835\udc652) can be approximated very well by a GAM. In this case, residual interaction term will be small. We demonstrate this phenomenon through more complex examples in Section 4. As noted earlier, when the underlying models are complex, it is not possible to recover the true form exactly. The fitted model will depend on the architecture. Sections 4 and 5 shows how AxNN works and that it provides excellent insights into the underlying structure.\n3.2 Algorithms for fitting AxNN with Boosting\nAs described in the AdaNet formulation, our goal is to minimize the objective function\n\ud835\udc39(\ud835\udc64, \u210e) = 1\n\ud835\udc41 \u2211 \u03a6 (\u2211 \ud835\udc64\ud835\udc57 \u210e\ud835\udc57 (\ud835\udc99\ud835\udc56)\n\ud835\udc3d1 \ud835\udc57=1 + \u2211 \ud835\udc64\ud835\udc57 \u210e\ud835\udc57 (\ud835\udc99\ud835\udc56) \ud835\udc3d2 \ud835\udc57=\ud835\udc3d1 +1 , \ud835\udc66\ud835\udc56 ) + \u2211 (\ud835\udf06\ud835\udc5f(\u210e\ud835\udc57 ) + \ud835\udefd)|\ud835\udc64\ud835\udc57 | \ud835\udc3d2 \ud835\udc57=1 \ud835\udc41 \ud835\udc56=1 (5)\nwhere \ud835\udc3d1 is the number of base learners (i.e., iterations) for first GAMnet stage, and \ud835\udc3d2 is total number of base learners for both GAMnet and xNN stages. We introduce the AxNN framework using\nboosting or stacking ensembles and then describe the decomposition of the ridge functions into main effects and interactions.\n3.2.1 AxNN with Boosting\nWe first introduce AxNN using the boosting ensemble approach. As shown in Figure 2 and\ndescribed in Algorithm 1, we use GAMs in the first stage (more precisely GAMnets) as base learners to capture the main effects. Specifically, in equation (5), for the first set of ridge functions, we take\n\u210e\ud835\udc57 (\ud835\udc99\ud835\udc56 ) = \u2211 \ud835\udc54\ud835\udc57 ,\ud835\udc5d(\ud835\udc65\ud835\udc56,\ud835\udc5d)\n\ud835\udc43\n\ud835\udc5d=1\n, \ud835\udc57 = 1, \u2026 , \ud835\udc3d1 ,\nwhere \ud835\udc54\ud835\udc57 ,\ud835\udc5d(\u22c5) is the ridge function for \ud835\udc65\ud835\udc5d at \ud835\udc57th iteration, which is modeled via a fully connected, multi-layer neural network using nonlinear activation functions (e.g., RELU). For each iteration in Stage\n1, we first train the base learner \u210e\ud835\udc57 (\ud835\udc99\ud835\udc56) by fixing the ensemble \u2211 \ud835\udc64\ud835\udc57 \u210e\ud835\udc57 (\ud835\udc65\ud835\udc56) \ud835\udc58\u22121 \ud835\udc57=1 learned from previous\nboosting iterations. We then optimize the weights of from both the previous and current iterations.\nWhen the validation performance converges, we move to Stage 2 where we use xNNs as base\nlearners to capture the remaining effects in Equation (4). Specifically, for the second set of ridge functions in Equation (5), we use\n\u210e\ud835\udc57 (\ud835\udc99\ud835\udc56) = \u2211 \ud835\udc54\ud835\udc57,\ud835\udc58 (\ud835\udf37\ud835\udc57,\ud835\udc58 \ud835\udc47 \ud835\udc99\ud835\udc8a)\n\ud835\udc3e\ud835\udc57\n\ud835\udc58=1\n, \ud835\udc57 = \ud835\udc3d1 + 1, \u2026 , \ud835\udc3d2\nwhere \ud835\udf37\ud835\udc57,\ud835\udc58 = (\ud835\udefd\ud835\udc57,\ud835\udc58,1 ,\u2026 , \ud835\udefd\ud835\udc57,\ud835\udc58,\ud835\udc43) \ud835\udc47 . We learn these base learners incrementally: for each iteration,\nwe first train \u210e\ud835\udc57 (\ud835\udc99\ud835\udc56) by fixing the ensemble learned from the first stage and all previous iterations in the second stage. Then we fix all the base learners and optimize the weights. Note that we do not reoptimize the weights of GAMnet base learners from the first stage, as the estimation problem is overparameterized: GAMnets are a subset of xNNs, so their effects can be reduced and the main effects can be absorbed by xNNs.\nFollowing the approach in AdaNet, in each iteration, multiple networks with different\narchitectures can be considered as candidates, and the best one will be picked up with the goal of minimizing the objective function in Equation (5).\nBoosting requires the use of weak learners so that the bias can be reduced over the iterations.\nTherefore, the architecture of the subnetworks we use for the ridge functions in GAMnet and xNNs should be shallow and narrow (i.e., a small number of layers and number of units in the layer). Moreover, these xNNs should have a small number of ridge functions, such as a single index model (SIM) structure.\nFigure 2 is an example of the flow of an AxNN architecture. The subnetworks for the ridge\nfunctions can vary with different number of layers and width. The base learners for the second stage can be single index or multiple index models.\n------------------------------------------------------------------------------------------------------------------------ Algorithm 1: AxNN with boosting ensemble3 ------------------------------------------------------------------------------------------------------------------------------------- 1) For the first stage\nFor \ud835\udc58 = 1, . . . , \ud835\udc3d1\na. Train \u210e\ud835\udc58(\ud835\udc99) by min\u210e\ud835\udc58\n1 \ud835\udc41 \u2211 \u03a6(\u2211 \ud835\udc64\ud835\udc57\u210e\ud835\udc57(\ud835\udc99\ud835\udc8a) + \u210e\ud835\udc58(\ud835\udc99\ud835\udc8a) \ud835\udc58\u22121 \ud835\udc57=1 , \ud835\udc66\ud835\udc56) \ud835\udc41 \ud835\udc56=1 with \u2211 \ud835\udc64\ud835\udc57\u210e\ud835\udc57(\ud835\udc99\ud835\udc8a) \ud835\udc58\u22121 \ud835\udc57=1 fixed, where\n\u210e\ud835\udc58(\ud835\udc99) is GAMnet.\nb. Train \ud835\udc641 ,\u2026 \ud835\udc64\ud835\udc58 by min \ud835\udc641,\u2026,\ud835\udc64\ud835\udc58\n1 \ud835\udc41 \u2211 \u03a6(\u2211 \ud835\udc64\ud835\udc57\u210e\ud835\udc57(\ud835\udc99\ud835\udc8a) + \ud835\udc64\ud835\udc58\u210e\ud835\udc58(\ud835\udc99\ud835\udc8a) \ud835\udc58\u22121 \ud835\udc57=1 , \ud835\udc66\ud835\udc56) \ud835\udc41 \ud835\udc56=1 with \u210e1,\u2026 , \u210e\ud835\udc58 fixed.\n2) For the second stage\nAssume \ud835\udc3f = \u2211 \ud835\udc64\ud835\udc57\u210e\ud835\udc57(\ud835\udc99\ud835\udc8a) \ud835\udc3d1 \ud835\udc57=1 are obtained from the first stage, and fix it. For \ud835\udc58 = \ud835\udc3d1 + 1, . . . , \ud835\udc3d2\n3 All penalty terms are ignored in Algorithms 1 and 2 for simplicity.\na. Train \u210e\ud835\udc58(\ud835\udc99) by min \u210e\ud835\udc58\n1 \ud835\udc41 \u2211 \u03a6(\ud835\udc3f + \u2211 \ud835\udc64\ud835\udc57\u210e\ud835\udc57(\ud835\udc99\ud835\udc8a) \ud835\udc58\u22121 \ud835\udc57=\ud835\udc3d1+1 + \u210e\ud835\udc58(\ud835\udc99\ud835\udc8a),\ud835\udc66\ud835\udc56) \ud835\udc41 \ud835\udc56=1 with \u2211 \ud835\udc64\ud835\udc57\u210e\ud835\udc57(\ud835\udc99\ud835\udc8a) \ud835\udc58\u22121 \ud835\udc57=\ud835\udc3d1+1\nfixed, where \u210e\ud835\udc58(\ud835\udc99) is xNN.\nb. Train \ud835\udc64\ud835\udc3d1+1,\u2026 \ud835\udc64\ud835\udc58 by min\ud835\udc64\ud835\udc3d1+1 ,\u2026\ud835\udc64\ud835\udc58\n1 \ud835\udc41 \u2211 \u03a6(\ud835\udc3f + \u2211 \ud835\udc64\ud835\udc57\u210e\ud835\udc57(\ud835\udc99\ud835\udc8a) \ud835\udc58\u22121 \ud835\udc57=\ud835\udc3d1+1 + \ud835\udc64\ud835\udc58\u210e\ud835\udc58(\ud835\udc99\ud835\udc8a),\ud835\udc66\ud835\udc56) \ud835\udc41 \ud835\udc56=1 with\n\u210e\ud835\udc3d1+1 ,\u2026 , \u210e\ud835\udc58 fixed.\n---------------------------------------------------------------------------------------------------------------------\n3.2.2 AxNN with Stacking\nThe original AdaNet package uses an approach called `stacking\u2019 with each base learner trained\nusing the original response variable rather than \u201cresiduals\u201d as we did in Section 3.1. For each iteration, AdaNet selects the best subsets among multiple candidate subsets; the candidate subnetworks can vary with different depths and over iterations, usually with increasing complexity manner. Thus, even though AdaNet base learner is trained against the responses, the base learners are different for different iterations. The details with stacking ensemble are given in Algorithm 2.\nThe rationale here is model (weighted) averaging and stacking, similar to random forest. The base\nlearner from each iteration is unbiased but with high variance, and the variance is reduced through weighted averaging/stacking. This method requires strong base learners: deeper or wider NN architecture. In contrast, the rationale behind boosting is similar to gradient boosting machine (GBM), where start with weak learners and boost performance over the iterations by removing bias through fitting the \u201cresiduals\u201d.\nAxNN with stacking is more sensitive to the initial subnetwork architecture in the first iteration. If\nthe true model is complicated but the first base learner is too weak, the performance can be poor. The weights of the base learners from iterations also behave differently between stacking and boosting. If the base learners get stronger over iterations, their weights with stacking generally decay over the iterations. However, in our studies, the weights of the base learners with boosting are usually stable over the iterations. ------------------------------------------------------------------------------------------------------------------------ Algorithm 2: AxNN with stacking ensemble -------------------------------------------------------------------------------------------------------------------------------------- 1) For the first stage\nFor \ud835\udc58 = 1, . . . , \ud835\udc3d1\na. Train \u210e\ud835\udc58(\ud835\udc99) by min\u210e\ud835\udc58\n1 \ud835\udc41 \u2211 \u03a6(\u210e\ud835\udc58(\ud835\udc99\ud835\udc8a), \ud835\udc66\ud835\udc56) \ud835\udc41 \ud835\udc56=1 , where \u210e\ud835\udc58(\ud835\udc99) is GAMnet.\nb. Train \ud835\udc641, \u2026 \ud835\udc64\ud835\udc58 by min\ud835\udc641,\u2026,\ud835\udc64\ud835\udc58\n1 \ud835\udc41 \u2211 \u03a6(\u2211 \ud835\udc64\ud835\udc57\u210e\ud835\udc57(\ud835\udc99\ud835\udc8a) + \ud835\udc64\ud835\udc58\u210e\ud835\udc58(\ud835\udc99\ud835\udc8a) \ud835\udc58\u22121 \ud835\udc57=1 , \ud835\udc66\ud835\udc56) \ud835\udc41 \ud835\udc56=1 with \u210e1 ,\u2026 , \u210e\ud835\udc58 fixed.\n2) For the second stage\nAssume \ud835\udc3f = \u2211 \ud835\udc64\ud835\udc57\u210e\ud835\udc57(\ud835\udc99\ud835\udc8a) \ud835\udc3d1 \ud835\udc57=1 are obtained from the first stage, and fix it. For \ud835\udc58 = \ud835\udc3d1 + 1, . . . , \ud835\udc3d2\na. Train \u210e\ud835\udc58(\ud835\udc99) by min\u210e\ud835\udc58\n1 \ud835\udc41 \u2211 \u03a6(\ud835\udc3f + \u210e\ud835\udc58(\ud835\udc99\ud835\udc8a),\ud835\udc66\ud835\udc56) \ud835\udc41 \ud835\udc56=1 , where \u210e\ud835\udc58(\ud835\udc99) is xNN.\nb. Train \ud835\udc64\ud835\udc3d1+1,\u2026 \ud835\udc64\ud835\udc58 by min\ud835\udc64\ud835\udc3d1+1,\u2026\ud835\udc64\ud835\udc58\n1 \ud835\udc41 \u2211 \u03a6(\ud835\udc3f + \u2211 \ud835\udc64\ud835\udc57\u210e\ud835\udc57(\ud835\udc99\ud835\udc8a) \ud835\udc58\u22121 \ud835\udc57=\ud835\udc3d1+1 + \ud835\udc64\ud835\udc58\u210e\ud835\udc58(\ud835\udc99\ud835\udc8a), \ud835\udc66\ud835\udc56) \ud835\udc41 \ud835\udc56=1 with\n\u210e\ud835\udc3d1+1, \u2026 , \u210e\ud835\udc58 fixed.\n---------------------------------------------------------------------------------------------------------------------\n3.3 Feature interactions with xNN and AxNN\nAs noted earlier, in the first stage of AxNN, the GAMnet base-learner captures the overall\n\u2018projected\u2019 main effects, including the embedded main effects in the interaction terms. In the second stage, AxNN with xNN base learners capture the remaining effects. These effects are estimated via a sum of AIMs using xNN. The details are discussed in Section 3.4. In calculating the interactions, we remove any main effects embedded in the interaction terms. This is important as any embedded main effects may distort the magnitude of the interaction effects. Therefore, most interaction measures in the literature are based on the elimination of main effects first. For example, H-statistics in (Friedman and Popescu 2008) considers the difference between 2-D PDP and the sum of 1D-PDP as interaction strength statistic. When (Tsang, Cheng and Liu 2017) use neural network for interaction detection, they separate the main effects by univariate network, which can reduce creating spurious interactions using the main effects.\n3.4 Ridge function decomposition for interpretation\nAlthough each base learner of AxNN in the form of GAMnet or xNN is interpretable, there can be\nmultiple iterations in the ensemble, making the interpretation more difficult. When there are \ud835\udc43 predictors and \ud835\udc3e ridge functions in a base learner, and \ud835\udc3d iterations, there will be \ud835\udc3e \u00d7 \ud835\udc3d ridge functions, and \ud835\udc3e \u00d7 \ud835\udc3d \u00d7 \ud835\udc43 xNN projection coefficients in total for the second stage, which makes the interpretation more difficult.\nTo enhance the interpretability of AxNN, we propose decomposing the ridge functions by\ngrouping those with the same projection coefficient patterns. For the first stage, the ridge functions with the same covariate are grouped together to account for the main effect of the corresponding covariate. For the second stage, we apply a coefficient threshold value to the projection layer coefficients of each ridge function, and the projection coefficients bigger than the given threshold value are considered as active, Furthermore, those ridge functions with the same set of active projection coefficients are aggregated. Different sets of active projection coefficients account for different interaction patterns.\nMore specifically, for the first stage, the main effect of covariate \ud835\udc65\ud835\udc5d, denoted by \ud835\udc40(\ud835\udc65\ud835\udc56,\ud835\udc5d), can be\ncalculated by aggregating all the ridge functions w.r.t \ud835\udc65\ud835\udc5d.\n\ud835\udc40(\ud835\udc65\ud835\udc56,\ud835\udc5d) = \u2211 \ud835\udc64\ud835\udc57 \ud835\udc54\ud835\udc57,\ud835\udc5d(\ud835\udc65\ud835\udc56,\ud835\udc5d)\n\ud835\udc3d1\n\ud835\udc57=1\n, \ud835\udc57 = 1, \u2026 , \ud835\udc3d1\nTo calculate the interaction effects, let \ud835\udc46 denote the set of all the combinations of the predictors.\nFor example, with covariates{\ud835\udc651, \ud835\udc652 , \ud835\udc653}, \ud835\udc46 = {{\ud835\udc651}, {\ud835\udc652}, {\ud835\udc653}, {\ud835\udc651 ,\ud835\udc652}, {\ud835\udc652 ,\ud835\udc653}, {\ud835\udc651 , \ud835\udc653}, {\ud835\udc651 ,\ud835\udc652 , \ud835\udc653}}.\nFor any ridge function, we expect only a subset of projection coefficients to be significantly from zero.\nWe will select these using a threshold \ud835\udf03 > 0. Define \ud835\udc59(\ud835\udf37\ud835\udc57 ,\ud835\udc58 ) as the set of predictors with projection\ncoefficients whose magnitude is greater than \ud835\udf03. Specifically,\n\ud835\udc59(\ud835\udf37\ud835\udc57,\ud835\udc58 ) = {\ud835\udc65\ud835\udc5d: |\ud835\udefd\ud835\udc57,\ud835\udc58,\ud835\udc5d | > \ud835\udf03, \ud835\udc5d = 1, \u2026 , \ud835\udc43}.\nWe call \ud835\udc59(\ud835\udf37\ud835\udc57,\ud835\udc58 ) the active set of projection indices.\nFor any interaction term \ud835\udc5e \u2208 \ud835\udc46, the corresponding interaction effect is defined as\n\ud835\udc3c\ud835\udc5e (\ud835\udc99\ud835\udc56) = \u2211 \u2211 \ud835\udc64\ud835\udc57 \ud835\udc54\ud835\udc57 ,\ud835\udc58 (\ud835\udf37\ud835\udc57,\ud835\udc58 \ud835\udc47 \ud835\udc99\ud835\udc8a)\n\ud835\udc3e\ud835\udc57\n\ud835\udc58=1\n\ud835\udc3d2\n\ud835\udc57=\ud835\udc3d1 +1\n\ud835\udc70(\ud835\udc59(\ud835\udf37\ud835\udc57,\ud835\udc58 ) = \ud835\udc5e),\nwhere \ud835\udc70(\ud835\udc65) is one when \ud835\udc65 is True; and zero otherwise.\nFinally, the fitted response \ud835\udc53 can be decomposed into\n\ud835\udc53 = \u2211 \ud835\udc64\ud835\udc57 \u210e\ud835\udc57 (\ud835\udc99\ud835\udc8a )\n\ud835\udc3d1\n\ud835\udc57=1\n+ \u2211 \ud835\udc64\ud835\udc57 \u210e\ud835\udc57 (\ud835\udc99\ud835\udc56 )\n\ud835\udc3d2\n\ud835\udc57=\ud835\udc3d1 +1\n= \u2211 \ud835\udc40(\ud835\udc65\ud835\udc56,\ud835\udc5d)\n\ud835\udc43\n\ud835\udc5d=1\n+ \u2211 \ud835\udc3c\ud835\udc5e (\ud835\udc99\ud835\udc56)\n\ud835\udc5e\u2208\ud835\udc46\nA major benefit of the ridge function decomposition is that we can visualize each pattern by plotting when the dimension of projection indices is low.\nNow, the importance of main and interaction effects can be measured by their standardized\nvariances. Letting \ud835\udc63\ud835\udc4e\ud835\udc5f(\u22c5) denote the sample variance, we have\n\ud835\udc40\ud835\udc5d = \ud835\udc63\ud835\udc4e\ud835\udc5f (\ud835\udc40(\ud835\udc65\ud835\udc56,\ud835\udc5d))\n\ud835\udc63\ud835\udc4e\ud835\udc5f(\ud835\udc53)\n\ud835\udc3c\ud835\udc5e = \ud835\udc63\ud835\udc4e\ud835\udc5f (\ud835\udc3c\ud835\udc5e (\ud835\udc99\ud835\udc56))\n\ud835\udc63\ud835\udc4e\ud835\udc5f(\ud835\udc53)\nTo make the discussion more concrete, we illustrate the ridge function decomposition with a\nsimple example based on three covariates {\ud835\udc651 ,\ud835\udc652 , \ud835\udc653}. Suppose there are two iterations for the first stage and two iterations for the second stage. For the second stage, we assume there are two ridge functions for xNN. The base learners and ridge function coefficients are listed in Table 1. To make the notation simpler, we remove the subscription \ud835\udc56 for sample index.\nWith threshold \ud835\udf03 = 0.15, \ud835\udf373 ,1 and \ud835\udf373,2 have the active set of {\ud835\udc651 , \ud835\udc652}, and \ud835\udf374,1 and \ud835\udf374,2\nhave the active set of {\ud835\udc652 , \ud835\udc653}. The ridge function decomposition regroups the ridge functions from the base learners from different iterations, and is showed in Table 2. There are three main effects and two interaction terms on {\ud835\udc651 ,\ud835\udc652} and {\ud835\udc652 , \ud835\udc653}.\nsmall threshold values can result in many non-zero project coefficients, i.e., many high order interaction effects, while too large threshold values can result in main effects from the second stage, which should be fully captured by the first stage. Our simulation studies revealed that, within a certain range, the ridge decomposition is generally stable. One reason is that the inputs of neural network are typically scaled before training. From our experiments, we recommend a threshold range between 0.15 and 0.3.\nAs the ridge function decomposition is conducted after training the algorithm and computation is\nvery fast, an empirical approach for determining the threshold value is to try several different candidate threshold values and choose a reasonable value. Alternatively, one can use the histogram of the absolute projection coefficient values to guide our choice of selection values.\n3.5 Implementation\nOur implementation of AxNN is based on modifying Google\u2019s open source AutoML tool AdaNet\npackage, version 5.2 (Weill, et al. 2019). AdaNet package is a lightweight TensorFlow-based framework (Abadi, et al. 2016) for automatically learning high-quality models with minimal expert intervention. We modified the source code to accommodate our two-stage approach and both boosting and stacking ensemble approaches. With Google AdaNet implementation, we inherit the benefits of adaptively learning a neural network architecture via multiple subnetwork candidates.\n3.6 One-stage AxNN\nThus far, we have studied a two-stage AxNN using GAMnet and xNN, but xNN is able to directly\ncapture main effects as well as interactions. Therefore, an alternative is to use a one-stage approach with xNN base learners. For this algorithm, we need to just remove the first stage from Algorithm 1, and set \ud835\udc3f as 0 in the second stage. Based on our experiments, one-stage AxNN also has reasonable performance.\nAn advantage of one-stage AxNN is that it does not artificially separate the main effects by\nprojecting into each individual covariate dimension, if the true model form does not have explicit main effect terms. Moreover, one-stage AxNN usually generates more parsimonious representation of main effects and interaction terms in the ridge decomposition in Section 3.4. On the other hand, the interpretation can be more difficult as the main effects and interaction effects may not be easily separable. As mentioned in Section 3.3, the main effects that are embedded in the nonlinear ridge function may contaminate the importance measure of the interaction effects. Further, when the ridge function is linear, multiple main effects can be absorbed into a single ridge function, causing the entanglement of main effects and interaction effects. To make interpretability more stable and clear, we focus on two-stage AxNNs in this paper."
    },
    {
      "heading": "4 Experiments",
      "text": "The performance of two-stage AxNN with both boosting stacking are studied on several synthetic\nexamples.\n4.1 Simple synthetic example\nWe first use the following simple synthetic example:\n\ud835\udc66 = \ud835\udc651 + \ud835\udc652 2 + \ud835\udc653 3 + \ud835\udc52 \ud835\udc654 + \ud835\udc651\ud835\udc652 + \ud835\udc653\ud835\udc654 + \ud835\udf16,\nwhere the predictors are independent and are uniform [-1, 1], the error term is normal \ud835\udc41(0, 0.1), and the sample sizes for training, validation, and testing data sets are 50K, 25K and 25K, respectively. We use the boosting case as an illustration.\nFigure 3 shows there is a steep decrease of training and validation errors after two GAMnet weak learners. We can automatically select the architecture from those of previous iterations and other candidate networks with the same number of layers but with one additional unit. The selected NN types and architectures over the boosting iterations are listed in Table 3.\nThe mean square error (MSE) and R square(R2) score of the testing data are 0.0107 and 0.9913\nrespectively, which is very close to the ground truth values of MSE 0.01 and R2 score 0.9919. The importance of the main effects and interaction effects from the ridge function in Figure 3 is well aligned with their true importance, calculated through the variance of each additive component in the true model. We also observe that some active sets of project coefficients from the second stage (e.g.,(\ud835\udc651, \ud835\udc653 , \ud835\udc654)) make almost no contributions to the response, meaning that the corresponding ridge functions are almost flat and close to 0. In the following figures, for clarity of exposition, insignificant active sets will not be plotted if their contributions are less than 0.1%.\nUsing the discussion in Section 3.4, the response can be decomposed into main effects and\ninteraction effects in an additive manner. As the order of the interactions is low, all the main and interaction effects can be visualized directly in Figures 4 and 5. The results are consistent with the true model.\n4.2 More complex examples\nWe consider four additional examples. The first dataset was generated by a function used in (Hooker 2004), one used widely for interaction detection testing (Sorokina, et al. 2008, Lou, et al. 2013, Tsang, Cheng and Liu 2017). The second case is our own. The third and fourth cases are from (Tsang, Cheng and Liu 2017). In all these, there are ten independent predictors. We used random train/valid/test splits of 50%/25%/25% on 200K data points.\nThe models are given below:\n Example 1\n\ud835\udc53(\ud835\udc65) = \ud835\udf0b \ud835\udc651\ud835\udc652\u221a2\ud835\udc653 \u2212 sin \u22121 \ud835\udc654 + log(\ud835\udc653 + \ud835\udc655) \u2212\n\ud835\udc659\n\ud835\udc6510 \u221a\n\ud835\udc657 \ud835\udc658 \u2212 \ud835\udc652\ud835\udc657,\nwhere \ud835\udc651 ,\ud835\udc652 , \ud835\udc653 , \ud835\udc656 , \ud835\udc657 , \ud835\udc659 \u223c \ud835\udc48(0, 1), \ud835\udc654 , \ud835\udc655 , \ud835\udc658 , \ud835\udc6510 \u223c \ud835\udc48(0.6, 1).\n Example 2:\n\ud835\udc53(\ud835\udc65) = \ud835\udc651 2 + \ud835\udc652 2 + \ud835\udc653 2 + \ud835\udc653\ud835\udc654 + 2\ud835\udc654\ud835\udc655\ud835\udc656 + \ud835\udc654 3\ud835\udc657 + \ud835\udc655\ud835\udc656\ud835\udc657 + \ud835\udc657\ud835\udc658\ud835\udc659\ud835\udc6510,\nwhere \ud835\udc651 ,\u2026 , \ud835\udc6510 \u223c \ud835\udc48(\u22121, 1)\n Example 3:\n\ud835\udc53(\ud835\udc65) = \ud835\udc651\ud835\udc652 + 2 \ud835\udc653+\ud835\udc655 +\ud835\udc656 + 2\ud835\udc653 +\ud835\udc654 +\ud835\udc655+\ud835\udc657 + \ud835\udc60\ud835\udc56\ud835\udc5b(\ud835\udc657\ud835\udc60\ud835\udc56\ud835\udc5b(\ud835\udc658 + \ud835\udc659)) + \ud835\udc4e\ud835\udc5f\ud835\udc50\ud835\udc50\ud835\udc5c\ud835\udc60(0.9\ud835\udc6510),\nwhere \ud835\udc651 ,\u2026 , \ud835\udc6510 \u223c \ud835\udc48(\u22121, 1)\n Example 4:\n\ud835\udc53(\ud835\udc65) = 1\n1 + \ud835\udc651 2 + \ud835\udc652 2 + \ud835\udc653 2 + \u221aexp(\ud835\udc654 + \ud835\udc655) + |\ud835\udc656 + \ud835\udc657| + \ud835\udc658\ud835\udc659\ud835\udc6510,\nwhere \ud835\udc651 ,\u2026 , \ud835\udc6510 \u223c \ud835\udc48(\u22121, 1)\nFor both boosting and stacking ensemble, we considered only one layer for the ridge function subnetworks. The automatic selection of the NN architecture proceeds in the same manner as described in Section 4.1. AxNN boosting starts with weak GAMnet and xNN networks: xNN with 2\nsubnets and each ridge subnetwork with 3 or 5 units. The stacking AxNN starts with stronger GAMnet and xNN networks: xNN with 15 or 20 subnets and each ridge subnetwork with 10 units.\nResults of test performances and comparisons with random forest (RF), XgBoost (XGB) and fully connected feed forward NN (FFNN) are given in Table 4. RF, XgBoost, and FFNN are tuned via grid search. FFNN has two layers with a compatible layer size of AxNN. The AxNN approaches were not tuned extensively as Adanets can do efficient NN architecture search. (say something here about replications and std errors).\nAxNN stacking has the best performance over all the four examples. AxNN boosting and FFNN\ncome close. As the true response surfaces are smooth, the tree-based ensemble algorithms do not perform as wells as NN approaches, with RF having the worst performance. The performance of AxNN boosting is not as good as AxNN stacking in these cases, but it is possible they can be improved with further tuning. We do not study this issue further in this paper.\nFigure 7 shows that AxNN boosting and stacking have similarities as well as differences in their behavior over iterations. Both have similar convergence behavior in each of the two stages and exhibit a steep decrease of validation error at the beginning of the second stage. But boosting converges much slower. This is likely due to the use of weak learners and the boosting mechanism.\nTo study the behavior of AxNN ridge functions, we first decompose the interaction term of the true model form into projected main effect and the remaining interaction effects, also called pure interaction effects. As it is not straightforward to obtain the analytical form of the projected main effect of each interaction term, we use GAMnet to get it numerically. Then, we aggregate all the projected main effects and original main effect of the same variable and generate the overall projected main effects. This allows us to compare the effects from the AxNN ridge decomposition with the projected main effects and pure interaction effects from the true model form in two approaches.\nFirst, we generate the similar importance plot for the decomposed effects from the true model form, and compare to the importance from AxNN ridge function decomposition. Second, to further evaluate the relationship between AxNN main/interaction effects and the true ones, for each AxNN main or interaction effect, we calculate the correlations with all the effects from the true model, and find and list the true model effect with the maximum correlation.\nThe ridge function decomposition results for the four synthetic examples are shown in Figure 8, Figure 9, Figure 10, and Figure 11 respectively. The left two plots in each figure show the importance measures from the AxNN boosting and stacking ensemble, respectively. The effects next to the left yaxis are the ranked main or interaction effects from the ridge function decomposition. The labels next to the right y-axis list the corresponding true effects with the maximum correlations. The rightmost plot depicts the true importance. Both boosting and stacking give reasonable main effect and interaction effect estimation from the ridge function decomposition. For all the four synthetic examples, almost all the main effects from the first stage correctly capture the true projected main effects (correlation close to 1).\nThe second stage is also able to detect and capture the significant high-order interactions correctly (high correlations with the true pure interaction terms for all the four synthetic examples). The estimation of the insignificant interactions are less accurate and unstable. In the first synthetic example, the top interactions (\ud835\udc651 , \ud835\udc652 , \ud835\udc653) and (\ud835\udc657 , \ud835\udc658 , \ud835\udc659, \ud835\udc6510) are correctly detected, and the estimated pure interaction effects have strong correlations over 0.8 with the true ones. However, the weak interaction effects (\ud835\udc652 , \ud835\udc657) and (\ud835\udc653 , \ud835\udc655) are missed. Furthermore, when the interactions have a big overlap, the union of the interactions (with higher order) can be detected instead. For example, in the second synthetic example, true interaction effect (\ud835\udc654 , \ud835\udc655 , \ud835\udc656 ) and (\ud835\udc655 , \ud835\udc656 , \ud835\udc657 ) are captured by their union (\ud835\udc654 ,\ud835\udc655 , \ud835\udc656 , \ud835\udc657) in the ridge function decomposition, and the true effect (\ud835\udc654 , \ud835\udc655 , \ud835\udc656) is listed due to its importance. However, if a large project threshold is applied,(\ud835\udc654 , \ud835\udc655 , \ud835\udc656) instead of (\ud835\udc654 , \ud835\udc655 , \ud835\udc656 , \ud835\udc657) will be shown as the first.\nTo evaluate AxNN performance in the presence of noise, we add normally distributed errors with standard deviation 0.5 to all the four synthetic examples and re-test the performances. Table 5 shows that the performances are consistent with those for the non-error cases in Table 4. Ridge function decomposition result is also generally consistent with the true model, but sometimes a little weaker in the presence of noise. These results are not shown here."
    },
    {
      "heading": "5 Applications",
      "text": "of them have been previously discussed in the literature: i) bike sharing data (Fanaee-T and Gama 2014) and ii) Higgs-Boson data (Adam-Bourdarios 2014). The third one is an application to home mortgages. For all the three datasets, we used random splits into train/valid/test sets of 50%/25%/25%. The starting GAMnet and xNN network architecture is similar to the synthetic examples in Section 4.2.\n5.1 Bike sharing data\nThe bike sharing dataset contains 17k data points, and the goal is to predict the hourly count of\nrental bikes in different weather environments. We used log-counts as the response to reduce skewness. We removed some non-meaningful information as well as two response-related information, which left us with 11 predictors to model. Table 6 shows the performance on the test dataset for the bike data. RF and AxNN stacking have the best predictive performance, followed by Xgboost and AxNN boosting, and FFNN has the worst performance.\nFigure 12 depicts the top two projected main effects of \u2018hr\u2019 and \u2019temp\u2019 for bike sharing data, and\nthey are consistent with the 1-D PDP plots for random forest. However, we had to remove two variables-- \u2018mnth\u2019 and \u2018atemp\u2019 which are highly correlated with \u2018season\u2019 and \u2018temp\u2019-- from the RF training to avoid problems with extrapolation in constructing the PDPs (see (Apley 2016, Liu, et al. 2018)). This reduced RF\u2019s testing R2 value to 0.924. The results for AxNN are illustrated in Figure 13. Both AxNN approaches detect the following main effects and have similar rankings: {\u2018hr\u2018, \u2019temp\u2019, \u2018season\u2019, \u2018hum\u2019, \u2018weathersit\u2019, \u2018atemp\u2019 and \u2018weekdays\u2019}. The top interaction effects identified by both are (\u2018holiday\u2019, \u2018hr\u2019, \u2018working day\u2019) and (\u2018hr\u2019, \u2018working day\u2019). There are a few smaller ones including the five-factor interactions (\u2018hr,\u2019 \u2018weekday\u2019, \u2018workingday\u2019, \u2018temp\u2019, \u2018atemp\u2019). To compare these results, we calculated the H-statistics (both scaled and unscaled). The top pairs of H-statistics are (\u2018hr\u2019, \u2018working day\u2019), (\u2018working day\u2019, \u2018holiday\u2019) (\u2018weekday\u2019, \u2018working day\u2019) and these are consistent with the two-factor interactions identified by AxNN. But our approach is able to identify higher-order interactions easily, while it is computationally expensive to calculate high-order H-statistics.\n5.2 Mortgage data\nThe second dataset is from the business line of home lending for residential mortgage. For\nillustration purposes, we used a randomly selected subset of one million observations from one portfolio segment. There are 14 predictors and some key ones are explained in Table 7. The goal is to predict the probability of default for the loans over the next nine quarter prediction horizons based on various loan characteristics (e.g., fico, loan-to-value ratio, etc.) as well as macro-economic variables (e.g., unemployment rate).\nthe performances of AxNN boosting and stacking are very competitive and pretty close to RF and FFNN. They have the advantage of being interpretable.\nThe results of the ridge-function decomposition for the mortgage dataset are shown in Figure 15.\nJust like the previous examples, both AxNN approaches give a consistent ordering of the main effects. Moreover, both algorithms identify the top interactions (dlq_new_delq0, h), (fico0, ltv_fcast) and (fico0, sato2). The main effects of the top three variables-- ltv_forcast, FICO0 and dlq_new_delq0-from AxNN boosting are plotted in Figure 16. The increasing trend of fitted probability of default over ltv_forcast implies that the higher loan to value ratio is, the higher default risk is; while the decreasing trend on FICO0 indicates the higher default risk for the loans with lower credit scores. Moreover, being delinquent at prediction time can also a potential indicator of default in future. The pure interaction\neffect of fico0 and ltv_fcast from AxNN boosting is plotted via the contour plot in Figure 17, where the positive slopes on ltv_fcast for high FICO (e.g. at 800) indicated by the color changing from blue to red implies that the loans with high FICO are more sensitive to the change of loan to value ratio. The decreasing trend of fitted probability of default for delinquent loans (dlq_new_delq0=0) over prediction horizons (h) in Figure 17 implies that bad loans will terminate and the quality of loans will improve in the future; while the increasing trend for current loans (dlq_new_delq0=1) shows the deterioration of the quality of current loans over the time as loans will start to be delinquent. Note that the surface for the interaction effects can be volatile as the mortgage data is extremely imbalanced with 0.1% default records.\n(Left: contour plot for fico0 vs ltv_fcast; Right: fitted probability of default vs h with legend on dql_new_delq0)\n5.3 Higgs_Boson data\nThis Higgs-Boson dataset has 800k data points and 30 predictors. The goal was to classify the\nobserved events as a signal (new event of interest) or background (something produced by already known processes). Table 6 shows XgBoost the performance on the test dataset for Higgs-Boson data, but the performances of AxNN boosting and stacking are close and competitive.\nSome of the predictors in the Higgs-Boson data are highly correlated. Figure 18 shows the\nidentified main effects and interactions. Both AxNN approaches give a consistent ordering of the main effects. The detection of interaction is challenging in the presence of highly correlated predictors. Nevertheless, we detect similar top interaction effects from both: (DER_mass_vis, DER_mass_transverse_met_lep), (DER_mass_vis, DER_deltar_tau_lep, PRI_met), (DER_mass_vis, DER_deltar_tau_lep, DER_pt_ratio_lep_tau). The variables DER_mass_vis and DER_deltar_tau_lep have strong correlation, so their effects may be entangled."
    },
    {
      "heading": "6 Concluding Remarks",
      "text": "AxNN is a new machine learning framework that achieves the dual goals of predictive\nperformance and model interpretability. We have introduced and studied the properties of two-stage approaches, with GAMnet base learners to capture the main effects and xNN base learners to capture the interactions. The stacking and boosting algorithms have comparable performances. Both decompose the fitted responses into main effects and higher-order interaction effects through ridge function decomposition. AxNN borrows strength of AdaNet and does efficient NN architecture search and requires less tuning."
    },
    {
      "heading": "7 Acknowledgements",
      "text": "We are grateful to Zhishi Wang who contributed to this research while he was at Wells Fargo and to Ming Yuan for useful suggestions."
    },
    {
      "heading": "8 References",
      "text": "Abadi, Mart\u0131n\u0301, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, et\nal. 2016. \"Tensorflow: A system for large-scale machine learning.\" 12th {USENIX} Symposium\non Operating Systems Design and Implementation ({OSDI} 16). 265-283.\nAdam-Bourdarios, Claire and Cowan, Glen and Germain, Cecile and Guyon, Isabelle and Kegl, Balazs\nand Rousseau, David. 2014. \"Learning to discover: the higgs boson machine learning\nchallenge.\" URL http://higgsml. lal. in2p3. fr/documentation 9.\nApley, Daniel W. 2016. \"Visualizing the effects of predictor variables in black box supervised learning\nmodels.\" arXiv preprint arXiv:1612.08468.\nBien, Jacob and Taylor, Jonathan and Tibshirani, Robert. 2013. \"A lasso for hierarchical interactions.\"\nAnnals of statistics (NIH Public Access) 41 (3): 1111.\nCaruana, Rich, Yin Lou, Johannes Gehrke, Paul Koch, Marc Sturm, and Noemie Elhadad. 2015.\n\"Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day\nreadmission.\" Proceedings of the 21th ACM SIGKDD international conference on knowledge\ndiscovery and data mining. 1721-1730.\nCortes, Corinna, Mehryar Mohri, and Umar Syed. 2014. \"Deep boosting.\"\nCortes, Corinna, Xavier Gonzalvo, Vitaly Kuznetsov, Mehryar Mohri, and Scott Yang. 2017. \"Adanet:\nAdaptive structural learning of artificial neural networks.\" Proceedings of the 34th\nInternational Conference on Machine Learning-Volume 70. 874-883.\nDodge, Yadolah, and Daniel} Commenges. 2006. The Oxford dictionary of statistical terms. Oxford\nUniversity Press on Demand.\nDudeja, Rishabh, and Daniel Hsu. 2018. \"Learning Single-Index Models in Gaussian Space.\" Conference\nOn Learning Theory. 1887--1930.\nFanaee-T, Hadi, and Joao Gama. 2014. \"Event labeling combining ensemble detectors and background\nknowledge.\" Progress in Artificial Intelligence (Springer) 2 (2-3): 113--127.\nFeurer, Matthias, Aaron Klein, Katharina Eggensperger, Jost Springenberg, Manuel Blum, and Frank\nHutter. 2015. \"Efficient and robust automated machine learning.\" Advances in neural\ninformation processing systems. 2962-2970.\nFriedman, Jerome H. 2001. \"Greedy function approximation: a gradient boosting machine.\" Annals of\nstatistics 1189--1232.\nFriedman, Jerome H., and Werner Stuetzle. 1981. \"Projection pursuit regression.\" Journal of the\nAmerican statistical Association 76 (376): 817-823.\nFriedman, Jerome, and Bogdan E Popescu. 2008. \"Predictive learning via rule ensembles.\" The Annals\nof Applied Statistics (Institute of Mathematical Statistics) 2 (3): 916--954.\nGoldstein, Alex, Adam Kapelner, Justin Bleich, and Emil Pitkin. 2015. \"Peeking inside the black box:\nVisualizing statistical learning with plots of individual conditional expectation.\" Journal of\nComputational and Graphical Statistics (Taylor & Francis) 24: 44-65.\nHooker, Giles. 2004. \"Discovering additive structure in black box functions.\" Proceedings of the tenth\nACM SIGKDD international conference on Knowledge discovery and data mining. 575-580.\nHooker, Giles. 2007. \"Generalized functional anova diagnostics for high-dimensional functions of\ndependent variables.\" Journal of Computational and Graphical Statistics (Taylor & Francis) 16:\n709-732.\nHu, Linwei, Jie Chen, Vijayan N. Nair, and Agus Sudjianto. 2018. \"Locally interpretable models and\neffects based on supervised partitioning (LIME-SUP).\" arXiv preprint arXiv:1806.00663.\nLiu, Xiaoyu, Jie Chen, Joel Vaughan, Vijayan Nair, and Agus Sudjianto. 2018. \"Model interpretation: A\nunified derivative-based framework for nonparametric regression and supervised machine\nlearning.\" arXiv preprint arXiv:1808.07216.\nLou, Yin, Rich Caruana, Johannes Gehrke, and Giles Hooker. 2013. \"Accurate intelligible models with\npairwise interactions.\" KDD.\nMendoza, Hector, Aaron Klein, Matthias Feurer, Jost Tobias Springenberg, and Frank Hutter. 2016.\n\"Towards automatically-tuned neural networks.\" Workshop on Automatic Machine Learning.\n58-65.\nPurushotham, Sanjay and Min, Martin Renqiang and Kuo, C-C Jay and Ostroff, Rachel. 2014. \"Factorized\nsparse learning models with interpretable high order feature interactions.\" 552--561.\nRabitti, Giovanni, and Emanuele Borgonovo. 2019. \"A Shapley--Owen Index for Interaction\nQuantification.\" SIAM/ASA Journal on Uncertainty Quantification (SIAM) 7 (3): 1060--1075.\nRuan, Lingyan, and Ming Yuan. 2010. \"Dimension reduction and parameter estimation for additive\nindex models.\" Statistics and its Interface (International Press of Boston) 3: 493-499.\nSobol, Ilya M. 1993. \"Sensitivity estimates for nonlinear mathematical models.\" Mathematical\nmodelling and computational experiments 1 (4): 407--414.\nSorokina, Daria, Rich Caruana, Mirek Riedewald, and Daniel Fink. 2008. \"Detecting statistical\ninteractions with additive groves of trees.\" Proceedings of the 25th international conference\non Machine learning. 1000-1007.\nTibshirani, Robert. 1996. \"Regression shrinkage and selection via the lasso.\" Journal of the Royal\nStatistical Society: Series B (Methodological) (Wiley Online Library) 58 (1): 267--288.\nTsang, Michael, Dehua Cheng, and Yan Liu. 2017. \"Detecting statistical interactions from neural\nnetwork weights.\" arXiv preprint arXiv:1705.04977.\nTsang, Michael, Hanpeng Liu, Sanjay Purushotham, Pavankumar Murali, and Yan Liu. 2018. \"Neural\ninteraction transparency (nit): Disentangling learned interactions for improved\ninterpretability.\" Advances in Neural Information Processing Systems. 5804-5813.\nVaughan, Joel, Agus Sudjianto, Erind Brahimi, Jie Chen, and Vijayan N. Nair. 2018. \"Explainable Neural\nNetworks based on Additive Index Models.\" The RMA Journal.\nWeill, Charles, Javier Gonzalvo, Vitaly Kuznetsov, Scott Yang, Scott Yak, Hanna Mazzawi, Eugen Hotaj,\net al. 2019. \"AdaNet: A Scalable and Flexible Framework for Automatically Learning\nEnsembles.\" arXiv preprint arXiv:1905.00080.\nYang, Zebin, Aijun Zhang, and Agus Sudjianto. 2019. \"Enhancing explainability of neural networks\nthrough architecture constraints.\" arXiv preprint arXiv:1901.03838.\nYang, Zhuoran, Krishna Balasubramanian, Zhaoran Wang, and Han Liu. 2017. \"Learning non-gaussian\nmulti-index model via second-order stein\u2019s method.\" Advances in Neural Information\nProcessing Systems 30: 6097--6106."
    }
  ],
  "title": "Adaptive Explainable Neural Networks (AxNNs)",
  "year": 2020
}
