{
  "abstractText": "Text data are increasingly handled in an automated fashion by machine learning algorithms. But the models handling these data are not always well-understood due to their complexity and are more and more often referred to as \u201cblack-boxes.\u201d Interpretability methods aim to explain how these models operate. Among them, LIME has become one of the most popular in recent years. However, it comes without theoretical guarantees: even for simple models, we are not sure that LIME behaves accurately. In this paper, we provide a first theoretical analysis of LIME for text data. As a consequence of our theoretical findings, we show that LIME indeed provides meaningful explanations for simple models, namely decision trees and linear models.",
  "authors": [
    {
      "affiliations": [],
      "name": "Dina Mardaoui"
    },
    {
      "affiliations": [],
      "name": "Damien Garreau"
    }
  ],
  "id": "SP:b055d5dc948cc4f7061e7ffd5ecd17637e834cf5",
  "references": [
    {
      "authors": [
        "A. Adadi",
        "M. Berrada"
      ],
      "title": "Peeking inside the blackbox: A survey on explainable artificial intelligence (XAI)",
      "venue": "IEEE Access,",
      "year": 2018
    },
    {
      "authors": [
        "T.B. Brown",
        "B. Mann",
        "N. Ryder",
        "M. Subbiah",
        "J. Kaplan",
        "P. Dhariwal",
        "A. Neelakantan",
        "P. Shyam",
        "G. Sastry",
        "A. Askell"
      ],
      "title": "Language models are few-shot learners",
      "year": 2005
    },
    {
      "authors": [
        "M. Danilevsky",
        "K. Qian",
        "R. Aharonov",
        "Y. Katsis",
        "B. Kawas",
        "P. Sen"
      ],
      "title": "A survey of the state of Explainable AI for Natural Language Processing",
      "year": 2010
    },
    {
      "authors": [
        "J. Devlin",
        "M.-W. Chang",
        "K. Lee",
        "K. Toutanova. Bert"
      ],
      "title": "Pre-training of deep bidirectional transformers for language understanding",
      "venue": "arXiv preprint arXiv:1810.04805,",
      "year": 2018
    },
    {
      "authors": [
        "S. Filipovski"
      ],
      "title": "Improved Cauchy-Schwarz inequality and its applications",
      "venue": "Turkish journal of inequalities,",
      "year": 2019
    },
    {
      "authors": [
        "D. Garreau",
        "U. von Luxburg"
      ],
      "title": "Explaining the explainer: A first theoretical analysis of LIME",
      "venue": "In Proceedings of the 33rd International Conference on Artificial Intelligence and Statistics (AISTATS),",
      "year": 2020
    },
    {
      "authors": [
        "D. Garreau",
        "U. von Luxburg"
      ],
      "title": "Looking Deeper into Tabular LIME",
      "venue": "arXiv preprint arXiv:2008.11092,",
      "year": 2020
    },
    {
      "authors": [
        "R. Guidotti",
        "A. Monreale",
        "S. Ruggieri",
        "F. Turini",
        "F. Giannotti",
        "D. Pedreschi"
      ],
      "title": "A survey of methods for explaining black box models",
      "venue": "ACM computing surveys (CSUR),",
      "year": 2018
    },
    {
      "authors": [
        "A.E. Hoerl",
        "R.W. Kennard"
      ],
      "title": "Ridge regression: Biased estimation for nonorthogonal problems",
      "year": 1970
    },
    {
      "authors": [
        "K.S. Jones"
      ],
      "title": "A statistical interpretation of term specificity and its application in retrieval",
      "venue": "Journal of documentation,",
      "year": 1972
    },
    {
      "authors": [
        "H.P. Luhn"
      ],
      "title": "A statistical approach to mechanized encoding and searching of literary information",
      "venue": "IBM Journal of research and development,",
      "year": 1957
    },
    {
      "authors": [
        "S.M. Lundberg",
        "S.-I. Lee"
      ],
      "title": "A unified approach to interpreting model predictions",
      "venue": "In Advances in Neural Information Processing Systems,",
      "year": 2017
    },
    {
      "authors": [
        "D.M.W. Powers"
      ],
      "title": "Applications and explanations of Zipf\u2019s law. In New methods in language processing and computational natural language",
      "year": 1998
    },
    {
      "authors": [
        "A. Radford",
        "J. Wu",
        "R. Child",
        "D. Luan",
        "D. Amodei",
        "I. Sutskever"
      ],
      "title": "Language models are unsupervised multitask learners",
      "venue": "OpenAI blog,",
      "year": 2019
    },
    {
      "authors": [
        "M.T. Ribeiro",
        "S. Singh",
        "C. Guestrin"
      ],
      "title": "Why should I trust you?\u201d Explaining the predictions of any classifier",
      "venue": "In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining,",
      "year": 2016
    },
    {
      "authors": [
        "P. Ross"
      ],
      "title": "Generalized hockey stick identities and ndimensional blockwalking",
      "venue": "The College Mathematics Journal,",
      "year": 1997
    },
    {
      "authors": [
        "P. Spyns"
      ],
      "title": "Natural language processing in medicine: an overview",
      "venue": "Methods of information in medicine,",
      "year": 1996
    },
    {
      "authors": [
        "A. Vaswani",
        "N. Shazeer",
        "N. Parmar",
        "J. Uszkoreit",
        "L. Jones",
        "A.N. Gomez",
        "L. Kaiser",
        "I. Polosukhin"
      ],
      "title": "Attention is all you need",
      "venue": "In Advances in neural information processing systems,",
      "year": 2017
    }
  ],
  "sections": [
    {
      "text": "Text data are increasingly handled in an automated fashion by machine learning algorithms. But the models handling these data are not always well-understood due to their complexity and are more and more often referred to as \u201cblack-boxes.\u201d Interpretability methods aim to explain how these models operate. Among them, LIME has become one of the most popular in recent years. However, it comes without theoretical guarantees: even for simple models, we are not sure that LIME behaves accurately. In this paper, we provide a first theoretical analysis of LIME for text data. As a consequence of our theoretical findings, we show that LIME indeed provides meaningful explanations for simple models, namely decision trees and linear models."
    },
    {
      "heading": "1 Introduction",
      "text": "Natural language processing has progressed at an accelerated pace in the last decade. This time period saw the second coming of artificial neural networks, embodied by the apparition of recurrent neural networks (RNNs) and more particularly long short-term memory networks (LSTMs). These new architectures, in conjunction with large, publicly available datasets and efficient optimization techniques, have allowed computers to compete with and sometime even beat humans on specific tasks.\nMore recently, the paradigm has shifted from recurrent neural networks to transformers networks (Vaswani et al., 2017). Instead of training models specifically for a task, large language models are trained on supersized datasets. For instance, Webtext2 contains the text data associated to 45 millions links (Radford et al., 2019). The growth in complexity of these models seems to know no limit, especially with regards\nExplaining a prediction with LIME Update! Went back last night for dinner, this place is still awesome. I had the Las Vegas Rolls, they were pure deep fried goodness. 0.05 0.00 0.05 for place is last were awesome\nFigure 1: Explaining the prediction of a random forest classifier on a Yelp review. Left panel: the document to explain. The words deemed important for the prediction are highlighted, in orange (positive influence) and blue (negative influence). Right panel: values of the largest 6 interpretable coefficients, ranked by absolute value.\nto their number of parameters. For instance, BERT (Devlin et al., 2018) has roughly 340 millions of parameters, a meager number compared to more recent models such as GTP-2 (Radford et al., 2019, 1.5 billions) and GPT-3 (Brown et al., 2020, 175 billions).\nFaced with such giants, it is becoming more and more challenging to understand how particular predictions are made. Yet, interpretability of these algorithms is an urgent need. This is especially true in some applications such as healthcare, where natural language processing is used for instance to obtain summaries of patients records (Spyns, 1996). In such cases, we do not want to deploy in the wild an algorithm making near perfect predictions on the test set but for the wrong reasons: the consequences could be tragic.\nIn this context, a flourishing literature proposing interpretability methods emerged. We refer to the survey papers of Guidotti et al. (2018) and Adadi and Berrada (2018) for an overview, and to Danilevsky et al. (2020) for a focus on natural language processing. With the notable exception of SHAP (Lundberg and Lee, 2017), these methods do not come with any guarantees. Namely, given a simple model already interpretable to some extent, we cannot be sure that these methods provide meaningful explanations. For\nar X\niv :2\n01 0.\n12 48\n7v 1\n[ st\nat .M\nL ]\n2 3\nO ct\n2 02\ninstance, explaining a model that is based on the presence of a given word should return an explanation that gives high weight to this word. Without such guarantees, using these methods on the tremendously more complex models aforementioned seems like a risky bet.\nIn this paper, we focus on one of the most popular interpretability method: Local Interpretable Modelagnostic Explanations Ribeiro et al. (2016, LIME), and more precisely its implementation for text data. LIME\u2019s process to explain the prediction of a model f for an example \u03be can be summarized as follows:\n(i). from a corpus of documents C, create a TF-IDF transformer \u03c6 embedding documents into RD;\n(ii). create n perturbed documents x1, . . . , xn by deleting words at random in \u03be;\n(iii). for each new example, get the prediction of the model yi := f(\u03c6(xi));\n(iv). train a (weighted) linear surrogate model with inputs the absence / presence of words and responses the yis.\nThe user is then given the coefficients of the surrogate model (or rather a subset of the coefficients, corresponding to the largest ones) as depicted in Figure 1. We call these coefficients the interpretable coefficients.\nThe model-agnostic approach of LIME has contributed greatly to its popularity: one does not need to know the precise architecture of f in order to get explanations, it is sufficient to be able to query f a large number of times. The explanations provided by the user are also very intuitive, making it easy to check that a model is behaving in the appropriate way (or not!) on a particular example.\nContributions. In this paper, we present the first theoretical analysis of LIME for text data. In detail,\n\u2022 we show that, when the number of perturbed samples is large, the interpretable coefficients concentrate with high probability around a fixed vector \u03b2 that depends only on the model, the example to explain, and hyperparameters of the method;\n\u2022 we provide an explicit expression of \u03b2, from which we gain interesting insights on LIME. In particular, the explanations provided are linear in f ;\n\u2022 for simple decision trees, we go further into the computations. We show that LIME provably provides meaningful explanations, giving large coefficients to words that are pivotal for the prediction;\n\u2022 for linear models, we come to the same conclusion by showing that the interpretable coefficient associate to a given word is approximately equal to the product of the coefficient in the linear model and the TF-IDF transform of the word in the example.\nWe want to emphasize that all our results apply to the default implementation of LIME for text data1 (as of October 12, 2020), with the only caveat that we do not consider any feature selection procedure in our analysis. All our theoretical claims are supported by numerical experiments, the code thereof can be found at https://github.com/dmardaoui/lime_ text_theory.\nRelated work. The closest related work to the present paper is Garreau and von Luxburg (2020a), in which the authors provided a theoretical analysis of a variant of LIME in the case of tabular data (that is, unstructured data belonging to RN ) when f is linear. This line of work was later extended by the same authors (Garreau and von Luxburg, 2020b), this time in a setting very close to the default implementation and for other classes of models (in particular partitionbased classifiers such as CART trees and kernel regressors built on the Gaussian kernel). While uncovering a number of good properties of LIME, these analyses also exposed some weaknesses of LIME, notably cancellation of interpretable features for some choices of hyperparameters.\nThe present work is quite similar in spirit, however we are concerned with text data. The LIME algorithm operates quite differently in this case. In particular, the input data goes first through a TF-IDF transform (a non-linear transformation) and there is no discretization step since interpretable features are readily available (the words of the document). Therefore both the analysis and our conclusions are quite different, as it will become clear in the rest of the paper."
    },
    {
      "heading": "2 LIME for text data",
      "text": "In this section, we lay out the general operation of LIME for text data and introduce our notation in the process. From now on, we consider a model f and look at its prediction for a fixed example \u03be belonging to a corpus C of size N , which is built on a dictionary D of size D. We let \u2016\u00b7\u2016 denote the Euclidean norm, and SD\u22121 the unit sphere of RD.\nBefore getting started, let us note that LIME is usually used in the classification setting: f takes values in {0, 1} (say), and f(\u03c6(\u03be)) represents the class at-\n1https://github.com/marcotcr/lime\ntributed to \u03be by f . However, behind the scenes, LIME requires f to be a real-valued function. In the case of classification, this function is the probability of belonging to a certain class according to the model. In other words, the regression version of LIME is used, and this is the setting that we consider in this paper. We now detail each step of the algorithm."
    },
    {
      "heading": "2.1 TF-IDF transform",
      "text": "LIME works with a vector representation of the documents. The TF-IDF transform (Luhn, 1957; Jones, 1972) is a popular way to obtain such a representation. The idea underlying the TF-IDF is quite simple: to any document, associate a vector of size D. If we set w1, . . . , wD to be our dictionary, the jth component of this vector represents the importance of word wj . It is given by the product of two terms: the term frequency (TF, how frequent the word is in the document), and the inverse term frequency (IDF, how rare the word is in our corpus). Intuitively, the TF-IDF of a document has a high value for a given word if this word is frequent in the document and, at the same time, not so frequent in the corpus. In this way, common words such as \u201cthe\u201d do not receive high weight.\nFormally, let us fix \u03b4 \u2208 C. For each word wj \u2208 D, we set mj the number of times wj appears in \u03b4. We also set vj := log\nN+1 Nj+1 + 1, where Nj is the number\nof documents in C containing wj . When presented with C, we can pre-compute all the vjs and at run time we only need to count the number of occurrences of wj in \u03b4. We can now define the normalized TF-IDF:\nDefinition 1 (Normalized TF-IDF). We define the normalized TF-IDF of \u03b4 as the vector \u03c6(\u03b4) \u2208 RD defined coordinate-wise by\n\u22001 \u2264 j \u2264 D, \u03c6(\u03b4)j := mjvj\u221a\u2211D j=1m 2 jv 2 j . (1)\nNote that there are many different ways to define the TF and IDF terms, as well as normalization choices. We restrict ourselves to the version used in the default implementation of LIME, with the understanding that different implementation choices would not change drastically our analysis. For instance, normalizing by the `1 norm instead of the `2 norm would lead to slightly different computations in Proposition 4.\nFinally, note that this transformation step does not take place for tabular data, since the data already belong to RD in this case."
    },
    {
      "heading": "2.2 Sampling",
      "text": "Let us now fix a given document \u03be and describe the sampling procedure of LIME. Essentially, the idea is\nto sample new documents similar to \u03be in order to see how f varies in a neighborhood of \u03be.\nMore precisely, let us denote by d the number of distinct words in \u03be and set D` := {w1, . . . , wd} the local dictionary. For each new sample, LIME first draws uniformly at random in {1, . . . , d} a number si of words to remove from \u03be. Subsequently, a subset Si \u2286 {1, . . . , d} of size si is drawn uniformly at random: all the words with indices contained in Si are removed from \u03be. Note that the multiplicity of removals is independent from si: if the word \u201cgood\u201d appears 10 times in \u03be and its index belongs to S, then all the instances of \u201cgood\u201d are removed from \u03be (see Figure 2). This process is repeated n times, yielding n new samples x1, . . . , xn. With these new documents come n new binary vectors z1, . . . , zn \u2208 {0, 1}d, marking the absence or presence of a word in xi. Namely, zi,j = 1 if wj belongs to xi and 0 otherwise. We call the zis the interpretable features. Note that we will write 1 := (1, . . . , 1)> for the binary feature associated to \u03be: all the words are present.\nAlready we see a difficulty appearing in our analysis: when removing words from \u03be at random, \u03c6(\u03be) is modified in a non-trivial manner. In particular, the denominator of Eq. (1) can change drastically if many words are removed.\nIn the case of tabular data, the interpretable features are obtained in a completely different fashion, by discretizing the dataset."
    },
    {
      "heading": "2.3 Weights",
      "text": "Let us start by defining the cosinus distance:\nDefinition 2 (Cosinus distance). For any u, v \u2208 RD, we define\ndcos(u, v) := 1\u2212 u \u00b7 v\n\u2016u\u2016 \u00b7 \u2016v\u2016 . (2)\nIntuitively, the cosinus distance between u and v is small if the angle between u and v is small. Each new\nsample xi receives a positive weight \u03c0i, defined by\n\u03c0i := exp\n( \u2212dcos(1, zi)2\n2\u03bd2\n) , (3)\nwhere \u03bd is a positive bandwidth parameter. The intuition behind these weights is that xi can be far away from \u03be if many words are removed (in the most extreme case, s = d, all the words from \u03be are removed). In that case, zi has mostly 0 components, and is far away from 1.\nNote that the cosinus distance in Eq. (3) is actually multiplied by 100 in the current implementation of LIME. Thus there is the following correspondence between our notation and the code convention: \u03bdLIME = 100\u03bd. For instance, the default choice of bandwidth, \u03bdLIME = 25, corresponds to \u03bd = 0.25.\nWe now make the following important remark: the weights only depends on the number of deletions. Indeed, conditionally to Si having exactly s elements, we have zi \u00b71 = d\u2212s and \u2016zi\u2016 = \u221a d\u2212 s. Since\n\u20161\u2016 = \u221a d, using Eq. (3), we deduce that \u03c0i = \u03c8(s/d), where we defined the mapping\n\u03c8 : [0, 1] \u2212\u2192 R (4) t 7\u2212\u2192 exp ( \u2212(1\u2212 \u221a 1\u2212 t)2\n2\u03bd2\n) .\nWe can see in Figure 3 how the weights are given to observations: when s is small, then \u03c8(s/d) \u2248 1 and when s \u2248 d, \u03c8(s/d) which is a small quantity depending on \u03bd. Note that the complicated dependency of the weights in s brings additional difficulty in our analysis, and that we will sometimes restrict ourselves to the large bandwidth regime (that is, \u03bd \u2192 +\u221e). In that case, \u03c0i \u2248 1 for any 1 \u2264 i \u2264 n.\nEuclidean distance between the interpretable features is used instead of the cosine distance in the tabular data version of the algorithm."
    },
    {
      "heading": "2.4 Surrogate model",
      "text": "The next step is to train a surrogate model on the interpretable features z1, . . . , zn, trying to approximate the responses yi := f(\u03c6(xi)). In the default implementation of LIME, this model is linear and is obtained by weighted ridge regression (Hoerl and Kennard, 1970). Formally, LIME outputs\n\u03b2\u0302\u03bbn \u2208 arg min \u03b2\u2208Rd+1 { n\u2211 i=1 \u03c0i(yi \u2212 \u03b2>zi)2 + \u03bb \u2016\u03b2\u20162 } , (5)\nwhere \u03bb > 0 is a regularization parameter. We call the components of \u03b2\u0302\u03bbn the interpretable coefficients, the 0th coordinate in our notation is by convention the intercept. Note that some feature selection mechanism\nis often used in practice, limiting the number of interpretable features in output from LIME. We do not consider such mechanism in our analysis.\nWe now make a fundamental observation. In its default implementation, LIME uses the default setting of sklearn for the regularization parameter, that is, \u03bb = 1. Hence the first term in Eq. (5) is roughly of order n and the second term of order d. Since we experiment in the large n regime (n = 5000 is default) and with documents that have a few dozen distinct words, n d. To put it plainly, we can consider that \u03bb = 0 in our analysis and still recover meaningful results. We will denote by \u03b2\u0302n the solution of Eq. (5) with \u03bb = 0, that is, ordinary least-squares.\nWe conclude this presentation of LIME by noting that the main free parameter of the method is the bandwidth \u03bd. As far as we know, there is no principled way of choosing \u03bd. The default choice, \u03bd = 0.25, does not seem satisfactory in many respects. In particular, other choices of bandwidth can lead to different values for interpretable coefficients. In the most extreme cases, they can even change sign, see Figure 4. This phenomenon was also noted for tabular data in Garreau and von Luxburg (2020b)."
    },
    {
      "heading": "3 Main result",
      "text": "Without further ado, let us present our main result. For clarity\u2019s sake, we split it in two parts: Section 3.1 contains the concentration of \u03b2\u0302n around \u03b2\nf whereas Section 3.2 presents the exact expression of \u03b2f ."
    },
    {
      "heading": "3.1 Concentration of \u03b2\u0302n",
      "text": "When the number of new samples n is large, we expect LIME to stabilize and the explanations not to vary too much. The next result supports this intuition.\n0.0 0.1 0.2 0.3 0.4 0.5 \u03bd\n\u22120.03 \u22120.02 \u22120.01\n0.00 0.01 0.02 0.03 \u0302\u03b23\nCancellation of interpretable coefficients\n\u0302\u03b23 \u03bd= 0.25\nFigure 4: In this experiment, we plot the interpretable coefficient associated to the word \u201ccame\u201d as a function of the bandwidth parameter. The red vertical line marks the default bandwidth choice (\u03bd = 25). We can see that LIME gives a negative influence for \u03bd \u2248 0.1 and a positive one for \u03bd > 0.2.\nTheorem 1 (Concentration of \u03b2\u0302n). Suppose that the model f is bounded by a positive constant M on SD\u22121. Recall that we let d denote the number of distinct words of \u03be, the example to explain. Let 0 < < M and \u03b7 \u2208 (0, 1). Then, there exist a vector \u03b2f \u2208 Rd such that, for every\nn & max { M2d9e 10 \u03bd2 ,Md5e 5 \u03bd2 } log 8d\u03b7 2 ,\nwe have P ( \u2016\u03b2\u0302n \u2212 \u03b2f\u2016 \u2265 ) \u2264 \u03b7.\nWe refer to the supplementary material for a complete statement (we omitted numerical constants here for clarity) and a detailed proof. In essence, Theorem 1 tells us that we can focus on \u03b2f in order to understand how LIME operates, provided that n is large enough. The main limitation of Theorem 1 is the dependency of n in d and \u03bd. The control that we achieve on \u2016\u03b2\u0302n\u2212\u03b2\u2016 becomes quite poor for large d or small \u03bd: we would then need n to be unreasonably large in order to witness concentration.\nWe notice that Theorem 1 is very similar in its form to Theorem 1 in Garreau and von Luxburg (2020b) except that (i) the dimension is replaced by the number of distinct words in the document to explain, and (ii) there is no discretization parameter in our case. The differences with the analysis in the tabular data framework will be more visible in the next section."
    },
    {
      "heading": "3.2 Expression of \u03b2f",
      "text": "Our next result shows that we can derive an explicit expression for \u03b2f . Before stating our result, we need to introduce more notation. From now on, we set x a random variable such that x1, . . . , xn are i.i.d. copies\nof x. Similarly, \u03c0 corresponds to the draw of the \u03c0is and z to that of the zis. Definition 3 (\u03b1 coefficients). Define \u03b10 := E [\u03c0] and, for any 1 \u2264 p \u2264 d,\n\u03b1p := E [\u03c0 \u00b7 z1 \u00b7 \u00b7 \u00b7 zp] . (6)\nIntuitively, when \u03bd is large, \u03b1p corresponds to the probability that p distinct words are present in x. The sampling process of LIME is such that \u03b1p does not depend on the exact set of indices considered. In fact, \u03b1p only depends on d and \u03bd. We show in the supplementary material that it is possible to compute the \u03b1 coefficients in closed-form as a function of d and \u03bd:\nProposition 1 (Computation of the \u03b1 coefficients). Let 0 \u2264 p \u2264 d. For any d \u2265 1 and \u03bd > 0, it holds that\n\u03b1p = 1\nd d\u2211 s=1 p\u22121\u220f k=0 d\u2212 s\u2212 k d\u2212 k \u03c8 ( s d ) .\nFrom these coefficients, we form the normalization constant\ncd := (d\u2212 1)\u03b10\u03b12 \u2212 d\u03b121 + \u03b10\u03b11 . (7)\nWe will also need the following.\nDefinition 4 (\u03c3 coefficients). For any d \u2265 1 and \u03bd > 0, define\uf8f1\uf8f4\uf8f2\uf8f4\uf8f3 \u03c31 := \u2212\u03b11 , \u03c32 := (d\u22122)\u03b10\u03b12\u2212(d\u22121)\u03b121+\u03b10\u03b11 \u03b11\u2212\u03b12 ,\n\u03c33 := \u03b121\u2212\u03b10\u03b12 \u03b11\u2212\u03b12 .\n(8)\nWith these notation in hand, we have:\nProposition 2 (Expression of \u03b2f). Under the assumptions of Theorem 1, we have cd > 0 and, for any 1 \u2264 j \u2264 d,\n\u03b2fj = c \u22121 d { \u03c31E [\u03c0f(\u03c6(x))] + \u03c32E [\u03c0zjf(\u03c6(x))] (9)\n+ \u03c33 d\u2211 k=1 k 6=j E [\u03c0zkf(\u03c6(x))] } .\nWe also have an expression for the intercept which can be found in the supplementary material, as well as the proof of Proposition 2. At first glance, Eq. (9) is quite similar to Eq. (6) in Garreau and von Luxburg (2020b), which gives the expression of \u03b2fj in the tabular data case. The main difference is the TF-IDF transform in the expectation, personified by \u03c6, and the additional terms (there is no \u03c33 factor in the tabular data\ncase). In addition, the expression of the \u03c3 coefficients is much more complicated than in the tabular data case. We now present some immediate consequences of Proposition 2.\nLinearity of explanations. Perhaps the most striking feature of Eq. (9) is that it is linear in f . More precisely, the mapping f 7\u2192 \u03b2f is linear in f : for any given two functions f and g, we have\n\u03b2f+g = \u03b2f + \u03b2g .\nTherefore, because of Theorem 1, the explanations \u03b2\u0302n obtained for a finite sample of new examples are also approximately linear in the model to explain. We illustrate this phenomenon in Figure 5. This is remarkable: many models used in machine learning can be written as a linear combination of smaller models (e.g., generalized linear models, kernel regressors, decision trees and random forests). In order to understand the explanations provided by these complicated models, one can try and understand the explanations for the elementary elements of the models first.\nLarge bandwidth. It can be difficult to get a good sense of the values taken by the \u03c3 coefficients, and therefore of \u03b2. Let us see how Proposition 2 simplifies in the large bandwidth regime and what insights we can gain. We denote by \u03b2\u221e the limit of \u03b2 when \u03bd \u2192 +\u221e. When \u03bd \u2192 +\u221e, we prove in the supplementary material that, for any 1 \u2264 j \u2264 d, up to O (1/d) terms and a numerical constant, the j-th coordinate of \u03b2\u221e is then approximately equal to( \u03b2f\u221e ) j \u2248E [f(\u03c6(x))|wj \u2208 x]\u2212 1\nd \u2211 k 6=j E [f(\u03c6(x))|wk \u2208 x] .\nIntuitively, the interpretable coefficient associated to the word wj is high if the expected value of the model when word wj is present is significantly higher than the typical expected value when other words are present. We think that this is reasonable: if the model predicts much higher values when wj belongs to the example, it surely means that wj being present is important for the prediction."
    },
    {
      "heading": "3.3 Sketch of the proof",
      "text": "We conclude this section with a brief sketch of the proof of Theorem 1, the full proof can be found in the supplementary material.\nSince we set \u03bb = 0 in Eq. (5), \u03b2\u0302n is the solution of a weighted least-squares problem. Denote by W \u2208 Rn\u00d7n the diagonal matrix such that Wi,i = \u03c0i, and set Z \u2208 {0, 1}n\u00d7(d+1) the matrix such that its ith line\nis (1, z>i ). Then the solution of Eq. (5) is given by\n\u03b2\u0302n = ( Z>WZ )\u22121 Z>Wy ,\nwhere we defined y \u2208 Rn such that yi = f(\u03c6(xi)) for all 1 \u2264 i \u2264 n. Let us set \u03a3\u0302n := 1nZ >WZ and \u0393\u0302fn := 1 nZ >Wy. By the law of large numbers, we know that both \u03a3\u0302n and \u0393\u0302 f n converge in probability towards their population counterparts \u03a3 := E[\u03a3\u0302n] and \u0393f := E[\u0393\u0302n]. Therefore, provided that \u03a3 is invertible, \u03b2\u0302n is close to \u03b2 f := \u03a3\u22121\u0393f with high probability.\nAs we have seen in Section 2, the main differences with respect to the tabular data implementation are (i) the interpretable features, and (ii) the TF-IDF transform. The first point lead to a completely different \u03a3 than the one obtained in Garreau and von Luxburg (2020b). In particular, it has no zero coefficients, leading to more complicated expression for \u03b2f and additional challenges when controlling \u2225\u2225\u03a3\u22121\u2225\u2225 op . The second point is quite challenging since, as noted in Section 2.1, the TF-IDF transform of a document changes radically when deleting words at random in the document. This is the main reason why we have to resort to approximations when dealing with linear models."
    },
    {
      "heading": "4 Expression of \u03b2f for simple models",
      "text": "In this section, we see how to specialize Proposition 2 to simple models f . Recall that our main goal in doing so is to investigate whether it makes sense or not to use LIME in these cases. We will focus on two classes of\nmodels: decision trees (Section 4.1) and linear models (Section 4.2)."
    },
    {
      "heading": "4.1 Decision trees",
      "text": "In this section we focus on simple decision trees built on the presence or absence of given words. For instance, let us look at the model returning 1 if the word \u201cfood\u201d is present, or if \u201cabout\u201d and \u201ceverything\u201d are present in the document. Ideally, LIME would give high positive weights to \u201cfood,\u201d \u201cabout,\u201d and \u201ceverything,\u201d if they are present in the document to explain, and small weight to all other words.\nWe first notice that such simple decision trees can be written as sums of products of the binary features. Indeed, recall that we defined zj = 1wj\u2208x. For instance, suppose that the first three words of our dictionary are \u201cfood,\u201d \u201cabout,\u201d and \u201ceverything.\u201d Then the model from the previous paragraph can be written\ng(x) = z1 + (1\u2212 z1) \u00b7 z2 \u00b7 z3 . (10)\nNow it is clear that the zjs can be written as function of the TF-IDF transform of a word, since wj \u2208 x if, and only if, \u03c6(x)j > 0. Therefore this class of models falls into our framework and we can use Theorem 1 and Proposition 2 in order to gain insight on the explanations provided by LIME. For instance, Eq. (10) can be written as f(\u03c6(x)) with, for any \u03b6 \u2208 RD,\nf(\u03b6) := 1\u03b61>0 + (1\u2212 1\u03b61>0) \u00b7 1\u03b62>0 \u00b7 1\u03b63>0 .\nBy linearity, it is sufficient to know how to compute \u03b2f when f is a product of indicator functions.\nWe now make an important remark: since the new example x1, . . . , xn are created by deleting words at random from the text \u03be, x only contains words that are already present in \u03be. Therefore, without loss of generality, we can restrict ourselves to the local dictionary (the distinct words of \u03be). Indeed, for any word w not already in \u03be, 1w\u2208x = 0 almost surely. As before, we denote by D` the local dictionary associated to \u03be, and we denote its elements by w1, . . . , wd. We can compute in closed-form the interpretable coefficients for a product of indicator functions:\nProposition 3 (Computation of \u03b2f , product of indicator functions). Let J \u2286 {1, . . . , d} be a set of p distinct indices and set f(x) = \u220f j\u2208J 1xj>0. Then, for any j \u2208 J ,\n\u03b2fj =c \u22121 d [ \u03c31\u03b1p + \u03c32\u03b1p + (d\u2212p)\u03c33\u03b1p+1 + (p\u22121)\u03c33\u03b1p ] and, for any j \u2208 {1, . . . , d} \\ J ,\n\u03b2fj =c \u22121 d [ \u03c31\u03b1p + \u03c32\u03b1p+1 + (d\u2212p\u22121)\u03c33\u03b1p+1 + p\u03c33\u03b1p ] .\nIn particular, when p = 0, Proposition 3 simplifies greatly and we find that 1 \u2264 k \u2264 d, \u03b2fk = 1k=j . It is already a reassuring result: when the model is just indicating if a given word is present, the explanation given by LIME is one for this word and zero for all the other words.\nIt is slightly more complicated to see what happens when p \u2265 1. To this extent, let us set j \u2208 J and k /\u2208 J . Then it follows readily from Proposition 14 that\n\u03b2fj \u2212 \u03b2 f k = c \u22121 d (\u03c32 + \u03c33)(\u03b1p \u2212 \u03b1p+1) .\nSince \u03b1p \u2248 1/(p + 1) and \u03c32 + \u03c33 \u2248 6, we deduce that \u03b2fj \u03b2 f k . Moreover, from Definition 3 and 4 one can show that \u03b2fk = O (1/d) when \u03bd is large. Thus Proposition 14 tells us that LIME gives large positive coefficients to words that are in the support of f and small coefficients to all the other words. This is a satisfying property.\nTogether with the linearity property, Proposition 14 allows us to compute \u03b2f for any decision tree that can be written as in Eq. (10). We give an example of our theoretical predictions in Figure 6. As predicted, the words that are pivotal in the prediction have high interpretable coefficients, whereas the other words receive near-zero coefficients. It is interesting to notice that words that are near the root of the tree receive a greater weight. We present additional experiments in the supplementary material."
    },
    {
      "heading": "4.2 Linear models",
      "text": "We now focus on linear models, that is, for any document x,\nf(\u03c6(x)) := d\u2211 j=1 \u03bbj\u03c6(x)j , (11)\nwhere \u03bb1, . . . , \u03bbd are arbitrary fixed coefficients. We have to resort to approximate computations in this case: from now on, we assume that \u03bd = +\u221e. We start with the simplest linear function: all coefficients are zero except one, that is, \u03bbk = 1 if k = j and 0 otherwise in Eq. (11), for a fixed index j. We need to introduce additional notation before stating or result. For any 1 \u2264 j \u2264 d, define\n\u03c9k := m2jv 2 j\u2211d\nk=1m 2 kv 2 k\n,\nwhere the mks and vks were defined in Section 2.1. For any J that is a strict subset of {1, . . . , d}, define HS := \u2211 j\u2208J \u03c9j . Recall that S denotes the random subset of indices chosen by LIME in the sampling step (see Section 2.2). Define Ej = E [ (1\u2212HS)\u22121/2 \u2223\u2223S 63 j]\nand for any k 6= j, Ej,k = E [ (1\u2212HS)\u22121/2\n\u2223\u2223S 63 j, k]. Then we have the following:\nProposition 4 (Computation of \u03b2f , linear case). Let 1 \u2264 j \u2264 d and assume that f(\u03c6(x)) = \u03c6(x)j. Then, for any 1 \u2264 k \u2264 d such that k 6= j,(\n\u03b2f\u221e ) k = [ 2Ej,1 \u2212 2\nd \u2211 6\u0300=k,j Ej,` ] \u03c6(\u03be)j +O ( 1 d ) ,\nand ( \u03b2f\u221e ) j = [ 3Ej \u2212 2\nd \u2211 k 6=j Ej,k ] \u03c6(\u03be)j +O ( 1 d ) .\nProposition 4 is proved in the supplementary material. The main difficulty is to compute the expected value of \u03c6(x)j : this is the reason for the Ej terms, for which we find an approximate expression as a function of the \u03c9ks. Assuming that the \u03c9k are small, we can further this approximation and show that Ej \u2248 1.22 and Ej,k \u2248 1.15. In particular, these expressions do not depend on j and k. Thus we can drastically simplify the statement of Proposition 4: for any k 6= j,( \u03b2f\u221e ) k \u2248 0 and ( \u03b2f\u221e ) j \u2248 1.36\u03c6(\u03be)j . We can now go back to our original goal, Eq. (11). By linearity, we deduce that\n\u22001 \u2264 j \u2264 d, ( \u03b2f\u221e ) j \u2248 1.36 \u00b7 \u03bbj \u00b7 \u03c6(\u03be)j . (12)\nIn other words, up to a numerical constant and small error terms depending on d, the explanation for a\nlinear f is the TF-IDF value of the word multiplied by the coefficient of the linear model. We believe that this behavior is desirable for an interpretability method: large coefficients in the linear model should intuitively be associated to large interpretable coefficients. But at the same time the TF-IDF of the term is taken into account.\nWe observe a very good match between theory and practice (see Figure 7). Surprisingly, this is the case even though we assume that \u03bd is large in our derivations, whereas \u03bd is chosen by default in all our experiments. We present experiments with other bandwidths in the supplementary."
    },
    {
      "heading": "5 Conclusion",
      "text": "In this work we proposed the first theoretical analysis of LIME for text data. In particular, we provided a closed-form expression for the interpretable coefficients when the number of perturbed samples is large. Leveraging this expression, we exhibited some desirable behavior of LIME such as the linearity with respect to the model. In specific cases (simple decision trees and linear models), we derived more precise expression, showing that LIME outputs meaningful explanations in these cases.\nAs future work, we want to tackle more complex models. More precisely, we think that it is possible to obtained approximate statements in the spirit of Eq. (12) for models that are not linear. In the long run, we also want to analyze LIME for images, which is a much more challenging task."
    },
    {
      "heading": "1 The study of \u03a3",
      "text": "We begin by the study of the covariance matrix. We show in Section 1.1 how to compute \u03a3. We will see how the \u03b1 coefficients defined in the main paper appear. In Section 1.2, we show that it is possible to invert \u03a3 in closed-form: it can be written in function of cd and the \u03c3 coefficients. We show how \u03a3\u0302n concentrates around \u03a3 in Section 1.3. Finally, Section 1.4 is dedicated to the control of \u2225\u2225\u03a3\u22121\u2225\u2225 op ."
    },
    {
      "heading": "1.1 Computation of \u03a3",
      "text": "In this section, we derived a closed-form expression for \u03a3 := E[\u03a3\u0302n] as a function of d and \u03bd. Recall that we defined \u03a3\u0302 = 1nZ >WZ. By definition of Z and W , we have\n\u03a3\u0302 =  1 n \u2211n i=1 \u03c0i\n1 n \u2211n i=1 \u03c0izi,1 \u00b7 \u00b7 \u00b7\n1 n \u2211n i=1 \u03c0izi,d\n1 n \u2211n i=1 \u03c0izi,1\n1 n \u2211n i=1 \u03c0izi,1 \u00b7 \u00b7 \u00b7 1 n \u2211n i=1 \u03c0izi,1zi,d\n... ...\n. . . ...\n1 n \u2211n i=1 \u03c0izi,d 1 n \u2211n i=1 \u03c0izi,1zi,d \u00b7 \u00b7 \u00b7\n1 n \u2211n i=1 \u03c0izi,d  \u2208 R(d+1)\u00d7(d+1) . Taking the expectation in the last display with respect to the sampling of new examples yields\n\u03a3 =  E [\u03c0] E [\u03c0z1] \u00b7 \u00b7 \u00b7 E [\u03c0zd] E [\u03c0z1] E [\u03c0z1] \u00b7 \u00b7 \u00b7 E [\u03c0z1zd] ... ... . . . ...\nE [\u03c0zd] E [\u03c0z1zd] \u00b7 \u00b7 \u00b7 E [\u03c0zd]\n \u2208 R(d+1)\u00d7(d+1) . (13)\nAn important remark is that E [\u03c0zj ] does not depend on j. Indeed, there is no privileged index in the sampling of S (the subset of removed indices). Thus we only have to look into E [\u03c0z1] (say). For the same reason, E [\u03c0zjzk] does not depend on the 2-uple (j, k), and we can limit our investigations to E [\u03c0z1z2]. This is the reason why we\ndefined \u03b10 = E [\u03c0] and, for any 1 \u2264 p \u2264 d,\n\u03b1p = E [\u03c0 \u00b7 z1 \u00b7 \u00b7 \u00b7 zp] (14)\nin the main paper. We recognize the definition of the \u03b1ps in Eq. (13) and we write\n\u03a3j,k =  \u03b10 if j = k = 0, \u03b11 if j = 0 and k > 0 or j > 0 and k = 0 or j = k > 0,\n\u03b12 otherwise.\nAs promised, we can be more explicit regarding the \u03b1 coefficients. Recall that we defined the mapping\n\u03c8 : [0, 1] \u2212\u2192 R (15) t 7\u2212\u2192 exp ( \u2212(1\u2212 \u221a 1\u2212 t)2/(2\u03bd2) ) .\nIt is a decreasing mapping (see Figure 8). With this notation in hand, we have the following expression for the \u03b1 coefficients (this is Proposition 1 in the paper):\nProposition 5 (Computation of the \u03b1 coefficients). For any d \u2265 1, \u03bd > 0, and p \u2265 0, it holds that\n\u03b1p = 1\nd d\u2211 s=1 p\u22121\u220f k=0 d\u2212 s\u2212 k d\u2212 k \u03c8 ( s d ) .\nIn particular, the first three \u03b1 coefficients can be written\n\u03b10 = 1\nd d\u2211 s=1 \u03c8 ( s d ) , \u03b11 = 1 d d\u2211 s=1 ( 1\u2212 s d ) \u03c8 ( s d ) , and \u03b12 = 1 d d\u2211 s=1 ( 1\u2212 s d )( 1\u2212 s d\u2212 1 ) \u03c8 ( s d ) .\nProof. The idea of the proof is to use the law of total expectation with respect to the collection of events {#S = s} for s \u2208 {1, . . . , d}. Since P (#S = s) = 1d for any 1 \u2264 s \u2264 d, all that is left to compute is the expectation of \u03c0z1 \u00b7 \u00b7 \u00b7 zp conditionally to #S = s. According to the remark in Section 2.3 of the main paper, \u03c0 = \u03c8(s/d) conditionally to {#S = s}. We can conclude since, according to Lemma 4,\nPs (w1 \u2208 x, . . . , wp \u2208 x) = (d\u2212 s)(d\u2212 s\u2212 1) \u00b7 \u00b7 \u00b7 (d\u2212 s\u2212 p+ 1)\nd(d\u2212 1) \u00b7 \u00b7 \u00b7 (d\u2212 p+ 1) .\nIt is important to notice that, when \u03bd \u2192 +\u221e, \u03c8(t) \u2192 0 for any t \u2208 (0, 1]. As a consequence, in the large bandwidth regime, the \u03c8(s/d) weights are arbitrarily close to one. We demonstrate this effect in Figure 9. In this situation, the \u03b1 coefficients take a simpler form.\nCorollary 1 (Large bandwidth approximation of \u03b1 coefficients). For any 0 \u2264 p \u2264 d, it holds that\nlim \u03bd\u2192+\u221e\n\u03b1p = d\u2212 p\n(p+ 1)d .\nWe report these approximate values in Figure 9. In particular, when both \u03bd and d are large, we can see that \u03b1p \u2248 1/(p+ 1). Thus \u03b10 \u2248 1, \u03b11 \u2248 12 , and \u03b12 \u2248 1 3 .\nProof. When \u03bd \u2192 +\u221e, we have \u03c8(s/d)\u2192 1 and we can conclude directly by using Lemma 5.\nNotice that we can be slightly more precise than Corollary 1. Indeed, \u03c8 is decreasing on [0, 1], thus for any t \u2208 [0, 1], exp ( \u22121/(2\u03bd2) ) \u2264 \u03c8(t) \u2264 1. Therefore we can present some efficient bounds for the \u03b1 coefficients when \u03bd is large.\nCorollary 2 (Bounds on the \u03b1 coefficients). For any 0 \u2264 p \u2264 d, it holds that\nd\u2212 p (p+ 1)d e \u22121 2\u03bd2 \u2264 \u03b1p \u2264 d\u2212 p (p+ 1)d .\nOne can further show that, for any 0 \u2264 t \u2264 1,\nexp\n( \u2212t2\n2\u03bd2\n) \u2264 \u03c8(t) \u2264 exp ( \u2212t2\n8\u03bd2\n) . (16)\nUsing Eq. (16) together with the series-integral comparison theorem would yield very accurate bounds for the \u03b1 coefficients and related quantities, but we will not follow that road."
    },
    {
      "heading": "1.2 Computation of \u03a3\u22121",
      "text": "In this section, we present a closed-form formula for the matrix inverse of \u03a3 as a function of d and \u03bd.\nProposition 6 (Computation of \u03a3\u22121). For any d \u2265 1 and \u03bd > 0, recall that we defined\ncd = (d\u2212 1)\u03b10\u03b12 \u2212 d\u03b121 + \u03b10\u03b11 .\nAssume that cd 6= 0 and \u03b11 6= \u03b12. Define \u03c30 := (d\u2212 1)\u03b12 + \u03b11 and recall that we set  \u03c31 = \u2212\u03b11 , \u03c32 = (d\u22122)\u03b10\u03b12\u2212(d\u22121)\u03b121+\u03b10\u03b11 \u03b11\u2212\u03b12 ,\n\u03c33 = \u03b121\u2212\u03b10\u03b12 \u03b11\u2212\u03b12 .\nThen it holds that\n\u03a3\u22121 = 1\ncd  \u03c30 \u03c31 \u03c31 \u00b7 \u00b7 \u00b7 \u03c31 \u03c31 \u03c32 \u03c33 \u00b7 \u00b7 \u00b7 \u03c33 \u03c31 \u03c33 \u03c32 . . . ... ... ... . . .\n. . . \u03c33 \u03c31 \u03c33 \u00b7 \u00b7 \u00b7 \u03c33 \u03c32\n \u2208 R (d+1)\u00d7(d+1) . (17)\nWe display the evolution of the \u03c3i/cd coefficients with respect to \u03bd in Figure 11.\nProof. From Eq. (13), we can see that \u03a3 is a block matrix. The result follows from the block matrix inversion formula and one can check directly that \u03a3 \u00b7 \u03a3\u22121 = Id+1.\nOur next result shows that the assumptions of Proposition 6 are satisfied: \u03b11\u2212\u03b12 and cd are positive quantities. In fact, we prove a slightly stronger statement which will be necessary to control the operator norm of \u03a3\u22121.\nProposition 7 (\u03a3 is invertible). For any d \u2265 2,\n\u03b11 \u2212 \u03b12 \u2265 e\n\u22121 2\u03bd2 6 > 0 , and cd \u2265 e \u22122 \u03bd2 40 > 0 .\nProof. By definition of the \u03b1 coefficients (Eq. (14)), we have\n\u03b11 \u2212 \u03b12 = 1\nd d\u2211 s=1 ( 1\u2212 s d ) s d\u2212 1 \u03c8 ( s d ) .\nSince e \u22121 2\u03bd2 \u2264 \u03c8(t) \u2264 1 for any t \u2208 [0, 1], we have\ne \u22121 2\u03bd2 \u00b7 1\nd d\u2211 s=1 ( 1\u2212 s d ) s d\u2212 1 = d+ 1 6d \u00b7 e \u22121 2\u03bd2 \u2264 \u03b11 \u2212 \u03b12 \u2264 d+ 1 6d . (18)\nThe right-hand side of Eq. (18) yields the promised bound. Note that the same reasoning gives\nd+ 1\n2d \u00b7 e\n\u22121 2\u03bd2 \u2264 \u03b10 \u2212 \u03b11 \u2264 d+ 1\n2d . (19)\nLet us now find a lower bound for cd. We first start by noticing that\ncd = d\u03b11(\u03b10 \u2212 \u03b11)\u2212 (d\u2212 1)\u03b10(\u03b11 \u2212 \u03b12) (20)\n= d\u2211 s=1 ( 1\u2212 s d ) \u03c8 ( s d ) \u00b7 1 d d\u2211 s=1 s d \u03c8 ( s d ) \u2212 d\u2211 s=1 \u03c8 ( s d ) \u00b7 1 d d\u2211 s=1 ( 1\u2212 s d ) \u03c8 ( s d )\ncd = 1\nd  d\u2211 s=1 \u03c8 ( s d ) \u00b7 d\u2211 s=1 s2 d2 \u03c8 ( s d ) \u2212 ( d\u2211 s=1 s d \u03c8 ( s d ))2 . Therefore, by Cauchy-Schwarz inequality, cd \u2265 0. In fact, cd > 0 since the equality case in Cauchy-Schwarz is attained for proportional summands, which is not the case here.\nHowever, we need to improve this result if we want to control \u2225\u2225\u03a3\u22121\u2225\u2225\nop more precisely. To this extent, we use a\nrefinement of Cauchy-Schwarz inequality obtained by Filipovski (2019). Let us set, for any 1 \u2264 s \u2264 d,\nas := \u221a \u03c8 ( s d ) , bs := s d \u221a \u03c8 ( s d ) , A := \u221a\u221a\u221a\u221a d\u2211 s=1 a2s , and B := \u221a\u221a\u221a\u221a d\u2211 s=1 b2s .\nWith these notation,\ncd = 1\nd A2B2 \u2212( d\u2211 s=1 asbs )2 , and Cauchy-Schwarz yields A2B2 \u2265 (\u2211d s=1 asbs )2 . Theorem 2.1 in Filipovski (2019) is a stronger result, namely\nAB \u2265 d\u2211 s=1 asbs + 1 4 d\u2211 s=1 (a2sB 2 \u2212 b2sA2)2 a4sB 4 + b4sA 4 asbs . (21)\nLet us focus on this last term. Since all the terms are non-negative, we can lower bound by the term of order d, that is,\n1\n4 d\u2211 s=1 (a2sB 2 \u2212 b2sA2)2 a4sB 4 + b4sA 4 asbs \u2265 1 4 (b2dA 2 \u2212 a2dB2)2 b4dA 4 + a4dB 4 adbd = 1 4 (A2 \u2212B2)2 A4 +B4 \u03c8(1) , (22)\nsince ad = bd = \u221a \u03c8(1). On one side, we notice that\nA2 \u2212B2 = d\u2211 s=1 ( 1\u2212 s 2 d2 ) \u03c8 ( s d ) \u2265 exp ( \u22121 2\u03bd2 ) \u00b7 d\u2211 s=1 ( 1\u2212 s 2 d2 ) (for any t \u2208 [0, 1], \u03c8(t) \u2265 e\u22121/(2\u03bd2))\n= exp ( \u22121 2\u03bd2 ) \u00b7 1 6 ( 4d\u2212 1 d \u2212 3 )\nA2 \u2212B2 \u2265 3d \u00b7 exp\n( \u22121 2\u03bd2 ) 8 ,\nwhere we used d \u2265 2 in the last display. We deduce that (A2 \u2212B2)2 \u2265 9d2e \u22121 2\u03bd2 /64. On the other side, it is clear that A2 \u2264 d, and\nB2 \u2264 d\u2211 s=1 s2 d2 = (d+ 1)(2d+ 1) 6d .\nFor any d \u2265 2, we have B2 \u2264 5d/8, and we deduce that A4 +B4 \u2264 8964d 2. Therefore,\n(A2 \u2212B2)2\nA4 +B4 \u2265 9e\n\u22121 \u03bd2\n89 .\nComing back to Eq. (22), we proved that\n1\n4 d\u2211 s=1 (a2sB 2 \u2212 b2sA2)2 a4sB 4 + b4sA 4 asbs \u2265 9e \u22123 2\u03bd2 356 .\nPlugging into Eq. (21) and taking the square, we deduce that\nA2B2 \u2265 ( d\u2211 s=1 asbs )2 + 2 \u00b7 d\u2211 s=1 asbs \u00b7 9e \u22123 2\u03bd2 356 + 81e \u22123 \u03bd2 126736 .\nBut \u2211 asbs \u2265 de \u22121 2\u03bd2 /2, therefore, ignoring the last term, we have\nA2B2 \u2212 ( d\u2211 s=1 asbs )2 \u2265 9de \u22122 \u03bd2 356 .\nWe conclude by noticing that 356/9 \u2264 40.\nRemark 1. We suspect that the correct lower bound for cd is actually of order d, but we did not manage to prove it. Careful inspection of the proof shows that this d factor is lost when considering only the last term of the summation in Eq. (21). It is however challenging to control the remaining terms, since B2 is roughly half of A2 and s 2\nd2B 2 \u2212A2 is close to 0 for some values of s.\nWe conclude this section by giving an approximation of \u03a3\u22121 for large bandwidth. This approximation will be particularly useful in Section 3.1.\nCorollary 3 (Large bandwidth approximation of \u03a3\u22121). For any d \u2265 2, when \u03bd \u2192 +\u221e, we have\ncd \u2212\u2192 d2 \u2212 1\n12d ,\nand, as a consequence,  \u03c30 cd \u2192 2(2d\u22121)d+1 = 4\u2212 6 d +O ( 1 d2 ) \u03c31 cd \u2192 \u22126d+1 = \u2212 6 d +O ( 1 d2 ) \u03c32 cd \u2192 6(d 2\u22122d+3) (d+1)(d\u22121) = 6\u2212 12 d +O ( 1 d2 ) \u03c33 cd \u2192 \u22126(d\u22123)(d+1)(d\u22121) = \u2212 6 d +O ( 1 d2 ) .\n(23)\nProof. The proof is straightforward from the definition of cd and the \u03c3 coefficients, and Corollary 1."
    },
    {
      "heading": "1.3 Concentration of \u03a3\u0302n",
      "text": "We now turn to the concentration of \u03a3\u0302n around \u03a3. More precisely, we show that \u03a3\u0302n is close to \u03a3 in operator norm, with high probability. Since the definition of \u03a3\u0302n is identical to the one in the Tabular LIME case, we can use the proof machinery of Garreau and von Luxburg (2020b).\nProposition 8 (Concentration of \u03a3\u0302n). For any t \u2265 0,\nP (\u2225\u2225\u2225\u03a3\u0302n \u2212 \u03a3\u2225\u2225\u2225 op \u2265 t ) \u2264 4d \u00b7 exp ( \u2212nt2 32d2 ) .\nProof. We can write \u03a3\u0302 = 1n \u2211 i \u03c0iZiZ > i . The summands are bounded i.i.d. random variables, thus we can apply the matrix version of Hoeffding inequality. More precisely, the entries of \u03a3\u0302n belong to [0, 1] by construction, and Corollary 2 guarantees that the entries of \u03a3 also belong to [0, 1]. Therefore, if we set Mi := 1 n\u03c0iZiZ > i \u2212\u03a3, then the Mi satisfy the assumptions of Theorem 21 in Garreau and von Luxburg (2020b) and we can conclude since 1 n \u2211 iMi = \u03a3\u0302n \u2212 \u03a3.\n1.4 Control of \u2225\u2225\u03a3\u22121\u2225\u2225\nop We now turn to the control of \u2225\u2225\u03a3\u22121\u2225\u2225\nop . Essentially, our strategy is to bound the entries of \u03a3\u22121, and then to derive an upper bound for \u2225\u2225\u03a3\u22121\u2225\u2225\nop by noticing that \u2225\u2225\u03a3\u22121\u2225\u2225 op \u2264 \u2225\u2225\u03a3\u22121\u2225\u2225 F . Thus let us start by controlling the \u03c3\ncoefficients in absolute value.\nLemma 1 (Control of the \u03c3 coefficients). Let d \u2265 2 and \u03bd \u2265 1.66. Then it holds that\n|\u03c30| \u2264 d\n3 , |\u03c31| \u2264 1 , |\u03c32| \u2264\n3d\n2 e\n1 2\u03bd2 , and |\u03c33| \u2264\n3 2 e 1 2\u03bd2 .\nProof. By its definition, we know that \u03c30 is positive. Moreover, from Corollary 2, we see that\n\u03c30 = (d\u2212 1)\u03b12 + \u03b11\n\u2264 (d\u2212 1)(d\u2212 2) 3d + d\u2212 1 2d = 2d2 \u2212 3d+ 3\n6d .\nOne can check that for any d \u2265 2, we have 2d2 \u2212 3d+ 3 \u2264 2d2, which concludes the proof of the first claim.\nSince |\u03c31| = \u03b11, the second claim is straightforward from Corollary 2.\nRegarding \u03c32, we notice that\n\u03c32 = cd + \u03b1\n2 1 \u2212 \u03b10\u03b12\n\u03b11 \u2212 \u03b12 .\nSince \u03b10 \u2265 \u03b11 \u2265 \u03b12, we have \u2212\u03b11(\u03b10 \u2212 \u03b11) \u2264 \u03b121 \u2212 \u03b10\u03b12 \u2264 \u03b10(\u03b11 \u2212 \u03b12) .\nUsing Eqs. (18) and (19) in conjunction with Corollary 2, we find that \u2223\u2223\u03b121 \u2212 \u03b10\u03b12\u2223\u2223 \u2264 1/4. Moreover, from Eq. (20), we see that cd \u2264 d/4. We deduce that\n|\u03c32| \u2264 ( d\n4 +\n1\n4\n) \u00b7 6e 1 2\u03bd2 ,\nwhere we used the first statement of Proposition 7 to lower bound \u03b11\u03b12. The results follows, since d \u2265 2.\nFinally, we write\n|\u03c33| = \u2223\u2223\u03b121 \u2212 \u03b10\u03b12\u2223\u2223 \u03b11 \u2212 \u03b12\n\u2264 1/4 d+1 6d \u00b7 e \u22121 2\u03bd2\naccording to Proposition 7.\nWe now proceed to bound the operator norm of \u03a3\u22121. Proposition 9 (Control of \u2225\u2225\u03a3\u22121\u2225\u2225\nop ). For any d \u2265 2 and any \u03bd > 0, it holds that\n\u2225\u2225\u03a3\u22121\u2225\u2225 op \u2264 70d3/2e 5 2\u03bd2 .\nRemark 2. We notice that the control obtained worsens as d \u2192 +\u221e and \u03bd \u2192 0. We conjecture that the dependency in d is not tight. For instance, showing that cd = \u2126(d) (that is, improving Proposition 7) would yield an upper bound of order d instead of d3/2. The discussion after Proposition 7 indicates that such an improvement may be possible. Moreover, we see in experiments that the concentration of \u03b2\u0302n does not degrade that much for large d (see, in particular, Figure 17 in Section 6.2), another sign that Proposition 9 could be improved. Proof. We will use the fact that \u2225\u2225\u03a3\u22121\u2225\u2225 op \u2264 \u2225\u2225\u03a3\u22121\u2225\u2225 F . We first write\n\u2225\u2225\u03a3\u22121\u2225\u22252 F = 1\nc2d\n( \u03c320 + 2d\u03c3 2 1 + d\u03c3 2 2 + (d 2 \u2212 d)\u03c323 ) ,\nby definition of the \u03c3 coefficients. On one hand, using Lemma 1, we write\n\u03c320 + 2d\u03c3 2 1 + d\u03c3 2 2 + (d 2 \u2212 d)\u03c323 \u2264 d2\n9 + 2d+ d \u00b7 (3d/2)2e\n1 \u03bd2 + (d2 \u2212 d) \u00b7 9\n4 e\n1 \u03bd2\n\u2264 3d3e 1 \u03bd2 , (24)\nwhere we used cd \u2264 d and d \u2265 2 in the last display. On the other hand, a direct consequence of Proposition 7 is that\n1 c2d \u2264 1600e 4 \u03bd2 . (25)\nPutting together Eqs. (24) and (25), we obtain the claimed result, since \u221a 3 \u00b7 1600 \u2264 70."
    },
    {
      "heading": "2 The study of \u0393f",
      "text": "We now turn to the study of the (weighted) responses. In Section 2.1, we obtain an explicit expression for the average responses. We show how to obtain closed-form expressions in the case of indicator functions in Section 2.2. In the case of a linear model, we have to resort to approximations that are detailed in Section 2.3. Section 2.4 contains the concentration result for \u0393\u0302n."
    },
    {
      "heading": "2.1 Computation of \u0393f",
      "text": "We start our study by giving an expression for \u0393f for any f under mild assumptions. Recall that we defined \u0393\u0302n = 1 nZ >Wy, where y \u2208 Rd+1 is the random vector defined coordinate-wise by yi = f(xi). From the definition of \u0393\u0302n, it is straightforward that\n\u0393\u0302n =  1 n \u2211n i=1 \u03c0if(\u03c6(xi)) 1 n \u2211n i=1 \u03c0izi,1f(\u03c6(xi))\n... 1 n \u2211n i=1 \u03c0izi,df(\u03c6(xi))  \u2208 Rd+1 . As a consequence, since we defined \u0393f = E[\u0393\u0302n], it holds that\n\u0393f =  E [\u03c0f(\u03c6(x))] E [\u03c0z1f(\u03c6(x))]\n... E [\u03c0zdf(\u03c6(x))]  . (26) Of course, Eq. (26) depends on the model f . These computations can be challenging. Nevertheless, it is possible to obtain exact results in simple situations.\nConstant model. As a warm up, let us show how to compute \u0393f when f is constant. Perhaps the simplest model of all: f always returns the same value, whatever the value of \u03c6(x) may be. By linearity of \u0393f (see Section 3.2 of the main paper), it is sufficient to consider the case f = 1. From Eq. (26), we see that\n\u0393fj = { E [\u03c0] if j = 0, E [\u03c0zj ] otherwise.\nWe recognize the definitions of the \u03b1 coefficients, and, more precisely, \u0393f0 = \u03b10 and \u0393 f j = \u03b11 if j \u2265 1."
    },
    {
      "heading": "2.2 Indicator functions",
      "text": "Let us turn to a slightly more complicated class of models: indicator functions, or rather products of indicator functions. As explained in the paper, these functions fall into our framework. We have the following result: Proposition 10 (Computation of \u0393f , product of indicator functions). Set J \u2286 {1, . . . , d} a set of p distinct indices. Define\nf(\u03c6(x)) := \u220f j\u2208J 1\u03c6(x)j>0 .\nThen it holds that\n\u0393f` = { \u03b1p if ` \u2208 {0} \u222a J \u03b1p+1 otherwise.\nProof. As noticed in the paper, f can be written as a product of zjs. Therefore, we only have to compute E [ \u03c0 \u220f j\u2208J zj ] and E [ \u03c0zk \u220f j\u2208J zj ] ,\nfor any 1 \u2264 k \u2264 d. The first term is \u03b1p by definition. For the second term, we notice that if ` \u2208 {0}\u222aJ , then two terms are identical in the product of binary features, and we recognize the definition of \u03b1p. In all other cases, there are no cancellation and we recover the definition of \u03b1p+1."
    },
    {
      "heading": "2.3 Linear model",
      "text": "We now consider a linear model, that is,\nf(\u03c6(x)) := d\u2211 j=1 \u03bbj\u03c6(x)j , (27)\nwhere \u03bb1, . . . , \u03bbd are arbitrary fixed coefficients. In order to simplify the computations, we will consider that \u03bd \u2192 +\u221e in this section. In that case, \u03c0 a.s.\u2212\u2192 1. It is clear that f is bounded on SD\u22121, thus, by dominated convergence,\n\u0393f \u2212\u2192 \u0393\u221e :=  E [f(\u03c6(x)] E [z1f(\u03c6(x)]\n... E [zdf(\u03c6(x)]  \u2208 Rd+1 . (28) By linearity of f 7\u2192 \u0393f\u221e, it is sufficient to compute E [\u03c6(x)j ] and E [zk\u03c6(x)j ] for any 1 \u2264 j, k \u2264 d.\nFor any 1 \u2264 j \u2264 d, recall that we defined\n\u03c9k = m2jv 2 j\u2211d\nk=1m 2 kv 2 k\n,\nand HS := \u2211 k\u2208S \u03c9k, where S is the random subset of indices chosen by LIME. The motivation for the definition of the random variable HS is the following proposition: it is possible to write the expected TF-IDF as an expression depending on HS .\nProposition 11 (Expected normalized TF-IDF). Let wj be a fixed word of \u03be. Then, it holds that\nE [\u03c6(x)j ] = E [zj\u03c6(x)j ] = d\u2212 1\n2d \u00b7 \u03c6(\u03be)j \u00b7 E\n[ 1\u221a\n1\u2212HS \u2223\u2223\u2223\u2223S 63 j] , (29) and, for any k 6= j,\nE [zk\u03c6(x)j ] = d\u2212 2\n3d \u00b7 \u03c6(\u03be)j \u00b7 E\n[ 1\u221a\n1\u2212HS \u2223\u2223\u2223\u2223S 63 j, k] . (30) Proof. We start by proving Eq (29). Let us split the expectation depending on wj \u2208 x. Since the term frequency is 0 if wj /\u2208 x, we have E [\u03c6(x)j ] = E [\u03c6(x)j |wj \u2208 x]P (wj \u2208 x) . (31) Lemma 5 gives us the value of P (wj \u2208 x). Let us focus on the TF-IDF term in Eq. (31). By definition, it is the product of the term frequency and the inverse document frequency, normalized. Since the latter does not change when words are removed from \u03be, only the norm changes: we have to remove all terms indexed by S. For any 1 \u2264 j \u2264 d, let us set mj (resp. vj) the term frequency (resp. the inverse term frequency) of wj Conditionally to {wj \u2208 x},\n\u03c6(x)j = mjvj\u221a\u2211 k/\u2208Sm 2 kv 2 k .\nLet us factor out \u03c6(\u03be)j in the previous display. By definition of HS , we have\n\u03c6(x)j = \u03c6(\u03be)j \u00b7 1\u221a\n1\u2212 \u2211 k\u2208S m2kv 2 k\n\u2016\u03d5(\u03be)\u20162\n= \u03c6(\u03be)j \u00b7 1\u221a\n1\u2212HS .\nSince {wj \u2208 x} is equivalent to {j /\u2208 S} by construction, we can conclude. The proof of the second statement is similar; one just has to condition with respect to {wj , wk \u2208 x} instead, which is equivalent to {S 63 j, k}.\nAs a direct consequence of Proposition 11, we can derive \u0393f\u221e = lim\u03bd\u2192+\u221e \u0393 f when f : x 7\u2192 xj . Recall that we set Ej = E [ (1\u2212HS)\u22121/2 \u2223\u2223S 63 j] and Ej,k = E [(1\u2212HS)\u22121/2\u2223\u2223S 63 j, k]. Then ( \u0393f\u221e ) k = {( 1 2 \u2212 1 2d ) \u00b7 Ej \u00b7 \u03c6(\u03be)j if k = 0 or k = j,(\n1 3 \u2212 2 3d ) \u00b7 Ej,k \u00b7 \u03c6(\u03be)j otherwise.\n(32)\nIn practice, the expectation computations required to evaluate Ej and Ej,k are not tractable as soon as d is large. Indeed, in that case, the law of HS is unknown and approximating the expectation by Monte-Carlo methods requires is hard since one has to sum over all subsets and there are O ( 2d )\nsubsets S such that S \u2286 {1, . . . , d}. Therefore we resort to approximate expressions for these expected values computations.\nWe start by writing\nE [\n1\u221a 1\u2212X\n] \u2248 1\u221a\n1\u2212 E [X] . (33)\nAll that is left to compute will be E [HS |S 63 j] and E [HS |S 63 j, k]. We see in Section 4 that after some combinatoric considerations, it is possible to obtain these expected values as a function of \u03c9j and \u03c9k. More precisely, Lemma 3 states that\nE [HS |S 63 j] = 1\u2212 \u03c9j\n3 +O\n( 1\nd\n) and E [HS |S 63 j, k] =\n1\u2212 \u03c9j \u2212 \u03c9k 4\n+O ( 1\nd\n) . (34)\nWhen d is large and the \u03c9ks are small, using Eq. (33), we obtain the following approximations:\nE [\u03c6(x)j ] \u2248 1 2 \u00b7 \u221a 1\n1\u2212 13 \u00b7 \u03c6(\u03be)j \u2248 0.61 \u00b7 \u03c6(\u03be)j , (35)\nand, for any k 6= j,\nE [zk\u03c6(x)j ] \u2248 1 3 \u00b7 \u221a 1\n1\u2212 14 \u00b7 \u03c6(\u03be)j \u2248 0.38 \u00b7 \u03c6(\u03be)j . (36)\nFor all practical purposes, we will use Eq. (35) and (36).\nRemark 3. One could obtain better approximations than above in two ways. First, it is possible to take into account the dependency in \u03c9j and \u03c9k in the expectation of HS . That is, plugging Eq. (34) into Eq. (33) instead of the numerical values 1/3 and 1/4. This yields more accurate, but more complicated formulas. Without being so precise, it is also possible to consider an arbitrary distribution for the \u03c9ks (for instance, assuming that the term frequencies follow the Zipf\u2019s law (Powers, 1998)). Second, since the mapping \u03b8 : x 7\u2192 1\u221a\n1\u2212x is convex, by\nJensen\u2019s inequality, we are always underestimating by considering \u03b8(E [X]) instead of E [\u03b8(X)]. Going further in the Taylor expansion of \u03b8 is a way to fix this problem, namely using\nE [\n1\u221a 1\u2212X\n] \u2248 1\u221a\n1\u2212 E [X] +\n3Var (X) 8 \u221a 1\u2212 E [X] ,\ninstead of Eq. (33). We found that it was not useful to do so from an experimental point of view: our theoretical predictions match the experimental results while remaining simple enough."
    },
    {
      "heading": "2.4 Concentration of \u0393\u0302n",
      "text": "We now show that \u0393\u0302n is concentrated around \u0393 f . Since the expression of \u0393\u0302n is the same than in the tabular case, and since f is bounded on the unit sphere SD\u22121, the same reasoning as in the proof of Proposition 24 in Garreau and von Luxburg (2020b) can be applied. Proposition 12 (Concentration of \u0393\u0302n). Assume that f is bounded by M > 0 on S D\u22121. Then, for any t > 0, it holds that\nP ( \u2016\u0393\u0302n \u2212 \u0393f\u2016 \u2265 t ) \u2264 4dexp ( \u2212nt2\n32Md2\n) .\nProof. Recall that \u2016\u03c6(x)\u2016 = 1 almost surely. Since f is bounded by M on SD\u22121, it holds that |f(\u03c6(x))| \u2264 M almost surely. We can then proceed as in the proof of Proposition 24 in Garreau and von Luxburg (2020b)."
    },
    {
      "heading": "3 The study of \u03b2f",
      "text": "In this section, we study the interpretable coefficients. We start with the computation of \u03b2f in Section 3.1. In Section 3.2, we show how \u03b2\u0302n concentrates around \u03b2 f ."
    },
    {
      "heading": "3.1 Computation of \u03b2f",
      "text": "Recall that, for any model f , we have defined \u03b2f = \u03a3\u22121\u0393f . Directly multiplying the expressions found for \u03a3\u22121 (Eq. (17)) and \u0393f (Eq. (26)) obtained in the previous sections, we obtain the expression of \u03b2f in the general case (this is Proposition 2 in the paper).\nProposition 13 (Computation of \u03b2f , general case). Assume that f is bounded on the unit sphere. Then\n\u03b2f0 = c \u22121 d { \u03c30E [\u03c0f(\u03c6(x))] + \u03c31 d\u2211 k=1 E [\u03c0zkf(\u03c6(x))] } , (37)\nand, for any 1 \u2264 j \u2264 d,\n\u03b2fj = c \u22121 d { \u03c31E [\u03c0f(\u03c6(x))] + \u03c32E [\u03c0zjf(\u03c6(x))] + \u03c33 d\u2211 k=1 k 6=j E [\u03c0zkf(\u03c6(x))] } . (38)\nThis is Proposition 2 in the paper, with the additional expression of the intercept \u03b2f0 . Let us see how to obtain an approximate, simple expression when both the bandwidth parameter and the size of the local dictionary are large. When \u03bd \u2192 +\u221e, using Corollary 3, we find that\n\u03b2f0 \u2212\u2192 ( \u03b2f\u221e ) 0 := 4d\u2212 2 d+ 1 E [\u03c0f(\u03c6(x))]\u2212 6 d+ 1 d\u2211 k=1 E [\u03c0zkf(\u03c6(x))] ,\nand, for any 1 \u2264 j \u2264 d,\n\u03b2fj \u2212\u2192 ( \u03b2f\u221e ) j := \u22126 d+ 1 E [\u03c0f(\u03c6(x))] + 6(d2 \u2212 2d+ 3) d2 \u2212 1 E [\u03c0zjf(\u03c6(x))]\u2212 6(d\u2212 3) d2 \u2212 1 \u2211 k 6=j E [\u03c0zkf(\u03c6(x))] .\nFor large d, since f is bounded on SD\u22121, we find that\n( \u03b2f\u221e ) 0 = 4E [\u03c0f(\u03c6(x))]\u2212 6 d d\u2211 k=1 E [\u03c0zkf(\u03c6(x))] +O ( 1 d ) ,\nand, for any 1 \u2264 j \u2264 d, ( \u03b2f\u221e ) j = 6E [\u03c0zjf(\u03c6(x))]\u2212 6\nd \u2211 k 6=j E [\u03c0zkf(\u03c6(x))] +O ( 1 d ) .\nNow, by definition of the interpretable features, for any 1 \u2264 j \u2264 d,\nE [\u03c0zjf(\u03c6(x))] = E [\u03c0zjf(\u03c6(x))|wj \u2208 x] \u00b7 P (wj \u2208 x) + E [\u03c0zjf(\u03c6(x))|wj /\u2208 x] \u00b7 P (wj /\u2208 x)\n= E [\u03c0f(\u03c6(x))|wj \u2208 x] \u00b7 d\u2212 1\n2d + 0 ,\nwhere we used Lemma 5 in the last display. Therefore, we have the following approximations of the interpretable coefficients: (\n\u03b2f\u221e ) 0 = 2E [\u03c0f(\u03c6(x))]\u2212 3 d \u2211 k E [\u03c0f(\u03c6(x))|wk \u2208 x] +O ( 1 d ) , (39)\nand, for any 1 \u2264 j \u2264 d,( \u03b2f\u221e ) j = 3E [\u03c0f(\u03c6(x))|wj \u2208 x]\u2212 3\nd \u2211 k E [\u03c0f(\u03c6(x))|wk \u2208 x] +O ( 1 d ) . (40)\nThe last display is the approximation of Proposition 13 presented in the paper.\nRemark 4. In Garreau and von Luxburg (2020b), it is noted that LIME for tabular data provably ignores unused coordinates. In other words, if the model f does not depend on coordinate j, then the explanation \u03b2fj is 0. We could not prove such a statement in the case of text data, even for simplified expressions such as Eq. (40).\nWe now show how to compute \u03b2f in specific cases, thus returning to generic \u03bd and d.\nConstant model. As a warm up exercise, let us assume that f is a constant, which we set to 1 without loss of generality (by linearity). Recall that, in that case, \u0393f0 = \u03b10 and \u0393 f j = \u03b11 for any 1 \u2264 j \u2264 d. From the definition of cd and the \u03c3 coefficients (Proposition 6), we find that{ \u03c30\u03b10 + d\u03c31\u03b11 = cd ,\n\u03c31\u03b10 + \u03c32\u03b11 + (d\u2212 1)\u03c33\u03b11 = 0 .\nWe deduce from Proposition 13 that \u03b2f0 = 1 and \u03b2 f j = 0 for any 1 \u2264 j \u2264 d. This is conform to our intuition: if the model is constant, then no word should receive nonzero weight in the explanation provided by Text LIME.\nIndicator functions. We now turn to indicator functions, more precisely products of indicator functions. We will prove the following (Proposition 3 in the paper):\nProposition 14 (Computation of \u03b2f , product of indicator functions). Let j \u2286 {1, . . . , d} be a set of p distinct indices and set f(x) = \u220f j\u2208J 1xj>0. Then \u03b2f0 = c \u22121 d (\u03c30\u03b1p + p\u03c31\u03b1p + (d\u2212 p)\u03c31\u03b1p+1) , \u03b2fj = c \u22121 d (\u03c31\u03b1p + \u03c32\u03b1p + (d\u2212 p)\u03c33\u03b1p+1 + (p\u2212 1)\u03c33\u03b1p) if j \u2208 J ,\n\u03b2fj = c \u22121 d (\u03c31\u03b1p + \u03c32\u03b1p+1 + (d\u2212 p\u2212 1)\u03c33\u03b1p+1 + p\u03c33\u03b1p) otherwise .\nProof. The proof is straightforward from Proposition 10 and Proposition 13.\nLinear model. In this last paragraph, we treat the linear case. As noted in Section 2.3, we have to resort to approximate computations: in this paragraph, we assume that \u03bd = +\u221e. We start with the simplest linear function: all coefficients are zero except one (this is Proposition 4 in the paper).\nProposition 15 (Computation of \u03b2f , linear case). Let 1 \u2264 j \u2264 d and assume that f(\u03c6(x)) = \u03c6(x)j. Recall that we set Ej = E [ (1\u2212HS)\u22121/2 \u2223\u2223S 63 j] and for any k 6= j, Ej,k = E [(1\u2212HS)\u22121/2\u2223\u2223S 63 j, k]. Then ( \u03b2f\u221e ) 0 = 5Ej \u2212 2d\u2211 k 6=j Ej,k \u03c6(\u03be)j +O ( 1 d )\nfor any k 6= j, ( \u03b2f\u221e ) k = 2Ej,1 \u2212 2d \u2211 ` 6=k,j Ej,` \u03c6(\u03be)j +O ( 1 d ) ,\nand ( \u03b2f\u221e ) j = 3Ej \u2212 2d\u2211 k 6=j Ej,k \u03c6(\u03be)j +O ( 1 d ) .\nProof. Straightforward from Eqs. (23) and (32).\nAssuming that the \u03c9k are small, we deduce from Eqs. (35) and (36) that Ej \u2248 1.22 and Ej,k \u2248 1.15. In particular, they do not depend on j and k. Thus we can drastically simplify the statement of Proposition 15:\n\u2200k 6= j, ( \u03b2f\u221e ) k \u2248 0 and ( \u03b2f\u221e ) j \u2248 1.36\u03c6(\u03be)j . (41)\nWe can now go back to our original goal: f(x) = \u2211d j=1 \u03bbjxj . By linearity, we deduce from Eq. (41) that\n\u22001 \u2264 j \u2264 d, ( \u03b2f\u221e ) j \u2248 1.36 \u00b7 \u03bbj \u00b7 \u03c6(\u03be)j . (42)\nIn other words, as noted in the paper, the explanation for a linear f is the TF-IDF of the word multiplied by the coefficient of the linear model, up to a numerical constant and small error terms depending on d."
    },
    {
      "heading": "3.2 Concentration of \u03b2\u0302",
      "text": "In this section, we state and prove our main result: the concentration of \u03b2\u0302n around \u03b2 f with high probability (this is Theorem 1 in the paper).\nTheorem 2 (Concentration of \u03b2\u0302n). Suppose that f is bounded by M > 0 on S D\u22121. Let > 0 be a small constant, at least smaller than M . Let \u03b7 \u2208 (0, 1). Then, for every\nn \u2265 max { 29 \u00b7 704M2d9e 10 \u03bd2 , 29 \u00b7 702Md5e 5 \u03bd2 } log 8d\u03b7 2 ,\nwe have P ( \u2016\u03b2\u0302n \u2212 \u03b2f\u2016 \u2265 ) \u2264 \u03b7.\nProof. We follow the proof scheme of Theorem 28 in Garreau and von Luxburg (2020b). The key point is to notice that \u2016\u03b2\u0302n \u2212 \u03b2f\u2016 \u2264 2 \u2225\u2225\u03a3\u22121\u2225\u2225\nop \u2016\u0393\u0302\u2212 \u0393f\u2016+ 2 \u2225\u2225\u03a3\u22121\u2225\u22252 op \u2225\u2225\u0393f\u2225\u2225 \u2016\u03a3\u0302\u2212 \u03a3\u2016op , (43) provided that \u2016\u03a3\u22121(\u03a3\u0302 \u2212 \u03a3)\u2016op \u2264 0.32 (this is Lemma 27 in Garreau and von Luxburg (2020b). Therefore, in order to show that \u2016\u03b2\u0302n \u2212 \u03b2f\u2016 \u2264 , it suffices to show that each term in Eq. (43) is smaller than /4 and that \u2016\u03a3\u22121(\u03a3\u0302\u2212\u03a3)\u2016op \u2264 0.32. The concentration results obtained in Section 1 and 2 guarantee that both \u2016\u03a3\u0302\u2212\u03a3\u2016op and \u2016\u0393\u0302 \u2212 \u0393f\u2016 are small if n is large enough, with high probability. This, combined with the upper bound on \u2016\u03a3\u22121\u2016op given by Proposition 9, concludes the proof.\nLet us give a bit more details. We start with the control of \u2016\u03a3\u22121(\u03a3\u0302 \u2212 \u03a3)\u2016op. Set t1 := (220d3/2e 5 2\u03bd2 )\u22121 and n1 := 32d 2 log 8d\u03b7 /t 2 1. Then, according to Proposition 8, for any n \u2265 n1,\nP ( \u2016\u03a3\u0302n \u2212 \u03a3\u2016op \u2265 t1 ) \u2264 4dexp ( \u2212nt21 32d2 ) \u2264 \u03b7 2 .\nSince \u2016\u03a3\u22121\u2016op \u2264 70d3/2e 5\n2\u03bd2 (according to Proposition 9), by sub-multiplicativity of the operator norm, it holds that\n\u2016\u03a3\u22121(\u03a3\u0302\u2212 \u03a3)\u2016op \u2264 \u2016\u03a3\u22121\u2016op\u2016\u03a3\u0302\u2212 \u03a3\u2016op \u2264 70/220 < 0.32 , (44)\nwith probability greater than 1\u2212 \u03b7/2.\nNow let us set t2 := (4 \u00b7 702Md7/2e 5 \u03bd2 )\u22121 and n2 := 32d 2 log 8d\u03b7 /t 2 2. According to Proposition 8, for any n \u2265 n2, it holds that\n\u2016\u03a3\u0302n \u2212 \u03a3\u2016op \u2264 4Md1/2 \u00b7 (702d3e5/\u03bd\n2\n)\u22121 ,\nwith probability greater than \u03b7/2. Since \u2016\u0393f\u2016 \u2264M \u00b7 d1/2 and \u2016\u03a3\u22121\u20162op \u2264 702d3e5/\u03bd 2\n,\u2225\u2225\u03a3\u22121\u2225\u2225 op \u2016\u0393\u0302\u2212 \u0393f\u2016 \u2264\n4\nwith probability grater than 1 \u2212 \u03b7/2. Notice that, since we assumed < M , t2 < t1, and thus Eq. (44) also holds.\nFinally, let us set t3 := /(4 \u00b7 70d3/2e 5 2\u03bd2 ) and n3 := 32Md 2 log 8d\u03b7 /t 2 3. According to Proposition 12, for any n \u2265 n3,\nP ( \u2016\u0393\u0302n \u2212 \u0393f\u2016 \u2265 t3 ) \u2264 4dexp ( \u2212nt23\n32Md2\n) \u2264 \u03b7\n2 .\nSince \u2016\u03a3\u22121\u2016op \u2264 70d3/2e 5\n2\u03bd2 , we deduce that\u2225\u2225\u03a3\u22121\u2225\u22252 op \u2225\u2225\u0393f\u2225\u2225 \u2016\u03a3\u0302\u2212 \u03a3\u2016op \u2264 2 ,\nwith probability greater than 1\u2212 \u03b7/2. We conclude by a union bound argument."
    },
    {
      "heading": "4 Sums over subsets",
      "text": "In this section, independent from the rest, we collect technical facts about sums over subsets. More particularly, we now consider arbitrary, fixed positive real numbers \u03c91, . . . , \u03c9d such that \u2211 k \u03c9k = 1. We are interested in\nsubsets S of {1, . . . , d}. For any such S, we define HS := \u2211 k\u2208S \u03c9k the sum of the \u03c9k coefficients over S. Our main goal in this section is to compute the expectation of HS conditionally to S not containing a given index (or two given indices), which is the key quantity appearing in Proposition 15.\nLemma 2 (First order subset sums). Let 1 \u2264 s \u2264 d and 1 \u2264 j, k \u2264 d with j 6= k. Then\n\u2211 #S=s S 63j HS = ( d\u2212 2 s\u2212 1 ) (1\u2212 \u03c9j) ,\nand \u2211 #S=s S 63j,k HS = ( d\u2212 3 s\u2212 1 ) (1\u2212 \u03c9j \u2212 \u03c9k) .\nProof. The main idea of the proof is to rearrange the sum, summing over all indices and then counting how many subsets satisfy the condition. That is,\n\u2211 #S=s S3j HS = d\u2211 k=1 \u03c9k \u00b7#{S s.t. j, k \u2208 S}\n= \u2211 k 6=j \u03c9k \u00b7 ( d\u2212 2 s\u2212 2 ) + \u03c9j \u00b7 ( d\u2212 1 s\u2212 1 )\n= ( d\u2212 2 s\u2212 2 ) + [( d\u2212 1 s\u2212 1 ) \u2212 ( d\u2212 2 s\u2212 2 )] \u03c9j .\nWe conclude by using the binomial identity( d\u2212 1 s\u2212 1 ) \u2212 ( d\u2212 2 s\u2212 2 ) = ( d\u2212 2 s\u2212 1 ) .\nNotice that, in the previous derivation, we had to split the sum to account for the case j = k. The proof of the second formula is similar.\nLet us turn to expectation computation that are important to derive approximation in Section 2.3. We now see S and HS as random variables. We will denote by Es [\u00b7] the expectation conditionally to the event {#S = s}. Lemma 3 (Expectation computation). Let j, k be distinct elements of {1, . . . , d}. Then\nE [HS |S 63 j] = (1\u2212 \u03c9j)(d+ 1)\n3(d\u2212 1) = 1\u2212 \u03c9j 3\n+O ( 1\nd\n) , (45)\nand\nE [HS |S 63 j, k] = (1\u2212 \u03c9j \u2212 \u03c9k)(d+ 1)\n4(d\u2212 2) = 1\u2212 \u03c9j \u2212 \u03c9k 4\n+O ( 1\nd\n) (46)\nProof. By the law of total expectation, we know that\nE [HS |S 63 j] = d\u2211 s=1 Es [HS |S 63 j] \u00b7 P (#S = s|S 63 j) .\nWe first notice that, for any s < d,\nP (#S = s|S 63 j) = P (S 63 j|#S = s)P (#S = s) P (j /\u2208 S)\n=\n( d\u22121 s ) / ( d s ) \u00b7 1d\nd\u22121 2d\nP (#S = s|S 63 j) = 2(d\u2212 s) d(d\u2212 1) .\nAccording to Lemma 2, for any 1 \u2264 s < d,\u2211 #S=s S 63j HS = ( d\u2212 2 s\u2212 1 ) (1\u2212 \u03c9j) .\nMoreover, there are ( d\u22121 s ) such subsets. Since ( d\u22121 s\u22121 )\u22121(d\u22122 s ) = sd\u22121 , we deduce that\nEs [HS |S 63 j] = s\nd\u2212 1 (1\u2212 \u03c9j) .\nFinally, we write\nE [HS |S 63 j] = d\u22121\u2211 s=1 s d\u2212 1 (1\u2212 \u03c9j) \u00b7 2(d\u2212 s) d(d\u2212 1)\n= (1\u2212 \u03c9j) \u00b7 2 d(d\u2212 1)2 d\u22121\u2211 s=1 s(d\u2212 s)\nE [HS |S 63 j] = (d+ 1)(1\u2212 \u03c9j)\n3(d\u2212 1) .\nThe second case is similar. One just has to note that\nP (#S = s|S 63 j, k) = P (S 63 j, k|#S = s) P (j, k /\u2208 S)\n= 3(d\u2212 s)(d\u2212 s\u2212 1) d(d\u2212 1)(d\u2212 2) . (Lemma 5)\nThen we can conclude since\nd\u22122\u2211 s=1 s(d\u2212 s)(d\u2212 s\u2212 1) = (d\u2212 2)(d\u2212 1)d(d+ 1) 12 ."
    },
    {
      "heading": "5 Technical results",
      "text": "In this section, we collect small probability computations that are ubiquitous in our derivations. We start with the probability for a given word to be present in the new sample x, conditionally to #S = s.\nLemma 4 (Conditional probability to contain given words). Let w1, . . . , wp be p distinct words of D`. Then, for any 1 \u2264 s \u2264 d,\nPs (w1 \u2208 x, . . . , wp \u2208 x) = (d\u2212 s)(d\u2212 s\u2212 1) \u00b7 \u00b7 \u00b7 (d\u2212 s\u2212 p+ 1)\nd(d\u2212 1) \u00b7 \u00b7 \u00b7 (d\u2212 p+ 1) = (d\u2212 s)! (d\u2212 s\u2212 p)! \u00b7 (d\u2212 p)! d! .\nIn the proofs, we use extensively Lemma 4 for p = 1 and p = 2, that is,\nPs (wj \u2208 x) = d\u2212 s d and Ps (wj \u2208 x,wk \u2208 x) = (d\u2212 s)(d\u2212 s\u2212 1) d(d\u2212 1) ,\nfor any 1 \u2264 j, k \u2264 d with j 6= k.\nProof. We prove the more general statement. Conditionally to #S = s, the choice of S is uniform among all subsets of {1, . . . , d} of cardinality s. There are ( d s ) such subsets, and only ( d\u2212p s ) of them do not contain the indices corresponding to w1, . . . , wp.\nWe have the following result, without conditioning on the cardinality of S:\nLemma 5 (Probability to contain given words). Let w1, . . . , wp be p distinct words of D`. Then\nP (w1, . . . , wp \u2208 x) = d\u2212 p\n(p+ 1)d .\nProof. By the law of total expectation,\nP (w1, . . . , wp \u2208 x) = 1\nd d\u2211 s=1 P (w1, . . . , wp \u2208 x|s)\n= 1\nd d\u2211 s=1 (d\u2212 s)! (d\u2212 s\u2212 p)! \u00b7 (d\u2212 p)! d! ,\nwhere we used Lemma 4 in the last display. By the hockey-stick identity (Ross, 1997), we have\nd\u2211 s=1 ( d\u2212 s p ) = d\u22121\u2211 s=p ( s p ) = ( d p+ 1 ) .\nWe deduce that d\u2211 s=1 (d\u2212 s)! (d\u2212 s\u2212 p)! = d! (p+ 1) \u00b7 (d\u2212 p\u2212 1)! . (47)\nWe deduce that\nP (w1, . . . , wp \u2208 x) = 1\nd (d\u2212 p)! d! d\u2211 s=1 (d\u2212 s)! (d\u2212 s\u2212 p)!\n= 1\nd (d\u2212 p)! d!\nd!\n(p+ 1) \u00b7 (d\u2212 p\u2212 1)! (by Eq. (47))\nP (w1, . . . , wp \u2208 x) = d\u2212 p\n(p+ 1)d ."
    },
    {
      "heading": "6 Additional experiments",
      "text": "In this section, we present additional experiments. We collect the experiments related to decision trees in Section 6.1 and those related to linear models in Section 6.2.\nSetting. All the experiments presented here and in the paper are done on Yelp reviews (the data are publicly available at https://www.kaggle.com/omkarsabnis/yelp-reviews-dataset). For a given model f , the general mechanism of our experiments is the following. For a given document \u03be containing d distinct words, we set a bandwidth parameter \u03bd and a number of new samples n. Then we run LIME nexp times on \u03be, with no feature selection procedure (that is, all words belonging to the local dictionary receive an explanation). We want to emphasize again that this is the only difference with the default implementation. Unless otherwise specified, the parameters of LIME are chosen by default, that is, \u03bd = 0.25 and n = 5000. The number of experiments nexp is set to 100. The whisker boxes are obtained by collecting the empirical values of the nexp runs of LIME: they give an indication as to the variability in explanations due to the sampling of new examples. Generally, we report a subset of the interpretable coefficients, the other having near zero values.\nLet us explain briefly how to read these whisker boxes: to each word corresponds a whisker box containing all the nexp values of interpretable coefficients provided by LIME (\u03b2\u0302j in our notation). The horizontal dark lines\nmark the quartiles of these values, and the horizontal blue line is the median. On top of these experimental results, we report with red crosses the values predicted by our analysis (\u03b2fj in our notation).\nThe Python code for all experiments is available at https://github.com/dmardaoui/lime_text_theory. We encourage the reader to try and run the experiments on other examples of the dataset and with other parameters."
    },
    {
      "heading": "6.1 Decision trees",
      "text": "In this section, we present additional experiments for small decision trees. We begin by investigating the influence of \u03bd and n on the quality of our theoretical predictions.\nInfluence of the bandwidth. Let us consider the same example \u03be and decision tree as in the paper. In particular, the model f is written as\n1\u201cfood\u201d + (1\u2212 1\u201cfood\u201d) \u00b7 1\u201cabout\u201d \u00b7 1\u201cEverything\u201d .\nWe now consider non-default bandwidths, that is, bandwidths different than 0.25. We present in Figure 12 the results of these experiments. In the left panel, we took a smaller bandwidth (\u03bd = 0.05) and in the right panel a larger bandwidth (\u03bd = 0.35). We see that while the numerical value of the coefficients changes slightly, their relative order is preserved. Moreover, our theoretical predictions remain accurate in that case, which is to be expected since we did not resort to any approximation in this case. Interestingly, the empirical results for small \u03bd seem more spread out, as hinted by Theorem 2.\nInfluence of the number of samples. Keeping the same model and example to explain as above, we looked into non-default number of samples n. We present in Figure 13 the results of these experiments. We took a very small n in the left panel (n = 50 is two orders of magnitude smaller than the default n = 5000) and a larger n in the right panel. As expected, when n is larger, the concentration around our theoretical predictions is even better. To the opposite, for small n, we see that the explanations vary wildly. This is materialized by much wider whisker boxes. Nevertheless, to our surprise, it seems that our theoretical predictions still contain some relevant information in that case.\nInfluence of depth. Finally, we looked into more complex decision trees. The decision rule used in Figure 14 is given by\n1\u201cfood\u201d + (1\u2212 1\u201cfood\u201d)1\u201cabout\u201d1\u201cEverything\u201d + 1\u201cbad\u201d + 1\u201cbad\u201d1\u201ccharacter\u201d .\nWe see that increasing the depth of the tree is not a problem from a theoretical point of view. It is interesting to see that words used in several nodes for the decision receive more weight (e.g., \u201cbad\u201d in this example)."
    },
    {
      "heading": "6.2 Linear models",
      "text": "Let us conclude this section with additional experiments for linear models. As in the paper, we consider an arbitrary linear model\nf(\u03c6(x)) = d\u2211 j=1 \u03bbj\u03c6(x)j .\nIn practice, the coefficients \u03bbj are drawn i.i.d. according to a Gaussian distribution.\nInfluence of the bandwidth. As in the previous section, we start by investigating the role of the bandwidth in the accuracy of our theoretical predictions. We see in the right panel of Figure 15 that taking a larger bandwidth does not change much neither the explanations nor the fit between our theoretical predictions and the empirical results. This is expected, since our approximation (Eq. (42)) is based on the large bandwidth approximation. However, the left panel of Figure 15 shows how this approximation becomes dubious when the bandwidth is small. It is interesting to note that in that case, the theory seems to always overestimate the empirical results, in absolute value. The large bandwidth approximation is definitely a culprit here, but it could also be the regularization coming into play. Indeed, the discussion at the end of Section 2.4 in the paper that lead us to ignore the regularization is no longer valid for a small \u03bd. In that case, the \u03c0is can be quite small and the first term in Eq. (5) of the paper is of order e\u22121/(2\u03bd 2)n instead of n.\nInfluence of the number of samples. Now let us look at the influence of the number of perturbed samples. As in the previous section, we look into very small values of n, e.g., n = 50. We see in the left panel of Figure 16 that, as expected, the variability of the explanations increases drastically. The theoretical predictions seem to overestimate the empirical results in absolute value, which could again be due to the regularization beginning to play a role for small n, since the discussion in Section 2.4 of the paper is only valid for large n.\nInfluence of d. To conclude this section, let us note that d does not seem to be a limiting factor in our analysis. While Theorem 2 hints that the concentration phenomenon may worsen for large d, as noted before in Remark 2, we have reason to suspect that it is not the case. All experiments presented on this section so far consider an example whose local dictionary has size d = 29. In Figure 17 we present an experiment on an example that has a local dictionary of size d = 52. We observed no visible change in the accuracy of our predictions."
    }
  ],
  "title": "An Analysis of LIME for Text Data",
  "year": 2020
}
