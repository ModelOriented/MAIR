{
  "abstractText": "Machine learning (ML) systems across many application areas are increasingly demonstrating performance that is beyond that of humans. In response to the proliferation of such models, the field of Explainable AI (XAI) has sought to develop techniques that enhance the transparency and interpretability of machine learning methods. In this work, we consider a question not previously explored within the XAI and ML communities: Given a computational system whose performance exceeds that of its human user, can explainable AI capabilities be leveraged to improve the performance of the human? We study this question in the context of the game of Chess, for which computational game engines that surpass the performance of the average player are widely available. We introduce the RationaleGenerating Algorithm, an automated technique for generating rationales for utility-based computational methods, which we evaluate with a multi-day user study against two baselines. The results show that our approach produces rationales that lead to statistically significant improvement in human task performance, demonstrating that rationales automatically generated from an AI\u2019s internal task model can be used not only to explain what the system is doing, but also to instruct the user and ultimately improve their task performance.",
  "authors": [
    {
      "affiliations": [],
      "name": "Devleena Das"
    },
    {
      "affiliations": [],
      "name": "Sonia Chernova"
    }
  ],
  "id": "SP:cf9f06d43be2f7fbc8afca8175525e8993282cda",
  "references": [
    {
      "authors": [
        "Ashraf Abdul",
        "Jo Vermeulen",
        "Danding Wang",
        "Brian Y Lim",
        "Mohan Kankanhalli"
      ],
      "title": "Trends and trajectories for explainable, accountable and intelligible systems: An hci research agenda",
      "venue": "In Proceedings of the 2018 CHI conference on human factors in computing systems",
      "year": 2018
    },
    {
      "authors": [
        "Amina Adadi",
        "Mohammed Berrada"
      ],
      "title": "Peeking inside the black-box: A survey on Explainable Artificial Intelligence (XAI)",
      "venue": "IEEE Access",
      "year": 2018
    },
    {
      "authors": [
        "Michael Bain",
        "Stephen Muggleton"
      ],
      "title": "Learning optimal chess strategies",
      "venue": "In Machine intelligence",
      "year": 1994
    },
    {
      "authors": [
        "Jacky Bourgeois",
        "Janet Van Der Linden",
        "Gerd Kortuem",
        "Blaine A Price",
        "Christopher Rimmer"
      ],
      "title": "Conversations with my washing machine: an inthe-wild study of demand shifting with self-generated energy",
      "venue": "In Proceedings of the 2014 ACM International Joint Conference on Pervasive and Ubiquitous Computing",
      "year": 2014
    },
    {
      "authors": [
        "Kristy Elizabeth Boyer",
        "Robert Phillips",
        "Amy Ingram",
        "Eun Young Ha",
        "Michael Wallis",
        "Mladen Vouk",
        "James Lester"
      ],
      "title": "Investigating the relationship between dialogue structure and tutoring effectiveness: a hidden Markov modeling approach",
      "venue": "International Journal of Artificial Intelligence in Education",
      "year": 2011
    },
    {
      "authors": [
        "Rich Caruana",
        "Yin Lou",
        "Johannes Gehrke",
        "Paul Koch",
        "Marc Sturm",
        "Noemie Elhadad"
      ],
      "title": "Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission",
      "venue": "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM,",
      "year": 2015
    },
    {
      "authors": [
        "Neil Charness",
        "Michael Tuffiash",
        "Ralf Krampe",
        "Eyal Reingold",
        "Ekaterina"
      ],
      "title": "The role of deliberate practice in chess expertise",
      "venue": "Vasyukova",
      "year": 2005
    },
    {
      "authors": [
        "Keith Cheverst",
        "Hee Eon Byun",
        "Dan Fitton",
        "Corina Sas",
        "Chris Kray",
        "Nicolas Villar"
      ],
      "title": "Exploring issues of user model transparency and proactive behaviour in an office environment control system",
      "venue": "User Modeling and User-Adapted Interaction",
      "year": 2005
    },
    {
      "authors": [
        "Enrico Costanza",
        "Joel E Fischer",
        "James A Colley",
        "Tom Rodden",
        "Sarvapali D Ramchurn",
        "Nicholas R Jennings"
      ],
      "title": "Doing the laundry with agents: a field trial of a future smart energy system in the home",
      "venue": "In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems",
      "year": 2014
    },
    {
      "authors": [
        "Anind K Dey"
      ],
      "title": "Context-Aware Computing. In Ubiquitous computing fundamentals",
      "year": 2018
    },
    {
      "authors": [
        "Upol Ehsan",
        "Pradyumna Tambwekar",
        "Larry Chan",
        "Brent Harrison",
        "Mark O Riedl"
      ],
      "title": "Automated rationale generation: a technique for explainable AI and its effects on human perceptions",
      "venue": "In Proceedings of the 24th International Conference on Intelligent User Interfaces",
      "year": 2019
    },
    {
      "authors": [
        "Shi Feng",
        "Jordan Boyd-Graber"
      ],
      "title": "What can ai do for me?: evaluating machine learning interpretations in cooperative play",
      "venue": "In Proceedings of the 24th International Conference on Intelligent User Interfaces",
      "year": 2019
    },
    {
      "authors": [
        "Ruth C Fong",
        "Andrea Vedaldi"
      ],
      "title": "Interpretable explanations of black boxes by meaningful perturbation",
      "venue": "In Proceedings of the IEEE International Conference on Computer",
      "year": 2017
    },
    {
      "authors": [
        "Riccardo Guidotti",
        "Anna Monreale",
        "Salvatore Ruggieri",
        "Franco Turini",
        "Fosca Giannotti",
        "Dino Pedreschi"
      ],
      "title": "A Survey of Methods for Explaining Black Box Models",
      "venue": "ACM Comput. Surv",
      "year": 2018
    },
    {
      "authors": [
        "David Gunning"
      ],
      "title": "Explainable artificial intelligence (xai). Defense Advanced Research Projects Agency (DARPA), nd Web 2 (2017)",
      "year": 2017
    },
    {
      "authors": [
        "David Gunning",
        "David W Aha"
      ],
      "title": "DARPA\u2019s Explainable Artificial Intelligence Program",
      "venue": "AI Magazine 40,",
      "year": 2019
    },
    {
      "authors": [
        "Mohanad M Hilles",
        "Samy S Abu Naser"
      ],
      "title": "Knowledge-based Intelligent Tutoring System for Teaching",
      "venue": "Mongo Database",
      "year": 2017
    },
    {
      "authors": [
        "Feng-hsiung Hsu"
      ],
      "title": "IBM\u2019s deep blue chess grandmaster chips",
      "venue": "IEEE Micro 19,",
      "year": 1999
    },
    {
      "authors": [
        "Daniel S Kermany",
        "Michael Goldbaum",
        "Wenjia Cai",
        "Carolina CS Valentim",
        "Huiying Liang",
        "Sally L Baxter",
        "Alex McKeown",
        "Ge Yang",
        "Xiaokang Wu",
        "Fangbing Yan"
      ],
      "title": "Identifying medical diagnoses and treatable diseases by image-based deep learning",
      "venue": "Cell 172,",
      "year": 2018
    },
    {
      "authors": [
        "Pang Wei Koh",
        "Percy Liang"
      ],
      "title": "Understanding Black-Box Predictions via Influence Functions",
      "venue": "In Proceedings of the 34th International Conference on Machine Learning - Volume",
      "year": 2017
    },
    {
      "authors": [
        "Himabindu Lakkaraju",
        "Stephen H Bach",
        "Jure Leskovec"
      ],
      "title": "Interpretable decision sets: A joint framework for description and prediction",
      "venue": "In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining",
      "year": 2016
    },
    {
      "authors": [
        "Benjamin Letham",
        "Cynthia Rudin",
        "Tyler H McCormick",
        "David Madigan"
      ],
      "title": "Interpretable classifiers using rules and bayesian analysis: Building a better stroke prediction model",
      "venue": "The Annals of Applied Statistics 9,",
      "year": 2015
    },
    {
      "authors": [
        "Ali O Mahdi",
        "Mohammed I Alhabbash",
        "Samy S Abu Naser"
      ],
      "title": "An intelligent tutoring system for teaching advanced topics in information security",
      "year": 2016
    },
    {
      "authors": [
        "Nick Montfort",
        "Ian Bogost"
      ],
      "title": "Racing the beam: The Atari video computer system",
      "year": 2009
    },
    {
      "authors": [
        "Marco Tulio Ribeiro",
        "Sameer Singh",
        "Carlos Guestrin"
      ],
      "title": "Why should i trust you?: Explaining the predictions of any classifier",
      "venue": "In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining",
      "year": 2016
    },
    {
      "authors": [
        "Herbert Simon",
        "William Chase"
      ],
      "title": "Skill in chess",
      "venue": "In Computer chess compendium",
      "year": 1988
    },
    {
      "authors": [
        "Alistair G Sutcliffe",
        "Frans van Assche",
        "David Benyon"
      ],
      "title": "Domain knowledge for interactive system",
      "year": 2016
    },
    {
      "authors": [
        "Marvin Teichmann",
        "Michael Weber",
        "Marius Zoellner",
        "Roberto Cipolla",
        "Raquel Urtasun"
      ],
      "title": "Multinet: Real-time joint semantic reasoning for autonomous driving",
      "venue": "IEEE Intelligent Vehicles Symposium (IV)",
      "year": 2018
    },
    {
      "authors": [
        "Mike Wu",
        "Michael C Hughes",
        "Sonali Parbhoo",
        "Maurizio Zazzi",
        "Volker Roth",
        "Finale Doshi-Velez"
      ],
      "title": "Beyond sparsity: Tree regularization of deep models for interpretability",
      "venue": "In Thirty-Second AAAI Conference on Artificial Intelligence",
      "year": 2018
    },
    {
      "authors": [
        "Quanshi Zhang",
        "Ruiming Cao",
        "Feng Shi",
        "Ying Nian Wu",
        "Song-Chun Zhu"
      ],
      "title": "Interpreting cnn knowledge via an explanatory graph",
      "venue": "In Thirty-Second AAAI Conference on Artificial Intelligence",
      "year": 2018
    },
    {
      "authors": [
        "Quanshi Zhang",
        "Yu Yang",
        "Haotian Ma",
        "Ying Nian Wu"
      ],
      "title": "Interpreting cnns via decision trees",
      "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
      "year": 2019
    },
    {
      "authors": [
        "Quan-shi Zhang",
        "Song-Chun Zhu"
      ],
      "title": "Visual interpretability for deep learning: a survey",
      "venue": "Frontiers of Information Technology & Electronic Engineering",
      "year": 2018
    }
  ],
  "sections": [
    {
      "text": "CCS CONCEPTS \u2022 Human-centered computing \u2192 Interaction paradigms.\nKEYWORDS Explainable AI, Machine Learning\nACM Reference Format: Devleena Das and Sonia Chernova. 2020. Leveraging Rationales to Improve Human Task Performance. In 25th International Conference on Intelligent User Interfaces (IUI \u201920), March 17\u201320, 2020, Cagliari, Italy. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/3377325.3377512"
    },
    {
      "heading": "1 INTRODUCTION",
      "text": "Machine learning (ML) systems across many application areas are increasingly demonstrating performance that is beyond that of humans. From games, such as Chess [19] and Atari [25], to high-risk applications, such as autonomous driving [30] and medical diagnosis [20], human users are becoming increasingly surpassed by,\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. IUI \u201920, March 17\u201320, 2020, Cagliari, Italy \u00a9 2020 Association for Computing Machinery. ACM ISBN 978-1-4503-7118-6/20/03. . . $15.00 https://doi.org/10.1145/3377325.3377512\nand reliant on, autonomous systems. In response to the proliferation of such models, the field of Explainable AI (XAI) has sought to develop techniques that enhance the transparency and interpretability of machine learning methods. XAI approaches have included taking advantage of intrinsically interpretable models (e.g., decision sets) [22, 23], as well as developing interpretable approximations to explain the behavior of non-interpretable black-box models (e.g., decision tree approximations of deep neural networks) [21, 26]. The vast majority of XAI research has focused on expert users, for example, medical personnel evaluating the decision-making capability of an automated diagnosis system [2, 26].\nIn this work, we consider a question not previously explored within the XAI and ML communities: Given a computational system whose performance exceeds that of its human user, can explainable AI capabilities be leveraged to improve the performance of the human? In other words, can the increasingly powerful machine learning systems that we develop be used to, in turn, further human capabilities? Within the context of XAI, we seek not to explain to the user the inner workings of some algorithm, but to instead communicate the rationale behind a given decision or choice. The distinction between explanation and rationale has been previously introduced by Ehsan et al. [11], who defined explanations as a way to expose the inner workings of a computational model through any communication modality (e.g., visual heatmap [13]), often in a way that is accessible only to experts. Rationales, on the other hand, are defined as natural language explanations that do not literally expose the inner workings of an intelligent system, but instead provide contextually appropriate natural language reasons. These natural language reasons are accessible and intuitive to non-experts (e.g., \u201cI had to go forward to avoid the red vehicle.\u201d), facilitating understanding and communicative effectiveness. Within the context of their work, Ehsan et al. introduced a computational method for automatically generating rationales and validated a set of human factors that influence human perception and preferences (i.e., contextual accuracy, intelligibility, awareness, reliability, strategic detail) [11].\nIn this work, we explore whether providing human users with a rationale of an intelligent system\u2019s behavior can lead to improvement in the user\u2019s performance. We study this question in the context of the game of Chess, for which computational game engines that surpass the performance of the average player are widely available. Our work makes the following contributions: \u2022 We introduce the Rationale-Generating Algorithm (RGA), an\nautomated technique for generating rationales for utility-based computational methods. \u2022 We study two variants of our approach, one that takes advantage only of the system\u2019s knowledge (RGA), and a second (RGA+) that additionally incorporates human domain expert knowledge.\nar X\niv :2\n00 2.\n04 20\n2v 1\n[ cs\n.A I]\n1 1\nFe b\n20 20\nWe evaluate both techniques in a multi-day user study, comparing against two baselines to measure human learning performance. The results demonstrate that our approach produces rationales that lead to improvement in human task performance in the context of chess endgames. Using winning percentage and percentile rank of player moves as a measure of performance, we observe that the inclusion of domain expert knowledge (RGA+) significantly improves human task performance over both baselines when compared to nonrationale baselines. Additionally, user self-reported performance ratings also show that rationale-based interfaces lead to greater perceived user understanding of the task domains than non-rationale baselines. In summary, our approach is the first to demonstrate that rationales automatically generated from an AI\u2019s internal task model can be used not only to explain what the system is doing, but also to instruct users in a manner that ultimately improves their own task performance in the absence of rationales."
    },
    {
      "heading": "2 RELATED WORKS",
      "text": "Traditionally, in the XAI community, an interpretable model can be described as one from which an AI expert or user can deduce model performance based on given inputs [11]. The methods of interpretability can vary based on the complexity of a model and a wide range of survey papers summarize the different XAI models currently developed [2, 26, 34]. While some of the existing models are inherently interpretable, suitable for model-intrinsic decision-making [6, 23], other complex models need model-agnostic approaches for interpretability [15, 31\u201333]. Despite the differing approaches for interpretability, XAI models have the common motivation of improving human understanding of AI systems and building trust with these systems [16, 17].\nOne existing method for developing interpretability is to use intrinsic models in which interpretability is embedded inside the model. In [23] a Bayesian Rule List (BRL) is created to produce posterior distributions over permutations of \u2019if, then, else\u2019 rules for a single prediction classification. Since these decision lists are intrinsically understandable, using them as the basis of BRL is successful in developing interpretability. However, the accuracy of BRL depends highly on the Bayesian prior favoring concise decision lists and a small number of total rules, limiting the applicability of this approach. To extend the applicability of model-intrinsic methods beyond the scope of concise decision lists, a generative additive model (GAM) is created to provide both high accuracy (better than random forest, logitboost and SVMs) and high interpretability for a single prediction classification [6]. To achieve model understanding, GAM creates a visual representation of the pairwise factors that result in a prediction and provides an ability for modular testing. [6] refers to modular testing as allowing model-experts to easily remove and insert individual factors or pairwise factors to examine their effects on a prediction. While methods such as GAM provide interpretability for regression and some classification models, the intrinsic nature of its model makes it hard to provide interpretability for more black-box models which need model-agnostic implementations of interpretability.\nAn alternative to the model-intrinsic approach described above is a model-agnostic method for interpretability. Model-agnostic methods are a significant focus within the XAI community since modelagnostic methods provide high performance accuracy, but do not\nallow for inherent decoding and visualization of model prediction. Most model-agnostic methods produce interpretability by developing surrogate models, post-hoc implementations of interpretability derived from inherently interpretable models. In [15] an ad-hoc genetic algorithm is used to generate neighbor nodes for a specific local instance, which is then trained by a decision tree classifier to generate a logic rule. This logic rule represents the path in the decision tree that explains the factors that lead to a prediction. Additionally, [33] uses inherently explainable decision tree models to learn CNN layers in order to provide CNN rationales at the semantic level. [33] defines a CNN rationale as an ability to quantify the set of objects that contribute to a CNN prediction from prediction scores. [32] also uses the concepts of explanatory graphs to visually reveal the hierarchy of knowledge inside CNNs in which each node summarizes the knowledge in the feature maps of a corresponding conv-layer. However, the decision tree rationales and explanatory graphs only provide quantitative distributions and visualizations to domain-experts, and are not interpretable to general users without domain knowledge. Moreover, the complexity of these decision trees vary greatly based on the complexity of the model, sometimes no longer staying interpretable due to a large node space. Thus, [31] creates a tree regularization penalty function that helps produce interpretability of complex models by using moderately-sized decision trees. Altogether though, the focus of the intrinsic and agnostic methodologies remain largely focused on giving insight into the inner workings of the AI systems, rather than providing humanly understandable rationales that extend beyond domain-expert usability.\nThe HCI community acknowledges the gap between XAI and the human usability of XAI, communicating that the explanations from AI and ML communities do not yet possess a large-scale efficacy on human users [1]. To address the importance of user-friendly explanations for any user, not only domain-experts, researchers in HCI have developed context-aware rules to focus interest on easyto-use interfaces that are aware of their environment and context of use [10]. However, many of these existing context-aware rules are developed as frameworks to aid decision-making in domainspecific applications such as smart homes [4, 9] and the office [8], instead of utilizing a model-agnostic approach encouraged by the AI/ML community. In addition to the context-aware rules used above, educators have focused on intelligent tutoring systems (ITS) [5, 18, 24] to generate explanations for learning. While we similarly seek to improve the performance of a non-expert user, our work differs from tutoring systems in that the rationales produced by our system are generated based on and reflect an AI\u2019s internal computational model, not a prescribed curriculum.\nIn the context of exploring effective methods of representing AI model predictions to human users, [12] investigates the helpfulness of different factors that can help interpret an AI decision. The author focuses on methods such as presenting evidence examples, highlighting important input features and visualizing uncertainty in the context of the trivia game Quizbowl. Human performance improvement is analyzed by the time taken to respond to a question and the accuracy of an answer. However, since improvement is always measured in the presence of interpretations, their work does not provide insight on whether long-term human knowledge and task performance increase in the absence of the assistance provided by the interface.\nTo address a more generic solution for generating human understandable explanations from AI/ML produced models outside of a specified curriculum, [11] develops human understandable rationale in the context of the game Frogger. The rationale generation uses natural language processing to contextualize state and action representations for a reinforcement learning system and studies the human factors that influence explanation preferences. The metrics for measuring interpretability of the rationales are primarily focused on perceived understanding of the rationale. In contrast, we generate rationales and measure task performance and self-perceived task performance improvement, as opposed to surveying understanding of a given rationale."
    },
    {
      "heading": "3 RATIONALE GENERATING ALGORITHM",
      "text": "XAI systems can be used to aid users in a decision-making process, such as medical diagnosis [20]. In this work, we introduce the Rationale Generating Algorithm (RGA), which provides the user with rationales designed to aid the user\u2019s decision-making process while also increasing their understanding of the underlying task domain. Figure 1 presents an overview of the RGA pipeline. For a given task, we assume an expert AI Engine is available that takes in the current task state x, and outputs a recommended action a and its associated utility function u1. In context of this work, a utility function is defined as a weighted sum of all variables used to decide an output. Within RGA, the utility function is decomposed to identify the most significant factors contributing to the selection of a over alternate actions. These factors take the form of variables, typically not very interpretable in their original form (e.g. \u2018PassedPawns = 0.7\u2019). Additional factors that support decision making may also be obtained from provided expert domain knowledge (see below). While a utility function may be made up of dozens of variables, RGA selects only the top k factors, which are then used to generate a human-readable rationale. Note that [14] has shown that in the context of an explainable planner, generating justifications to explain both good and bad action choices leads to a more robust explainable system. Motivated 1The presented variant of RGA requires a normalized utility function to generate rationales. Our objective in this work is to evaluate the effect of rationales on user performance, thus we did not focus on developing a fully model-agnostic rationale generation technique. In general, approaches such as [11] can be used to generate rationales for models that do not incorporate a utility function\nAlgorithm 1 generateExp( u, a, d) Input : u - utility function, a - selected action, d- domain\nknowledge Output : r - rationale\n1: P=[namep:{}, wp:{}] 2: D=[named :{}, wd :{}]\n3: P= decomposeUtility(u)\n4: if d is not \u00d8 then 5: D = decomposeDomainKnowledge(d) 6: end if\n7: Factors = P\u222aD 8: Factors.sortByWeight() 9: for f in Factors[i : k] do\n10: if isPositive( f ) then 11: r.append(genPos( f .name, a)) 12: else 13: r.append(genNeg( f .name, a)) 14: end if 15: end for 16: return r\nby this, RGA is able to generate rationales for both positively and negatively contributing actions.\nAlgorithm 1 further details the manner in which RGA is implemented and a final rationale is chosen. Variables P and D (lines 1-2) store the name of each decision factor contributing to the selection of a given action, and its associated weight for utility-based (wp) and domain-based (wd) knowledge, respectively. Given the utility function u used to select the given action a, RGA first decomposes u to obtain the list of all factors involved in the determination of the action (namep) and their contributing normalized weight (wp). The resulting list is stored in P (line 3).\nRGA next optionally processes the domain knowledge, if it is available (lines 4-6). We define domain knowledge as any externally available data, typically encoded a priori by a domain expert [29]. The domain knowledge is encoded as a supplemental utility function d and used to supplement RGA with information that is not encoded in u but might be helpful to include in a rationale to aid human understanding. For example, within the Chess domain we found that the default u generated by our game engine did not include a variable for \u2019Checkmate\u2019 (an important winning condition in chess), whereas providing information about a checkmate within a rationale would likely be helpful to the user. As a result, we include the concept of domain knowledge, and in our experimental section compare RGA performance with and without domain knowledge. If domain knowledge is present, RGA decomposes it similarly to the utility function into a list of factor names named and their associated weights wd (line 5).\nOnce decision factors from the utility function and the domain knowledge are identified, the weighted lists for both sets of factors are merged and sorted by weight in descending order (lines 7-8). The top k factors are then used to generate a rationale, with appropriate wording being dependent on whether the factor positively or negatively contributes to the task objective. For each of the k features\nthat aid in task completion, a positive rationale is scripted (e.g.\u2019A boat can help cross the river because it floats on water\u2019). If the top k factors represent negative contributions to the objective, then a negative rationale is scripted (e.g. \u2019A car cannot help cross the river because it does not float on water\u2019).\nOnce a rationale is returned by RGA, we display it to the user along with the recommended action a selected by the AI Engine. The user may then leverage the rationale, and their own knowledge of the domain, to decide whether to perform a or to select a different action. As shown in Figure 1, the user\u2019s final selected action a\u0302, which may be the same or different from a, is then applied back to the task domain."
    },
    {
      "heading": "4 RGA AND CHESS",
      "text": "To measure the effect of human-understandable rationales on task performance, we apply RGA to the game of chess, specifically focusing our attention on endgame configurations, defined by no more than 12 pieces on the board. Chess is a good application for RGA as the utility function for chess is complex in nature, involving many parameters, and the game environment continuously changes over time. We focus specifically on endgame scenarios in which decision making is crucial to an outcome, and there is a relatively small amount of moves left. With endgame scenarios, we are able to analyze the effects of RGA through a measured experimental design.\nFor this research, we use the the utility function from the open source Chess AI Engine, Stockfish [28], and utilize RGA to generate human-understandable rationale for both an optimal and any non optimal moves taken. This section discusses RGA within the chess domain."
    },
    {
      "heading": "4.1 Chess Utility Function",
      "text": "The utility function from Stockfish includes over 100 utility factors that are combined in a multi-level hierarchical fashion. While all of these factors can be crucial to explaining different moves within a chess board configuration, only a subset of these main factors are useful for explaining moves in context of endgame configurations [3]. The relevant endgame utility factors are determined after sorting all factors by maximum weight (line 8 of Algorithm 1). They include but\nare not limited to: \u2019Mobility\u2019, \u2019KingDanger\u2019, \u2019King\u2019, \u2019HangingPiece\u2019, \u2019PawnPromotion\u2019, and \u2019Passed\u2019.\nAs a brief summary, \u2019Mobility\u2019 refers to the number of legal moves a player has for a given position\u2013the more choices a player has, the stronger their position on the board. Also for endgames, the king is a powerful piece and is preferred to keep in the center of the board due to its limited range; \u2019KingDanger\u2019 is the highest weighted feature that affects the \u2019King\u2019 score. Additionally, \u2019Threats\u2019 play a huge role in the outcome of an endgame, and are primarily represented by crucially attacking pieces such as rooks and kings, and by \u2019HangingPiece\u2019 which refers to weak enemy pieces not defended by the opponent. \u2019Passed\u2019 refers to the concept of \u2019Passed Pawns\u2019 which checks to see if a pawn can continue to safely advance to the other side of the board. \u2019Passed\u2019, includes the feature \u2019PawnPromotion\u2019 which details when a pawn has reached the opponent\u2019s top rank, allowing the player to switch its pawn out for a queen, rook, bishop or knight. These described utility factors represent the utility factors that we deemed appropriate from the Stockfish utility function, based on their weights, to use in justifying a non-optimal or optimal move choice."
    },
    {
      "heading": "4.2 Data Pre-Processing",
      "text": "To meet RGA\u2019s requirement of having a normalized utility function input, we standardize the Stockfish utility function using Z-score scaling. For Z-score scaling, we perform a heuristic ad-hoc analysis by collecting average standard deviations and means for each relevant factor over several game configurations. Specifically, we collect 120 game configurations using a random FEN generator. To ensure completeness, we represent an equal distribution of game configurations within the range of 2-32 pieces. We started with 30 random game configurations, and doubled them twice, until the change in both average standard deviation and mean for each factor was negligible."
    },
    {
      "heading": "4.3 RGA In Chess",
      "text": "In the context of chess, RGA generates human understandable rationales with the objective of checkmating the opponent. As portrayed in Figure 1, the Stockfish AI engine generates a recommended action for the current board state x, and its associated utility u. In specific relation to chess, Figure 2 depicts the overall interaction between the Chess Engine, RGA, and the user. For each given board configuration, all relevant factors, actions and utilities are updated before generating a rationale. These utilities are sorted to find the highest utility ubest and corresponding optimal move abest , which RGA uses to generate a rationale to the user to justify the optimal move he/she should take. Additionally, we detect non-optimal moves by comparing the user\u2019s proposed action to the set of all possible actions A for the given board configuration; if the proposed action falls in the bottom 1/3 of A, then we use the factors in Factors (line 7 of Algorithm 1) to generate a cautionary rationale justifying why a user should not make the proposed action. The user can ultimately decide upon the final action, considering or disregarding the rationales produced by RGA. Figure 3 provides an example of a best-move and a non-optimal move explanation using both a possible Stockfish and domain-knowledge factor."
    },
    {
      "heading": "4.4 Domain Knowledge Factors",
      "text": "In the application of chess, we use domainKnowledge to adds three important criteria that the Stockfish utility function does not explicitly represent: (1) explicit piece capture on next move, (2) check on next move, (3) checkmate on next or subsequent move. Considering the objective of chess, we weigh these additional domain-knowledge factors higher than those from the utility function. Shown in Figure 2, we distinguish RGA+ to be a superset of RGA. The overarching RGA+ denotes rationales that are reasoned from both domain knowledge and the chess utility function, whereas the RGA requires a utility function for rationale generation, but leaves domain knowledge inputs as optional to domain experts."
    },
    {
      "heading": "5 EXPERIMENTAL DESIGN",
      "text": "To evaluate the effectiveness of RGA in improving task performance, we conducted a four-way between-subjects user study in which participants took part in chess gaming sessions over three consecutive days. We selected a multi-day study design to ensure observation of longer-term learning effects. Given the complexity of chess, which requires an estimated 10 years [27] or 5000 hours [7] to master, we conducted our study using only simplified game scenarios consisting of end games.\nThe study design consisted of the following four study conditions, which determine what guidance was provided to the participant: \u2022 None (baseline): The player receives no hints or rationales. This\ncondition is equivalent to practicing chess independently with no guidance. (Figure 4(a)) \u2022 Hints (baseline): The player receives a visual hint highlighting in color the best currently available move, as determined by the game engine utility function. No textual rationales are provided beyond the highlighting. This condition is equivalent to the hints system available in the vast majority of online and app-based chess programs. (Figure 4(b)) \u2022 RGA: The player receives a visual hint highlighting in color the best currently available move (as in Hints). Additionally, the system displays a textual rationale based the Stockfish utility function only. (Figure 4(c))\n\u2022 RGA+: The player receives a visual hint highlighting in color the best currently available move (as in Hints). Additionally, the system displays a textual rationale based on both the Stockfish utility function and domain knowledge. (Figure 4(c))\nThe sections below further detail our evaluation metrics, study hypotheses, participant recruitment method, and study design."
    },
    {
      "heading": "5.1 Metrics",
      "text": "Each chess session consisted of diagnostic games, during which participants were evaluated on their performance and received no suggestions or hints, and instructional games, during which participants received guidance according to their assigned study condition. During diagnostic games, the following metrics were used to evaluate performance: \u2022 Win Percentage (Win%): a metric commonly used in sports\nthat takes into account the number of wins, losses and ties. In our domain, we additionally account for cases in which the maximum number of allowed moves has been reached2, maxmoves, which are weighted the same as ties. The final win percentage is calculated as:\nWin% = wins+0.5 \u00b7 ties+0.5 \u00b7maxmoves wins+ ties+ losses+maxmoves\n(1)\n\u2022 Percentile Rank (Percentile): a metric used to measure the distribution of scores below a certain percentage. In our domain, the distribution of scores is the move rating of all possible moves for a given board configuration. The move ratings are calculated by Stockfish. For each board configuration, we use the following formula to calculate the percentile rank of a chosen move k with N corresponding to all possible moves:\nPercentileRank = 100 \u00b7\n( k\u22121 )( N \u22121\n) (2) Additionally, at the end of each study day, participants were given a short post-session questionnaire from which we obtain the following metric: 2We limit the total number of moves to prevent novice players from moving around the board indefinitely.\n\u2022 Perceived Performance (SelfEval): a metric that seeks to capture the participants\u2019 self-reported perceived progress toward learning chess. Perceived performance is measured using a 5- Point Likert Scale rating based on the question \u2018Do you believe your performance improved this session?\u2019 (1 = Strongly disagree, 5 = Strongly agree\u2019)"
    },
    {
      "heading": "5.2 Hypotheses",
      "text": "We formulate the following hypotheses on the ability of interpretable rationales to improve participants\u2019 chess performance defined by the Win% and Percentile Rank metrics: \u2022 H1a: Participants who received only utility-based rationales\n(RGA) will perform better than those who received no guidance (None). \u2022 H1b: Participants who received only utility-based rationales (RGA) will perform better than participants who received only suggestions (Hints). \u2022 H1c: Participants who received rationales that incorporate domain knowledge (RGA+) will perform better than participants who received no guidance (None). \u2022 H1d: Participants who received rationales that incorporate domain knowledge (RGA+) will perform better participants who received only suggestions (Hints). \u2022 H1e: Participants who received rationales that incorporate domain knowledge (RGA+) will perform better than participants who received only utility-based rationales (RGA).\nAdditionally, we hypothesize that participants\u2019 measure of their own perceived performance, evaluated by the SelfEval metric, will follow similar trends as above. Specifically: \u2022 H2a: Participants who received only utility-based rationales\n(RGA) will have higher perceived performance ratings than those who received no guidance (None). \u2022 H2b: Participants who received only utility-based rationales (RGA) will have higher perceived performance ratings than those who received only suggestions (Hints). \u2022 H2c: Participants who received rationales that incorporate domain knowledge (RGA+) will have higher perceived performance than participants who received no guidance (None)\n\u2022 H1d: Participants who received rationales that incorporate domain knowledge (RGA+) will have higher perceived performance than who received only suggestions (Hints). \u2022 H2e: Participants who received rationales that incorporate domain knowledge (RGA+) will have higher perceived performance ratings than those who received only utility-based rationales (RGA)."
    },
    {
      "heading": "5.3 Participants",
      "text": "We recruited 68 participants from Amazon\u2019s Mechanical Turk. Participants were required to demonstrate basic knowledge of chess by passing a short test verifying the rules of the game (e.g.\u2019Which piece can move straight forward, but captures diagonally?\u2019). Participants were also required to participate three days in a row, and to not already be expert players. Eight players were removed from the study for either not participating for three days or for winning every game (suggesting that they were experts to begin with). The final set of participants included 60 individuals (44 males and 16 females), who ranged in age from 18 to 54 (6 between 18-24 years, 31 between 25-34 years, 18 between 35-44, and 3 between 45-54 years). Participants were randomly assigned to one of the four study conditions. Each daily session took approximately 15-20 minutes, and participants were compensated $2.00, $4.00, and $6.00 on days 1, 2 and 3, respectively."
    },
    {
      "heading": "5.4 Study Design",
      "text": "The study consisted of three sessions performed on three consecutive days. In each session, participants played 9 games of chess: three diagnostic games, followed by three instructional games, followed by three more diagnostic games. The use of diagnostic games at the beginning and end of each session enabled us to study participant performance both across and within sessions3.\nThe participant always played white, and the opponent black pieces were controlled by the Stockfish Engine AI, which always played optimally. For each board, the optimal number of player moves needed to win, O, was determined by the Stockfish Engine, and participants were limited to 10 moves during the game. Starting 3Our analysis showed no significant trends for within-session performance differences, likely due to the short duration of the sessions. However, we do observe significant learning effects across sessions, as discussed in the Section 6.\nboard configurations were obtained from a popular online learning website (https://lichess.org/), selected such that O < 5. All participants received the same boards in the same order to ensure uniformity; each board was unique and did not repeat. Players were allowed to make any legal chess move, and each game consisted of an average of 6 moves (SD=2.62). As a result, participants in the Hints and RGA conditions received approximately 18 move suggestions per day on average. Furthermore, participants in the RGA condition received one rationale per move suggestion, receiving 18 rationales per day. Participants in the RGA+ conditions occasionally received an additional rationale per move suggestion to denote a possible checkmate in less than three moves."
    },
    {
      "heading": "6 RESULTS",
      "text": "The participant performance data followed a normal distribution; as a result, we used ANOVA with a Tukey HSD post-hoc test to evaluate statistical significance across the experimental conditions with respect to the Win% and PercentileRank metrics. Additionally, we conducted a Mann-Whitney U test to analyze the Likert scale data for the SelfEval metric."
    },
    {
      "heading": "6.1 Participant Performance",
      "text": "Figure 5 presents the average participant win percentage (Win%) for each study condition over the three days. We observe that while no statistical differences are observed between conditions on the first\nday, the differences in performance grow on subsequent days. In particular we see the greatest rate of overall task improvement from Day 1 to Day 3 for the RGA+ condition. Our results indicate a correlation between the amount of explanation participants are given and their Win%, with more justifications leading to more wins. This is further supported by the results in Figure 6, which present the percentile rank (PercentileRank) of each participant\u2019s average move\u2013with a percentile rank of 100 denoting the most optimal move and percentile rank of 0 denoting the least optimal move. Similar to Figure 5, we observe a correlation between the PercentileRank of move ratings and the amount of justifications provided, with the upper quartile for PercentileRank being higher with more explanations.\nOur statistical analysis shows that for both Win% and for PercentileRank, participants in \u2019RGA\u2019 did not perform statistically better than those in \u2019None\u2019, not validating H1a, nor statistically better those in \u2019Hints\u2019, not validating H1b. We do observe that \u2019RGA+\u2019 participants performed significantly better than \u2019None\u2019 participants validating H1c. However, with respect to the \u2019Hints\u2019 condition, \u2019RGA+\u2019 had statistically better Win% performance but not PercentileRank, therefore only partially validating H1d. Finally, we see no significant difference between \u2019RGA+\u2019 and \u2019RGA\u2019 conditions in this study, not validating H1e.\nIn summary, these results indicate that humanly interpretable rationales can improve task performance, as long as the rationales fit a complete representation of the domain. Our results show that gathering both utility-based features and additional domain knowledge features (not represented by the utility) can accomplish completeness. It is also important to note that for task improvement, a decision not only needs to be interpretable, but more importantly humanly interpretable by accompanying rationales. The \u2019Hints\u2019 conditions also provided interpretability by highlighting the best move, but the lack of accompanying rationales may be the cause of why no significance was seen between \u2019Hints\u2019 and \u2019None\u2019. Furthermore the inclusion of domain knowledge in RGA+ significantly improved participant Win% over the baseline conditions, compared to RGA, validating the need for a more complete domain representation. However, for this reason, it is interesting that no Win% significance was seen between \u2019RGA+\u2019 and \u2019RGA\u2019, but in seeing a trend of increasing difference between \u2019RGA+\u2019 and \u2019RGA\u2019, we suspect that over a longer period of time, a visible significance would be seen."
    },
    {
      "heading": "6.2 Participant Perceived Performance",
      "text": "Figure 7 presents the perceived performance rating (SelfEval) of participants in each experimental condition. The Likert scale data shows that participant groups that received more justifications (\u2019RGA+\u2019, \u2019RGA\u2019) had higher ratings of \u2019Agree\u2019 and \u2019Strongly Agree\u2019 than participant groups that received little to no justifications (\u2019Hints\u2019, \u2019None\u2019), showing the value justifications had on SelfEval. Furthermore, participant groups that received some justifications (\u2019RGA\u2019, \u2019Hints\u2019) had more \u2019Neutral\u2019 ratings than participant groups that were on the extreme ends of the justification spectrum (\u2019RGA+\u2019, \u2019None\u2019), showing higher levels of uncertainty in their SelfEval.\nThe Mann-Whitney U tests in Table 1 further detail the specific significance between each experimental condition. As seen in Table 1, the SelfEval of \u2019RGA\u2019 participants were not significantly stronger than \u2019None\u2019 participants\u2019 ratings, not validating H2a, nor\nsignificantly stronger than \u2019Hint\u2019 participant ratings, not validating H2b. However, the \u2019RGA+\u2019 participants did have a statistically higher SelfEval than \u2019None\u2019 participants and \u2019Hints\u2019 participants on all three days, validating H2c and validating H2d respectively. Additionally, from day two onward, \u2019RGA+\u2019 participants rated their perceived performance higher than those from \u2019RGA\u2019, validating H2e.\nOverall, the SelfEval metric data aligns with the performance analysis from Section 6.1, showing that perceived performance ratings are significantly stronger with the presence of humanly interpretable rationales that are domain representative. The results above also portray additional significance not seen with the Win% and PercentileRank metrics. Unlike the trend in Section 6.1, SelfEval does show a significant difference in performance rating between \u2019RGA+\u2019 and \u2019RGA\u2019, reiterating the importance of holistic domain representation. Interestingly, similar to the analysis from Win% and PercentileRank, SelfEval also does not portray a significant difference between \u2019RGA\u2019 and \u2019Hints\u2019, implying that rationales from the utility function alone were not different enough from \u2019Hints\u2019. In Figure 7, we see an increasing difference in \u2019Disagree\u2019 ratings and \u2019Agree\u2019 ratings over Day 1 and Day 3 between \u2019RGA\u2019 and \u2019Hints\u2019, implying that over a longer period of observation, a potential significance could be seen."
    },
    {
      "heading": "7 DISCUSSION AND CONCLUSIONS",
      "text": "In this work, we are the first to explore whether human-interpretable rationales automatically generated based on an AI\u2019s internal task representation can be used to not just explain the AI\u2019s reasoning, but also enable end users to better understand the task itself, thereby leading to improved user task performance. Our work introduces the Rationale-Generating Algorithm that utilizes utility based computational methods to produce rationales understandable beyond the scope of domain-expert users. To validate RGA, we applied it to the domain of chess and measured human task performance using both qualitative user self-reported data (self-perceived performance ratings) and quantitative performance measures (winning percentages and rank percentiles of the strength of moves played by each participant).\nOur results show that rationales from RGA are effective in improving performance when information from the AI\u2019s utility function is combined with additional domain knowledge from an expert. The resulting system was able to statistically significantly improve user performance in chess compared to study participants who practiced the same number of games but did not receive rationales. Simply showing participants the optimal action without an accompanying rationale did not produce the same results, indicating the importance of interpretable rationales in elucidating the task.\nThe presented approach is the first study of how rationales can affect learning. While it contributes a number of important insights, it is also limited in several ways. First, RGA is limited to utilitybased methods and can not be applied to arbitrary machine learning methods. Future work in generating rationales for alternate ML representations, such as reinforcement learning discussed in [11], should be explored. Exploration of such methodologies can lead to developing a rationale generating system that is model agnostic. Second, our work does not compare among the many different ways in which rationales can be phrased or structured. It is beneficial to investigate if and how non-verbal explanations and dynamic forms of explanations can provide better humanly understandable explanations. For example, it would be interesting to see whether visual animations of future chess moves help RGA rationales be as effective as RGA+ rationales as well as the effect visual animations have (applied to the Hints and None category) without RGA. Also, additional research is needed to evaluate how to present rationales in the most accessible and interpretable manner based on individual needs. Currently, RGA is developed to help beginner chess players improve their task performance, but it is worth exploring how to build a learned RGA that can tailor its explanations to varying levels of expertise. Another important area to investigate is the long term effects of RGA. While RGA has been shown as valuable in improving human task performance in a short period of time, it would be beneficial to see whether these trends are upheld over a longer time frame. Learning when the effects of RGA are minimal and when they are maximum can help establish its best time of usage, as well as measure its broader impact in improving human learning and human task performance."
    },
    {
      "heading": "ACKNOWLEDGMENTS",
      "text": "This material is based upon work supported by the NSF Graduate Research Fellowship under Grant No. DGE-1650044, and also in part by NSF IIS 1564080 and ONR N000141612835."
    }
  ],
  "title": "Leveraging Rationales to Improve Human Task Performance",
  "year": 2020
}
