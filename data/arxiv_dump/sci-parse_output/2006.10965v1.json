{
  "abstractText": "Machine learning transparency calls for interpretable explanations of how inputs relate to predictions. Feature attribution is a way to analyze the impact of features on predictions. Feature interactions are the contextual dependence between features that jointly impact predictions. There are a number of methods that extract feature interactions in prediction models; however, the methods that assign attributions to interactions are either uninterpretable, model-specific, or non-axiomatic. We propose an interaction attribution and detection framework called Archipelago which addresses these problems and is also scalable in real-world settings. Our experiments on standard annotation labels indicate our approach provides significantly more interpretable explanations than comparable methods, which is important for analyzing the impact of interactions on predictions. We also provide accompanying visualizations of our approach that give new insights into deep neural networks.",
  "authors": [
    {
      "affiliations": [],
      "name": "Michael Tsang"
    },
    {
      "affiliations": [],
      "name": "Sirisha Rambhatla"
    },
    {
      "affiliations": [],
      "name": "Yan Liu"
    }
  ],
  "id": "SP:c572c994828c931a6d5b670ab651a48c0bf8b821",
  "references": [
    {
      "authors": [
        "Chunrong Ai",
        "Edward C Norton"
      ],
      "title": "Interaction terms in logit and probit models",
      "venue": "Economics letters,",
      "year": 2003
    },
    {
      "authors": [
        "Leona S Aiken",
        "Stephen G West",
        "Raymond R Reno"
      ],
      "title": "Multiple regression: Testing and interpreting interactions",
      "year": 1991
    },
    {
      "authors": [
        "Marco Ancona",
        "Enea Ceolini",
        "Cengiz Oztireli",
        "Markus Gross"
      ],
      "title": "Towards better understanding of gradient-based attribution methods for deep neural networks",
      "venue": "In 6th International Conference on Learning Representations,",
      "year": 2018
    },
    {
      "authors": [
        "William A Belson"
      ],
      "title": "Matching and prediction on the principle of biological classification",
      "venue": "Journal of the Royal Statistical Society: Series C (Applied Statistics),",
      "year": 1959
    },
    {
      "authors": [
        "Jacob Bien",
        "Jonathan Taylor",
        "Robert Tibshirani"
      ],
      "title": "A lasso for hierarchical interactions",
      "venue": "Annals of statistics,",
      "year": 2013
    },
    {
      "authors": [
        "Alexander Binder",
        "Gr\u00e9goire Montavon",
        "Sebastian Lapuschkin",
        "Klaus-Robert M\u00fcller",
        "Wojciech Samek"
      ],
      "title": "Layer-wise relevance propagation for neural networks with local renormalization layers",
      "venue": "In International Conference on Artificial Neural Networks,",
      "year": 2016
    },
    {
      "authors": [
        "Jianbo Chen",
        "Le Song",
        "Martin Wainwright",
        "Michael Jordan"
      ],
      "title": "Learning to explain: An information-theoretic perspective on model interpretation",
      "venue": "In International Conference on Machine Learning,",
      "year": 2018
    },
    {
      "authors": [
        "Muhammad EH Chowdhury",
        "Tawsifur Rahman",
        "Amith Khandakar",
        "Rashid Mazhar",
        "Muhammad Abdul Kadir",
        "Zaid Bin Mahbub",
        "Khandakar R Islam",
        "Muhammad Salman Khan",
        "Atif Iqbal",
        "Nasser Al-Emadi"
      ],
      "title": "Can ai help in screening viral and covid-19 pneumonia",
      "venue": "arXiv preprint arXiv:2003.13145,",
      "year": 2020
    },
    {
      "authors": [
        "Joseph Paul Cohen",
        "Paul Morrison",
        "Lan Dao"
      ],
      "title": "Covid-19 image data collection",
      "venue": "arXiv 2003.11597,",
      "year": 2020
    },
    {
      "authors": [
        "Angela Dean",
        "Max Morris",
        "John Stufken",
        "Derek Bingham"
      ],
      "title": "Handbook of design and analysis of experiments, volume 7",
      "year": 2015
    },
    {
      "authors": [
        "J. Deng",
        "W. Dong",
        "R. Socher",
        "L.-J. Li",
        "K. Li",
        "L. Fei-Fei"
      ],
      "title": "ImageNet: A Large-Scale Hierarchical Image Database",
      "venue": "In CVPR09,",
      "year": 2009
    },
    {
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova. Bert"
      ],
      "title": "Pre-training of deep bidirectional transformers for language understanding",
      "year": 2019
    },
    {
      "authors": [
        "Kedar Dhamdhere",
        "Ashish Agarwal",
        "Mukund Sundararajan"
      ],
      "title": "The shapley taylor interaction index",
      "venue": "arXiv preprint arXiv:1902.05622,",
      "year": 2019
    },
    {
      "authors": [
        "Ronald A Fisher"
      ],
      "title": "On the\u2019probable error\u2019of a coefficient of correlation deduced from a small sample",
      "venue": "Metron, 1:1\u201332,",
      "year": 1921
    },
    {
      "authors": [
        "Ronald Aylmer Fisher"
      ],
      "title": "Statistical methods for research workers",
      "venue": "Genesis Publishing Pvt Ltd,",
      "year": 1925
    },
    {
      "authors": [
        "Ronald Aylmer Fisher"
      ],
      "title": "048: The arrangement of field experiments",
      "year": 1926
    },
    {
      "authors": [
        "Jerome H Friedman",
        "Bogdan E Popescu"
      ],
      "title": "Predictive learning via rule ensembles",
      "venue": "The Annals of Applied Statistics,",
      "year": 2008
    },
    {
      "authors": [
        "Muriel Gevrey",
        "Ioannis Dimopoulos",
        "Sovan Lek"
      ],
      "title": "Two-way interaction of input variables in the sensitivity analysis of neural network models",
      "venue": "Ecological modelling,",
      "year": 2006
    },
    {
      "authors": [
        "Michel Grabisch",
        "Marc Roubens"
      ],
      "title": "An axiomatic approach to the concept of interaction among players in cooperative games",
      "venue": "International Journal of game theory,",
      "year": 1999
    },
    {
      "authors": [
        "Huifeng Guo",
        "Ruiming Tang",
        "Yunming Ye",
        "Zhenguo Li",
        "Xiuqiang He"
      ],
      "title": "Deepfm: a factorization-machine based neural network for ctr prediction",
      "venue": "In Proceedings of the 26th International Joint Conference on Artificial Intelligence,",
      "year": 2017
    },
    {
      "authors": [
        "Michael Hamada",
        "CF Jeff Wu"
      ],
      "title": "Analysis of designed experiments with complex aliasing",
      "venue": "Journal of Quality Technology,",
      "year": 1992
    },
    {
      "authors": [
        "Ning Hao",
        "Hao Helen Zhang"
      ],
      "title": "Interaction screening for ultrahigh-dimensional data",
      "venue": "Journal of the American Statistical Association,",
      "year": 2014
    },
    {
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "title": "Deep residual learning for image recognition",
      "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,",
      "year": 2016
    },
    {
      "authors": [
        "Joseph D Janizek",
        "Pascal Sturmfels",
        "Su-In Lee"
      ],
      "title": "Explaining explanations: Axiomatic feature interactions for deep networks",
      "venue": "arXiv preprint arXiv:2002.04138,",
      "year": 2020
    },
    {
      "authors": [
        "Xisen Jin",
        "Junyi Du",
        "Zhongyu Wei",
        "Xiangyang Xue",
        "Xiang Ren"
      ],
      "title": "Towards hierarchical importance attribution: Explaining compositional semantics for neural sequence models",
      "year": 1911
    },
    {
      "authors": [
        "Gordon V Kass"
      ],
      "title": "An exploratory technique for investigating large quantities of categorical data",
      "venue": "Journal of the Royal Statistical Society: Series C (Applied Statistics),",
      "year": 1980
    },
    {
      "authors": [
        "Jiwei Li",
        "Will Monroe",
        "Dan Jurafsky"
      ],
      "title": "Understanding neural networks through representation erasure",
      "venue": "arXiv preprint arXiv:1612.08220,",
      "year": 2016
    },
    {
      "authors": [
        "Tsung-Yi Lin",
        "Michael Maire",
        "Serge Belongie",
        "James Hays",
        "Pietro Perona",
        "Deva Ramanan",
        "Piotr Doll\u00e1r",
        "C Lawrence Zitnick"
      ],
      "title": "Microsoft coco: Common objects in context",
      "venue": "In European conference on computer vision,",
      "year": 2014
    },
    {
      "authors": [
        "Yin Lou",
        "Rich Caruana",
        "Johannes Gehrke",
        "Giles Hooker"
      ],
      "title": "Accurate intelligible models with pairwise interactions",
      "venue": "In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining,",
      "year": 2013
    },
    {
      "authors": [
        "Scott M Lundberg",
        "Gabriel G Erion",
        "Su-In Lee"
      ],
      "title": "Consistent individualized feature attribution for tree ensembles",
      "venue": "arXiv preprint arXiv:1802.03888,",
      "year": 2018
    },
    {
      "authors": [
        "Scott M Lundberg",
        "Su-In Lee"
      ],
      "title": "A unified approach to interpreting model predictions",
      "venue": "In Advances in neural information processing systems,",
      "year": 2017
    },
    {
      "authors": [
        "Christopher D. Manning",
        "Prabhakar Raghavan",
        "Hinrich Sch\u00fctze"
      ],
      "title": "Introduction to Information Retrieval",
      "year": 2008
    },
    {
      "authors": [
        "James N Morgan",
        "John A Sonquist"
      ],
      "title": "Problems in the analysis of survey data, and a proposal",
      "venue": "Journal of the American statistical association,",
      "year": 1963
    },
    {
      "authors": [
        "W James Murdoch",
        "Peter J Liu",
        "Bin Yu"
      ],
      "title": "Beyond word importance: Contextual decomposition to extract interactions from lstms",
      "venue": "International Conference on Learning Representations,",
      "year": 2018
    },
    {
      "authors": [
        "JA Nelder"
      ],
      "title": "A reformulation of linear models",
      "venue": "Journal of the Royal Statistical Society: Series A (General),",
      "year": 1977
    },
    {
      "authors": [
        "Sanjay Purushotham",
        "Martin Renqiang Min",
        "C-C Jay Kuo",
        "Rachel Ostroff"
      ],
      "title": "Factorized sparse learning models with interpretable high order feature interactions",
      "venue": "In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,",
      "year": 2014
    },
    {
      "authors": [
        "Marco Tulio Ribeiro",
        "Sameer Singh",
        "Carlos Guestrin"
      ],
      "title": "Why should i trust you?: Explaining the predictions of any classifier",
      "venue": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
      "year": 2016
    },
    {
      "authors": [
        "Ramprasaath R Selvaraju",
        "Michael Cogswell",
        "Abhishek Das",
        "Ramakrishna Vedantam",
        "Devi Parikh",
        "Dhruv Batra"
      ],
      "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
      "venue": "In Proceedings of the IEEE international conference on computer vision,",
      "year": 2017
    },
    {
      "authors": [
        "Avanti Shrikumar",
        "Peyton Greenside",
        "Anshul Kundaje"
      ],
      "title": "Learning important features through propagating activation differences",
      "venue": "In Proceedings of the 34th International Conference on Machine Learning-Volume",
      "year": 2017
    },
    {
      "authors": [
        "Karen Simonyan",
        "Andrea Vedaldi",
        "Andrew Zisserman"
      ],
      "title": "Deep inside convolutional networks: Visualising image classification models and saliency maps",
      "venue": "arXiv preprint arXiv:1312.6034,",
      "year": 2013
    },
    {
      "authors": [
        "Chandan Singh",
        "W James Murdoch",
        "Bin Yu"
      ],
      "title": "Hierarchical interpretations for neural network predictions",
      "venue": "International Conference on Learning Representations,",
      "year": 2019
    },
    {
      "authors": [
        "Richard Socher",
        "Alex Perelygin",
        "Jean Wu",
        "Jason Chuang",
        "Christopher D Manning",
        "Andrew Ng",
        "Christopher Potts"
      ],
      "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
      "venue": "In Proceedings of the 2013 conference on empirical methods in natural language processing,",
      "year": 2013
    },
    {
      "authors": [
        "Weiping Song",
        "Chence Shi",
        "Zhiping Xiao",
        "Zhijian Duan",
        "Yewen Xu",
        "Ming Zhang",
        "Jian Tang"
      ],
      "title": "Autoint: Automatic feature interaction learning via self-attentive neural networks",
      "year": 1810
    },
    {
      "authors": [
        "Daria Sorokina",
        "Rich Caruana",
        "Mirek Riedewald",
        "Daniel Fink"
      ],
      "title": "Detecting statistical interactions with additive groves of trees",
      "venue": "In Proceedings of the 25th international conference on Machine learning,",
      "year": 2008
    },
    {
      "authors": [
        "Mukund Sundararajan",
        "Ankur Taly",
        "Qiqi Yan"
      ],
      "title": "Axiomatic attribution for deep networks",
      "venue": "In Proceedings of the 34th International Conference on Machine Learning-Volume",
      "year": 2017
    },
    {
      "authors": [
        "Michael Tsang",
        "Dehua Cheng",
        "Hanpeng Liu",
        "Xue Feng",
        "Eric Zhou",
        "Yan Liu"
      ],
      "title": "Feature interaction interpretability: A case for explaining ad-recommendation systems via neural interaction detection",
      "venue": "In International Conference on Learning Representations,",
      "year": 2020
    },
    {
      "authors": [
        "Michael Tsang",
        "Dehua Cheng",
        "Yan Liu"
      ],
      "title": "Detecting statistical interactions from neural network weights",
      "venue": "International Conference on Learning Representations,",
      "year": 2018
    },
    {
      "authors": [
        "Michael Tsang",
        "Hanpeng Liu",
        "Sanjay Purushotham",
        "Pavankumar Murali",
        "Yan Liu"
      ],
      "title": "Neural interaction transparency (nit): Disentangling learned interactions for improved interpretability",
      "venue": "In Advances in Neural Information Processing Systems,",
      "year": 2018
    },
    {
      "authors": [
        "Michael Tsang",
        "Youbang Sun",
        "Dongxu Ren",
        "Yan Liu"
      ],
      "title": "Can i trust you more? model-agnostic hierarchical explanations",
      "venue": "arXiv preprint arXiv:1812.04801,",
      "year": 2018
    },
    {
      "authors": [
        "John W Tukey"
      ],
      "title": "One degree of freedom for non-additivity",
      "year": 1949
    },
    {
      "authors": [
        "Andrea Vedaldi",
        "Stefano Soatto"
      ],
      "title": "Quick shift and kernel methods for mode seeking",
      "venue": "In European Conference on Computer Vision,",
      "year": 2008
    },
    {
      "authors": [
        "Linda Wang",
        "Alexander Wong"
      ],
      "title": "Covid-net: A tailored deep convolutional neural network design for detection of covid-19 cases from chest radiography",
      "year": 2003
    },
    {
      "authors": [
        "Martin B Wilk"
      ],
      "title": "The randomization analysis of a generalized randomized block",
      "venue": "design. Biometrika,",
      "year": 1955
    },
    {
      "authors": [
        "Thomas Wolf",
        "Lysandre Debut",
        "Victor Sanh",
        "Julien Chaumond",
        "Clement Delangue",
        "Anthony Moi",
        "Pierric Cistac",
        "Tim Rault",
        "R\u2019emi Louf",
        "Morgan Funtowicz",
        "Jamie Brew"
      ],
      "title": "Huggingface\u2019s transformers: State-of-the-art natural language processing",
      "year": 1910
    },
    {
      "authors": [
        "Simon N Wood"
      ],
      "title": "Generalized additive models: an introduction with R",
      "venue": "CRC press,",
      "year": 2017
    }
  ],
  "sections": [
    {
      "heading": "1 Introduction",
      "text": "The success of state-of-the-art prediction models such as neural networks is driven by their capability to learn complex feature interactions. When such models are used to make predictions for users, we may want to know how they personalize to us. Such model behaviors can be explained via interaction detection and attribution, i.e. if features influence each other and how these interactions contribute to predictions, respectively. Interaction explanations are useful for applications such as sentiment analysis [35], image classification [47], and recommendation tasks [21, 47].\nRelevant methods for attributing predictions to feature interactions are black-box explanation methods based on axioms (or principles), but these methods lack interpretability. One of the core issues is that an interaction\u2019s importance is not the same as its attribution. Techniques like Shapley Taylor Interaction Index (STI) [14] and Integrated Hessians (IH) [25] combine these concepts in order to be axiomatic. Specifically, they base an interaction\u2019s attribution on non-additivity, i.e. the degree that features non-additively affect an outcome. While non-additivity can be used for interaction detection, it is not interpretable as an attribution measure as we see in Fig. 1. In addition, neither STI nor IH is tractable for higher-order feature interactions [14, 45]. Hence, there is a need for interpretable, axiomatic, and scalable methods for interaction attribution and corresponding interaction detection.\nTo this end, we propose a novel framework called Archipelago, which consists of an interaction attribution method, ArchAttribute, and a corresponding interaction detector, ArchDetect, to address the challenges of being interpretable, axiomatic, and scalable. Archipelago is named after its ability to provide explanations by isolating feature interactions, or feature \u201cislands\u201d. The inputs to Archipelago are a black-box model f and data instance x?, and its outputs are a set of interactions and individual features {I} as well as an attribution score \u03c6(I) for each of the feature sets I. ArchAttribute satisfies attribution axioms by making relatively mild assumptions: a) disjointness of interaction sets, which is easily obtainable, and b) the availability of a generalized additive\nPreprint. Under review.\nar X\niv :2\n00 6.\n10 96\n5v 1\n[ st\nat .M\nL ]\n1 9\nJu n\nfunction which is a good approximator to any function, as is leveraged in earlier works [48\u201350]. On the other hand, ArchDetect circumvents intractability issues of higher-order interaction detection by removing certain uninterpretable higher-order interactions and leveraging a property of feature interactions that allows pairwise interactions to merge for disjoint arbitrary-order interaction detection. In practice, where any assumptions may not hold in real-world settings, Archipelago still performs well. In particular, Archipelago effectively detects relevant interactions and is more interpretable than state-of-the-art methods [14, 20, 25, 26, 46, 50] when evaluated on annotation labels in sentiment analysis and image classification. We visualize Archipelago explanations on sentiment analysis, COVID-19 prediction on chest X-rays, and ad-recommendation.\nOur main contributions are summarized below.\n\u2022 Interaction Attribution: We propose ArchAttribute, a feature attribution measure that leverages feature interactions. It has advantages of being model-agnostic, interpretable, and runtime-efficient as compared to other state-of-the-art interaction attribution methods.\n\u2022 Principled Attribution: ArchAttribute obeys standard attribution axioms [46] that are generalized to work for feature sets, and we also propose a new axiom for interaction attribution to respect the additive structure of a function.\n\u2022 Interaction Detection: We propose a complementary feature interaction detector, ArchDetect, that is also model-agnostic and O(p2)-efficient for pairwise and disjoint arbitrary-order interaction detection (p is number of features).\nOur empirical studies on ArchDetect and ArchAttribute demonstrate their superior properties as compared to state-of-the-art methods."
    },
    {
      "heading": "2 Notations and Background",
      "text": "We first introduce preliminaries that serve as a basis for our discussions.\nNotations: We use boldface lowercase symbols, such as x, to represent vectors. The i-th entry of a vector x is denoted by xi. For a set S, its cardinality is denoted by |S|, and the operation \\S means all except S . For p features in a dataset, let I be a subset of feature indices: I \u2286 {1, 2, . . . , p}. For a vector x \u2208 Rp, let xI \u2208 Rp be defined element-wise in (1). In our discussions, a context means x\\I .\n(xI)i = { xi, if i \u2208 I 0 otherwise (1)\nProblem Setup: Let f denote a black-box model with scalar output. For multi-class classification, f is assumed to be a class logit. We use a target vector x? \u2208 Rp to denote the data instance where we wish to explain f , and x\u2032 \u2208 Rp to denote a neutral baseline. Here, the baseline is a reference vector for x? and conveys an \u201cabsence of signal\u201d as per [46]. These vectors form the space of X \u2282 Rp, where each element comes from either x?i or x \u2032 i, i.e. X = {(x1, . . . , xp) | xi \u2208 {x?i , x\u2032i},\u2200i = 1, . . . , p}.\nFeature Interaction: The definition of the feature interaction of interest is formalized as follows.\nDefinition 1 (Statistical Non-Additive Interaction). A function f contains a statistical non-additive interaction of multiple features indexed in set I if and only if f cannot be decomposed into a sum of |I| subfunctions fi , each excluding the i-th interaction variable: f(x) 6= \u2211 i\u2208I fi(x\\{i}).\nDef. 1 identifies a non-additive effect among all features I on the output of function f [18, 45, 48]. For example, this means that the function ReLU(x1 + x2) creates a feature interaction because it cannot be represented as an addition of univariate functions, i.e., ReLU(x1 + x2) 6= f1(x2) + f2(x1) (Fig. 2b). We refer to individual feature effects which do not interact with other features as main effect. Higher-order feature interactions are captured by |I| > 2, i.e. interactions larger than pairs. Additionally, if a higher-order interaction exists, all of its subsets also exist as interactions [45, 48].\n3 Archipelago Interaction Attribution We begin by presenting our feature attribution measure. Our feature attribution analyzes and assigns scores to detected feature interactions. Our corresponding interaction detector is presented in \u00a74.\n3.1 ArchAttribute Let I be the set of feature indices that correspond to a desired attribution score. Our proposed attribution measure, called ArchAttribute, is given by\n\u03c6(I) = f(x?I + x\u2032\\I)\u2212 f(x \u2032). (2)\nArchAttribute essentially isolates the attribution of x?I from the surrounding baseline context while also satisfying axioms (\u00a73.2). We call this isolation an \u201cisland effect\u201d, where the target features {x?i }i\u2208I do not specifically interact with the baseline features { x\u2032j } j\u2208\\I . For example, consider sentiment analysis on a phrase x? = \u201cnot very bad\u201d with a baseline x\u2032 = \u201c_ _ _\u201d . Suppose that we want to examine the attribution of an interaction I that corresponds to {very, bad} in isolation. In this case, the contextual word \u201cnot\u201d also interacts with I, which becomes apparent when small perturbations to the word \u201cnot\u201d causes large changes to prediction probabilities. However, as we move further away from the word \u201cnot\u201d towards the empty-word \u201c_\u201d in the word-embedding space, small perturbations no longer result in large prediction changes, meaning that \u201c_\u201d does not specifically interact with {very, bad}. This intuition motivates our use of the baseline context x\u2032\\I in (2)."
    },
    {
      "heading": "3.2 Axioms",
      "text": "We now show how ArchAttribute obeys standard feature attribution axioms [46]. Since ArchAttribute operates on feature sets, we generalize the notion of standard axioms to feature sets. To this end, we also propose a new axiom, Set Attribution, which allows us to work with feature sets.\nLet S = {Ii}ki=1 be all k feature interactions and main effects of f in the space X (defined in \u00a72), where we take the union of overlapping sets in S. Later in \u00a74, we explain how to obtain S. Completeness: We consider a generalization of the completeness axiom for which the sum of all attributions equals f(x?)\u2212 f(x\u2032). The axiom tells us how much feature(s) impact a prediction.\nLemma 2 (Completeness on S). The sum of all attributions by ArchAttribute for the disjoint sets in S equals the difference of f between x? and the baseline x\u2032: f(x?)\u2212 f(x\u2032).\nThe proof is in Appendix C. We can easily see ArchAttribute satisfying this axiom in the limiting case where k = 1, I1 = {i}pi=1 because (2) directly becomes f(x?)\u2212 f(x\u2032). Existing interaction / group attribution methods: Sampling Contextual Decomposition (SCD) [26], its variant (CD) [35,42], Sampling Occlusion (SOC) [26], and Shapley Interaction Index (SI) [20] do not satisfy completeness, whereas Integrated Hessians (IH) [25] and Shapley Taylor Interaction Index (STI) [14] do.\nSet Attribution: We propose an axiom for interaction attribution called Set Attribution to work with feature sets as opposed to individual features and follow the additive structure of a function.\nAxiom 3 (Set Attribution). If f : Rp \u2192 R is a function in the form of f(x) = \u2211k i=1 \u03d5i(xIi) where {Ii}ki=1 are disjoint and functions {\u03d5i(\u00b7)}ki=1 have roots, then an interaction attribution method admits an attribution for feature set Ii as \u03d5i(xIi) \u2200i = 1, . . . , k.\nFor example, if we consider a function y = x1x2 + x3; it makes sense for the attribution of the x1x2 interaction to be the value of x1x2 and the attribution for the x3 main effect to be the value of x3. Lemma 4 (Set Attribution on S). For x = x? and a baseline x\u2032 such that \u03d5i(x\u2032Ii) = 0 \u2200i = 1, . . . , k, ArchAttribute satisfies the Set Attribution axiom and provides attribution \u03d5i(xIi) for set Ii \u2200i.\nThe proof is in Appendix E, which follows from Lemma 2. Neither SCD, CD, SOC, SI, IH, nor STI satisfy Set Attribution (shown in Appendix E.1). We can enable Integrated Gradients (IG) [46] to satisfy our axiom by summing its attributions within each feature set of S. ArchAttribute differs from IG by its \u201cisland effect\u201d (\u00a73.1) and model-agnostic properties.\nOther Axioms: ArchAttribute also satisfies the remaining axioms: Sensitivity, Implementation Invariance, Linearity, and Symmetry-Preserving, which we show via Lemmas 7-11 in Appendix F.\nDiscussion: Several axioms required disjoint interaction and main effect sets in S. Though interactions are not necessarily disjoint by definition (Def. 1), it is reasonable to merge overlapping interactions to obtain compact visualizations, as shown in Fig. 1 and later experiments (\u00a75.3). The disjoint sets also allow ArchAttribute to yield identifiable non-additive attributions in the sense that it can identify the attribution given a feature set in S. This contrasts with Model-Agnostic Hierarchical Explanations (MAHE) [50], which yields unidentifiable attributions [56].\n4 Archipelago Interaction Detection Our axiomatic analysis of ArchAttribute relied on S, which contains interaction sets of f on the space X (defined in \u00a72). To develop an interaction detection method that works in tandem with ArchAttribute, we draw inspiration from the discrete interpretation of mixed partial derivatives."
    },
    {
      "heading": "4.1 Discrete Interpretation of Mixed Partial Derivatives",
      "text": "Consider the plots in Fig. 2, which consist of points a, b, c, and d that each contain two features. From a top-down view of each plot, the points form the corners of a rectangle, whose side lengths are h1 = |a1 \u2212 b1| = |c1 \u2212 d1| and h2 = |a2 \u2212 c2| = |b2 \u2212 d2|. When h1 and h2 are small, the mixed partial derivative w.r.t variables x1 and x2 is computed as follows. First,\n\u2202f(a) \u2202x1 \u2248 1h1 (f(a)\u2212 f(b)) and \u2202f(c)\u2202x1 \u2248 1 h1\n(f(c)\u2212 f(d)). Similarly, the mixed partial derivative is approximated as: \u22022f \u2202x1x2 \u2248 1h2 ( \u2202f(a) \u2202x1 \u2212 \u2202f(c)\u2202x1 ) \u2248 1h1h2 ((f(a)\u2212 f(b))\u2212 (f(c)\u2212 f(d))) . (3)\nWhen h1 and h2 become large, (3) tells us if a plane can fit through all four points a,b,c, d (Fig. 2a), which occurs when (3) is zero. In this domain where x1 and x2 only take two possible values each, a plane in the linear form f(x) = w1x1 + w2x2 + b is functionally equivalent to all functions of the form f(x) = f1(x1) + f2(x2) + b, so any deviation from the plane, e.g. Fig. 2b, becomes non-additive. Consequently, a non-zero value of (3) identifies a non-additive interaction by the definition of statistical interaction (Def. 1). What\u2019s more, the magnitude of (3) tells us the degree of deviation from the plane, or the degree of non-additivity. (Additional details in Appendix G)\n4.2 ArchDetect\nLeveraging these insights about mixed partial derivatives, we now discuss the two components of our proposed interaction detection technique \u2013 ArchDetect.\n4.2.1 Handling Context: As defined in \u00a73.2 and \u00a74, our problem is how to identify interactions of p features in X for our target data instance x? and baseline x\u2032. If p = 2, then we can almost directly use (3), where a = (x?1, x ? 2), b = (x \u2032 1, x ? 2), c = (x ? 1, x \u2032 2), and d = (x \u2032 1, x \u2032 2). However if p > 2, all possible combinations of features in X would need to be examined to thoroughly identify just one pairwise interaction. To see this, we first rewrite (3) to accommodate p features, and square the result to measure interaction strength and be consistent with previous interaction detectors [18, 19]. The interaction strength between features i and j for a context x\\{i,j} is then defined as\n(4)\u03c9i,j(x) = ( 1 hihj ( f(x?{i,j} + x\\{i,j})\u2212 f(x \u2032 {i} + x ? {j} + x\\{i,j})\u2212 f(x ? {i} + x \u2032 {j} + x\\{i,j})\n+ f(x\u2032{i,j} + x\\{i,j}) ))2 ,\nwhere hi = |x?i \u2212 x\u2032i| and hj = \u2223\u2223x?j \u2212 x\u2032j\u2223\u2223. The thorough way to identify the {i, j} feature interaction is given by \u03c9\u0304i,j = Ex\u2208X [\u03c9i,j(x)], where each element of x\\{i,j} is Bernoulli (0.5). This expectation is intractable because X has an exponential search space, so we propose the first component of ArchDetect for efficient pairwise interaction detection:\n(5)\u03c9\u0304i,j = 1\n2 (\u03c9i,j(x\n?) + \u03c9i,j(x \u2032)) .\nHere, we estimate the expectation by leveraging the physical meaning of the interactions and ArchAttribute\u2019s axioms via the different contexts of x in (5) as follows:\n\u2022 Context of x?: An important interaction is one due to multiple x? features. As a concrete example, consider an image representation of a cat which acts as our target data instance. The following higher-order interaction, if xear = x?ear and xnose = x ? nose and xfur =\nx?fur then f(x) = high cat probability, is responsible for classifying \u201ccat\u201d. We can detect any pairwise subset {i, j} of this interaction by setting the context as x?\\{i,j} using \u03c9i,j(x ?).\n\u2022 Context of x\u2032: Next, we consider x\u2032\\{i,j} to detect interactions via \u03c9i,j(x \u2032), which helps\nus establish ArchAttribute\u2019s completeness (Lemma 2). This also separates out effects of any higher-order baseline interactions from f(x\u2032) in (8) (Appendix C) and recombine their effects in (11). From an interpretability standpoint, the x\u2032\\{i,j} context ranks pairwise interactions w.r.t. a standard baseline. This context is also used by ArchAttribute (2).\n\u2022 Other Contexts: The first two contexts accounted for any-order interactions created by either target or baseline features and a few interactions created by a mix of baseline and target features. The remaining interactions specifically require a mix of > 3 target and baseline features. This case is unlikely and is excluded, as we discuss next.\nThe following assumption formalizes our intuition for the Other Contexts setting where there is a mix of higher-order (> 3) target and baseline feature interactions.\nAssumption 5 (Higher-Order Mixed-Interaction). For any feature set I where |I| > 3 and any pair of non-empty disjoint setsA and B whereA\u222aB = I , the instances x \u2208 X such that xi = x?i \u2200i \u2208 A and xj = x\u2032j \u2200j \u2208 B do not cause a higher-order interaction of all features {xk}k\u2208I via f .\nAssumption 5 has a similar intuition as ArchAttribute in \u00a73.1 that target features do not specifically interact with baseline features. To understand this assumption, consider the original sentiment analysis example in Fig. 1 simplified as x? = \u201cbad terrible awful horrible movie\u201d where x\u2032 = \u201c_ _ _ _ _\u201d. It is reasonable to assume that there is no special interaction created by token sets such as {bad, terrible, _ , horrible} or {_ , _ , _ , horrible} due to the meaningless nature of the \u201c_\u201d token.\nEfficiency: In (5), ArchDetect attains interaction detection over all pairs {i, j} in O(p2) calls of f . Note that in (4), most function calls are reusable during pairwise interaction detection.\n4.2.2 Detecting Disjoint Interaction Sets: In this section, the aim here is to recover arbitrary size and disjoint non-additive feature sets S = {Ii} (not just pairs). ArchDetect looks at the union of overlapping pairwise interactions to obtain disjoint feature sets. Merging these pairwise interactions captures any existing higher-order interactions automatically since the existence of a higher-order interaction automatically means all its subset interactions exist (\u00a72). In addition, ArchDetect merges these overlapped pairwise interactions with all individual feature effects to account for all features. The time complexity of this merging process is also O(p2).\nTable 1: Comparison of interaction detectors (b) on synthetic ground truth in (a).\n(a) Functions with Ground Truth Interactions F1(x) = \u221110 i=1 \u221110 j=1 xixj + \u221120 i=11 \u221130 j=21 xixj + \u221140 k=1 xk\nF2(x) = \u2227 (x; {x?i }20i=1) + \u2227 (x; {x?i }30i=11) + \u221140 j=1 xj\nF3(x) = \u2227 (x; {x\u2032i}20i=1) + \u2227 (x; {x?i }30i=11) + \u221140 j=1 xj\nF4(x) = \u2227 (x; {x?1, x?2} \u222a {x\u20323}) + \u2227 (x; {x?i }30i=11) + \u221140 j=1 xj\n(b) Pairwise Interaction Ranking AUC. The baseline methods fail to detect interactions suited for the desired contexts in \u00a74.2.1.\nMethod F1 F2 F3 F4 Two-way ANOVA 1.0 0.51 0.51 0.55 Integrated Hessians 1.0 N/A N/A N/A Neural Interaction Detection 0.94 0.52 0.48 0.56 Shapley Interaction Index 1.0 0.50 0.50 0.51 Shapley Taylor Interaction Index 1.0 0.50 0.53 0.51 ArchDetect (this work) 1.0 1.0 1.0 1.0"
    },
    {
      "heading": "5 Experiments",
      "text": ""
    },
    {
      "heading": "5.1 Setup",
      "text": "We conduct experiments first on ArchDetect in \u00a75.2 then on ArchAttribute in \u00a75.3. We then visualize their combined form as Archipelago in \u00a75.3. Throughout our experiments, we commonly study BERT [13, 55] on text-based sentiment analysis and ResNet152 [24] on image classification. BERT was fine-tuned on the SST dataset [43], and ResNet152 was pretrained on ImageNet [12].\nFor sentiment analysis, we set the baseline vector x\u2032 to be the tokens \u201c_\u201d, in place of each word-token from x?. For image classification, we set x\u2032 to be an all-zero image, and use the Quickshift superpixel segmenter [52] as per the need for input dimensionality reduction [47] (details in Appendix B). We set h1 = h2 = 1 for both domains. Several methods we compare to are common across experiments, in particular IG, IH, (disjoint) MAHE, SI, STI, and Difference, defined as \u03c6d(I) = f(x?)\u2212f(x\u2032I+x?\\I).\n5.2 ArchDetect\nWe validate ArchDetect\u2019s performance via synthetic ground truth and redundancy experiments.\nSynthetic Validation: We set x? = [1, 1, . . . , 1] \u2208 R40 and x\u2032 = [\u22121,\u22121, . . . ,\u22121] \u2208 R40. Let z[\u00b7] be a key-value pair function such that z[i] = xi for key i \u2208 z.keys and value xi, so we can define\u2227\n(x; z) := { 1, if xi = z[i] \u2200i \u2208 z.keys \u22121 for all other cases.\nTable 1a shows functions with ground truth interactions suited for the desired contexts in \u00a74.2.1. Table 1b shows interaction detection AUC on these functions by ArchDetect, IH, SI, STI, Two-way ANOVA [16] and the state-of-the-art Neural Interaction Detection [48]. On F2, F3, & F4, the baseline methods fail because they are not designed to detect the interactions of our desired contexts (\u00a74.2.1).\nInteraction Redundancy: The purpose of the next experiments is to see if ArchDetect can omit certain higher-order interactions. We study the form of (5) by examining the redundancy of interactions as new contexts are added to (5), which we now write as \u03c9\u0304i,j(C) = 1C \u2211C c=1 \u03c9i,j(xc). Let n be the number of contexts considered, and k be the number of top pairwise interactions selected after running pairwise interaction detection via \u03c9\u0304i,j for all {i, j} pairs. Interaction redundancy is the overlap ratio of two sets of top-k pairwise interactions, one generated via \u03c9\u0304i,j(n) and the other one via \u03c9\u0304i,j(n \u2212 1) for some integer n \u2265 2. We generally expect the redundancy to increase as n increases, which we initially observe in Fig. 3. Here, \u201cfixed\u201d and \u201crandom\u201d correspond to different context sequences x1,x2, . . . ,xN . The \u201crandom\u201d sequence uses random samples from X for all {xi}Ni=1, whereas the \u201cfixed\u201d sequence is fixed in the sense that x1 = x?, x2 = x\u2032, and the remaining {xi}Ni=3 are random samples. Experiments are done on the SST test set for BERT and 100 random test images in ImageNet for ResNet152. Notably, the \u201cfixed\u201d setting has very low redundancy at n = 2 (ArchDetect) versus \u201crandom\u201d. As soon as n = 3, the redundancy jumps and stabilizes quickly. These experiments support Assumption 5 and (5) to omit specified higher-order interactions.\n5.3 ArchAttribute & Archipelago\nWe study the interpetability of ArchAttribute by comparing its attribution scores to ground truth annotation labels on subsets of features. For fair comparison, we look at extreme attributions (top and bottom 10%) for each baseline method. We then visualize the combined Archipelago framework. Additional comparisons on attributions, runtime, and visualizations are shown in Appendices I, J, K.\nSentiment Analysis: For this task, we compare ArchAttribute to other explanation methods on two metrics: phrase correlation (Phrase \u03c1) and word correlation (Word \u03c1) on the SST test set (metrics are from [26]). Phrase \u03c1 is the Pearson correlation between estimated phrase attributions and SST phrase labels (excluding prediction labels) on a 5-point sentiment scale. Word \u03c1 is unlike our labelbased evaluations by computing the Pearson correlation between estimated word attributions and the corresponding coefficients of a global bag-of-words linear model, which is also trained on the SST dataset. In addition to the aforementioned baseline methods in \u00a75.1, we include the state-of-the-art SCD and SOC methods for sequence models [26] in our evaluation. In Table 2, ArchAttribute compares favorably to all methods where we consider the top and bottom 10% of the attribution scores for each method. We obtain similar performance across all other percentiles in Appendix I.\nWe visualize Archipelago explanations on S generated by top-3 pairwise interactions (\u00a74.2.2) in Fig. 4. The sentence examples are randomly selected from the SST test set. The visualizations show interactions and individual feature effects which all have reasonable polarity and intensity. Interestingly, some of the interactions, e.g. between \u201clou-sy\u201d and \u201cun\u201d, are long range.\nImage Classification: On image classification, we compare ArchAttribute to relevant baseline methods on a \u201cSegment AUC\u201d metric, which computes the agreement between the estimated attribution of an image segment and that segment\u2019s label. We obtain segment labels from the MS COCO dataset [29] and match them to the label space of ImageNet. All explanation attributions are computed relative to ResNet152\u2019s top-classification in the joint label space. The segment label thus becomes whether or not the segment belongs to the same class as the top-classification. Evaluation is\nconducted on all segments with valid labels in the MS COCO dev set. ArchAttribute performs especially well on extreme attributions in Table 2, as well as all attributions (in Appendix I).\nFig. 5 visualizes Archipelago on an accurate COVID-19 classifier for chest X-rays [53], where S is generated by top-5 pairwise interactions (\u00a74.2.2). Shown is a random selection of test X-rays [9, 10] that are classified COVID-positive. The explanations tend to detect the \u201cgreat vessels\u201d near the heart.\nRecommendation Task: Fig. 6 shows Archipelago\u2019s result for this task using a state-of-the-art AutoInt model [44] for adrecommendation. Here, our approach finds a positive interaction between\u201cdevice_id\u201d and \u201cbanner_pos\u201d in the Avazu dataset [1], meaning that the online advertisement model decides the banner position based on user device_id. Note that for this task, there are no ground truth annotations."
    },
    {
      "heading": "6 Related Works",
      "text": "Attribution: Individual feature attribution methods distill any interactions of a data instance as attribution scores for each feature. Many methods require the scores to sum to equal the output [7, 32, 38, 40, 46], such as LIME and SHAP, which train surrogate linear explainer models on feature perturbations, and IG which invokes the fundamental theorem of calculus. Other\nmethods compute attributions from an information theoretic perspective [8] or strictly from model gradients [4, 39, 41]. These methods interpret feature importance but not feature interactions.\nFeature Interaction: Feature interaction explanation methods tend to either perform interaction detection [2,6,16,18,19,45,48] or combined interaction detection and attribution [14,25,30,31,37,50]. Relevant black-box interaction explainers are STI [14] which uses random feature orderings to identify contexts for a variant of (4) so that interaction scores satisfy completeness, IH [25] which extends IG with path integration for hessian computations, and MAHE [50], which trains surrogate explainer models for interaction detection and attribution. STI and IH are axiomatic and satisfy completeness but their attributions are uninterpretable (Table 2) and inefficient. MAHE\u2019s attributions are unidentifiable by training additive attribution models on overlapping feature sets. Several methods compute attributions on feature sequences or sets, such as SOC [26], SCD [26], and CD [35, 42], but they do not obey basic axioms. Finally, many methods are not model-agnostic, such as SCD, CD, IG, IH, GA2M [30], and Tree-SHAP [31]. Additional earlier works are discussed in Appendix H."
    },
    {
      "heading": "7 Discussion",
      "text": "Understandable and accessible explanations are cornerstones of interpretability which informed our isolation and disjoint designs of ArchAttribute and ArchDetect, respectively. Here, we develop an interpretable, model-agnostic, axiomatic, and efficient interaction explainer which achieves state-of-the-art results on multiple attribution tasks. In addition, we introduce a new axiom and generalize existing axioms to higher-order interaction settings. This provides guidance on how to design interaction attribution methods. To be able to solve the transparency issue, we need to understand feature attribution better. This work proposes interpretable and axiomatic feature interaction explanations to motivate future explorations in this area.\nBroader Impact\nThe purpose of this work is to provide new insights into existing and future prediction models. The explanations from Archipelago can be used by both machine learning practitioners and audiences without background expertise. The societal risk of this work is any overdependence on Archipelago. Users of this explanation method should consider the merits of not only this method but also other explanation methods for their use cases. For example, users may want fine-grained pixel-level explanations of image classifications whereas our explanations may require superpixel segmentation. Nevertheless, we believe this work can help reveal biases in prediction models, assist in scientific discovery, and stimulate discussions on how to debug models based on feature interactions."
    },
    {
      "heading": "Appendix",
      "text": "A Acronyms\nB Input Dimensionality Reduction\nFor a black-box model f : Rp\u2032 \u2192 R which takes as input a vector with p\u2032 dimensions (e.g. an image, input embedding, etc.) and maps it to a scalar output (e.g. a class logit), we can make ArchDetect more efficient by operating on a lower dimensional input encoding x \u2208 Rp with p dimensions. To match the dimensionality p\u2032 of the input argument of f , we define a transformation function \u03be : Rp \u2192 Rp\u2032 which takes the input encoding x in the lower dimensional space p and brings it back to the input space of f with dimensionality p\u2032. In other words, (4) becomes\n\u03c9i,j(x) = (\n1 hihj ( f \u2032(x?{i,j} + x\\{i,j})\u2212 f \u2032(x\u2032{i} + x ? {j} + x\\{i,j})\u2212 f \u2032(x?{i} + x \u2032 {j} + x\\{i,j})\n+ f \u2032(x\u2032{i,j} + x\\{i,j}) ))2 ,\nwhere f \u2032 = f \u25e6 \u03be. Correspondingly, ArchAttribute (2) becomes\n\u03c6(I) = f \u2032(x?I + x\u2032\\I)\u2212 f \u2032(x\u2032).\nExamples of input encodings are discussed for the following data types:\n\u2022 For an image, we use a superpixel segmenter, which selects regions on the image. The selection is covered by the vector x \u2208 {0, 1}p, which encodes which image segments have been selected. Note that wherever x is 0 corresponds to a baseline feature value (e.g. zeroed image pixels).\n\u2022 For text, we use the natural correspondence between an input embedding and a word token. The selection of input embedding vectors is also covered by the vector x \u2208 {0, 1}p.\n\u2022 For recommendation data, we use the same type of correspondence between an input embedding and a feature field.\nSimilar notions of input encodings have also been used in [38, 47]."
    },
    {
      "heading": "C Completeness Axiom",
      "text": "Lemma 2 (Completeness on S). The sum of all attributions by ArchAttribute for the disjoint sets in S equals the difference of f between x? and the baseline x\u2032: f(x?)\u2212 f(x\u2032).\nProof. Based on the definition of non-additive statistical interaction (Def. 1), a function f can be represented as a generalized additive function [48\u201350], here on the domain of X :\nf(x) = \u03b7\u2211 i=1 qi(xIui ) + p\u2211 j=1 q\u2032j(xj) + b, (6)\nwhere qi(xIui ) is a function of each interaction I u i on X \u2200i = 1, . . . , \u03b7 interactions, q\u2032j(xj) is a function for each feature \u2200j = 1, . . . , p, and b is a bias. The u in Iu stands for \u201cunmerged\u201d. The disjoint sets of S = {Ii}ki=1 are the result of merging overlapping interaction sets and main effect sets, so we can merge the subfunctions q(\u00b7) and q\u2032(\u00b7) of (6) whose input sets overlap to write f(x) as a sum of new functions gi(xIi) \u2200i = 1, . . . , k:\nf(x) = k\u2211 i=1 gi(xIi) + b. (7)\nFor some {gi}ki=1 of the form of (7), we rewrite (2) by separating out the effect of index i: \u03c6(Ii) = f(x?Ii + x \u2032 \\Ii)\u2212 f(x \u2032) \u2200i = 1, . . . , k\n= gi(x?Ii) + k\u2211 j=1 j 6=i gj(x \u2032 Ij ) + b \u2212 gi(x\u2032Ii) + k\u2211 j=1 j 6=i gj(x \u2032 Ij ) + b  (8) = gi(x ? Ii)\u2212 gi(x \u2032 Ii). (9)\nSince all I \u2208 S are disjoint, gj(x\u2032Ij ) can be canceled in (8) \u2200j, leading to (9). The result at (9) can also be obtained with an alternative attribution approach, as shown in Corollary 6.\nNext, we compute the sum of attributions: k\u2211 i=1 \u03c6(Ii) = k\u2211 i=1 ( gi(x ? Ii)\u2212 gi(x \u2032 Ii) )\n(10)\n= k\u2211 i=1 gi(x ? Ii)\u2212 k\u2211 i=1 gi(x \u2032 Ii) (11) = f(x?)\u2212 f(x\u2032)"
    },
    {
      "heading": "D Completeness of a Complementary Attribution Method",
      "text": "Corollary 6 (Completeness of a Complement). An attribution approach: \u03c6(I) = f(x?)\u2212 f(x\u2032I + x?\\I), similar to what is mentioned in [26, 28], also satisfies the completeness axiom.\nProof. Based on Eqs. 7 - 9 of Lemma 2:\n\u03c6(Ii) = f(x?)\u2212 f(x\u2032Ii + x ? \\Ii)\n= gi(x?Ii) + k\u2211 j=1 j 6=i gj(x ? Ij ) + b \u2212 gi(x\u2032Ii) + k\u2211 j=1 j 6=i gj(x ? Ij ) + b  = gi(x ? Ii)\u2212 gi(x \u2032 Ii)\nWe can then resume with (10) of Lemma 2."
    },
    {
      "heading": "E Set Attribution Axiom",
      "text": "Axiom 3 (Set Attribution). If f : Rp \u2192 R is a function in the form of f(x) = \u2211k i=1 \u03d5i(xIi) where {Ii}ki=1 are disjoint and functions {\u03d5i(\u00b7)}ki=1 have roots, then an interaction attribution method admits an attribution for feature set Ii as \u03d5i(xIi) \u2200i = 1, . . . , k. Lemma 4 (Set Attribution on S). For x = x? and a baseline x\u2032 such that \u03d5i(x\u2032Ii) = 0 \u2200i = 1, . . . , k, ArchAttribute satisfies the Set Attribution axiom and provides attribution \u03d5i(xIi) for set Ii \u2200i.\nProof. From (9) in Lemma 2, ArchAttribute can be written as\n\u03c6(Ii) = gi(x?Ii)\u2212 gi(x \u2032 Ii) \u2200i = 1, . . . , k, where f(x) = \u2211k i=1 gi(xIi) + b. Since S = {Ii}ki=1 are disjoint feature sets for the same function f in Axiom 3, gi(\u00b7) and \u03d5i(\u00b7) are related by a constant bias bi: \u03d5i(x) = gi(x) + bi Each \u03d5i(\u00b7) has roots, so gi(x) + bi has roots. x\u2032 is set such that \u03d5i(x\u2032Ii) = gi(x \u2032 Ii) + bi = 0. Rearranging,\n\u2212gi(x\u2032Ii) = bi. Adding gi(x?Ii) to both sides,\ngi(x ? Ii)\u2212 gi(x \u2032 Ii) = gi(x ? Ii) + bi,\nwhich becomes\n\u03c6(Ii) = \u03d5i(x?Ii) \u2200i = 1, . . . , k."
    },
    {
      "heading": "E.1 Set Attribution Counterexamples",
      "text": "We now provide counterexamples to identify situations in which the related methods do not satisfy the Set Attribution axiom.\nLet f(x) = ReLU(x1 + x3 + 1) + ReLU(x2) + 1.\nf(x) can be written as f(x) = \u03d51(x{1,3}) + \u03d52(x{2}) where \u03d51(x) = ReLU(x1 + x3 + 1), and \u03d52(x) = ReLU(x2) + 1. According to the Set Attribution axiom, an interaction attribution method admits attributions as\n\u2022 ReLU(x1 + x3 + 1) for features I1 = {1, 3} \u2022 ReLU(x2) + 1 for feature I2 = {2}.\nThe above setting serves as counterexamples to the related methods as follows:\n\u2022 CD always assigns \u03b1+ \u03b1\u03b1+\u03b2 to I1 and \u03b2 + \u03b2 \u03b1+\u03b2 to I2, where \u03b1 = ReLU(x1 + x3 + 1) and \u03b2 = ReLU(x2).\n\u2022 SCD uses an expectation over an activation decomposition, which does not guarantee admission of ReLU(x1 + x3 + 1) for I1 and ReLU(x2) for I2 through their respective decompositions. In the ideal case SCD becomes CD, which still does not satisfy Set Attribution from above. \u2022 IH always assigns a zero attribution to I2 from hessian computations. IH also does not\nassign attributions to general sets of features. \u2022 SOC does not assign attributions to general feature sets, only contiguous feature sequences. \u2022 Both SI and STI assign the following attribution score to I1:\nReLU(x1 + x3 + 1)\u2212 ReLU(x1 + x\u20323 + 1)\u2212 ReLU(x\u20321 + x3 + 1) + ReLU(x\u20321 + x\u20323 + 1). (12)\nThere do not exist a selection of x\u20321 and x \u2032 3 such that this attribution becomes ReLU(x1 + x3 + 1) for all values of x1 and x3.\nProof. We prove via case-by-case contradiction. Only the ReLU(x1 + x3 + 1) term can create an interaction between x1 and x3, and this term is also the target result, so any nonzero deviation from this term via independent x1 or x3 effects in (12) must be countered. These independent effects manifest as the ReLU(x1 + x\u20323 + 1) or ReLU(x \u2032 1 + x3 + 1) terms respectively. Since ReLU is always non-negative, the only way either of these terms is nonzero is if it is positive, which implies that ReLU(x1 + x\u20323 + 1) = x1 + x \u2032 3 + 1 or ReLU(x\u20321 + x3 + 1) = x \u2032 1 + x3 + 1. If both terms are positive, their substitution into (12) yields ReLU(x1 + x3 + 1)\u2212 x1 \u2212 x\u20323 \u2212 1\u2212 x\u20321 \u2212 x3 \u2212 1 + ReLU(x\u20321 + x\u20323 + 1). Even if ReLU(x\u20321 + x \u2032 3 + 1) is positive, we obtain ReLU(x1 + x3 + 1)\u2212 x1 \u2212 x\u20323 \u2212 1\u2212 x\u20321 \u2212 x3 \u2212 1 + x\u20321 + x \u2032 3 + 1 = ReLU(x1 + x3 + 1)\u2212 x1 \u2212 x3 \u2212 1. Asserting \u2212x1 \u2212 x3 \u2212 1 = 0 is a contradiction. If only one of the independent effects was positive, we also cannot assert 0 through similar simplifications. Now consider the remaining case where ReLU(x1 + x\u20323 + 1) = ReLU(x \u2032 1 + x3 + 1) = ReLU(x\u20321 + x \u2032 3 + 1) = 0. For any real-valued x \u2032 1 or x \u2032 3 , there can also be a negative realvalued x3 or x1 respectively. From either terms ReLU(x1 +x\u20323 + 1) or ReLU(x \u2032 1 +x3 + 1), we obtain ReLU(1) = 0, which is a contradiction."
    },
    {
      "heading": "F Other Axioms",
      "text": ""
    },
    {
      "heading": "F.1 Sensitivity Axiom",
      "text": "Lemma 7 (Sensitivity (a)). If x? and x\u2032 only differ at features indexed in I and f(x?) 6= f(x\u2032), then \u03c6(I) (2) yields a nonzero attribution.\nProof. Since x? and x\u2032 only differ at I , the following is true: x?\\I = x \u2032 \\I . We can therefore write x ? as\nx? = x?I + x ? \\I\n= x?I + x \u2032 \\I\nSubstituting this equivalence in (2), we have\n\u03c6(I) = f(x?I + x\u2032\\I)\u2212 f(x \u2032)\n= f(x?)\u2212 f(x\u2032).\nSince f(x?)\u2212 f(x\u2032) 6= 0, we directly obtain \u03c6(I) 6= 0.\nLemma 8 (Sensitivity (b)). If f does not functionally depend on I, then \u03c6(I) is always zero.\nProof. Since f does not functionally depend on I,\nf(x?I + x \u2032 \\I) = f(x \u2032 I + x \u2032 \\I)\n= f(x\u2032)\nTherefore,\n\u03c6(I) = f(x?I + x\u2032\\I)\u2212 f(x \u2032) = 0.\nF.2 Implementation Invariance\nLemma 9 (Implementation Invariance). For functionally equivalent models (with the same inputoutput mapping), \u03c6(\u00b7) are the same.\nThe definition of (2) only relies on function calls to f , which implies Implementation Invariance."
    },
    {
      "heading": "F.3 Linearity",
      "text": "Lemma 10 (Linearity on S). If two models f1, f2 have the same disjoint feature sets S and f = c1f1 + c2f2 where c1, c2 are constants, then \u03c6(I) = c1\u03c61(I) + c2\u03c62(I) \u2200I \u2208 S .\nProof. Since f1 and f2 have the same S = {Ii}ki=1, we can write f1 and f2 as follows via (7) in Lemma 2:\nf1(x) = k\u2211 i=1 g (1) i (xIi) + b (1),\nf2(x) = k\u2211 i=1 g (2) i (xIi) + b (2).\nSince f = c1f1 + c2f2,\nf(x) = c1f1(x) + c2f2(x)\n= ( k\u2211 i=1 c1 \u00d7 g(1)i (xIi) + c1 \u00d7 b (1) ) + ( k\u2211 i=1 c2 \u00d7 g(2)i (xIi) + c2 \u00d7 b (2) )\n= k\u2211 i=1 ( c1 \u00d7 g(1)i (xIi) + c2 \u00d7 g (2) i (xIi) ) + c1b (1) + c2b (2). (13)\nBy grouping terms as gi(xIi) = c1 \u00d7 g (1) i (xIi) + c2 \u00d7 g (2) i (xIi) and b = c1b (1) + c2b (2), we write (13) as\nf(x) = k\u2211 i=1 gi(xIi) + b. (14)\nFrom the form of (14), we can invoke (9): \u03c6(Ii) = gi(x?Ii)\u2212 gi(x \u2032 Ii) via Lemma 2. This equation is rewritten as\n\u03c6(Ii) = gi(x?Ii)\u2212 gi(x \u2032 Ii) = ( c1 \u00d7 g(1)i (x ? Ii) + c2 \u00d7 g (2) i (x ? Ii) ) \u2212 ( c1 \u00d7 g(1)i (x \u2032 Ii) + c2 \u00d7 g (2) i (x \u2032 Ii) )\n= c1 ( g (1) i (x ? Ii)\u2212 g (1) i (x \u2032 Ii) ) + c2 ( g (2) i (x ? Ii)\u2212 g (2) i (x \u2032 Ii) )\n= c1\u03c61(Ii) + c2\u03c62(Ii).\nBy noting that S = {Ii}ki=1, this concludes the proof."
    },
    {
      "heading": "F.4 Symmetry-Preserving",
      "text": "We first define symmetric feature sets as a generalization of \u201csymmetric variables\u201d from [46]. Feature index sets I1 and I2 are symmetric with respect to function f if swapping features in I1 with the features in I2 does not change the function, This implies that for symmetric I1 and I2, their cardinalities are the same |I1| = |I2|, and they are disjoint sets in order to swap the features to any valid set index. Lemma 11 (Symmetry-Preserving). For x? and x\u2032 that each have identical feature values between symmetric feature sets with respect to f , the symmetric feature sets receive identical attributions \u03c6(\u00b7).\nProof. Since x? and x\u2032 each have identical feature values between the symmetric feature sets,\n{x?i }i\u2208I1 = {x?j}j\u2208I2 , {x\u2032i}i\u2208I1 = {x\u2032j}j\u2208I2 .\nTherefore, the symmetry implies the following for any x in the domain of f . f ( x?I1 + x \u2032 I2 + x\\(I1\u222aI2) ) = f ( x\u2032I1 + x ? I2 + x\\(I1\u222aI2) ) (15)\nSetting x = x\u2032, we rewrite (15) as f ( x?I1 + x \u2032 I2 + x \u2032 \\(I1\u222aI2) ) \u2212 f ( x\u2032I1 + x ? I2 + x \u2032 \\(I1\u222aI2) ) = 0\n= f(x?I1 + x \u2032 \\I1)\u2212 f(x ? I2 + x \u2032 \\I2) = ( f(x?I1 + x \u2032 \\I1)\u2212 f(x \u2032) ) \u2212 ( f(x?I2 + x \u2032 \\I2)\u2212 f(x \u2032) )\n= \u03c6(I1)\u2212 \u03c6(I2)\nTherefore, \u03c6(I1) = \u03c6(I2)."
    },
    {
      "heading": "G Discrete Mixed Partial Derivatives Detect Non-Additive Statistical Interactions",
      "text": "A generalized additive model fg is given by\nfg(x) = p\u2211 i=1 gi(xi) + b, (16)\nwhere gi(\u00b7) can be any function of individual features xi and b is a bias. Since each xi of x \u2208 X only takes on two values, a line can connect all valid points in each feature. Therefore, (16) is equivalent to\nf`(x) = p\u2211 i=1 wixi + b, (17)\nfor weights wi \u2208 R and the function domain being X . For the case where p = 2, the discrete mixed partial derivative is given by (3) or\n\u22022f\n\u2202x1\u2202x2 =\n1\nh1h2 (f([x?1, x ? 2])\u2212 f([x?1, x\u20322])\u2212 f([x\u20321, x?2]) + f([x\u20321, x\u20322])) ,\nwhere h1 = |x?1 \u2212 x\u20321| and h2 = |x?2 \u2212 x\u20322|. Since any three points (not on the same line) define a plane of the form (17) (p = 2), we can write the fourth point as having a function value with deviation \u03b4 from the plane.\n(18)\n\u22022f\n\u2202x1\u2202x2 =\n1\nh1h2 (f([x?1, x ? 2])\u2212 f([x?1, x\u20322])\u2212 f([x\u20321, x?2]) + f([x\u20321, x\u20322]))\n= 1\nh1h2 ((w1x\n? 1 + w2x ? 2 + b+ \u03b4)\u2212 (w1x?1 + w2x\u20322 + b)\u2212 (w1x\u20321 + w2x?2 + b)\n+ (w1x \u2032 1 + w2x \u2032 2 + b))\n= \u03b4\nh1h2 .\nIf (18) is 0, then \u03b4 = 0, which implies that f can be written as (17). \u03b4 6= 0 implies the opposite, that f cannot be written in linear form (by definition). Since (17) is equivalent to (16) in the domain of X , this implies that \u03b4 6= 0 if and only if f(x) 6= g1(x1) + g2(x2) + b. Based on Def. 1, we can conclude that a nonzero discrete mixed partial derivative w.r.t. x1 and x2 in the space X at p = 2 detects a non-additive statistical interaction between the two features. For the case where p > 2, Def. 1 states that a pairwise interaction {i, j} exists in f if and only if f(x) 6= fi(x\\{i}) + fj(x\\{j}) for functions fi(\u00b7) and fj(\u00b7). This means that {i, j} is declared to be an interaction if a local {i, j} interaction occurs at any x\\{i,j}, x \u2208 X .\nTherefore, we can detect non-additive statistical interactions {i, j} for general p \u2265 2 via\nEx [ \u22022f\n\u2202xi\u2202xj\n]2 > 0,\nwhich mirrors the definition of pairwise interaction for real-valued x in [18]."
    },
    {
      "heading": "H Early Works on Feature Interaction Interpretation",
      "text": "We discuss early works on feature interaction interpretation and provide a timeline for this research history in Table 4. We also discuss mixed partial derivatives on dichotomous variables in H.3."
    },
    {
      "heading": "H.1 Origins",
      "text": "The notion of a feature interaction has been studied at least since the 19th century when John Lawes and Joseph Gilbert used factorial designs in agricultural research at the Rothamsted Experimental Station [11]. A factorial design is an experiment that includes observations at all combinations of categories of each factor or feature. However, the \u201cadvantages [of factorial design] had never been clearly recognised, and many research workers believed that the best course was the conceptually simple one of investigating one question at a time\u201d [57]. In the early 20th century, Fisher et al. (1926) [17] emphasized the importance of factorial designs as being the only way to obtain information about feature interactions. Near the same time, Fisher (1921) [15] also developed one of the foundations of statistical analysis called Analysis of Variance (ANOVA) including twoway ANOVA [16], which is a factorial method to detect pairwise feature interactions based on differences among group means in a dataset. Tukey (1949) [51] extended two-way ANOVA to test if two categorical features are non-additively related to the expected value of a outcome variable. This work set a precedent for later research on detecting feature interactions based on their nonadditive definition. Soon after, experimental designs were generalized to study feature interactions, in particular the generalized randomized block design [54], which assigns test subjects to different categories (or blocks) between features in a way where cross-categories between features serve as interaction terms in linear regression.\nThere was a surge of interest in improving the analysis of feature interactions after the mid 20th century. Belsion (1959) [5] and Morgan & Sonquist (1963) [34] proposed Automatic Interaction Detection (AID) originally under a different name. AID detects interactions by subdividing data into disjoint exhaustive subsets to model an outcome based on categorical features. Based on AID, Kass (1980) [27] developed Chi-square Automatic Interaction Detection (CHAID), which determines how categorical features best combine in decision trees via a chi-square test. AID and CHAID were precursors to modern decision tree prediction models. Concurrently, Nelder (1977) [36] introduced the \u201cPrinciple of Marginality\u201d arguing that a feature interaction and its marginal variables should not be considered separately, for example in linear regression. Hamada & Wu (1992) [22] provided a contrasting view that an interaction is only important if one or both of its marginal variables are important. Around the same time, an influential book on interpreting feature interactions was published on how to test, plot, and understand interactions of two or three continuous or categorical features [3]."
    },
    {
      "heading": "H.2 Early 21st Century Works",
      "text": "At the start of the 21st century, efforts began to focus on interpreting interactions in accurate prediction models. Ai & Norton (2003) [2] proposed extracting interactions from logit and probit models via mixed partial derivatives. Gevrey (2006) [19] followed up by proposing mixed partial derivatives to extract interactions from multilayer perceptrons with sigmoid activations when at the time, only shallow neural networks were studied. Friedman & Popescu (2008) [18] proposed using hybrid models to capture interactions with decision trees and univariate effects with linear regression. Sorokina et al. (2008) [45] proposed to use high-performance additive trees to detect feature interactions based on their non-additive definition. At the turn of the decade, we saw Bien et al. [6] capture interactions with different heredity conditions using a hierarchical lasso on linear regression models. Then, Hao & Zhang (2014) [23] drew attention towards interaction screening in high dimensional data. This summarizes feature interaction research before 2015."
    },
    {
      "heading": "H.3 Note on Mixed Partial Derivatives on Dichotomous Variables",
      "text": "To our knowledge, the usage of mixed partial derivatives for interaction detection on dichotomous variables (features that only take two possible values) originated at the turn of the 21st century [2,20], but existing methods rely on single contexts [2] or random contexts [14, 20]. Furthermore, these methods do not consider the union of overlapping pairwise interactions for disjoint higher-order interaction detection. Our choice of contexts and our disjoint interaction detection are both important to the Archipelago framework, as we discussed in \u00a74.2 and showed through axiomatic analysis (\u00a73.2) and experiments (\u00a75.2)."
    },
    {
      "heading": "J Runtime",
      "text": ""
    },
    {
      "heading": "I Attributions Compared to Annotation Labels",
      "text": "K Visualization Comparisons"
    },
    {
      "heading": "K.1 Sentiment Analysis",
      "text": "Visualization comparisons of different attribution methods on BERT are shown in Figs. 11-15 for random test sentences from SST. The visualization format is the same as Fig. 4. Note that all individual feature attributions that correspond to stop words (from [33]) are omitted in these comparisons and Figs. 1, 4.\nK.2 Image Classification\nIn Fig. 10, we visualize Archipelago explanations on S via top-5 pairwise interactions (\u00a74.2.2), where positive attribution interactions are shown for clarity. The images are randomly selected from the ImageNet test set. It is interesting to see which image parts interact, such as the eyes of the \u201cgreat dane\u201d image.\nVisualization comparisons of different attribution methods on ResNet152 are shown in Figs. 16-20 for the same random test images from ImageNet.\nL ArchDetect Ablation Visualizations\nWe run an ablation study removing the x\u2032\\{i,j} baseline context from (5) for disjoint interaction detection and examine its effect on visualizations. The visualizations are shown in Fig. 21 for sentiment analysis and Figs. 22 and 23 for image classification. Top-3 and top-5 pairwise interactions are used in sentiment analysis and image classification respectively before merging the interactions.\nText input: \"I regret to report that these ops are just not extreme enough .\" Classification: neg\nArchipelago i regret to report that these ops are just not extreme enough .\nneg\npos"
    },
    {
      "heading": "Difference +",
      "text": "ArchDetect i regret to report that these ops are just not extreme enough ."
    },
    {
      "heading": "IG i regret to report that these ops are just not extreme enough .",
      "text": "IG + ArchDetect i regret to report that these ops are just not extreme enough .\nIH\n0.50 0.25 0.00 0.25 0.50 0.75 1.00 Attribution (normalized)\nregret, not not, extreme\ni, not i, regret ops, enoughin ter\nac tio\nn\nLIME i regret to report that these ops are just not extreme enough .\nMahe\n1.0 0.5 0.0 0.5 1.0 Attribution (normalized)\nregret, report\nregret, just, not, extreme, enoughin\nter ac\ntio n"
    },
    {
      "heading": "SI i regret to report that these ops are just not extreme enough .",
      "text": "1.0 0.5 0.0 0.5 Attribution (normalized)\nto, extreme not, enough not, extreme regret, just\nare, justin ter\nac tio\nn\nSTI i regret to report that these ops are just not extreme enough .\n0.0 0.1 0.2 0.3 0.4 0.5 0.6 Attribution (normalized)\nnot, extreme regret, not regret, just\nregret, enough just, enoughin\nter ac\ntio n\nText input: \"It 's a worse sign when you begin to envy her condition .\" Classification: neg\nArchipelago it ' s a worse sign when you begin to envy her condition ."
    },
    {
      "heading": "Difference +",
      "text": "ArchDetect it ' s a worse sign when you begin to envy her condition ."
    },
    {
      "heading": "IG it ' s a worse sign when you begin to envy her condition .",
      "text": "IG + ArchDetect it ' s a worse sign when you begin to envy her condition .\nIH\n1.0 0.5 0.0 0.5 1.0 Attribution (normalized)\nit, worse a, worse worse, you worse, envy\nworse, conditionin ter\nac tio\nn\nLIME it ' s a worse sign when you begin to envy her condition .\nMahe\n0.5 0.0 0.5 1.0 Attribution (normalized)\nworse, condition\nsign, when, you, begin, to, envyin\nter ac\ntio n"
    },
    {
      "heading": "SI it ' s a worse sign when you begin to envy her condition .",
      "text": "0.50 0.25 0.00 0.25 0.50 0.75 1.00 Attribution (normalized)\nworse, condition you, condition\nbegin, envy sign, condition worse, youin ter\nac tio\nn\nSTI it ' s a worse sign when you begin to envy her condition .\n0.1 0.0 0.1 0.2 Attribution (normalized)\nworse, condition to, envy\nworse, envy envy, her worse, beginin ter\nac tio\nn\nFigure 11: Text Viz. Comparison A. In the first text example, \u201cregret, not extreme enough\u201d is a meaningful and strongly negative interaction. In the second example, \u201cwhen you begin to\u201d interacts to diminish its overall attribution magnitude.\nText input: \"It 's solid and affecting and exactly as thought-provoking as it should be .\" Classification: pos\nArchipelago it ' s solid and affecting and exactly as thought - pro -voking as it should be .\nneg\npos"
    },
    {
      "heading": "Difference +",
      "text": "ArchDetect it ' s solid and affecting and exactly as thought - pro -voking as it should be ."
    },
    {
      "heading": "IG it ' s solid and affecting and exactly as thought - pro -voking as it should be .",
      "text": "IG + ArchDetect it ' s solid and affecting and exactly as thought - pro -voking as it should be .\nIH\n0.0 0.2 0.4 0.6 0.8 1.0 Attribution (normalized)\ns, solid solid, affecting\nsolid, as solid, and and, affectingin ter\nac tio\nn\nLIME it ' s solid and affecting and exactly as thought - pro -voking as it should be .\nMahe\n1.0 0.8 0.6 0.4 0.2 0.0 Attribution (normalized)\nsolid, affecting, -voking\nsolid, pro solid, andin ter\nac tio\nn"
    },
    {
      "heading": "SI it ' s solid and affecting and exactly as thought - pro -voking as it should be .",
      "text": "0.25 0.00 0.25 0.50 0.75 1.00 Attribution (normalized)\nas, it and, -voking\nsolid, -voking solid, affecting pro, -vokingin ter\nac tio\nn\nSTI it ' s solid and affecting and exactly as thought - pro -voking as it should be .\n0.2 0.0 0.2 0.4 Attribution (normalized)\npro, -voking solid, affecting solid, -voking and, affecting\nas, shouldin ter\nac tio\nn\nText input: \"A lousy movie that 's not merely unwatchable , but also unlistenable .\" Classification: neg\nArchipelago a lou -sy movie that ' s not merely un -watch -able , but also un -list -ena -ble ."
    },
    {
      "heading": "Difference +",
      "text": "ArchDetect a lou -sy movie that ' s not merely un -watch -able , but also un -list -ena -ble ."
    },
    {
      "heading": "IG a lou -sy movie that ' s not merely un -watch -able , but also un -list -ena -ble .",
      "text": "IG + ArchDetect a lou -sy movie that ' s not merely un -watch -able , but also un -list -ena -ble .\nIH\n1.0 0.5 0.0 0.5 1.0 Attribution (normalized)\nlou, un lou, -sy a, lou -sy, un\n,, butin ter\nac tio\nn\nLIME a lou -sy movie that ' s not merely un -watch -able , but also un -list -ena -ble .\nMahe\n0.0 0.2 0.4 0.6 0.8 1.0 Attribution (normalized)\nlou, -sy, un\nnot, merely, un in ter\nac tio\nn"
    },
    {
      "heading": "SI a lou -sy movie that ' s not merely un -watch -able , but also un -list -ena -ble .",
      "text": "0.5 0.0 0.5 1.0 Attribution (normalized)\n-sy, un lou, un lou, -sy un, un\nbut, unin ter\nac tio\nn\nSTI a lou -sy movie that ' s not merely un -watch -able , but also un -list -ena -ble .\n1.0 0.5 0.0 0.5 1.0 Attribution (normalized)\nlou, -sy not, merely\nun, un lou, un -sy, unin\nter ac\ntio n\nFigure 12: Text Viz. Comparison B. In the first text example, \u201cthought provoking\u201d is a meaningful and strongly positive interaction. In the second example, the \u201clousy, un\u201d interaction factors in a large context to make a negative text classification.\nText input: \"Tsai Ming-liang has taken his trademark style and refined it to a crystalline point .\" Classification: pos\nArchipelago ts -ai ming - liang has taken his trademark style and refined it to a crystalline point .\nneg\npos"
    },
    {
      "heading": "Difference +",
      "text": "ArchDetect ts -ai ming - liang has taken his trademark style and refined it to a crystalline point ."
    },
    {
      "heading": "IG ts -ai ming - liang has taken his trademark style and refined it to a crystalline point .",
      "text": "IG + ArchDetect ts -ai ming - liang has taken his trademark style and refined it to a crystalline point .\nIH\n1.0 0.8 0.6 0.4 0.2 0.0 Attribution (normalized)\nrefined, crystalline style, refined trademark, refined ming, refined\n-ai, refinedin ter\nac tio\nn\nLIME ts -ai ming - liang has taken his trademark style and refined it to a crystalline point .\nMahe\n0.50 0.25 0.00 0.25 0.50 0.75 1.00 Attribution (normalized)\nhas, taken a, crystalline\nrefined, a his, refined refined, itin ter\nac tio\nn"
    },
    {
      "heading": "SI ts -ai ming - liang has taken his trademark style and refined it to a crystalline point .",
      "text": "0.2 0.0 0.2 0.4 0.6 0.8 1.0 Attribution (normalized)\ntrademark, a has, taken refined, crystalline a, crystalline\nand, itin ter\nac tio\nn\nSTI ts -ai ming - liang has taken his trademark style and refined it to a crystalline point .\n0.2 0.1 0.0 0.1 0.2 Attribution (normalized)\nrefined, crystalline refined, to has, taken\nhis, trademark a, crystallinein\nter ac\ntio n\nText input: \"As an actor , The Rock is aptly named .\" Classification: pos\nArchipelago as an actor , the rock is apt -ly named ."
    },
    {
      "heading": "Difference +",
      "text": "ArchDetect as an actor , the rock is apt -ly named ."
    },
    {
      "heading": "IG as an actor , the rock is apt -ly named .",
      "text": "IG + ArchDetect as an actor , the rock is apt -ly named .\nIH\n0.5 0.0 0.5 1.0 Attribution (normalized)\napt, named an, apt\nan, , apt, -ly an, rockin ter\nac tio\nn\nLIME as an actor , the rock is apt -ly named .\nMahe\n0.25 0.00 0.25 0.50 0.75 1.00 Attribution (normalized)\napt, -ly, named actor, the actor, apt\nis, -lyin ter\nac tio\nn"
    },
    {
      "heading": "SI as an actor , the rock is apt -ly named .",
      "text": "0.4 0.2 0.0 0.2 0.4 0.6 0.8 Attribution (normalized)\nan, -ly is, apt -ly, named rock, -ly\nrock, aptin ter\nac tio\nn\nSTI as an actor , the rock is apt -ly named .\n0.50 0.25 0.00 0.25 0.50 0.75 1.00 Attribution (normalized)\nis, apt -ly, named\napt, -ly an, actor actor, namedin ter\nac tio\nn\nFigure 13: Text Viz. Comparison C. In the first text example, \u201crefined, to a crystalline\u201d is a meaningful and strongly positive interaction. In the second example, \u201cis aptly named\u201d is also a meaningful and strongly positive interaction.\nText input: \"The ending is a cop-out .\" Classification: neg\nArchipelago the ending is a cop - out .\nneg\npos"
    },
    {
      "heading": "Difference +",
      "text": "ArchDetect the ending is a cop - out ."
    },
    {
      "heading": "IG the ending is a cop - out .",
      "text": "IG + ArchDetect the ending is a cop - out .\nIH\n0.0 0.2 0.4 0.6 0.8 1.0 Attribution (normalized)\nis, cop is, out\ncop, out ending, is is, ain ter\nac tio\nn\nLIME the ending is a cop - out .\nMahe\n1.0 0.5 0.0 0.5 Attribution (normalized)\nthe, ending, cop is, a\na, out is, copin\nter ac\ntio n"
    },
    {
      "heading": "SI the ending is a cop - out .",
      "text": "0.50 0.25 0.00 0.25 0.50 0.75 Attribution (normalized)\nis, . ending, out the, ending\nis, a cop, -in\nter ac\ntio n\nSTI the ending is a cop - out .\n0.0 0.1 0.2 0.3 Attribution (normalized)\nthe, ending ending, out\na, cop is, a is, .in\nter ac\ntio n\nText input: \"A feel-good picture in the best sense of the term .\" Classification: pos\nArchipelago a feel - good picture in the best sense of the term ."
    },
    {
      "heading": "Difference +",
      "text": "ArchDetect a feel - good picture in the best sense of the term ."
    },
    {
      "heading": "IG a feel - good picture in the best sense of the term .",
      "text": "IG + ArchDetect a feel - good picture in the best sense of the term .\nIH\n0.25 0.00 0.25 0.50 0.75 1.00 Attribution (normalized)\nthe, best best, of best, sense best, the\nin, bestin ter\nac tio\nn\nLIME a feel - good picture in the best sense of the term .\nMahe\n1.00 0.75 0.50 0.25 0.00 0.25 0.50 Attribution (normalized)\nfeel, good, best\ngood, in, best in ter\nac tio\nn"
    },
    {
      "heading": "SI a feel - good picture in the best sense of the term .",
      "text": "1.0 0.8 0.6 0.4 0.2 0.0 0.2 Attribution (normalized)\nin, best a, feel good, best feel, good\ngood, inin ter\nac tio\nn\nSTI a feel - good picture in the best sense of the term .\n0.6 0.4 0.2 0.0 0.2 0.4 Attribution (normalized)\ngood, best a, feel feel, good in, best\nbest, sensein ter\nac tio\nn\nFigure 14: Text Viz. Comparison D. In the first text example, \u201cthe ending, out\u201d is a meaningful and negative interaction. In the second example, \u201ca feel good, best\u201d is a meaningful and strongly positive interaction.\nText input: \"All prints of this film should be sent to and buried on Pluto .\" Classification: neg\nArchipelago all prints of this film should be sent to and buried on pluto .\nneg\npos"
    },
    {
      "heading": "Difference +",
      "text": "ArchDetect all prints of this film should be sent to and buried on pluto ."
    },
    {
      "heading": "IG all prints of this film should be sent to and buried on pluto .",
      "text": "IG + ArchDetect all prints of this film should be sent to and buried on pluto .\nIH\n1.0 0.5 0.0 0.5 Attribution (normalized)\nshould, be all, this and, on of, and\nto, buriedin ter\nac tio\nn\nLIME all prints of this film should be sent to and buried on pluto .\nMahe\n0.25 0.00 0.25 0.50 0.75 1.00 Attribution (normalized)\nall, prints to, buried sent, and, buried buried, plutoin ter ac\ntio n"
    },
    {
      "heading": "SI all prints of this film should be sent to and buried on pluto .",
      "text": "1.00 0.75 0.50 0.25 0.00 0.25 Attribution (normalized)\nfilm, sent film, on to, buried all, buried\nbe, sentin ter\nac tio\nn\nSTI all prints of this film should be sent to and buried on pluto .\n0.75 0.50 0.25 0.00 0.25 0.50 Attribution (normalized)\nbe, buried sent, buried\nsent, to all, should be, toin ter\nac tio\nn\nText input: \"Arguably the year 's silliest and most incoherent movie .\" Classification: neg\nArchipelago arguably the year ' s si -llie -st and most inc -oh -ere -nt movie ."
    },
    {
      "heading": "Difference +",
      "text": "ArchDetect arguably the year ' s si -llie -st and most inc -oh -ere -nt movie ."
    },
    {
      "heading": "IG arguably the year ' s si -llie -st and most inc -oh -ere -nt movie .",
      "text": "IG + ArchDetect arguably the year ' s si -llie -st and most inc -oh -ere -nt movie .\nIH\n1.0 0.5 0.0 0.5 Attribution (normalized)\nyear, s arguably, .\ninc, -oh year, movie arguably, yearin ter\nac tio\nn\nLIME arguably the year ' s si -llie -st and most inc -oh -ere -nt movie .\nMahe\n0.5 0.0 0.5 1.0 Attribution (normalized)\nthe, year arguably, most\n-oh, -ere, -nt arguably, the inc, -ohin ter\nac tio\nn"
    },
    {
      "heading": "SI arguably the year ' s si -llie -st and most inc -oh -ere -nt movie .",
      "text": "0.2 0.0 0.2 0.4 0.6 0.8 1.0 Attribution (normalized)\nsi, most the, inc\n', -nt -st, most arguably, thein ter\nac tio\nn\nSTI arguably the year ' s si -llie -st and most inc -oh -ere -nt movie .\n0.6 0.4 0.2 0.0 0.2 0.4 0.6 Attribution (normalized)\ninc, -oh arguably, the\nyear, movie -ere, movie si, -lliein ter\nac tio\nn\nFigure 15: Text Viz. Comparison E. In the first text example, \u201cfilm should be, buried\u201d is a meaningful and strongly negative interaction. In the second example, \u201c-oherent\u201d belongs to a negative word \u201cincohorent\u201d.\nText input: \"I regret to report that these ops are just not extreme enough .\" Classification: neg\nw/ Baseline Context i regret to report that these ops are just not extreme enough .\nneg\npos\nw/o Baseline Context i regret to report that these ops are just not extreme enough .\nText input: \"It 's a worse sign when you begin to envy her condition .\" Classification: neg\nw/ Baseline Context it ' s a worse sign when you begin to envy her condition .\nw/o Baseline Context it ' s a worse sign when you begin to envy her condition .\nText input: \"It 's solid and affecting and exactly as thought-provoking as it should be .\" Classification: pos\nw/ Baseline Context it ' s solid and affecting and exactly as thought - pro -voking as it should be .\nw/o Baseline Context it ' s solid and affecting and exactly as thought - pro -voking as it should be .\nText input: \"A lousy movie that 's not merely unwatchable , but also unlistenable .\" Classification: neg\nw/ Baseline Context a lou -sy movie that ' s not merely un -watch -able , but also un -list -ena -ble .\nw/o Baseline Context a lou -sy movie that ' s not merely un -watch -able , but also un -list -ena -ble .\nText input: \"Tsai Ming-liang has taken his trademark style and refined it to a crystalline point .\" Classification: pos\nw/ Baseline Context ts -ai ming - liang has taken his trademark style and refined it to a crystalline point .\nw/o Baseline Context ts -ai ming - liang has taken his trademark style and refined it to a crystalline point .\nText input: \"As an actor , The Rock is aptly named .\" Classification: pos\nw/ Baseline Context as an actor , the rock is apt -ly named .\nw/o Baseline Context as an actor , the rock is apt -ly named .\nText input: \"The ending is a cop-out .\" Classification: neg\nw/ Baseline Context the ending is a cop - out .\nw/o Baseline Context the ending is a cop - out .\nText input: \"A feel-good picture in the best sense of the term .\" Classification: pos\nw/ Baseline Context a feel - good picture in the best sense of the term .\nw/o Baseline Context a feel - good picture in the best sense of the term .\nText input: \"All prints of this film should be sent to and buried on Pluto .\" Classification: neg\nw/ Baseline Context all prints of this film should be sent to and buried on pluto .\nw/o Baseline Context all prints of this film should be sent to and buried on pluto .\nText input: \"Arguably the year 's silliest and most incoherent movie .\" Classification: neg"
    }
  ],
  "title": "How does this interaction affect me? Interpretable attribution for feature interactions",
  "year": 2020
}
