{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "SOURCE_DIR = '../data/arxiv_dump/sources'\n",
    "sources = [f for f in os.listdir(SOURCE_DIR) if not f.endswith(\"tar.gz\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_files(s):\n",
    "    paper_dir = os.path.join(SOURCE_DIR, s)\n",
    "    return [os.path.join(paper_dir, f) for f in os.listdir(paper_dir)]\n",
    "\n",
    "\n",
    "def get_tex_files(s):\n",
    "    return [f for f in get_all_files(s) if f.endswith(\".tex\")]\n",
    "\n",
    "def clean_text(text, prune_document=True):\n",
    "    pos = text.find(\"\\\\begin{abstract}\")\n",
    "    if pos >= 0:\n",
    "        text = text[:pos]\n",
    "    lines = text.split(\"\\n\")\n",
    "    lines = [line for line in lines if not line.startswith(\"%\")]\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "def get_text(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        try:\n",
    "            return f.read()\n",
    "        except UnicodeDecodeError:\n",
    "            # print('Failed to read the file {}'.format(path))\n",
    "            return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_mail_domains(s):\n",
    "    emails = []\n",
    "    tex_files = get_tex_files(s)\n",
    "    for f in tex_files:\n",
    "        text = clean_text(get_text(f))\n",
    "        emails += re.findall('@[a-z-_\\.]+\\.[a-z-_\\.]+', text)\n",
    "    emails = [m[1:] for m in emails]\n",
    "    return emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "587"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.shuffle(sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_set = OrderedDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_affiliations_entry(name_version):\n",
    "    d = dict()\n",
    "    d['names'] = []\n",
    "    d['mail_domains'] = extract_mail_domains(name_version)\n",
    "    d['dbpedia_ids'] = []\n",
    "    d['types'] = []\n",
    "    return d\n",
    "\n",
    "def generate_entry(name, name_version):\n",
    "    entry = dict()\n",
    "    entry['id'] = name\n",
    "    entry['versioned_id'] = name_version\n",
    "    entry['manually_annotated'] = False\n",
    "    entry['affiliations'] = generate_affiliations_entry(name_version)\n",
    "    return entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in sources:\n",
    "    pruned_version = s.split('v')[0]\n",
    "    annotated_set[pruned_version] = generate_entry(pruned_version, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_phrases = {\n",
    "    \"universit\",\n",
    "    \"school\",\n",
    "    \"college\",\n",
    "    \"institut\",\n",
    "    \"academy\",\n",
    "    \"universidad\",\n",
    "    \"polyte\",\n",
    "    \"schule\",\n",
    "    \"ecole\",\n",
    "    \"escuela\",\n",
    "}\n",
    "\n",
    "def detect_type(aff_name):\n",
    "    for uni in uni_phrases:\n",
    "        if uni in str.lower(aff_name):\n",
    "            return 'academic'\n",
    "    return 'company'\n",
    "\n",
    "def annotate_types(aff):\n",
    "    if aff['types'] == []:\n",
    "        return [detect_type(aff_name) for aff_name in aff['names']]\n",
    "    else:\n",
    "        return aff['types']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 659 column 80 (char 14718)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-163-b161d22af110>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mold_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mold_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/json/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m         parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \"\"\"\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 659 column 80 (char 14718)"
     ]
    }
   ],
   "source": [
    "import json\n",
    "dataset = '../data/affiliations_annotated.json'\n",
    "try:\n",
    "    with open(dataset, 'r') as f:\n",
    "        old_data = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    old_data = dict()\n",
    "    \n",
    "new_data = old_data.copy()\n",
    "manual_count = 0\n",
    "\n",
    "for k, v in annotated_set.items():\n",
    "    if k not in old_data or old_data[k]['manually_annotated'] == False:\n",
    "        new_data[k] = v\n",
    "    else:\n",
    "        types = annotate_types(new_data[k][\"affiliations\"])\n",
    "        print(types)\n",
    "        new_data[k][\"affiliations\"][\"types\"] = types\n",
    "        manual_count += 1\n",
    "    \n",
    "print('Manually labelled: {}/{}'.format(manual_count, len(sources)))\n",
    "        \n",
    "        \n",
    "with open('../data/affiliations_annotated.json', 'w') as f:\n",
    "    json.dump(new_data, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(s):\n",
    "    tex_files = get_tex_files(s)\n",
    "    for f in tex_files:\n",
    "        text = clean_text(get_text(f))\n",
    "        print(text)\n",
    "        print()\n",
    "        print('-' * 50)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Types of affiliation\n",
    "* academic\n",
    "* company\n",
    "* government"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2009.10990v1\n",
      "\\documentclass[letterpaper]{article} % DO NOT CHANGE THIS\n",
      "\\usepackage{aaai21}  % DO NOT CHANGE THIS\n",
      "\\usepackage{times}  % DO NOT CHANGE THIS\n",
      "\\usepackage{helvet} % DO NOT CHANGE THIS\n",
      "\\usepackage{courier}  % DO NOT CHANGE THIS\n",
      "\\usepackage[hyphens]{url}  % DO NOT CHANGE THIS\n",
      "\\usepackage{graphicx} % DO NOT CHANGE THIS\n",
      "\\urlstyle{rm} % DO NOT CHANGE THIS\n",
      "\\def\\UrlFont{\\rm}  % DO NOT CHANGE THIS\n",
      "\\usepackage{natbib}  % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT\n",
      "\\usepackage{lipsum}\n",
      "\\usepackage{booktabs}\n",
      "\\usepackage{amsmath}\n",
      "\\usepackage{caption} % DO NOT CHANGE THIS AND DO NOT ADD ANY OPTIONS TO IT\n",
      "\\frenchspacing  % DO NOT CHANGE THIS\n",
      "\\setlength{\\pdfpagewidth}{8.5in}  % DO NOT CHANGE THIS\n",
      "\\setlength{\\pdfpageheight}{11in}  % DO NOT CHANGE THIS\n",
      "\\usepackage{endnotes}\n",
      "\\let\\footnote=\\endnote\n",
      "\\usepackage{etoolbox}\n",
      "\\makeatletter\n",
      "\\patchcmd{\\@verbatim}\n",
      "  {\\verbatim@font}\n",
      "  {\\verbatim@font\\tiny}\n",
      "  {}{}\n",
      "\\makeatother\n",
      "\n",
      "\\setcounter{secnumdepth}{0} %May be changed to 1 or 2 if section numbers are desired.\n",
      "\n",
      "\n",
      "\\title{Accurate and Interpretable Machine Learning for Transparent Pricing of Health Insurance Plans}\n",
      "\\author {\n",
      "    % Authors\n",
      "\\parbox{\\linewidth}{\\centering\n",
      "        Rohun Kshirsagar,\n",
      "        Li-Yen Hsu,\n",
      "        Charles H. Greenberg, \n",
      "        Matthew McClelland,\n",
      "        Anushadevi Mohan,\n",
      "        Wideet Shende,\n",
      "        Nicolas P. Tilmans,\n",
      "        Min Guo,\n",
      "        Ankit Chheda,\n",
      "        Meredith Trotter,\n",
      "        Shonket Ray,\n",
      "        Miguel Alvarado\n",
      "        }\n",
      "}\n",
      "\\affiliations{\n",
      "   Lumiata Inc. 489 S. El Camino Real, San Mateo, CA 94402 USA\n",
      "    \n",
      "    Corresponding Author: Rohun Kshirsagar - rohun@lumiata.com\n",
      "}\n",
      "\n",
      "\\begin{document}\n",
      "\\nocopyright\n",
      "\\maketitle\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "no = 49\n",
    "print(sources[no])\n",
    "display(sources[no])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
