{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from mair.pdf_parsing import parse\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.pipeline.functions import merge_entities\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.dummy import DummyClassifier\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from typing import List, Dict\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "from joblib import dump, load\n",
    "import seaborn as sns\n",
    "\n",
    "tqdm.pandas()\n",
    "DATASET_PATH = '../data/legal_docs_recognition/'\n",
    "LEGAL_DOCS_PATH = os.path.join(DATASET_PATH, 'legal_docs.json')\n",
    "NON_LEGAL_DOCS_PATH = os.path.join(DATASET_PATH, 'nonlegal_docs.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_docs = glob('../data/policydemics/other/*/*.pdf')\n",
    "singapore_docs = glob('../data/policydemics/singapore/*/*.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(other_docs), len(singapore_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_texts_from_pdfs(paths: List[str]) -> Dict[str,str]:\n",
    "    \"\"\"Extract text from pdfs under given paths. Returns maping path -> text \"\"\"\n",
    "    parsed_texts = dict()\n",
    "    for path in tqdm(paths):\n",
    "        try:\n",
    "            parsed_pdf = parse(path)\n",
    "            parsed_texts[path] = parsed_pdf.full_text\n",
    "        except Exception as e:\n",
    "            print(path, 'exception:', e)\n",
    "    return parsed_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_texts = get_texts_from_pdfs(other_docs + singapore_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split to legal docs and not legal docs\n",
    "legal_docs = dict()\n",
    "not_legal_docs = dict()\n",
    "for original_path, text in parsed_texts.items():\n",
    "    name = os.path.basename(original_path)[:-4]\n",
    "    if \"S_L\" in original_path or \"nS_L\" in original_path:\n",
    "        legal_docs[name] = text\n",
    "    elif \"S_nL\" in original_path or \"nS_nL\" in original_path:\n",
    "        not_legal_docs[name] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(legal_docs, open(LEGAL_DOCS_PATH, 'w'))\n",
    "json.dump(not_legal_docs, open(NON_LEGAL_DOCS_PATH, 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    legal_docs_texts = json.load(open(LEGAL_DOCS_PATH, 'r')).values()\n",
    "    nonlegal_docs_texts = json.load(open(NON_LEGAL_DOCS_PATH, 'r')).values()\n",
    "\n",
    "    nonlegal_docs_texts\n",
    "    df = pd.DataFrame(\n",
    "        [{\"text\": text, \"label\": 1} for text in legal_docs_texts]\n",
    "        + [{\"text\": text, \"label\": 0} for text in nonlegal_docs_texts]\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.label.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, stratify=df.label, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def get_processed_words(doc):\n",
    "    words = [t.ent_type_ if t.ent_type_ else t.lemma_.lower() for t in doc if not t.is_stop and (t.is_alpha or t.ent_type_)]\n",
    "    joined_words = ' '.join(words)\n",
    "    return joined_words\n",
    "\n",
    "def preprocess_text(texts):\n",
    "    d = texts.str.replace('\\n',' ')\n",
    "    d = d.str.replace(' +', ' ')\n",
    "    docs = d.progress_apply(nlp)\n",
    "    docs = docs.progress_apply(merge_entities)\n",
    "    out_texts = docs.progress_apply(get_processed_words)\n",
    "    return out_texts\n",
    "\n",
    "text_preprocessing = FunctionTransformer(func = preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing, model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutual_info_selector = SelectKBest(mutual_info_classif, k=10000)\n",
    "recurse_importance_selector = RFECV(\n",
    "    estimator=LogisticRegression(penalty=\"l1\", solver=\"saga\"),\n",
    "    min_features_to_select=20,\n",
    "    n_jobs=-1,\n",
    "    verbose=True,\n",
    "    step=10,\n",
    ")\n",
    "classifier = LogisticRegression(penalty=\"l2\")\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"preprocessing\",\n",
    "            Pipeline(\n",
    "                [\n",
    "                    (\"text_processor\", text_preprocessing),\n",
    "                    (\"count_vectorizer\", CountVectorizer()),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "        (\n",
    "            \"feature_selection\",\n",
    "            Pipeline(\n",
    "                [\n",
    "                    (\"mutual_info_selector\", mutual_info_selector),\n",
    "                    (\"recurse_importance_selector\", recurse_importance_selector),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "        (\"classifier\", classifier),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train.text\n",
    "y_train = train.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(X_train)\n",
    "\n",
    "accuracy_score(y_train,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test.text\n",
    "y_test = test.label\n",
    "\n",
    "pred_test = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test,pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = DummyClassifier().fit(X_train, y_train)\n",
    "accuracy_score(y_test,baseline.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = np.array(\n",
    "    pipeline.named_steps[\"preprocessing\"]\n",
    "    .named_steps[\"count_vectorizer\"]\n",
    "    .get_feature_names()\n",
    ")[\n",
    "    pipeline.named_steps[\"feature_selection\"]\n",
    "    .named_steps[\"mutual_info_selector\"]\n",
    "    .get_support()\n",
    "][\n",
    "    pipeline.named_steps[\"feature_selection\"]\n",
    "    .named_steps[\"recurse_importance_selector\"]\n",
    "    .get_support()\n",
    "]\n",
    "\n",
    "data = {\n",
    "    \"feature_names\": feature_names,\n",
    "    \"feature_importance\": pipeline.named_steps[\"classifier\"].coef_[0],\n",
    "}\n",
    "fi_df = pd.DataFrame(data)\n",
    "\n",
    "# Sort the DataFrame in order decreasing feature importance\n",
    "fi_df.sort_values(by=[\"feature_importance\"], ascending=False, inplace=True)\n",
    "\n",
    "# Define size of bar plot\n",
    "plt.figure(figsize=(12, 18))\n",
    "# Plot Searborn bar chart\n",
    "sns.barplot(x=fi_df[\"feature_importance\"], y=fi_df[\"feature_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_proba_test = pipeline.predict_proba(X_test)\n",
    "fpr, tpr, _ = metrics.roc_curve(y_test,  pred_proba_test[:,1])\n",
    "plt.plot(fpr,tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dump(pipeline, 'classification_pipeline.joblib') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[y_test!=pred_test][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
