{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from mair.pdf_parsing import parse\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "import spacy\n",
    "from collections import defaultdict\n",
    "from spacy import displacy\n",
    "from mair.data_loading import load_legal_documents, load_legal_documents_metadata\n",
    "from mair.doc_ids import legal_doc_path_to_id\n",
    "import joblib\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import mair.coreference_resulution #importing to set spacy's extensions\n",
    "import types\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EmptyToken:\n",
    "    lemma_=\"\"\n",
    "    children=[]\n",
    "    def __len__(self):\n",
    "        return 0\n",
    "\n",
    "EMPTY_TOKEN = EmptyToken()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.load('en_core_web_sm')\n",
    "# nlp.max_length=3000000\n",
    "tqdm.pandas()\n",
    "os.chdir('../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"../data/oecd_meta.csv\")\n",
    "COULD = \"can\"\n",
    "SHOULD = \"shall\"\n",
    "MUST = \"must\"\n",
    "MODAL_VERBS_MAPPING = {\n",
    "    \"can\": COULD,\n",
    "    \"could\": COULD,\n",
    "    \"may\": COULD,\n",
    "    \"might\": COULD,\n",
    "    \"shall\": SHOULD,\n",
    "    \"should\": SHOULD,\n",
    "    \"must\": MUST,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = joblib.load(\"data/processed/intermediate/parsed_legal_texts.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.Series(r, name=\"doc\")\n",
    "df = df.reset_index()\n",
    "df = df.set_index(\"index\")\n",
    "docs = df[\"doc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_conjucted_tokens(token):\n",
    "    tokens = []\n",
    "    for child in token.children:\n",
    "        if child.dep_ == \"conj\":\n",
    "            tokens.append(child)\n",
    "    return tokens\n",
    "\n",
    "def find_subjects(verb_token, modal_token):\n",
    "    subject = [child for child in verb_token.children if child.dep_ == \"nsubj\"]\n",
    "    passive_subject = [child for child in verb.children if child.dep_ == \"nsubjpass\"]\n",
    "    csubj = [\n",
    "        c\n",
    "        for child in verb.children\n",
    "        if child.dep_ == \"csubj\"\n",
    "        for c in child.children\n",
    "        if c.dep_ == \"nsubj\"\n",
    "    ]\n",
    "\n",
    "    if len(subject) == 0 and len(passive_subject) == 0:\n",
    "        # if no subject found, check if there is conjunction on verb, and add subjects of conjucted verb\n",
    "        if verb_token.dep_ == \"conj\":\n",
    "            head = verb_token.head\n",
    "            subject = [child for child in head.children if child.dep_ == \"nsubj\"]\n",
    "            passive_subject = [\n",
    "                child for child in head.children if child.dep_ == \"nsubjpass\"\n",
    "            ]\n",
    "    if len(subject) == 0 and len(passive_subject) == 0:\n",
    "        subject = [child for child in modal_token.children if child.dep_ == \"nsubj\"]\n",
    "        passive_subject = [\n",
    "            child for child in modal_token.children if child.dep_ == \"nsubjpass\"\n",
    "        ]\n",
    "\n",
    "    if len(subject) != 0:\n",
    "        subject += get_all_conjucted_tokens(subject[0])\n",
    "        # check conjucted subjects, and add them to subjects\n",
    "\n",
    "    return subject, passive_subject, csubj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modal_sentences = defaultdict(list)\n",
    "results = []\n",
    "for id, doc in docs.iteritems():\n",
    "    for token in doc:\n",
    "        modal = token.lemma_.lower()\n",
    "        if modal_category := MODAL_VERBS_MAPPING.get(modal):\n",
    "            verb = next(token.ancestors, EMPTY_TOKEN)\n",
    "            if len(verb) != 0:\n",
    "                subject, passive_subject, clausal_subject = find_subjects(verb, token)\n",
    "                negated = any([c.dep_ == \"neg\" for c in verb.children])\n",
    "            else:\n",
    "                subject = []\n",
    "                passive_subject = []\n",
    "                clausal_subject = []\n",
    "                negated = False\n",
    "            is_question = token.sent[-1].norm_ == \"?\" or token.sent[-2].norm_ == \"?\"\n",
    "            result = {\n",
    "                \"modal\": modal_category,\n",
    "                \"sent\": token.sent,  # .text.replace('\\n', ' '),\n",
    "                \"raw_text_path\": id,\n",
    "                \"verb\": verb,\n",
    "                \"subject\": subject,\n",
    "                \"passiveSubject\": passive_subject,\n",
    "                \"clausalSubject\": clausal_subject,\n",
    "                \"token\": token,\n",
    "                \"isQuestion\": is_question,\n",
    "                \"negated\": negated,\n",
    "            }\n",
    "            results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coref_text(tokens):\n",
    "    if len(tokens) == 0:\n",
    "        return \"\"\n",
    "    token = tokens[0]\n",
    "    corefs = token._.corefs\n",
    "    if len(corefs) == 0 or token.pos_ != \"PRON\":\n",
    "        return \"\"\n",
    "    return corefs[0]\n",
    "\n",
    "\n",
    "def get_subjects_from_noun_phrase(text):\n",
    "    \"\"\"Get all subjects from noun phrase\"\"\"\n",
    "    if text == \"\":\n",
    "        return []\n",
    "    doc = nlp(text)\n",
    "    root = None\n",
    "    for t in doc:\n",
    "        if t.dep_ == \"ROOT\":\n",
    "            root = t\n",
    "            break\n",
    "    if not root:\n",
    "        return []\n",
    "    subjects = [root] + get_all_conjucted_tokens(root)\n",
    "\n",
    "    return subjects\n",
    "\n",
    "\n",
    "result_df[\"subjectCorefText\"] = result_df[\"subject\"].apply(get_coref_text)\n",
    "result_df[\"subjectCoref\"] = result_df[\"subjectCorefText\"].apply(\n",
    "    get_subjects_from_noun_phrase\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_subjects(subjects, coref_subjects):\n",
    "    results=[]\n",
    "    append_coref=False\n",
    "    for s in subjects:\n",
    "        if s.pos_ == \"PRON\":\n",
    "            append_coref = True\n",
    "        else:\n",
    "            results.append(s.lemma_.lower())\n",
    "    \n",
    "    if append_coref:\n",
    "        for s in coref_subjects:\n",
    "            results.append(s.lemma_.lower())\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame()\n",
    "final_df[\"id\"] = result_df[\"raw_text_path\"].apply(legal_doc_path_to_id)\n",
    "final_df[\"verb\"] = result_df[\"verb\"].apply(lambda t: t.lemma_.lower())\n",
    "final_df[\"subject\"] = result_df.apply(\n",
    "    lambda row: final_subjects(row[\"subject\"], row[\"subjectCoref\"]), axis=1\n",
    ")\n",
    "final_df[\"sent\"] = result_df[\"sent\"].apply(\n",
    "    lambda sent: sent.text.replace(\"\\n\", \" \"),\n",
    ")\n",
    "final_df[\"modal\"] = result_df[\"modal\"]\n",
    "final_df[\"isQuestion\"] = result_df[\"isQuestion\"]\n",
    "final_df[\"negated\"] = result_df[\"negated\"]\n",
    "\n",
    "df_meta = load_legal_documents_metadata()\n",
    "final_df = final_df.merge(df_meta, on=\"id\")\n",
    "final_df = final_df.explode(\"subject\")  # convert lists to multiple rows}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('data/processed/deontics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.serve(result_df.sent.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pronouns distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subjs = result_df['subject'].str[0]\n",
    "subjs = subjs[~subjs.isna()]\n",
    "subjs = subjs[subjs.apply(lambda x: x.pos_=='PRON')]\n",
    "Counter([s.orth_.lower() for s in subjs])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
