{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "SOURCE_DIR = '../data/arxiv_dump/sources'\n",
    "sources = [f for f in os.listdir(SOURCE_DIR) if not f.endswith(\"tar.gz\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_files(s):\n",
    "    paper_dir = os.path.join(SOURCE_DIR, s)\n",
    "    return [os.path.join(paper_dir, f) for f in os.listdir(paper_dir)]\n",
    "\n",
    "\n",
    "def get_tex_files(s):\n",
    "    return [f for f in get_all_files(s) if f.endswith(\".tex\")]\n",
    "\n",
    "def clean_text(text, prune_document=True):\n",
    "    pos = text.find(\"\\\\begin{abstract}\")\n",
    "    if pos >= 0:\n",
    "        text = text[:pos]\n",
    "    lines = text.split(\"\\n\")\n",
    "    lines = [line for line in lines if not line.startswith(\"%\")]\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "def get_text(path):\n",
    "    with open(path, \"r\") as f:\n",
    "        try:\n",
    "            return f.read()\n",
    "        except UnicodeDecodeError:\n",
    "            # print('Failed to read the file {}'.format(path))\n",
    "            return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_mail_domains(s):\n",
    "    emails = []\n",
    "    tex_files = get_tex_files(s)\n",
    "    for f in tex_files:\n",
    "        text = clean_text(get_text(f))\n",
    "        emails += re.findall('@[a-z-_\\.]+\\.[a-z-_\\.]+', text)\n",
    "    emails = [m[1:] for m in emails]\n",
    "    return emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.shuffle(sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotated_set = OrderedDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_affiliations_entry(name_version):\n",
    "    d = dict()\n",
    "    d['names'] = []\n",
    "    d['mail_domains'] = extract_mail_domains(name_version)\n",
    "    d['dbpedia_ids'] = []\n",
    "    d['types'] = []\n",
    "    return d\n",
    "\n",
    "def generate_entry(name, name_version):\n",
    "    entry = dict()\n",
    "    entry['id'] = name\n",
    "    entry['versioned_id'] = name_version\n",
    "    entry['manually_annotated'] = False\n",
    "    entry['affiliations'] = generate_affiliations_entry(name_version)\n",
    "    return entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in sources:\n",
    "    pruned_version = s.split('v')[0]\n",
    "    annotated_set[pruned_version] = generate_entry(pruned_version, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from babelpy import babelfy\n",
    "API_KEY = '62cb486d-98aa-4f4b-b0cc-f1b6430491b4'\n",
    "babelfy_params = {\n",
    "    'lang': 'EN'\n",
    "    }\n",
    "babel_client = babelfy.BabelfyClient(API_KEY, babelfy_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dbpedia_links(text):\n",
    "    babel_client.babelfy(text)\n",
    "    ents_info = babel_client.merged_entities\n",
    "    return [ent['DBpediaURL'] for ent in ents_info]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_phrases = {\n",
    "    \"universit\",\n",
    "    \"school\",\n",
    "    \"college\",\n",
    "    \"institut\",\n",
    "    \"academ\",\n",
    "    \"universidad\",\n",
    "    \"polyte\",\n",
    "    \"schule\",\n",
    "    \"ecole\",\n",
    "    \"escuela\",\n",
    "}\n",
    "\n",
    "def detect_type(aff_name):\n",
    "    for uni in uni_phrases:\n",
    "        if uni in str.lower(aff_name):\n",
    "            return 'academic'\n",
    "    return 'company'\n",
    "\n",
    "def annotate_types(aff):\n",
    "    if aff['types'] == []:\n",
    "        return [detect_type(aff_name) for aff_name in aff['names']]\n",
    "    else:\n",
    "        return aff['types']\n",
    "    \n",
    "def flatten(ls):\n",
    "    return [el for l in ls for el in l]\n",
    "    \n",
    "def link_dbpedia(aff):\n",
    "    return flatten([get_dbpedia_links(name) for name in aff['names']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_dbpedia({'names': ['University of Warsaw', 'Google Research']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "dataset = '../data/affiliations_annotated.json'\n",
    "try:\n",
    "    with open(dataset, 'r') as f:\n",
    "        old_data = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    old_data = dict()\n",
    "    \n",
    "new_data = old_data.copy()\n",
    "manual_count = 0\n",
    "\n",
    "for k, v in annotated_set.items():\n",
    "    if k not in old_data or old_data[k]['manually_annotated'] == False:\n",
    "        new_data[k] = v\n",
    "    else:\n",
    "        affiliation_dict = new_data[k][\"affiliations\"]\n",
    "        types = annotate_types(affiliation_dict)\n",
    "        dbpedia_ids = link_dbpedia(affiliation_dict)\n",
    "        print(types)\n",
    "        print(affiliation_dict[\"names\"])\n",
    "        print(dbpedia_ids)\n",
    "        new_data[k][\"affiliations\"][\"types\"] = types\n",
    "        new_data[k][\"affiliations\"][\"dbpedia_ids\"] = dbpedia_ids\n",
    "        manual_count += 1\n",
    "    \n",
    "print('Manually labelled: {}/{}'.format(manual_count, len(sources)))\n",
    "        \n",
    "        \n",
    "with open('../data/affiliations_annotated.json', 'w') as f:\n",
    "    json.dump(new_data, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(s):\n",
    "    tex_files = get_tex_files(s)\n",
    "    for f in tex_files:\n",
    "        text = clean_text(get_text(f))\n",
    "        print(text)\n",
    "        print()\n",
    "        print('-' * 50)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Types of affiliation\n",
    "* academic\n",
    "* company\n",
    "* government"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no = 49\n",
    "print(sources[no])\n",
    "display(sources[no])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
