{"abstractText": "Artificial Intelligence has the potential to exacerbate societal bias and set back decades of advances in equal rights and civil liberty. Data used to train machine learning algorithms may capture social injustices, inequality or discriminatory attitudes that may be learned and perpetuated in society. Attempts to address this issue are rapidly emerging from different perspectives involving technical solutions, social justice and data governance measures. While each of these approaches are essential to the development of a comprehensive solution, often discourse associated with each seems disparate. This paper reviews ongoing work to ensure data justice, fairness and bias mitigation in AI systems from different domains exploring the interrelated dynamics of each and examining whether the inevitability of bias in AI training data may in fact be used for social good. We highlight the complexity associated with defining policies for dealing with bias. We also consider technical challenges in addressing issues of societal bias.", "authors": [], "id": "SP:fb491ee22867a4bc1574e66612ab8c5cae326309", "references": [{"authors": ["G. Adamson", "J.C. Havens", "R. Chatila"], "title": "Designing a value-driven future for ethical autonomous and intelligent systems", "venue": "Proceedings of the IEEE, 107(3):518\u2013525", "year": 2019}, {"authors": ["Ivana Bartoletti"], "title": "An Artificial Revolution: On Power", "venue": "Politics and AI. The Indigo Press,", "year": 2020}, {"authors": ["Ruha Benjamin"], "title": "Race after technology: Abolitionist tools for the new jim code", "venue": "John Wiley & Sons,", "year": 2019}, {"authors": ["Bessiere et al", "2009] Christian Bessiere", "Emmanuel Hebrard", "Barry O\u2019Sullivan"], "title": "Minimising decision tree size as combinatorial optimisation", "venue": "Principles and Practice of Constraint Programming CP 2009,", "year": 2009}, {"authors": ["Bolukbasi et al", "2016] Tolga Bolukbasi", "Kai-Wei Chang", "James Y Zou", "Venkatesh Saligrama", "Adam T Kalai"], "title": "Man is to computer programmer as woman is to homemaker? debiasing word embeddings", "venue": "In Advances in Neural Information Processing Systems,", "year": 2016}, {"authors": ["Joy Buolamwini", "Timnit Gebru"], "title": "Gender shades: Intersectional accuracy disparities in commercial gender classification", "venue": "Conference on Fairness, Accountability and Transparency, pages 77\u201391,", "year": 2018}, {"authors": ["Aylin Caliskan", "Joanna J Bryson", "Arvind Narayanan. Semantics derived automatically from language corpora contain human-like biases"], "title": "Science", "venue": "356(6334):183\u2013186,", "year": 2017}, {"authors": ["Kate Crawford", "Vladan Joler"], "title": "Anatomy of an ai system", "venue": "Retrieved from: https://anatomyof.ai/,", "year": 2018}, {"authors": ["Amit Datta", "Michael Carl Tschantz", "Anupam Datta. Automated experiments on ad privacy settings"], "title": "Proceedings on privacy enhancing technologies", "venue": "2015(1):92\u2013112,", "year": 2015}, {"authors": ["D\u2019Ignazio", "Klein", "2020] Catherine D\u2019Ignazio", "Lauren F Klein"], "title": "Data feminism", "year": 2020}, {"authors": ["Virginia Dignum"], "title": "Responsible Artificial Intelligence: How to Develop and Use AI in a Responsible Way", "venue": "Springer International Publishing,", "year": 2019}, {"authors": ["EU-HLEG-AI"], "title": "High-level expert group on artificial intelligence: Ethics guidelines for trustworthy ai", "venue": "European Commission, 09.04,", "year": 2019}, {"authors": ["Virginia Eubanks"], "title": "Automating inequality: How high-tech tools profile", "venue": "police, and punish the poor. St. Martin\u2019s Press,", "year": 2018}, {"authors": ["Feldman et al", "2015] Michael Feldman", "Sorelle A Friedler", "John Moeller", "Carlos Scheidegger", "Suresh Venkatasubramanian"], "title": "Certifying and removing disparate impact", "venue": "In proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data min-", "year": 2015}, {"authors": ["Timnit Gebru"], "title": "Oxford handbook on ai ethics book chapter on race and gender", "venue": "arXiv preprint arXiv:1908.06165,", "year": 2019}, {"authors": ["Amy Hawkins"], "title": "Beijing\u2019s big brother tech needs african faces", "venue": "Foreign Policy, 24,", "year": 2018}, {"authors": ["Christine B Hickman"], "title": "The devil and the one drop rule: Racial categories", "venue": "african americans, and the us census. Michigan Law Review, 95(5):1161\u20131265,", "year": 1997}, {"authors": ["Faisal Kamiran", "Toon Calders. Data preprocessing techniques for classification without discrimination"], "title": "Knowledge and Information Systems", "venue": "33(1):1\u201333,", "year": 2012}, {"authors": ["Kleinberg et al", "2017] Jon M. Kleinberg", "Sendhil Mullainathan", "Manish Raghavan"], "title": "Inherent trade-offs in the fair determination of risk scores", "venue": "editor, 8th Innovations in Theoretical Computer Science Conference,", "year": 2017}, {"authors": ["Anja Lambrecht", "Catherine Tucker"], "title": "Algorithmic bias? an empirical study of apparent gender-based discrimination in the display of stem career ads", "venue": "Management Science,", "year": 2019}, {"authors": ["Kristian Lum", "William Isaac"], "title": "To predict and serve? Significance", "venue": "13(5):14\u201319,", "year": 2016}, {"authors": ["Ninareh Mehrabi", "Fred Morstatter", "Nripsuta Saxena", "Kristina Lerman", "Aram Galstyan"], "title": "A survey on bias and fairness in machine learning", "venue": "arXiv preprint arXiv:1908.09635,", "year": 2019}, {"authors": ["Safiya Umoja Noble"], "title": "Algorithms of oppression: How search engines reinforce racism", "venue": "nyu Press,", "year": 2018}, {"authors": ["Ziad Obermeyer", "Brian Powers", "Christine Vogeli", "Sendhil Mullainathan. Dissecting racial bias in an algorithm used to manage the health of populations"], "title": "Science", "venue": "366(6464):447\u2013453,", "year": 2019}, {"authors": ["Ji Ho Park", "Jamin Shin", "Pascale Fung"], "title": "Reducing gender bias in abusive language detection", "venue": "arXiv preprint arXiv:1808.07231,", "year": 2018}, {"authors": ["Caroline Criado Perez"], "title": "Invisible Women: Exposing data bias in a world designed for men", "venue": "Random House,", "year": 2019}, {"authors": ["Rashida Richardson", "Jason Schultz", "Kate Crawford. Dirty data"], "title": "bad predictions: How civil rights violations impact police data", "venue": "predictive policing systems, and justice. New York University Law Review Online, Forthcoming,", "year": 2019}, {"authors": ["David Sumpter"], "title": "Outnumbered: From Facebook and Google to Fake News and Filter-bubbles \u2013 The Algorithms That Control Our Lives", "venue": "[Sumpter,", "year": 2018}, {"authors": ["Yi Chern Tan", "L Elisa Celis. Assessing social", "intersectional biases in contextualized word representations"], "title": "In Advances in Neural Information Processing Systems", "venue": "pages 13209\u201313220,", "year": 2019}, {"authors": ["Sandra Wachter", "Brent Mittelstadt"], "title": "A right to reasonable inferences: re-thinking data protection law in the age of big data and ai", "venue": "Colum. Bus. L. Rev., page 494,", "year": 2019}, {"authors": ["Jieyu Zhao", "Tianlu Wang", "Mark Yatskar", "Vicente Ordonez", "Kai-Wei Chang"], "title": "Gender bias in coreference resolution: Evaluation and debiasing methods", "venue": "arXiv preprint arXiv:1804.06876,", "year": 2018}], "sections": [{"text": "ar X\niv :2\n00 8.\n07 34\n1v 1\n[ cs\n.C Y\n] 2\n8 Ju\nl 2 02"}, {"heading": "1 Introduction", "text": "The detrimental effects of unfairness and bias in artificial intelligence (AI) on equal rights in society have been well documented. Predictive policing systems trained on historical police records are being increasingly used to forecast criminal behaviour even though the accuracy of this data has been called into question [Richardson et al., 2019]. Racial bias was uncovered in an algorithm used to support healthcare decisions [Obermeyer et al., 2019]. While bio-metric technology such as facial recognition is being increasingly integrated into core security and border control infrastructures, their accuracy is lowest on darker-skinned females [Buolamwini and Gebru, 2018]. In search engines and recommender systems, race and gender discrimination was uncovered across multiple online platforms [Datta et al., 2015; Lambrecht and Tucker, 2019; Noble, 2018]. In each of these cases the effects of bias in AI was to discriminate against those already marginalised in society.\nBias and discrimination in AI follows a long history of injustice resulting from the way people are counted, represented and classified in data [Perez, 2019; Hickman, 1997]. To those affected by injustice, whether unfair decisions are made by human or machine is not the most pressing issue. It is perhaps for this reason that the focus of analysis in terms of social justice, critical race studies and feminism is not solely restricted to artificial intelligence but algorithms, socio-technical systems and automated decision making.\nIn parallel, within the field of AI, approaches to ensuring fairness and justice often assume that entirely new theoretical and ethical frameworks are required. However, as is clear in relation to the use of historical police records to predict future crime [Lum and Isaac, 2016], the same critical perspectives and activism that brought social justice in the past are fundamental to preventing bias and discrimination in AI. This paper explores this issue examining how research grounded in social justice, critical race studies and feminism could form the foundation of a new approach to data collection and curation for AI. We also consider technical challenges in addressing issues of societal bias."}, {"heading": "2 The Myth of Objectivity", "text": "Approaches to the prevention of bias and discrimination in AI have included developing fairness-aware machine learning algorithms, addressing bias in data and modifying learned models. The central aim of this work is to assume the status of objectivity and neutrality. However, there is increasingly widespread acknowledgement of the impossibility of objectivity in data-driven AI [Meredith, 2018; O\u2019Neil, 2016] and of bias as \u201can unavoidable characteristic of data collected from human processes \u201d [Dignum, 2019]. This sentiment was aptly captured by Bartoletti [2020] in her assertion that \u201cit is time we acknowledge that data is simply not neutral, and, as such, every single decision and action around data is a political one.\u201d This acknowledgement of the inevitability of bias necessitates a reform of the process of collating and curating training data for machine learning, incorporating perspectives across multiple disciplines.\nBringing to light the extent and complexity of damaging constructs of race and gender and how it is embedded in both contemporary and historical data makes it clear how unavoidable it is that data reflects some form of bias or captures the effects of social injustice. Noble [2018] uncovered a series\nof cases where data-driven racial profiling of individuals resulted in the repetition of historical injustices against people of colour through what she termed \u2018technologically redlining\u2019. How \u2018runaway feedback loops\u2019 are generated by machine learning algorithms when they are trained on data large datasets that capture the results of a history social injustice and how this exacerbates existing discriminatory practices in society is detailed by Gebru [2019].\nThe process of data collection and curation for machine learning algorithms, from the standpoint of feminism or critical race studies, can never be objective. Every dataset that aims to represent humans and their behaviour does so in accordance with a world-view or ideology that may be assimilated into an AI system. Data collection and curation for machine learning algorithms is therefore transformed into a political act of curating a world-view that one intends to perpetuate through an AI system."}, {"heading": "3 Beyond Bias", "text": "In contrast to the increasingly widespread acknowledgement of the impossibility of objectivity in data, most current technical approaches to bias in training data for machine learning aim to re-balance data and render it neutral in relation to protected classes such as race or gender. These include methods for data augmentation, re-sampling, re-weighting, swapping labels and removing dependencies between characteristics [Kamiran and Calders, 2012; Feldman et al., 2015].\nIn relation to language based datasets, methods to uncover and address bias include the use of word embedding models trained on text to uncover social and intersectional bias [Tan and Celis, 2019]. Stereotypical associations in text were amended by disassociating relationships between entities in the trained models [Bolukbasi et al., 2016] and by swapping the gender of entities in text [Zhao et al., 2018; Park et al., 2018]. However, the evaluation of bias in these studies focus on specific aspects such as stereotypical associations of race and gender in relation to employment or use implicit association tests as evaluation frameworks for bias in the study by Caliskan [2017].\nWithin narrowly defined contexts, approaches to modifying data have been shown to reduce bias in algorithms. However, grounding the work in critical studies of race, feminist theories and social justice may serve to enrich the frameworks for evaluating fairness in how groups are represented in data. Furthermore, incorporating such perspectives diverts efforts away from a search for bias towards a more comprehensive interrogation of the power structures and ideologies of gender, race and social class that may be captured in the data.\nIn assessing the ideology underlying a dataset, the critical framework proposed by D\u2019Ignazio and Klein [2020] presents a comprehensive approach to the evaluation of how gender is represented in data, grounded in intersectional feminist thought. The authors develop feminist principles for data collection asserting that \u201cdata are not neutral or objective.\u201d This approach if embedded into the data collection process for machine learning would highlight potentially problematic patterns in existing data that may lead to discriminatory AI systems. It may also work towards the transformation of the\nprocess to one involving the curation of data collections that intentionally reflect an intersectional feminist representation of gender.\nBy bridging critical theories of race, social justice or feminist theory with training machine learning algorithms in this way, less focus may be placed on attempting to \u2018fix\u2019 specific issues of bias in datasets that contain embedded problematic ideologies. Rather, a dataset may be designed and selected in order to capture a representation of the kind of concepts of race, gender and social class that should be perpetuated in an AI algorithm."}, {"heading": "4 Power in Data", "text": "One response to bias in data resulting from an underrepresentation of particular groups, is to collect more data from that group of people. This has led to questions of unethical data gathering practices with the goal of balancing datasets [Hawkins, 2018]. Benjamin [2019] examines the complex socio-political ramifications of this in the context of race, questioning \u201cwhat does it mean to be included, and hence more accurately identifiable, in an unjust set of social relations?.\u201d This historical legacy of inequitable treatment of the poorer in society in relation to data collection is described by Eubanks [2018] as what she termed the \u2018Digital Poorhouse.\u2019 In attempting to balance data sets for training machine learning algorithms, it is crucial then to consider the social and political effects of data collection for different groups in society.\nThe central importance of fair data in preventing discrimination in AI, highlights the central role played by curators of data for machine learning algorithms. The cognitive labour of classifying and labelling AI training data is increasingly conducted by underpaid women of colour from the global south [Crawford and Joler, 2018; D\u2019Ignazio and Klein, 2020]. However, with the increased recognition that bias and discrimination in AI emanates largely from data, along with the deconstruction of the myth of algorithmic objectivity, it should follow that the human labour and value involved in data curation for AI will become more visible. It also emphasises the case for what Benjamin [2019] describes as the \u2018democratization of data\u2019 through the inclusion of all perspectives, especially from those most vulnerable to discrimination, into the design of technology."}, {"heading": "5 Governing Artificial Intelligence", "text": "The protection of autonomy and the right to selfdetermination of every individual is central to research and activism at the intersection of technology and social justice. By virtue of its capacity to learn, AI raises foundational ethical questions in relation to human autonomy and introduces new dimensions in terms of accountability for discrimination. Ensuring humans are accountable for decisions made by AI systems is key to ethical AI [O\u2019Neil, 2016]. To reflect the foundational nature of such questions, governing bodies are developing new frameworks for ethical, trustworthy and responsible AI primarily grounded in a human rights perspective [EU-HLEG-AI, 2019; OECD, 2019;\nUS-Govt, 2019]. However, given the accelerating pace of the influence of AI in society there is also an urgency on the implementation of measures to prevent discrimination.\nIn addressing the urgent need for governance, organisations are examining the use of existing data governance policies in the context of the collection and curation of machine learning training data [Adamson et al., 2019; ISO, 2017]. There are also calls for standards on the kind of inferences that AI algorithms can make based on an individual\u2019s data [Wachter and Mittelstadt, 2019]. However, the implementability of data governance regulations to ensure biasfree data remains a challenge. The central aim of ensuring bias-free data contrasts with the increasingly accepted position that data, particularly in relation to human behaviour, is inevitably biased. Additionally, within AI literature there is a range of different definitions of fairness and methods of representing the concept mathematically [Mehrabi et al., 2019]. Departing from the concept of bias, which implies the possibility of bias-free data and grounding data regulation instead in concepts of discrimination derived from well established legal definitions, may contribute towards unifying research across the disciplines of AI, policy and social justice."}, {"heading": "6 Technical Challenges and Bias", "text": "A considerable challenge in dealing with bias in AI generally, and in machine learning in particular, is that there can be different definition of bias or what it means to be fair. We consider here the often discussed COMPAS system for predicting recidivism.1 Tables 1 and 2 are based on an analysis presented by Sumpter [2018] that considered whether or not COMPAS exhibits racial bias. The answer is that it depends on what one defines as bias.\nKleinberg et al. [2017] considered three different fairness conditions that capture what is means for an algorithm to be fair and avoid exhibiting bias. The first of these is that the algorithm should be well calibrated. Essentially, this means that the proportion of people that are positive in a population should be equal to the proportion of people that are positive in each subgroup of the population. Comparing Tables 1 and 2 we see the that 1369/2174 = 63% of African American defendants classified as high risk, while 505/854 = 59.1% of White defendants were classified as such. This data is wellcalibrated, suggesting the system does not exhibit a racial bias, since the proportion of high-risk people in each population is predicted to be roughly equal.\nThe second (and third) condition relates to balancing for the positive (resp. negative) class. If this condition were to be violated it would mean that the likelihood that a positive (resp. negative) instance in one population is more likely to be identified than in the other. For example, as Sumpter argues, Tables 1 and 2 show that 2174/3615 = 60% of African Americans in COMPAS are considered higher risk, while this only 854/2464 = 35% for Whites; note that 1901/3615 = 53% of African Americans reoffended while this was 966/2454 = 39% for White, suggesting that African Americans actually reoffended less than predicted while the opposite is true for Whites. This suggests a strong racial bias.\n1https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm\nConsidering the mistakes that COMPAS makes for each racial group, Sumpter goes on to show that 805/1714 = 47% of African Americans were wrongly predicted to reoffend, as compared with only 349/1488 = 24% of Whites. On the other hand, only 532/1901 = 28% of African Americans who reoffended were wrongly predicted as being lower risk, as compared with 461/966 = 48% of Whites. Again, this suggests a strong racial bias.\nIn other words, while COMPAS satisfies one of Kleinberg et al.\u2019s definition of fairness (calibration), is fails the second and third. This is not unusual. In fact, Kleinberg et al. rigorously prove that these three fairness conditions are incompatible except in very rare situations. This implies that one is usually faced with a choice of which bias must be traded-off against another. Eliminating bias from AI systems is often provably impossible.\nThe challenges in dealing with bias are complex. To address these issues, in addition to the many point made earlier, we must develop a set of AI methods that reason about bias as systems are being developed. It might be helpful to think of bias-aware learning as an optimisation problem in which the properties of the learned concept and the acceptable tradeoff between fairness criteria could be be imposed as constraints on the learning process [Bessiere et al., 2009]."}, {"heading": "7 Conclusion", "text": "In considering the challenges of bias in AI, it is important to build upon existing principles and frameworks, e.g. critical intersectional feminist and critical race theories. Dealing with bias in an objective way is very challenging and the possibility of unbiased data is not a sufficient condition for unbiased AI. In reasoning about bias and fairness is is critically important to involve those who might be directly impacted by AI system in the process of designing, curating, testing these systems, and especially in the \u2018deconstruction\u2019 of languagebased data. It is important that we interrogate notions of accuracy that may lead to more surveillance and data gathering on marginalised groups, and the role of such marginalised groups in data curation is important to acknowledge.\nAcknowledgement. This publication has emanated from research conducted with the financial support of Science Foundation Ireland under Grant number 12/RC/2289-P2\nwhich is co-funded under the European Regional Development Fund."}], "year": 2020}