{"abstractText": "Before any publication, data analysis of high-energy physics experiments must be validated. This validation is granted only if a perfect understanding of the data and the analysis process is demonstrated. Therefore, physicists prefer using transparent machine learning algorithms whose performances highly rely on the suitability of the provided input features. To transform the feature space, feature construction aims at automatically generating new relevant features. Whereas most of previous works in this area perform the feature construction prior to the model training, we propose here a general framework to embed a feature construction technique adapted to the constraints of high-energy physics in the induction of tree-based models. Experiments on two high-energy physics datasets confirm that a significant gain is obtained on the classification scores, while limiting the number of built features. Since the features are built to be interpretable, the whole model is transparent and readable.", "authors": [{"affiliations": [], "name": "No\u00eblie Cherrier"}, {"affiliations": [], "name": "Maxime Defurne"}, {"affiliations": [], "name": "Jean-Philippe Poli"}, {"affiliations": [], "name": "Franck Sabati\u00e9"}], "id": "SP:63f0bad967751bacabaa9d071a4750f77726942f", "references": [{"authors": ["Peter W. Higgs"], "title": "Broken symmetries and the masses of gauge bosons", "venue": "Phys. Rev. Lett.,", "year": 1964}, {"authors": ["Craig D. Roberts"], "title": "Perspective on the origin of hadron masses", "venue": "Few Body Syst.,", "year": 2017}, {"authors": ["Leilani H. Gilpin", "David Bau", "Ben Z. Yuan", "Ayesha Bajwa", "Michael Specter", "Lalana Kagal"], "title": "Explaining Explanations: An Approach to Evaluating Interpretability of Machine Learning", "year": 2018}, {"authors": ["George H. John", "Ron Kohavi", "Karl Pfleger"], "title": "Irrelevant Features and the Subset Selection Problem", "venue": "In Machine Learning Proceedings", "year": 1994}, {"authors": ["Parikshit Sondhi"], "title": "Feature Construction Methods: A Survey", "year": 2009}, {"authors": ["Krzysztof Krawiec"], "title": "Genetic Programming-based Construction of Features for Machine Learning and Knowledge Discovery Tasks", "year": 2002}, {"authors": ["Fernando E.B. Otero", "Monique M.S. Silva", "Alex A. Freitas", "Julio C. Nievola"], "title": "Genetic Programming for Attribute Construction in Data Mining", "venue": "In Proceedings of the 6th European Conference on Genetic Programming,", "year": 2003}, {"authors": ["Matthew G. Smith", "Larry Bull"], "title": "Genetic Programming with a Genetic Algorithm for Feature Construction and Selection", "venue": "Genetic Programming and Evolvable Machines,", "year": 2005}, {"authors": ["Kourosh Neshatian", "Mengjie Zhang"], "title": "Genetic Programming and Class-Wise Orthogonal Transformation for Dimension Reduction in Classification Problems. In Genetic Programming, Lecture Notes in Computer Science, pages 242\u2013253", "year": 2008}, {"authors": ["N. Cherrier", "J. Poli", "M. Defurne", "F. Sabati\u00e9"], "title": "Consistent Feature Construction with Constrained Genetic Programming for Experimental Physics", "venue": "IEEE Congress on Evolutionary Computation (CEC),", "year": 2019}, {"authors": ["Anik\u00f3 Ek\u00e1rt", "Andr\u00e1s M\u00e1rkus"], "title": "Using genetic programming and decision trees for generating structural descriptions of four bar mechanisms", "venue": "AI EDAM,", "year": 2003}, {"authors": ["Francis Maes", "Pierre Geurts", "Louis Wehenkel"], "title": "Embedding Monte Carlo Search of Features in Tree-Based Ensemble Methods", "venue": "In Machine Learning and Knowledge Discovery in Databases,", "year": 2012}, {"authors": ["J. Ross Quinlan"], "title": "Programs for Machine Learning", "year": 1993}, {"authors": ["Leo Breiman"], "title": "Classification and regression trees. Routledge, 2017", "year": 2017}, {"authors": ["Ji Zhu", "Saharon Rosset", "Hui Zou", "Trevor Hastie"], "title": "Multi-class adaboost", "venue": "Statistics and its interface,", "year": 2006}, {"authors": ["Jerome H. Friedman"], "title": "Greedy function approximation: A gradient boosting machine", "venue": "Ann. Statist., 29(5):1189\u20131232,", "year": 2001}, {"authors": ["Claire Adam-Bourdarios", "Glen Cowan", "Cecile Germain", "Isabelle Guyon", "Balazs Kegl", "David Rousseau"], "title": "Learning to discover: the Higgs boson machine learning challenge - Documentation", "year": 2014}], "sections": [{"heading": "1 Introduction", "text": "The entire universe is made of elementary particles such as leptons and quarks as explained by the standard model. To test this model, particle physicists try to recover properties of composite particles from these of elementary particles. The Higgs mechanism [1] and the strong interaction [2] are actively studied in particle collisions to understand the origin of mass.\nThe principle of a data analysis is always the same: detect and combine particles from collisions and use energy/momentum conservation to isolate the signals of interest. These signals are usually rare with several sources of background. Machine learning techniques can make a major contribution to such an analysis as long as they pass the mandatory interpretability requirement, sometimes at the expense of performances: any detail of an analysis must be explainable and understood.\nDecision trees are very good candidates to meet this transparency requirement [3]. However it is also well known that the choice of features can greatly affect their performances [4]. Generally, a preprocessing step of feature engineering is performed manually based on domain expertise. In particle physics, quantities related to energy/mass/momentum balances and highly dependent on the process of interest are derived from base variables. Although understandable and analyzable by construction, nothing guarantees that these quantities are optimized for the analysis of the process of interest.\nThe field of feature construction (FC) aims at automating the feature engineering step. In particular, embedded FC permits to better adjust the built features to a local data discrimination problem during the model induction. In this work, we aim at developing techniques of embedded FC with a special attention to the readability of the final model. The global transparency of a model also requires the interpretability of the built features themselves, which is the second focus of our work.\nSecond Workshop on Machine Learning and the Physical Sciences (NeurIPS 2019), Vancouver, Canada.\nar X\niv :1\n91 2.\n07 99\n9v 1\n[ cs\n.L G\n] 1\n7 D\nec 2\nWe first review related work in the field, before presenting our method and conducting experiments on two HEP datasets from two different physics studies: the Higgs mechanism at CERN on one hand and the strong interaction at Jefferson Laboratory on the other hand."}, {"heading": "2 Related work", "text": "Most of the automatic feature generation algorithms are actually preprocessing methods: new features are built from mathematical functions of the original ones and the training is performed in a subsequent step with the new features.\nThe literature in automatic FC is very abundant and a survey can be found in [5]. Genetic programming (GP) algorithm is a popular method in the FC field [6, 7, 8, 9]. It consists in evolving a population of N -tree-like individuals through mutations and crossovers while selecting good individuals for the next generation.\nIn [10], the authors propose a method to adapt a GP algorithm to handle the constraints of HEP. Indeed, in HEP, a feature is interpretable if it resembles some of the physics laws ruling our universe. These physics laws connect variables carrying compatible units: for instance, no physics formula exhibits the raw sum of an angle and an energy. Therefore, to enforce the combination of compatible features, Cherrier et al. [10] constrain the GP algorithm with a grammar and a transition matrix. The resulting features are interpretable to physics experts and the classification score is improved compared to the unconstrained version of the algorithm.\nOn the other hand, the field of embedded FC has not been so much explored yet. Ek\u00e1rt and M\u00e1rkus [11] are the first to use a GP algorithm to find the best splitting feature at each node in the induction of a decision tree. Maes et al. [12] embed a Monte Carlo search of features in several tree-based ensemble methods, building one feature at each node of the tree."}, {"heading": "3 Embedded constrained feature construction method during tree induction", "text": "Cherrier et al. [10] mainly use a machine learning algorithm to evaluate the candidate features during the evolution. In this work, we propose to use the split criterion in the decision trees or ensemble methods as the fitness function in the constrained GP algorithm of Cherrier et al. [10], thus improving the speed of the algorithm. Computing a single information gain is indeed faster than training a whole decision tree.\nBesides, we experimentally limit the number of built features per tree to support overall intelligibility of the model and also to prevent overfitting. When the number of allowed constructions is restrained, the features are built from the root and level by level, going down as the tree is formed.\nWe propose hereafter a generic framework for embedding FC into classical tree induction algorithms. The induction of the most commonly used tree classifiers (e.g. C4.5 [13] and CART [14]) is made by sequentially constructing their nodes. For each node, the problem is to separate the data into two subsets while optimizing a criterion which depends on the particular induction algorithm. Most algorithms perform a search among the existing features to find the best split according to this criterion. We modify this feature search step by the function findBestSplit described in Algorithm 1.\nAt each node during the induction of a decision tree, if the constructionCondition in Equation (1) below is met with depth d and number of built features so far nf , with a maximal allowed number Nmax of feature constructions, we replace the classical search among existing features by a FC method. d \u2264 log2(1 +Nmax) and nf < Nmax. (1) This condition is designed to ensure that the feature constructions will be performed in the first layers of the tree. The final feature obtained by the FC algorithm is used as the feature to split the data at the current node.\nWe use the C4.5 [13], adaptive boosting [15] and gradient boosting [16] algorithms in our experiments. The induction of a C4.5 decision tree is made by maximizing the information gain at each node. Adaptive boosting with decision trees only changes the weights of training examples. However, the gradient boosting classifier uses the mean squared error (MSE) as the splitting criterion since the global classification problem becomes a regression problem in the weak tree classifiers. Feature\nAlgorithm 1: Generic induction of a node of a decision tree with embedded FC Input :data in the current node\nnf the number of built features so far in the tree depth the depth of the current node Nmax the maximum number of features to build in the tree\nResult: the inducted node Function findBestSplit(data, depth, nf)\nif constructionCondition (nf , depth) then build splittingFeature with splittingCriterion as fitness function nf \u2190 nf + 1 else foreach feature in data do\ncompute splittingCriterion on feature splittingFeature\u2190 feature obtaining the best criterion\nif splittingFeature does not satisfy specific requirements then return null splitted\u2190 split data along splittingFeature return splitted\nconstruction is done by replacing the fitness function by the splitting criterion of the induction algorithm, computed using the considered candidate feature."}, {"heading": "4 Experiments", "text": "We consider two HEP classification problems in this study.\nDVCS dataset At Jefferson Laboratory, an electron beam scatters off protons at rest in the lab frame. The objective is to discriminate between the DVCS interaction whose final state is composed of an electron, a proton, and a photon, and the \u03c00 production event which has a similar final state, except that the photon is replaced by a \u03c00. The later immediately decays into two photons and one of them may not be detected, mimicking a DVCS event. The available features are the three-dimensional momentum for each identified particle.\nHiggs dataset [17] At CERN, two protons collide head-on with each other and Higgs particles are notably produced out of the collisions. The objective of this dataset is to detect Higgs bosons decaying into two \u03c4 -particles. The simulated data is publicly available on the Open Data platform of CERN. 17 primitive features per event are available, including notably several geometrical features for each detected particle.\nFor both datasets, we use 100000 instances: 80% of the them are dedicated to training and 20% to performance evaluation. One should note that both datasets come from simulated data since the truth information is needed for training. Features already present in the datasets include the three-dimensional momenta of the particles as well as their \u03b8 and \u03c6 angles. For the GP algorithm, we evolve a population of 500 individuals during 70 generations. For ensemble methods, we use a downgraded version of GP called \u201cGP down\u201d performing only 6 generations."}, {"heading": "4.1 Performance evaluation", "text": "In this paragraph, we evaluate the performances of C4.5, AdaBoost and GradientBoosting classifiers with the embedded GP algorithm described above.\nWe vary the number of features built in total for C4.5 or per tree for ensemble methods. We use the Cohen\u2019s kappa metric to evaluate the performances, with at least 10 independent runs per displayed value. Figure 1 displays the evolution of the Cohen\u2019s kappa score of these three tree-based models depending on the number of built features. For ensemble methods, we reserve the possibility to build less than one feature per tree in the ensemble: in the x-axis on the top of the graph, a value p below 1 means that each tree in the ensemble has probability p to build one feature. Above 1, the value p can only be an integer and is the number of features built per tree. The expectation for the total number\nof built features in the whole algorithm is then the product of this value p with the number of trees in the ensemble.\nThe results show that the score increases with the number of built features per tree for ensemble methods. However, it stagnates for C4.5 algorithm after around 10-15 built features. This shows that embedded FC with constrained GP brings a significant gain to the classification score compared to using the same tree-based algorithm without FC (score corresponding to zero built feature on Figure 1. Moreover, a reduction of the number of built features in C4.5 can be performed without impairing the score.\nTable 1 provides a few baselines, including the three tree-based algorithms without FC, a neural network and a SVM. These baselines are compared to the best scores (i.e. the highest point on Figure 1) obtained with the three tree-based algorithms with embedded FC. Even compared to black-box models which are supposed to have an internal complex representation of the feature space, the three presented algorithms with embedded FC perform better."}, {"heading": "4.2 Model readability", "text": "Model conciseness is supported by the smaller number of built features required by C4.5 to get an optimal classification score, and by the small size of weak tree classifiers which are limited in depth in the used implementations of the two presented ensemble methods (depth 1 for AdaBoost and 3 for GradientBoosting). The final feature space obtained with embedded FC is hence of reduced size.\nEquations (2) and (3) display two features built for the Higgs dataset that are either recurrent in their simple form or recurrent as a pattern in more complex features (they appear in respectively 61% and 74% of runs):\ncos ( \u03b8lep \u2212 \u03b8\u03c4 ) (2) cos ( \u03c6lep \u2212 \u03c6\u03c4 ) (3)\nThese features visibly compares the geometrical angles of a lepton and a \u03c4 , two particles which are indeed expected to be the products of the decay of the same particle. The mass of that particle can be computed from the features displayed above.\nEquation (4) shows a recurrent feature built for the DVCS dataset (it appears in 79% of runs):\npez + p \u03b31 z + p p z (4)\nThis feature is a momentum conservation check along the beam direction, absolutely relevant considering the detector and event geometries of a fixed-target experiment. A \u03c00 event would obviously miss a second photon momentum in the pz sum so this feature would not take the same value."}, {"heading": "5 Conclusion", "text": "In the machine learning field, there is generally a need to strike a balance between the interpretability and the performances of a model. In this work, without loss of generality, we improved three tree-based models by embedding a constrained FC technique adapted to physics problems during the induction. In any case, embedded FC permitted to improve the classification score while experimenting on two datasets of HEP. The constrained versions of the FC methods enables to build features that are interpretable at least to the experts of the field.\nWith the proposed method, the final model is more performant while remaining readable for further interpretation. Finally, instead of choosing between performance and interpretability, we increased performance while focusing on keeping the readability of the final model."}], "title": "Embedded Constrained Feature Construction for High-Energy Physics Data Classification", "year": 2019}