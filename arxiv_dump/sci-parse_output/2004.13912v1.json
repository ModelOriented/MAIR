{
  "abstractText": "Deep neural networks (DNNs) are powerful black-box predictors that have achieved impressive performance on a wide variety of tasks. However, their accuracy comes at the cost of intelligibility: it is usually unclear how they make their decisions. This hinders their applicability to high stakes decision-making domains such as healthcare. We propose Neural Additive Models (NAMs) which combine some of the expressivity of DNNs with the inherent intelligibility of generalized additive models. NAMs learn a linear combination of neural networks that each attend to a single input feature. These networks are trained jointly and can learn arbitrarily complex relationships between their input feature and the output. Our experiments on regression and classification datasets show that NAMs are more accurate than widely used intelligible models such as logistic regression and shallow decision trees. They perform similarly to existing state-of-the-art generalized additive models in accuracy, but can be more easily applied to real-world problems.",
  "authors": [
    {
      "affiliations": [],
      "name": "Rishabh Agarwal"
    },
    {
      "affiliations": [],
      "name": "Nicholas Frosst"
    },
    {
      "affiliations": [],
      "name": "Xuezhou Zhang"
    }
  ],
  "id": "SP:e4b5057c5855293ce8c40aefc17a9df93510cd10",
  "references": [
    {
      "authors": [
        "Julia Angwin",
        "Jeff Larson",
        "Lauren Kirchner",
        "Surya Mattu"
      ],
      "title": "Machine Bias: There\u2019s software used across the country to predict future criminals. And it\u2019s biased against blacks, 2016",
      "venue": "URL https://www.propublica. org/article/machine-bias-risk-assessments-in-criminal-sentencing. [Accessed",
      "year": 2020
    },
    {
      "authors": [
        "Devansh Arpit",
        "Stanislaw Jastrzkebski",
        "Nicolas Ballas",
        "David Krueger",
        "Emmanuel Bengio",
        "Maxinder S Kanwal",
        "Tegan Maharaj",
        "Asja Fischer",
        "Aaron Courville",
        "Yoshua Bengio"
      ],
      "title": "A closer look at memorization in deep networks",
      "year": 2017
    },
    {
      "authors": [
        "Rich Caruana",
        "Yin Lou",
        "Johannes Gehrke",
        "Paul Koch",
        "Marc Sturm",
        "Noemie Elhadad"
      ],
      "title": "Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission",
      "year": 2015
    },
    {
      "authors": [
        "Tianqi Chen",
        "Carlos Guestrin"
      ],
      "title": "XGBoost: A scalable tree boosting system",
      "year": 2016
    },
    {
      "authors": [
        "Andrea Dal Pozzolo"
      ],
      "title": "Adaptive machine learning for credit card fraud detection",
      "venue": "PhD Thesis,",
      "year": 2015
    },
    {
      "authors": [
        "Julia Dressel",
        "Hany Farid"
      ],
      "title": "The accuracy, fairness, and limits of predicting recidivism",
      "venue": "Science advances,",
      "year": 2018
    },
    {
      "authors": [
        "Xavier Glorot",
        "Yoshua Bengio"
      ],
      "title": "Understanding the difficulty of training deep feedforward neural networks",
      "year": 2010
    },
    {
      "authors": [
        "Daniel Golovin",
        "Benjamin Solnik",
        "Subhodeep Moitra",
        "Greg Kochanski",
        "John Karro",
        "D Sculley"
      ],
      "title": "Google vizier: A service for black-box optimization",
      "year": 2017
    },
    {
      "authors": [
        "Antoine Guisan",
        "Thomas C Edwards Jr.",
        "Trevor Hastie"
      ],
      "title": "Generalized linear and generalized additive models in studies of species distributions: setting the scene",
      "venue": "Ecological modelling,",
      "year": 2002
    },
    {
      "authors": [
        "Trevor Hastie",
        "Robert Tibshirani"
      ],
      "title": "Generalized Additive Models",
      "venue": "Chapman and Hall/CRC,",
      "year": 1990
    },
    {
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "title": "Delving deep into rectifiers: Surpassing human-level performance on imagenet classification",
      "year": 2015
    },
    {
      "authors": [
        "Kaiming He",
        "Xiangyu Zhang",
        "Shaoqing Ren",
        "Jian Sun"
      ],
      "title": "Deep residual learning for image recognition",
      "year": 2016
    },
    {
      "authors": [
        "Kurt Hornik",
        "Maxwell Stinchcombe",
        "Halbert White"
      ],
      "title": "Multilayer feedforward networks are universal approximators",
      "venue": "Neural Networks,",
      "year": 1989
    },
    {
      "authors": [
        "Diederik P Kingma",
        "Jimmy Ba"
      ],
      "title": "Adam: A method for stochastic optimization",
      "venue": "arXiv preprint arXiv:1412.6980,",
      "year": 2014
    },
    {
      "authors": [
        "Alex Krizhevsky"
      ],
      "title": "Convolutional deep belief networks on cifar-10",
      "year": 2010
    },
    {
      "authors": [
        "Yin Lou",
        "Rich Caruana",
        "Johannes Gehrke"
      ],
      "title": "Intelligible models for classification and regression",
      "year": 2012
    },
    {
      "authors": [
        "Yin Lou",
        "Rich Caruana",
        "Johannes Gehrke",
        "Giles Hooker"
      ],
      "title": "Accurate intelligible models with pairwise interactions",
      "year": 2013
    },
    {
      "authors": [
        "Vinod Nair",
        "Geoffrey E Hinton"
      ],
      "title": "Rectified linear units improve restricted boltzmann machines",
      "year": 2010
    },
    {
      "authors": [
        "Harsha Nori",
        "Samuel Jenkins",
        "Paul Koch",
        "Rich Caruana"
      ],
      "title": "Interpretml: A unified framework for machine learning interpretability",
      "venue": "arXiv preprint arXiv:1909.09223,",
      "year": 2019
    },
    {
      "authors": [
        "R Kelley Pace",
        "Ronald Barry"
      ],
      "title": "Sparse spatial autoregressions",
      "venue": "Statistics & Probability Letters,",
      "year": 1997
    },
    {
      "authors": [
        "Fabian Pedregosa",
        "Ga\u00ebl Varoquaux",
        "Alexandre Gramfort",
        "Vincent Michel",
        "Bertrand Thirion",
        "Olivier Grisel",
        "Mathieu Blondel",
        "Peter Prettenhofer",
        "Ron Weiss",
        "Vincent Dubourg"
      ],
      "title": "Scikit-learn: Machine learning in python",
      "year": 2011
    },
    {
      "authors": [
        "William JE Potts"
      ],
      "title": "Generalized additive neural networks",
      "year": 1999
    },
    {
      "authors": [
        "Alec Radford",
        "Jeffrey Wu",
        "Rewon Child",
        "David Luan",
        "Dario Amodei",
        "Ilya Sutskever"
      ],
      "title": "Language models are unsupervised multitask learners",
      "year": 2018
    },
    {
      "authors": [
        "Nasim Rahaman",
        "Aristide Baratin",
        "Devansh Arpit",
        "Felix Draxler",
        "Min Lin",
        "Fred A Hamprecht",
        "Yoshua Bengio",
        "Aaron Courville"
      ],
      "title": "On the spectral bias of neural networks",
      "year": 2018
    },
    {
      "authors": [
        "Marco Tulio Ribeiro",
        "Sameer Singh",
        "Carlos Guestrin"
      ],
      "title": " why should i trust you?\" explaining the predictions of any classifier",
      "year": 2016
    },
    {
      "authors": [
        "Cynthia Rudin"
      ],
      "title": "Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead",
      "venue": "Nature Machine Intelligence,",
      "year": 2019
    },
    {
      "authors": [
        "David E Rumelhart",
        "Geoffrey E Hinton",
        "Ronald J Williams"
      ],
      "title": "Learning representations by back-propagating errors",
      "year": 1986
    },
    {
      "authors": [
        "Mohammed Saeed",
        "Mauricio Villarroel",
        "Andrew T Reisner",
        "Gari Clifford",
        "Li-Wei Lehman",
        "George Moody",
        "Thomas Heldt",
        "Tin H Kyaw",
        "Benjamin Moody",
        "Roger G Mark"
      ],
      "title": "Multiparameter intelligent monitoring in intensive care ii (mimic-ii): a public-access intensive care unit database",
      "venue": "Critical care medicine,",
      "year": 2011
    },
    {
      "authors": [
        "David Silver",
        "Aja Huang",
        "Chris J Maddison",
        "Arthur Guez",
        "Laurent Sifre",
        "George Van Den Driessche",
        "Julian Schrittwieser",
        "Ioannis Antonoglou",
        "Veda Panneershelvam",
        "Marc Lanctot"
      ],
      "title": "Mastering the game of go with deep neural networks and tree search",
      "year": 2016
    },
    {
      "authors": [
        "Jasper Snoek",
        "Hugo Larochelle",
        "Ryan P Adams"
      ],
      "title": "Practical bayesian optimization of machine learning algorithms",
      "venue": "NeurIPS,",
      "year": 2012
    },
    {
      "authors": [
        "Nitish Srivastava",
        "Geoffrey Hinton",
        "Alex Krizhevsky",
        "Ilya Sutskever",
        "Ruslan Salakhutdinov"
      ],
      "title": "Dropout: a simple way to prevent neural networks from overfitting",
      "year": 2014
    },
    {
      "authors": [
        "Sarah Tan",
        "Rich Caruana",
        "Giles Hooker",
        "Yin Lou"
      ],
      "title": "Distill-and-compare: Auditing black-box models using transparent model distillation",
      "venue": "AAAI/ACM Conference on AI, Ethics, and Society,",
      "year": 2018
    }
  ],
  "sections": [
    {
      "heading": "1 Introduction",
      "text": "Deep neural networks are powerful function approximators that have achieved impressive results on a variety of tasks, such as computer vision [He et al., 2016], language modeling [Radford et al., 2018] and reinforcement learning [Silver et al., 2016]. However, it is notoriously difficult to understand how they make their predictions, and they are often considered as black-box models. This hinders their applicability to high-stakes domains such as healthcare and criminal justice.\nVarious efforts have been made to demystify the predictions of neural networks (NNs). For example, one family of methods, represented by LIME [Ribeiro et al., 2016], attempt to explain individual predictions of a neural network by approximating it locally with interpretable models such as linear models and shallow trees2. However, these approaches often fail to provide a global view of the model and their explanations often are not faithful to what the original model computes or do not provide enough detail to understand the model\u2019s behavior [Rudin, 2019]. In this paper, we make restrictions on the structure of neural networks, which yields a family of models called Neural Additive Models (NAMs), that are inherently interpretable while suffering little loss in prediction accuracy when applied to tabular data.\nMethodologically, NAMs belong to a larger model family called Generalized Additive Models (GAMs) [Hastie and Tibshirani, 1990]. GAMs have the form:\ng(E[y]) = \u03b2 + f1(x1) + f2(x2) + \u00b7 \u00b7 \u00b7+ fK(xK) (1) \u2217Correspondence to: Rishabh Agarwal <rishabhagarwal@google.com>, Rich Caruana <rcaruana@microsoft.com>. \u2021 Work done at Google Research, Brain Team. 2Linear models, shallow decision trees and GAMs are interpretable only if the features they are trained on are interpretable.\nPreprint. Under review.\nar X\niv :2\n00 4.\n13 91\n2v 1\n[ cs\n.L G\n] 2\n9 A\npr 2\nwhere x = (x1, x2, . . . , xK) is the input with K features, y is the target variable, g(.) is the link function (e.g., logistic function) and each fi is a univariate shape function with E[fi] = 0. Generalized linear models, such as logistic regression, are a special form of GAMs where each fi is restricted to be linear.\nNAMs learn a linear combination of networks that each attend to a single input feature: each fi in (1) is parametrized by a neural network. These networks are trained jointly using backpropagation and can learn arbitrarily complex shape functions. Interpreting NAMs is easy as the impact of a feature on the prediction does not rely on the other features and can be understood by visualizing its corresponding shape function (e.g., plotting fi(xi) vs. xi).\nTraditionally, GAMs were fitted using smooth low-order splines, which reduce overfitting and can be fit analytically. More recently, GAMs [Caruana et al., 2015] were fitted with boosted decision trees to improve accuracy and to allow GAM models to learn jumps in the feature shaping functions to better match patterns seen in real data that smooth splines could not easily capture. This paper examines using deep neural nets to fit GAMs (NAMs). NAMs provide the following advantages:\n\u2022 NAMs introduce an expressive yet intelligible class of models to the deep learning community, a much larger community than the one using tree-based GAMs. We will open-source the code in popular deep learning frameworks to maximize their adoption.\n\u2022 Once NAMs are widely available, they are likely to be combined with other deep learning methods in ways we don\u2019t foresee. This is important because one of the key drawbacks of deep learning is intelligibility.\n\u2022 The graphs learned by NAMs are not just an explanation but an exact description of how NAMs compute a prediction. This could help harness the expressivity of neural nets on high-stakes domains with intelligibility requirements.\n\u2022 NAMs, due to the flexibility of NNs, can be easily extended to various settings problematic for boosted decision trees. For example, extending boosted tree GAMs to multitask, multiclass or multi-label learning requires significant changes to how trees are trained, but is easily accomplished with NAMs without requiring changes to how neural nets are trained.\n\u2022 NAMs are more scalable as they can be trained on GPUs or other specialized hardware using the same toolkits developed for deep learning over the past decade \u2013 GAMs currently cannot.\n\u2022 Accurate GAMs [Caruana et al., 2015] currently require millions of decision trees to fit each shape function while NAMs only use a small ensemble (10 - 100) of neural nets. Furthermore, the extra accuracy DNNs demonstrate on many learning problems might yield better accuracy and intelligibility for NAMs over existing GAM algorithms."
    },
    {
      "heading": "2 Neural Additive Models",
      "text": ""
    },
    {
      "heading": "2.1 Fitting Jagged Shape Functions",
      "text": "Modeling jagged functions is required to learn accurate additive models as there are often sharp jumps in real-world datasets, e.g., see Figure 6 for jumps in graphs for PFRatio and Bilirubin which correspond to real patterns in the MIMIC-II dataset (Section 3.1.2). Similarly, Caruana et al. [2015] observe that GAMs fit using splines tend to over regularize and miss genuine details in real data, yielding less accuracy than tree-based GAMs. Therefore, we require that neural networks (NNs) are able to learn highly non-linear shape functions, to fit these patterns.\nAlthough NNs can approximate arbitrarily complex functions [Hornik et al., 1989], we find that standard NNs fail to model highly jumpy 1D functions, and demonstrate this failure empirically using a toy dataset. The toy dataset is constructed as follows: For the input x, we sample 100 evenly spaced points in [-1, 1]. For each x, we sample p uniformly random in [0.1, 0.9) and generate 100 labels from a Bernoulli random variable which takes the value 1 with probability p. This creates a binary classification dataset of (x, y) tuples with 10,000 points. Figure 2 shows the log-odds of the empirical probability p (i.e., log p1\u2212p ) of classifying the label of x as 1 for each input x. This dataset tests the NN\u2019s ability to \u201coverfit\u201d the data, rather than its ability to generalize.\nOver-parameterized NNs with ReLUs [Nair and Hinton, 2010] and standard initializations such as Kaiming initialization [He et al., 2015] and Xavier initialization [Glorot and Bengio, 2010]) struggle to overfit this toy dataset when trained using mini-batch gradient descent, despite the NN architecture being expressive enough3(see Figures 2(a) and 9). This difficulty of learning large local fluctuations with ReLU networks without affecting their global behavior when fitting jagged functions might be due to their bias towards learning smooth functions [Rahaman et al., 2018, Arpit et al., 2017].\nWe propose exp-centered (ExU) hidden units to overcome this neural net failure: we simply learn the weights in the logarithmic space with inputs shifted by a bias. Specifically, for a scalar input x, each hidden unit using an activation function f computes h(x) given by\nh(x) = f (ew \u2217 (x\u2212 b)) (2)\nwhere w and b are the weight and bias parameters. The intuition behind ExU units is as follows: For modeling jagged functions, a hidden unit should be able to change its output significantly, with a tiny change in input. This requires the unit to have extremely large weight values depending on the sharpness of the jump. The ExU unit computes a linear function of input where the slope can be very steep with small weights, making it easier to modify the output easily during training. ExU units do not improve the expressivity of neural nets, however they do improve their learnability for fitting jumpy functions.\n3This problem doesn\u2019t occur with full-batch gradient descent.\nWe noticed that ExU units with standard weight initialization also struggle to learn jagged curves; instead initializing the weights using a normal distribution N (x, 0.5) with x \u2208 [3, 4] works well in practice. This initialization simply ensures that the initial network starts with a jagged (random) function which we empirically find to be crucial for fitting any jumpy function.\nFurthermore, we use ReLU activations capped at n (ReLU-n) [Krizhevsky, 2010] to ensure that each ExU unit is active in a small input range, making it easier to model sharp jumps in a function without significantly affecting the global behavior. ExU-units can be combined with any activation function (i.e., any f can be used in (2)), but ReLU-n performs well in practice. Figure 2(b) shows that NNs with ExU units are able to fit the toy dataset significantly better than standard NNs.\n2.2 Regularization and Training\nExU units encourage learning highly jagged curves, however, most realistic shape functions tend to be smooth with large jumps at only a few points. To avoid overfitting, we use the following regularization techniques:\n\u2022 Dropout [Srivastava et al., 2014]: It regularizes ExUs in each feature net, allowing them to learn smooth functions while being able to represent jumps (Figure 3). \u2022 Weight decay: This is done by penalizing\nthe L2 norm of weights in each feature net. \u2022 Output Penalty: We penalize the L2 norm\nof the prediction of each feature net, so that its contribution stays close to zero unless evident otherwise from the data. \u2022 Feature Dropout: We also drop out individ-\nual feature networks during training. When there are correlated input features, an additive model can possibly learn multiple explanations by shifting contributions across these features. This term encourages NAMs to spread out those contributions.\nTraining. Let D = {(x(i), y(i))}Ni=1 be a training dataset of size N , where each input x = (x1, x2, . . . , xK) contains K features and y is the target variable. In this work, we train NAMs using the loss L(\u03b8) given by\nL(\u03b8) = Ex,y\u223cD [ l(x, y; \u03b8) + \u03bb1\u03b7(x; \u03b8) ] + \u03bb2\u03b3(\u03b8) (3)\nwhere \u03b7(x; \u03b8) = 1K \u2211 x \u2211 k(f \u03b8 k (xk))\n2 is the output penalty, \u03b3(\u03b8) is the weight decay and f\u03b8k is the feature network for the kth feature. Each individual network is also regularized using feature dropout and dropout with coefficients \u03bb3 and \u03bb4 respectively. l(x, y; \u03b8) is the task dependent loss function. We use the cross-entropy loss for binary classification:\nl(x, y; \u03b8) = \u2212y log(p\u03b8(x))\u2212 (1\u2212 y) log(1\u2212 p\u03b8(x)), where p\u03b8(x) = \u03c3 ( \u03b2\u03b8 + \u2211K k=1 f \u03b8 k (xk) ) and mean squared error (MSE) for regression:\nl(x, y; \u03b8) = ( \u03b2\u03b8 + K\u2211 k=1 f\u03b8k (xk)\u2212 y )2"
    },
    {
      "heading": "2.3 Intelligibility and Modularity",
      "text": "The intelligibility of NAMs results in part from the ease with which they can be visualized. Because each feature is handled independently by a learned shape function parameterized by a neural net, one can get a full view of the model by simply graphing the individual shape functions. For data with\na small number of inputs, it is possible to have an accessible explanation of the model\u2019s behavior visualized fully on a single page. Please note these shape function plots are not just an explanation but an exact description of how NAMs compute a prediction.\nWe set the average score for each graph (i.e., each feature) averaged across the entire training dataset to zero by subtracting the mean score. To make individual shape functions identifiable and modular, a single bias term is then added to the model so that the average predictions across all data points matches the observed baseline. This makes interpreting the contribution of each term easier: e.g., on binary classification tasks, negative scores decrease probability, and positive scores increase probability compared to the baseline probability of observing that class. This property also allows each graph to be removed from the NAM (zeroed out) without introducing bias to the predictions."
    },
    {
      "heading": "3 Experiments",
      "text": "Baselines. We compare NAMs with the following baselines on two regression and three classification datasets:\n\u2022 Logistic/Linear Regression: Prevalent, simple, intelligible models. This comparison tells us how much gain we get from capturing non-linear relationships between individual features and the target using NAMs. We use the sklearn implementation [Pedregosa et al., 2011], and tune the hyperparameters with grid search.\n\u2022 Decision Trees: This comparison shows the performance benefits of NAMs which are more intelligible than decision trees unless the trees are very small. We use the sklearn implementation [Pedregosa et al., 2011], and tune the hyperparameters with grid search.\n\u2022 Explainable Boosting Machines (EBMs): Current state-of-the-art GAMs [Caruana et al., 2015, Lou et al., 2012] which use gradient boosting of shallow bagged trees that cycle oneat-a-time through the features. We use the open-source implementation [Nori et al., 2019] with the parameters specified by prior work [Caruana et al., 2015] for a fair comparison. \u2022 Deep Neural Networks (DNNs): Unrestricted, full-complexity models which can model\nhigher-order interaction between the input features. This gives us a sense of how much accuracy we sacrifice in order to gain interpretability with NAMs. We train DNNs with 10 hidden layers containing 100 units each with ReLU activation using the Adam optimizer. This architecture choice ensures that this network had the capacity to achieve perfect training accuracy on datasets used in our experiments. We use weight decay and dropout to prevent overfitting and tune hyperparameters using a similar protocol as NAMs.\n\u2022 Gradient Boosted Trees: Another class of full-complexity models that provides an upper bound on the achievable test accuracy in our experiments. We use the XGBoost implementation [Chen and Guestrin, 2016] and tune the hyperparameters using random search.\nFor each dataset, we select the feature nets amongst (1) DNNs containing 3 hidden layers with 64, 64 and 32 units and ReLU activation, and (2) single hidden layer NNs with 1024 ExU units and ReLU-1 activation. We perform 5-fold cross validation to evaluate the accuracy of the learned models. To measure performance, we use area under the precision-recall curve (AUC) for binary classification (as the datasets are unbalanced) and root mean-squared error (RMSE) for regression. More details about training and evaluation can be found in Section A.2 in the supplementary material.\nVisualization. We plot each shape function and the corresponding data density on the same graph. Specifically, we plot each learned shape function fk(xk) vs. xk for an ensemble of NAMs using a semi transparent blue line, which allows us to see when the models in the ensemble learned the same shape function and when they diverged. This provides a sense of the confidence of the learned shape functions. We also plot on the same graphs the normalized data density, in the form of pink bars. The darker the shade of pink, the more data there is in that region. This allows us to know when the model had adequate training data to learn appropriate shape functions."
    },
    {
      "heading": "3.1 Classification",
      "text": ""
    },
    {
      "heading": "3.1.1 COMPAS: Risk Prediction in Criminal Justice",
      "text": "COMPAS is a proprietary score developed to predict recidivism risk, which is used to inform bail, sentencing and parole decisions and has been the subject of scrutiny for racial bias [Angwin et al.,\n2016, Dressel and Farid, 2018, Tan et al., 2018]. In 2016, ProPublica released recidivism data4 on defendants in Broward County, Florida along with the predictions from the COMPAS model.\nHere, we ask whether this training dataset is biased using the transparency of NAMs. Figure 4 shows the learned NAM which is as accurate as black-box models on this dataset. The shape function for race indicates that the learned NAM may be racially biased: black defendants are predicted to be higher risk for reoffending than white or Asian defendants. This suggests that the recidivism training dataset may be racially-biased. The modularity of NAMs allow this bias to be easily corrected to a certain extent; we can simply remove the contributions learned using the race attribute by zeroing out its mean-centered graph (i.e., set the feature to zero in the learned NAM). Although this would drop the AUC score as we are removing a discriminative feature, it may be a more fair model to use for making bail decisions.\nDiscussion. Machine learning (ML) models trained on data will learn any biases in the data: e.g., ML for recidivism prediction will learn race bias if the data has a race bias. It is important to keep the potentially offending attributes in the model during training so that the bias can be detected and then removed after training. If the offending variables are eliminated before training, it makes debiasing the model more difficult: if the offending attributes are correlated with other training attributes, the bias is likely to spread to those attributes. The transparency and modularity of NAMs allows one to detect unanticipated biases in data and makes it easier to correct the bias in the learned model."
    },
    {
      "heading": "3.1.2 MIMIC-II: Mortality Prediction in ICUs",
      "text": "Figure 5 shows 16 of the shape functions learned by the Neural Additive Model for the MIMIC-II dataset [Saeed et al., 2011] to predict mortality in intensive care unit (ICUs). (The 17th graph for Admission Type is flat and we omit it to save space.) The plot for HIV/AIDS shows that patients\n4https://github.com/propublica/compas-analysis\nwith AIDS have less risk of ICU mortality. While this might seem counter-intuitive, we confirmed with doctors that this is probably correct: among the various reasons why one might be admitted to the ICU, AIDS is a relatively treatable illness and is one of the less risky reasons for ICU admission. In other words, being admitted to the ICU for AIDS suggests that the patient was not admitted for another riskier condition, and thus the model correctly predicts that the patient is at less risk than other non-AIDS patients.\nThe shape plot for Age shows, as expected, that mortality risk tends to increase with age, with the most rapid rise in risk happening above age 80. There is detail in the graph that is interesting and warrants further study such as the small increase in risk at ages 18 and 19, and the bump in risk at age 90 \u2014 jumps in risk that happen at round numbers are often due to social effects.\nThe shape plot for Bilirubin (a by product of the breakdown of red blood cells) shows that risk is low for normal levels below 2-3, and rises significantly for levels above 15-20, until risk drops again above 50. There is also a surprising drop in risk near 35 that requires further investigation. We believe the drop in risk above 50 is because patients above 50 begin to receive dialysis and other critical care and these treatments are very effective. The drop in risk that occurs for Urea above 175 is also likely due to dialysis.\nThe plot for the Glasgow Coma Index (GCS) is monotone decreasing as would be expected: higher GCS indicates less severe coma. Note that NAMs are not biased to learn monotone functions such\nas this and the shape of the plot is driven by the data. The NAM also learns a monotone increasing shape plot for risk as a function of renal function. This, too, is as expected: 0.0 codes for normal renal function and 4.0 indicates severe renal failure.\nThe NAM has learned that risk is least for normal heart rate (HR) in the range 60-80, and that risk rises as heart rate climbs above 100. Also, as expected, both Lymphoma and Metastatic Cancer increase mortality risk. The CO2 graph shows low risk for the normal range 22-24.\nThe shape plot for PFratio (a measure of the effectiveness of converting O2 in air to O2 in blood) shows a drop at PFratio = 332 which upon further inspection is due to missing values in PFratio being imputed with the mean: because most patients do not have their PFratio measured, the highest density of patients are actually missing their PFratio which was then imputed with the mean value of 332. One way to detect that imputed missing values are responsible for a dip (or rise) in a shape plot is when risk at the mean value of the attribute suddenly drops (or rises) to a risk level similar to what the model learns for patients who are considered normal/healthy in this dimension: the jump happens at the mean when imputation is done with the mean value, and the level jumps towards the risk of normal healthy patients because often the variable was not measured because the patients were considered normal (for this attribute), a medical assessment which is often correct.\nNormal temperature is coded as 4.0 on the temperature plot, and risk rises for abnormal temperature above and below this value. It\u2019s not clear if the non-monotone risk for hypothermic patients with temperatures 1 or 2 is due to variance, an unknown problem with the data, or an unexplained but real effect in the training signals and warrants further investigation. Similarly, the shape plot for Systolic Blood Pressure (SBP) shows lowest risk for normal SBP near 120, with risk rising for abnormally elevated or depressed SBP. The jumps in risk that happen at 175, 200, and 225 are probably due to treatments that doctors start to apply at these levels: the risk rises to left of these thresholds as SBP rises to more dangerous levels but before the treatment threshold is reached, and then drops a little to the right of these thresholds when most patients above the treatment threshold are receiving a more aggressive treatment that is effective at lowering their risk.\nDiscussion. In summary, most of what the NAM has learned appears to be consistent with medical knowledge, though a few details on some of the graphs (e.g., the increase in risk for young patients, and the drop in risk for patients with Bilirubin near 35) require further investigation. NAMs are attractive models because they often are very accurate, while remaining interpretable, and if a detail in some graph is found to be incorrect, the model can be edited by re-drawing the graph. However, NAMs (like all GAMs), are not causal models. Although the shape plots can be informative, and can help uncover problems with the data that might need correction before deploying the models, the plots do not tell us why the model learned what it did, or what the impact of intervention (e.g., actively lowering a patient\u2019s fever or blood pressure) would be. The shape plots do, however, tell us exactly how the model makes its predictions."
    },
    {
      "heading": "3.1.3 Credit Fraud: Financial Fraud Detection",
      "text": "This is a large dataset [Dal Pozzolo, 2015] containing 284,807 transactions made by European credit cardholders where the task is to predict whether a given transaction is fraudulent or not. It is highly unbalanced and contains only 492 frauds (0.172% of the entire dataset) of all transactions. Table 1 shows that on this dataset, NAMs outperform EBMs and perform comparably to the XGBoost baseline. This shows the benefit of using NAMs instead of tree-based GAMs and suggests that NAMs can provide highly accurate and intelligible models on large datasets. NAMs using ExU units perform much better compared to NAMs with standard DNNs (AUC \u2248 0.974).\nTable 1: AUC on the classification datasets for different learning methods. Each cell contains the mean AUC \u00b1 one standard deviation obtained via 5-fold cross validation. Higher AUCs are better.\nModel COMPAS MIMIC-II Credit Fraud\nLogistic Regression 0.730\u00b1 0.014 0.791 \u00b1 0.007 0.975 \u00b1 0.010 Decision Trees 0.723 \u00b1 0.010 0.768 \u00b1 0.008 0.956 \u00b1 0.004\nNAMs 0.741 \u00b1 0.009 0.830 \u00b1 0.008 0.980 \u00b1 0.002 EBMs 0.740 \u00b1 0.012 0.835 \u00b1 0.007 0.976 \u00b1 0.009\nXGBoost 0.742 \u00b1 0.009 0.844 \u00b1 0.006 0.981 \u00b1 0.008 DNNs 0.735 \u00b1 0.006 0.832 \u00b1 0.009 0.978 \u00b1 0.003\nTable 2: RMSE on regression datasets for different learning methods. Each cell contains the mean RMSE \u00b1 one standard deviation obtained via 5-fold cross validation. Lower RMSE is better.\nModel California Housing FICO Score\nLinear Regression 0.728 \u00b1 0.015 4.344 \u00b1 0.056 Decision Trees 0.720 \u00b1 0.006 4.900 \u00b1 0.113\nNAMs 0.562 \u00b1 0.007 3.490 \u00b1 0.081 EBMs 0.557 \u00b1 0.009 3.512 \u00b1 0.095\nXGBoost 0.532 \u00b1 0.014 3.345 \u00b1 0.071 DNNs 0.492 \u00b1 0.009 3.324 \u00b1 0.092"
    },
    {
      "heading": "3.2 Regression",
      "text": "3.2.1 California Housing: Predicting Housing Prices\nCalifornia Housing dataset [Pace and Barry, 1997] is a canonical machine learning dataset derived from the 1990 U.S. census to understand the influence of community characteristics on housing prices. The task is regression to predict the median price of houses (in million dollars) in each California district. The learned NAM considers the median income as well as the house location (latitude, longitude) as the most important features (we omit the other six graphs to save space, see Figure 10 in appendix). As shown by Figure 7, the house prices increase linearly with median income in high data density regions. Furthermore, the graph for longitude shows sharp jumps in price prediction around 122.5\u25e6W and 118.5\u25e6W which roughly correspond to San Francisco and Los Angeles respectively."
    },
    {
      "heading": "3.2.2 FICO Score: Understanding Credit Scores",
      "text": "The FICO score is a widely used proprietary credit score to determine credit worthiness for loans in the United States. The FICO dataset [FICO, 2018] is comprised of real-world anonymized credit applications made by customers and their assigned FICO Score, based on their credit report information. We visualize the feature contributions of a NAM trained using the FICO dataset (see Figure 11 in appendix) for two applicants (Table 6) with low and high scores respectively.\nFigure 8 shows that the most important features for the high scoring applicant are (1) Average Months on File and (2) Net Fraction Revolving Burden (i.e., percentage of credit limit used) which take the value 235 months and 0% respectively. This makes sense, as generally, the longer a person\u2019s credit history, the better it is for their credit score. Although there is a strong inverse correlation between Net Fraction Revolving Burden and the score, it is positively correlated for small values (< 10). This means that making use of some credit increases your credit score, but using too much of it is bad. We are confident in this interpretation because most of the data density is in small values of Net Fraction Revolving Burden, and each NAM in the ensemble displays a similar shape function (Figure 11).\nFor the low scoring applicant, the main contributing factors are (1) Total Number of Trades5 and (2) Net Fraction Installment Burden (Installment balance divided by original loan amount) which take the values 57 and 68% respectively. This applicant used their credit quite frequently and has a large burden, thus resulting in a low score."
    },
    {
      "heading": "4 Related Work",
      "text": "The Generalized Additive Neural Networks (GANNs) proposed by Potts [1999] are somewhat similar to the Neural Additive Models we propose here. Like our NAMs, GANNs used a restriction in the neural network architecture to force the net to learn additive functions of individual input features. GANNs, however, predate deep learning and use a single hidden layer, did not use backpropagation [Rumelhart et al., 1986] and required human-in-the-loop evaluation and were not successful in training accurate GAM models with neural nets.\nGANNs begin with subnets containing a single hidden unit for each input feature, and use a humanin-the-loop process to add (or subtract) hidden units to the architecture based on human evaluation of plotted partial residuals. This means that the training procedure cannot be automated. In practice, the laborious manual effort required to evaluate all of the partial residual plots to decide what to do next, and then retrain the model after adding or subtracting hidden units from the architecture meant that GANN nets remained very small and simple \u2014 typically only one hidden unit per feature.\nIn contrast, the NAMs we develop in this paper benefit from the advances in deep learning. They use a large number of hidden units and multiple hidden layers per input feature subnet to allow more complex, more accurate shape functions to be learned. Finally, NAMs use a specific hidden unit structure to allow subnets to learn the more non-linear functions often required for accurate GAM models, and then form an ensemble of these nets to provide uncertainty estimates, further improve accuracy and reduce the high-variance that can result from encouraging the model to learn highly non-linear functions. Taken together, these methods allow NAMs to train GAM models with sufficient representational power to achieve state-of-the-art accuracy while remaining intelligible.\nPrior to NAMs, the state-of-the-art in high-accuracy, interpretable GAM models [Hastie and Tibshirani, 1990, Guisan et al., 2002] are the GAM [Lou et al., 2012] and GA2M [Lou et al., 2013] models\n5Credit trades refer to any agreement between a lending agency and consumers.\nbased on regularized boosted decision trees which were successfully applied to healthcare datasets by Caruana et al. [2015]. We compare the accuracy of NAMs to these models in Section 3."
    },
    {
      "heading": "5 Conclusion",
      "text": "We present Neural Additive Models (NAMs), a new class of models which combines the intelligibility of GAMs with the expressivity of DNNs. We find that NAMs, which don\u2019t model any higher-order feature interactions, are comparable in performance to DNNs on a variety of tabular datasets. In addition, NAMs outperform widely used intelligible models and perform similarly to state-of-the-art GAMs while being more easily applicable to a broader set of real-world problems and straightforward to combine with deep learning techniques."
    },
    {
      "heading": "6 Acknowledgments",
      "text": "We would like to thank Kevin Swersky for reviewing an early draft of the paper. We also thank Sarah Tan for providing us with pre-processed versions of some of the datasets used in the paper. RA would also like to thank Marlos C. Machado and Marc G. Bellemare for helpful discussions."
    },
    {
      "heading": "A Supplementary Material",
      "text": "A.1 Fitting Jagged Curves\nA.2 Experimental Details\nTraining and Hyperparameter Details. The NAM feature networks (f\u03b8k ) are trained jointly using the Adam optimizer [Kingma and Ba, 2014] with a batch size of 1024 for a maximum of 1000 epochs with early stopping using the validation dataset. The learning rate is annealed by a factor of 0.995 every training epoch. For all the tasks, we tune the learning rate, output penalty coefficient (\u03bb1), weight decay coefficient (\u03bb2), dropout rate (\u03bb3) and feature dropout rate (\u03bb4). For computational efficiency, we tune these hyperparameters using Bayesian optimization [Snoek et al., 2012, Golovin et al., 2017] based on cross-validation performance with a single train-validation split for each fold.\nEvaluation. We perform 5-fold cross validation to evaluate the accuracy of the learned models. To measure performance, we use area under the precision-recall curve (AUC) for binary classification (as the datasets are unbalanced) and root mean-squared error (RMSE) for regression. For NAMs and DNNs, one of the 5 folds (20% data) is used as a held-out test set while the remaining 4 folds are used for training (70% data) and validation (10% data). The training and validation splits are randomly subsampled from the 4 folds and this process is repeated 20 times. For each run, the validation set is used for early stopping. For each fold, we ensemble the NAMs and DNNs trained on the 20 and EBMs on 100 train-validation splits respectively to make the prediction on the held-out test set.\nA.3 Hyperparameters\nWe use a batch size of 1024 with the Adam optimizer and a learning rate decay of 0.995 every epoch in our experiments for NAMs and DNNs.\nNAMs. We tune the dropout coefficient (\u03bb3) in the discrete set {0, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9}, weight decay coefficient (\u03bb2) in the continuous interval [0.000001, 0.0001), learning rate in the interval [0.001, 0.1), feature dropout coefficient (\u03bb4) in the discrete set {0, 0.05, 0.1, 0.2} and output penalty coefficient (\u03bb1) in the interval [0.001, 0.1). Note that the weight decay is implemented as the average weight decay over the individual feature networks in NAMs. Refer to Table 4 and Table 3 for hyperparameters found on regression and classification datasets.\nDNNs. We tune the dropout coefficient (\u03bb3) in {0, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5}, weight decay coefficient (\u03bb2) in the continuous interval [0.0000001, 0.1) and learning rate in the interval [0.001, 0.1).\nA.4 Additional Graphs and Tables"
    }
  ],
  "title": "Neural Additive Models: Interpretable Machine Learning with Neural Nets",
  "year": 2020
}
