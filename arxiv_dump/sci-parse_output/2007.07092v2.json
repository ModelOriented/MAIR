{
  "abstractText": "Digital discrimination is a form of discrimination whereby users are automatically treated unfairly, unethically or just differently based on their personal data by a machine learning (ML) system. Examples of digital discrimination include low-income neighborhood\u2019s targeted with high-interest loans or low credit scores, and women being undervalued by 21% in online marketing. Recently, different techniques and tools have been proposed to detect biases that may lead to digital discrimination. These tools often require technical expertise to be executed and for their results to be interpreted. To allow non-technical users to benefit from ML, simpler notions and concepts to represent and reason about digital discrimination are needed. In this paper, we use norms as an abstraction to represent different situations that may lead to digital discrimination. In particular, we formalise non-discrimination norms in the context of ML systems and propose an algorithm to check whether ML systems violate these norms.",
  "authors": [
    {
      "affiliations": [],
      "name": "Natalia Criado"
    },
    {
      "affiliations": [],
      "name": "Xavier Ferrer"
    },
    {
      "affiliations": [],
      "name": "Jose M. Such"
    }
  ],
  "id": "SP:ae29a3845e478f828772e385da1b4300c0ad1e3a",
  "references": [
    {
      "authors": [
        "Solon Barocas",
        "Andrew"
      ],
      "title": "Selbst, \u2018Big Data\u2019s Disparate Impact",
      "venue": "California law review,",
      "year": 2016
    },
    {
      "authors": [
        "Rachel KE Bellamy",
        "Kuntal Dey",
        "Michael Hind",
        "Samuel C Hoffman",
        "Stephanie Houde",
        "Kalapriya Kannan",
        "Pranay Lohia",
        "Jacquelyn Martino",
        "Sameep Mehta",
        "A Mojsilovi\u0107"
      ],
      "title": "Ai fairness 360: An extensible toolkit for detecting and mitigating algorithmic bias",
      "venue": "IBM Journal of Research and Development,",
      "year": 2019
    },
    {
      "authors": [
        "Flavio Calmon",
        "Dennis Wei",
        "Bhanukiran Vinzamuri",
        "Karthikeyan Natesan Ramamurthy",
        "Kush R Varshney"
      ],
      "title": "Optimized pre-processing for discrimination prevention",
      "venue": "Advances in Neural Information Processing Systems,",
      "year": 2017
    },
    {
      "authors": [
        "William G Cochran"
      ],
      "title": "The \u03c72 test of goodness of fit",
      "venue": "The Annals of Mathematical Statistics,",
      "year": 1952
    },
    {
      "authors": [
        "Cody Cook",
        "Rebecca Diamond",
        "Jonathan Hall",
        "John A List",
        "Paul Oyer"
      ],
      "title": "The gender earnings gap in the gig economy: Evidence from over a million rideshare drivers",
      "year": 2018
    },
    {
      "authors": [
        "Thomas M Cover",
        "Joy A Thomas"
      ],
      "title": "Elements of information",
      "year": 2012
    },
    {
      "authors": [
        "Natalia Criado",
        "Jose M"
      ],
      "title": "Such, \u2018Digital discrimination\u2019, in Algorithmic Regulation",
      "year": 2019
    },
    {
      "authors": [
        "Anupam Datta",
        "Shayak Sen",
        "Yair Zick"
      ],
      "title": "Algorithmic transparency via quantitative input influence: Theory and experiments with learning systems",
      "venue": "IEEE symposium on security and privacy (SP),",
      "year": 2016
    },
    {
      "authors": [
        "C. Dwork",
        "M. Hardt",
        "T. Pitassi",
        "O. Reingold",
        "R. Zemel"
      ],
      "title": "Fairness through awareness",
      "venue": "ITCS 2012, pp. 214\u2013226. ACM, ",
      "year": 2012
    },
    {
      "authors": [
        "Michael Feldman",
        "Sorelle A Friedler",
        "John Moeller",
        "Carlos Scheidegger",
        "Suresh Venkatasubramanian"
      ],
      "title": "Certifying and removing disparate impact",
      "venue": "in proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining,",
      "year": 2015
    },
    {
      "authors": [
        "Frank Freese"
      ],
      "title": "Elementary statistical methods for foresters, number 317",
      "venue": "US Department of Agriculture,",
      "year": 1967
    },
    {
      "authors": [
        "N. Grgi\u0107-Hla\u010da",
        "M. Zafar",
        "K. Gummadi",
        "A. Weller"
      ],
      "title": "Beyond Distributive Fairness in Algorithmic Decision Making",
      "venue": "AAAI, 51\u201360, ",
      "year": 2018
    },
    {
      "authors": [
        "Sara Hajian",
        "Josep Domingo-Ferrer"
      ],
      "title": "A methodology for direct and indirect discrimination prevention in data mining",
      "venue": "IEEE transactions on knowledge and data engineering,",
      "year": 2012
    },
    {
      "authors": [
        "Faisal Kamiran",
        "Toon Calders"
      ],
      "title": "Data preprocessing techniques for classification without discrimination",
      "venue": "Knowledge and Information Systems,",
      "year": 2012
    },
    {
      "authors": [
        "N. Kilbertus",
        "M. Carulla",
        "G. Parascandolo",
        "M. Hardt",
        "D. Janzing",
        "B. Sch\u00f6lkopf"
      ],
      "title": "Avoiding discrimination through causal reasoning",
      "venue": "NIPS\u201917, pp. 656\u2013666, ",
      "year": 2017
    },
    {
      "authors": [
        "Cathy O\u2019neil"
      ],
      "title": "Weapons of math destruction: How big data increases inequality and threatens democracy",
      "venue": "Broadway Books,",
      "year": 2016
    },
    {
      "authors": [
        "Dino Pedreschi",
        "Salvatore Ruggieri",
        "Franco Turini"
      ],
      "title": "Integrating induction and deduction for finding evidence of discrimination",
      "venue": "Proceedings of the 12th International Conference on Artificial Intelligence and Law, p",
      "year": 2009
    },
    {
      "authors": [
        "Florian Tramer",
        "Vaggelis Atlidakis",
        "Roxana Geambasu",
        "Daniel Hsu",
        "Jean-Pierre Hubaux",
        "Mathias Humbert",
        "Ari Juels",
        "Huang Lin"
      ],
      "title": "Fairtest: Discovering unwarranted associations in data-driven applications",
      "venue": "IEEE European Symposium on Security and Privacy (EuroS&P),",
      "year": 2017
    },
    {
      "authors": [
        "Sahil Verma",
        "Julia Rubin"
      ],
      "title": "Fairness definitions explained",
      "venue": "IEEE/ACM International Workshop on Software Fairness (FairWare),",
      "year": 2018
    }
  ],
  "sections": [
    {
      "heading": "1 Introduction",
      "text": "Digital discrimination is a form of discrimination in which automated decisions taken by algorithms, increasingly based on artificial intelligence techniques like machine learning, treat users unfairly, unethically, or just differently based on their personal data [8] such as income, education, gender, age, ethnicity, or religion. Digital discrimination is becoming a serious problem [18], as more and more tasks are delegated to computers, mobile devices, and autonomous systems, for example some UK firms base their hiring decisions on automated algorithms.\nFrequently the users of such machine learning (ML) systems are not technical experts and cannot assess by themselves if these algorithms are discriminatory. For example, many public organizations would like to reduce operational costs and delegate some decisions to algorithms, but at the same time need some guarantees about the ML systems not breaking anti-discrimination laws. Our approach has been precisely designed to allow non-technical users to determine if ML systems are potentially discriminatory and to make explicit under which assumptions the systems are discrimination free.\nThis paper is organised as follows: Section 2 introduces background knowledge on discrimination legislation; Section\n1 Department of Informatics, King\u2019s College London, UK, emails: {natalia.criado,xavier.ferrer aran,jose.such}@kcl.ac.uk \u2217 Author\u2019s copy of the manuscript accepted in the Advancing To-\nwards the SDGS Artificial Intelligence for a Fair, Just and Equitable World Workshop of the 24th European Conference on Artificial Intelligence (ECAI\u201920).\n3 introduces our formalization of non-discrimination norms in the context of ML systems; Section 4 contains our attesting algorithm; Section 5 illustrates the performance of our algorithm in two case studies; Section 6 contains related work; and Section 7 contains a discussion of the paper contribution."
    },
    {
      "heading": "2 Background",
      "text": "Many nation states and international organizations have enacted legislation prohibiting discrimination; e.g., the European Convention for the Protection of Human Rights. Most anti-discrimination legislation simply consists of a nonexhaustive list of criteria or protected attributes (e.g., race, gender, sexual orientation) on the basis of which discrimination is forbidden. Thus, discrimination are actions, procedures, etc., that disadvantage citizens based on their membership of particular protected groups defined by those attributes.\nLegal systems traditionally distinguish between two main types of discrimination [1]:\n1. Direct discrimination (also known as disparate treatment) considers the situations in which an individual is treated differently because of their membership of a particular social group. This ultimately means that different social groups are being treated differently, with some of them effectively being disadvantaged by these differences in treatment. A clear example of direct discrimination would be a company having the policy of not hiring women with young children. Note, however, that direct discrimination does not necessarily involve that the discrimination process is explicit:\n(a) Direct discrimination can be explicit, exemplified in the previous case of a member of a particular social group (women with young children) explicitly disadvantaged by a decision process (hiring policy).\n(b) Direct discrimination can be implicit, in situations in which the discriminated group is not explicitly mentioned. For example, the same company may replace the explicit hiring policy with a policy of not hiring candidates who have had a career break in recent years. Although the policy does not explicitly refer to the relevant social group (women with children), it accomplishes the same task since woman with young children are statistically more likely to have had a recent career break.\n2. Indirect discrimination (also known as disparate impact) considers the situations in which an apparently neutral act\nar X\niv :2\n00 7.\n07 09\n2v 2\n[ cs\n.A I]\n3 A\nug 2\n02 0\nhas a disproportionately negative effect on the members of a particular social group. This is considered discrimination even if there is no intention to discriminate against that particular group or if there is not any unconscious prejudice motivating the discriminatory act. For example, a company having the policy to only consider customer satisfaction scores to award promotions may have a disproportionate impact on women, as there is empirical evidence suggesting that women are under evaluated when compared to their male counterparts with a similar objective performance. In this case, the company may not have an intention to discriminate against female employees, but the promotion criteria set may effectively disadvantage them disproportionally."
    },
    {
      "heading": "3 Digital Discrimination Normative Model",
      "text": "The term digital discrimination refers to those direct or indirect discriminatory acts that are based on the automatic decisions made by an ML system. In this section we formalise the notion of digital discrimination norms accounting for the different types of discrimination: explicit, implicit, and indirect.\nAn ML system can be defined by a set of input features I = {I1, ...Im}, where each feature Ii takes values from a discrete domain DIi ; and an output feature O, which also takes values from a discrete domain DO\n2. Note in this paper we are interested in ML systems where the input contains personal information about individuals in order to attest discrimination. For this reason, the set of protected features also needs to be defined; i.e., P = {P1, .., Pn}, where each protected feature Pi \u2208 P takes values from a discrete domain DPi3.\nThe decisions of an ML system can be represented as a dataset DS formed by tuples (p1, ...pn, i1, ...im, o) representing a previous decision made by the ML system about a particular individual with protected attributes p1, ...pn, input attributes i1, ...im, and algorithm outcome o\n4. In particular, each pi \u2208 DPi , ii \u2208 DIi and o \u2208 DO.\nIn the following, we provide a formalization of nondiscrimination norms for ML systems and define how domain knowledge can be represented using norm exceptions. These normative notions are illustrated with an example."
    },
    {
      "heading": "3.1 Digital Discrimination Norms",
      "text": "As aforementioned, in the legislation we find the following types of discrimination: direct (also known as disparate treatment), which further classifies into explicit and implicit; and indirect (disparate impact) [21]. In the following, we contextualise these notions in the context of digital discrimination and we formally represent them as computational norms using deontic logic5.\n2 For simplicity we assume domains are discrete, but any continuous domain can be discretized. 3 Note that it is possible that protected features are part of the input used by a ML system, but is not necessary. 4 Note it is possible to consider the discrimination in an algorithm by considering also the ground-truth labels. See Appendix B for more details about this type of discrimination. 5 For simplicity, we don\u2019t consider compound discrimination. For a definition of compound discrimination norms see Appendix A\n\u2022 Direct Discrimination: is the unequal behavior toward someone because of a protected characteristic. We identify two types of direct discrimination:\n\u2013 Explicit Discrimination. In terms of ML systems this equals to having some of the protected attributes considered in the algorithm input. Norms preventing explicit discrimination can be formalised as prohibitions to include protected attributes in the input of the system:\n\u2200Pi \u2208 P : F(Pi \u2208 I)\nThe set of all explicit discrimination norms is denoted by NE and has a size of |P|.\n\u2013 Implicit Discrimination can be formalised as a situation where the values of a set of input attributes correlate with the value of some protected attribute.\n\u2200Pi \u2208 P : F(Pi is a function of I)\nNote the fact that Pi is a function of I needs to be defined in terms of a process to detect correlations or dependencies between attributes (Section 6 provides more details about techniques that can be used for this).\nThe set of all implicit discrimination norms is denoted by NI and has a size of |P|\n\u2022 Indirect Discrimination (disparate impact) refers to decisions that adversely affect one group of people of a protected characteristic more than another. This equals to state that for a particular protected attribute value p \u2208 DPi the probability of a given outcome o \u2208 Do is x times lower than that of the values of the same protected attribute p with the highest probability:\n\u2200Pi \u2208 P, \u2200p \u2208 DPi , \u2200o \u2208 DO : F(Pi \u2193 p o)\nwhere Pi \u2193po denotes:\nPr(O = o|Pi = p) < x\u00d7 max \u2200p\u2032\u2208DPi\nPr(O = o|Pi = p\u2032)\nPr(O = o|Pi = p) stands for probability that outcome o is given to an individual with protected attribute p. Note that different methods can be used to estimate this probability, in Section 6 we provide a review of different techniques that can be used. The value x \u2208 [0, 1] is a constant representing the disproportion allowed in a particular domain6. The set of all disparate impact norms is denoted by ND and has a size of |P| \u00d7DP \u00d7 |DO|, where DP denotes the average number of values belonging to the domain of protected attributes.\nDiscrimination norms are represented as a collection denoted by N = (NE , NI , ND).\nRemark 1. If an explicit discrimination norm for a protected feature Pi is violated, then the implicit discrimination norm for Pi is also violated. The inverse inference is not true.\n6 For example the US fourth-fifth rule from the Equal Employment Opportunity Commission (1978) states that a job selection rate for the protected group of less than 4/5 of the selection rate for the unprotected group [2].\nRemark 2. If an explicit discrimination norm for a protected feature Pi is violated and no indirect discrimination norm for Pi is violated, then the violation is inconsequential as the protected feature Pi is not affecting significantly the decision-making process. If an implicit discrimination norm for protected feature Pi is violated and no indirect discrimination norm for Pi is violated, then the violation is inconsequential as the protected feature Pi is not affecting significantly the decision-making process.\nIn this paper we will define inconsequential norm violations as those violations which can be considered trivial as they do have little effect on the decisions made by algorithms. Importantly, inconsequential violations are anyway worth considering as they may be an indicator of bad practice (e.g., considering disability status of students in university admissions may be immoral even if that information is not influencing much the decision)."
    },
    {
      "heading": "3.2 Norm Exceptions",
      "text": "The previous section formalises the general definition of antidiscrimination norms. In general, when these norms are violated there is a potential case of digital discrimination. However, there are domains in which the violation of these norms is justifiable, and hence not result in discrimination. To allow for such type of domain knowledge to be explicitly represented and accounted for, we use the notion of domain permission norms, which define exceptions to the general antidiscrimination norms.\n\u2022 Permission to use protected attributes in decision making. For example, legislation usually does not consider discriminatory to use religion as a criteria for hiring a religion teacher at a school. An explicit permission to use a protected attribute Pi \u2208 P can be defined as follows:\nP : P(Pi \u2208 I)\nThe set of all exceptions to explicit discrimination norms is denoted by EE . \u2022 Permission to allow for correlations between a protected attribute and input attributes. For example, in some employments (e.g., firefighters) the employees should demonstrate physical strength, which is correlated with gender. In such cases, it is lawful to consider the results of fitness tests in hiring decisions. This allowed correlation between a protected attribute Pi \u2208 P and a subset of the input attributes I \u2282 I can be represented as a permission as follows:\nP(Pi is a function of I)\nThe set of all exceptions to implicit discrimination norms is denoted by EI . \u2022 Permission to adversely affect one group. For example, on average women Uber drivers are paid less than men drivers [6], but that is explained by factors such as driver experience, time and location of rides, etc. An exception to allow for a significant difference on an outcome o \u2208 Do for a particular protected group p \u2208 DPi where Pi \u2208 P can be formalised as follows:\nP(Pi \u2193po)\nThe set of all exceptions to indirect discrimination norms is denoted by ED.\nDomain exceptions to discrimination norms are represented as a collection denoted by E = (EE , EI , ED).\nRemark 3. An exception to an explicit discrimination norm about protected attribute Pi entails an exception for the implicit discrimination norm related to Pi and all input attributes. The inverse relationship does not hold.\nRemark 4. An exception to an explicit discrimination norm about protected attribute Pi does not entail an exception to any indirect discrimination norms for Pi. An exception to an implicit discrimination norm about protected attribute Pi does not entail an exception to any indirect discrimination norms for Pi.\nThere may be cases in which it is lawful to consider protected attributes in the decision-making process, either explicitly or implicitly, as long as that information is not used to disproportionately disadvantage the members of a certain group; e.g., positive discrimination practices may use gender information can be used to increase the number of employees from minority groups in a company or business, which are known to have been discriminated against in the past. In this case there is an exception to an explicit discrimination norm about gender, as long as that information is not used to adversely affect any group."
    },
    {
      "heading": "3.3 Example: Credit Risk Assessment",
      "text": "To illustrate the different types of norms and exceptions let us consider an example of a decision making system that classifies people as high or low credit risks.\nIn particular, the attributes used to describe people are:\nI = {Age, Job, Salary}\nwhere and Age \u2208 {[20, 30], [30, 40], ...}, Job \u2208 {Unemployed, Unskilled, ...}, and Salary \u2208 {[0, 20k], [20k, 30k], ...}. According to common discrimination law, protected attributes are defined as:\nP = {Gender,Age}\nwhere Gender \u2208 {Male, Female}. The output variable is:"
    },
    {
      "heading": "O = Risk",
      "text": "where Risk \u2208 {High, Low}. In this example the following norms are generated considering protected attributes:\nF(Gender \u2208 I),F(Age \u2208 I), F(Gender is a function of I),F(Age is a function of I)\nF(Gender \u2193MaleHigh),F(Gender \u2193MaleLow ),\nF(Gender \u2193FemaleHigh ),F(Gender \u2193FemaleLow ),\nF(Age \u2193[20,30]High ),F(Age \u2193 [20,30] Low ), ...\n...,F(Age \u2193[70,80]High ),F(Age \u2193 [70,80] Low ),\nHowever, in this example there are several exceptions to the norms:\nP(Age \u2208I) P(Gender is a function of {Salary}),\nP(Age \u2193[20,30]High ),P(Age \u2193 [20,30] Low ), ...\n...,P(Age \u2193[70,80]High ),P(Age \u2193 [70,80] Low ),\nIn particular, it is considered lawful to use age in credit risk assessment, as it is common practice to use age to estimate health, unemployment probabilities, etc. By Remark 3, it is implicitly permitted that age is a function of input attributes. Moreover, it is considered permitted to allow age to have a significant impact on credit decisions. The pay gap phenomenon also explains a degree of correlation between salary and gender. In this case, however, the use of salary for credit risk assessment is lawful (i.e., salary has not been used as a way to discriminate women, but as a way to determine the capability of individuals to pay a credit back)."
    },
    {
      "heading": "4 Digital Discrimination Attesting Process",
      "text": "The digital discrimination attesting process (see Figure 1) takes as input a dataset and the domain exceptions defined by the user, and it returns a discrimination report with information about any potential discrimination cases (i.e., a minimal list of norm violations) and about the assumptions made in the attesting process (i.e., the list exceptions provided by the user and the allowed disproportion ratio)7.\nThe attesting algorithm (see Algorithm 1) starts by generating the list of discrimination norms based on the input, protected and output attributes (line 7) then it checks for:\n\u2022 Explicit direct discrimination (lines 8-15). For each explicit norm that is violated a new inconsequential violation is added (later on the algorithm will confirm if this violation is inconsequential or not) and the implicit norm related to that protected attribute is removed. Note our goal is to produce the minimal set of violations and by Remark 1 the explicit norm is more general.\n7 Note the purpose of our paper is to allow non-technical users to attest whether ML systems discriminate. For examples of research on mitigating discrimination see [14, 4, 15, 11].\n\u2022 Implicit direct discrimination (lines 16-24). For each implicit norm the algorithm checks if there is an exception to an explicit norm for the same protected attribute (as stated in Remark 3). If not, the algorithm checks if the norm is violated using the dataset as a representative sample (line 21)8. If the norm is violated, the algorithm checks for a permission allowing for that particular violation (lines 18-21). An implicit norm is violated when there is a set of input attributes determining the value of a protected attribute. In particular, the algorithm checks if there is an exception for that set of input attributes, or a subset of it, determining the protect attribute. Again, if the norm is violated, a new inconsequential violation is created. \u2022 Indirect discrimination (lines 25-39). For each indirect norm that is violated a new violation is created (line 28). As stated in Remark 2, if there were some inconsequential violations related to that protected attribute, these are converted into consequential ones (lines 29-36). The violation of the indirect norm associated to a protected attribute demonstrates that that decisions are having a disproportionate impact based on that protected attribute.\nThe algorithm outputs the list of inconsequential and consequential violations. Note that the discrimination report will contain not only the information about norm violations (if any), but also the information about the exceptions considered in this analysis and the level of allowed disproportion specified by the user.\nThe attesting algorithm complexity is determined by the size of the biggest norm set (or exception set). In this case, the complexity is given by O(|P|\u00d7DP \u00d7|DO|). This assumes that the norm violation checks are performed offline and can be retrieved in constant time. Section 6 discusses different methods to check compliance of implicit and indirect norms (note checking compliance of explicit norms equates to checking set membership)."
    },
    {
      "heading": "5 Case studies",
      "text": "In this section, we illustrate the performance of our digital discrimination attesting algorithm using two well-known datasets: the German dataset9 and the Adult dataset10.\nIn our implementation11, we have used the sklearn library for normalised mutual information [7] to detect violations of implicit discrimination norms. The normalised mutual information (NMI) is a measure of the mutual dependence between the two variables that quantifies the \u201damount of information\u201d obtained about one random variable through observing the other random variable. The NMI returns 0 when there is no mutual information between the variables tested, and 1 when there exist a perfect correlation. In the implementation, the minimum coefficient for mutual information can be configured; we used a minimum threshold of 0.6 in the experiments below as indicative of a strong correlation between\n8 Different statistical methods can be used to determine if there is a correlation between input attributes and protected attributes, see Section 6 for more details. 9 https://archive.ics.uci.edu/ml/datasets/statlog+(german+ credit+data) 10 https://archive.ics.uci.edu/ml/datasets/adult 11 Available on Github at https://github.com/xfold/\nNormativeApproachToDiscrimination\nAlgorithm 1: Digital Discrimination Attesting\n1 DiscriminationAttesting (P, I, O,DS,E, x) inputs : A set of protect attributes P\nA set of input attributes I An output attribute O, a dataset DS A collection of exceptions (EE , EI , ED) x \u2208 [0, 1] a constant representing the\ndisproportion allowed output: A collection of violated norms (VE , VI , VD) A collection of norms that have been violated inconsequentially (ID, II)\n2 VE \u2190 \u2205 3 VI \u2190 \u2205 4 VD \u2190 \u2205 5 IE \u2190 \u2205 6 II \u2190 \u2205 7 (NE , NI , ND)\u2190 GenerateNorms(P, I, O)\n// Attesting Explicit Discrimination\n8 foreach F(Pi \u2208 I) \u2208 NE do 9 if 6 \u2203P(Pi \u2208 I) \u2208 EE then\n10 if Pi \u2208 I then 11 IE \u2190 IE \u222a {F(Pi \u2208 I)} 12 NI \u2190 NI \\ {F(Pi is a function of I)} 13 end\n14 end\n15 end // Attesting Implicit Discrimination 16 foreach F(Pi is a function of I) \u2208 NI do 17 if 6 \u2203P(Pi \u2208 I) \u2208 EE then 18 foreach I \u2286 I : I is the minimal set such that Pi is a function of I do 19 if 6 \u2203P(Pi is a function of I \u2032) : I \u2286 I \u2032 then 20 II \u2190 II \u222a {F(Pi is a function of I)} 21 end\n22 end\n23 end\n24 end // Attesting Indirect Discrimination 25 foreach F(Pi \u2193po) \u2208 ND do 26 if \u00ac\u2203P(Pi \u2193po) \u2208 ED then 27 if \u2203p\u2032 \u2208 DPi : Pr(O=o|Pi=p) Pr(O=o|Pi=p\u2032) < x then 28 VD \u2190 VD \u222a {F(Pi \u2193po)} 29 if F(Pi \u2208 I) \u2208 IE then 30 IE \u2190 IE \\ {F(Pi \u2208 I)} 31 VE \u2190 VE \u222a {F(Pi \u2208 I)} 32 end 33 if F(Pi is a function of I) \u2208 II then 34 II \u2190 II \\ {F(Pi is a function of I)} 35 VI \u2190 VI \u222a {F(Pi is a function of I)} 36 end\n37 end\n38 end\n39 end 40 V \u2190 (VE , VI , VD) 41 I \u2190 (IE , II) 42 return V,I\ninput and protected attributes. To detect indirect discrimination we have set to 0.8 the allowed disproportion ratio, inspired by the US fourth-fifth rule from the Equal Employment Opportunity Commission (1978), a threshold commonly used to detect disparate impact in domains like employee selection procedures12. Also, due to the small size of the datasets used in the case studies, we have used the Chi-Squared Test [5] to determine those violations of indirect discrimination norms that are statistically significant (p-value < 0.05). To discretise numeric values, we have used quantile discretisation, which is a well-known method for discretising continuous variables in ML [12]."
    },
    {
      "heading": "5.1 Adult Dataset",
      "text": "The Adult dataset uses 14 attributes to determine if a given person makes over 50K a year. The attributes include education, work class, age, sex, race, and occupation, among others. The dataset contains 48842 instances.\nLet us assume that the gender, age, native country and race are protected and that the other attributes are the inputs of a ML system.\nI = {workclass, education, education num, occupation, capital gain, capital loss, hours per week, fnlwgt}\nP = {age, gender, native country, relationship, marital status, race}"
    },
    {
      "heading": "O = income",
      "text": "where income = {<= 50k,> 50k}. In this case age is related to experience and seniority so it is considered lawful to use age to discriminate:\nP(age \u2193[0,16)<=50k),P(age \u2193 [0,16) >50k ),\n...\nP(age \u2193[75,99)<=50k),P(age \u2193 [75,99) >50k )\nAfter executing our algorithm several violations of indirect discrimination norms are detected. For example:\nF gender \u2193female>50k\nF race \u2193black>50k"
    },
    {
      "heading": "F native country \u2193Nicaragua>50k",
      "text": "F marital status \u2193Married\u2212civ\u2212spouse<=50k The violations above indicate that females, black people and nicaraguans have a disproportionate low probability of being classified as making more than 50k when compared with other groups, in accordance with previous reports of discrimination in the dataset [3]. On the contrary, married people are significantly less likely of being classified as making less than 50k. Found violations are associated with particular values of gender, native country, relationship and marital-status attributes. This indicates that the decision making process may have a disparate impact on people belonging to particular protected groups.\n12 http://www.uniformguidelines.com"
    },
    {
      "heading": "5.2 German Credit Dataset",
      "text": "The German dataset contains information about people who ask for a credit. Each person is classified as good or bad credit risks. This is the inspiration for the small example contained in section 3.3. In particular, the full dataset uses 20 attributes to represent each person, which include information like age, employment status, gender and personal status of the applicant; and the duration, amount and purpose of the credit. The dataset contains 1000 instances.\nLet\u2019s us assume an ML system where age, personal status and sex, and being a foreign worker are considered protected attribues, and the rest of the features in the German dataset are considered inputs:\nI = {job, housing, savings, .., amount, duration, purpose}\nP = {age, personal status and sex, foreign worker}"
    },
    {
      "heading": "O = risk",
      "text": "where risk = {high, low}. In this case, it is considered lawful to use age to discriminate credit risks as people are less likely to repay credits as they become older, hence, we consider age as an exception:\nP(age \u2193[0,16)good ),P(age \u2193 [0,16) bad ),\n...\nP(age \u2193[75,99)good ),P(age \u2193 [75,99) bad )\nAfter executing our algorithm, the following violation is detected:\nF(foreign worker \u2193yesgood)\nThe violation means that foreign workers have a disproportionate low probability of being considered a good credit risk."
    },
    {
      "heading": "6 Related work",
      "text": "Recent research has addressed the problem of discrimination and bias in machine learning. These novel tools are most of the time aimed at technical users capable of interpreting different statistical results, programming, etc. Our algorithm is, on the contrary, aimed at non-technical users (albeit they may be domain experts). The notion of norm and exception is a suitable abstraction to represent the results these statistical analysis to non-technical users. For example, IBM\u2019s AI Fairness 360 Open Source Toolkit13 and Google\u2019s What-if-tool14, are probably two of the most comprehensive toolkits offering a great choice of bias metrics. However, its intended audience are technical users with previous knowledge of machine learning and statistics. Indeed, there are a large number of fairness metrics that may be appropriate for a given application [21]. Also it is difficult for non-technical users to represent domain knowledge in a way that it can be taken into account by the metrics.\nClosely related to our work is [19], where the authors proposed to infer classification rules from a given dataset and to detect those classification rules that can cause direct and indirect discrimination. They also allow for domain knowledge,\n13 https://aif360.mybluemix.net 14 https://pair-code.github.io/what-if-tool/\nexpressed as rules, to be taken into account. Despite the similarities with this work, our proposal has 2 potential benefits: it doesn\u2019t assume that meaningful rules can be inferred, note that it may be impossible to infer rules from complex decisionmaking algorithms; and it hides to the user the complexities of the analysis process using the notion of norm and exception.\nImplicit Discrimination. Trame\u0300r et al. [20] developed a methodology and toolkit combining different metrics for discovering associations, or proxies in observational data. In particular, they studied different metrics that can be used to analyse the relationship between protected attributes and input attributes such as the Pearson correlation, which only works for scalar attributes linearly related; and Mutual Information, which can be applied to categorical attributes.\nIndirect Discrimination. Within this line of research, [22] surveys different metrics that have been proposed to measure indirect discrimination in data and the decisions made by algorithms. The study also discusses other traditional statistical measures that could be applied to measure discrimination. In particular, discrimination measures are classified by the authors into: statistical tests, which indicate the presence of discrimination at dataset level; absolute measures, which measure the magnitude of the discrimination present in a dataset; conditional measures, which capture the extent to which the differences between groups are due to protected attributes or other characteristics of individuals; and structural measures, which identify for each individual in the dataset if they are discriminated. In [9], the authors proposed quantitative metrics to determine the degree of influence of inputs on outputs of decision making systems. Their paper is not primarily intended to detect indirect discrimination, but the measures they propose have the potential to increase transparency of decisions made by opaque machine learning algorithms, which, in turn, may provide useful information for the detection of discrimination.\nIn addition to this work, there is work also focusing on ML fairness. For instance, in [10], they test for fairness based on a similarity measure between individuals. For fairness to hold, the distance between the distributions of outputs for individuals should at most be the distance between the two individuals as estimated by means of the similarity metric. In [13], the authors first gather human judgments about the different protected features in the context of two real-world scenarios using Amazon Mechanical Turk. Using the set of human-assessed protected features, they compare the accuracy of different classifiers to test the trade-off between process fairness and output accuracy. In [16], they assume fairness can be attested by means of a directed causal graph, in which attributes are presented as nodes joined by edges which, by means of equations, represent the relations between attributes. Finally, the set of violations presented in our approach could also be extended with recent advances in explainable AI. One example is the post-hoc approach of Local Interpretable Model-Agnostic Explanations (LIME), which makes use of adversarial learning to generate counterfactual explanations [17]."
    },
    {
      "heading": "7 Conclusion",
      "text": "Digital discrimination is becoming a significant problem as more decisions are delegated to ML systems. Indeed, recent legislation and citizen initiatives are demanding more transparency about the way in which decisions are made using their data. In response to that, several metrics and tools have been proposed to analyse biases in ML systems. However, these tools often require expert ML or statistical knowledge that many users of ML systems do not necessarily possess.\nIn this paper, we proposed to use normative notions as an abstraction that may be more easily understood by nontechnical users; simplifying the representation of the potential discrimination risks and the input of domain knowledge. Our digital discrimination attesting algorithm not only checks if ML systems are potentially discriminatory but also makes explicit under which assumptions these systems are discrimination free.\nAs future work, we plan to: i) investigate different metrics to be used in the attesting algorithm and to identify the most usable ones; ii) conduct user studies to further refine the way in which norms could be accessed and influenced by non-technical users to help them understand discrimination risks."
    },
    {
      "heading": "Acknowledgements",
      "text": "This work was supported by EPSRC under grant EP/R033188/1. It is part of the Discovering and Attesting Digital Discrimination (DADD) project \u2013 see https: //dadd-project.org."
    },
    {
      "heading": "A Compound Discrimination",
      "text": "Compound discrimination is discrimination based on a combination of protected attributes. In that case of compound discrimination the previous discrimination norms are rewritten as follows:\n\u2022 Direct. \u2013 Explicit. There is no need to change the definition of\nexplicit discrimination norms to account for compound discrimination, since the prohibition to include a set of protected attributes in the input can be represented by a set of explicit norms referring to each individual protected attribute.\n\u2013 Implicit. There is no need to change the definition of implicit discrimination norms to account for compound discrimination, since the prohibition to have a set of protected attributes as a function of input attributes can be represented by a set of implicit norms referring to each individual protected attribute.\n\u2022 Indirect (disparate impact). In this case the norms need to represent that for a particular combination of protected\nattribute values p1, ..., pk, where each pi \u2208 Pi; the probability of a given outcome o \u2208 Do is x times lower than for values of the same protected attributes with the highest probability:\n\u2200{P1, ..., Pk} \u2286 P, (p1, ..., pk) \u2208DP1 \u00d7 ...\u00d7DPk , o \u2208 DO :\nF({P1, ..., Pk} \u2193(p1,...,pk)o )\nwhere {P1, ..., Pk} \u2193(p1,...,pk)o denotes: Pr(O = o|P1 = p1, ..., Pk = pk) < x\u00d7max\n\u2200{(p\u20321,...,p \u2032 k )}\u2208DP1\u00d7...\u00d7DPk\nPr(O = o|P1 = p\u20321, ..., Pk = p\u2032k)"
    },
    {
      "heading": "B Discrimination in Classification Process",
      "text": "In this paper we have focused on digital discrimination; i.e., discriminatory acts facilitated by the automatic decisions made by a ML system. However, it is possible to consider the discrimination in the algorithm itself. In those cases it is necessary consider not only the outcome of the algorithm but also the ground-truth labels for the individuals, denoted by G. In those cases, it could be possible to formalise that for no particular value of a protected attribute the ML system can perform significantly worse than for the others groups\n\u2200Pi \u2208 P, p \u2208 DPi , g \u2208 DG : F(Pi \u2191 p g)\nwhere Pi \u2191pg represents:\nPr(O = g|Pi = p,G = g) < x\u00d7 max \u2200p\u2032\u2208DPi\nPr(O = g|Pi = p\u2032, G = g)\nPr(O = g|Pi = p,G = g) stands for probability that the algorithm outcome O is equal to the ground-truth label g for an individual with protected attribute Pi = p."
    }
  ],
  "title": "A Normative approach to Attest Digital Discrimination",
  "year": 2020
}
