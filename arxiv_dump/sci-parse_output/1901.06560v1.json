{"abstractText": "There is a disconnect between explanatory artificial intelligence (XAI) methods and the types of explanations that are useful for and demanded by society (policy makers, government officials, etc.) Questions that experts in artificial intelligence (AI) ask opaque systems provide inside explanations, focused on debugging, reliability, and validation. These are different from those that society will ask of these systems to build trust and confidence in their decisions. Although explanatory AI systems can answer many questions that experts desire, they often don\u2019t explain why they made decisions in a way that is precise (true to the model) and understandable to humans. These outside explanations can be used to build trust, comply with regulatory and policy changes, and act as external validation. In this paper, we focus on XAI methods for deep neural networks (DNNs) because of DNNs\u2019 use in decision-making and inherent opacity. We explore the types of questions that explanatory DNN systems can answer and discuss challenges in building explanatory systems that provide outside explanations for societal requirements and benefit.", "authors": [{"affiliations": [], "name": "Leilani H. Gilpin"}], "id": "SP:e2d3e414e11a1ef29d46c88d3e7f688b04bfeeb4", "references": [{"authors": ["Robert Andrews", "Joachim Diederich", "Alan B Tickle"], "title": "Survey and critique of techniques for extracting rules from trained artificial neural networks", "venue": "Knowledge-based systems,", "year": 1995}, {"authors": ["David Bau", "Bolei Zhou", "Aditya Khosla", "Aude Oliva", "Antonio Torralba"], "title": "Network dissection: Quantifying interpretability of deep visual representations", "venue": "In Computer Vision and Pattern Recognition,", "year": 2017}, {"authors": ["Mariusz Bojarski", "Davide Del Testa", "Daniel Dworakowski", "Bernhard Firner", "Beat Flepp", "Prasoon Goyal", "Lawrence D Jackel", "Mathew Monfort", "Urs Muller", "Jiakai Zhang"], "title": "End to end learning for self-driving cars", "venue": "arXiv preprint arXiv:1604.07316,", "year": 2016}, {"authors": ["Chaofan Chen", "Oscar Li", "Alina Barnett", "Jonathan Su", "Cynthia Rudin"], "title": "This looks like that: deep learning for interpretable image recognition", "venue": "arXiv preprint arXiv:1806.10574,", "year": 2018}, {"authors": ["Joan Claybrook", "Shaun Kildare"], "title": "Autonomous vehicles: No driver", "venue": "regulation? Science,", "year": 2018}, {"authors": ["Jeffrey Dastin"], "title": "Amazon scraps secret AI recruiting tool that showed bias against women", "year": 2018}, {"authors": ["Finale Doshi-Velez", "Been Kim"], "title": "Towards a rigorous science of interpretable machine learning", "venue": "arXiv preprint arXiv:1702.08608,", "year": 2017}, {"authors": ["Finale Doshi-Velez", "Mason Kortz", "Ryan Budish", "Chris Bavitz", "Sam Gershman", "David O\u2019Brien", "Stuart Schieber", "James Waldo", "David Weinberger", "Alexandra Wood"], "title": "Accountability of AI under the law: The role of explanation", "year": 2017}, {"authors": ["Jerome H Friedman"], "title": "Greedy function approximation: a gradient boosting machine", "venue": "Annals of statistics,", "year": 2001}, {"authors": ["Leilani H Gilpin", "David Bau", "Ben Z Yuan", "Ayesha Bajwa", "Michael Specter", "Lalana Kagal"], "title": "Explaining explanations: An approach to evaluating interpretability of machine learning", "venue": "arXiv preprint arXiv:1806.00069,", "year": 2018}, {"authors": ["Bryce Goodman", "Seth Flaxman"], "title": "European union regulations on algorithmic decisionmaking and a\" right to explanation", "venue": "arXiv preprint arXiv:1606.08813,", "year": 2016}, {"authors": ["David Gunning"], "title": "Explainable artificial intelligence (xai)", "venue": "Defense Advanced Research Projects Agency (DARPA), nd Web,", "year": 2017}, {"authors": ["Shachar Kaufman", "Saharon Rosset", "Claudia Perlich", "Ori Stitelman"], "title": "Leakage in data mining: Formulation, detection, and avoidance", "venue": "ACM Transactions on Knowledge Discovery from Data (TKDD),", "year": 2012}, {"authors": ["Been Kim", "Justin Gilmer", "Fernanda Viegas", "Ulfar Erlingsson", "Martin Wattenberg"], "title": "Tcav: Relative concept importance testing with linear concept activation vectors", "venue": "arXiv preprint arXiv:1711.11279,", "year": 2017}, {"authors": ["Been Kim", "Cynthia Rudin", "Julie A Shah"], "title": "The bayesian case model: A generative approach for case-based reasoning and prototype classification", "venue": "In Advances in Neural Information Processing Systems,", "year": 2014}, {"authors": ["Pang Wei Koh", "Percy Liang"], "title": "Understanding black-box predictions via influence functions", "venue": "arXiv preprint arXiv:1703.04730,", "year": 2017}, {"authors": ["Susan Landau"], "title": "Control use of data to protect", "venue": "privacy. Science,", "year": 2015}, {"authors": ["Timothy B. Lee"], "title": "Report: Software bug led to death in Uber\u2019s self-driving crash, May 2018", "year": 2018}, {"authors": ["Benjamin Letham", "Cynthia Rudin", "Tyler H McCormick", "David Madigan"], "title": "Interpretable classifiers using rules and bayesian analysis: Building a better stroke prediction model", "venue": "The Annals of Applied Statistics,", "year": 2015}, {"authors": ["Gilles Louppe", "Louis Wehenkel", "Antonio Sutera", "Pierre Geurts"], "title": "Understanding variable importances in forests of randomized trees", "venue": "In Advances in neural information processing systems,", "year": 2013}, {"authors": ["Aarian Marshall"], "title": "The Uber Crash Won\u2019t Be the Last Shocking Self-Driving Death", "year": 2018}, {"authors": ["Deirdre K Mulligan", "Colin Koopman", "Nick Doty"], "title": "Privacy is an essentially contested concept: a multi-dimensional analytic for mapping privacy", "venue": "Phil. Trans. R. Soc. A,", "year": 2016}, {"authors": ["Dong Huk Park", "Lisa Anne Hendricks", "Zeynep Akata", "Anna Rohrbach", "Bernt Schiele", "Trevor Darrell", "Marcus Rohrbach"], "title": "Multimodal explanations: Justifying decisions and pointing to the evidence", "year": 2018}, {"authors": ["Ali Sharif Razavian", "Hossein Azizpour", "Josephine Sullivan", "Stefan Carlsson"], "title": "Cnn features off-the-shelf: an astounding baseline for recognition", "venue": "In Computer Vision and Pattern Recognition Workshops (CVPRW),", "year": 2014}, {"authors": ["Cynthia Rudin"], "title": "Algorithms for interpretable machine learning", "venue": "Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "year": 2014}, {"authors": ["Sara Sabour", "Nicholas Frosst", "Geoffrey E Hinton"], "title": "Dynamic routing between capsules", "venue": "In Advances in Neural Information Processing Systems,", "year": 2017}, {"authors": ["Ramprasaath R Selvaraju", "Michael Cogswell", "Abhishek Das", "Ramakrishna Vedantam", "Devi Parikh", "Dhruv Batra"], "title": "Grad-cam: Visual explanations from deep networks via gradientbased localization", "venue": "See https://arxiv. org/abs/1610.02391 v3,", "year": 2016}, {"authors": ["Raymond Sheh", "Isaac Monteath"], "title": "Defining explainable ai for requirements analysis", "venue": "KI- Ku\u0308nstliche Intelligenz,", "year": 2018}, {"authors": ["Erik \u0160trumbelj", "Igor Kononenko"], "title": "Explaining prediction models and individual predictions with feature contributions", "venue": "Knowledge and information systems,", "year": 2014}, {"authors": ["Sarah Tan", "Rich Caruana", "Giles Hooker", "Yin Lou"], "title": "Detecting bias in black-box models using transparent model distillation", "venue": "arXiv preprint arXiv:1710.06169,", "year": 2017}, {"authors": ["David C Vladeck"], "title": "Machines without principals: liability rules and artificial intelligence", "venue": "Wash. L. Rev.,", "year": 2014}, {"authors": ["Jason Yosinski", "Jeff Clune", "Yoshua Bengio", "Hod Lipson"], "title": "How transferable are features in deep neural networks? In Advances in neural information processing", "year": 2014}, {"authors": ["Zhenlong Yuan", "Yongqiang Lu", "Zhaoguo Wang", "Yibo Xue"], "title": "Droid-sec: deep learning in android malware detection", "venue": "In ACM SIGCOMM Computer Communication Review,", "year": 2014}, {"authors": ["Xin Zhang", "Armando Solar-Lezama", "Rishabh Singh"], "title": "Interpreting neural network judgments via minimal, stable, and symbolic corrections", "venue": "CoRR, abs/1802.07384,", "year": 2018}, {"authors": ["Qingyuan Zhao", "Trevor Hastie"], "title": "Causal interpretations of black-box models", "year": 2017}, {"authors": ["Bolei Zhou", "Aditya Khosla", "Agata Lapedriza", "Aude Oliva", "Antonio Torralba"], "title": "Learning deep features for discriminative localization", "venue": "In Computer Vision and Pattern Recognition (CVPR),", "year": 2016}, {"authors": ["Jan Ruben Zilke", "Eneldo Loza Menc\u00eda", "Frederik Janssen"], "title": "Deepred\u2013rule extraction from deep neural networks", "venue": "In International Conference on Discovery Science,", "year": 2016}], "sections": [{"text": "ar X\niv :1\n90 1.\n06 56\n0v 1\n[ cs\n.A I]\n1 9\nJa n\n20 19"}, {"heading": "1 Introduction", "text": "There has been a recent surge of work in explanatory artificially intelligent (XAI) systems. One reason these systems are gaining traction is due to changes in policy, law, and regulation. Indeed, with the rise of AI-based decision making in areas of societal interest\u2013from finance and employment to driving and journalism\u2013policymakers see the need to discuss certain standards around XAI.\nFor example, the European Union\u2019s General Data Protection Regulation (GDPR) creates obligations for automatic decision making processes [11], with a provision including right to explanation. This broad obligation puts the burden on those who process data with (and develop for) AI systems to generate reasonable explanations for their systems\u2019 decision making processes. We can also observe broad societal concerns with issues of AI liability [33]. For example, as autonomous vehicles are being introduced, we need a better understanding of what happened in the case of an accident (as it has already happened [5]). How can there be an appropriate investigation process if opaque decisionmaking algorithms are involved? How can we ensure that these machines are acting in our best interest?\nAI algorithms and more general, complex AI systems cannot currently provide answers to these prior questions. These algorithms and systems are not built to explain to the general public nor policy-makers. Although there have been calls for work on creating systems and algorithms that can interpret [7, 27] and explain [12] some parts of their decisions, the current state-of-the-art explanatory systems are made for the programmer or expert, not an end user or policy-maker. The key difference here is that the current systems produce what we refer to as inside explanations. They point to a plausible technical explanation, either by looking at the relationships between the inputs and outputs of a model, or examining the role of individual parts, or by producing a surface-level explanation itself. Crucially, though, these explanations do not answer why questions. Continuing with the autonomous vehicle example, when an accident happens involving this autonomous ma-\n32nd Conference on Neural Information Processing Systems (NIPS 2018), Montr\u00e9al, Canada.\nchine, police officials, insurance companies, and the people who are harmed will want to know who or what is accountable for the accident and why it happened.\nIn this paper, we examine the types of questions that explanatory DNN algorithms can and cannot answer. We focus on DNNs specifically because of the recent shift in AI research from symbolic approaches to machine learning and deep learning 1, and because these are the systems are making safety-critical decisions in applications like autonomous driving[3] and malware detection[35]. In order to bridge the gap between the current, technical deep neural network explanations (which we refer to as inside explanations) and the explanations that answer why questions that would benefit society (which we refer to as outside explanations) we must develop explanations for DNNs that can answer these questions and be probed. We extend the work of a previously defined [10] taxonomy of explanations by looking at the specific questions each class can and cannot answer, and stress the necessity and technical challenges for these systems to be probed and answer why questions. We motivate future work on bridging the gap between current explainable methods by incorporating the types of questions and explanations society would like to know.\nBy doing this, we attempt to \u201cbridge the gap\u201d between current, technical deep neural network explanations and explanations that answer why questions beneficial to society. We approach this task in four main ways:\n\u2022 We differentiate between inside (technical) and outside (why) explanations. \u2022 We extend the work of a previously defined [10] taxonomy of explanations by looking at\nthe specific questions each class can and cannot answer. \u2022 We discuss the necessity of probing of AI systems and the technical challenges inherent in\ncreating outside explanations. \u2022 We motivate future work on bridging the gap between current explainable methods by\nincorporating the types of questions and explanations society would like to know."}, {"heading": "2 Related Work", "text": "In this work, we focus on the types of questions and explanations that explanatory DNN methods can answer. Recent work has looked at ways to correct neural network judgments [36] and different ways to audit such networks by detecting biases [32]. But these judgments are not enough to completely understand the model\u2019s decisions-making. Other work answers why questions by finding similar data points [4]. Although these methods are clearly interpretable, they do not provide any unique insights into why the model made those decisions. Other work examining best practices for explanation [30] provides a set of categories, but does not evaluate the questions that explanatory systems should be able to answer; which is necessary for policy makers and societal trust in DNN decision processes.\nSince we are interested in the societal expectation of explanations, it is important to examine prior initiatives on the legal side. The desire for explanations in certain sectors is not new. For example, the U.S. Fair Credit Reporting Act creates obligations for transparency in certain financial decisionmaking processes, even if they are automated [17]. The role of explanation has been examined to enforce accountability under the law [8]. Similar recommendations in using explanations in law have been examined in promoting ethics for design [23], for privacy [24], and liability for machines [33].\nIn this paper, we explicitly examine DNNs, even though there have been important developments in explanatory systems not tailored to DNNs, such as randomized importance [20], rule lists [19], partial dependence plots [9, 22, 37], Shapely scores [22, 31], and Bayesian case based models [15]."}, {"heading": "3 Definitions", "text": "We follow from previous work [10] that a proper explanation should be both interpretable and complete. By intepretable, we mean that the explanation should be understandable to humans. That does not necessarily imply that the explanation must be in human-readable form, in fact, visual cues are well-understood by humans. When we say that the methods must be complete, we mean the resulting explanation should be true to the model. For example, while using a simplified model that\n1http://www.aiindex.org/2017-report.pdf\nis explainable (like a linear model) to fit the input to the output results in a nice explanation, it is not a true and complete representation of the internal concepts, representations, and decisions of the model.\nIn this paper, we refer to inside and outside explanations for explaining DNNs. When we refer to inside explanations, we are referring to the type of explanations that currently exist, that are catered towards AI developers and experts. They are tailored to people inside the field. We encourage the development of outside explanations that are interpretable, complete, and answer why questions. They build trust not only to their technical developers, but also those outside the technical scope that may use their technology without a technical background."}, {"heading": "4 Current Limitations", "text": "To show the strengths, benefits, and challenges of current explanatory approaches for opaque, DNN systems, we use a previously defined taxonomy [10] . The taxonomy consists of 3 classes. The first class are systems that explain processing by looking at the relationships between the inputs and the outputs. These include salience mapping [29, 38], decision trees [39], automatic rule-extraction [1], and influence functions [16]. The second class are systems that explain representation for DNNs either in terms of layers [26, 34], neurons [2] or vectors [14]. The final class is explanation-producing systems that look at attention-based visual question answers [25] or disentangled representations [28] to create self-explaining systems.\nWhen examining this taxonomy for policy purposes, the biggest shortcoming is that these systems cannot explain why. There are two types of questions that we should ask of a decision making algorithm:\n1. Why did this output happen? 2. How could this output have changed?\nA summary of the types of questions that current DNN XAI systems can and cannot answer are in Table 1. Explanation producing systems nearly answer the first question we would want to ask a decision making algorithm: why did this output happen? But the problem is that their explanation may not be complete and true to the model\u2019s internal decisions and processes. In order to illustrate the necessity of answering these questions, we proceed by walking-through examples of an AI algorithm, a larger AI system, and illustrate problems with data to motivate why explanatory systems should strive to answer the preceding questions."}, {"heading": "4.1 Societal needs for explanations", "text": "Imagine you do not receive a loan, you would want to know what was the key attribute that limited the algorithm. You would want to know why you were denied a loan. But further, you may also want a sensitivity analysis: what would you need to change to be able to get the loan. There may be several possibilities. For example, you may have received a loan if you made $1,000 more per month; something you may be able to change in the future. However, other factors may be things you cannot control, such as the specific time you applied or your gender or ethnicity. So, in this\ncase, we would like to have system that is able to explain why it decided to give or not a loan to each person.\nMoreover, consider again the AI system example mentioned earlier of a self-driving car involved in an accident. The first thing we would want to know is why the accident happened. In this case there are many algorithms interacting. Finding if there was a faulty component is extremely challenging, making it even more relevant for each part of the system to be able to explain its decisions. In the recent Uber accident where the vehicle struck and killed a pedestrian, detecting the root-cause of the accident took several weeks to uncover in the complex AI software system [21].\nBut the other, more challenging question we would want to ask is if the accident could have been avoided. This is a more difficult question than the previous, single algorithm question. In complex systems, an error could be local (caused by a single failure), or it could be caused by an inconsistency between parts working together. The latter is much more difficult to detect, diagnosis, and explain.\nIn the Uber case, since the accident was deemed to be caused by a false positive[18] on the error detection monitoring the pedestrian, several explanations could provide evidence of how this could have been avoided. Again, some inconsistencies are easier to fix than others (which may not be possible). Perhaps the sensitivity on the error detection monitor should be decreased or increased. Or perhaps the pedestrian would have been detected with higher certainty during the daytime, or if they were walking slower. It is still left to question whether the training data was at fault, which introduces a new set of questions."}, {"heading": "4.2 The risk of opaque models", "text": "Generally explaining model behavior is not enough to build trust in these sorts of models. Another way these algorithms and systems can behave badly is due to a inconsistency in the training data and/or knowledge bases. This does not necessarily mean that the training data is \u201cbad\u201d per say, but that there is a misalignment between the expected data and the actual training data used. We have seen this recently with the Amazon recruiting algorithm [6]. This algorithm was eventually disbanded because the results were extremely biased; since the algorithm had been trained on applicants data for the past 10 years (where males are dominant), it was teaching itself to choose male candidates. Even if the algorithm was modified, there was no way to ensure it was unbiased. Although this is an extremely compelling case for inquisitive explanatory systems, an even more persuasive case is for safety-critical tasks.\nEqually important, consider a machine learning classifier to diagnose breast cancer from an image, where the training set was carefully selected to be fairly close to a 50-50 split of breast cancer and non-breast cancer scans. Even if the classifier is very accurate, without having access to complete explanations to understand how decisions are made in the model, it is not certain that it is making decisions for the right reasons\u2014the model may in fact, learn a feature it should not rely on despite predicting breast cancer very accurately. In [13], one classifier learned the resolution of the scanner camera, therefore predicting cancerous images from a high resolution very accurately. Figuring out this sort of data problem is extremely difficult. It requires either an attuned intuition of the model\u2019s inner workings or the model to be able to answer questions to do a fine-grain sensitivity analysis."}, {"heading": "5 Discussion and Conclusion", "text": "As humans, we start to build trust by asking questions. We should be able to judge the behavior of opaque DNN algorithms by asking similar questions as we would ask of a person\u2019s behavior in similar circumstances. The key idea here is that explainability exceeds transparency and interpretability, to empower the public to understand the decisions and underlying mechanisms.\nWe have focused mainly on explainable DNN algorithms, but when a DNN algorithm is a part of a larger system, explainability is not enough. Explainability does not necessary imply that complex systems are accountable and responsible; this may have to be tackled with other requirements. For example, a system can provide an \u201coutside\u201d explanation without addressing who or what is responsible and why. At the same time, an explainable system may be transparent without being receptive to human feedback. In future work, we will examine how explainability may interact with other parts of systems (including the human operator or user) to produce systems that can be augmented and learn from feedback.\nBut in order to truly trust AI systems, people will not only need to feel confident that they understand certain how decisions are made, but also that they have recourse. If a person disagrees with a system\u2019s output, they should be empowered and able to change the system. Designing explainable AI is important, but only when opaque systems are auditable, explainable, answer questions, and interpret feedback will we be confident enough to trust their decision-making."}], "title": "Explaining Explanations to Society", "year": 2019}