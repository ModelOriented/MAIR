{"abstractText": "Building explainable systems is a critical problem in the field of Natural Language Processing (NLP), since most machine learning models provide no explanations for the predictions. Existing approaches for explainable machine learning systems tend to focus on interpreting the outputs or the connections between inputs and outputs. However, the fine-grained information (e.g. textual explanations for the labels) is often ignored, and the systems do not explicitly generate the human-readable explanations. To solve this problem, we propose a novel generative explanation framework that learns to make classification decisions and generate fine-grained explanations at the same time. More specifically, we introduce the explainable factor and the minimum risk training approach that learn to generate more reasonable explanations. We construct two new datasets that contain summaries, rating scores, and fine-grained reasons. We conduct experiments on both datasets, comparing with several strong neural network baseline systems. Experimental results show that our method surpasses all baselines on both datasets, and is able to generate concise explanations at the same time.", "authors": [{"affiliations": [], "name": "Hui Liu"}, {"affiliations": [], "name": "Qingyu Yin"}, {"affiliations": [], "name": "William Yang Wang"}], "id": "SP:65e28ca76feafcdb55ac96030032b64b8adb80c6", "references": [{"authors": ["Shiqi Shen Ayana", "Zhiyuan Liu", "Maosong Sun."], "title": "Neural headline generation with minimum risk training", "venue": "arXiv preprint arXiv:1604.01904.", "year": 2016}, {"authors": ["Oana-Maria Camburu", "Tim Rockt\u00e4schel", "Thomas Lukasiewicz", "Phil Blunsom."], "title": "e-snli: Natural language inference with natural language explanations", "venue": "S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, ed-", "year": 2018}, {"authors": ["Bjarke Felbo", "Alan Mislove", "Anders S\u00f8gaard", "Iyad Rahwan", "Sune Lehmann."], "title": "Using millions of emoji occurrences to learn any-domain representations for detecting sentiment, emotion and sarcasm", "venue": "Proceedings of the 2017 Conference on", "year": 2017}, {"authors": ["J Gu", "Z Lu", "H Li", "VOK Li."], "title": "Incorporating copying mechanism in sequence-to-sequence learning", "venue": "Annual Meeting of the Association for Computational Linguistics (ACL), 2016. Association for Computational Linguistics.", "year": 2016}, {"authors": ["David Gunning."], "title": "Explainable artificial intelligence (xai)", "venue": "Defense Advanced Research Projects Agency (DARPA), nd Web.", "year": 2017}, {"authors": ["Braden Hancock", "Paroma Varma", "Stephanie Wang", "Martin Bringmann", "Percy Liang", "Christopher R\u00e9."], "title": "Training classifiers with natural language explanations", "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational", "year": 2018}, {"authors": ["Yoon Kim."], "title": "Convolutional neural networks for sentence classification", "venue": "EMNLP.", "year": 2014}, {"authors": ["Diederik P Kingma", "Jimmy Lei Ba."], "title": "Adam: A method for stochastic optimization", "venue": "Proc. 3rd Int. Conf. Learn. Representations.", "year": 2014}, {"authors": ["Siwei Lai", "Liheng Xu", "Kang Liu", "Jun Zhao."], "title": "Recurrent convolutional neural networks for text classification", "venue": "AAAI, volume 333, pages 2267\u2013 2273.", "year": 2015}, {"authors": ["Tao Lei", "Regina Barzilay", "Tommi Jaakkola."], "title": "Rationalizing neural predictions", "venue": "EMNLP.", "year": 2016}, {"authors": ["Pengfei Liu", "Xipeng Qiu", "Xuanjing Huang."], "title": "Recurrent neural network for text classification with multi-task learning", "venue": "Proceedings of International Joint Conference on Artificial Intelligence.", "year": 2016}, {"authors": ["Xuezhe Ma", "Zecong Hu", "Jingzhou Liu", "Nanyun Peng", "Graham Neubig", "Eduard Hovy."], "title": "Stackpointer networks for dependency parsing", "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long", "year": 2018}, {"authors": ["Christopher Manning", "Mihai Surdeanu", "John Bauer", "Jenny Finkel", "Steven Bethard", "David McClosky."], "title": "The stanford corenlp natural language processing toolkit", "venue": "Proceedings of 52nd annual meeting of the association for computational lin-", "year": 2014}, {"authors": ["Sixun Ouyang", "Aonghus Lawlor", "Felipe Costa", "Peter Dolog."], "title": "Improving explainable recommendations with synthetic reviews", "venue": "arXiv preprint arXiv:1807.06978.", "year": 2018}, {"authors": ["Kishore Papineni", "Salim Roukos", "Todd Ward", "WeiJing Zhu."], "title": "Bleu: a method for automatic evaluation of machine translation", "venue": "Proceedings of the 40th annual meeting on association for computational linguistics, pages 311\u2013318. Association for", "year": 2002}, {"authors": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning."], "title": "Glove: Global vectors for word representation", "venue": "Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532\u20131543.", "year": 2014}, {"authors": ["Matthew E. Peters", "Mark Neumann", "Mohit Iyyer", "Matt Gardner", "Christopher Clark", "Kenton Lee", "Luke Zettlemoyer."], "title": "Deep contextualized word representations", "venue": "Proc. of NAACL.", "year": 2018}, {"authors": ["Marco Tulio Ribeiro", "Sameer Singh", "Carlos Guestrin."], "title": "Why should i trust you?: Explaining the predictions of any classifier", "venue": "Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining,", "year": 2016}, {"authors": ["Wojciech Samek", "Thomas Wiegand", "Klaus-Robert M\u00fcller."], "title": "Explainable artificial intelligence: Understanding, visualizing and interpreting deep learning models", "venue": "ITU Journal: ICT Discoveries - Special Issue 1 - The Impact of Artificial Intel-", "year": 2018}, {"authors": ["Kihyuk Sohn", "Honglak Lee", "Xinchen Yan."], "title": "Learning structured output representation using deep conditional generative models", "venue": "Advances in Neural Information Processing Systems, pages 3483\u20133491.", "year": 2015}, {"authors": ["Duyu Tang", "Bing Qin", "Ting Liu."], "title": "Document modeling with gated recurrent neural network for sentiment classification", "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1422\u20131432, Lisbon, Portu-", "year": 2015}, {"authors": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin."], "title": "Attention is all you need", "venue": "Advances in Neural Information Processing Systems, pages 5998\u20136008.", "year": 2017}, {"authors": ["Wei Wang", "Ming Yan", "Chen Wu."], "title": "Multigranularity hierarchical attention fusion networks for reading comprehension and question answering", "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume", "year": 2018}, {"authors": ["Qingyu Yin", "Yu Zhang", "Weinan Zhang", "Ting Liu", "William Yang Wang."], "title": "Deep reinforcement learning for chinese zero pronoun resolution", "venue": "ACL.", "year": 2018}, {"authors": ["Xiang Zhang", "Junbo Zhao", "Yann LeCun."], "title": "Character-level convolutional networks for text classification", "venue": "Advances in neural information processing systems, pages 649\u2013657.", "year": 2015}, {"authors": ["Xianda Zhou", "William Yang Wang."], "title": "Mojitalk: Generating emotional responses at scale", "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, Melbourne, Victoria, Australia. ACL.", "year": 2018}], "sections": [{"heading": "1 Introduction", "text": "Deep learning methods have produced state-ofthe-art results in many natural language processing (NLP) tasks (Vaswani et al., 2017; Yin et al., 2018; Peters et al., 2018; Wang et al., 2018; Hancock et al., 2018; Ma et al., 2018). Though these deep neural network models achieve impressive performance, it is relatively difficult to convince people to trust the predictions of such neural networks since they are actually black boxes for human beings (Samek et al., 2018). For instance, if an essay scoring system only tells the scores of a given essay without providing explicit reasons, the users can hardly be convinced of the\njudgment. Therefore, the ability to explain the rationale is essential for a NLP system, a need which requires traditional NLP models to provide human-readable explanations.\nIn recent years, lots of works have been done to solve text classification problems, but just a few of them have explored the explainability of their systems (Camburu et al., 2018; Ouyang et al., 2018). Ribeiro et al. (2016) try to identify an interpretable model over the interpretable representation that is locally faithful to the classifier. Samek et al. (2018) use heatmap to visualize how much each hidden element contributes to the predicted results. Although these systems are somewhat promising, they typically do not consider finegrained information that may contain information for interpreting the behavior of models. However, if a human being wants to rate a product, s/he may first write down some reviews, and then score or summarize some attributes of the product, like price, packaging, and quality. Finally, the overall rating for the product will be given based on the fine-grained information. Therefore, it is crucial to build trustworthy explainable text classification models that are capable of explicitly generating fine-grained information for explaining their predictions.\nTo achieve these goals, in this paper, we propose a novel generative explanation framework for text classification, where our model is capable of not only providing the classification predictions but also generating fine-grained information as explanations for decisions. The novel idea behind our hybrid generative-discriminative method is to explicitly capture the fine-grained information inferred from raw texts, utilizing the information to help interpret the predicted classification results and improve the overall performance. Specifically, we introduce the notion of an explainable factor and a minimum risk training method that learn to\nar X\niv :1\n81 1.\n00 19\n6v 2\n[ cs\n.C L\n] 1\n1 Ju\nn 20\n19\ngenerate reasonable explanations for the overall predict results. Meanwhile, such a strategy brings strong connections between the explanations and predictions, which in return leads to better performance. To the best of our knowledge, we are the first to explicitly explain the predicted results by utilizing the abstractive generative fine-grained information.\nIn this work, we regard the summaries (texts) and rating scores (numbers) as the fine-grained information. Two datasets that contain these kinds of fine-grained information are collected to evaluate our method. More specifically, we construct a dataset crawled from a website called PCMag1. Each item in this dataset consists of three parts: a long review text for one product, three short text comments (respectively explains the property of the product from positive, negative and neutral perspectives) and an overall rating score. We regard the three short comments as fine-grained information for the long review text. Besides, we also conduct experiments on the Skytrax User Reviews Dataset2, where each case consists of three parts: a review text for a flight, five sub-field rating scores (seat comfortability, cabin stuff, food, in-flight environment, ticket value) and an overall rating score. As for this dataset, we regard the five sub-field rating scores as fine-grained information for the flight review text.\nEmpirically, we evaluate our model-agnostic method on several neural network baseline models (Kim, 2014; Liu et al., 2016; Zhou and Wang, 2018) for both datasets. Experimental results suggest that our approach substantially improves the performance over baseline systems, illustrating the advantage of utilizing fine-grained information. Meanwhile, by providing the fine-grained information as explanations for the classification results, our model is an understandable system that is worth trusting. Our major contributions are three-fold:\n\u2022 We are the first to leverage the generated finegrained information for building a generative explanation framework for text classification, propose an explanation factor, and introduce minimum risk training for this hybrid generative-discriminative framework;\n\u2022 We evaluate our model-agnostic explanation 1https://www.pcmag.com/ 2https://github.com/quankiquanki/\nskytrax-reviews-dataset\nframework with different neural network architectures, and show considerable improvements over baseline systems on two datasets;\n\u2022 We provide two new publicly available explainable NLP datasets that contain finegrained information as explanations for text classification."}, {"heading": "2 Task Definition and Notations", "text": "The research problem investigated in this paper is defined as: How can we generate fine-grained explanations for the decisions our classification model makes? To answer this question, we may first investigate what are good fine-grained explanations. For example, in sentiment analysis, if a product A has three attributes: i.e., quality, practicality, and price. Each attribute can be described as \u201cHIGH\u201d or \u201cLOW\u201d. And we want to know whether A is a \u201cGOOD\u201d or \u201cBAD\u201d product. If our model categorizes A as \u201cGOOD\u201d and it tells that the quality of A is \u201cHIGH\u201d, the practicality is \u201cHIGH\u201d and the price is \u201cLOW\u201d, we can regard these values of attributes as good explanations that illustrate why the model judges A to be \u201cGOOD\u201d. On the contrary, if our model produces the same values for the attributes, but it tells that A is a \u201cBAD\u201d product, we then think the model gives bad explanations. Therefore, for a given classification prediction made by the model, we would like to explore more on the fine-grained information that can explain why it comes to such a decision for the current example. Meanwhile, we also want to figure out whether the fine-grained information inferred from the input texts can help improve the overall classification performance.\nWe denote the input sequence of texts to be S{s1, s2, . . . , s|S|}, and we want to predict which category yi(i \u2208 [1, 2, . . . , N ]) the sequence S belongs to. At the same time, the model can also produce generative fine-grained explanations ec for yi."}, {"heading": "3 Generative Explanation Framework", "text": "In this part, we introduce our proposed Generative Explanation Framework (GEF). Figure 1 illustrates the architecture of our model."}, {"heading": "3.1 Base Classifier and Generator", "text": "A common way to do text classification tasks is using an Encoder-Predictor architecture (Zhang\net al., 2015; Lai et al., 2015). As shown in Figure 1, a text encoder E takes the input text sequence S, and encodes S into a representation vector ve. A category predictor P then gets ve as input and outputs the category yi and its corresponding probability distribution Ppred.\nAs mentioned above, a desirable model should not only predict the overall results yi, but also provide generative explanations to illustrate why it makes such predictions. A simple way to generate explanations is to feed ve to an explanation generator G to generate fine-grained explanations ec. This procedure is formulated as:\nve = Encoder([s1, s2, \u00b7 \u00b7 \u00b7 , s|S|]) (1) Ppred = Predictor(ve) (2)\ny = arg max i (Ppred,i) (3) ec = fG(WG \u00b7 ve + bG) (4)\nwhere Encoder maps the input sequence [s1, s2, \u00b7 \u00b7 \u00b7 , s|S|] into the representation vector ve;\nthe Predictor takes the ve as input and outputs the probability distribution over classification categories by using the softmax.\nDuring the training process, the overall loss L is composed of two parts, i.e., the classification loss Lp and explanation generation loss Le:\nL(eg, S, \u03b8) = Lp + Le (5)\nwhere \u03b8 represents all the parameters."}, {"heading": "3.2 Explanation Factor", "text": "The simple supervised way to generate explanations, as demonstrated in the previous subsection, is quite straightforward. However, there is a significant shortcoming of this generating process: it fails to build strong connections between the generative explanations and the predicted overall results. In other words, the generative explanations seem to be independent of the predicted overall results. Therefore, in order to generate more reasonable explanations for the results, we propose to use an explanation factor to help build stronger connections between the explanations and predictions.\nAs we have demonstrated in the introduction section, fine-grained information will sometimes reflect the overall results more intuitively than the original input text sequence. For example, given a review sentence, \u201cThe product is good to use\u201d, we may not be sure if the product should be rated as 5 stars or 4 stars. However, if we see that the attributes of the given product are all rated as 5 stars, we may be more convinced that the overall rating for the product should be 5 stars.\nSo in the first place, we pre-train a classifier C, which also learns to predict the category y by directly taking the explanations as input. More specifically, the goal of C is to imitate human beings\u2019 behavior, which means that C should predict the overall results more accurately than the base model that takes the original text as the input. We prove this assumption in the experiments section.\nWe then use the pre-trained classifier C to help provide a strong guidance for the text encoder E, making it capable of generating a more informative representation vector ve. During the training process, we first get the generative explanations ec by utilizing the explanation generator G. We then feed this generative explanations ec to the classifier C to get the probability distribution of the predicted results Pclassified. Meanwhile, we can\nalso get the golden probability distribution Pgold by feeding the golden explanations eg to C. The process can be formulated as:\nPclassified = softmax(fC(WC \u00b7 ec + bC)) (6) Pgold = softmax(fC(WC \u00b7 eg + bC)) (7)\nIn order to measure the distance among predicted results, generated explanations and golden generations, we extract the ground-truth probability p\u0303classified, p\u0303pred, p\u0303gold from Pclassified, Ppred, Pgold respectively. They will be used to measure the discrepancy between the predicted result and ground-truth result in minimum risk training.\nWe define our explanation factor EF (S) as:\nEF (S) = |p\u0303classified \u2212 p\u0303gold|+ |p\u0303classified \u2212 p\u0303pred| (8)\nThere are two components in this formula.\n\u2022 The first part |p\u0303classified \u2212 p\u0303gold| represents the distance between the generated explanations ec and the golden explanations eg. Since we pre-train C using golden explanations, we hold the view that if similar explanations are fed to C, similar predictions should be generated. For instance, if we feed a golden explanation \u201cGreat performance\u201d to the classifier C and it tells that this explanation means \u201ca good product\u201d, then we feed another explanation \u201cExcellent performance\u201d to C, it should also tell that the explanation means \u201ca good product\u201d. For this task, we hope that ec can express the same or similar meaning as eg.\n\u2022 The second part |p\u0303classified \u2212 p\u0303pred| represents the relevance between the generated explanations ec and the original texts S. The generated explanations should be able to interpret the overall result. For example, if the base model predicts S to be \u201ca good product\u201d, but the classifier tends to classify ec to be the explanations for \u201ca bad product\u201d, then ec cannot properly explain the reason why the base model gives such predictions."}, {"heading": "3.3 Minimum Risk Training", "text": "In order to remove the disconnection between finegrained information and input text, we use Minimum risk training (MRT) to optimize our models, which aims to minimize the expected loss, i.e., risk\nover the training data (Ayana et al., 2016). Given a sequence S and golden explanations eg, we define Y(eg, S, \u03b8) as the set of predicted overall results with parameter \u03b8. We define \u2206(y, y\u0303) as the semantic distance between predicted overall results y and ground-truth y\u0303. Then, the objective function is defined as:\nLMRT (eg, S, \u03b8) = \u2211\n(eg ,S)\u2208D\nEY(eg ,S,\u03b8)\u2206(y, y\u0303)\n(9)\nwhere D presents the whole training dataset. In our experiment, EY(eg ,S,\u03b8) is the expectation over the set Y(eg, S, \u03b8), which is the overall loss in Equation 5. And we define Explanation Factor EF (S) as the semantic distance of input texts, generated explanations and golden explanations. Therefore, the objective function of MRT can be further formalized as:\nLMRT (eg, S, \u03b8) = \u2211\n(eg ,S)\u2208D\nL(eg, S, \u03b8)EF (S)\n(10)\nMRT exploits EF (S) to measure the loss, which learns to optimize GEF with respect to the specific evaluation metrics of the task. Though LMRT can be 0 or close to 0 when p\u0303classified, p\u0303pred and p\u0303gold are close, this cannot guarantee that generated explanations are close to the golden explanations. In order to avoid the total degradation of loss, we define our final loss function as the sum of MRT loss and explanation generation loss:\nLfinal = \u2211\n(eg ,S)\u2208D\nL+ LMRT (11)\nWe try different weighting scheme for the overall loss, and get best performance with 1 :1."}, {"heading": "3.4 Application Case", "text": "Generally, the fine-grained explanations are in different forms for a real-world dataset, which means that ec can be in the form of texts or in the form of numerical scores. We apply GEF to both forms of explanations using different base models."}, {"heading": "3.4.1 Case 1: Text Explanations", "text": "To test the performance of GEF on generating text explanations, we apply GEF to Conditional Variational Autoencoder (CVAE) (Sohn et al., 2015). We here utilize CVAE because we want to generate explanations conditioned on different emotions (positive, negative and neural) and CVAE\nis found to be capable of generating emotional texts and capturing greater diversity than traditional SEQ2SEQ models.\nWe give an example of the structure of CVAE+GEF in Figure 2. For space consideration, we leave out the detailed structure of CVAE, and will elaborate it in the supplementary materials. In this architecture, golden explanations eg and generated explanations ec are both composed of three text comments: positive comments, negative comments, and neutral comments, which are finegrained explanations for the final overall rating. The classifier is a skip-connected model of bidirectional GRU-RNN layers (Felbo et al., 2017). It takes three kinds of comments as inputs, and outputs the probability distribution over the predicted classifications."}, {"heading": "3.4.2 Case 2: Numerical Explanations", "text": "Another frequently employed form of the finegrained explanations for the overall results is numerical scores. For example, when a user wants to rate a product, s/he may first rate some attributes of the product, like the packaging, price, etc. After rating all the attributes, s/he will give an overall rating for the product. So we can say that the rating for the attributes can somewhat explain\nwhy the user gives the overall rating. LSTM and CNN are shown to achieve great performance in text classification tasks (Tang et al., 2015), so we use LSTM and CNN models as the encoder E respectively. The numerical explanations are also regarded as a classification problem in this example."}, {"heading": "4 Dataset", "text": "We conduct experiments on two datasets where we use texts and numerical ratings to represent finegrained information respectively. The first one is crawled from a website called PCMag, and the other one is the Skytrax User Reviews Dataset. Note that all the texts in the two datasets are preprocessed by the Stanford Tokenizer3 (Manning et al., 2014)."}, {"heading": "4.1 PCMag Review Dataset", "text": "This dataset is crawled from the website PCMag. It is a website providing reviews for electronic products, like laptops, smartphones, cameras and so on. Each item in the dataset consists of three parts: a long review text, three short comments, and an overall rating score for the product. Three short comments are summaries of the long review respectively from positive, negative, neutral perspectives. An overall rating score is a number ranging from 0 to 5, and the possible values that the score could be are {1.0, 1.5, 2.0, ..., 5.0}.\nSince long text generation is not what we focus on, the items where review text contains more than 70 sentences or comments contain greater than 75 tokens are filtered. We randomly split the dataset into 10919/1373/1356 pairs for train/dev/test set. The distribution of the overall rating scores within this corpus is shown in Table 1."}, {"heading": "4.2 Skytrax User Reviews Dataset", "text": "We incorporate an airline review dataset scraped from Skytraxs Web portal. Each item in this dataset consists of three parts: i.e., a review text, five sub-field scores and an overall rating score. The five sub-field scores respectively stand for the user\u2019s ratings for seat comfortability, cabin stuff, food, in-flight environment, and ticket value, and each score is an integer between 0 and 5. The overall score is an integer between 1 and 10.\nSimilar to the PCMag Review Dataset, we filter out the items where the review contains more than\n3https://nlp.stanford.edu/software/ tokenizer.html\n300 tokens. Then we randomly split the dataset into 21676/2710/2709 pairs for train/dev/test set. The distribution of the overall rating scores within this corpus is shown in Table 2."}, {"heading": "5 Experiments and Analysis", "text": ""}, {"heading": "5.1 Experimental Settings", "text": "As the goal of this study is to propose an explanation framework, in order to test the effectiveness of proposed GEF, we use the same experimental settings on the base model and on the base model+GEF. We use GloVe (Pennington et al., 2014) word embedding for PCMag dataset and minimize the objective function using Adam (Kingma and Ba, 2014). The hyperparameter settings for both datasets are listed in Table 3. Meanwhile, since the generation loss is larger than classification loss for text explanations, we stop updating the predictor after classification loss reaches a certain threshold (adjusted based on dev set) to avoid overfitting."}, {"heading": "5.2 Experimental Results", "text": ""}, {"heading": "5.2.1 Results of Text Explanations", "text": "We use BLEU (Papineni et al., 2002) scores to evaluation the quality of generated text explanations. Table 4 shows the comparison results of explanations generated by CVAE and CVAE+GEF.\nThere are considerable improvements on the BLEU scores of explanations generated by CVAE+GEF over the explanations generated by CVAE, which demonstrates that the explanations generated by CVAE+GEF are of higher quality.\nCVAE+GEF can generate explanations that are closer to the overall results, thus can better illustrate why our model makes such a decision.\nIn our opinion, the generated fine-grained explanations should provide the extra guidance to the classification task, so we also compare the performance of classification on CVAE and CVAE+GEF. We use top-1 accuracy and top-3 accuracy as the evaluation metrics for the performance of classification. In Table 5, we compare the results of CVAE+GEF with CVAE in both test and dev set. As shown in the table, CVAE+GEF has better classification results than CVAE, which indicates that the fine-grained information can really help enhance the overall classification results.\nAs aforementioned, we have an assumption that if we use fine-grained explanations for classifica-\ntion, we shall get better results than using the original input texts. Therefore, we list the performance of the classifier C in Table 5 to make the comparison. Experiments show that C has better performance than both CVAE and CVAE+GEF, which proves our assumption to be reasonable."}, {"heading": "5.2.2 Results of Numerical Explanations", "text": "In the Skytrax User Reviews Dataset, the overall ratings are integers between 1 to 10, and the five sub-field ratings are integers between 0 and 5. All of them can be treated as classification problems, so we use accuracy to evaluate the performance.\nThe accuracy of predicting the sub-field ratings can indicate the quality of generated numerical explanations. In order to prove that GEF can help generate better explanations, we show the accuracy of the sub-field rating classification in Table 6. The 5 ratings evaluate the seat comfortability, cabin stuff, food, in-flight environment, and ticket value, respectively. As we can see from the results in Table 6, the accuracy for 5 sub-field ratings all get enhanced comparing with the baseline. Therefore, we can tell that GEF can improve the quality of generated numerical explanations.\nThen we compare the result for classification in Table 7. As the table shows, the accuracy or top-3 accuracy both get improved when the models are combined with GEF.\nMoreover, the performances of the classifier are better than LSTM (+GEF) and CNN (+GEF), which further confirms our assumption that the classifier C can imitate the conceptual habits of human beings. Leveraging the explanations can provide guidance for the model when doing final results prediction."}, {"heading": "5.3 Human Evaluation", "text": "In order to prove our model-agnostic framework can make the basic model generate explanations more closely aligned with the classification results, we employ crowdsourced judges to evaluate\na random sample of 100 items in the form of text, each being assigned to 5 judges on the Amazon Mechanical Turk. All the items are correctly classified both using the basic model and using GEF, so that we can clearly compare the explainability of these generated text explanations. We report the results in Table 8, and we can see that over half of the judges think that our GEF can generate explanations more related to the classification results. In particular, for 57.62% of the tested items, our GEF can generate better or equal explanations comparing with the basic model.\nIn addition, we show some the examples of text explanations generated by CVAE+GEF in Table 11. We can see that our model can accurately capture some key points in the golden explanations. And it can learn to generate grammatical comments that are logically reasonable. All these illustrate the efficient of our method. We will demonstrate more of our results in the supplementary materials."}, {"heading": "5.4 Error and Analysis", "text": "We focus on the deficiency of generation for text explanation in this part.\nFirst of all, as we can see from Table 11, the generated text explanation tend to be shorter than golden explanations. It is because longer explanations tend to bring more loss, so GEF tends to leave out the words that are of less informative, like function words, conjunctions, etc. In order to solve this problem, we may consider adding length reward/penalty by reinforcement learning to control the length of generated texts.\nSecond, there are \u3008UNK\u3009s in the generated explanations. Since we are generating abstractive comments for product reviews, there may exist some domain-specific words. The frequency of these special words is low, so it is relatively hard for GEF to learn to embed and generated these words. A substituted way is that we can use copymechanism (Gu et al., 2016) to generate these domain-specific words."}, {"heading": "6 Related Work", "text": "Our work is closely aligned with Explainable Artificial Intelligence (Gunning, 2017), which is claimed to be essential if users are to understand, and effectively manage this incoming generation of artificially intelligent partners. In artificial intelligence, providing an explanation of individual decisions has attracted attention in recent years. The traditional way of explaining the results is to build connections between the input and output, and figure out how much each dimension or element contributes to the final output. Some previous works explain the result in two ways: evaluating the sensitivity of output if input changes and analyzing the result from a mathematical perspective by redistributing the prediction function backward (Samek et al., 2018). There are some works connecting the result with the classification model. Ribeiro et al. (2016) selects a set of representative instances with explanations via submodular optimization. Although the method is promising and mathematically reasonable, they cannot generate explanations in natural forms. They focus on how to interpret the result.\nSome of the previous works have similar motivations as our work. Lei et al. (2016) rationalize neural prediction by extracting the phrases from the input texts as explanations. They conduct their\nwork in an extractive way, and focus on rationalizing the predictions. However, our work aims not only to predict the results but also to generate abstractive explanations, and our framework can generate explanations both in the forms of texts and numerical scores. Hancock et al. (2018) proposes to use a classifier with natural language explanations that are annotated by human beings to do the classification. Our work is different from theirs in that we use the natural attributes as the explanations which are more frequent in reality. Camburu et al. (2018) proposes e-SNLI4 by extending SNLI dataset with text explanations. And their simple but effective model proves the feasibility of generating text explanations for neural classification models."}, {"heading": "7 Conclusion", "text": "In this paper, we investigate the possibility of using fine-grained information to help explain the decision made by our classification model. More specifically, we design a Generative Explanation Framework (GEF) that can be adapted to different models. Minimum risk training method is applied to our proposed framework. Experiments demonstrate that after combining with GEF, the performance of the base model can be enhanced. Meanwhile, the quality of explanations generated by our model is also improved, which demonstrates that GEF is capable of generating more reasonable explanations for the decision.\nSince our proposed framework is modelagnostic, we can combine it with other natural processing tasks, e.g. summarization, extraction, which we leave to our future work.\n4The dataset is not publicly available now. We would like to conduct further experiments on this dataset when it is released."}], "title": "Towards Explainable NLP: A Generative Explanation Framework for Text Classification", "year": 2019}