{
  "abstractText": "The shift to electronic medical records (EMRs) has engendered research into machine learning and natural language technologies to analyze patient records, and to predict from these clinical outcomes of interest. Two observations motivate our aims here. First, unstructured notes contained within EMR often contain key information, and hence should be exploited by models. Second, while strong predictive performance is important, interpretability of models is perhaps equally so for applications in this domain. Together, these points suggest that neural models for EMR may benefit from incorporation of attention over notes, which one may hope will both yield performance gains and afford transparency in predictions. In this work we perform experiments to explore this question using two EMR corpora and four different predictive tasks, that: (i) inclusion of attention mechanisms is critical for neural encoder modules that operate over notes fields in order to yield competitive performance, but, (ii) unfortunately, while these boost predictive performance, it is decidedly less clear whether they provide meaningful support for predictions. Code to reproduce all experiments is available at https://github.com/successar/ AttentionExplanation.",
  "authors": [
    {
      "affiliations": [],
      "name": "Sarthak Jain"
    },
    {
      "affiliations": [],
      "name": "Ramin Mohammadi"
    },
    {
      "affiliations": [],
      "name": "Byron C. Wallace"
    }
  ],
  "id": "SP:e58f0da52b297d38863b84d441ba82612f700af7",
  "references": [
    {
      "authors": [
        "Dzmitry Bahdanau",
        "Kyunghyun Cho",
        "Yoshua Bengio."
      ],
      "title": "Neural machine translation by jointly learning to align and translate",
      "venue": "arXiv preprint arXiv:1409.0473.",
      "year": 2014
    },
    {
      "authors": [
        "Willie Boag",
        "Dustin Doss",
        "Tristan Naumann",
        "Peter Szolovits."
      ],
      "title": "What\u2018s in a note? unpacking predictive value in clinical note representations",
      "venue": "AMIA Summits on Translational Science Proceedings, 2017:26.",
      "year": 2018
    },
    {
      "authors": [
        "Shi Feng",
        "Eric Wallace",
        "Alvin Grissom II",
        "Pedro Rodriguez",
        "Mohit Iyyer",
        "Jordan Boyd-Graber."
      ],
      "title": "Pathologies of neural models make interpretation difficult",
      "venue": "Empirical Methods in Natural Language Processing.",
      "year": 2018
    },
    {
      "authors": [
        "Marzyeh Ghassemi",
        "Tristan Naumann",
        "Finale Doshi-Velez",
        "Nicole Brimmer",
        "Rohit Joshi",
        "Anna Rumshisky",
        "Peter Szolovits."
      ],
      "title": "Unfolding physiological state: Mortality modelling in intensive care units",
      "venue": "Proceedings of the 20th ACM",
      "year": 2014
    },
    {
      "authors": [
        "Hrayr Harutyunyan",
        "Hrant Khachatrian",
        "David C Kale",
        "Aram Galstyan."
      ],
      "title": "Multitask learning and benchmarking with clinical time series data",
      "venue": "arXiv preprint arXiv:1703.07771.",
      "year": 2017
    },
    {
      "authors": [
        "Sarthak Jain",
        "Byron C Wallace."
      ],
      "title": "Attention is not explanation",
      "venue": "arXiv preprint arXiv:1902.10186.",
      "year": 2019
    },
    {
      "authors": [
        "Mengqi Jin",
        "Mohammad Taha Bahadori",
        "Aaron Colak",
        "Parminder Bhatia",
        "Busra Celikkaya",
        "Ram Bhakta",
        "Selvan Senthivel",
        "Mohammed Khalilia",
        "Daniel Navarro",
        "Borui Zhang"
      ],
      "title": "Improving hospital mortality prediction with medical",
      "year": 2018
    },
    {
      "authors": [
        "Alistair EW Johnson",
        "Tom J Pollard",
        "Lu Shen",
        "H Lehman Li-wei",
        "Mengling Feng",
        "Mohammad Ghassemi",
        "Benjamin Moody",
        "Peter Szolovits",
        "Leo Anthony Celi",
        "Roger G Mark"
      ],
      "title": "Mimic-iii, a freely accessible critical care database",
      "year": 2016
    },
    {
      "authors": [
        "Jiwei Li",
        "Will Monroe",
        "Dan Jurafsky."
      ],
      "title": "Understanding neural networks through representation erasure",
      "venue": "arXiv preprint arXiv:1612.08220.",
      "year": 2016
    },
    {
      "authors": [
        "Yu-Wei Lin",
        "Yuqian Zhou",
        "Faraz Faghri",
        "Michael J Shaw",
        "Roy H Campbell."
      ],
      "title": "Analysis and prediction of unplanned intensive care unit readmission using recurrent neural networks with long short-term memory",
      "venue": "bioRxiv.",
      "year": 2018
    },
    {
      "authors": [
        "Zachary C Lipton",
        "David C Kale",
        "Charles Elkan",
        "Randall Wetzel."
      ],
      "title": "Learning to diagnose with lstm recurrent neural networks",
      "venue": "arXiv preprint arXiv:1511.03677.",
      "year": 2015
    },
    {
      "authors": [
        "Sampo Pyysalo",
        "Filip Ginter",
        "Hans Moen",
        "Tapio Salakoski",
        "Sophia Ananiadou."
      ],
      "title": "Distributional semantics resources for biomedical text processing",
      "venue": "Proceedings of LBM, pages 39\u201344.",
      "year": 2013
    },
    {
      "authors": [
        "Andrew Slavin Ross",
        "Michael C Hughes",
        "Finale Doshi-Velez."
      ],
      "title": "Right for the right reasons: Training differentiable models by constraining their explanations",
      "venue": "arXiv preprint arXiv:1703.03717.",
      "year": 2017
    },
    {
      "authors": [
        "Benjamin Shickel",
        "Patrick James Tighe",
        "Azra Bihorac",
        "Parisa Rashidi."
      ],
      "title": "Deep ehr: A survey of recent advances in deep learning techniques for electronic health record (ehr) analysis",
      "venue": "IEEE journal of biomedical and health informatics, 22(5):1589\u2013",
      "year": 2018
    }
  ],
  "sections": [
    {
      "heading": "1 Introduction",
      "text": "The adoption of electronic medical records (EMRs) has spurred development of machine learning (ML) and natural language processing (NLP) methods that analyze the data these records contain; for a recent survey of such efforts, see (Shickel et al., 2018). Key information for downstream predictive tasks (e.g., forecasting whether a patient will need to be readmitted within 30 days) may be contained within unstructured notes fields (Boag et al., 2018; Jin et al., 2018).\nIn this work we focus on the modules within neural network architectures responsible for encoding text (notes) into a fixed-size representation for consumption by downstream layers. Patient histories are often long and may contain information mostly irrelevant to a given target. Encoding this may thus be difficult, and text encoder modules may benefit from attention mechanisms (Bahdanau et al., 2014), which may be imposed to emphasize relevant tokens.\nIn addition to mitigating noise introduced by irrelevant tokens, attention mechanisms are often seen as providing interpretability, or insight into model behavior. However, recent work (Jain and Wallace, 2019) has argued that treating attention as explanation may, at least in some cases, be misguided. Interpretability is especially important for clinical tasks, but incorrect or misleading rationales supporting predictions may be particularly harmful in this domain; this motivates our focused study in this space.\nTo summarize, our contributions are as follows. First, we empirically investigate whether incorporating standard attention mechanisms into RNN-based text encoders improves the performance of predictive models learned over EMR. We find that they do; inclusion of standard additive attention mechanism in LSTMs consistently yields absolute gains of\u223c10 points in AUC, compared to an LSTM without attention.1 Second, we evaluate the induced attention distributions with respect to their ability to \u2018explain\u2019 model predictions. We find mixed results here, similar to (Jain and Wallace, 2019): attention distributions correlate only weakly (though almost always significantly) with\n1Indeed, across both corpora and all tasks considered, inattentive LSTMs perform considerably worse than logistic regression and bag-of-words (BoW); introducing attention makes the neural variants competitive, but not decisively better. We hope to explore this point further in future work.\nar X\niv :1\n90 4.\n03 24\n4v 1\n[ cs\n.C L\n] 5\nA pr\n2 01\n9\ngradient measures of feature importance, and we are often able to identify very different attention distributions that nonetheless yield equivalent predictions. Thus, one should not in general treat attention weights as meaningful explanation of predictions made using clinical notes."
    },
    {
      "heading": "2 Models",
      "text": "We experiment with multiple standard encoding architectures, including: (i) a standard BiLSTM model; (ii) a convolutional model, and (iii) an embedding projection based model. We couple each of these with an attention layer, following (Jain and Wallace, 2019). Concretely, each encoder yields hidden state vectors {h1, ..., hT }, and an attention distribution {\u03b11, ..., \u03b1T } is induced over these according to a scoring function \u03c6: \u03b1\u0302 = softmax(\u03c6(h)) \u2208 RT . In this work we consider Additive similarity functions \u03c6(h) = vT tanh(W1h+b) (Bahdanau et al., 2014), where v,W1,W2,b are model parameters. Predictions are made on the basis of induced representations: y\u0302 = \u03c3(\u03b8 \u00b7 h\u03b1) \u2208 R|Y|, where h\u03b1 = \u2211T t=1 \u03b1\u0302t \u00b7 ht and \u03b8 are top-level discriminative (e.g., softmax) parameters."
    },
    {
      "heading": "3 Datasets and Tasks",
      "text": "We consider five tasks over two independent EMR datasets. The first EMR corpus is MIMIC-III (Johnson et al., 2016), a publicly available set of records from patients in the Intensive Care Unit (ICU). We follow prior work in modeling aims and setup on this dataset. Specifically we consider the following predictive tasks on MIMIC.\n1. Readmission. The task here is to predict patient readmission within 30 days of discharge or transfer from the ICU. We follow the cohort selection of (Lin et al., 2018). We assume the model has access to all notes from patient admission up until the discharge or transfer from the ICU (the point of prediction).\n2. Retrospective 1-yr mortality. We aim to predict patient mortality within one year. In this we follow the experimental setup of (Ghassemi et al., 2014). The model is provided all notes up until patient discharge (excluding the discharge summary).\n3. Phenotyping. Here we aim to predict the top 25 acute care phenotypes for patients (associated at discharge with the admission). For\nthis we again rely on the framing established in prior work (Harutyunyan et al., 2017). The model has access to all notes from admission up until the end of the ICU stay. Note that this may be viewed as a multilabel classification task, similar to (Harutyunyan et al., 2017; Lipton et al., 2015).\nThe second EMR dataset we use comprises records for 7174 patients from Mass General Hospital who underwent hip or knee arthroplasty procedures. Use of this data was approved by an Institutional Review Board (IRB protocol number 2016P002062) at Partners Healthcare.\n1. Predicting Hip and Knee Surgery Complications. We consider patients who underwent hip or knee arthroplasty procedure; we aim to classify these patients with respect to whether or not they will be readmitted within 30 days due to surgery-related complications. We run experiments over hip and knee surgery patients separately."
    },
    {
      "heading": "4 Experiments",
      "text": "Following the analysis of (Jain and Wallace, 2019) but focusing on clinical tasks, we perform a set of experiments on these corpora that aim to assess the degree to which attention mechanisms aid (or hamper) predictive performance, and the degree to which the induced attention weights might be viewed as providing explanations for predictions.\nThe latter can be assessed in many ways, depending on one\u2019s view of interpretability. To address the question of whether it is reasonable to treat attention as providing interpretability broadly, we perform experiments that interrogate multiple properties we might expect these weights to exhibit if so. Specifically, we: probe the degree to which attention weights correlate with alternative gradient-based feature importance measures, which have a more straight-forward interpretation (Ross et al., 2017; Li et al., 2016); evaluate whether we are able to identify \u2018counterfactual\u2019 attention distributions that change the attention weights (focus) but not the prediction; and, in an exercise novel to the present work, we consider replacing attention weights with log odds scores from a logistic regression (linear) model. We provide a web interface to interactively browse the plots for all datasets, model variants, and experiment types: https://successar.github. io/AttentionExplanation/docs/.\n0.0 0.2 0.4 0.6 Max JS Divergence within\n0.0\n0.1\n0.2\n0.3\n0.0 0.2 0.4 0.6 Max JS Divergence within\n[0.00, 0.25) [0.25, 0.50) [0.50, 0.75) [0.75, 1.00) M ax A tte nt io n\n(a) Readmission\n.0 0.2 0.4 0.6\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.0 0.2 0.4 0.6\n[0.00, 0.25) [0.25, 0.50) [0.50, 0.75) [0.75, 1.00)\n(b) Mortality\n.0 0.2 0.4 0.6\n0.00\n0.05\n0.10\n.15\n0.20\n0.25\n0.0 0.2 0.4 0.6\n[0.00, 0.25) [0.25, 0.50) [0.50, 0.75) [0.75, 1.00)\n(c) Knee Surgery\n.0 0.2 0.4 0.6\n0.0\n0.1\n0.2\n0.3\n0.4\n0.0 0.2 0.4 0.6\n[0.00, 0.25) [0.25, 0.50) [0.50, 0.75) [0.75, 1.00)\n(d) Hip Surgery\n.0 0.2 0.4 0.6\n0.00\n0.02\n.04\n0.06\n0.08\n0.0 0.2 0.4 0.6\n[0.00, 0.25) [0.25, 0.50) [0.50, 0.75) [0.75, 1.00)\n(e) Phenotyping\n0.0 0.2 0.4 0.6 Max JS Divergence within\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.0 0.2 0.4 0.6 Max JS Divergence within\n[0.00, 0.25) [0.25, 0.50) [0.50, 0.75) [0.75, 1.00) M ax A tte nt io n\n(f) Readmission\n.0 0.2 0.4 0.6\n0.0\n0.1\n0.2\n0.3\n0.0 0.2 0.4 0.6\n[0.00, 0.25) [0.25, 0.50) [0.50, 0.75) [0.75, 1.00)\n(g) Mortality\n.0 0.2 0.4 0.6\n0.00\n0.05\n0.10\n0.15\n0.20\n0.0 0.2 0.4 0.6\n[0.00, 0.25) [0.25, 0.50) [0.50, 0.75) [0.75, 1.00)\n(h) Knee Surgery\n.0 0.2 0.4 0.6\n0.0\n0.2\n0.4\n0.6\n0.0 0.2 0.4 0.6\n[0.00, 0.25) [0.25, 0.50) [0.50, 0.75) [0.75, 1.00)\n(i) Hip Surgery\n.0 0.2 0.4 0.6\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\n0.0 0.2 0.4 0.6\n[0.00, 0.25) [0.25, 0.50) [0.50, 0.75) [0.75, 1.00)\n(j) Phenotyping\n0.0 0.2 0.4 0.6 Max JS Divergence within\n0.0\n0.1\n0.2\n0.3\n0.4\n0.0 0.2 0.4 0.6 Max JS Divergence within\n[0.00, 0.25) [0.25, 0.50) [0.50, 0.75) [0.75, 1.00) M ax A tte nt io n\n(k) Readmission\n.0 0.2 0.4 0.6\n0.0\n0.1\n0.2\n0.3\n0.4\n0.0 0.2 0.4 0.6\n[0.00, 0.25) [0.25, 0.50) [0.50, 0.75) [0.75, 1.00)\n(l) Mortality\n.0 0.2 0.4 0.6\n0.00\n0.05\n0.10\n0.15\n0.20\n.25\n0.0 0.2 0.4 0.6\n[0.00, 0.25) [0.25, 0.50) [0.50, 0.75) [0.75, 1.00)\n(m) Knee Surgery\n.0 0.2 0.4 0.6\n0.0\n0.1\n0.2\n0.3\n0.0 0.2 0.4 0.6\n[0.00, 0.25) [0.25, 0.50) [0.50, 0.75) [0.75, 1.00)\n(n) Hip Surgery\n.0 0.2 0.4 0.6\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\n0.0 0.2 0.4 0.6\n[0.00, 0.25) [0.25, 0.50) [0.50, 0.75) [0.75, 1.00)\n(o) Phenotyping\nFigure 2: Densities of maximum JS divergences ( -max JSD) (x-axis) as a function of the max attention (y-axis) in each instance for obtained between original and adversarial attention weights. Colors are as above. Top row shows results for BiLSTM encoders; middle for CNNs; bottom for Embedding Projection."
    },
    {
      "heading": "4.1 Gradient Experiments",
      "text": "To evaluate correlations between attention weights and gradient based feature importance scores, we compute Kendall-\u03c4 measure (1) between attention scores and gradients with respect to the tokens comprising documents. Across both corpora and all tasks we observe only a modest correlation between the two for BiLSTM model (the projection based model have higher correspondence, which is expected for such simple architectures). This may be problematic for attention as an explanatory mechanism, given the explicit relationship between gradients and model outputs. (Although we note that gradient based methods themselves pose difficulty with respect to interpretation (Feng et al., 2018))."
    },
    {
      "heading": "4.2 Counterfactual Experiments",
      "text": "We investigate if model predictions would have differed, had the model attended to different words (i.e., under counterfactual attention distributions).\nWe follow the two strategies from (Jain and Wallace, 2019) for constructing counterfactual attention distributions. In the first we randomly permute the empirical weights obtained from the attention module prior to inducing the weighted representation h\u03b1. We repeat this process 100 times and record the median change in output.\nThe second strategy is adversarial; we explicitly aim to identify attention weights that are maximally different from the observed weights, with\nthe constraint that this does not change the model output by more some small value . In both cases, all other model parameters are held constant.\nIn Figures 1 and 2, we observe that predictions are unchanged under alternative attention configurations in a significant majority of cases across all architectures. Thus, attention cannot be viewed casually in the sense of \u2018the model made these predictions because these words were attended to\u2019. Alternative attention distributions that yield equivalent predictions would seem to be equally plausible under the view of attention as explanation."
    },
    {
      "heading": "4.3 Log Odds Experiments",
      "text": "As a novel exercise, we also consider swapping log-odds scores for features (from an LR model operating over BoW) in for attention weights in BiLSTM model. Specifically, we induce a \u2018log odds attention\u2019 over an input by substituting the absolute value of log odds (as estimated via LR) of\n0.0 0.2 0.4 0.6 JSD (logodds vs normal)\n[0.00, 0.25) [0.25, 0.50) [0.50, 0.75) [0.75, 1.00) M ax A tte nt io n\n0.0 0.2 0.4 0.6 JSD (logodds vs normal)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nOu tp\nut D\niff er\nen ce\n(a) Readmission\n.0 0.2 0.4 0.6\n[0.00, 0.25) [0.25, .50) [ .50, 0.75) [0.75, 1.00)\n0.0 0.2 0.4 0.6\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(b) Mortality\n.0 0.2 0.4 0.6\n[0.00, 0.25) [0.25, .50) [ .50, 0.75) [0.75, 1.00)\n0.0 0.2 0.4 0.6\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(c) Knee Surgery\n.0 0.2 0.4 0.6\n[0.00, 0.25) [0.25, .50) [ .50, 0.75) [0.75, 1.00)\n0.0 0.2 0.4 0.6\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(d) Hip Surgery\n.0 0.2 0.4 0.6\n[0.00, 0.25) [0.25, .50) [ .50, 0.75) [0.75, 1.00)\n0.0 0.2 0.4 0.6\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(e) Phenotyping\nFigure 4: Change in output (y-axis) using original attention vs Log Odds attention during predictions against JSD between these two distributions (x-axis). These results are for LSTM encoders.\nthe word present at each position and passing this through a softmax: \u03b1LO = softmaxt({\u03b2wt}Tt=1) where wt is the word at position t and \u03b2 are logodds estimates.\nThese scores enjoy a clear interpretation under a linear regime. We thus explore two ways of using them with attentive neural models: (1) Swapping in these in as attention weights place of h\u03b1 at test (prediction) time; (2) Use the (fixed) \u2018log-odds attention\u2019 during training, in place of learning the attention distribution end-to-end.\nTable 2 shows that using log odds attention at test time does not degrade the performance significantly in most datasets (and actually improves performance for the Knee Surgery Complications task). Similarly, using log odds attention during training also yields similar performance to standard attention variants. But as we see in Figure 4, log odds attention distributions can differ considerably from learned attention distributions, again highlighting the difficulty of interpreting attention weights."
    },
    {
      "heading": "5 Discussion and Conclusions",
      "text": "Across two EMR datasets and five predictive tasks, we have shown that (i) attention mechanisms substantially boost the performance of LSTM text encoders passed over clinical notes, but, (ii) treating attention weights as \u2018explanations\u2019 for predictions is unwarranted. The latter confirms that the recent general findings of (Jain and Wallace, 2019) hold in the clinical domain; this is important because interpretability in this space is critical for obvious reasons.\nWe hope that this paper inspires work on transparent attention mechanisms for models that make predictions on the basis of EMR."
    },
    {
      "heading": "Acknowledgments",
      "text": "This work was supported by the Army Research Office (ARO), award W911NF1810328."
    },
    {
      "heading": "A Dataset Statistics",
      "text": "The Phenotypes studied in Phenotyping task are - Acute and unspecified renal failure, Acute cerebrovascular disease, Acute myocardial infarction, Cardiac dysrhythmias, Chronic kidney disease, Chronic obstructive pulmonary disease and bronchiectasis, Complications of surgical procedures or medical care, Conduction disorders, Congestive heart failure - nonhypertensive, Coronary atherosclerosis and other heart disease, Diabetes mellitus with complications, Diabetes mellitus without complication, Disorders of lipid metabolism, Essential hypertension, Fluid and electrolyte disorders, Gastrointestinal hemorrhage, Hypertension with complications and secondary hypertension, Other liver diseases, Other lower respiratory disease, Other upper respiratory disease, Pleurisy - pneumothorax - pulmonary collapse, Pneumonia (except that caused by tuberculosis or sexually transmitted disease), Respiratory failure - insufficiency - arrest (adult), Septicemia (except in labor), Shock ."
    },
    {
      "heading": "B Model Details",
      "text": "For all datasets, we use spaCy for tokenization. We map out of vocabulary words to a special <unk> token and map any word with numeric characters to \u2018qqq\u2019. Each word in the vocabulary was initialized using pretrained embeddings (Pyysalo et al., 2013). We initialize words not present in the vocabulary using samples from a standard Gaussian (\u00b5 = 0, \u03c32 = 1).\nB.1 BiLSTM We use an embedding size of 300 and hidden size of 128 for all datasets. The model was regularized with L2 regularization (\u03bb = 10\u22125) applied to all parameters. We use a sigmoid activation function for all binary classification tasks. We treat each phenotype classification as binary classification and take the mean loss over labels during training. We trained the model using maximum likelihood loss function with Adam Optimizer with default parameters in PyTorch.\nB.2 CNN We use an embedding size of 300 and 4 kernels of sizes [1, 3, 5, 7], each with 64 filters, giving a final hidden size of 256. We use ReLU activation function on the output of the filters. All other configurations remain same as BiLSTM.\nB.3 Average We use the embedding size of 300 and a projection size of 256 with ReLU activation on the output of the projection matrix. All other configurations remain same as BiLSTM."
    }
  ],
  "title": "An Analysis of Attention over Clinical Notes for Predictive Tasks",
  "year": 2019
}
