{"abstractText": "Deep learning is currently playing a crucial role toward higher levels of artificial intelligence. This paradigm allows neural networks to learn complex and abstract representations, that are progressively obtained by combining simpler ones. Nevertheless, the internal \"black-box\" representations automatically discovered by current neural architectures often suffer from a lack of interpretability, making of primary interest the study of explainable machine learning techniques. This paper summarizes our recent efforts to develop a more interpretable neural model for directly processing speech from the raw waveform. In particular, we propose SincNet, a novel Convolutional Neural Network (CNN) that encourages the first layer to discover more meaningful filters by exploiting parametrized sinc functions. In contrast to standard CNNs, which learn all the elements of each filter, only low and high cutoff frequencies of band-pass filters are directly learned from data. This inductive bias offers a very compact way to derive a customized filter-bank front-end, that only depends on some parameters with a clear physical meaning. Our experiments, conducted on both speaker and speech recognition, show that the proposed architecture converges faster, performs better, and is more interpretable than standard CNNs.", "authors": [{"affiliations": [], "name": "Mirco Ravanelli"}, {"affiliations": [], "name": "Yoshua Bengio"}], "id": "SP:e3c0099b44589f85ad0206ac3a41cf85852c9e14", "references": [{"authors": ["I. Goodfellow", "Y. Bengio", "A. Courville"], "title": "Deep Learning", "venue": "MIT Press", "year": 2016}, {"authors": ["D. Yu", "L. Deng"], "title": "Automatic Speech Recognition - A Deep Learning Approach", "venue": "Springer", "year": 2015}, {"authors": ["D. Bahdanau", "J. Chorowski", "D. Serdyuk", "P. Brakel", "Y. Bengio"], "title": "End-to-end attention-based large vocabulary speech recognition", "venue": "Proc. of ICASSP, pages 4945\u20134949", "year": 2016}, {"authors": ["A. Graves", "N. Jaitly"], "title": "Towards end-to-end speech recognition with recurrent neural networks", "venue": "Proc. of ICML, pages 1764\u20131772", "year": 2014}, {"authors": ["I. Goodfellow", "J. Shlens", "C. Szegedy"], "title": "Explaining and harnessing adversarial examples", "venue": "Proc.of ICLR", "year": 2015}, {"authors": ["C. Molnar"], "title": "Interpretable Machine Learning: A Guide for Making Black Box Models Explainable", "venue": "Leanpub", "year": 2018}, {"authors": ["S. Chakraborty"], "title": "Interpretability of deep learning models: A survey of results", "venue": "In Proc. of SmartWorld,", "year": 2017}, {"authors": ["M.D. Zeiler", "R. Fergus"], "title": "Visualizing and understanding convolutional networks", "venue": "Proc. of ECCV", "year": 2014}, {"authors": ["Q.-S. Zhang", "S.-C. Zhu"], "title": "Visual interpretability for deep learning: a survey", "venue": "Frontiers of Information Technology & Electronic Engineering,", "year": 2018}, {"authors": ["M.T. Ribeiro", "S. Singh", "C. Guestrin"], "title": "why should i trust you?\": Explaining the predictions of any classifier", "venue": "Proc. of ACM SIGKDD, pages 1135\u20131144", "year": 2016}, {"authors": ["Q. Zhang", "R. Cao", "F. Shi", "Y.N. Wu", "S.-C. Zhu"], "title": "Interpreting CNN Knowledge via an Explanatory Graph", "venue": "Proc. of AAAI", "year": 2018}, {"authors": ["S. Sabour", "N. Frosst", "G. E Hinton"], "title": "Dynamic routing between capsules", "venue": "In Proc. of NIPS,", "year": 2017}, {"authors": ["S. Becker", "M. Ackermann", "S. Lapuschkin", "K.-R. M\u00fcller", "W. Samek"], "title": "Interpreting and explaining deep neural networks for classification of audio signals", "venue": "CoRR, abs/1807.03418", "year": 2018}, {"authors": ["S. Hochreiter", "J. Schmidhuber"], "title": "Long short-term memory", "venue": "Neural Computation,", "year": 1997}, {"authors": ["J. Chung", "\u00c7. G\u00fcl\u00e7ehre", "K. Cho", "Y. Bengio"], "title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "venue": "Proc. of NIPS", "year": 2014}, {"authors": ["M. Ravanelli", "P. Brakel", "M. Omologo", "Y. Bengio"], "title": "Improving speech recognition by revising gated recurrent units", "venue": "Proc. of Interspeech", "year": 2017}, {"authors": ["M. Ravanelli", "P. Brakel", "M. Omologo", "Y. Bengio"], "title": "Light gated recurrent units for speech recognition", "venue": "IEEE Transactions on Emerging Topics in Computational Intelligence,", "year": 2018}, {"authors": ["Y. LeCun", "P. Haffner", "L. Bottou", "Y. Bengio"], "title": "Object recognition with gradient-based learning", "venue": "Shape, Contour and Grouping in Computer Vision, London, UK, UK", "year": 1999}, {"authors": ["E. Variani", "X. Lei", "E. McDermott", "I.L. Moreno", "J. Gonzalez-Dominguez"], "title": "Deep neural networks for small footprint text-dependent speaker verification", "venue": "Proc. of ICASSP, pages 4052\u20134056", "year": 2014}, {"authors": ["F. Richardson", "D.A. Reynolds", "N. Dehak"], "title": "A unified deep neural network for speaker and language recognition", "venue": "Proc. of Interspeech, pages 1146\u20131150", "year": 2015}, {"authors": ["D. Snyder", "D. Garcia-Romero", "D. Povey", "S. Khudanpur"], "title": "Deep neural network embeddings for text-independent speaker verification", "venue": "Proc. of Interspeech, pages 999\u20131003", "year": 2017}, {"authors": ["C. Zhang", "K. Koishida", "J. Hansen"], "title": "Text-independent speaker verification based on triplet convolutional neural network embeddings", "venue": "IEEE/ACM Trans. Audio, Speech and Lang. Proc., 26(9):1633\u20131644", "year": 2018}, {"authors": ["G. Bhattacharya", "J. Alam", "P. Kenny"], "title": "Deep speaker embeddings for short-duration speaker verification", "venue": "Proc. of Interspeech, pages 1517\u20131521", "year": 2017}, {"authors": ["A. Nagrani", "J.S. Chung", "A. Zisserman"], "title": "Voxceleb: a large-scale speaker identification dataset", "venue": "Proc. of Interspech", "year": 2017}, {"authors": ["D. Palaz", "M. Magimai-Doss", "R. Collobert"], "title": "Analysis of CNN-based speech recognition system using raw speech as input", "venue": "Proc. of Interspeech", "year": 2015}, {"authors": ["T.N. Sainath", "R.J. Weiss", "A.W. Senior", "K.W. Wilson", "O. Vinyals"], "title": "Learning the speech front-end with raw waveform CLDNNs", "venue": "Proc. of Interspeech", "year": 2015}, {"authors": ["Y. Hoshen", "R. Weiss", "K.W. Wilson"], "title": "Speech acoustic modeling from raw multichannel waveforms", "venue": "Proc. of ICASSP", "year": 2015}, {"authors": ["T.N. Sainath", "R.J. Weiss", "K.W. Wilson", "A. Narayanan", "M. Bacchiani", "A. Senior"], "title": "Speaker localization and microphone spacing invariant acoustic modeling from raw multichannel waveforms", "venue": "Proc. of ASRU", "year": 2015}, {"authors": ["Z. T\u00fcske", "P. Golik", "R. Schl\u00fcter", "H. Ney"], "title": "Acoustic modeling with deep neural networks using raw time signal for LVCSR", "venue": "Proc. of Interspeech", "year": 2014}, {"authors": ["G. Trigeorgis", "F. Ringeval", "R. Brueckner", "E. Marchi", "M.A. Nicolaou", "B. Schuller", "S. Zafeiriou"], "title": "Adieu features? end-to-end speech emotion recognition using a deep convolutional recurrent network", "venue": "Proc. of ICASSP, pages 5200\u20135204", "year": 2016}, {"authors": ["A. van den Oord", "S. Dieleman", "H. Zen", "K. Simonyan", "O. Vinyals", "A. Graves", "N. Kalchbrenner", "A. Senior", "K. Kavukcuoglu"], "title": "Wavenet: A generative model for raw audio", "venue": "In Arxiv,", "year": 2016}, {"authors": ["S. Mehri", "K. Kumar", "I. Gulrajani", "R. Kumar", "S. Jain", "J. Sotelo", "A.C. Courville", "Y. Bengio"], "title": "Samplernn: An unconditional end-to-end neural audio generation model", "venue": "CoRR, abs/1612.07837", "year": 2016}, {"authors": ["P. Ghahremani", "V. Manohar", "D. Povey", "S. Khudanpur"], "title": "Acoustic modelling from the signal domain using CNNs", "venue": "Proc. of Interspeech", "year": 2016}, {"authors": ["H. Dinkel", "N. Chen", "Y. Qian", "K. Yu"], "title": "End-to-end spoofing detection with raw waveform CLDNNS", "venue": "Proc. of ICASSP", "year": 2017}, {"authors": ["H. Muckenhirn", "M. Magimai-Doss", "S. Marcel"], "title": "Towards directly modeling raw speech signal for speaker verification using CNNs", "venue": "Proc. of ICASSP", "year": 2018}, {"authors": ["J.-W. Jung", "H.-S. Heo", "I.-H. Yang", "H.-J. Shim", "H.-J. Yu"], "title": "A complete end-to-end speaker verification system using deep neural networks: From raw signals to verification", "year": 2018}, {"authors": ["J.-W. Jung", "H.-S. Heo", "I.-H. Yang", "H.-J. Shim", "H.-J. Yu"], "title": "Avoiding Speaker Overfitting in End-to-End DNNs using Raw Waveform for Text-Independent Speaker Verification", "venue": "Proc. of Interspeech", "year": 2018}, {"authors": ["M. Ravanelli", "Y. Bengio"], "title": "Speaker Recognition from raw waveform with SincNet", "venue": "Proc. of SLT", "year": 2018}, {"authors": ["J.S. Garofolo", "L.F. Lamel", "W.M. Fisher", "J.G. Fiscus", "D.S. Pallett"], "title": "and N", "venue": "L. Dahlgren. DARPA TIMIT Acoustic Phonetic Continuous Speech Corpus CDROM", "year": 1993}, {"authors": ["V. Panayotov", "G. Chen", "D. Povey", "S. Khudanpur"], "title": "Librispeech: An ASR corpus based on public domain audio books", "venue": "Proc. of ICASSP, pages 5206\u20135210", "year": 2015}, {"authors": ["M. Ravanelli", "L. Cristoforetti", "R. Gretter", "M. Pellin", "A. Sosi", "M. Omologo"], "title": "The DIRHA-ENGLISH corpus and related tasks for distant-speech recognition in domestic environments", "venue": "In Proc. of ASRU", "year": 2015}, {"authors": ["M. Ravanelli", "P. Svaizer", "M. Omologo"], "title": "Realistic multi-microphone data simulation for distant speech recognition", "venue": "Proc. of Interspeech", "year": 2016}, {"authors": ["L.R. Rabiner", "R.W. Schafer"], "title": "Theory and Applications of Digital Speech Processing", "venue": "Prentice Hall, NJ", "year": 2011}, {"authors": ["S.K. Mitra"], "title": "Digital Signal Processing", "venue": "McGraw-Hill", "year": 2005}, {"authors": ["M. Ravanelli", "D. Serdyuk", "Y. Bengio"], "title": "Twin regularization for online speech recognition", "venue": "Proc. of Interspeech", "year": 2018}, {"authors": ["T.N. Sainath", "B. Kingsbury", "A.R. Mohamed", "B. Ramabhadran"], "title": "Learning filter banks within a deep neural network framework", "venue": "Proc. of ASRU, pages 297\u2013302", "year": 2013}, {"authors": ["H. Yu", "Z.H. Tan", "Y. Zhang", "Z. Ma", "J. Guo"], "title": "DNN Filter Bank Cepstral Coefficients for Spoofing Detection", "venue": "IEEE Access, 5:4779\u20134787", "year": 2017}, {"authors": ["H. Seki", "K. Yamamoto", "S. Nakagawa"], "title": "A deep neural network integrated with filterbank learning for speech recognition", "venue": "Proc. of ICASSP, pages 5480\u20135484", "year": 2017}, {"authors": ["N. Zeghidour", "N. Usunier", "I. Kokkinos", "T. Schatz", "G. Synnaeve", "E. Dupoux"], "title": "Learning filterbanks from raw speech for phone recognition", "venue": "Proc. of ICASSP, pages 5509\u20135513", "year": 2018}, {"authors": ["V. Papyan", "Y. Romano", "M. Elad"], "title": "Convolutional neural networks analyzed via convolutional sparse coding", "venue": "Journal of Machine Learning Research, 18:83:1\u201383:52", "year": 2017}, {"authors": ["S. Mallat"], "title": "Understanding deep convolutional networks", "venue": "CoRR, abs/1601.04920", "year": 2016}, {"authors": ["D. Palaz", "M.R. Magimai-Doss", "Collobert"], "title": "End-to-end acoustic modeling using convolutional neural networks for automatic speech recognition", "year": 2016}, {"authors": ["H. Muckenhirn", "M. Magimai-Doss", "S. Marcel"], "title": "On Learning Vocal Tract System Related Speaker Discriminative Information from Raw Signal Using CNNs", "venue": "Proc. of Interspeech", "year": 2018}, {"authors": ["M. Ravanelli"], "title": "Deep learning for Distant Speech Recognition", "venue": "PhD Thesis, Unitn", "year": 2017}, {"authors": ["M. Ravanelli", "P. Brakel", "M. Omologo", "Y. Bengio"], "title": "A network of deep neural networks for distant speech recognition", "venue": "Proc. of ICASSP, pages 4880\u20134884", "year": 2017}, {"authors": ["M. Ravanelli", "M. Omologo"], "title": "Contaminated speech training methods for robust DNN-HMM distant speech recognition", "venue": "In Proc. of Interspeech", "year": 2015}, {"authors": ["M. Ravanelli", "T. Parcollet", "Y. Bengio"], "title": "The PyTorch-Kaldi Speech Recognition Toolkit", "venue": "arXiv:1811.07453", "year": 2018}, {"authors": ["A.K. Sarkar", "D Matrouf", "P.M. Bousquet", "J.F. Bonastre"], "title": "Study of the effect of i-vector modeling on short and mismatch utterance duration for speaker verification", "venue": "Proc. of Interspeech, pages 2662\u20132665", "year": 2012}, {"authors": ["R. Travadi", "M. Van Segbroeck", "S. Narayanan"], "title": "Modified-prior i-Vector Estimation for Language Identification of Short Duration Utterances", "venue": "Proc. of Interspeech, pages 3037\u20133041", "year": 2014}, {"authors": ["A. Kanagasundaram", "R. Vogt", "D. Dean", "S. Sridharan", "M. Mason"], "title": "i-vector based speaker recognition on short utterances", "venue": "Proc. of Interspeech, pages 2341\u20132344", "year": 2011}, {"authors": ["M. Matassoni", "R. Astudillo", "A. Katsamanis", "M. Ravanelli"], "title": "The DIRHA-GRID corpus: baseline and tools for multi-room distant speech recognition using distributed microphones", "venue": "In Proc. of Interspeech", "year": 2014}, {"authors": ["E. Zwyssig", "M. Ravanelli", "P. Svaizer", "M. Omologo"], "title": "A multi-channel corpus for distant-speech interaction in presence of known interferences", "venue": "In Proc. of ICASSP", "year": 2015}, {"authors": ["L. Cristoforetti", "M. Ravanelli", "M. Omologo", "A. Sosi", "A. Abad", "M. Hagmueller", "P. Maragos"], "title": "The DIRHA simulated corpus", "venue": "In Proc. of LREC", "year": 2014}, {"authors": ["Douglas P", "J.M. Baker"], "title": "The design for the wall street journal-based csr corpus", "venue": "In Proceedings of the Workshop on Speech and Natural Language, Proc. of HLT,", "year": 1992}, {"authors": ["M. Ravanelli", "A. Sosi", "P. Svaizer", "M. Omologo"], "title": "Impulse response estimation for robust speech recognition in a reverberant environment", "venue": "In Proc. of EUSIPCO", "year": 2012}, {"authors": ["M. Ravanelli", "M. Omologo"], "title": "On the selection of the impulse responses for distant-speech recognition based on contaminated speech training", "venue": "In Proc. of Interspeech", "year": 2014}, {"authors": ["J. Ba", "R. Kiros", "G.E. Hinton"], "title": "Layer normalization", "venue": "CoRR, abs/1607.06450", "year": 2016}, {"authors": ["S. Ioffe", "C. Szegedy"], "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "venue": "Proc. of ICML, pages 448\u2013456", "year": 2015}, {"authors": ["M. Ravanelli", "P. Brakel", "M. Omologo", "Y. Bengio"], "title": "Batch-normalized joint training for dnn-based distant speech recognition", "venue": "Proc. of SLT", "year": 2016}, {"authors": ["A.L. Maas", "A.Y. Hannun", "A.Y. Ng"], "title": "Rectifier nonlinearities improve neural network acoustic models", "venue": "Proc. of ICML", "year": 2013}, {"authors": ["X. Glorot", "Y. Bengio"], "title": "Understanding the difficulty of training deep feedforward neural networks", "venue": "Proc. of AISTATS, pages 249\u2013256", "year": 2010}, {"authors": ["D. Povey"], "title": "The Kaldi Speech Recognition Toolkit", "venue": "In Proc. of ASRU,", "year": 2011}, {"authors": ["A. Larcher", "K.A. Lee", "S. Meignier"], "title": "An extensible speaker identification sidekit in python", "venue": "Proc. of ICASSP, pages 5095\u20135099", "year": 2016}], "sections": [{"heading": "1 Introduction", "text": "Deep learning has recently contributed to achieving unprecedented performance levels in numerous tasks, mainly thanks to the progressive maturation of supervised learning techniques [1]. The increased discrimination power of modern neural networks, however, is often obtained at the cost of a reduced interpretability of the model. Modern end-to-end systems, whose popularity is increasing in many fields such as speech recognition [2, 3, 4], often discover \"black-box\" internal representations that make sense for the machine but are arguably difficult to interpret by humans. The remarkable sensitivity of current neural networks toward adversarial examples [5], for instance, not only highlights how superficial the discovered representations could be but also raises crucial concerns about our capabilities to really interpret neural models. Such a lack of interpretability can be a major bottleneck for the development of future deep learning techniques. Having more meaningful insights on the logic behind network predictions and errors, in fact, can help us to better trust, understand, and diagnose our model, eventually guiding our efforts toward more robust deep learning. In recent years, a growing interest has been thus devoted to the development of interpretable machine learning [6, 7], as witnessed by the numerous works in the field, ranging from visualization [8, 9], diagnosis of DNNs [10], explanatory graphs [11], and explainable models [12], just to name a few.\nInterpretability is a major concern for audio and speech applications as well [13]. CNNs and Recurrent Neural Networks (RNNs) are the most popular architectures nowadays used in speech and speaker recognition [2]. RNN can be employed to capture the temporal evolution of the speech signal [14, 15, 16, 17], while CNNs, thanks to their weight sharing, local filters, and pooling networks are normally employed to extract robust and invariant representations [18]. Even though standard hand-crafted features such as FBANK and Mel-Frequency Cepstral Coefficients (MFCC) are still\n32nd Conference on Neural Information Processing Systems (NIPS 2018) IRASL workshop, Montr\u00e9al, Canada.\nar X\niv :1\n81 1.\n09 72\n5v 2\n[ ee\nss .A\nS] 9\nA ug\n2 01\nemployed in many state-of-the-art systems [19, 20, 21], directly feeding a CNN with spectrogram bins [22, 23, 24] or even with raw audio samples [25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37] is an approach of increasing popularity. The engineered features, in fact, are originally designed from perceptual evidence and there are no guarantees that such representations are optimal for all speech-related tasks. Standard features, for instance, smooth the speech spectrum, possibly hindering the extraction of crucial narrow-band speaker characteristics such as pitch and formants. Conversely, directly processing the raw waveform allows the network to learn low-level representations that are possibly more customized on each specific task.\nThe downside of raw speech processing lies in the possible lack of interpretability of the filter bank learned in the first convolutional layer. According to us, the latter layer is arguably the most critical part of current waveform-based CNNs. This layer deals with high-dimensional inputs and is also more affected by vanishing gradient problems, especially when employing very deep architectures. As will be discussed in this paper, the filters learned by CNNs often take noisy and incongruous multi-band shapes, especially when few training samples are available. These filters certainly make some sense for the neural network, but they do not appeal to human intuition, nor appear to lead to an efficient representation of the speech signal.\nTo help the CNNs discover more meaningful filters, this work proposes to add some constraints on their shape. Compared to standard CNNs, where the filter-bank characteristics depend on several parameters (each element of the filter vector is directly learned), SincNet convolves the waveform with a set of parametrized sinc functions that implement band-pass filters [38]. The low and high cutoff frequencies are the only parameters of the filter learned from data. This solution still offers considerable flexibility but forces the network to focus on high-level tunable parameters that have a clear physical meaning. Our experimental validation has considered both speaker and speech recognition tasks. Speaker recognition is carried out on TIMIT [39] and Librispeech [40] datasets under challenging but realistic conditions, characterized by minimal training data (i.e., 12-15 seconds for each speaker) and short test sentences (lasting from 2 to 6 seconds). With the purpose of validating SincNet in both clean and noisy conditions, speech recognition experiments are conducted on both the TIMIT and DIRHA dataset [41, 42]. Results show that the proposed SincNet converges faster, achieves better performance, and is more interpretable than a more standard CNN.\nThe remainder of the paper is organized as follows. The SincNet architecture is described in Sec. 2. Sec. 3 discusses the relation to prior work. The experimental activity on both speaker and speech recognition is outlined in Sec. 4. Finally, Sec. 5 discusses our conclusions."}, {"heading": "2 The SincNet Architecture", "text": "The first layer of a standard CNN performs a set of time-domain convolutions between the input waveform and some Finite Impulse Response (FIR) filters [43]. Each convolution is defined as follows1:\ny[n] = x[n] \u2217 h[n] = L\u22121\u2211 l=0 x[l] \u00b7 h[n\u2212 l] (1)\nwhere x[n] is a chunk of the speech signal, h[n] is the filter of length L, and y[n] is the filtered output. In standard CNNs, all the L elements (taps) of each filter are learned from data. Conversely, the proposed SincNet (depicted in Fig. 1) performs the convolution with a predefined function g that depends on few learnable parameters \u03b8 only, as highlighted in the following equation:\ny[n] = x[n] \u2217 g[n, \u03b8] (2)\nA reasonable choice, inspired by standard filtering in digital signal processing, is to define g such that a filter-bank composed of rectangular bandpass filters is employed. In the frequency domain, the magnitude of a generic bandpass filter can be written as the difference between two low-pass filters:\nG[f, f1, f2] = rect ( f 2f2 ) \u2212 rect ( f 2f1 ) , (3)\n1Most deep learning toolkits actually compute correlation rather than convolution. The obtained flipped (mirrored) filters do not affect the results.\nPooling\nDropout\nCNN/DNN layers\nSoftmax\nSpeaker Classification\nSpeech Waveform\nLayer Norm\nLeaky ReLU\nFigure 1: Architecture of SincNet.\nwhere f1 and f2 are the learned low and high cutoff frequencies, and rect(\u00b7) is the rectangular function in the magnitude frequency domain2. After returning to the time domain (using the inverse Fourier transform [43]), the reference function g becomes:\ng[n, f1, f2] = 2f2sinc(2\u03c0f2n)\u2212 2f1sinc(2\u03c0f1n), (4) where the sinc function is defined as sinc(x) = sin(x)/x.\nThe cut-off frequencies can be initialized randomly in the range [0, fs/2], where fs represents the sampling frequency of the input signal. As an alternative, filters can be initialized with the cutoff frequencies of the mel-scale filter-bank, which has the advantage of directly allocating more filters in the lower part of the spectrum, where crucial speech information is located. To ensure f1 \u2265 0 and f2 \u2265 f1, the previous equation is actually fed by the following parameters:\nfabs1 = |f1| (5) fabs2 = f1 + |f2 \u2212 f1| (6)\nNote that no bounds have been imposed to force f2 to be smaller than the Nyquist frequency, since we observed that this constraint is naturally fulfilled during training. Moreover, the gain of each filter is not learned at this level. This parameter is managed by the subsequent layers, which can easily attribute more or less importance to each filter output.\nAn ideal bandpass filter (i.e., a filter where the passband is perfectly flat and the attenuation in the stopband is infinite) requires an infinite number of elements L. Any truncation of g thus inevitably leads to an approximation of the ideal filter, characterized by ripples in the passband and limited attenuation in the stopband. A popular solution to mitigate this issue is windowing [43]. Windowing\n2The phase of the rect(\u00b7) function is considered to be linear.\nis performed by multiplying the truncated function g with a window function w, which aims to smooth out the abrupt discontinuities at the ends of g:\ngw[n, f1, f2] = g[n, f1, f2] \u00b7 w[n]. (7)\nThis paper uses the popular Hamming window [44], defined as follows: w[n] = 0.54\u2212 0.46 \u00b7 cos (2\u03c0n L ) . (8)\nThe Hamming window is particularly suitable to achieve high frequency selectivity [44]. However, results not reported here reveal no significant performance difference when adopting other functions, such as Hann, Blackman, and Kaiser windows. Note also that the filters g are symmetric and thus do not introduce any phase distortions. Due to the symmetry, the filters can be computed efficiently by considering one side of the filter and inheriting the results for the other half\nAll operations involved in SincNet are fully differentiable and the cutoff frequencies of the filters can be jointly optimized with other CNN parameters using Stochastic Gradient Descent (SGD) or other gradient-based optimization routines. As shown in Fig. 1, a standard CNN pipeline (pooling, normalization, activations, dropout) can be employed after the first sinc-based convolution. Multiple standard convolutional, fully-connected or recurrent layers [15, 16, 17, 45] can then be stacked together to finally perform a classification with a softmax classifier.\nFig. 2 shows some examples of filters learned by a standard CNN and by the proposed SincNet for a speaker identification task trained on Librispeech (the frequency response is plotted between 0 and 4 kHz). As observed in the figures, the standard CNN does not always learn filters with a well-defined frequency response. In some cases, the frequency response looks noisy (see the first CNN filter), while in others assuming multi-band shapes (see the third CNN filter). SincNet, instead, is specifically designed to implement rectangular bandpass filters, leading to more a meaningful filter-bank."}, {"heading": "2.1 Model properties", "text": "The proposed SincNet has some remarkable properties:\n\u2022 Fast Convergence: SincNet forces the network to focus only on the filter parameters with a major impact on performance. The proposed approach actually implements a natural inductive bias, utilizing knowledge about the filter shape (similar to feature extraction methods generally deployed on this task) while retaining flexibility to adapt to data. This prior knowledge makes learning the filter characteristics much easier, helping SincNet to converge significantly faster to a better solution. Fig. 3 shows the learning curves of SincNet\n0 1000 2000 3000 4000\nFrequency [Hz]\n0\n0.2\n0.4\n0.6\n0.8\n1\nSincNet\nCNN\n2nd Formant\n1st Formant\nPitch\nFigure 4: Cumulative frequency response of SincNet and CNN filters on speaker-id.\nand CNN obtained in a speaker-id task. These results are achieved on the TIMIT dataset and highlight a faster decrease of the Frame Error Rate (FER%) when SincNet is used. Moreover, SincNet converges to better performance leading to a FER of 33.0% against a FER of 37.7% achieved with the CNN baseline.\n\u2022 Few Parameters: SincNet drastically reduces the number of parameters in the first convolutional layer. For instance, if we consider a layer composed of F filters of length L, a standard CNN employs F \u00b7 L parameters, against the 2F considered by SincNet. If F = 80 and L = 100, we employ 8k parameters for the CNN and only 160 for SincNet. Moreover, if we double the filter length L, a standard CNN doubles its parameter count (e.g., we go from 8k to 16k), while SincNet has an unchanged parameter count (only two parameters are employed for each filter, regardless its length L). This offers the possibility to derive very selective filters with many taps, without actually adding parameters to the optimization problem. Moreover, the compactness of the SincNet architecture makes it suitable in the few sample regime.\n\u2022 Interpretability: The SincNet feature maps obtained in the first convolutional layer are definitely more interpretable and human-readable than other approaches. The filter bank, in fact, only depends on parameters with a clear physical meaning. Fig. 4, for instance, shows the cumulative frequency response of the filters learned by SincNet and CNN on a speaker-id task. The cumulative frequency response is obtained by summing up all the discovered filters and is useful to highlight which frequency bands are covered by the learned filters. Interestingly, there are three main peaks which clearly stand out from the SincNet plot (see the red line in the figure). The first one corresponds to the pitch region (the average pitch is 133 Hz for a male and 234 for a female). The second peak (approximately located at 500 Hz) mainly captures first formants, whose average value over the various English vowels is indeed 500 Hz. Finally, the third peak (ranging from 900 to 1400 Hz) captures some important second formants, such as the second formant of the vowel /a/, which is located on average at 1100 Hz. This filter-bank configuration indicates that SincNet has successfully adapted its characteristics to address speaker identification. Conversely, the standard CNN does not exhibit such a meaningful pattern: the CNN filters tend to correctly focus on the lower part of the spectrum, but peaks tuned on first and second formants do not clearly appear. As one can observe from Fig. 4, the CNN curve stands above the SincNet one. SincNet, in fact, learns filters that are, on average, more selective than CNN ones, possibly better capturing narrow-band speaker clues. Fig. 5 shows the cumulative frequency response of a CNN and SincNet obtained on a noisy speech recognition task. In this experiment, we have artificially corrupted TIMIT with a significant quantity of noise in the band between 2.0 and 2.5 kHz (see the spectrogram) and we have analyzed how fast the two architectures learn to avoid such a useless band. The second row of sub-figures compares the CNN and the SincNet at a very early training stage\n(i.e., after having processed only one hour of speech in the first epoch), while the last row shows the cumulative frequency responses after completing the training. From the figures emerges that both CNN and SincNet have correctly learned to avoid the corrupted band at end of training, as highlighted by the holes between 2.0 and 2.5 kHz in the cumulative frequency responses. SincNet, however, learns to avoid such a noisy band much earlier. In the second row of sub-figures, in fact, SincNet shows a visible valley in the cumulative spectrum even after processing only one hour of speech, while CNN has only learned to give more importance to the lower part of the spectrum."}, {"heading": "3 Related Work", "text": "Several works have recently explored the use of low-level speech representations to process audio and speech with CNNs. Most prior attempts exploit magnitude spectrogram features [22, 23, 24, 46, 47, 48]. Although spectrograms retain more information than standard hand-crafted features, their design still requires careful tuning of some crucial hyper-parameters, such as the duration, overlap, and typology of the frame window, as well as the number of frequency bins. For this reason, a more\nrecent trend is to directly learn from raw waveforms, thus completely avoiding any feature extraction step. This approach has shown promise in speech [25, 26, 27, 28, 29], including emotion tasks [30], speaker recognition [35], spoofing detection [34], and speech synthesis [31, 32].\nSimilar to SincNet, some previous works have proposed to add constraints on the CNN filters, for instance forcing them to work on specific bands [46, 47]. Differently from the proposed approach, the latter works operate on spectrogram features and still learn all the L elements of the CNN filters. An idea related to the proposed method has been recently explored in [48], where a set of parameterized Gaussian filters are employed. This approach operates on the spectrogram domain, while SincNet directly considers the raw waveform in the time domain. Similarly to our work, in [49] the convolutional filters are initialized with a predefined filter shape. However, rather than focusing on cut-off frequencies only, all the basic taps of the FIR filters are still learned.\nSome valuable works have recently proposed theoretical and experimental frameworks to analyze CNNs [50, 51]. In particular, [52, 35, 53] feed a standard CNN with raw audio samples and analyze the filters learned in the first layer on both speech recognition and speaker identification tasks. The authors highlight some interesting properties emerged from analyzing the cumulative frequency response and propose a spectral dictionary interpretation of the learned filters. Similarly to our findings, the latter works noticed that the filters tend to focus more on the lower part of the spectrum and they can sometimes highlight some peaks that likely corresponds to the fundamental frequency. In this work, we argue that all of these interesting properties can be observed more clearly and at an earlier training stage with SincNet.\nThis paper extends our previous studies on the SincNet [38]. To the best of our knowledge, this paper is the first that shows the effectiveness of the proposed SincNet in a speech recognition application. Moreover, this work not only considers standard close-talking speech recognition, but it also extends the validation of SincNet to distant-talking speech recognition [54, 55, 56]."}, {"heading": "4 Results", "text": "The proposed SincNet has been evaluated on both speech and speaker recognition using different corpora. This work considers a challenging but realistic speaker recognition scenario: for all the adopted corpora, we only employed 12-15 seconds of training material for each speaker, and we tested the system performance on short sentences lasting from 2 to 6 seconds. In the spirit of reproducible research, we release the code of SincNet for speaker identification3 and speech recognition4 (under the PyTorch-Kaldi project [57]). More details on the adopted datasets as well as on the SincNet and baseline setups can found in the appendix."}, {"heading": "4.1 Speaker Recognition", "text": "Table 1 reports the Classification Error Rates (CER%) achieved on a speaker-id task. The table shows that SincNet outperforms other systems on both TIMIT (462 speakers) and Librispeech (2484 speakers) datasets. The gap with a standard CNN fed by raw waveform is larger on TIMIT, confirming the effectiveness of SincNet when few training data are available. Although this gap is reduced when LibriSpeech is used, we still observe a 4% relative improvement that is also obtained with faster convergence (1200 vs 1800 epochs). Standard FBANKs provide results comparable to SincNet only on TIMIT, but are significantly worse than our architecture when using Librispech. With few training data, the network cannot discover filters that are much better than that of FBANKs, but with more data a customized filter-bank is learned and exploited to improve the performance.\nTable 2 extends our validation to speaker verification, reporting the Equal Error Rate (EER%) achieved with Librispeech. All DNN models show promising performance, leading to an EER lower than 1% in all cases. The table also highlights that SincNet outperforms the other models, showing a relative performance improvement of about 11% over the standard CNN model. Note that the speaker verification system is derived from the speaker-id neural network using the d-vector technique. The d-vector [19, 24] is extracted from the last hidden layer of the speaker-id network. A speaker-dependent d-vector is computed and stored for each enrollment speaker by performing an L2 normalization and averaging all the d-vectors of the different speech chunks. The cosine distance\n3 at https://github.com/mravanelli/SincNet/. 4 at https://github.com/mravanelli/pytorch-kaldi/.\nbetween enrolment and test d-vectors is then calculated, and a threshold is then applied on it to reject or accept the speaker. Ten utterances from impostors were randomly selected for each sentence coming from a genuine speaker. To assess our approach on a standard open-set speaker verification task, all the enrolment and test utterances were taken from a speaker pool different from that used for training the speaker-id DNN.\nFor the sake of completeness, experiments have also been conducted with standard i-vectors. Although a detailed comparison with this technology is out of the scope of this paper, it is worth noting that our best i-vector system achieves an EER=1.1%, rather far from what is achieved with DNN systems. It is well-known in the literature that i-vectors provide competitive performance when more training material is used for each speaker and when longer test sentences are employed [58, 59, 60]. Under the challenging conditions faced in this work, neural networks achieve better generalization."}, {"heading": "4.2 Speech Recognition", "text": "Tab. 3 reports the speech recognition performance obtained by CNN and SincNet using the TIMIT and the DIRHA dataset [41]. To ensure a more accurate comparison between the architectures, five experiments varying the initialization seeds were conducted for each model and corpus. Table 3 thus reports the average speech recognition performance. Standard deviations, not reported here, range between 0.15 and 0.2 for all the experiments.\nFor all the datasets, SincNet outperforms CNNs trained on both standard FBANK and raw waveforms. The latter result confirms the effectiveness of SincNet not only in close-talking scenarios but also in challenging noisy conditions characterized by the presence of both noise and reverberation. As emerged in Sec.2, SincNet is able to effectively tune its filter-bank front-end to better address the characteristics of the noise."}, {"heading": "5 Conclusions and Future Work", "text": "This paper proposed SincNet, a neural architecture for directly processing waveform audio. Our model, inspired by the way filtering is conducted in digital signal processing, imposes constraints on the filter shapes through efficient parameterization. SincNet has been extensively evaluated on challenging speaker and speech recognition tasks, consistently showing some performance benefits.\nBeyond performance improvements, SincNet also significantly improves convergence speed over a standard CNN, is more computationally efficient due to the exploitation of filter symmetry, and it is more interpretable than standard black-box models. Analysis of the SincNet filters, in fact, revealed\nthat the learned filter-bank is tuned to the specific task addressed by the neural network. In future work, we would like to evaluate SincNet on other popular speaker recognition tasks, such as VoxCeleb. Inspired by the promising results obtained in this paper, in the future we will explore the use of SincNet for supervised and unsupervised speaker/environmental adaptation. Moreover, although this study targeted speaker and speech recognition only, we believe that the proposed approach defines a general paradigm to process time-series and can be applied in numerous other fields."}, {"heading": "Acknowledgement", "text": "This research was enabled in part by support provided by Calcul Qu\u00e9bec and Compute Canada."}], "title": "Interpretable Convolutional Filters with SincNet", "year": 2019}