{
  "abstractText": "Explainable machine learning and artificial intelligence models have been used to justify a model\u2019s decisionmaking process. This added transparency aims to help improve user performance and understanding of the underlying model. However, in practice, explainable systems face many open questions and challenges. Specifically, designers might reduce the complexity of deep learning models in order to provide interpretability. The explanations generated by these simplified models, however, might not accurately justify and be truthful to the model. This can further add confusion to the users as they might not find the explanations meaningful with respect to the model predictions. Understanding how these explanations affect user behavior is an ongoing challenge. In this paper, we explore how explanation veracity affects user performance and agreement in intelligent systems. Through a controlled user study with an explainable activity recognition system, we compare variations in explanation veracity for a video review and querying task. The results suggest that low veracity explanations significantly decrease user performance and agreement compared to both accurate explanations and a system without explanations. These findings demonstrate the importance of accurate and understandable explanations and caution that poor explanations can sometimes be worse than no explanations with respect to their effect on user performance and reliance on an AI system.",
  "authors": [
    {
      "affiliations": [],
      "name": "MAHSAN NOURANI"
    },
    {
      "affiliations": [],
      "name": "CHIRADEEP ROY"
    },
    {
      "affiliations": [],
      "name": "TAHRIMA RAHMAN"
    },
    {
      "affiliations": [],
      "name": "ERIC D. RAGAN"
    }
  ],
  "id": "SP:d06954f2a64af0091f5cadca98f73fee907d271f",
  "references": [
    {
      "authors": [
        "Ashraf Abdul",
        "Jo Vermeulen",
        "Danding Wang",
        "Brian Y Lim",
        "Mohan Kankanhalli"
      ],
      "title": "Trends and trajectories for explainable, accountable and intelligible systems: An hci research agenda",
      "venue": "In Proceedings of the 2018 CHI conference on human factors in computing systems",
      "year": 2018
    },
    {
      "authors": [
        "Amina Adadi",
        "Mohammed Berrada"
      ],
      "title": "Peeking inside the black-box: A survey on Explainable Artificial Intelligence (XAI)",
      "venue": "IEEE Access",
      "year": 2018
    },
    {
      "authors": [
        "Yongsu Ahn",
        "Yu-Ru Lin"
      ],
      "title": "FairSight: Visual Analytics for Fairness in Decision Making. IEEE transactions on visualization and computer graphics (2019)",
      "year": 2019
    },
    {
      "authors": [
        "Jos\u00e9 M Alonso",
        "Alejandro Ramos-Soto",
        "Ciro Castiello",
        "Corrado Mencar"
      ],
      "title": "Explainable AI Beer Style Classifier",
      "venue": "In SICSA ReaLX",
      "year": 2018
    },
    {
      "authors": [
        "Andrew Anderson",
        "Jonathan Dodge",
        "Amrita Sadarangani",
        "Zoe Juozapaitis",
        "Evan Newman",
        "Jed Irvine",
        "Souti Chattopadhyay",
        "Alan Fern",
        "Margaret Burnett"
      ],
      "title": "Explaining Reinforcement Learning to Mere Mortals: An Empirical Study",
      "year": 2019
    },
    {
      "authors": [
        "Martin Atzmueller",
        "Naveed Hayat",
        "Matthias Trojahn",
        "Dennis Kroll"
      ],
      "title": "Explicative human activity recognition using adaptive association rule-based classification",
      "venue": "IEEE International Conference on Future IoT Technologies (Future IoT)",
      "year": 2018
    },
    {
      "authors": [
        "Christine L Borgman"
      ],
      "title": "The user\u2019s mental model of an information retrieval system: An experiment on a prototype online catalog",
      "venue": "International Journal of man-machine studies 24,",
      "year": 1986
    },
    {
      "authors": [
        "Adrian Bussone",
        "Simone Stumpf",
        "Dympna O\u2019Sullivan"
      ],
      "title": "The role of explanations on trust and reliance in clinical decision support systems",
      "venue": "In 2015 International Conference on Healthcare Informatics",
      "year": 2015
    },
    {
      "authors": [
        "Carrie J Cai",
        "Jonas Jongejan",
        "Jess Holbrook"
      ],
      "title": "The effects of example-based explanations in a machine learning interface",
      "venue": "In Proceedings of the 24th International Conference on Intelligent User Interfaces",
      "year": 2019
    },
    {
      "authors": [
        "Zhiyong Cheng",
        "Xiaojun Chang",
        "Lei Zhu",
        "Rose C Kanjirathinkal",
        "andMohan Kankanhalli"
      ],
      "title": "MMALFM: Explainable recommendation by leveraging reviews and images",
      "venue": "ACM Transactions on Information Systems (TOIS) 37,",
      "year": 2019
    },
    {
      "authors": [
        "Noel CF Codella",
        "Michael Hind",
        "Karthikeyan Natesan Ramamurthy",
        "Murray Campbell",
        "Amit Dhurandhar",
        "Kush R Varshney",
        "Dennis Wei",
        "Aleksandra Mojsilovi\u0107"
      ],
      "title": "Teaching AI to Explain its Decisions Using Embeddings and Multi-Task Learning",
      "venue": "arXiv preprint arXiv:1906.02299",
      "year": 2019
    },
    {
      "authors": [
        "Henriette Cramer",
        "Vanessa Evers",
        "Satyan Ramlal",
        "Maarten Van Someren",
        "Lloyd Rutledge",
        "Natalia Stash",
        "Lora Aroyo",
        "Bob Wielinga"
      ],
      "title": "The effects of transparency on trust in and acceptance of a content-based art recommender",
      "venue": "User Modeling and User-Adapted Interaction",
      "year": 2008
    },
    {
      "authors": [
        "Mary Cummings"
      ],
      "title": "Automation bias in intelligent time critical decision support systems",
      "venue": "In AIAA 1st Intelligent Systems Technical Conference",
      "year": 2004
    },
    {
      "authors": [
        "Piotr Dabkowski",
        "Yarin Gal"
      ],
      "title": "Real time image saliency for black box classifiers",
      "venue": "In Advances in Neural Information Processing Systems",
      "year": 2017
    },
    {
      "authors": [
        "Alexandra Dimitroff"
      ],
      "title": "Mental models and error behavior in an interactive bibliographic retrieval system",
      "year": 1992
    },
    {
      "authors": [
        "Mengnan Du",
        "Ninghao Liu",
        "Xia Hu"
      ],
      "title": "Techniques for interpretable machine learning",
      "venue": "Commun. ACM 63,",
      "year": 2019
    },
    {
      "authors": [
        "Mary T Dzindolet",
        "Scott A Peterson",
        "Regina A Pomranky",
        "Linda G Pierce",
        "Hall P Beck"
      ],
      "title": "The role of trust in automation reliance",
      "venue": "International journal of human-computer studies 58,",
      "year": 2003
    },
    {
      "authors": [
        "Malin Eiband",
        "Hanna Schneider",
        "Mark Bilandzic",
        "Julian Fazekas-Con",
        "Mareike Haug",
        "Heinrich Hussmann"
      ],
      "title": "Bringing transparency design into practice",
      "venue": "In 23rd International Conference on Intelligent User Interfaces",
      "year": 2018
    },
    {
      "authors": [
        "Shi Feng",
        "Eric Wallace",
        "II Grissom",
        "Mohit Iyyer",
        "Pedro Rodriguez",
        "Jordan Boyd-Graber"
      ],
      "title": "Pathologies of neural models make interpretations difficult",
      "year": 2018
    },
    {
      "authors": [
        "Francisco Javier Chiyah Garcia",
        "David A Robb",
        "Xingkun Liu",
        "Atanas Laskov",
        "Pedro Patron",
        "Helen Hastie"
      ],
      "title": "Explainable autonomy: A study of explanation styles for building clear mental models",
      "venue": "In Proceedings of the 11th International Conference on Natural Language Generation",
      "year": 2018
    },
    {
      "authors": [
        "Randy Goebel",
        "Ajay Chander",
        "Katharina Holzinger",
        "Freddy Lecue",
        "Zeynep Akata",
        "Simone Stumpf",
        "Peter Kieseberg",
        "Andreas Holzinger"
      ],
      "title": "Explainable AI: the new 42",
      "venue": "In International Cross-Domain Conference for Machine Learning and Knowledge",
      "year": 2018
    },
    {
      "authors": [
        "Chloe Gui",
        "Victoria Chan"
      ],
      "title": "Machine learning in medicine",
      "venue": "University of Western Ontario Medical Journal 86,",
      "year": 2017
    },
    {
      "authors": [
        "Riccardo Guidotti",
        "Anna Monreale",
        "Salvatore Ruggieri",
        "Franco Turini",
        "Fosca Giannotti",
        "Dino Pedreschi"
      ],
      "title": "A survey of methods for explaining black box models",
      "venue": "ACM computing surveys (CSUR) 51,",
      "year": 2018
    },
    {
      "authors": [
        "Riccardo Guidotti",
        "Anna Monreale",
        "Salvatore Ruggieri",
        "Franco Turini",
        "Fosca Giannotti",
        "Dino Pedreschi"
      ],
      "title": "A survey of methods for explaining black box models",
      "venue": "ACM computing surveys (CSUR) 51,",
      "year": 2019
    },
    {
      "authors": [
        "Kotaro Hara",
        "Abigail Adams",
        "Kristy Milland",
        "Saiph Savage",
        "Chris Callison-Burch",
        "Jeffrey P Bigham"
      ],
      "title": "A data-driven analysis of workers\u2019 earnings on amazon mechanical turk",
      "venue": "In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems",
      "year": 2018
    },
    {
      "authors": [
        "Adam W Harley"
      ],
      "title": "An interactive node-link visualization of convolutional neural networks",
      "venue": "In International Symposium on Visual Computing",
      "year": 2015
    },
    {
      "authors": [
        "Lisa Anne Hendricks",
        "Zeynep Akata",
        "Marcus Rohrbach",
        "Jeff Donahue",
        "Bernt Schiele",
        "Trevor Darrell"
      ],
      "title": "Generating Visual Explanations",
      "venue": "In Computer Vision \u2013 ECCV 2016,",
      "year": 2016
    },
    {
      "authors": [
        "Michael Hind",
        "Dennis Wei",
        "Murray Campbell",
        "Noel CF Codella",
        "Amit Dhurandhar",
        "Aleksandra Mojsilovi\u0107",
        "Karthikeyan Natesan Ramamurthy",
        "Kush R Varshney"
      ],
      "title": "TED: Teaching AI to explain its decisions",
      "venue": "In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",
      "year": 2019
    },
    {
      "authors": [
        "Robert R Hoffman",
        "Matthew Johnson",
        "Jeffrey M Bradshaw",
        "Al Underbrink"
      ],
      "title": "Trust in automation",
      "venue": "IEEE Intelligent Systems 28,",
      "year": 2013
    },
    {
      "authors": [
        "Robert R Hoffman",
        "Shane T Mueller",
        "Gary Klein",
        "Jordan Litman"
      ],
      "title": "Metrics for explainable AI: Challenges and prospects",
      "year": 2018
    },
    {
      "authors": [
        "Fred Hohman",
        "Minsuk Kahng",
        "Robert Pienta",
        "Duen Horng Chau"
      ],
      "title": "Visual analytics in deep learning: An interrogative survey for the next frontiers",
      "venue": "IEEE transactions on visualization and computer graphics 25,",
      "year": 2018
    },
    {
      "authors": [
        "Fred Hohman",
        "Haekyu Park",
        "Caleb Robinson",
        "Duen Horng Polo Chau"
      ],
      "title": "Summit: Scaling Deep Learning Interpretability by Visualizing Activation and Attribution Summarizations",
      "venue": "IEEE transactions on visualization and computer graphics 26,",
      "year": 2019
    },
    {
      "authors": [
        "Andreas Holzinger"
      ],
      "title": "From machine learning to explainable AI",
      "venue": "World Symposium on Digital Intelligence for Systems and Machines (DISA)",
      "year": 2018
    },
    {
      "authors": [
        "Mark Ibrahim",
        "Melissa Louie",
        "Ceena Modarres",
        "John Paisley"
      ],
      "title": "Global explanations of neural networks: Mapping the landscape of predictions",
      "venue": "In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",
      "year": 2019
    },
    {
      "authors": [
        "Christian P Janssen",
        "Stella F Donker",
        "Duncan P Brumby",
        "Andrew L Kun"
      ],
      "title": "History and future of humanautomation interaction",
      "venue": "International Journal of Human-Computer Studies",
      "year": 2019
    },
    {
      "authors": [
        "Minsuk Kahng",
        "Pierre Y Andrews",
        "Aditya Kalro",
        "Duen Horng Polo Chau"
      ],
      "title": "A cti v is: Visual exploration of industry-scale deep neural network models",
      "venue": "IEEE transactions on visualization and computer graphics 24,",
      "year": 2017
    },
    {
      "authors": [
        "Mark T Keane",
        "Eoin M Kenny"
      ],
      "title": "How case based reasoning explained neural networks: An XAI survey of post-hoc explanation-by-example in ANN-CBR twins",
      "year": 2019
    },
    {
      "authors": [
        "Frank C Keil"
      ],
      "title": "Explanation and understanding",
      "venue": "Annu. Rev. Psychol",
      "year": 2006
    },
    {
      "authors": [
        "Zafar A Khan",
        "Won Sohn"
      ],
      "title": "Abnormal human activity recognition system based on R-transform and kernel discriminant technique for elderly home care",
      "venue": "IEEE Transactions on Con sumer Electronics 57,",
      "year": 2011
    },
    {
      "authors": [
        "Youngwoo Kim",
        "James Allan"
      ],
      "title": "Unsupervised Explainable Controversy Detection from Online News",
      "venue": "In European Conference on Information Retrieval",
      "year": 2019
    },
    {
      "authors": [
        "Moritz K\u00f6rber",
        "Lorenz Prasch",
        "Klaus Bengler"
      ],
      "title": "Why do I have to drive now? Post hoc explanations of takeover requests",
      "venue": "Human factors 60,",
      "year": 2018
    },
    {
      "authors": [
        "Josua Krause",
        "Adam Perer",
        "Kenney Ng"
      ],
      "title": "Interacting with predictions: Visual inspection of black-box machine learning models",
      "venue": "In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems",
      "year": 2016
    },
    {
      "authors": [
        "Todd Kulesza",
        "Simone Stumpf",
        "Margaret Burnett",
        "Sherry Yang",
        "Irwin Kwan",
        "Weng-Keen Wong"
      ],
      "title": "Too much, too little, or just right? Ways explanations impact end users\u2019 mental models",
      "venue": "IEEE Symposium on Visual Languages and Human Centric Computing",
      "year": 2013
    },
    {
      "authors": [
        "Tai Yu Lai",
        "Jong Yih Kuo",
        "Yong-Yi Fanjiang",
        "Shang-Pin Ma",
        "Yi Han Liao"
      ],
      "title": "Robust little flame detection on real-time video surveillance system",
      "venue": "In 2012 Third International Conference on Innovations in Bio-Inspired Computing and Applications",
      "year": 2012
    },
    {
      "authors": [
        "Ellen J Langer",
        "Arthur Blank",
        "Benzion Chanowitz"
      ],
      "title": "The mindlessness of ostensibly thoughtful action: The role of\" placebic\" information in interpersonal interaction",
      "venue": "Journal of personality and social psychology 36,",
      "year": 1978
    },
    {
      "authors": [
        "Thibault Laugel",
        "Marie-Jeanne Lesot",
        "Christophe Marsala",
        "Xavier Renard",
        "Marcin Detyniecki"
      ],
      "title": "The dangers of post-hoc interpretability: Unjustified counterfactual explanations",
      "year": 2019
    },
    {
      "authors": [
        "Charles Layton",
        "Philip J Smith",
        "C Elaine McCoy"
      ],
      "title": "Design of a cooperative problem-solving system for en-route flight planning: An empirical evaluation",
      "venue": "Human Factors 36,",
      "year": 1994
    },
    {
      "authors": [
        "Tao Lei",
        "Regina Barzilay",
        "Tommi Jaakkola"
      ],
      "title": "Rationalizing Neural Predictions",
      "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, Austin, ACM Trans. Comput.-Hum. Interact.,",
      "year": 2016
    },
    {
      "authors": [
        "Timothy R Levine"
      ],
      "title": "An Overview of Detecting Deceptive Communication",
      "venue": "In The Palgrave Handbook of Deceptive Communication",
      "year": 2019
    },
    {
      "authors": [
        "Brian Y Lim",
        "Anind K Dey"
      ],
      "title": "Investigating intelligibility for uncertain context-aware applications",
      "venue": "In Proceedings of the 13th international conference on Ubiquitous computing",
      "year": 2011
    },
    {
      "authors": [
        "Brian Y Lim",
        "Anind K Dey",
        "Daniel Avrahami"
      ],
      "title": "Why and why not explanations improve the intelligibility of context-aware intelligent systems",
      "venue": "In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems",
      "year": 2009
    },
    {
      "authors": [
        "Zachary C Lipton"
      ],
      "title": "The mythos of model interpretability",
      "venue": "Queue 16,",
      "year": 2018
    },
    {
      "authors": [
        "Tania Lombrozo"
      ],
      "title": "Simplicity and probability in causal explanation",
      "venue": "Cognitive psychology 55,",
      "year": 2007
    },
    {
      "authors": [
        "Prashan Madumal",
        "Tim Miller",
        "Liz Sonenberg",
        "Frank Vetere"
      ],
      "title": "Explainable Reinforcement Learning Through a Causal Lens",
      "venue": "arXiv preprint arXiv:1905.10958",
      "year": 2019
    },
    {
      "authors": [
        "Gary Marcus"
      ],
      "title": "Deep learning: A critical appraisal",
      "venue": "arXiv preprint arXiv:1801.00631",
      "year": 2018
    },
    {
      "authors": [
        "Lili Meng",
        "Bo Zhao",
        "Bo Chang",
        "Gao Huang",
        "Frederick Tung",
        "Leonid Sigal"
      ],
      "title": "Where and When to Look? Spatio-temporal Attention for Action Recognition in Videos",
      "year": 2018
    },
    {
      "authors": [
        "Sina Mohseni",
        "Niloofar Zarei",
        "Eric D Ragan"
      ],
      "title": "A Multidisciplinary Survey and Framework for Design and Evaluation of Explainable AI Systems",
      "year": 2019
    },
    {
      "authors": [
        "Gr\u00e9goire Montavon",
        "Wojciech Samek",
        "Klaus-Robert M\u00fcller"
      ],
      "title": "Methods for interpreting and understanding deep neural networks",
      "venue": "Digital Signal Processing",
      "year": 2018
    },
    {
      "authors": [
        "Kevin Patrick Murphy",
        "Stuart Russell"
      ],
      "title": "Dynamic bayesian networks: representation, inference and learning",
      "year": 2002
    },
    {
      "authors": [
        "Donald A Norman"
      ],
      "title": "Design rules based on analyses of human error",
      "venue": "Commun. ACM 26,",
      "year": 1983
    },
    {
      "authors": [
        "Mahsan Nourani",
        "Donald R Honeycutt",
        "Jeremy E Block",
        "Chiradeep Roy",
        "Tahrima Rahman",
        "Eric D Ragan",
        "Vibhav Gogate"
      ],
      "title": "Investigating the Importance of First Impressions and Explainable AI with Interactive Video Analysis",
      "venue": "Extended Abstracts,",
      "year": 2020
    },
    {
      "authors": [
        "Mahsan Nourani",
        "Samia Kabir",
        "Sina Mohseni",
        "Eric D. Ragan"
      ],
      "title": "The effects of meaningful and meaningless explanations on trust and perceived system accuracy in intelligent systems",
      "venue": "Seventh AAAI Conference on Human Computation and Crowdsourcing",
      "year": 2019
    },
    {
      "authors": [
        "Gabriele Paolacci",
        "Jesse Chandler",
        "Panagiotis G Ipeirotis"
      ],
      "title": "Running experiments on amazon mechanical turk",
      "venue": "Judgment and Decision making 5,",
      "year": 2010
    },
    {
      "authors": [
        "Andrea Papenmeier",
        "Gwenn Englebienne",
        "Christin Seifert"
      ],
      "title": "How model accuracy and explanati-on fidelity influence user trust in AI",
      "venue": "In IJCAI Workshop on Explainable Artificial Intelligence",
      "year": 2019
    },
    {
      "authors": [
        "Raja Parasuraman",
        "Victor Riley"
      ],
      "title": "Humans and automation: Use, misuse, disuse, abuse",
      "venue": "Human factors 39,",
      "year": 1997
    },
    {
      "authors": [
        "Alyssa M Pena",
        "Ehsanul Haque Nirjhar",
        "Andrew Pachuilo",
        "Theodora Chaspari",
        "Eric D Ragan"
      ],
      "title": "Detecting Changes in User Behavior to Understand Interaction Provenance during Visual Data Analysis",
      "venue": "In IUI Workshops",
      "year": 2019
    },
    {
      "authors": [
        "Vitali Petsiuk",
        "Abir Das",
        "Kate Saenko"
      ],
      "title": "Rise: Randomized input sampling for explanation of black-box models",
      "venue": "arXiv preprint arXiv:1806.07421",
      "year": 2018
    },
    {
      "authors": [
        "Nicola Pezzotti",
        "Thomas H\u00f6llt",
        "Jan Van Gemert",
        "Boudewijn PF Lelieveldt",
        "Elmar Eisemann",
        "Anna Vilanova"
      ],
      "title": "Deepeyes: Progressive visual analytics for designing deep neural networks",
      "venue": "IEEE transactions on visualization and computer graphics 24,",
      "year": 2017
    },
    {
      "authors": [
        "T. Rahman",
        "P. Kothalkar",
        "V. Gogate"
      ],
      "title": "Cutset networks: A simple, tractable, and scalable approach for improving the accuracy of Chow-Liu trees",
      "venue": "In ECML PKDD",
      "year": 2014
    },
    {
      "authors": [
        "Michaela Regneri",
        "Marcus Rohrbach",
        "Dominikus Wetzel",
        "Stefan Thater",
        "Bernt Schiele",
        "Manfred Pinkal"
      ],
      "title": "Grounding action descriptions in videos",
      "venue": "Transactions of the Association for Computational Linguistics",
      "year": 2013
    },
    {
      "authors": [
        "Marco Tulio Ribeiro",
        "Sameer Singh",
        "Carlos Guestrin"
      ],
      "title": "Why should i trust you?: Explaining the predictions of any classifier",
      "venue": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
      "year": 2016
    },
    {
      "authors": [
        "Marco Tulio Ribeiro",
        "Sameer Singh",
        "Carlos Guestrin"
      ],
      "title": "Anchors: High-precision model-agnostic explanations",
      "venue": "In Thirty-Second AAAI Conference on Artificial Intelligence",
      "year": 2018
    },
    {
      "authors": [
        "Caleb Robinson",
        "Fred Hohman",
        "Bistra Dilkina"
      ],
      "title": "A deep learning approach for population estimation from satellite imagery",
      "venue": "In Proceedings of the 1st ACM SIGSPATIAL Workshop on Geospatial Humanities",
      "year": 2017
    },
    {
      "authors": [
        "Ericka Rovira",
        "Kathleen McGarry",
        "Raja Parasuraman"
      ],
      "title": "Effects of imperfect automation on decision making in a simulated command and control task",
      "venue": "Human factors 49,",
      "year": 2007
    },
    {
      "authors": [
        "Chiradeep Roy",
        "Mahesh Shanbhag",
        "Tahrima Rahman",
        "Vibhav Gogate",
        "Nicholas Ruozzi",
        "Mahsan Nourani",
        "Eric D Ragan",
        "Samia Kabir"
      ],
      "title": "Explainable Activity Recognition in Videos",
      "venue": "IUI ExSS workshop",
      "year": 2019
    },
    {
      "authors": [
        "O. Russakovsky",
        "J. Deng",
        "H. Su",
        "J. Krause",
        "S. Satheesh",
        "S. Ma",
        "Z. Huang",
        "A. Karpathy",
        "A. Khosla",
        "M. Bernstein",
        "A.C. Berg",
        "L. Fei-Fei"
      ],
      "title": "2015",
      "venue": "ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV) 115, 3 ",
      "year": 2015
    },
    {
      "authors": [
        "Dairazalia S\u00e1nchez",
        "Monica Tentori",
        "Jes\u00fas Favela"
      ],
      "title": "Activity recognition for the smart hospital",
      "venue": "IEEE intelligent systems 23,",
      "year": 2008
    },
    {
      "authors": [
        "Juergen Sauer",
        "Alain Chavaillaz",
        "David Wastell"
      ],
      "title": "Experience of automation failures in training: effects on trust, automation bias, complacency and performance",
      "venue": "Ergonomics 59,",
      "year": 2016
    },
    {
      "authors": [
        "James Schaffer",
        "John O\u2019Donovan",
        "James Michaelis",
        "Adrienne Raglin",
        "Tobias H\u00f6llerer"
      ],
      "title": "2019. I can do better than your AI: expertise and explanations",
      "venue": "In Proceedings of the 24th International Conference on Intelligent User Interfaces",
      "year": 2019
    },
    {
      "authors": [
        "Thilo Spinner",
        "Udo Schlegel",
        "Hanna Sch\u00e4fer",
        "Mennatallah El-Assady"
      ],
      "title": "explAIner: A Visual Analytics Framework for Interactive and Explainable Machine Learning. IEEE transactions on visualization and computer graphics (2019)",
      "year": 2019
    },
    {
      "authors": [
        "Nancy Staggers",
        "Anthony F. Norcio"
      ],
      "title": "Mental models: concepts for human-computer interaction research",
      "venue": "International Journal of Man-machine studies 38,",
      "year": 1993
    },
    {
      "authors": [
        "Miroslaw Staron",
        "Riccardo Scandariato"
      ],
      "title": "Data veracity in intelligent transportation systems: the slippery road warning scenario",
      "venue": "IEEE Intelligent Vehicles Symposium (IV)",
      "year": 2016
    },
    {
      "authors": [
        "Hendrik Strobelt",
        "Sebastian Gehrmann",
        "Michael Behrisch",
        "Adam Perer",
        "Hanspeter Pfister",
        "Alexander M Rush"
      ],
      "title": "2018. S eq 2s eq-v is: A visual debugging tool for sequence-to-sequence models",
      "venue": "IEEE transactions on visualization and computer graphics 25,",
      "year": 2018
    },
    {
      "authors": [
        "S Shyam Sundar",
        "Jinyoung Kim"
      ],
      "title": "Machine heuristic: When we trust computers more than humans with our personal information",
      "venue": "In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems",
      "year": 2019
    },
    {
      "authors": [
        "C. Szegedy"
      ],
      "title": "Googlenet pre-trained model. http://dl.caffe.berkeleyvision.org/bvlc_googlenet.caffemodel",
      "year": 2014
    },
    {
      "authors": [
        "C. Szegedy",
        "W. Liu",
        "Y. Jia",
        "P. Sermanet",
        "S. Reed",
        "D. Anguelov",
        "D. Erhan",
        "V. Vanhoucke",
        "A. Rabinovich"
      ],
      "title": "Going deeper with convolutions",
      "year": 2015
    },
    {
      "authors": [
        "Paolo Tamagnini",
        "Josua Krause",
        "Aritra Dasgupta",
        "Enrico Bertini"
      ],
      "title": "Interpreting black-box classifiers using instance-level visual explanations",
      "venue": "In Proceedings of the 2nd Workshop on Human-In-the-Loop Data Analytics. ACM,",
      "year": 2017
    },
    {
      "authors": [
        "Rajesh Kumar Tripathi",
        "Anand Singh Jalal",
        "Subhash Chand Agrawal"
      ],
      "title": "Suspicious human activity recognition: a review",
      "venue": "Artificial Intelligence Review 50,",
      "year": 2018
    },
    {
      "authors": [
        "Gilles Vandewiele",
        "Olivier Janssens",
        "Femke Ongenae",
        "Filip De Turck",
        "Sofie Van Hoecke"
      ],
      "title": "Genesim: genetic extraction of a single, interpretable model",
      "year": 2016
    },
    {
      "authors": [
        "Danding Wang",
        "Qian Yang",
        "Ashraf Abdul",
        "Brian Y Lim"
      ],
      "title": "Designing Theory-Driven User-Centric Explainable AI",
      "venue": "In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems",
      "year": 2019
    },
    {
      "authors": [
        "Junpeng Wang",
        "Liang Gou",
        "Wei Zhang",
        "Hao Yang",
        "Han-Wei Shen"
      ],
      "title": "DeepVID: Deep Visual Interpretation and Diagnosis for Image Classifiers via Knowledge Distillation",
      "venue": "IEEE transactions on visualization and computer graphics 25,",
      "year": 2019
    },
    {
      "authors": [
        "Xiang Wang",
        "Xiangnan He",
        "Fuli Feng",
        "Liqiang Nie",
        "Tat-Seng Chua"
      ],
      "title": "Tem: Tree-enhanced embedding model for explainable recommendation",
      "venue": "In Proceedings of the 2018 World Wide Web Conference. International World Wide Web Conferences Steering Committee,",
      "year": 2018
    },
    {
      "authors": [
        "Christine T Wolf"
      ],
      "title": "Explainability scenarios: towards scenario-based XAI design",
      "venue": "In Proceedings of the 24th International Conference on Intelligent User Interfaces",
      "year": 2019
    },
    {
      "authors": [
        "Bingjun Xie",
        "Jia Zhou"
      ],
      "title": "The Influence of Mental Model Similarity on User Performance: Comparing Older and Younger Adults",
      "year": 2017
    },
    {
      "authors": [
        "Bingjun Xie",
        "Jia Zhou",
        "Huilin Wang"
      ],
      "title": "How influential are mental models on interaction performance? exploring the gap between users\u00e2\u0102\u0179 and designers\u00e2\u0102\u0179 mental models through a new quantitative method",
      "venue": "Advances in Human-Computer Interaction",
      "year": 2017
    },
    {
      "authors": [
        "Serena Yeung",
        "Francesca Rinaldo",
        "Jeffrey Jopling",
        "Bingbin Liu",
        "Rishab Mehra",
        "N Lance Downing",
        "Michelle Guo",
        "Gabriel M Bianconi",
        "Alexandre Alahi",
        "Julia Lee"
      ],
      "title": "A computer vision system for deep learning-based detection of patient mobilization activities in the ICU",
      "venue": "NPJ digital medicine",
      "year": 2019
    },
    {
      "authors": [
        "Ming Yin",
        "Jennifer Wortman Vaughan",
        "Hanna Wallach"
      ],
      "title": "Understanding the Effect of Accuracy on Trust in Machine Learning Models",
      "year": 2019
    },
    {
      "authors": [
        "Tom Zahavy",
        "Nir Ben-Zrihem",
        "Shie Mannor"
      ],
      "title": "Graying the black box: Understanding dqns",
      "venue": "In International Conference on Machine Learning",
      "year": 2016
    },
    {
      "authors": [
        "Erin Zaroukian",
        "Jonathan Z Bakdash",
        "Alun Preece",
        "Will Webberley"
      ],
      "title": "Automation bias with a conversational interface: User confirmation of misparsed information",
      "venue": "IEEE Conference on Cognitive and Computational Aspects of Situation Management (CogSIMA)",
      "year": 2017
    },
    {
      "authors": [
        "Jiawei Zhang",
        "Yang Wang",
        "Piero Molino",
        "Lezhi Li",
        "David S Ebert"
      ],
      "title": "Manifold: A model-agnostic framework for interpretation and diagnosis of machine learning models",
      "venue": "IEEE transactions on visualization and computer graphics 25,",
      "year": 2018
    },
    {
      "authors": [
        "Yongfeng Zhang",
        "Guokun Lai",
        "Min Zhang",
        "Yi Zhang",
        "Yiqun Liu",
        "Shaoping Ma"
      ],
      "title": "Explicit factor models for explainable recommendation based on phrase-level sentiment analysis",
      "venue": "In Proceedings of the 37th international ACM SIGIR conference on Research & development in information retrieval",
      "year": 2014
    },
    {
      "authors": [
        "Bolei Zhou",
        "Alex Andonian",
        "Aude Oliva",
        "Antonio Torralba"
      ],
      "title": "Temporal relational reasoning in videos",
      "venue": "In Proceedings of the European Conference on Computer Vision (ECCV)",
      "year": 2018
    },
    {
      "authors": [
        "Martina Ziefle",
        "Susanne Bay"
      ],
      "title": "Mental models of a cellular phone menu. Comparing older and younger novice users",
      "venue": "In International Conference on Mobile Human-Computer Interaction",
      "year": 2004
    }
  ],
  "sections": [
    {
      "text": "Don\u2019t Explain without Verifying Veracity: An Evaluation of Explainable AI with Video Activity Recognition\nMAHSAN NOURANI, University of Florida CHIRADEEP ROY, University of Texas in Dallas TAHRIMA RAHMAN, University of Texas in Dallas ERIC D. RAGAN, University of Florida NICHOLAS RUOZZI, University of Texas in Dallas VIBHAV GOGATE, University of Texas in Dallas\nExplainable machine learning and artificial intelligence models have been used to justify a model\u2019s decisionmaking process. This added transparency aims to help improve user performance and understanding of the underlying model. However, in practice, explainable systems face many open questions and challenges. Specifically, designers might reduce the complexity of deep learning models in order to provide interpretability. The explanations generated by these simplified models, however, might not accurately justify and be truthful to the model. This can further add confusion to the users as they might not find the explanations meaningful with respect to the model predictions. Understanding how these explanations affect user behavior is an ongoing challenge. In this paper, we explore how explanation veracity affects user performance and agreement in intelligent systems. Through a controlled user study with an explainable activity recognition system, we compare variations in explanation veracity for a video review and querying task. The results suggest that low veracity explanations significantly decrease user performance and agreement compared to both accurate explanations and a system without explanations. These findings demonstrate the importance of accurate and understandable explanations and caution that poor explanations can sometimes be worse than no explanations with respect to their effect on user performance and reliance on an AI system.\nCCS Concepts: \u2022Human-centered computing\u2192 Empirical studies inHCI;User studies; \u2022Computing methodologies\u2192 Machine learning approaches.\nAdditional Key Words and Phrases: Explainable AI, Activity Recognition in Videos\nACM Reference Format: Mahsan Nourani, Chiradeep Roy, Tahrima Rahman, Eric D. Ragan, Nicholas Ruozzi, and Vibhav Gogate. 2020. Don\u2019t Explain without Verifying Veracity: An Evaluation of Explainable AI with Video Activity Recognition. ACM Trans. Comput.-Hum. Interact. 1, 1 (January 2020), 26 pages. https://doi.org/10.1145/XXX.XXX"
    },
    {
      "heading": "1 INTRODUCTION",
      "text": "Many machine learning models\u2014especially those induced using deep learning approaches\u2014are seen as black-boxes in that they do not allow users to understand \u201cwhy\u201d the system made a\nAuthors\u2019 addresses: Mahsan Nourani, mahsannourani@ufl.edu, University of Florida, Gainesville, Florida; Chiradeep Roy, cxr161630@utdallas.edu, University of Texas in Dallas, Dallas, Texas; Tahrima Rahman, tahrima.rahman@utdallas.edu, University of Texas in Dallas, Dallas, Texas; Eric D. Ragan, eragan@ufl.edu, University of Florida, Gainesville, Florida; Nicholas Ruozzi, nicholas.ruozzi@utdallas.edu, University of Texas in Dallas, Dallas, Texas; Vibhav Gogate, vibhav.gogate@ utdallas.edu, University of Texas in Dallas, Dallas, Texas.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. \u00a9 2020 Association for Computing Machinery. 1073-0516/2020/1-ART $15.00 https://doi.org/10.1145/XXX.XXX\nACM Trans. Comput.-Hum. Interact., Vol. 1, No. 1, Article . Publication date: January 2020.\nar X\niv :2\n00 5.\n02 33\n5v 1\n[ cs\n.H C\n] 5\nM ay\n2 02\n0\nparticular decision or produced a particular output [33, 42, 55]. At a high level, in order to maximize prediction accuracy, deep neural networks use a large number of hidden nodes where each hidden node represents a complex feature over the input attributes and other hidden nodes that it is connected to. A hidden node is activated when the feature it represents evaluates to \u201ctrue\u201d and thus in principle, it is possible to explain the decision made by a neural network by tracing the activated hidden nodes and constructing a (complex) feature over these nodes. Unfortunately, since the feature associated with the hidden node is likely to be complex, it will not be human interpretable. As a result, neural networks are unable to generate meaningful, human interpretable explanations for their decisions [55]. A lack of transparency can cause many problems for end users, as they would not know precisely how the outputs are generated, whether they are properly justified, and when they are wrong [22]. These challenges have motivated interest in explainable artificial intelligence (XAI), which seeks to bring more transparency to an intelligent system and make AI output and rationale more human understandable [24, 33, 87].\nTo date, a major focus of XAI research has been on introducing novel explanation types and then developing and evaluating models and algorithms for these new types [27, 48, 67]. Unfortunately, little focus has been given to studying the effect of explanations, including their quality or lack thereof on end users. In fact, in a recent survey, Addadi and Berrada [2] argue that 95% of the papers in the XAI community are focused on evaluating the accuracy of XAI algorithms rather than meaningful human use cases. As a result, advancements in XAI are overlooking fundamental knowledge of how explanations and human understanding of models affect user performance [30], trust [12, 17], and other behaviors when working with intelligent systems. This knowledge is crucial for designing effective XAI interfaces with meaningful, human understandable explanations, and to acquire such knowledge we need to conduct rigorous user studies to provide the empirical foundations for interpretability and trust in XAI systems [1]. Effective evaluation of XAI systems is challenging because it must not only assess how the addition of explanations can improve user understanding and trust in the system but also whether improvements in understanding allows users to work more efficiently [57]. Human evaluation should also aim to understand which aspects or types of explanations aid human understanding\u2014especially when several types of explanations are provided to the user. Some machine learning models may be inherently harder to explain than others [16, 21]. Explanation designs are often approximate representations rather than fully-detailed or perfectly accurate representations. It is important that explanations are truthful to the model in order to appropriately reflect the logic and rational behind its decision-making. In this paper, we refer to this alignment between truthful and accurate reflection of machine as veracity. While important, explanation veracity is not necessarily sufficient for a understandable and meaningful explanations. For end users, even explanations that accurately communicate model logic can sometimes be jarring if they cannot be interpreted as aligning with human logic or if they do not match expectations for how that logic would be portrayed [62, 64]. Meaningfulness and truthfulness are sometimes distinct properties of an explanation (i.e., an explanation can be truthful but not meaningful, or vice versa). Although, in other cases, this truthfulness can affect how meaningful users find the explanations, as well as users\u2019 understanding of the model. In this paper, we explore such phenomena through a study to better understand the impact of explanation veracity for end users. We study how the veracity of explanations can affect user task performance and system understanding through a controlled experiment conducted with an explainable activity recognition system for video. To circumvent the poor interpretability of neural networks, our system is built on a probabilistic model that takes the output of the neural network as input and models relationships between entities recognized by the neural network. The main virtue of this approach is that we can generate high quality explanations by reasoning over the probabilistic model [75], and this\nACM Trans. Comput.-Hum. Interact., Vol. 1, No. 1, Article . Publication date: January 2020.\nmethod provides an explainable model as the basis for human-subjects evaluation. By conducting an in-depth user evaluation with this XAI model, we are able to maintain experimental control of the presence and nature of available explanation information for our evaluation. At the same time, evaluating with a practical XAI implementation grounds the research in a real system with capabilities similar to those desired for real-world applications of activity recognition, e.g., security monitoring, medical analysis, or disaster response. The results of this study contribute empirical evidence of how high-veracity explanations can benefit human-machine operations while also validating concerns about the risks of inaccurate or non-meaningful explanations."
    },
    {
      "heading": "2 RELATEDWORK",
      "text": "In this section, we discuss the current body of work in the explainable AI literature with two motivations, one focusing on generating, refining, and combining AI/ML algorithms and the other, focusing on evaluating and studying the existing systems and models from a human viewpoint."
    },
    {
      "heading": "2.1 Explainable Machine Learning and Artificial Intelligence",
      "text": "Over the last two decades, machine learning and artificial intelligence (ML/AI) have fundamentally changed user experience by making computer systems smarter and more intelligent. In fact, a large number of existing ML/AI systems have achieved relative autonomy in that they can decide and act on their own with minimal human intervention. However, a major limitation of these existing systems is that they are black-boxes and cannot explain why they made a particular decision. As a result, users who interact with these autonomous systems on a daily basis are unable to understand and trust them, especially when they make a counter-intuitive decision. Therefore, recently there has been growing interest in building explainable artificial intelligence (XAI)\u2014specifically explainable machine learning\u2014systems. Notable examples include explainable recommendation systems (e.g., [10, 92, 101]), classification systems (e.g., [4, 40, 71, 91]), and activity recognition systems (e.g., [6, 56, 75, 102]). In this paper, we focused on an explainable activity recognition system for videos based on our previous work [75].\nVarious researchers in the ML/AI communities have explored explainability and interpretability techniques in ML models and systems. For instance, Du et al. [16] present a survey on different interpretability techniques, including post-hoc explanations (where explanations are extracted from the model from local or global perspectives, e.g., [23, 41, 46]), intrinsic explanations (where the model is self-explanatory and is built to be interpretable globally or locally, e.g., [89]), and model specific/agnostic explanations (e.g., [72]). Other researchers have explored explanation by example, where a model provides examples of relevant instances from the training set for a given input instead of attempting to explicitly explaining how the model\u2019s logic. For instance, Cai, Jongejan, and Holbrook [9] defined and explored two types of example-based explanations in the visual domain and investigated their effectiveness with humans: (1) normative explanations establish a norm/trend for the target class by showing training examples, which would help the users understand classifications, and (2) comparative explanations show the most similar examples (which can be of different classifications) from the training set to the input.\nResearchers also describe types of focus for what is being explained by different explanations. For example, Keane and Kenny [37] argue that transparency tries to reflect how an AI system works, while post-hoc interpretability focuses on the whys in the AI system, providing justification for its outputs. In another work, Hohman et al. [31] provide an interrogative survey on a large number of works in Deep Neural Network (DNN) visualization papers and organize the literature into six categorise based on how the visualization could reveal different aspects of a DNN. These categorise are: (1) Why visualize deep learning models? (e.g., [52, 58]), (2) What data, features,\nACM Trans. Comput.-Hum. Interact., Vol. 1, No. 1, Article . Publication date: January 2020.\nand relationships can be visualized? (e.g.,[26]), (3) When is visualization used in deep learning? (e.g., [36, 68]), (4) Who would use and benefit from visualization of deep learning?, (5) How to visualize data, features, and relationships? (e.g., [36]), and (6)Where has the visualization of deep learning been used? (e.g., [73, 98]. While these categories are presented for visualization of deep learning approaches, similar questions are relevant for all interpretable and explainable AI systems. In our work presented in this paper, our explainable system follows a similar approach and targets a critical explainable system (explainable video activity recognition), though it differs from much of the work from machine learning community in that we aim to evaluate and understand user behaviors with explainable systems rather than focus on improving the model itself. Next, we will discuss work on strategies for designing how to explain to human users and how explanations can affect humans."
    },
    {
      "heading": "2.2 Design and Human Factors in Explainable AI",
      "text": "A variety of different research communities have studied the design of user-centered XAI systems. In the visualization community, researchers have focused on building and designing tools for XAI systems in an attempt to improve understanding [80], analytical process [66, 100], debugging [83, 100], and fairness [3]. Work in the HCI community has resulted in user-centered design guidelines and frameworks for XAI systems [18, 90, 93], while others have compared and evaluated design variations for explanations to understand how they can improve user experience in an XAI system [38, 53]. Some HCI researchers focused on building empirical knowledge around explainable intelligent systems by studying user behaviors with such systems. Evaluation of user mental models of intelligent systems is one the topics explored in the HCI community. In the context of intelligent systems, mental model refers to the user\u2019s built and formed concept of how a system works in their mind from a period of working with the system. In 1993, Staggers and Norcio [81] argued that most of the research attempts infer the presence of mental models by comparing users\u2019 performances and observing differences in problem-solving between novice and expert users within a certain domain; however, it is hard in general to measure mental models. Some researchers focused on studying how transparency can improve user mental model of an intelligent system. Eiband et al. [18] proposed guidelines on how to improve the transparency in intelligent user interfaces by studying expert, user, and target mental models iteratively to understand what to explain to the users. More recently, Hoffman et al. [30] has proposed several different metrics such as trust, user-machine task performance, and mental model for user evaluation in XAI systems. In his paper, he lists multiple techniques to elicit mental models in XAI systems, e.g., think-aloud problem solving, card sorting, and prediction task (where users are presented with a test case and asked to predict the results of the system given the input). For instance, Kulesza et al. [43] studied how explanation soundness and completeness affect user mental model through the think-aloud approach. Here, we adapt a prediction task as used in several existing XAI user evaluations, e.g., [5, 54, 62], to assess user mental model. In other work, Nourani et al. [61] studied the order of observed system output and explanations on user mental model and found that order of encountering accurate or erroneous system predictions has a more significant effect on user mental model and performance than explanations.\nPrevious research in HCI has also been conducted to understand the relationship between user mental model and performance [95]. Some work has shown that a proper mental model of a system can have positive effects on user performance [15, 103], while other work shows a contradicting effect [7, 60]. Overall, the existing body of work in HCI and psychology does provide evidence that user performance and mental model are correlated [95]. How they are correlated, however, could depend on task, context, study population, or other external factors [94].\nACM Trans. Comput.-Hum. Interact., Vol. 1, No. 1, Article . Publication date: January 2020.\nHelping users build a proper mental model in intelligent systems is critical for various reasons. One goal, which also aligns with a goal of adding transparency to intelligent systems, is to help the users\u2019 trust-building with the system [20]. For instance, Yin et al. [97] found that users\u2019 observed system accuracy in intelligent systems can significantly affect user trust. While adding transparency and explanations can improve user trust in intelligent systems, it can introduce further issues with user behaviors. Bussone et al. [8] studied the relationships between explanations and trust and reliance through a study facilitated by a clinical decision support system. Their results demonstrate that more detailed explanations can improve user trust, though may also lead to over-reliance issues. However, given less detailed explanations had a counter effect and caused self-reliance.\nEither over-reliance or under-reliance can affect the quality of user experience and more importantly, could cause fatal errors in decision-making processes. One common example of over-reliance is automation bias [13], a psychological bias where users sometimes tend to default to trusting and relying on the outputs of an intelligent system when they become comfortable with the system\u2019s performance or think it \u201cseems smart\u201d. Automation bias can cause problems such as subconsciously ignoring or missing system errors (omission), and ignoring the contradictory factors in the decision-making process and following system suggestions (commission) [13, 65]. On the other hand, self-reliance and mistrust in an automated system can cause disuse, inadequate user performance, or disuse [65]. Both over-reliance (misuse) and self-reliance (disuse) are of particularly relevant as many intelligent systems target novice users, who might bring incorrect assumptions about the capabilities of intelligent systems [35]. Numerous studies have investigated automation bias in XAI systems. For example, Schaffer et al. [79] designed an experiment to test how presence of explanations, level of automation, and level of system error influences user\u2019s acceptance of advice from the intelligent system. Their results show explanations helped people with less task familiarity more, while showing explanations to users with higher task familiarity led to automation bias. Furthermore, Lim and Dey [50] found that more descriptive, detailed explanations can lead users to disagree with the system. This was an opposite effect from automation bias and might be helpful in avoiding such scenarios. In our experiment, we focus on learning more about how explanation quality and understandability can affect user agreement with the system and whether any effects interact with user reliance and trust."
    },
    {
      "heading": "2.3 Explanation Meaningfulness and Veracity",
      "text": "Explanations come in different formats, such as textual [8], confidence scores [8] and prediction accuracies [79], and saliency maps [14, 72]. They can also cover different scopes of model operation and logic.Global explanations try to provide an overview of how amodel generates its outputs [2, 32]. Many explainable systems designed for data experts have focused on visualizing models as global explanations [57]. For instance, Hohman et al. [32] built an interactive visual system to summarize and visualize deep-learning models and show how much each layer and what features were used to make predictions. Global explanations are beneficial as they might reveal biases, help diagnose model problems, and allow potential for changing hyper parameters [34]. A downside of global explanations is they are harder to achieve in practice, especially for complex or deep learning models [2]. In contrast to global explanation, local explanation aims to provide justification for particular outputs given specific instances of input. For instance, Bussone et al. [8] show users why a certain diagnosis was suggested based on the patient\u2019s symptoms and medical history. An individual local explanation generally does not give an overall view of the how the model works, but users can gain an understanding as they continue working with the system over time and view multiple instances. Depending on the task, instance-level explanation can be critical in aiding user\u2019s understanding of system output. Our study uses local explanations as we focus on explaining\nACM Trans. Comput.-Hum. Interact., Vol. 1, No. 1, Article . Publication date: January 2020.\nrelevant features of instances responsible for an output, and the goal was to study system and scenario that did not require any particular data expertise. One specific problem with local explanations is how they can affect user\u2019s perception of the system quickly. Hoffman [29] noted that it is easy to lose trust in automation, but harder to reestablish trust once it is lost. For that matter, poor or incorrect instance-level explanations can cause an immediate loss of trust. Hence, the quality of these explanations is of high importance. Researchers address aspects of \u201cquality\u201d of explanation from different perspectives. Recently, Papenmeier et al. [64] investigated how explanation presence and fidelity (i.e., \u201chow truthfully the explanation represents the underlying model\u201d) and system accuracy influence user trust. Utilizing two levels of high and low fidelity explanations, they found that system accuracy plays an important role in building user trust, while low fidelity explanations can potentially harm trust. Other work has explored \u201cnonsensical explanations\u201d, which by their definition, mean explanations that users cannot make sense of. One of the earliest uses of this term was by Langer et al. [45], when three behavioural field experiments led to the finding that people adhere to an explanation when it is more informative rather than senseless (nonsensical). More recently, Feng et al. [19] introduced a method of input reduction in their neural model that reduced explanations by removing unimportant words while maintaining its accuracy. However, their human evaluation showed that these shortened explanations and summaries confuse humans as they found them nonsensical, resulting in a drop in their task accuracy.\nOther research also considers howmeaningful explanations are for users. By some interpretations, explanations can be considered meaningful if they can be understood in alignment with human understanding and logic [62]. Hind et al. [28] argue that it is not possible to provide one single instance of explanation that is meaningful to everyone, as some factors regarding the users, such as their domain knowledge and level of sophistication, play an important role in designing explanations. They introduced a framework, so-called TED (Teaching Explanations for Decisions) to provide meaningful explanations that matches user mental models while maintaining prediction accuracy for the algorithm. Codella et al [11] explored and evaluated the TED framework further and demonstrated that machine learning approaches can incorporate meaningful explanations reliably, and in some cases, these explanations can be used to improve model accuracy.\nIn our priorwork [62], we conducted an experiment to investigate the role of human-meaningfulness in explanations for influencing users\u2019 perception of model accuracy. Using a wizard-of-Oz study with an image classification task, the study compared meaningful explanations with nonsensical or meaningless explanations intentionally created to misalign with natural human interpretation. The experiment compared meaningful and meaningless explanations alongside a control with no explanations, and it also controlled two levels of system accuracy through between-subjects user study. After viewing repeated instances of the (fictitious) system\u2019s classification for sample input images, participants performed a prediction task (see [30]) for a new set of images. The guessed accuracy from the prediction task was used as an implicit measure for perceived system accuracy, and participants also provided an explicit numerical estimation. For both measures of perception of accuracy, the results showed participants who are exposed to the nonsensical explanations significantly underestimated accuracy compared to both users with no explanations and meaningful explanations. These results\u2014taken along with existing empirical evidence that user perception of accuracy can directly affect user trust [97]\u2014suggest explanation meaningfulness can significantly affect user trust. In the current paper, we expand on the study of meaningful explanations with an experiment using a real system rather than a simulated scenario, and we also account for additional user outcomes such as user-task performance (as assessed in many XAI studies relevant work [50, 51, 74]). Highly relevant to meaningfulness is the concept of veracity (also called soundness), which we\nACM Trans. Comput.-Hum. Interact., Vol. 1, No. 1, Article . Publication date: January 2020.\nuse to refer to the extent that explanations are truthful to the reality of the system or model they explain [43, 82]. Veracity differs from explanation meaningfulness in that veracity is based on correctness in comparison to the underlyingmodel, whereas meaningfulness is based on comparison to human logic or thinking. Studying veracity in transportation systems as a component of big data, Staron and Scandariato [82] defined veracity as how data is truthful compared to reality, or the ability of the data \u201cto be free from lies\u201d. They believe data veracity is an important factor for relying on data accuracy and truthfulness. Thus, they define a list of characteristics where one could verify the veracity in big data. Other studies of veracity have investigated similar perspectives for information and data accuracy. For instance, Levine [49] defines truth accuracy as the ability of humans to detect honest information within a collection of information, and lie accuracy as the human ability to detect lies correctly in this collection. The author describes truth-bias, a phenomenon in which a human tends to judge information more honest when the truth accuracy is higher than the lie accuracy, and calls it a veracity effect. This may be similar to automation bias in intelligent systems."
    },
    {
      "heading": "3 EXPLAINABLE VIDEO ACTIVITY RECOGNITION SYSTEM",
      "text": "Our experiments were conducted using a custom-developed explainable activity recognition system for video. Activity recognition is an ideal test bed for XAI research due to its many potential realworld applications (e.g., fire detection [44], airport security [88], smart hospitals [77, 96], and elderly care [39]) and because most activity recognition systems involve a substantial human-computer interaction component. Our goal is to study system understanding and system effectiveness among a non-specialist population, i.e., users without any particular domain expertise or AI knowledge. That is why we chose to design a system to identify cooking activities in a kitchen setting.\nOur system outputs human understandable explanations using a two-layer architecture with a tractable, interpretable model on top of a deep, uninterpretable layer. We note that we presented a preliminary version of the system in an earlier workshop paper [75], but the current paper is based on an updated system and a completely new evaluation. In this section, we describe the dataset, model, and interface for the explainable activity recognition system used in the evaluation. For convenience, in the remainder of this paper, we will refer to this system as the XAI system."
    },
    {
      "heading": "3.1 Dataset",
      "text": "The model for the activity detection system was trained using a publicly-available video dataset, the Textually Annotated Cooking Scenes (TACoS) dataset [70], which consists of videos of a many different cooking-related activities. For example, a typical video will have a person take out a vegetable from the refrigerator, wash it, cut it, and then cook it. The cooking context has the advantage of being easily understandable, even without particular domain expertise. The dataset includes hand-annotated labels of actions, objects, and locations for each frame of video. We isolate 28 such labels and use videos with only these labels for our experiments. Most videos are around 2 minutes in length (although videos as long as 15 minutes are also present). For training, we used 60313 frames divided over 17 videos. For testing, we used a different set of 9355 frames over the same 17 videos."
    },
    {
      "heading": "3.2 Model",
      "text": "This section describes the model that was used for our experiments. We use a two-layered architecture whose high-level overview can be found in Figure 1. More complete system details can be found in the supplementary material.\nACM Trans. Comput.-Hum. Interact., Vol. 1, No. 1, Article . Publication date: January 2020.\nVideo Classification Layer\n3.2.1 Video Classification Layer. This layer takes a frame (image) as input and outputs a 0/1 label for each activity (1 indicates that the activity happened while a 0 indicates that it did not) in the frame. It is implemented using a 22-layer deep CNN architecture based on the BAIR/BLVC GoogleNet model [86]. This model is pre-trained [85] on the dataset used for the ILSVRC\u201915 object detection challenge [76]. This dataset contains over 500,000 images of different objects from around 200 categories. We added a soft-max layer on top that contains 28 nodes for our labels and then re-trained the network for our labeled data, i.e., the model was trained using the TaCOS video frames as inputs and the annotated ground labels as the desired outputs. The accuracy of the model with respect to the test set was measured using information retrieval metrics such as the Jaccard Index (higher is better) and the Hamming Loss (smaller is better). The video classification does not model temporal relationships between frames. Nevertheless, it yields a competitive Jaccard Index of 0.8608 and hamming loss of 0.1392.\n3.2.2 Explanation Layer. The explanation layer consists of two parts: (1) Training: The predicted labels from the Video Classification Layer are used to train a tem-\nporal Dynamic Conditional Cutset Network (DCCN). Similar to Dynamic Bayesian Networks (DBNs) [59], DCCNs are temporal probabilistic models whose emission and transition distributions are represented using Conditional Cutset Networks [69], instead of the Bayesian Networks used in DBNs. Since this layer models the temporal relationship between the predicted labels and the ground truth labels, it enables us to answer complex temporal inference\nACM Trans. Comput.-Hum. Interact., Vol. 1, No. 1, Article . Publication date: January 2020.\nqueries and explain the answers via probabilistic reasoning. It also achieves a higher Jacard index (0.8687) and a smaller Hamming loss (0.1200) than the deep net alone. (2) Query Resolution: The system allows users to enter a query about an activity composed of an action, object, and location. For example, a query of \u201cDid the person cut the orange on the plate?\u201d can be broken down into its corresponding activity tuple of (cut, orange, plate). The system then uses the explanation layer to search for frame segments that are the most likely to contain the activity that we are searching for. It also generates the top-k likely activities for each frame segment and ranks them in descending order of likelihood score (i.e., the most likely explanation will be ranked highest). The explanation information from this layer can then be presented in the system\u2019s visual interface."
    },
    {
      "heading": "3.3 Interface",
      "text": "The interface was designed to communicate the most relevant explanatory elements from the XAI activity recognition model for each given query and video input. The system was implemented with an interactive web-based interface. Figure 2 shows the components of the interface from the study (though available elements depended on the experimental conditions, as described in the following sections). To allow regular video viewing, the interface included a custom a video player with typical functionality (e.g., play/pause, stop, replay) and an interactive progress bar for jumping to any particular play time (see Figure 2a). To help explain to users the relevant segments of the video that\nACM Trans. Comput.-Hum. Interact., Vol. 1, No. 1, Article . Publication date: January 2020.\ncontributed to the XAI answer for each query, the video player showed key video segments directly under the progress bar. The width of each segment bar showed the length of the relevant segment at the appropriate time in the video. Figure 2a shows two segment bars for the query shown. Clicking on a segment bar would jump the video and progress bar to the start of that period of the video. Different queries could have different numbers of segments, where the currently-selected segment bar would be shown in orange to distinguish from the default blue color. Upon submitting a new query, the first segment of the video was selected by default.\nFor each selected segment, the system also displays more detailed explanation information from the model. Explanations included component scores, which are probability scores ranging from 0 to 1, which specify the contribution of every individual label (i.e., action, object, or location), in a segment for the most likely explanation from the explanation layer. Scores are shown graphically with simple bar representations (see Figure 2c). Additionally, the top three most likely combinations of components together are shown in a list of detected combination of components ordered from the most likely explanation at the top to the least likely at the bottom (see Figure 2d). While the full version of the system can process any user-entered query about actions, objects, and locations, the available functionality of the application was limited for purposes of the study. To ensure experimental control and consistency for what queries and explanations participants encountered in the user study, queries were pre-determined with a single yes-or-no query given per trial (see Figure 2b)."
    },
    {
      "heading": "4 EXPERIMENT",
      "text": "We conducted a controlled experiment to study how the inclusion and veracity of explanations in explainable systems affect user performance, mental model of the system, and user perception of system accuracy. The study was run using the system described in Section 3. In this section, we describe the experimental design and evaluation methodology."
    },
    {
      "heading": "4.1 Research Goals and Hypotheses",
      "text": "The primary motivation for this study was to understand how explanation veracity affects user performance and agreement with intelligent systems. Since one of the main goals of adding explanations to an intelligent system to improve understanding and, ultimately, to enhance user performance [30] (i.e., allowing users to complete a task faster and with less error), our evaluation prioritizes assessment of human task performance with the assistance of the system.\nWe summarize our goals with three research questions: \u2022 RQ1: Does the explainable AI model with full explanations improve user performance? \u2022 RQ2: How do presence and veracity of explanations affect user performance and user agreement with model output?\n\u2022 RQ3: How do presence and veracity of explanations affect perceived accuracy and mental model of intelligent systems?\nWhile RQ1 is dependent on the specifics of the XAI implementation, we argue for the importance of evaluating specific models and explanation designs before using those systems as the basis for investigating broader research questions, such as our questions about veracity. Furthermore, establishing empirical knowledge about the effects of explanations over a variety of systems and domains is also necessary to advance generalizable understanding of XAI effectiveness over broad contexts. Assessment of the effectiveness of our particular XAI system is also important when considering the additional research questions RQ2 and RQ3. If the explainable model used in the study is not effective and does not improve user performance, we cannot be confident that the design was sufficient or appropriate to provide a basis for comparison with system alternatives. We\nACM Trans. Comput.-Hum. Interact., Vol. 1, No. 1, Article . Publication date: January 2020.\ntherefore consider RQ1 to provide a baseline of XAI effectiveness with the intended design, and we hypothesized that the XAI system would significantly improve user performance compared to when no explanations or no AI model answers are provided.\nRQ2 and RQ3 focus on effects of the level of veracity of provided explanations. For RQ2, we hypothesized that lower veracity explanations would cause performance penalties compared to high veracity explanations. We also hypothesized that users would exhibit greater agreement of decisions with system answers when provided high veracity explanations than with low veracity explanations. We also questioned whether agreement without explanations would differ from cases with explanations since there is the possibility that the presence of any explanations could potentially persuade users to think that the system is functioning intelligently and correctly (that is, more details might appear impressive and convincing, thus artificially improving perception of machine ability).\nFinally, for RQ3, we hypothesized that users would have a higher perception of model accuracy and be able to more reliably predict system correctness when given higher veracity explanations compared to poor or no explanations. We also hypothesized that users with poor explanations might underestimate system accuracy significantly more than users with high veracity explanations. This hypothesis is informed by results observed in our previous user study [62] using a simple classification scenario and a wizard-of-oz approach, but our study investigates the extension to an actual XAI model with explanation veracity (rather than explanation meaningfulness) in a full system implementation."
    },
    {
      "heading": "4.2 Experimental Design",
      "text": "To address our research goals, we designed an experiment around a user task requiring participants to use the activity recognition system to review given queries about videos.\n4.2.1 User Task. The core of the experiment involved two consecutive tasks: a query review task and a user prediction task. The query review task was designed to assess whether the system affected how well participants could accurately determine the correct answer to queries with the aid of the system. Each trial involved review of a unique yes-no query about whether certain activities or content could be found in a given video of cooking activity from the TaCOS corpus.\nIn addition to providing user performance data for query assessment, the query review task also gave participants a chance to develop a mental model of the system from the period of use with multiple queries. After this period, users moved on to a prediction task to provide a measure of participant\u2019s mental model. For this task, participants were given a new set of queries and new videos, but the system did not produce output (neither query answers nor explanations). Participants were asked to predict what themodel would answer for each query. The prediction task (as suggested by others [30]) is a method for assessing a user\u2019s mental model of the system based on the ability to predict cases where the model provides correct answers and where it fails. Additionally, the predicted ratio of correct and incorrect responses provides an implicit approximation of the user\u2019s perceived model accuracy (as similarly done in our previous study [62]).\n4.2.2 Conditions. The study followed a between-subject design to control XAI system output as a single independent variable with four conditions:\n\u2022 No AI : The system did not provide the AI answers for the queries, and no explanatory information was given. The system did allow full access to the video player to inspect videos for queries, but without relevant segment highlights or component information. This was the only condition that did not include the AI output, and it was included as a baseline reference for human performance of the query review task.\nACM Trans. Comput.-Hum. Interact., Vol. 1, No. 1, Article . Publication date: January 2020.\n\u2022 No Exp: The system did show the AI answer to the query, but no explanatory information was given (i.e., no video segment highlights or component information). This condition served as a reference for human performance with the help of the AI answers to queries but without explanations. \u2022 High Veracity Exp: All intended functionality was provided as shown in Figure 2. The system provided the AI answer from themodel, alongwith the appropriate corresponding explanatory information (segment highlighting and component information) for the model\u2019s computation of that answer. \u2022 Low Veracity Exp: All intended functionality was provided as shown in Figure 2, and the system explanations for our study of explanation veracity. However, the explanatory information shown in this condition was not correct for the query. Model explanations (segment highlighting and component information) was generated based on other queries to simulate inaccurate explanations.\nThe experimental conditions only affected the version of the system available during the query review task but the prediction task was not affected since this was done to record data about user understanding and perception of the model; participants in all AI conditions completed the prediction task without AI query answers and without explanations. We also clarify that all conditions included use of the video player, but the highlighted segments were only included under the play bar for conditions with explanations (High Veracity Exp and Low Veracity Exp).\nWe also note that both the High Veracity Exp and Low Veracity Exp conditions included the same AI query answers from the model (i.e., the AI accuracy was identical), but the difference was the veracity of the explanations. While the High Veracity Exp condition showed the actual intended XAI explanations as generated by the model, the purpose of the Low Veracity Exp condition was to show inaccurate explanation information. To achieve this, incorrect explanations were simulated in the Low Veracity Exp condition by using explanations from the model for different queries. That is, we mixed-and-matched explanations for and among queries that were used for the study. We also used a priori experimenter review to make sure the assigned explanation was not obviously helpful or serendipitously highlighting relevant video content.\n4.2.3 Measures. Outcomes for the experiment included participant performance from the query review task in terms of completion time and error while determining the correct query answers with the aid of the system. Responses were recorded per trial and measures were calculated as the average of all trials per participant. To evaluate perceived usefulness and willingness to rely on model outputs, we considered whether participants\u2019 answers for the queries matched the system\u2019s given answer. The expectation is that participants demonstrating more trust and reliance in the system would be more likely to use the system\u2019s answer as their own answer. Additionally, for the two study conditions that included explanations, participants rated explanation usefulness on a 5-point scale from \u201cvery unhelpful\u201d to \u201cvery helpful\u201d (see Figure 2e).\nTo help assess perception and mental model of the system\u2019s AI, percentage of correct predictions from all trials of the prediction task served as an indicator of participant understanding of the system. In addition, the percentage of guessed correct/incorrect responses was used as an implicit measure of how accurate the participant perceived the system to be. Participants also provided a direct numerical estimation of system accuracy in a post-study questionnaire.\n4.2.4 Query and Data Configuration. The query review task and prediction task each had 20 queries (one query per trial). In each task, participants reviewed four unique videos with five queries for each video. In both the query review task and prediction task, participants completed the task for\nACM Trans. Comput.-Hum. Interact., Vol. 1, No. 1, Article . Publication date: January 2020.\nall queries one at a time for each video, though the order of queries for each video was randomly determined per participant. In addition, the video order was also randomized. To control the question composition to avoid any confounds, we used the same set of queries for all the conditions. Since our research goals included evaluating whether participants agreed with system results and could accurately assess model accuracy, it was necessary to ensure that all participants observed examples of the cases where the system produced both correct and incorrect answers to queries in the query review task. We therefore composed the set of queries for the study such that participants would observe incorrect outputs more often than would normally be expected (in other words, we artificially reduced the accuracy by intentionally controlling the number of queries with right and wrong model outputs). Of the 20 queries, the system gave correct query answers for 16 (i.e., 80% accuracy). This use of a constant, simulated accuracy provided consistency and ensured all participants had a sufficient number of observations of the system giving wrong and right answers.\nTo assist with quality assurance for the online study, we also included two additional trials (with the existing videos) as attention checks during the query review task. These trials affected neither the question composition and controlled system accuracy nor the final results. These attention checks were simple questions that did not require inspection of the videos or system output to answer correctly (e.g., \u201cIs the sky dark during the night?\u201d). These attention check questions were used to determine whether participants were reviewing queries and making an effort to answer correctly. If participants did not answer these attention checks correctly, we assumed they may not have been giving sufficient attention to the queries and task, and hence their data was not included for analysis."
    },
    {
      "heading": "4.3 Procedure",
      "text": "The experiment was run as a single-session online user study via Amazon Mechanical Turk (AMT). The research was approved by the organization\u2019s institutional review board (IRB), and the participants were compensated based on a fixed rate per hour. Average completion time depended on the assigned condition, with total time varying from approximately 25 to 55 minutes.\nACM Trans. Comput.-Hum. Interact., Vol. 1, No. 1, Article . Publication date: January 2020.\nFigure 3 provides a visual overview of the study procedure. Participants first viewed a consent form, followed by completing a background questionnaire asking about participants\u2019 age, gender, education level, occupation, and knowledge and understanding of AI and ML. After the questionnaire, participants were given a brief tutorial of the system and instructions for the query review task. Tutorials and instructions were crafted based on the assigned condition. In order to make the tutorials more engaging, the task was described in the context of a cooking competition scenario and assessing a system that could help in contest judging.\nThe participants then completed the query review task, which consisted of 20 queries. In addition to being asked to determine the correct answer for each query, participants in the High Veracity Exp and Low Veracity Exp conditions were also asked to rate whether they found the explanations helpful (see Figure 2e). After participants confirmed their response for each query answer, we showed the correct answer to provide feedback and to allow participants a clear indicator of whether the system\u2019s answer matched the true answer. After showing the correct answer, the system allowed participants to continue to the next trial. The users could spend as much time on each trial as they desired.\nAfter the query review task, participants took a short questionnaire about how much they used the different components and features of the interface and how helpful they found them. Participants then continued to the prediction task, which was not affected by the experimental condition. A new set of instructions were given before the 20 prediction trials (again with no time limit). No feedback about correct answers was available during the prediction task. Note that participants in the No AI condition did not complete the prediction task since they did not observe any model output, so there was no model for them to predict. After the prediction task, participants were asked to explicitly estimate the accuracy as well as answer various additional questions asking for general feedback about the experience and system."
    },
    {
      "heading": "4.4 Participants",
      "text": "We recruited a total of 160 AMT workers, with 40 workers per condition. After removing the data from the users who failed quality checks (described in the following section), we reached a total of 38, 40, 40, and 38 workers for High Veracity Exp, Low Veracity Exp, No Exp, and No AI . We did not reject any HITs that were submitted for the study, so all participants were paid regardless of outlier or quality removal. There were no significant difference with participants\u2019 age distribution among conditions (M = 41.26 and SD = 11.37). Also, 54 of the total participants were male and 64 female. The gender distribution among the conditions were not significantly different."
    },
    {
      "heading": "5 RESULTS",
      "text": "We analyzed the results from the study based on the measures from both performance and prediction tasks, as well as from questionnaires."
    },
    {
      "heading": "5.1 Pre-processing and Data Cleaning",
      "text": "Before analysis, we conducted data filtering to account for outliers and potential quality issues common in crowdsourced user studies [25, 63]. Data was filtered based on task completion time and errors. Completion was measured for each trial, and we first removed outlier trials based on time following the 1.5xIQR rule within each condition (i.e., we calculated \u00b11.5xIQR of every trial, and only included the results that fell within this range). Secondly, we again applied 1.5xIQR outlier filtering for performance error based on the percentage of times each participant answered a query wrong. From the outlier and quality filtering, two participants were discarded for having extremely poor performance (i.e., answering all tasks wrong, and having 85% of trials as high time outliers). Of the remaining participants, 1.25% were removed as outliers.\nACM Trans. Comput.-Hum. Interact., Vol. 1, No. 1, Article . Publication date: January 2020."
    },
    {
      "heading": "5.2 Query Review of Performance Results",
      "text": "We have two main goals for performance assessment. First, we want to evaluate whether or not the XAI system supports user performance improvements by providing AI answers both with and without explanations. This goal involves comparisons of the No AI , No Exp, and High Veracity Exp conditions, where No AI provides a reference for unassisted human performance. Second, we want to understand how explanation veracity affects performance. This goal is based on comparisons among the High Veracity Exp, Low Veracity Exp, and No Exp conditions. For this purpose, the No\nACM Trans. Comput.-Hum. Interact., Vol. 1, No. 1, Article . Publication date: January 2020.\nExp condition serves as a baseline to understand the effects of different types of explanations while preserving consistency in the same AI answers available among these three conditions.\nFor statistical analysis, the data for the performance measures did not meet the assumptions for parametric testing, therefore we used Kruskal-Wallis non-parametric tests for analysis of time and error comparing the four conditions. We conducted one test for time and one for error. Following significant results from the omnibus testing, we usedWilcoxon rank sum post-hoc tests for pairwise comparisons. Table 1 shows a summary of test outputs and significance for user performance results, and Figure 4 shows these results graphically. For both time and error metrics, participants who used the High Veracity Exp system had significantly better performance compared to both No Exp and No AI conditions. This means that the availability of AI answers alone did not result in significant performance improvements, but significant improvements were detected when the proper explanations were also added. This\nACM Trans. Comput.-Hum. Interact., Vol. 1, No. 1, Article . Publication date: January 2020.\nfinding informs RQ1 and provides significant evidence that the designed XAI system is effective at enhancing user performance.\nThe effect of explanation veracity was also significant. The High Veracity Exp group was significantly more accurate (fewer errors) than the Low Veracity Exp condition. Task completion time was not significantly different between these two conditions. Another important finding is that the Low Veracity Exp condition had significantly more errors than the from No Exp condition. Interestingly, performance in the Low Veracity Exp group was also significantly faster than No Exp.\nThese results serve as strong empirical evidence that appropriate explanations can be beneficial for user task performance with the assistance of an intelligent system. In this case, AI support was only beneficial when accurate explanations were also included (that is, AI output was not significantly helpful without explanations). Moreover, having inaccurate explanations resulted in worse performance than having no explanations."
    },
    {
      "heading": "5.3 Agreement with the System and Explanation Usefulness",
      "text": "By comparing user responses from the query review task to the system\u2019s answer for each query, we evaluated agreement between the participant\u2019s decision and the system\u2019s suggested answer. Note that this measure of agreement is only possible for conditions were the system provides an AI answer, and we reiterate that all conditions provided the same accuracy of given AI answers. Thus, we compared agreement percentages among High Veracity Exp, Low Veracity Exp, and No Exp conditions that included the model output for the query. We used a one-way independent ANOVA with Tukey HSD post-hoc testing. The main effect was significant, and all pairwise comparisons among the three conditions were significant. Table 2 shows the test results, and Figure 5a shows agreement results graphically.\nResults show the highest level of agreement in the High Veracity Exp condition while Low Veracity Expwas theworst. Consider theNo Exp condition as reference point, participants decided on answers aligning with the system\u2019s answers at a rate close to the 80% simulated system accuracy. Along with the performance results, this suggests that the appropriate explanations allowed participants to correctly identify system errors and disagree with the system when the system was wrong. Participants in the Low Veracity Exp condition showed overall lower agreement with the system\u2019s output, which suggests lower trust in the AI output even though the fraction of correct AI answers was the same. This suggests that poor explanations led participants to disagree with the system even in cases in which the AI answers were correct. This outcome is important as user disagreement with the system relates to perception of system accuracy and is an indication of reduced trust in the system. To provide a clearer mapping of this agreement behavior to perception of the explanations themselves, we also evaluated user\u2019s responses from the secondary question from each trial of the query review task (i.e., \u201cWas the explanation Helpful?\u201d ) for High Veracity Exp and Low Veracity Exp conditions. For analysis of overall perceived usefulness of explanations from the entire query review task, we calculated the percentage of explanations rated as useful by counting trials where participants indicated responses of agree or strongly agree for the question about usefulness. Results for this metric met the assumptions for parametric testing after applying a x2 transformation. A one-way independent ANOVA found a significant difference showing participants found low veracity explanations less useful. Table 2 shows the results."
    },
    {
      "heading": "5.4 Mental Model and Perception of Accuracy",
      "text": "As described in the Experimental Design section, a prediction task was used to measure user\u2019s mental model and perception of accuracy in the XAI system based on the question (i.e., \u201cWhat would the system\u2019s answer to this query be?\u201d and \u201cWould the system\u2019s answer be correct?\u201d). Participants\nACM Trans. Comput.-Hum. Interact., Vol. 1, No. 1, Article . Publication date: January 2020.\ncompleted this task in the three conditions where participants had AI output (No Exp, High Veracity Exp, and Low Veracity Exp). We evaluated prediction accuracy (i.e., rate of correctly guessing the model\u2019s answer) with one-way independent ANOVA, which did not show a significant difference (F (2, 115) = 0.64). This means we did not find evidence that explanation veracity and presence affected user ability to accurately predict the system\u2019s AI output.\nIn addition to analyzing correct predictions that matched the system\u2019s output, we also analyzed an implicit measure of perceived model accuracy based on the percentage of instances where participants predicted that the system would give correct answers. The implicit user perceived accuracy did not show a significant difference among the conditions (F (2, 115) = 0.40). We also explicitly asked participants to report a numerical estimation of the accuracy as a percentage. An ANOVA also did not detect a significant effect across the conditions (F (2, 115) = 1.9).\nThus, while veracity of explanations did help user assessment of system results enough to help significantly improve user performance, the understanding of the model was not affected enough to provide a significant effect on prediction of model outputs or accuracy in this task."
    },
    {
      "heading": "6 DISCUSSION",
      "text": "In this section, we discuss the results, implications for XAI design, and limitations of the research."
    },
    {
      "heading": "6.1 Results Interpretation",
      "text": "In this study, our main goal was to understand how the explanation veracity (i.e., the accuracy and capability of the explanations to justify the model\u2019s output) can affect user performance. We also wanted to understand how explanation veracity and presence would affect user\u2019s agreement with the system output, perception of the accuracy, and mental model of an XAI system. In order to make this comparison and generalize the findings for other XAI systems, we utilized a customized XAI system and analyzed if the explanations generated by the model can improve user performance. The results found evidence that users of this system had significantly better performance (i.e., were faster with less error).\nTo test the effects of veracity, we compared three conditions of high and low veracity explanations and no explanations. For this comparison, while the accuracy of the system answers remained constant (80%) across all conditions, the accuracy of explanations was controlled to be high (as determined by the model) for High Veracity Exp and low (crafted specifically to be inaccurate) for Low Veracity Exp conditions. Our results show user performance was significantly better with the model-generated high veracity explanations compared to low veracity explanations.\nFrom another perspective, performance times were always significantly faster with any explanations than without, though errors were significantly lower with high-veracity explanations while the addition of low-veracity explanations resulted in faster performance and more errors. Users were more error prone with low-veracity explanations even compared to cases with no explanations at all. This phenomenon suggests that the presence of explanations can encourage users to mistrust the system even when it is right. Another interpretation is that users found low veracity explanations less meaningful, resulting in their loss of trust and confidence in system outputs. This may have demotivated participants or promoted rushed and reduced effort. Additional studies are required in order to better understand the implications of effective and poor explanations. In either case, the results clearly indicate the importance of being conscientious and careful with including explanations before understanding whether the explanations seem meaningful to end users.\nFrom the agreement results, we can infer that the participants from Low Veracity Exp thought the system was wrong more times than the system actually was (see Figure 5a). This happened while the users in this condition also made more errors in their task compared to the other two conditions. Given that the output accuracy of the system was constantly 80% across the conditions with AI\nACM Trans. Comput.-Hum. Interact., Vol. 1, No. 1, Article . Publication date: January 2020.\nanswers available, the difference in agreement and error level is caused by explanation veracity. This means that given two systems with the same model but different explanations, users with less accurate (or even less \u201chuman meaningful\u201d) explanations might misunderstand the system, which is expected to cause mistrust in its output. Furthermore, users of such a system might falsely disagree with the system for the sole reason that the explanations do not make sense to them.\nAnother issues for consideration is whether users with Low Veracity Exp suffer from automation bias [8, 13, 78, 84, 99]. Automation bias is often times referred to as over-reliance on automated outputs [8], over-trusting output from automated systems even when it is wrong [47, 84]. The results show participants with Low Veracity Exp were faster and made more errors compared to No Exp based on the results of the query review task. At first glance, this result could look like automation bias since users were faster in agreeing with the system even though the system justifications were wrong, but the user agreement results refute such a conclusion. In this experiment, the performance measure captures when users both agree with the model when it is correct and disagree when it is wrong\u2014in other words, correctly identifying when to agree or disagree with the system (Bussone et al. [8] refer to this as right decisions). In contrast, the user agreement measure captures when users agree with the model\u2019s answer regardless of whether it was correct or incorrect. Since users with Low Veracity Exp had worse performance error and lower agreement, this means they were not over-relying on the system, but rather they were under-relying on its outputs.\nIn other words, given that participants in the Low Veracity Exp condition were significantly faster than the No Exp condition, they may have quickly disagreed with the system answer out of mistrust of the system\u2019s answers, as indicated by both lower agreement and greater error with low-veracity explanations. With poor explanations, participants lost trust in the system, and they were so untrusting that they were quick to discount the system\u2019s answer and instead were more likely to answer counter of the system\u2019s answer. This, as it turns out, is the opposite behavior as we would expect from automation bias. Since the model was usually correct (80%) and participants were more likely to disagree due to mistrust, the Low Veracity Exp group ended up with worse performance overall."
    },
    {
      "heading": "6.2 Implications for XAI Design and Evaluation",
      "text": "The findings from this experiment are important for designers who are interested in providing explanations for end users of intelligent systems. In our previous work [62], our main finding indicated that explanations that are not meaningful to the users lead to user underestimation of system accuracy. Previous work [97] showed that underestimation of system accuracy can directly influence user trust and can cause mistrust. Hence, combining these two findings, we can conclude that non-meaningful explanations can significantly affect user trust. Although the results of our current study did not show significant differences for user perception of accuracy across the conditions, the participants who observed low veracity explanations did not rely on the system, found explanations less meaningful, and more importantly, they even tended to disagree with the system even when it was correct on the prediction. These studies highlight the importance of both explanation meaningfulness and veracity together for affecting how much users rely on its predictions. Both can affect human-task performance and trust in the automated system. Especially since many XAI projects do not include human evaluations [2, 57], a major concern is that system designers might include explanations without first evaluating them, as developers might expect the presence of any explanations to help improve user performance. However, the results of this study suggest that veracity and perceived usefulness of explanations also play an important role in how added explanations affect user performance and behavior. Human evaluation is, thus, essential\nACM Trans. Comput.-Hum. Interact., Vol. 1, No. 1, Article . Publication date: January 2020.\nto make sure they are understood and working as intended. While such a claim is perhaps not surprising for an HCI audience, such empirical results are important for XAI system designers. The findings also highlight the importantance for consideration of the level of task complexity and user domain knowledge. For example, in explainable decision-support systems, it is critical that users, who may be domain experts such as medical professionals, security experts, or emergency responders, do not make serious or fatal errors in decision making. Disagreement with such systems could cause serious problems or even be fatal. For instance, given such explainable system with a high accuracy of 96%, providing low veracity explanations could cause a less experienced doctor to lose trust in the system\u2019s output and disagree with the output, which in this case is a diagnosis. These situations where the user disagreed with the diagnosis falsely could lead to a false diagnosis. Similarly, a more experienced user could lose trust and stop using an explainable system completely after observing the low veracity explanations. In this case, the user\u2019s task-performance might significantly decrease to a level compared to when there is no system available (without automation). In an example from prior literature, Bussone et al. [8] designed a study to test trust and reliance on explanations in a clinical decision support system, which was targeting less experienced healthcare practitioners without a specific specialty knowledge. Their results showed that more detailed explanations would cause over-reliance while little information can cause self-reliance on the automated system. Explanation meaningfulness and veracity might also affect a user\u2019s decision-making accuracy over time. This situation and similar situations could be avoided if the system designers approach with cautious when designing and providing explanations in an intelligent system.\nAnother important consideration is designing systems that would provide the option of improving system predictions and outcomes through user feedback. In these situations, both over-reliance and under-reliance can cause problems with changes of the model over time and its accuracy. With mistrust and under-reliance, users might assume the system is wrong even with the cases when it is not, thus causing them to provide feedback that would falsely change the model. Some of this feedback, depending on how much and when they are implemented in the system, could cause the model prediction and performance to change drastically. It could later affect user\u2019s decision accuracy, especially if they are new users and trying to establish trust, or if the users already over-rely on the model\u2019s outcomes. As our results show, explanation veracity can affect user reliance, and hence, designers of such systems should consider that low explanation veracity might eventually affect the accuracy and quality of their model."
    },
    {
      "heading": "6.3 Limitations and Opportunities",
      "text": "Although the study found significant effects of explanation types on human-machine task performance, the study did not detect evidence that the addition of explanations helped participants to develop a mental model accurate enough to improve responses in the prediction task. Taken together, these results indicate that although the XAI system was able to be understood sufficiently to judge whether the output was accurately meaningful and to determine when to rely on the output, it was not sufficient for forming an overall understanding of why the system was right when it was right or why it was wrong when it was wrong. We believe this is likely related to limitations on study duration and number of trials in the query performance phase. We aimed to keep the study time low (around 30 minutes) to accommodate online testing, but this also limited the number of trials a participant had to interact with a system. With the chosen design, we suspect participants did not have sufficient opportunities to observe incorrect model results. We suspect longer periods of interaction would have been needed to develop any type of meaningful mental model of the XAI model. It may be necessary to consider extended study times and observe more model weaknesses.\nACM Trans. Comput.-Hum. Interact., Vol. 1, No. 1, Article . Publication date: January 2020.\nFor future consideration, it may also be interesting to consider how user expertise and experience could affect their performance and how they interpret explanations in intelligent systems. The task in our study was intentionally designed such that completion did not require any specific domain knowledge. In some cases, users with domain knowledge might have different performance and mental model of the underlying AI system. Experience and understanding of ML/AI can also affect how users use a system and how they build a mental model of how it is working. Using AMT for participant recruiting, our experiment did not target participants with expertise or significant knowledge of AI. It would be interesting to conduct similar studies that also accounts for how the level of expertise in ML/AI affects user performance.\nAs advances in research contribute new knowledge of potential benefits and concerns of explainable systems, it is important to consider implications across different types of intelligent systems for different purposes. Testing the concepts we explored in this paper with different systems or with environments that require proficiency in a specific domain can bring about a deeper understanding and a more generalizable knowledge for the future of XAI systems. Our XAI system in this study was operating with component-level explanations at a high level \u2013 geared for end users \u2013 rather than low-level explanations with model details. While these results are generalizable for component-level explanations, future studies need to test these findings on low-level explanations. In other words, we expect generalization to depend on the understandability of the task and explanations, and more studies would be needed to assess different depth of explanation and alternate data contexts or explanation types. With continued developments in AI and machine learning approaches as well as with the evolution of public understanding and perception of intelligent systems, we argue for the importance of continuing empirical studies over a broad set of applications, models, and contexts. In addition, as part of building a strong empirical basis for our knowledge of the effects of explanations on human understanding and behavior, it is essential to study a variety of different forms of user tasks. We are interested in exploring more complex user tasks that involve more freedom and decision-making in how to use the AI and explanations to complete the tasks. While more open-ended user tasks increase variance in user activity and can introduce challenges for controlled comparisons of conditions, they can also serve as more realistic scenarios for study and can allow us to study when users are willing to rely on the system over time and when frustrations might arise."
    },
    {
      "heading": "7 CONCLUSION",
      "text": "Explainable AI systems aim to reduce problems of many of the black-box models for end users to develop appropriate understanding and trust in algorithms. However, if explanations lack accuracy or are not able to meaningfully describe the model, they might result in negative or unintended user behaviors. Motivated to understand the effects of such explanations on user behaviour, the presented user study evaluated differences in user performance, agreement, perception of accuracy, and mental model. The findings demonstrate that the quality of explanations can have significant\u2014 and potentially opposite\u2013effects on effectiveness and utility based on human interpretability. This research provides a clear case for the importance of evaluating explanation design through human-subjects evaluation in addition to emphasizing model accuracy and computational measures of explanation soundness alone. An explainable model whose explanations are not accurately describing its logic can cause several problems for its end-users, such as mistrust, lower performance, and lack of proper understanding of the model."
    },
    {
      "heading": "ACKNOWLEDGMENTS",
      "text": "This work was supported by the DARPA Explainable Artificial Intelligence (XAI) Program under contract number N66001-17-2-4032.\nACM Trans. Comput.-Hum. Interact., Vol. 1, No. 1, Article . Publication date: January 2020."
    }
  ],
  "title": "Don\u2019t Explain without Verifying Veracity: An Evaluation of Explainable AI with Video Activity Recognition",
  "year": 2020
}
