{
  "abstractText": "With the development of Internet of Things (IoT), IoT intelligence becomes emerging technology. \u201cCurse of Dimensionality\u201d is the barrier of data fusion in edge devices for the success of IoT intelligence. Deep learning has attracted great attention recently, due to the successful applications in several areas, such as image processing and natural language processing. However, the success of deep learning benefits from GPU computing. A Linguistic Attribute Hierarchy (LAH), embedded with Linguistic Decision Trees (LDTs) can represent a new attribute deep learning. In contrast to the conventional deep learning, an LAH could overcome the shortcoming of missing interpretation by providing transparent information propagation through the rules, produced by LDTs in the LAH. Similar to the conventional deep learning, the computing complexity of optimising LAHs blocks the applications of LAHs. In this paper, we propose a heuristic approach to constructing an LAH, embedded with LDTs for decision making or classification by utilising the distance correlations between attributes and between attributes and the goal variable. The set of attributes is divided to some attribute clusters, and then they are heuristically organised to form a linguistic attribute hierarchy. The proposed approach was validated with some benchmark decision making or classification problems from the UCI machine learning repository. The experimental results show that the proposed self-organisation algorithm can construct an effective and efficient linguistic attribute hierarchy. Such a self-organised linguistic attribute hierarchy embedded with LDTs can not only efficiently tackle \u2018curse of dimensionality\u2019 in a single LDT for data fusion with massive attributes, but also achieve better or comparable performance on decision making or classification, compared to the single LDT for the problem to be solved. The self-organisation algorithm is much efficient than the Genetic Algorithm in Wrapper for the optimisation of LAHs. This makes it feasible to embed the self-organisation algorithm in edge devices for IoT intelligence. Dr Hongmei He is with Cranfield University, Uk, email:h.he@cranfield.ac.uk or maryhhe@gmail.com Dr Zhenhuan Zhu is with Smart Perception Lab, UK, email:spl.z.zhu@outlook.com or zhenhuan2001@hotmail.com Preprint submitted to Information Fusion June 9, 2020",
  "authors": [
    {
      "affiliations": [],
      "name": "Hongmei He"
    },
    {
      "affiliations": [],
      "name": "Zhenhuan Zhu"
    }
  ],
  "id": "SP:eb73152f341599acc8eb93a5e458ed1da83bdc88",
  "references": [
    {
      "authors": [
        "A.L. Blum",
        "R.L. Rivest"
      ],
      "title": "Training a 3-Node Neural Network is NP-Complete",
      "venue": "Neural Networks,",
      "year": 1992
    },
    {
      "authors": [
        "G.U. Raju",
        "J. Zhou",
        "R.A. Kiner"
      ],
      "title": "Hierarchical fuzzy control",
      "venue": "Int. J. Control,",
      "year": 1991
    },
    {
      "authors": [
        "Campello RJGB",
        "Amaral WC"
      ],
      "title": "Hierarchical Fuzzy Relational Models: Linguistic Interpretation and Universal Approximation",
      "venue": "IEEE Transactions on Fuzzy Systems,",
      "year": 2006
    },
    {
      "authors": [
        "W Pedrycs"
      ],
      "title": "Fuzzy Control and Fuzzy Systems, 2nd Ed",
      "year": 1993
    },
    {
      "authors": [
        "J Lawry",
        "H He"
      ],
      "title": "Multi-Attribute Decision Making Based on Label Semantics, the International Journal of Uncertainty, Fuzziness and Knowledge- Based Systems",
      "venue": "supp, pp",
      "year": 2008
    },
    {
      "authors": [
        "H He",
        "J Lawry"
      ],
      "title": "Optimal Cascade Hierarchies of Linguistic Decision Trees for Decision Making",
      "venue": "in Proc. of IAENG International Conference on Artificial Intelligence and Applications (ICAIA\u201909),",
      "year": 2009
    },
    {
      "authors": [
        "H He",
        "J Lawry"
      ],
      "title": "Optimal Cascade Linguistic Attribute Hierarchies for Information Propagation",
      "venue": "IAENG International Journal of Computer Science,",
      "year": 2009
    },
    {
      "authors": [
        "H. He",
        "J. Lawry"
      ],
      "title": "Linguistic Attribute Hierarchy and Its Optimisation for Classification Problems",
      "venue": "Soft Computing,",
      "year": 2014
    },
    {
      "authors": [
        "H. He",
        "Z. Zhu",
        "E. Makinen"
      ],
      "title": "Task-oriented Distributed Data Fusion in Autonomous Wireless Sensor Networks",
      "venue": "Soft Computing,",
      "year": 2015
    },
    {
      "authors": [
        "S. Mitra",
        "S.K.K.M. Konwar"
      ],
      "title": "Pal,Fuzzy decision tree, linguistic rules and fuzzy knowledge-based network: Generation and evaluation",
      "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part C: Applications and Reviews,",
      "year": 2002
    },
    {
      "authors": [
        "H. He",
        "Z. Zhu",
        "A. Tiwari",
        "Andrew Mills"
      ],
      "title": "A Cascade of Linguistic CMAC Neural Networks for Decision Making",
      "venue": "Proceedings of the International Joint Conference on Neural Networks (IJCNN2015),",
      "year": 2015
    },
    {
      "authors": [
        "H. He",
        "T. Watson",
        "C. Maple",
        "J. Mehnen",
        "A. Tiwari"
      ],
      "title": "Semantic Attribute Deep Learning with A Hierarchy of Linguistic Decision Trees for Spam Detection, IJCNN2017",
      "venue": "Anchorage, Alaska,",
      "year": 2017
    },
    {
      "authors": [
        "M. Lichman"
      ],
      "title": "UCI Machine Learning Repository [http://archive.ics.uci.edu/ml",
      "year": 2013
    },
    {
      "authors": [
        "Y. Miao",
        "M. Gowayyed",
        "F. Metze"
      ],
      "title": "EESEN: End-to-End Speech Recognition using Deep RNN Models and WFST-based Decoding",
      "venue": "CoRR, abs/1507.08240,",
      "year": 2015
    },
    {
      "authors": [
        "Z. Zhang",
        "J. Geiger",
        "J. Pohjalainen",
        "A.E.-D. Mousa",
        "W. Jin",
        "B. Schuller"
      ],
      "title": "Deep Learning for Environmentally Robust Speech Recognition: An Overview of Recent Developments, arXiv:1701.02720v1 [cs.CL",
      "year": 2017
    },
    {
      "authors": [
        "K. He",
        "X. Zhang",
        "S. Ren",
        "J. Sun"
      ],
      "title": "Deep Residual Learning for Image Recognition",
      "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "year": 2016
    },
    {
      "authors": [
        "R. Girshick",
        "J. Donahue",
        "T. Darrell",
        "J. Malik"
      ],
      "title": "Rich feature hierarchies for accurate object detection and semantic segmentation",
      "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "year": 2014
    },
    {
      "authors": [
        "S. Ren",
        "K. He",
        "R. Girshick",
        "J. Sun"
      ],
      "title": "Faster R-CNN: Towards Real- Time Object Detection with Region Proposal Networks",
      "venue": "Journal IEEE Transactions on Pattern Analysis and Machine Intelligence,",
      "year": 2017
    },
    {
      "authors": [
        "E. Gawehn",
        "J.A. Hiss",
        "G. Schneider"
      ],
      "title": "Deep Learning in Drug Discovery",
      "venue": "Molecular Informatics,",
      "year": 2016
    },
    {
      "authors": [
        "Y. Park",
        "M. Kellis"
      ],
      "title": "Deep learning for regulatory genomics",
      "venue": "Nature Biotechnology",
      "year": 2015
    },
    {
      "authors": [
        "L. Chen",
        "G. Papandreou",
        "I. Kokkinos",
        "K. Murphy",
        "A.L. Yuille"
      ],
      "title": "DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs",
      "venue": "CoRR, abs/1606.00915,",
      "year": 1606
    },
    {
      "authors": [
        "A. Karpathy",
        "G. Toderici",
        "S. Shetty",
        "T. Leung",
        "R. Sukthankar",
        "F.F. Li"
      ],
      "title": "Largescale Video Classification with Convolutional Neural Networks, IEEE Conference on Computer Vision and Pattern Recognition",
      "year": 2014
    },
    {
      "authors": [
        "W. Zhang",
        "M. Zhai",
        "Z. Huang",
        "C. Liu",
        "W. Li"
      ],
      "title": "Towards End-to-End Speech Recognition with Deep Multipath Convolutional Neural Networks,In",
      "venue": "Intelligent Robotics and Applications. ICIRA 2019. Lecture Notes in Computer Science,",
      "year": 2019
    },
    {
      "authors": [
        "H. Lee",
        "Y. Largman",
        "P. Pham",
        "A.Y. Ng"
      ],
      "title": "Unsupervised feature learning for audio classification using convolutional deep belief networks",
      "venue": "Proc. of the 22nd International Conference on Neural Information Processing Systems (NIPS\u201909),",
      "year": 2009
    },
    {
      "authors": [
        "Y. Taigman",
        "M. Yang",
        "M. Ranzato",
        "L. Wolf"
      ],
      "title": "Deepface: closing the gap to human-level performance in face verification",
      "venue": "In Proc. Conference on Computer Vision and Pattern Recognition,",
      "year": 2014
    },
    {
      "authors": [
        "V.D.P.N. Druzhkov"
      ],
      "title": "Kustikova, A survey of deep learning methods and software tools for image classification and object detection, Pattern Recognition and Image Analysis",
      "year": 2016
    },
    {
      "authors": [
        "C. Szegedy",
        "A. Toshev",
        "D. Erhan"
      ],
      "title": "Deep neural networks for object detection",
      "venue": "in Proc. of NIPS (Lake Tahoe,",
      "year": 2013
    },
    {
      "authors": [
        "29] K. Simonyan",
        "A. Vedaldi",
        "A. Zisserman"
      ],
      "title": "Deep fisher networks for largescale image classification",
      "venue": "Proc. of NIPS (Lake Tahoe, 2013), pp. 163\u2013171. 24",
      "year": 2013
    },
    {
      "authors": [
        "30] A. Krizhevsky",
        "I. Sutskever",
        "G.E. Hinton"
      ],
      "title": "ImageNet classification with deep convolutional neural networks",
      "venue": "Proc. of NIPS (Lake Tahoe, 2012), pp. 1097\u20131105.",
      "year": 2012
    },
    {
      "authors": [
        "31] Y. He",
        "K. Kavukcuoglu",
        "Y. Wang",
        "A. Szlam",
        "Y. Qi"
      ],
      "title": "Unsupervised feature learning by deep sparse coding",
      "venue": "Proc. of SIAM International Conference on Data Mining (Philadelphia, 2014), pp. 902\u2013910.",
      "year": 2014
    },
    {
      "authors": [
        "G.E. Dahl"
      ],
      "title": "Deep learning approaches to problems in speech recognition, computational chemistry, and natural language text processing",
      "venue": "PhD thesis, University of Toronto,",
      "year": 2015
    },
    {
      "authors": [
        "Abdel-rahman Mohamed",
        "George E. Dahl",
        "Geoffrey E. Hinton"
      ],
      "title": "Acoustic modeling using deep belief networks. Audio, Speech, and Language Processing",
      "venue": "IEEE Transactions on,",
      "year": 2012
    },
    {
      "authors": [
        "G.E. Dahl",
        "Tara N. Sainath",
        "Geoffrey E. Hinton"
      ],
      "title": "Improving deep neural networks for LVCSR using rectified linear units and dropout",
      "venue": "In ICASSP,",
      "year": 2013
    },
    {
      "authors": [
        "D. Yu",
        "L. Deng"
      ],
      "title": "Automatic Speech Recognition \u2013 A Deep Learning Approach, 2015",
      "year": 2015
    },
    {
      "authors": [
        "R. Socher",
        "A. Perelygin",
        "J.Y. Wu",
        "J. Chuang",
        "C.D. Manning",
        "A.Y. Ng",
        "C. Potts"
      ],
      "title": "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank",
      "venue": "Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,",
      "year": 2013
    },
    {
      "authors": [
        "Z. Qin",
        "J. Lawry"
      ],
      "title": "Decision tree learning with fuzzy labels",
      "venue": "Information Sciences,",
      "year": 2005
    },
    {
      "authors": [
        "J. Lawry"
      ],
      "title": "Appropriateness Measures: An Uncertainty Measure for Vague Concepts",
      "venue": "Synthese, Vol. 161,",
      "year": 2008
    },
    {
      "authors": [
        "J Lawry"
      ],
      "title": "Modeling and Reasoning with Vague Concepts",
      "year": 2006
    },
    {
      "authors": [
        "R.C. Jeffrey"
      ],
      "title": "The Logic of Decision",
      "year": 1990
    },
    {
      "authors": [
        "G.J. Sz\u00e9kely",
        "M.L. Rizzo"
      ],
      "title": "Brownnian Distance Covariance",
      "venue": "The Annals of Applied Statistics,",
      "year": 2009
    },
    {
      "authors": [
        "JR Quinlan"
      ],
      "title": "Induction of Decision Trees",
      "venue": "Machine Learning,",
      "year": 1986
    },
    {
      "authors": [
        "T.A. Almeida",
        "J.M.G. Hidalgo",
        "A. Yamakami"
      ],
      "title": "Contributions to the study of sms spam filtering:new collection and results",
      "venue": "In DocEng\u201911, Mountain View, California,",
      "year": 2011
    },
    {
      "authors": [
        "H. He",
        "A. Tiwari",
        "J. Mehnen",
        "T. Watson",
        "C. Maple",
        "Y. Jin",
        "B. Gabrys"
      ],
      "title": "Incremental Information Gain Analysis of Input Attribute Impact on RBF-Kernel SVM Spam Detection, WCCI2016",
      "year": 2016
    },
    {
      "authors": [
        "D. Justus",
        "J. Brennan",
        "S. Bonner",
        "A. S",
        "McGough"
      ],
      "title": "Predicting the Computational Cost of Deep Learning Models",
      "venue": "IEEE International Conference on Big Data,",
      "year": 2018
    },
    {
      "authors": [
        "T. Ching",
        "D.S. Himmelstein"
      ],
      "title": "Opportunities and obstacles for deep learning in biology and medicine",
      "venue": "Journal of the Royal Society Interface,",
      "year": 2018
    },
    {
      "authors": [
        "X. Yuan",
        "P. He",
        "Q. Zhu",
        "X. Li"
      ],
      "title": "Adversarial Examples: Attacks and Defenses for Deep Learning",
      "venue": "IEEE Transactions on Neural Networks and Learning Systems,",
      "year": 2019
    }
  ],
  "sections": [
    {
      "text": "ar X\niv :2\n00 6.\n04 76\n6v 1\n[ cs\n.A I]\nWith the development of Internet of Things (IoT), IoT intelligence becomes emerging technology. \u201cCurse of Dimensionality\u201d is the barrier of data fusion in edge devices for the success of IoT intelligence. Deep learning has attracted great attention recently, due to the successful applications in several areas, such as image processing and natural language processing. However, the success of deep learning benefits from GPU computing. A Linguistic Attribute Hierarchy (LAH), embedded with Linguistic Decision Trees (LDTs) can represent a new attribute deep learning. In contrast to the conventional deep learning, an LAH could overcome the shortcoming of missing interpretation by providing transparent information propagation through the rules, produced by LDTs in the LAH. Similar to the conventional deep learning, the computing complexity of optimising LAHs blocks the applications of LAHs.\nIn this paper, we propose a heuristic approach to constructing an LAH, embedded with LDTs for decision making or classification by utilising the distance correlations between attributes and between attributes and the goal variable. The set of attributes is divided to some attribute clusters, and then they are heuristically organised to form a linguistic attribute hierarchy. The proposed approach was validated with some benchmark decision making or classification problems from the UCI machine learning repository. The experimental results show that the proposed self-organisation algorithm can construct an effective and efficient linguistic attribute hierarchy. Such a self-organised linguistic attribute hierarchy embedded with LDTs can not only efficiently tackle \u2018curse of dimensionality\u2019 in a single LDT for data fusion with massive attributes, but also achieve better or comparable performance on decision making or classification, compared to the single LDT for the problem to be solved. The self-organisation algorithm is much efficient than the Genetic Algorithm in Wrapper for the optimisation of LAHs. This makes it feasible to embed the self-organisation algorithm in edge devices for IoT intelligence.\n1Dr Hongmei He is with Cranfield University, Uk, email:h.he@cranfield.ac.uk or maryhhe@gmail.com 2Dr Zhenhuan Zhu is with Smart Perception Lab, UK, email:spl.z.zhu@outlook.com or zhen-\nhuan2001@hotmail.com\nPreprint submitted to Information Fusion June 9, 2020\nKeywords: IoT Intelligence in Edges, Linguistic Attribute Deep Learning, Linguistic Decision Tree, Semantics of a Linguistic Attribute Hierarchy (LAH), Distance Correlation Clustering, Self-Organisation of an LAH"
    },
    {
      "heading": "1. Introduction",
      "text": "The Internet-of-Things (IoT) provides us with a large amount of sensor data. However, the data by themselves do not provide value unless we can turn them into actionable and/or contextualized information. Big data analysis allows us to gain new insights by batch-processing and off-line analysis. Currently, a microprocessor-based sensor node can support many channels (e.g. a Microchip processor can support up to 49 channel inputs [1]). Real-time sensor data analysis and decision-making is preferably automated on-board of IoT devices, which will make IoT intelligence towards reality.\nAlthough the computing capability of a microprocessor has improved very much, the \u2018curse of dimensionality\u2019 is still a big challenge in data driven machine intelligence, as the computing complexity of designed model function increases as the increasing of input space. Blum and Rivest [2] have proved that training a 2-layer, 3-nodes and n inputs neural network is NP-Complete. Obviously, the big barrier of blocking the applications of deep-learning is the computing complexity, although it shows great attractive on solving complex nonlinear problems. With the strong capability of GPU, deep learning for 2-20 depth networks is successful (e.g. Google AlphaGo). To save the cost, an edge device of IoT systems may not need to equip with GPUs, if the machine intelligence algorithm inside the device is efficient enough. Hence, the performance improvement of computational intelligence is continuous work, especially for creating effective and efficient computing model in edge devices to implement IoT intelligence.\nLinguistic decision tree (LDT), a probabilistic tree, has been well used for decision making and classification problems. Given an input space of n attributes, each of which can be described with limit labels. An LDT consists of a set of attribute nodes and a set of edges. An edge, linking from an attribute node, represents a label expression that describes the attribute node (see Section 3.2). However, the branch number of a decision tree exponentially increases as the number of input attributes increases. This shortcoming of an LDT greatly blocks its applications.\nA hierarchical model could help overcome the \u201cCurse of Dimensionality\u201d in fuzzy rule-based learning [3]. Campello and Amaral [4] provided a cascade hierarchy of submodels, using fuzzy relational equations [5], and unilaterally transformed the cascade models into the mathematically equivalent non-hierarchical one. Lawry and He[6] proposed a linguistic attribute hierarchy (LAH). It is a hierarchy of LDTs, each of which has a reduced input space, and represents different functions in the problem space. However, as the relationship between inputs and the output in the whole problem space could be strong non-linear and uncertain, different LAHs will have different performance for the problem to be solved. All of the research in [4, 6] neither investigated the performance of the proposed hierarchies, nor studied how the hierarchies can be constructed optimally.\nHe and Lawry [7, 8] investigated cascade LAHs, and developed a genetic wrapper algorithm (GWA) to optimise the cascade LAHs. It was shown that a cascade hierarchy can derive much less rules, compared to the single decision tree. However, the accuracy tends to drop as decision threshold increases. Later, they also investigated the optimisation of generic LAHs [9]. The optimisation of LAHs using the GWA is NP-complete, as the branch number of a decision tree is increasing exponentially as the input attribute number increases, and the convergency of GWA directly affects the speed of optimisation. Hence, the evolutionary algorithm takes very long time to optimise LAHs, even if the evolution takes a small number of iterations. When the attribute number is over 60, the time of evolution process on a PC is not acceptable.\nAnother issue of hierarchical approaches, including generic deep learning, is the lack of linguistic interpretability, especially neural networks have been viewed as a black box. This is because that intermediate variables, being arbitrarily introduced to interconnect the sub-models, do not present any meanings in a real system, and hence it is difficult to give a clear intuitive interpretation [4]. However, in an LAH, information propagation is completed via the linguistic rules extracted from LDTs. Hence, a transparent hierarchical decision making or classification can be provided.\nIn a distributed system, we may get partial information from different information resources, based on which, an initial estimate/decision could be done locally. Correspondingly, a collective decision is required. How do these information resources make contribution to the final collective decision? In an LAH, an intermediate variable is equivalent to the initial decision in terms of partial information [7, 8]. Hence, the structure of the hierarchy determines the process of decision or fusion process. He et. al. [10] further proposed an off-line optimisation of sensor arrangement for taskoriented sensor fusion, aligning with the structure of an self-organised wireless sensor network.\nMitra et al.[11] proposed a fuzzy knowledge-based network, whose structure can be automatically optimised by using linguistic rules of a fuzzy decision tree, thus to help in reducing the complexity of network training for a specific task. An LAH is equivalent to a forward neural network, in which, each neuron is the function of lower layer attributes, but represented by an LDT. The structure of an LAH is different to that of a classic multi-layer feed-forward neural network, in which, all input attributes are in the input layer. In an LAH, input attributes can feed a neuron (e.g. an LDT) at any layer, but for the simplicity of network structure, they cannot repeatedly feed more than one neuron.\nThe information propagation from bottom to top of an LAH through the LDTs in the LAH provides a new approach to attribute deep learning. In fact, a neural network or any other machine learning models can be used, instead of an LDT in the LAH. For example, a cascade of CMACs has been proposed in [12]. However, LAH embedded with linguistic decision trees could provide good interpretation for decision making or classification.\nHe et al. [13] used an LAH to interpret the process of semantic attribute deep learning for spam detection. The LAH was constructed manually in terms of the semantics of attributes. For IoT intelligence or edge computing, the onboard adaptive sensor fusion is a critical challenge, due to the limit of resources in edge devices (e.g. computing capacity, memory and even power supply). Hence, efficiently automatic constructing a\nlinguistic attribute hierarchy becomes necessary for this purpose.\nIn this research, a self-organised LAH (SOLAH), embedded with LDTs is proposed for the deep learning of attribute semantics in decision making or classification. A distance correlation-based clustering algorithm is proposed to decompose attributes to several clusters, and a linguistic attribute hierarchy is constructed with the produced clusters in terms of the average distance correlation of cluster members to the goal variable. The LDT, fed by a cluster of attributes with lower distance correlation, will be placed in the lower layer of the LAH. The preliminary experiments are conducted on the SMS spam database from UCI machine learning repository [14]. A set of databases will be used to validate the performance of the SOLAHs. The experimental results are compared with that of the single LDTs for different databases."
    },
    {
      "heading": "2. Existing work in Deep Learning",
      "text": "Recently, Deep Learning has been put a lot of attention by researchers. It uses multiple processing layers of computational models to learn representations of data with multiple levels of abstraction. Each successive layer is fed by the outputs of previous layer, forming a hierarchy of attributes from bottom to top. Deep learning uses the back-propagation algorithm to learn internal parameters that are used to compute the representation in each layer from the representation in the previous layer, thus to find the hidden structure or information in large data sets. Various deep learning architectures such as deep neural networks, convolutional deep neural networks, deep belief networks and recurrent neural networks have been developed. Deep learning has been successfully applied in the areas of speech recognition [15, 16, 24], image processing [17], object detection [18, 19], drug discovery [20] and genomics [21], etc. Especially, deep convolutional nets (ConvNets) have brought about breakthroughs in processing images [22], video [23], speech [24] and audio [25], whereas recurrent nets have shone light on sequential data such as text and speech [26].\nConvNets are constructed layer by layer for data processing. A classic application of a ConvNet is in image processing, utilising the properties of a colour image, consisting of the RGB channels of pixel values. Since the early 2000s, ConvNets have been successfully applied for the detection, segmentation and recognition of objects and regions based on images, for example, face recognition [27].\nDruzhkov and Kustikova [28] did a survey on deep learning methods for image classification and object detection, covering autoencoders, restricted Boltzmann machines and convolutional neural networks. For example, Szegedy et al. [29] used deep neural networks to solve the problem of object detection for both classifying and precisely locating objects of various classes; Simonyan et al. [30] used deep fisher networks for the classification of large-scale images from ImageNet (http://image-net.org/); Krizhevsky et al. [31] trained a large deep convolutional neural network to classify the 1.3 million high-resolution images in the LSVRC-2010 ImageNet training set into the 1000 different classes; and He et al. [32] proposed an unsupervised feature learning framework, Deep Sparse Coding, which extends sparse coding to a multi-layer architecture for visual object recognition tasks.\nAcoustic modeling is another area, where large, deep neural networks have been successfully applied. Like ImageNet, the massive quantities of existing transcribed\nspeech data provide rich resources for deep learning. Dahl [33] provided a brief review of the deep neural net approach to large vocabulary speech recognition (LVSR) in his thesis. Mohamed et al.[34] showed that hybrid acoustic models of pre-trained deep neural networks, instead of Gaussian mixture models (GMMs), could greatly improve the performance of a small-scale phone recognition; By using rectified linear units and dropout, Dahl et al. [35] further improve the model for a large vocabulary voice search task. A combination of a set of deep learning techniques has led to more than 1/3 error rate reduction over the conventional state-of-the-art GMM-HHM (Hidden Markov Model) framework on many real-world LVCSR tasks [36].\nThe qualitative properties of text data, very different to that of other modality data, provide critical challenges in use of machine learning. Recently Socher et al. [37] developed recursive deep models for semantic compositionality over a sentiment treebank, and improved the accuracy. Natural language understanding is another exploration of deep learning application, which could make a large impact over the next few years [26].\nThe property of compositional hierarchies of some signals is well exploited by deep neural networks through composing lower-level features to abstract the high-level one. For example, an image can be represented by a hierarchy from local edges, motifs, parts, to objects. Similarly, speech and text also have a hierarchy from sounds to phones, phonemes, syllables, words and sentences. In other words, this properties promote the capacities of deep neural networks. However, the interpretability of the decision making process is still an issue. Also, we cannot always explicitly see the semantics of higher-level features in other application domains.\nWith rapid progress and significant successes in a wide spectrum of applications, deep learning is being applied in many safety-critical environments. Ching et al. [47] forecasted that deep learning enabling changes at both bench and bedside with the potential to transform several areas of biology and medicine, although the limited amount of labelled data for training presents problems as well as legal and privacy constraints on work with sensitive health records. However, deep neural networks (DNNs) have been recently found vulnerable to well-designed input samples called adversarial examples, and adversarial perturbations are imperceptible to human but can easily fool DNNs in the testing/deploying stage [48]. The transparency of LAH allows the observation of the decision process. He et al. [13] have investigated the effect of different input attributes on different positions in a linguistic attribute hierarchy. This might indicate a linguistic attribute deep learning could provide good transparency to help people to defend adversarial perturbations to IoT Intelligence, which is out of the scope of this research.\nGPU has become the necessary hardware facility for the research on deep learning due to its complexity. Justus et al. [46] analysed various factors that influence the complexity of deep learning, and divided these factors to three categorises: (1) Layer features, such as activate function, optimiser of weights (e.g. Gradient Descent) and number of training samples; (2) layer specific features, including four subcategories: i. Multi- Layer Perception features (e.g. number of inputs, number of neurons and number of layers), ii. Convolutional features (e.g. matrix size, kernel size, stride size, input padding, input depth and output depth, etc.); iii. Pooling features (e.g. matrix size, stride size, input padding), iv. Recurrent features (e.g. Recurrence type,\nbi-directionality); (3) Hardware Features (e.g. GPU technology, GPU count, GPU memory, GPU memory bandwidth, GPU clock speed, GPU cord count, GPU peak performance, Card Connectivity). The first two categories of factors determine the performance of the function represented by a deep learning model, while the GPU feature directly affect the efficiency of the computing. It is hard to express the polynomial relationship between the computing cost and all the various factors, or use a big O(*) to represent the computing complexity."
    },
    {
      "heading": "3. Linguistic attribute deep learning with a LAH",
      "text": ""
    },
    {
      "heading": "3.1. A linguistic attribute hierarchy (LAH)",
      "text": "The process of aggregation of evidence in multi-attribute decision making or classification based on attributes x1, ..., xn can be represented with a functional mapping y = f(x1, ..., xn), where y is the goal variable. However, this mapping is often dynamic and uncertain, and it is difficult to find a mathematic equation to precisely describe the mapping function. An attribute hierarchy represents the function f with a hierarchy of sub-functions, each of which represents a new intermediate attribute. The set of original attributes {x1, ..., xn} is categorised into m clusters s1, ..., sm. When a cluster of attributes is used to make initial decision, an new intermediate attribute is produced by the function of the clustering attributes. One or more intermediate attributes can be combined with another cluster of attributes to make further decision in next level, or multiple intermediate attributes can be directly used to make decision in next level. As there are m attribute clusters, at least m subfunctions are produced. Namely, zi = Gi(si) for i = 1, ...,m and n \u2212 1 \u2265 m, as the maximal level of hierarchy with n input attributes is the cascade hierarchy with n \u2212 1 subfunctions. The mapping function f is represented by a new function F of the intermediate attributes z1, .., z\u03c4 and/or a cluster (si) of input attributes. The intermediate attributes can be represented by subfunctions of G1...G\u03c4 , fed by lower level of attribute set, which could include intermediate attributes and/or another cluster sj of input attributes. Hence, y = f(x1, ..., xn) = F (z1, ..., z\u03c4 , si) = F (G1(S1),...,G\u03c4 (S\u03c4 ), si).\nA linguistic decision tree (LDT) can explicitly model both the uncertainty and vagueness in a mapping system, and linguistic rules extracted from the LDT often implies our knowledge of aggregation. When an LAH uses LDTs to represent the functional mappings between parent and child attribute nodes, the semantic information will be transparently propagated through the hierarchy from bottom to top. Assume the goal variable y \u2208 \u2126y , and a set Ly of labels can be used to describe the goal variable. An introduced intermediate attribute z will represent an approximate of the goal variable y. Namely, z \u2208 \u2126y , and the label set Ly can be used to describe z. Hence, an LAH can present a hierarchical decision making, and it can provide transparent linguistic interpretation, which helps perform semantic attribute deep learning.\nFor examples, assume five attributes are fed to an LAH. Fig. 1 (a) shows a cascade LAH, embedded with 4 LDTs, through which, the decision information is cascaded from the bottom model LDT1 to the top model LDT4; Fig. 1 (b) illustrates a general LAH, embedded with 3 LDTs, through which, the final decision is made by LDT3, based on the two intermediate attributes from LDT1 and LDT2 in the bottom of the\nLAH, and the additional attribute x5. The intermediate attributes z1 and z2 represent an approximate of the goal variable y, z1 is decided by x1 and x2, and z2 is decided by x3 and x4."
    },
    {
      "heading": "3.2. A Linguistic decision tree based on Label Semantics",
      "text": "An LDT [38, 39] is a probabilistic tree under the framework of label semantics [40]. Label semantics introduces two fundamental and interrelated measures: Appropriateness measure (\u00b5L(x)) and mass assignment (\u03bdx), where, \u00b5L(x) quantifies how a label L is appropriate to describe x based on agent\u2019s knowledge of the current labeling conventions, shown in the collected samples or evidences; \u03bdx quantifies how a particular subset of labels is appropriate to describe x. The particular subset of all and only the labels that are appropriate to describe x is called focal set, denoted as F . Given a set of labels L = {s,m, l}, the focal set can be F = {{s}, {s,m}, {m}, {m, l}, {l}}.\nAn LDT consists of a node set V and an edge set E. A node v \u2208 V is associated to an attribute, and an edge e \u2208 E from a node is a focal set, which is appropriate to describe the attribute, indicated by the node. A path from the top node to a leaf of the LDT is called a branch, denoted as B. The branch length (l) is the number of nodes on the branch B. Each branch provides a conjunction of focal sets: F1\u2227 ... \u2227Fl, with the inference of a set of mass assignments conditional to the branch B, denoted as \u03bdy(F |B), for each focal set F \u2208 Fy, where, Fy is a set of focal sets that are appropriate to describe different values distributed in the domain of the goal variable y. The semantics of an LDT is described in Definition 3.1.\nDefinition 3.1 (Semantics of an LDT). The rule derived from a branch Bi in an LDT is presented as:\nFi1 \u2227 ... \u2227 Fil \u2192 F : \u03bdy(F |Bi), (1)\nwhere, Fik is the k-th focal set on branch Bi, and Fik \u2208 Fxik . Given attribute values ~x = (x1, . . . , xn). The mass assignment \u03bdy can be obtained using Jeffrey\u2019s rule [41]:\n\u03bdy(F ) =\n\u03b2 \u2211\ni=1\n\u00b5Bi(~x)\u03bdy(F |Bi), F \u2208 Fy (2)\nwhere \u03b2 is the branch number of the LDT, and \u03bdy(F |Bi) is equivalent to the conditional probability p(F |Bi). xij denotes the j-th attribute on the i-th branch. Assume a focal set Fij is appropriate to describe attribute xij on the branch Bi. The appropriateness measure of ~x on branch Bi is the product of all mass assignments on focal sets that are appropriate to describe the corresponding nodes on the branch, as expressed in Formula (3).\n\u00b5Bi(~x) =\nl \u220f\nj=1\n\u03bdxij (Fij ). (3)"
    },
    {
      "heading": "4. The semantics of a linguistic attribute hierarchy",
      "text": "When an LAH is represented a hierarchy of LDTs, the information is propagated through the LDTs from information sources (e.g. sensors) in the bottom to the decision variable on the top. Therefore, the rules can be derived as conditional expressions through Formula 1 in the label semantics framework. The output variable (either an intermediate attribute or a goal variable) of each LDT in the LAH can be calculated with Formula (2). This provides an approach to quantifying the degree of our belief how each focal set F \u2208 Fy is appropriate to describe the goal, given partial input attributes and/or the previous results of decision or classification (intermediate attributes) in lower level.\nNow, we use the LAH in Fig. 1 (b) as an example to demonstrate the upwards information propagation through an LAH. In Fig. 1 (b), two LDTs (LDT1, LDT2) are located in the bottom of the LAH. y = f(z1, z2, x5) = f(g1(x1, x2), g2(x3, x4), x5). The mappings g1, g2, and f are represented in the form of linguistic decision trees (i.e. LDT1, LDT2 and LDT3). LDT1 provides the function of quantifying our belief of z1 on goal labels in terms of input attributes x1 and x2, LDT2 presents the function of quantifying our belief of z2 on goal labels in terms of input attributes x3 and x4, and LDT3 offers the function of quantifying our final belief of the goal variable y on its labels in terms of lower level believes of z1 and z2, as well as another input attribute x5 from information source (e.g. sensors). Assume the input attributes are clustered to \u03ba subsets s1, ...s\u03ba, which can feed the intermediate attributes or/and the goal variable in an LAH. As an intermediate attribute zi \u2208 \u2126y is the approximate of the goal variable y, it can be estimated by a decision making model (e.g. LDT), fed with si. Hence, the decision making model can be trained with the samples of (Si, y). Given the values of all input attributes ~x = (x1, . . . , xn), the mass assignments of all intermediate attributes and the goal variable can be estimated with Formula (2) through all the LDTs in the LAH. Namely, the decision information is propagated through the bottom LDTs to the top LDT in the LAH. The semantics of an LAH is defined as Definition 4.1.\nDefinition 4.1 (Semantics of an LAH). The semantics of an LAH is the synthetisation of rules extracted from the branches in the LAH, which are allocated by the given sample, ~x = (x1, . . . , xn). Assume k outputs (i.e. k intermediate attributes) of LDT(t1) ... LDT(tk) are the inputs of LDT(ti), the rule will be B t1 \u2227 ...\u2227Btk \u2192 Bti , where Bt1\n... Btk can be derived in the form of Formula (1), by an LDT, either directly based on information sources (i.e. the input attributes), or based on the intermediate attributes from the lower level of LDTs.\nFor the instance of Fig. 1 (b), given all the input attribute values, ~x = {x1, x2, x3, x4, x5} in all samples. For generality, we use \u2113 to denote the label expression, associated to an edge in an LDT. For the special case, the edges of an LDT are associated to a focal set, \u2113 represents a focal set F . Hence, the semantics of each decision tree can be described as:\nFor LDT1(x1, x2), as it is fed with two attributes, the maximum branch length is 2. Hence, the rule corresponding to the branch B1i can be:\n\u21131i1 \u2227 \u2113 1 i2 \u2192 F : \u03bdy(F |B 1 i ), F \u2208 Fy.\nSimilarly, for LDT2(x3, x4), the rule corresponding to the branch B 2 j can be:\n\u21132j1 \u2227 \u2113 2 j2 \u2192 F : \u03bdy(F |B 2 j ), F \u2208 Fy.\nFor LDT3(z1, z2, x5), the maximum branch length is 3. Hence, the rule corresponding to the branch B3k can be:\n\u21133k1 \u2227 \u2113 3 k2 \u2227 \u21133k3 \u2192 F : \u03bdy(F |B 3 k), F \u2208 Fy.\nThe synthetic semantics of the LAH is:\n(\u21131i1 \u2227 \u2113 1 i2 ) \u2228 (\u21132j1 \u2227 \u2113 2 j2 ) \u2192 \u21133k1 \u2227 \u2113 3 k2 \u2227 \u21133k3\n\u2192 F : \u03bdy(F |B 3 k), F \u2208 Fy."
    },
    {
      "heading": "5. Construction of Linguistic Attribute Hierarchy",
      "text": "The basic idea for the construction of a Linguistic Attribute Hierarchy includes two steps: (1) Decomposition of attributes with a distance correlation based clustering algorithm; (2) Self-organisation of a linguistic attribute hierarchy in terms of the distance correlation between clustered attributes and the goal variable."
    },
    {
      "heading": "5.1. Attribute decomposition based on their distance correlation",
      "text": ""
    },
    {
      "heading": "5.1.1. Distance correlation",
      "text": "Distance correlation can be used to statistically measure the dependence between two random variables or vectors, which could have different dimensions. It is zero if and only if the random variables are statistically independent. Assume (xi, yi), i = 1, 2, ..., n be a set of samples. The n by n distance matrices (ai,j) and (bi,j) present the all pairwise distances for x and y in all the samples, respectively.\nai,j =\u2016 xi \u2212 xj \u2016; bi,j =\u2016 yi \u2212 yj \u2016 . (4)\nwhere || \u2022 || denotes Euclidean norm. Then take all doubly centered distances:\nAi,j = ai,j \u2212 a\u0304i. \u2212 a\u0304.j + a\u0304.., Bi,j = bi,j \u2212 b\u0304i. \u2212 b\u0304.j + b\u0304.., (5)\nwhere, a\u0304i. is the i-th row mean, a\u0304.j is the j-th column mean, and a\u0304.. is the grand mean of the distance matrix for the x samples. The notation is similar for the b values on the y samples. The squared sample distance covariance (a scalar) is simply the arithmetic average of the products Ai,j and Bi,j :\ndCov2n(x, y) = 1\nn2\nn \u2211\ni=1\nn \u2211\nj=1\nAi,jBi,j . (6)\nThe sample distance variance is the square root of\ndV ar2n(x) = dCov 2 n(x, x) =\n1\nn2\n\u2211\ni,j\nA2i,j , (7)\nThe distance correlation of two random variables can be calculated through dividing their distance covariance by the product of their distance standard deviations, as Fomula (8).\ndCorr(x, y) = dCov(X,Y ) \u221a\ndV ar(x) dV ar(y) . (8)\nDistance correlation has the properties:\n(1) 0 \u2264 dCorrn(x, y) \u2264 1, 0 \u2264 dCorr(x, y) \u2264 1;\n(2) The distance correlation matrix is symmetric (i.e. dCorr(i, j) = dCorr(j, i)), and dCorr(i, i)=1.\n(3) dCorr(x, y) = 0, if and only if x and y are independent;\n(4) dCorrn(x, y) = 1 and dCorr(x, y) = 1 indicates that the linear subspaces spanned by x and y samples respectively almost surely have an identical dimension [42]."
    },
    {
      "heading": "5.1.2. Clustering attributes based on distance correlation",
      "text": "As we do not need to consider the distance correlation between an attribute and itself, we set the diagonal of the distance correlation matrix to zeros for the convenience of computing. First, we find the maximum value in the distance correlation matrix dCorr. The maximum value is denoted as dmax, the column of the maximum value in the matrix is denoted as imax. Here we set a value \u03b1 as the range of distance correlation difference in a cluster. Namely, the distance correlation of attributes in a cluster, correlating to attribute imax will be in (dmax \u2212 \u03b1, dmax]. A feasible approach to setting the value of \u03b1 is:\n(max(dCorr) \u2212min(dCorr))/k, (9)\nUsually, min(dCorr) is not a zero, and k is the preset number of clusters. Secondly, all relevant columns and rows, where the identified cluster members in the cluster are located, are set to zeros. The process is repeated with starting to find next attribute with largest distance correlation in the unvisited attributes for the next cluster, until all elements of dCorr are zeros. Algorithm 1 provides the pseudo-code of the clustering algorithm, where the produced clusters are saved in S.\nAlgorithm 1 Distance Correlation Clustering (dCorr,k)\n1: Initialise(S); 2: t=0; 3: while (dCorr 6= [0]) do 4: t = t+1; 5: \u03b1 = (max(dCorr) \u2212min(dCorr))/k; 6: [dmax, imax] = max(dCorr); 7: T \u2190 imax; 8: T \u2190 find_i(dmax > dCorri,imax \u2265 dmax \u2212 \u03b1)); 9: St \u2190 T ;\n10: clearCols(dCorr, T ); 11: clearRows(dCorr, T ); 12: end while"
    },
    {
      "heading": "5.2. Self-organisation of linguistic attribute hierarchy",
      "text": "Once attributes are clustered based on their distance correlation, we can construct a linguistic attribute hierarchy in terms of the distance correlation between clustered attributes and the goal variable. If an attribute has stronger distance correlation with decision variable, this indicates the attribute has stronger linear correlation to the goal variable, namely, the mapping function between the attribute and the goal could be more linear. In other words, it is easier to make decision in terms of the attribute. Our preliminary experimental results show that attributes, which have stronger distance correlation to the goal variable, should be fed to the LDT in higher layer of the linguistic attribute hierarchy. The average of distance correlation between attributes in a cluster and the goal variable can be calculated with Formula (10):\ndCorr(s) = \u2211\nx\u2208s\ndCorr(x, y)/t, t = |s|. (10)\nwhere, x is the attributes in cluster s, y is the goal variable, and t is the size of cluster (i.e. number of attributes in cluster s).\nThe basic idea of constructing an LAH based on the produced clusters is that:\n(1) calculating average distance correlation for each produced cluster;\n(2) Sort all average distance correlations between clusters and decision variable;\n(3) The LDT fed by the cluster with lower average distance correlation will be at\nlower level of the LAH;\n(4) The LDTs fed by the clusters that have similar average values of distance corre-\nlation will be at the same level of the LAH. A threshold \u03b8 is set for the assessment\nof clusters at the same level. The outputs of all LDTs at the same level will be the partial inputs of the next LDT in the next level of LAH;\n(5) The LDT fed by the cluster with the highest average distance correlation will be\non the top of the LAH.\nAssume dCorr(si) represents the average distance correlation of cluster si to the goal variable, and \u03b8 represents the maximum difference of distance correlation values of clusters at the same level to the goal variable. Namely, if\n|dCorr(si)\u2212 dCorr(sj)| < \u03b8, (11)\nthen the LDTs fed with si and sj will be at the same level in the LAH. Algorithm 2 provides the pseudo-code of the self-organisation of LAH, where dCorrXY is the vector of distance correlation of all attributes to the goal variable, S is the set of clusters produced by the distance correlation-based clustering algorithm above, R is the set of all the average distance correlations for cluster set S with K clusters to the goal variable, Z is the index of an intermediate attribute, which starts from n+1, the input attribute number, I is used to save the indices of intermediate attributes and the corresponding clusters at the same level, H is used to save the hierarchy to be constructed. For all clusters, if Ri\u2212Ra < \u03b8, save the intermediate attribute index Z to I, and append Z and the cluster Sti to the hierarchy H, else, start a new level, append current Z , other intermediate attributes saved in I and corresponding cluster Sti to the hierarchy H, and save current Z to I. If more than one LDTs at the same level in the LAH, and there is no further cluster to be constructed, then the outputs of those LDTs will be the inputs of the top LDT in the LAH. If the LDT constructed with the last cluster does not share the same level with other LDTs, then it will be the top LDT of LAH."
    },
    {
      "heading": "5.3. Training of a linguistic attribute hierarchy",
      "text": "In 2009, He and Lawry [7, 8] first time proposed using the domain and labels of the goal variable to describe intermediate attributes. This made the hierarchy training became possible, and the hierarchical decision making became meaningful. In [9], He and Lawry developed a non-recursively post-order traversal algorithm to train a given LAH, where all LDTs are trained with LID3 in a bottom-up way. Intermediate attribute values are estimated with the trained LDTs, and then they are input to next level LDTs. For example, in Fig. 1 (b), LDT1, LDT2 and LDT3 are trained in turn, where, the intermediate variables z1 and z2 use the corresponding y values in all training points, as the goal values ofLDT1 andLDT2 respectively. AfterLDT1 andLDT2 are trained, the intermediate variables z1 and z2 for the training samples can be estimated by LDT1 and LDT2, respectively. The estimated values of z1 and z2, the values of x5, and the values of the goal variable y in the training samples are used to train the LDT3, and finally the goal variable y can be estimated by LDT3. Here, we introduce a recursive postorder implementation of the LAH training algorithm (see Algorithm 3).\nH is a global variable, indicating the LAH to be trained. Initially, v is the root of the LAH to be trained, and all leaves in the LAH are tagged as visited, as they are the input attributes from sensor nodes. v.ch represents the child set of node v, indicating\nAlgorithm 2 SOLAH(dCorrXY , S, \u03b8) 1: [R] = avDistCorr(dCorrXY, S); 2: Z = n; K=|S |; 3: [R, t] = sort(R); %increasingly 4: I = \u03c6;H = \u03c6; 5: i = 0, a = 0; 6: Hi = \u03c6; 7: while (i < K) do 8: i = i+1; Z = Z+1, 9: if ((Ri \u2212 Ra < \u03b8) then 10: I \u2190 (Z); 11: H =\u2190 (Z,Sti ); 12: else 13: a = i; % starting a new level 14: H \u2190 (Z,I,Sti ); 15: I = Z; 16: end if 17: end while 18: if (|I| > 1) then 19: H \u2190 (Z + 1, I); 20: Y = Z+1; 21: else 22: Y = Z; 23: end if\nthe attributes that feed to the node v. S is used to save the attributes that are not on the branch from the top root to the current node v, and S \\ v is to remove v from set S.\nAlgorithm 3 postorder(v,S) if (node v \u2208 H has been visited) then\nreturn;\nelse\nfor (i=1..k) do postorder(v.chi, S \\ v); end for LDTv=LID3(v, S); zv = LDTv(D); D \u2190 zv ;\nend if"
    },
    {
      "heading": "5.4. The LID3 algorithm for LDT training",
      "text": "LID3 was used to train an LDT with a given database [38, 7]. It is an update of the classic ID3 algorithm [43], through combining label semantics. The training process is conducted through selecting the attribute that obtains the maximum information gain to extend a branch. The functions used in LID3 have been formulated in [38, 7], such as information entropy of a branch (E), expected entropy (EE) when an attribute node x is added to a branch, and information gain (IG). Here, a recursive algorithm implementation of LID3 is proposed (Algorithm 4).\nA threshold \u03d1 is setup to stop the branch extension when the conditional probability P (C|B) reaches the threshold \u03d1. The maximum of branch length is the number of\nattributes that feed the LDT. Initially, T = \u03c6 (empty), hence, current node point Bv = \u03c6 as well. For each branch, the conditional probabilities for all classes C \u2208 C are calculated. The most informative attribute is selected as the next node to be extended to the existing LDT, and all focal sets that are appropriated to describe the attribute in the domain of \u2126x will be appended to the tree T . The process is continued until the maximum conditional probability arrives the specified threshold.\nAlgorithm 4 LID3(Bv, S) for (all Ci \u2208 Cy) do\nP (Ci|Bv) = conditionProb(Bv ); end for if (max({P}) \u2265 \u03d1) or S = \u03c6 then return; end if for (all x \u2208 S) do IG(Bv , x) = E(Bv) \u2212EE(Bv, x); end for x\u0302 = argmaxx\u2208S({IG}); Bv\u0302 = Bv + x T \u2190 F \u2208 Fx\u0302; S = S \\ x\u0302; for (all F \u2208 Fx\u0302) do\nLID3(Bv\u0302 , S); end for"
    },
    {
      "heading": "6. Experiments and Evaluation",
      "text": ""
    },
    {
      "heading": "6.1. Experiment methodologies",
      "text": "All databases, used for the experiments, are from UCI machine learning repository\n[14]. The experiments are conducted in two stages:\n(1) A case study will be done on the benchmark database of Message Spams. This experiment is to demonstrate the use of developed approach for self-organisation of a linguistic attributes hierarchy, given the data.\n(2) A set of databases will be tested with the SOLAHs. The performance are used to validate the developed approach to self-organising LAHs by comparing with the performance obtained with the single LDT.\nExperimental environment: The experiments are carried out on a laptop with 64-bit Windows 10 and x64-based processor with Intel (R) Core (TM) i5-4210U CPU @1.7GHZ 2.4GHZ, 8GB memory. The LAH training algorithm is implemented in C++.\nDiscretisation: For simplicity, all attributes are expressed with three labels, except binary variables, decision or goal variables, which are expressed with the labels as they have. All neighbouring fuzzy intervals are overlapping with 50%.\nMass assignments of attributes: In terms of Label Semantics, if n labels are used to describe a continuous variable x, then there will be 2n \u2212 1 focal sets that are appropriate to describe x \u2208 \u2126x. For discrete variables, the mass assignment on a focal set that contains only one label is 1, but the mass assignment on a focal set that contains two successive labels is 0.\nTen-fold cross validation: 90% of data is used for training, 10% is used for test.\nTherefore, data is partitioned to ten parts equally.\nPerformance measure: The performance is measured with: (1) Accuracy: A = TP+TN N\n, where, TP is the number of true positive estimates and TN is the number of true negative estimates, and N is the number of samples in the database.\n(2) Area under ROC curve (AUR): A ROC curve is used to measure how well the classifier separates the two classes without reference to a goal threshold. The area under the ROC curve has been formalized in [9].\n(3) Rule number \u03b2: As defined in [9], the rule number of an LAH is the sum of branch numbers, extracted from all LDTs in the LAH. Since a probability threshold was introduced during the process of LDT training, a branch training may stop earlier. Hence, the actual branch number in an LDT may be much less than a full LDT.\n(4) The running time t: it is the time, spending on the process of ten-fold crossing validation, including training, testing, and overhead for data splitting and exchanging."
    },
    {
      "heading": "6.2. A case study on a benchmark database",
      "text": "To demonstrate the developed approach, the first experiment is carried out on the benchmark database, \u2019SMSSpamCollection\u2019. The performance for the LAH that was constructed by the proposed approach, the LAH in [13], obtained manually, and the single LDT, is evaluated and compared."
    },
    {
      "heading": "6.2.1. The database",
      "text": "The SMSSpamCollection database [44], has 5574 raw messages, including 747 spams. The two sets of features, extracted for the research [45, 13], are used for the experiments. Table 1 shows the set of 20 features, and Table 2 presents the set of 14 features."
    },
    {
      "heading": "6.2.2. Experimental results on SMS-20",
      "text": "gorithm are: S1:{x7,x1,x4,x18}, S2:{x0,x19}, S3:{x15,x16}, S4:{x6,x2,x3,x8}, S5:{x9,x11}, S6:{x5,x10,x12,x13,x14,x17}.\nS6 has the lowest average distance correlation to the decision variable. Hence, the LDT fed by S6 should be at the bottom of the SOLAH, and produces an intermediate attribute z20. Here z is used to represent an intermediate attribute. z20 with set S4 feeds an LDT in the second level according to the average distance correlation of S4 to decision variable. Sets S5 and S4 have similar average distance correlations to decisional variable. Hence, they sit at the same level in the SOLAH, and their outputs z21 and z22 feed the third level of LDT constructed with set S1. Sets S2 and S3 have similar average distance correlations to the decision variable. Hence they are at the same level. As they are at the top level, S2, S3 and the output z23 of the LDT at the third level will be an input of the top LDT. Figs. 2 and 3 illustrate the SOLAH and the best LAHm, manually produced in [13]. The two LAHs have the same levels and the same number of LDTs. But the compositions are different. Table 3 shows the\nperformance for the SOLAH, the LAHm in [13] and the single LDT fed by the whole feature vector (denoted as LDT (~x). It can be seen that the accuracy A of SOLAH is very close to the performance of LAHm and LDT (~x), and the performance of the\nAUC is slightly smaller than that of LAHm, but better than that of LDT (~x). However, SOLAH has the smallest branch number beta and running time T , which is the time of ten-fold crossing validation on a solution, measured in milliseconds (ms)."
    },
    {
      "heading": "6.2.3. Experimental results on SMS-14",
      "text": "the DCC algorithm are: S1:{x6,x11}, S2:{x9,x4,x8,x10}, S3:{x5,x0,x1,x2,x3,x12,x13}. Set S1 has the lowest average distance correlation to the decision variable, thus the LDT fed by S1 is placed at the bottom of the SOLAH. The LDT fed by set S1 and the output of the first level at the second level of the SOLAH, and the LDT fed by set S3 sits at the top level of the LAH.\nFigs. 4 and 5 illustrate the SOLAH and the LAHm with the best AUC, which was obtained by the attribute composition 3 in [13]. The two LAHs have the same levels, and the attributes are decomposed to 3 clusters. But the compositions of LAHs are different. In the LAHm, all LDTs fed by the clusters of input attributes are placed at the bottom of the LAH. Hence a new LDT, fed by all intermediate attributes, which were produced by the bottom level of LDTs, is added on the top of the LAH. SOLAH has higher level than LAHm, and all LDTs fed by the attribute clusters are cascaded.\nTable 4 shows the performance for the SOLAH, the LAHm, constructed with composition 3 manually in [13] and the single LDT (LDT (~x)). All solutions in Table 4\nobtained similar performance in accuracy and the area under ROC. All performance values of SOLAH are in between LAHm and LDT (x\u0304)."
    },
    {
      "heading": "6.2.4. Impact of the preset cluster number",
      "text": "Similar to classic k-means clustering algorithm, the preset cluster number k is important for the construction of LAHs, and thus has important impact on the performance of decision making or classification. Here, the relationships between k and structures of LAHs and between k and performance are observed. It should be noticed that the real cluster number may be different to the preset cluster number, as the present cluster number decides the range (\u03b1) of distance correlation of a cluster C, which is formed with the attributes that have the distance correlations in the range \u03b1 to an attribute x, which has the largest distance correlation in the distance correlation matrix. The rest attributes could have larger distance correlations than the members except attribute x in cluster C. Table 5 shows the solutions for k= 2...10. It can be seen that the real cluster numbers are different to the preset cluster number k in some cases. The cluster numbers varies from 3 to 6. All SOLAHs have similar performance. The SOLAH constructed with 3 clusters obtained the highest accuracy, but it has more branches. The SOLAHs constructed with 4 clusters have the highest AUC values. The clusters obtained when k = 7 is the same as that when k=8, which is happened when k = 9, 10 as well. In Table 5, \u03b2 denotes the branch number, \u03b9 denotes the level of the hierarchy.\nSimilarly, Table 6 lists the performance of SOLAHs constructed with various k values from 2 to 10. The AUC values of all SOLAHs are very close. When the preset k = 2, the SOLAH was constructed with two clusters. It obtained the highest accuracy, but lowest AUC value and the largest branch number. When the preset k=9,10, the same cluster sets were produced, with which, the SOLAH was produced. It has the lowest accuracy, but the smallest branch number. For all preset k=3...6, different sets of 5 clusters were produced, with which, four different SOLAHs were produced. They have the same level, and their performance values (A and AUC) are very close. When\nthe preset k=5,6,7, the produced SOLAHs have the same performance values (A and AUC). But they have different branch numbers.\nFrom the data on both SMS20 and SMS14, it can be seen that a suitable cluster size K will have a good trade-off on accuracy performance and time complexity (branch number)."
    },
    {
      "heading": "6.3. Validation on some benchmark databases",
      "text": ""
    },
    {
      "heading": "6.3.1. The 12 benchmark databases",
      "text": "The experiments on 12 benchmark databases from UCI machine learning repository are conducted for validating the proposed approach of constructing LAHs. Table 7 provides the basic properties of the 12 data sets, including, database name, attribute number (n), goal state number (Ng), total sample number (N ), class distribution (Nc)."
    },
    {
      "heading": "6.3.2. Comparisons with LDT",
      "text": "Table 8 shows the performance of SOLAHs and corresponding single LDTs on the 12 benchmark data sets. It can be seen that the SOLAH achieved dominated better performance in accuracy A and AUC than the single LDT for most databases. For\ndatabases, Ecoli and Liver, SOLAHs achieve similar accuracy to LDTs, but their performance in AUC is better than that of LDTs. The SOLAHs for most databases have less branches than the single LDT trained by the same database.\nAs databases Ionosphere and Sonar have 33 and 60 attributes, respectively, the ten-fold crossing validation of a single LDT on the two databases is not acceptable. Therefore, the two-fold crossing validation of single LDTs on the two high dimension databases are performed. For comparison, the two-fold crossing validation of the constructed LAHs on the two databases are carried out as well. The results are provided in Table 10. For database Ionosphere, the LAH has the same accuracy as the LDT, but the area under ROC obtained by the SOLAH is smaller than that by the single LDT. Namely, the half size of data may not be enough to train the SOLAH. However, the running times of SOLAHs for the two databases are 14s and 52s respectively, which are much less than that of LDTs for the two databases. The database Sonar provides a very interesting case: the SOLAH has 26328 branches, while the single LDT has only 9253 branches. Regarding the branch number, the complexity of the SOLAH model is worse than the single LDT. But the running time for SOLAH is 52s, while 4968s for LDT. This is because that the computing time is not only related to branch number, but also related to branch length. The computing complexity can be O(\u03b2 \u00d7 l). It can be seen that the SOLAH for the Sonar database has 22 layers, which means that at least 22 LDTs are embedded in the SOLAH. There are total 60 input attributes, plus 22 intermediate attributes, the average input attributes of an LDT in the SOLAH is 3.68. Hence, the average length of branch is 3,68. We can roughly say that the running time of the SOLAH is proportional to 26328\u00d7 3.68 = 96887, but the single LDT has 4968 branches, the length of which is 60, then the running time of the single LDT is proportional to 4968 \u00d7 60 = 298080, which is much larger than the figure for the SOLAH.\nIn order to observe the performance improvement of the LAH that is trained by 90% of data when ten-fold crossing validation is applied, Table 9 particularly lists the performance of the SOLAHs with ten-fold crossing validation on the two high\ndimensional databases. The performance in both accuracy and the area under ROC is improved, especially for the database, Ionosphere, the performance in accuracy and the area under ROC are improved very much, compared to the SOLAH with two-fold crossing validation. Moreover, the running times of the SOLAHs for the two databases are 143s and 507s, respectively, which are much shorter than that of the single LDT with two-fold crossing validation in Table 8."
    },
    {
      "heading": "6.3.3. Comparisons with other machine learning algorithms",
      "text": "The average accuracy and standard deviations of SOLAH for ten runs on the 12 data sets from UCI machine learning repository are further compared with the optimal LAH, obtained by a Genetic Algorithm Wapper in [9], as well as three well-known machine learning algorithms, such as C4.5, Naive Bayes (NB) and Neural Networks (NN). The performance of these algorithms WAS evaluated with WEKA [49] by Qin and Lawry in [38], where, WEKA [49] was used to generate the results of J48 (C4.5 in WEKA) unpruned tree, Naive Bayes and Neural Networks with default parameter settings for ten runs of 50%-50% splitting training and test data.\nTable 10 shows the performance of SOLAHs and the existing results in literature for the 12 data sets. SOLAH wins 4 data sets, which is next to OptLAH, which wins 5 data sets. SOLAH obtains comparable performance in accuracy, compared to other algorithms, and even achieves better performance than the LAHs, optimised by GAW, for some tested data sets."
    },
    {
      "heading": "7. Conclusions",
      "text": "(1) We proposed heuristic algorithm to construct a linguistic attribute hierarchy for semantic attribute deep learning in decision making or classification. The selfconstructed linguistic attribute hierarchy provides a new form of deep learning, in contrast to conventional deep learning.\n(2) The proposed algorithm for the self-organisation of an LAH is much more efficient than meta-heuristic algorithm, and the self-organised linguistic attribute hierarchy can obtain the fusion performance better than or comparable to the single linguistic decision tree, fed with the full set of attributes.\n(3) The most important is that the heuristical self-organisation of such linguistic attribute hierarchy can effectively solve the \u2019curse of dimensionality\u2019 in machine learning, which is critical challenge in the implementation of IoT intelligence. Hence, the research results will promote a wider of applications of the linguistic attribute hierarchy in big data analysis and IoT intelligence.\n(4) A linguistic attribute hierarchy, embedded with linguistic decision trees, will provide a transparent hierarchical decision making or classification. Hence, it could help us to look insight of the decision making process for different purposes (e.g. the effect of adversary samples in decision making).\n(5) Comparing with other machine learning in literature, the self-organised LAH obtains comparable performance on the tested data sets, and even achieved better performance than the optimal LAHs, obtained by GAW for some tested data sets.\nWe will implement the LAH on an embedded system (e.g. a Raspberry Pi system) for a specific task, and will further improve the algorithms and develop new algorithms to construct high efficient and effective linguistic attribute hierarchy, embedded with other machine learning models for decision making or classification in future."
    }
  ],
  "title": "A Heuristically Self-Organised Linguistic Attribute Deep Learning in Edge Computing For IoT Intelligence",
  "year": 2020
}
