{"abstractText": "A novel evolutionary approach for Explainable Artificial Intelligence is presented: the \u201cEvolved Explanations\u201d model (EvEx). This methodology consists in combining Local Interpretable Model Agnostic Explanations (LIME) with MultiObjective Genetic Algorithms to allow for automated segmentation parameter tuning in image classification tasks. In this case, the dataset studied is Patch-Camelyon, comprised of patches from pathology whole slide images. A publicly available Convolutional Neural Network (CNN) was trained on this dataset to provide a binary classification for presence/absence of lymph node metastatic tissue. In turn, the classifications are explained by means of evolving segmentations, seeking to optimize three evaluation goals simultaneously. The final explanation is computed as the mean of all explanations generated by Pareto front individuals, evolved by the developed genetic algorithm. To enhance reproducibility and traceability of the explanations, each of them was generated from several different seeds, randomly chosen. The observed results show remarkable agreement between different seeds. Despite the stochastic nature of LIME explanations, regions of high explanation weights proved to have good agreement in the heat maps, as computed by pixel-wise relative standard deviations. The found heat maps coincide with expert medical segmentations, which demonstrates that this methodology can find high quality explanations (according to the evaluation metrics), with the novel advantage of automated parameter fine tuning. These results give additional insight into the inner workings of neural network black box decision making for medical data.", "authors": [], "id": "SP:ef47f3802692ce4b245c23bc58fe761c648ec8ac", "references": [{"authors": ["O. Ronneberger", "P. Fischer", "T. Brox"], "title": "U-net: Convolutional networks for biomedical image segmentation", "venue": "International Conference on Medical image computing and computer-assisted intervention, 2015.", "year": 2015}, {"authors": ["D.S. Kermany", "M. Goldbaum", "W. Cai", "C.C.S. Valentim", "H. Liang", "S.L. Baxter", "A. McKeown", "G. Yang", "X. Wu", "F. Yan", "others"], "title": "Identifying medical diagnoses and treatable diseases by image-based deep learning", "venue": "Cell, vol. 172, pp. 1122-1131, 2018.", "year": 2018}, {"authors": ["R. Miotto", "F. Wang", "S. Wang", "X. Jiang", "J.T. Dudley"], "title": "Deep learning for healthcare: review, opportunities and challenges", "venue": "Briefings in bioinformatics, vol. 19, pp. 1236-1246, 2017.", "year": 2017}, {"authors": ["B.S. Veeling", "J. Linmans", "J. Winkens", "T. Cohen", "M. Welling"], "title": "Rotation equivariant CNNs for digital pathology", "venue": "International Conference on Medical image computing and computer-assisted intervention, 2018.", "year": 2018}, {"authors": ["Y. Liu", "K. Gadepalli", "M. Norouzi", "G.E. Dahl", "T. Kohlberger", "A. Boyko", "S. Venugopalan", "A. Timofeev", "P.Q. Nelson", "G.S. Corrado", "others"], "title": "Detecting cancer metastases on gigapixel pathology images", "venue": "arXiv preprint arXiv:1703.02442, 2017.", "year": 2017}, {"authors": ["B.E. Bejnordi", "M. Veta", "P.J. Van Diest", "B. Van Ginneken", "N. Karssemeijer", "G. Litjens", "J.A.W.M. Van Der Laak", "M. Hermsen", "Q.F. Manson", "M. Balkenhol", "others"], "title": "Diagnostic assessment of deep learning algorithms for detection of lymph node metastases in women with breast cancer", "venue": "Jama, vol. 318, pp. 2199- 2210, 2017.", "year": 2017}, {"authors": ["A. Adadi", "M. Berrada"], "title": "Peeking inside the black-box: A survey on Explainable Artificial Intelligence (XAI)", "venue": "IEEE Access, vol. 6, pp. 52138-52160, 2018.", "year": 2018}, {"authors": ["M.T. Ribeiro", "S. Singh", "C. Guestrin"], "title": "Why should i trust you?: Explaining the predictions of any classifier", "venue": "Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, 2016.", "year": 2016}, {"authors": ["M. Alber", "S. Lapuschkin", "P. Seegerer", "M. H\u00e4gele", "K.T. Sch\u00fctt", "G. Montavon", "W. Samek", "K.-R. M\u00fcller", "S. D\u00e4hne", "P.-J. Kindermans"], "title": "iNNvestigate neural networks", "venue": "arXiv preprint arXiv:1808.04260, 2018.", "year": 1808}, {"authors": ["I. Palatnik de Sousa", "M.M.B.R. Vellasco", "E.C. da Silva"], "title": "Local Interpretable Model-Agnostic Explanations for Classification of Lymph Node Metastases", "venue": "Sensors, vol. 19, p. 2969, 2019.", "year": 2019}, {"authors": ["P.-J. Kindermans", "S. Hooker", "J. Adebayo", "M. Alber", "K.T. Sch\u00fctt", "S. D\u00e4hne", "D. Erhan", "B. Kim"], "title": "The (un) reliability of saliency methods", "venue": "arXiv preprint arXiv:1711.00867, 2017.", "year": 2017}, {"authors": ["P.F. Felzenszwalb", "D.P. Huttenlocher"], "title": "Efficient graph-based image segmentation", "venue": "International journal of computer vision, vol. 59, pp. 167-181, 2004.", "year": 2004}, {"authors": ["R. Achanta", "A. Shaji", "K. Smith", "A. Lucchi", "P. Fua", "S. S\u00fcsstrunk"], "title": "SLIC superpixels compared to state-of-theart superpixel methods", "venue": "IEEE transactions on pattern analysis and machine intelligence, vol. 34, pp. 2274- 2282, 2012.", "year": 2012}, {"authors": ["A. Vedaldi", "S. Soatto"], "title": "Quick shift and kernel methods for mode seeking", "venue": "European Conference on Computer Vision, 2008.", "year": 2008}, {"authors": ["K. Deb", "S. Agrawal", "A. Pratap", "T. Meyarivan"], "title": "Deb, K., Agrawal, S., Pratap, A. and Meyarivan, T., 2000, September. A fast elitist non-dominated sorting genetic algorithm for multi-objective optimization: NSGA-II", "venue": "International conference on parallel problem solving from nature, pp. 849-858, 18 September 2000.", "year": 2000}, {"authors": ["L. Bradstreet"], "title": "The hypervolume indicator for multiobjective optimisation: calculation and use", "venue": "Perth,", "year": 2011}, {"authors": ["C.M. Fonseca", "L. Paquete", "M. L\u00f3pez-Ib\u00e1nez"], "title": "An improved dimension-sweep algorithm for the hypervolume indicator", "venue": "IEEE international conference on evolutionary computation, pp. 1157-1163, 2006.", "year": 2006}, {"authors": ["B. Veeling"], "title": "The PatchCamelyon (PCam) deep learning classification benchmark", "venue": "[Online]. Available: https://github.com/basveeling/pcam. [Accessed April 2020].", "year": 2020}], "sections": [{"text": "Artificial Intelligence is presented: the \u201cEvolved Explanations\u201d model (EvEx). This methodology consists in combining Local Interpretable Model Agnostic Explanations (LIME) with MultiObjective Genetic Algorithms to allow for automated segmentation parameter tuning in image classification tasks. In this case, the dataset studied is Patch-Camelyon, comprised of patches from pathology whole slide images. A publicly available Convolutional Neural Network (CNN) was trained on this dataset to provide a binary classification for presence/absence of lymph node metastatic tissue. In turn, the classifications are explained by means of evolving segmentations, seeking to optimize three evaluation goals simultaneously. The final explanation is computed as the mean of all explanations generated by Pareto front individuals, evolved by the developed genetic algorithm. To enhance reproducibility and traceability of the explanations, each of them was generated from several different seeds, randomly chosen. The observed results show remarkable agreement between different seeds. Despite the stochastic nature of LIME explanations, regions of high explanation weights proved to have good agreement in the heat maps, as computed by pixel-wise relative standard deviations. The found heat maps coincide with expert medical segmentations, which demonstrates that this methodology can find high quality explanations (according to the evaluation metrics), with the novel advantage of automated parameter fine tuning. These results give additional insight into the inner workings of neural network black box decision making for medical data.\nIndex Terms\u2014 Artificial Intelligence, Biomedical Imaging, Convolutional Neural Networks, Explainable AI, Multi-Objective Genetic Algorithms\nI. INTRODUCTION\nHE medical domain has seen an increasing use of Artificial Intelligence (AI) related methods. The advent of Deep Learning (DL) and increasingly high performance neural networks \u2013 particularly Convolutional Neural Networks (CNN) \u2013 in computer vision tasks created promising applications for clinical and diagnostic domain. This is true for both, classification and segmentation, tasks related to medical\nThis work was supported in part by the Conselho Nacional de Desenvolvimento Cient\u00edfico e Tecnol\u00f3gico (CNPq) and Funda\u00e7\u00e3o de Amparo \u00e0 Pesquisa do Rio de Janeiro (FAPERJ) under the process E-26/200.816/2019.\nP.S. Iam, is with the Electrical Engineering Department, Pontifical Catholic University of Rio de Janeiro. (e-mail: iam.palat@gmail.com). M.R.V. Marley, is with the Electrical Engineering Department, Pontifical Catholic University of Rio de Janeiro. (e-mail: marley@ele.puc-rio.br).\nC.S. Eduardo is with the Electrical Engineering Department, Pontifical Catholic University of Rio de Janeiro. (e-mail: edusilva@ele.puc-rio.br).\nMore recently, the performance of human pathologists has sometimes been surpassed by these networks, such as in the task of detecting metastases in patches extracted from whole slide histopathology images (WSI) [4] [5].\nDeep Learning architectures achieved very high levels of accuracy in these tasks, offering promising solutions for multiple issues pertaining to this problem. For instance, pathology experts performing the same task can disagree in more than 20% of the cases [6]. The related literature also mentions this type of task as tedious and time consuming, increasing the chance of human error.\nThese AI systems could greatly aid in balancing work loads of pathologists, who often are in great demand, and allow for more accurate and consistent diagnosis.\nHowever, in general, a key issue within these promising results is that there is no clear indication of what makes a particular DL system, such as a CNN, output a certain prediction or classification for a given patch. Several authors cite this black-box behavior as a central problem of deep learning systems concerning medical data. The recent related literature calls for a solution to this issue, before AI can truly be adopted for this task in the medical world [3] [7] [8].\nSince around 2016, Explainable AI (XAI) has been an increasingly blooming topic of research. A number of original papers and systematic reviews have been focusing on the development of techniques to explain decisions taken by AI models [7].\nOn the realm of image classification problems, multiple XAI techniques have been developed using different ideas. Many of these approaches involve backpropagation of the output to the input neurons, with specific functions or other specific operations conceived specifically for CNNs. Alber et al. provide a summary of these methods [9].\nIn this context, explanations consist in heat maps where each pixel in the image has a value according to its relevance for a given classification.\nAnother example is the Local Interpretable Model-Agnostic Explanations (LIME), developed by Ribeiro et al. [8]. In this approach, for image explanations, a given image is initially divided into super-pixels (also termed \u2018segments\u2019) and then their relevance, for a given classification, is determined using a linear model. The idea is that this simpler, and more easily interpretable, linear model could learn how to approximate the behavior of a more complex, non-explainable, non-linear\nT\nmodel \u2013 such as a CNN.\nThe LIME algorithm is called model-agnostic because it only requires the outputs of the classifier for a given input image. In fact, LIME can be used for any image classification system, not just neural networks, as it doesn't employ any specific backpropagation procedures or any specific steps related to particular model architectures. Implementations of these approaches, most often in Python, have become available recently [8] [9].\nAlthough scientific literature clearly states the need for XAI studies in medical imaging, there aren\u2019t many published works applying the existing approaches to medical imaging data.\nIn 2019, Palatnik-de-Sousa et al. [10] presented pioneering results using LIME to generate explanations for a WSI patch classifier trained on the Patch Camelyon dataset (P-CAM) [11], for detection of lymph node metastases. The choice of LIME over other techniques was justified both due to issues regarding the reliability of saliency based techniques [12], as well as the non-specificity of LIME when it comes to particular architectures. The model-agnostic nature of this technique meant it can be easily applied to any other type of architecture or classification model, unlike gradient based techniques that are more specific to Convolutional Neural Networks [9]. Considering these aspects, LIME could allow for more general comparisons to future studies.\nFurthermore, the results achieved in [10] demonstrated the high influence of different segmentation methods on the generation of LIME explanations, for that particular dataset. Besides, the results also highlight that Felzenszwalb\u2019s segmentation algorithm [13] generates high explanation weight heat maps. These explanations agreed with medical expert segmentations for the same patches.\nHowever, one difficulty was the necessity to choose segmentation parameters for generating super-pixels. Using default segmentation parameters within the LIME function might not generate meaningful explanations, and deciding the ideal set of parameters can be challenging without defining specific metrics for evaluating the heat-maps.\nOne of the alternatives proposed in [10] was the use of simplified, parameter-less, square grids to generate very rough simplified explanations. Despite showing that these \u201csquaregrid explanations\u201d in general agree with the more sophisticated segmentation algorithms, it was also noticed that a lot of finer detail is lost.\nConsidering these issues, the present study aims to explore a novel solution to this problem. In this way, the main goal of this paper is to present a more reliable and reproducible explainable model for explaining image classifiers. This methodology is based on multi-objective genetic algorithms, allowing for an automation of the parameter tuning process in image classification explanations. As this method/model entails using an evolutionary approach, it will be termed as \u201cEvolved Explanations\u201d (EvEx) throughout the manuscript.\nBy developing this methodology, the hope is to create a reliable way to peer into the black-box decision making process of convolutional neural networks, which are the most common type of classifier applied for medical imaging\nclassification problems.\nTo demonstrate the performance of this new model, the manually optimized results obtained by Palatnik-de-sousa et al. [10] are compared to EvEx, using a previously studied CNN trained on the Patch Camelyon (P-CAM) dataset [11]. The idea is to show that the EvEx model can eliminate the necessity of manual parameter tuning for super-pixel creation. By defining metrics to evaluate the quality of explanations, and by exploiting the observed behaviors of Felzenszwalb\u2019s segmentation algorithm for this dataset [10], the generation of explanations can be automated, creating more detailed explanations than with squaregrid.\nAdditional contributions described along this paper involve studies about the reproducibility of the generated explanations. Namely, LIME is a technique with stochastic components, which can cause explanations to vary slightly in different runs for the same image. This study presents two approaches to counteract this potential issue, showing that it is possible to generate high quality, reproducible explanations, without requiring human expert fine tuning of parameters or the tradeoff between parameter setting complexity and explanation details, like with squaregrid, a parameter-less method.\nSection II contains a description of the Evolved explanation model developed in this study, highlighting the different parts that compose it. Section III describes the application of EvEx to the case study of lymph node metastases classification, to improve previously existing results. Section IV presents the achieved results and discussions of this approach, and, finally, Section V is the conclusion for this manuscript."}, {"heading": "II. THE EVEX MODEL", "text": "In this section the EvEx model is described, and each one of its main components is detailed. Fig 1. shows a block diagram of EvEx, with the colored arrows representing the input/output relationship between each component.\nAs a brief overview, an input image passes through an image classifier. This image also undergoes a segmentation step, and the LIME methodology is applied to generate explanations. From these explanations a series of goal metrics are calculated. A Genetic Algorithm (GA) repeats this process a number of times generating multiple sets of segmentation parameters and creating a Pareto Front of the best explanations, which are averaged onto a final explanation,\nonce the evolution process is over. An early stop component can control the duration of the GA process, in case no new Pareto Fronts are found after a given number of generations. Random Number Generator (RNG) seeds are used to evaluate reproducibility, and allow a proper comparison among the obtained results.\nIn the following subsections EvEx components, as well as\nthe process of applying them, are described in more detail."}, {"heading": "A. LIME", "text": "LIME explanations [8] are generated by first dividing a given image into super-pixels or segments, using a segmentation algorithm. Super-pixels/segments are groups of pixels with similar colors, textures or other characteristics that hold some form of contextual information of a given image region. In section II.B, the choice of the segmentation algorithm for this study is discussed.\nAn example of this procedure can be seen in Fig. 2. Panel (a) shows a sample image. Panel (b) shows the super-pixels generated after applying a segmentation algorithm (see section II.B). For illustrative purposes only, these super-pixels are highlighted in yellow. However, these yellow markings are not applied to the real LIME implementation.\nThe segments are then randomly covered in black, creating a number of perturbed images, as shown in panel (c). Sections II.D.4 and 5, further below, contain a more detailed discussion on the number of perturbed images used in this case.\nThese images are then presented to the CNN model (see section II.C) being studied, that computes the prediction probabilities. With the perturbed images and their respective prediction probabilities, a linear model is trained, and the explanation weights (xw) for each super-pixel are then plotted in a blue-red heat map \u2013 panel (d), Fig. 2.\nColor intensity is proportional to the absolute value of the weight. As such, blue regions indicate super-pixels that contribute towards the correct classification, and red indicate the opposite. This LIME implementation allows the user to\ngenerate explanations for each class separately, if requested. For a more in-depth discussion of the particularities of LIME see [8] [10]."}, {"heading": "B. Segmentation Algorithm", "text": "Felzenszwalb's efficient graph based image segmentation (FHA) [13] generates an over-segmentation of an RGB image, using tree based clustering. The current implementation of LIME uses the FHA function from the scikit-image Python library [14]. Previously published results, by Palatnik-de-Sousa et al. [10], suggest that explanations generated with FHA typically result in at least one large superpixel with high explanation weights, especially when compared to two other segmentation algorithms that were also evaluated with LIME: Simple Linear Iterative Clustering (SLIC) [15] and Quickshift [16]. For this reason, FHA was chosen as the segmentation algorithm for this study.\nThe idea is that FHA segmentations can be used to simultaneously maximize the explanation weight (xw) of the most relevant superpixel, while minimizing its area, if a Multi Objective Optimization (MOO) is used. By defining such objectives, the segmentation process could be automated, eliminating the need for manual parameter tuning, while, at the same time, highlighting the most relevant areas of the image, with finer detail than previously obtained by the manually tuned FHA. Essentially, these optimized explanations would allow for a more nuanced understanding of the image classifications.\nBesides the already mentioned desirable behaviors of FHA for the purpose of multi-objective optimization, this algorithm also has two other marked advantages compared to SLIC and Quickshift, reinforcing it as an ideal candidate for this study. First, at least in the scikit-image implementation, FHA is the fastest segmentation algorithm. Secondly, it has less tunable parameters than either SLIC or Quickshift, greatly reducing the search space.\nWhen combined, these factors contribute to massively reducing the computational cost of optimization \u2013 saving hours of runtime for each optimization. However, despite this being extremely relevant given the limited available computational resources, the main factor for prioritizing FHA is the desirable segmentation results reached for this algorithm, which returns large super-pixels with high explanation weights.\nThe FHA function parameters are:\n Scale: Indirectly controls the number of segments\nproduced. In general, a larger value leads to larger segments. More specifically, as per [13], it sets a scale of observation for the threshold function, when calculating the minimum internal difference between components;\n Sigma: Standard deviation of the Gaussian Kernel\nused for blurring in preprocessing;\n Min_size: Minimum segment size, enforced in\npostprocessing.\nC. Image Classifier\nThis study uses a publicly available model trained on PCAM for a Kaggle competition [17], also used in [10]. It is a convolutional neural network, summarized in Fig. 3."}, {"heading": "D. Multi-Objective Optimization", "text": "As discussed in subsection II.B, FHA segmentation is a promising candidate for automatically adjusting parameters and generating high quality explanations.\nFor the case of FHA segmentation, each individual of the developed GA has 3 genes, representing the FHA parameters: scale, sigma and min_size. The evolutionary process seeks to find the best set of parameters, responsible for generating the best segmentation as well as the best explanation heat-maps."}, {"heading": "1) Optimization Goals", "text": "Considering the size of the search space and complexity of the evaluation function, a genetic algorithm was selected to perform the pursued multi-objective optimization. As such, a non-dominated sorting genetic algorithm function (NSGA-II) [18] was used. This MOO aims to optimize three objectives:\n Explanation score (Es): LIME provides this\nquantity as a value between 0 and 1. It corresponds to the R\u00b2 of the ridge regression model, created on the last step of LIME;\n Largest weight (Lw): Once the explanation is\ngenerated, each super-pixel has an explanation weight. Lw is the largest weight among all superpixels. In other words, it is the explanatory weight of the most relevant super-pixel. It can range from 0 to 1, with typical values being closer to 0.7~0.9;\n Relative area of most relevant segment (Ar): This\nis the relative area covered by the most relevant super-pixel. It is computed by dividing the number of pixels of this segment, by the overall number of pixels in the image. The choice of relative area, instead of just the area, is so that this quantity also ranges from 0 to 1, as the other optimization\nparameters. This is a key metric to explore the desirable behavior where FHA tends to find large segments that dominate the explanation.\nWith these goals, the NSGA-II algorithm is set up to maximize the Explanation score (Es) and Largest weight (Lw), while minimizing the Relative area of most relevant segment (Ar). In other words, the NSGA-II tries to find the highest scoring explanations, with the highest weighted most relevant super-pixels, while at the same time minimizing the chance of explanations that just cover the entire image in one single giant super-pixel.\nAs a result of the MOO, a Pareto front is generated, with one or multiple explanations that can be considered equally optimized. The final explanation given by the MOO is the average of these Pareto front explanations.\nTo justify why this average is used, one must imagine the process of trying to obtain a single best ideal explanation. This would mean optimizing the explanation down to effects caused by single pixels, which is not realistic, even from a medical standpoint. Thereby, the implemented MOO is less concerned with finding a perfect unique explanation down to single pixel precision, and more with finding the consensus between plausible explanations of the highest quality solutions achievable with the given goals."}, {"heading": "2) Hyper-volume", "text": "If each solution for an MOO problem is considered a point in an n-dimensional space, a common indicator of MOO performance is the n-dimensional volume contained within the solution set [19]. That is to say, the space contained within the solution points. This metric is called Hyper-volume (HV).\nSince the MOO here involves 3 objectives, the HV is the three dimensional volume contained within the solution sets, which in this case are the Pareto fronts.\nTo keep track of the GA performance, this HV is calculated for the Pareto fronts found in each generation. The HV computation function described by Fonseca et al. [20] was used in this work.\nThis HV function, however, implicitly assumes that the MOO aims to minimize all goals. For the Ar goal this doesn\u2019t change anything, since it was already a minimization problem. But Es and Lw should be maximized. Therefore, the MOO and HV functions consider minimizing (1-Es), (1-Lw), and Ar instead, respectively.\nThe HV is then computed using the (1,1,1) point as reference, meaning that (1,1,1) would correspond to the worst possible solution, and (0,0,0) to the best, for all goals.\nThis hyper-volume is just used to observe how the Pareto front is evolving. It is not used to interfere or influence the evolution process in any way. The expected result is to see the HV value increasing as the evolution process occurs and the Pareto front grows away from the (1,1,1) point towards the neighborhood of (0,0,0)."}, {"heading": "3) Early Stop", "text": "During the first tests with EvEx, the GAs were set to run for a number of generations (in this case 200), but seemed to converge before that. In order to reduce computation times, an Early Stopping criterion was adopted.\nFor each generation, the current Pareto front is saved into a list. If no new Pareto fronts are found, compared to the ones already existing on this list, a control variable is incremented by 1, starting from 0. If this control variable reaches a chosen value (in the case of this study, 70), the run is terminated."}, {"heading": "4) Reproducibility", "text": "One of the key aspects of performing this analysis is the fact that LIME explanations have some inherent variability. That is to say, generating multiple LIME explanations for the same image can create slightly different explanations. This happens because of the random generation of perturbed images, within LIME. Even if the same segmentation algorithm is always used, the resulting explanation weights, for each segment, can vary depending on which super-pixels are covered in the randomly generated perturbed images, as depicted in Fig.1.\nThis could be a problem for the genetic algorithm, since it implies that each individual in the GA population does not have a unique evaluation. Therefore, it could end with multiple identical individuals with slightly different evaluations. To counter this, two approaches were evaluated and added to the EvEx model as a reproducibility component.\nThe first one consisted in simply increasing the number of perturbed images used in LIME, which decreases variability in the goal metrics, but can\u2019t eliminate it completely. This approach still returned populations that had identical individuals with different evaluations, albeit this difference becomes almost negligible.\nThe other approach is to set randomizer seeds, so that the LIME explanations will always use the same perturbed image distributions. This makes the outcomes consistent when LIME is called multiple times on a given image. However, on a different seed the results could be somewhat different.\nIn practice, a combination of both approaches was used. Multiple runs were performed, each with different random seeds, so that within each run the LIME results are consistent, but they can be compared between different runs to see how the explanations behave. Simultaneously, the number of perturbed images was tuned so as it is low enough to allow for fast computation, while sufficiently high to still decrease variability to within a controlled number of decimal places, for the goal metrics. Such differences were computed as standard deviation heat maps, described on the next sections.\nIn other words, the same individual would always have the same evaluation within a given run, and would still have similar evaluations between different runs. Setting the number of perturbed images in 200 demonstrated to be enough.\nAs discussed further below in the results, this combined approach generated explanations that largely reach the same consensus between the Pareto fronts evolved.\nMore importantly, this makes the generation of explanations fully deterministic for a given set of parameters and random seed, which is crucial for the reproducibility of explanations."}, {"heading": "5) Comparing Heat Maps", "text": "In order to compare the explanation results of each seed and analyze how much they may vary, each patch had its pixelwise standard deviation (SD) computed over the four explanations/heat maps, corresponding to the four different seeds studied. Similarly, the pixel-wise means were computed, and subsequently its absolute values.\nThen, the pixel-wise relative standard deviation (RSD) (sometimes also called coefficient of variation) was computed by dividing the above described quantities. This quantity is plotted as a grayscale heat map, with a color scale ranging from 0 to 1, as an RSD value greater than or equal to 1 serves as indicator of high variance.\nAn explanation score threshold was set. Pixels that had mean explanation scores below this threshold were excluded from the RSD heat map, because regions with low explanation scores are not meaningful to interpret a classification.\nThis last step is further justified since, as seen in [10], regions of low explanation scores may change a lot between different LIME runs. On the other hand, regions of high explanation scores, which are the most relevant ones to describe the classifications, typically fluctuate much less."}, {"heading": "III. CASE STUDY: LYMPH NODE METASTASES", "text": ""}, {"heading": "A. Dataset", "text": "The CNN model used in this manuscript and [10] was trained on the Patch Camelyon (P-CAM) dataset, derived by Veeling et. al [4] from the Camelyon 16 hematoxylin and eosin stained WSIs.\nEach 96 by 96 patch has binary labelling that indicates the presence (label 1) or absence (label 0) of at least one pixel of tumor tissue in the center of the patch \u2013 a 32 by 32 pixel square. The dataset has a class balancing close to 50/50.\nAlthough the dataset was originally made available on a github repository [21], the version used here is the one available on the Kaggle website [11]. The latter is similar to the github version, except for the removal of duplicate patches caused by probabilistic sampling. Throughout this manuscript, whenever the P-Cam dataset or patches are mentioned, they refer to the Kaggle version."}, {"heading": "1) Medical Annotation", "text": "The WSI Camelyon16 dataset includes manual annotations of which parts of the images are metastatic tissue. This annotation/segmentation was done and verified by students and expert pathologists at two different Dutch hospitals (Radboud University Medical Center and University Medical Center Utrecht) [6].\nMappings of these WSI annotations to the patches were made available by Veeling et al. on the PCAM github\nrepository [21]. In [10] they were then mapped onto the Kaggle version, and these mappings are once again used here."}, {"heading": "2) Patch Selection", "text": "Considering the results achieved in [10], patches classified as true positives, where the medical annotation is a sub-region of the patch, can be a helpful option to evaluate the methodology here presented.\nThe key reason is that, in these patches, it is immediately apparent if the evolved explanations agree or not with the medical segmentation, as seen in Fig.5 panel (a), for instance."}, {"heading": "B. LIME Parameters", "text": "Like in [10], the original LIME implementation in Python was used [8]. For the most part, the LIME parameters used in this work were similar to the previous study [11]. The main difference is the number of perturbed images, which was decreased for this study as previously explained in section II."}, {"heading": "C. FHA Parameters", "text": "Although manual tuning has allowed for results that match medical expert segmentations [10], the manual search for the ideal set of parameters for the FHA can be challenging and considerably time consuming. Comparing heat maps visually with no defined metrics can also often be non-trivial. As discussed in the previous section, this motivates the use of a multi-objective optimization. The search space for the FHA parameters is described in more detail on the next subsection."}, {"heading": "D. Genetic Algorithm Parameters", "text": "The DEAP Python library [22] was used for the GA component of EvEx. The basic parameters used in the developed genetic algorithm are as follows:\n Population size: 80\n Maximum number of generations: 200\n Mutation operator probability (mutpb): 20%\n Crossover operator probability (cxpb): 50%\nThe search space is defined considering the three FHA parameters (scale, sigma and min_size). An individual in the population corresponds to a set of these three parameters, defined as:\n Scale: 3 decimal float, range: [1,1000]\n Sigma: 2 decimal float, range: [0, 5]\n Min_size: int, range: [15,500]\nThe lower and upper limits for these ranges were chosen based on the tests ran in [10], as well as in preliminary tests ran for this manuscript. Values outside these ranges don\u2019t seem to generate useful explanations, and increase the size of the search space needlessly. Specifically, regarding the min_size parameter, values around 15 and below tend to enable segmentations with an exceedingly large number of small segments, which not only increase computation times for LIME, but also do not generate good explanations. Previous results presented in [11] also indicate that generating a very large number of small segments doesn\u2019t allow them to\nindividually hold much relevant explanatory information [10].\nRegarding mutation and crossover, DEAP uses two sets of probability parameters. The first set, already mentioned above (mutpb and cxpb), refers to the probabilities that the specified mutation and crossover operators will be applied.\nEach of these operators however may consist on standard functions of the DEAP library, or custom ones defined by the user. They may have individual internal probabilities (indpb_mutation and indpb_crossover) specific to the type of calculations performed on individuals.\nThe crossover function used in this study was the DEAP uniform crossover (cxUniform), with an indpb_crossover of 20%. This function was chosen over one-point and two-point crossover, because preliminary tests performed with these other functions seemed to show premature convergence of the solutions, with Pareto fronts not changing anymore after the first generations. The use of uniform crossover successfully corrected this behavior.\nAs for the mutation parameters, each gene in the individuals had a specific mutation function associated with it. Gaussian mutations were used for the FHA scale parameter (Mu = 0, sigma =10), as well as the FHA sigma parameter (Mu = 0, sigma = 0.05). Uniform integer mutations were used for the integer parameter (min_size). The int range for the mutation was [15,500] The value of indp_mutation was set to 20%."}, {"heading": "IV. RESULTS AND DISCUSSION", "text": ""}, {"heading": "A. Pareto Fronts and Hyper-volume", "text": "Fig. 4 shows an example of Pareto front and HV behavior from a given evolved explanation. More specifically, it is the generated result for the image highlighted in Fig. 5 row (a), seed 45. The initial population and final Pareto front evaluations are plotted in panel (a). The shape of the optimized Pareto front becomes apparent, closer to the optimal (0,0,0) point.\nIndeed, most of the initial 80 individuals of the starting population, in orange, are clumped in two groups. The first one is composed by individuals around the (1,1,1) point of the plot, which correspond to segmentations where the most explanatory super-pixel covers a large area of the image, but fails to explain the classification, once they have low explanation score and explanation weight.\nOn the other hand, the second clump of individuals are grouped close to the origin, upwards along the Z-axis. This second clump corresponds to the individuals that have large explanation scores and weights, by sheer virtue of having the most explanatory super-pixel covering a large area of the image, once the relative areas lie between 0,3 and 1,0. Indeed, this seems to be the majority of cases. As previously discussed, this is caused by the FHA segmentation, and this behavior is what the optimization seeks to exploit.\nPanel (b) shows that the hyper-volume of the Pareto front increases throughout the run, demonstrating the algorithm is able to find individuals optimizing the defined goals.\nWhen comparing the Pareto front of the initial population with the one achieved for the final population, it is clear that low performing individuals progressively were replaced in the populations by better counterparts. The final front includes individuals with super-pixels covering much smaller regions of the image, while still maintaining high explanation and score metrics all around."}, {"heading": "B. Evolved Explanations", "text": "In total, considering all images studied, and all seeds, there were 32 runs of EvEx. The average size of the final Pareto fronts was 37 individuals. The largest front observed had 61 individuals and the smallest 15. Fig. 5 shows the outputs of these 32 runs. These outputs consist on the averaged explanation generated with the individuals from the final Pareto front, after the last generation of the GA.\nFor the cases in rows (a), (b), (c), (d), (f), (g), (h), the most highlighted area of the image (deepest blue, corresponding to highest explanation weights) is contained within the medical segmentation (displayed as a green transparent overlay). The image on row (e) has lower explanation weight areas scattered across the entire patch, but the deepest blue regions are still mostly contained within the medical segmentation, extending outwards of it to the top left of the patch.\nBecause of the deterministic approach adopted in the methodology for this study, results for each seed are exactly reproducible on different runs. However, even comparing the results from different seeds, they also seem to indicate agreement as to which areas of the images are most relevant.\nThis happens because each final Pareto Front might have a different number of individuals, focusing on different parts of the images. However, it seems that aggregating those individual explanations led to consistent averaged explanations. This averaging might also lead to robustness against the randomness expected for LIME explanations, making this technique a better candidate for such applications.\nC. Variability and Reproducibility\nTo further test and quantify this agreement between the seeds, a second set of plots analyzing the relative standard deviation (RSD) between images was generated, as described on section II.D.5. Fig. 6 shows these results.\nThe standard deviation (SD) column shows the pixel-wise standard deviation computed between the 4 seeds, for each image. The maximum SD occurs in a sub region of image (h); however, it comprises very few pixels of the image. For most cases, very light colors are observed throughout the SD heat maps, meaning the standard deviations assume low values.\nHowever, it is hard to draw conclusions from just this quantity alone, as different pixels have different mean explanation weights on the original blue-red heat maps. A standard deviation of 0.1, for instance, is drastically more expressive for a pixel with mean explanation weight of 0.12, than for another one with 0.9.\nTo better visualize this effect, as explained in section II.D.5, RSD heat maps were also generated, using a common grayscale from 0 to 1 for all analyzed cases. In this scale, values equal or greater than 1 indicate that the standard deviation is equal or greater than the mean, indicating high variability. Then, the scale is capped at that value. Consequently, points with RSD > 1 will show up as black on those heat maps.\nTaking Fig. 6 (a) as an example, it can be immediately noticed that there are in fact regions with RSD of 1 or more. However, comparing Figs. 6 and 7, it becomes apparent that these high variability regions are areas of extremely low, negligible explanation weights. This result is, in fact, in line with previous observations by Palatnik-de-sousa et al. [10] that low explanation weight regions tend to vary more between LIME runs, most likely because they are not relevant to the explanation. Although their weights fluctuate, they do so in very small absolute values, with many decimal places. Thus, it is both expected for these areas to be highly variable in regards to RSD, and also to be highly irrelevant for the explanation. Therefore, they can be safely disregarded from the explanation. This pattern is, in fact, observed for most of the images studied, although Fig. 6 (c), (e) and (g) seem to show high agreement (low RSD) for the entire patch.\nMore relevantly, as expected, the areas of high explanation weights (strong blue colors in Fig. 5) seem to coincide with\nareas of low RSD in Fig. 6, meaning that the explanations generated by applying this methodology, with different seeds, seem to agree in general as to what areas of the patch hold the most information to explain the classification. This agrees with the previous discussion; however the RSD analysis helps to quantify and solidify this interpretation.\nTo better visualize this behavior, a third set of RSD heat maps was generated, by using an xw threshold, that excludes from the RSD plot any pixels with mean explanation score (averaged between the 4 seeds) below this threshold.\nThe threshold used was 0.5, which overall is not a high explanation weight, considering most images displayed regions with Es above 0.6. But, even for this low threshold, there is remarkable agreement between different seeds, with the maximum RSDs observed being at most around 0.3, as highlighted in column \u2018thresholded maximum RSD\u2019 of Fig. 6. Furthermore, these maximum RSDs (stronger gray hues) are only observed in small sub-regions of the heat maps, with the majority of the areas having even lower RSD values.\nNotably, taking an image such Fig. 6 (a), that originally had regions of high explanation weights, above 0.8, it is possible to vary the threshold and see how it affects the RSD. Namely, the maximum RSD of 0.295, at 0.5 xw threshold, could be considerably lowered to 0.073, at a 0.8 xw threshold. This behavior indicates that for a given patch, using different seeds, explanations generated with this methodology show agreement between areas with increasingly higher explanation weights. It also means that it seems reasonable to expect areas of high xw, above 0.7 or more, to agree considerably between different seeds. It is encouraging that even areas of moderately low xw seem to show RSD values much smaller than 1.\nThe xw threshold column of Fig. 6 also resembles, from a quantitative standpoint, the intuitive similarity between the heat maps generated from different seeds, for each original image, that might be noticed at looking at Fig. 5. The areas that seem to agree, in general, are indeed the ones that correspond to low RSD values.\nAs such, the methodology here employed seems to be fully deterministic within individual seeds, as well as agreeing considerably between different seeds. It makes this GA MOO approach a very robust, automated alternative to manual parameter tuning for segmentation and generation of explanations. Aiming to generate more explainable and reliable systems in medical AI settings, this seems to be a promising result. The very important factor of results reproducibility is also greatly enhanced with the proposed methodology, allowing for more easily traceable explanations.\nIf medical experts so desired, they could dissect a given explanation by taking a specific seed, viewing each of Pareto front individual explanations separately, for that seed, as well as varying xw thresholds on the heat-maps, varying the range of the color scales, among other such ideas that might aid in visualizing results. The more cumbersome, time consuming and non-trivial task of finding appropriate parameters for segmenting patches and generating explanations can be left for the multi-objective GA here presented."}, {"heading": "D. Further considerations", "text": "The main drawback of EvEx is its computational cost. Each 200 generation GA run took between 6 and 8 hours, to generate an explanation for one patch. However, upon analysis, it was shown that the factor that contributes the most for this high computational time is the LIME function. It is possible that more computationally efficient implementations of LIME can be developed in the future, or that this same methodology could be employed in much more powerful hardware than the Kaggle cloud kernels used in this study, allowing faster processing of the patches.\nAkin to the previous results reported in [10], a notable feature of the explanations here obtained is that, starting from a very simple binary label on the P-CAM dataset, the explanations generated show heat maps rich in information, largely in agreement with medical expert segmentations.\nHowever, a key difference is that when using manual parameter tuning, as in [11], one might use human expertise or expert segmentations to guide the decision of which explanations are best. In the case of this evolutionary algorithm, the only factors guiding the explanations are the goal metrics to be optimized. The medical segmentations are only used for comparison, after the evolution process is over. Yet they show remarkable agreement, as shown in Fig. 5.\nThis confirms the proposed technique as a valuable, reproducible methodology that can be further applied in other medical image datasets. This approach could prove to be extremely valuable, especially for cases where medical expert segmentations are not available, or in searching for explanations in medical imagining problems, where it is not yet known what regions of a given image should be relevant for classification."}, {"heading": "V. CONCLUSION", "text": "In this manuscript, EvEx, a novel XAI model that uses LIME explanations combined with a multi-objective genetic algorithm, was presented. The explanations generated are the averaged contributions of the individuals of the evolved Pareto front. The algorithm seeks to adjust the three parameters of an FHA segmentation function, which has the desirable trait of finding large super-pixels with high explanation weights. The goals defined attempt to capitalize on this property by simultaneously trying to maximize the explanation score and the largest weight presented by the super-pixels, while minimizing the area of the super-pixel with largest weight.\nThe EvEx model presented deterministic explanations for classifications of lymph node metastases, within random seeds. Additionally, it shows agreement between different seeds, especially in areas with high explanation weights.\nThese evolved explanations further agree with medical expert segmentations for the same images, while not requiring any expert manual tuning of LIME or segmentation parameters. Therefore, they provide an important step towards peering reliably into the black-box decision making of neural networks used for medical image classifications. Future projects may focus on improving computation times and\napplying EvEx to other types of image classification tasks outside of the medical area."}], "year": 2020}