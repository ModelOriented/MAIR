{"abstractText": "Data analyses based on linear methods constitute the simplest, most robust, and transparent approaches to the automatic processing of large amounts of data for building supervised or unsupervised machine learning models. Principal covariates regression (PCovR) is an underappreciated method that interpolates between principal component analysis and linear regression, and can be used to conveniently reveal structure-property relations in terms of simple-to-interpret, low-dimensional maps. Here we provide a pedagogic overview of these data analysis schemes, including the use of the kernel trick to introduce an element of non-linearity, while maintaining most of the convenience and the simplicity of linear approaches. We then introduce a kernelized version of PCovR and a sparsified extension, and demonstrate the performance of this approach in revealing and predicting structure-property relations in chemistry and materials science, showing a variety of examples including elemental carbon, porous silicate frameworks, organic molecules, amino acid conformers, and molecular materials.", "authors": [{"affiliations": [], "name": "Benjamin A. Helfrecht"}, {"affiliations": [], "name": "Rose K. Cersonsky"}, {"affiliations": [], "name": "Guillaume Fraux"}, {"affiliations": [], "name": "Michele Ceriotti"}], "id": "SP:ea19a07d1ad144b6669883777908da2636853e8d", "references": [], "sections": [{"text": "Structure-Property Maps with Kernel Principal Covariates Regression Benjamin A. Helfrecht,1 Rose K. Cersonsky,1 Guillaume Fraux,1 and Michele Ceriotti1, a)\nLaboratory of Computational Science and Modeling, IMX, E\u0301cole Polytechnique Fe\u0301de\u0301rale de Lausanne, 1015 Lausanne, Switzerland\nData analyses based on linear methods constitute the simplest, most robust, and transparent approaches to the automatic processing of large amounts of data for building supervised or unsupervised machine learning models. Principal covariates regression (PCovR) is an underappreciated method that interpolates between principal component analysis and linear regression, and can be used to conveniently reveal structure-property relations in terms of simple-to-interpret, low-dimensional maps. Here we provide a pedagogic overview of these data analysis schemes, including the use of the kernel trick to introduce an element of non-linearity, while maintaining most of the convenience and the simplicity of linear approaches. We then introduce a kernelized version of PCovR and a sparsified extension, and demonstrate the performance of this approach in revealing and predicting structure-property relations in chemistry and materials science, showing a variety of examples including elemental carbon, porous silicate frameworks, organic molecules, amino acid conformers, and molecular materials.\nI. Introduction\nOver the past decade, there has been a tremendous increase in the use of data-driven and machine learning (ML) methods in materials science, ranging from the prediction of materials properties1\u20134, to the construction of interatomic potentials5\u20138 and searches for new candidate materials for a particular application9\u201312. Broadly speaking, these methods can be divided into two categories: those that are focused on predicting the properties of new materials (supervised learning), and those that are focused on finding or recognising patterns, particularly in atomic structures (unsupervised learning). While supervised methods are useful for predicting properties of materials with diverse atomic configurations, they are not as well-suited for classifying structural diversity. Conversely, unsupervised methods are useful for finding structural patterns, but often fail to directly predict materials properties. Moreover, it can be difficult to validate motifs identified by an unsupervised learning algorithm, as the results obtained from the clustering algorithm depend on the choice of the structural representation and can therefore be biased by preconceived expectations on what the most relevant features should be13.\nMethods that combine the predictive power of supervised ML and the pattern recognition capabilities of unsupervised ML stand to be very useful in materials informatics, making it possible to increase data efficiency and more clearly reveal structure-property relations. A number of statistical methods have been developed for augmenting regression models to incorporate information about the structure of the input data, including principal component regression14, partial least squares regression15, clusterwise regression16, continuum regression17, and principal covariates regression (PCovR)18\u201321. Among these, PCovR is particularly appealing, because it transparently combines linear regression (LR; a supervised learning method) with principal component analysis (PCA; an unsupervised learning method). The method has found previous\na)Electronic mail: michele.ceriotti@epfl.ch\napplications in climate science22, macroeconomics23, social science20, and bio-informatics24,25, but has yet to be widely adopted. A handful of extensions have been developed for PCovR, including a combination with clusterwise regression26, and regularised models22,24.\nIn this paper, we propose a kernel-based variation on the original PCovR method, which we call Kernel Principal Covariates Regression (KPCovR), with the aim of making it even more versatile for statistics and machine learning applications. We begin by summarising the required background concepts and constituent methods used in the construction of linear PCovR in addition to the kernel trick, which can be used to incorporate an element of non-linearity in otherwise linear methods. We then introduce KPCovR, both for full and sparse kernels, and demonstrate their application to several different classes of materials and chemical systems."}, {"heading": "II. Background Methods", "text": "We start by giving a concise but complete overview of established linear methods for dimensionality reduction and regression, as well as their kernelized counterparts. This is done to set a common notation and serve as a pedagogic introduction to the problem, complemented by a set of interactive Jupyter notebooks27. Expert readers can skip this section and proceed to Section III, where we introduce kernelized PCovR methods. Throughout this section, we demonstrate the methods on the CSD-1000r dataset28, which contains the NMR chemical shielding of nuclei in a collection of 1000 organic crystals and their 129,580 atomic environments, of which we use 25,600 in this study. To simplify this into a more illustrative example, we classify and predict simultaneously the chemical shieldings of all nuclei, even though in actual applications one usually would deal with one element at a time. As the input features, we use the SOAP power spectrum vectors, which discretise a three-body correlation function including information on each atom, its relationships with neighbouring atoms, and the relationships between sets of neighbours29,30. ar X iv :2\n00 2.\n05 07\n6v 2\n[ st\nat .M\nL ]\n2 1\nM ay\n2 02\n0\n2"}, {"heading": "A. Notation", "text": "In the following, we assume that the input data has been processed in such a way that the nature of each sample (e.g., the composition and structure of a molecule) is encoded as a row of a feature matrix X. Each sample is therefore a vector x of length nfeatures, so that X has the shape nsamples \u00d7 nfeatures. Similarly, the properties associated with each sample are stored in a property matrix Y, which has the shape nsamples \u00d7 nproperties. We denote the data in latent space (i.e., a low-dimensional approximation of X) as T. We denote each projection matrix from one space to another as PAB, where A is the original space and B is the projected space. As such, the projection matrix from the input space to T is PXT , and vice versa PTX is the projection matrix from T to the input space. Note that in general projectors PAB are not assumed to be orthogonal nor full-rank. A graphical summary of the mappings that we consider in this paper is depicted in Figure 1.\nTo simplify notation and to work with unit-less quantities, we assume in our derivations that both X and Y are centred according to their respective column means and are scaled to have unit variance. A similar centring and scaling procedure is also applied when working with kernels31. Centring and scaling is discussed in more detail in appendix A, and demonstrated in the companion Jupyter notebooks27. To make notation less cumbersome, variables names are not defined uniquely across the entirety of the paper. We re-use variable names for common elements among the different subsections\u2014for example, using T to represent a low-dimensional latent space in all methods\u2014but the precise definitions of the re-used variables may differ between subsections and should not be confused with one another.\nWe also use throughout a few additional conventions: (1) we write an approximation or truncation of a given ma-\ntrix A as A\u0302; (2) we use A\u0303 to signify an augmented version\nof A; that is, A\u0303 is defined differently from A, but occupies the same conceptual niche (3) we represent the eigendecomposition of a symmetric matrix as A = UA\u039bAU T A, where \u039bA is a diagonal matrix containing the eigenvalues and UA the matrix having the corresponding eigenvectors as columns; (4) we use throughout the Frobenius norm \u2016A\u2016 = \u221a\nTr ATA; and (5) we define and report the values of the different losses normalised by the number of sample points, but we omit such normalisation in derivations to declutter equations."}, {"heading": "B. Linear Methods", "text": "We begin by discussing models of the form:\nB = APAB (1)\nwhere B is a target quantity (e.g., a property that one wants to predict or an alternative, lower-dimensional representation of the feature matrix A), and PAB is a linear projection that maps the features to the target quantity32,33."}, {"heading": "1. Principal Component Analysis", "text": "In principal component analysis34,35, the aim is to reduce the dimensionality of the feature matrix X by determining the orthogonal projection T = XPXT which incurs minimal information loss. More formally, we wish to minimise the error ` of reconstructing X from the low-dimensional projection:\n`proj = \u2016X\u2212TPTX\u20162/nsamples. (2) The requirement that PXT is orthonormal implies that PTX = P T XT . Using the properties of the Frobenius norm, ` can be rewritten as\n` = Tr ( X ( I\u2212PXTPTXT ) XT )\n(3)\nwhich is minimised when the similarity\n\u03c1 = Tr(PTXTX TXPXT ) (4)\nis maximised. Given the orthogonality constraint on PXT , the similarity is maximised when PXT corresponds to the eigenvectors of the covariance C = XTX that are associated with the nlatent largest eigenvalues. We introduce the eigendecomposition C = UC\u039bCU T C, where UC is the matrix of the eigenvectors and \u039bC the diagonal matrix of the eigenvalues, so that\nT = XU\u0302C, (5)\nwhere we use the notation U\u0302 to indicate the matrix containing only the top nlatent components. The outcomes of a PCA with nlatent = 2 of the CSD-1000r dataset are shown in Fig. 2(a). The atomic environments are split clearly according to the nature of the atom sitting at the centre of the environment, reflecting the prominence of this information in the SOAP features we use.\n3"}, {"heading": "2. Multidimensional scaling", "text": "A reduction in the dimensionality of the feature space can also be achieved with a different logic that underlies several methods grouped under the label of multidimen-\nsional scaling (MDS)36. In MDS, the projected feature space is chosen to preserve the pairwise distances of the original space, defining the loss\n` = 1\nnsamples \u2211 i<j ( |xi \u2212 xj |2 \u2212 |ti \u2212 tj |2 )2 , (6)\nwhere xi and ti refer to the full and projected feature vector of the i-th sample. In general, Eq. (6) requires an iterative optimisation. When the distance between features is the Euclidean distance, as in classical MDS, the link between the metric and the scalar product suggests minimising the alternative loss\n`gram = \u2016K\u2212TTT \u20162/nsamples (7) Note that the solutions of Eqs. (6) and (7) concur only if one can find a solution that zeroes `. If the eigenvalue decomposition of the Gram matrix reads K = XXT = UK\u039bKU T K, ` is minimised when TT\nT is given by the singular value decomposition of K, that is by taking\nT = U\u0302K\u039b\u0302 1/2 K (8)\nrestricted to the largest nlatent eigenvectors. However, C and K have the same (non-zero) eigenvalues, and the (normalised) eigenvectors are linked by UK = XUC\u039b \u22121/2 C . Hence, one sees that T = XU\u0302C, consistent with Eq. (5). Thus, classical MDS yields the same result as PCA in Fig. 2(a)."}, {"heading": "3. Linear Regression", "text": "In linear regression, one aims to determine a set of weights PXY to minimise the error between the true properties Y and the properties predicted via Y\u0302 = XPXY , which is equivalent to minimising the loss\n`regr = \u2016Y \u2212TPTY \u20162/nsamples (9) In the following, we consider the case of an L2 regularised regression with regularisation parameter \u03bb, i.e., ridge regression32. The loss to be minimised is\n` = \u2016Y \u2212XPXY \u20162 + \u03bb\u2016PXY \u20162. (10) Minimising the loss with respect to PXY yields the\nsolution PXY = ( XTX + \u03bbI )\u22121 XTY. If one chooses to perform the regression using the low-dimensional latent space T = XPXT and approximate Y with TPTY , then\nPTY = ( TTT + \u03bbI )\u22121 TTY.\nThe ridge regression of the CSD-1000r dataset is shown in Fig. 2(b). Given the small train set size, and the difficulty of fitting simultaneously different elements with shieldings across a large range (\u2248 800 ppm), the model achieves a very good accuracy, with a RMSE below 23 ppm."}, {"heading": "C. Principal Covariates Regression", "text": "Principal covariates regression (PCovR)18 utilises a combination between a PCA-like and a LR-like loss, and therefore attempts to find a low-dimensional projection of the feature vectors that simultaneously minimises information loss and error in predicting the target properties\nusing only the latent space vectors T. A mixing parameter \u03b1 determines the relative weight given to the PCA and LR tasks,\n` = \u03b1\nnsamples \u2016X\u2212XPXTPTX\u20162+ (1\u2212 \u03b1) nsamples \u2016Y\u2212XPXTPTY \u20162.\n(11) The derivation we report here, albeit in our notation, follows closely that in the original article18. PCovR can be implemented in a way that diagonalises a modified Gram matrix (sample-space PCovR) or in a way that requires computing and diagonalising a modified covariance (feature-space PCovR). The two approaches yield the same latent-space projections, and which one should be used depends on the relative magnitudes of nsamples and nfeatures."}, {"heading": "1. Sample-space PCovR", "text": "It is easier to minimise Eq. (11) by looking for a projec-\ntion T\u0303 in an auxiliary latent space for which we enforce orthonormality, T\u0303T T\u0303 = I, known as a whitened projection.\nThis allows us to write PT\u0303X = T\u0303 TX and PT\u0303 Y = T\u0303 TY.\nBy definition T\u0303 = XPXT\u0303 , thus we can express the loss as\n` = \u03b1\u2016X\u2212 T\u0303T\u0303TX\u20162 + (1\u2212 \u03b1)\u2016Y \u2212 T\u0303T\u0303TY\u20162. (12) This loss is minimised by maximising the associated similarity\n\u03c1 = Tr ( \u03b1T\u0303T\u0303TXXT + (1\u2212 \u03b1)T\u0303T\u0303T Y\u0302Y\u0302T ) (13)\n= Tr ( \u03b1T\u0303T\u0303TXXT + (1\u2212 \u03b1)T\u0303T\u0303TXPXY PTXY XT ) ,\n(14)\nwhere we have substituted Y with the regression approximation Y\u0302 = XPXY \u2014 given that a linear approximation of Y in the latent space can only, at best, reproduce the part of the properties that can be represented in the full feature space. If we define the modified Gram matrix\nK\u0303 = \u03b1XXT + (1\u2212 \u03b1)XPXY PTXY XT , (15) we can further write the similarity as\n\u03c1 = Tr ( T\u0303T K\u0303T\u0303 ) . (16)\nThe latent space projections T\u0303 that maximise the similarity correspond to the principal eigenvectors of the\n5 matrix K\u0303, T\u0303 = U\u0302K\u0303. By analogy with multidimensional scaling\u2014and to ensure that in the limit of \u03b1\u2192 1 we obtain the same latent space as in classical MDS\u2014one can obtain de-whitened projections T = U\u0302K\u0303\u039b\u0302 1/2 K\u0303 = K\u0303U\u0302K\u0303\u039b\u0302 \u22121/2 K\u0303 , reminiscient to Eq. (8). The projector from feature space to the latent space is then given by\nPXT = ( \u03b1XT + (1\u2212 \u03b1)PXY PTXY XT ) U\u0302K\u0303\u039b\u0302 \u22121/2 K\u0303 . (17)\nThe projector matrix from the latent space to the properties Y can be computed from the LR solution\nPTY = ( TTT + \u03bbI )\u22121 TTY =\n\u03bb\u21920 \u039b\u0302 \u22121/2 K\u0303 U\u0302 T K\u0303 Y. (18)"}, {"heading": "2. Feature-space PCovR", "text": "Rather than determining the optimal PCovR projections by diagonalising the equivalent of a Gram matrix, one can tackle the problem in a way that more closely resembles PCA by instead diagonalising a modified covariance matrix. Given that I = T\u0303T T\u0303 = PT\nXT\u0303 XTXPXT\u0303 =\nPT XT\u0303 CPXT\u0303 , we see that C 1/2PXT\u0303 is orthogonal. We can thus rewrite the similarity function from Eq. (16) as\n\u03c1 = Tr ( PT XT\u0303 C1/2C\u0303C1/2PXT\u0303 ) , (19)\nintroducing\nC\u0303 =C\u22121/2XT K\u0303XC\u22121/2\n=\u03b1C + (1\u2212 \u03b1)C\u22121/2XT Y\u0302Y\u0302TXC\u22121/2\n=UC\u0303\u039bC\u0303U T C\u0303 .\n(20)\nThe similarity is maximised when the orthogonal matrix C1/2PXT\u0303 matches the principal eigenvalues of C\u0303, i.e. PXT\u0303 = C \u22121/2U\u0302C\u0303. In general PXT\u0303PT\u0303X = C\u22121/2U\u0302C\u0303U\u0302 T C\u0303\nC1/2 is not a symmetric matrix, and so it is not possible to define an orthonormal PXT such that PTX = P T XT . Consistently with the case of sample-space PCovR, we obtain\nPXT =C \u22121/2U\u0302C\u0303\u039b\u0302\n1/2 C\u0303\nPTX =\u039b\u0302 \u22121/2 C\u0303 U\u0302 T C\u0303 C1/2 PTY =\u039b\u0302 \u22121/2 C\u0303 U\u0302 T C\u0303 C\u22121/2XTY,\n(21)\nwhich minimise the PCovR loss in Eq. (11). These projections reduce to PCA as \u03b1\u2192 1 and\u2014if the dimension of the latent space is at least as large as the number of target properties in Y\u2014reduce to LR as \u03b1\u2192 0.\nFigure 3 demonstrates the behaviour of PCovR when applied to the analysis of the CSD-1000r dataset. Here we plot `proj and `regr as a function of \u03b1. The thumbnails above the losses correspond to the projections T = XPXY and regressions Y\u0302 = TPTY for the indicated \u03b1 below. Animations of these thumbnails are also given in the SI in gif format.\nFor \u03b1 = 0, we recover the accuracy of pure LR in predicting the values of the chemical shielding, but obtain a latent space that misses completely the structure of the dataset. The first principal component reflects the LR\nweight vector PXY , and the second carries no meaningful information. For \u03b1 = 1, we recover the PCA projection, that separates clearly the environments based on the nature of the central atom. A linear model built in the twodimensional latent space, however, performs very poorly, because there is no linear correlation between the position in latent space and the shielding values. Intermediate values of \u03b1 yield a projection that achieves the best of both worlds. The regression error is close to that of pure LR, but the error in the reconstruction of the input data from the latent space is now only marginally increased compared to pure PCA.\nThe PCovR map that corresponds to this \u201coptimal\u201d value of \u03b1 achieves `proj = 0.585 (comparable to the PCA value of 0.460) and `proj = 0.112 (comparable to the LR value of of 0.111). Considering the poor performance of PCA in regression (`regr = 0.928) and LR in projection (`proj = 0.963), it is clear that the latent-space description of the dataset achieves a more versatile representation of structure-property relations. There is still a recognisable clustering of the environments according to central atom species, but the O cluster, that exhibits the largest variance in the values of the shielding, is spread out diagonally so as to achieve maximal correlation between the position in latent space and value of the target properties. We propose that \u2013 in the absence of specific reasons suggesting to emphasise solely the regression or the projection accuracy \u2013 an optimal value of \u03b1 can be obtained looking for the minimum in `proj + `regr."}, {"heading": "D. Kernel Methods", "text": "While linear methods have the beauty of simplicity, they rely on the knowledge of a sufficient number of informative features that reflect the relation between inputs and properties. Kernel methods introduce a possibly nonlinear relation between samples in the form of a positivedefinite kernel function k(x,x\u2032) (e.g. the Gaussian kernel exp(\u2212\u2016x \u2212 x\u2032\u20162), or the linear kernel x \u00b7 x\u2032), and use it to define a higher-dimensional space in which data points serve effectively as an adaptive basis37. Unless otherwise specified, here we use a radial basis function (RBF) kernel, exp(\u2212\u03b3\u2016x\u2212 x\u2032\u20162), with the hyperparameter \u03b3 optimized for each data set, as shown in the SI. Doing so can help uncover non-linear relationships between the samples, resulting ultimately in more effective determination of a low-dimensional latent space and increased regression performance.\nMercer\u2019s theorem38 guarantees that given a positive definite kernel there is a linear operator \u03c6(x) that maps input features into a (possibly infinite-dimensional) reproducing kernel Hilbert space (RKHS)37 whose scalar product generates the kernel, i.e. \u03c6(x) \u00b7 \u03c6(x\u2032) = k(x,x\u2032). \u03c6(x) is not necessarily known explicitly, but as we will see it can be approximated effectively for a given dataset, and we will use the notation \u03a6 to indicate the feature matrix that contains the (approximate) values of the kernel features for all of the sample points. We indicate with K = \u03a6\u03a6T the nsamples \u00d7 nsamples matrix that contains as entries the values of the kernel function between\n6 every pair of samples. In the case of a linear kernel, this is simply the Gram matrix computed for the input features, while for a non-linear kernel its entries can be computed by evaluating the kernel between pairs of samples, Kij = k(xi,xj). Analogously to what we did for linear methods, we centre and normalise all the kernels we use in this work. Some subtleties connected to the centring operation are discussed in Appendix A"}, {"heading": "1. Kernel Principal Component Analysis", "text": "Kernel principal component analysis31 proceeds parallel to classical MDS, to which it corresponds exactly when a linear kernel is used. To construct a KPCA decomposition, one computes the eigendecomposition of the kernel matrix K = UK\u039bKU T K and defines the projections as the principal components T = U\u0302K\u039b\u0302 1/2 K . i The projections can also be computed as T = KPKT = KU\u0302K\u039b\u0302 \u22121/2 K , and this second expression can be used to project new data (in place of K we use the matrix containing the values of the kernel matrix between new and reference points) in the approximate RKHS defined by the original samples. One can also approximate the kernel using the projector PTK = \u039b\u0302 \u22121 K T\nTK. As shown in Fig. 2c, for this dataset there is little qualitative difference between what we obtained with plain PCA and the KPCA projection. This is because SOAP features exhibit very clear correlations with the nature of the central environments, which is already well represented with a linear model. While it is possible to compute the loss `proj = \u2016X \u2212 TPTX\u20162/nsamples associated with the approximation of X based on T, the aim of KPCA is to approximate the kernel, and it is more appropriate to judge the methods performance based on a Gram loss `gram = \u2016K \u2212 TTT \u20162/nsamples, which reduces to Eq. (7) for linear kernels. Alternatively, one can compute a projection loss based on the approximation of RHKS features, `proj = \u2016\u03a6 \u2212 TPT\u03a6\u20162/nsamples, as detailed in Appendix B."}, {"heading": "2. Kernel Ridge Regression", "text": "Kernel ridge regression39,40 is analogous to ridge regression, except that the kernel feature space vectors \u03a6 are substituted for the original input data X, giving the loss\n` = \u2016Y \u2212\u03a6P\u03a6Y \u20162 + \u03bb\u2016P\u03a6Y \u20162, (22) so that the optimal weights are\nP\u03a6Y = ( \u03a6T\u03a6 + \u03bbI )\u22121 \u03a6TY\n= \u03a6T ( \u03a6\u03a6T + \u03bbI )\u22121 Y.\n(23)\nPredicted properties Y\u0302 can then be evaluated with Y\u0302 = \u03a6P\u03a6Y . One can avoid computing explicitly the\ni If one retains all the nsamples eigenvectors, T corresponds to an exact approximation of the kernel features for the given dataset, as TTT = K = \u03a6\u03a6T .\nRKHS features by redefining the weights as PKY =( \u03a6\u03a6T + \u03bbI )\u22121 Y = (K + \u03bbI) \u22121 Y so that P\u03a6Y =\n\u03a6TPKY . 41 We can then write the predicted properties as\nY\u0302 = \u03a6\u03a6TPKY = KPKY . (24)\nAs shown in Fig. 2d, the greater flexibility afforded by a kernel model reduces the error by over 70%."}, {"heading": "E. Sparse Kernel Methods", "text": "Since the size of kernel matrices grows in n2 with respect to the number of samples, one wants to avoid computing (and inverting) the whole kernel matrix for large datasets. Instead, we can formulate a low-rank approximation to the kernel matrix through the Nystro\u0308m approximation42, using a sub-selection of the data points, the active set, to define an approximate RKHS. These representative points can be selected in a variety of ways; two straightforward methods that have been used successfully in atomistic modelling are farthest point sampling (FPS)43 and a CUR matrix decomposition44\u201346.\nUsing the subscript N to represent the full set of training data and M to indicate the active set, one can explicitly construct the approximate feature matrix as \u03a6NM = KNMUKMM \u039b \u22121/2 KMM\n, where UMM and \u039bMM are from the eigendecomposition of KMM . All sparse kernel methods can be derived in terms of a linear method based on the RKHS, although it is often possible to avoid explicitly computing \u03a6NM . For instance, the approximate kernel matrix takes the form42\nK \u2248 K\u0302NN = \u03a6NM\u03a6TNM = KNMK\u22121MMK T NM . (25)\nFor the following methods, we consider the approximate feature matrix \u03a6NM to be centred and scaled as discussed in Appendix A)."}, {"heading": "1. Sparse Kernel Principal Component Analysis", "text": "We can define the covariance in the kernel feature space along with its eigendecomposition,\nC = \u03a6TNM\u03a6NM = UC\u039bCU T C, (26)\nand subsequently compute the projections analogously to standard KPCA\nT = \u03a6NMU\u0302C = KNMUKMM \u039b \u22121/2 KMM\nU\u0302C = KNMPKT , (27)\nwhich effectively determine the directions of maximum variance of the samples in the active RHKS.\nFig. 2e shows that with an active set size of just 50 samples (out of more than 12,000), selected by FPS46, one can obtain a KPCA latent projection that matches very well the qualitative features of the full KPCA construction."}, {"heading": "2. Sparse Kernel Ridge Regression", "text": "In sparse KRR we proceed as in standard KRR, but use the feature matrix from the Nystro\u0308m approximation. The corresponding regularised LR loss in the kernel feature space is\n` = \u2016Y \u2212\u03a6NMP\u03a6Y \u20162 + \u03bb\u2016P\u03a6Y \u20162 (28)\nfor which the solution is P\u03a6Y = ( \u03a6TNM\u03a6NM + \u03bbI )\u22121 \u03a6TNMY\n= ( \u03a6TNM\u03a6NM + \u03bbI )\u22121 \u039b \u22121/2 KMM UTKMM K T NMY.\n(29)\nAlternatively, we can redefine the weights so that\nY\u0302 = \u03a6NMP\u03a6Y = KNMUKMM \u039b \u22121/2 KMM\nP\u03a6Y = KNMPKY , (30)\nfrom which we see that\nPKY = UKMM \u039b \u22121/2 KMM P\u03a6Y\n= UKMM \u039b \u22121/2 KMM ( \u03a6TNM\u03a6NM + \u03bbI )\u22121 \u00d7\u039b\u22121/2KMM U T KMM K T NMY.\n(31)\nBy writing out explicitly \u03a6TNM\u03a6NM in terms of KNM we obtain40\nPKY = ( KTNMKNM + \u03bbKMM )\u22121 KTNMY. (32)\nAs shown in Fig. 2f, an active set size of 50 is not sufficient to achieve an accurate regression model, and the error is larger than with a linear regression method. However,\nthe error can be reduced systematically by increasing the size of the active set, finding the best balance between accuracy and cost (see SI)."}, {"heading": "III. Extensions to Principal Covariates Regression", "text": "After having summarised existing linear and kernel methods for feature approximation and property prediction, we now introduce kernelized PCovR (KPCovR), as a way to combine the conceptual framework of PCovR and the non-linear features afforded by a kernel method."}, {"heading": "A. Full kernel PCovR", "text": "We start by constructing the augmented kernel matrix as a combination of KPCA and KRR. In particular, we substitute \u03a6 for X and the KRR solution of Y, Y\u0302 = K (K + \u03bbI) \u22121 Y, for Y, so that we have\nK\u0303 = \u03b1K + (1\u2212 \u03b1)Y\u0302Y\u0302T , (33) where we consider the kernel matrix to be standardised in a way that is equivalent to normalising \u03a6 (see Appendix A). Just as in PCovR, the unit variance projections\nT\u0303 are given by the top eigenvectors U\u0302K\u0303 of K\u0303, and the non-whitened projections as T = U\u0302K\u0303\u039b\u0302 1/2 K\u0303 = K\u0303U\u0302K\u0303\u039b\u0302 \u22121/2 K\u0303 , corresponding to the RKHS \u03a6\u0303 associated with the PCovR kernel [Eq. (33)].\n8\nProjecting a new set of structures in the kernel PCovR space entails computing the RHKS between the samples that were originally used to determine the KPCovR features and the new samples. Given that one may not want to compute these explicitly, it is useful to define a projection acting directly on the kernel, such that T = KPKT :\nPKT =( \u03b1I + (1\u2212 \u03b1) (K + \u03bbI)\u22121 YY\u0302T ) U\u0302K\u0303\u039b\u0302 \u22121/2 K\u0303 .\n(34)\nWe also determine the matrix that enables predictions of properties from the latent space T through LR, just as in the linear case [Eq. (18)]. Computing the projection loss minimised by KPCovR, `proj = \u2016\u03a6\u2212TPT\u03a6\u20162/nsamples, is trivial if one computes explicitly a RKHS approximation of \u03a6, but it requires some work if one wants to avoid evaluating \u03a6 (see Appendix B).\nAs shown in Fig. 4, the method combines a behaviour similar to linear PCovR with the improved property prediction accuracy afforded by kernel methods. In the the low-\u03b1 regime the regression accuracy approaches that of KRR, and the projection accuracy converges to a KPCAlike behaviour for \u03b1 \u2248 1. For the optimal value of \u03b1, the latent-space map (shown in Fig. 5b) combines a clear separation of structurally-distinct clusters with a 70% reduction in regression error when compared to linear PCovR (`regr = 0.026 vs. `regr = 0.112)."}, {"heading": "B. Sparse Kernel PCovR", "text": "Our derivation of the sparse version of KPCovR can be obtained almost directly from that of feature-space PCovR by taking explicitly the projection of the kernel on the active RKHS \u03a6NM = KNMUKMM \u039b \u22121/2 KMM\n. One can then define the covariance of the active kernel features\nC = \u03a6TNM\u03a6NM (35)\n= \u039b \u22121/2 KMM UTKMM K T NMKNMUKMM \u039b \u22121/2 KMM , (36)\nand use it in the definition of the modified KPCovR covariance\nC\u0303 =\u03b1C\n+(1\u2212 \u03b1)C1/2 (C + \u03bbI)\u22121 \u039b\u22121/2KMM U T KMM K T NMY \u00d7YTKNMUKMM \u039b \u22121/2 KMM (C + \u03bbI) \u22121 C1/2.\n(37)\nWith these definitions, the projection matrices onto the (sparse) KPCovR latent space and onto the latent-spacerestricted properties, analogous to Eq. (21), read\nPKT =UKMM \u039b \u22121/2 KMM C\u22121/2U\u0302C\u0303\u039b\u0302 1/2 C\u0303 PTY =\u039b\u0302 \u22121/2 C\u0303 U\u0302 T C\u0303 C\u22121/2\u039b \u22121/2 KMM UTKMM K T NMY. (38)\nSimilar to what we observed for sparse KPCA and KRR, reducing the active space to 50 active samples preserves the qualitative features of the latent-space map, but leads to substantial loss of performance for regression (Fig. 5c). The error, however, is equal to that observed for sparse KRR, which indicates that it is due to the limited active space size, and not by the dimensionality reduction."}, {"heading": "IV. Examples", "text": "Up until this point, we have talked about the definition of KPCovR in an abstract, equations-heavy manner. Here,\nwe will demonstrate the usage of such method for a wide range of materials science datasets. These datasets have all been already published elsewhere, and we leave to the SI a precise discussion of their structure, content and provenance, as well as a thorough analysis of the behaviour of the different linear and kernel methods applied to each data set. Here we limit ourselves to the most salient observations, and summarise the insights that could be relevant to the application of (K)PCovR to other materials and molecular datasets. We also distribute data files in the supplementary information that can be viewed with the interactive structure-property explorer chemiscope47. We encourage the reader to use them to gain a more interactive support to follow the discussion in this section.\nFor each dataset, we trained machine learning models on a randomly-chosen half of the included samples, and then evaluated these models on the remaining samples. In the following section, we report losses, errors and figures on the validation set of points only. The total number of samples we considered are available in Table I, together with performance metrics that show that PCovR-like methods achieve consistently an excellent compromise between the optimisation of `proj and `regr, and demonstrate that KPCovR outperforms by a large margin its non-kernelized counterpart in terms of regression performance."}, {"heading": "A. Carbon", "text": "We apply KPCovR to the C-VII carbon dataset, which contains roughly 11,000 carbon structures generated using Ab Initio Random Structure Searching (AIRSS) at\n10GPa48,49. Here, a KRR model predicts the average per-atom energy of each structure with a RMSE of 0.055 eV/atom (equivalent to `regr = 0.0707) yet can only describe 4% of the latent space variance (i.e. `proj = 0.96). The KPCA model retains 76% of the latent space variance but with a RMSE of 0.46 eV/atom (`proj = 0.9447). By comparison, at the optimal \u03b1 value, KPCovR sacrifices little in regression accuracy (0.062 eV/atom, `regr = 0.0753) and latent space variance retention (55%).\nAdditionally, KPCovR provides a more intuitive qualitative picture for understanding the dataset. In the original KPCA projection, the principal components correlate strongly with the dimensionality of the carbon structures, with linear nanowires in the lower right of the projection, sheets and planar structures above to the left, and 3D structures conglomerated in the upper left. The KPCovR projection does not only show a much clearer correlation between the position on the map and the stability of each configuration, but also provides more compact clustering of similar structures (Fig. 6(a)). The nanowires (typically linear carbon bonds) are located in the upper centre, the planar structures are partitioned in smaller clusters to the upper left, with clear sub-clusters associated with different ring patterns, and 3D crystals are distributed throughout the lower left and centre, with a well-separated cluster for tetrahedral carbon. The structural homogeneity of the clusters is confirmed by bond analysis \u2013 typically, the most common environments found in low energy 1D, 2D, and 3D carbon structures correspond to sp, sp2, and sp3 geometry. Angles between neighbouring bonds typically serve as a good proxy for detecting this environments,\n10\n\u2212154 -153 -152 -151 Energy (eV/atom)\n1 2 3 4 5 6 # Bonds per Carbon Atom\n90\n120\n150\n180\nBo nd\nA ng\nle [D\neg re\nes ]\nsp3sp 2\nsp\nPC1\nAIRSS Carbon\nDiamond-Like\nGraphiteLike Sheets\nMixedRing Sheets\nNanowires\nSimple Cubic-Like\nProjection Coloured by Bond Analysis\nKPCA Projection\nKPCA Projection\nKPCovR Projection\nKPCovR Projection\nPC 2\n(b)\n(a)\n0.0 2.0 4.0\n2.0\n0.0\n1.0\n-1.0\n0.0 2.0\n0.0\n2.0\nPC1\nPC 2\n1.0\n-1.0\n-2.0\nFIG. 6: KPCovR for AIRSS Carbon Structures and Energies. (a) Projections coloured by the per-atom energy. KPCovR projection at \u03b1 = 0.5. Clusters identified using chemiscope have been labelled with representative snapshots provided. The KPCA projection is given in the upper right inset, with highlighted structures in the KPCovR projection denoted by enlarged points. (b) Projections in (a) recoloured by the bond analysis, where yellow, blue and purple denote similarity to sp, sp2, and sp3 geometries, respectively, and grey signifying no resemblance.\nwhere the ideal values are 180\u25e6, 120\u25e6, 109.5\u25e6, respectively. Both KPCA and KPCovR detect clusters delineated by the number of bonds and bond angles (Fig. 6(b)), with these clusters arranged right-to-left in the KPCA projection, and top-to-bottom in the KPCovR projection. However, in the KPCovR projection, there is another gradient visible, with structures to the left of the projection more strongly coinciding with the ideal sp, sp2, and\nsp3, and those to the right being increasingly distorted.ii Thus, bond analysis reveals that, in addition to the clear delineation between structure dimensionality provided by KPCA, the inclusion of an energy regression criterion in the KPCovR loss leads to a map that more closely coincides with the conventional understanding of stable structures as those that have low distortion relative to the ideal carbon bonding geometries."}, {"heading": "B. Zeolites", "text": "We apply KPCovR to a subset of the Deem data set of hypothetical silica zeolites51, where the use of KPCA to construct an atlas of the building blocks of a zeolite was previously demonstrated by the authors50. By construction all frameworks in the dataset are based on tetrahedrally coordinated SiO4 units yet differ considerably in terms of molar energies and volumes. A KRR model based on an additive combination of environment Gaussian kernel, built using atom-centred SOAP features with a cutoff of 6.0A\u030a achieves an excellent accuracy in predicting the lattice energy (with an error around 1.88 kcal/mol), and molar volume (with an error around 1.97 cm3/mol), with `regr = 0.0686. However, the first 2 KPCA components correlate rather weakly with these properties. A data representation based on those provides information on the structural diversity, but describes only qualitatively structure-property relations. As shown in Table I KPCovR at the optimal \u03b1 provides a much more effective description; the latent space covers 61% of the structural diversity, while providing enough information to predict accurately lattice energy (2.64 kcal/mol) and molar volume (2.37 cm3/mol), `regr = 0.13. The map naturally orders structures between regular frameworks that have intermediate densities and low lattice energy 7(a,iii)., to the bottom, to frameworks with very large pores that have very low density and usually intermediate of high lattice energies. While there are no clear clusters emerging (which is not unexpected given the origin of the dataset as a high-throughput, random search) one can often observe that nearby structures exhibit similar structural motifs. For instance, most of the structures on the top left side of the map are associated with large 1D channels, as shown in Fig. 7(a,iii).\nFor a system exhibiting a combinatorial number of metastable structures, such as silica frameworks, an analysis based on the structural building blocks is often more insightful than the analysis of the overall structures. When using atom-centred features, or additive kernels built on them, it is natural to regard additive properties such as volume or energy as arising from a sum of environmental contributions, and to use these atomcentred environments as the building blocks to rationalise\nii The two notable exceptions\u2013a dark purple cluster middle centre and a grey cluster upper left\u2013 can be shown to be associated with bond angle distributions which are multimodal, leading to incorrect classification by a criterion based on the mean bond angles.\n11\nstructure-property relations. As discussed in appendix C, the construction of a regression model for the framework properties yields as a side-effect a data-driven partitioning, that can be used for a (K)PCovR analysis of such building blocks. The resulting representation (Fig. 7(b)) shows an excellent correlation between the position in a 2D representation and the predicted contributions to lattice energy and molar volume (site energy RMSE: 3.18 kJ/mol Si, site volume RMSE: 3.67 kJ/mol Si, 34% of structural variance). As observed in Ref. 50, and consistent with what is seen in the framework analysis, the thorough search of potential frameworks that underlies the construction of this dataset is reflected in the lack of substantial clustering of the environments. The bulk of the latent space is uniformly covered, and one does not see an obvious qualitative relation between properties and the local topology of the framework. Regular structures are mapped side-by-side to disordered environments. Only at the extremes one can recognise clearer patterns. (A few extremal structures and their location in the KPCovR projection are highlighted in Fig. 7(b).) High-energy (and hence poorly stable) building blocks can be distinguished between high-density structures that contain highly-strained 3-fold rings, and low-density structures, associated to the surface of a large cavity, and to \u2018pillar-like\u201d motifs that are present in the most highly porous frameworks (Fig. 7(b,vii)). Low-energy structures, in the lower side of the map, tend to have low and intermediate volume, and are predominantly associated with six- and four-member rings. These are however weak correlations, and in general there are no apparent patterns that correlate the framework topology and the position on the map. This confirms, in a more direct manner, the structure-property insights that were inferred by separate application of supervised and unsupervised algorithms in Ref. 50 \u2013 namely that, for four-coordinated silica frameworks, the topology of the network is a good predictor of energy and density only for\n12\nextremes. For the bulk of the possible binding motifs, a low-dimensional representation is not sufficient to capture the extreme structural diversity, and to rationalise the multitude of alternative building blocks that give rise to similar macroscopic materials properties."}, {"heading": "C. QM9", "text": "Our next case study regards the QM9 dataset, which contains over 133,000 molecules consisting of carbon, hydrogen, nitrogen, oxygen, and fluorine52,53, of which we use 10,000 for this study. To demonstrate the application of KPCovR to multi-target learning, we construct our models using all the 12 properties available in the dataset: internal energy at 0K and 298.15K, free energy at 298.15K, enthalpy at 298.15K, highest occupied molecular orbital (HOMO), lowest unoccupied molecular orbital (LUMO), HOMO-LUMO gap, heat capacity, zero point vibrational energy (ZPVE), dipole moment, isotropic polarisability, electronic spatial extent (ESE).\nHere, there is not a large qualitative difference between the two-dimensional KPCovR projection at intermediate \u03b1 and that constructed via KPCA, with the former retaining 63% of the variance compared to the latter\u2019s 66%. The energetic properties, which are well-represented by these first two principal components, are also well-correlated with the degree of unsaturation, and thus the structural diversity of the dataset. This is summarised in Fig. 8(a), where the projections for \u03b1 = 0.5 and \u03b1 = 1.0 are shown, coloured by the degree of unsaturationiii and with representative molecules highlighted. Besides the left-to-right saturated-to-unsaturated trend, the map position also correlates with the presence of O, N, F atoms, that increase from bottom to top.\nDue to the large number of properties used as targets, a low-dimensional latent space cannot achieve the same prediction accuracy as (kernel) ridge regression, for \u03b1 = 0 and by extension at intermediate values of \u03b1. It is necessary to retain a larger number of latent space components to obtain a model capable of effective regression, as seen in Table I, where `regr goes from 0.31 with nlatent = 2 to 0.07 with nlatent = 12, and seen in Fig. 8(b). For a given value of \u03b1, both regression and projection errors are bound to decrease when retaining a larger number of PCs. The optimal value of \u03b1, however, is not necessarily the same for increasing numbers of PCs particularly in datasets where `regr and `proj have a magnitude that varies with nlatent in a different way. In this case, however, the optimal \u03b1 is nearly constant, as shown in Fig. 8(b). The figure also shows a sudden drop in `proj for \u03b1 > 0. This discontinuity in the variance suggests that insufficient information is contained in the 12 properties to construct an orthogonal set of 12 principal components, and thus some properties must be highly correlated.\niii The degree of unsaturation, defined as d = C \u2212 H 2 \u2212 X 2 + N 2\n+ 1, where X is a halogen, estimates the number of rings and \u03c0 bonds in the molecule.\nModels constructed with fewer principal components can provide insight into the nature of the properties included, particularly in the cases weighted towards regression as \u03b1\u2192 0. In Fig. 8(c), we show the regression errors of the individual properties as a function of nlatent. For each property, the decay of `regr when incorporating a new principal component indicates how strongly the new feature and the property are correlated, and gives indirect information on the correlation between properties. For instance, we can see that (unsurprisingly) the internal energies, enthalpy and free energy are heavily correlated with each other, as their associated `regr decreases precisely in the same way, indicative of a strong correlation the first and fourth principal components. Fig. 8(d) shows color-coded maps of the 12 targets, using for each of them the two PCs that lead to the largest decrease in `regr."}, {"heading": "D. Arginine Dipeptide", "text": "We also applied KPCovR to the 4219 arginine dipeptide conformers that are collected in the Berlin amino acid database54, that was also investigated using a purely unsupervised dimensionality-reduction scheme55. The conformer energy was used as the target property for the purpose of constructing the KPCovR model. Fig. 9 shows the two-component KPCovR projection of the conformers in the test set at the optimal value of \u03b1 = 0.5 coloured by energy (a), radius of gyration (b), and peptide bond isomerism (c). Several individual conformers are also highlighted, including those with the highest and lowest energy and radius of gyration. For comparison, a KPCA projection of the same conformers is plotted in the inset in the upper right corner of each subplot. The KPCA projection alone represents well the different structural features (peptide bond isomerism and radius of gyration), but leads to rather poor energy regression (`regr = 0.60 as opposed to `regr = 0.004 for KPCovR). The KPCovR projection separates more clearly a group of high-energy conformers, to the left, and a cluster of very stable configurations, to the lower right. The former are characterized by having both peptide bonds in the cis configuration, and by an unfavourable steric interaction between the terminating methyl groups. The stable conformers, on the other hand, all have the naturally-preferred all-trans isomerism, and the backbone takes an extended \u03b2-strand structure. They only differ by the hydrogen-bonding pattern of the side-chain, that modulates in a more subtle way the conformational stability.\nThe inclusion of an explicit supervised learning component in KPCovR does not only lead to a dimensionality reduction that preserves with much higher accuracy the underlying structure-property relations, but reveals more clearly the molecular motifs that stabilize (or de-stabilize) the different conformers."}, {"heading": "E. Azaphenacenes", "text": "As a last example, we consider a dataset containing different crystallyne polymorphs of 28 isomers of a pyrrolebased azaphenacene compound, for which total energy and electronic mobility have been previously computed\n13\nin Ref. 58. This dataset present a series of challenges for KPCovR in particular and machine learning techniques in general, and as such is a good test for the new method presented in this paper. First, the dataset contains a very small number of structures (311), half of which were used for training the different models. Second, electronic mobility is an inherently non-local property, which makes it hard to predict it using local descriptors such as SOAP, even when the descriptors are grouped together in a structure kernel as discussed in appendix C. It provides, therefore, a demonstration of the robustness of KPCovR in presence of target properties that are noisy, or otherwise un-learnable.\nWe present the results of KPCovR on the validation set for different \u03b1 values Fig. 10, panels (a) and (b). On the pure regression, \u03b1 = 0 side, the prediction of energies is surprisingly good given the very low number of training points. The RMSE of 3.83 kJ/mol (equivalent to `regr = 0.038) is around a quarter of the dataset in intrinsic standard deviation of 14.3 kJ/mol. The predic-\ntion of electronic mobility is much harder. We used the logarithm of electronic mobilities instead of the raw values as the prediction target, as detailed in the supplementary information. Although this transformation improved our ability to learn electronic mobilities, the regression loss is very high (RMSE of 0.9; `regr = 0.482 which is more than 90% of the expected variance of 0.5). We are overall unable to learn electronic mobility for this dataset, similarly to what was already observed for this dataset58,59.\nLooking now at the optimal \u03b1 = 0.65 (i.e. the value of \u03b1 minimising the sum of the projection and regression losses), we observe that we are still unable to learn electronic mobility with a relative loss of 0.485 (equivalent to an error that is approximately 95% of the intrinsic variability of the data, and only marginally worse of the error for \u03b1 = 0), and the resulting prediction is visibly skewed in the parity plot. The poor regression performance for the log-mobility is reflected in the lack of a clear correlation between the position in latent space and the value of mobility.\n14\nEven if we are unable to predict the electronic mobility, the cohesive energy of the different polymorphs can be learned very effectively, and the optimal \u03b1 = 0.65 corresponds to an excellent balance between `regr and `proj. The latent-space projection separates the data set in two clusters along the vertical axis. This separation is related to the a bi-modal energy distribution, with low and high energy structures, which is lost in the limit of pure KPCA at \u03b1 = 1. In figure 10, we show an annotated map of the different crystal stackings, coloured by molecular identity. Different symbols indicate the average number of hydrogen bonds between molecules in the crystals, which we identified using a Probabilistic Analysis of Molecular Motifs (PAMM)56. We find that the cluster of low energy stacking contains only structures with 2 hydrogen bonds per molecules (the maximal possible value), while the high energy cluster contains mostly structures with 1 hydrogen bond per molecule. Additionally, the majority of structures in the low energy cluster are linked by hydrogen bonds in a DNA-like fashion, i.e. by having pairs of matching hydrogen bond donors and acceptors facing each another. Finally, the low energy cluster only contains crystal created from molecules 5, 6, 12 and 16 (following the notation from the original paper58), the 24 other molecules being in the high energy cluster. These four molecules are the only ones with just the right geometry to create matching, DNA-like hydrogen bonding patterns with two bonds per molecules, and high symmetry crystals. Isomer 9 and 18 also contain a similar N\u2013C\u2013NH motif, but cannot form a paired-HB pattern\nbecause of steric hindrance (9) and asymmetry(18)."}, {"heading": "V. Conclusions", "text": "In this paper we provide a comprehensive overview of linear and kernel-based methods for supervised and unsupervised learning, showing an example of their application to elucidate and predict structure-property relations in solid-state NMR. We also discuss a simple combination of principal component analysis and linear regression, PCovR18, that has as yet received far less attention than in our opinion it deserves. We derive extensions to PCovR that make it possible to use it in the context of kernel methods (KPCovR and sparse KPCovR), and demonstrate their application to five distinct datasets of molecules and materials. We also prepared a set of Jupyter notebooks27 that provide a pedagogic introduction to both traditional and novel methods we discuss, and allow exporting structure-property maps in a format that can be visualised with an interactive tool that we also developed as part of this work47.\nThe flexibility afforded by a kernel method allows improving substantially (typically by a factor of two) the regression performance relative to linear PCovR. Compared to kernel PCA, KPCovR maps reflect more explicitly structure-property relations, and \u2013 in all the diverse cases we considered \u2013 are more revealing, helping to identify the molecular motifs that determine the performance of the different structures, and that often reflect intuitive chemical concepts such as hybridisation, chemical composition, H-bond patterns. This study highlights the promise of\n15\ncombining supervised and unsupervised schemes in the analysis of data generated by atomistic modelling, to obtain clearer insights to guide the design of molecules and materials with improved performance, and to build more effective models to directly predict atomic-scale behaviour."}, {"heading": "Acknowledgments", "text": "MC, RKC, BAH acknowledge funding by the from the European Research Council (Horizon 2020 grant agreement no. 677013-HBMAP). GF acknowledge support by the SCCER Efficiency of Industrial Processes, and by the European Center of Excellence MaX, Materials at the Hexascale - GA No. 676598.\nSupporting Information\nThe electronic supporting information contains a comprehensive discussions of the parameters used for the analysis of each of the five examples, together with a comprehensive comparison of the performance of different linear, kernel and PCovR-like methods for the five data sets. For each data set we also provide an interactive map that can be visualized with the on-line viewer chemiscope47. A set of Juypyter notebooks that provide a hands-on tutorial for the application of KPCovR is available in a separate repository27."}, {"heading": "A. Centring and scaling", "text": "In this paper, as it is often done in machine-learning applications, we centre and scale (or standardise) the original input data, which removes the dependency of results on a trivial shifting or scaling of the data set. centring and scaling are of particular importance in PCovR-based methods, as the model can be inherently biased towards the projection or regression if X and Y data are of different relative magnitudes. To avoid ambiguity, we centre and scale our raw data X\u2032 and Y\u2032 in the following manner,\nX =\n\u221a ntrain ( X\u2032 \u2212 X\u0304\u2032train ) \u2016X\u2032train \u2212 X\u0304\u2032train\u2016\n(A1)\nYi =\n\u221a ntrain ( Y\u2032i \u2212 Y\u0304\u2032i,train ) \u221a nproperties\u2016Y\u2032i,train \u2212 Y\u0304\u2032i,train\u2016 , (A2)\nwhere Ai denotes the i th property (column) of A, A\u0304 is the columnwise mean of A, and Atrain indicates the subset of samples in A that belong to the training set. By centring and scaling the data in this manner, we ensure that the squared Frobenius norms of Xtrain and Ytrain are equal to nsamples, and that individual property variances of Ytrain are all equal to 1/nproperties.\nWe perform a similar centring and scaling procedure when constructing kernels. Kernel standardization can\n16\nbe viewed as simply centring and scaling the data in the RKHS feature space. If N indicates the dataset that defines the centring (typically the train set) and i, j two data points between which we want to compute the centred kernel,\nKij = nN ( \u03c6i \u2212 \u03a6\u0304N )T ( \u03c6j \u2212 \u03a6\u0304N ) Tr (( \u03a6N \u2212 \u03a6\u0304N ) ( \u03a6N \u2212 \u03a6\u0304N )T) (A3)\nwhere \u03a6\u0304N is the column mean of the training set feature matrix \u03a6N , and is computed once and for all for the train set, together with the normalisation factor. This can be written avoiding to compute explicitly the RKHS features:\nKij = ntrain\nTr KNN\n( K \u2032ij \u2212 \u2211 n\u2208N K \u2032in +K \u2032 jn ntrain + \u2211 nn\u2032\u2208N K \u2032nn\u2032 n2train ) .\n(A4) Centring is achieved by computing column averages of the raw kernels between points i, j and the train set points. Note that kernel matrix elements may refer to different matrices, depending on whether i and j are themselves train set points, or new inputs. The scaling factor ntrain/Tr KNN is computed using the centred train set kernel.\nIn sparse KPCA and sparse KPCovR, a slightly different approach is required, as the goal is to ensure that the Nystro\u0308m approximation to the full kernel matrix is centred and scaled properly \u2013 i.e. the active set kernel defines the RKHS, but centring and scaling should be computed based on the training set N . A centred and scaled kernel between an input i and an active point m \u2208M can then be computed as\nKim = ntrain\u221a Tr ( KNMK \u22121 MMK T NM\n) K \u2032im \u2212\u2211\nj\u2208N\nK \u2032jm ntrain  , (A5)\nwhere once more the normalisation factor is computed using the centred version of KNM ."}, {"heading": "B. Projection loss in kernel methods and KPCovR", "text": "Rewriting Eq. (11) in terms of the RKHS, we get:\n` = \u03b1\u2016\u03a6\u2212TPT\u03a6\u20162/nsamples+(1\u2212\u03b1)\u2016Y\u2212TPTY \u20162/nsamples. (B1) The latter portion of this equation, `regr, can be written in terms of the KRR loss given in Eq. (22), where P\u03a6Y also encapsulates the loss incurred from the latent space projection. The former portion, `proj is straightforward to compute given an explicit RKHS. In case one wants to avoid evaluating the RKHS, however, `proj may be computed in terms of the kernel.\nIndicating the kernel between set A and B as KAB , the projection of set A as TA, and with N and V as the train and validation/test set, one obtains\n`proj = Tr [ KV V \u2212 2KV NTN (TTNTN )\u22121TTV\n+TV (T T NTN ) \u22121TTNKNN TN (T T NTN ) \u22121TTV . ]\n(B2)\nWhen the loss is evaluated on the train set, so that N \u2261 V , this expression reduces to\n`proj = Tr (KNN \u2212KNNPKTPTK) . (B3) where PTK = (T T NTN )\n\u22121TTNKNN . A full derivation of this loss equation can be found in the SI."}, {"heading": "C. Structures and environments", "text": "When analyzing molecular or materials structures, there are several possible scenarios, involving the prediction of atom-centred or global properties, and the search for structural correlations between atomic environments or overall structures. Whenever the nature of the property and that of the structural entity match, the formalism we have reviewed in Section II applies straightforwardly. A common scenario that deserves a separate discussion involves the case in which one seeks to reveal how atomic environments or molecular fragments contribute to global properties of a material. Often, this means that the properties of a structure y(A) are written as a sum over contributions from the atom-centred environments in each structure, y(A) = \u2211 i\u2208A y(Xi). For a linear model, this means that the regression loss reads\n`regr = 1\nnsamples \u2211 A \u2016y(A)\u2212 \u2211 i\u2208A xiPXY \u20162 =\n= 1\nnsamples \u2211 A \u2016y(A)\u2212 x\u0303(A)PXY \u20162,\n(C1)\nwhere we defined x\u0303(A) = \u2211 i\u2208A xi. In other terms, one can formulate the regression using features that describe the structures as a sum of the features of their atoms, and then proceed to determine the weights PXY as in conventional linear regression. A similar expression holds for kernel methods, where the kernels between structures can be built as sums over kernels between environments, resulting in an additive property model. In a (K)PCovR framework, where one is restricted to learning the fraction of the properties that can be approximated as a (kernelized) linear function of x, one should first train a model based on the full structures, and then compute the predictions for individual environments. These are combined to form the approximate property matrix Y\u0302. The model can then be built as in the homogeneous case of environment features and atom-centred properties.\n1F. A. Faber, A. Lindmaa, O. A. von Lilienfeld, and R. Armiento, Physical Review Letters 117, 135502 (2016). 2F. A. Faber, L. Hutchison, B. Huang, J. Gilmer, S. S. Schoenholz, G. E. Dahl, O. Vinyals, S. Kearnes, P. F. Riley, and O. A. von Lilienfeld, Journal of Chemical Theory and Computation 13, 5255 (2017). 3K. Hansen, G. Montavon, F. Biegler, S. Fazli, M. Rupp, M. Scheffler, O. A. von Lilienfeld, A. Tkatchenko, and K.-R. Mller, Journal of Chemical Theory and Computation 9, 3404 (2013). 4M. Rupp, A. Tkatchenko, K.-R. Mller, and O. A. von Lilienfeld, Physical Review Letters 108, 058301 (2012). 5V. L. Deringer and G. Csnyi, Physical Review B 95, 094203 (2017). 6D. Dragoni, T. D. Daff, G. Csnyi, and N. Marzari, Physical Review Materials 2, 013808 (2018).\n17\n7J.-B. Maillet, C. Denoual, and G. Csnyi, AIP Conference Proceedings 1979, 050011 (2018). 8W. J. Szlachta, A. P. Bartk, and G. Csnyi, Physical Review B 90, 104108 (2014). 9C. M. Simon, J. Kim, D. A. Gomez-Gualdron, J. S. Camp, Y. G. Chung, R. L. Martin, R. Mercado, M. W. Deem, D. Gunter, M. Haranczyk, D. S. Sholl, R. Q. Snurr, and B. Smit, Energy & Environmental Science 8, 1190 (2015). 10A. D. Sendek, Q. Yang, E. D. Cubuk, K.-A. N. Duerloo, Y. Cui, and E. J. Reed, Energy & Environmental Science 10, 306 (2017). 11L. Kahle, A. Marcolongo, and N. Marzari, Energy & Environmental Science (2020), 10.1039/C9EE02457C. 12S. Kirklin, B. Meredig, and C. Wolverton, Advanced Energy Materials 3, 252 (2013). 13M. Ceriotti, J. Chem. Phys. 150, 150901 (2019). 14I. T. Jolliffe, Journal of the Royal Statistical Society. Series C\n(Applied Statistics) 31, 300 (1982). 15S. Wold, M. Sjstrm, and L. Eriksson, Chemometrics and Intelli-\ngent Laboratory Systems PLS Methods, 58, 109 (2001). 16H. Spth, Computing 22, 367 (1979). 17M. Stone and R. J. Brooks, Journal of the Royal Statistical Society.\nSeries B (Methodological) 52, 237 (1990). 18S. de Jong and H. A. L. Kiers, Chemometrics and Intelligent Lab-\noratory Systems Proceedings of the 2nd Scandinavian Symposium on Chemometrics, 14, 155 (1992). 19M. Vervloet, K. Van Deun, W. Van den Noortgate, and E. Ceulemans, Chemometrics and Intelligent Laboratory Systems 123, 36 (2013). 20M. Vervloet, H. A. L. Kiers, W. V. d. Noortgate, and E. Ceulemans, Journal of Statistical Software 65, 1 (2015). 21M. Vervloet, K. Van Deun, W. Van den Noortgate, and E. Ceulemans, Chemometrics and Intelligent Laboratory Systems 151, 26 (2016). 22M. J. Fischer, Journal of Geophysical Research: Atmospheres 119, 1266 (2014). 23C. Heij, P. J. Groenen, and D. van Dijk, Computational Statistics & Data Analysis 51, 3612 (2007). 24K. Van Deun, E. A. V. Crompvoets, and E. Ceulemans, BMC Bioinformatics 19, 104 (2018). 25M. K. Taylor, D. K. Sullivan, E. F. Ellerbeck, B. J. Gajewski, and H. D. Gibbs, Public Health Nutrition 22, 21572169 (2019). 26T. F. Wilderjans, E. Vande Gaer, H. A. L. Kiers, I. Van Mechelen, and E. Ceulemans, Psychometrika 82, 86 (2017). 27\u201cA set of utilities and pedagogic notebooks for the use of linear and kernel methods in atomistic modeling.\u201d . 28M. Ceriotti, L. Emsley, P. Federico, A. Hofstetter, F. Musil, S. De, E. A. Engel, and A. Anelli, (2019), 10.24435/MATERIALSCLOUD:2019.0023. 29A. P. Barto\u0301k, R. Kondor, and G. Csa\u0301nyi, Physical Review B 87, 184115 (2013). 30M. J. Willatt, F. Musil, and M. Ceriotti, J. Chem. Phys. 150, 154110 (2019). 31B. Schlkopf, A. Smola, and K. Mller, Neural Computation 10, 1299 (1998). 32T. Hastie, R. Tibshirani, and J. Friedman, Elements of Statistical Learning: Data Mining, Inference, and Prediction, 2nd ed., Springer Series in Statistics (Springer, 2008).\n33C. M. Bishop, Pattern Recognition and Machine Learning, Information Science and Statistics (Springer, 2006). 34K. P. F.R.S, The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science 2, 559 (1901). 35H. Hotelling, Journal of Educational Psychology 24, 417 (1933). 36W. S. Torgerson, Psychometrika 17, 401 (1952). 37M. Cuturi, ArXiv Prepr. ArXiv09115367 (2009). 38J. Mercer and A. R. Forsyth, Philosophical Transactions of the\nRoyal Society of London. Series A, Containing Papers of a Mathematical or Physical Character 209, 415 (1909). 39F. Girosi, M. Jones, and T. Poggio, Neural Computation 7, 219 (1995). 40A. J. Smola and B. Schkopf, in Proceedings of the Seventeenth International Conference on Machine Learning , ICML \u201900 (Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 2000) pp. 911\u2013918. 41K. P. Murphy, Machine Learning: A Probabilistic Perspective (MIT Press, 2012). 42C. K. I. Williams and M. Seeger, in Advances in Neural Information Processing Systems 13 , edited by T. K. Leen, T. G. Dietterich, and V. Tresp (MIT Press, 2001) pp. 682\u2013688. 43Y. Eldar, M. Lindenbaum, M. Porat, and Y. Zeevi, IEEE Transactions on Image Processing 6, 1305 (1997). 44M. W. Mahoney and P. Drineas, Proceedings of the National Academy of Sciences 106, 697 (2009), https://www.pnas.org/content/106/3/697.full.pdf. 45A. P. Barto\u0301k and G. Csa\u0301nyi, Int. J. Quantum Chem. 115, 1051 (2015). 46G. Imbalzano, A. Anelli, D. Giofre\u0301, S. Klees, J. Behler, and M. Ceriotti, J. Chem. Phys. 148, 241730 (2018). 47\u201cChemiscope: an interactive structure/property explorer for materials and molecules,\u201d https://chemiscope.org. 48C. J. Pickard and R. J. Needs, J. Phys. Condens. Matter 23, 053201 (2011). 49C. J. Pickard, \u201cAirss data for carbon at 10gpa and the c+n+h+o system at 1gpa,\u201d (2020). 50B. A. Helfrecht, R. Semino, G. Pireddu, S. M. Auerbach, and M. Ceriotti, J. Chem. Phys. 151, 154112 (2019). 51R. Pophale, P. A. Cheeseman, and M. W. Deem, Physical Chemistry Chemical Physics 13, 12407 (2011), the SLC-PCOD database can be accessed at: http://www.hypotheticalzeolites. net/DATABASE/DEEM/DEEM_PCOD/index.php. 52R. Ramakrishnan, P. O. Dral, M. Rupp, and O. A. von Lilienfeld, Scientific Data 1, 140022 (2014). 53R. Ramakrishnan, P. Dral, M. Rupp, and O. Anatole von Lilienfeld, \u201cQuantum chemistry structures and properties of 134 kilo molecules,\u201d (2014). 54M. Ropo, M. Schneider, C. Baldauf, and V. Blum, Scientific Data 3, 1 (2016), the Berlin amino acid database can be accessed at: https://aminoaciddb.rz-berlin.mpg.de/. 55S. De, F. Musil, T. Ingram, C. Baldauf, and M. Ceriotti, J. Cheminformatics 9, 1 (2017). 56P. Gasparotto and M. Ceriotti, The Journal of Chemical Physics 141, 174110 (2014). 57K. Momma and F. Izumi, Journal of Applied Crystallography 44, 1272 (2011). 58J. Yang, S. De, J. E. Campbell, S. Li, M. Ceriotti, and G. M. Day, Chemistry of Materials 30, 4361 (2018). 59F. Musil, S. De, J. Yang, J. E. Campbell, G. M. Day, and M. Ceriotti, Chem. Sci. 9, 1289 (2018)."}], "title": "Structure-Property Maps with Kernel Principal Covariates Regression", "year": 2020}