{
  "abstractText": "Feature selection is an important challenge in machine learning. It plays a crucial role in the explainability of machine-driven decisions that are rapidly permeating throughout modern society. Unfortunately, the explosion in the size and dimensionality of real-world datasets poses a severe challenge to standard feature selection algorithms. Today, it is not uncommon for datasets to have billions of dimensions. At such scale, even storing the feature vector is impossible, causing most existing feature selection methods to fail. Workarounds like feature hashing, a standard approach to large-scale machine learning, helps with the computational feasibility, but at the cost of losing the interpretability of features. In this paper, we present MISSION, a novel framework for ultra large-scale feature selection that performs stochastic gradient descent while maintaining an efficient representation of the features in memory using a Count-Sketch data structure. MISSION retains the simplicity of feature hashing without sacrificing the interpretability of the features while using only O(log p) working memory. We demonstrate that MISSION accurately and efficiently performs feature selection on real-world, large-scale datasets with billions of dimensions.",
  "authors": [
    {
      "affiliations": [],
      "name": "Amirali Aghazadeh"
    },
    {
      "affiliations": [],
      "name": "Ryan Spring"
    },
    {
      "affiliations": [],
      "name": "Daniel LeJeune"
    },
    {
      "affiliations": [],
      "name": "Gautam Dasarathy"
    },
    {
      "affiliations": [],
      "name": "Anshumali Shrivastava"
    },
    {
      "affiliations": [],
      "name": "Richard G. Baraniuk"
    }
  ],
  "id": "SP:ae2ef0ff8de98a5e0ecf18e57a6f5e668c6352d7",
  "references": [
    {
      "authors": [
        "A. Agarwal",
        "O. Chapelle",
        "M. Dud\u0131\u0301k",
        "J. Langford"
      ],
      "title": "A reliable effective terascale linear learning system",
      "venue": "Journal of Machine Learning Research,",
      "year": 2014
    },
    {
      "authors": [
        "A. Aghazadeh",
        "A.Y. Lin",
        "M.A. Sheikh",
        "A.L. Chen",
        "L.M. Atkins",
        "C.L. Johnson",
        "J.F. Petrosino",
        "R.A. Drezek",
        "R.G. Baraniuk"
      ],
      "title": "Universal microbial diagnostics using random dna probes",
      "venue": "Science advances,",
      "year": 2016
    },
    {
      "authors": [
        "S. Basu",
        "K. Kumbier",
        "J.B. Brown",
        "B. Yu"
      ],
      "title": "Iterative random forests to discover predictive and stable highorder interactions",
      "venue": "Proceedings of the National Academy of Sciences,",
      "year": 2018
    },
    {
      "authors": [
        "T. Blumensath",
        "M.E. Davies"
      ],
      "title": "Iterative hard thresholding for compressed sensing",
      "venue": "Applied and Computational Harmonic Analysis,",
      "year": 2009
    },
    {
      "authors": [
        "N. Bray",
        "H. Pimentel",
        "P. Melsted",
        "L. Pachter"
      ],
      "title": "Near-optimal RNA-Seq quantification",
      "venue": "arXiv preprint arXiv:1505.02710,",
      "year": 2015
    },
    {
      "authors": [
        "A. Broder",
        "M. Mitzenmacher"
      ],
      "title": "Network applications of bloom filters: A survey",
      "venue": "Internet mathematics,",
      "year": 2004
    },
    {
      "authors": [
        "M. Charikar",
        "K. Chen",
        "M. Farach-Colton"
      ],
      "title": "Finding frequent items in data streams",
      "venue": "In International Colloquium on Automata, Languages, and Programming,",
      "year": 2002
    },
    {
      "authors": [
        "J. Duchi",
        "S. Shalev-Shwartz",
        "Y. Singer",
        "T. Chandra"
      ],
      "title": "Efficient projections onto the `1-ball for learning in high dimensions",
      "venue": "In Proceedings of the 25th International Conference on Machine Learning,",
      "year": 2008
    },
    {
      "authors": [
        "P. Indyk"
      ],
      "title": "Sketching via hashing: From heavy hitters to compressed sensing to sparse fourier transform",
      "venue": "In Proceedings of the 32nd ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems,",
      "year": 2013
    },
    {
      "authors": [
        "P. Jain",
        "A. Tewari",
        "P. Kar"
      ],
      "title": "On iterative hard thresholding methods for high-dimensional m-estimation",
      "venue": "In Advances in Neural Information Processing Systems,",
      "year": 2014
    },
    {
      "authors": [
        "P. Jain",
        "A. Tewari",
        "I.S. Dhillon"
      ],
      "title": "Partial hard thresholding",
      "venue": "IEEE Transactions on Information Theory,",
      "year": 2017
    },
    {
      "authors": [
        "J. Langford",
        "L. Li",
        "T. Zhang"
      ],
      "title": "Sparse online learning via truncated gradient",
      "venue": "Journal of Machine Learning Research,",
      "year": 2009
    },
    {
      "authors": [
        "A. Maleki"
      ],
      "title": "Coherence analysis of iterative thresholding algorithms",
      "venue": "47th Annual Allerton Conference on,",
      "year": 2009
    },
    {
      "authors": [
        "T. Mikolov",
        "K. Chen",
        "G. Corrado",
        "J. Dean"
      ],
      "title": "Efficient estimation of word representations in vector space",
      "venue": "arXiv preprint arXiv:1301.3781,",
      "year": 2013
    },
    {
      "authors": [
        "S. Shalev-Shwartz",
        "A. Tewari"
      ],
      "title": "Stochastic methods for `1-regularized loss minimization",
      "venue": "Journal of Machine Learning Research,",
      "year": 2011
    },
    {
      "authors": [
        "M. Tan",
        "I.W. Tsang",
        "L. Wang"
      ],
      "title": "Towards ultrahigh dimensional feature selection for big data",
      "venue": "Journal of Machine Learning Research,",
      "year": 2014
    },
    {
      "authors": [
        "K. Vervier",
        "P. Mah\u00e9",
        "M. Tournoud",
        "Veyrieras",
        "J.-B",
        "Vert",
        "J.-P"
      ],
      "title": "Large-scale machine learning for metagenomics sequence classification",
      "year": 2016
    },
    {
      "authors": [
        "K. Weinberger",
        "A. Dasgupta",
        "J. Langford",
        "A. Smola",
        "J. Attenberg"
      ],
      "title": "Feature hashing for large scale multitask learning",
      "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,",
      "year": 2009
    },
    {
      "authors": [
        "D.E. Wood",
        "S.L. Salzberg"
      ],
      "title": "Kraken: Ultrafast metagenomic sequence classification using exact alignments",
      "venue": "Genome Biology,",
      "year": 2014
    }
  ],
  "sections": [
    {
      "heading": "1. Introduction",
      "text": "Feature selection is an important step in extracting interpretable patterns from data. It has numerous applications in a wide range of areas, including natural-language processing, genomics, and chemistry. Suppose that there are n ordered pairs (Xi, yi)i\u2208[n], where Xi \u2208 Rp are p-dimensional\n*These authors contributed equally and are listed alphabetically 1Department of Electrical Engineering, Stanford University, Stanford, California 2Department of Computer Science, Rice University, Houston, Texas 3Department of Electrical and Computer Engineering, Rice University, Houston, Texas. Correspondence to: Anshumali Shrivastava <anshumali@rice.edu>.\nfeature vectors, and yi \u2208 R are scalar outputs. Feature selection aims to identify a small subset of features (coordinates of the p-dimensional feature vector) that best models the relationship between the data Xi and the output yi.\nA significant complication that is common in modern engineering and scientific applications is that the feature space p is ultra high-dimensional. For example, Weinberger introduced a dataset with 16 trillion (p = 1013) unique features (Weinberger et al., 2009). A 16 trillion dimensional feature vector (of double 8 bytes) requires 128 terabytes of working memory. Problems from modern genetics are even more challenging. A particularly useful way to represent a long DNA sequence is by a feature vector that counts the occurrence frequency of all length-K sub-strings called K-mers. This representation plays an important role in large-scale regression problems in computational biology (Wood & Salzberg, 2014; Bray et al., 2015; Vervier et al., 2016; Aghazadeh et al., 2016). Typically, K is chosen to be larger than 12, and these strings are composed of all possible combinations of 16 characters ({A,T,C,G} in addition to 12 wild card characters). In this case, the feature vector dimension is p = 1612 = 248. A vector of size 248 single-precision variables requires approximately 1 petabyte of space!\nFor ultra large-scale feature selection problems, it is impossible to run standard explicit regularization-based methods like `1 regularization (Shalev-Shwartz & Tewari, 2011; Tan et al., 2014) or to select hyperparameters with a constrained amount of memory (Langford et al., 2009). This is not surprising, because these methods are not scalable in terms of memory and computational time (Duchi et al., 2008). Another important operational concern is that most datasets represent features in the form of strings or tokens. For example, with DNA or n-gram datasets, features are represented by strings of characters. Even in click-through data (McMahan et al., 2013), features are indexed by textual tokens. Observe that mapping each of these strings to a vector component requires maintaining a dictionary whose size equals the length of the feature vector. As a result, one does not even have the capability to create a numerical exact vector representation of the features.\nTypically, when faced with such large machine learning\nar X\niv :1\n80 6.\n04 31\n0v 1\n[ cs\n.D S]\n1 2\nJu n\n20 18\ntasks, the practitioner chooses to do feature hashing (Weinberger et al., 2009). Consider a 3-gram string \u201cabc\u201d. With feature hashing, one uses a lossy, random hash function h : strings \u2192 {0, 1, 2, . . . , R} to map \u201cabc\u201d to a feature number h(abc) in the range {0, 1, 2, . . . , R}. This is extremely convenient because it enables one to avoid creating a large look-up dictionary. Furthermore, this serves as a dimensionality reduction technique, reducing the problem dimension to R. Unfortunately, this convenience comes at a cost. Given that useful dimensionality reduction is strictly surjective (i.e., R < p), we lose the identity of the original features. This is not a viable option if one cares about both feature selection and interpretability.\nOne reason to remain hopeful is that in such highdimensional problems, the data vectors Xi are extremely sparse (Wood & Salzberg, 2014). For instance, the DNA sequence of an organism contains only a small fraction (at most the length of the DNA sequence) of p = 1612 features. The situation is similar whether we are predicting clickthrough rates of users on a website or if we seek n-gram representations of text documents (Mikolov et al., 2013). In practice, ultra high-dimensional data is almost always ultrasparse. Thus, loading a sparse data vector into memory is usually not a concern. The problem arises in the intermediate stages of traditional methods, where dense iterates need to be tracked in the main memory. One popular approach is to use greedy thresholding methods (Maleki, 2009; Mikolov et al., 2013; Jain et al., 2014; 2017) combined with stochastic gradient descent (SGD) to prevent the feature vector \u03b2 from becoming too dense and blowing up in memory. In these methods, the intermediate iterates are regularized at each step, and a full gradient update is never stored nor computed (since this is memory and computation intensive). However, it is well known that greedy thresholding can be myopic and can result in poor convergence. We clearly observe this phenomenon in our evaluations. See Section 5 for details.\nIn this paper we tackle the ultra large-scale feature selection problem, i.e., feature selection with billions or more dimensions. We propose a novel feature selection algorithm called MISSION, a Memory-efficient, Iterative Sketching algorithm for Sparse feature selectION. MISSION, that takes on all the concerns outlined above. MISSION matches the accuracy performance of existing large-scale machine learning frameworks like Vowpal Wabbit (VW) (Agarwal et al., 2014) on real-world datasets. However, in contrast to VW, MISSION can perform feature selection exceptionally well. Furthermore, MISSION significantly surpasses the performance of classical algorithms such as Iterative Hard Thresholding (IHT), which is currently the popular feature selection alternative concerning the problem sizes we consider.\nContributions: In this work, we show that the two-decade\nold Count-Sketch data structure (Charikar et al., 2002) from the streaming algorithms literature is ideally suited for ultra large-scale feature selection. The Count-Sketch data structure enables us to retain the convenience of feature hashing along with the identity of important features. Moreover, Count-Sketch can accumulate gradients updates over several iterations because of linear aggregation. This aggregation eliminates the problem of myopia associated with existing greedy thresholding approaches.\nIn particular, we force the parameters (or feature vector) to reside in a memory-efficient Count-Sketch data structure (Charikar et al., 2002). SGD gradient updates are easily applied to the Count-Sketch. Instead of moving in the gradient direction and then greedily projecting into a subspace defined by the regularizer (e.g., in the case of LASSObased methods), MISSION adds the gradient directly into the Count-Sketch data structure, where it aggregates with all the past updates. See Fig. 1 for the schematic. At any point of time in the iteration, this data structure stores a compressed, randomized, and noisy sketch of the sum of all the gradient updates, while preserving the information of the heavyhitters\u2014the coordinates that accumulate the highest amount of energy. In order to find an estimate of the feature vector, MISSION queries the Count-Sketch. The Count-Sketch is used in conjunction with a top-k heap, which explicitly stores the features with the heaviest weights. Only the features in the top-k heap are considered active, and the rest are set to zero. However, a representation for every weight is stored, in compressed form, inside the Count-Sketch.\nWe demonstrate that MISSION surpasses the sparse recovery performance of classical algorithms such as Iterative Hard Thresholding (IHT), which is the only other method we could run at our scale. In addition, experiments suggest that the memory requirements of MISSION scale well with the dimensionality p of the problem. MISSION matches the accuracy of existing large-scale machine learning frameworks like Vowpal Wabbit (VW) on real-world, large-scale datasets. Moreover, MISSION achieves comparable or even\nbetter accuracy while using significantly fewer features."
    },
    {
      "heading": "2. Review: Streaming Setting and the Count-Sketch Algorithm",
      "text": "In the streaming setting, we are given a very highdimensional vector \u03b2 \u2208 Rp that is too costly to store in memory. We see only a very long sequence of updates over time. The only information available at time t is of the form (i,\u2206), which means that coordinate i is incremented (or decremented) by the amount \u2206. We are given a limited amount of storage, on the order of O(log p), which means that we can never store the entire sequence of updates. Sketching algorithms aim to estimate the value of current item i, after any number of updates using only O(log p) memory. Accurate estimation of heavy coordinates is desirable.\nCount-Sketch is a popular algorithm for estimation in the streaming setting. Count-Sketch keeps a matrix of counters (or bins) S of size d \u00d7 w \u223c O(log p), where d and w are chosen based on the accuracy guarantees. The algorithm uses d random hash functions hj j \u2208 {1, 2, ..., d} to map the vector\u2019s components to bins w. hj : {1, 2, ..., p} \u2192 {1, 2, ..., w} Every component i of the vector is hashed to d different bins. In particular, for any row j of sketch S, component i is hashed into bin S(j, hj(i)). In addition to hj , Count-Sketch uses d random sign functions to map the components of the vectors randomly to {+1, \u22121}. i.e., si : {1, 2, ..., D} \u2192 {+1,\u22121} A picture of this sketch data structure with three hash functions in shown inside Fig. 1.\nThe Count-Sketch supports two operations: UPDATE(item i, increment \u2206) and QUERY(item i). The UPDATE operation updates the sketch with any observed increment. More formally, for an increment \u2206 to an item i, the sketch is updated by adding sj(i)\u2206 to the cell S(j, hj(i)) \u2200j \u2208 {1, 2, ..., d}. The QUERY operation returns an estimate for component i, the median of all the d different associated counters.\nIt has been shown that, for any sequence of streaming updates (addition or subtraction) to the vector \u03b2, Count-Sketch provides an unbiased estimate of any component i, \u03b2\u0302i such that the following holds with high probability,\n\u03b2i \u2212 ||\u03b2||2 \u2264 \u03b2\u0302i \u2264 \u03b2i + ||\u03b2||2. (1)\nIt can be shown that the Eq. (1) is sufficient to achieve near-optimal guarantees for sparse recovery with the given space budget. Furthermore, these guarantees also meet the best compressed sensing lower bounds in terms of the number of counters (or measurements) needed for sparse recovery (Indyk, 2013)."
    },
    {
      "heading": "3. Problem Formulation",
      "text": "Consider the feature selection problem in the ultra highdimensional setting: We are given the dataset (Xi, yi) for i \u2208 [n] = {1, 2, . . . , n}, where Xi \u2208 Rp and yi \u2208 R denote the ith measured and response variables. We are interested in finding the k-sparse (k non-zero entries) feature vector (or regressor) \u03b2 \u2208 Rp from the optimization problem\nmin \u2016\u03b2\u20160=k\n\u2016y \u2212X\u03b2\u20162, (2)\nwhere X = {X1,X2, . . . ,Xn} and y = [y1, y1, . . . , yn] denote the data matrix and response vector and the `0-norm \u2016\u03b2\u20160 counts the number of non-zero entries in \u03b2.\nWe are interested in solving the feature selection problem for ultra high-dimensional datasets where the number of features p is so large that a dense vector (or matrix) of size p cannot be stored explicitly in memory."
    },
    {
      "heading": "3.1. Hard Thresholding Algorithms",
      "text": "Among the menagerie of feature selection algorithms, the class of hard thresholding algorithms have the smallest memory footprint: Hard thresholding algorithms retain only the top-k values and indices of the entire feature vector using O(klog(p)) memory (Jain et al., 2014; Blumensath & Davies, 2009). The iterative hard thresholding (IHT) algorithm generates the following iterates for the ith variable in an stochastic gradient descent (SGD) framework\n\u03b2t+1 \u2190 Hk(\u03b2t \u2212 2\u03bb ( yi \u2212Xi\u03b2t )T Xi) (3)\nThe sparsity of the feature vector \u03b2t, enforced by the hard thresholding operator Hk, alleviates the need to store a vector of size O(p) in the memory in order to keep track of the changes of the features over the iterates.\nUnfortunately, because it only retains the top-k elements of \u03b2, the hard thresholding procedure greedily discards the information of the non top-k coordinates from the previous iteration. In particular, it clips off coordinates that might add to the support set in later iterations. This drastically affects the performance of hard thresholding algorithms, especially in real-world scenarios where the design matrix X is not random, normalized, or well-conditioned. In this regime, the gradient terms corresponding to the true support typically arrive in lagging order and are prematurely clipped in early iterations by Hk. The effect of these lagging gradients is present even in the SGD framework, because the gradients are quite noisy, and only a small fraction of the energy of the true gradient is expressed in each iteration. It is not difficult to see that these small energy, high noise signals can easily cause the greedy hard thresholding operator to make sub-optimal or incorrect decisions. Ideally, we want to accumulate the gradients to get enough confidence in\nAlgorithm 1 MISSION Initialize: \u03b20 = 0, S (Count-Sketch), \u03bb (Learning Rate) while not stopping criteria do\nFind the gradient update gi = \u03bb ( 2 (yi \u2212Xi\u03b2t) T Xi ) Add the gradient update to the sketch gi \u2192 S Get the top-k heavy-hitters from the sketch \u03b2t+1 \u2190 S end while Return: The top-k heavy-hitters from the Count-Sketch\nsignal and to average out any noise. However, accumulating gradients will make the gradient vector dense, blowing up the memory requirements. This aforementioned problem is in fact symptomatic of all other thresholding variants including the Iterative algorithm with inversion (ITI) (Maleki, 2009) and the Partial hard thresholding (PHT) algorithm (Jain et al., 2017)."
    },
    {
      "heading": "4. The MISSION Algorithm",
      "text": "We now describe the MISSION algorithm. First, we initialize the Count-Sketch S and the feature vector \u03b2t=0 with zeros entries. The Count-Sketch hashes a p-dimensional vector into O(log2p) buckets (Recall Fig. 1). We discuss this particular choice for the size of the Count-Sketch and the memory-accuracy trade offs of MISSION in Sections 5.3 and 6.1.\nAt iteration t, MISSION selects a random row Xi from the data matrix X and computes the stochastic gradient update term using the learning rate \u03bb. gi = 2\u03bb (yi \u2212Xi\u03b2t) T Xi i.e. the usual gradient update that minimizes the unconstrained quadratic loss \u2016y \u2212 X\u03b2\u201622. The data vector Xi and the corresponding stochastic gradient term are sparse. We then add the non-zero entries of the stochastic gradient term {gij : \u2200j gij > 0} to the Count-Sketch S . Next, MISSION queries the top-k values of the sketch to form \u03b2t+1. We repeat the same procedure until convergence. MISSION returns the top-k values of the Count-Sketch as the final output of the algorithm. The MISSION algorithm is detailed in Alg. 1. MISSION easily extends to other loss functions such as the hinge loss and logistic loss.\nMISSION is Different from Greedy Thresholding: Denote the gradient vector update at any iteration t as ut. It is not difficult to see that starting with an all-zero vector \u03b20, at any point of time t, the Count-Sketch state is equivalent to the sketch of the vector \u2211t i=1 ut. In other words, the sketch aggregates the compressed aggregated vector. Thus, even if an individual SGD update is noisy and contains small signal energy, thresholding the Count-Sketch is based on the average update over time. This averaging produces a robust signal that cancels out the noise. We can therefore expect MISSION to be superior over thresholding.\nIn the supplementary materials, we present initial theoretical results on the convergence of MISSION. Our results show that, under certain assumptions, the full-gradient-descent version of MISSION converges geometrically to the true parameter \u03b2 \u2208 Rp up to some additive constants. The exploration of these assumptions and the extension to the SGD version of MISSION are exciting avenues for future work.\nFeature Selection with the Ease of Feature Hashing: As argued earlier, the features are usually represented with strings, and we do not have the capability to map each string to a unique index in a vector without spendingO(p) memory. Feature hashing is convenient, because we can directly access every feature using hashes. We can use any lossy hash function for strings. MISSION only needs a few independent hash functions (3 in our Count-Sketch implementation) to access any component. The top-k estimation is done efficiently using a heap data structure of size k. Overall, we only access the data using efficient hash functions, which can be easily implemented in large-scale systems."
    },
    {
      "heading": "5. Simulations",
      "text": "We designed a set of simulations to evaluate MISSION in a controlled setting. In contrast to the ultra large-scale, real-world experiments of Section 6, in the section the data matrices are drawn from a random Gaussian distribution and the ground truth features are known."
    },
    {
      "heading": "5.1. Phase Transition",
      "text": "We first demonstrate the advantage of MISSION over greedy thresholding in feature selection. For this experiment, we modify MISSION slightly to find the root of the algorithmic advantage of MISSION: we replace the Count-Sketch with an \u201cidentity\u201d sketch, or a sketch with a single hash function, h(i) = i. In doing so, we eliminate the complexity that Count-Sketch adds to the algorithm, so that the main difference between MISSION and IHT is that MISSION accumulates the gradients. To improve stability, we scale the non top-k elements of S by a factor \u03b3 \u2208 (0, 1) that begins very near 1 and is gradually decreased until the algorithm converges. Note: it is also possible to do this scaling in the Count-Sketch version of MISSION efficiently by exploiting the linearity of the sketch.\nFig. 2 illustrates the empirical phase transition curves for sparse recovery using MISSION and the hard thresholding algorithms. The phase transition curves show the points where the algorithm successfully recovers the features in > 50% of the random trails. MISSION shows a better phase transition curve compared to IHT by a considerable gap.\nTable 1. Comparison of MISSION against hard thresholding algorithms in subset selection under adversarial effects. We first report the percentage of instances in which the algorithms accurately find the solution (ACC) with no attenuation (\u03b1 = 1) over 100 random trials. We then report the mean of the maximum level of attenuation \u03b1 applied to the columns of design X before the algorithms fail to recover the support of \u03b2 (over the trials that all algorithms can find the solution with \u03b1 = 1).\n(n, k) MISSION IHT ITI PHT ACC\u03b1=1 \u03b1 ACC\u03b1=1 \u03b1 ACC\u03b1=1 \u03b1 ACC\u03b1=1 \u03b1 (100, 2) 100% 2.68 \u00b1 0.37 100% 1.49 \u00b1 0.33 91% 1.33 \u00b1 0.23 64% 2.42 \u00b1 0.87 (100, 3) 100% 2.52 \u00b1 0.36 92% 1.36 \u00b1 0.46 70% 1.15 \u00b1 0.20 42% 2.05 \u00b1 0.93 (100, 4) 100% 2.53 \u00b1 0.23 72% 1.92 \u00b1 0.91 37% 1.03 \u00b1 0.09 39% 2.13 \u00b1 1.07 (200, 5) 100% 4.07 \u00b1 0.36 99% 2.34 \u00b1 1.12 37% 1.15 \u00b1 0.22 83% 2.75 \u00b1 1.30 (200, 6) 100% 4.17 \u00b1 0.24 97% 2.64 \u00b1 1.14 23% 1.11 \u00b1 0.12 73% 2.26 \u00b1 1.33 (200, 7) 100% 4.07 \u00b1 0.11 83% 1.64 \u00b1 1.01 14% 1.11 \u00b1 0.12 75% 3.39 \u00b1 1.36\n0.0 0.2 0.4 0.6 0.8 1.0\nn/p\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\ns/ n\nMISSION IHT ITI PHT\nFigure 2. Empirical phase transition in recovering a binary feature vector \u03b2 in p = 1000-dimensional space with a Gaussian data matrix X. We illustrate the empirical 50% probability of success curves averaged over T = 20 trials. MISSION outperforms the thresholding algorithms by a large margin."
    },
    {
      "heading": "5.2. Lagging Gradient: Superiority of Count-Sketches over Greedy Thresholding",
      "text": "A major problem with the IHT algorithm, especially in largescale SGD settings, is with thresholding the coordinates with small gradients in the earlier iterations. IHT misses these coordinates, since they become prominent only after the gradients accumulate with the progression of the algorithm. The problem is amplified with noisy gradient updates such as SGD, which is unavoidable for large datasets.\nThis phenomenon occurs frequently in sparse recovery problems. For example, when the coordinates that correspond to the columns of the data matrix with smaller energy lag in the iterations of gradient descent algorithm, IHT thresholds these lagging-gradient coordinates in first few iterations, and they never show up again in the support. In contrast, MISSION retains a footprint of the gradients of all the previous iterations in the Count-Sketch. When the total sum of the gradient of a coordinate becomes prominent, the coordinate joins the support after querying the top-k heavy hitters from the Count-Sketch. We illustrate this phenomena in sparse recovery using synthetic experiments. We recover sparse vector \u03b2 from its random linear measurements y = X\u03b2, where the energy of X is imbalanced across its columns. In this case, the gradients corresponding to the\ncolumns (coordinates) with smaller energy typically lag and are thresholded by IHT.\nTo this end, we first construct a random Gaussian data matrix X \u2208 R900\u00d71000, pick a sparse vector \u03b2 that is supported on an index set I , and then attenuate the energy of the columns of X supported by the indices in I by an attenuation factor of \u03b1 = {1, 1.25, 1.5, 1.75, 2, . . . , 5}. Note that \u03b1 = 1 implies that no attenuation is applied to the matrix. In Table 1, we report the maximum attenuation level applied to a column of data matrix X before the algorithms fail to fully recover the support set I from y = \u03b2X. We observe that MISSION is consistently and up to three times more robust against adversarial attenuation of the columns of the data matrix in various design settings.\nThe robustness of MISSION to the attenuation of the columns of X in sparse recovery task suggests that the Count-Sketch data structure enables gradient-based optimization methods such as IHT to store a footprint (or sketch) of all the gradients from the previous iterations and deliver them back when they become prominent."
    },
    {
      "heading": "5.3. Logarithmic Scaling of the Count-Sketch Memory",
      "text": "in MISSION\nIn this section we demonstrate that the memory requirements of MISSION grows polylogarithmically in the dimension of the problem p. We conduct a feature selection experiment with a data matrix X \u2208 R100\u00d7p whose entries are drawn from i.i.d. random Gaussian distributions with zero mean and unit variance. We run MISSION and IHT to recover the feature vector \u03b2 from the output vector y = X\u03b2, where the feature vector \u03b2 is a k = 5-sparse vector with random support. We repeat the same experiment 1000 times with different realizations for the sparse feature vector \u03b2 and report the results in Fig. 3. The left plot illustrates the feature selection accuracy of the algorithms as the dimension of the problem p grows. The right plot illustrates the minimum memory requirements of the algorithms to recover the features with 100% accuracy.\nThe plots reveal an interesting phenomenon. The size of the Count-Sketch in MISSION scales only polylogarithmically with the dimension of the problem. This is surprising since the aggregate gradient in a classical SGD framework becomes typically dense in early iterations and thus requires a memory of order O(p). MISSION, however, stores only the essential information of the features in the sketch using a poly-logarithmic sketch size. Note that IHT sacrifices accuracy to achieve a small memory footprint. At every iteration IHT eliminates all the information except for the top-k features. We observe that, using only a logarithmic factor more memory, MISSION has a significant advantage over IHT in recovering the ground truth features."
    },
    {
      "heading": "6. Experiments",
      "text": "All experiments were performed on a single machine, 2x Intel Xeon E5-2660 v4 processors (28 cores / 56 threads) with 512 GB of memory. The code 1 for training and running our randomized-hashing approach is available online. We designed the experiments to answer these questions:\n1. Does MISSION outperform IHT in terms of classification accuracy? In particular, how much does myopic thresholding affect IHT in practice?\n2. How well does MISSION match the speed and accuracy of feature hashing (FH)?\n3. How does changing the number of top-k features affect the accuracy and behaviour of the different methods?\n4. What is the effect of changing the memory size of the Count-Sketch data structure on the classification accuracy of MISSION in read-world datasets?\n5. Does MISSION scale well in comparison to the different methods on the ultra large-scale datasets (> 350 GB in size)?\n1https://github.com/rdspring1/MISSION"
    },
    {
      "heading": "6.1. Large-scale Feature Extraction",
      "text": "Datasets: We used four datasets in the experiments: 1) KDD2012, 2) RCV1, 3) Webspam\u2013Trigram, 4) DNA 2. The statistics of these datasets are summarized in Table 2.\nThe DNA metagenomics dataset is a multi-class classification task where the model must classify 15 different bacteria species using DNA K-mers. We sub-sampled the first 15 species from the original dataset containing 193 species. We use all of the species in the DNA Metagenomics dataset for the large-scale experiments (See Section 6.2). Following standard procedures, each bacterial species is associated with a reference genome. Fragments are sampled from the reference genome until each nucleotide is covered c times on average. The fragments are then divided into K-mer sub-strings. We used fragments of length 200 and K-mers of length 12. Each model was trained and tested with mean coverage c = {0.1, 1} respectively. For more details, see (Vervier et al., 2016). The feature extraction task is to find the DNA K-mers that best represent each bacteria class.\nWe implemented the following approaches to compare and contrast against our approach: For all methods, we used the logistic loss for binary classification and the cross-entropy loss for multi-class classification.\nMISSION: As described in Section 4. Iterative Hard Thresholding (IHT): An algorithm where, after each gradient update, a hard threshold is applied to the features. Only the top-k features are kept active, while the rest are set to zero. Since the features are strings or integers, we used a sorted heap to store and manipulate the top-k elements. This was the only algorithm we could successfully run over the large datasets on our single machine. Batch IHT: A modification to IHT that uses mini-batches such that the gradient sparsity is the same as the number of elements in the count-sketch. We accumulate features and then sort and prune to find the top-k features. This accumulate, sort, prune process is repeated several times during training. Note: This setup requires significantly more memory than MISSION, because it explicitly stores the feature strings. The memory cost of maintaining a set of string features can be orders of magnitude more than the flat array used by MISSION. See Bloom Filters (Broder & Mitzenmacher, 2004) and related literature. This setup is not scalable to large-scale datasets.\n2http://projects.cbio.mines-paristech.fr/ largescalemetagenomics/\nFeature Hashing (FH): A standard machine learning algorithm for dimensionality reduction that reduces the memory cost associated with large datasets. FH is not a feature selection algorithm and cannot identify important features. (Agarwal et al., 2014)\nExperimental Settings: The MISSION and IHT algorithms searched for the same number of top-k features. To ensure fair comparisons, the size of the Count-Sketch and the feature vector allocated for the FH model were equal. The size of the MISSION and FH models were set to the nearest power of 2 greater than the number of features in the dataset. For all the experiments, the Count-Sketch data structure used 3 hash functions, and the model weights were divided equally among the hash arrays. For example, with the (Tiny) DNA metagenomics dataset, we allocated 24 bits or 16,777,216 weights for the FH model. Given 3 hash functions and 15 classes, roughly 372,827 elements were allocated for each class in the Count-Sketch.\nMISSION, IHT, FH Comparison: Fig. 4 shows that MISSION surpasses IHT in classification accuracy in all four datasets, regardless of the number of features. In addition, MISSION closely matches FH, which is significant because FH is allowed to model a much larger set of features than MISSION or IHT. MISSION is 2\u20134\u00d7 slower than FH, which is expected given that MISSION has the extra overhead of using a heap to track the top-k features.\nMISSION\u2019s accuracy rapidly rises with respect to the number of top-k features, while IHT\u2019s accuracy plateaus and then grows slowly to match MISSION. This observation corroborates our insight that the greedy nature of IHT hurts performance. When the number of top-k elements is small, the capacity of IHT is limited, so it picks the first set of features that provides good performance, ignoring the rest. On the other hand, MISSION decouples the memory from the top-k ranking, which is based on the aggregated gradients in the compressed sketch. By the linear property of the count-sketch, this ensures that the heavier entries occur in the top-k features with high probability.\nCount-Sketch Memory Trade-Off: Fig. 5 shows how MISSION\u2019s accuracy degrades gracefully, as the size of the Count-Sketch decreases. In this experiment, MISSION only used the top 500K features for classifying the Tiny DNA metagenomics dataset. When the top-k to Count-Sketch ratio is 1, then 500K weights were allocated for each class and hash array in the Count-Sketch data structure. The Batch IHT baseline was given 8,388,608 memory elements per class, enabling it to accumulate a significant number of features before thresholding to find the top-k features. This experiment shows that MISSION immediately outperforms IHT and Batch IHT, once the top-k to Count-Sketch ratio is 1:1. Thus, MISSION provides a unique memory-accuracy knob at any given value of top-k."
    },
    {
      "heading": "6.2. Ultra Large-Scale Feature Selection",
      "text": "Here we demonstrate that MISSION can extract features from three large-scale datasets: Criteo 1TB, Splice-Site, and DNA Metagenomics.\nCriteo 1TB: The Criteo 1TB 3 dataset represents 24 days of click-through logs\u201423 days (training) + 1 day (testing). The task for this dataset is click-through rate (CTR) prediction\u2014 How likely is a user to click an ad? The dataset contains over 4 billion (training) and 175 million (testing) examples (2.5 TB of disk space). The performance metric is Area Under the ROC Curve (AUC). The VW baseline 4 achieved 0.7570 AUC score. MISSION and IHT scored close to the VW baseline with 0.751 AUC using only the top 250K features.\nSplice-Site: The task for this dataset is to distinguish between true and fake splice sites using the local context around the splice site in-question. The dataset is highly skewed (few positive, many negative values), and so the performance metric is average precision (AP). Average precision is the precision score averaged over all recall scores ranging from 0 to 1. The dataset contains over 50 million (training) and 4.6 million (testing) examples (3.2 TB of disk space). All the methods were trained for a single epoch with a learning rate of 0.5. MISSION, Batch IHT, and SGD IHT tracked the top 16,384 features. FH, MISSION, and Batch IHT used 786,432 extra memory elements. MISSION significantly outperforms Batch IHT and SGD IHT by 2.3%. Also, unlike in Fig. 5, the extra memory did not help Batch IHT, since it performed the same as SGD IHT. MISSION (17.5 hours) is 15% slower than FH (15 hours) in wall-clock running time."
    },
    {
      "heading": "AP 0.522 0.510 0.498 0.498",
      "text": "DNA Metagenomics: This experiment evaluates MISSION\u2019s performance on a medium-sized metagenomics dataset. The parameters from the Tiny (15 species) dataset in Section 6.1 are shared with this experiment, except the\n3https://www.kaggle.com/c/criteo-display-ad-challenge 4https://github.com/rambler-digital-solutions/\ncriteo-1tb-benchmark\nDNA - Tiny (15 Species) - Top-K: 500K\nnumber of species is increased to 193. The size of a sample batch with mean coverage c = 1 increased from 7 GB (Tiny) to 68 GB (Medium). Each round (mean coverage c = 0.25) contains 3.45 million examples and about 16.93 million unique non-zero features (p). MISSION and IHT tracked the top 2.5 million features per class. The FH baseline used 231 weights, about 11.1 million weights per class, and we allocated the same amount of space for the Count-Sketch. Each model was trained on a dataset with coverage c = 5.\nFig. 6 shows the evolution of classification accuracy over time for MISSION, IHT, and the FH baseline. After 5 epochs, MISSION closely matches the FH baseline. Note: MISSION converges faster than IHT such that MISSION is 1\u20134 rounds ahead of IHT, with the gap gradually increasing over time. On average, the running time of MISSION is 1\u20132\u00d7 slower than IHT. However, this experiment demonstrates that since MISSION converges faster, it actually needs less time to reach a certain accuracy level. Therefore, MISSION is effectively faster and more accurate than IHT."
    },
    {
      "heading": "7. Implementation Details and Discussion",
      "text": "Scalability and Parallelism: IHT finds the top-k features after each gradient update, which requires sorting the features based on their weights before thresholding. The speed of the sorting process is improved by using a heap data structure, but it is still costly per update. MISSION also uses\nDNA - Medium (193 Species) - Top-K: 2.5M\na heap to store its top-k elements, but it achieves the same accuracy as IHT with far fewer top-k elements because of the Count-Sketch. (Recall Section 4)\nAnother suggested improvement for the top-k heap is to use lazy updates. Updating the weight of a feature does not change its position in the heap very often, but still requires an O(log n) operation. With lazy updates, the heap is updated only if it the change is significant. |xt \u2212 x0| \u2265 , i.e. the new weight at time t exceeds the original value by some threshold. This tweak significantly reduces the number of heap updates at the cost of slightly distorting the heap."
    },
    {
      "heading": "8. Conclusion and Future Work",
      "text": "In this paper, we presented MISSION, a new framework for ultra large-scale feature selection that performs hard thresholding and SGD while maintaining an efficient, approximate representation for all features using a Count-Sketch data structure. MISSION retains the simplicity of feature hashing without sacrificing the interpretability of the features.\nInteraction features are important for scientific discovery with DNA Metagenomics (Basu et al., 2018). Traditionally, the polynomial kernel trick enabled machine learning algorithms to explore this feature space implicitly without the exponential increase in dimensionality. However, this exponential cost is unavoidable with feature extraction. Going forward, we are interested in leveraging our MISSION framework to explore pairwise or higher interaction features."
    },
    {
      "heading": "Acknowledgements",
      "text": "AAA, DL, GD, and RB were supported by the DOD Vannevar Bush Faculty Fellowship grant N00014-18-1-2047, NSF grant CCF-1527501, ARO grant W911NF-15-1-0316, AFOSR grant FA9550-14-1-0088, ONR grant N00014-17-12551, DARPA REVEAL grant HR0011-16-C-0028, and an ONR BRC grant for Randomized Numerical Linear Algebra. RS and AS were supported by NSF-1652131, AFOSR-YIP FA9550-18-1-0152, and ONR BRC grant for Randomized Numerical Linear Algebra. The authors would also like to thank NVIDIA and Amazon for gifting computing resources."
    },
    {
      "heading": "9. Appenix",
      "text": "In this appendix, we present some preliminary results on the convergence of MISSION. For the sake of exposition, we will consider the full-gradient descent version of MISSION, and we will prove that the iterates converge geometrically upto a small additive error. In order to establish this proof, we make an assumption (Assumption 1) about the hashing scheme; see Section 9.1 for more on this.\nWe begin by establishing some notation. We will assume that the data satisfies the following linear model:\ny = X\u03b2\u2217 + w, (4)\nwhere y \u2208 Rn is the vector of observation,X \u2208 Rn\u00d7p is the data matrix, w \u2208 Rn is the noise vector, and \u03b2\u2217 \u2208 Rp is the unknown k\u2212sparse regression vector. We will let \u03c8 and \u03d5 respectively denote the hashing and the (top-k) heavy-hitters operation. We will let \u03b2t denote the output of MISSION in step t. In general, we will let the vector h \u2208 Rm denote the hash table. Finally, as before, we will let Hk denote the projection operation onto the set of all k\u2212sparse vectors. We will make the following assumption about the hashing mechanism:\nAssumption 1. For any h \u2208 Rm, there exists an \u03b2h \u2208 Rp such that the following hold\n1. \u03c8(\u03b2h) = h, that is, the hash table contents can be set to h by hashing the vector \u03b2h.\n2. \u2016\u03b2h \u2212Hk(\u03b2h)\u20162 \u2264 \u03b51\nThis assumption requires the hashing algorithm to be such that there exists a nearly sparse vector that can reproduce any state of the hash table exactly. This is reasonable since the hash table is a near optimal \u201ccode\u201d for sparse vectors in Rp. See Section 9.1 for more on this.\nWe will next state a straightforward lemma about the sketching procedure\nLemma 1. There exist constants \u03b52, C1 > 0 such that provided that the size m of the hash table satisfies m \u2265 C1k log\n2 p, the following holds for any \u03b2 \u2208 Rp with probability at least 1\u2212 \u03b41:\n\u2016\u03d5(\u03c8(\u03b2))\u2212Hk(\u03b2)\u20162 \u2264 \u03b52 (5)\nThis lemma follows directly from the definition of the CountSketch, and we will not prove here.\nWe next state the main theorem that we will show. Theorem 1. For any \u03b4 \u2208 ( 0, 13 )\nand \u03c1 \u2208 (0, 0.5), there is a constant C > 0 such that the following statement holds\nwith probability at least 1\u2212 3\u03b4 \u2225\u2225\u03b2t+1 \u2212 \u03b2\u2217\u2225\u2225 2 \u2264 2\u03c1 \u2225\u2225\u03b2t \u2212 \u03b2\u2217\u2225\u2225 2 + 2 \u221a \u03c32w(1 + \u00b5)k log p\nn\n+ 2\u03b51 + 3\u03b52, (6)\nprovided that n > Ck log p, m > Ck log2 p, and that Assumption 1 holds.\nNotice that since \u03c1 < 0.5, the above theorem guarantees geometric convergence. This implies that the overall error is of the order of the additive constants \u03b51 and \u03b52.\nBefore we prove this theorem, we will collect some lemmas that will help us prove our result.\nLemma 2. Suppose X \u2208 Rn\u00d7p has i.i.d N (0, 1n ) entries. Then for constants \u03c1, \u03b42 > 0, there exists a constant C2(\u03b4) > 0 such that if n \u2265 C2k log p such that for any pair of unit-norm k\u2212sparse vectors \u03b21, \u03b22 \u2208 Sp\u22121, the following holds with probability at least 1\u2212 \u03b42.\n|\u3008X\u03b21, X\u03b22\u3009 \u2212 \u3008\u03b21, \u03b22\u3009| \u2264 \u03c1. (7)\nProof. Note that E[\u3008X\u03b21, X\u03b22\u3009] = \u3008\u03b21, \u03b22\u3009. For a fixed pair of \u03b21, \u03b22, the proof follows from a standard Chernoff bound argument after observing that \u3008X\u03b21, X\u03b22\u3009 can be written as a sum of products of independent Gaussian random variables. The rest of the proof follows from a standard covering argument, which gives the requirement on n.\nLemma 3. Suppose X has i.i.d entries drawn according to N (0, n\u22121), and w \u223c N (0, \u03c32wIn) is drawn independently of X . Then, for any constant \u03b43 > 0, there are constants C3, \u00b5 > 0 such that for all unit norm k\u2212sparse \u03b2 \u2208 Sp\u22121, the following holds with probability at least 1\u2212 \u03b43:\n\u3008\u03b2,XTw\u3009 \u2264 \u221a \u03c32w(1 + \u00b5)k log p\nn (8)\nprovided n \u2265 C3k log p.\nProof. Notice that for a fixed \u03b2, \u3008\u03b2,XTw\u3009 = \u3008X\u03b2,w\u3009 has the same distribution as 1\u221a\nn \u2016w\u20162 \u3008\u03b2,w2\u3009, where w2 \u223c\nN (0, In) is independent of w. Now, we can use concentration inequalities of chi-squared random variables to show that there is a constant C \u20323 > 0\nP [ \u2016w\u201622 \u2265 \u03c3 2 w(1 + \u00b51)n ] \u2264 e\u2212C \u2032 3n. (9)\nSimilarly, from chi-squared concentration, there is a constant C \u2032\u20323 > 0\nP [ |\u3008\u03b2,w2\u3009|2 \u2265 1 + \u00b52 ] \u2264 e\u2212C \u2032\u2032 3 (10)\nNow, with a standard covering argument, we know that there is a constant C \u2032\u2032\u20323 > 0 such that provided n > C \u2032\u2032\u2032 3 k log p, the following holds for at least 1\u2212 \u03b43 for any k\u2212sparse \u03b2:\n\u3008\u03b2,ATw\u3009 = \u3008A\u03b2,w\u3009 \u2264 \u221a \u03c32w(1 + \u00b5)nk log p\nn .\nProof of Theorem 1 If we let ht denote the contents of the hash table at round t, notice that we have the following: xt+1 = \u03d5(ht+1). The (full gradient descent version of the) MISSION algorithm proceeds by updating the hash table with hashes of the gradient updates. Therefore, we have the following relationship:\nht+1 = ht + \u03c8 ( \u03b7XTX(\u03b2\u2217 \u2212 \u03b2t) +XTw ) , (11)\nwhere \u03b2t is the output of the algorithm at round t. Notice that \u03b2t = \u03d5(ht). According to Assumption 1, we know that there exists a vector \u03b2\u0303t such that \u03c8(\u03b2\u0303t) = ht. We will use this observation next. Notice that the output of round t+ 1 maybe written as follows:\n\u03b2t+1 = \u03d5 ( ht + \u03c8 ( \u03b7XTX(\u03b2\u2217 \u2212 \u03b2t) +XTw )) = \u03d5 ( \u03c8 ( \u03b2\u0303t + \u03b7XTX(\u03b2\u2217 \u2212 \u03b2t) +XTw )) .\nNow, we will estimate how close the output of the algorithm gets to \u03b2\u2217 in round t+1 in terms of how close the algorithm got in round t. Notice that\u2225\u2225\u03b2t+1 \u2212 \u03b2\u2217\u2225\u2225\n2 = \u2225\u2225\u2225\u03d5(\u03c8 (\u03b2\u0303t + \u03b7XTX(\u03b2\u2217 \u2212 \u03b2t) +XTw))\u2212 \u03b2\u2217\u2225\u2225\u2225\n2 \u2264 \u2225\u2225\u2225Hk (\u03b2\u0303t + \u03b7XTX(\u03b2\u2217 \u2212 \u03b2t) +XTw)\u2212 \u03b2\u2217\u2225\u2225\u2225\n2 + \u03b52,\n(12)\nwhich follows from Lemma 1. We will next consider the first term from above. For notational ease, we will set \u03b3t+1 , \u03b2\u0303t + \u03b7XTX(\u03b2\u2217\u2212\u03b2t) +XTw. Observe that Hk is an orthogonal projection operator, and that \u03b2\u2217 is k\u2212sparse, therefore we have that\u2225\u2225Hk (\u03b3t+1)\u2212 \u03b3t+1\u2225\u222522 \u2264 \u2225\u2225\u03b3t+1 \u2212 \u03b2\u2217\u2225\u222522 . (13) Adding and subtracting \u03b2\u2217 on the left side and cancelling out the common terms, we have the following.\n\u2225\u2225Hk(\u03b3t+1)\u2212 \u03b2\u2217\u2225\u222522 \u2264 2\u3008Hk(\u03b3t+1)\u2212 \u03b2\u2217, \u03b3t+1 \u2212 \u03b2\u2217\u3009 = 2\u3008Hk(\u03b3t+1)\u2212 \u03b2\u2217, \u03b2\u0303t + \u03b7XTX(\u03b2\u2217 \u2212 \u03b2t) +XTw \u2212 \u03b2\u2217\u3009 = 2\u3008Hk(\u03b3t+1)\u2212 \u03b2\u2217, \u03b2t + \u03b7XTX(\u03b2\u2217 \u2212 \u03b2t) +XTw \u2212 \u03b2\u2217\u3009\n+ 2\u3008Hk(\u03b3t+1)\u2212 \u03b2\u2217, \u03b2t \u2212 \u03b2\u0303t\u3009 (a) \u2264 2\u3008Hk(\u03b3t+1)\u2212 \u03b2\u2217, \u03b2t + \u03b7XTX(\u03b2\u2217 \u2212 \u03b2t) +XTw \u2212 \u03b2\u2217\u3009\n+ 2 \u2225\u2225Hk(\u03b3t+1)\u2212 \u03b2\u2217\u2225\u22252 \u2225\u2225\u2225\u03d5(\u03c8(\u03b2\u0303t))\u2212 \u03b2\u0303t\u2225\u2225\u22252\n\u2264 2\u3008Hk(\u03b3t+1)\u2212 \u03b2\u2217, \u03b2t + \u03b7XTX(\u03b2\u2217 \u2212 \u03b2t) +XTw \u2212 \u03b2\u2217\u3009 + 2 \u2225\u2225Hk(\u03b3t+1)\u2212 \u03b2\u2217\u2225\u22252 (\u2225\u2225\u2225Hk(\u03b2\u0303t)\u2212 \u03b2\u0303t\u2225\u2225\u22252 +\u2225\u2225\u2225Hk(\u03b2\u0303t)\u2212 \u03d5(\u03c8(\u03b2\u0303t))\u2225\u2225\u2225\n2 ) (b) \u2264 2\u3008Hk(\u03b3t+1)\u2212 \u03b2\u2217, \u03b2t + \u03b7XTX(\u03b2\u2217 \u2212 \u03b2t) +XTw \u2212 \u03b2\u2217\u3009 + 2\n\u2225\u2225Hk(\u03b3t+1)\u2212 \u03b2\u2217\u2225\u22252 (\u03b51 + \u03b52) , (14) where (a) follows form the Cauchy-Schwarz inequality and from the definition of \u03b2\u0303t, (b) follows from Assumption 1 and Lemma 1. We will now turn our attention to the first inner-product in (14). With some rearrangement of terms, one can see that\n\u3008Hk(\u03b3t+1)\u2212 \u03b2\u2217, \u03b2t + \u03b7XTX(\u03b2\u2217 \u2212 \u03b2t) +XTw \u2212 \u03b2\u2217\u3009 = \u3008Hk(\u03b3t+1)\u2212 \u03b2\u2217, \u03b2t \u2212 \u03b2\u2217\u3009 \u2212 \u03b7\u3008X ( Hk(\u03b3 t+1)\u2212 \u03b2\u2217 ) ,\nX(\u03b2t \u2212 \u03b2\u2217)\u3009+ \u03b7\u3008Hk(\u03b3t+1)\u2212 \u03b2\u2217, XTw\u3009 (a)\n\u2264 \u03c1 \u2225\u2225Hk(\u03b3t+1)\u2212 \u03b2\u2217\u2225\u22252 \u2225\u2225\u03b2t \u2212 \u03b2\u2217\u2225\u22252 + \u3008Hk(\u03b3t+1)\u2212 \u03b2\u2217, XTw\u3009 (b)\n\u2264 \u03c1 \u2225\u2225Hk(\u03b3t+1)\u2212 \u03b2\u2217\u2225\u22252 \u2225\u2225\u03b2t \u2212 \u03b2\u2217\u2225\u22252\n+ \u2225\u2225Hk(\u03b3t+1)\u2212 \u03b2\u2217\u2225\u22252\n\u221a \u03c32w(1 + \u00b5)k log p\nn (15)\nwhere (a) follows from Lemma 2 and setting \u03b7 = 1. (b) follows from Lemma 3.\nPutting (14) and (15), we get\u2225\u2225Hk (\u03b3t+1)\u2212 \u03b3t\u2225\u22252 \u2264 2\u03c1\u2225\u2225\u03b2t \u2212 \u03b2\u2217\u2225\u22252 + 2 \u221a \u03c32w(1 + \u00b5)k log p\nn + 2 (\u03b51 + \u03b52) .\n(16)\nPutting this together with (12) gives us the desired result."
    },
    {
      "heading": "9.1. On Assumption 1",
      "text": "In the full-gradient version of the MISSION algorithm, one might modify the algorithm explicitly to ensure that Assumption 1. Towards this end, one would simply ensure that the gradients vector is attenuated on all but its top k entries at each step.\nIt is not hard to see that this clean-up step will ensure that Assumption 1 holds and the rest of the proof simply goes through. In MISSION as presented in the manuscript, we employ stochastic gradient descent (SGD). While the above proof needs to be modified for it to be applicable to this\ncase, our simulations suggest that this clean-up step is unnecessary here. We suspect that this is due to random cancellations that are introduced by the SGD. This is indeed an exciting avenue for future work."
    }
  ],
  "title": "MISSION: Ultra Large-Scale Feature Selection using Count-Sketches",
  "year": 2018
}
