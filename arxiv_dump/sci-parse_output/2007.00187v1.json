{"abstractText": "Thompson sampling is a heuristic algorithm for the multi-armed bandit problem which has a long tradition in machine learning. The algorithm has a Bayesian spirit in the sense that it selects arms based on posterior samples of reward probabilities of each arm. By forging a connection between combinatorial binary bandits and spike-and-slab variable selection, we propose a stochastic optimization approach to subset selection called Thompson Variable Selection (TVS). TVS is a framework for interpretable machine learning which does not rely on the underlying model to be linear. TVS brings together Bayesian reinforcement and machine learning in order to extend the reach of Bayesian subset selection to non-parametric models and large datasets with very many predictors and/or very many observations. Depending on the choice of a reward, TVS can be deployed in offline as well as online setups with streaming data batches. Tailoring multiplay bandits to variable selection, we provide regret bounds without necessarily assuming that the arm mean rewards be unrelated. We show a very strong empirical performance on both simulated and real data. Unlike deterministic optimization methods for spike-and-slab variable selection, the stochastic nature makes TVS less prone to local convergence and thereby more robust.", "authors": [{"affiliations": [], "name": "Yi Liu"}, {"affiliations": [], "name": "Veronika Ro\u010dkov\u00e1"}], "id": "SP:92b7afdd608b71027eff52b6b62f2770c0a05d6c", "references": [{"authors": ["R. Agrawal"], "title": "Sample mean based index policies by o(log n) regret for the multiarmed bandit problem", "venue": "Advances in Applied Probability 27 (4), 1054\u20131078.", "year": 1995}, {"authors": ["S. Agrawal", "N. Goyal"], "title": "Analysis of Thompson Sampling for the Multi-armed Bandit Problem", "venue": "Conference on Learning Theory.", "year": 2012}, {"authors": ["R.F. Barber", "E.J. Cand\u00e8s"], "title": "Controlling the false discovery rate via knockoffs", "venue": "The Annals of Statistics", "year": 2015}, {"authors": ["M. Barbieri", "J.O. Berger", "E.I. George", "V. Rockova"], "title": "The median probability model and correlated variables", "venue": "arXiv:1807.08336.", "year": 2018}, {"authors": ["M.M. Barbieri", "J.O. Berger"], "title": "Optimal predictive model selection", "venue": "Annals of Statistics", "year": 2004}, {"authors": ["A. Bhattacharya", "A. Chakraborty", "B.K. Mallick"], "title": "Fast sampling with Gaussian scale mixture priors in high-dimensional regression", "venue": "Biometrika 103 (4), 985.", "year": 2016}, {"authors": ["J. Bleich", "A. Kapelner", "E. George", "S. Jensen"], "title": "Variable selection for BART: An application to gene regulation", "venue": "Annals of Applied Statistics 8 (3), 1750\u20131781.", "year": 2014}, {"authors": ["L. Bottolo", "S. Richardson"], "title": "Evolutionary stochastic search for bayesian model exploration", "venue": "Bayesian Analysis", "year": 2010}, {"authors": ["L. Breiman"], "title": "Random forests", "venue": "Machine learning 45 (1), 5\u201332.", "year": 2001}, {"authors": ["P.J. Brown", "M. Vannucci", "T. Fearn"], "title": "Multivariate bayesian variable selection and prediction", "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology) 60 (3), 627\u2013641.", "year": 1998}, {"authors": ["E. Candes", "Y. Fan", "L. Janson", "J. Lv"], "title": "Panning for gold:model-xknockoffs for high dimensional controlled variable selection", "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology) 80 (3), 551\u2013577.", "year": 2018}, {"authors": ["P. Carbonetto", "M. Stephens"], "title": "Scalable variational inference for bayesian variable selection in regression, and its accuracy in genetic association studies", "venue": "Bayesian analysis", "year": 2012}, {"authors": ["I. Castillo", "J. Schmidt-Hieber", "A. Van der Vaart"], "title": "Bayesian linear regression with sparse priors", "venue": "The Annals of Statistics", "year": 2015}, {"authors": ["J. Castro", "D. G\u00f3mez", "J. Tejada"], "title": "Polynomial calculation of the shapley value based on sampling", "venue": "Computers & Operations Research 36 (5), 1726\u20131730.", "year": 2009}, {"authors": ["N. Cesa-Bianchi", "G. Lugosi"], "title": "Combinatorial bandits", "venue": "Journal of Computer and System Sciences 78 (5), 1404\u20131422.", "year": 2012}, {"authors": ["W. Chen", "Y. Wang", "Y. Yuan"], "title": "Combinatorial multi-armed bandit: General framework and applications", "venue": "International Conference on Machine Learning.", "year": 2013}, {"authors": ["H. Chipman", "E.I. George", "R.E. McCulloch"], "title": "The Practical Implementation of Bayesian Model Selection", "venue": "In Institute of Mathematical Statistics Lecture Notes Monograph Series. Institute of Mathematical Statistics", "year": 2001}, {"authors": ["H.A. Chipman", "E.I. George", "R.E. McCulloch"], "title": "BART: Bayesian additive regression trees", "venue": "The Annals of Applied Statistics 4 (1), 266\u2013298.", "year": 2010}, {"authors": ["R. Combes", "A. Proutiere"], "title": "Unimodal bandits: Regret lower bounds and optimal algorithms", "venue": "International Conference on Machine Learning.", "year": 2014}, {"authors": ["A. Fisher", "C. Rudin", "F. Dominici"], "title": "All models are wrong, but many are useful: Learning a variables importance by studying an entire class of prediction models simultaneously", "venue": "Journal of Machine Learning Research 20 (177), 1\u201381.", "year": 2019}, {"authors": ["D.P. Foster", "R.A. Stine"], "title": "\u03b1-investing: a procedure for sequential control of expected false discoveries", "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology) 70 (2), 429\u2013444.", "year": 2008}, {"authors": ["J.H. Friedman"], "title": "Multivariate Adaptive Regression Splines", "venue": "The Annals of Statistics 19 (1), 1\u2013141.", "year": 1991}, {"authors": ["Y. Gai", "B. Krishnamachari", "R. Jain"], "title": "Combinatorial network optimization with unknown variables: Multi-armed bandits with linear rewards and individual observations", "venue": "IEEE/ACM Transactions on Networking 20 (5), 1466\u20131478.", "year": 2012}, {"authors": ["G.D. Garson"], "title": "A comparison of neural network and expert systems algorithms with common multivariate procedures for analysis of social science data", "venue": "Social Science Computer Review 9 (3), 399\u2013434.", "year": 1991}, {"authors": ["E.I. George", "R.E. McCulloch"], "title": "Variable selection via Gibbs sampling", "venue": "Journal of the American Statistical Association 88 (423), 881\u2013889.", "year": 1993}, {"authors": ["E.I. George", "R.E. McCulloch"], "title": "Approaches for Bayesian variable selection", "venue": "Statistica Sinica 7, 339\u2013373.", "year": 1997}, {"authors": ["S. Gupta", "G. Joshi", "O. Ya\u011fan"], "title": "Correlated multi-armed bandits with a latent random source", "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "year": 2020}, {"authors": ["G. Hooker"], "title": "Generalized functional anova diagnostics for high-dimensional functions of dependent variables", "venue": "Journal of Computational and Graphical Statistics 16 (3), 709\u2013 732.", "year": 2007}, {"authors": ["E. Horel", "K. Giesecke"], "title": "Towards explainable AI: Significance tests for neural networks", "venue": "arXiv:1902.06021.", "year": 2019}, {"authors": ["H Ishwaran"], "title": "Variable importance in binary regression trees and forests", "venue": "Electronic Journal of Statistics 1, 519\u2013537.", "year": 2007}, {"authors": ["E. Je\u0159\u00e1bek"], "title": "Dual weak pigeonhole principle, boolean complexity, and derandomization", "venue": "Annals of Pure and Applied Logic 129 (1-3), 1\u201337.", "year": 2004}, {"authors": ["T. Jiang", "A.B. Owen"], "title": "Quasi-regression with shrinkage", "venue": "Mathematics and Computers in Simulation 62 (3-6), 231\u2013241.", "year": 2003}, {"authors": ["V.E. Johnson", "D. Rossell"], "title": "Bayesian model selection in high-dimensional settings", "venue": "Journal of the American Statistical Association 107 (498), 649\u2013660.", "year": 2012}, {"authors": ["J. Kazemitabar", "A. Amini", "A. Bloniarz", "A.S. Talwalkar"], "title": "Variable importance using decision trees", "venue": "Advances in Neural Information Processing Systems.", "year": 2017}, {"authors": ["J. Komiyama", "J. Honda", "H. Nakagawa"], "title": "Optimal regret analysis of thompson sampling in stochastic multi-armed bandit problem with multiple plays", "venue": "International Conference on Machine Learning.", "year": 2015}, {"authors": ["B. Kveton", "Z. Wen", "A. Ashkan", "H. Eydgahi", "B. Eriksson"], "title": "Matroid bandits: fast combinatorial optimization with learning", "venue": "Proceedings of the Thirtieth Conference on Uncertainty in Artificial Intelligence, pp. 420\u2013429.", "year": 2014}, {"authors": ["B. Kveton", "Z. Wen", "A. Ashkan", "C. Szepesvari"], "title": "Combinatorial cascading bandits", "venue": "Advances in Neural Information Processing Systems.", "year": 2015}, {"authors": ["T. Lai", "H. Robbins"], "title": "Asymptotically efficient adaptive allocation rules", "venue": "Advances in Applied Mathematics", "year": 1985}, {"authors": ["J. Lei", "M. GSell", "A. Rinaldo", "R.J. Tibshirani", "L. Wasserman"], "title": "Distributionfree predictive inference for regression", "venue": "Journal of the American Statistical Association 113 (523), 1094\u20131111.", "year": 2018}, {"authors": ["J. Leike", "T. Lattimore", "L. Orseau", "M. Hutter"], "title": "Thompson Sampling is Asymptotically Optimal in General Environments", "venue": "Conference on Uncertainty in Artificial Intelligence. AUAI Press.", "year": 2016}, {"authors": ["F. Liang", "Q. Li", "L. Zhou"], "title": "Bayesian neural networks for selection of drug sensitive genes", "venue": "Journal of the American Statistical Association 113 (523), 955\u2013972.", "year": 2018}, {"authors": ["A.R. Linero"], "title": "Bayesian regression trees for high-dimensional prediction and variable selection", "venue": "Journal of the American Statistical Association 113 (522), 626\u2013636.", "year": 2018}, {"authors": ["A.R. Linero", "Y. Yang"], "title": "Bayesian regression tree ensembles that adapt to smoothness and sparsity", "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology) 80 (5), 1087\u20131110.", "year": 2018}, {"authors": ["Y. Liu", "V. Rockova", "Y. Wang"], "title": "ABC Variable Selection with Bayesian Forests", "venue": "arXiv:1806.02304.", "year": 2018}, {"authors": ["G. Louppe", "L. Wehenkel", "A. Sutera", "P. Geurts"], "title": "Understanding variable importances in forests of randomized trees", "venue": "Advances in Neural Information Processing Systems.", "year": 2013}, {"authors": ["Y. Lu", "Y. Fan", "J. Lv", "W.S. Noble"], "title": "DeepPINK: reproducible feature selection in Deep Neural Networks", "venue": "Advances in Neural Information Processing Systems.", "year": 2018}, {"authors": ["M. Mase", "A.B. Owen", "B. Seiler"], "title": "Explaining black box decisions by shapley cohort refinement", "venue": "arXiv:1911.00467.", "year": 2019}, {"authors": ["T.J. Mitchell", "J.J. Beauchamp"], "title": "Bayesian variable selection in linear regression", "venue": "Journal of the American Statistical Association 83 (404), 1023\u20131032.", "year": 1988}, {"authors": ["N.N. Narisetty", "X. He"], "title": "Bayesian variable selection with shrinking and diffusing priors", "venue": "The Annals of Statistics", "year": 2014}, {"authors": ["J.D. Olden", "D.A. Jackson"], "title": "Illuminating the black box: a randomization approach for understanding variable contributions in artificial neural networks", "venue": "Ecological modelling 154 (1-2), 135\u2013150.", "year": 2002}, {"authors": ["A.B. Owen", "C. Prieur"], "title": "On shapley value for measuring importance of dependent inputs", "venue": "SIAM/ASA Journal on Uncertainty Quantification 5 (1), 986\u20131002.", "year": 2017}, {"authors": ["S. Pandey", "D. Chakrabarti", "D. Agarwal"], "title": "Multi-armed bandit problems with dependent arms", "venue": "International Conference on Machine learning.", "year": 2007}, {"authors": ["E. Patterson", "M. Sesia"], "title": "knockoff: The Knockoff Filter for Controlled Variable Selection", "venue": "Statistics Department, Stanford University. R package version 0.3.2.", "year": 2018}, {"authors": ["Rhee", "S.-Y", "W.J. Fessel", "A.R. Zolopa", "L. Hurley", "T. Liu", "J. Taylor", "D.P. Nguyen", "S. Slome", "D. Klein", "M. Horberg"], "title": "HIV-1 protease and reverse-transcriptase mutations: correlations with antiretroviral therapy in subtype b isolates and implications for drug-resistance surveillance", "venue": "The Journal of infectious diseases", "year": 2005}, {"authors": ["Rhee", "S.-Y.", "J. Taylor", "G. Wadhera", "A. Ben-Hur", "D.L. Brutlag", "R.W. Shafer"], "title": "Genotypic predictors of Human Immunodeficiency Virus type 1 drug resistance", "venue": "Proceedings of the National Academy of Sciences 103 (46), 17355\u201317360.", "year": 2006}, {"authors": ["V. Rockova", "E.I. George"], "title": "EMVS: The EM Approach to Bayesian Variable Selection", "venue": "Journal of the American Statistical Association 109 (506), 828\u2013846.", "year": 2014}, {"authors": ["V. Rockova", "E.I. George"], "title": "The spike-and-slab LASSO", "venue": "Journal of the American Statistical Association 113 (521), 431\u2013444.", "year": 2018}, {"authors": ["D. Rossell", "D. Telesca"], "title": "Nonlocal priors for high-dimensional estimation", "venue": "Journal of the American Statistical Association 112 (517), 254\u2013265.", "year": 2017}, {"authors": ["P.N. Sabes", "M.I. Jordan"], "title": "Reinforcement learning by probability matching", "venue": "Advances in Neural Information Processing Systems.", "year": 1996}, {"authors": ["S.L. Scott"], "title": "A modern Bayesian look at the multi-armed bandit", "venue": "Applied Stochastic Models in Business and Industry", "year": 2010}, {"authors": ["L.S. Shapley"], "title": "A value for n-person games", "venue": "Contributions to the Theory of Games 2 (28), 307\u2013317.", "year": 1953}, {"authors": ["E. Song", "B.L. Nelson", "J. Staum"], "title": "Shapley effects for global sensitivity analysis: Theory and Computation", "venue": "SIAM/ASA Journal on Uncertainty Quantification 4 (1), 1060\u20131083.", "year": 2016}, {"authors": ["M. Sundararajan", "A. Najmi"], "title": "The many shapley values for model explanation", "venue": "arXiv:1908.08474.", "year": 2019}, {"authors": ["W.R. Thompson"], "title": "On the likelihood that one unknown probability exceeds another in view of the evidence of two sample", "venue": "Biometrika 25 (3/4), 285\u2013294.", "year": 1933}, {"authors": ["M. Vannucci", "F.C. Stingo"], "title": "Bayesian models for variable selection that incorporate biological information", "venue": "Bayesian Statistics 9, 1\u201320.", "year": 2010}, {"authors": ["S. Wang", "W. Chen"], "title": "Thompson sampling for combinatorial semi-bandits", "venue": "International Conference on Machine Learning, pp. 5114\u20135122.", "year": 2018}, {"authors": ["M. Ye", "Y. Sun"], "title": "Variable selection via penalized neural network: a drop-out-one loss approach", "venue": "International Conference on Machine Learning.", "year": 2018}, {"authors": ["T. Zhang", "S.S. Ge", "C.C. Hang"], "title": "Adaptive neural network control for strictfeedback nonlinear systems using backstepping design", "venue": "Automatica 36 (12), 1835\u20131846.", "year": 2000}, {"authors": [], "title": "Additional Results for the HIV Dataset In this section, we show additional results on the analysis of the HIV dataset. First, we present some basic statistics about the data. The entire data comes from Stanford HIV Drug Resistance Database", "year": 2015}], "sections": [{"text": "Keywords: BART, Combinatorial Bandits, Interpretable Machine Learning, Spike-andSlab, Thompson Sampling, Variable Selection"}, {"heading": "1 Interpretable Machine Learning", "text": "A fundamental challenge in statistics that goes beyond mere prediction is to glean inter-\npretable insights into the nature of real-world processes by identifying important correlates\n\u2217Yi Liu is a 3rd year PhD student at the Department of Statistics at the University of Chicago \u2020Veronika Rockova is Assistant Professor in Econometrics and Statistics at the Booth School of Business of the University of Chicago. The author gratefully acknowledges the support from the James S. Kemper Foundation Research Fund at the Booth School of Business.\nar X\niv :2\n00 7.\n00 18\n7v 1\n[ cs\n.L G\n] 1\nJ ul\n2 02\nof variation. Unfortunately, many today\u2019s most powerful machine learning prediction algo-\nrithms lack the capacity to perform variable screening in a principled and/or reliable way.\nFor example, deep learning (DL) is widely accepted as one of the best performing artificial\nintelligence (AI) platforms. However, DL prediction mappings lack an intuitive algebraic\nform which renders their interpretability/explainability (i.e. insight into the black box de-\ncision process) far from straightforward. Substantial effort has been recently devoted to\nenhancing the explainability of machine (deep) learning through the identification of key\nvariables that drive predictions (Garson (1991); Olden and Jackson (2002); Zhang et al.\n(2000); Lu et al. (2018)). For instance, Lu et al. (2018) integrated the idea of a knock-off\nfilter with deep neural networks and derived \u2019DeepPINK\u2019 for variable selection that controls\nthe false discovery rate. In a similar vein, Burns et al. (2019) proposed a formal testing\nprocedure for whether the model prediction is significantly different after features have\nbeen replaced with uninformative counterfactuals. Horel and Giesecke (2019) proposed\na test statistic for a single-layer feed forward network. While possessing nice theoretical\nguarantees, many of these procedures are not yet feasible for large-scale applications.\nA variable can be important because its change has a causal impact or because leaving it\nout reduces overall prediction capacity (Jiang and Owen (2003)). Such leave-one-covariate-\nout type inference has a long tradition, going back to at least Breiman (2001). In random\nforests, for example, variable importance is assessed by the difference between prediction er-\nrors in the out-of-bag sample before and after noising the covariate through a permutation.\nLei et al. (2018) propose the LOCO method which gauges local effects of removing each\ncovariate on the overall prediction capability and derives an asymptotic distribution for\nthis measure to conduct proper statistical tests. There is a wealth of literature on variable\nimportance measures, see Fisher et al. (2019) for a recent overview. In Bayesian forests,\nsuch as BART (Chipman et al. (2001)), one keeps track of predictor inclusion frequencies\nand outputs an average proportion of all splitting rules inside a tree ensemble that split on\na given variable. In deep learning, one can construct variable importance measures using\nnetwork weights (Garson (1991); Ye and Sun (2018)). Owen and Prieur (2017) introduce a\nvariable importance based on a Shapley value and Hooker (2007) investigates diagnostics\nof black box functions using functional ANOVA decompositions with dependent covari-\nates. While useful for ranking variables, importance measures are less intuitive for model\nselection and are often not well-understood theoretically (with a few exceptions including\nIshwaran et al. (2007); Kazemitabar et al. (2017)).\nThis work focuses on high-dimensional applications (either very many predictors or very\nmany observations, or both), where computing importance measures and performing tests\nfor predictor effects quickly becomes infeasible. We consider the non-parametric regression\nmodel which provides a natural statistical framework for supervised machine learning. The data setup consists of a continuous response vector Y(n) = (Y1, \u00b7 \u00b7 \u00b7 , Yn)\u2032 that is linked stochastically to a fixed set of predictors xi = (xi1, \u00b7 \u00b7 \u00b7 , xip)\u2032 for 1 \u2264 i \u2264 n through\nYi = f0(xi) + i where i iid\u223c N(0, \u03c32), (1)\nand where f0 is an unknown regression function. The variable selection problem occurs when there is a subset S0 \u2282 {1, \u00b7 \u00b7 \u00b7 , p} of q0 = |S0| predictors which exert influence on the mixing function f0 and we do not know which subset it is. In other words, f0 is constant in directions outside S0 and the goal is to identify active directions (regressors) in S0 while, at the same time, permitting nonlinearities and interactions. The traditional Bayesian approach to this problem starts with a prior distribution over the 2p sets of\nactive variables. This is typically done in a hierarchical fashion by first assigning a prior distribution \u03c0(q) on the subset size q = |S| and then a conditionally uniform prior on S, given q, i.e. \u03c0(S|q) = 1 (pq) . This prior can be translated into the spike-and-slab prior where, for each coordinate 1 \u2264 i \u2264 p, one assumes a binary indicator \u03b3i for whether or not the variable xi is active and assigns a prior\nP(\u03b3i | \u03b8) = \u03b8, \u03b8 \u223c Beta(a, b) for some a, b > 0. (2)\nThe active subset S is then constructed as S = {j : \u03b3j = 1}. There is no shortage of literature on spike-and-slab variable selection in the linear model, addressing prior choices\n(Mitchell and Beauchamp (1988); Rockova and George (2018); Rossell and Telesca (2017);\nVannucci and Stingo (2010); Brown et al. (1998)), computational aspects (Carbonetto\net al. (2012); Rockova and George (2014); Bottolo et al. (2010), George and McCulloch\n(1993),George and McCulloch (1997)) and/or variable selection consistency results (Castillo\net al. (2015); Johnson and Rossell (2012); Narisetty et al. (2014)). In this work, we leave\nbehind the linear model framework and focus on interpretable machine learning linking\nspike-and-slab methods with binary bandits.\nThis paper introduces Thompson Variable Selection (TVS), a stochastic optimization\napproach to subset selection based on reinforcement learning. The key idea behind TVS\nis that variable selection can be regarded as a combinatorial bandit problem where each\nvariable is treated as an arm. TVS sequentially learns promising combinations of arms\n(variables) that are most likely to provide a reward. Depending on the learning tool for\nmodeling f0 (not necessarily a linear model), TVS accommodates a wide range of rewards for both offline and online (streaming batches) setups. The fundamental appeal of active\nlearning for subset selection (as opposed to MCMC sampling) is that those variables which\nprovided a small reward in the past are less likely to be pulled again in the future. This\nexploitation aspect steers model exploration towards more promising combinations and\noffers dramatic computational dividends. Indeed, similarly as with backward elimination\nTVS narrows down the inputs contributing to f0 but does so in a stochastic way by learning from past mistakes. TVS aggregates evidence for variable inclusion and quickly separates\nsignal from noise by minimizing regret motivated by the median probability model rule\n(Barbieri and Berger, 2004). We provide regret bounds which do not necessarily assume\nthat the arm outcomes be unrelated. In addition, we show strong empirical performance\nand demonstrate the potential of TVS to meet demands of very large datasets.\nThis paper is structured as follows. Section 2 revisits known facts about multi-armed\nbandits. Section 3 develops the bandits framework for variable selection and Section 4\nproposes Thompson Variable Selection and presents a regret analysis. Section 5 presents\ntwo implementations (offline and online) on two benchmark simulated data. Section 6\npresents a thorough simulation study and Section 7 showcases TVS performance on real\ndata. We conclude with a discussion in Section 8."}, {"heading": "2 Multi-Armed Bandits Revisited", "text": "Before introducing Thompson Variable Selection, it might be useful to review several known\nfacts about multi-armed bandits. The multi-armed bandit (MAB) problem can be moti-\nvated by the following gambling metaphor. A slot-machine player needs to decide between multiple arms. When pulled at time t, the ith arm gives a random payout \u03b3i(t). In the Bernoulli bandit problem, the rewards \u03b3i(t) \u2208 {0, 1} are binary and P(\u03b3i(t) = 1) = \u03b8i. The distributions of rewards are unknown and the player can only learn about them through\nplaying. In doing so, the player faces a dilemma: exploiting arms that have provided high\nyields in the past and exploring alternatives that may give higher rewards in the future.\nMore formally, an algorithm for MAB must decide which of the p arms to play at time t, given the outcome of the previous t \u2212 1 plays. A natural goal in the MAB game is to\nminimize regret, i.e. the amount of money one loses by not playing the optimal arm at each step. Denote with i(t) the arm played at time t, with \u03b8? = max 1\u2264i\u2264p \u03b8i the best average reward and with \u2206i = \u03b8 ?\u2212\u03b8i the gap between the rewards of an optimal action and a chosen action.\nThe expected regret after T plays can be then written as E[R(T )] = \u2211p\ni=1 \u2206iE[ki(T )], where kj(T ) = \u2211T t=1 I[i(t) = j] is the number of times an arm j has been played up to step T . There have been two main types of algorithms designed to minimize regret in the MAB\nproblem: Upper Confidence Bound (UCB) of Lai and Robbins (1985) and Thompson\nSampling (TS) of Thompson (1933). Tracing back to Agrawal (1995), UCB consists of\ncomputing, at each round t and for each arm i, a reward index (e.g. an upper bound of the\nmean reward of the considered arm that holds with high confidence) and then selecting the\narm with the largest index. Thompson Sampling, on the other hand, is a Bayesian-inspired\nheuristic algorithm that achieves a logarithmic expected regret (Agrawal and Goyal, 2012) in the Bernoulli bandit problem. Starting with a non-informative prior \u03b8i iid\u223c Beta(1, 1) for 1 \u2264 i \u2264 p, this algorithm: (a) updates the distribution of \u03b8i as Beta(ai(t) + 1, bi(t) + 1), where ai(t) and bi(t) are the number of successes and failures of the arm i up to time t, (b) samples \u03b8i(t) from these posterior distributions, and (c) plays the arm with the highest \u03b8i(t). Agrawal and Goyal (2012) extended this algorithm to the general case where rewards are not necessarily Bernoulli but general random variables on the interval [0, 1]. Scott\n(2010) and Sabes and Jordan (1996) proposed a Randomized Probability Matching variant\nwhich allocates observations to arms according to their probability of being the best arm.\nThe MAB problem is most often formulated as a single-play problem, where only one\narm can be selected at each round. Komiyama et al. (2015) extended Thompson sampling to a multi-play scenario, where at each round t the player selects a subset St of L < p arms and receives binary rewards of all selected arms. For each 1 \u2264 i \u2264 p, these rewards ri(t) are iid Bernoulli with unknown success probabilities \u03b8i where \u03b3i(t) and \u03b3j(t) are independent for i 6= j and where, without loss of generality, \u03b81 > \u03b82 > \u00b7 \u00b7 \u00b7 > \u03b8p. The player is interested in maximizing the sum of expected rewards over drawn arms, where the optimal action is\nplaying the top L arms S0 = {1, . . . , L}. The regret depends on the combinatorial structure of arms drawn and, similarly as before, is defined as the gap between an expected cumulative\nreward and the optimal drawing policy, i.e. E[R(T )] = E \u2211T\nt=1 (\u2211 i\u2208S0 \u03b8i \u2212 \u2211 i\u2208St \u03b8i ) Fixing\nL, the number of arms played, Komiyama et al. (2015) propose a Thompson sampling\nalgorithm for this problem and show that it has a logarithmic expected regret with respect\nto time and a linear regret with respect to the number of arms. Our metamorphosis of\nmulti-armed bandits into a variable selection algorithm will ultimately require that the\nnumber L of arms played is random and that the rewards at each time t can be dependent.\nCombinatorial bandit problems (Chen et al. (2013); Gai et al. (2012); Cesa-Bianchi and\nLugosi (2012)) can be seen as a generalization of multi-play bandits, where any arbitrary combination of arms S (called super-arms) is played at each round and where the reward r(S) can be revealed for the entire collective S (a full-bandit feedback) or for each contributing arm i \u2208 S (a semi-bandit feedback), see e.g. Wang and Chen (2018); Combes\nand Proutiere (2014); Kveton et al. (2015); Combes and Proutiere (2014); Kveton et al.\n(2015)."}, {"heading": "3 Variable Selection as a Bandit Problem", "text": "Bayesian model selection is regarded as more or less synonymous to finding the MAP (maximum-a-posteriori) model S\u0302 = arg maxS \u03c0(S |Y (n)). Even when the marginal likelihood is available, this model can computationally unattainable for p as small as 20. In\norder to accelerate Bayesian variable selection using multi-armed bandits techniques one idea immediately comes to mind. One could treat each of the 2p models as a base arm. Assigning prior model probabilities according to \u03b8i \u223c Beta(ai, bi) for 1 \u2264 i \u2264 2p for some1 ai > 0 and bi > 0, one could play a game by sequentially trying out various arms (variable subsets) and collect rewards to prioritize subsets that were suitably \u201cgood\u201d. Identifying\nthe arm with the highest mean reward could then serve as a proxy for the best model. This\nnaive strategy, however, would not be operational due to the exponential number of arms\nto explore.\nInstead of the MAP model, it has now been standard practice to report the median\nprobability model (MPM) (Barbieri and Berger (2004)) consisting of those variables whose\n1chosen to correspond to marginals of a Dirichlet distribution\nposterior inclusion probability \u03c0i \u2261 P(\u03b3i = 1 |Y (n)) is at least 0.5. More formally, MPM is defined, for \u03c0 = (\u03c01, . . . , \u03c0p) \u2032, as\nS\u0302MPM = arg max S r\u03c0(S) = {i : \u03c0i \u2265 0.5} where r\u03c0(S) = {\u220f i\u2208S \u03c0i \u220f i/\u2208S (1\u2212 \u03c0i) } . (3) This model is the optimal predictive model in linear regression under some assumptions\n(Barbieri et al., 2018). Obtaining \u03c0i\u2019s, albeit easier than finding the MAP model, requires posterior sampling over variable subsets. While this can be done using standard MCMC\nsampling techniques in linear regression (George and McCulloch (1997); Narisetty et al.\n(2014); Bhattacharya et al. (2016)), here we explore new curious connections to bandits\nin order to develop a much faster stochastic optimization routine for finding MPM-alike\nmodels when the true model is not necessarily linear.\nWhile the MAP model suggests treating each model as a bandit arm, the MPM model\nsuggests treating each variable as a bandit arm. Under the MAP framework, the player\nwould be required to play a single arm (i.e. a model) at each step. The MPM framework,\non the other hand, requires playing a random subset of arms (i.e. a model) at each play\nopportunity. This is appealing for at least two reasons: (1) there are fewer arms to explore more efficiently, (2) the quantity r\u03c0(S) can be regarded as a mean regret of a combinatorial arm (more below) which, given \u03c0, has MPM as its computational oracle.\nWe view Bayesian spike-and-slab selection through the lens of combinatorial bandit\nproblems by treating variable selection indicators \u03b3i\u2019s in (2) as Bernoulli rewards. From now on, we will refer to each \u03b8i as an unknown mean reward, i.e. a probability that the i th variable exerts influence on the outcome. In sharp contrast to (2) which deploys one \u03b8 for all arms, each arm i \u2208 {1, . . . , p} now has its own prior inclusion probability \u03b8i, i.e.\nP(\u03b3i = 1 | \u03b8i) = \u03b8i, \u03b8i ind\u223c Beta(ai, bi) for some ai, bi > 0. (4)\nIn the original spike-and-slab setup (2), the mixing weight \u03b8 served as a global shrinkage\nparameter determining the level of sparsity and linking coordinates to borrow strength\n(Rockova and George (2018)). In our new bandit formulation (4), on the other hand,\nthe reward probabilities \u03b8i serve as a proxy for posterior inclusion probabilities \u03c0i whose distribution we want to learn by playing the bandits game. Recasting the spike-and-slab\nprior in this way allows one to approach Bayesian variable selection from a more algorithmic\n(machine learning) perspective."}, {"heading": "3.1 The Global Reward", "text": "Before proceeding, we need to define the reward in the context of variable selection. One conceptually appealing strategy would be to collect a joint reward R(St) (e.g. goodness of model fit) reflecting the collective effort of all contributing arms and then redistribute it among arms inside the super-arm St played at time t. One example would be the Shapley value (Shapley (1953); Owen and Prieur (2017)), a construct from cooperative game theory\nfor the attribution problem that distributes the value created by a team to its individual\nmembers. The Shapley value has become a popular method for prediction attribution in\nmachine learning (Sundararajan and Najmi (2019); Mase et al. (2019)). Although the\nShapley value has a strong intuitive appeal, computation remains a challenge (Castro et al.\n(2009); Song et al. (2016)).\nWe try a different route. Instead of collecting a global reward first and then redistributing it, our strategy consists of first collecting individual rewards \u03b3ti \u2208 {0, 1} for each played arm i \u2208 St and then weaving them into a global reward R(St). We assume that \u03b3ti \u2019s are iid from (4) for each i \u2208 {1, . . . , p}. Unlike traditional combinatorial bandits that define\nthe global reward R(St) = \u2211 i\u2208S \u03b3 t i as a sum of individual outcomes (Gai et al., 2012), we consider a global reward for variable selection motivated by the median probability model.\nOne natural choice would be a binary global reward R(St) = \u220f\ni\u2208St \u03b3 t i \u220f i/\u2208St(1 \u2212 \u03b3 t i) \u2208\n{0, 1} for whether or not all arms inside St yielded a reward and, at the same time, none of the arms outside St did. Assuming independent arms, the expected reward equals\nE[R(St)] = \u220f i\u2208St \u03b8i \u220f i/\u2208St(1 \u2212 \u03b8i) = r\u03b8(St) and has the \u201cmedian probability model\u201d as its computational oracle, as can be seen from (3). However, this expected reward is not\nmonotone in \u03b8i\u2019s (a requirement needed for regret analysis) and, due to its dichotomous nature, it penalizes all mistakes (false positives and negatives) equally.\nWe consider an alternative reward function which also admits a computational oracle but treats mistakes differentially. For some 0 < C < 1, we define the global reward RC(St) for a subset St at time t as\nRC(St) = \u2211 i\u2208St log ( C + \u03b3ti ) . (5)\nSimilarly as R(St) (defined above) the reward is maximized for the model which includes all the positive arms and none of the negative arms, i.e. arg maxS RC(S) = {i : \u03b3ti = 1}.\nUnlike R(St), however, the reward will penalize subsets with false positives, a penalty log(C) for each, and there is an opportunity cost of log(1 + C) for each false negative. The expected global reward depends on the subset St and the vector of yield probabilities \u03b8 = (\u03b81, . . . , \u03b8p) \u2032, i.e.\nrC\u03b8 (St) = E [RC(St)] = \u2211 i\u2208St [ \u03b8i log ( C + 1 C ) \u2212 log ( 1 C )] . (6)\nNote that this expected reward is monotone in \u03b8i\u2019s and is Lipschitz continuous. Moreover, it also has the median probability model as its computational oracle.\nLemma 1 Denote with SO = arg maxS rC\u03b8 (S) the computational oracle. Then we have SO = { i : \u03b8i \u2265 log(1/C)\nlog[(C + 1)/C]\n} . (7)\nWith C = ( \u221a 5\u2212 1)/2, the oracle is the median probability model {i : \u03b8i \u2265 0.5}.\nProof: It follows immediately from the definition of RC(St) and the fact that log(1/C) = 0.5 log[(1 + C)/C] for C = ( \u221a 5\u2212 1)/2.\nNote that the choice of C = ( \u221a 5 \u2212 1)/2 incurs the same penalty/opportunity cost for false positives and negatives since log(1 + C) = \u2212 log(C). The existence of the computational oracle for the expected reward rC\u03b8 (S) is very comforting and will be exploited in our Thompson sampling algorithm introduced in Section 4"}, {"heading": "3.2 The Local Rewards", "text": "Having introduced the global reward (5), we now clarify the definition of local rewards \u03b3ti . We regard St as a smaller pool of candidate variables, which can contain false positives and false negatives. The goal is to play a game by sequentially trying out different subsets\nand reward true signals so that they are selected in the next round and to discourage false positives from being included again in the future. Denote with S the set of all subsets of {1, . . . , p} and with D the \u201cdata\u201d at hand consisting of |D| observations (Yi,xi) from (1). We introduce a feedback rule\nr(St,D) : S\u00d7 R|D| \u2192 {0, 1}|St|, (8)\nwhich, when presented with data D and a subset St, outputs a vector of binary rewards r(St,D) = (\u03b3ti : i \u2208 St)\u2032 for whether or not a variable xi for i \u2208 St is relevant for predicting or\nexplaining the outcome. This feedback is only revealed if i \u2208 St. We consider two sources of randomness that implicitly define the reward distribution r(St,D): (1) a stochastic feedback rule r(\u00b7) assuming that data D is given, and (2) a deterministic feedback rule r(\u00b7) assuming that data D is stochastic.\nThe first reward type has a Bayesian flavor in the sense that it is conditional on the observed data Dn = {(Yi,xi) : 1 \u2264 i \u2264 n}, where rewards can be sampled using Bayesian stochastic computation (i.e. MCMC sampling). Such rewards are natural in offline settings\nwith Bayesian feedback rules, as we explore in Section 5.1. As a lead example of this strategy\nin this paper, we consider a stochastic reward based on BART (Chipman et al. (2001)). In particular, we use the following binary local reward r(St,Dn) = (\u03b3ti : i \u2208 St)\u2032 where\n\u03b3ti = I(M th sample from the BART posterior splits on the variable xi). (9)\nThe mean reward \u03b8i = P(\u03b3ti = 1) = P[i \u2208 F |Dn] can be interpreted as the posterior probability that a variable xi is split on in a Bayesian forest F given the entire data Dn. The stochastic nature of the BART computation allows one to regard the reward (9) as\nan actual random variable, whose values can be sampled from using standard software. Since BART is run only with variables inside St (where |St| << p) and only for M burn-in MCMC iterations, computational gains are dramatic (as we will see in Section 5.1).\nThe second reward type has a frequentist flavor in the sense that rewards are sampled by applying deterministic feedback rules on new streams (or bootstrap replicates) Dt of data. Such rewards are natural in online settings, as we explore in Section 5.2. As a lead example of this strategy in this paper, we assume that the dataset Dn consist of n = sT observations and is partitioned into minibatches Dt = {(Yi,xi) : (t\u2212 1)s+ 1 \u2264 i \u2264 ts} for t = 1, . . . , T . One could think of these batches as new independent observations arriving\nin an online fashion or as manageable snippets of big data. The \u2018deterministic\u2019 screening\nrule we consider here is running BART for a large number M of MCMC iterations and collecting an aggregated importance measure IM(i; Dt,St) for each variable.2 We define IM(i; Dt,St as the average number of times a variable xi was used in a forest where the average is taken over the M iterations and we then reward those arms which were used at\n2 This rule is deterministic in the sense that computing it again on the same data should in principle provide the same answer. One could, in fact, deploy any other machine learning method that outputs some measure of variable importance.\nleast once on average,\n\u03b3ti = I[IM(i; Dt,St) \u2265 1]. (10)\nThe mean reward \u03b8i = P(\u03b3ti = 1) can be then interpreted as the (frequentist) probability that BART, when run on s = n/T observations arising from (1), uses a variable xi at least once on average over M iterations. We illustrate this online variant in Section 5.2.\nRemark 1 Instead of binary local rewards (8), one can also consider continuous rewards \u03b3ti \u2208 [0, 1]|St| by rescaling variable importance measures obtained by a machine learning method (e.g. random forests (Louppe et al. (2013)), deep learning (Horel and Giesecke\n(2019)), and BART (Chipman et al. (2010))). Our Thompson sampling algorithm can\nbe then modified by dichotomizing these rewards through independent Bernoulli trials with\nprobabilities equal to the continuous rewards (Agrawal and Goyal, 2012)."}, {"heading": "4 Introducing Thompson Variable Selection (TVS)", "text": "This section introduces Thompson Variable Selection (TVS), a reinforcement learning al-\ngorithm for subset selection in non-parametric regression environments. The computation\nalternates between Choose, Reward and Update steps that we describe in more detail below.\nThe unknown mean rewards will be denoted with \u03b8?i and the ultimate goal of TVS is to learn their distribution once we have seen the \u2018data\u20193. To this end, we take the\ncombinatorial bandits perspective (Chen et al. (2013), Gai et al. (2010)) where, instead of playing one arm at each play opportunity t, we play a random subset St \u2286 {1, . . . , p} of multiple arms. Each such super-arm St corresponds to a model configuration and the goal is to discover promising models by playing more often the more promising variables.\nSimilarly as with traditional Thompson Sampling, the tth iteration of TVS starts off by sampling mean rewards \u03b8i(t) \u223c Beta(ai(t), bi(t)) from a posterior distribution that incorporates past reward experiences up to time t (as we discussed in Section 2). The\nChoose Step then decides which arms will be played in the next round. While the single-\nplay Thompson sampling policy dictates playing the arm with the highest sampled expected\nreward, the combinatorial Thompson sampling policy (Wang and Chen (2018)) dictates\nplaying the subset that maximizes the expected global reward, given the vector of sampled\n3The \u2018data\u2019 here refers to the sequence of observed rewards.\nAlgorithm 1: Thompson Variable Selection with BART\nDefine C\u0303 = log(1/C)log[(1+C)/C for some 0 < C < 1 and pick M,a, b > 0\nInitialize ai(0) := a and bi(0) := b for 1 \u2264 i \u2264 p and for t = 1, . . . , T Choose Step\nC: Set St = \u2205 and for i = 1 \u00b7 \u00b7 \u00b7 p do C1: Sample \u03b8i(t) \u223c Beta(ai(t), bi(t)) C2: Compute St = {i : \u03b8i(t) \u2265 C\u0303} from (7) C2\u2217: Compute St from (13) Reward Step R: Collect local rewards for each 1 \u2264 i \u2264 p from (9) (offline) or (10) (online) Update Step U: If \u03b3ti = 1 then set ai(t+ 1) = ai(t) + 1, else bi(t+ 1) = bi(t) + 1\nAlgorithm 1: Thompson Variable Selection with BART (? is an alternative with known q?)\nprobabilities \u03b8(t) = (\u03b81(t), . . . , \u03b8p(t)) \u2032. The availability of the computational oracle (from Lemma 1) makes this step awkwardly simple as it boils down to computing SO in (7). Unlike multi-play bandits where the number of played arms is predetermined (Komiyama\n(2015)), this strategy allows one to adapt to the size of the model. We do, however, consider a variant of the computational oracle (see (13) below) for when the size q? = |S?| of the \u2018true\u2019 model S? = arg maxS rC\u03b8?(S) is known. The Choose Step is then followed by the Reward Step (step R in Table 1) which assigns a prize to the chosen subset St by collecting individual rewards \u03b3ti (for the offline setup in (9) or for the online setup (10)). Finally, each TVS iteration concludes with an Update Step which updates the beta posterior distribution\n(step U in Table 1).\nThe fundamental goal of Thompson Variable Selection is to learn the distribution of\nmean rewards \u03b8i\u2019s by playing a game, i.e. sequentially creating a dataset of rewards by sampling from beta posterior4 distributions that incorporate past rewards and the observed data D. One natural way to distill evidence for variable selection is through the means \u03c0(t) = (\u03c01(t), . . . , \u03c0p(t)) \u2032 of these beta distributions\n\u03c0i(t) = ai(t)\nai(t) + bi(t) , 1 \u2264 i \u2264 p, (11)\nwhich serve as a proxy for posterior inclusion probabilities. Similarly as with the classical\nmedian probability model (Barbieri and Berger (2004)), one can deem important those\nvariables with \u03c0i(t) above 0.5 (this corresponds to one specific choice of C in Lemma 1). More generally, at each iteration t TVS outputs a model S\u0302t, which satisfies S\u0302t = 4This posterior treats the past rewards as the actual data.\narg maxS r C \u03c0(t). From Lemma 1, this model can be simply computed by truncating individual \u03c0i(t)\u2019s. Upon convergence, i.e. when trajectories \u03c0i(t) stabilize over time, TVS will output the same model S\u0302t. We will see from our empirical demonstrations in Section 5 that the separation between signal and noise (based on \u03c0i(t)\u2019s) and the model stabilization occurs fast. Before our empirical results, however, we will dive into the regret analysis of TVS."}, {"heading": "4.1 Regret Analysis", "text": "Thompson sampling (TS) is a policy that uses Bayesian ideas to solve a fundamentally\nfrequentist problem of regret minimization. In this section, we explore regret properties\nof TVS and expand current theoretical understanding of combinatorial TS by allowing for\ncorrelation between arms. Theory for TS was essentially unavailable until the path-breaking\npaper by Agrawal and Goyal (2012) where the first finite-time analysis was presented for\nsingle-play bandits. Later, Leike et al. (2016) proved that TS converges to the optimal\npolicy in probability and almost surely under some assumptions. Several theoretical and\nempirical studies for TS in multi-play bandits are also available. In particular, Komiyama\net al. (2015) extended TS to multi-play problems with a fixed number of played arms\nand showed that it achieves the optimal regret bound. Recently, Wang and Chen (2018)\nintroduced TS for combinatorial bandits and derived regret bounds for Lipschitz-continuous\nrewards under an offline oracle. We build on their development and extend their results to\nthe case of related arms.\nRecall that the goal of the player is to minimize the total (expected) regret under time\nhorizon T defined below\nReg(T ) = E [ T\u2211 t=1 ( rC\u03b8?(S?)\u2212 rC\u03b8?(St) )] , (12)\nwhere S? = arg maxS rC\u03b8?(St) with q? = |S?|, \u03b8?i = E[\u03b3ti ] and where the expectation is taken over the unknown drawing policy. Choosing C as in Lemma 1, one has log(1 +C) = \u2212 log(C) = D and thereby\nReg(T ) = DE [ T\u2211 t=1 p\u2211 i=1 (2\u03b8?i \u2212 1)[I(i \u2208 S?\\St)\u2212 I(i \u2208 St\\S?)] ] .\nNote that (2\u03b8?i \u2212 1) is positive iff i \u2208 S?. Upper bounds for the regret (12) under the drawing policy of our TVS Algorithm 1 can be obtained under various assumptions.\nAssuming that the size q? of the optimal model S? is known, one can modify Algorithm 1 to confine the search to models of size up to q?. Denoting I = {S \u2282 {1, . . . , p} : |S| \u2264 q?}, one plays the optimal set of arms within the set I, i.e. replacing the computational oracle in (7) with Sq ?\nO = arg maxS\u2208I r\u03b8(S). We denote this modification with C2? in Table 1. It\nturns out that this oracle can also be easily computed, where the solution consists of (up to) the top q? arms that pass the selection threshold, i.e.\nSq ?\nO = { i : \u03b8i \u2265\nlog(1/C)\nlog[(1 + C)/C]\n} \u2229 J(\u03b8) = {(i1, . . . , iq?)\u2032 \u2208 Nq ? : \u03b8i1 \u2265 \u03b8i2 \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03b8i?q}.\n(13)\nWe have the following regret bound which, unlike the majority of existing results for Thompson sampling, does not require the arms to have independent outcomes \u03b3ti . The regret bound depends on the amount of separation between signal and noise. Lemma 2 Define the identifiability gap \u2206i = min { \u03b8?j : \u03b8 ? j > \u03b8 ? i for j \u2208 S? } for each arm i /\u2208 S?. The Algorithm 1 with a computational oracle Sq ?\nO in C2 ? achieves the following\nregret bound\nReg(T ) \u2264 \u2211 i/\u2208S? (\u2206i \u2212 \u03b5) log T (\u2206i \u2212 2\u03b5)2 + C ( p \u03b54 ) + p2\nfor any \u03b5 > 0 such that \u2206i > 2\u03b5 for each i /\u2208 S? and for some constant C > 0.\nProof: Since I is a matroid (Kveton et al. (2014)) and our mean regret function is\nLipschitz continuous and it depends only on expected rewards of revealed arms, one can\napply Theorem 4 of Wang and Chen (2018).\nAssuming that the size q? of the optimal model is unknown and the rewards \u03b3ti are independent, we obtain the following bound for the original Algorithm 1 (without restricting the solution to up to q? variables).\nLemma 3 Define the maximal reward gap \u2206max = maxS \u2206S where \u2206S = [r\u03b8(S?)\u2212 r\u03b8(S)] and for each arm i \u2208 {1, . . . , p} define \u03b7i \u2261 maxS:i\u2208S 8B 2|S|\n\u2206S\u22122B(q?2+2)\u03b5 for B = log[(C + 1)/C].\nAssume that \u03b3ti \u2019s are independent for each t. Then the Algorithm 1 achieves the following regret bound\nReg(T ) \u2264 log(T ) p\u2211 i=1 \u03b7i + p ( p2 \u03b52 + 3 ) \u2206max + C 8\u2206max \u03b52 ( 4 \u03b52 + 1 )q? log(q?/\u03b52)\nfor some constant C > 0 and for any \u03b5 > 0 such that \u2206S > 2B(q ?2 + 2)\u03b5 for each S.\nProof: Follows from Theorem 1 of Wang and Chen (2018).\nWe now extend Lemma 3 to the case when the rewards obtained from pulling different\narms are related to one another. Gupta et al. (2020) introduced a correlated single-play\nbandit version of Thompson sampling using pseudo-rewards (upper bounds on the con-\nditional mean reward of each arm). Similarly as with structured bandits (Pandey et al. (2007)) we instead interweave the arms by allowing their mean rewards to depend on St, i.e. instead of a single success probability \u03b8i we now have\n\u03b8i(S) = P(\u03b3ti = 1|St = S). (14)\nWe are interested in obtaining a regret bound for the Algorithm 1 assuming (14) in which\ncase the expected global regret (6) writes as\nrC\u03b8 (St) = E [RC(St)] = \u2211 i\u2208St [ \u03b8i(St) log ( C + 1 C ) \u2212 log ( 1 C )] . (15)\nTo this end we impose an identifiability assumption, which requires a separation gap be-\ntween the reward probabilities of signal and noise arms.\nAssumption 1 Denote with S? = arg maxS rC\u03b8?(St) the optimal set of arms. We say that S? is strongly identifiable if there exists 0 < \u03b1 < 1/2 such that\n\u2200i \u2208 S\u2217 we have \u03b8i(S?) \u2265 \u03b8i(S) > 0.5 + \u03b1 \u2200S such that i \u2208 S,\n\u2200i /\u2208 S\u2217 we have \u03b8i(S) < 0.5\u2212 \u03b1 \u2200S such that i \u2208 S.\nUnder this assumption we provide the following regret bound.\nTheorem 1 Suppose that S? is strongly identifiable with \u03b1 > 0. Choosing C as in Lemma"}, {"heading": "1, the regret of Algorithm 1 satisfies", "text": "Reg(T ) \u2264 \u2206max\n[ 8p log(T )\n\u03b12 + c(\u03b1)q\u2217 +\n( 2 + 4\n\u03b12\n) p ] , (16)\nwhere c(\u03b1) = C\u0303 [ e\u22124\u03b1\n1\u2212e\u2212\u03b12/2\n] + 8\n\u03b12 1 e2\u03b1\u22121 + e\u22121 1\u2212e\u2212\u03b1/8 + \u2308 8 \u03b1 \u2309 ( 3 + 1 \u03b1 ) and C\u0303 = ( C1 + C2 1\u22122\u03b1 32\u03b1 ) for\nsome C1, C2 > 0 not related to the Algorithm 1, and \u2206max = maxS [r C \u03b8 (S\u2217)\u2212 rC\u03b8 (S)].\nProof: Appendix (Section A)"}, {"heading": "5 Interpretable Machine Learning with TVS", "text": "This section serves to illustrate Thompson Variable Selection on benchmark simulated\ndatasets and to document its performance. While various implementations are possible (by choosing different rewards r(St,D) in (8)), we will focus on two specific choices that we broadly categorize into offline variants for when p >> n (Section 5.1) and streaming/online\nvariants for when n >> p (Section 5.2)."}, {"heading": "5.1 Offline TVS", "text": "As a lead example in this section, we consider the benchmark Friedman data set (Friedman (1991)) with a vastly larger number of p = 10 000 predictors xi \u2208 [0, 1]p obtained by iid sampling from Uniform(0, 1) and responses Y = (Y1, \u00b7 \u00b7 \u00b7Yn)\u2032 obtained from (1) with \u03c32 = 1 and\nf0(xi) = 10 sin(\u03c0xi1xi2) + 20(xi3 \u2212 0.5)2 + 10xi4 + 5xi5 for i = 1, . . . , 300.\nDue to the considerable number of covariates, feeding all 10 000 predictors into a black box\nto obtain variable importance may not be computationally feasible and/or reliable. How-\never, TVS can overcome this limitation by deploying subsets of predictors. For instance, we\nconsidered variable importance using the BART method (using the option sparse=TRUE\nfor variable selection) with D \u2208 {10, 50} trees and M \u2208 {5 000, 50 000} MCMC iterations\nare plotted them in Figure 1. While increasing the number of iterations certainly helps\nin separating signal from noise, it is not necessarily obvious where to set the cutoff for\nselection. One natural rule would be selecting those variables which have been used at\nleast once on average over the M iterations. With D = 50 and M = 50 000, this rule would\nidentify 4 true signals, leaving out the quadratic signal variable x3. The computation took around 8.5 minutes.\nThe premise of TVS is that one can deploy a weaker learner (such as a forest with\nfewer trees) which generates a random reward that roughly captures signal and is allowed\nto make mistakes. With reinforcement learning, one hopes that each round will be wrong\nin a different way so that mistakes will not be propagated over time. The expectation is that (a) feeding only a small subset St in a black box and (b) reinforcing positive outcomes, one obtains a more principled way of selecting variables and speeds up the computation.\nWe illustrate the effectiveness of this mechanism below.\nWe use the offline local binary reward defined in (9). We start with a non-informative prior ai(0) = bi(0) = 1 for 1 \u2264 i \u2264 p and choose T = 10 trees in BART so that variables are discouraged from entering the model too wildly. This is a weak learner which does\nnot seem to do perfectly well for variable selection even after very many MCMC iterations\n(see Figure 1(d)). We use the TVS implementation in Table 1 with a dramatically smaller number M \u2208 {100, 500, 1 000} of MCMC burn-in iterations for BART inside TVS. We will\nsee below that large M is not needed for TVS to unravel signal even with as few as 10 trees.\nTVS results are summarized in Figure 2, which depicts \u2018posterior inclusion probabilities\u2019\n\u03c0i(t) defined in (11) over time t (the number of plays), one line for each of the p = 10 000 variables. To better appreciate informativeness of \u03c0i(t)\u2019s, true variables x1, . . . , x5 are depicted in red while the noise variables are black. Figure 2 shows a very successful\ndemonstration for several reasons. The first panel (Figure 2a) shows a very weak learner\n(as was seen from Figure 1) obtained by sampling rewards only after M = 100 burnin\niterations. Despite the fact that learning at each step is weak, it took only around T = 300\niterations (obtained in less than 40 seconds!) for the \u03c0i(t) trajectories of the 5 signals to cross the 0.5 decision boundary. After T = 300 iterations, the noise covariates are safely\nsuppressed below the decision boundary and the trajectories \u03c0i(t) stabilize towards the end\nset. The plot depicts posterior inclusion probabilities \u03c0i(t) in (11) over time (number of TVS\niterations). Red lines indicate the 5 signal variables and black lines indicate the noise variables.\nof the plot. Using more MCMC iterations M , fewer TVS iterations are needed to obtain a\ncleaner separation between signal and noise (noise \u03c0t\u2019s are closer to zero while signal \u03c0t\u2019s are closer to one). With enough internal MCMC iterations (M = 1 000 in the right panel),\nTVS is able to effectively separate signal from noise in around 200 iterations (obtained\nin less than 2 minutes). We have not seen such conclusive separation with plain BART\n(Figure 1) even after very many MCMC iterations which took considerably longer. Note\nthat each iteration of TVS uses only a small subset of predictors and much fewer iterations\nM and TVS is thereby destined to be faster than BART on the entire dataset (compare 8.5\nminutes for 20 000 iterations with 40 seconds in Figure 2a). Applying the more traditional\nvariable selection techniques was also not as successful. For example, the Spike-and-Slab LASSO (SSL) method (\u03bb1 = 0.1 and \u03bb0 \u2208 {0.1 + k \u00d7 10 : k = 1, . . . , 10}) which relies on a linear model missed the quadratic predictor but identified all 4 remaining signals with no\nfalse positives."}, {"heading": "5.2 Online TVS", "text": "As the second TVS example, we focus on the case with many more observations than\ncovariates, i.e. n >> p. As we already pointed out in Section 3.2, we assume that the\ndataset Dn = {(Yi,xi) : 1 \u2264 i \u2264 n} has been partitioned into mini-batches Dt of size s = n/T . We deploy our online TVS method (Table 1 with C2?) to sequentially screen\neach batch and transmit the posterior information onto the next mini-batch through a\nprior. This should be distinguished from streaming variable selection, where new features arrive at different time points (Foster and Stine (2008)). Using the notation rt \u2261 r(St,Dt) in (8) with Dt = {(Yi,xi) : (t\u2212 1)s+ 1 \u2264 i \u2264 ts} and having processed t\u2212 1 mini-batches, one can treat the beta posterior as a new prior for the incoming data points, where\n\u03c0(\u03b8 | r1, . . . , rt) \u221d \u03c0(\u03b8 | r1, . . . , rt\u22121) \u220f i\u2208St \u03b8 \u03b3ti i (1\u2212 \u03b8i)1\u2212\u03b3 t i .\nParsing the observations in batches will be particularly beneficial when processing the entire\ndataset (with overwhelmingly many rows) is not feasible for the learning algorithm. TVS\nleverages the fact that applying a machine learning method T times using only a subset of s observations and a subset St of variables is a lot faster than processing the entire data. While the posterior distribution5 of \u03b8i\u2019s after one pass through the entire dataset will have seen all the data Dn, \u03b8i\u2019s can be interpreted as the frequentist probability that the screening rule picks a variable xi having access to only s measurements.\nWe illustrate this sequential learning method on a challenging simulated example from (Liang et al., 2018, Section 5.1) . We assume that the explanatory variables xi \u2208 [0, 1]p have been obtained from xij = (ei + zij)/2 for 1 \u2264 i \u2264 n and 1 \u2264 j \u2264 p, where e, zij iid\u223c N (0, 1). This creates a sample correlation of about 0.5 between all variables. The responses Y = (Y1, \u00b7 \u00b7 \u00b7 , Yn)\u2032 are then obtained from (1), where\nf0(xi) = 10xi2\n1 + x2i1 + 5 sin(xi3xi4) + 2xi5\nwith \u03c32 = 0.5. This is a challenging scenario due to (a) the non-negligible correlation between signal and noise, and (b) the non-linear contributions of x1 \u2212 x4. Unlike Liang et al. (2008) who set n = p = 500, we make the problem considerably more difficult by\nchoosing n = 20 000 and p = 1 000. We would expect a linear model selection method\nto miss these two nonlinear signals. Indeed, the Spike-and-Slab LASSO method (using \u03bb1 = 0.1 and \u03bb0 \u2208 {0.1 + k \u00d7 10 : k = 1, . . . , 10} only identifies variables x1, x2 and x5. Next, we deploy the BART method with variable selection (Linero 2016) by setting\n5Treating the rewards as data.\nsparse=TRUE (Linero and Yang (2018)) and 50 trees6 in the BART software (Chipman\net al. (2010)). The choice of 50 trees for variable selection was recommended in Bleich\net al. (2014) and was seen to work very well. Due to the large size of the dataset, it\nmight be reasonable to first inquire about variable selection from smaller fractions of data. We consider random subsets of different sizes s \u2208 {100, 500, 1 000} as well as the entire\ndataset and we run BART for M = 20 000 iterations. Figure 3 depicts BART importance\nmeasures (average number of times each variable was used in the forest). We have seen BART separating the signal from noise rather well on batches of size s \u2265 1 000 and with MCMC iterations M \u2265 10 000. The scale of the importance measure depends on s and it\nis not necessarily obvious where to make the cut for selection. A natural (but perhaps ad\nhoc) criterion would be to pick variables which were on average used at least once. This\nwould produce false negatives for smaller s and many false positives (29 in this example)\nfor s = 20 000. The Hamming distance between the true and estimated model as well\nas the computing times are reported in Table 1. This illustrates how selection based on\nthe importance measure is difficult to automate. While visually inspecting the importance\nmeasure for M = 20 000 and s = 20 000 (the entire dataset) in Figure 3d is very instructive\nfor selection, it took more than 70 minutes on this dataset. To enhance the scalability, we\ndeploy our reinforcement learning TVS method for streaming batches of data.\nUsing the online local reward (10) with BART (with 10 trees and sparse=TRUE and M iterations) on batches of data Dn of size s. This is a weaker learning rule than the one considered in Figure 3 (50 trees). Choosing s = 100 and M = 10 000, BART may\n6Results with 10 were not nearly as satisfactory.\nnot be able to obtain perfect signal isolation on a single data batch (see Figure 3(a) which\nidentifies only one signal variable). However, by propagating information from one batch\nonto the next, TVS is able to tease out more signal (Figure 4(a)). Comparing Figure 3(b))\nand Figure 4(c) is even more interesting, where TVS inclusion probabilities for all signals\neventually cross the decision boundary after merely 40 TVS iterations. There is ultimately\na tradeoff between the batch size s and the number of iterations needed for the TVS to\nstabilize. For example, with s = 1 000 one obtains a far stronger learner (Figure 3(c)), but\nthe separation may not be as clear after only T = n/s = 20 TVS iterations (Figure 4(d)).\nOne can increase the number of TVS iterations by performing multiple passes through the\ndata after bootstrapping the entire dataset and chopping it into new batches which are\na proxy for future data streams. Plots of TVS inclusion probabilities after 5 such passes\nthrough the data are in Figure 5. Curiously, one obtains much better separation even for\ns = 200 and with larger batches (s = 500) the signal is perfectly separated. Note that TVS\nis a random algorithm and thereby the trajectories in Figure 5 at the beginning are slightly\ndifferent from Figure 4. Despite the random nature, however, we have seen the separation\napparent from Figure 5 occur consistently across multiple runs of the method.\nSeveral observations can be made from the timing and performance comparisons pre-\nsented in Table 2. When the batch size is not large enough, repeated runs will not help.\nThe Hamming distance in all cases only consists of false negatives and can be decreased by\nincreasing the batch size or increasing the number of iterations and rounds. Computation-\nally, it seems beneficial to increase the batch size s and supply enough MCMC iterations.\nVariable selection accuracy can also be increased with multiple rounds."}, {"heading": "6 Simulation Study", "text": "We further evaluate the performance of TVS in a more comprehensive simulation study.\nWe compare TVS with several related non-parametric variable selection methods and with\nclassical parametric ones. We assess these methods based on the following performance\ncriteria: False Discovery Proportion (FDP) (i.e. the proportion of discoveries that are\nfalse), Power (i.e. the proportion of true signals discovered as such), Hamming Distance\n(between the true and estimated model) and Time."}, {"heading": "6.1 Offline Cases", "text": "For a more comprehensive performance evaluation, we consider the following 4 mean functions f0(\u00b7) to generate outcomes using (1). For each setup, we summarize results over 50 datasets of a dimensionality p \u2208 {1 000, 10 000} and a sample size n = 300.\n\u2022 Linear Setup: The regressors xi are drawn independently from N(0,\u03a3), where\n\u03a3 = (\u03c3jk) p,p j,k=1,1 with \u03c3jj = 1 and \u03c3jk = 0.9 |j\u2212k| for j 6= k. Only the first 5 variables are related to the outcome (which is generated from (1) with \u03c32 = 5) via the mean function f0(xi) = xi1 + 2xi2 + 3xi3 \u2212 2xi4 \u2212 xi5.\n\u2022 Friedman Setup: The Friedman setup was described earlier in Section 4. In addi-\ntion, we now introduce correlations of roughly 0.3 between the explanatory variables.\n\u2022 Forest Setup: We generate xi from N(0,\u03a3), where \u03a3 = (\u03c3jk)p,pj,k=(11) with \u03c3jj = 1\nand \u03c3jk = 0.3 |j\u2212k| for j 6= k. We then draw the mean function f0(\u00b7) from a BART prior with 200 trees, using only first 5 covariates for splits. The outcome is generated from (1) with \u03c32 = 0.5.\n\u2022 Liang et al (2016) Setup: This setup was described earlier in Section 5.2. We now\nuse \u03c32 = 0.5.\nWe run TVS with M = 500 and M = 1 000 internal BART MCMC iterations and\nwith T = 500 TVS iterations. As two benchmarks for comparison, we consider the original\nBART method (in the R-package BART) and a newer variant called DART (Linero and\nYang (2018)) which is tailored to high-dimensional data and which can be obtained in\nBART by setting sparse=TRUE ( a=1, b=1). We ran BART and DART for M = 50 000\nMCMC iterations using the default prior settings with D = 20, D = 50 and D = 200\ntrees for BART and D = 10, D = 50 and D = 200 trees for DART. We considered two\nvariable selection criteria: (1) posterior inclusion probability (calculated as the proportion\nof sampled forests that split on a given variable) at least 0.5 (see Linero (2018) and Bleich\net al. (2014) for more discussion on variable selection using BART), (2) the average number\nof splits in the forest (where the average is taken over the M iterations) at least 1. We\nreport the settings with the best performance, i.e. BART with D = 20 trees and DART\nwith D = 50 trees using the second inclusion criterion. The third benchmark method we use\nfor comparisons is the Spike-and-Slab LASSO (Rockova and George (2018)) implemented\nin the R-package SSLASSO with lambda1=0.1 and the spike penalty ranging from \u03bb1 to the number of variables p (i.e. lambda0 = seq(1, p, length=p)). We choose the same set of\nvariable chosen by SSLASSO function after the regularization path has stabilized using the\nmodel output.\nWe report the average performance (over 50 datasets) for p = 10 000 in Figure 6 and\nthe rest (for p = 1 000) in the Appendix. Recall that the model estimated by TVS is\nobtained by truncating \u03c0i(500)\u2019s at 0.5. In terms of the Hamming distance, we notice that TVS performs best across-the-board. DART (with D = 50) performs consistently well in\nthe 4 choices of f0 assuming p = 10 000 and n = 300. The x-axis denotes the choice of f0 and the\nvarious methods are marked with various shades of gray. For TVS, we have two choices M = 500\nand M = 1 000.\nterms of variable selection, but the timing comparisons are less encouraging. BART (with\nD = 20) takes a relatively comparable amount of time as TVS with M = 1 000, but suffers\nfrom less power. SS-LASSO\u2019s performance is strong, in particular for the less non-linear\ndata setups. The performance of TVS is seen to improve with M .\nWe also implement a stopping criterion for TVS based on the stabilization of the in-\nclusion probabilities \u03c0i(t) = ai(t)\nai(t)+bi(t) . One possibility is to stop TVS when the estimated\nmodel S\u0302t obtained by truncating \u03c0i(t)\u2019s at 0.5 has not changed for over, say, 100 consecutive TVS iterations. With this convergence criterion, the convergence times differs across the different data set-ups. Generally, TVS is able to converge in \u223c 200 iterations for p = 1 000\nand \u223c 300 iterations for p = 10 000. While the computing times are faster, TVS may be\nmore conservative (lower FDP but also lower Power). The Hamming distance is hence a\nbit larger, but comparable to TVS with 500 iterations (Appendix B.1)"}, {"heading": "6.2 Online Cases", "text": "We now consider a simulation scenario where n >> p, i.e. p = 1 000 and n = 10 000. As described earlier in Section 5.2, we partition the data into minibatches (Y (b),X(b)) of size s, where Y (b) = (Yi : (b \u2212 1)s + 1 \u2264 i \u2264 bs) and X(b) = [xi : (b \u2212 1)s + 1 \u2264 i \u2264 bs]\u2032 for b = 1, . . . , n/s with s \u2208 {500, 1000} and M \u2208 {500, 1000} using D = 10 trees. In\nthis study, we consider the same four set-ups as in Section 6.1. We implemented TVS with a fixed number of rounds r \u2208 {1, 5, 10} and a version with a stopping criterion based on the stabilization of the inclusion probabilities \u03c0i(t) = ai(t)\nai(t)+bi(t) . This means that TVS\nwill terminate when the estimated model S\u0302t obtained by truncating \u03c0(t)\u2019s at 0.5 has not changed for 100 consecutive iterations. The results using the stopping criterion are reported\nin Figure 7 and the rest is in the Appendix (Section B.2 ). As before, we report the best\nconfiguration for BART and DART, namely D = 20 for BART and D = 50 for DART\n(both with 50 000 MCMC iterations). For both methods, there are non-negligible false\ndiscoveries and the timing comparisons are not encouraging. In addition, we could not apply both BART and DART with n \u2265 50 000 observations due to insufficient memory. For\nTVS, we found the batch size s = 1 000 to work better, as well as running the algorithm\nfor enough rounds until the inclusion probabilities have stabilized (Figure 7 reports the\nresults with a stopping criterion). The results are very encouraging. While SSLASSO\u2019s\nperformance is very strong, we notice that in the non-linear setup of Liang et al. (2018)\nthere are false non-discoveries."}, {"heading": "7 Application on Real Data", "text": ""}, {"heading": "7.1 HIV Data", "text": "We will apply (offline) TVS on a benchmark Human Immunodeficiency Virus Type I (HIV-\nI) data described and analyzed in Rhee et al. (2006) and Barber et al. (2015). This publicly\nthe 4 choices of f0 assuming p = 1 000 and n = 10 000. The x-axis denotes the choice of f0 and the\nvarious methods are marked with various shades of gray. For TVS, we have two choices M = 500\nand M = 1 000, both with s = 1000.\navailable7 dataset consists of genotype and resistance measurements (decrease in suscep-\ntibility on a log scale) to three drug classes: (1) protease inhibitors (PIs), (2) nucleoside\nreverse transcriptase inhibitors (NRTIs), and (3) non-nucleoside reverse transcriptase in-\nhibitors (NNRTIs).\nThe goal in this analysis is to find mutations in the HIV-1 protease or reverse tran-\nscriptase that are associated with drug resistance. Similarly as in Barber et al. (2015) we\nanalyze each drug separately. The response Yi is given by the log-fold increase of lab-tested\n7Stanford HIV Drug Resistance Database https://hivdb.stanford.edu/pages/published_\nanalysis/genophenoPNAS2006/\ndrug resistance in the ith sample with the design matrix X consisting of binary indicators xij \u2208 {0, 1} for whether or not the jth mutation has occurred at the ith sample.8\nIn an independent experimental study, (Rhee et al., 2005) identified mutations that\nare present at a significantly higher frequency in virus samples from patients who have\nbeen treated with each class of drugs as compared to patients who never received such\ntreatments. While, as with any other real data experiment, the ground truth is unknown,\nwe treat this independent study are a good approximation to the ground truth. Similarly\nas Barber et al. (2015), we only compare mutation positions since multiple mutations in\nthe same position are often associated with the same drug resistance outcomes.\nFor illustration, we now focus on one particular drug called Lopinavir (LPV). There\nare p = 206 mutations and n = 824 independent samples available for this drug. TVS\nwas applied to this data for T = 500 iterations with M = 1 000 inner BART iterations.\nIn Figure 8, we differentiate those mutations whose position were identified by Rhee et al.\n(2005) and mutations which were not identified with blue and red colors, respectively.\nFrom the plot of inclusion probabilities in Figure 8a, it is comforting to see that only one\nunidentified mutation has a posterior probability \u03c0j(t) stabilized above the 0.5 threshold. Generally, we observe the experimentally identified mutations (blue curves) to have higher\ninclusion probabilities.\nComparisons are made with DART, Knockoffs (Barber et al., 2015), LASSO (10-fold\ncross-validation), and Spike-and-Slab LASSO (Rockova and George (2018)), choosing \u03bb1 = 0.1 and \u03bb0 \u2208 {\u03bb1 + 10 \u00d7 k; k = 0, 1, . . . , p}). Knockoffs, LASSO, and the Spike-and-Slab LASSO assume a linear model with no interactions. DART was implemented using T = 50\ntrees and 50 000 MCMC iterations, where we select those variables whose average number\nsplits was at least one. The numbers of discovered Positions for each method are plotted\nin Figure 8b. While LASSO selects many more experimentally validated mutations, it also\nincludes many unvalidated ones. TVS, on the other hand, has a very small number of\n\u201cfalse discoveries\u201d while maintaining good power. Additional results are included in the\nAppendix (Section C).\n8As suggested in the analysis of Barber et al. (2015), when analyzing each drug, only mutations that\nappear 3 or more times in the samples are taken into consideration.\nprobabilities \u03c0j(t). (Right) Number of signals discovered, where blue denotes the experimentally\nvalidated signals and red are the unvalidated ones."}, {"heading": "7.2 Durable Goods Marketing Data Set", "text": "Our second application examines a cross-sectional dataset described in Ni et al. (2012)\nconsisting of durable goods sales data from a major anonymous U.S. consumer electronics\nretailer. The dataset features the results of a direct-mail promotion campaign in November\n2003 where roughly half of the n = 176 961 households received a promotional mailer\nwith 10$ off their purchase during the promotion time period (December 4-15). If they\ndid purchase, they would get 10% off on a subsequent purchase through December. The\ntreatment assignment was random. The data contains p = 146 descriptors of all customers\nincluding prior purchase history, purchase of warranties etc. We will investigate the effect\nof the promotional campaign (as well as other covariates) on December sales. In addition,\nwe will interact the promotion mail indicator with customer characteristics to identify the\n\u201cmail-deal-prone\u201d customers.\nWe dichotomized December purchase (in dollars) to create a binary outcome Yi = I(December-salesi > 0) for whether or not the ith customer made any purchase in December. Regarding predictor variables, we removed any variables with missing values and\nany binary variables with less than 10 samples in one group. This pre-filtering leaves us\nprobabilities \u03c0j(t) without knockoffs. (Right) Trajectories of the inclusion probabilities \u03c0j(t) with\nknockoffs (in red).\nwith 114 variables whose names and descriptive statistics are reported in Section D in the\nAppendix. We interact the promotion mail indicator with these variables to obtain p = 227 predictors. Due to the large volume of data (n \u2248 180 000), we were unable to run DART\nand BART (BART package implementation) due to memory problems. This highlights the\nneed for TVS as a variable selector which can handle such voluminous data.\nUnlike the HIV-I data in Section 7.1, there is no proxy for the ground truth. To\nunderstand the performance quality of TVS, we added 227 normally distributed knock-\noffs. The knockoffs are generated using create.second order function in the knockoff\nR package (Patterson and Sesia (2018)) using a Gaussian Distributions with the same\nmean and covariance structure (Candes et al. (2018)). We run TVS with a batch size s \u2208 {1 000, 2 000, 5 000} and M = 1 000 inner iterations until the posterior probabilities\nhave stabilized. The inclusion probabilities are plotted in Figure 9 for two cases (a) with-\nout knockoffs (the first row) and (b) with knockoffs (the second row). It is interesting\nto note that, apart from one setting with s = 1 000, the knockoff trajectories are safely\nsuppressed below 0.5 (dashed line). Both with and without knockoffs, TVS chooses \u2018the\nnumber of months with purchases in past 24 month\u2019 and \u2018the November Promotion Sales\u2019\nas important variables. The selected variables for each combination of settings are summa-\nrized in Table 3.\nFinally, we used the same set of variables (including knockoff variables) for different\nvariable selection methods and recorded the number of knockoffs chosen by each one. We\nused BART (D = 20, MCMC iteration = 50 000), and DART (D = 50, MCMC iteration\n= 50 000) with the same selection criteria as before, i.e. a variable is selected if it was split\non average at least once. We also consider LASSO where the sparsity penalty \u03bb was chosen\nby cross-validation. BART and DART cannot be run on the entire data set so we only\nrun it on a random subset of 10 000 data points. While TVS with large enough s does not\ninclude any of the knockoffs, LASSO does include 14 and DART includes 4."}, {"heading": "8 Discussion", "text": "Our work pursues an intriguing connection between spike-and-slab variable selection and bi-\nnary bandit problems. This pursuit has lead to a proposal of Thompson Variable Selection,\na reinforcement learning wrapper algorithm for fast variable selection in high dimensional\nnon-parametric problems. In related work, Liu et al. (2018) developed an ABC sampler for variable subsets through a split-sample approach by (a) first proposing a subset St from a prior, (b) keeping only those subsets that yield pseudo-data that are sufficiently close to the\nleft-out sample. TVS can be broadly regarded as a reinforcement learning elaboration of this strategy where, instead of sampling from a (non-informative) prior \u03c0(St), one \u201cupdates the prior \u03c0(St)\u201d by learning from previous successes.\nTVS can be regarded as a stochastic optimization approach to subset selection which\nbalances exploration and exploitation. TVS is suitable in settings when very many pre-\ndictors and/or very many observations can be too overwhelming for machine learning. By\nsequentially parsing subsets of data and reinforcing promising covariates, TVS can effec-\ntively separate signal from noise, providing a platform for interpretable machine learning.\nTVS minimizes regret by sequentially computing a median probability model rule obtained\nby truncating sampled mean rewards. We provide bounds for this regret without neces-\nsarily assuming that the mean arm rewards be unrelated. We observe strong empirical\nperformance of TVS under various scenarios, both on real and simulated data."}, {"heading": "A Proof of Theorem 1", "text": "We will denote with S? = arg maxS rC\u03b8?(S) the optimal model where\nrC\u03b8?(S) = \u2211 i\u2208S [ \u03b8?i (S) log ( C + 1 C ) \u2212 log ( 1 C )] .\nFirst, we define the reward gap of a set of arms St as\n\u2206St = E[rC\u03b8?(S?)\u2212 rC\u03b8 (St)].\nand write the expected regret in (12) as Reg(T ) = E \u2211T\nt=1 \u2206St . Before proceeding, we need to introduce some notation. We denote with Ni(t) = \u2211 k<t I(i \u2208 Sk) the number of times an arm i has been pulled up to time t. Next,\n\u00b5\u0302i(t) = ai(t)\u2212 1 Ni(t) = 1 Ni(t) \u2211 k<t I(i \u2208 Sk)\u03b3ti (17)\ndenotes the empirical mean of an arm i, i.e. the proportion of times an arm i has yielded a reward when pulled, i.e. \u03b3ti = 1 when i \u2208 St. We will be using the following usual Chernoff-Hoeffding bounds which we state without a proof.\nLemma 4 (Chernoff-Hoeffding Bound) Let X1, ..., Xn be independent Bernoulli random variables with E[Xi] = pi and denote with X = 1n \u2211n i=1 = Xi and \u00b5 = 1 n \u2211n i=1 pi. Then, for any 0 < \u03bb < 1\u2212 \u00b5, we have\nP(X \u2265 \u00b5+ \u03bb) \u2264 exp{\u2212nd(\u00b5+ \u03bb, \u00b5)},\nand, for any 0 < \u03bb < \u00b5,\nP(X \u2264 \u00b5\u2212 \u03bb) \u2264 exp{\u2212nd(\u00b5\u2212 \u03bb, \u00b5)},\nwhere d(a, b) = a ln a/b+ (1\u2212 a) ln(1\u2212 a)/(1\u2212 b).\nSimilarly as in other regret bound proofs (Komiyama et al., 2015; Wang and Chen,\n2018) we will bound the expected regret separately on the intersection of combinations of\nthe following events:\nA(t) = {St 6= S\u2217}, B(t) = { \u2203i \u2208 S\u2217 s.t. \u00b5\u0302i(t) < 0.5 + \u03b1\n2 or \u2203i \u2208 St\\S\u2217 s.t. \u00b5\u0302i(t) > 0.5\u2212\n\u03b1 2\n} ,\nC(t) = { \u2203i \u2208 S\u2217 s.t. \u00b5\u0302i(t)\u2212 \u03b8i(t) > \u03b1\n2 or \u2203i \u2208 St\\S\u2217 s.t. \u03b8i(t)\u2212 \u00b5\u0302i(t) >\n\u03b1 2\n} ,\nD(t) = \u22c2 i\u2208St { Ni(t) > 8 log T \u03b12 } ,\nwhere \u03b1 occurred in Assumption 1. First, we focus on the following term\nReg1(T ) = T\u2211 t=1 E[\u2206St \u00d7 I(A(t) \u2229 B(t))]. (18)\nThe following Lemma finds an upper bound on Reg1(T ): Lemma 5 Under the Assumption 1, the TVS sampling policy in Table 1 with C = ( \u221a 5\u2212\n1)/2 yields, for \u2206max = maxS \u2206S\nReg1(T ) \u2264\u2206max \u2211 i\u2208S\u2217 E ( T\u2211 t=1 I { i \u2208 St, \u00b5\u0302i(t) < 0.5 + \u03b1 2 }) (19)\n+ \u2206max \u2211 i/\u2208S\u2217 E ( T\u2211 t=1 I { i \u2208 St, \u00b5\u0302i(t) > 0.5\u2212 \u03b1 2 }) (20)\n\u2264\u2206max \u00d7 p ( 1 + 4\n\u03b12max\n) . (21)\nProof: We will first prove that\nE ( T\u2211 t=1 I { i \u2208 St, \u00b5\u0302i(t) < 0.5 + \u03b1 2 }) \u2264 1 + 4 \u03b12max . (22)\nThe second inequality for the term in (38) can be obtained analogously. With \u03b8i(St) as in (14) we denote with\n\u03b8\u0304i(T ) = 1\nNi(T ) T\u2211 t=1 I(i \u2208 St)\u03b8i(St)\nand with d(a, b) = a log[a/b] + (1 \u2212 a) log[(1 \u2212 a)/(1 \u2212 b)]. Note that for any b \u2208 (0, 1)\nboth functions d(x, b) and d(b, x) are monotone increasing on [b, 1] and, at the same time, monotone decreasing on [0, b]. Under the Assumption 1 and for i \u2208 S? we have \u03b8\u0304i(T ) > 0.5 + \u03b1 and thereby\nd(0.5 + \u03b1/2, \u03b8\u0304i(T )) > d(0.5 + \u03b1/2, 0.5 + \u03b1).\nSimilarly as in the proof of Lemma 3 in Wang and Chen (2018), we denote with \u03c41, \u03c42, ... the times such that i \u2208 St, define \u03c40 = 0 and write\nE ( T\u2211 t=1 I{i \u2208 St, \u00b5\u0302i(t) < 0.5 + \u03b1/2} ) \u2264 1 + T\u2211 w=0 P(\u00b5\u0302i(\u03c4w) < 0.5 + \u03b1/2, Ni(t) = w)\n\u2264 1 + T\u2211 w=0 exp ( \u2212w d ( 0.5 + \u03b1/2, \u03b8\u0304(T ) )) \u2264 1 +\nT\u2211 w=0 exp (\u2212w d(0.5 + \u03b1/2, 0.5 + \u03b1))\n\u2264 1 + \u221e\u2211 w=0 exp (\u2212w d(0.5 + \u03b1/2, 0.5 + \u03b1)) \u2264 1 + exp (\u2212d(0.5 + \u03b1/2, 0.5 + \u03b1)) 1\u2212 exp (\u2212d(0.5 + \u03b1/2, 0.5 + \u03b1)) \u2264 1 + 4 \u03b12 .\nThis concludes the proof of (45). The second term can be bounded analogously, which\nconcludes the proof of the Lemma.\nNext, we focus on the following term\nReg2(T ) = T\u2211 t=1 E[\u2206St \u00d7 I(A(t) \u2229 Bc(t) \u2229 C(t) \u2229 D(t))]. (23)\nTo bound this term, we will be using the following Lemma (Lemma 4 from Wang and Chen\n(2018)) which we, again, state without a proof.\nLemma 6 Denote with \u03b8i(t) the mean reward for an arm i sampled from B(ai(t), bi(t)) during the step C1 in Table 1. Using the TVS algorithm from Table 1, we have the following\ntwo inequalities for any base arm i:\nP [ \u03b8i(t)\u2212 \u00b5\u0302i(t) > \u221a 2 log T\nNi(t)\n] \u2264 1 T ,\nP [ \u00b5\u0302i(t)\u2212 \u03b8i(t) > \u221a 2 log T\nNi(t)\n] \u2264 1 T .\nProof: The proof relies on the observation that \u03b8i(t)\u2019s only depend on values ai(t) and bi(t). The proof is then the same as in Lemma 4 in Wang and Chen (2018).\nThe following lemma bounds the regret term (23).\nLemma 7 Using the TVS algorithm from Table 1, we have the following bound:\nReg2(T ) = T\u2211 t=1 E[\u2206St \u00d7 I(A(t) \u2229 Bc(t) \u2229 C(t) \u2229 D(t))] \u2264 \u2206max \u00d7 p.\nProof: On the event D(t), we have Ni(t) > 8 log T\u03b12 and thereby \u03b1 2 > \u221a 2 log T Ni(t) . The set Bc(t) \u2229 C(t) \u2229 D(t) is then subsumed within{ \u2203i \u2208 St\\S\u2217 : \u03b8i(t)\u2212 \u00b5\u0302i(t) > \u221a 2 log T\nNi(t) or \u2203i \u2208 St \u2229 S\u2217 : \u00b5\u0302i(t)\u2212 \u03b8i(t) >\n\u221a 2 log T\nNi(t)\n} .\nWe can then directly apply Lemma 6 to write\nT\u2211 t=1 \u2206StP (A(t) \u2229 Bc(t) \u2229 C(t) \u2229 D(t)) \u2264 \u2206max T\u2211 t=1 P (Bc(t) \u2229 C(t) \u2229 D(t))\n\u2264 \u2206max T\u2211 t=1 p/T.\nFinally, we focus on the following term\nReg3 = T\u2211 t=1 E (\u2206St \u00d7 I (A(t) \u2229 Bc(t) \u2229 C(t) \u2229 Dc(t))) . (24)\nLemma 8 Let i \u2208 S\u2217 and let f \u2217i (j, s) be the probability that after j pulls of an arm i, s of those pulls result in a reward. If s \u2264 b0.5jc , then\nf \u2217i (j, s) \u2264 ( j\ns\n) (0.5 + \u03b1)s(0.5\u2212 \u03b1)j\u2212s (25)\nProof: We denote with \u03c4 ij the j th time such that the arm i has been pulled (i.e. \u03b8i(t) > 0.5). We denote the probability of yielding a reward at time \u03c4j as pj = P ( \u03b3 \u03c4j i = 1|S\u03c4j ) and, for a given j and s write f \u2217j,s(p1, \u00b7 \u00b7 \u00b7 , pj) := f \u2217i (j, s). Since we are studying one particular arm, we have dropped the subscript i without any loss of generality. Consider now a vector of binary indicators B = (B1, B2, B3, \u00b7 \u00b7 \u00b7 , Bj)\u2032 \u2208 {0, 1}j where Bk = \u03b3\u03c4ki \u2208 {0, 1} for whether\nor not the kth pull of an arm i has yielded a reward. Denoting |B| = \u2211p\nj=1Bk, we can\nwrite\nf \u2217j,s(p1, \u00b7 \u00b7 \u00b7 , pj) = \u2211\nB:|B|=s j\u220f l=1 pBll (1\u2212 pl) 1\u2212Bl .\nWe want to show that p? = (p?1, . . . , p ? j) \u2032 = arg max f \u2217j,s(p1, \u00b7 \u00b7 \u00b7 , pj) when p?i = 0.5 + \u03b1 for all 1 \u2264 i \u2264 j. First, we notice is that this is a multi-linear polynomial in the sense that \u2202f\u2217j,s(p1,\u00b7\u00b7\u00b7 ,pj)\n\u2202pk is independent of pk. Keeping every other coordinate constant, the value\np?k that maximizes f \u2217 j,s(\u00b7) in the kth direction has to be either 0.5 + \u03b1 or 1. The vector (p?1, \u00b7 \u00b7 \u00b7 , p?j)\u2032 maximizing f \u2217j,s(p1, \u00b7 \u00b7 \u00b7 , pj) will thus have each coordinate p\u2217k either equal to 1 or 0.5+\u03b1. Let r \u2208 N\u222a{0} be the number of coordinates k for which p?k = 1 and j\u2212r be the number of coordinates k for which p?k = 0.5 + \u03b1 (notice that r \u2264 s). Since f \u2217j,s(p1, \u00b7 \u00b7 \u00b7 , pj) is a symmetric polynomial (i.e. the value of the function is not affected by a permutation of its argument) we assume, without loss of generality, that p?1 = p ? 2 = \u00b7 \u00b7 \u00b7 = p?r = 1 and p?r+1 = p ? r+2 = \u00b7 \u00b7 \u00b7 = p?j = 0.5 + \u03b1. In this case, we have the constraint on the binary indicators B where the first r indices have to be 1 and the remaining s \u2212 r 1\u2032s can be anywhere between the index r + 1 and j (j \u2212 r indices). Therefore, we have\nf \u2217j,s,r(p ?) = ( j \u2212 r s\u2212 r ) (0.5 + \u03b1)s\u2212r(0.5\u2212 \u03b1)j\u2212s.\nIt is sufficient to prove that this function is maximized at r = 0. We have\nf \u2217j,s,r+1(p ?)\nf \u2217j,s,r(p ?)\n=\n( j\u2212r\u22121 s\u2212r\u22121 ) (0.5 + \u03b1)s\u2212r\u22121(0.5\u2212 \u03b1)j\u2212s(\nj\u2212r s\u2212r\n) (0.5 + \u03b1)s\u2212r(0.5\u2212 \u03b1)j\u2212s\n= s\u2212 r\n(j \u2212 r)(1/2 + \u03b1)\n\u2264 j/2\u2212 r (j \u2212 r)(1/2 + \u03b1) (using the assumption s \u2264 bj/2c) = 1\u2212 (1/2\u2212 \u03b1)r + \u03b1j (j \u2212 r)(1/2 + \u03b1) < 1\nsince 1/2 \u2212 \u03b1 \u2265 0 and \u03b1 > 0. Since this is true for all r, the function f \u2217 j,s,r+1(p1,\u00b7\u00b7\u00b7 ,pj) f\u2217j,s,r(p1,\u00b7\u00b7\u00b7 ,pj) is maximized at r = 0. This concludes the proof.\nLemma 9 Let i \u2208 S\u2217 and let \u03c4 ij be the jth time such that \u03b8i(t) > 0.5. Suppose that Assumption 1 is true, then the TVS Algorithm 1 with C = ( \u221a 5\u2212 1)/2 satisfies\nE [\u03c4j+1 \u2212 \u03c4j] \u2264 4 + 1 \u03b1 when j \u2264 8 \u03b1\n1 + 1 e\u03b1 2j/4\u22121 + e\u2212\u03b1\n2j/2 ( C1 + C2 1\u22122\u03b1 4\u03b12(j+1) ) when j > 8 \u03b1 ,\n(26)\nwhere constants C1, C2 > 0 are not related to Algorithm 1.\nProof: We denote with \u03c4 ij the j th time such that the arm i has been pulled (i.e. \u03b8i(t) > 0.5). First, we consider the time interval [\u03c4 ij , \u03c4 i j+1). For any t \u2208 [\u03c4 ij , \u03c4 ij+1) we know that the arm\ni has been played j times and, thereby, \u03b8i(t) comes from a beta distribution\n\u03b8i(t) \u223c B [ai(t), bi(t)] ,\nwhere j = ai(t) + bi(t) \u2212 2. The parameters of the beta distribution are only updated if the arm i is pulled and the distribution thus does not change until we reach the iteration \u03c4j+1. Therefore, given \u00b5\u0302i(\u03c4 i j) the expected difference between \u03c4 i j+1 \u2212 \u03c4 ij has a geometric distribution with an expectation\nE [ \u03c4j+1 \u2212 \u03c4j | \u00b5\u0302i(\u03c4 ij) ] =\n1\nP(Bij > 0.5) =\n1\npi,j(0.5)\nwhere Bij \u223c B [ai(\u03c4j+1), bi(\u03c4j+1)]. We let Fn,p(\u00b7) and fn,p(\u00b7) denote the cumulative distribution function (CDF) and the probability density function of a Binomial distribution with\nparameters (n, p). We now recall the following fact (see e.g. Fact 3 in Agrawal and Goyal (2012)) about the CDF F beta\u03b1,\u03b2 (x) of a beta distribution with parameters (\u03b1, \u03b2). We have the following identity which links the CDF of a beta distribution and a CDF of a binomial\ndistribution:\nF beta\u03b1,\u03b2 (y) = 1\u2212 F\u03b1+\u03b2\u22121,y(\u03b1\u2212 1) (27)\nfor all positive integers \u03b1, \u03b2. Let f \u2217i (j, s) be the probability that after j pulls of an arm i, s out of those j pulls result in a reward. Here, we have the following relationship ai(\u03c4j) = s+1 and bi(\u03c4j) = j + 1\u2212 s. Using the identity (27) and given s successes among j pulls, we can write pi,j(0.5) = F beta s+1,j\u2212s+1(0.5) = 1\u2212 Fj+1,0.5(s) and thereby\nE [\n1\npi,j(0.5)\n] = j\u2211 s=0 f \u2217i (j, s) Fj+1,0.5(s) . (28)\nFirst, we consider the case when j \u2264 8 \u03b1 . In the following calculations, we will use the\nresult from Lemma 8. Let pmax = \u03b1 + 0.5 and R = pmax 1\u2212pmax . Using the fact Fj+1,0.5(s) \u2265 0.5Fj,0.5(s) and Fj,0.5(s) \u2265 1/2 when s \u2265 dj/2e (since the median of Binomial distribution\nwith parameters (j, 1/2) is either bj/2c or dj/2e) we have\nE [\n1\npi,j(0.5)\n] \u22642 j\u2211 s=0 f \u2217i (j, s) Fj,0.5(s) (29)\n\u22642 bj/2c\u2211 s=0 ( j s ) psmax(1\u2212 pmax)j\u2212s fj,0.5(s) + 4 j\u2211 s=dj/2e f \u2217i (j, s) (30) \u22642 bj/2c\u2211 s=0 psmax(1\u2212 pmax)j\u2212s 1/2j + 4 (31) \u22642(1\u2212 pmax) j\n1/2j\nbj/2c\u2211 s=0 Rs + 4 (32)\n\u22642 ( Rbj/2c+1 \u2212 1\nR\u2212 1\n) (1\u2212 pmax)j\n1/2j + 4 (33) \u22642 ( R\nR\u2212 1\n) Rj/2 (1\u2212 pmax)j\n1/2j + 4 (34)\n\u2264 1 \u03b1 e\u2212j d(1/2, pmax) + 4 (35) \u2264 1 \u03b1 + 4, (36)\nwhere from (34) to (35) we have used the following two facts. First, using the definition of d(\u00b7, \u00b7) in Lemma 4 and the fact that d(p1, p2) > (p1 \u2212 p2)2 we obtain\n(1\u2212 pmax)j 1/2j Rbj/2c \u2264 (1\u2212 pmax) j 1/2j Rj/2 = ej log(1\u2212pmax)\u2212j log(1/2)+j/2 log(pmax)\u2212j/2 log(1\u2212pmax)\n= e\u2212j{ 1 2 log( 12/pmax)+ 1 2 log[ 12/(1\u2212pmax)]} = e\u2212j d(1/2, pmax) \u2264 e\u2212\u03b12j.\nSecond, since pmax = 0.5 + \u03b1, we have R R\u22121 = pmax 2\u03b1 \u2264 1 2\u03b1 . When j > 8 \u03b1 , we will divide the sum \u03a3(0, j) \u2261 \u2211j s=0 f\u2217i (j,s) Fj+1,0.5(s) into 4 pieces and bound each one of them\n\u03a3 (0, bj/2c \u2212 1) \u2264 c2 [ e\u2212\u03b1\n2j 1\u2212 2\u03b1 4\u03b12(j + 1)\n] + c3 e \u22122\u03b12j (37)\n\u03a3 (bj/2c, bj/2c) \u2264 3 e\u2212\u03b12j (38) \u03a3 (dj/2e, b(1/2 + \u03b1/2)jc) \u2264 c3 ( e\u2212\u03b1 2j/2 ) (39)\n\u03a3 (d(1/2 + \u03b1/2)je , j) \u2264 1 + 1 e\u03b12j/4 \u2212 1 , (40)\nwhere c2 > 0 and c3 > 0 are constants unrelated to Algorithm 1. This will complete the proof. We now prove the bounds in the last display. We start with the first inequality in\n(37). When s \u2264 (j+ 1)/2\u2212 \u221a (j + 1)/4, we use the following bound for the Binomial CDF\n(Jer\u030ca\u0301bek (2004))\nFj+1,0.5(s) \u2265 1\nc2\n j + 1\u2212 s j + 1\u2212 2s  j + 1 s  1 2j+1  , for some c2 > 0. When s \u2265 (j + 1)/2\u2212 \u221a (j + 1)/4 we use that fact that, for some c3 > 0,\nFj+1,0.5(s) \u2265 1\nc3 > 0.\nAltogether, we arrive at the following bound (using again Lemma 8 and denoting pmax = 1/2 + \u03b1 and R = pmax 1\u2212pmax )\n\u03a3 (0, bj/2c \u2212 1) \u2264 c2 d(j+1)/2\u2212\n\u221a (j+1)/4e\u2211\ns=0\nf \u2217i (j, s)\nj+1\u2212s j+1\u22122s  j + 1 s  1 2j+1 + c3\nbj/2c\u22121\u2211 s=d(j+1)/2\u2212 \u221a (j+1)/4e+1 f \u2217i (j, s)\n\u2264 c2 bj/2c\u22121\u2211 s=0\nf \u2217i (j, s)\nj+1\u2212s j+1\u22122s  j + 1 s  1 2j+1 + c3\nbj/2c\u22121\u2211 s=0 f \u2217i (j, s)\n\u2264 c2 (1\u2212 pmax)j 1/2j+1 bj/2c\u22121\u2211 s=0 ( 1\u2212 2s j + 1 ) Rs + c3 bj/2c\u22121\u2211 s=0 f \u2217i (j, s). (41)\nNow we bound the first term in (41) to obtain\n(1\u2212 pmax)j\n1/2j+1\nbj/2c\u22121\u2211 s=0 ( 1\u2212 2s j + 1 ) Rs\n= (1\u2212 pmax)j\n1/2j+1\n{ Rbj/2c \u2212 1 R\u2212 1 \u2212 2 j + 1 [( bj/2c \u2212 1)Rbj/2c R\u2212 1 \u2212 R bj/2c \u2212R (R\u2212 1)2 ]}\n\u2264 (1\u2212 pmax) j\n1/2j+1\n{ Rbj/2c\nR\u2212 1 \u2212 2 j + 1\n[( bj/2c \u2212 1)Rbj/2c\nR\u2212 1 \u2212 R\nbj/2c\n(R\u2212 1)2\n]}\n\u2264 (1\u2212 pmax) j\n1/2j+1\n[ 2\nj + 1\nRbj/2c\n(R\u2212 1)2 + 2[(j + 1)/2\u2212 bj/2c+ 1] j + 1 Rbj/2c R\u2212 1 ] \u2264 (1\u2212 pmax) j\n1/2j+1 6 j + 1\nRbj/2c+1 (R\u2212 1)2\n\u2264 e\u2212\u03b12j 12 j + 1\nR\n(R\u2212 1)2 ,\nwhere we have used the following facts. First, for any x > 1 we have\nn\u2211 s=0 s xs = nxn+2 \u2212 (n+ 1)xn+1 + x (1\u2212 x)2 = nxn+1 x\u2212 1 \u2212 x n+1 \u2212 x (x\u2212 1)2 .\nSecond, j/2 + 1/2\u2212 bj/2c+ 1 < 3 and (similarly as before)\n(1\u2212 pmax)j\n1/2j Rbj/2c \u2264 e\u2212j d(1/2, pmax) \u2264 e\u2212\u03b12j.\nFinally, since R/(R\u2212 1) \u2264 1 2\u03b1 and 1/(R\u2212 1) = 1\u22122\u03b1 4\u03b1 , we have\n1\n(j + 1)/4\nR (R\u2212 1)2 \u2264 1\u2212 2\u03b1 4\u03b12(j + 1) .\nFor the second term in (41), we notice that \u2211bj/2c\u22121\ns=0 f \u2217 i (j, s) is equal to the probability that\nthe total number of successes is less than bj/2c \u2212 1. Here, we invoke Lemma 4 and note\nthat the success probability of each pull is always greater than 1/2 + \u03b1 and the difference\nbetween the average success probability over the j pulls and 1/2 is thereby greater than \u03b1. Hence, \u2211bj/2c\u22121\ns=0 f \u2217 i (j, s) \u2264 e\u22122\u03b1 2j. We put the two terms together to finally obtain\n\u03a3(0, bj/2c \u2212 1) \u2264 c2 [ e\u2212\u03b1\n2j 1\u2212 2\u03b1 4\u03b12(j + 1)\n] + c3 bj/2c\u22121\u2211 s=0 f \u2217i (j, s) (42)\n\u2264 c2 [ e\u2212\u03b1\n2j 1\u2212 2\u03b1 4\u03b12(j + 1)\n] + c3 e \u22122\u03b12j. (43)\nNext, to bound the term \u03a3(bj/2c, bj/2c) in (38), we use Lemma 8 and the fact that\npmax > 1/2 to find that for s = bj/2c\n\u03a3(bj/2c, bj/2c) = f \u2217 i (j, s)\nFj+1,0.5(s) \u2264 f\n\u2217 i (j, s)\nfj+1,0.5(s) \u2264 2\n( 1\u2212 s\nj + 1\n) Rs (\n1\u2212 pmax 1/2 )j \u2264 2 j + 1 ( j 2 + 2 ) Rj/2 ( 1\u2212 pmax 1/2\n)j \u2264 ( 1 + 3\nj + 1\n) e\u2212\u03b1 2j \u2264 2 e\u2212\u03b12j,\nwhere we used the assumption j \u2265 1/\u03b1 > 2.\nIn order to bound the third term \u03a3(dj/2e, b(\u03b1 + 1)j/2c) in (39), we first note that if\nj \u2265 8 \u03b1 > 1 \u03b1 \u2265 2 (our assumption above), we have\n\u221a (j + 1)/4 > \u221a 3/4 \u2265 \u221a 1/2 > 1/2 and\nthereby (j + 1)/2 \u2212 \u221a (j + 1)/4 < j/2 \u2264 dj/2e \u2264 s. This implies that the condition in\nJer\u030ca\u0301bek (2004) is satisfied and we can apply the bound Fj+1,0.5(s) \u2265 1c3 . Then we have\n\u03a3(dj/2e, b(\u03b1 + 1)j/2c) \u2264 c3 b(\u03b1+1)j/2c\u2211 s=dj/2e f \u2217i (j, s)  \u2264 c3 b(\u03b1+1)j/2c\u2211 s=0 f \u2217i (j, s)  \u2264 c3 ( e\u2212\u03b1 2j/2 ) ,\nwhere the last inequality stems from the Chernoff-Hoeffding inequality in Lemma 4 and\nAssumption 1 which guarantees that the success probability of each pull is greater than\n1/2 + \u03b1. This implies that the difference between the average probability of success over\nall the j pulls and 1/2 + \u03b1/2 is greater than \u03b1/2.\nFinally, to bound the term \u03a3(d(\u03b1 + 1)j/2e , j) in (40) we use the Hoeffding inequality in Lemma 4 with \u03bb = (\u03b1 + 1)j/[2(j + 1)] \u2212 1/2 \u2264 s/(j + 1) \u2212 1/2 to find (for a r.v. X \u223c Bin(j + 1, 1/2)) that\nFj+1,0.5(s) \u2265 1\u2212 P ( X\nj + 1 \u2212 1 2 > \u03bb\n) \u2265 1\u2212 e\u22122(j+1)\u03bb2 = 1\u2212 e(\u2212 j\u03b12 2 + j\u03b1 2 2(j+1) +\u03b1 j j+1 \u2212 1 2(j+1) )\n\u2265 1\u2212 e\u2212\u03b12j/2,\nwhere we used the fact that 1/j \u2264 \u03b1/8 and thereby 2\u03b1 \u2265 j\u03b12 2(j+1) + \u03b1 j j+1 \u2212 1 2(j+1) . Finally, we write\n\u03a3(d(\u03b1 + 1)j/2e , j) \u2264 j\u2211\ns=d(\u03b1+1)j/2e\nf \u2217i (j, s)\nFj+1,0.5(s) \u2264 1 1\u2212 e\u2212\u03b12j/4 = 1 +\n1\ne\u03b12j/4 \u2212 1 .\nNow, denoting C1 = 2 + 2 c3 and C2 = c2 we get the statement in the Lemma. This concludes our proof.\nUsing Lemma 9, we can achieve a similar bound in Lemma 6 of Wang and Chen (2018),\nLemma 10 Under Assumption 1, the TVS Algorithm 1 with C = ( \u221a 5\u2212 1)/2 satisfies the following property. For any signal arm i \u2208 S\u2217, the expected number of total pulls before the given arm i is pulled 8 log(T ) \u03b12\ntimes is bounded by\u2308 8 log(T )\n\u03b12\n\u2309 + \u2308 8\n\u03b1\n\u2309( 3 + 1\n\u03b1\n) + C\u0303\ne\u22124\u03b1\n1\u2212 e\u2212\u03b12/2 +\n8 \u03b12 1 e2\u03b1 \u2212 1 +\ne\u22121\n1\u2212 e\u2212\u03b1/8 ,\nwhere C\u0303 = C1 + C2 1\u22122\u03b1 32\u03b1 for some C1 > 0 and C2 > 0 not related to the Algorithm 1.\nProof: We use the notation from Lemma 9, where \u03c4 ij is the time when the arm i has been pulled for the jth time. Denoting with T\u0303 = b8 log(T ) \u03b12 c, we want to find an upper bound for E[\u03c4 iT\u0303 ]. We can write\nE [ \u03c4 iT\u0303 ] = T\u0303\u2211 j=0 E [ \u03c4 ij+1 \u2212 \u03c4 ij ] = T\u0303\u2211 j=0 E (\n1\npi,j(0.5) ) and using Lemma 9 we obtain\nE [ \u03c4 iT\u0303 ] \u2264 b 8 \u03b1 c\u2211\nj=0\n( 4 + 1\n\u03b1\n) + T\u0303\u2211 j=b 8\n\u03b1 c+1\n[ 1 +\n1\ne\u03b12j/4 \u2212 1 + e\u2212\u03b1 2j/2\n( C1 + C2\n1\u2212 2\u03b1 4\u03b12(j + 1)\n)] . (44)\nFirst, we note that \u2211b 8 \u03b1 c\nj=0\n( 4 + 1\n\u03b1\n) + \u2211T\u0303\nj=b 8 \u03b1 c+1 1 \u2264 d 8 log(T ) \u03b12 e+ d 8 \u03b1 e(3 + 1 \u03b1 ). Next, we write\nT\u0303\u2211 j=d 8\n\u03b1 e+1\ne\u2212 j\u03b12 2 \u2264 e \u22124\u03b1\n1\u2212 e\u2212\u03b12/2\nand\nT\u0303\u2211 j=d 8\n\u03b1 e+1\ne\u2212 j\u03b12 2 ( C1 + C2\n1\u2212 2\u03b1 4\u03b12(j + 1)\n) < T\u0303\u2211 j=d 8\n\u03b1 e+1\ne\u2212 j\u03b12 2 ( C1 + C2\n1\u2212 2\u03b1 32\u03b1\n) \u2264 C\u0303 e \u22124\u03b1\n1\u2212 e\u2212\u03b12/2 .\nFinally, we use the fact that 1 ex\u22121 \u2264 e \u2212x/2 for x \u2265 1 to obtain\nT\u0303\u2211 j=d 8\n\u03b1 e+1\n1\ne j\u03b12 4 \u2212 1 \u2264\nb 8 \u03b12 c\u2211\nj=d 8 \u03b1 e+1\n1\ne j\u03b12 4 \u2212 1 +\nb 8 log(T ) \u03b12 c\u2211 j=d 8\n\u03b12 e\n1\ne j\u03b12 4 \u2212 1\n\u2264 8 \u03b12\n1\ne2\u03b1 \u2212 1 +\nb 8 log(T ) \u03b12 c\u2211 j=d 8\n\u03b12 e\ne\u2212\u03b1 2j/8 \u2264 8 \u03b12 1 e2\u03b1 \u2212 1 +\ne\u22121\n1\u2212 e\u2212\u03b12/8\nUsing these lemmas we can prove the following lemma about Reg3(T ).\nLemma 11 We denote with Reg3(T ) the regret term in (24). Under Assumption 1, the TVS Algorithm 1 with C = ( \u221a 5\u2212 1)/2 yields\nReg3(T ) \u2264 \u2206max { 8p log(T )\n\u03b12 + C\u0303 q\u2217\ne\u22124\u03b1\n1\u2212 e\u2212\u03b12/2 + q\u2217\n[ 8\n\u03b12 1 e2\u03b1 \u2212 1 +\ne\u22121\n1\u2212 e\u2212\u03b1/8 +\n\u2308 8\n\u03b1\n\u2309( 3 + 1\n\u03b1\n)]} ,\nwhere C\u0303 = C1 + C2 1\u22122\u03b1 32\u03b1 for some C1 > 0 and C2 > 0 not related to the Algorithm 1.\nProof: We start with the following facts Reg3(T ) \u2264 \u2211T t=1 \u2206StE [I (A(t) \u2229 Dc(t))] and\nReg3(T ) \u2264 T\u2211 t=1 \u2211 i\u2208St \u2206StE [ I ( A(t) \u2229 { Ni(t) \u2264 8 log(T ) \u03b12 })] ,\nwhere we used the fact that on the event Dc(t), there exists at least one arm i \u2208 St such that Ni(t) \u2264 8 log(T )\u03b12 . We now decompose the sum above into signal arms and noise arms\nReg3(T ) \u2264 T\u2211 t=1 [ \u2211 i\u2208St\u2229S? \u2206StE [ I ( A(t) \u2229 { Ni(t) \u2264 8 log(T ) \u03b12 })]] (45)\n+ T\u2211 t=1  \u2211 i\u2208St\\S? \u2206StE [ I ( A(t) \u2229 { Ni(t) \u2264 8 log(T ) \u03b12 })] . (46) If i \u2208 St\\S?, then St contributes to the regret but this can only happen 8 log(T )\u03b12 times so the total regret contribution of pulling a subset St including an arm i before Ni(t) > 8 log(T )\u03b12 is bounded by maxS:i\u2208S 8\u2206S log(T ) \u03b12 . There are p \u2212 q\u2217 noise arms i /\u2208 S\u2217 and the term (46) can be thus bounded by (p\u2212 q\u2217)\u2206max 8 log(T )\u03b12 .\nIf i \u2208 S? \u2229 St, then the arm i contributes to the regret when St 6= S?. However, by Lemma 9 and Lemma 10 we can bound the expected number of pulls of an arm i before Ni(t) reaches 8 log(T ) \u03b12\n. This means that the contribution to the regret when i \u2208 S? \u2229 St is bounded by \u2206max (\u2308 8 log(T ) \u03b12 \u2309 + \u2308 8 \u03b1 \u2309 ( 3 + 1 \u03b1 ) + C\u0303 e \u22124\u03b1 1\u2212e\u2212\u03b12/2 + 8 \u03b12 1 e2\u03b1\u22121 + e\u22121 1\u2212e\u2212\u03b1/8 ) . Because there are q? signal arms, we can combine (45) and (46) to arrive at the bound in the\nstatement of this lemma.\nWe now put the various pieces together to finally prove Theorem 1.\nProof: We start by noticing that\nI [A(t)] = I[A(t) \u2229 B(t)] + I [A(t) \u2229 Bc(t)]\n= I[A(t) \u2229 B(t)] + I [A(t) \u2229 Bc(t) \u2229 C(t)] + I[A(t) \u2229 Bc(t) \u2229 Cc(t)].\nNow we note that I[A(t) \u2229 Bc(t) \u2229 Cc(t)] = 0 because\nBc(t)\u2229Cc(t) = {\u2200i \u2208 S\u2217 we have \u03b8i(t) > 0.5 and \u2200i \u2208 St\\S\u2217 we have \u03b8i(t) < 0.5} = Ac(t).\nThereby we can write\nI [A(t)] =I[A(t) \u2229 B(t)] + I [A(t) \u2229 Bc(t) \u2229 C(t) \u2229 Dc(t)] + I [A(t) \u2229 Bc(t) \u2229 C(t) \u2229 D(t)]\nwhich leads to the following decomposition\nReg(T ) = T\u2211 t=1 \u2206StE [I (A(t))]\n\u2264 T\u2211 t=1 E [ \u2206St \u00d7 I[A(t) \u2229 B(t)] + \u2206St \u00d7 I [A(t) \u2229 Bc(t) \u2229 C(t) \u2229 D(t)]\n+ \u2206St \u00d7 I [A(t) \u2229 Bc(t) \u2229 C(t) \u2229 Dc(t)] ] = Reg1(T ) +Reg2(T ) +Reg3(T ).\nNow, we bound Reg1(T ), Reg2(T ) and Reg3(T ) with Lemma 5,Lemma 7, and Lemma 11. This gives us Theorem 1."}, {"heading": "B Additional Simulation Results", "text": "B.1 Offline Cases\nThe Figure 10 below shows simulation results for p = 1 000 under the same settings as\nDART and BART. We also considered a different variable selection rule, i.e. the Median\nProbability Model using the inclusion probability of BART and DART (as mentioned in\nLinero (2018)). Due to space constraints, we showed only the best settings for BART\nand DART in Section 6.1. The following tables present the entire simulation study and\nshow that TVS yields better Hamming distance and computational speed gains. Tables\n5-8 present the offline simulation study, one table for each data setup.\nB.2 Online Cases\nIn Table 9, we report convergence diagnostics of the simulation from Section 6.2, where the\nconvergence criterion is chosen as \u201cS\u0302t stays the same for 100 consecutive TVS iterations\u201d.\nIn addition to the results shown in Section 6.2, we tried a different number of trees D\nfor DART and BART. We also considered a different variable selection rule, i.e. the Median\nProbability Model using the inclusion probability of BART and DART (as mentioned in\nLinero (2018)). Due to space constraints, we showed only the best settings for BART and\nDART in Section 6.2. Now we present additional simulation results for n = 50 000 and\nn = 100 000. Since BART and DART cannot be run with such large n, we only show the\nresults for TVS."}, {"heading": "C Additional Results for the HIV Dataset", "text": "In this section, we show additional results on the analysis of the HIV dataset. First,\nwe present some basic statistics about the data. The entire data comes from Stanford\nHIV Drug Resistance Database. The raw data can be downloaded from https://hivdb.\nstanford.edu/pages/published_analysis/genophenoPNAS2006/. Barber et al. (2015)\npublished cleaning codes (available at https://web.stanford.edu/group/candes/knockoffs/\nsoftware/knockoffs/tutorial-4-r.html) which we adopt. We provide a basic overview\nof the dataset in Table 18.\nIn Section 7.1, we illustrated TVS on only the drug LPV. Here, we present the rest\nof the results. As is done in Barber et al. (2015), we record both the number of verified\npositions discovered (True Discoveries) and the number of discovered unverified positions\n(False Discoveries) for each of the five methods."}, {"heading": "D Details ont the Marketing Data", "text": "Summary of Predictor Variables\nName Indicator Mean Sd Max Min Skewness mail indicator 1 0.50 NA NA NA NA largest sale amount 0 169.02 286.77 7999.99 -3999.99 4.63 count of product categories that make up 20% or more of total sales 0 2.15 0.37 4.00 2.00 2.10 number of months since first esp or first esp return 0 30.14 16.05 60.00 2.00 0.17 number of months since most recent esp purchase or return 0 23.00 14.22 60.00 2.00 0.60 day of first purchase: weekend day 1 0.37 NA NA NA NA day of first purchase: weekday day 1 0.63 NA NA NA NA total number of sales in previous 12 months 0 5.09 4.76 189.00 2.00 5.79 total number of sales in previous 24 months 0 5.95 6.16 270.00 2.00 6.23 total number of sales in previous 36 months 0 6.53 7.10 344.00 2.00 6.44 total number of large ticket items in previous 12 months 0 2.37 0.94 21.00 2.00 6.95 total number of large ticket items in previous 24 months 0 2.50 1.08 32.00 2.00 5.79 total number of large ticket items in previous 36 months 0 2.58 1.24 46.00 2.00 6.94 total number of large ticket items in previous 60 months 0 2.80 1.47 50.00 -1.00 5.37 total number of medium ticket items in previous 12 months 0 3.06 1.92 46.00 2.00 4.62 total number of medium ticket items in previous 24 months 0 3.43 2.49 81.00 2.00 5.10 total number of medium ticket items in previous 36 months 0 3.71 2.90 124.00 2.00 5.48 total number of medium ticket items in previous 60 months 0 4.36 3.68 198.00 2.00 6.39 total number of small ticket items in previous 12 months 0 4.49 7.83 507.00 2.00 34.98 total number of small ticket items in previous 24 months 0 5.14 10.18 667.00 2.00 29.59 total number of small ticket items in previous 36 months 0 5.54 10.73 667.00 2.00 25.12 total number of small ticket items in previous 60 months 0 6.55 12.73 673.00 2.00 20.36 total sales amount in previous 12 months 0 524.90 766.12 18380.40 -437.80 4.31 total sales amount in previous 24 months 0 650.24 935.70 33664.26 -250.01 4.36 count of unique categories in previous 12 months 0 2.94 1.28 12.00 2.00 1.78 count of unique categories in previous 24 months 0 3.15 1.47 12.00 2.00 1.62 count of unique class numbers in previous 12 months 0 4.13 2.84 55.00 2.00 3.00 count of unique class numbers in previous 24 months 0 4.74 3.58 60.00 2.00 3.00 percent gift cards category sales of total sales 0 0.09 0.13 1.03 0.00 3.22 percent home ins category sales of total sales 0 0.02 0.05 0.42 0.00 4.48 percent imaging category sales of total sales 0 0.29 0.28 1.00 -0.20 1.08 percent mobile category sales of total sales 0 0.35 0.26 1.00 -0.03 0.60 percent music category sales of total sales 0 0.18 0.21 1.00 0.00 1.72 percent other category sales of total sales 0 0.36 0.33 0.99 0.01 0.63 percent pc hardware category sales of total sales 0 0.50 0.30 1.49 -2.88 -0.02 percent pst category sales of total sales 0 0.17 0.19 1.11 -0.49 1.89 percent tv category sales of total sales 0 0.40 0.29 1.00 -0.03 0.38 percent vcr category sales of total sales 0 0.29 0.25 3.13 -0.16 1.04 percent wireless category sales of total sales 0 0.24 0.24 1.45 -0.06 1.26 percent audio category sales of total sales 0 0.24 0.24 1.12 -0.08 1.17 percent dss category sales of total sales 0 0.32 0.28 1.00 0.00 0.94 largest return amount 0 176.57 273.43 4999.99 -2699.99 4.14 number of months since oldest return 0 26.20 15.33 56.00 2.00 0.28 number of months since most recent return 0 20.61 13.93 56.00 2.00 0.67 number of distinct merchandise classes returned 0 2.35 0.78 11.00 2.00 3.61 total return amount in previous 12 months 0 301.28 530.07 24926.85 0.01 10.03 total return amount in previous 24 months 0 326.14 584.04 31826.61 0.01 12.74 total number of items returned in previous 12 months 0 3.16 2.87 100.00 2.00 17.05 total number of items returned in previous 24 months 0 3.40 5.50 527.00 2.00 63.78 number of months shopped once in previous 12 months 0 2.59 1.07 12.00 2.00 2.89 number of months shopped once in previous 24 months 0 3.01 1.67 24.00 2.00 3.19 count of unique purchase trips in previous 12 months 0 2.95 1.97 81.00 2.00 8.23 count of unique purchase trips in previous 24 months 0 3.41 2.74 126.00 2.00 8.76 total number of items purchased in previous 12 months 0 5.23 7.24 513.00 2.00 28.99 total number of items purchased in previous 24 months 0 6.17 9.55 672.00 2.00 24.33 total number of weekday items in previous 12 months 0 4.65 6.68 506.00 2.00 35.47\nName Indicator Mean Sd Max Min Skewness total number of weekend items in previous 12 months 0 4.15 5.18 403.00 2.00 40.58 total number of weekend items in previous 24 months 0 4.62 6.76 504.00 2.00 34.99 total christmas sales amount in previous 12 months 0 379.85 560.04 9877.93 -660.00 4.33 total christmas sales amount in previous 24 months 0 430.10 611.41 10500.09 -660.00 4.05 total christmas items in previous 12 months 0 4.13 6.63 506.00 2.00 44.49 total christmas items in previous 24 months 0 4.46 7.03 506.00 2.00 41.25 total amount of back to school sales in previous 12 months 0 396.37 619.21 10159.37 -150.00 4.23 total amount of back to school sales in previous 24 months 0 416.13 623.76 10159.37 -485.02 3.93 total amount of graduation sales in previous 12 months 0 376.62 564.66 13670.70 -200.00 5.34 total amount of graduation sales in previous 24 months 0 405.84 585.14 13678.65 -252.00 4.33 total spring sales amount in previous 12 months 0 381.35 567.06 13190.30 -308.90 4.63 total spring sales amount in previous 24 months 0 421.00 612.91 13190.30 -340.00 4.31 total summer sales amount in previous 12 months 0 407.91 623.00 13205.62 -300.00 4.51 total summer sales amount in previous 24 months 0 439.29 643.85 17671.37 -300.00 4.23 total autumn sales amount in previous 12 months 0 401.56 616.92 10650.45 -437.80 4.12 total autumn sales amount in previous 24 months 0 447.43 663.93 11819.31 -372.00 4.05 total winter sales amount in previous 24 months 0 453.60 645.40 13921.45 -189.99 4.02 total spring items in previous 12 months 0 4.02 4.72 325.00 2.00 30.25 total spring items in previous 24 months 0 4.35 5.95 400.00 2.00 28.17 total summer items in previous 12 months 0 4.44 4.79 362.00 2.00 32.61 total summer items in previous 24 months 0 4.69 7.63 504.00 2.00 38.59 total autumn items in previous 12 months 0 4.20 7.47 501.00 2.00 41.44 total autumn items in previous 24 months 0 4.46 7.51 501.00 2.00 42.06 total winter items in previous 12 months 0 4.12 5.76 506.00 2.00 48.15 total winter items in previous 24 months 0 4.54 6.97 506.00 2.00 38.14 total count of back to school items in previous 12 months 0 4.29 5.38 362.00 2.00 38.67 total count of back to school items in previous 24 months 0 4.41 7.12 502.00 2.00 36.16 total count of graduation items in previous 12 months 0 4.15 3.83 190.00 2.00 14.61 total count of graduation items in previous 24 months 0 4.41 6.34 504.00 2.00 37.55 total number of net instore esps in previous 12 months 0 2.60 1.17 17.00 2.00 3.74 total number of net instore esps in previous 24 months 0 2.81 1.45 35.00 2.00 4.48 total number of net instore esps lifetime 0 3.14 1.98 43.00 2.00 4.09 avg term of all esps in previous 12 months 0 25.32 14.48 120.00 0.48 0.89 total number of returned instore esps in previous 12 months 0 2.55 1.19 14.00 2.00 4.00 total number of returned instore esps in previous 24 months 0 2.61 1.38 23.00 2.00 5.36 total number of returned instore esps lifetime 0 2.71 1.52 25.00 2.00 5.19 total items purchased during back to school gift guide 2002 promotion 0 4.26 9.39 170.00 -1.00 15.76 total items purchased during bond 2002 promotion 0 3.97 9.15 307.00 -2.00 29.86 total items purchased during expo 2001 promotion 0 3.68 3.95 76.00 -2.00 11.07 total items purchased during holiday gift guide 2001 promotion 0 2.94 1.97 20.00 -2.00 3.43 total items purchased during holiday gift guide 2002 promotion 0 3.31 2.42 26.00 -1.00 3.36 total items purchased during holiday mailer 2001 promotion 0 3.59 2.22 18.00 -3.00 2.09 total items purchased during holiday mailer 2002 promotion 0 3.81 2.84 44.00 -3.00 4.20 promo nov period: total sales 0 333.94 674.07 11633.21 -111.09 5.97 total $ spent during bond 2002 promotion 0 338.26 526.43 6821.51 -134.00 4.29 total $ spent during expo 2001 promotion 0 366.73 570.37 6491.80 -372.00 3.94 total $ spent during holiday mailer 2002 promotion 0 344.76 493.71 5639.97 -165.00 3.85 total $ spent during holiday mailer promotions 0 366.64 498.45 5818.81 -165.00 3.52 mailed in holiday 2001 mailer 1 0.18 NA NA NA NA mailed in holiday 2002 mailer 1 0.20 NA NA NA NA indicator of holiday gift guide 2002 promotion response 1 0.01 NA NA NA NA indicator of back to school gift guide 2002 promotion response 1 0.01 NA NA NA NA indicator of bond 2002 promotion response 1 0.01 NA NA NA NA indicator of expo 2001 promotion response 1 0.01 NA NA NA NA indicator of holiday gift guide 2001 promotion response 1 0.00 NA NA NA NA indicator of holiday mailer 2001 promotion response 1 0.01 NA NA NA NA indicator of holiday mailer 2002 promotion response 1 0.01 NA NA NA NA"}], "title": "Variable Selection via Thompson Sampling", "year": 2020}