{"abstractText": "Within the last decade, neural network based predictors have demonstrated impressive \u2014 and at times superhuman \u2014 capabilities. This performance is often paid for with an intransparent prediction process and thus has sparked numerous contributions in the novel field of explainable artificial intelligence (XAI). In this paper, we focus on a popular and widely used method of XAI, the Layer-wise Relevance Propagation (LRP). Since its initial proposition LRP has evolved as a method, and a best practice for applying the method has tacitly emerged, based however on humanly observed evidence alone. In this paper we investigate \u2014 and for the first time quantify \u2014 the effect of this current best practice on feedforward neural networks in a visual object detection setting. The results verify that the layerdependent approach to LRP applied in recent literature better represents the model\u2019s reasoning, and at the same time increases the object localization and class discriminativity of LRP.", "authors": [{"affiliations": [], "name": "Maximilian Kohlbrenner"}, {"affiliations": [], "name": "Alexander Bauer"}, {"affiliations": [], "name": "Shinichi Nakajima"}, {"affiliations": [], "name": "Alexander Binder"}, {"affiliations": [], "name": "Wojciech Samek"}], "id": "SP:87440b509ab4788aa9aed26477c3451eaaa6bcf3", "references": [{"authors": ["Parliament", "Council of the European Union"], "title": "General data protection regulation", "venue": "2016.", "year": 2016}, {"authors": ["B. Goodman", "S.R. Flaxman"], "title": "European Union regulations on algorithmic decision-making and a \u201cright to explanation", "venue": "AI Magazine, vol. 38, no. 3, pp. 50\u201357, 2017.", "year": 2017}, {"authors": ["J.T. Springenberg", "A. Dosovitskiy", "T. Brox", "M.A. Riedmiller"], "title": "Striving for simplicity: The all convolutional net", "venue": "Proc. of International Conference on Learning Representations (ICLR), 2015.", "year": 2015}, {"authors": ["P.-J. Kindermans", "K.T. Sch\u00fctt", "M. Alber", "K.-R. M\u00fcller", "D. Erhan", "B. Kim", "S. D\u00e4hne"], "title": "Learning how to explain neural networks: Patternnet and patternattribution", "venue": "Proc. of International Conference on Learning Representations (ICLR), 2018.", "year": 2018}, {"authors": ["M. Sundararajan", "A. Taly", "Q. Yan"], "title": "Axiomatic attribution for deep networks", "venue": "Proc. of International Conference on Machine Learning (ICML), 2017, pp. 3319\u20133328.", "year": 2017}, {"authors": ["D. Smilkov", "N. Thorat", "B. Kim", "F.B. Vi\u00e9gas", "M. Wattenberg"], "title": "Smoothgrad: removing noise by adding noise", "venue": "CoRR, vol. abs/1706.03825, 2017.", "year": 2017}, {"authors": ["S. Bach", "A. Binder", "G. Montavon", "F. Klauschen", "K.-R. M\u00fcller", "W. Samek"], "title": "On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation", "venue": "PloS one, vol. 10, no. 7, pp. e0130140, 2015.", "year": 2015}, {"authors": ["Y. Yang", "V. Tresp", "M. Wunderle", "P.A. Fasching"], "title": "Explaining therapy predictions with layer-wise relevance propagation in neural networks", "venue": "Proc. of IEEE International Conference on Healthcare Informatics (ICHI), 2018, pp. 152\u2013162.", "year": 2018}, {"authors": ["A W Thomas", "H R Heekeren", "K-R M\u00fcller", "W Samek"], "title": "Analyzing neuroimaging data through recurrent deep learning models", "venue": "Frontiers in Neuroscience, vol. 13, pp. 1321, 2019.", "year": 2019}, {"authors": ["S. Lapuschkin", "S. W\u00e4ldchen", "A. Binder", "G. Montavon", "W. Samek", "K.-R. M\u00fcller"], "title": "Unmasking clever hans predictors and assessing what machines really learn", "venue": "Nature Communications, vol. 10, no. 1, pp. 1096, 2019.", "year": 2019}, {"authors": ["S. Lapuschkin", "A. Binder", "G. Montavon", "K.-R. M\u00fcller", "W. Samek"], "title": "Analyzing classifiers: Fisher vectors and deep neural networks", "venue": "Proc. of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 2912\u20132920.", "year": 2016}, {"authors": ["M. Ancona", "E. Ceolini", "C. \u00d6ztireli", "M. Gross"], "title": "Gradient-based attribution methods", "venue": "Explainable AI: Interpreting, Explaining and Visualizing Deep Learning, pp. 169\u2013191. Springer, 2019.", "year": 2019}, {"authors": ["G. Montavon", "A. Binder", "S. Lapuschkin", "W. Samek", "K.-R. M\u00fcller"], "title": "Layer-wise relevance propagation: an overview", "venue": "Explainable AI: Interpreting, Explaining and Visualizing Deep Learning, pp. 193\u2013209. Springer, 2019.", "year": 2019}, {"authors": ["S. Lapuschkin", "A. Binder", "K.-R. M\u00fcller", "W. Samek"], "title": "Understanding and comparing deep neural networks for age and gender classification", "venue": "Proc. of IEEE International Conference on Computer Vision Workshops (ICCVW), 2017, pp. 1629\u20131638.", "year": 2017}, {"authors": ["M. H\u00e4gele", "P. Seegerer", "S. Lapuschkin", "M. Bockmayr", "W. Samek", "F. Klauschen", "K.-R. M\u00fcller", "A. Binder"], "title": "Resolving challenges in deep learning-based analyses of histopathological images using explanation methods", "venue": "CoRR, vol. abs/1908.06943, 2019.", "year": 1908}, {"authors": ["L.Y.W. Hui", "A. Binder"], "title": "Batchnorm decomposition for deep neural network interpretation", "venue": "International Work-Conference on Artificial Neural Networks (IWANN), 2019, pp. 280\u2013291.", "year": 2019}, {"authors": ["D. Balduzzi", "M. Frean", "L. Leary", "J.P. Lewis", "K.W.-D. Ma", "B. McWilliams"], "title": "The shattered gradients problem: If resnets are the answer, then what is the question", "venue": "Proc. of International Conference on Machine Learning (ICML), 2017, pp. 342\u2013350.", "year": 2017}, {"authors": ["W. Samek", "A. Binder", "G. Montavon", "S. Lapuschkin", "K.-R. M\u00fcller"], "title": "Evaluating the visualization of what a deep neural network has learned", "venue": "IEEE Transactions on Neural Network Learning Systems, vol. 28, no. 11, pp. 2660\u20132673, 2017.", "year": 2017}, {"authors": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "Ma S.", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg", "L. Fei-Fei"], "title": "ImageNet Large Scale Visual Recognition Challenge", "venue": "International Journal of Computer Vision (IJCV), vol. 115, no. 3, pp. 211\u2013252, 2015.", "year": 2015}, {"authors": ["M. Everingham", "L. Van Gool", "C.K.I. Williams", "J. Winn", "A. Zisserman"], "title": "The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results", "venue": "\u201dhttp://www.pascalnetwork.org/challenges/VOC/voc2012/workshop/index.html\u201d.", "year": 2012}, {"authors": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "title": "Gradient-based learning applied to document recognition", "venue": "Proc. of the IEEE, vol. 86, no. 11, pp. 2278\u20132324, 1998.", "year": 1998}, {"authors": ["C. Szegedy", "V. Vanhoucke", "S. Ioffe", "J. Shlens", "Z. Wojna"], "title": "Rethinking the inception architecture for computer vision", "venue": "Proc. of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016, pp. 2818\u20132826.", "year": 2016}, {"authors": ["K. Simonyan", "A. Zisserman"], "title": "Very deep convolutional networks for large-scale image recognition", "venue": "International Conference on Learning Representations (ICLR), 2015.", "year": 2015}, {"authors": ["G. Montavon", "S. Lapuschkin", "A. Binder", "W. Samek", "K.-R. M\u00fcller"], "title": "Explaining nonlinear classification decisions with deep taylor decomposition", "venue": "Pattern Recognition, vol. 65, pp. 211\u2013222, 2017.", "year": 2017}, {"authors": ["J. Zhang", "S.A. Bargal", "Z. Lin", "J. Brandt", "X. Shen", "S. Sclaroff"], "title": "Topdown neural attention by excitation backprop", "venue": "International Journal of Computer Vision (IJCV), vol. 126, no. 10, pp. 1084\u20131102, 2018.", "year": 2018}, {"authors": ["S. Bach", "A. Binder", "K.-R. M\u00fcller", "W. Samek"], "title": "Controlling explanatory heatmap resolution and semantics via decomposition depth", "venue": "Proc. of IEEE International Conference on Image Processing (ICIP). IEEE, 2016, pp. 2271\u20132275.", "year": 2016}, {"authors": ["S. Lapuschkin", "A. Binder", "G. Montavon", "K.-R. M\u00fcller", "S. Samek"], "title": "The LRP toolbox for artificial neural networks", "venue": "Journal of Machine Learning Research (JMLR), vol. 17, pp. 114:1\u2013114:5, 2016.", "year": 2016}, {"authors": ["F. Horst", "S. Lapuschkin", "W. Samek", "K.-R. M\u00fcller", "W.I. Sch\u00f6llhorn"], "title": "Explaining the unique nature of individual gait patterns with deep learning", "venue": "Scientific Reports, vol. 9, pp. 2391, 2019.", "year": 2019}, {"authors": ["I. Sturm", "S. Lapuschkin", "W. Samek", "K.-R. M\u00fcller"], "title": "Interpretable deep neural networks for single-trial eeg classification", "venue": "Journal of Neuroscience Methods, vol. 274, pp. 141\u2013145, 2016.", "year": 2016}, {"authors": ["J. Gu", "Y. Yang", "V. Tresp"], "title": "Understanding individual decisions of cnns via contrastive backpropagation", "venue": "Proc. of Asian Conference on Computer Vision (ACCV), 2018, pp. 119\u2013134.", "year": 2018}, {"authors": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "title": "Imagenet classification with deep convolutional neural networks", "venue": "Proc. of Advances in Neural Information Processing Systems (NIPS), 2012, pp. 1106\u20131114.", "year": 2012}, {"authors": ["C.J. Anders", "T. Marin\u010d", "Neumann D.", "W. Samek", "K.-R. M\u00fcller", "S. Lapuschkin"], "title": "Analyzing imagenet with spectral relevance analysis: Towards imagenet un-hans\u2019ed", "venue": "CoRR, vol. abs/1912.11425, 2019.", "year": 1912}, {"authors": ["M. Alber", "S. Lapuschkin", "P. Seegerer", "M. H\u00e4gele", "K.T. Sch\u00fctt", "G. Montavon", "W. Samek", "K.-R. M\u00fcller", "S. D\u00e4hne", "P.-J. Kindermans"], "title": "innvestigate neural networks", "venue": "Journal of Machine Learning Research (JMLR), vol. 20, pp. 93:1\u201393:8, 2019.", "year": 2019}, {"authors": ["F. Chollet"], "title": "Keras", "venue": "https://keras.io, 2015.", "year": 2015}, {"authors": ["M. Abadi"], "title": "Tensorflow: Large-scale machine learning on heterogeneous distributed systems", "venue": "CoRR, vol. abs/1603.04467, 2016.", "year": 2016}, {"authors": ["Y. Jia", "E. Shelhamer", "J. Donahue", "S. Karayev", "J. Long", "R. Girshick", "S. Guadarrama", "T. Darrell"], "title": "Caffe: Convolutional architecture for fast feature embedding", "venue": "Proc. of 22nd ACM international conference on Multimedia. ACM, 2014, pp. 675\u2013678.", "year": 2014}], "sections": [{"text": "ar X\niv :1\n91 0.\n09 84\n0v 3\n[ cs\n.L G\n] 1\n3 Ju\nl 2 02\nIndex Terms\u2014layer-wise relevance propagation, explainable artificial intelligence, neural networks, visual object recognition, quantitative evaluation\nI. INTRODUCTION\nIn recent years, deep neural networks (DNN) have become the state of the art method in many different fields, but are mainly applied as black-box predictors. Since understanding the decisions of artificial intelligence systems is crucial in numerous scenarios and partially demanded by law1, neural network interpretability has been established as an important and active research area. Consequently, many approaches to explaining neural network decisions have been proposed in recent years, e.g. [3]\u2013[6]. The Layer-wise Relevance Propagation (LRP) [7] framework has proven successful at providing a meaningful intuition and measurable quantities describing a network\u2019s feature processing and decision making [8]\u2013[10]. LRP attributes relevance scores Ri to the model inputs or intermediate neurons i by decomposing a model output of interest. The method follows the principles of relevance conservation and proportional decomposition. Therefore, attribu-\nThis work was partly supported by the German Ministry for Education and Research as BIFOLD (refs. 01IS18025A and 01IS18037A) and TraMeExCo (ref. 01IS18056A). A. Binder is grateful for the support by the Ministry of Education of Singapore (MoE) Tier 2 grant MOE2016-T2-2-154. This publication only reflects the authors views. Funding agencies are not liable for any use that may be made of the information contained herein.\n1e.g. via the \u201cright to explanation\u201d proclaimed in the General Data Protection Regulation of the European Union [1], [2]\ntions computed with LRP maintain a strong connection to the predictor output. While early applications of LRP administer a single decomposition rule uniformly to all layers of a model [7], [11], [12], more recent work describes a trend towards assigning specific decomposition rules purposedly to layers wrt. function and position within the network [10], [13]\u2013[16]. This trend has tacitly emerged and formulates a best practice for applying LRP. Under qualitative evaluation, the attribution maps resulting from this current approach seem to be more robust against the well-known effects of shattered gradients [12], [13], [17] and demonstrate an increased discriminativity between different target classes [13], [14] compared to the uniform application of a single rule.\nHowever, recent literature applying LRP-rules in a layerdependent manner do not justify the beneficial effects of this novel variant quantitatively, but only based on human observation. In this paper, we design and conduct a series of experiments in order to verify whether a layer-specific application of different decomposition rules actually constitutes an improvement above earlier descriptions and applications of LRP [11], [18]. That is, we measure and compare capabilities of various methods from explainable AI \u2014 with a focus on earlier and more recent approaches to LRP \u2014 to precisely localize the ground-truth objects in images via attribution of relevance scores. Our experiments are conducted on popular computer vision data sets with ground truth object localizations, the ImageNet [19] and PascalVOC [20] datasets, using different neural network models."}, {"heading": "II. FEEDFORWARD NEURAL NETWORKS AND LRP", "text": "Feedforward neural networks constitute a popular architecture type, ranging from simple multi-layer perceptrons and shallower convolutional architectures such as the LeNet-5 [21] to deeper and more complex Inception [22] and VGG-like architectures [23]. These types of neural network commonly use ReLU non-linearities and first pass information through a stack of convolution and pooling layers, followed by several fully connected layers. The good performance of feedforward architectures in numerous problem domains, and the availability as pre-trained models makes them a valuable standard architecture in neural network design."}, {"heading": "A. Layer-wise Relevance Propagation", "text": "Consequently, feedforward networks have been subject to investigations in countless contributions towards neural network interpretability, including applications of LRP [7], [11], [18], which finds its mathematical foundation in Deep Taylor Decomposition (DTD) [24].\nThe most basic attribution rule of LRP (to which we will\nrefer to as LRPz) is defined as\nR (l) i =\n\u2211\nj\nzij zj R (l+1) j (1)\nand performs a proportional decomposition of a given upper layer relevance value R (l+1) j at some layer (l + 1) and neuron j to obtain lower layer relevance scores R (l) i for neurons i at layer (l), wrt. to the localized preactivations zij and their respective aggregations zj at the layer output. Here, the localized preactivations zij describe quantities propagated through the model during prediction time, e.g. zij = xiwij and zj = \u2211\ni zij within a neural network layer with learned weight parameters wij . Note that Eq. (1) is conservative between layers and in general maintains an equality \u2211\niR (l) i = f(x)\nat any layer (l) of the model. Further purposed LRP-rules beyond Eq. (1) are introduced in [7], which can be understood as advancements thereof:\nSo does the LRP\u03b5 decomposition rule [7] add a signed and small constant \u03b5 to the denominator in order to prevent divisions by zero and to diminish the effect of recessive (e.g. weak and noisy) mappings zij to the relevance decomposition.\nR (l) i =\n\u2211\nj\nzij zj + \u03b5 \u00b7 sign(zj) R (l+1) j (2)\nThe LRP\u03b1\u03b2-rule [7] performs and then merges separate decompositions for the activatory (z+ij) and inhibitory (z \u2212 ij ) parts of the forward pass\nR (l) i =\n\u2211\nj\n( \u03b1 z+ij\nz+j + \u03b2\nz\u2212ij z\u2212j ) R (l+1) j (3)\nwhere\nz+ij =\n{\nzij ; zij > 0 0 ; else z\u2212ij =\n{\n0 ; else zij ; zij < 0 (4)\nHere, the non-negative \u03b1 parameter permits a weighting of relevance distribution towards activations and inhibitions. The \u03b2 parameter is given implicitly s.t. \u03b1+\u03b2 = 1 in order to uphold conservativity of relevance between layers. The commonly used parameter \u03b1 = 1 can be derived from DTD and has been rediscovered in ExcitationBackprop [25].\nLater work [14], [26] introduces LRP\u266d 2, a decomposition rule which spreads the relevance of a neuron uniformly across all its inputs. This rule assumes zij = 1 and zj = \u2211\ni 1 in Eq. (1) only for backpropagating given relevance scores R (l+1) j to lower layers (l), and has seen application in the input layer(s) of neural networks. The LRP\u266d-rule provides invariance to the decomposition process wrt. to translations in the input domain and effectively propagates relevance scores of higher layer neurons \u2014 encoding \u201cexplanations\u201d of more abstract concepts \u2014 towards the input via the neurons\u2019 receptive fields, without further transformation. Note that the LRP\u266d decomposition rule is thus unsuitable for decomposing fully connected layers.\nEarlier applications of LRP (e.g. [7], [11]) did use one single decomposition rule uniformly over the whole network, which often resulted in suboptimal \u201cexplanations\u201d of model behavior [13]. So are network-wide applications of LRPz (in the following denoted as LRPz , in order to distinguish this specific configuration of LRP from the rule LRPz) and networkwide applications of LRP\u03b5 (denoted as LRP\u03b5) respectively identical and highly similar to Gradient\u00d7Input (G\u00d7I) in ReLU-activated DNNs [12]. LRPz and LRP\u03b5 demonstrate \u2014 albeit working well for shallower convolutional models [27], [28] such as the LeNet-5 [21] or simpler fullyconnected networks [29] \u2014 the effect of gradient shattering as overly complex attributions for deeper models [12], [13]\n2read: \u266d =\u201cflat\u201d, as in the musical \u266d.\n(cf. Fig. 1(a)). A Network-wide application of LRP\u03b1\u03b2 (denoted as LRP\u03b1\u03b2) demonstrates robustness against gradient shattering and produces visually pleasing attribution maps, however is lacking in class- or object discriminativity [13], [30]. By separately considering activatory and inhibitory mappings zij during the decomposition process, LRP\u03b1\u03b2 tends to attribute relevance to similar sets of input features activating sequences of neurons throughout the network, regardless of the output class chosen for relevance decomposition (cf. Fig. 1(b)). Further, LRP\u03b1\u03b2 introduces the constraint of strictly positive layer activations [24], which is in general not guaranteed, especially at the (logit) output of a model. A dissatisfaction of this constraint may result in a sign inversion of all backpropagated relevance scores."}, {"heading": "B. A Current Best Practice for LRP", "text": "A recent trend among XAI researchers and practitioners employing LRP is the use of a composite strategy of rule applications for decomposing the prediction of a neural network [10], [13]\u2013[16]. That is, different parts of the DNN are decomposed using purposed rules, which in combination are robust against gradient shattering while sustaining object discriminativity. Common among these works is the utilization of LRP\u03b5 with \u03b5 \u226a 1 (or just LRPz) to decompose fully connected layers close to the model output, followed by an application of LRP\u03b1\u03b2 to the underlying convolutional layers (usually with \u03b1 \u2208 {1, 2}). Here, the separate decomposition of the positive and negative forward mappings complements the localized feature activation of convolutional filters activated by, and feeding into ReLUs. A final decomposition step within the convolution layers near the input uses the LRP\u266d-rule. Most commonly this rule (or alternatively the DTDzB -rule defined in context of Deep Taylor Decompositon [24]) is applied to the input layer only. In summary, we here describe this pattern of rule application as LRPCMP (for CoMPosite). Fig. 1 provides a qualitative overview of the effect of LRPCMP in contrast to other parameterizations and methods, which we will further discuss in Sec. IV-B. Note that the option to apply the LRP\u266d decomposition to the first n layers near the input (instead of only the first one layer) provides control over the local and semantic scale [26] of the computed attributions (see Fig. 1(e)-(g)). Previous works profit from this option for comparing DNNs of varying depth, and differently configured convolutional stacks [14], or by increasing readability of attributions maps aligned to the requirements of human investigators [15]."}, {"heading": "III. METRIC AND ASSUMPTIONS", "text": ""}, {"heading": "A. Motivation", "text": "The declared purpose of LRP is to precisely and quantitatively inform about the (image or intermediate) features which contribute towards or against the decision of a model wrt. to a specific predictor output [7]. While the recent LRPCMP exhibits improved properties above previous variants of LRP by eyeballing, an objective verification requires quantification. The visual object detection setting, as it is described by the Pascal VOC (PVOC) [20] or ImageNet [31] datasets \u2014 both\nof which include object bounding box annotations \u2014 delivers an optimal experimental setting for this purpose.\nAn assumed ideal model would, in such a setting, exhibit true object understanding by only predicting based on the object itself. A good and representative attribution method should therefore reflect that object understanding of the model closely i.e. by marking (parts of) the shown object as relevant and disregarding visual features not representing the object itself. Similar to [11], we therefore rely on a measure based on localization of attribution scores. In the following, we will evaluate LRPCMP against other methods and variants of LRP on ImageNet using a pre-trained VGG-16 network, and on PVOC 2007 using a pre-trained (on PVOC 2012) CaffeNet model [11]. Both models perform well on their respective task and have been obtained from https://modelzoo.co/ .\nB. Verifying Object-centricity During Prediction\nIn practice, both datasets can not be assumed to be free from contextual biases (cf. [10], [32]), and in both settings models are trained to categorize images rather than localize objects. Still, we (necessarily) assume that the models we use dominantly base their decision on the target object, as opposed to the image context.\nWe verify our hypothesis in Fig. 2, by showing for both models and datasets the reaction of the corresponding predictor f to the occlusion of the object area vs. the occlusion of the image background. That is, for each image x of the respective dataset, we leverage the available bounding box annotations and compute partially occluded versions x\u2032 where either the object area or class-specific image background (i.e. the non-object area) are replaced with mean color values per corresponding pixel and dataset. We then measure the \u2206f(x) = f(x\u2032) \u2212 f(x) for the ground truth label(s) of x based on the network\u2019s logit outputs, and plot this value as a function of relative bounding box size. Fig. 2 shows the average values and standard deviation for \u2206f(x) per bounding box size (discretized into 100 uniform bins) when replacing\neither the object (area within the bounding box) or the context (rest of the image).\nOccluding the object area consistently leads to a sharper decrease in the output for the specific class. The trend is especially evident for smaller objects. This supports our claim that the networks base their decision mainly on the object itself."}, {"heading": "C. Attribution Localization as a Quantitative Measure", "text": "This gives us a performance criterion for attribution methods in object detection and classification. In order to track the fraction of the total amount of relevance that is attributed to the object, we use the inside-total relevance ratio \u00b5 without, and a weighted variant \u00b5w within consideration of the object size:\n\u00b5 = Rin Rtot\n\u00b5w = \u00b5 \u00b7 Stot Sin\n(5)\nWhile conceptually similar to the inside-outside ratio used in [11], \u00b5 and \u00b5w avoid numerical issues in edge cases wrt. bounding box size. Here, Rin is the sum of positive relevance in the bounding box, Rtot the total sum of positive relevance in the image and Sin and Stot are the size of the bounding box and the image respectively, in pixels. The subscript w signals the addition of a normalization factor in \u00b5w considering the size of image and object.\nCorrectly locating small objects is more difficult than locating image-sized objects. Since the ratio Stot/Sin is always greater than or equal to 1 and increases for smaller objects, \u00b5w puts additional emphasis on measuring the outcome for small bounding box sizes. In both cases, higher values indicate larger fractions of relevance attributed to the object area (and not background), and therefore are the desirable outcome."}, {"heading": "IV. EXPERIMENTS AND RESULTS", "text": ""}, {"heading": "A. Experimental Setup", "text": "We perform our experiments on both the ImageNet and the PVOC 2007 datasets, since both collections provide large numbers of ground truth object bounding boxes.\nFor PVOC, we compute attribution maps for all samples (approx. 10.000) from PVOC 2007, using a model which has been pre-trained on the multi label setting of PVOC 2012 [11], [20]. The respective model performs with a mean AP of 72.12 on PVOC 2007. Since PVOC describes a multi label setting, multiple classes can be present in the same image. We therefore evaluate \u00b5 and \u00b5w once for each unique existing pair of { class \u00d7 sample }, yielding approximately 15.000 measurements. Images with a higher number of (smaller) bounding boxes thus effectively have a stronger impact on the results than images with larger (and fewer), image-filling objects, while at the same time describing a more difficult setting. Many of the objects shown in PVOC images are not centered. In order to use all available object information in our evaluation, we rescale the input images to the network\u2019s desired input shape, avoiding the (partial) cropping of objects.\nOn ImageNet [19] (2012 version), bounding box information does only exist for the 50.000 validation samples\n(displaying one class per image) and can be downloaded from the official website3. We evaluate a pre-trained VGG-16 model from the keras model zoo, obtained via the iNNvestigate [33] toolbox. The model performs with a 90.1% top-5 accuracy on the ImageNet test set. For all images the shortest side is rescaled to fit the model input and the longest side is center-cropped to obtain a quadratic input shape. Bounding box information is adjusted correspondingly.\nFor computing attribution maps, we make use of existing XAI software packages, depending on the models\u2019 formats. That is, for the VGG-16 model we use the Keras [34] and Tensorflow [35] based iNNvestigate [33] toolbox. For the PVOC data and the CaffeNet architecture, we compute attributions using the Caffe [36] based LRP Toolbox [27].\nBoth XAI packages support the same functionality regarding LRP, yet differ in the provided selection of other attribution methods. Our study, however, shall be focussed on the beneficial or detrimental effects between the variants of LRP used in literature.\nWe compute attribution maps and values for \u00b5 and \u00b5w for both models and different variants of LRP: LRPz , LRP\u03b1\u03b2 (both for \u03b1 = 1 and \u03b1 = 2), and several parameterizations of LRPCMP . For the latter we distinguish parameter choices for \u03b1 in a subscript when discussing quantitative results in Sec. IV-C. Additionally, in case LRP\u266d is applied to the input layer, we add \u201c+\u266d\u201d to the subscript, e.g. as \u201cLRPCMP :\u03b11+\u266d\u201d.\nWe complement the results with Guided Backprop (GB) [3] and for ImageNet with Pattern Attribution (PA) [4] only available in iNNvestigate. On both datasets, we evaluate attributions for the ground truth class labels, independent of the network prediction."}, {"heading": "B. Qualitative Observations", "text": "Fig. 1 exemplarily shows attribution maps computed with different methods based on the VGG-16 model, for two object classes present in the ImageNet labels and the input image; \u201cBernese Mountain Dog\u201d and \u201cTiger Cat\u201d. Attributions in Figs. 1(a)-(d) result from uniform rule application to the whole network. Next to applications of LRPz and LRP\u03b1\u03b2 with \u03b1 = 1, this includes Guided Backprop [3] and Pattern Attribution [4]. Neither of these maps demonstrate class-discriminativeness and prominently attribute scores to the same areas, regardless of the target class chosen for attribution. LRPz additionally shows the effects of gradient shattering in a highly complex attribution structure due to its equivalence to G\u00d7I. Such attributions would be difficult to use and juxtapose in further algorithmic or manual analyses of model behavior.\nTo the right, attribution maps in Figs. 1(e)-(g) correspond to variants of LRPCMP , which apply different decomposition rules depending on layer type and position. In Fig. 1(e), the LRP\u266d-rule is not applied at all, while in Fig. 1(f) it is used for the first three convolutional layers, and the whole\n3http://www.image-net.org/challenges/LSVRC/2012\nconvolutional stack \u2014 including pooling layers \u2014 in Fig. 1(g). Both heatmaps in Fig. 1(e) and Fig. 1(f) use \u03b1 = 1. Here altogether, the visualized attribution maps correspond more to an \u201cintuitive expectation\u201d of how relevance should be attributed compared to Figs. 1(a)-(d), assuming a model predicts based on object understanding. Figs. 1(e)-(g) demonstrate the change in scale and semantic, from attributions to local features to a very coarse localization map, with changing placements of the LRP\u266d-rule. Further, it becomes clear that with an application of the LRP\u03b1\u03b2-rule in upper layers, object localization is lost (see Fig. 1(b) vs. Fig. 1(g)), while an application in lower layers avoids issues related to gradient shattering, as shown in Figs. 1(e)-(f) compared to Fig. 1(a).\nNote that the special case shown in Fig. 1(g) is highly similar to an application of the Class Activation Mapping (CAM) [37] method in the fully connected part of the model, however replaces the upsampling over the model\u2019s convolutional stack of the CAM approach with the LRP\u266d decomposition based approach of the LRP framework, and is thus naturally capable of distributing negative relevance scores.\nNote that the VGG-16 network used here never has been trained in a multi-label setting. Despite only receiving one object category per input sample, it has learned to distinguish between different object types shown in the same image, e.g. that a dog is not a cat. This in turn reflects well in the attribution maps computed after the LRPCMP pattern.\nFurther examples akin to Fig. 1 are given in the Appendix."}, {"heading": "C. Quantitative Results", "text": "Figs. 3(a) and (b) show the average in-total ratio \u00b5 as a function of bounding box size, discretized over 100 equally spaced intervals, for PVOC 2007 and ImageNet. Averages for \u00b5 and \u00b5w over the whole (and partial) datasets can be found in Tab. I. Large values indicate more precise attribution to the relevant object.\nThe inside-total relevance ratio highly depends on the size of the bounding box. In addition to the average \u00b5 and \u00b5w as an aggregate over all classes and images, we also report \u00b5\u22640.25 and \u00b5\u22640.5, the average values over all objects whose\nbounding box does not span more than 0.25 and 0.5 times the area of the whole image respectively. The assumed Baseline is the uniform attribution of relevance over the whole image, which is outperformed by all methods.\nLRPz performs noticeably worse on ImageNet than on PVOC, which we trace back to the significant difference in model depth (13 vs 21 layers) affecting gradient shattering. We omit LRP\u03b5 in Tab. I due to the identity in results to LRPz . LRP\u03b1\u03b2 has the tendency to attribute to all shown objects (via generally neuron-activating features) and suffers from the multiple object classes per image in PVOC, where ImageNet shows only one class. Also, the similarity of attributions between PA and LRP\u03b1\u03b2 with \u03b1 = 1 observed in Fig. 1 seem consistent on ImageNet and result in close measurements in Tab. I.\nTab. I demonstrates that LRPCMP clearly outperforms other methods consistently on large datasets. That is, the increased precision in attribution to relevant objects is especially evident in the presence of smaller bounding boxes in \u00b5w. This can also be seen in \u00b5\u22640.25 and \u00b5\u22640.5 in Tab. I and the left parts of Figs. 3(a) and (b), where a majority of the image shows contextual information or other classes. Once bounding boxes become (significantly) larger and cover over 50% of the image, all methods converge towards perfect performance, as expected. In both settings, LRPCMP :\u03b12+\u266d yields the best results, while overall the composite strategy is more effectful than a fine tuning of decomposition rule parameters."}, {"heading": "D. Conclusion", "text": "In this study, we discuss a recent development in the application of Layer-wise Relevance Propagation. We summarize this emerging strategy of a composite application of multiple purposed decomposition rules as LRPCMP and juxtapose its effects to previous approaches to LRP and other methods, which uniformly apply a single decomposition rule to all\nlayers of the model. For the first time, our results show that LRPCMP does not only yield measurably more representative attribution maps, but also provides a solution against gradient shattering affecting previous approaches, and improves properties related to object localization and class discrimination via attribution. Moreover, LRPCMP is able to precisely attribute negative relevance scores to class-contradicting features while requiring only one modified backward pass though the model, using established tools from the LRP framework. The discussed beneficial effects are demonstrated qualitatively and verified quantitatively at hand of two large and widely used computer vision datasets."}], "title": "Towards Best Practice in Explaining Neural Network Decisions with LRP", "year": 2020}