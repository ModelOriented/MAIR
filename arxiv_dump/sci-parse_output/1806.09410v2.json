{
  "abstractText": "Failure cases of black-box deep learning, e.g. adversarial examples, might have severe consequences in healthcare. Yet such failures are mostly studied in the context of real-world images with calibrated attacks. To demystify the adversarial examples, rigorous studies need to be designed. Unfortunately, complexity of the medical images hinders such study design directly from the medical images. We hypothesize that adversarial examples might result from the incorrect mapping of image space to the low dimensional generation manifold by deep networks. To test the hypothesis, we simplify a complex medical problem namely pose estimation of surgical tools into its barest form. An analytical decision boundary and exhaustive search of the one-pixel attack across multiple image dimensions let us localize the regions of frequent successful one-pixel attacks at the image space.",
  "authors": [
    {
      "affiliations": [],
      "name": "David K\u00fcgler"
    },
    {
      "affiliations": [],
      "name": "Alexander Distergoft"
    },
    {
      "affiliations": [],
      "name": "Arjan Kuijper"
    },
    {
      "affiliations": [],
      "name": "Anirban Mukhopadhyay"
    }
  ],
  "id": "SP:0b33e9f641202b50ceb608b7e22d56d17df50e10",
  "references": [
    {
      "authors": [
        "K. Eykholt",
        "I. Evtimov",
        "E. Fernandes",
        "B. Li",
        "A. Rahmati",
        "C. Xiao",
        "A. Prakash",
        "T. Kohno",
        "D. Song"
      ],
      "title": "Robust physical-world attacks on deep learning models (2017)",
      "year": 2017
    },
    {
      "authors": [
        "S.G. Finlayson",
        "H.W. Chung",
        "I.S. Kohane",
        "A.L. Beam"
      ],
      "title": "Adversarial attacks against medical deep learning systems",
      "year": 2018
    },
    {
      "authors": [
        "J. Gilmer",
        "L. Metz",
        "F. Faghri",
        "S.S. Schoenholz",
        "M. Raghu",
        "M. Wattenberg",
        "I. Goodfellow"
      ],
      "title": "Adversarial spheres (2018)",
      "year": 2018
    },
    {
      "authors": [
        "I.J. Goodfellow",
        "J. Shlens",
        "C. Szegedy"
      ],
      "title": "Explaining and harnessing adversarial examples",
      "year": 2014
    },
    {
      "authors": [
        "D. K\u00fcgler",
        "A. Stefanov",
        "A. Mukhopadhyay"
      ],
      "title": "i3posnet: Instrument pose estimation from x-ray (2018)",
      "year": 2018
    },
    {
      "authors": [
        "Michael Walter"
      ],
      "title": "Fda reclassification proposal could ease approval process for cad software",
      "venue": "https://www.healthimaging.com/topics/healthcareeconomics/fda-reclassification-proposal-could-ease-approval-processcad-software",
      "year": 2018
    },
    {
      "authors": [
        "A. Nguyen",
        "J. Yosinski",
        "J. Clune"
      ],
      "title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images (2014), http://arxiv.org/pdf/ 1412.1897",
      "venue": "Exploring Adversarial Examples",
      "year": 2014
    },
    {
      "authors": [
        "J. Rauber",
        "W. Brendel",
        "M. Bethge"
      ],
      "title": "Foolbox: A python toolbox to benchmark the robustness of machine learning models (2017)",
      "year": 2017
    },
    {
      "authors": [
        "J. Su",
        "D.V. Vargas",
        "S. Kouichi"
      ],
      "title": "One pixel attack for fooling deep neural networks",
      "year": 2017
    },
    {
      "authors": [
        "C. Szegedy",
        "W. Zaremba",
        "I. Sutskever",
        "J. Bruna",
        "D. Erhan",
        "I. Goodfellow",
        "R. Fergus"
      ],
      "title": "Intriguing properties of neural networks",
      "year": 2013
    }
  ],
  "sections": [
    {
      "text": "Keywords: CNN, Adversarial Examples, One-pixel Attack, Deep Learning Fails"
    },
    {
      "heading": "1 Introduction",
      "text": "End-to-end Deep Learning pipelines (image in, classification out) have achieved significant success in Medical Image Computing (MIC) across multiple scenarios, even stretching to Computer-Aided Interventions (CAI) [6]. This success in comparison to traditional methods (including based on learning) has hurried an AI-summer in Healthcare seen in the prevalence of Deep Learning-publications in the MICCAI community. Political authorities have recognized this shift towards Deep Learning-based methods and are taking action. In the United States, the FDA has embraced this change by approving AI devices for diabetic retinopathy detection [1] and is currently in the discussion towards easing the approval process for AI-based medical software [7]. The European Union, on the other hand, has introduced the General Data Protection Regulation, which necessitates the right to explanation of any decisions taken by a computerized system.\nSince researchers struggle to explain decisions by Deep Learning models, the underlying function is yet a Black Box in practice. Its analysis is hindered by the difficulty of deriving and understanding the decision boundary. In fact, recent studies [2, 5, 8, 9, 11] have shown that these Deep Learning models are vulnerable to adversarial examples \u2013 these are images which cause incorrect classifications despite either models predicting with high certainty or being clear classifications to humans. Adversarial examples are not understood as a consequence of the black-box-characteristic. They can even ar X\niv :1\n80 6.\n09 41\n0v 2\n[ cs\n.C V\n] 1\n3 N\nov 2\n01 8\nbe as simple as only changing a single pixel leading to different classification results (one-pixel-attacks). For medical applications, the exploitation of this vulnerability is thoroughly analyzed by Finlayson et al. [3]. However, this vulnerability is largely ignored across evaluations of Deep Learning in MIC and CAI.\nThough impressive, the general example attacks shown by Finlayson et al. [3] address the traditional image-in-diagnosis-out setting. However, real medical images and annotations are complex further obscuring the situation and adding to the mystery \u2013 for example complex image structure, confounding situations (device vendors, acquisition parameters), non-conformity between radiologists, multi-class and multi-label decisions. Without disentangling these factors it is impossible to understand adversarial examples, which stem from Deep Learning only. Simplifications of MIC scenarios are needed to draw systematic conclusions regarding adversarial examples. For example: to consider segmentation masks rather than the real-world images (binary instead of continuous pixel values) or to define decision boundaries in a closed form (which is not available in MIC).\nIn this paper, we provide the first systematic analysis of one-pixel-attacks on convolutional neural networks (CNNs) in a simplified CAI application and provide a first intuition of patterns. With inherent limits on the knowledge and processable size of both the likely image space and a complete description of the decision boundary, it will be impossible to analyze adversarial attacks due to the complexity of CNNs \u2013 even here simplifications are needed. To break the problem down to its core, we a) simplify the range of images (image space), b) train multiple classifiers and c) search exhaustively for one-pixel adversarial examples. We consider the problem of instrument pose estimation studied by K\u00fcgler et al. [6], who have ignored adversarial attacks, where the orientation of instruments is to be determined. To gain control over the image and annotation complexity, we define a continuous generation manifold with a perfectly defined binary decision boundary. From individual manifold coordinates, we generate binary images at various dimensions with the instrument being simplified to a line for different levels of discretization. We define all images that can be generated through this pipeline as \u201cpossible images\u201d and train multiple simple classifiers based on convolutional neural networks with \u201cALL\u201d these uniquely possible images. Finally, we exhaustively search the space of all single pixel-flip adversarial candidates, identify successful attacks and localize the regions of frequent successful one-pixel-attacks. The most surprisingly, the overwhelming majority of attacks are localized at a distance of the instrument, which implies the one-pixel-flip did not change the information of the image."
    },
    {
      "heading": "2 Related Work",
      "text": "Goodfellow et al.[5] demonstrate that standard image models exhibit a strange phenomenom: most randomly chosen images from a data distribution are correctly classified and yet are close to visually similar images that are incorrectly classified. By adding some certain kind of pertubation to an image this behaviour can be reproduced on most CNNs. A hypothesis on that behaviour is that neural network classifiers are too linear in various regions of the input space.\nSu et al. [10] specialize on so called \u2019one pixel attacks\u2019. They show that even by adding a pertubation with the size a single pixel to an image, the output of deep learning networks can easily be altered.\nGilmer et al. [4] try to get an insight on adversarial examples by training different neural networks on a synthetic dataset of two concentric spheres with different radii. The idea is to classify whether a point belongs to one or the other sphere. Even though the data manifold as well as the theoretical max margin boundary are clearly defined and enough input is provided for networks to train on, adversarials can still be found near correctly classified points."
    },
    {
      "heading": "3 Methods",
      "text": "We generate a custom dataset to study adversarial examples for image classification. Using a exhaustive search, all one-pixel-flip candidate images are tested for misclassification.\nAnalyzing the space I of images, we differentiate between images belonging to an application (I\u2217f \u2282 I, often termed \u201cnatural images\u201d) and images holding no information on the application. By introducing a generation manifold M \u2013 a higher-level parameter space describing all images possible for our application \u2013 we are able to describe all features of the image relevant to our application.\nFor a systematic analysis of adversarial examples, we need a closed-form representation of the decision boundary. Defining the decision boundary dependent on the generation manifold allows us to determine the distance of an image to the decision boundary in terms of the manifolds coordinates. The generation function f : M \u2192 I\u2217f maps from the generation manifold to the image space I\u2217f restricted to images that can be generated by f . Since all generations from a region around mi \u2208 M lead to identical images in I, those cases are in-differentiable, i.e. f is non-injective. In addition, since not all images from I can be created by this function, some images x \u2208 I do not have a\ncorresponding coordinate m inM. For images that cannot be directly generated through the generation function, but that are largely similar to images directly generated (e.g. one-pixel attacks x \u2032i of image xi), the association to the corresponding mi is implicit (dotted line). Some images x\u0303 will have the property of being equally similar to two different m even belonging to different classes. For these ambiguous images, no association to a m or even a class can be found."
    },
    {
      "heading": "3.1 Dataset",
      "text": "With our chosen generation manifold M as (L, \u03b1), our generation function maps to images of lines of varying length L and rotation \u03b1 (see Fig. 2). Lines are centered on the image center, leading to a scenario where images always differ in at least 2 pixels because of symmetry. A line-like structure is being used, to keep the generation manifold as simple as possible. Finally, we define a simple decision boundary classifying images into 2 categories, where y = 1 for \u03b1 in the range from 0\u00b0 to 40\u00b0, and y = 0 for all other cases. The chosen range for \u03b1 is arbitrary.\nWe generate 15 complete sets of all unique binary images X = I\u2217f for 15 different generation functions f by different manifold discretization (only allowing \u03b1 in steps of 0.5\u00b0, 1.0\u00b0 and 2.0\u00b0) and image dimensions D \u00d7 D (16 \u00d7 16, 32 \u00d7 32, 48 \u00d7 48, 64 \u00d7 64 and 80 \u00d7 80). The length L is varied between 12 Pixel and D \u2212 2, where D is the width and the height of the image. This procedure leads to varying numbers of unique images (|X |) as described in Table 1."
    },
    {
      "heading": "3.2 Training",
      "text": "We train 5 models for each of the 15 synthetic combinations (Table 1), resulting in a total of 75 trained networks. Training repititions were performed to average out the stochastic properties of the training process.\nSince we are looking at off-manifold images rather than the gereralization error of the networks, there is no need for a testing set. Moreover, since we use every single image in a subset (i.e. every possible image) for training, our models are not restricted by the choice of training data. All models feature the same architecture only differing in the dimensions of the input images. We design a simple network architecture with\nthree layers: First, two 2D convolutional layers of size 3\u00d7 3 with 32 channels and ReLU activation each followed by max pooling (stride 2) process the input. On this, a fully connected layer with 128 units and ReLU activation is applied, followed by an output layer with 2 units and a Sigmoid activation. We regulize by dropout (p = 0.25) just before the fully connected layer.\nFor optimization, we use the Adam optimizer with recommended parameters (\u03b21 = 0.9 and \u03b22 = 0.999) and a learning rate of 0.001. Finally, binary crossentropy is used as the loss function. We achieved an accuracy of 1.0 with all models indicating perfect convergence on the training dataset."
    },
    {
      "heading": "3.3 Adversarial Data Creation",
      "text": "The goal of creating adversarial data is to identify whether our trained networks can be fooled into predicting the wrong output y for a given image x. By flipping one pixel at a time, we perform an exhaustive search of all combinations of all images in X (see Fig. 3 for examples). The total number of adversarial candidates Nadv = ND2, where N = |X | and D being the width and length of the image. Unlike Su et al. [10], we brute-force our way to a complete list of all possible adversarial examples instead of finding single instances by optimization, which did not work for binary images."
    },
    {
      "heading": "4 Results",
      "text": "By testing the classification of all adversarial candidates, we found all networks to be vulnerable to one-pixel-adversarial attacks. All experiments were performed on 5 networks, so all values are averages over 5 networks.\nWe evaluated the relative number of adversarial examples w.r.t. the adversarial candidates. We also determined this ratio ADVcnt/(ND2) of actual adversarial examples to adversarial candidates dependent on the angle rotation \u03b1 (Advcnt/(n\u03b1D2), see Fig. 4) and on the pixel-position in the image (see Fig. 5). These ratios can also be interpreted as experimentally determined average likelihood of an image being adversarial given the specific conditions.\nFigure 4 shows the distribution of the relationship of misclassifications at a particular angle. Adversarial examples were only found around the decision boundary (\u03b1 \u2248 0\u00b0 or \u03b1 \u2248 40\u00b0). With an increase of the image dimension D the likelihood to find adversarial examples decreases.\nWe created heatmaps, shown in figure 5, to display the spatial likelihoods for a flip to cause a missclassification. The \u201cX\u201d represented the edges of lines from two images,\nwhich belonged to the values of \u03b1 on the decision boundary, i.e. \u03b1 = 0\u00b0 and \u03b1 = 40\u00b0. Surprisingly, the highest likelihoods to cause an image to be confirmed as adversarial were not situated at positions, where the discrimination between classes got harder from an information perspective, but were removed from the edges, i.e. the overwhelming majority of images was not ambiguous. Increasing the image dimension led to more pronounced patterns and better overall robustness. This can also be seen in Fig. 6.\nFinally, we analyzed whether the relationship between the relative number of adversarial examples and the \u201cpossible redundancy\u201d of the reconstructable manifold information in image space exits. The latter is the ratio of information contained in the reconstructable manifold and the image. Higher \u201credundancy\u201d seems to indicated a strong relationship to increased robustness to adversarial attacks.\nUnlike other studies, these results were obtained for networks trained on \u201cALL\u201d possible images that can be generated from the generation manifold and achieved an accuracy of 1.0."
    },
    {
      "heading": "5 Discussion and Conclusion",
      "text": "This paper provides a systematic evaluation of one pixel adversarial attacks on convolutional neural networks. By leveraging a simple toy CAI scenario against a simple yet perfect (accuracy 1.0) convolutional neural network, we find one-pixel-adversarialcandidates with an astonishing regularity. These candidates are deliberately placed close to but off the manifold training images are drawn from. In particular, we identify the\nvulnerable regions to be close to the decision boundary and not explainable by loss of informations caused by the introduction of the \u201cattacking pixel\u201d.\nIn Future Work, we will generalize these observations to toy examples derived from other scenarios, increase the depth of the neural network and investigate causes for adversarial examples.\nThe systematic analysis of adversarial examples presented in this paper initiates a much needed process of understanding adversarial examples in medical images."
    }
  ],
  "title": "Exploring Adversarial Examples Patterns of One-Pixel Attacks",
  "year": 2018
}
