{"abstractText": "Modern requirements for machine learning (ML) models include both high predictive performance and model interpretability. A growing number of techniques provide model interpretations, but can lead to wrong conclusions if applied incorrectly. We illustrate pitfalls of ML model interpretation such as bad model generalization, dependent features, feature interactions or unjustified causal interpretations. Our paper addresses ML practitioners by raising awareness of pitfalls and pointing out solutions for correct model interpretation, as well as ML researchers by discussing open issues for further research.", "authors": [{"affiliations": [], "name": "Christoph Molnar"}, {"affiliations": [], "name": "Gunnar K\u00f6nig"}, {"affiliations": [], "name": "Julia Herbinger"}, {"affiliations": [], "name": "Timo Freiesleben"}, {"affiliations": [], "name": "Susanne Dandl"}, {"affiliations": [], "name": "Christian A. Scholbeck"}, {"affiliations": [], "name": "Giuseppe Casalicchio"}, {"affiliations": [], "name": "Moritz Grosse-Wentrup"}, {"affiliations": [], "name": "Bernd Bischl"}], "id": "SP:f30ee825339382e708df765c2fbb1c7526cd2377", "references": [{"authors": ["D.W. Apley", "J. Zhu"], "title": "Visualizing the effects of predictor variables in black box supervised learning models", "venue": "arXiv preprint arXiv:1612.08468,", "year": 2016}, {"authors": ["S. Arlot", "A. Celisse"], "title": "A survey of cross-validation procedures for model selection", "venue": "Statist. Surv.,", "year": 2010}, {"authors": ["F.R. Bach", "M.I. Jordan"], "title": "Kernel independent component analysis", "venue": "Journal of Machine Learning Research, 3(Jul):1\u2013", "year": 2002}, {"authors": ["B. Baesens", "T. Van Gestel", "S. Viaene", "M. Stepanova", "J. Suykens", "J. Vanthienen"], "title": "Benchmarking state-ofthe-art classification algorithms for credit scoring", "venue": "Journal of the Operational Research Society,", "year": 2003}, {"authors": ["M.I. Belghazi", "A. Baratin", "S. Rajeshwar", "S. Ozair", "Y. Bengio", "A. Courville", "D. Hjelm"], "title": "Mutual information neural estimation", "venue": "In International Conference on Machine Learning,", "year": 2018}, {"authors": ["B. Bischl", "O. Mersmann", "H. Trautmann", "C. Weihs"], "title": "Resampling methods for meta-model validation with recommendations for evolutionary computation", "venue": "Evolutionary Computation,", "year": 2012}, {"authors": ["L. Breiman", "J.H. Friedman"], "title": "Estimating optimal transformations for multiple regression and correlation", "venue": "Journal of the American statistical Association,", "year": 1985}, {"authors": ["Britton", "M. Vine"], "title": "Visualizing statistical interactions in black box models", "venue": "arXiv preprint arXiv:1904.00561,", "year": 2019}, {"authors": ["E. Candes", "Y. Fan", "L. Janson", "J. Lv"], "title": "Panning for gold:model-xknockoffs for high dimensional controlled variable selection", "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "year": 2018}, {"authors": ["G. Casalicchio", "C. Molnar", "B. Bischl"], "title": "Visualizing the feature importance for black box models", "venue": "Machine Learning and Knowledge Discovery in Databases,", "year": 2019}, {"authors": ["G. Claeskens", "Hjort", "N. L"], "title": "Model selection and model averaging", "venue": "Cambridge Books,", "year": 2008}, {"authors": ["T.M. Cover", "J.A. Thomas"], "title": "Elements of Information Theory", "year": 2012}, {"authors": ["S. Dandl", "C. Molnar", "M. Binder", "B. Bischl"], "title": "Multiobjective counterfactual explanations", "venue": "arXiv preprint arXiv:2004.11165,", "year": 2020}, {"authors": ["T. Dickhaus"], "title": "Simultaneous Statistical Inference", "venue": "SpringerVerlag Berlin Heidelberg,", "year": 2014}, {"authors": ["F. Doshi-Velez", "B. Kim"], "title": "Towards a rigorous science of interpretable machine learning", "venue": "arXiv preprint arXiv:1702.08608,", "year": 2017}, {"authors": ["O.J. Dunn"], "title": "Multiple comparisons among means", "venue": "Journal of the American Statistical Association,", "year": 1961}, {"authors": ["L. Fahrmeir", "T. Kneib", "S. Lang", "B. Marx"], "title": "Regression: Models, Methods and Applications", "year": 2013}, {"authors": ["H. Fanaee-T", "J. Gama"], "title": "Event labeling combining ensemble detectors and background knowledge", "venue": "Progress in Artificial Intelligence, pp", "year": 2013}, {"authors": ["M. Fern\u00e1ndez-Delgado", "E. Cernadas", "S. Barro", "D. Amorim"], "title": "Do we need hundreds of classifiers to solve real world classification problems", "venue": "Journal of Machine Learning Research,", "year": 2014}, {"authors": ["A. Fisher", "C. Rudin", "F. Dominici"], "title": "All models are wrong, but many are useful: Learning a variables importance by studying an entire class of prediction models simultaneously", "venue": "Journal of Machine Learning Research,", "year": 2019}, {"authors": ["J.H. Friedman", "B.E. Popescu"], "title": "Predictive learning via rule ensembles", "venue": "Annals of Applied Statistics,", "year": 2008}, {"authors": ["Friedman", "J. H"], "title": "Multivariate adaptive regression splines", "venue": "The Annals of Statistics,", "year": 1991}, {"authors": ["A. Goldstein", "A. Kapelner", "J. Bleich", "E. Pitkin"], "title": "Peeking inside the black box: Visualizing statistical learning with plots of individual conditional expectation", "venue": "Journal of Computational and Graphical Statistics,", "year": 2015}, {"authors": ["P.I. Good", "J.W. Hardin"], "title": "Common errors in statistics (and how to avoid them)", "year": 2012}, {"authors": ["B.M. Greenwell"], "title": "pdp: An R package for constructing partial dependence plots", "venue": "The R Journal,", "year": 2017}, {"authors": ["B.M. Greenwell", "B.C. Boehmke", "A.J. McCarthy"], "title": "A simple and effective model-based variable importance measure", "year": 2018}, {"authors": ["A. Gretton", "O. Bousquet", "A. Smola", "B. Sch\u00f6lkopf"], "title": "Measuring statistical dependence with hilbert-schmidt norms", "venue": "In International Conference on Algorithmic Learning Theory,", "year": 2005}, {"authors": ["P. Hall"], "title": "On the art and science of machine learning explanations", "venue": "arXiv preprint arXiv:1810.02909,", "year": 2018}, {"authors": ["S. Holm"], "title": "A simple sequentially rejective multiple test procedure", "venue": "Scandinavian Journal of Statistics,", "year": 1979}, {"authors": ["G. Hooker"], "title": "Discovering additive structure in black box functions", "venue": "Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD", "year": 2004}, {"authors": ["G. Hooker"], "title": "Generalized functional anova diagnostics for high-dimensional functions of dependent variables", "venue": "Journal of Computational and Graphical Statistics,", "year": 2007}, {"authors": ["G. Hooker", "L. Mentch"], "title": "Please stop permuting features: An explanation and alternatives", "venue": "arXiv preprint arXiv:1905.03151,", "year": 2019}, {"authors": ["D. Janzing", "L. Minorics", "P. Bl\u00f6baum"], "title": "Feature relevance quantification in explainable ai: A causality problem", "venue": "arXiv preprint arXiv:1910.13413,", "year": 2019}, {"authors": ["Karimi", "A.-H", "B. Schlkopf", "I. Valera"], "title": "Algorithmic Recourse: from Counterfactual Explanations to Interventions", "year": 2002}, {"authors": ["H. Khamis"], "title": "Measures of association: how to choose", "venue": "Journal of Diagnostic Medical Sonography,", "year": 2008}, {"authors": ["M. Krishnan"], "title": "Against interpretability: a critical examination of the interpretability problem in machine learning", "venue": "Philosophy & Technology,", "year": 2019}, {"authors": ["G. Knig", "M. Grosse-Wentrup"], "title": "A Causal Perspective on Challenges for AI in Precision Medicine, 2019", "venue": "URL https://koenig.page/pdf/koenig2019_ pmbc.pdf", "year": 2019}, {"authors": ["A. Liebetrau"], "title": "Measures of Association", "venue": "Number Bd. 32;Bd. 1983 in 07. SAGE Publications,", "year": 1983}, {"authors": ["Z.C. Lipton"], "title": "The mythos of model interpretability", "venue": "Queue, 16(3):31\u201357,", "year": 2018}, {"authors": ["D. Lopez-Paz", "P. Hennig", "B. Sch\u00f6lkopf"], "title": "The randomized dependence coefficient", "venue": "In Advances in Neural Information Processing Systems, pp. 1\u2013", "year": 2013}, {"authors": ["S.M. Lundberg", "G.G. Erion", "Lee", "S.-I"], "title": "Consistent individualized feature attribution for tree ensembles", "venue": "arXiv preprint arXiv:1802.03888,", "year": 2018}, {"authors": ["S. Makridakis", "E. Spiliotis", "V. Assimakopoulos"], "title": "Statistical and machine learning forecasting methods: Concerns and ways forward", "venue": "PloS one,", "year": 2018}, {"authors": ["J. Matejka", "G. Fitzmaurice"], "title": "Same stats, different graphs: generating datasets with varied appearance and identical statistics through simulated annealing", "venue": "In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems,", "year": 2017}, {"authors": ["C. Molnar", "G. Casalicchio", "B. Bischl"], "title": "iml: An R package for interpretable machine learning", "venue": "Journal of Open Source Software,", "year": 2018}, {"authors": ["C. Molnar", "G. Casalicchio", "B. Bischl"], "title": "Quantifying model complexity via functional decomposition for better post-hoc interpretability", "venue": "In Joint European Conference on Machine Learning and Knowledge Discovery in Databases,", "year": 2019}, {"authors": ["C. Molnar", "G. K\u00f6nig", "B. Bischl", "G. Casalicchio"], "title": "Model-agnostic feature importance and effects with dependent features\u2013a conditional subgroup approach", "venue": "arXiv preprint arXiv:2006.04628,", "year": 2020}, {"authors": ["J. Pearl", "D. Mackenzie"], "title": "The ladder of causation. The book of why: the new science of cause and effect", "venue": "New York (NY): Basic Books,", "year": 2018}, {"authors": ["T.V. Perneger"], "title": "What\u2019s wrong with bonferroni", "venue": "adjustments. BMJ,", "year": 1998}, {"authors": ["J. Peters", "D. Janzing", "B. Scholkopf"], "title": "Elements of Causal Inference - Foundations and Learning Algorithms", "year": 2620}, {"authors": ["M. Philipp", "T. Rusch", "K. Hornik", "C. Strobl"], "title": "Measuring the stability of results from supervised statistical learning", "venue": "Journal of Computational and Graphical Statistics,", "year": 2018}, {"authors": ["D.N. Reshef", "Y.A. Reshef", "H.K. Finucane", "S.R. Grossman", "G. McVean", "P.J. Turnbaugh", "E.S. Lander", "M. Mitzenmacher", "P.C. Sabeti"], "title": "Detecting novel associations in large data", "venue": "sets. Science,", "year": 2011}, {"authors": ["M.T. Ribeiro", "S. Singh", "C. Guestrin"], "title": "Why should i trust you?: Explaining the predictions of any classifier", "venue": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "year": 2016}, {"authors": ["C. Rudin"], "title": "Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead", "venue": "Nature Machine Intelligence,", "year": 2019}, {"authors": ["C.A. Scholbeck", "C. Molnar", "C. Heumann", "B. Bischl", "G. Casalicchio"], "title": "Sampling, intervention, prediction, aggregation: A generalized framework for model-agnostic interpretations", "venue": "Communications in Computer and Information Science,", "year": 2020}, {"authors": ["M. Shahhosseini", "G. Hu", "S.V. Archontoulis"], "title": "Forecasting corn yield with machine learning ensembles", "venue": "arXiv preprint arXiv:2001.09055,", "year": 2020}, {"authors": ["S. Shalev-Shwartz", "S. Ben-David"], "title": "Understanding machine learning: From theory to algorithms", "venue": "Cambridge university press,", "year": 2014}, {"authors": ["R. Simon"], "title": "Resampling strategies for model assessment and selection. In Fundamentals of data mining in genomics and proteomics", "year": 2007}, {"authors": ["C. Stachl", "Q. Au", "R. Schoedel", "D. Buschek", "S. V\u00f6lkel", "T. Schuwerk", "M. Oldemeier", "T. Ullmann", "H. Hussmann", "B Bischl"], "title": "Behavioral patterns in smartphone usage predict big five personality", "venue": "traits. 2019", "year": 2019}, {"authors": ["C. Strobl", "Boulesteix", "A.-L", "T. Kneib", "T. Augustin", "A. Zeileis"], "title": "Conditional variable importance for random forests", "venue": "BMC bioinformatics,", "year": 2008}, {"authors": ["M. Sundararajan", "A. Najmi"], "title": "The many shapley values for model explanation", "venue": "arXiv preprint arXiv:1908.08474,", "year": 2019}, {"authors": ["G.J. Sz\u00e9kely", "M.L. Rizzo", "Bakirov", "N. K"], "title": "Measuring and testing dependence by correlation of distances", "venue": "The Annals of Statistics,", "year": 2007}, {"authors": ["D. Tjstheim", "H. Otneim", "B. Stve"], "title": "Statistical dependence: Beyond pearson\u2019s", "venue": "p. arXiv preprint arXiv:1809.10455,", "year": 2018}, {"authors": ["S. Wachter", "B. Mittelstadt", "C. Russell"], "title": "Counterfactual explanations without opening the black box: Automated decisions and the gdpr", "venue": "Harv. JL & Tech.,", "year": 2017}, {"authors": ["J. Walters-Williams", "Y. Li"], "title": "Estimation of mutual information: A survey", "venue": "In International Conference on Rough Sets and Knowledge Technology,", "year": 2009}, {"authors": ["D.S. Watson", "M.N. Wright"], "title": "Testing Conditional Independence in Supervised Learning Algorithms", "venue": "arXiv preprint arXiv:1901.09917,", "year": 2019}, {"authors": ["J. Wu", "J. Roy", "W.F. Stewart"], "title": "Prediction modeling using ehr data: challenges, strategies, and a comparison of machine learning approaches", "venue": "Medical Care, pp. S106\u2013", "year": 2010}, {"authors": ["Z. Zhang", "Y. Jin", "B. Chen", "P. Brown"], "title": "California almond yield prediction at the orchard level with a machine learning approach", "venue": "Frontiers in Plant Science,", "year": 2019}, {"authors": ["Q. Zhao", "T. Hastie"], "title": "Causal interpretations of black-box models", "venue": "Journal of Business & Economic Statistics,", "year": 2019}, {"authors": ["X. Zhao", "R. Lovreglio", "D. Nilsson"], "title": "Modelling and interpreting pre-evacuation decision-making using machine learning", "venue": "Automation in Construction,", "year": 2020}], "sections": [{"heading": "1. Introduction", "text": "Traditionally, researchers have used parametric models, e.g., linear models, to conduct inference. However, a noticeable shift has happened over the last years towards more non-parametric and non-linear ML models. Models such as random forests, boosting or neural networks often outperform interpretable models on many prediction tasks, as most ML models handle feature interactions and non-linear effects automatically1 (Ferna\u0301ndez-Delgado et al., 2014). Many disciplines benefit from the predictive performance of ML models and answer scientific questions using ML interpretation techniques. Examples of such efforts include modeling pre-evacuation decision making (Zhao et al., 2020), mapping canopy covers in savannas (Anchang et al.,\n1Department of Statistics, LMU Munich, Munich, Germany 2Research Group Neuroinformatics, Faculty for Computer Science, University of Vienna 3Munich Center for Mathematical Philosophy, LMU Munich 4Graduate School of Systemic Neurosciences, LMU Munich 5Research Platform Data Science @ Uni Vienna 6Vienna Cognitive Science Hub. Correspondence to: Christoph Molnar <christoph.molnar@gmail.com>.\nProceedings of the 37 th International Conference on Machine Learning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by the author(s).\n1While the inclusion of non-linear and interactions effects in classical statistical models is possible, it comes with the increased cost of going more or less manually over many possible modelling options.\n2020), understanding wildlife diseases (Fountain-Jones et al., 2019), forecasting crop yield (Shahhosseini et al., 2020; Zhang et al., 2019), inferring behavior from smartphone usage (Stachl et al., 2019), and analyzing risk for teacher burnout (Posada-Quintero et al., 2020). Practitioners are usually interested in the global effect that features have on the outcome and their importance for correct predictions. For certain model classes, e.g., linear models or decision trees, feature effects or importance scores can be inferred from the learned parameters and model structure. In contrast, complex non-linear models that, e.g., do not have intelligible parameters, make it more difficult to extract such knowledge. Therefore, interpretation methods necessarily simplify the relationships between features and the target, e.g., by marginalizing over other features. Prominent techniques for global feature effects include the partial dependence plot (PDP) (Friedman et al., 1991), accumulated local effects (ALE) (Apley & Zhu, 2016) and individual conditional expectation (ICE) (Goldstein et al., 2015). A common feature importance technique is the permutation feature importance (PFI) (Breiman, 2001; Fisher et al., 2019; Casalicchio et al., 2019). This paper will mainly focus on pitfalls of global interpretation techniques when the full functional relationship underlying the data is to be analyzed. Out of scope is the discussion of \u201clocal\u201d interpretation methods such as LIME (Ribeiro et al., 2016) or counterfactual explanations (Wachter et al., 2017; Dandl et al., 2020), where individual predictions are to be explained \u2013 usually to explain decisions to individuals. The shift towards ML modeling entails numerous pitfalls for model interpretations. ML models usually contain non-linear effects and higher-order interactions. Therefore, lower-dimensional or linear approximations can be inappropriate and misleading masking effects can occur. As interpretations are based on simplifying assumptions, the associated conclusions are only valid if we have checked that the assumptions underlying our simplifications are not substantially violated. In classical statistics this process is called \u201cmodel diagnostics\u201d (Fahrmeir et al., 2013) and we believe that a similar process is necessary for interpretable machine learning (IML) based techniques. Contributions: We review pitfalls of global modelar X iv :2 00 7. 04 13 1v 1 [ st at .M L ] 8 J ul 2 02 0\nagnostic2 interpretation techniques. Each section describes the pitfall, reviews (partial) solutions for practitioners and discusses open issues that require further research. Related Work: A general warning about using and explaining ML models for high stakes decisions has been brought forward by Rudin (2019). She strictly argues against modelagnostic techniques in favour of inherently interpretable models. Krishnan (2019) criticizes the general conceptual foundation of interpretability, but does not dispute the usefulness of available methods. Likewise, Lipton (2018) criticizes interpretable ML (IML) for its lack of causal conclusions, trust and insights, but the author does not discuss any pitfalls in detail. Specific pitfalls due to dependent features are discussed by Hooker (2007) for partial dependence and functional ANOVA and by Hooker & Mentch (2019) for feature importance computations. Hall (2018) discusses recommendations for the application of particular IML methods, but does not address general pitfalls."}, {"heading": "2. Bad Model Generalization", "text": "Pitfall: Under- or overfitting models will result in misleading interpretations regarding true feature effects and importance scores, as the model does not match the underlying data generating process well (Good & Hardin, 2012). In-sample evaluation (i.e., on training data) should not be used for ML models due to the danger of overfitting. We have to resort to out-of-sample validation such as crossvalidation procedures. These resampling procedures are readily available in software and well-studied in theory and practice (Arlot & Celisse, 2010), although rigorous analysis of cross-validation is still considered an open problem (Shalev-Shwartz & Ben-David, 2014). Formally, IML methods are designed to interpret the model instead of drawing inferences about the data generating process. In practice, however, the latter is the goal of the analysis, not the former. If a model approximates the data generating process well enough, its interpretation should reveal insights into the underlying process. Solution: An interpretation can only be as good as its underlying model. It is crucial to properly evaluate models using training and test splits, ideally using a resampling scheme like (repeated) cross-validation for smaller sample sizes and nested setups, when computational model selection and hyperparameter tuning are involved (Bischl et al., 2012; Simon, 2007). Flexible models should be part of the model selection process so that the true data generating function is more likely to be discovered (Claeskens et al., 2008). This is important, as the Bayes error for most practical situations is unknown, and we cannot make absolute statements about whether a model already fits the data optimally.\n2Model-agnostic methods can be applied to any ML model"}, {"heading": "3. Unnecessary Use of Complex Models", "text": "Pitfall: A common mistake is to use an opaque, complex ML model when an interpretable model would have been sufficient, i.e., when the performance of interpretable models is only negligibly worse \u2013 or maybe the same or even better \u2013 than the ML model. Although there are many modelagnostic methods to interpret complex ML models, it is usually preferable to use an interpretable model (Rudin, 2019). There are also some examples where complex ML models such as neural networks were not able to beat interpretable models (Makridakis et al., 2018; Baesens et al., 2003; Kuhle et al., 2018; Wu et al., 2010). Solution: We recommend to start with simple, interpretable models such as (generalized) linear models, LASSO, generalized additive models, decision trees or decision rules and gradually increase complexity in a controlled, step-wise manner, where predictive performance is carefully measured and compared. Complex models should only be analyzed if the additional performance gain is both significant and relevant \u2013 a judgment call that the practitioner must ultimately make. Starting with simple models is considered best practice in data science, independent of the question of interpretability (Claeskens et al., 2008). The comparison of predictive performance between model classes of different complexity can add further insights for interpretation. Open Issues: Measures of model complexity allow to quantify the trade-off between complexity and performance and to automatically optimize for multiple objectives beyond performance. Some steps have been made towards quantifying model complexity like Molnar et al. (2019) and Philipp et al. (2018). However, further research is required as there is no single perfect definition of interpretability but rather multiple, depending on the context (Doshi-Velez & Kim, 2017; Rudin, 2019)."}, {"heading": "4. Ignoring Feature Dependence", "text": ""}, {"heading": "4.1. Interpretation with Extrapolation", "text": "Pitfall: When features are dependent, perturbation-based IML methods such as the PFI and PDP extrapolate in areas where the model was trained with little or no training data, which can cause misleading interpretations (Hooker & Mentch, 2019). Perturbations produce artificial data points that are used for model predictions, which in turn are aggregated to produce global interpretations (Scholbeck et al., 2020). Feature values can be perturbed by replacing original values with values from an equidistant grid of that feature, with permuted or with randomly subsampled values (Casalicchio et al., 2019), or with quantiles. We highlight two major issues. First, if features are dependent, all three perturbation approaches produce unrealistic data points, i.e., the new data points are located outside of the multivariate joint distribution of the data (see Figure 1). Second, even if\nfeatures are independent, using an equidistant grid can produce unrealistic values for the feature of interest. Consider a feature that follows a skewed distribution with outliers. An equidistant grid would generate a lot of values in between outliers and non-outliers. In contrast to the grid-based approach, the other two approaches maintain the marginal distribution of the feature of interest. Both issues can result in misleading interpretations (illustrative examples given in Hooker & Mentch (2019); Molnar et al. (2020)) since the model is evaluated in areas of the feature space with few or no observed data points, where model uncertainty can be expected to be very high. This issue is aggravated if global interpretation methods integrate over such points with the same weight and confidence as for much more realistic samples with high model confidence. Solution: Before applying interpretation methods, practitioners should check for dependencies between features in the data, e.g., via descriptive statistics or measures of dependence (see Section 4.2). When it is unavoidable to include dependent features in the model, which is usually the case in ML scenarios, additional information regarding the strength and shape of the dependence structure should be provided. Sometimes alternative interpretation methods can be used as a workaround or to provide additional information. ALE (Apley & Zhu, 2016) plots are preferable to the PDP when visualizing feature effects of dependent features. For other methods such as the PFI, conditional variants exist (Molnar et al., 2020; Candes et al., 2018; Strobl et al., 2008). Note, however, that conditional interpretations are often different and should not be used as a substitute for unconditional interpretations (see Section 4.3). Furthermore, dependent features should not be interpreted separately but rather jointly. This can be achieved by visualizing, e.g., a 2-dimensional ALE plot of two dependent features, which, admittedly, only works for very low-dimensional combinations. We recommend using quantiles or randomly subsampled values over equidistant grids. By default, many implementations of interpretability methods use an equidistant grid to perturb feature values (Greenwell, 2017; Molnar et al., 2018; Pedregosa et al., 2011), although some also allow to use user-defined values. Open Issues: A comprehensive comparison of strategies addressing extrapolation, and how they affect an interpretation method, is currently missing. This also includes studying interpretation methods and their conditional variants when they are applied to data with different dependence structures."}, {"heading": "4.2. Confusing Correlation with Dependence", "text": "Pitfall: Features with a Pearson correlation coefficient (PCC) close to zero can still be dependent and cause misleading model interpretations (see Figure 2). While independence between two features implies that the PCC is zero, the converse is generally false. The PCC, which is often used to\nanalyze dependence, only tracks linear correlations and has other shortcomings such as sensitivity to outliers (Tjstheim et al., 2018). Any type of dependence between features can have a strong impact on the interpretation of the results of IML methods (see Section 4.1). Thus, knowledge about the (possibly non-linear) dependencies between features is crucial for an informed use of IML methods. Solution: Low-dimensional data can be visualized to detect dependence (e.g., scatter plots) (Matejka & Fitzmaurice, 2017). For high-dimensional data, several other measures of dependence in addition to PCC can be used. If dependence is monotonic, Spearman\u2019s rank correlation coefficient (Liebetrau, 1983) can be a simple, robust alternative to PCC. For categorical or mixed features, separate dependence measures have been proposed, such as Kendall\u2019s tau for ordinal features, or the phi coefficient and Goodman & Kruskals lambda for nominal features. (Khamis, 2008) Studying non-linear dependencies is more difficult since a vast variety of possible associations have to be checked. Nevertheless, several non-linear association measures with sound statistical properties exist. Kernel-based measures such as kernel canonical correlation analysis (KCCA) (Bach & Jordan, 2002) or the Hilbert-Schmidt independence criterion (HSIC) (Gretton et al., 2005) are commonly used. They have a solid theoretical foundation, are computationally feasible and robust (Tjstheim et al., 2018). In addition, there are information-theoretical measures such as (conditional) mutual information (Cover & Thomas, 2012) or the maximal information coefficient (MIC) (Reshef et al., 2011), that can however be difficult to estimate (Walters-Williams & Li, 2009; Belghazi et al., 2018). Other important measures are, e.g., the distance correlation (Sze\u0301kely et al., 2007), the randomized dependence coefficient (RDC) (Lopez-Paz et al., 2013), or the alternating conditional expectations (ACE) algorithm (Breiman & Friedman, 1985). In addition to using PCC we recommend using at least one measure that detects non-linear dependencies (e.g. HSIC)."}, {"heading": "4.3. Misunderstanding Conditional Interpretation", "text": "Pitfall: Conditional variants to estimate feature effects and importance scores require a different interpretation. While\nconditional variants for feature effects, e.g., the marginal plot (Apley & Zhu, 2016), feature importance scores (Candes et al., 2018; Watson & Wright, 2019; Molnar et al., 2020; Strobl et al., 2008), and conditional Shapley values (Lundberg et al., 2018) avoid model extrapolations, these methods answer a different question and have been argued to violate fundamental properties in the case of Shapley values (Janzing et al., 2019; Sundararajan & Najmi, 2019). Interpretation methods that perturb features independently of others also yield an unconditional interpretation, i.e., for feature effect methods such as the PDP, the effect can be interpreted as the isolated, average effect the feature has on the prediction. For the PFI, the importance can be interpreted as the drop in performance when the feature\u2019s information is \u201cdestroyed\u201d (by perturbing it). Conditional variants do not replace values independently of other features, but in such a way that they conform to the conditional distribution. This changes the interpretation as the effects of all dependent features become entangled3. For dependent features, the PFI drops when using conditional variants since the conditional permutation answers the question: \u201cHow much does the model performance drop if we permute a feature, but given that we know the values of the other features?\u201d4. To demonstrate how the interpretation can change, we trained a random forest to predict bike rentals (Fanaee-T & Gama, 2013), using the features \u201cTemperature\u201d, \u201cApparent Temperature\u201d and \u201cHumidity\u201d. Temperature and the apparent temperature are highly linearly correlated, with a Pearson correlation coefficient of 0.992. The importance scores (measured as drop in mean absolute error) of the temperature (PFI 729; conditional PFI 285) and the apparent temperature (689; 266) drop considerably when using the\n3E.g., a feature that did not show an effect in the PDP might show an effect when using the marginal plot, when a dependent feature impacts the prediction.\n4E.g., two highly dependent features might be individually important (based on the unconditional PFI), but have a very low conditional importance, since the information of one feature is contained in the other and vice versa.\nconditional PFI instead of the marginal PFI. For the humidity, the importance scores of both variants are similar (578; 597). Solution: The safest option would be to remove dependent features, but this is usually infeasible in practice. When features are highly dependent and conditional effects and importance scores are used, the practitioner has to be aware of the distinct interpretation. For feature effects, ALE plots (Apley & Zhu, 2016) provide an alternative with an unconditional interpretation. However, they only allow for an interval-wise interpretation. Open Issues: Currently, no approach allows to simultaneously avoid model extrapolations and to allow a conditional interpretation of effects and importance scores for dependent features."}, {"heading": "5. Misleading Effect due to Interactions", "text": "Pitfall: Global interpretation methods such as PDP or ALE plots can produce misleading interpretations when features interact. Figure 3 shows two examples where the global aggregated effects show almost no influence on the target, although an effect is clearly there by construction. Solution: For the PDP, we recommend to additionally consider the corresponding ICE curves (Goldstein et al., 2015). While PDP and ALE average out interaction effects, ICE curves directly show the heterogeneity between individual predictions, as in Figure 3 A. Particularly for continuous interactions with ICE curves starting on different predictions, we recommend the use of derivative or centered ICE curves, which eliminate differences in intercepts and leave only differences due to interactions (Goldstein et al., 2015). As an example the diverging centered ICE curves of X5 in Figure 3 B indicate that there must be an interaction with another feature. Other visualization techniques for discovering second-order interactions are 2-dimensional PDP or ALE plots and methods based on clustering ICE curves such as Visual Interaction Effects (VINE) (Britton, 2019). Pitfall: Many interpretation methods cannot separate interactions from main effects. The PFI, for example, includes\nboth the importance of a feature and the importance of all its interactions with other features (Casalicchio et al., 2019). Solution: Based on a PDP decomposition, the H-Statistic (Friedman & Popescu, 2008) quantifies the interaction strength between two features or between one feature and all others. Another similar interaction score based on partial dependencies is defined by Greenwell et al. (2018). Based on Shapley values Lundberg et al. (2018) proposed SHAP interaction values and Casalicchio et al. (2019) proposed a fair attribution of the importance of interactions to the individual features. Open issues: Most methods that identify and visualize interactions are not able to identify higher-order interactions and interactions of dependent features. Instead of 2-dimensional PDPs, practitioners can use 2-dimensional ALE plots to visualize two-way interactions of dependent features. Furthermore, Hooker (2007) considers dependent features and decomposes the predictions in main and interaction effects. A way to identify higher-order interactions is shown in Hooker (2004). However, these issues are still a matter of further research. Furthermore, the presented solutions lack in automatic detection and ranking of all interactions of a model as well as specifying the type of modelled interaction."}, {"heading": "6. Ignoring Estimation Uncertainty", "text": "Pitfall: Due to variance in the estimation process, interpretations of ML models can become misleading. Methods such as PDP and PFI use Monte Carlo sampling techniques to approximate expected values. These estimates vary, depending on the data used for the estimation. In particular, estimates may vary strongly for feature dependencies and interactions. Furthermore, the obtained ML model is also a random variable, as it is generated on randomly sampled data and the inducing algorithm might contain stochastic components as well. Hence, model variance has to be taken into account. The true effect of a feature may be flat, but purely by chance, especially on smaller data, an effect might algorithmically be detected. This effect could cancel out once averaged over multiple model fits. Figure 4 shows that a single PDP can be misleading because it does not show the variance due to PDP estimation and model fitting. Solution: By repeatedly computing PDP and PFI with a given model, but with different permutations/bootstrap samples, the uncertainty of the estimate can be quantified, for example in the form of confidence intervals. For PFI, frameworks for confidence intervals and hypothesis tests exist (Watson & Wright, 2019; Altmann et al., 2010), but they assume a fixed model. If the practitioner wants to condition the analysis on the modeling process and capture the process\u2019 variance instead of conditioning on a fixed model, PDP and PFI should be computed on multiple model fits. Open Issues: To the best of our knowledge, the uncertainty in feature effect methods such as ALE (Apley & Zhu, 2016)\nand PDP (Friedman et al., 1991) has not been studied in detail."}, {"heading": "7. Ignoring Multiple Comparisons", "text": "Pitfall: Simultaneously testing the importance of multiple features will result in false positive interpretations if the multiple comparisons problem (MCP) is ignored. The MCP is well known in significance tests for linear models and similarly exists in testing for feature importance in ML. For example, suppose we simultaneously test the importance of 50 features (with the H0-hypothesis of zero importance) at the significance level 0.05. Even if all features are unimportant, the probability of observing that at least one feature is significantly important is 1\u2212P(\u2018no feature important\u2019) = 1\u2212 (1\u2212 0.05)50 \u2248 0.923. Multiple comparisons will even be more problematic, the higher dimensional our dataset is. Solution: Methods such as Model-X knockoffs (Candes et al., 2018) directly control for the false discovery rate (FDR). For all other methods that provide p-values or confidence intervals, such as PIMP (Altmann et al., 2010), MCP is often ignored in practice to the best of our knowledge. Exceptions are, e.g., Stachl et al. (2019) and Watson & Wright (2019). One of the most popular MCP adjustment methods is the Bonferroni correction (Dunn, 1961), but it has the major disadvantage of increasing the probability of false negatives (Perneger, 1998). Since MCP is well known in statistics, we refer the practitioner to Dickhaus (2014) for an overview and discussion of alternative adjustment methods such as the Bonferroni-Holm method (Holm, 1979)."}, {"heading": "8. Unjustified Causal Interpretation", "text": "Pitfall: Practitioners are often interested in causal insights into the underlying data generating mechanisms, which IML methods in general do not provide. Common causal questions include the identification of causes and effects, predicting the effects of interventions, and answering counterfac-\ntual questions (Pearl & Mackenzie, 2018). E.g., a medical researcher might want to identify risk factors or predict average and individual treatment effects (Knig & GrosseWentrup, 2019). In search for answers, a researcher can therefore be tempted to interpret the result of IML methods from a causal perspective. However, a causal interpretation of predictive models is often not possible. Standard supervised ML models are not designed to model causal relationships but to merely exploit associations. A model may therefore rely on causes and effects of the target variable as well as on variables that help to reconstruct unobserved influences on Y , e.g., causes of effects (Weichwald et al., 2015). Consequently, the question whether a variable is relevant to a predictive model (indicated, e.g., by PFI > 0) does not directly indicate whether a variable is a cause, an effect or does not stand in any causal relation to the target variable. Furthermore, even if a model would rely solely on direct causes for the prediction, the causal structure between features has to be taken into account. Intervening on a variable in the real world may affect not only Y but also other variables in the feature set. Without assumptions about the underlying causal structure IML methods cannot account for these adaptions and guide action (Karimi et al., 2020). As an example, we constructed a dataset by sampling from a structural causal model (SCM), for which the corresponding causal graph is depicted in Figure 5. All relationships are linear Gaussian with variance 1 and coefficients 1. For a linear model fitted on the dataset all features were considered relevant based on the model coefficients (y\u0302 = 0.329x1 + 0.323x2 \u2212 0.327x3 + 0.342x4 + 0.334x5, R2 = 0.943), although x3, x4 and x5 do not cause Y . Solution: The practitioner has to carefully assess whether sufficient assumptions can be made about the underlying data generating process, the learned model and the interpretation technique. If these assumptions are met, a causal interpretation may be possible. The PDP between a feature and the target can be interpreted as the respective average causal effect if the model performs well and the set of remaining variables is a valid adjustment set (Zhao & Hastie, 2019). When it is known whether a model is deployed in a causal or anti-causal setting, i.e., whether the models attempts to predict an effect from its causes or the other way round, a partial identification of the causal roles based on feature relevance is possible (under strong and non-testable assumptions) (Weichwald et al., 2015). Designated tools and approaches are available for causal discovery and inference (Peters et al., 2017). Open issues: The challenge of causal discovery and inference remains an open key issue in the field of machine learning. Careful research is required to make explicit under which assumptions what insight about the underlying data generating mechanism can be gained by interpreting a machine learning model."}, {"heading": "9. Discussion", "text": "In this paper, we have reviewed numerous pitfalls of global model-agnostic interpretation techniques, e.g., in the case of bad model generalization, dependent features, interactions between features, or causal interpretations. Although these pitfalls are far from complete, we believe that we cover common ones that pose a particularly high risk. We hope to encourage a more cautious approach when interpreting ML models in practice, to point practitioners to already (partially) available solutions and to stimulate further research on these issues. The stakes are high: ML algorithms are increasingly used for socially relevant decisions, and model interpretations play an important role in every empirical science. We therefore believe that users need concrete guidance on properties, dangers and problems of IML techniques \u2013 especially as the field is advancing at high speed. We need to strive towards a recommended, well-understood set of tools, which will require much more careful research. This especially concerns the meta-issues of comparisons of IML techniques, IML diagnostic tools to warn against misleading interpretations, and tools for analyzing multiple dependent or interacting features."}, {"heading": "Acknowledgements", "text": "This work is funded by the Bavarian State Ministry of Science and the Arts in the framework of the Centre Digitisation.Bavaria (ZD.B) and supported by the German Federal Ministry of Education and Research (BMBF) under Grant No. 01IS18036A and the Graduate School of Systemic Neurosciences (GSN) Munich. The authors of this work take full responsibility for its content."}], "title": "Pitfalls to Avoid when Interpreting Machine Learning Models", "year": 2020}