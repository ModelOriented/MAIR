{
  "abstractText": "The wide adoption of machine learning approaches in the industry, government, medicine and science has renewed the interest in interpretable machine learning: many decisions are too important to be delegated to black-box techniques such as deep neural networks or kernel SVMs. Historically, problems of learning interpretable classifiers, including classification rules or decision trees, have been approached by greedy heuristic methods as essentially all the exact optimization formulations are NP-hard. Our primary contribution is a MaxSAT-based framework, called MLIC, which allows principled search for interpretable classification rules expressible in propositional logic. Our approach benefits from the revolutionary advances in the constraint satisfaction community to solve large-scale instances of such problems. In experimental evaluations over a collection of benchmarks arising from practical scenarios we demonstrate its effectiveness: we show that the formulation can solve large classification problems with tens or hundreds of thousands of examples and thousands of features, and to provide a tunable balance of accuracy vs. interpretability. Furthermore, we show that in many problems interpretability can be obtained at only a minor cost in accuracy. The primary objective of the paper is to show that recent advances in the MaxSAT literature make it realistic to find optimal (or very high quality near-optimal) solutions to large-scale classification problems. We also hope to encourage researchers in both interpretable classification and in the constraint programming community to take it further and develop richer formulations, and bespoke solvers attuned to the problem of interpretable ML.",
  "authors": [
    {
      "affiliations": [],
      "name": "Dmitry Malioutov"
    },
    {
      "affiliations": [],
      "name": "Kuldeep S. Meel"
    }
  ],
  "id": "SP:ccf7295a121fb49a0135585b439d6921829dc124",
  "references": [
    {
      "authors": [
        "R. Andrews",
        "J. Diederich",
        "A. Tickle"
      ],
      "title": "Survey and critique of techniques for extracting rules from trained artificial neural networks",
      "venue": "Knowledge-based systems 8(6), 373\u2013389",
      "year": 1995
    },
    {
      "authors": [
        "P. van Beek",
        "H.F. Hoffmann"
      ],
      "title": "Machine learning of bayesian networks using constraint programming",
      "venue": "Proc. of CP. pp. 429\u2013445",
      "year": 2015
    },
    {
      "authors": [
        "J. Berg",
        "P. Saikko",
        "M. J\u00e4rvisalo"
      ],
      "title": "Improving the effectiveness of sat-based preprocessing for maxsat",
      "venue": "Proc. of IJCAI",
      "year": 2015
    },
    {
      "authors": [
        "D. Bertsimas",
        "A. Chang",
        "C. Rudin"
      ],
      "title": "An integer optimization approach to associative classification",
      "venue": "Adv. Neur. Inf. Process. Syst. 25, pp. 269\u2013277",
      "year": 2012
    },
    {
      "authors": [
        "C. Bessiere",
        "E. Hebrard",
        "B. O\u2019Sullivan"
      ],
      "title": "Minimising decision tree size as combinatorial optimisation",
      "venue": "Proc. of CP. pp. 173\u2013187. Springer",
      "year": 2009
    },
    {
      "authors": [
        "C. Blake",
        "C.J. Merz"
      ],
      "title": "UCI} repository of machine learning databases",
      "year": 1998
    },
    {
      "authors": [
        "E. Boros",
        "P. Hammer",
        "T. Ibaraki",
        "A. Kogan",
        "E. Mayoraz",
        "I. Muchnik"
      ],
      "title": "An implementation of logical analysis of data",
      "venue": "IEEE Transactions on Knowledge and Data Engineering 12(2), 292\u2013306",
      "year": 2000
    },
    {
      "authors": [
        "L. Breiman",
        "J. Friedman",
        "C. Stone",
        "R. Olshen"
      ],
      "title": "Classification and regression trees",
      "venue": "CRC press",
      "year": 1984
    },
    {
      "authors": [
        "P. Clark",
        "T. Niblett"
      ],
      "title": "The CN2 induction algorithm",
      "venue": "Mach. Learn. 3(4), 261\u2013283",
      "year": 1989
    },
    {
      "authors": [
        "W.W. Cohen"
      ],
      "title": "Fast effective rule induction",
      "venue": "Proc. Int. Conf. Mach. Learn. pp. 115\u2013123. Tahoe City, CA",
      "year": 1995
    },
    {
      "authors": [
        "W.W. Cohen",
        "Y. Singer"
      ],
      "title": "A simple, fast, and effective rule learner",
      "venue": "Proc. Nat. Conf. Artif. Intell. pp. 335\u2013342. Orlando, FL",
      "year": 1999
    },
    {
      "authors": [
        "M.W. Craven",
        "J.W. Shavlik"
      ],
      "title": "Extracting tree-structured representations of trained networks",
      "venue": "Proc. of NIPS pp. 24\u201330",
      "year": 1996
    },
    {
      "authors": [
        "M.W. Craven",
        "J.W. Shavlik"
      ],
      "title": "Extracting tree-structured representations of trained networks",
      "venue": "Proc. of NIPS pp. 24\u201330",
      "year": 1996
    },
    {
      "authors": [
        "J. Davies"
      ],
      "title": "Solving MAXSAT by Decoupling Optimization and Satisfaction",
      "venue": "Ph.D. thesis, University of Toronto",
      "year": 2013
    },
    {
      "authors": [
        "J. Davies",
        "F. Bacchus"
      ],
      "title": "Solving maxsat by solving a sequence of simpler sat instances",
      "venue": "Proc. of CP. pp. 225\u2013239",
      "year": 2011
    },
    {
      "authors": [
        "L. De Raedt",
        "T. Guns",
        "S. Nijssen"
      ],
      "title": "Constraint programming for itemset mining",
      "venue": "Proc. of KDD. pp. 204\u2013212",
      "year": 2008
    },
    {
      "authors": [
        "K. Dembczy\u0144ski",
        "W. Kot lowski",
        "R. S lowi\u0144ski"
      ],
      "title": "Ender: a statistical framework for boosting decision rules",
      "venue": "Data Mining and Knowledge Discovery 21(1), 52\u201390",
      "year": 2010
    },
    {
      "authors": [
        "A. Emad",
        "K.R. Varshney",
        "D.M. Malioutov"
      ],
      "title": "A semiquantitative group testing approach for learning interpretable clinical prediction rules",
      "venue": "Proc. Signal Process. Adapt. Sparse Struct. Repr. Workshop, Cambridge, UK",
      "year": 2015
    },
    {
      "authors": [
        "A. Emad",
        "K.R. Varshney",
        "D.M. Malioutov"
      ],
      "title": "A semiquantitative group testing approach for learning interpretable clinical prediction rules",
      "venue": "Proc. Signal Process. Adapt. Sparse Struct. Repr. Workshop, Cambridge, UK",
      "year": 2015
    },
    {
      "authors": [
        "A. Freitas"
      ],
      "title": "Comprehensible classification models: a position paper",
      "venue": "ACM SIGKDD explorations newsletter 15(1), 1\u201310",
      "year": 2014
    },
    {
      "authors": [
        "J.H. Friedman",
        "B.E. Popescu"
      ],
      "title": "Predictive learning via rule ensembles",
      "venue": "The Annals of Applied Statistics pp. 916\u2013954",
      "year": 2008
    },
    {
      "authors": [
        "P. Jawanpuria",
        "S.N. Jagarlapudi",
        "G. Ramakrishnan"
      ],
      "title": "Efficient rule ensemble learning using hierarchical kernels",
      "venue": "Proc. of ICML",
      "year": 2011
    },
    {
      "authors": [
        "B. Letham",
        "C. Rudin",
        "T.H. McCormick",
        "D. Madigan"
      ],
      "title": "Building interpretable classifiers with rules using Bayesian analysis",
      "venue": "Tech. Rep. 609, Dept. Stat., Univ. Washington",
      "year": 2012
    },
    {
      "authors": [
        "D.M. Malioutov",
        "K.R. Varshney"
      ],
      "title": "Exact rule learning via boolean compressed sensing",
      "venue": "Proc. of ICML. pp. 765\u2013773",
      "year": 2013
    },
    {
      "authors": [
        "M. Marchand",
        "J. Shawe-Taylor"
      ],
      "title": "The set covering machine",
      "venue": "Journal of Machine Learning Research 3(Dec), 723\u2013746",
      "year": 2002
    },
    {
      "authors": [
        "A. Morgado",
        "F. Heras",
        "M. Liffiton",
        "J. Planes",
        "J. Marques-Silva"
      ],
      "title": "Iterative and core-guided maxsat solving: A survey and assessment",
      "venue": "Constraints 18(4), 478\u2013534",
      "year": 2013
    },
    {
      "authors": [
        "N. Narodytska",
        "F. Bacchus"
      ],
      "title": "Maximum satisfiability using core-guided maxsat resolution",
      "venue": "AAAI. pp. 2717\u20132723",
      "year": 2014
    },
    {
      "authors": [
        "S. Nijssen",
        "T. Guns",
        "L. De Raedt"
      ],
      "title": "Correlated itemset mining in roc space: a constraint programming approach",
      "venue": "KDD. pp. 647\u2013656. ACM",
      "year": 2009
    },
    {
      "authors": [
        "J.R. Quinlan"
      ],
      "title": "C4",
      "venue": "5: Programming for machine learning. Morgan Kauffmann p. 38",
      "year": 1993
    },
    {
      "authors": [
        "R.L. Rivest"
      ],
      "title": "Learning decision lists",
      "venue": "Mach. Learn. 2(3), 229\u2013246",
      "year": 1987
    },
    {
      "authors": [
        "U. R\u00fcckert",
        "S. Kramer"
      ],
      "title": "Margin-based first-order rule learning",
      "venue": "Mach. Learn. 70(2\u2013 3), 189\u2013206",
      "year": 2008
    },
    {
      "authors": [
        "L.G. Valiant"
      ],
      "title": "Learning disjunctions of conjunctions",
      "venue": "Proc. Int. Joint Conf. Artif. Intell. pp. 560\u2013566. Los Angeles, CA",
      "year": 1985
    },
    {
      "authors": [
        "K.R. Varshney"
      ],
      "title": "Data science of the people, for the people, by the people: A viewpoint on an emerging dichotomy",
      "venue": "Proc. Data for Good Exchange Conf",
      "year": 2015
    },
    {
      "authors": [
        "T. Wang",
        "C. Rudin",
        "F. Doshi-Velez",
        "Y. Liu",
        "E. Klampfl",
        "P. MacNeille"
      ],
      "title": "Or\u2019s of and\u2019s for interpretable classification, with application to context-aware recommender systems",
      "venue": "arXiv preprint arXiv:1504.07614",
      "year": 2015
    },
    {
      "authors": [
        "T. Wang",
        "C. Rudin",
        "Y. Liu",
        "E. Klampfl",
        "P. MacNeille"
      ],
      "title": "Bayesian or\u2019s of and\u2019s for interpretable classification with application to context aware recommender systems",
      "year": 2015
    }
  ],
  "sections": [
    {
      "text": "The primary objective of the paper is to show that recent advances in the MaxSAT literature make it realistic to find optimal (or very high quality near-optimal) solutions to large-scale classification problems. We also hope to encourage researchers in both interpretable classification and in the constraint programming community to take it further and develop richer formulations, and bespoke solvers attuned to the problem of interpretable ML."
    },
    {
      "heading": "1 Introduction",
      "text": "The last decade has witnessed an unprecedented adoption of machine learning techniques to make sense of available data and make predictions to support decision making for a wide variety of applications ranging from health-care analytics to customer churn predictions, movie recommendations and macro-economic\n? The names of authors are sorted alphabetically by last name and the order does not reflect contribution\nar X\niv :1\n81 2.\n01 84\n3v 1\n[ cs\n.A I]\n5 D\nec 2\npolicy. The focus in the machine learning literature has been on increasingly sophisticated systems with the paramount goal of improving the accuracy of their predictions at the cost of making such systems essentially black-box. While in certain tasks such as ad predictions, accuracy is the main objective, in other domains, e.g., in legal, medical, and government, it is essential that the human decision makers who may not have been trained in machine learning can interpret and validate the predictions [20,33].\nThe most popular interpretable techniques that tend to be adopted and trusted by decision makers include classification rules, decision trees, and decision lists [10,29,8,30]. In particular, decision rules with a small number of Boolean clauses tend to be the most interpretable. Such models can be used both to learn interpretable models from the start, and also as proxies that provide post-hoc explanations to pre-trained black-box models [12,1].\nOn the theoretical front, the problem of rule learning was shown to be computationally intractable [32]. Consequently, the earliest practical efforts such as decision list and decision tree approaches relied on a combination of heuristically chosen optimization objectives and greedy algorithmic techniques, and the size of the rule was controlled by either early stopping or ad-hoc rule pruning. Only recently there have been some formulations that attempt to balance the accuracy and the size of the rule in a principled optimization objective either through combinatorial optimization, linear programming (LP) relaxations, submodular optimization, or Bayesian methods [4,25,24][7,34] as we review in Section 5.\nMotivated by the significant progress in the development of combinatorial solvers (in particular, MaxSAT), we ask: can we design a combinatorial framework to efficiently construct interpretable classification rules that takes advantage of these recent advances? The primary contribution of this paper is to present a combinatorial framework that enables a precise control of accuracy vs. interpretability, and to verify that the computational advances in the MaxSATcommunity can make it practical to solve large-scale classification problems.\nIn particular, this paper makes following contributions:\n1. A MaxSAT-based framework, MLIC, that provably trades off accuracy vs. interpretability of the rules 2. A prototype implementation of MLIC based on MaxSAT that is capable of finding optimal (or high-quality near-optimal) classification rules from modern large-scale data-sets 3. We show that in many classification problems interpretability can be achieved at only a minor loss of accuracy, and furthermore,MLIC, which specifically looks for interpretable rules, can learn from much fewer samples than blackbox ML techniques.\nFurthermore, we hope to share our excitement with applications of constraint programming/MaxSAT in Machine Learning, and to encourage researchers in both interpretable classification and in the CSP/SAT communities to consider this topic further: both in developing new SAT-based formulations for interpretable ML, and in designing bespoke solvers attuned to the problem of interpretable ML.\nThe rest of the paper is organized as follows: We discuss notations and preliminaries in Section 2. We then presentMLIC, which is the primary contribution of this paper, in Section 3 and follow up with experimental setup and results over a large set of benchmarks in Section 4. We then discuss related work in Section 5 and finally conclude in Section 7."
    },
    {
      "heading": "2 Preliminaries",
      "text": "We use capital boldface letters such as X to denote matrices while lower boldface letters y are reserved for vectors/sets. For a matrix X, Xi represents i-th row of X while for a vector/set y, yi represents i-th element of y.\nLet F be a Boolean formula and b = {b1, b2, \u00b7 \u00b7 \u00b7 bn} be the set of variables appearing in F . A literal is a variable (bi) or its complement(\u00acbi). A satisfying assignment or a witness of F is an assignment of variables in b that makes F evaluate to true. If \u03c3 is an assignment of variables and bi \u2208 b, we use \u03c3(bi) to denote the value assigned to bi in \u03c3. F is in Conjunctive Normal Form (CNF) if F := C1 \u2227 C2 \u00b7 \u00b7 \u00b7Cm, where each clause Ci is represented as disjunction of literals. We use |Ci| to denote the number of literals in Ci. For two vectors u and v over propositional variable/constants, we define u\u2228v = \u2228 i(ui\u2227vi), where ui and vi denote variables/constants at i-th index of u and v respectively. In this context, note that the operation \u2227 between a variable and a constant follows standard interpretation, i.e. 0 \u2227 b = 0 and 1 \u2227 b = b .\nWe consider standard binary classification, where we are given a collection of training samples {Xi, yi} where each vector Xi \u2208 X contains valuation of the features x = {x1, x2, \u00b7 \u00b7 \u00b7xm} for sample i, and yi \u2208 {0, 1} is the binary label for sample i. A classifier R is a mapping that takes in a feature vector x and return a class y, i.e. y = R(x). The goal is not only to design R to approximate our training set, but also to generalize to unseen samples arising from the same distribution. In this work, we restrict x and y to be Boolean3 and focus on classifiers that can be expressed compactly in Conjunctive Normal Form (CNF). We use Ci to denote the ith clause of R. Furthermore, we use |R| to denote the sum of the counts of literals in all the clauses, i.e. |R| = \u03a3i|Ci|.\nIn this work, we focus on weighted variant of CNF wherein a weight function is defined over clauses. For a clause Ci and weight functionW (\u00b7), we useW (Ci) to denote the weight of clause Ci. We say that a clause Ci is hard if W (Ci) = \u2212\u221e, otherwise Ci is called as soft clause. To avoid notational clutter, we overload W (\u00b7) to denote the weight of an assignment or clause, depending on the context. We define weight of an assignment \u03c3 as the sum of weight of clauses that \u03c3 does not satisfy. Formally, W (\u03c3) = \u03a3i|\u03c3 6|=CiW (Ci).\nGiven F and weight function W (\u00b7), the problem of MaxSAT is to find an assignment \u03c3\u2217 that has the maximum weight, i.e. \u03c3\u2217 = MaxSAT(F,W ) if \u2200\u03c3 6= \u03c3\u2217,W (\u03c3\u2217) \u2265 W (\u03c3). Our formulation will have negative clause weights, hence MaxSATcorresponds to satisfying as many clauses as possible, and picking the\n3 We discuss in Section 3 that such a restriction can be achieved without loss of generality\nweakest clauses among the unsatisfied ones. Note that the above formulation is different from the typical definition of MaxSAT but the difference is only syntactic. Borrowing terminology of community focused on developing MaxSAT solvers, we are solving a partial weighted MaxSAT instance wherein we mark all the clauses with \u2212\u221e weight as hard and negate weight of all the other clauses and ask for a solution that optimizes the partial weighted MaxSAT formula. The knowledge of inner working of MaxSAT solvers and encoding of our representation into weighted MaxSAT is not required for this paper and we defer the details to release of source code post-publication.\n3 MLIC: MaxSAT-based Learning of Interpretable Classifiers\nWe now discuss the primary technical contribution of this paper,MLIC: MaxSATbased Learning of Interpretable Classifiers. We first describe a metric for interpretability of CNF rules. Since our formulation employs binary features, we discuss how non-binary features such as categorical and continuous features can be represented as binary features. We then move on to formulate the problem of learning interpretable classification rules as a MaxSAT query and provide a proof of its theoretical soundness regarding controlling sparsity of the rules. As discussed in Section 5, prior work does not provide a sound procedure for controlling sparsity and accuracy. We then discuss the representational power of our CNF framework \u2013 in particular, we demonstrate that the proposed framework generalizes to handle complex objective function and rules in forms other than CNF."
    },
    {
      "heading": "3.1 Balancing Accuracy and Intrepretability",
      "text": "While in general interpretability may be hard to define precisely, in the context of decision rules, an effective proxy is merely the count of clauses or literals used in the rule. Rules involving few clauses with few literals are natural for humans to evaluate and understand, while complex rules involving hundreds of clauses will not be interpretable even if the individual clauses are. In addition to interpretability, such sparsity also controls model complexity and gives a handle of the generalization error.4\nFirst, suppose that there exists a rule R that perfectly classifies all the examples, i.e. \u2200i, yi = R(Xi). Among all possible functions that satisfy this we would like to find the most interpretable (sparse) one:\nmin R |R| such that R(Xi) = yi, \u2200i\nSince most ML datasets do not allow perfect classification, we introduce a penalty on classification errors. We balance the two terms by a parameter \u03bb,\n4 The framework proposed in this paper allows generalization to other forms of rules, as we discuss in Section 3.6.\nwhere large \u03bb gives more accurate but more complex rules, and smaller \u03bb gives smaller rules at the cost of reduced accuracy. Let ER be the set of examples on which our classifier R makes an error, then our objective is5:\nmin R |R|+ \u03bb|ER| such that R(Xi) = yi, \u2200i /\u2208 ER (1)"
    },
    {
      "heading": "3.2 Discretization of Features",
      "text": "In our MaxSAT-based formulation, we focus on learning rules based on Boolean variables. We do also allow categorical and continuous features for our classifier, which are pre-processed before being presented to the MaxSAT-formulation. To handle categorical features one may use the common \u2018one-hot\u2019 encoding, where a Boolean vector variable is introduced with the cardinality equal to the number of categories. For example a categorical feature with values \u2019red\u2019, \u2019green\u2019, \u2019blue\u2019 would get converted to three binary variables, which take values 100, 010, and 001 for the three categorical values. [[[ If you think this sentence is obvious \u2013 please drop ]]]\nFor continuous features, we introduce discretization, by comparing feature values to a collection of thresholds. The thresholds may be chosen for example based on quantiles of their distribution, or alternatively, on uniform partition of the range of feature values. Specifically, for a continuous feature xc we consider a number of thresholds {\u03c4k} and define two separate Boolean features I[xc \u2265 \u03c4k] and I[xc < \u03c4k] for each \u03c4k. The number of thresholds may vary by feature. Thus, each continuous feature is represented using a collection of 2q Boolean features, where q is the number of thresholds.\nIn principle, one could use all the values occurring in the data as thresholds, and this would be equivalent to the original continuous features. In practice, however, such granularity is typically not necessary, and a handful of thresholds could be used, e.g., age-groups for each 5 years to discretize a continuous age variable. This typically leads to only a very minor (if any) loss in accuracy, and in fact improves the presentations and understanding of the rules to human users. In our experiments, we used 10 thresholds based on the quantiles of the feature distribution (10-th, 20-th, ... 100-th percentile), unless the number of unique values of the feature was less than 10, in which case we kept all of them.\nWe note that we could easily define arbitrary other Boolean functions of continuous or categorical variables within our framework. For example, categorical variables with many possible values (e.g. states or countries) may be grouped into more interpretable coarser units ( regions or continents). Such groupings are application specific and wpuld typically require relevant domain knowledge. They could perhaps be learned from data, but this is outside the scope of the current paper.\n5 Cost-sensitive classification is defined analogously by allowing a separate parameter for false positives and false negatives."
    },
    {
      "heading": "3.3 Transformation to Max-SAT query",
      "text": "We now describe our Max-SAT formulation for learning interpretable rules. MLIC takes in four inputs: (i) a (0,1)-matrix X of dimension n\u00d7m describing values of all m features for n samples with Xi corresponding to feature vector x = {x1, x2, \u00b7 \u00b7 \u00b7xm} for sample i, (ii) (0,1)-vector y containing class labels yi for sample i, (iii) k, the desired number of clauses in CNF rule, (iv) the regularization parameter \u03bb. Consequently, MLIC constructs a MaxSAT query and invokes a MaxSAT solver to compute the underlying rule R as we now describe.\nThe key idea ofMLIC is to define a MaxSAT query over k\u00d7m propositional variables, denoted by {b11, b21, \u00b7 \u00b7 \u00b7 bm1 \u00b7 \u00b7 \u00b7 bmk }, such that every truth assignment \u03c3 defines a k-clause CNF rule R, where feature xj appears in clause Ri if \u03c3(bji ) = 1. Corresponding to every sample i, we introduce a noise variable \u03b7i that is employed to distinguish whether the labeling for sample i should be considered as noise or not. Let Bi = {bji | j \u2208 [m]}.\nThe Max-SAT query constructed by MLIC consists of the following three sets of constraints:\n1. Ni := (\u00ac\u03b7i); W (Ni) = \u2212\u03bb 2. V ji := (\u00acb j i ); W ( V ji ) = \u22121\n3. Di := (\u00ac\u03b7i \u2192 (yi \u2194 \u2227k l=1(Xi \u2228Bl)));W (Di) = \u2212\u221e\nPlease refer to Section 2 for the interpretation of (Xi \u2228Bj). Finally, the set of constraints Qk constructed by MLIC is defined as follows:\nQk := n\u2227 i=1 Ni \u2227 i=k,j=m\u2227 i=1,j=1 V ji \u2227 n\u2227 i=1 Di (2)\nNote that the elements of Xi and yi are not variables but constants whose values (0 or 1) are provided as inputs. Therefore, the set of variables for Qk is {\u03b71, \u03b72, \u00b7 \u00b7 \u00b7 , \u03b7n, b11, b21, \u00b7 \u00b7 \u00b7 bm1 \u00b7 \u00b7 \u00b7 bmk }. We now explain the intuition behind the design of Qk.\nWe assign a weight of \u2212\u03bb to every Ni as we would like to satisfy as many Ni, i.e. falsify as many \u03b7i as possible. Similarly, we assign a weight of \u22121 to every clause V ji as we are, again, interested in sparse solutions (i.e., ideally, we would prefer as many V ji to be satisfied as possible). Every clause Di can be read as follows: if \u03b7i is assigned to false, i.e. sample i is not considered as noise, then yi = R. As noted in Section 2, equivalent representation of the W (\u00b7), as described above, for MaxSAT solvers involves usage of hard clauses.\nNext, we extract R from the solution of Qk as follows.\nConstruction 1 Let \u03c3\u2217 = MaxSAT(Qk,W ), then xj \u2208 Ri iff \u03c3\u2217(bji ) = 1.\nBefore proceeding further, it is important to discuss CNF encodings for the above sets of constraints. The constraints arising from Ni and Vi are unit clauses and do not require further processing. Furthermore, note that yi is already\nknown and is a constant. Therefore, when yi is 1, the constraint Di can be directly encoded as CNF by using equivalence of (a \u2192 b) \u2261 (\u00aca \u2228 b). Finally, when yi is 0, we use Tseitin encoding wherein we introduce an auxiliary variable zji corresponding to each clause (Xi \u2228Bj). Formally, we replace Di := (\u00ac\u03b7i \u2192 ( \u2228k j=1 \u00ac(Xi\u2228Bj))) with \u2227k j=0D j i where D 0 i := (\u00ac\u03b7i \u2192 \u2228 j z j i )), and D j i := (z j i \u2192\n\u00ac(Xi \u2228Bj). Furthermore, W ( Dji ) = \u2212\u221e. The following lemma establishes the theoretical soundness of parameter \u03bb.\nLemma 1. For all \u03bb2 > \u03bb1 > 0, if R1 \u2190 MLIC(X,y, k, \u03bb1) and R2 \u2190 MLIC(X,y, k, \u03bb2), then |R1| \u2264 |R2| and ER1 \u2265 ER2 .\nProof. First, note that construction of Qk depends only on X and y. Furthermore, the parameter \u03bb influences only the associated weight function. We denote weight functions corresponding to \u03bb1 and \u03bb2 as W\u03bb1 and W\u03bb2 respectively. Furthermore, let \u03c31 = MaxSAT(Q k,W\u03bb1) and \u03c32 = MaxSAT(Q k,W\u03bb1). If \u03c31 = \u03c32, the lemma trivially holds. We now complete proof by contradiction argument for the case when \u03c31 6= \u03c32.\nLet |R1| > |R2|. As \u03c31 6= \u03c32, we have W\u03bb2(\u03c31) \u2264 W\u03bb2(\u03c32). Since W\u03bb(\u03c3) = |R| + \u03bbER, where R is extracted from \u03c3 as stated above. Therefore, we have \u03bb2(ER2 \u2212 ER1) \u2265 |R1| \u2212 |R2|. But we also have W\u03bb1(\u03c31) \u2264 W\u03bb1(\u03c32), which implies that \u03bb1(ER2 \u2212ER1) \u2264 |R1|\u2212 |R2|. Since \u03bb1 > \u03bb2, we have contradiction. Therefore, it must be the case that |R1| \u2264 |R2|."
    },
    {
      "heading": "3.4 Illustrate Example",
      "text": "We illustrate our encoding with the help of a toy example. Let n = 2,m = 3, k =\n2 and X = 1 0 1 0 1 1  and y = 0 1 . Then we have following clauses: N1 := (\u00ac\u03b71); N2 := (\u00ac\u03b72);\nV 11 = (\u00acb11); V 21 = (\u00acb21); V 31 = (\u00acb31);\nV 12 = (\u00acb12); V 22 = (\u00acb22); V 32 = (\u00acb32);\nD1 := (\u00ac\u03b71 \u2192 (\u00ac(b11 \u2228 b31) \u2228 \u00ac(b12 \u2228 b32));\nD2 := (\u00ac\u03b72 \u2192 ((b21 \u2228 b31) \u2227 (b22 \u2228 b32))"
    },
    {
      "heading": "3.5 Beyond CNF Rules",
      "text": "While CNF formulas are general enough to express every Boolean formula, the length of representation may not be polynomial size. Therefore, one might wonder if we can extend MLIC to learn rules in other canonical forms as well. In fact, early CSP based approaches to rule learning focused on rules in DNF form.\nWe now show that with a minor change, we are able to learn rules expressible in DNF. Suppose that we are interested in learning a rule S that is expressible in DNF, such that y = S(x), where S is a DNF formula. We note that (y = S(x)) \u2194 \u00ac(y = \u00acS(x)). And if S is a DNF formula, then \u00acS is a CNF formula. Therefore, to learn rule S, we simply callMLIC with \u00acy as input and negate the learned rule."
    },
    {
      "heading": "3.6 Complex Objective Functions",
      "text": "We now discuss howMLIC can be easily extended to handle complex objective functions. The objective function for MLIC as defined in Equation 1 treats all features equally. In some cases, the user might prefer rules that contain certain features. Such an extension is fairly easy to achieve as we need only to change the weight function corresponding to clauses V ji . Furthermore, in certain cases, one might want to minimize the total number of different features across different clauses rather than minimize the total number of terms. Such an extension is fairly easy to handle as we can simply replace \u2227k j=1 V j i with V\u0302i where\nV\u0302i = ( \u2228k j=1 \u00acb j i ). It is worth noting that the proposed modifications impact only the MaxSAT query and does not require any modifications to the underlying MaxSAT solver. We believe that such a separation is a key strength of MLIC as it separates modeling and solving completely."
    },
    {
      "heading": "4 Evaluation",
      "text": "To evaluate the performance ofMLIC, we implemented a prototype implementation in Python that employs MaxHS [15] to handle MaxSAT instances. We also experimented with LMHS [3], another state of the art MaxSAT solver and MaxHS outperformed LMHS for our benchmarks 6. We conducted an extensive set of experiments on diverse publicly available benchmarks, seeking to answer the following questions7:\n1. Do advancements in MaxSAT solving enableMLIC to be run with datasets involving tens of thousands of variables with thousands of binary features? 2. How does the accuracy of MLIC compare to that of state of the art but typically non-interpretable classifiers? 3. How does the accuracy of MLIC vary with the size of training set? 4. How does the accuracy of MLIC vary with \u03bb? 5. How does the size of learnt rules of MLIC vary with \u03bb?\nIn summary, our experiments demonstrate that MLIC can handle datasets involving tens of thousands of variables with thousands of binary features. Furthermore, MLIC can generate rules that are not only interpretable but with 6 A detailed evaluation among different MaxSAT solvers is beyond the scope of this\nwork and left for future work 7 The source code of MLIC and benchmarks can be viewed at https://github.com/ meelgroup/mlic\naccuracy comparable to that of other competitive classifiers, which often produce hard to interpret rules/models. We demonstrate that MLIC is able to achieve sufficiently high accuracy with very few samples."
    },
    {
      "heading": "4.1 Experimental Methodology",
      "text": "We conducted extensive experiments on publicly available data sets obtained from UCI repository [6]. The data sets involved both real- and categoricalvalued features. Specifically, the specific datasets are: buzz events from two different social networks: Twitter, Tom\u2019s Hardware, Adult Data (adult data), Credit Approval Data Set (credit data), Ionosphere (Ionos), Pima Indians Diabetes (PIMA), Parkinsons, connectionist bench sonar (Sonar), blood transfusion service center (Trans), and breast cancer Wisconsin diagnostic (WDBC).\nFor purposes of comparison of the accuracy of MLIC, we considered a variety of popular classifiers: `1-penalized Logistic regression (LogReg), Nearest\nneighbors classifier (NN), and the black box random forests (RF), and support vector classification (SVC).\nWe perform 10-fold cross-validation to perform an assessment of accuracy on a validation set. We compute the mean across the 10 folds for each choice of a regularization (or complexity control) parameter for each technique (baseline and MLIC), and report the best cross-validation accuracy. The number of parameter values is comparable ( 10) for each technique. For RF and RIPPER we use control based on the cutoff of the number of examples in the leaf node. For SVC and LogReg we discretize the regularization parameter on a logarithmic grid. In case of MLIC we have 2 choices of \u03bb \u2208 {1, 10} and number of clauses, k \u2208 {1, 2, 3} and the type of rule as {CNF, DNF}. We set the training time cutoff for each classifier (on each fold) to be 2000 seconds. Again, note that some classifiers can be much faster than others, but in this paper we focus on the best tradeoff of accuracy vs interpretability in mission-critical settings, and the training time (which can be off-line) is secondary, as long as it is realistic. In this context, note that testing time for each of these techniques is less than 0.01 seconds for a given set of labels."
    },
    {
      "heading": "4.2 Illustrative Example",
      "text": "We illustrate the interpertable rules that are computed by MLIC on the iris data set, which is a simple benchmark and widely used by machine learning community to illustrate new classification techniques. We consider the binary problem of classifying iris versicolor from the other two species, setosa and virginica. Of the four features, sepal length, sepal width, petal length, and petal width, we learn the following rule: R:=\n1. (sepal length > 6.3 \u2228 sepal width > 3.0 \u2228 petal width <= 1.5 ) \u2227 2. ( sepal width <= 2.7 \u2228 petal length > 4.0 \u2228 petal width > 1.2 ) \u2227\n3. ( petal length <= 5.0)\nLet us pause a bit to understand how to apply the above rule. The above rule implies that when the three constraints are satisfied, the flower must be classified as Iris otherwise, non-iris. The size of the above rule, i.e. |R| = \u03a3i|Ci| = 3 + 3 + 1 = 7.\n4.3 Results\nTable 1 presents results of comparison ofMLIC vis-a-vis typical non-interpretable classifiers. The first three columns list the name, size (number of samples) and the number of binary features for each Dataset. The next five columns present test accuracy of the classifiers RIPPER, Logistic Regression (Log Reg), Nearest Neighbor (NN), Random Forest (RF), and SVC. The final column contain the median test accuracy forMLIC. For every cell in the last five columns, the top value represents the accuracy, while the value sorrounded by parenthesis represent average training time. We draw the following two conclusions from the table: First,MLIC is able to handle datasets with tens of thousands of examples with hundreds of features. The scalability of MLIC demonstrates the potential presented by remarkable progress in SAT solving. Recent research efforts have often used NP-hardness of the problem to justify the usage of heuristics but our experience withMLIC shows that SAT solving is able to solve many large-scale problems directly. Note that when MaxHS times out, it is able to provide the best solution found so far. In this context, it is worth noting that for some of the benchmarks, even state of the art classifiers such as SVC time out. Secondly, MLIC is often able to achieve accuracy that is sufficiently close to accuracy\nachieved by typical non-interpretable classifiers but produces easy to state rules that often have just a few literals.\nTo demonstrateMLIC\u2019s ability to compute easy to state rules in comparison to the state of the art classifiers such as RIPPER, we computed the size of rules returned by RIPPER and MLIC. Table 2 presents results of comparison of MLIC vis-a-vis RIPPER. The first three columns list the name, size (number of samples) and the number of binary features for each Dataset. The next two columns state the median size of rules returned by RIPPER and MLIC. The size of a rule is computed as the number of terms involved in a rule. First, note that except for two cases where RIPPER has produced marginally shorter rules compared toMLIC,MLIC produces significantly shorter rules and sometimes, these rules could be orders of magnitude larger than those produced byMLIC. For example, for Toms hardware, the rule produced by RIPPER has 57 terms compared to just 4 literals forMLIC. Note that withMLIC has better accuracy than RIPPER. One might wonder if the rule learned by RIPPER could have been simply transformed into a sparser rule; it is not the case here. Furthermore, it is worth noting that RIPPER does not provide sound handle to tune rule size and therefore, user is left to trying out combination of input parameters without any guarantee of improvement of the interpretability of generated rules, which we experienced in this case. A in-depth study into failure of RIPPER to generate sparser rules than MLIC is beyond the scope of this work.\nTo measure the accuracy ofMLIC w.r.t. the size of training data, we consider test errors when only a fraction of training data is available (we vary it from 10 % to 90 % in steps of 10 %). . Due to lack of space, we present result for only one benchmark, WDBC, for \u03bb = 1 and 5 and k = 1 in Figure 1. We plot median training and test accuracy ofMLIC over 10 trials, which is also known as learning curve in machine learning literature. The y-axis represents the error as the ratio of incorrect predictions to total examples while the x-axis represents the size of training set. The plot shows how training and test error vary for \u03bb = 1 and 5. Note that MLIC is able to achieve sufficiently high test accuracy with just 40% of the complete dataset. We observe similar behavior for other benchmarks as well.\nFigures 2 and 3 illustrate how training accuracy and rule sizes vary with \u03bb for one of the representative benchmark, parkinsons. CNF1, CNF2, DNF1, DNF2 refer to invocations of MLIC with (rule type, k) set to (CNF, 1), (CNF, 2), (DNF,1), and (DNF,2) respectively. For each of the plots, x-axis refers to the value of \u03bb while y-axis represents Rule size (i.e. |R|) and accuracy for Figure 3 and Figure 2 respectively. First, note that for both CNF and DNF, the accuracy of rules is generally higher for larger k. Significantly, the plots clearly demonstrate monotonicity of rule size and accuracy with respect to \u03bb. In contrast, the state of the art interpretable classifier, RIPPER, can lead to rules that can be order of magnitude larger than those produced by MLIC. For example, for Toms hardware, the rule produced by RIPPER has 57 terms compared to just 4 literals for MLIC. In this context, it is worth noting that RIPPER does not provide sound handle to tune rule size and therefore, user is left to trying out combination\nof input parameters without any guarantee of improvement of the interpretability of generated rules."
    },
    {
      "heading": "5 Related Work",
      "text": "There is a long history of learning interpretable classification models from data, including popular approaches such as decision trees [5,29], decision lists [30], and classification rules [10]. While the form of such classifiers is highly amenable to human interpretation, unfortunately, most of the objective functions that arise for these problems are intractable combinatorial optimization problems. Hence, most popular existing approaches rely on various greedy heuristics, pruning, and ad-hoc local criteria such as maximizing information gain, coverage, e.t.c. For example vaious popular decision rule approaches, such as C4.5.rules [29], CN2 [9], RIPPER [10], SLIPPER [11], all make different trade-offs in how they use these heuristic criteria for growing and pruning the rules.\nRecent advances in large-scale optimization and scalable Bayesian inference gave rise to state-of-the-art black box models. However, many of the same advances can also be used in the context of interpretable machine learning models. Some of such recent proposals include Bayesian approaches [23,35], constraint programming [2], integer programming approaches to learn decision trees [4], quadratic programming relaxation with a variance-penalized margin objective [31]. Greedy approaches are used with a principled objective function in ENDER [17] and Set covering machines [25]. [22] propose a hierarchical kernel learning approach and [21] use optimization to combine basic Boolean clauses obtained from decision trees. Linear Programming relaxations based on Boolean Compressed Sensing formulation have been used to learn sparse interpretable rules\nand checklists8 in [24,18]. Prior work has considered applications of constraint programming to learning Bayesian networks [2] and itemset mining [16,28]. In contrast, we focus on learning sparse interpretable classification rules allowing control of accuracy vs. interpretability."
    },
    {
      "heading": "6 Extensions",
      "text": "In the paper, we have focused on decision rules in the DNF or CNF form, which is among the most interpretable classification methods available. We now describe a few related classification formulations, which are also amenable to being learned from data using a SAT-based framework. A simple AND-clause can be considered as a requirement that all of the N literals in the clause are satisfied, while a simple OR-clause requires that at least 1 of the N literals are satisfied. A useful generalization is a \u201cK-of-N\u201d clause [13], which is true when at least K of the N literals are satisfied. In particular, it leads to a very popular decision rubric called checklists or scorecards, widely used in medicine and finance, where a questionnaire asks some questions (e.g., risk factors), and the total number of positive answers is compared to a pre-determined threshold. LP relaxations have been considered for learning scorecards from data [19], and our MaxSAT-based framework can be directly extended. In the case of multi-class classification, a decision rule may be ambiguous, as it does not specify what multi-class label to\n8 Note, however, that the objective functions for the integer program and the LP relaxation in these papers are not the same as sparsity-penalized cost-sensitive classification error.\nuse when several contradictory clauses pointing to different labels are satisfied simultaneously. Decision lists [30] enforces an order of evaluation of the rules, resolving this ambiguity. Bayesian frameworks for learning decision lists have been considered recently [23]. Perhaps the most well known interpretable classification scheme is a decision tree, where literals are arranged as nodes in a binary tree, and a decision is made by following the path from the root node to one of the leafs. The decision tree can be converted to an equivalent set of classification rules which correspond to all the paths from the root to the leafs, a more expensive representation. On the other side, however, certain small decision rules can lead to very complex decision trees, for example, the \u201dK-of-N\u201d rule cannot be efficiently encoded using a decision tree. Recent work has considered combinatorial optimization to learn compact interpretable decision trees [4]. Beyond simple Boolean expressions, a variety of weighted classification methods can be used, for example, a weighted linear combination of simple AND clauses \u2013 for instance by using Boosting on a set of classifiers based on simple logical clauses. In future work, we plan to extend our MaxSAT-based framework for all these related interpretable classification approaches."
    },
    {
      "heading": "7 Conclusion",
      "text": "We proposed a new approach to learn interpretable classification rules via reduction to (MaxSAT). Due to the impressive advances in MaxSAT-solving, our formulation can find optimal or near-optimal rules balancing accuracy and interpretability (sparsity) for large data-sets involving tens or hundreds of thousands of data points, and hundreds or thousands of features. Furthermore, the approach separates the modeling from the optimization, and this framework could be used to solve a wide variety of interpretable classification formulations, including decision lists, decision trees, and decision rules with different cost functions (including group-sparsity, sharing of the variables, and having prior knowledge on variable importance). Finally, we demonstrate on experiments that for many classification problems interpretability does not have to come at a high cost in terms of accuracy.\nFurthermore, we hope to share our excitement with applications of constraint programming/MaxSAT in Machine Learning, and to encourage researchers in both interpretable classification and in the CSP/SAT communities to consider this topic further: both in developing new SAT-based formulations for interpretable ML, and in designing bespoke solvers attuned to the problem of interpretable ML.\nAcknowledgements This work was supported in part by NUS ODPRT Grant, R-252-000-685-133 and IBM PhD Fellowship. The computational work for this article was performed on resources of the National Supercomputing Centre, Singapore https://www.nscc.sg"
    }
  ],
  "title": "MLIC: A MaxSAT-Based framework for learning interpretable classification rules",
  "year": 2018
}
