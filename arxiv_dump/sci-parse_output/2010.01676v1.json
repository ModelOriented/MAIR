{"abstractText": "Procedural Content Generation via Machine Learning (PCGML) refers to a group of methods for creating game content (e.g. platformer levels, game maps, etc.) using machine learning models. PCGML approaches rely on black box models, which can be difficult to understand and debug by human designers who do not have expert knowledge about machine learning. This can be even more tricky in co-creative systems where human designers must interact with AI agents to generate game content. In this paper we present an approach to explainable artificial intelligence in which certain training instances are offered to human users as an explanation for the AI agent\u2019s actions during a co-creation process. We evaluate this approach by approximating its ability to provide human users with the explanations of AI agent\u2019s actions and helping them to more efficiently cooperate with the AI", "authors": [{"affiliations": [], "name": "Faraz Khadivpour"}, {"affiliations": [], "name": "Matthew Guzdial"}], "id": "SP:910b5d79d5b576ed77c53762644e45d230b43e7d", "references": [{"authors": ["Isard"], "title": "Tensorflow: A system for largescale machine learning. In 12th {USENIX} symposium on operating systems design and implementation ({OSDI", "year": 2016}, {"authors": ["Adadi", "A. Berrada 2018] Adadi", "M. Berrada"], "title": "Peeking inside the black-box: A survey on explainable artificial intelligence (xai)", "venue": "IEEE Access", "year": 2018}, {"authors": ["Bach"], "title": "Controlling explanatory heatmap resolution and semantics via decomposition depth", "venue": "IEEE International Conference on Image Processing (ICIP),", "year": 2016}, {"authors": ["Baldwin"], "title": "Mixed-initiative procedural generation of dungeons using game design patterns", "venue": "IEEE Conference on Computational Intelligence and Games (CIG),", "year": 2017}, {"authors": ["Biran", "O. Cotton 2017] Biran", "C. Cotton"], "title": "Explanation and justification in machine learning: A survey", "venue": "In IJCAI-17 workshop on explainable AI (XAI),", "year": 2017}, {"authors": ["Boz", "O. Hillman 2000] Boz", "D. Hillman"], "title": "Converting a trained neural network to a decision tree dectext-decision tree extractor. Citeseer", "year": 2000}, {"authors": ["Che"], "title": "Distilling knowledge from deep networks with applications to healthcare", "year": 2015}, {"authors": ["Cook"], "title": "Framing in computational creativitya survey and taxonomy", "venue": "In ICCC,", "year": 2019}, {"authors": ["Cortez", "P. Embrechts 2011] Cortez", "M.J. Embrechts"], "title": "Opening black box data mining models using sensitivity analysis", "venue": "IEEE Symposium on Computational Intelligence and Data Mining (CIDM),", "year": 2011}, {"authors": ["Cortez", "P. Embrechts 2013] Cortez", "M.J. Embrechts"], "title": "Using sensitivity analysis and visualization techniques to open black box data mining models", "venue": "Information Sciences", "year": 2013}, {"authors": ["Dazeley Cruz", "F. Vamplew 2019] Cruz", "R. Dazeley", "P. Vamplew"], "title": "Memory-based explainable reinforcement learning", "venue": "In Australasian Joint Conference on Artificial Intelligence,", "year": 2019}, {"authors": ["Dabkowski", "P. Gal 2017] Dabkowski", "Y. Gal"], "title": "Real time image saliency for black box classifiers", "venue": "In Advances in Neural Information Processing Systems,", "year": 2017}, {"authors": ["Dahlskog", "S. Togelius 2014] Dahlskog", "J. Togelius"], "title": "A multi-level level generator", "venue": "In 2014 IEEE Conference on Computational Intelligence and Games,", "year": 2014}, {"authors": ["K. Compton"], "title": "Mixed-initiative creative interfaces", "venue": "Proceedings of the 2017 CHI Conference Extended Abstracts on Human Factors in Computing Systems, 628\u2013635.", "year": 2017}, {"authors": ["Ehsan"], "title": "Rationalization: A neural machine translation approach to generating natural language explanations", "venue": "In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society,", "year": 2018}, {"authors": ["Erhan"], "title": "Visualizing higher-layer features of a deep network", "year": 2009}, {"authors": ["Courville Erhan", "D. Bengio 2010] Erhan", "A. Courville", "Y. Bengio"], "title": "Understanding representations learned in deep architectures. Department dInformatique et Recherche Operationnelle, University of Montreal, QC", "year": 2010}, {"authors": ["Fong", "R.C. Vedaldi 2017] Fong", "A. Vedaldi"], "title": "Interpretable explanations of black boxes by meaningful perturbation", "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "year": 2017}, {"authors": ["S.", "A. Fern\u00e1ndez", "F. Herrera"], "title": "Enhancing the effectiveness and interpretability of decision tree and rule induction classifiers with evolutionary training set selection over imbalanced problems", "venue": "Applied Soft Computing", "year": 2009}, {"authors": ["Ghorbani", "A. Zou 2019] Ghorbani", "J. Zou"], "title": "Data shapley: Equitable valuation of data for machine learning", "venue": "arXiv preprint arXiv:1904.02868", "year": 2019}, {"authors": ["Verma Guan", "L. Kambhampati 2020] Guan", "M. Verma", "S. Kambhampati"], "title": "Explanation augmented feedback in human-in-the-loop reinforcement learning", "venue": "arXiv preprint arXiv:2006.14804", "year": 2020}, {"authors": ["Guzdial"], "title": "A general level design editor for cocreative level design", "venue": "In Thirteenth Artificial Intelligence and Interactive Digital Entertainment Conference", "year": 2017}, {"authors": ["Guzdial"], "title": "Explainable pcgml via game design patterns", "venue": "arXiv preprint arXiv:1809.09419", "year": 2018}, {"authors": ["Guzdial"], "title": "2019. Friend, collaborator, student, manager: How design of an ai-driven game level editor affects creators", "venue": "In Proceedings of the 2019 CHI Conference on Human Factors", "year": 2019}, {"authors": ["Liao Guzdial", "M. Riedl 2018] Guzdial", "N. Liao", "M. Riedl"], "title": "Co-creative level design via machine learning", "venue": "arXiv preprint arXiv:1809.09420", "year": 2018}, {"authors": ["Hara", "S. Hayashi 2018] Hara", "K. Hayashi"], "title": "Making tree ensembles interpretable: A bayesian model selection approach", "venue": "In International Conference on Artificial Intelligence and Statistics,", "year": 2018}, {"authors": ["Jain"], "title": "2016. Autoencoders for level generation, repair, and recognition", "venue": "In Proceedings of the ICCC Workshop on Computational Creativity and Games", "year": 2016}, {"authors": ["Khalifa"], "title": "Pcgrl: Procedural content generation via reinforcement learning", "venue": "arXiv preprint arXiv:2001.09212", "year": 2020}, {"authors": ["H. Kumar"], "title": "Explainable ai: Deep reinforcement learning agents for residential demand side cost savings in smart grids. arXiv preprint arXiv:1910.08719", "year": 2019}, {"authors": ["Letham"], "title": "Interpretable classifiers using rules and bayesian analysis: Building a better stroke prediction model", "venue": "The Annals of Applied Statistics", "year": 2015}, {"authors": ["Yannakakis Liapis", "A. Togelius 2013] Liapis", "G.N. Yannakakis", "J. Togelius"], "title": "Sentient sketchbook: computer-assisted game level authoring", "year": 2013}, {"authors": ["Madumal"], "title": "Explainable reinforcement learning through a causal lens. arXiv preprint arXiv:1905.10958", "year": 2019}, {"authors": ["Nguyen"], "title": "Plug & play generative networks: Conditional iterative generation of images in latent space", "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "year": 2017}, {"authors": ["Yosinski Nguyen", "A. Clune 2015] Nguyen", "J. Yosinski", "J. Clune"], "title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images", "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,", "year": 2015}, {"authors": ["Yosinski Nguyen", "A. Clune 2016] Nguyen", "J. Yosinski", "J. Clune"], "title": "Multifaceted feature visualization: Uncovering the different types of features learned by each neuron in deep neural networks. arXiv preprint arXiv:1602.03616", "year": 2016}, {"authors": ["Olah"], "title": "The building blocks of interpretability", "venue": "Distill 3(3):e10", "year": 2018}, {"authors": ["Puiutta", "E. Veith 2020] Puiutta", "E. Veith"], "title": "Explainable reinforcement learning: A survey", "venue": "arXiv preprint arXiv:2005.06247", "year": 2020}, {"authors": ["Schrum"], "title": "Interactive evolution and exploration within latent level-design space of generative adversarial networks. arXiv preprint arXiv:2004.00151", "year": 2020}, {"authors": ["Selvaraju"], "title": "Gradcam: Visual explanations from deep networks via gradientbased localization", "venue": "In Proceedings of the IEEE international conference on computer vision,", "year": 2017}, {"authors": ["Vedaldi Simonyan", "K. Zisserman 2013] Simonyan", "A. Vedaldi", "A. Zisserman"], "title": "Deep inside convo", "year": 2013}, {"authors": ["Whitehead Smith", "G. Mateas 2010] Smith", "J. Whitehead", "M. Mateas"], "title": "Tanagra: A mixed-initiative level design tool", "venue": "In Proceedings of the Fifth International Conference on the Foundations of Digital Games,", "year": 2010}, {"authors": ["Snodgrass", "S. Ontan\u00f3n 2016] Snodgrass", "S. Ontan\u00f3n"], "title": "Learning to generate video game maps using markov models. IEEE transactions on computational intelligence and AI in games 9(4):410\u2013422", "year": 2016}, {"authors": ["Summerville", "A. Mateas 2016] Summerville", "M. Mateas"], "title": "Super mario as a string: Platformer level generation via lstms", "venue": "arXiv preprint arXiv:1603.00930", "year": 2016}, {"authors": ["Summerville"], "title": "Procedural content generation via machine learning (pcgml)", "venue": "IEEE Transactions on Games 10(3):257\u2013270", "year": 2018}, {"authors": ["Philip Summerville", "A.J. Mateas 2015] Summerville", "S. Philip", "M. Mateas"], "title": "Mcmcts pcg 4 smb: Monte carlo tree search to guide platformer level generation", "venue": "In Eleventh Artificial Intelligence and Interactive Digital Entertainment Conference", "year": 2015}, {"authors": ["Tan"], "title": "Detecting bias in black-box models using transparent model distillation", "venue": "arXiv preprint arXiv:1710.06169", "year": 2017}, {"authors": ["Volz"], "title": "Evolving mario levels in the latent space of a deep convolutional generative adversarial network", "venue": "In Proceedings of the Genetic and Evolutionary Computation Conference,", "year": 2018}, {"authors": ["Strobelt Weidele", "D. Martino 2019] Weidele", "H. Strobelt", "M. Martino"], "title": "Deepling: Avisual interpretability system for convolutional neural networks", "venue": "Proceedings SysML", "year": 2019}, {"authors": ["Xu"], "title": "Interpreting deep classifier by visual distillation of dark knowledge", "venue": "arXiv preprint arXiv:1803.04042", "year": 2018}, {"authors": ["Liapis Yannakakis", "A. Liapis", "C. Alexopoulos"], "title": "Mixedinitiative co-creativity", "year": 2014}, {"authors": ["Zeiler", "M.D. Fergus 2014] Zeiler", "R. Fergus"], "title": "Visualizing and understanding convolutional networks", "venue": "In European conference on computer vision,", "year": 2014}, {"authors": ["Zhu"], "title": "Explainable ai for designers: A human-centered perspective on mixed-initiative co-creation", "venue": "IEEE Conference on Computational Intelligence and Games (CIG),", "year": 2018}], "sections": [{"heading": "Introduction", "text": "In science and engineering, a black box is a component that cannot have its internal logic or design directly examined. In artificial intelligence (AI), \u201cThe black box problem\u201d refers to certain kinds of AI agents for which it is difficult or impossible to naively determine how they came to a particular decision (Zednik 2019). Explainable artificial intelligence (XAI) is an assembly of methods and techniques to deal with the black box problem (Biran and Cotton 2017). Machine Learning (ML) is a subset of artificial intelligence that focuses on computer algorithms that automatically learn and improve through experience. (Goodfellow, Bengio, and Courville 2016). The current state-of-the-art models in ML, deep neural networks, are black box models. Intuitively, it is difficult to cooperate with an individual when you cannot understand them. This is critical in co-creative systems (also called mixed-initiative systems), in which a human and an AI agent work together to produce the final output. (Yannakakis, Liapis, and Alexopoulos 2014).\nThere is a wealth of existing methods in the field of XAI (Adadi and Berrada 2018). For example, those that draw\nCopyright \u00a9 2020 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).\ncomparisons between the input and the output of a model (Cortez and Embrechts 2011; Cortez and Embrechts 2013; Simonyan, Vedaldi, and Zisserman 2013; Bach et al. 2016; Dabkowski and Gal 2017; Selvaraju et al. 2017), or analyze the output in terms of the model\u2019s parameters (Boz and Hillman 2000; Garc\u0131\u0301a, Ferna\u0301ndez, and Herrera 2009; Letham et al. 2015; Hara and Hayashi 2018). Alternatively, there is the strategy to attempt to simplify the model (Che et al. 2015; Tan et al. 2017; Xu et al. 2018). The major difference between our approach and these previous ones is that we present a method which makes it possible to explain an AI agent\u2019s action through a detailed inspection of what it has learned during the training phase.\nQuestions we might want to ask an AI agent include \u201cHow did you learn to do that action?\u201d or \u201cWhat did you learn that led you to make that decision?\u201d (Cook et al. 2019). We sought to develop an approach that could answer these questions. Thus, our approach needed to find explanations for the AI agent\u2019s decisions based on its training data.\nIn this paper, we make use of the problem domain of a co-creative Super Mario Bros. level design agent. We use this domain since XAI is critical in co-creative systems. We introduce an approach to detect the training instance that is most responsible for an AI agent\u2019s action. We can then present the most responsible training instance to the human user as an answer to how the AI agent learned to make a particular decision. To evaluate this approach we compare the quality of these responsible training instances to random instances as explanations in two experiments on existing data."}, {"heading": "Related Work", "text": "Our problem domain is generating explanations for a PCGML co-creative agent. Therefore we separate the prior related work into three main areas: Procedural Content Generation via Machine Learning (PCGML), co-creative systems, and Explainable Artificial Intelligence (XAI)."}, {"heading": "Procedural Content Generation via Machine Learning (PCGML)", "text": "Procedural Content Generation via Machine Learning (PCGML) is a field of research focused on the creation of game content by machine learning models that have been\nar X\niv :2\n01 0.\n01 67\n6v 1\n[ cs\n.A I]\n4 O\nct 2\n02 0\ntrained on existing game content (Summerville et al. 2018). Super Mario Bros. level design represents the most consistent area of research into PCGML. Researchers have applied many machine learning methods such as Markov chains (Snodgrass and Ontano\u0301n 2016), Monte-Carlo Tree Search (MCTS) (Summerville, Philip, and Mateas 2015), Long Short-Term Recurrent Neural Networks (LSTMs) (Summerville and Mateas 2016), Autoencoders (Jain et al. 2016), Generative Adversarial Neural Networks (GANs) (Volz et al. 2018), and genetic algorithms through learned evaluation functions (Dahlskog and Togelius 2014) to generate these levels. In a recent work, Khalifa et al proposed a framework to generate game levels using Reinforcement Learning (RL), though they did not evaluate it in Super Mario Bros. (Khalifa et al. 2020). We also draw on reinforcement learning for our agent, however our approach differs from this prior work in terms of focusing on explainability."}, {"heading": "Co-creative systems", "text": "There are numerous prior co-creative systems for game design. These approaches traditionally have not made use of ML, instead they rely on approaches like heuristics search, evolutionary algorithms, and grammars (Smith, Whitehead, and Mateas 2010; Liapis, Yannakakis, and Togelius 2013; Yannakakis, Liapis, and Alexopoulos 2014; Deterding et al. 2017; Baldwin et al. 2017; Charity, Khalifa, and Togelius 2020). ML methods have only recently been incorporated into co-creative game content generation. Guzdial et al. proposed a Deep RL agent for co-creative Procedural Level Generation via Machine Learning (PLGML) (Guzdial, Liao, and Riedl 2018). In another recent work, Schrum et al. presented a tool for applying interactive latent variable evolution to generative adversarial network models that produce video game levels (Schrum et al. 2020). The major difference between our approach and previous ones is that it explains an AI partner\u2019s actions based on what it learned during training.\nIt is important to note that we are not actually evaluating our approach in the context of co-creative interaction with a human subject study. We are only making use of data from prior studies in which humans interacted with ML and RL agents in co-creative systems."}, {"heading": "Explainable Artificial Intelligence (XAI)", "text": "The majority of existing XAI approaches can be separated according to which of two general methods they rely on: (A) visualizing the learned features of a model (Erhan et al. 2009; Simonyan, Vedaldi, and Zisserman 2013; Nguyen, Yosinski, and Clune 2015; Nguyen, Yosinski, and Clune 2016; Nguyen et al. 2017; Olah, Mordvintsev, and Schubert 2017; Weidele, Strobelt, and Martino 2019) and (B) demonstrating the relationship between neurons (Zeiler and Fergus 2014; Fong and Vedaldi 2017; Selvaraju et al. 2017). Olah et al. developed a unified framework that included both (A) and (B) methods. (Olah et al. 2018).\nThere are a few prior works focused on XAI applied to game design and game playing. Guzdial et al. presented an approach to Explainable PCGML via Design Patterns in which the design patterns act as a vocabulary and mode of\ninteraction between user and model (Guzdial et al. 2018). Ehsan et al. introduced AI rationalization, an approach for explaining agent behavior for automated game playing based on how a human would explain a similar behavior (Ehsan et al. 2018). Zhu et al. proposed a new research area of eXplainable AI for Designers (XAID) to help game designers better utilize AI and ML in their design tasks through co-creation (Zhu et al. 2018).\nThere exist a few approaches to explain RL agent\u2019s actions (Puiutta and Veith 2020). Madmul et al. presented an approach that learns structural causal models to derive causal explanations of the behavior of model-free RL agents (Madumal et al. 2019). Kumar et al. presented a deep reinforcement learning approach to control an energy storage system. They visualized the learned policies of the RL agent through the course of training and visualized the strategies followed by the agent to users (Kumar 2019). Cruz et al. proposed a memory-based explainable reinforcement learning (MXRL) where an agent explained the reasons why some decisions were taken in certain situations using an episodic memory (Cruz, Dazeley, and Vamplew 2019). In another recent paper, an approach was presented that employs explanations as feedback from humans in a human-in-the-loop reinforcement learning system (Guan, Verma, and Kambhampati 2020).\nTo the best of our knowledge, this is the first XAI work focused on the training data of a target ML model. Our approach differs from existing XAI work in detailed inspection and alteration of the training phase."}, {"heading": "System Overview", "text": "In this paper, we present an approach for Explainable AI (XAI) that aims to answer the question \u201cWhat did the AI agent learn during training that led it to make that specific action?\u201d. As is shown in Figure 1, the general steps of the approach are as follows: First, during training a DNN, we detect the training instance (or instances) that maximally alters each neuron inside the network. Secondly, during testing, we pass each instance through the network and find the neuron that is most activated (Erhan, Courville, and Bengio 2010). Then given the information from the first step, we can easily identify an instance (or instances) from the training data that maximally impacted the most activated neuron. We refer to this as \u201cthe most responsible training instance\u201d for the AI agent\u2019s action. The intuition is that the user can take this explanation as something akin to the end goal of\nthe agent taking that action. Our hope is that it will be helpful in the user deciding whether to keep or remove some addition by the AI. For example in Figure 3, given the most responsible level as the explanation, the user might keep the lower of the two Goombas, despite the fact that it seems to be floating, if they can match it to the Goombas from the most responsible level.\nFor this purpose, we pre-trained a Deep RL agent using data from interactions of human users with three different ML level design partners (LSTM, Markov Chain, and Bayes Net) to generate the Super Mario Bros level. This is the same Deep RL architecture and data from prior work by Guzdial et al. (Guzdial, Liao, and Riedl 2018) for co-creative Procedural Level Generation via Machine Learning (PLGML), in which they made use of the level design editor from (Guzdial et al. 2017) which is publicly online.1 The agent is designed to take in a current level design state and to output additions to that level design, in order to iteratively complete a level with a human partner.\nOur training inputs are states and the outputs are the Q table values for taking a particular action for the particular state. The input comes into the network as a state of shape (40x15x34). The 40 is the width and 15 is the height of a level chunk. At each x,y location there are 34 possible level components (e.g. ground, goomba, pipe, mushroom, tree, Mario, flag, ...) that could be placed there. As is shown in the visualized architecture of the Convolutional Neural Network (CNN) in Figure 2, it has three convolutional layers and a fully connected layer followed by a reshaping function to make the output in the form of the action matrix which is (40x15x32). The player (Mario) and flag are the level entities that cannot be counted as an action, so there are 32 possible action components instead of the 34 state entities. Our activation function is \u201cLeaky ReLu\u201d for every layer and the loss function is \u201cMean Squared Error\u201d and the optimizer is \u201cAdam\u201d, with the network built in Tensorflow (Abadi et al. 2016). We make use of this existing agent and data since it is the only example of a co-creative PCGML agent where the data from a human subject study is publicly available.\nDuring each training epoch we employ a batch size of one to track when each training instance passes through the network. We calculate and store the change of neuron weights between batches. After training, by summing over the changes of each neuron weight with respect to training data, we are able to identify which training instance maximally results in alteration of a neuron. Since positive and negative values can counteract each other\u2019s effects, it is important to not look at the absolute values until the end of the training. We can then sum and store this information inside eight arrays of shape (4x4x34) for the first convolutional layer, 16 arrays of shape (3x3x8) for the second convolutional layer, and 32 arrays of shape (3x3x16) for the third convolutional layer. These are the shapes of the filters in each layer. We name these arrays Most Responsible Instance for each Neuron in each Convolutional layer (MRIN-Conv1, MRIN-Conv2, and MRIN-Conv3). These data representations link neurons to IDs representing a particular instance\n11https://github.com/mguzdial3/Morai-Maker-Engine\nof a human user working with the AI in the co-creative tool. We can then search these arrays and find the ID of a training instance that is the most responsible for changes to a particular weight.\nOur end goal is to determine the most responsible training instance for a particular prediction made by our trained CNN. To do that, we need to find out what part of the network was most important in making that prediction. We can then determine the most responsible instance for the final weights of this most important part of the network. The most activated filter of each convolutional layer is a filter that contributes to the slice with the largest magnitude in the output of that layer. Hence the most activated filter can be considered the most important part of the convolutional layer for that specific test instance (Erhan, Courville, and Bengio 2010). For example, we pass a test instance into the network. A test instance is a (40x15x34) state that is a chunk of a partially designed level. Since the first convolutional layer has 8 4x4x34 filters with the same padding, the output would be in the shape of (40x15x8). Then we find the (40x15) slice with the largest values. The most activated filter is a (4x4x34) array in our convolutional layer which led to the slice with the greatest magnitude.\nFinally, once we have the maximally activated filter we can identify the most responsible training instance (or instances) by querying the MRIN-Conv arrays we built during training. The most responsible training instance is the ID that most repeated in the MRIN-Conv array associated with the maximally activated filter. We chose the most repeated ID since it is the one that most frequently impacted the majority of the neurons in the filter during training."}, {"heading": "Evaluation", "text": "In this section, we present two evaluations of our system. We call the first evaluation our \u201cExplainability Evaluation\u201d as it addresses the ability of our system to provide explanations that help a user predict an AI agent\u2019s actions. We call the second evaluation our \u201cUser Labeling Error Evaluation\u201d as it addresses the ability of our system to help human users identify positive and negative AI additions during the cocreative process. Both evaluations approximate the impact of our approach on human partners by using existing data of AI-human interactions. Essentially, we act as though the pre-\nrecorded actions of the AI agent were outputs from our Deep RL agent and identify the responsible training instances as if this were the case. Due to the fact that our system derives examples as explanations for the behavior of a co-creative Deep RL agent, a human subject study would be the natural way to evaluate our system. However, prior to a human subject study, we first wanted to gather some evidence of the value of this approach."}, {"heading": "Explainability Evaluation", "text": "The first claim we made was that this approach can help human users better understand and predict the actions of an AI agent. In this experiment we use the most responsible level as an approximation of the AI agent\u2019s goal, in other words what final level the AI agent is working towards. The most responsible level refers to a level at the end of a human user\u2019s interactions with an AI agent. We identify this level by finding the most responsible training instance as above and identifying the level at the end of that training sequence. This experiment is meant to determine if this can help a user to predict the AI agent\u2019s actions. To do this, we passed test instances into our network and found the most responsible training instances. We then compared the most responsible level for some current test instance to the AI agent\u2019s action in the next test instance. If the most responsible level is similar to the action it would indicate that the most responsible level can be a potential explanation for the AI agent\u2019s action by priming the user to better predict future actions by the AI agent. In comparison, we randomly selected 20 levels from the training data and found their similarities to the AI agent\u2019s action in the next test instance. If our approach outperforms the random levels, it will support the claim that the responsible level is better suited to helping predict future AI agent actions compared to random levels.\nWe used two different sets of test data:\n(A) Our first testset is derived from a study in which users interacted with pairs of three different ML agents as mentioned in our System Overview section (Guzdial, Liao, and Riedl 2018). We used the same testset identified in that paper.\n(B) Our second testset is obtained from a study in which expert level designer users interacted with the trained Deep RL agent (Guzdial et al. 2019). If we find success with the first testset then that would in-\ndicate that our trained Deep RL agent is a good surrogate for the original three ML agents, since we would be in effect predicting the next action of one of these agents. Good results for the second testset would demonstrate the capability for prediction of the Deep RL agent\u2019s actions itself. Since the first convolutional layer is the layer that most directly reasons over the level structure, we decided to find the most responsible training instance of just the first convolutional layer. However, this setup puts our approach at a disadvantage, since we are going to compare only one most responsible level to 20 random ones.\nFor comparing the most responsible level and the random levels to the actions, we needed to define a suitable metric. We desired a metric that detects local overlaps and represents the similarity between a level and action. We wanted to pick square windows which are not the same size as the first convolutional layer, to capture some local structures without biasing the metric too far towards our first convolutional layer. As a result, we found all three-by-three nonempty patches for both a given level and an action. Then we counted the number of exact matches of these patches on both sides, removing the matched ones from the dataset since we wanted to count the same patches only once. Finally, we divided the total number of the matched patches by the total number of patches in the action, since this was always smaller than the number from the level. We refer to this metric as the local overlap ratio."}, {"heading": "Explainability Evaluation Results", "text": "We had 242 samples in the first testset and 69 samples in the second one. Since we wanted to compare instances in which the AI agent actually made some serious changes, we chose instances where the AI agent added more than 10 components in its next action. Thus we came to 38 and 46 instances from the first and second testsets, respectively.\nOur approach outperforms the random baseline in 78.94 percent of 38 instances for the ML agents data and 67.29 percent of 46 instances for the Deep RL agent data. The average of the local overlap ratios is shown in Table 1 (higher is better). The minimum value here would be 0 for zero overlap and the maximum value would be 1 for complete overlap between the action and the most responsible level or the random level. This normalization means that even small differences in this metric represent large perceptual differences. For example, a 0.04 difference in the local overlap ratio between the most responsible level and the random levels in Table 1 indicates the most responsible level has 20 more three-by-three non-empty overlaps. We expect that the reason that the Deep RL agent values are generally lower is that the second study made use of published level designers rather than novices and an adaptive Deep RL Agent, meaning that there was more varied behavior compared with the three ML agents.\nAn example of explainability is demonstrated in Figure 3. As is shown in the figure, the AI agent made an action and\nadded some components (e.g. goomba and ground) to the existing state. By looking at the chunk of the most responsible level, the user might realize that the AI agent wants to generate a level including some goombas as enemies and some blocks in the middle of the screen. The AI agent also added ground at the bottom and top of the screen, which the user could identify as being consistent with both their input to the agent and the most responsible level."}, {"heading": "User Labeling Error Evaluation", "text": "For the second evaluation, we wanted to get some sense of whether this approach could be successful in terms of assisting a human user in better understanding good and bad agent actions during the co-creation process. To do this, we needed to identify specific instances where our tool could be helpful in the data we have available. We defined two such concepts: (A) false-positive decisions and (B) false-negative decisions, based on the interactions between users and AI partner during level generation:\n(A) False-positive decisions are additions by the AI partner that the user kept at first but then deleted later.\n(B) False-negative decisions are additions by the AI partner that the user deleted at first but then added later.\nGiven these concepts, if we could help the user avoid making these kinds of decisions, our approach could help a human user during level generation. We anticipated that one reason that users made these kinds of decisions was from a lack of context of the AI agent\u2019s action. Thus, if the user had context they may not delete or keep what they would otherwise keep or delete, respectively.\nTo accomplish this, we implemented an algorithmic way to determine false-positives and false-negatives among the two testsets described in the previous evaluation. In this algorithm, we first find all user decisions in terms of deleting or keeping an addition by the AI agent. Then we look at the level at the end of the user and the AI agent\u2019s interaction. If a deleted AI addition exists in the final level, it is counted as a false-negative example, and if a kept addition does not exist in the final level it is counted as a false-positive example.\nOnce we discovered all false-negative and false-positive examples, we found the state before the example was added by the AI agent and named it the Introductionstate (I-state). We found the state in which false-positivity or false-negativity occurred (i.e. when a user re-added a false-negative or deleted a false-positive) and named it the Contradiction-state (C-state). Since some change between the I-state and the C-state led to the user altering their decision, we wanted to see some sign that presenting the most responsible level to the user could change their mind before\nthey reached this point. Thus we compared these two states to find all the changes that the AI agent or the user made and named this the Difference-state (D-state).\nWe compared each D-state with the final generated level derived from the most responsible training instance. We also compared each D-state with 20 other randomly selected levels from the existing data. For the comparison, we used the local overlap ratio defined in the previous evaluation. If our approach outperforms the random baseline, we will be able to say that there is some support for the responsible level helping the user avoid false-positives and false-negatives in comparison to random levels."}, {"heading": "User Labeling Error Evaluation Results", "text": "We found five false-negative and 24 false-positive examples in the first testset and five false-negative and 54 falsepositive examples in the second one. The results of the evaluation are demonstrated in Figures 4.\nFor the first dataset which included the actions of the three ML agents, our approach outperformed the random baseline in 65.51 percent of the examples. The average of the local overlap ratio values for our approach was 0.1717 which is more than the 0.1647 for the random levels. For the second dataset obtained from the Deep RL agent, our approach outperformed the baseline in 59.32 percent of the examples. The average of the local overlap ratio values were 0.2665 and 0.2328 for the most responsible level and random levels, respectively. Again this represents a large perceptual difference of roughly 15 more non-empty 3x3 overlaps.\nInterestingly, our approach outperforms the random levels in all of the false-negative examples in the second dataset, compared with just 20 percent of false-negatives in the first dataset. Further, our approach performs around 1.5 times better than the random levels in 15 false-positive examples in the second dataset. These instances come from the study that used the same RL agent as we used to derive our explanations, which could account for this performance."}, {"heading": "Discussion", "text": "In this paper, we present an XAI approach for a pre-trained Deep RL agent. Our hypothesis was that our method could be helpful to human users. We evaluated it by approximating this process for two tasks using two existing datasets. These datasets are obtained from studies using three ML partners and an RL agent. Essentially, we used the XAIenabled agent in this paper as if it were the agents used in these datasets. The results of our first evaluation demonstrates that our method is able to represent examples as explanations to help users predict an agent\u2019s next action. The results of our second evaluation support our hypothesis and give us an initial signal that this approach could be successful in order to help human users more efficiently cooperate with a Deep RL agent. This indicates the ability of our approach to help human designers by presenting an explanation for an AI agent\u2019s actions during a co-creation process.\nA human subject study would be a more reasonable way to evaluate this system since human users might be able to derive meaning from the responsible level that our similarity metric could not capture. Our approach performs better\nthan our baseline of random levels in both evaluation methods and this presents evidence towards its value at this task. However, we look forward to investigating a human subject study in order to fully validate these results.\nThere could be other alternatives to a human subject study. For example, a secondary AI agent that predicts our primary AI agent\u2019s actions can play a human partner\u2019s role in the co-creative system. Thus making use of a secondary AI agent to evaluate our system before running a human subject study might be a simple next step.\nIt is important to mention that we only offer one most responsible level from only the first convolutional layer as an explanation. Looking into providing a user with multiple responsible levels or looking into the most responsible levels of the other layers could be a potential way to further improve our approach. Our metric for determining the most responsible training instance is based on finding the most repeated instance inside the MRIN-Conv arrays associated with the most activated filter. We identified the most activated filter by looking at the absolute values. We plan to investigate other metrics such as looking for the most activated neurons outside of the filters. In addition, considering negative and positive values separately in the maximal activation process could also lead to improved behavior. Negative values might indicate that an instance negatively impacted a neuron. It could be the case then that the filter might be maximally activated because it was giving a very strong signal against some action.\nOne quirk of our current approach is that the most responsible training instance depends on the order in which it was presented to the model during the training. Thus, this measure does not tell us about any inherent quality of a particular training data instance, only it\u2019s relevance to a particular model that has undergone a particular training regimen.\nIn the future, we intend to explore how more general representations of responsibility such as Shapely values might intersect with this approach (Ghorbani and Zou 2019).\nOnly the domain of a co-creative system for designing Super Mario Bros. levels is explored in this paper. Thus making use of other games will be required to ensure this is a general method for level design co-creativity. Beyond that, we anticipate a need to demonstrate our approach on different domains outside of games. We look forward to running another study to apply our approach to human-in-the-loop reinforcement learning or other co-creative domains."}, {"heading": "Conclusions", "text": "In this paper we present an approach to XAI that provides human users with the most responsible training instance as an explanation for an AI agent\u2019s action. In support of this approach, we present results from two evaluations. The first evaluation demonstrates the ability of our approach to offer explanations and to help a human partner predict an AI agent\u2019s actions. The second evaluation demonstrates the ability of our approach to help human users better identify good and bad instances of an AI agent\u2019s behavior. To the best of our knowledge this represents the first XAI approach focused on training instances."}, {"heading": "Acknowledgements", "text": "We acknowledge the support of the Natural Sciences and Engineering Research Council of Canada (NSERC) and the Alberta Machine Intelligence Institute (Amii)."}], "title": "Explainability via Responsibility", "year": 2020}