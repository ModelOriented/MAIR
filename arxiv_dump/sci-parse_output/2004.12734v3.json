{"abstractText": "We propose an epistemic approach to formalizing statistical properties of machine learning. Specifically, we introduce a formal model for supervised learning based on a Kripke model where each possible world corresponds to a possible dataset and modal operators are interpreted as transformation and testing on datasets. Then we formalize various notions of the classification performance, robustness, and fairness of statistical classifiers by using our extension of statistical epistemic logic (StatEL). In this formalization, we show relationships among properties of classifiers, and relevance between classification performance and robustness. As far as we know, this is the first work that uses epistemic models and logical formulas to express statistical properties of machine learning, and would be a starting point to develop theories of formal specification of machine learning.", "authors": [{"affiliations": [], "name": "Yusuke Kawamoto"}], "id": "SP:a7936807b45d5cf4cfcdac36e70291df02d1e9ad", "references": [{"authors": ["R. Angell", "B. Johnson", "Y. Brun", "A. Meliou"], "title": "Themis: automatically testing software for discrimination", "venue": "Proc. ESEC/SIGSOFT FSE, pp. 871\u2013875. ACM", "year": 2018}, {"authors": ["A. Athalye", "L. Engstrom", "A. Ilyas", "K. Kwok"], "title": "Synthesizing robust adversarial examples", "venue": "Proc. ICML, pp. 284\u2013293", "year": 2018}, {"authors": ["M. Balliu", "M. Dam", "G.L. Guernic"], "title": "Epistemic temporal logic for information flow security", "venue": "Proc. of PLAS, p. 6", "year": 2011}, {"authors": ["G. Bana"], "title": "Models of objective chance: An analysis through examples", "venue": "Making it Formally Explicit, pp. 43\u201360. Springer International Publishing", "year": 2017}, {"authors": ["S. Barocas", "M. Hardt", "A. Narayanan"], "title": "Fairness and Machine Learning", "venue": "fairmlbook.org", "year": 2019}, {"authors": ["R. Berk", "H. Heidari", "S. Jabbari", "M. Kearns", "A. Roth"], "title": "Fairness in criminal justice risk assessments: The state of the art", "venue": "Sociological Methods & Research", "year": 2018}, {"authors": ["P. Blackburn", "M. de Rijke", "Y. Venema"], "title": "Modal Logic", "venue": "Cambridge Tracts in Theoretical Computer Science. Cambridge University Press", "year": 2001}, {"authors": ["M. Burrows", "M. Abadi", "R.M. Needham"], "title": "A logic of authentication", "venue": "ACM Trans. Comput. Syst. 8(1), 18\u201336", "year": 1990}, {"authors": ["T. Calders", "S. Verwer"], "title": "Three naive bayes approaches for discrimination-free classification", "venue": "Data Min. Knowl. Discov. 21(2), 277\u2013292", "year": 2010}, {"authors": ["N. Carlini", "D.A. Wagner"], "title": "Towards evaluating the robustness of neural networks", "venue": "Prc. S&P, pp. 39\u201357", "year": 2017}, {"authors": ["R. Chadha", "S. Delaune", "S. Kremer"], "title": "Epistemic logic for the applied pi calculus", "venue": "Proc. of FMOODS/FORTE, pp. 182\u2013197", "year": 2009}, {"authors": ["A. Chakraborty", "M. Alam", "V. Dey", "A. Chattopadhyay", "D. Mukhopadhyay"], "title": "Adversarial attacks and defences: A survey", "venue": "CoRR abs/1810.00069", "year": 2018}, {"authors": ["T. Dreossi", "S. Ghosh", "A.L. Sangiovanni-Vincentelli", "S.A. Seshia"], "title": "A formalization of robustness for deep neural networks", "venue": "Proc. VNN", "year": 2019}, {"authors": ["C. Dwork"], "title": "Differential privacy", "venue": "Proc. of ICALP, pp. 1\u201312", "year": 2006}, {"authors": ["C. Dwork", "M. Hardt", "T. Pitassi", "O. Reingold", "R.S. Zemel"], "title": "Fairness through awareness", "venue": "Proc. of ITCS, pp. 214\u2013226. ACM", "year": 2012}, {"authors": ["R. Fagin", "J. Halpern", "Y. Moses", "M. Vardi"], "title": "Reasoning about Knowledge", "venue": "The MIT Press", "year": 1995}, {"authors": ["P. Gajane"], "title": "On formalizing fairness in prediction with machine learning", "venue": "CoRR abs/1710.03184", "year": 2017}, {"authors": ["S. Galhotra", "Y. Brun", "A. Meliou"], "title": "Fairness testing: testing software for discrimination", "venue": "Proc. ESEC/FSE, pp. 498\u2013510. ACM", "year": 2017}, {"authors": ["F.D. Garcia", "I. Hasuo", "W. Pieters", "P. van Rossum"], "title": "Provable anonymity", "venue": "Proc. of FMSE, pp. 63\u201372", "year": 2005}, {"authors": ["I.J. Goodfellow", "J. Shlens", "C. Szegedy"], "title": "Explaining and harnessing adversarial examples", "venue": "Proc. of ICLR", "year": 2015}, {"authors": ["J.Y. Halpern"], "title": "Reasoning about uncertainty", "venue": "The MIT press", "year": 2003}, {"authors": ["M. Hardt", "E. Price", "N. Srebro"], "title": "Equality of opportunity in supervised learning", "venue": "proc. NIPS, pp. 3315\u20133323", "year": 2016}, {"authors": ["X. Huang", "M. Kwiatkowska", "S. Wang", "M. Wu"], "title": "Safety verification of deep neural networks", "venue": "Proc. CAV, pp. 3\u201329", "year": 2017}, {"authors": ["D. Hughes", "V. Shmatikov"], "title": "Information hiding, anonymity and privacy: a modular approach", "venue": "J. of Comp. Security 12(1), 3\u201336", "year": 2004}, {"authors": ["H.L. Jonker", "W. Pieters"], "title": "Receipt-freeness as a special case of anonymity in epistemic logic", "venue": "Proc. Workshop On Trustworthy Elections (WOTE\u201906)", "year": 2006}, {"authors": ["G. Katz", "C.W. Barrett", "D.L. Dill", "K. Julian", "M.J. Kochenderfer"], "title": "Reluplex: An efficient SMT solver for verifying deep neural networks", "venue": "Proc. CAV, pp. 97\u2013117", "year": 2017}, {"authors": ["Y. Kawamoto"], "title": "Statistical epistemic logic", "venue": "The Art of Modelling Computational Systems: A Journey from Logic and Concurrency to Security and Privacy - Essays Dedicated to Catuscia Palamidessi on the Occasion of Her 60th Birthday, LNCS, vol. 11760, pp. 344\u2013362. Springer", "year": 2019}, {"authors": ["Y. Kawamoto"], "title": "Towards logical specification of statistical machine learning", "venue": "Proc. SEFM, LNCS, vol. 11724, pp. 293\u2013311. Springer", "year": 2019}, {"authors": ["Y. Kawamoto", "K. Mano", "H. Sakurada", "M. Hagiya"], "title": "Partial knowledge of functions and verification of anonymity", "venue": "Transactions of the Japan Society for Industrial and Applied Mathematics 17(4), 559\u2013576", "year": 2007}, {"authors": ["S.A. Kripke"], "title": "Semantical analysis of modal logic i normal modal propositional calculi", "venue": "Mathematical Logic Quarterly 9(5-6), 67\u201396", "year": 1963}, {"authors": ["D. Lewis"], "title": "A subjectivist\u2019s guide to objective chance", "venue": "Studies in Inductive Logic and Probability, Volume II, pp. 263\u2013293. Berkeley: University of California Press", "year": 1980}, {"authors": ["A. Madry", "A. Makelov", "L. Schmidt", "D. Tsipras", "A. Vladu"], "title": "Towards deep learning models resistant to adversarial attacks", "venue": "Proc. ICLR", "year": 2018}, {"authors": ["S. Moosavi-Dezfooli", "A. Fawzi", "P. Frossard"], "title": "Deepfool: A simple and accurate method to fool deep neural networks", "venue": "Proc. CVPR, pp. 2574\u20132582", "year": 2016}, {"authors": ["R. Pardo", "G. Schneider"], "title": "A formal privacy policy framework for social networks", "venue": "Proc. SEFM, pp. 378\u2013392", "year": 2014}, {"authors": ["K. Pei", "Y. Cao", "J. Yang", "S. Jana"], "title": "Deepxplore: Automated whitebox testing of deep learning systems", "venue": "Proc. SOSP, pp. 1\u201318", "year": 2017}, {"authors": ["A.N. Prior"], "title": "Time and modality", "year": 1957}, {"authors": ["M. Richardson", "P.M. Domingos"], "title": "Markov logic networks", "venue": "Mach. Learn. 62(1-2), 107\u2013136", "year": 2006}, {"authors": ["S.A. Seshia", "A. Desai", "T. Dreossi", "D.J. Fremont", "S. Ghosh", "E. Kim", "S. Shivakumar", "M. Vazquez-Chanlatte", "X. Yue"], "title": "Formal specification for deep neural networks", "venue": "Proc. ATVA, pp. 20\u201334", "year": 2018}, {"authors": ["P.F. Syverson", "S.G. Stubblebine"], "title": "Group principals and the formalization of anonymity", "venue": "World Congress on Formal Methods (1), pp. 814\u2013833", "year": 1999}, {"authors": ["C. Szegedy", "W. Zaremba", "I. Sutskever", "J. Bruna", "D. Erhan", "I.J. Goodfellow", "R. Fergus"], "title": "Intriguing properties of neural networks", "venue": "Proc. ICLR", "year": 2014}, {"authors": ["Y. Tian", "K. Pei", "S. Jana", "B. Ray"], "title": "Deeptest: automated testing of deep-neural-network-driven autonomous cars", "venue": "Proc. ICSE, pp. 303\u2013314", "year": 2018}, {"authors": ["S. Udeshi", "P. Arora", "S. Chattopadhyay"], "title": "Automated directed fairness testing", "venue": "Proc. ASE, pp. 98\u2013108. ACM", "year": 2018}, {"authors": ["L. Vaserstein"], "title": "Markovian processes on countable space product describing large systems of automata", "venue": "Probl. Peredachi Inf. 5(3), 64\u201372", "year": 1969}, {"authors": ["G.H. von Wright"], "title": "An Essay in Modal Logic", "venue": "Amsterdam: North-Holland Pub. Co.", "year": 1951}, {"authors": ["L. Zadeh"], "title": "Fuzzy sets", "venue": "Information and Control 8(3), 338 \u2013 353", "year": 1965}, {"authors": ["M.B. Zafar", "I. Valera", "M. Gomez-Rodriguez", "K.P. Gummadi"], "title": "Fairness beyond disparate treatment & disparate impact: Learning classification without disparate mistreatment", "venue": "Proc. WWW, pp. 1171\u20131180", "year": 2017}], "sections": [{"text": "malizing statistical properties of machine learning. Specifically, we introduce a formal model for supervised learning based on a Kripke model where each possible world corresponds to a possible dataset and modal operators are interpreted as transformation and testing on datasets. Then we formalize various notions of the classification performance, robustness, and fairness of statistical classifiers by using our extension of statistical epistemic logic (StatEL). In this formalization, we show relationships among properties of classifiers, and relevance between classification performance and robustness. As far as we know, this is the first work that uses epistemic models and logical formulas to express statistical properties of machine learning, and would be a starting point to develop theories of formal specification of machine learning.\nKeywords Modal logic \u00b7 Possible world semantics \u00b7 Machine learning \u00b7 Classification performance \u00b7 Robustness \u00b7 Fairness"}, {"heading": "1 Introduction", "text": "With the increasing use of machine learning in reallife applications, the safety and security of learningbased systems have been of great interest. In particular, many recent studies [40],[12] have found vulnerabilities\nThis work was supported by the New Energy and Industrial Technology Development Organization (NEDO), by ERATO HASUO Metamathematics for Systems Design Project (No. JPMJER1603), JST, and by Inria under the project LOGIS.\nYusuke Kawamoto AIST, Tsukuba, JAPAN\nORCID: 0000-0002-2151-9560\non the robustness of deep neural networks (DNNs) to malicious inputs, which can lead to disasters in security critical systems, such as self-driving cars. To find out these vulnerabilities in advance, there have been researches on the formal verification and testing methods for the robustness of DNNs in recent years [23,26,35, 41]. However, relatively little attention has been paid to the formal specification of machine learning [38].\nIn the research filed of formal specification and verification, logical approaches have been shown useful to characterize desired properties and to develop theories to discuss those properties. For example, temporal logic [36] is a branch of modal logic for expressing time-dependent propositions, and has been widely used to describe requirements of hardware and software systems. For another example, epistemic logic [44] is a modal logic for knowledge and belief that has been employed as formal policy languages for distributed systems (e.g., for the authentication [8] and the anonymity [39] of security protocols). As far as we know, however, no prior work has employed logical formulas to rigorously describe various statistical properties of machine learning, although there are some papers that (often informally) list various desirable properties of machine learning [38].\nIn this paper, we present a first logical formalization of statistical properties of machine learning. To describe the statistical properties in a simple and abstract way, we extend statistical epistemic logic (StatEL) [27], which has recently been proposed to describe statistical knowledge and is applied to formalize statistical hypothesis testing and statistical privacy of databases.\nA key idea in our modeling of statistical machine learning is that we formalize logical aspects in the syntax level, and statistical distances and dataset operations in the semantics level by using accessibility rela-\nar X\niv :2\n00 4.\n12 73\n4v 3\n[ cs\n.L O\n] 2\n0 Se\np 20\ntions of a Kripke model [30]. In this model, we formalize supervised learning and some of its desirable properties, including performance, robustness, and fairness. More specifically, classification performance and robustness are described as the differences between the correct class label and the classifier\u2019s prediction, whereas fairness is expressed as a conditional indistinguishability between different groups.\nOur contributions. The main contributions of this work are as follows:\n\u2013 We propose a logical approach to formalizing statis-\ntical properties of machine learning in a simple and abstract way. Specifically, we introduce a principle that logical aspects of statistical properties are described in the syntax level, and statistical distances and datasets are formalized in the semantics level. \u2013 We formalize supervised learning models and test\ndatasets (used to check whether the learning models satisfy specification) by employing a distributional Kripke model [27] where each possible world corresponds to a possible test dataset, and modal operators are interpreted as transformation and testing on datasets. Then we show how the sampling from a dataset and non-deterministic adversarial inputs are formalized in the distributional Kripke model. \u2013 We propose an extension of statistical epistemic logic\n(StatEL) as a formal language to describe various properties of machine learning models, including the performance, robustness, and fairness of statistical classifiers. Then the satisfaction of logical formulas representing those properties is associated with their testing using a test dataset. As far as we know, this is the first work that uses logical formulas to formalize various statistical properties of machine learning, and that provides an epistemic view on those properties. \u2013 We show some relationships among properties of\nclassifiers, such as different levels of robustness. We also present certain relationships between classification performance and robustness, which suggest robustness-related properties that have not been formalized in the literature as far as we know.\nCautions and limitations. In this paper, we focus on formalizing properties of supervised learning models that may be tested by using a dataset; i.e., we do not deal with unsupervised learning, reinforcement learning, the properties of learning algorithms, quality of training data (e.g., sample bias), quality of testing (e.g., coverage criteria), explainability, temporal properties, or system-level specification. It should be noted that most of the properties formalized in this paper have been\nknown in machine learning literatures, and the novelty of this work lies in the logical formulation of those statistical properties.\nWe also highlight that this work aims to provide a logical approach to the modeling of statistical properties tested with a dataset, and does not present methods for checking, guaranteeing, or improving the performance/robustness/fairness of machine learning models. As for the satisfiability of logical formulas, we leave the development of testing and (statistical) model checking algorithms as future work, since the research area on the testing and verification of machine learning is relatively new and needs further techniques to improve the scalability. Moreover, in some applications such as image recognition, some atomic formulas (e.g., representing whether an input image is a panda) cannot be defined mathematically, and require additional techniques based on experiments. Nevertheless, we demonstrate that describing various properties using logical formulas is useful to explore desirable properties and to discuss their relationships in a framework.\nFinally, we emphasize that our work is the first attempt to use epistemic models and logical formulas to express statistical properties of machine learning models, and would be a starting point to develop theories of formal specification of machine learning in future research.\nRelationship with the preliminary version. The main novelties of this paper with respect to the preliminary version [28] are as follows:\n\u2013 We add how the satisfaction of a formula at a pos-\nsible world can be regarded as the testing of a specification using a test dataset (Sect. 3.1). \u2013 We show how modal operators are used to model\nthe transformation and testing on datasets. For example, data preparation T (e.g., data cleaning, data augmentation) can also be formalized as a modal operator \u2206T (Sect. 3.2). \u2013 We re-interpret the non-classical implication \u2283 for conditional probabilities in StatEL as a modal oper-\nator associated with a conditioning relation (Sect. 3.3).\n\u2013 We introduce a modal operator \u223c\u03b5,Dx for conditional indistinguishability (Sect. 3.4). Then we provide a\nmore comprehensible formalization of the fairness of supervised learning (Sect. 7) without using counterfactual epistemic operators [28], because the formalization using these operators requires an additional formula and makes the presentation more complicated and unintuitive. \u2013 We add a formalization of generalization error to\ncapture how accurately a classifier is able to classify previously unseen input data (Sect. 5.3).\n\u2013 We add a formalization of other fairness notions\ncalled separation (Sect. 7.3) and sufficiency (Sect. 7.4) so that this paper covers all three categories of fairness notions [5].\n\u2013 We show a running example of a pedestrian detec-\ntion to illustrate the formalization of various notions of performance, robustness, and fairness.\nPaper organization. The rest of this paper is organized as follows. Sect. 2 presents notations used in this paper and provides background on statistical distances and statistical epistemic logic (StatEL). Sect. 3 introduces a different view on the modal operators in StatEL and extends the logic with additional operators. Sect. 4 introduces a formal model for describing the behaviors of statistical classifiers and non-deterministic adversarial inputs. Sects. 5, 6, and 7 respectively formalize various notions of the performance, robustness, and fairness of classifiers by using our extension of StatEL. Sect. 8 presents related work and Sect. 9 concludes."}, {"heading": "2 Preliminaries", "text": "In this section we introduce some notations, and review background on statistical distance notions and the syntax and semantics of statistical epistemic logic (StatEL), introduced in [27].\n2.1 Notations\nLet R\u22650 be the set of non-negative real numbers, and [0, 1] be the set of non-negative real numbers not greater than 1. We denote by DO the set of all probability distributions over a finite set O. Given a finite set O and a probability distribution \u00b5 \u2208 DO, the probability of sampling a value v from \u00b5 is denoted by \u00b5[v]. For a\nsubset R \u2286 O, let \u00b5[R] = \u2211 v\u2208R \u00b5[v]. For a distribution \u00b5 over a finite set O, its support is defined by supp(\u00b5) = {v \u2208 O : \u00b5[v] > 0}.\n2.2 Statistical Distance\nWe recall popular notions of distance between probability distributions: total variation and \u221e-Wasserstein distance.\nInformally, total variation between two distributions \u00b50 and \u00b51 over a set O represents the largest difference between the probabilities that \u00b50 and \u00b51 assign to an identical subset R of O.\nDefinition 1 (Total variation) For a finite set O, the total variation Dtv of two distributions \u00b50, \u00b51 \u2208 DO is defined by:\nDtv(\u00b50 \u2016 \u00b51) def = sup\nR\u2286O |\u00b50[R]\u2212 \u00b51[R]|.\nWe then recall the\u221e-Wasserstein metric [43]. Intuitively, the \u221e-Wasserstein metric Wd(\u00b50, \u00b51) between two distributions \u00b50, \u00b51 represents the minimum largest move between points in a transportation from \u00b50 to \u00b51.\nDefinition 2 (\u221e-Wasserstein metric) Let O be a finite set and d : O \u00d7 O \u2192 R\u22650 be a metric over O. The \u221e-Wasserstein metric Wd w.r.t. d between two distributions \u00b50, \u00b51 \u2208 DO is defined by:\nWd(\u00b50, \u00b51) = min \u00b5\u2208cp(\u00b50,\u00b51) max (v0,v1)\u2208supp(\u00b5) d(v0, v1)\nwhere cp(\u00b50, \u00b51) is the set of all couplings 1 of \u00b50 and \u00b51.\n2.3 Syntax of StatEL\nWe next recall the syntax of statistical epistemic logic (StatEL) [27], which has two levels of formulas: static and epistemic formulas. Intuitively, a static formula describes a proposition satisfied at a (deterministic) state, while an epistemic formula describes a proposition satisfied at a probability distribution of states. In this paper, the former is used only to define the latter.\nFormally, let Mes be a set of symbols called measurement variables, and \u0393 be a set of atomic formulas of the form \u03b3(x1, x2, . . . , xn) for a predicate symbol \u03b3, n \u2265 0, and x1, x2, . . . , xn \u2208 Mes. Let I \u2286 [0, 1] be a finite union of disjoint intervals, and A be a finite set of indices (e.g., associated with statistical divergences). Then the formulas are defined by:\nStatic formulas: \u03c8 ::= \u03b3(x1, x2, . . . , xn) | \u00ac\u03c8 |\u03c8 \u2227 \u03c8 Epistemic formulas: \u03d5 ::= PI \u03c8 | \u00ac\u03d5 |\u03d5 \u2227 \u03d5 |\u03c8\u2283\u03d5 |Ka\u03d5\nwhere a \u2208 A. We denote by F the set of all epistemic formulas. Note that we have no quantifiers over measurement variables. (See Sect. 2.5 for more details.)\nThe probability quantification PI \u03c8 represents that a static formula \u03c8 is satisfied with a probability belonging to a set I. For instance, P(0.95,1] \u03c8 represents that \u03c8 holds with a probability greater than 0.95. By\n1 A coupling of two distributions \u00b50, \u00b51 \u2208 DO is a joint distribution \u00b5 \u2208 D(O\u00d7O) such that \u00b50 and \u00b51 are \u00b5\u2019s marginal distributions, i.e., for each v0 \u2208 O, \u00b50[v0] = \u2211 v\u2032\n1 \u2208O \u00b5[v0, v\n\u2032 1] and for each v1 \u2208 O, \u00b51[v1] = \u2211 v\u2032\n0 \u2208O \u00b5[v\n\u2032 0, v1]. For a cou-\npling \u00b5, the support supp(\u00b5) is the maximum subset of O\u00d7O whose elements are assigned non-zero probabilities in \u00b5.\n\u03c8 \u2283 PI \u03c8\u2032 we represent that the conditional probability of \u03c8\u2032 given \u03c8 is included in a set I. The epistemic knowledge Ka \u03d5 expresses that we know \u03d5 when our capability of observation is denoted by a \u2208 A.\nAs syntax sugar, we use disjunction \u2228, classical implication \u2192, and epistemic possibility Pa, defined as usual by: \u03d50\u2228\u03d51 ::=\u00ac(\u00ac\u03d50\u2227\u00ac\u03d51), \u03d50 \u2192 \u03d51 ::=\u00ac\u03d50\u2228\u03d51, and Pa \u03d5 ::=\u00acKa \u00ac\u03d5. When I is a singleton {i}, we abbreviate PI as Pi.\n2.4 Distributional Kripke Model\nNext we recall the notion of a distributional Kripke model [27], where each possible world is associated with a probability distribution over a set of states, and with a stochastic assignment of data to measurement variables.\nDefinition 3 (Distributional Kripke model) Let A be a finite set of indices (typically associated with operations and tests on datasets), S be a finite set of states, and O be a finite set of data, called a data domain. A distributional Kripke model is a tuple M = (W, (Ra)a\u2208A, (Vs)s\u2208S) consisting of:\n\u2013 a non-empty set W of multisets of states belonging to S; \u2013 for each a \u2208 A, an accessibility relationRa \u2286 W\u00d7W; \u2013 for each s \u2208 S, a valuation Vs : \u0393 \u2192 P(Ok) that\nmaps each k-ary predicate \u03b3 to a set Vs(\u03b3) of ktuples of data.\nThe set W is called a universe, and its elements are called possible worlds. A world is said to be finite if it is a finite multiset, i.e., it has a finite number of (possibly duplicated) elements. A world is said to be infinite if it is an infinite multiset.\nThe relationRa determines an accessibility between two worlds. For example, (w,w\u2032) \u2208 Ra means that a world w\u2032 is accessible from a world w when our capability of distinguishing possible worlds is denoted by a \u2208 A. The valuation Vs may give a possibly different interpretation of a predicate \u03b3 at a different state s. We assume that all measurement variables range over the same data domain O in every world. The interpretation of measurement variables at a state s is given by a deterministic assignment \u03c3s defined below.\nDefinition 4 (Deterministic assignment) For any distributional Kripke model M=(W, (Ra)a\u2208A, (Vs)s\u2208S), we assume that each world w \u2208 W is associated with a function \u03c1w : Mes \u00d7 S \u2192 O that maps each measurement variable x to its value \u03c1w(x, s) that is observed at a state s belonging to the world w. We also\nassume that each state s in a world w is associated with the deterministic assignment \u03c3s : Mes \u2192 O defined by \u03c3s(x) = \u03c1w(x, s).\nSince each world w is a multiset of states, we abuse the notation and denote by w[s] the probability that a state s is randomly chosen from w (i.e., the number of occurrences of s in the multiset w, divided by the total number of elements in w). Here we regard each world w as a probability distribution over the states that corresponds to the multiset.\nThe probability that a measurement variable x \u2208 Mes has a value v \u2208 O is: \u03c3w(x)[v] = \u2211 s\u2208w,\u03c3s(x)=v w[s]. Note that \u03c3w : Mes \u2192 DO maps each measurement variable x to a probability distribution \u03c3w(x) over the data domain O. Hence \u03c3w represents the joint probability distribution of all variables in Mes, and is called the stochastic assignment at w. When a state s is uniformly drawn from a multiset w of states, a datum \u03c3s(x) is sampled from the distribution \u03c3w(x).\nIn later sections, a possible world corresponds to a dataset (i.e., a multiset of data tuples) from which data are sampled. For example, suppose that we have only three measurement variables Mes = {x, y, z}. Then for each state s in a world w, the deterministic assignment \u03c3s : Mes \u2192 O represents the tuple of data (\u03c3s(x), \u03c3s(y), \u03c3s(z)). Hence each state s corresponds to a tuple of data, and the world w corresponds to the dataset {(\u03c3s(x), \u03c3s(y), \u03c3s(z)) | s \u2208 w}.\n2.5 Stochastic Semantics of StatEL\nNow we recall the stochastic semantics [27] for the StatEL formulas over a distributional Kripke model M = (W, (Ra)a\u2208A, (Vs)s\u2208S) with W = DS.\nThe interpretation of a static formulas \u03c8 at a state\ns is given by:\ns |= \u03b3(x1, . . . , xk) iff (\u03c3s(x1), . . . , \u03c3s(xk)) \u2208 Vs(\u03b3) s |= \u00ac\u03c8 iff s 6|= \u03c8\ns |= \u03c8 \u2227 \u03c8\u2032 iff s |= \u03c8 and s |= \u03c8\u2032.\nThe restriction w|\u03c8 of a world w to a static formula \u03c8 is defined by w|\u03c8[s] = w[s]\u2211\ns\u2032:s\u2032|=\u03c8 w[s \u2032] if s |= \u03c8, and\nw|\u03c8[s] = 0 otherwise. Note that w|\u03c8 is undefined if there is no state s that satisfies \u03c8 and has a non-zero probability in w.\nThen the interpretation of epistemic formulas in a\nworld w is defined by: M, w |= PI \u03c8 iff Pr [ s $\u2190 w : s |= \u03c8 ] \u2208 I\nM, w |= \u00ac\u03d5 iff M, w 6|= \u03d5 M, w |= \u03d5 \u2227 \u03d5\u2032 iff M, w |= \u03d5 and M, w |= \u03d5\u2032"}, {"heading": "M, w |= \u03c8 \u2283 \u03d5 iff w|\u03c8 is defined and M, w|\u03c8 |= \u03d5", "text": "M, w |= Ka \u03d5 iff for every w\u2032 s.t. (w,w\u2032) \u2208 Ra,\nM, w\u2032 |= \u03d5,\nwhere s $\u2190 w represents that a state s is sampled from the distribution w.\nThen M, w |= \u03c80 \u2283 PI \u03c81 represents that the conditional probability of satisfying a static formula \u03c81 given another \u03c80 is included in a set I at a world w.\nIn each world w, measurement variables can be interpreted using \u03c3w. This allows us to assign different values to different occurrences of a variable in a formula; E.g., in \u03d5(x) \u2192 Ka \u03d5\u2032(x), x occurring in \u03d5(x) is interpreted by \u03c3w in a world w, while x in \u03d5 \u2032(x) is interpreted by \u03c3w\u2032 in another w \u2032 s.t. (w,w\u2032) \u2208 Ra.\nFinally, the interpretation of an epistemic formula\n\u03d5 in M is given by:\nM |= \u03d5 iff for every world w in M, M, w |= \u03d5.\nHereafter we mainly focus on the satisfaction local to a possible world, and M may be omitted when it is clear from the context."}, {"heading": "3 Modality as Transformation and Testing on Datasets", "text": "In this section we introduce a different view on the modal operators in statistical epistemic logic (StatEL), and define additional modal operators that are used to formalize various properties of machine learning in Sects. 5 to 7.\n3.1 Checking Satisfaction at a World as Testing with a Dataset\nWe first show how we regard the satisfaction of a formula \u03d5 as testing a system\u2019s specification expressed by \u03d5 as follows.\nAs explained in Sect. 2.4, a possible world corresponds to a possible dataset. Thus, given a model M, a world w, and a formula \u03d5, checking the satisfaction M, w |= \u03d5 can be regarded as testing whether the specification \u03d5 of a system (e.g., a machine learning model we formalize in Sect. 4) is satisfied when the dataset w\nprovides inputs to the system. For example, let \u03d5 be a formula representing that a machine learning task (e.g., classification) C fails with probability at most 5%. Then M, w |= \u03d5 represents that when the learning task C is performed using a test dataset w, then it fails for at most 5% of the test data in w.\nFor simplicity, we discuss the satisfaction of the formulas \u03d5 in which neither Ka nor Pa occurs as follows. For each state (namely, data tuple) s \u2208 w and for each static sub-formula \u03c8 of \u03d5, we can efficiently check whether s |= \u03c8. When the dataset w is finite (i.e., it is a finite multiset of data tuples), we can check the satisfaction w |= \u03d5 in finite time, more precisely, in linear time in the number of elements in w.\nWhen the dataset w is infinite, however, we cannot check whether w |= \u03d5 in general. For example, suppose that w is the infinite dataset representing a true distribution from which data are sampled and observed. When we cannot learn w itself, we usually obtain a finite dataset w fin by sampling data from w repeatedly and independently and check a specification \u03d5 only with this test dataset w fin.\nHereafter, we mainly deal with distributional Kripke models M that have infinite numbers of finite worlds. In the following sections except Sect. 6, we deal only with formulas without Ka nor Pa, 2 hence can check their satisfaction at a finite world in finite time.\n3.2 Modal Operators for Dataset Transformation\nIn the rest of Sect. 3, we show that modal operators can be used to model the transformation and testing on datasets.\nFirst, we introduce modal operators for dataset trans-\nformation. The modal operator \u2206T defined below is unary (i.e., taking a single formula as argument), and is parameterized with a transformation T between datasets. Intuitively, w |= \u2206T\u03d5 represents that a formula \u03d5 is satisfied for the dataset w\u2032 that is obtained by transforming the current dataset w by T . Formally, the modal operator \u2206T is interpreted as follows.\nDefinition 5 (Modality \u2206T for a dataset transformation T ) Given a function T : W \u2192 W, we de2 The testing of a formula \u03d5 is not feasible when an epistemic operator Ka or Pa occurs in \u03d5 and the model M has a large number of possible worlds. Detailed analysis of time complexity of StatEL is out of the scope of this paper, and should be included in the journal version of our paper [27] that proposed StatEL. As we will discuss in Sect. 6, the robustness of machine learning is formalized using these epistemic operators, hence cannot be tested in practical time unless M is comprised of a small number of worlds.\nfine an accessibility relation as RT def = {(w,w\u2032) | w\u2032 = T (w)}. Then we define the interpretation of \u2206T by:\nM, w |= \u2206T\u03d5 iff there is a w\u2032 s.t. (w,w\u2032) \u2208 RT and M, w\u2032 |= \u03d5.\nFor example, machine learning often require data preparation to manipulate a given raw dataset into a form that makes a machine learning task feasible and more effective (e.g., data cleaning, data augmentation). For a dataset w and two ways of data preparation T0 and T1, w |= \u2206T0\u03d5 \u2227\u2206T1\u03d5 represents that a property \u03d5 holds for the two prepared datasets T0(w) and T1(w).\nFor another example, the security of machine learning often assumes a certain malicious adversary that can manipulate a given dataset to make a machine learning task fail. Such adversarial operations T on datasets can also be formalized using a different modal operator corresponding to T as we will explain in Sect. 6.\nIn the next section, we show that the logical connective \u2283 can be re-interpreted as the modality \u2206T for some dataset transformation T .\n3.3 Modality for Conditioning\nWe then present another interpretation of the logical connective \u2283 (defined in Sect. 2.5) used to express conditional probabilities in Sects. 5 and 6. Roughly speaking, we regard the restriction w|\u03c8 of a world w to a static formula \u03c8 as a transformation R\u03c8 of w. Then we redefine \u2283 as a modal operator associated with R\u03c8, and call it the conditioning operator. Formally, the interpretation of \u2283 is defined as follows.\nDefinition 6 (Conditioning operator \u2283) Assume that the universe W includes all sub-multisets of each w \u2208 W. Given a static formula \u03c8, we define an accessibility relation as the conditioning relation R\u03c8 def = {(w,w|\u03c8) | w \u2208 W}. Then the interpretation of the conditioning operator \u2283 is given by:\nM, w |= \u03c8 \u2283 \u03d5 iff there is a w\u2032 s.t. (w,w\u2032) \u2208 R\u03c8 and M, w\u2032 |= \u03d5.\nIntuitively, w |= \u03c8 \u2283 \u03d5 corresponds to the two operations: (i) transforming the given dataset w to the sub-dataset w|\u03c8 and (ii) testing whether a property \u03d5 holds for the sub-dataset w|\u03c8. When no data in the dataset w satisfies the property \u03c8, we can describe this as M, w |= \u03c8 \u2283 \u22a5 by using the propositional constant falsum \u22a5.\nNote that the conditioning \u03c8 \u2283 \u03d5 can be regarded as the modal formula \u2206T\u03d5 with the dataset transformation T where T (w) = w|\u03c8 for all w \u2208 W.\nIn Sects. 5 and 6, we show concrete examples using the conditioning operator \u2283, i.e., the classification performance and robustness of statistical classifiers.\n3.4 Modality for Conditional Indistinguishability\nNext, we introduce a modal operator that is used to formalize the fairness of machine learning in Sect. 7.\nGiven two static formulas \u03c80, \u03c81 (e.g., representing male and female), w|\u03c80(x) (resp. w|\u03c81(x)) represents the probability distribution of values of a measurement variable x generated from the sub-dataset w|\u03c80 , e.g., the sub-dataset about male (resp. w|\u03c81 , e.g., about female). To formalize a certain similarity between x\u2019s values generated from the two sub-datasets (e.g., between the benefits for male and for female), we introduce a modal operator \u223c\u03b5,Dx for conditional indistinguishability as follows. We write \u03c80 \u223c\u03b5,Dx \u03c81 to represent that the two distributions w|\u03c80(x) and w|\u03c81(x) are indistinguishable up to a threshold \u03b5 in terms of a divergence or distance D. Formally, this modality is defined as follows.3\nDefinition 7 (Conditional indistinguishability operator \u223c\u03b5,Dx ) Assume that the universe W includes all sub-multisets of each w \u2208 W. Given an x \u2208 Mes, an \u03b5 \u2208 R\u22650, and a divergence or distance D : DO\u00d7DO \u2192 R\u22650, we define an accessibility relation by:\nR\u03b5,Dx def = {(w0, w1) \u2208 W \u00d7W |D(\u03c3w0(x)\u2016\u03c3w1(x)) \u2264 \u03b5}.\nThen for static formulas \u03c80 and \u03c81, we define the interpretation of \u03c80 \u223c\u03b5,Dx \u03c81 by:\nM, w |= \u03c80 \u223c\u03b5,Dx \u03c81 iff there exist w0, w1 s.t. (w,w0) \u2208 R\u03c80 ,\n(w,w1) \u2208 R\u03c81 , and (w0, w1) \u2208 R\u03b5,Dx ,\nwhere R\u03c80 and R\u03c81 are two conditioning relations in Definition 6.\nNote that two worlds are related by R\u03b5,Dx if they have close probability distributions of the values of x. Intuitively, w |= \u03c80 \u223c\u03b5,Dx \u03c81 corresponds to the two operations: (i) transforming the given dataset w to the two sub-datasets w|\u03c80 and w|\u03c81 , and (ii) testing whether the probability distribution of x generated by the dataset w|\u03c80 is indistinguishable from the distribution generated by the dataset w|\u03c81 .\nWhen \u03b5 = 0, the operator \u223c\u03b5,Dx represents the identity of two distributions.\n3 The semantics for the (binary) composite operator in the arrow logic [7] resembles that for \u223c\u03b5,Dx in Definition 7, although it has a totally different meaning and motivation.\nProposition 1 For a world w, static formulas \u03c80, \u03c81, and a measurement variable x, w |= \u03c80 \u223c0,Dx \u03c81 iff the distribution w|\u03c80(x) is identical to w|\u03c81(x).\nThis proposition is immediate from the following\nlemma.\nLemma 1 For a world w, static formulas \u03c80, \u03c81, and a measurement variable x,\nw |= \u03c80 \u223c\u03b5,Dx \u03c81 iff D(\u03c3w|\u03c80 (x) \u2016 \u03c3w|\u03c81 (x)) \u2264 \u03b5.\nProof Let w0 = w|\u03c80 and w1 = w|\u03c81 . Then by Definition 6, we have (w,w0) \u2208 R\u03c80 and (w,w1) \u2208 R\u03c81 . Hence this lemma follows from Definition 7. ut\nIn Sect. 7, we present examples using the conditional indistinguishability operator, i.e., we formalize various notions of fairness in machine learning by using this operator and the above proposition and lemma.\n3.5 Summary on the Modal Language\nIn summary, modal operators are used to represent transformation and testing on datasets. The unary modal operator\u2206T is regarded as a transformation T on datasets, while the binary modal operators \u2283 and \u223c\u03b5,Dx are regarded as transforming-then-testing on datasets.\nNow the syntax of the formulas is given by:\nStatic formulas: \u03c8 ::= \u03b3(x1, x2, . . . , xn) | \u00ac\u03c8 | \u03c8 \u2227 \u03c8 Dataset formulas: \u03d5 ::= PI \u03c8 | \u00ac\u03d5 |\u03d5 \u2227 \u03d5 |\u2206T\u03d5 |\u03c8 \u2283 \u03d5 |\u03c80 \u223c\u03b5,Dx \u03c81 | Ka \u03d5,\nwhere the epistemic formulas with the additional modality are called dataset formulas, since they are interpreted in a world that corresponds to a dataset.\nWhen multiple transformations/testing are sequentially applied to datasets, we can use dataset formulas in which different modal operators are nested. For example, w |= \u2206T (\u03c8 \u2283 \u03d5) represents that after applying a data preparation T to a dataset w, a property \u03d5 holds for the sub-dataset T (w)|\u03c8 that satisfies \u03c8."}, {"heading": "4 Epistemic Model for Supervised Learning", "text": "In this section we introduce a formal model for supervised learning. Specifically, we employ a distributional Kripke model (Definition 3), and formalize a behavior of a classifier C and a non-deterministic input x from an adversary in the model. In this formalization, we focus only on the testing of supervised learning models, and do not formalize the training of supervised learning models or learning algorithms themselves.\n4.1 Classification Problems\nMulticlass classification is the problem of classifying a given input into one of multiple classes. Let L be a finite set of class labels4, and D be a finite set of input data (called feature vectors) that we want to classify. Then a classifier is a function C : D \u2192 L that receives an input datum v and predicts which class (among L) the input v belongs to. In this work, we deal with a situation where some classifier C has already been obtained and its properties should be evaluated, and do not model or reason about how classifiers are trained from a training dataset.\nWe assume a scoring function f : D \u00d7 L \u2192 R that gives a score f(v, `) of predicting the class of an input datum (feature vector) v as a label `. Then for each input v \u2208 D, we denote by H(v) = ` to represent that a label ` maximizes f(v, `). For example, when the input v is an image of an animal and ` is the animal\u2019s name, then H(v) = ` may represent that an oracle (or a \u201chuman\u201d) classifies the image v as `.\n4.2 Modeling the Behaviors of Classifiers\nA classifier is formalized on a distributional Kripke model M = (W, (Ra)a\u2208A, (Vs)s\u2208S) withW = DS. Then W is an infinite set of possible worlds that corresponds to all possible datasets from which the classifier can receive input data. We denote by wtest \u2208 W a real world that corresponds to a test dataset. Recall that each world w \u2208 W is a multiset of states over S and is associated with a stochastic assignment \u03c3w : Mes \u2192 DO that is consistent with the deterministic assignments \u03c3s for all s \u2208 w, as explained in Sect. 2.4.\nWe present an overview of our formalization in Fig. 1.\nWe denote by x \u2208 Mes an input datum given to the classifier C (and to the oracle H), by y \u2208 Mes a correct label given by the oracle H, and by y\u0302 \u2208 Mes a label predicted by C. We assume that the input variable x (resp. the output variables y, y\u0302) ranges over the set D of input data (resp. the set L of labels); i.e., the deterministic assignment \u03c3s at each state s \u2208 S has the range O = D\u222aL and satisfies \u03c3s(x) \u2208 D and \u03c3s(y), \u03c3s(y\u0302) \u2208 L.\nA key idea in our modeling is that we describe logical aspects of statistical properties in the syntax level by using logical formulas, and model statistical distances and dataset operations in the semantics level by using accessibility relations in the distributional Kripke\n4 The regression can be regarded as the classification problem when the label ranges over the real numbers, hence it can be formalized using a distributional Kripke model analogously. For simplicity, however, we deal only with the classification problems in this paper.\nmodel. In this way, we can formalize various statistical properties of classifiers in a simple and abstract way.\nTo formalize the classifier C, we introduce a static formula \u03c8(x, y\u0302) to represent that C classifies a given input x as a class y\u0302. We also introduce a static formula h(x, y) to represent that y is the actual class of an input x. As an abbreviation, we write \u03c8`(x) (resp. h`(x)) to denote \u03c8(x, `) (resp. h(x, `)). Formally, these static formulas are interpreted at each state s \u2208 S as follows:\ns |= \u03c8(x, y\u0302) iff C(\u03c3s(x)) = \u03c3s(y\u0302). s |= h(x, y) iff H(\u03c3s(x)) = \u03c3s(y).\n4.3 Modeling the Non-deterministic Inputs from Adversaries\nWe first observe that a distributional Kripke model M can formalize an input x that is probabilistically chosen from a given dataset. As explained in Sect. 2.4, each world w corresponds to a test dataset. When a state s is drawn from a multiset w of states, an input value \u03c3s(x) is sampled from the distribution \u03c3w(x), and assigned to the measurement variable x. The set of all possible probability distributions of inputs is represented by \u039b def = {\u03c3w(x) | w \u2208 W}, which is possibly an infinite set. For example, let us consider testing the classifier C with the actual test dataset \u03c3wtest(x). When C classifies an input x as a label ` with probability 0.2, i.e.,\nPr [ v $\u2190 \u03c3wtest(x) : C(v) = ` ] = 0.2,\nthen this can be expressed by:\nM,wtest |= P0.2 \u03c8`(x).\nNext we observe that our model can formalize a nondeterministic input x from an adversary as follows. Although each state s in a possible world w is assigned the\nprobability w[s], each world w itself is not assigned a probability. Thus, each input distribution \u03c3w(x) \u2208 \u039b itself is also not assigned a probability, hence our model assumes no probability distribution over \u039b. In other words, we assume that a world w and thus an input distribution \u03c3w(x) are non-deterministically chosen. This is useful to model an adversary that provides malicious inputs to the classifier C to make its prediction fail, because we usually do not have a prior knowledge of the probability distribution of malicious inputs from adversaries, and need to reason about the worst cases caused by the attack. In Sect. 6, this formalization of non-deterministic inputs is used to express the robustness of classifiers.\nFinally, it should be noted that we cannot enumerate all possible adversarial inputs, hence cannot enumerate all possible datasets to construct the universe W. Since W can be an infinite set and is unspecified, we cannot check whether a formula expressing a security property against an adversary is satisfied in all possible worlds of W. Nevertheless, as shown in later sections, describing various properties using our extension of StatEL is useful to explore desirable properties and to discuss relationships among them."}, {"heading": "5 Formalizing the Classification Performance", "text": "In this section we show a formalization of classification performance using our extension of StatEL. We formalize popular measures of classification performance, including precision, recall, and accuracy, and measures for evaluating overfitting, such as the generalization error. See Fig. 2 for basic ideas on these formalizations.\n5.1 Classifier\u2019s Prediction and its Correctness\nIn classification problems, the terms positive/negative represent the result of the classifier\u2019s prediction, and the terms true/false represent whether the classifier predicts correctly or not. Then the following terminologies are commonly used:\n\u2013 true positive (tp): both the prediction and actual\nclass are positive;\n\u2013 true negative (tn): both the prediction and actual\nclass are negative;\n\u2013 false positive (fp): the prediction is positive but the\nactual class is negative;\n\u2013 false negative (fn): the prediction is negative but\nthe actual class is positive.\nThese terminologies can be formalized using static formulas as shown in Table 1. For example, when an input x shows true positive at a state s, this can be expressed as s |= \u03c8`(x) \u2227 h`(x). Note that the value of the measurement variable x is uniquely determined by the assignment \u03c3s at the state s. True negative, false positive (type I error), and false negative (type II error) are respectively expressed as s |= \u00ac\u03c8`(x)\u2227\u00ach`(x), s |= \u03c8`(x) \u2227 \u00ach`(x), and s |= \u00ac\u03c8`(x) \u2227 h`(x).\n5.2 Precision, Recall, Accuracy, and Other Performance Measures\nNext we formalize three popular measures for binary classification performance: precision, recall, and accuracy. In Table 1 we summarize the formalization of various notions of classification performance using our dataset formulas.\nIn theory, these notions should be formalized with the infinite dataset w true representing the true distribution. However, we usually cannot obtain w true or test the performance measures using w true. Hence, we often\nsample a finite test dataset wtest from the true distribution and regard it as an approximation of w true. 5\nGiven a test dataset wtest, precision (positive predictive value) is defined as the conditional probability that the prediction is correct given that the prediction is positive; i.e., precision = tptp+fp . Since the probability distribution of the input x in the world wtest is expressed by \u03c3wtest(x) as explained in Sect. 4.3, the precision being within an interval I is given by:\nPr [ v $\u2190 \u03c3wtest(x) : H(v) = ` \u2223\u2223\u2223 C(v) = ` ] \u2208 I,\nwhich can be written as: Pr [ s $\u2190 wtest : s |= h`(x) \u2223\u2223\u2223 s |= \u03c8`(x) ] \u2208 I.\nBy using StatEL, this can be formalized as:\nM,wtest |= Precision`,I(x)\nwhere Precision`,I(x) def = \u03c8`(x) \u2283 PI h`(x).\nHere \u2283 is the conditioning operator defined in Sect. 3.3. The value of precision depends on the test dataset wtest, and can be computed in finite time since wtest is finite.\nSymmetrically, recall (true positive rate) is defined as the conditional probability that the prediction is correct given that the actual class is positive; i.e., recall = tp tp+fn . Then the recall being within I is formalized as:\nRecall`,I(x) def = h`(x) \u2283 PI \u03c8`(x).\nFinally, accuracy is the probability that the classifier predicts correctly; i.e., accuracy = tp+tntp+tn+fp+fn . Then the accuracy being within I is formalized as:\nAccuracy`,I(x) def = PI ( \u03c8`(x)\u2194 h`(x) ) ,\n5 Since the test dataset wtest is finite, there can be missing data that are not included in wtest but are sampled from the true distribution w true with a very small probability.\nwhich can also be defined as PI ( tp(x) \u2228 tn(x) ) . When we measure the accuracy after a data preparation operation T (e.g., data cleaning) to the test dataset wtest, this can be represented by wtest |= \u2206TAccuracy`,I(x).\nExample 1 (Performance of pedestrian detection) Let us consider an autonomous car that uses a machine learning classifier to detect a person crossing the road. For the sake of simplicity, we formalize an example of a binary classifier C that detects whether or not a pedestrian is crossing the road in a photo image in a test dataset wtest. We write sunny(x) (resp. snowy(x)) to represent that a photo x was taken on a sunny (resp. snowy) day. Let \u03c8`(x) (resp. h`(x)) represent that the classifier C (resp. the human) detects a pedestrian crossing the road in an image x.\nWe empirically measure recall (i.e., the conditional probability that C detects a pedestrian crossing the road when the input image x actually includes it) by using the data collected on sunny days. When C achieves a recall of 0.95 on sunny days, this is represented by wtest |= sunny(x) \u2283 Recall`,0.95(x). Since C should detect a pedestrian also on a snowcovered road, it should be tested with the data collected on snowy days. If we have a recall of 0.8 on snowy days, this is represented by wtest |= snowy(x) \u2283 Recall`,0.8(x). More generally, if the classifier C achieves a recall of more than 0.9 in situations \u03b31, \u03b32, . . . , \u03b3m, this can be\nrepresented by wtest |= \u2227m i=1 ( \u03b3i(x) \u2283 Recall`,(0.9,1](x) ) .\n5.3 Generalization Error\nWe next formalize the generalization error of a classifier, i.e., a measure of how accurately a classifier is able\nto predict the class of previously unseen input data. Since a classifier has been trained on a finite sample training dataset w train, it may be overfitted to w train and have worse classification performance on new input data that have not been included in w train.\nTo formalize the generalization error, we introduce a formula \u03bbL(y, y\u0302) to represent that given a correct label y and a predicted label y\u0302, the expected value of losses (i.e., real numbers representing the penalty for incorrect classification) is at most a non-negative real number L. Formally, the semantics of \u03bbL(y, y\u0302) is given by:\nw |= \u03bbL(y, y\u0302) iff E (v,v\u0302)\u223c\u03c3w(y,y\u0302) loss(v, v\u0302) \u2264 L,\nwhere loss is a loss function selected according to the data domain O, and a pair (v, v\u2032) of a correct label and a predicted label follows the joint distribution \u03c3w(y, y\u0302).\nNow the generalization error being L or smaller at a true distribution w true is written as w true |= GEL(x, y, y\u0302) where:\nGEL(x, y, y\u0302) def = ( h(x, y) \u2227 \u03c8(x, y\u0302) ) \u2283 \u03bbL(y, y\u0302).\nSince we usually cannot obtain the true distribution w true and cannot check the satisfaction w true |= GEL(x, y, y\u0302), we often compute an empirical error (as an approximation of the generalization error) by using a finite test dataset wtest that is believed to be an approximation of w true. This testing can be expressed as wtest |= GEL(x, y, y\u0302). On the other hand, given a training dataset w train, the training error being at most Ltrain is represented by w train |= GELtrain(x, y, y\u0302). Then the overfitting of the classifier can be evaluated by comparing the empirical error L with the training error Ltrain. When the empirical error is smaller than Ltrain +\u03b5 for some error bound \u03b5 > 0, this can be represented by wtest |= GELtrain+\u03b5(x, y, y\u0302)."}, {"heading": "6 Formalizing the Robustness of Classifiers", "text": "Many recent studies have found attacks on machine learning where a malicious adversary manipulates the input to cause a malfunction in a machine learning task [12]. Such input data, called adversarial examples [40], are designed to make a classifier fail to predict the actual class ` of the input, but are recognized to belong to ` from human eyes. In computer vision, for example, Goodfellow et al. [20] create an adversarial example by adding undetectable noise to a panda\u2019s photo so that humans can still recognize the perturbed image as a panda, but a classifier misclassifies it as a gibbon. To prevent or mitigate such attacks, the classifier should be robust against perturbed input, i.e., it should return similar predicted labels given similar input data.\nIn this section we formalize robustness notions for classifiers by using epistemic operators in StatEL (See Fig. 3 for an overview of the formalization). Furthermore, we show certain relationships between classification performance and robustness, and suggest a class of robustness properties that have not been formalized in the literature as far as we know. We present an overview of these formalizations and relationships in Fig. 4.\n6.1 Total Correctness of Classifiers\nWe first note that the total correctness of classifiers could be formalized as a classification performance (e.g., precision, recall, or accuracy) in the presence of all possible inputs from adversaries. For example, the total correctness could be formalized as M |= Recall`,I(x), which represents that Recall`,I(x) is satisfied in all possible worlds of M.\nIn practice, however, it is not possible or tractable to test whether the classification performance is achieved\nfor all possible test datasets (corresponding to an infinite number of possible worlds in M). Hence we need a weaker form of a correctness notion, which may be verified or tested in some way. In the following sections, we deal with robustness notions that are weaker than total correctness.\n6.2 Accessibility Relation for Robustness\nTo formalize robustness notions, we introduce an accessibility relation R\u03b5,Wdx that relates two worlds having closer inputs as follows.\nDefinition 8 (Accessibility relation for robustness) We define an accessibility relation R\u03b5,Wdx \u2286 W \u00d7 W by:\nR\u03b5,Wdx def = {(w,w\u2032) \u2208 W \u00d7W | Wd(\u03c3w(x), \u03c3w\u2032(x)) \u2264 \u03b5},\nwhere Wd is \u221e-Wasserstein distance w.r.t. a metric d in Definition 2.\nThen (w,w\u2032) \u2208 R\u03b5,Wdx represents that the two distributions \u03c3w(x) and \u03c3w\u2032(x) of inputs to the classifier C are close in terms of the distance Wd . 6 Intuitively, for example, Wd means the distance between two image datasets \u03c3w(x) and \u03c3w\u2032(x) when the distance between individual images are measured by a metric d .\nThen an epistemic formula K\u03b5,Wd \u03d5 represents that we are confidence that \u03d5 is true even when the input data are perturbed by noise of the level \u03b5 or smaller.\n6 Wd(\u03c3w(x), \u03c3w\u2032(x)) \u2264 \u03b5 expresses that each value of the input x from the dataset w is close to the corresponding value of x from w\u2032 in terms of the metric d between individual data. For example, each input image x in the dataset w looks similar to the corresponding image in w\u2032 from the human\u2019 eyes.\n6.3 Probabilistic Robustness against Targeted Attacks\nWhen a robustness attack aims at misclassifying an input as a specific target label \u02c6\u0300tar, then it is called a targeted attack. For instance, in the above-mentioned attack by [20], a gibbon is the target into which a panda\u2019s photo is misclassified.\nIn this section, we discuss how we formalize robustness using the epistemic operator K\u03b5,Wd . We denote by v \u2208 D an original input image in the test dataset wtest, and by v\u0303 \u2208 D an image obtained by perturbing the original image v by noise.\nA first definition of robustness against targeted at-\ntacks might be:\nFor any v, v\u0303 \u2208 D, ifH(v) = panda and d(v, v\u0303) \u2264 \u03b5, then C(v\u2032) 6= gibbon,\nwhich represents that when an image v\u0303 is obtained by perturbing a panda\u2019s photo v by noise, then it will not be classified as the target label gibbon at all. This can be formalized using StatEL by:\nM,wtest |= hpanda(x) \u2283 K\u03b5,Wd P0 \u03c8gibbon(x).\nHowever, this notion does not accept a negligible probability of misclassification, and does not cover the case where the human cannot recognize the perturbed image v\u0303 as panda (e.g., when the perturbed image v\u0303 is obtained by linear displacement, rescaling, and rotation [2], then H(v\u0303) 6= panda may hold). To overcome these issues, we introduce the following definition with some conditional probability \u03b4 of misclassification as follows.\nDefinition 9 (Targeted robustness) Let \u03b4 \u2208 [0, 1]. Given a dataset wtest, a classifier C satisfies probabilistic targeted robustness w.r.t. an actual label ` and a target label \u02c6\u0300tar if for any input v \u2208 supp(\u03c3wtest(x)) from the dataset wtest, and for any perturbed input v\u0303 \u2208 D s.t. d(v, v\u2032) \u2264 \u03b5, we have:\nPr[C(v\u0303) = \u02c6\u0300tar | H(v\u0303) = ` ] \u2264 \u03b4. (1)\nFor instance, when the actual class ` is panda and the target label \u02c6\u0300tar is gibbon, then the classifier C misclassifies a panda\u2019s photo as gibbon with only a small probability \u03b4.\nNow we express this robustness notion with I =\n[1\u2212 \u03b4, 1] by using StatEL.\nProposition 2 (Targeted robustness) Let I \u2286 [0, 1]. The probabilistic targeted robustness w.r.t. an actual label ` and a target label \u02c6\u0300tar under a given test dataset wtest is expressed by wtest |= TRobust`,\u02c6\u0300tar,I(x) where:\nTRobust`,\u02c6\u0300tar,I(x) def = K\u03b5,Wd ( h`(x) \u2283 PI \u00ac\u03c8\u02c6\u0300 tar (x) ) .\nProof Let w\u2032 be a possible world such that (wtest, w \u2032) \u2208 R\u03b5,Wdx . Then w\u2032 corresponds to the dataset obtained by perturbing each data in w. Let v\u0303 \u2208 supp(\u03c3w\u2032(x)). Then v\u0303 represents a perturbed input. Let w\u2032\u2032 = w\u2032|h`(x). Then (1) is logically equivalent to w\u2032\u2032 |= P[0,\u03b4] \u03c8\u02c6\u0300tar (x). By Definition 6, w\u2032 |= h`(x) \u2283 P[0,\u03b4] \u03c8\u02c6\u0300tar (x). By I = [1 \u2212 \u03b4, 1], w\u2032 |= h`(x) \u2283 PI \u00ac\u03c8\u02c6\u0300\ntar (x). Therefore this\nproposition follows from the semantics for K\u03b5,Wd . ut\nSince the Lp-distances7 are often regarded as reasonable approximations of human perceptual distances [10], they are used as distance constraints on the perturbation in many researches on targeted attacks (e.g. [40,20, 10]). Our model can represent the robustness against these attacks by using the Lp-distance as a metric d for R\u03b5,Wdx .\n6.4 Probabilistic Robustness against Non-Targeted Attacks\nIn this section we formalize non-targeted attacks [33,32] in which adversaries try to misclassify inputs as some arbitrary incorrect labels (i.e., not as a specific label like a gibbon). Compared to targeted attacks, this kind of attacks are easier to mount, but harder to defend.\nWe first define the notion of robustness against non-\ntargeted attacks as follows.\nDefinition 10 (Non-targeted robustness) Let \u03b4 \u2208 [0, 1]. Given a dataset wtest, a classifier C satisfies probabilistic non-targeted robustness w.r.t. an actual label ` if for any input v \u2208 supp(\u03c3wtest(x)) from the dataset wtest, and for any perturbed input v\u0303 \u2208 D s.t. d(v, v\u2032) \u2264 \u03b5, we have:\nPr[C(v\u0303) = ` | H(v\u0303) = ` ] > 1\u2212 \u03b4.\nNow we express this robustness notion with I =\n[1\u2212 \u03b4, 1] by using StatEL.\nProposition 3 (Non-targeted robustness) Let I \u2286 [0, 1]. The probabilistic non-targeted robustness under a test dataset wtest is expressed by wtest |= Robust`,I(x) where:\nRobust`,I(x) def = K\u03b5,Wd ( h`(x) \u2283 PI \u03c8`(x) ) = K\u03b5,Wd Recall`,I(x).\nProof The proof is analogous to that for Proposition 2.\nut 7 The Lp-distance between n-dimensional real vectors x and x\u2032 is written \u2016x \u2212 x\u2032\u2016p where the p-norm is defined by \u2016v\u2016p = ( \u2211n i=1 |vi|p)1/p.\n6.5 Relationships among Robustness Notions\nIn this section we present relationships among notions of robustness and performance, and discuss properties related to robustness.\nWe first present the following proposition immediate\nfrom the definitions.\nProposition 4 (Relationships among notions) Let I \u2286 [0, 1] and `, \u02c6\u0300tar \u2208L. Then we have: 1. wtest |= Robust ,\u0300I(x) implies wtest |= TRobust ,\u0300\u02c6\u0300tar,I(x). 2. wtest |= Robust`,I(x) implies M,wtest |= Recall`,I(x).\nThe first claim means that probabilistic non-targeted\nrobustness is not weaker than probabilistic targeted robustness for the same I. The second claim means that probabilistic non-targeted robustness implies recall without perturbation noise. Note that this is immediate from the reflexivity of R\u03b5,Wdx . Next we remark that our extension of StatEL can be used to describe a certain situation where adversarial attacks are mitigated. When we apply some mechanism T that preprocesses a given input to mitigate attacks on robustness, then the probabilistic targeted robustness is expressed as wtest |= \u2206TRobust`,I(x) where \u2206T is the modality for the dataset transformation T .\nFinally, we recall that by Proposition 3, robustness can be regarded as recall in the presence of perturbed noise. This implies that for each property \u03d5 in the table of confusion (Table 1), we could consider K\u03b5,Wd \u03d5 as a property to evaluate the classification performance in the presence of adversarial inputs although this has not been formalized in the literature of robustness of machine learning as far as we recognize. For example, precision robustness K\u03b5,Wd Precision`,i(x) represents that in the presence of perturbed noise, the prediction is correct with a probability i given that it is positive. For another example, accuracy robustness K\u03b5,Wd Accuracy`,i(x) represents that in the presence of perturbed noise, the prediction is correct (whether it is positive or negative) with a probability i.\nExample 2 (Robustness of pedestrian detection) We illustrate robustness notions using the pedestrian detection in Example 1 in Section 5.2. We deal with a binary classifier C that detects whether a pedestrian is crossing the road in a photo image x.\nThe non-targeted robustness K\u03b5,Wd Recall`,0.9(x) represents that in the presence of perturbed noise to the input image x, with probability 0.9 the classifier C can detect a person crossing the road when the human can actually recognize. This robustness is crucial for an autonomous car not to hit a pedestrian.\nThe precision robustness K\u03b5,Wd Precision`,0.9(x) represents that in the presence of perturbed noise to x, with probability 0.9 the human can actually recognize a person crossing the road when the classifier C detects it. This type of robustness is important for an autonomous car to avoid stopping suddenly due to a false alarm (not take the crash from the car behind)."}, {"heading": "7 Formalizing the Fairness of Classifiers", "text": "Many studies have proposed and investigated various notions of fairness in machine learning [5]. Informally, these fairness notions mean that the results of machine learning tasks are irrelevant of some sensitive attributes, e.g., gender, age, race, disease, political/religious view. In a recently few years, there have been studies on the testing methods for fairness of machine learning [18,1, 42].\nIn this section, we formalize popular notions of fairness of supervised learning by using our extension of StatEL. Here we focus on the fairness that should be maintained in the impact (i.e., the results of machine learning tasks) rather than the treatment (i.e., the process of machine learning tasks). This is because previous research show that many seemingly neutral features have statistical relationships with sensitive attributes, and hence just ignoring or removing sensitive attributes\nin the process of data preparation and training8 is often ineffective or harmful to achieve the fairness and performance of learning tasks.\n7.1 Basic Ideas and Notations\nVarious notions of fairness in supervised learning are classified into three categories: independence, separation, and sufficiency [5]. All of these have the form of (conditional) independence or its relaxation, and thus can be formalized using the modal operator \u223c\u03b5,Dx for conditional indistinguishability (defined in Sect. 3.4) in our extension of StatEL.9\nIn the formalization of fairness notions, we use a distributional Kripke model M = (W, (Ra)a\u2208A, (Vs)s\u2208S). Recall that x, y, and y\u0302 are measurement variables respectively denoting the input datum, the actual class label (given by the oracle H), and the predicted label (output by the classifier C). Given a real world wtest (corresponding to a given test dataset), \u03c3wtest(x) is the probability distribution of C\u2019s test input over D, \u03c3wtest(y) is the distribution of the actual label over L, and \u03c3wtest(y\u0302) is the distribution of C\u2019s output over L.\nFairness notions are usually defined in terms of some sensitive attribute (e.g., gender, age, race, disease, political/religious view), which is defined as a tuple of subsets of the input data domain D. For example, a sensitive attribute based on ages can be defined as a pair of groups G0 (input data with ages 21 to 60) and G1 (ages 61 to 100). For each group G \u2286 D of inputs, we introduce a static formula \u03b7G(x) representing that an input x belongs to G. Formally, this is interpreted by:\nFor each state s \u2208 S, s |= \u03b7G(x) iff \u03c3s(x) \u2208 G.\nRoughly speaking, a machine learning task is said to be fair if the performance of the task for a group G0\u2019s input is similar to that for another group G1\u2019s input.10 In the following sections, we formalize the three categories of fairness of classifiers and their relaxation. A summary of this formalization is presented in Table 2.\n8 Such unawareness requires that sensitive attributes are not explicitly used in the learning process. However, StatEL may not be suited to formalizing this requirement. 9 Compared to the preliminary version [28] of this paper, we corrected errors and changed the formalization into a more comprehensible form by introducing the operator \u223c\u03b5,Dx and by removing the counter factual epistemic operators and a formula \u03bed representing that the input is drawn from a dataset d. 10 Some fairness notions (e.g., equal opportunity) assume G1 = D \\G0.\n7.2 Independence (a.k.a. Group Fairness, Statistical Parity) and its Relaxation\nIn this section we explain and formalize the notion of independence [9], which is also known as group fairness [15] 11, and its relaxed notion. Intuitively, independence means that the predicted label y\u0302 does not have statistical relationships with the membership in a sensitive group. For example, independence does not allow a bank\u2019s lending rate to be correlated with a sensitive attribute such as gender.\nWe first present the definition of a relaxed notion of independence, called group fairness up to bias \u03b5 [15] as follows. Intuitively, this is the property that the output distributions of the classifier are roughly identical when input data belong to different groups.\nFormally, this fairness notion is defined as follows.\nDefinition 11 (Independence, group fairness) Let G0, G1 \u2286 D be sets of input data constituting a sensitive attribute. For each b = 0, 1, let \u00b5Gb \u2208 DL be the probability distribution of the predicted label \u02c6\u0300 output by a classifier C when an input v is sampled from a test dataset wtest and belongs to Gb; i.e., for each \u02c6\u0300\u2208 L,\n\u00b5Gb [ \u02c6\u0300] def = Pr[C(v) = \u02c6\u0300 | v $\u2190 \u03c3wtest(x) and v \u2208 Gb ]. (2)\nThen a classifier C satisfies the group fairness between groups G0 and G1 up to bias \u03b5 if Dtv(\u00b5G0\u2016\u00b5G1) \u2264 \u03b5, where Dtv is the total variation between distributions (defined in Sect. 2.2). A classifier C satisfies independence w.r.t. groups G0 and G1 if it satisfies the group fairness between G0 and G1 up to bias 0.\nNow we express this fairness notion using our ex-\ntension of StatEL as follows.\nProposition 5 (Independence, group fairness) The group fairness between groups G0 and G1 up to bias \u03b5 under a given test dataset wtest is expressed as wtest |= GrpFair\u03b5(x, y\u0302) where:\nGrpFair\u03b5(x, y\u0302) def = ( \u03b7G0(x)\u2227\u03c8(x, y\u0302) ) \u223c\u03b5,Dtvy\u0302 ( \u03b7G1(x)\u2227\u03c8(x, y\u0302) ) .\nIndependence (without bias \u03b5) is expressed by wtest |= GrpFair0(x, y\u0302).\nProof Let wb = wtest|\u03b7Gb (x)\u2227\u03c8(x,y\u0302). It follows from (2) that for each \u02c6\u0300\u2208 L, \u00b5Gb [\u02c6\u0300] = Pr[\u03c3s(y\u0302) = \u02c6\u0300 | s $\u2190 wb ], hence \u00b5Gb = \u03c3wb(y\u0302). Thus, by Definition 11, the group fairness between groups G0 and G1 up to bias \u03b5 is given by Dtv(\u03c3w0(y\u0302)\u2016\u03c3w1(y\u0302)) \u2264 \u03b5. Therefore, this proposition follows from Lemma 1. ut 11 In previous literature, independence has been referred to also as different terminologies, such as statistical parity, demographic parity, and disparate impact.\nExample 3 (Independence in pedestrian detection) We illustrate independence using the pedestrian detection in Example 1 in Section 5.2. We deal with a binary classifier C that detects whether or not a pedestrian is crossing the road in an image x. We write \u03b7m(x) (resp. \u03b7w(x)) to represent that an image x includes a man (resp. woman) that may or not be crossing the road. Let \u03c8(x, y\u0302) represent that given an input image x, the classifier C returns y\u0302 (that is either the detection of a person crossing the road or not).\nThen the independence between men and women GrpFair0(x, y\u0302) def = ( \u03b7m(x)\u2227\u03c8(x, y\u0302) ) \u223c0,Dtvy\u0302 ( \u03b7w(x)\u2227\u03c8(x, y\u0302) ) means that the probability of detecting a pedestrian crossing the road is the same between men and women. This fairness guarantees that men and women are equally detectable as pedestrians, hence equally safe against an autonomous car. Here independence does not rely on the actual label y, i.e., on whether there is a pedestrian crossing the road that can be detected by human eyes.\n7.3 Separation (a.k.a. Equalized Odds) and its Relaxation (Equal Opportunity)\nIn this section we explain and formalize the notion of separation [5] 12, which is well-known as equalized odds [22], and its relaxed notion called equal opportunity [22]. The motivation behind these notions is to capture typical scenarios in which sensitive characteristics may have statistical relationships with the actual class label. For instance, even when some sensitive attribute is correlated with an actual default rate on loans, banks might want to have a different lending rate for people who have a higher default rate. However, independence\n12 In previous literature, separation has been referred to also as disparate mistreatment [46] and conditional procedure accuracy equality [6].\n(group fairness) does not allow this, since it requires that the lending rate should be statistically independent of the sensitive attribute.\nTo overcome this problem, the notion of separation allows statistical relationships between a sensitive attribute and the predicted label y\u0302 output by the classifier C to the extent that this is justified by the actual class label y. More precisely, separation means that the predicted label y\u0302 is conditionally independent of the membership in a sensitive group, given an actual class label y.\nFormally, separation is defined as a property that recall (true positive rate) and specificity (true negative rate, explained in Table 1) are the same for all the groups, and equal opportunity is defined as a special case of separation only for an advantageous class label.\nDefinition 12 (Separation & equal opportunity) Given a group Gb \u2286 D and an actual class label `, let \u00b5Gb,` \u2208 DL be the probability distribution of the predicted label \u02c6\u0300 output by a classifier C when an input v \u2208 Gb is sampled from a test dataset wtest and is associated with an actual label `; i.e., for each \u02c6\u0300\u2208 L,\n\u00b5Gb,`[ \u02c6\u0300] def = Pr[C(v) = \u02c6\u0300| v $\u2190\u03c3wtest(x), v \u2208 Gb, H(v)= ` ].\n(3)\nA classifier C satisfies separation between two groups G0 and G1 if \u00b5G0,` = \u00b5G1,` holds for all ` \u2208 L. A classifier C satisfies equal opportunity of an advantageous label ` w.r.t. a group G0 if \u00b5G0,` = \u00b5G1,` where G1 = D \\G0.\nNow we express these two notions using our exten-\nsion of StatEL as follows.\nProposition 6 (Separation) Let \u03b3(x, `, y\u0302) def = \u03c8(x, y\u0302) \u2227h`(x). The separation between two groups G0 and G1\nunder a given test dataset wtest is expressed as wtest |= EqOdds0(x, y\u0302) where:\nEqOdds\u03b5(x, y\u0302) def =\u2227\n`\u2208L\n(( \u03b7G0(x) \u2227 \u03b3(x, `, y\u0302) ) \u223c\u03b5,Dtvy\u0302 ( \u03b7G1(x) \u2227 \u03b3(x, `, y\u0302) )) .\nProof Let ` \u2208 L and wb,` = wtest|\u03b7Gb (x)\u2227\u03c8(x,y\u0302)\u2227h`(x). It follows from (3) that:\n\u00b5Gb,`[ \u02c6\u0300] = Pr[\u03c3s(y\u0302) = \u02c6\u0300 | s $\u2190 wb,` ],\nhence \u00b5Gb,` = \u03c3wb,`(y\u0302). Thus, by Definition 12, the separation between G0 and G1 is given by \u03c3w0,`(y\u0302) = \u03c3w1,`(y\u0302) for all ` \u2208 L. Therefore, this proposition follows from Proposition 1. ut\nIt should be noted that for \u03b5 > 0, EqOdds\u03b5(x, y\u0302) represents a relaxation of separation up to bias \u03b5 in terms of total variation Dtv.\nProposition 7 (Equal opportunity) Let \u03b3(x, `, y\u0302) def = \u03c8(x, y\u0302)\u2227h`(x). The equal opportunity of a label ` w.r.t. a group G0 under a given test dataset wtest is expressed as wtest |= EqOpp(x, y\u0302) where:\nEqOpp(x, y\u0302) def =( \u03b7G0(x) \u2227 \u03b3(x, `, y\u0302) ) \u223c0,Dtvy\u0302 ( \u00ac\u03b7G0(x) \u2227 \u03b3(x, `, y\u0302) ) .\nProof The proof of this proposition is similar to that of Proposition 6. LetG1 = D\\G0. By \u00b5Gb,` = \u03c3wb,`(y\u0302), the equal opportunity of ` w.r.t. G0 is given by \u03c3w0,`(y\u0302) = \u03c3w1,`(y\u0302). Therefore, this proposition follows from Proposition 1. ut\nExample 4 (Separation in pedestrian detection) We illustrate separation using the pedestrian detection in Example 3 where a binary classifier C detects whether a pedestrian is crossing the road in an image x. Let \u03c8(x, y\u0302) (resp. h(x, y)) represent that given an image x, the classifier C (resp. human) returns y\u0302 (resp. y) representing either detection or not.\nThe level of the inherent technical difficulty of detecting a female pedestrian may be different from that of a male pedestrian, because, for example, the physical appearance may tend to be different between women and men. If we take this possible difference into account, separation can be suited instead of independence.\nThe separation EqOdds0(x, y\u0302) between men and women guarantees that the conditional probability of detecting a pedestrian crossing the road when the human can actually recognize it, is the same between men and women. This fairness implies that (from the viewpoint of a pedestrian crossing the road) male and female pedestrians may be hit by an autonomous car as fairly as by the human-driven car.\n7.4 Sufficiency (a.k.a. Conditional Use Accuracy Equality)\nIn this section we explain and formalize the notion of sufficiency [5], which is also known as conditional use accuracy equality [6].\nWhile separation guarantees the equality of recall among different groups, sufficiency requires the equality of precision. More precisely, sufficiency is defined as the property that precision (positive predictive value) and negative predictive value (presented as NPV in Table 1) are the same for all the groups as follows.\nDefinition 13 (Sufficiency) Given a group Gb \u2286 D and a predicted label \u02c6\u0300, let \u00b5Gb,\u02c6\u0300 \u2208 DL be the probability distribution of the actual class label ` when an input v \u2208 Gb is sampled from a test dataset wtest and the classifier C outputs the predicted label \u02c6\u0300; i.e., for each ` \u2208 L,\n\u00b5Gb,\u02c6\u0300[` ] def = Pr[H(v) = ` | v $\u2190\u03c3wtest(x), v \u2208 Gb, C(v)= \u02c6\u0300 ].\n(4)\nA classifier C satisfies sufficiency between two groups G0 and G1 if \u00b5G0,\u02c6\u0300 = \u00b5G1,\u02c6\u0300 holds for all \u02c6\u0300\u2208 L.\nThen this notion can be expressed using our exten-\nsion of StatEL as follows.\nProposition 8 (Sufficiency) Let \u03b3\u2032(x, y, \u02c6\u0300) def = \u03c8\u02c6\u0300(x) \u2227h(x, y). The sufficiency between two groups G0 and G1 under a given test dataset wtest is expressed as wtest |= Sufficency0(x, y) where:\nSufficency\u03b5(x, y) def =\u2227\n\u02c6\u0300\u2208L\n(( \u03b7G0(x) \u2227 \u03b3\u2032(x, y, \u02c6\u0300) ) \u223c\u03b5,Dtvy ( \u03b7G1(x) \u2227 \u03b3\u2032(x, y, \u02c6\u0300) )) .\nProof Let \u02c6\u0300 \u2208 L and wb,\u02c6\u0300 = wtest|\u03b7Gb (x)\u2227\u03c8\u02c6\u0300(x)\u2227h(x,y). It follows from (4) that:\n\u00b5Gb,\u02c6\u0300[` ] = Pr[\u03c3s(y) = ` | s $\u2190 wb,\u02c6\u0300 ],\nhence \u00b5Gb,\u02c6\u0300 = \u03c3wb,\u02c6\u0300(y). Thus, by Definition 13, the sufficiency between G0 and G1 is given by \u03c3w0,\u02c6\u0300(y) = \u03c3w1,\u02c6\u0300(y) for all \u02c6\u0300\u2208 L. Therefore, this proposition follows from Proposition 1. ut\nIt should be noted that for \u03b5 > 0, Sufficency\u03b5(x, y) represents a relaxation of sufficiency up to bias \u03b5 in terms of total variation Dtv.\nExample 5 (Sufficiency in pedestrian detection) We illustrate sufficiency using the pedestrian detection in Example 3 where a classifier C detects whether a pedestrian is crossing the road in an image x. As mentioned in\nExample 4, the level of the inherent technical difficulty of detecting a male pedestrian may be different from that of a female pedestrian. Whereas separation guarantees the equality of recall between men and women, sufficiency guarantees that of precision.\nThe sufficiency Sufficency0(x, y) between men and women implies that the conditional probability that there is no pedestrian crossing the road when C detects it, is the same between men and women. From the viewpoint of the car driver, when C raises a false alarm and stops the car suddenly, we have no bias about which of men and women are more likely to trigger false alarms and to be blamed for that."}, {"heading": "8 Related Work", "text": "In this section, we provide a brief overview of related work on the specification of statistical machine learning and on epistemic logic for describing specification.\nDesirable properties of statistical machine learning. There have been a large number of papers on attacks and defences for deep neural networks [40,12]. Compared to them, however, not much work has been done to explore the formal specification of various properties of machine learning. Seshia et al. [38] present a list of desirable properties of DNNs (deep neural networks) although most of the properties are presented informally without mathematical formulas. As for robustness, Dreossi et al. [13] propose a unifying formalization of adversarial input generation in a rigorous and organized manner, although they formalize and classify attacks (as optimization problems) rather than define the robustness notions themselves.\nConcerning the fairness notions, Barocas et al. [5] survey various fairness notions and classify them into the three categories: independence, separation, and sufficiency. Gajane [17] surveys the formalization of fairness notions for machine learning and present some justification based on social science literature.\nEpistemic logic for describing specification. Epistemic logic [44] has been studied to represent and reason about knowledge and belief [16,21], and has been applied to describe various properties of distributed systems.\nThe BAN logic [8], proposed by Burrows, Abadi and Needham, is a notable example of epistemic logic used to model and verify the authentication in cryptographic protocols. To improve the formalization of protocols\u2019 behaviors, some epistemic approaches integrate process calculi [24,11].\nEpistemic logic has also been used to formalize and\nreason about privacy properties, including anonymity [39,\n19,29], receipt-freeness of electronic voting protocols [25], and privacy policy for social network services [34]. Temporal epistemic logic is used to express information flow security policies [3].\nConcerning the formalization of fairness notions, pre-\nvious work in formal methods has modeled different kinds of fairness involving timing by using temporal logic rather than epistemic logic. As far as we know, no previous work has formalized fairness notions of machine learning by using modal logic.\nFormalization of statistical properties. In studies of philosophical logic, Lewis [31] shows the idea that when a random value has various possible probability distributions, then those distributions should be represented on distinct possible worlds. Bana [4] puts Lewis\u2019s idea in a mathematically rigorous setting. Recently, a modal logic called statistical epistemic logic (StatEL) [27] has been proposed and used to formalize statistical hypothesis testing and the notion of differential privacy [14].\nTo describe statistical properties of machine learning models, this work uses StatEL to formalize the probabilistically chosen input to a learning model and the non-deterministically chosen dataset. However, we could possibly employ other logics (e.g., fuzzy logic [45] or Markov logic network [37]) by extending them to deal with statistical sampling and non-deterministic inputs. Exploring the possibility of different formalization using other logics is left for future work."}, {"heading": "9 Conclusion", "text": "In this paper we proposed an epistemic approach to the modeling of supervised learning and its desirable properties. Specifically, we employed a distributional Kripke model in which each possible world corresponds to a possible dataset and modal operators are interpreted as transformation and testing on datasets. Then we formalized various notions of the classification performance, robustness, and fairness of statistical classifiers by using our extension of statistical epistemic logic (StatEL). In this formalization, we clarified relationships among properties of classifiers, and relevance between classification performance and robustness.\nWe emphasize that this is the first attempt to use epistemic models and logical formulas to describe statistical properties of machine learning, and would be a starting point to develop theories of formal specification of machine learning.\nIn future work, we are planning to extend our framework to formally reason about system-level properties of learning-based systems. We are also interested in\ndeveloping a more general framework for the formal specification of machine learning associated with testing methods, as well as in implementing a prototype tool. Our future work will also include an extension of StatEL to formalize unsupervised learning and reinforcement learning.\nAcknowledgements I would like to thank the reviewers for their helpful and insightful comments. I am also grateful to Gergei Bana for his useful comments on part of a preliminary manuscript."}], "title": "An Epistemic Approach to the Formal Specification of Statistical Machine Learning", "year": 2020}