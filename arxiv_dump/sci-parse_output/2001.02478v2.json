{"abstractText": "A surge of interest in explainable AI (XAI) has led to a vast collection of algorithmic work on the topic. While many recognize the necessity to incorporate explainability features in AI systems, how to address real-world user needs for understanding AI remains an open question. By interviewing 20 UX and design practitioners working on various AI products, we seek to identify gaps between the current XAI algorithmic work and practices to create explainable AI products. To do so, we develop an algorithm-informed XAI question bank in which user needs for explainability are represented as prototypical questions users might ask about the AI, and use it as a study probe. Our work contributes insights into the design space of XAI, informs efforts to support design practices in this space, and identifies opportunities for future XAI work. We also provide an extended XAI question bank and discuss how it can be used for creating user-centered XAI.", "authors": [{"affiliations": [], "name": "Q. Vera Liao"}, {"affiliations": [], "name": "Daniel Gruen"}], "id": "SP:eede0f4f07ded6cc801259881dc2d692f28353fb", "references": [{"authors": ["Ashraf Abdul", "Jo Vermeulen", "Danding Wang", "Brian Y Lim", "Mohan Kankanhalli"], "title": "Trends and trajectories for explainable, accountable and intelligible systems: An hci research agenda", "venue": "In Proceedings of the 2018 CHI conference on human factors in computing systems", "year": 2018}, {"authors": ["Amina Adadi", "Mohammed Berrada"], "title": "Peeking inside the black-box: A survey on Explainable Artificial Intelligence (XAI)", "venue": "IEEE Access", "year": 2018}, {"authors": ["Saleema Amershi", "Maya Cakmak", "William Bradley Knox", "Todd Kulesza"], "title": "Power to the people: The role of humans in interactive machine learning", "venue": "AI Magazine 35,", "year": 2014}, {"authors": ["Daniel W Apley"], "title": "Visualizing the effects of predictor variables in black box supervised learning models", "venue": "arXiv preprint arXiv:1612.08468", "year": 2016}, {"authors": ["Osbert Bastani", "Carolyn Kim", "Hamsa Bastani"], "title": "Interpretability via model extraction", "venue": "arXiv preprint arXiv:1706.09773", "year": 2017}, {"authors": ["Victoria Bellotti", "Keith Edwards"], "title": "Intelligibility and accountability: human considerations in context-aware systems", "venue": "Human\u2013Computer Interaction", "year": 2001}, {"authors": ["Reuben Binns", "Max Van Kleek", "Michael Veale", "Ulrik Lyngs", "Jun Zhao", "Nigel Shadbolt"], "title": "It\u2019s Reducing a Human Being to a Percentage\u2019: Perceptions of Justice in Algorithmic Decisions", "venue": "In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems", "year": 2018}, {"authors": ["Nadia Boukhelifa", "Marc-Emmanuel Perrin", "Samuel Huron", "James Eagan"], "title": "How data workers cope with uncertainty: A task characterisation study", "venue": "In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems", "year": 2017}, {"authors": ["Glenn A Bowen"], "title": "Grounded theory and sensitizing concepts", "venue": "International journal of qualitative methods 5,", "year": 2006}, {"authors": ["Sylvain Bromberger"], "title": "On what we know we don\u2019t know: Explanation, theory, linguistics, and how questions shape them", "year": 1992}, {"authors": ["Carrie J Cai", "Jonas Jongejan", "Jess Holbrook"], "title": "The effects of example-based explanations in a machine learning interface", "venue": "In Proceedings of the 24th International Conference on Intelligent User Interfaces", "year": 2019}, {"authors": ["Diogo V Carvalho", "Eduardo M Pereira", "Jaime S Cardoso"], "title": "Machine Learning Interpretability: A Survey on Methods and Metrics", "venue": "Electronics 8,", "year": 2019}, {"authors": ["Bruce Chandrasekaran", "Michael C Tanner", "John R Josephson"], "title": "Explaining control strategies in problem solving", "venue": "IEEE Intelligent Systems", "year": 1989}, {"authors": ["Hao-Fei Cheng", "Ruotong Wang", "Zheng Zhang", "Fiona O\u2019Connell", "Terrance Gray", "F Maxwell Harper", "Haiyi Zhu"], "title": "Explaining Decision-Making Algorithms through UI: Strategies to Help Non-Expert Stakeholders", "venue": "In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems", "year": 2019}, {"authors": ["William J Clancey"], "title": "The epistemology of a rule-based expert system\u00e2\u0102\u0164a framework for explanation", "venue": "Artificial intelligence 20,", "year": 1983}, {"authors": ["Juliet Corbin", "Anselm L Strauss", "Anselm Strauss"], "title": "Basics of qualitative research", "year": 2015}, {"authors": ["Sanjeeb Dash", "Oktay Gunluk", "Dennis Wei"], "title": "Boolean decision rules via column generation", "venue": "In Advances in Neural Information Processing Systems", "year": 2018}, {"authors": ["Amit Dhurandhar", "Pin-Yu Chen", "Ronny Luss", "Chun-Chen Tu", "Paishun Ting", "Karthikeyan Shanmugam", "Payel Das"], "title": "Explanations based on the missing: Towards contrastive explanations with pertinent negatives", "venue": "In Advances in Neural Information Processing Systems", "year": 2018}, {"authors": ["Nicholas Diakopoulos"], "title": "Algorithmic accountability: Journalistic investigation of computational power structures", "venue": "Digital journalism 3,", "year": 2015}, {"authors": ["Jonathan Dodge", "Q Vera Liao", "Yunfeng Zhang", "Rachel KE Bellamy", "Casey Dugan"], "title": "Explaining models: an empirical study of how explanations impact fairness judgment", "venue": "In Proceedings of the 24th International Conference on Intelligent User Interfaces", "year": 2019}, {"authors": ["Finale Doshi-Velez", "Been Kim"], "title": "Towards a rigorous science of interpretable machine learning", "venue": "arXiv preprint arXiv:1702.08608", "year": 2017}, {"authors": ["Malin Eiband", "Hanna Schneider", "Mark Bilandzic", "Julian Fazekas-Con", "Mareike Haug", "Heinrich Hussmann"], "title": "Bringing transparency design into practice", "venue": "In 23rd International Conference on Intelligent User Interfaces", "year": 2018}, {"authors": ["Thomas Erickson", "Catalina M Danis", "Wendy A Kellogg", "Mary E Helander"], "title": "Assistance: the work practices of human administrative assistants and their implications for it and organizations", "venue": "In Proceedings of the 2008 ACM conference on Computer supported cooperative work", "year": 2008}, {"authors": ["Jerome H Friedman"], "title": "Greedy function approximation: a gradient boosting machine", "venue": "Annals of statistics", "year": 2001}, {"authors": ["Leilani H Gilpin", "David Bau", "Ben Z Yuan", "Ayesha Bajwa", "Michael Specter", "Lalana Kagal"], "title": "Explaining explanations: An overview of interpretability of machine learning", "venue": "IEEE 5th International Conference on data science and advanced analytics (DSAA)", "year": 2018}, {"authors": ["Alyssa Glass", "Deborah L McGuinness", "Michael Wolverton"], "title": "Toward establishing trust in adaptive agents", "venue": "In Proceedings of the 13th international conference on Intelligent user interfaces", "year": 2008}, {"authors": ["Alex Goldstein", "Adam Kapelner", "Justin Bleich", "Emil Pitkin"], "title": "Peeking inside the black box: Visualizing statistical learning with plots of individual conditional expectation", "venue": "Journal of Computational and Graphical Statistics 24,", "year": 2015}, {"authors": ["Ian J Goodfellow", "Jonathon Shlens", "Christian Szegedy"], "title": "Explaining and harnessing adversarial examples", "venue": "arXiv preprint arXiv:1412.6572", "year": 2014}, {"authors": ["Shirley Gregor", "Izak Benbasat"], "title": "Explanations from intelligent systems: Theoretical foundations and implications for practice", "venue": "MIS quarterly", "year": 1999}, {"authors": ["Riccardo Guidotti", "Anna Monreale", "Salvatore Ruggieri", "Dino Pedreschi", "Franco Turini", "Fosca Giannotti"], "title": "Local rule-based explanations of black box decision systems", "year": 2018}, {"authors": ["Riccardo Guidotti", "Anna Monreale", "Salvatore Ruggieri", "Franco Turini", "Fosca Giannotti", "Dino Pedreschi"], "title": "A survey of methods for explaining black box models", "venue": "ACM computing surveys (CSUR) 51,", "year": 2019}, {"authors": ["Andreas Henelius", "Kai Puolam\u00e4ki", "Henrik Bostr\u00f6m", "Lars Asker", "Panagiotis Papapetrou"], "title": "A peek into the black box: exploring classifiers by randomization. Data mining and knowledge discovery", "year": 2014}, {"authors": ["Jonathan L Herlocker", "Joseph A Konstan", "John Riedl"], "title": "Explaining collaborative filtering recommendations", "venue": "In Proceedings of the 2000 ACM conference on Computer supported cooperative work", "year": 2000}, {"authors": ["Michael Hind"], "title": "Explaining explainable AI. XRDS: Crossroads", "venue": "The ACM Magazine for Students 25,", "year": 2019}, {"authors": ["Robert R Hoffman", "Shane T Mueller", "Gary Klein", "Jordan Litman"], "title": "Metrics for explainable AI: Challenges and prospects", "year": 2018}, {"authors": ["Fred Hohman", "Andrew Head", "Rich Caruana", "Robert DeLine", "Steven M Drucker"], "title": "Gamut: A design probe to understand how data scientists understand machine learning models", "venue": "In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems", "year": 2019}, {"authors": ["Kenneth Holstein", "Jennifer Wortman Vaughan", "Hal Daum\u00e9 III", "Miro Dudik", "Hanna Wallach"], "title": "Improving fairness in machine learning systems: What do industry practitioners need", "venue": "In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems", "year": 2019}, {"authors": ["Ulf Johansson", "Lars Niklasson"], "title": "Evolving decision trees using oracle guides", "venue": "In 2009 IEEE Symposium on Computational Intelligence and Data Mining", "year": 2009}, {"authors": ["Been Kim", "Cynthia Rudin", "Julie A Shah"], "title": "The bayesian case model: A generative approach for case-based reasoning and prototype classification", "venue": "In Advances in Neural Information Processing Systems", "year": 2014}, {"authors": ["Rafal Kocielnik", "Saleema Amershi", "Paul N Bennett"], "title": "Will You Accept an Imperfect AI?: Exploring Designs for Adjusting End-user Expectations of AI Systems", "venue": "In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems", "year": 2019}, {"authors": ["Pang Wei Koh", "Percy Liang"], "title": "Understanding black-box predictions via influence functions", "venue": "In Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org,", "year": 2017}, {"authors": ["Josua Krause", "Adam Perer", "Kenney Ng"], "title": "Interacting with predictions: Visual inspection of black-box machine learning models", "venue": "In Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems", "year": 2016}, {"authors": ["R Krishnan", "G Sivakumar", "P Bhattacharya"], "title": "Extracting decision trees from trained neural networks", "venue": "Pattern recognition 32,", "year": 1999}, {"authors": ["Todd Kulesza", "Margaret Burnett", "Weng-Keen Wong", "Simone Stumpf"], "title": "Principles of explanatory debugging to personalize interactive machine learning", "venue": "In Proceedings of the 20th international conference on intelligent user interfaces", "year": 2015}, {"authors": ["Vivian Lai", "Chenhao Tan"], "title": "On Human Predictions with Explanations and Predictions of Machine Learning Models: A Case Study on Deception Detection", "year": 2018}, {"authors": ["Thibault Laugel", "Marie-Jeanne Lesot", "Christophe Marsala", "Xavier Renard", "Marcin Detyniecki"], "title": "Inverse Classification for Comparison-based Interpretability in Machine Learning", "year": 2017}, {"authors": ["Brian Y Lim", "Anind K Dey"], "title": "Assessing demand for intelligibility in context-aware applications", "venue": "In Proceedings of the 11th international conference on Ubiquitous computing", "year": 2009}, {"authors": ["Brian Y Lim", "Anind K Dey"], "title": "Toolkit to support intelligibility in context-aware applications", "venue": "In Proceedings of the 12th ACM international conference on Ubiquitous computing", "year": 2010}, {"authors": ["Brian Y Lim", "Anind K Dey", "Daniel Avrahami"], "title": "Why and why not explanations improve the intelligibility of context-aware intelligent systems", "venue": "In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems", "year": 2009}, {"authors": ["Zachary C Lipton"], "title": "The mythos of model interpretability", "venue": "arXiv preprint arXiv:1606.03490", "year": 2016}, {"authors": ["Yin Lou", "Rich Caruana", "Johannes Gehrke", "Giles Hooker"], "title": "Accurate intelligible models with pairwise interactions", "venue": "In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining", "year": 2013}, {"authors": ["Scott M Lundberg", "Su-In Lee"], "title": "A unified approach to interpreting model predictions", "venue": "In Advances in Neural Information Processing Systems", "year": 2017}, {"authors": ["Prashan Madumal", "Tim Miller", "Liz Sonenberg", "Frank Vetere"], "title": "A Grounded Interaction Protocol for Explainable Artificial Intelligence", "venue": "In Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems. International Foundation for Autonomous Agents and Multiagent Systems,", "year": 2019}, {"authors": ["Tim Miller"], "title": "Explanation in artificial intelligence: Insights from the social sciences", "venue": "Artificial Intelligence", "year": 2018}, {"authors": ["Sina Mohseni", "Niloofar Zarei", "Eric D Ragan"], "title": "A survey of evaluation methods and measures for interpretable machine learning", "year": 2018}, {"authors": ["Ramaravind K Mothilal", "Amit Sharma", "Chenhao Tan"], "title": "Explaining machine learning classifiers through diverse counterfactual explanations", "venue": "In Proceedings of the 2020 Conference on Fairness, Accountability,", "year": 2020}, {"authors": ["Michael Muller", "Ingrid Lange", "Dakuo Wang", "David Piorkowski", "Jason Tsay", "Q Vera Liao", "Casey Dugan", "Thomas Erickson"], "title": "How Data Science Workers Work with Data: Discovery, Capture, Curation, Design, Creation", "venue": "In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems", "year": 2019}, {"authors": ["Menaka Narayanan", "Emily Chen", "Jeffrey He", "Been Kim", "Sam Gershman", "Finale Doshi-Velez"], "title": "How do humans understand explanations from machine learning systems? an evaluation of the human-interpretability of explanation", "year": 2018}, {"authors": ["Anh Nguyen", "Jason Yosinski", "Jeff Clune"], "title": "Multifaceted feature visualization: Uncovering the different types of features learned by each neuron in deep neural networks", "year": 2016}, {"authors": ["Forough Poursabzi-Sangdeh", "Daniel G Goldstein", "Jake M Hofman", "Jennifer Wortman Vaughan", "Hanna Wallach"], "title": "Manipulating and measuring model interpretability", "year": 2018}, {"authors": ["Emilee Rader", "Kelley Cotter", "Janghee Cho"], "title": "Explanations as mechanisms for supporting algorithmic transparency", "venue": "In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems", "year": 2018}, {"authors": ["Ashwin Ram"], "title": "Question-driven understanding: An integrated theory of story understanding, memory and learning", "year": 1989}, {"authors": ["Gabri\u00eblle Ras", "Marcel van Gerven", "Pim Haselager"], "title": "Explanation methods in deep learning: Users, values, concerns and challenges", "venue": "In Explainable and Interpretable Models in Computer Vision and Machine Learning", "year": 2018}, {"authors": ["Marco Tulio Ribeiro", "Sameer Singh", "Carlos Guestrin"], "title": "Why should i trust you?: Explaining the predictions of any classifier", "venue": "In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining", "year": 2016}, {"authors": ["Marco Tulio Ribeiro", "Sameer Singh", "Carlos Guestrin"], "title": "Anchors: High-precision model-agnostic explanations", "venue": "In Thirty-Second AAAI Conference on Artificial Intelligence", "year": 2018}, {"authors": ["David Ribes"], "title": "Notes on the concept of data interoperability: Cases from an ecology of AIDS research infrastructures", "venue": "In Proceedings of the 2017 ACM Conference on Computer Supported Cooperative Work and Social Computing", "year": 2017}, {"authors": ["Marko Robnik-\u0160ikonja", "Marko Bohanec"], "title": "Perturbation-Based Explanations of Prediction Models", "venue": "In Human and Machine Learning", "year": 2018}, {"authors": ["Adam Rule", "Aur\u00e9lien Tabard", "James D Hollan"], "title": "Exploration and explanation in computational notebooks", "venue": "In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems", "year": 2018}, {"authors": ["Wojciech Samek", "Klaus-Robert M\u00fcller"], "title": "Towards explainable artificial intelligence. In Explainable AI: Interpreting", "venue": "Explaining and Visualizing Deep Learning", "year": 2019}, {"authors": ["Christian Sandvig", "Kevin Hamilton", "Karrie Karahalios", "Cedric Langbort"], "title": "Auditing algorithms: Research methods for detecting discrimination on internet platforms. Data and discrimination: converting critical concerns into productive inquiry", "year": 2014}, {"authors": ["Johanes Schneider", "Joshua Handali"], "title": "Personalized explanation in machine learning", "venue": "arXiv preprint arXiv:1901.00770", "year": 2019}, {"authors": ["Milene Selbach Silveira", "Clarisse Sieckenius de Souza", "Simone DJ Barbosa"], "title": "Semiotic engineering contributions for designing online help systems", "venue": "In Proceedings of the 19th annual international conference on Computer documentation", "year": 2001}, {"authors": ["Karen Simonyan", "Andrea Vedaldi", "Andrew Zisserman"], "title": "Deep inside convolutional networks: Visualising image classification models and saliency maps", "year": 2013}, {"authors": ["Aaron Springer", "Steve Whittaker"], "title": "Progressive disclosure: empirically motivated approaches to designing effective transparency", "venue": "In Proceedings of the 24th International Conference on Intelligent User Interfaces", "year": 2019}, {"authors": ["Erik \u0160trumbelj", "Igor Kononenko"], "title": "Explaining prediction models and individual predictions with feature contributions", "venue": "Knowledge and information systems 41,", "year": 2014}, {"authors": ["Simone Stumpf", "Vidya Rajaram", "Lida Li", "Margaret Burnett", "Thomas Dietterich", "Erin Sullivan", "Russell Drummond", "Jonathan Herlocker"], "title": "Toward harnessing user feedback for machine learning", "venue": "In Proceedings of the 12th international conference on Intelligent user interfaces", "year": 2007}, {"authors": ["William R Swartout"], "title": "XPLAIN: A system for creating and explaining expert consulting programs", "venue": "Artificial intelligence 21,", "year": 1983}, {"authors": ["William R Swartout", "Stephen W Smoliar"], "title": "On making expert systems more like experts", "venue": "Expert Systems 4,", "year": 1987}, {"authors": ["Andrea L Thomaz", "Cynthia Breazeal"], "title": "Transparency and socially guided machine learning", "venue": "In 5th Intl. Conf. on Development and Learning (ICDL)", "year": 2006}, {"authors": ["Gabriele Tolomei", "Fabrizio Silvestri", "Andrew Haines", "Mounia Lalmas"], "title": "Interpretable predictions of tree-based ensembles via actionable feature tweaking", "venue": "In Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining", "year": 2017}, {"authors": ["Sandra Wachter", "Brent Mittelstadt", "Chris Russell"], "title": "Counterfactual Explanations without Opening the Black Box: Automated Decisions and the GPDR", "venue": "Harv. JL & Tech", "year": 2017}, {"authors": ["Danding Wang", "Qian Yang", "Ashraf Abdul", "Brian Y Lim"], "title": "Designing Theory-Driven User-Centric Explainable AI", "venue": "In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems", "year": 2019}, {"authors": ["Dennis Wei", "Sanjeeb Dash", "Tian Gao", "Oktay Gunluk"], "title": "Generalized Linear Rule Models", "venue": "In International Conference on Machine Learning", "year": 2019}, {"authors": ["Daniel S Weld", "Gagan Bansal"], "title": "The challenge of crafting intelligible intelligence", "venue": "arXiv preprint arXiv:1803.04263", "year": 2018}, {"authors": ["Adrian Weller"], "title": "Challenges for transparency", "venue": "arXiv preprint arXiv:1708.01870", "year": 2017}, {"authors": ["Christine T Wolf"], "title": "Explainability scenarios: towards scenario-based XAI design", "venue": "In Proceedings of the 24th International Conference on Intelligent User Interfaces", "year": 2019}, {"authors": ["Qian Yang"], "title": "Machine Learning as a UX Design Material: How Can We Imagine Beyond Automation, Recommenders, and Reminders", "venue": "AAAI Spring Symposium Series", "year": 2018}, {"authors": ["Ming Yin", "Jennifer Wortman Vaughan", "Hanna Wallach"], "title": "Understanding the Effect of Accuracy on Trust in Machine Learning Models", "venue": "In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems", "year": 2019}, {"authors": ["Quanshi Zhang", "Yu Yang", "Haotian Ma", "Ying Nian Wu"], "title": "Interpreting cnns via decision trees", "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "year": 2019}, {"authors": ["Xin Zhang", "Armando Solar-Lezama", "Rishabh Singh"], "title": "Interpreting neural network judgments via minimal, stable, and symbolic corrections", "venue": "In Advances in Neural Information Processing Systems", "year": 2018}, {"authors": ["Bolei Zhou", "Aditya Khosla", "Agata Lapedriza", "Aude Oliva", "Antonio Torralba"], "title": "Learning deep features for discriminative localization", "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition", "year": 2016}, {"authors": ["Zhi-Hua Zhou", "Yuan Jiang", "Shi-Fu Chen"], "title": "Extracting symbolic rules from trained neural network ensembles", "venue": "Ai Communications 16,", "year": 2003}, {"authors": ["Jichen Zhu", "Antonios Liapis", "Sebastian Risi", "Rafael Bidarra", "G Michael Youngblood"], "title": "Explainable AI for designers: A human-centered perspective on mixed-initiative co-creation", "venue": "IEEE Conference on Computational Intelligence and Games (CIG)", "year": 2018}], "sections": [{"heading": "Author Keywords", "text": "Explainable AI; human-AI interaction; User experience"}, {"heading": "INTRODUCTION", "text": "The rapidly growing adoption of Artificial Intelligence (AI), and Machine Learning (ML) technologies using opaque deep neural networks in particular, has spurred great academic and public interest in explainability to make AI algorithms understandable by people. This issue appears in popular press, industry practices [2, 10], regulations [24], as well as hundreds of recent papers published in AI and related disciplines. These XAI works often express an algorithm-centric view, relying on \u201cresearchers\u2019 intuition of what constitutes a \u2018good\u2019 explanation\u201d [63]. This is problematic because AI explanations are often demanded by lay users, who may not have deep technical understanding of AI, but hold preconception of what constitutes useful explanations for decisions made in a familiar domain. As an example, one of the most popular approaches to explain a prediction made by a ML classifier, as dozens of XAI algorithms strive to do [40], is by listing the features with the highest weights contributing to a model\u2019s prediction.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. CHI \u201920, April 25\u201330, 2020, Honolulu, HI, USA. \u00a9 2020 Association for Computing Machinery. ACM ISBN 978-1-4503-6708-0/20/04 ...$15.00. http://dx.doi.org/10.1145/3313831.3376590\nFor example, a model predicting a patient having the flu may explain by saying \u201cthe symptoms of sneeze and headache are contributing to this prediction\u201d [74]. However, it is questionable whether such an explanation satisfies a doctor\u2019s needs to understand the AI, or adds significant value to a clinical decision-support tool.\nTo close the gap between XAI algorithms and user needs for effective transparency, the HCI community has called for interdisciplinary collaboration [4] and user-centered approaches to explainability [92]. This emerging area of work tends to either build on frameworks of human explanations from social science, or empirically study how explanation features impact user interaction with AI. In this paper, we take a complementary approach by investigating challenges faced by industry practitioners to create explainable AI products, with the goal of identifying gaps between the algorithmic work of XAI and what is needed to address real-world user needs.\nRecently, an increasing number of open-source toolkits (e.g. [1, 2, 3, 10]) are making XAI techniques, which produce various forms of explanation for \u201cblack-box\u201d ML models, accessible to practitioners. However, little is known about how to put these techniques from research literature into practice. As we will show, it is challenging work to bridge user needs and technical capabilities to create effective explainabilty features in AI products. This kind of work often falls to those with a bridging role in product teams\u2013the design and user experience (UX) practitioners, whose job involves identifying user needs, communicating with developers and stakeholders, and creating design solutions based on demands and constraints on both sides. We study, therefore, how AI explainability is approached by design and UX practitioners, explore together with them how XAI techniques can be applied in various products, and identify opportunities to better support their work and thus the creation of user-centered explainable AI applications.\nGiven the early status of XAI in industry practices, we anticipate a lack of established means to uncover user needs or a shared technical understanding. Therefore, we develop a novel probe to ground our investigation, namely an XAI algorithm informed question bank. As an explanation can be seen as an answer to a question [63], we represent user needs for explainability in terms of the questions a user might ask about the AI. Drawn on relevant ML literature and prior work on question-driven explanations in various domains, we create a list of prototypical user questions that can be addressed by current XAI algorithms. These questions thus represent the\nar X\niv :2\n00 1.\n02 47\n8v 2\n[ cs\n.H C\n] 8\nF eb\n2 02\n0\ncurrent availability of algorithmic methods for AI explainability, allowing us to explore how they can be applied in various AI products, and identify their limitations for addressing realworld user needs. Our contributions are threefold: \u2022 We provide insights into how user needs for different types\nof explainability are presented in various AI products. We suggest how these user needs should be understood, prioritized and addressed. We also identify opportunities for future XAI work to better satisfy these user needs.\n\u2022 We summarize current challenges faced by design practitioners to create explainable AI products, including variability of user needs for explainability, discrepancies between algorithmic explanations and human explanations and a lack of support for design practices.\n\u2022 We present an extended XAI question bank (Figure 1) by combining algorithm-informed questions and user questions identified in the study. We discuss how it can be used as guidance and tool to support the needs specification work to create user-centered XAI applications.\nBACKGROUND Explainable artificial intelligence (XAI) Although XAI first appeared in expert systems almost four decades ago [22, 87], it is gaining widespread visibility as a field focusing on ML interpretability [19]. The term explainability is used by the research community with varying scope. In much of the ML literature, XAI aims to make the reasons behind a ML model\u2019s decisions comprehensible to humans [40, 59, 74]. In a broader view, explainability encompasses everything that makes ML models transparent and understandable, also including information about the data, performance, etc. [10, 45]. Our view aligns with the latter.\nRecent papers surveyed this rapidly growing field and identified its key research threads [5, 19, 34, 40, 64, 73]. We will discuss taxonomies of XAI techniques in the next section. Another core thread is the evaluation of explanations, which answers whether an explanation is good enough, and how to compare different explanations. These questions are not only critical for choosing appropriate XAI techniques, but also underlie the development of intelligent systems that optimize the choice of explanation, such as interactive or personalized explanations [4, 81, 94]. Toward this goal, many sought to define the desiderata of XAI [19, 40, 45, 77], including fidelity, completeness, robustness, etc. Despite the conceptual discussions, there are few established means of quantifying explainability. Partly, the reason is that the effectiveness of an explanation is relative to the recipient, and on a philosophical ground, the question asked [17]. So the same explanation may be seen as more or less comprehensible to different users, or even to the same user engaged in a different understanding. Many therefore advocate that the evaluation of XAI needs to involve real users within the targeted application [30, 44].\nGiven its recipient-dependent nature, it is clear that work on XAI must take a human-centered approach. By conducting a literature review in social science on how humans give and receive explanations, Miller identified a list of human-friendly characteristics of explanation that are not given sufficient attention in the algorithmic work of XAI, including contrastiveness\n(to a counterfactual case), selectivity, social process, focusing on the abnormal, etc. Wang et al. [92] proposed a conceptual framework to connect XAI techniques and cognitive patterns in human-decision making to guide the design of XAI systems.\nWith a fundamental interest in creating user-centered technologies, the HCI community is seeing burgeoning efforts around designing and studying user interactions with explainable AI [14, 18, 21, 29, 45, 49, 54, 71]. As a literature analysis performed by Abdul et al. [4] shows, before this wave of work on ML systems, the HCI community have studied explainable systems in various contexts, most notably context-aware applications [12, 56, 57], recommender systems [42], debugging tools [53] and algorithmic transparency [28, 80]. Specific to XAI, recent studies largely focused on empirically understanding the effect of explanation features on users\u2019 interaction with and perception of ML systems, usually through controlled lab or field experiments. Notably, although explanations were found to improve user understanding of the AI systems, conclusions about its benefits for user trust and acceptance were mixed [21, 49, 54, 70], suggesting potential gaps between algorithmic explanations and end user needs.\nOur work shares the goal of bridging between the algorithmcentric XAI and user-centered explanations. In contrast to prior work centered around end users, we focus on the people that engage in this bridging work, namely UX and design practitioners. By studying their current practices, we explore the largely undefined design space of XAI and identify challenges for creating explainable AI products."}, {"heading": "Supporting AI practitioners", "text": "We join a growing group of scholars studying the work of industry practitioners who create AI products [7, 15, 45, 46, 67, 78]. By better supporting their work, we can ameliorate downstream usability, ethical and societal problems associated with AI. For example, Boukhelifa et al. [15] interviewed 12 data scientists to understand their coping strategies around uncertainty in data science work, and proposed a process model for uncertainty-aware data analytics. Holstein et al. interviewed 35 ML practitioners to conduct the first investigation of commercial product teams\u2019 challenges for developing fairer ML systems, and identified the disconnect between their needs and the solutions proposed in the fair ML research literature.\nMost studies of AI practitioners focused on data scientists. As creating explainable AI products requires a user-centered approach, design practitioners should also perform an indispensable role. Despite a growing body of HCI work on AI technologies, there is a lack of design guidelines for AI systems. One notable exception is a recent paper by Amershi et al. [7], which synthesized a set of 18 usability guidelines for AI systems. Several of these guidelines (e.g., make clear what the system can do, how well it can do, why it did what it did) are relevant to explainability, but they do not provide actionable guidance on how to actualize these capabilities. Meanwhile, recent papers explored design methods supporting the creation of explainable AI systems. Wolf [96] proposed a scenario-based approach to identify user needs for explainability (\u201cwhat people might need to understand about AI systems\u201d) early on in system development. Eiband et al. [31]\nproposed a stage-based participatory design process, which guides product-specific needs specification\u2013what to explain, followed by iterative design of solutions\u2013how to explain.\nOur work is motivated by a similar pragmatic goal of supporting design practices of XAI. More specifically, in the face of increasingly available XAI techniques, we are interested in the design work that connects user needs and these technical capabilities. In particular, we probe the challenges to identify the suitability of XAI techniques. A recent stream of guidance in the public domain (e.g. [1, 2, 10]) on how to select among XAI algorithms suggest their suitability can be difficult to determine. More problematically, these guidelines are targeting data scientists, using criteria grounded in the development process (e.g., explaining data or features, pre- or post-training). They do not directly address end user needs for understanding AI, nor support the navigation of the design space of XAI."}, {"heading": "Question-driven explanations", "text": "Outside the ML field, many explored the space of user needs for explanation using a question-driven framework. Fundamentally, an explanation is \u201can answer to a (why-) question [63].\u201d These questions are also user and context dependent, described as \u201ctriggers\u201d by Hoffman et al. [44] representing \u201ctacitly an expression of a need for a certain kind of explanation...to satisfy certain user purposes of user goals. \u201d\nIn the early generation of AI work, question-driven frameworks were used to generate explanations for knowledge-based systems [20, 38, 88]. AQUA [72] is a reasoning model that uses questions to generate explanations and identify knowledge gaps for learning. AQUA was built upon a taxonomy of questions, including anomaly detection questions, hypothesis verification questions, etc. Silveira et al. provided a taxonomy of user questions about software to drive the design of help systems [82]. Building on it, Glass et al. [35] investigated users\u2019 explanation requirements in using an adaptive agent and showed that user needs for different types of explanation vary. In context-aware computing, Lim and Dey [56] developed a taxonomy of user needs for intelligibility by crowdsourcing user questions in scenarios of context-aware applications. These questions were coded into intelligibility types, including input, output, conceptual model (why, how, why not, what else, what if) and non-functional types (certainty, control). This taxonomy enabled a toolkit that supports the generation of explanations for context-aware applications [57].\nInspired by the prior work, we use an XAI question bank, containing prototypical questions that users may ask for understanding AI systems, as a study probe representing user needs for AI explanability. Instead of using question taxonomies that are not specific to ML, we start by performing a literature review to arrive at a taxonomy of existing XAI techniques, and use it to guide the creation of user questions. Thereby we constrain the probe to reflect the current availability of XAI techniques to understand how user needs for such explainability are presented in real-world AI products."}, {"heading": "XAI QUESTION BANK", "text": "We now describe how we developed the XAI question bank by first identifying a list of explanation methods supported by\ncurrent XAI algorithms, for which we focus on those generating post-hoc explanations for opaque ML models [9, 40]. For the scope of this paper, we will leave out the technical details of the algorithms but provide references for interested readers. There have been many efforts to create taxonomies of XAI methods [5, 9, 19, 34, 40, 64, 65, 73, 79]. Commonly, they differentiate between an explanation method\u2013a pattern or mechanism to explain ML models\u2013and specific XAI algorithms. One type of explanation method can be generated by multiple algorithms, which may vary in performance or applicability to specific ML models. Common dimensions to categorize explanation methods include: 1) The scope of the explanation, i.e. whether to support understanding the entire model (global) versus a single prediction (local); 2) The complexity of the model to be explained; 3) The dependency on the model used, i.e., whether the technique applies to any ML model or to only one type [5]; and 4) The stage of model development to apply the explanation [19].\nExcept for the first one, these dimensions are data scientist centric as they are concerned with the characteristics of the underlying model. For our purpose of mapping user questions, we seek a taxonomy that lists the forms of explanation as presented to users. For example, we disregard the complexity of the model or the explanation\u2019s applicability to specific models, but instead differentiate between methods that describe the model logic as rules, decision trees or feature importance. Also, to identify user questions an explanation addresses, we believe it is sufficient to stay at the general mechanism, and ignore the specificity of the presentation such as textual or by visualization [74]. Guided by these principles, we found the taxonomy of explanators in Guidotti et al. [40] closest to our purpose. Using it as a starting point, we consulted other survey papers and iteratively consolidated a taxonomy of explanation methods. In addition to the three categories in [40]\u2013methods that explain the entire model (global), an individual outcome (local), and inspect how the output changes with instance changes (inspect counterfactual), we added example based explanations [45, 65], since they represent a distinct mechanism to explain. Finally, we arrived at the taxonomy presented in the second column of Table 1.\nTo map the explanation methods to user questions they can answer, we consulted prior work that provided taxonomies of questions for explanations [56, 57, 72, 82]. The closest to our purpose is the intelligibility types by Lim et al. [56, 57], developed by eliciting user questions in scenarios of contextaware computing. In particular, the intelligibility types of How (system logic), Why (a prediction), Why not, What if are directly applicable to ML systems. By mapping these questions to explanation methods listed in Table 1, we identified two additional types of question that can be addressed by existing XAI techniques: 1) How to be that: what are the changes required, often implying minimum changes, for an instance to get a different target prediction; 2) How to still be this: what are the permitted changes, often implying maximum changes, for an instance to still get the same prediction. We note that the questions of What if, How to be... are considered counterfactual questions and best answered by inspection or example based explanations, which allow users to understand the deci-\nsion boundaries of a ML model. Table 1 was reviewed by 4 additional experts working in the field of XAI.\nTaking a broad view on explainability, we also consider descriptive information that could make a ML model more transparent. We added three more types based on [45, 56, 57]\u2013 questions regarding model Input (training data), Output, and Performance. In the rest of the paper, we refer these 9 types of questions as 9 explainability needs categories as they represent categories of prototypical questions users may ask to understand AI. For each category, we created a leading question (e.g.,\u201cWhy is this instance given this prediction\u201d for the Why category1), and supplemented 2-3 additional example questions, inquiring about features and examples whenever applies. The list of questions developed in this step are shown in Figure 1 without an asterisk. We do not claim the exhaustiveness of this list, but deem it to be sufficient as a study probe."}, {"heading": "STUDY DESIGN", "text": "We conducted semi-structured interviews with 20 UX and design practitioners recruited from multiple product lines at IBM. All but two (I-6 and I-20) informants worked on different products without shared AI models. Three informants were design team leads overseeing multiple products. The AI products included mature commercial platforms, systems in the testing phase, and internal platforms used by IBM employees. 50% of informants were female. All but two were based in the United States, in 7 different locations. Table 2 summarizes the primary areas of the products and informants\u2019 job titles. Our samples focused on AI systems that support utility tasks such as analytics and decision-support, as explainability is critical for high-stakes tasks where people would want to understand the AI\u2019s decisions [19, 30]. Informants were recruited from internal chat groups relevant to design and UX of AI products. The recruiting criteria indicated that one should have worked on the design of an AI product and had a good understanding\n1We instructed that \u2018prediction\u2019 is used to refer the AI output for an instance. In the context of a product, it can mean a score/ recommendation/ classification/ answer, etc.\nof its users, and mentioned that the interview would focus on user needs around understanding the AI.\nWe noticed that the current status of explainability in commercial AI products vary\u2013about two thirds of the products (68.8%) have descriptive explanations about the data or algorithm, only a subset (37.5%) provide explanations for individual decisions, and certain products (e.g., chatbot) have neither. To explore the design space of XAI, we were interested in user needs for explainability uncovered by the design practitioners instead of the current system capabilities. The XAI question bank was able to scaffold the discussions, both to enumerate on the explainability needs categories, and to ground the discussion on user questions instead of venturing into the technical details.\nUsing MURAL\u2013a visual collaboration tool, we created a card for each question category listed in Figure 1, with the leading and example questions (without an asterisk). Informant went through each card and discussed whether they encountered these questions from users; If not, we asked whether they saw the questions would apply and in what situations. After pilot testing, for efficiency, we combined the Why and Why not into one card to represent user needs to understand a prediction; and What if, How to be that, How to still be this into one card to\nrepresent user needs to understand counterfactual cases. Thus there were 6 cards plus a blank card if one wanted to create an additional category. If time permitted, we asked informants to sort the cards according to their priority to address, and elicited the reasons for the ordering.\nInterviews lasted 45-60 minutes, conducted remotely using a video conferencing system and MURAL. We started by asking informants to pick an AI product they worked on and had good knowledge of the users, in which they saw user needs for understanding the AI. We asked them to describe the system and the AI components. They could either use screen sharing or send screenshots to show us the system. We then asked whether the users had needs to understand the AI, and probed on why, when and where they had such needs (or lack thereof), and how the needs could be addressed, currently or speculatively. We then asked informants to reflect on what questions users would ask about the AI and listed as many as they could. User questions were also added to MURAL by the researchers if they appeared in other parts of the discussion. Thereby, we gathered user questions in a bottom-up fashion that allowed us to identify gaps in the algorithm-informed XAI question bank. It also prepared informants to move to discussions around the question cards. We closed the interview by asking informants to reflect on common challenges to build explainability features in AI products, and what kind of support they wished to have. For the three informants on lead roles, we focused on discussing the general status of explainability in AI products."}, {"heading": "Analysis", "text": "Around 1000 minutes of interviews were recorded and transcribed, from which we extracted 607 passages broadly relevant to explainability. We performed open coding and axial coding on these passages as informed by Grounded Theory research [25]. The iterative coding was conducted by one researcher, with frequent discussions with the other researchers. We returned to the passages, interview video and the AI products repeatedly as necessary. The iterative coding process resulted in a set of 24 axial codes. We combined them into selective codes to be discussed as the main themes in the results section, where the axial codes are presented in bold.\nTwo additional sets of code were applied: 1)We identified 170 user questions appeared in the question-listing activity and the rest of the interviews. 2)We coded these questions and other passages, wherever applied, with the explainability needs category. The intersection of the two sets of code was 124 covered questions, as covered by the categories of the question bank, and the remaining 46 uncovered questions.\nTo perform gap analysis on the XAI question bank, we followed two steps. For the covered questions in each needs category, we identified new forms of questions that were not covered by the original example questions, as shown with asterisks in Figure 1. By forms, we grouped together questions with the same intent but phrased differently. For example, \u201chow was the data created\u201d, and \u201cwhere did the data come from\u201d were both regarding the source of the training data, and covered by an original question in the Input category, while \u201cwhat is the sample size\u201d would be regarding a different characteristic of the input. In the second step, we examined the\n46 uncovered questions. We first excluded 22 questions not generalizable to AI products, such as \u201cwhat is the summary of the article?\u201d. We then iteratively grouped and coded the intent of the remaining 24 questions and identified 5 additional forms of question in the Others category in Figure 1. Insights from the analysis will be discussed the results section."}, {"heading": "RESULTS", "text": "The results are divided into two parts. We start by discussing the general themes emerged in the interviews around the design work to create explainable AI products, which highlight some of the gaps between the algorithmic perspective of XAI and the practices to address user needs to understand AI. We then discuss how each category of user needs for explainability is presented in real-world AI products and based on that, reflect on the opportunities and limitations of XAI work.\nFrom XAI algorithms to design practices The diverse motivations for and utility of explainability The historical context for the surge of XAI can be attributed to a fear of lacking understanding and control on increasingly complex ML models. Explanation is often embraced as a cure for \u201cblack box\u201d models to gain user trust and adoption. So a common pursuit is to produce interpretable, often simplified, descriptions of model logic to make an opaque ML model seen as transparent. This is a necessary effort, but insufficient to deliver a satisfying user experience if we ignore users\u2019 motivation for explanations. As I-8 put: \u201cExplainability isn\u2019t just telling me how you get there, but also, can you expand on what you just told me...explanation has its own utility\u201d.\nWe identified several utility goals driving user demands for explanations of AI. In the context of AI-assisted decisionmaking, explanations are most frequently sought to gain further insights or evidence, as users are not satisfied by merely seeing a recommendation or score given by the AI. There are several ways people use these insights. When seeing disagreeable, unexpected or unfamiliar output, explanations are critical for people to assess the AI\u2019s judgment to make an informed decision. Even when users\u2019 decision aligns with the AI\u2019s, explanations could help enhance decision confidence or generate hypothesis about the causality for follow-up actions, as illustrated by I-5\u2019s comment, who worked on a tool supporting supply chain management: \u201cusers need to know why the system is saying this will be late because the reason is going to determine what their next action is...If it\u2019s because of a weather event, so no matter what you do you\u2019re not going to improve this number, versus something small,if you just make a quick call, you can get that number down.\u201d In some cases, users also deem explanations of the AI\u2019s decision as potential mitigation of their own decision biases.\nTo appropriately evaluate the capability of the AI system is identified as the second theme of motivation, both to determine the overall system adoption (e.g., evaluating data quality, transferability to a new context, whether the model logic aligns with domain knowledge), and at the operational level to beware of the system\u2019s limitations. I-6 commented on why explanations matter for users of a medical imaging system: \u201cThere is a calibration of trust, whether people will use it over time. But also saying hey, we know this fails in this\nway.\u201d We note that appropriating trust should be distinguished from enhancing trust. Though from a product team\u2019s perspective, the concern is often on users\u2019 under-trusting of the AI system and explanations are sought to improve adoption.\nThe third theme of motivation for explainability is to adapt usage or interaction behaviors to better utilize the AI. I-7 described users\u2019 desire to understand how the AI extracted information from clinic notes so they could adapt their notestaking practices. I-17 mentioned users of a sales inventory management tool would want to focus on cases where the AI prediction was likely to err. I-13 commented that explanation could suggest to chatbot users what kind of things they could ask. Furthermore, explanations could also convince users to invest in the system \u201cif they know how the system will improve\u201d(I-11) (e.g., access to personal information, feedback).\nSeveral informants working on AI systems supporting analysts\u2019 work or model training tools considered explanations as an integral part of a \u201cfeedback loop\u201d (I-11) to improve AI performance. Such needs are not only seen in debugging tools [45], but also in cases where the user could manipulate the data or correct the instance: \u201c[Explaining] why it thinks we are where we are and the opportunity to say, \u2018no, I need you to just understand that we\u2019re in Phase 2\u201d\u2019 (I-5).\nLast but not least, informants reflected on their ethical responsibilities to provide explanations:\u201c What are we responsible for as creators of tools... whether it\u2019s out of the kindness of our hearts or whether it\u2019s because there\u2019s a true risk to others or society... We have to provide that level of explainability\u201d(I-8).\nWhile some of these motivations have been discussed conceptually in prior work [5, 9, 19, 30, 40], our study provides concrete examples in real-world AI products. It is worth noting that the motivation for explainability is grounded in users\u2019 system goals such as improving decision-making, so explanation is not merely to provide transparency but support downstream\nuser actions such as adapting interactions or acting on the decisions. Unpacking the motivation and downstream user actions should ultimately guide the selection of explanation methods. For example, if the goal is to gain further insights, example-based explanations could be more useful than featurebased explanations that describe the algorithm\u2019s logic, as I-2 described: \u201c[Users of natural resource analytic tools] already rely a lot on the analogy... [Similar examples] are very good to their study, e.g. give clues on which year this was formed.\u201d\nMoreover, unpacking the motivation may help foresee the limitations of algorithmic explanations and fill the gaps in designing user experiences. For example, if the motivation is to mitigate biases, then users may desire to see \u201cboth positive and negative evidence\u201d (I-1). If it is to support adaption of interaction, the system could supplement information of \u201cthis is what other people do\u201d (I-13). Some informants criticized designing for the mental model of AI as a decision-maker explaining its rationale, and argued to focus on what utility explanations could provide to support users\u2019 end goals. As I-1, who worked on a clinical decision-support tool, put:\u201c[explanations by system rationale] are essentially \u2018this is how I do it, take it or leave it\u2019. But doctors don\u2019t like this approach...Thinking that [AI is] giving treatment recommendations is the wrong place to start, because doctors know how to do it. It\u2019s everything that happens around that decision they need help with... more discussions about the output, rather than how you get there.\u201d"}, {"heading": "In quest for human-like explanations", "text": "Explanation is an integral part of human communication, and invites preconception of what constitutes effective and natural ways to explain. Informants were constantly dealing with discrepancies between algorithmic and human explanations. Some of the discrepancies are inherent to the mechanisms of AI algorithms, such as using features or learned patterns that are not aligned with how people make decisions. Others are in the forms constrained by technical capabilities and foreign to human interactions. For example: \u201cPeople have this unspoken\nnorm, I trust you and if you are not sure you would let me know. But nobody goes around saying how confident they are in the thing that they\u2019re saying. It may be implied in the language they\u2019re using. So a system has high precision but 67% confidence...it is a stupid and hard to use metric \u201d (I-5).\nSeveral informants attempted to mimic how people, especially domain experts, explain in their design work. By aligning with how humans explain, it aligns user perception of the AI with existing mental model of decision-making in the domain, suggested by prior work as critical to build trust [84]. This is best exemplified by I-1\u2019s work in designing explanations for a clinical-decision support system that performs information extraction from medical literature: \u201cWe mirror the way a doctor would do. So if a doctor was asked, how would you go and find the evidence? ... You went to PubMed you found a paper, the paper matches my patient... you\u2019re showing me the statements in the paper [on whether] it was a good or bad idea, and putting all that together...So when you manage to reflect with AI literally how the doctor thinks about the problem, the black box kind of disappears.\u201d\nWe identified several themes on the desirable properties of human explanation that echoed Miller\u2019s arguments on informing XAI with how humans explain [63]. First, explanations are selected, often focusing on one or two causes from a sometimes infinite number of causes [63]. Informants discussed the importance of selectivity as \u201ca balance of providing enough information that is trustworthy and compelling without overwhelming the user\u201d (I-11) and acknowledging that \u201cAI will have a degree of [randomness] and may not be 100% explainable\u201d (I-8). Second, explanations are social, as part of a conversation and presented based on the recipient\u2019s current beliefs [63]. This social aspect is not only seen in tailoring explanations \u201cfor people with different backgrounds\u201d (I-4), but also in accommodating the evolving needs for explanation as one builds understanding and trust during the interaction process, \u201conce we trust it, it\u2019s about going deeper into it, the kind of questions goes from broad to ultra-specific \u201d (I-8).\nThe selective and social nature of explanation has made many to argue that XAI has to be interactive or even conversational [62, 63, 94], tailoring explanation to different questions asked by different users, who would also ask follow-up questions to keep closing the gap of understanding, a process known as grounding in human communication [23]. Following prior work [56, 57, 72, 94], we postulate that a question-driven framework provides a viable path to interactive explanations.\nXAI: challenges and needs in design practices It is challenging work to create design solutions that bridge user needs for explainability and technical capabilities. The satisfaction of user needs is frequently hampered by the current availability of XAI techniques, which we will discuss in detail in the next section. Informants also had to work with other product goals that are at odds with explainability, such as protecting proprietary data, avoiding legal or marketing concerns from exposing the AI algorithm. Sometimes, explainability presents challenges to other aspects of user experience \u2013\u201cany opportunities we have to give them more explainability comes at the cost of the seamless integration. And\n[doctors] are just so clear that not breaking their workflow is the most important factor to their happiness\u201d(I-6). Or, it might expose corner cases or rationales that some individuals found \u201cwrong\u201d (I-2), \u201cover-simplified\u201d (I-9), or \u201coutdated\u201d (I-5), resulting in unnecessary user aversion and making the product \u201cvictim of trying to be too transparent\u201d (I-2).\nIn addition, unlike XAI algorithmic work\u2019s focus on one or a class of AI algorithm, creating explainable AI products requires a holistic approach that links multiple algorithms or system components to accommodate users\u2019 goal of better understanding and interacting with the system, as described by I-4: \u201cThere is the traditional what we think about XAI, explaining what the model is doing. But there is this huge wrapper or the situation around it that people are really uncertain... what do I need to do with this output, how do I incorporate it into other processes, other tools? So it is thinking about it as part of complex systems rather than one model and one output.\u201d\nIn short, inherent tension often exits between explainability and other system and business goals. Design practitioners are eager to advocate for explainability, but the realization requires teamwork with data scientists, developers and other stakeholders. Their advocacy is often hindered by skill gaps to engage themselves and the team in \u201cfinding the right pairing to put the ideas of what\u2019s right for the user together with what\u2019s doable given the tools or the algorithms\u201d(I-8), and the cost of time and resource that a product team is reluctant to invest with a release schedule. These challenges can potentially be overcome by having resources that help sensitizing designers to the design opportunities in algorithmic explanations [97] and enable conversations with the rest of the team, as expressed by many informants. We summarize informants\u2019 desirable support for XAI design in two areas: 1. Guidance for explainability needs specification, for which\nwe saw requests for both: 1) general principles of what types of explainability should be provided, as heuristic guidelines that a product can be developed or evaluated with; 2) guidance to identify product, user, and context specific needs to help the product team prioritize the effort. 2. Guidance for creating explainability solutions to address user needs, paired with example artifacts (e.g., UI elements, design patterns), to support the exploration of tangible solutions and communication with developers and stakeholders.\nThe two areas correspond to the what to explain and how to explain stages in Eiband et al.\u2019s design process for transparent interfaces [31]. We argue that the question bank could potentially support needs specification work, as it essentially lays out the space of users\u2019 prototypical questions to understand AI systems. The above requests suggest the needs to further understand the key factors that may lead to variability of user questions, and how these questions should be appropriately answered. We work towards these goals in the next section.\nUnderstanding user needs for explainability We use the explainability needs category codes to guide our analysis on each category. We focus on two questions: 1) The variability of explainability needs, i.e., what factors make a category of user questions more or less likely to be asked. 2) The potential gaps between algorithmic explanations and\nuser needs, by examining passages coded as design challenge, and the additional questions identified in the gap analysis (Figure 1). To help answer the former, we first discuss key factors that may lead to the variability of explainability needs, which we identified by coding informants\u2019 reasons to include, exclude or prioritize a needs category. \u2022 Motivation for explainability:The diverse motivations dis-\ncussed in the last section for demanding explainability could lead to wanting different kinds of explanation.\n\u2022 Usage point: Informants mentioned common points during the usage of AI systems where certain type of explianabiltiy was of particular interest, including on-boarding, reliance or delegation to AI, encountering abnormal results, system breakdown, and seeing changes in the system.\n\u2022 Algorithm or data type: Different algorithms invoke different questions. For lay users, it might be more relevant to consider the type of data the AI is used with rather than specific algorithms, e.g., tabular data, text, images or video.\n\u2022 Decision context: We identified codes describing the nature of the decision context that led to prominent needs for certain type of explainability, including outcome criticality, time-sensitivity, and decision complexity.\n\u2022 User type: Codes describing the characteristics of users include AI knowledge, domain knowledge, attitude towards AI, roles or responsibilities.\nIn prior work, the variability of user needs for explaianability has been discussed regarding the roles of the users [9, 10, 43, 79, 95], e.g., regulators, model developers, decision-makers, consumers. The diverse criteria used by our informants suggest many other factors to consider for the suitability of XAI techniques. This paper does not conclude on how these factors vary user needs. Rather, they should be seen as sensitizing concepts by Bowen\u2019s [16] and Ribes\u2019s [76] definitions\u2013\u201ctell where to look but not what to see.\u201d The sheer number of these factors highlight the challenge to pre-define users\u2019 explainability needs, vindicating the recent effort [31, 96] to provide structured guidance to support empirically identifying application-specific user needs. Below we present informants\u2019 discussions on each category of explainability needs and highlight how these factors heighten the needs (in italic)."}, {"heading": "Input/data", "text": "Understanding training data for the AI model was most frequently seen to serve the motivation to appropriately evaluate AI capabilities for use. It was considered a prominent need during the on-boarding stage, and by both the decision-makers and people in quality-control roles. Explanations of data were also important in cases where the users could directly manipulate the data to either adapt the usage to better utilize the AI or to improve the AI performance.\nAdditional questions identified from the gap analysis indicate a desire to gauge the AI\u2019s limitations by inquiring about the sample size, potential biases, sampling of critical sub-groups, missing data, etc. Additional codes include to understand the system\u2019s compliance with regulations regarding data sampling, and transferability of the AI model: \u201cNot necessarily source, but more conceptual like...[are we] making the solutions based on what occurred yesterday\u201d (I-4). These patterns\nimply that users demand comprehensive transparency of training data, especially the limitations."}, {"heading": "Output", "text": "While understanding the output is often an neglected aspect in algorithmic work of XAI, we saw frequent questions on it, indicating users\u2019 desire to understand the value of the AI system to appropriately evaluate the capability and to better utilize the AI, often in the on-boarding stage or dealing with complex decisions. Explaining output and explaining input/data were considered as \u201cstatic explanations\u201d that more likely come up in the early stage of system usage, instead of frequent \u201cday-today, or transaction-to-transaction interactions\u201d (I-8).\nThe most frequently asked questions were not regarding descriptive information of the algorithmic output, but at a high level, inquiring how to best utilize the output. We also identified two additional questions\u2013\u201cthe scope of the capability\u201d, and \u201chow the output impacts other system components.\u201d To address such user needs requires contextualizing explaining the system\u2019s output in downstream system tasks and the users\u2019 overall workflow."}, {"heading": "Performance", "text": "To our surprise, the performance category was repeatedly ranked at the bottom, especially for users without AI background and in decision contexts considered less critical. There was a common hesitation among informants to present ML performance metrics such as accuracy, not only because a numerical value could be hard to interpret by lay users, but also there may be discrepancy between performances on the test data and the actual data, creating different \u201cexperienced accuracy\u201d [98] that might deter users. Some also believed that small differences in these metrics would not change how users interact: \u201cTechnically that\u2019s great, but, it\u2019s still not a hundred... there\u2019s always going to be work that the users have to do to verify or double check\u201d (I-4).\nAs many informants pointed out, and suggested by the additional questions, the goal of explaining performance should be to help users understand the limitations of the AI, and make it actionable as to answer \u201cIs the performance good enough for....\u201d There are constraints of technical capabilities. For example, confidence scores were repeatedly dismissed as not providing enough actionability \u2013\u201c[users] struggle to really understand, does it mean it\u2019s going to do what I want it to do, or, can I trust it? \u201d (I-15). Regarding the additional question on \u201cWhat kind of mistake\u201d, informants mentioned the precision-recall trade-off is a deliberately decided limitation that should be explained as it might change users\u2019 course of actions [49] :\u201cIt\u2019s use case dependent... for the [doctors] if they miss a tumor, that\u2019s a life changing. So they have a very high tolerance for false positives \u201d(I-7)."}, {"heading": "How\u2013global model", "text": "Informants recognized the importance of providing global explanations on how the AI made decisions, both to help users appropriately evaluate the system capabilities, and build a mental model to better interact with or improve the system. Such needs were prominent in cases where users were in a quality-control role, or in a position able to adjust the model or the data-collection process: \u201cThe company really care about\nwhich of these attributes are the most important... then they will forward the manufacturer to include those in the data\u201d (I-17). Informants also agreed that users with AI or analytic background were more likely to seek global explanations.\nAs Table 1 shows, to answer the How question, XAI algorithms commonly employ ranked features, decision trees or rules. However, some informants were referring to high-level descriptions, such as \u201cI would just say keywords matching, it is intuitive, and it\u2019s been around\u201d (I-3). Some were also concerned about fitting a complete How explanation into the users\u2019 workflow: \u201cI can\u2019t imagine [doctors are] going into their workflow and be like, I\u2019m so busy, let me read more about this AI. But, they would probably want some kind of confirmation about how it makes decisions\u201d (I-11). So the design challenge is to identify the appropriate level of details to explain the model globally. This challenge is reflected in the question bank as well. While most XAI methods focus on answering \u201cWhat is the overall logic\u201d, we discovered that many questions were simply asking about the top features or whether certain feature was used, meanwhile a small set of questions by users with AI background were regarding the technical details of the model."}, {"heading": "Why, Why not\u2013local prediction", "text": "Understanding a particular decision was often ranked at the top, and in user questions mentioned in all products. These questions were naturally raised after a surprising or abnormal event: \u201cFor everyday interactions, most likely it\u2019s how did the system give me this answer? Not just any answer, but all of a sudden, here\u2019s this thing that I\u2019m [not expecting] seeing\u201d (I-8). This pattern is pointed out by Miller [63] as the contrastive nature of human explanations, which are often implying Why not the expected event. We observed a shared struggle with available technical solutions answering Why but not Why not. Several informants working on text-based ML commented on the inadequacy of the common approach by highlighting keywords that contribute to the prediction:\u201ceven though we explained conceptually how it\u2019s working, it wouldn\u2019t be able to explain that error. So it would actually be counter-intuitive why it should make that error\u201d (I-4). I-17 discussed the limitation of a state-of-the-art explanation algorithm, LIME [77], which generates feature importance for \u201cblack-box\u201d ML models. She found the static explanation to be unsatisfying: \u201cLIME would say \u2018it is boot cut which is why [it\u2019s not going to sell]\u2019, but would it be different if it was a skinny cut?\u201d\nMany current XAI algorithms focus on the Why question. We note that a challenge for algorithmic explanations is that the contrastive outcome is often not explicitly available to the model. These observations again suggest the benefit of interactive explanations, allowing users to explicitly reference the contrastive outcome and asking follow-up What if questions."}, {"heading": "What if, How to be\u2013inspecting counterfactual", "text": "This category of explainability needs was not ranked high, and informants mentioned only 3 related user questions. Currently, these kinds of explanation are not widely adopted in commercial AI products. As prior work suggested, awareness of new types of explainability could change user demand [56]. In fact, informants recognized its potential utility as system features\nto test different scenarios for users to gain further insights for the decision, and to understand the boundary of system capabilities to enable adapting interaction behaviors. Informants also identified that such features align with how data scientists currently debug to improve ML models. For example, I-4 was excited to consider how What if explanations might support supply chain managers make decisions\u2013\u201cyou can run different scenarios... the system can make an initial recommendation and then they can tweak it to see, the impact on the cost after that.\u201d I-13 speculated that How to be that explanations (how the chatbot would understand differently) could help chatbot users better phrase their queries. I-15 working on a tool for customizing entity recognition models commented that seeing how instance changes impact the output could help users debug the training data.\nAs seen in Table 1, there is a growing collection of XAI techniques addressing the counterfactual questions. However, currently the feature influence methods are mostly used in data science tools [45]. Contrastive feature and example based methods are relatively new areas of XAI work [27, 91, 100]. Our results suggest their potentials as utility features in a broad spectrum of AI products. Future work should explore these potentials and sensitize practitioners to these possibilities."}, {"heading": "Additional explainability needs", "text": "We also identified a set of questions that were not covered by the algorithm-informed needs categories. They point to additional areas of interest that users have for understanding AI. A critical area is to understand the changes and adaption of AI, both in explaining changes in the system, and how users can change the system. Other areas are follow-up questions by further inquiring why a certain feature or data is used, and terminological questions such as \u201cwhat do you mean by...\u201d, both of which may naturally emerge in an interactive explanation paradigm. Lastly, some users might be interested in knowing other people\u2019s experience with the system, suggesting a new mechanism for an AI system to provide social explanations with regard to other users\u2019 outcomes and actions."}, {"heading": "DISCUSSION", "text": "With widespread calls for transparent and responsible AI, industry practitioners are eager to take up the ideas and solutions from the XAI literature. However, despite recent effort toward a scientific understanding of human-AI interaction [29, 68, 103], XAI research still struggles with a lack of understanding of real-world user needs for AI transparency, and by far little consideration of what practitioners need to create explainable AI products. Our study suggests the following directions both for algorithmic work to close the gaps addressing user needs, and design support to reduce technical and practical barriers to create user-friendly XAI products. \u2022 XAI research should direct its attention to techniques that\naddress user needs, and we suggest a question-driven framework to embody these needs. Our results point to a few common questions and their desired answers that future work of XAI should explore, for example, How question answered by multi-level details describing the algorithm, Why not question referencing an expected alternative outcome, and How/Why will the system change. Considering\nthe coverage of user questions, especially common and new questions identified, could help the community move toward more human-centric effort. The question bank presented in this paper is just a starting point. Future work could continue building the repository by directly eliciting questions from end users of different types of AI systems [56].\n\u2022 Practitioners struggle with the gaps between algorithmic output and creating human-consumable explanations. To close the gap requires inter-disciplinary work that studies how humans explain, and formalizes the patterns in algorithmic forms. Such a practice has already been engaged in interactive ML [6, 86] and \"socially guided machine learning\" [89]. Prior work repeatedly pointed out that a prerequisite for explanations to be truly human-like is to be interactive [62, 63, 94], because explanation is a grounding process where people incrementally close the belief gaps. Indeed, our study found that some user questions are closely connected with or followed by other questions. Future work could explore interaction protocols, for example through statistical modeling of how humans ask different explanation-seeking questions [62], to drive the flow of interactive or conversational explanations.\n\u2022 Our study revealed the variability of user questions and its complex mechanisms, highlighting the challenge to identify product-specific user needs. While prior work attempted at top-down descriptions of needs of users in different roles [9, 10, 43, 79, 95], it may not be sufficient for design work that has to consider specific actions, usage points, models, etc. Recent HCI work on XAI encourages empirically identifying user needs with structured procedures [31, 96]. We suggest several ways the XAI question bank can be used for needs specification. First as heuristic guidance, a product team could enumerate on whether each question category has been addressed and which should be prioritized. Second, it can be used in user research to scaffold the elicitation of user needs. For example, card-sorting exercises of the questions can be performed with users (adaptation may be required for specific AI applications). We invite practitioners to use, revise and expand the XAI question bank.\n\u2022 The technical barriers for designers and practitioners in general to navigate the space of XAI remains a primary challenge for product teams to optimize XAI user experiences. To support design work for ML, Yang suggested research opportunities to \u201csensitize designers to the breadth of ML capabilities\u201d [97]. Informants also expressed strong desire for support of technical discussions with data scientists and stakeholders, as mitigating the friction is critical for the success of their advocacy for explainability. An opportunity for sensitizing support is to create concrete mapping between user questions and algorithmic capabilities, serving as a shared cognitive artifact between the designers and data scientists. One example, perhaps over-simplified, is the taxonomy of XAI methods we presented in Table 1. We may envision a question-driven design process: by user research, a design practitioner identifies the primary type of user question as what to explain (e.g. How to be that), and also the requirements to address the question as How to explain. Table 1 then suggests candidate explanation method(s) to answer the question (e.g. contrastive features). Together with\na data scientist, the team find the most suitable solution to implement from the list of suggested algorithms, then work toward closing the gaps between the algorithmic output and user requirements to answer the question. By suggesting conceptually this question-driven design process, we invite the research community to develop more fine-grained frameworks of XAI features (e.g., considering UI patterns) that connect user questions and technical capabilities."}, {"heading": "Limitations", "text": "First of all, the user questions were explored through design practitioners instead of end users, so we cannot claim this is a complete analysis of user needs for explainability. The results only reflect design practitioners\u2019 views. Future work could study other roles involved in AI product development (e.g., data scientists) to better understand the challenges to create XAI products. Our product samples focus on ones supporting high-stakes tasks, where needs for explainability might have been greater, and the current status of XAI more advanced. We do not claim the completeness of the XAI methods discussed, especially as this is a fast advancing research field. Practitioners\u2019 increasing accessibility to XAI techniques may also change the demands and concerns expressed in the study. Finally, our informants worked for the same organization. Although this is not uncommon for studies of practitioners [7, 32, 67] and we recruited informants from diverse product lines and locations, we acknowledge that design practices may be different in other companies or organizations."}, {"heading": "CONCLUSION", "text": "Although the research field of XAI is experiencing exponential growth, there is little shared practices of designing userfriendly explainable AI applications. We take the position that the suitability of explanations is question dependent and requires an understanding of user questions for a specific AI application. We develop an XAI question bank to bridge the spaces of user needs for AI explainability and technical capabilities provided by XAI work. Using it as a study probe, we explored together with industry design practitioners the opportunities and challenges in putting XAI techniques into practice. We illustrated the great variability of user questions that may subject to many motivational, contextual and individual factors. We also identified gaps between current algorithmic solutions of XAI and what\u2019s needed to deliver satisfying user experiences, in the types of user questions to address and how they are addressed. We join many others in this field advocating a user-centered approach to XAI [4, 30, 63, 92]. Our work suggests opportunities for the HCI and AI communities, as well as industry practitioners and academics, to work together to advance the field of XAI through translational work and shared knowledge repository that maps between user needs for explainability and XAI technical solutions."}, {"heading": "ACKNOWLEDGEMENTS", "text": "We thank all our anonymous participants. We also thank Zahra Ashktorab, Rachel Bellamy, Amit Dhurandhar, Werner Geyer, Michael Hind, Stephanie Houde, David Millen, Michael Muller, Chenhao Tan, Richard Tomsett, Kush Varshney, Justin Weisz, Yunfeng Zhang, and anonymous CHI 2020 reviewers for their helpful feedback."}], "title": "Questioning the AI: Informing Design Practices for Explainable AI User Experiences", "year": 2020}