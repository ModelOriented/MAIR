{"abstractText": "More and more software practitioners are tackling towards industrial applications of artificial intelligence (AI) systems, especially those based on machine learning (ML). However, many of existing principles and approaches to traditional systems do not work effectively for the system behavior obtained by training not by logical design. In addition, unique kinds of requirements are emerging such as fairness and explainability. To provide clear guidance to understand and tackle these difficulties, we present an analysis on what quality concepts we should evaluate for AI systems. We base our discussion on ISO/IEC 25000 series, known as SQuaRE, and identify how it should be adapted for the unique nature of ML and Ethics guidelines for trustworthy AI from European Commission. We thus provide holistic insights for quality of AI systems by incorporating the ML nature and AI ethics to the traditional software quality concepts.", "authors": [{"affiliations": [], "name": "Hiroshi Kuwajima"}, {"affiliations": [], "name": "Fuyuki Ishikawa"}], "id": "SP:d624536cdbc5ffcc51004dcfaad94609cf860f63", "references": [{"authors": ["ISO/IEC 25000:2014"], "title": "Systems and software engineering \u2013 Systems and software Quality Requirements and Evaluation (SQuaRE) \u2013 Guide to SQuaRE", "venue": "International Organization for Standardization, International Electrotechnical Commission, Standard, 2014.", "year": 2014}, {"authors": ["F. Ishikawa", "N. Yoshioka"], "title": "How do engineers perceive difficulties in engineering of machine-learning systems? - questionnaire survey", "venue": "Joint International Workshop on Conducting Empirical Studies in Industry and 6th International Workshop on Software Engineering Research and Industrial Practice (CESSER-IP 2019), May 2018.", "year": 2019}, {"authors": ["M. Zinkevich"], "title": "Rules for reliable machine learning: Best practices for ML engineering", "venue": "NIPS 2016 Workshop on Reliable Machine Learning in the Wild, December 2017.", "year": 2016}, {"authors": ["E. Breck", "S. Cai", "E. Nielsen", "M. Salib", "D. Sculley"], "title": "What\u2019s your ML test score? a rubric for ML production systems", "venue": "NIPS 2016 Workshop on Reliable Machine Learning in the Wild, December 2017.", "year": 2016}, {"authors": ["D. Sculley", "G. Holt", "D. Golovin", "E. Davydov", "T. Phillips", "D. Ebner", "V. Chaudhary", "M. Young"], "title": "Machine learning: The high interest credit card of technical debt", "venue": "NIPS 2014 Workshop on Software Engineering for Machine Learning (SE4ML), December 2014.", "year": 2014}, {"authors": ["N. Polyzotis", "S. Roy", "S.E. Whang", "M. Zinkevich"], "title": "Data management challenges in production machine learning", "venue": "The 2017 ACM International Conference on Management of Data (SIGMOD 2017), May 2017, pp. 1723\u20131726.", "year": 2017}, {"authors": ["Q. Liu", "P. Li", "W. Zhao", "W. Cai", "S. Yu", "V.C.M. Leung"], "title": "A survey on security threats and defensive techniques of machine learning: A data driven view", "venue": "IEEE Access, vol. 6, pp. 12 103\u201312 117, February 2018.", "year": 2018}, {"authors": ["D. Gunning"], "title": "Explainable artificial intelligence (XAI)", "venue": "IJCAI 2016 Workshop on Deep Learning for Artificial Intelligence (DLAI), July 2016.", "year": 2016}, {"authors": ["I. Goodfellow", "J. Shlens", "C. Szegedy"], "title": "Explaining and harnessing adversarial examples", "venue": "International Conference on Learning Representations (ICLR), May 2015.", "year": 2015}, {"authors": ["S. Corbett-Davies", "S. Goel"], "title": "The measure and mismeasure of fairness: A critical review of fair machine learning", "venue": "https://arxiv.org/abs/1808.00023, August 2018.", "year": 1808}, {"authors": ["J.M. Zhang", "M. Harman", "L. Ma", "Y. Liu"], "title": "Machine learning testing: Survey, landscapes and horizons", "venue": "https://arxiv.org/abs/1906.10742, June 2019.", "year": 1906}, {"authors": ["K. Czarnecki", "R. Salay"], "title": "Towards a framework to manage perceptual uncertainty for safe automated driving", "venue": "The 1st International Workshop on Artificial Intelligence Safety Engineering (WAISE 2018), September 2018, pp. 439\u2013445.", "year": 2018}, {"authors": ["M.T. Ribeiro", "S. Singh", "C. Guestrin"], "title": "why should I trust you?\u201d: Explaining the predictions of any classifier", "venue": "The 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD 2016), August 2016, pp. 1135\u20131144.", "year": 2016}, {"authors": ["H. Kuwajima", "M. Tanaka", "M. Okutomi"], "title": "Improving transparency of deep neural inference process", "venue": "Progress in Artificial Intelligence, vol. 8, no. 2, pp. 273\u2013285, Jun 2019.", "year": 2019}], "sections": [{"text": "ar X\niv :1\n90 8.\n02 13\n4v 1\n[ cs\n.C Y\n] 3\n1 Ju\nl 2 01\nIndex Terms\u2014machine learning, artificial intelligence, software quality, SQuaRE, ethics\nI. INTRODUCTION\nThere has been increasing effort for industrial applications of artificial intelligence (AI) systems. This is particularly driven by technical advance in machine learning (ML) techniques including deep learning. Quality, dependability, or trust of such AI systems has been attracting wide attention both from the technical and social aspects.\nTraditionally, the ML community has focused on accuracy over the whole data set (often just given). However, it is necessary to have more granular and specific evaluations in terms of requirements, to be reflected to data design, as well as consideration on a variety of other quality aspects.\nWe naturally consider adopting existing principles for traditional systems. The ISO/IEC 25000 series provides a framework or set of models for evaluation of software product quality, known as SQuaRE (System and Software Quality Requirements and Evaluation) [1]. Although SQuaRE provides useful insights, it will not be directly applied to ML-based AI systems as it is. The essential difference is that ML components, such as deep neural networks (DNN), consist of enormous parameters and are constructed from training data. The resulting component is black-box and unexplainable, implementing a large fuzzy function such as image recognition and anomaly detection. Such functions implemented by ML components often directly constitute the core functions of the whole system, quality of which is thus affected by the\nnature of ML. In one survey, more than 40% of the survey participants answered existing approaches do not work for quality assurance of ML-based AI systems [2].\nOn the other hand, new requirements are emerging given the advance of AI systems as shown in Ethics guidelines for trustworthy AI published by European Commission (hereafter just \u201cEthics guidelines\u201d). Unique requirements have been discussed including human rights under AI autonomy, fairness, and explainability given the high impact of AI systems on human activities as well as the unique ML nature.\nIndustrial practitioners are required to holistically examine the whole picture of quality evaluation for emerging AI systems. Our objective in this paper is to support them by identifying the necessary updates on SQuaRE at its conceptual level (called quality characteristic and sub-characteristic) for AI systems. Our analysis is conducted from two viewpoints: what should be modified and what should be added. The first point is investigated by checking which existing concepts in SQuaRE are invalidated by the unique nature of ML. The second point is investigated by checking how the Ethics guidelines should be reflected to SQuaRE. We thus provide holistic insights incorporating the ML nature and AI ethics to SQuaRE from traditional software engineering.\nThe remainder of this paper is organized as follows. We first introduce the targets of our analysis, SQuaRE and the Ethics guideline, and also discuss related work in Section II. After presenting the methodology of our analysis in Section III, we present the results of our analysis from two approaches in Section IV. We conclude the paper with remarks for future perspective in Section V."}, {"heading": "II. BACKGROUND", "text": "A. SQuaRE\nThe standard series of ISO/IEC 25000 or SQuaRE provides a framework or set of models for evaluation of software product quality. The core of SQuaRE is hierarchical (treestructured) definition of quality models, characteristics, and sub-characteristics, which defines the concepts or terminology about what we should evaluate in systems. Quality measures define how to quantitatively evaluate each quality subcharacteristic in the form of a mathematical formula. An example branch of a quality model, a characteristic, a subcharacteristic, and a quality measure are Product quality,\nReliability, Maturity, and Mean time between failure (MTBF). The MTBF measure is defined as a rate of Operation time and Number of system/software failures that actually occurred. Here these two elements of the measure are called Quality measure elements (QMEs).\nIn this paper, we discuss two of the top-level quality models about evaluating systems, specifically, Product quality model and Quality in use model. These two models consider the development-time and run-time evaluation of systems, respectively. We use abbreviations for product and for use when we want to clarify which quality model a (sub-)characteristic belongs to, i.e., Product quality or Quality in use, respectively.\nB. Ethics Guidelines for Trustworthy AI\nEthics guidelines for trustworthy AI (Ethics guidelines) were written by High-Level Expert Group on AI (AI HLEG), an independent expert group that was set up by the European Commission (EC) in June 2018. The guidelines were published on April 8th, 2019, following the first draft released on December 18th, 2018. The guidelines have four ethical principles: 1) Respect for human, 2) Prevention of harm, 3) Fairness, and 4) Explicability; and seven key (ethical) requirements. The guidelines also have a pilot assessment list that includes concrete assessment items associated to requirements.\nThe list has a tree structure and the top-level requirements are 1) Human agency and oversight, 2) Technical robustness and safety, 3) Privacy and data governance, 4) Transparency, 5) Diversity, non-discrimination and fairness, 6) Societal and environmental well-being, and 7) Accountability, as illustrated in Fig. 1. The list also defines subcategories of the requirements. We call them as sub-requirements. Boxes with thick lines and those with thin lines in Fig. 1 represent requirements and sub-requirements, respectively.\nEach sub-requirement includes a hierarchical check-list that consists of assessment items in the form of questions. An example branch of a requirement, a sub-requirement, and an assessment item is Technical robustness and safety, Resilience to attack and security, and \u201cDid you assess potential forms of attacks to which the AI system could be vulnerable?\u201d\nC. Related Work\nLeading companies such as Google have published problem statements and guidelines based on their experience regarding testing (quality evaluation) or maintenance (technical debts) [3]\u2013[5]. The ML community summarized challenges in the data-centric nature, e.g., data management [6] and security [7]. There have been notably active discussions for explainability [8], adversarial examples [9], and fairness [10].\nThe research community of software engineering has been actively moving towards principles and techniques for MLbased AI systems. However, the initial outcome is almost limited to testing and verification techniques [11].\nThe industrial practitioners need to work holistically on various aspects of quality. The objective of this paper is to provide clear guidance from this viewpoint, which will complement the above focused studies and support planning, decision making, and management activities in the industry."}, {"heading": "III. ANALYSIS METHODOLOGY", "text": "We analyze the latest standards of SQuaRE series to identify how we should adapt them for ML-based AI systems, and how they cover Ethics guidelines for trustworthy AI. Specifically, we analyze what should be modified and what should be added, respectively, as depicted in the following research questions.\nRQ1 How should we extend the existing (sub-\n)characteristics in SQuaRE when applied to ML-based AI systems?\nRQ2 What quality (sub-)characteristics should we add to\nSQuaRE for AI ethics?\nFor RQ1, we exhaustively check the existing QMEs, at the concrete level, to discover metrics that are not useful or that are not applicable when we consider ML-based AI systems. By focusing on the concrete metrics, we try to identify the gaps even though the quality characteristics or sub-characteristics are defined to be generic with abstract, broad terms.\nFor RQ2, we exhaustively check the assessment items in the Ethics guidelines. We attempt to map all the sub-requirements to the quality (sub-)characteristics, thus identifying what are essentially missing in SQuaRE."}, {"heading": "IV. ANALYSIS RESULTS", "text": "A. Modifying Existing (Sub-)Characteristics in SQuaRE\nWe first present the results of analysis on the unique nature of ML for RQ1. We identified two kinds of driving forces to adapt SQuaRE and discuss each of them in Sections IV-A1 and IV-A2, respectively.\n1) Metrics with Large Fuzzy Evaluation Targets: One driving force for adapting SQuaRE is that large fuzzy functions are implemented with ML, such as image recognition for detecting different kinds of objects in a variety of situations. This point is different from traditional software systems where functions are originally decomposed and given different logical (caseby-case) specifications and implementations. Due to the difference, QMEs that count \u201csuccessful\u201d elements do not work in a straightforward way for ML-based AI systems (specifically, functions, tasks, and contexts). Some examples of the QMEs that are affected by this point are illustrated in Table I.\nNow we particularly focus on an example sub-characteristic Functional completeness that is the fraction of 1) Number of functions missing and 2) Number of functions specified, as illustrated in Table I. The following discussion can be generalized to other (sub-)characteristics using number of successful functions, tasks, and contexts. Being evaluated for ML-based AI systems, we interpret that functions \u201cmissing\u201d are the functions that were not successfully trained, even though developers specified to train them. In other words, an ML-based AI system under evaluation fails on these missing functions, even though developers specified the training or test data sets for the ML components used in the system so that the data sets include such functions. On the other hand, we interpret that functions \u201cspecified\u201d are the functions that are included in such data sets, as well. Then, in order to measure\nFunctional completeness, we must count such functions in a data set. However in an ML-based AI system, a large fuzzy function is specified by a large data set for the ML components used in the system, e.g. a function to detect any objects in a data set with a certain dataset wide accuracy. Measuring Functional completeness does not make sense for such a fuzzy large function.\nIn order to measure such QMEs for a large fuzzy function trained in a data driven manner, we decompose it into finegrained functions [12] by splitting a data set into partitions that include small functions. The decomposition must be done in an application-specific way, greatly incorporating domain experts and their domain knowledge. ML-based AI systems will be evaluated for each partition, and the functions associated with that partition are counted for quality measurement. In the case of pedestrian detection, we can consider more meaningful evaluation by splitting the data set to capture specific types of pedestrian and weather condition (functions and contexts).\nExtension A1: Decomposition of Evaluation Target\n(Sub-)characteristics to measure number of successful tasks/functions/contexts, i.e, a characteristic Effectiveness (Quality in use) and sub-characteristics Functional completeness, Functional correctness, Functional Appropriateness, Testability (Product quality), Context completeness, Flexibility (Quality in use), should be extended\nto consider application-specific decomposition of a large fuzzy task/function/context implemented by ML components.\nSecond, if a large fuzzy function is decomposed by splitting data sets, then selection of data sets to split is important. In the current practice, an ML components is evaluated as the total performance on a test data set. However, Functional completeness is not the accuracy for a given test data set but for a given specification, and it should be extended to connect data sets and specifications. Collecting training, test, and operational data are (part of) designing, building requirements, and investigating actual usage of ML components, respectively. Thus, the functions specified by design specifications and those specified by requirements specifications should be included in the training and the test data, respectively. Those counted for use should be included in the operational data. For example in a measure Task completed, unique tasks completed and those attempted are the successful tasks and the tasks attempted both in operational data, respectively.\nExtension A2: Quality Measure and Data Set\n(Sub-)characteristics to measure successful tasks/functions/contexts should be extended to consider the relationships to training, test, and operational data\nsets that have different roles for ML components.\nThird, we need to count \u201csuccessfully trained\u201d functions. It is not simple for ML-based AI systems, because ML components normally do not achieve 100% accuracy. ML components can success or fail on the same function, i.e., we must handle the uncertainty of measurement, even if we get fine-grained functions. The behavior of ML components, DNN in particular, changes unstably, because they are highly nonlinear. It should be considered that uncertainty indices such as bias and variance are added for each quality measure.\nExtension A3: Handling of Uncertainty\n(Sub-)characteristics to measure number of successful tasks/functions/contexts should be extended to consider the uncertainty of measurement.\nExtension A1-3 address the following inabilities of an ML component: 1) It does not have specification and functions are not explicitly specified; 2) It is highly nonlinear and the behavior is not robust even within a small function. These properties are important for quality measurement for conventional software. First, we implement conventional software based on specifications, and thus, we are able to identify functions based on them. Then, conventional software can be evaluated for each function. Second, robustness is prerequisite for quality evaluation based on specifications. In conventional software, the behavior of a function is robust, therefore it can be treated as a single unit of quality evaluation.\n2) Metrics with Missing Techniques: The other kind of driving force to adapt SQuaRE is that there are subcharacteristics for which effective measurement technique has not been established in the case of ML-based implementation, especially DNN.\nA quality sub-characteristic Operability has a measure Monitoring capability. Explainable AI (XAI) [8] is a rapid growing area in the artificial intelligence research, and techniques to explain or interpret ML components are proposed in recent years [13], [14]. They can be used for monitoring capacities for ML-based AI systems, but XAI research is still in the very early stage. Recommended techniques of XAI have yet to be developed.\nExtension A4: Investigation of Monitoring Capacity\nA sub-characteristic Operability (Product quality) that has a measure Monitoring capacity should be extended with recommended monitoring (XAI) techniques.\nIt has been reported that image recognition models incorrectly infer classes or objects with high confidence, due to small noise that cannot be recognized by humans. Such noise is called adversarial examples (AEs) [9], and can be a new type of data corruption for ML components. Corruption prevention\nmethods against AEs are studied in recent years, but we do not have the established or recommended techniques until now. A sub-characteristic Integrity that has a measure Internal data corruption prevention should address such AEs in addition to the conventional data corruption.\nExtension A5: Investigation of Data Integrity\nA sub-characteristic Integrity (Product quality) should be extended to evaluate the impact of ML specific data corruption such as adversarial examples.\nSQuARE defines a characteristic Maintainability that has a measure Coupling of components. However, coupling of ML components is not well studied, that is, we do not know whether a component has no impact on others. If an ML component is trained along with specific surrounding components and systems, then, it is tightly fit them and cannot be decoupled. However, we cannot measure such modularity with current technologies.\nExtension A6: Investigation of Modularity\nA sub-characteristic Modularity (Product quality) should be extended to evaluate the expected ML components\u2019 independency from surrounding components and systems.\nWork time to make a specific type of modification is also an important aspect of maintainability, and that is defined as a measure Modification efficiency in a characteristic Maintainability. That work time for ML-based AI systems should include the training time to make a specific type of modification for ML components. However, expected work time, i.e., expected training time is generally unknown in advance to training with current technology.\nExtension A7: Investigation of Modifiability\nA sub-characteristic Modifiability (Product quality) should be extended to evaluate the expected training time of ML components used in ML-based AI systems, without actually conducting training.\nThe complexity of systems is one of key factors of maintainability. In SQuARE, cyclomatic complexity score is used in a measure Cyclomatic complexity adequecy. It is represented by the number of linearly independent execution paths, and is not suitable for ML components, because they have only one linearly independent path and the score always equals to a constant. ML components can show different complexity in other manners. Number of parameters and layers, numerical precision like 8-bit unsigned integer and 16-bit floating point, or simply number of multiply-accumulate operations (MACs) performed can be used to measure the structural (or computational) complexity of ML components. In addition to such structural complexity, we need the behavioral complexity,\ni.e. robustness, for ML components. ML components with the same structure (architecture) but with different parameters can behave differently; a component can show highly nonlinear behavior but another can show robust behavior.\nExtension A8: Investigation of Complexity\nA sub-characteristic Maintainability (Product quality) should be extended to evaluate both structural complexity and behavioral complexity (robustness) of ML components.\nB. Adding New (Sub-)Characteristics in SQuaRE\nWe present the results of the second analysis on the Ethics guidelines for RQ2. We exhaustively checked all the subrequirements in the Ethics guidelines, map all them items to the quality (sub-)characteristics in SQuaRE, and identified extensions to SQuaRE for AI systems.\n1) Autonomy and Human: The sub-requirement of Fundamental rights includes assessment items on negative impacts on fundamental rights, unintended interference on human decision making, and notification about existence of non-human agents. This sub-requirement basically reflects the respect on human autonomy.\nThere are four sub-requirements about interaction between human and AI systems in the Ethics guideline. Specifically, Human agency and Human oversight mention prevention of overconfidence and appropriate control by human, respectively. The sub-requirement of Explainability is about user understanding the decision and outcome by AI systems. The subrequirement of Communication also mentions similar points but put more focus on clarification for the target audience, feedback cycles, and psychological aspects.\nAmong the characteristics for use in SQuaRE, the quality characteristic of Freedom from risk matches with this aspect. Currently, sub-characteristics regarding economic risk, health and safety risk, and environmental risk are included. It is natural to add a sub-characteristic about risk on human rights.\nExtension B1: Risk on Human Autonomy\nA sub-characteristic Mitigation of risk on human autonomy should be added in the Freedom from risk characteristic (Quality in use).\nRegarding the characteristics for product, we interpret all of the above aspects are extending the traditional notion of usability; now human not only use systems by their commands but allow and rather expect autonomous systems to run proactively but still under control and understanding. We therefore propose to extend the characteristic of Usability to incorporate this change.\nExtension B2: Collaboratability\nA characteristic Usability should be extended to Collabo-\nratability to reflect autonomous roles of AI systems. Additional sub-characteristics should include Controllability and Explainability as well as Collaboration Effectiveness (Product quality).\n2) Fairness: The sub-requirement of Unfair bias avoidance mentions the demand for avoiding unfair bias or considering diversity of users. As this point has been one of the key issues for ML-based AI systems [10], we extend SQuaRE to include it. In the Ethics guideline, one description of fairness refers to equal and just distribution of both benefits and costs as well as freedom from unfair bias, discrimination, and stigmatisation. This point is included in parallel with the Extension B1 as a different kind of risks.\nExtension B3: Fairness\nA sub-characteristic Mitigation of risk by unfair bias should be added in the Freedom from risk characteristic (Quality in use).\n3) Accuracy: The requirement Technical robustness and safety describes technical points that have been basically common for dependable systems. Differences in AI systems are the focus on Accuracy (one of the sub-requirement). Capturing this point has been discussed as one of the core topics in Section IV-A regarding evaluations of functionality, e.g., completeness and correctness.\n4) Privacy: The sub-requirement Respect for privacy and data protection mentions data protection, minimal use of sensitive or personal data, control over personal data, and other similar issues. Surprisingly, SQuaRE did not include specific items for privacy though some parts are covered by the Security characteristic. This is probably because the demand for privacy recently emerged given increasing use of data.\nWe should naturally extend SQuaRE to include privacy\nconcerns, probably not limited to targeting AI systems.\nExtension B4: Privacy\nThe Security characteristic should be extended to Security and Privacy, to incorporate an additional subcharacteristic of Privacy (Product quality).\n5) Accountability: The sub-requirement of Reliability and reproducibility includes, among the common concepts of reliability, the concept of reproducibility, that is, whether the same behavior can be exhibited in experiments with the same condition. This is a point attracting wide attention in AI research where outcomes may be affected by randomness and configuration parameters.\nThe sub-requirement of Traceability mentions documentation of how the system is constructed, e.g., algorithms and testing methods. This sub-requirement suggests the demand for responsibility in algorithmic decisions, as already included\nin GDPR (General Data Protection Regulation). The concept is different from the traditional notion of \u201ctraceability\u201d in software engineering, which was about management of (often internal) deliverables for the purpose of maintainability. Indeed, traceablity appears as a note for the Reusability subcharacteristic in SQuaRE.\nThe sub-requirement Auditability, in the requirement Accountability, mentions traceability and logging of processes and outcomes as well as separated auditability for each aspect.\nGiven the increasing demand for accountability, as in GDPR, we note the significance of these aspects. However, we interpret these aspects come at the meta-level of SQuaRE (sub-)characteristics, rather than a (sub-)characteristic.\nMeta-Level Consideration: Accountability\nThere is increasing demand for accounting the evidences that justify the evaluation results for (sub-)characteristics. This point should be noted when evaluation activities are planned and conducted.\n6) Other Requirements: The sub-requirement of Fallback plan and general safety mentions fallback plans and safety risks. The sub-requirement of Reliability and reproducibility also mentions general aspects of reliability assessment, except for the reproducibility part. These aspects have been covered in the Reliability and Security characteristics for product as well as the Freedom from risk characteristic for use.\nThe sub-requirements of Quality and integrity of data and Access to data mention data management, monitoring, access control, and so on. These aspects have been partially covered in the Security characteristic but are most assessment items are about the internal implementations, which is out of the scope for this paper. Data quality of SQuARE may cover them.\nThe sub-requirement of Accessibility and universal design mentions consideration of disabilities and people from different backgrounds. This is included in the Accessibility subcharacteristic for product.\nThe sub-requirement of Stakeholder participation was considered as out of the scope. This is rather a recommendation on the process, not evaluation of the system itself, and such an aspect has not been included in SQuaRE. We also exclude the sub-requirements of Social impact and Society and democracy about social impacts of the AI systems.\nIn the analysis, we have excluded assessment items regarding organizational activities, such as insurance policy. For the same reason, the sub-requirements of Minimising and reporting negative impact, Documenting trade-offs, and Ability to redress are out of the scope."}, {"heading": "V. CONCLUDING REMARKS", "text": "In this paper, we have presented our analysis on how to adapt SQuaRE for ML-based AI systems. Obviously, the current version of SQuaRE did not take ML-based implementations into consideration as at that time ML was almost in laboratory. Nevertheless, not limited to the specific nature of ML,\nSQuaRE could take updates to reflect the increasing impacts of systems on human activities, for example to consider risks for human rights and privacy. We also reviewed the coverage of Ethics guidelines for trustworthy AI with SQuaRE, which revealed most part of the Ethics guidelines are not covered by SQuaRE. We believe this preliminary work provides proper guidance for industrial practitioners without waiting for the long-lasting update process of the standards.\nOur focus in this paper has been so-called external quality of systems, not about internal quality about intermediate activities and deliverables through the process of development and operation. We will continue our investigation to consider internal quality aspects so that guidance is provided for concrete activities on training data, specification documents, test design, runtime monitoring, and maintenance."}], "title": "Adapting SQuaRE for Quality Assessment of Artificial Intelligence Systems", "year": 2019}