{"abstractText": "Explainable Artificial Intelligence (XAI) systems, including intelligent agents, must be able to explain their internal decisions, behaviours and reasoning that produce their choices to the humans (or other systems) with which they interact. In this paper, we focus on how an extended model of BDI (Beliefs-Desires-Intentions) agents can be able to generate explanations about their reasoning, specifically, about the goals he decides to commit to. Our proposal is based on argumentation theory, we use arguments to represent the reasons that lead an agent to make a decision and use argumentation semantics to determine acceptable arguments (reasons). We propose two types of explanations: the partial one and the complete one. We apply our proposal to a scenario of rescue robots.", "authors": [], "id": "SP:095a818a564cf03a1b9e261c97c61c05b985bd9f", "references": [{"authors": ["Anand S Rao", "Michael P Georgeff"], "title": "BDI agents: from theory to practice", "venue": "In ICMAS,", "year": 1995}, {"authors": ["Cristiano Castelfranchi", "Fabio Paglieri"], "title": "The role of beliefs in goal dynamics: Prolegomena to a constructive theory of intentions", "year": 2007}, {"authors": ["Phan Minh Dung"], "title": "On the acceptability of arguments and its fundamental role in nonmonotonic reasoning, logic programming and n-person games", "venue": "Artificial intelligence,", "year": 1995}, {"authors": ["Wassila Ouerdane", "Nicolas Maudet", "Alexis Tsoukias"], "title": "Argumentation theory and decision aiding", "venue": "In Trends in Multiple Criteria Decision Analysis,", "year": 2010}, {"authors": ["Mariela Morveli-Espinoza", "Ayslan Trevisam Possebom", "Josep Puyol-Gruart", "Cesar Augusto Tacla"], "title": "Argumentation-based intention formation", "venue": "process. DYNA,", "year": 2019}, {"authors": ["Leila Amgoud", "Philippe Besnard"], "title": "A formal characterization of the outcomes of rule-based argumentation systems", "venue": "In International Conference on Scalable Uncertainty Management,", "year": 2013}, {"authors": ["Sule Anjomshoae", "Amro Najjar", "Davide Calvaresi", "Kary Fr\u00e4mling"], "title": "Explainable agents and robots: Results from a systematic literature review", "venue": "In Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems,", "year": 2019}, {"authors": ["Joost Broekens", "Maaike Harbers", "Koen Hindriks", "Karel Van Den Bosch", "Catholijn Jonker", "John-Jules Meyer"], "title": "Do you get it? user-evaluated explainable bdi agents", "venue": "In German Conference on Multiagent System Technologies,", "year": 2010}, {"authors": ["Maaike Harbers", "Karel van den Bosch", "John-Jules Meyer"], "title": "Design and evaluation of explainable bdi agents", "venue": "In 2010 IEEE/WIC/ACM International Conference on Web Intelligence and Intelligent Agent Technology,", "year": 2010}, {"authors": ["Pat Langley", "Ben Meadows", "Mohan Sridharan", "Dongkyu Choi"], "title": "Explainable agency for intelligent autonomous systems", "venue": "In Twenty-Ninth IAAI Conference,", "year": 2017}, {"authors": ["Isabel Sassoon", "Elizabeth Sklar", "Nadin Kokciyan", "Simon Parsons"], "title": "Explainable argumentation for wellness consultation", "venue": "In Proceedings of 1st International Workshop on eXplanable TRansparent Autonomous Agents and Multi-Agent Systems (EXTRAAMAS2019),", "year": 2019}], "sections": [{"text": "ar X\niv :2\n00 9.\n05 89\n7v 1\nExplainable Artificial Intelligence (XAI) systems, including intelligent agents, must be able to explain their internal decisions, behaviours and reasoning that produce their choices to the humans (or other systems) with which they interact. In this paper, we focus on how an extended model of BDI (Beliefs-Desires-Intentions) agents can be able to generate explanations about their reasoning, specifically, about the goals he decides to commit to. Our proposal is based on argumentation theory, we use arguments to represent the reasons that lead an agent to make a decision and use argumentation semantics to determine acceptable arguments (reasons). We propose two types of explanations: the partial one and the complete one. We apply our proposal to a scenario of rescue robots.\nKeywords Intelligent Agents \u00b7 Goal Reasoning \u00b7 Explainable Agency \u00b7 Argumentation"}, {"heading": "1 Introduction", "text": "Explainability of intelligent agents has gained attention in recent years due to their growing utilization in human applications such as recommendation or coaching systems in domains as e-health, UAVs (Unmanned Aerial Vehicle), or smart environments. In these applications, the outcomes returned by the agent-based systems can be negatively affected due to the lack of clarity and explainability about their dynamics and rationality. Thus, if these systems would have explainability abilities, then their understanding, reliability, and acceptance could be enhanced.\nThe BDI model [1] has become possibly the best-known and best-studied model of practical reasoning agents. In this model, agents decide which actions to perform in order to achieve their goals, which are selected during the deliberation process1. This means that BDI agents are able to select the goals they are going to commit to \u2013 which are called intentions \u2013 from a set of desires; however, they are not endowed with explainability abilities.\nConsider a scenario of a natural disaster, where a set of robot agents wander an area in search of people needing help. When a person is seriously injured, he/she must be taken to the hospital; otherwise, he/she must be sent to a shelter.\n1Practical reasoning involves a deliberation process, to decide what states of affairs to achieve, and a means-ends reasoning, to decide how to achieve such states.\nAfter the rescue work, the robots can be asked \u2013 by another robot or by a human \u2013 for an explanation of why a wounded person was sent to the shelter instead of taking him/her to the hospital, or why the robot decided to take to the hospital a person x first, instead of taking another person y. In this case, it is clear that it is important to endow the agents with the ability of explain their decisions, that is, to explain how and why a certain desire became (or not) an intention. In the case of BDI agents, the path of this explanation is made up of only one step, which happens because in BDI agents there is only two stages in the intention formation process: desires and intentions. This means that there is a lack of a fine-grained analysis of this process, which may improve and enrich the informational quality of the explanations.\nAn extended model for intention formation has been proposed by Castelfranchi and Paglieri [2], which was named the Belief-based Goal Processing model (let us denote it by BBGP model). The BBGP model has four stages: (i) activation (denoted ac)2, (ii) evaluation (denoted ev), (iii) deliberation (denoted de), and (iv) checking (denoted ck). Consequently, four different statuses for a goal are defined: (i)active (=desire and denoted ac), (ii) pursuable (denoted pu), (iii) chosen (denoted ch) and (iv) executive (=intention and denoted ex). When a goal passes the activation (resp. evaluation, deliberation, checking) stage, it becomes active (resp. pursuable, chosen, executive). Unlike the BDI model, where desires and intentions are different mental states, Castelfranchi and Paglieri argue that desires and intentions may be considered as goals at different stages of processing.\nArgumentation is a process of constructing and comparing arguments considering the conflicts \u2013 which are called attacks \u2013 among them. The output of argumentation process is a set (or sets) of arguments \u2013 called extensions \u2013 which are internally consistent [3]. In the intention formation process, arguments can represent reasons for a goal to change (or not) its status. Thus, one can see the intention formation process as a decision-making process, where an agent has to decide which goal(s) passes a given stage and which does not. Adopting an argumentation-based approach in a decision making problem has some benefits. For example, a (human) user will obtain a \u201cgood\u201d choice along with the reasons underlying this recommendation. Besides, argumentation-based decision-making is more similar with the way humans deliberate and finally make a choice [4].\nIn [5], the authors proposed an argumentation-based approach to computationally formalize the BBGP model. They used argumentation to support and justify the passage (change of the status) of the goals from their initial status until their final status. However, they did not endow BBGP-based agents with the ability of explaining the decisions about their goals. In this article, we aim to fill up this gap by equipping BBGP-based agents with a structure and a mechanism to generate such explanations.\nNext section focuses on the knowledge representation. Section 3 presents the building blocks necessary for the generation of explanations. Section 4 presents our proposal for generating partial and complete explanations. Section 5 is devoted to the application of the proposal to the scenario of the rescue robots. Section 6 presents the main related work. Finally, Section 7 is devoted to conclusions and future work."}, {"heading": "2 Preliminaries", "text": "In this paper, BBGP-based agents use rule-based systems3 as their basic reasoning model. The underlying logical language \u2013 denoted by L \u2013 consists of a set of literals4 in first-order logical language. We represent non-ground formulae with Greek letters (\u03d5, \u03c8, ...), variables with Roman letters (x, y, ...) and we name rules with r1, r2, .... Strict rules are of the form r = \u03d51, ..., \u03d5n \u2192 \u03c8, and defeasible rules are of the form r = \u03d51, ..., \u03d5n \u21d2 \u03c8. Thus, a theory is a triple T = \u3008F ,S,D\u3009 where F \u2286 L is a set of facts, S is a set of strict rules, and D is a set of defeasible rules. New information is produced from a given theory by applying the following concept, which was given in [6].\nDefinition 1 (Derivation schema) Let T = \u3008F ,S,D\u3009 be a theory and \u03c8 \u2208 L. A derivation schema for \u03c8 from T is a finite sequence T = {(\u03d51, r1), ..., (\u03d5n, rn)} such that:\n\u2022 \u03d5n = \u03c8\n\u2022 for i = 1...n, \u03d5i \u2208 F and ri = \u2205, or ri \u2208 S \u222aD\nBased on a derivation scheme T we also define the following sets: SEQ(T ) = {\u03d51, ..., \u03d5n}, FACTS(T ) = {\u03d5i|i \u2208 {1, ..., n}, ri = \u2205}, STRICT(T ) = {ri | i \u2208 {1, ..., n}, ri \u2208 S}, DEFE(T ) = {ri | i \u2208 {1, ..., n}, ri \u2208 D}.\n2Hereafter, these notations are used for differentiate the stages and the statuses of goals. 3These systems distinguish between facts, strict rules, and defeasible rules. A strict rule encodes strict information that has no\nexception, whereas a defeasible rule expresses general information that may have exceptions. 4Literals are defined as positive or negative atoms where an atom is an n-ary predicate."}, {"heading": "3 The Building Blocks", "text": "In this section, we present the main structures necessary for the generation of partial and complete explanations.\nFrom L, we distinguish the following finite sets: (i) F is the set of facts of the agent and (ii) G is the set of goals of the agent. F and G are subsets of ground literals from the language L and are pairwise disjoint. Besides, G = Gac \u222a Gpu \u222a Gch \u222a Gex, where Gac (resp. Gpu, Gch, Gex) stands for the set of active (resp. pursuable, chosen, executive) goals. It holds that Gx \u2229 Gy = \u2205, for x, y \u2208 {ac, pu, ch, ex} with x 6= y.\nOther important structures are the rules, which express the relation between the beliefs and the goals. The rules can be classified in standard and non-standard rules (activation, evaluation, deliberation, and checking rules). The former are made up of beliefs in both their premises and their conclusions and the latter are made up of beliefs in their premises and goals or beliefs about goals in their conclusions. Both standard and non-standard rules can be strict or defeasible rules. Standard rules can be used in any of the stages of the goal processing whereas non-standard rules are distinct for each stage. Thus, we have: - Standard rules (rst): \u2227 \u03d5i \u2192 \u03c6 (or \u2227 \u03d5i \u21d2 \u03c6). - Activation rules (rac): \u2227 \u03d5i \u2192 \u03c8 (or \u2227 \u03d5i \u21d2 \u03c8). - Evaluation rules (rev): \u2227 \u03d5i \u2192 \u00ac\u03c8 (or \u2227 \u03d5i \u21d2 \u00ac\u03c8). - Deliberation rules: r1de : \u00achas_incompatibility(g)\u2192 chosen(g) r2de : most_valuable(g) \u2192 chosen(g) - Checking rule: rck = has_plans_for(g)\u2227 satisfied_context_for(g) \u2192 executive(g)\nWhere \u03d5i and \u03c8 denote non-ground literals that represent beliefs and goals, respectively. g denote ground literals that represent a goal5. Notice that standard, activation, and evaluation rules are designed and entered by the programmer of the agent, and their content is dependent on the application domain. Otherwise, rules in deliberation and checking stages are pre-defined and no new rules of these types can be defined by the user. Finally, let Rst,Rac,Rev ,Rde, and Rck denote the set of standard, activation, evaluation, deliberation, and checking rules, respectively. Next, we define the theory of a BBGP-based agent.\nDefinition 2 (BBGP-based Agent Theory) A theory is a triple T = \u3008F ,S,D\u3009 such that: (i) F is the set of beliefs of the agent, (ii) S = R\u2032st\u222aR \u2032 ac\u222aR \u2032 ev \u222aR \u2032 de\u222aR \u2032 ck is the set of strict rules, and (ii) D = R \u2032\u2032 st\u222aR \u2032\u2032 ac\u222aR \u2032\u2032 ev \u222aR \u2032\u2032 de\u222aR \u2032\u2032 ck is the set of defeasible rules, where Rx = R\u2032x \u222aR \u2032\u2032 x (for x \u2208 {st, ac, ev, de, ck}). It holds that R \u2032 x \u2229R \u2032\u2032 x = \u2205.\nFrom a theory, a BBGP-based agent can build arguments. There are two categories of arguments. The first one \u2013 called epistemic arguments \u2013 justifies or attacks beliefs, while the other one \u2013 called stage arguments \u2013 justifies or attacks the passage of a goal from one stage to another. There is a set of arguments for each stage of the BBGP model.\nDefinition 3 (Arguments) Let T = \u3008F ,S,D\u3009 be a BBGP-based agent theory and T \u2032 = \u3008F ,R\u2032st,R \u2032\u2032 st\u3009 and T \u2032\u2032 = \u3008F ,S \u2032,D\u2032\u3009 be two sub-theories of T , where S \u2032 = S \\R\u2032st and D \u2032 = D \\R\u2032\u2032st. An epistemic argument constructed from T \u2032 is a pair A = \u3008T, \u03d5\u3009 such that:\n(1) \u03d5 \u2208 L (2) T is a derivation schema for \u03d5 from T \u2032\nOn the other hand, a stage argument constructed from T \u2032\u2032 is a pair A = \u3008T, g\u3009 such that:\n(1) g \u2208 G (2) For the activation and evaluation stages: T is a derivation schema for g from T \u2032\u2032. For the deliberation stage: T is a derivation schema for chosen(g) from T \u2032\u2032. For the checking stage: T is a derivation schema for executive(g) from T \u2032\u2032.\nFor both kinds of arguments, it holds that SEQ(T ) is consistent6 and T must be minimal7. Finally, ARGep, ARGac, ARGev , ARGde, and ARGck denote the set of all epistemic, activation, evaluation, deliberation, and checking arguments, respectively. As for notation, CLAIM(A) = \u03d5 (or g) and SUPPORT(A) = T denote the conclusion and the support of an argument A, respectively.\nAn argument may have a set of sub-arguments. Thus, an argument \u3008T \u2032, \u03d5\u2032\u3009 is a sub-argument of \u3008T, \u03d5\u3009 iff FACTS(T \u2032) \u2286 FACTS(T ), STRICT(T \u2032) \u2286 STRICT(T ), and DEFE(T \u2032) \u2286 DEFE(T ). SUB(A) denotes the set of all sub-arguments of A.\n5In any of the states of the goal processing, a goal is represented by a ground atom. However, before a goal becomes active, it has the form of a non-ground atom; in this case, we call it a sleeping goal. Thus, \u03c8 is a sleeping goal and g a goal in some status.\n6A set L\u2032 \u2286 L is consistent iff \u2204\u03d5,\u03d5\u2032 \u2208 L\u2032 such that \u03d5 = \u00ac\u03d5\u2032. It is inconsistent otherwise. 7Minimal means that there is no T \u2032 \u2282 T such that \u03d5 (g, chosen(g), or executive(g)) is a derivation schema of T \u2032.\nStage arguments built from T constitute a cause for a goal changes its status. However, it is not a proof that the the goal should adopt another status. The reason is that an argument can be attacked by other arguments. Two kinds of attacks are distinguished: (i) the attacks between epistemic arguments, and (ii) the mixed attacks, in which an epistemic argument attacks a stage argument. The former is defined over ARGep and is captured by the binary relation attep \u2286 ARGep \u00d7 ARGep. The latter is defined over ARGep and ARGx (for x \u2208 {ac, ev, de, ck}); and is captured by the binary relation attmx \u2286 ARGep \u00d7 ARGx. For both kinds of attacks, (A,B) \u2208 attmx (or (A,B) \u2208 attep) denotes that there is an attack relation between arguments A and B. Next definition captures both kinds of attacks; thus, rebuttal may occur only between epistemic arguments and undercut may occur in both kinds of attacks.\nDefinition 4 (Attacks) Let \u3008T \u2032, \u03d5\u2032\u3009 and \u3008T, \u03d5\u3009 be two epistemic arguments, and \u3008T, g\u3009 be a stage argument. \u3008T \u2032, \u03d5\u2032\u3009 rebuts \u3008T, \u03d5\u3009 if \u03d5 = \u00ac\u03d5\u2032. \u3008T, \u03d5\u3009 undercuts \u3008T \u2032,\u00ac\u03d5\u2032\u3009 (or \u3008T, g\u3009) if \u03d5\u2032 \u2208 FACTS(T ).\nFrom epistemic and stage arguments and the attacks between them, it is generated a different Argumentation Framework (AF) for each stage of the BBGP model.\nDefinition 5 (Argumentation Framework) An argumentation framework AFx is a pair AFx = \u3008ARG, att\u3009 (x \u2208 {ac, ev, de, ck}) such that:\n\u2022 ARG = ARGx \u222a ARG\u2032ep \u222a SUBARGS , where ARGx is a set of stage arguments, ARG \u2032 ep = {A | A \u2208 ARGep and (A,B) \u2208 attmx or (A,C) \u2208 attep}, where B \u2208 ARGx and C \u2208 ARG\u2032ep, and SUBARGS =\u22c3\nA\u2208ARGx,A\u2208ARG\u2032ep SUB(A).\n\u2022 att = att\u2032ep \u222a att \u2032 mx, where att \u2032 ep \u2286 ARG \u2032 ep \u00d7 ARG \u2032 ep and att \u2032 mx \u2286 ARG \u2032 ep \u00d7 ARGx.\nThe next step is to evaluate the arguments that make part of the AF. This evaluation is important because it determines which goals pass (acceptable goals) from one stage to the next. The aim is to obtain a subset of ARG without conflicting arguments. In order to obtain it, we use an acceptability semantics, which return one or more sets \u2013 called extensions \u2013 of acceptable arguments. The fact that a stage argument belong to an extension determines the change of status of the goal in its claim. Next, the main semantics introduced by Dung [3] are recalled8.\nDefinition 6 (Semantics) Let AFx = \u3008ARG, att\u3009 be an AF (with x \u2208 {ac, ev, de, ck}) and E \u2286 ARG:\n- E is conflict-free if \u2200A,B \u2208 E , (A,B) /\u2208 att or (B,A) /\u2208 att - E defends A iff \u2200B \u2208 ARG, if (B,A) \u2208 att, then \u2203C \u2208 E s.t. (C,B) \u2208 att. - E is admissible iff it is conflict-free and defends all its elements. - A conflict-free E is a complete extension iff we have E = {A|E defends A}. - E is a preferred extension iff it is a maximal (w.r.t the set inclusion) complete extension. - E is a grounded extension iff is a minimal (w.r.t. set inclusion) complete extension. - E is a stable extension iff E is conflict-free and \u2200A \u2208 ARG, \u2203B \u2208 E such that (B,A) \u2208 att."}, {"heading": "4 Partial and Complete Explanations", "text": "In order to able to generate partial and complete explanations, a BBGP-based agent needs to store information about the progress of his goals, that is, the changes of the statuses of such goals and the causes of these changes. The latter are stored in each AF in form of accepted arguments; however, the former cannot be stored in an AF. Thus, we need a structure to saves both the status of each goal and the AF that supports this status. Considering that at each stage, the agent generates arguments and attacks for more than one goal \u2013 which are stored in each AFx \u2013 and we only need those arguments and attacks related to one goal, we have to extract from AFx such arguments and attacks. In other words, we need to obtain a sub-AF.\nDefinition 7 (Sub-AF) Let AFx = \u3008ARG, att\u3009 (with x \u2208 {ac, ev, de, ck}) be the an AF and g \u2208 G a goal. An AF AF \u2032x = \u3008ARG \u2032, att\u2032\u3009 is a sub-AF of AFx (denoted AF \u2032x \u2291 AFx), if ARG \u2032 \u2286 ARG and att\u2032 = att\u2297 ARG\u2032, s.t.:\n- ARG\u2032 = {{A|A \u2208 ARGx, CLAIM(A) = g} \u222a {B|(B,A) \u2208 attmx or (B,C) \u2208 att\u2032ep}, where B \u2208 ARGep, C \u2208 ARG\u2032ep, att \u2032 ep \u2282 att \u2032, and ARG\u2032ep \u2282 ARG\n\u2032}}, and - att\u2297 ARG\u2032 returns a subset of att that involves just the arguments in ARG\u2032.\n8It is not the scope of this article to study the most adequate semantics for goal processing or the way to select an extension when more than one is returned by a semantics. For a brief study of these issues, the reader is referred to [5].\nNext, we define the structure that stores the causes of the change of the status of a goal, which must be updated after a new change occurs. We can see this structure as a table record, where each row saves the status of a goal along with the AF that supports such status.\nDefinition 8 (Goal Memory) Let AFx = \u3008ARG, att\u3009 be an AF (with x \u2208 {ac, ev, de, ck}), AF \u2032x \u2291 AFx a sub-AF, and g \u2208 G a goal. The goal memory GMg for goal g is a set of ordered pairs (STA, REASON) such that:\n\u2022 STA \u2208 {ac, pu, ch, ex,not ac, not pu, not ch, not ex} where {ac, pu, ch, ex} represent the status g attains due to the arguments in REASON whereas {not ac, not pu, not ch, not ex} represent the status g cannot attain due to the arguments in REASON.\n\u2022 REASON = AF \u2032x \u2291 AFx is a sub-AF whose selected extension supports the current status of g.\nLet GM+ be the set of all goal memories and NUM_REC : GM+ \u2192 N a function that returns the number of records of a given GM. From the goal memory structure, the partial and complete explanation can be generated.\nDefinition 9 (Partial and Complete Explanations) Let g \u2208 G be a goal, GMg the memory of g, and AFac, AFev , AFde, and AFck the four argumentation frameworks involved in the goal processing. Besides, let x \u2208 {ac, pu, ch, ex} denote the current status of g:\n\u2022 A complete explanation CEg for g \u2208 Gx is obtained as follows: CEg = \u22c3i=NUM_REC(GMg)\ni=1 REASONi, where REASONi \u2291 AFac, REASONi \u2291 AFev , REASONi \u2291 AFde, and REASONi \u2291 AFck.\n\u2022 A partial explanation PEg is obtained as follows: PEg = \u22c3i=NUM_REC(GMg)\ni=1 Ei, where Ei is the selected extension obtained from REASONi."}, {"heading": "5 Application: Rescue robots scenario", "text": "In this section, we present the application of the proposed approach to the rescue robots scenario.\nFirstly, let us present the mental states of the robot agent, let us call him BOB:\nGsl = {g1sl, g 2 sl, g 3 sl}, G = {}, which means that Gac = {}, Gpu = {}, Gch = {}, and Gex = {} Rst = {r1st, r 2 st, r 3 st, r 4 st}, Rac = {r 1 ac, r 2 ac, r 3 ac}, Rev = {r1ev, r 2 ev}, Rde = {r 1 de, r 2 de}, and Rck = {rck} F = {b1, b2, b3, b4, b5,\u00acb6, b8, b9, b10, b11, b12, b13}\nThe detail of each set is presented below:\nSleeping goals - g1s = take_hospital(x) //take a person x to the hospital - g2s = go(x, y) //go to zone (x,y) - g3s = send_shelter(x) //send a person x to the shelter Standard Rules - r1st = new_supply(x) \u21d2 available(x) //if there is a new supply x, then x is available - r2st = has_fract_bone(x) \u21d2 injured_severe(x) //if x has a fractured bone, then x is severely injured - r3st = fract_bone(x, arm) \u21d2 \u00acinjured_severe(x) //if the fractured bone is in the arm, then x is not severely injured - r4st = open_fracture(x) \u2192 injured_severe(x) //if x has an open fracture, then x is severely injured Activation rules - r1ac = injured_severe(x) \u21d2 take_hospital(x) //if person x is severely injured, then take x to the hospital - r2ac = \u00acinjured_severe(x) \u21d2 send_shelter(x) //if person x is not severely injured, then send x to the shelter - r3ac = asked_for_help(x, y) \u21d2 go(x, y) //if BOB is asked for help in zone (x, y), then go to that zone Evaluation rules - r1ev = greater(weight(x), 80) \u2192 \u00actake_hospital(x) //If person x weights more than 80 kilos, then it is not possible to take him/her to the hospital - r2ev = \u00acavailable(bed, x) \u21d2 \u00actake_hospital(x) //If there is no available bed for x, then it is not possible to take x to hospital\nBeliefs - b1 = be_operative(me) - b2 = has_fract_bone(man_32) //There is a 32-year-old man with a fractured bone - b3 = fract_bone(man_32, arm) //The 32-year-old man has a fractured arm. - b4 = asked_for_help(2, 6) //There is an aid request in slot (2,6). - b5 = open_fracture(man_32) //The 32-year-old man has an open fracture. - \u00acb6 = \u00acavailable(bed,man_32) //There is no an available bed. - b8 = new_supply(bed) //There is a new supply. - b9 = weight(man_32, 70) //man_32 weights 70 kg. - b10 = has_plans_for(\n\u2032take_hospital(man_32)\u2032) - b11 = has_plans_for(\n\u2032go(2, 6)\u2032) - b12 = most_valuable_goal(\n\u2032take_hospital(man_32)\u2032) - b13 = satisfied_context_for( \u2032take_hospital(man_32)\u2032)\nThus, the theory of agent BOB is: T = \u3008F ,S,D\u3009 where F = {b1, b2, b3, b4, b5,\u00acb6, b8, b9,b10, b11, b12, b13}, S = {r1ev, r 1 de, r 2 de, rck, r 4 st,}, and D = {r 1 st, r 1 ac, r 2 ac, r 2 ev, r 2 st, r 3 st, r 3 ac}."}, {"heading": "5.1 Generated Argumentation Frameworks", "text": "Based on the mental state of agent BOB, the arguments and AFs for each stage can be generated. For this application, we will use preferred semantics for calculating the extensions.\nFor the activation stage seven epistemic arguments and four activation arguments can be built: A2ep = \u3008{(b2, \u2205)}, b2\u3009, A 3 ep = \u3008{(b3, \u2205)}, b3\u3009 A4ep = \u3008{(b4, \u2205)}, b4\u3009, A 5 ep = \u3008{(b5, \u2205)}, b5\u3009 A7ep = \u3008{(b2, \u2205), (b7, r 2 st)}, b7\u3009,A 8 ep = \u3008{(b3, \u2205), (\u00acb7, r 3 st)},\u00acb7\u3009 A9ep = \u3008{(b5, \u2205), (b7, r 4 st)}, b7\u3009,A 1 ac = \u3008{(b4, \u2205), (g1, r 3 ac)}, g1\u3009 A2ac = \u3008{(b2, \u2205), (b7, r 2 st),, A 3 ac = \u3008{(b5, \u2205), (b7, r 4 st), \u3009 A4ac = \u3008{(b3, \u2205), (\u00acb7, r 3 st), \u3009\nThe AF for this stage is: AFac = \u3008{A1ep,A 2 ep,A 3 ep,A 4 ep,A 5 ep,A 7 ep,A 1 ac,A 2 ac,A 3 ac,A 4 ac},{(A 7 ep,A 8 ep), (A 8 ep,A 7 ep), (A 8 ep,A 9 ep), (A9ep,A 8 ep), (A 7 ep,A 4 ac), (A 8 ep,A 3 ac), (A 8 ep,A 2 ac), (A 9 ep,A 4 ac)}\u3009. The next step is to evaluate the acceptability of the arguments. We first apply the preferred semantics to AFac, the result is: E = {A2ep,A 3 ep,A 4 ep,A 5 ep,A 7 ep,A 9 ep,A 1 ac,A 2 ac,A 3 ac}. Therefore, we have that the set of justified conclusions is: {b2, b3, b4, b5, b7, g1, g2}. Thus, the set of justified goals is {g1, g2}. This means that robot agent BOB activates goals g1 = go(2, 6) and g2 = take_hospital(man_32) but he does not activate goal g3 = send_shelter(man_32). Therefore, we have that Gac = {g1, g2},Gpu = {},Gch = {}, and Gex = {}.\nFor the evaluation stage, the epistemic and evaluation arguments that can be built from the mental state of BOB are: A6ep = \u3008{(\u00acb6, \u2205)},\u00acb6\u3009, A 10 ep = \u3008{(b8, \u2205)}, b8\u3009 A11ep = \u3008{(b8, \u2205), (b6, r 1 st)}, b6\u3009 A1ev = \u3008{(\u00acb6, \u2205), (\u00acg2, r 2 ev)},\u00acg2\u3009\nThe AF for this stage is AFev = \u3008{A6ep,A 10 ep ,A 11 ep ,A 1 ev}, {(A 6 ep,A 11 ep), (A 11 ep ,A 6 ep),(A 11 ep ,A 1 ev)}\u3009. We have two preferred extensions for AFev: E = {{A10ep,A 11 ep}, {A 6 ep,A 10 ep ,A 1 ev}}. Since the second preferred extension refrains a goal of becoming pursuable, the agent chooses the first preferred extension. Since there is no evaluation argument that belongs to the selected extension, we can say that the passage of both currently active goals is justified. Therefore, both g1 = go(2, 6) and g2 = take_hospital(man_32) are now pursuable goals.\nRegarding deliberation stage, the arguments generated for this stage are: A12ep = \u3008{(b12, \u2205)}, b12\u3009 A1de = \u3008{(b12, \u2205), (chosen(g2), r 2 de)}, g2\u3009\nThe AF for this stage is AFde = \u3008{A12ep ,A 1 de}, {}\u3009. In this case, there is no attacks between the arguments and we have only one preferred extension for AFde: E = {A12ep ,A 1 de}. We have that a deliberation argument belongs to the extension, so we can say that the passage of goal g2 is justified. Therefore, g2 = take_hospital(man_32) is now a chosen goal.\nFinally, for the checking stage, the generated arguments are: A13ep = \u3008{(b10, \u2205)}, b10\u3009, A 11 ep = \u3008{(b13, \u2205)}, b13\u3009 A1ch = \u3008{(b10, \u2205), (b13, \u2205),(executive(g2), rch)}, g2\u3009\nThe AF for this stage is AFck = \u3008{A13ep ,A 14 ep ,A 1 ch}, {}\u3009. There is only one preferred extension for AFck: E = {A13ep ,A 14 ep ,A 1 ch}. We have that a checking argument belongs to the extension, so we can say that the passage of goal g2 is justified. Therefore, g2 = take_hospital(man_32) is now an executive goal.\nAt last, we present the final configuration of G: Gac = {}, Gpu = {g1},Gch = {}, and Gex = {g2}."}, {"heading": "5.2 Partial and Complete Explanations", "text": "First of all, let us present the goal memories of goals g1, g2, and g3. Next table shows the sub-AFs that allow goal g1 to become pursuable, it also shows it cannot become chosen because there is no deliberation argument that supports its change of status.\nSTA REASON\nac AF g1ac = \u3008{A 1 ac,A 4 ac}, {}\u3009\npu AF g1ev = \u3008{}, {}\u3009 not ch AF g1de = \u3008{}, {}\u3009\nIn the following table, we have all the sub-AFs that allow goal g2 to become executive. Notice that g1 becomes pursuable because no evaluation argument refrains its passage to the deliberation stage; on the contrary, in sub-AF AF g2ev there is an evaluation argument against g2, which is attacked by an epistemic argument. Thus, g2 becomes pursuable due to the defence of one of the epistemic arguments of the sub-AF.\nSTA REASON\nac AF g2ac = \u3008{A 2 ac,A 3 ac,A 7 ep,A 2 ep,A 3 ep,A 9 ep,A 5 ep,A 8 ep},\n{(A8ep,A 2 ac), (A 7 ep,A 8 ep), (A 8 ep,A 7 ep), (A 9 ep,A 8 ep)}\u3009\npu AF g2ev = \u3008{A 1 ev,A 11 ep ,A 6 ep}, {(A 11 ep ,A 1 ev), (A 11 ep ,A 6 ep), (A 6 ep,A 11 ep)}\u3009 ch AF g2de = \u3008{A 1 de,A 12 ep}, {}\u3009 ex AF g2ck = \u3008{A 1 ck,A 13 ep ,A 11 ep}, {}\u3009\nFinally, next table shows that goal g3 cannot become active. Even though there is an activation argument, it is attacked by two epistemic arguments. Thus, after applying the semantics the activation argument is not part of the preferred extension.\nSTA REASON\nnot ac AF g3ac = \u3008{A 4 ac,A 8 ap,A 2 ap,A 3 ap,A 5 ap,A 7 ap,A 9 ap},\n{(A7ap,A 4 ac), (A 9 ap,A 4 ac), (A 8 ap,A 7 ap), (A 7 ap,A 8 ap), (A9ap,A 8 ap)}\u3009\nFor the sake of simplicity, suppose that rescue robots can communicate with humans by means of natural language. Now, suppose that at the end of a rescue day, BOB is interrogated with the following question: Why have you taken to the hospital man_32 instead of sending him to the shelter? BOB can give a partial explanation or a complete explanation. Next, we show both of them:\nPARTIAL EXPLANATION\nBOB only uses the arguments that are part of the selected preferred extension. Thus, he answers with: PEg2 = {A2ep,A 3 ep,A 5 ep,A 7 ep,A 9 ep,A 2 ac,A 3 ac}. In natural language he would give the following answer: man_32 had a fractured bone (A2ep), the fractured bone was of his arm (A 3 ep), and it was an open fracture (A 5 ep); therefore, he was severely injured (A7ep,A 9 ep). Since he was severely injured I took him to the hospital (A 2 ac,A 3 ac).\nBOB can also use the preferred extension of AF g3ac . In this case, he gives the reasons for not sending man_32 to the shelter. Thus, this he answers with: PEg3 = {A 2 ep,A 3 ep,A 5 ep,A 7 ep,A 9 ep,A 4 ac}. In natural language he would give the\nfollowing answer: man_32 had a fractured bone (A2ep), the fractured bone was of his arm (A 3 ep), and it was an open fracture (A5ep). A fractured bone might be considered a severe injury (A 7 ep), but since it it was an open fracture it was indeed a severe injury (A9ep).\nNotice that this explanation does not clarify completely the reasons for not sending the man to the shelter.\nCOMPLETE EXPLANATION\nIn this case, BOB uses the sub-AFs of his individual memory records. Thus, he answers CEg2 = AF g2 ac = \u3008{A2ep,A 5 ep,A 7 ep,A 8 ep,A 9 ep,A 2 ac,A 3 ac}, (A 8 ep,A 2 ac), {(A 7 ep,A 8 ep), (A 8 ep,A 7 ep), (A 9 ep,A 8 ep)}\u3009 to justify why he decided to take man_32 to the hospital. In natural language, this would be the answer: man_32 had a fractured bone (A2ep), the fractured bone was of his arm (A3ep), and it was an open fracture (A 5 ep). Given that he had a fractured bone, he might be considered severe injured (A7ep); however, since such fracture was of his arm, it might not be considered a severe injure (A8ep). Finally, I noted that it was an open fracture, which determines \u2013 without exception \u2013 that it was a severe injury (A9ep). For these reasons I took him to the hospital (A 2 ac,A 3 ac).\nThe answer above answers only half the question. Let\u2019s see now the complete reason for not sending him to the shelter, which is indeed a complement of the above answer. Thus, he uses the sub-AF related to goal g3: CEg3 = AF g3 ac = \u3008{A2ap,A 3 ap,A 5 ap,A 7 ap,A 8 ap,A 9 ap,A 4 ac}, {(A 7 ap,A 4 ac), (A 9 ap,A 4 ac), (A 8 ap,A 7 ap), (A 7 ap,A 8 ap), (A 9 ap,A 8 ap)}\u3009. In natural language, this would be the answer: man_32 had a fractured bone (A2ep) and the fractured bone was of his arm (A 3 ep). Given that he had a fractured bone, he might be considered severely injured (A7ep); however, since such fracture was of his arm, it might not be considered a severe injury (A8ep). Since the injury is not severe, the man might be sent to the shelter; however, I noted that it was an open fracture, which determines \u2013 without exception \u2013 that it was a severe injury (A9ep). This last reason refutes the action of sending him to the shelter.\nNote that the complete explanation is more accurate, especially when the agent has to clarify why he did not send the man to the shelter. Note also that both complete explanations are complementary. We can say that depending on the question, the agent can use part of the entire memory goal. The agent may also use more than one memory goal in order to give satisfactory answers."}, {"heading": "6 Related Work", "text": "Since XAI is a recently emerged domain in Artificial Intelligence, there are few reviews about the works in this area. In [7], Anjomshoae et al. make a Systematic Literature Review about goal-driven XAI, i.e., explainable agency for robots and agents. According to them, some papers propose conceptual studies and there is a lack of evaluations; almost all of the papers deal with agents that explain their behaviors to human users, and very few works tackle interagent explainability. One interesting research question was about Design, that is, the platforms and architectures that have been used to design Explainable Agency. Their results show that 22% of the platforms and architectures have not explicitly indicate their method for generating explanations, 18% of papers relied on ad-hoc methods, 9% implemented their explanations in BDI architecture. At last, other platforms and architectures used to extract explanations are: Markov Decision Process (3%), Neural Networks (3%), Partially Observable Markov Decision Process (3%), Parallelrooted-ordered Slip-stack Hierarchical Action Selection (2%), and STRIPS (2%).\nSome works relied on the BDI model are the following. In [8] and [9], Broekens et al. and Harbers et al., respectively, focus on generating explanations for humans about their goals and actions. They construct a tree with beliefs, goals, and actions, from which they explanations are constructed. Unlike our proposal, their explanations do not detail the progress of the goals and are not complete in the sense that do not express why an agent did not commit to a given goal. Langley et al. [10] focus on settings in which an agent receives instructions, performs them, and then describes and explains its decisions and actions afterwards.\nFinally, Sassoon et al. [11] propose an approach of explainable argumentation based on argumentation schemes and argumentation-based dialogues. In this approach, an agent provides explanations to patients (human users) about their treatments. In this case, argumentation is applied in a different way than in our proposal and with other focus, they generate explanations for information seeking and persuasion."}, {"heading": "7 Conclusions and Future Work", "text": "This work presented an approach for explainable agency based on argumentation theory. The chosen architecture was the BBGP model, which can be considered an extension of the BDI model. Our objective was that BBGP-based agents\nbe able to explain their decision about the statuses of their goals, especially those goals they committed to. In order to achieve our objectives, we equipped BBGP-based agents with a structure and a mechanism to generate both partial and complete explanations. Thus, BBGP-agents not only are able to explain why their goals change their statuses but also why a goal (or goals) did not progress in the intention formation process.\nIn the formalization of the BBGP model proposed in [5], the authors also include the status cancelled. As a future work, we intend that BBGP-based agents also generate explanations for this status. This was not done in this article because this generation goes beyond the AFs built at each stage of the intention formation process. A goal can also go back in the intention formation process. This was not taken into account and it is an interesting future research. Finally, we want to deal with more complex questions, which require more elaborate and adequate explanations. As we saw in the example, better explanations include elements of more than one AF. In this sense, a \u201cgood\u201d explanation may include elements from different AFs."}, {"heading": "Acknowledgment", "text": "This work is fully founded by CAPES (Coordena\u00e7\u00e3o de Aperfei\u00e7oamento de Pessoal de N\u00edvel Superior)."}], "year": 2020}