{
  "abstractText": "A key promise of AI applications in healthcare is in increasing access to quality medical care in under-served populations and emerging markets. However, deep learning models are often only trained on data from advantaged populations that have the infrastructure and resources required for large-scale data collection. In this paper, we aim to empirically investigate the potential impact of such biases on breast cancer detection in mammograms. We specifically explore how a deep learning algorithm trained on screening mammograms from the US and UK generalizes to mammograms collected at a hospital in China, where screening is not widely implemented. For the evaluation, we use a top-scoring model developed for the Digital Mammography DREAM Challenge. Despite the change in institution and population composition, we find that the model generalizes well, exhibiting similar performance to that achieved in the DREAM Challenge, even when controlling for tumor size. We also illustrate a simple but effective method for filtering predictions based on model variance, which can be particularly useful for deployment in new settings. While there are many components in developing a clinically effective system, these results represent a promising step towards increasing access to lifesaving screening mammography in populations where screening rates are currently low.",
  "authors": [
    {
      "affiliations": [],
      "name": "Kevin Wu"
    },
    {
      "affiliations": [],
      "name": "Eric Wu"
    },
    {
      "affiliations": [],
      "name": "Yaping Wu"
    },
    {
      "affiliations": [],
      "name": "Hongna Tan"
    },
    {
      "affiliations": [],
      "name": "Greg Sorensen"
    },
    {
      "affiliations": [],
      "name": "Meiyun Wang"
    },
    {
      "affiliations": [],
      "name": "Bill Lotter"
    }
  ],
  "id": "SP:07f8c7df7eb8edd0be8a6723691f2d84a2510f36",
  "references": [
    {
      "authors": [
        "R Edward Hendrick",
        "Jay A Baker",
        "Mark A Helvie"
      ],
      "title": "Breast cancer deaths averted over 3 decades",
      "venue": "Cancer, 125(9):1482\u20131488,",
      "year": 2019
    },
    {
      "authors": [
        "Lei Fan",
        "Kathrin Strasser-Weippl",
        "Jun-Jie Li",
        "Jessica St Louis",
        "Dianne M Finkelstein",
        "Ke-Da Yu",
        "Wan-Qing Chen",
        "Zhi-Ming Shao",
        "Paul E Goss"
      ],
      "title": "Breast cancer in china",
      "venue": "The lancet oncology,",
      "year": 2014
    },
    {
      "authors": [
        "Qing-Kun Song",
        "Xiao-Li Wang",
        "Xin-Na Zhou",
        "Hua-Bing Yang",
        "Yu-Chen Li",
        "Jiang-Ping Wu",
        "Jun Ren",
        "Herbert Kim Lyerly"
      ],
      "title": "Breast cancer challenges and screening in china: Lessons from current registry data and population screening studies",
      "venue": "The oncologist,",
      "year": 2015
    },
    {
      "authors": [
        "Adam Yala",
        "Tal Schuster",
        "Randy Miles",
        "Regina Barzilay",
        "Constance Lehman"
      ],
      "title": "A deep learning model to triage screening mammograms: a simulation study",
      "year": 2019
    },
    {
      "authors": [
        "Gustavo Carneiro",
        "Jacinto Nascimento",
        "Andrew P Bradley"
      ],
      "title": "Unregistered multiview mammogram analysis with pre-trained deep learning models",
      "venue": "In International Conference on Medical Image Computing and Computer-Assisted Intervention,",
      "year": 2015
    },
    {
      "authors": [
        "William Lotter",
        "Greg Sorensen",
        "David Cox"
      ],
      "title": "A multi-scale cnn and curriculum learning strategy for mammogram classification. In Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support",
      "year": 2017
    },
    {
      "authors": [
        "Dezs\u0151 Ribli",
        "Anna Horv\u00e1th",
        "Zsuzsa Unger",
        "P\u00e9ter Pollner",
        "Istv\u00e1n Csabai"
      ],
      "title": "Detecting and classifying lesions in mammograms with deep learning",
      "venue": "Scientific reports,",
      "year": 2018
    },
    {
      "authors": [
        "Stephen Morrell",
        "Zbigniew Wojna",
        "Can Son Khoo",
        "Sebastien Ourselin",
        "Juan Eugenio Iglesias"
      ],
      "title": "Largescale mammography cad with deformable conv-nets. In Image Analysis for Moving Organ, Breast, and Thoracic Images",
      "year": 2018
    },
    {
      "authors": [
        "Thijs Kooi",
        "Geert Litjens",
        "Bram Van Ginneken",
        "Albert Gubern-M\u00e9rida",
        "Clara I S\u00e1nchez",
        "Ritse Mann",
        "Ard den Heeten",
        "Nico Karssemeijer"
      ],
      "title": "Large scale deep learning for computer aided detection of mammographic lesions",
      "venue": "Medical image analysis,",
      "year": 2017
    },
    {
      "authors": [
        "Nan Wu",
        "Jason Phang",
        "Jungkyu Park",
        "Yiqiu Shen",
        "Zhe Huang",
        "Masha Zorin",
        "Stanis\u0142aw Jastrz\u0119bski",
        "Thibault F\u00e9vry",
        "Joe Katsnelson",
        "Eric Kim",
        "Stacey Wolfson",
        "Ujas Parikh",
        "Sushma Gaddam",
        "Leng Leng Young Lin",
        "Kara Ho",
        "Joshua D. Weinstein",
        "Beatriu Reig",
        "Yiming Gao",
        "Hildegard Toth",
        "Kristine Pysarenko",
        "Alana Lewin",
        "Jiyon Lee",
        "Krystal Airola",
        "Eralda Mema",
        "Stephanie Chung",
        "Esther Hwang",
        "Naziya Samreen",
        "S. Gene Kim",
        "Laura Heacock",
        "Linda Moy",
        "Kyunghyun Cho",
        "Krzysztof J. Geras"
      ],
      "title": "Deep neural networks improve radiologists\u2019 performance in breast cancer screening",
      "venue": "IEEE Trans Med Imaging,",
      "year": 2019
    },
    {
      "authors": [
        "Dong Wook Kim",
        "Hye Young Jang",
        "Kyung Won Kim",
        "Youngbin Shin",
        "Seong Ho Park"
      ],
      "title": "Design characteristics of studies reporting the performance of artificial intelligence algorithms for diagnostic analysis of medical images: results from recently published papers",
      "venue": "Korean journal of radiology,",
      "year": 2019
    },
    {
      "authors": [
        "John R Zech",
        "Marcus A Badgeley",
        "Manway Liu",
        "Anthony B Costa",
        "Joseph J Titano",
        "Eric Karl Oermann"
      ],
      "title": "Variable generalization performance of a deep learning model to detect pneumonia in chest radiographs: A cross-sectional study",
      "venue": "PLoS medicine,",
      "year": 2018
    },
    {
      "authors": [
        "Ian Pan",
        "Saurabh Agarwal",
        "Derek Merck"
      ],
      "title": "Generalizable inter-institutional classification of abnormal chest radiographs using efficient convolutional neural networks",
      "venue": "Journal of digital imaging,",
      "year": 2019
    },
    {
      "authors": [
        "Jong-Myon Bae",
        "Eun Hee Kim"
      ],
      "title": "Breast density and risk of breast cancer in asian women: a meta-analysis of observational studies",
      "venue": "Journal of Preventive Medicine and Public Health,",
      "year": 2016
    },
    {
      "authors": [
        "Michael Heath",
        "Kevin Bowyer",
        "Daniel Kopans",
        "Richard Moore",
        "W Philip Kegelmeyer"
      ],
      "title": "The digital database for screening mammography",
      "venue": "In Proceedings of the 5th international workshop on digital mammography,",
      "year": 2000
    },
    {
      "authors": [
        "Andrew G Howard",
        "Menglong Zhu",
        "Bo Chen",
        "Dmitry Kalenichenko",
        "Weijun Wang",
        "Tobias Weyand",
        "Marco Andreetto",
        "Hartwig Adam"
      ],
      "title": "Mobilenets: Efficient convolutional neural networks for mobile vision applications",
      "venue": "arXiv preprint arXiv:1704.04861,",
      "year": 2017
    },
    {
      "authors": [
        "H Gilbert Welch",
        "Philip C Prorok",
        "A James O\u2019Malley",
        "Barnett S Kramer"
      ],
      "title": "Breast-cancer tumor size, overdiagnosis, and mammography screening effectiveness",
      "venue": "New England Journal of Medicine,",
      "year": 2016
    },
    {
      "authors": [
        "Murat Seckin Ayhan",
        "Philipp Berens"
      ],
      "title": "Test-time data augmentation for estimation of heteroscedastic aleatoric uncertainty in deep neural networks",
      "venue": "1st Conference on Medical Imaging with Deep Learning,",
      "year": 2018
    },
    {
      "authors": [
        "Adam Yala"
      ],
      "title": "Oncoserve. https://github.com/yala/OncoServe_public, 2019",
      "year": 2019
    }
  ],
  "sections": [
    {
      "heading": "Introduction",
      "text": "In the United States, screening mammography has been attributed as a major factor in averting hundreds of thousands of breast cancer deaths [1]. The prevalence of screening mammography, however, is often dramatically lower in many emerging markets, including China [2, 3]. Inadequate funding and a shortage of qualified readers are contributing factors to China\u2019s low screening rates [2]. AI could increase access to screening via its scalability and potential to provide low cost and accurate initial interpretation for many mammograms. For instance, a triage-style clinical implementation could significantly reduce overall workload for clinicians, while maintaining high diagnostic accuracy [4]. In this scenario, the majority of women with mammograms confidently interpreted as normal by the AI software could be recommended for subsequent routine mammography the following year, where the remainder of women would be referred for further clinician interpretation and work-up, thus lightening the clinical load.\nThere has indeed been much recent work illustrating the promise of AI in mammogram interpretation [5, 6, 7, 8, 9, 10]. However, it is often unclear how well these methods generalize, as they are usually evaluated on data drawn from the same distribution as the training set, i.e. similar populations and/or hospitals [11]. With goals of deploying such solutions in different markets, their performance on the intended population must be verified, especially as it is often infeasible to collect enough training data from these populations. Generalization is certainly not guaranteed given the high\nNeurIPS 2019. Fair ML for Health Workshop.\nar X\niv :1\n91 1.\n00 36\n4v 1\n[ ee\nss .I\nV ]\n1 N\nov 2\nsensitivity of deep learning models to input statistics [12, 13]. For mammography in particular, there are known (and possibly unknown) biological differences in mammograms between US and Chinese populations, such as a greater proportion of dense breasts in the Chinese population [14], which can make mammogram interpretation more difficult. Here, we tested how well a top-performing model from Digital Mammography DREAM Challenge [15], a large-scale data science competition that concluded in early 2018, generalizes to a collection of mammograms acquired at a Chinese hospital. Additionally, we explore the use of an empirical measure of prediction uncertainty, quantified as the variance over models and data augmentations in an ensemble, as a prediction deferment strategy, which could be useful in deployment in novel settings."
    },
    {
      "heading": "Methods",
      "text": ""
    },
    {
      "heading": "Model Architecture and Training Procedure",
      "text": "The tested algorithm was designed for the Digital Mammography DREAM Challenge [15], a unique competition where neither the training data nor the testing data (all from the United States) were available for download. Instead, competitors made submissions using Docker containers, which would have access to the data when run on the DREAM servers. Another challenge was that only breast-level labels were available for training; specifically there were only binary cancer/no-cancer labels for each patient and laterality (left/right), with no tumor localization information. Given the \u201cneedlein-a-haystack\u201d nature of detecting cancer in mammograms, training standard image classification approaches using only image-level labels quickly leads to overfitting. To more effectively train on the DREAM data, we developed a two-stage training scheme, which first consisted of training convolutional neural networks (CNNs) on cropped mammogram patches using publicly-available, strongly-labeled data; and then using these CNNs to initialize a fully-convolutional network that outputs a classification given a full-scale image [6] (Figure 1). By first training to predict the presence of a cancerous lesion in a 256x256px mammogram patch, the CNN is better initialized for end-to-end training on full-scale images, for which we resize to have a height of 1750px. We used the DDSM [16] and Optimam [17] datasets for patch pre-training. DDSM consists of 2,602 scanned film mammography studies from the US (35% cancers, 33% benigns, and 31% normals), while the version of Optimam used here contains 13,973 digital mammography studies from the UK (26% cancers, 2.5% benigns, and 72% normals).\nFor the backbone of our network, we used MobileNet [18], as its memory efficient structure enabled full-image training on the GPUs available in the DREAM Challenge. Converting the patch model to the full-image model consisted of using a global average pooling layer on top of the final convolutional feature map layer, followed by a single fully-connected layer. The overall training procedure consisted of 1) patch-level training on DDSM & Optimam, 2) image-level training on DDSM & Optimam, and 3) image-level training on the DREAM dataset. Due to the class imbalance between cancer/nocancer images, we sampled equally from each class during all three training stages. The final model consisted of an ensemble of three models trained in a similar fashion. Each model averages scores from vertically mirrored orientations of the same image, and an image-level score is computed as a weighted average between the three models. A study-level score is computed by averaging image-level scores across views (cranio-caudal (CC) and mediolateral oblique (MLO)), before taking the max score over lateralities (left and right). This submission achieved an area under the receiver operating characteristic curve (AUROC) of 0.90, which was the highest submission score over all phases of the Challenge."
    },
    {
      "heading": "Evaluation on Data from a Chinese Hospital",
      "text": "To test the generalization of the DREAM model, an evaluation dataset consisting of 2533 cases (533 pathologically-proven cancers, 1000 pathologically-proven benigns, and 1000 normals) was gathered from an urban hospital in China. Each case consisted of the four standard mammographic views (CC and MLO of both the left and right breast). The data was retrospectively collected from a contiguous period of time from 2012-2017. Given low screening rates, the data came from diagnostic exams, i.e. exams where the patient presented with symptoms, so the distribution of tumor sizes from the cancer cases contained more large tumors (64% larger than 2cm) than would be expected in a typical United States screening population [19]. Thus, we report results on the original data distribution, as well as a tumor-size normalized performance, using bootstrap re-sampling simulations to approximately\nmatch the tumor size distribution observed in US screening data [20]. All results were calculated by running the containerized DREAM submission locally at the Chinese hospital with no transfer of imaging data. As a measure of model uncertainty for a given image [21], we quantify the variance in prediction scores across the three models and two vertical orientations (6 total scores). An uncertainty score per breast is then calculated as the average in variances across views for that breast."
    },
    {
      "heading": "Results",
      "text": "On the original Chinese dataset, the DREAM model achieved 0.93\u00b1 0.01 AUROC for breast-level predictions, as illustrated in Figure 2. In terms of a simulated triage scenario, these results would translate to interpreting 60% of normal mammograms as such, while operating at 95% sensitivity. When controlling for tumor size using 1000 bootstrap simulations to match US screening statistics, the model achieved 0.90 \u00b1 0.03 AUROC. This level of performance is thus consistent with the 0.90 AUROC results from the DREAM Challenge. Estimating breast density using an open source tool [22], we observe an AUROC of 0.914 on dense breasts and 0.946 non-dense breasts, respectively. The difference of 0.03 AUROC is also consistent with results from the DREAM Challenge. Finally, as an additional performance measure, the model achieves an AUROC of 0.94 when only considering normal and malignant cases (i.e., excluding the benigns).\nAdditionally, we find that the variance between prediction scores in the model ensemble (3 models across 2 image orientations) provides a practical empirical method of filtering model predictions. The cases that exhibit high prediction variance seem to also be the cases that the algorithm tends to misclassify, as excluding cases based on this metric tends to increase AUROC (Fig. 3). In analyzing the proportion of each type of case (normal, benign, malignant) that is filtered as the percentage of filtered cases increases, we observe that there is a slight trend towards initially excluding benign cases (Fig. 3), though the relative proportion of each type remains relatively consistent up to excluding ~40% of cases."
    },
    {
      "heading": "Discussion",
      "text": "While the development of better performing models will always be a goal in AI research, validating that existing algorithms can perform equally well in new clinical settings is crucial to ensure that AI can effectively serve broad populations. In this study, we demonstrated that a model that was trained on data from the US and UK and achieved state-of-the-art performance on the Digital Mammography DREAM Challenge generalizes well to data collected at an urban Chinese hospital. While the testing data used in this study largely came from diagnostic exams, as screening rates are relatively low in China, the performance was comparable to the United States results even when sampling to approximately match tumor size statistics found in the US. We also note that, while the data was diagnostic, each case consisted of the four standard mammographic (screening) views. The multi-faceted data used to train our model, consisting of film and digital mammograms and three datasets in total, could be a factor in the demonstrated generalizability. It could also be the case that the variation in mammographic image statistics within a demographic population tends to be larger than the variability between demographics. Since the evaluation performed here is without additional training, it is also reasonable to expect performance improvements in a setting where new data is available for fine-tuning the model. In addition to demonstrating generalizability, we propose a method for the deferring of model predictions via an empirical uncertainty measure formulated as the variance of scores between models in an ensemble. This can be especially useful in novel settings, where cases exhibiting a level of uncertainty above a threshold can be reserved solely for physician interpretation. Overall, while further work is needed, these results are a promising initial step towards the deployment of a mammography AI system to a population where screening mammography is currently limited."
    }
  ],
  "title": "Validation of a deep learning mammography model in a population with low screening rates",
  "year": 2019
}
