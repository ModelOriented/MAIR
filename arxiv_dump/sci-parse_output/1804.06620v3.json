{
  "abstractText": "In recent years, a large amount of model-agnostic methods to improve the transparency, trustability, and interpretability of machine learning models have been developed. Based on a recent method for model-agnostic global feature importance, we introduce a local feature importance measure for individual observations and propose two visual tools: partial importance (PI) and individual conditional importance (ICI) plots which visualize how changes in a feature affect the model performance on average, as well as for individual observations. Our proposed methods are related to partial dependence (PD) and individual conditional expectation (ICE) plots, but visualize the expected (conditional) feature importance instead of the expected (conditional) prediction. Furthermore, we show that averaging ICI curves across observations yields a PI curve, and integrating the PI curve with respect to the distribution of the considered feature results in the global feature importance. Another contribution of our paper is the Shapley feature importance, which fairly distributes the overall performance of a model among the features according to the marginal contributions and which can be used to compare the feature importance across different models.",
  "authors": [
    {
      "affiliations": [],
      "name": "Giuseppe Casalicchio"
    },
    {
      "affiliations": [],
      "name": "Bernd Bischl"
    }
  ],
  "id": "SP:f4798a7dfc1a06469737ed52be124a1e3faf49f9",
  "references": [
    {
      "authors": [
        "B. Bischl",
        "O. Mersmann",
        "H. Trautmann",
        "C. Weihs"
      ],
      "title": "Resampling methods for meta-model validation with recommendations for evolutionary computation",
      "venue": "Evol. Comput",
      "year": 2012
    },
    {
      "authors": [
        "G. Casalicchio",
        "B. Bischl",
        "A.L. Boulesteix",
        "M. Schmid"
      ],
      "title": "The residual-based predictiveness curve: A visual tool to assess the performance of prediction models. Biometrics",
      "year": 2016
    },
    {
      "authors": [
        "G. Casalicchio",
        "J. Bossek",
        "M. Lang",
        "D. Kirchhoff",
        "P. Kerschke",
        "B. Hofner",
        "H. Seibold",
        "J. Vanschoren",
        "B. Bischl"
      ],
      "title": "OpenML: An R package to connect to the machine learning platform OpenML",
      "year": 2017
    },
    {
      "authors": [
        "S. Cohen",
        "G. Dror",
        "E. Ruppin"
      ],
      "title": "Feature selection via coalitional game theory",
      "venue": "Neural Comput",
      "year": 2007
    },
    {
      "authors": [
        "A. Datta",
        "S. Sen",
        "Y. Zick"
      ],
      "title": "Algorithmic transparency via quantitative input influence: Theory and experiments with learning systems",
      "venue": "Proceedings - 2016 IEEE Symposium on Security and Privacy,",
      "year": 2016
    },
    {
      "authors": [
        "A. Fisher",
        "C. Rudin",
        "F. Dominici"
      ],
      "title": "Model class reliance: Variable importance measures for any machine learning model class, from the \u201dRashomon\u201d perspective",
      "year": 2018
    },
    {
      "authors": [
        "J.H. Friedman"
      ],
      "title": "Greedy function approximation: A gradient boosting machine",
      "venue": "Annals of statistics pp",
      "year": 2001
    },
    {
      "authors": [
        "A. Goldstein",
        "A. Kapelner",
        "J. Bleich",
        "E. Pitkin"
      ],
      "title": "Peeking inside the black box: Visualizing statistical learning with plots of individual conditional expectation",
      "venue": "J. Comput. Graph. Stat. 24(1),",
      "year": 2015
    },
    {
      "authors": [
        "B. Gregorutti",
        "B. Michel",
        "P. Saint-Pierre"
      ],
      "title": "Correlation and variable importance in random forests",
      "venue": "Stat. Comput",
      "year": 2017
    },
    {
      "authors": [
        "M. Lang",
        "B. Bischl",
        "D. Surmann"
      ],
      "title": "batchtools: Tools for R to work on batch systems",
      "venue": "The Journal of Open Source Software",
      "year": 2017
    },
    {
      "authors": [
        "Z.C. Lipton"
      ],
      "title": "The mythos of model interpretability",
      "venue": "ICML WHI",
      "year": 2016
    },
    {
      "authors": [
        "S.M. Lundberg",
        "G.G. Erion",
        "S.I. Lee"
      ],
      "title": "Consistent individualized feature attribution for tree ensembles",
      "year": 2018
    },
    {
      "authors": [
        "S.M. Lundberg",
        "S.I. Lee"
      ],
      "title": "A unified approach to interpreting model predictions",
      "venue": "In: NIPS,",
      "year": 2017
    },
    {
      "authors": [
        "C. Molnar",
        "G. Casalicchio",
        "B. Bischl"
      ],
      "title": "iml: An R package for interpretable machine learning",
      "venue": "The Journal of Open Source Software",
      "year": 2018
    },
    {
      "authors": [
        "R.J. Serfling"
      ],
      "title": "Approximation Theorems of Mathematical Statistics, vol. 162",
      "year": 2009
    },
    {
      "authors": [
        "L.S. Shapley"
      ],
      "title": "A value for n-person games",
      "venue": "Contributions to the Theory of Games 2(28),",
      "year": 1953
    },
    {
      "authors": [
        "C. Strobl",
        "A.L. Boulesteix",
        "T. Kneib",
        "T. Augustin",
        "A. Zeileis"
      ],
      "title": "Conditional variable importance for random forests",
      "venue": "BMC Bioinf. 9,",
      "year": 2008
    },
    {
      "authors": [
        "E. \u0160trumbelj",
        "I. Kononenko",
        "S. Wrobel"
      ],
      "title": "An efficient explanation of individual classifications using game theory",
      "venue": "J. Mach. Learn. Res. 11(Jan),",
      "year": 2010
    },
    {
      "authors": [
        "E. \u0160trumbelj",
        "I. Kononenko"
      ],
      "title": "A general method for visualizing and explaining black-box regression models",
      "venue": "In: Int. Conf. on Adaptive and Natural Computing Algorithms",
      "year": 2011
    },
    {
      "authors": [
        "J. Vanschoren",
        "J.N. Van Rijn",
        "B. Bischl",
        "L. Torgo"
      ],
      "title": "OpenML: Networked science in machine learning",
      "venue": "ACM SIGKDD Explor. Newsl. 15(2),",
      "year": 2014
    }
  ],
  "sections": [
    {
      "text": "Keywords: Interpretable Machine Learning \u00b7 Explainable AI \u00b7 Feature Importance \u00b7 Variable Importance \u00b7 Feature Effect \u00b7 Partial Dependence."
    },
    {
      "heading": "1 Introduction and Related Work",
      "text": "Machine learning (ML) algorithms such as neural networks and support vector machines (SVM) are often considered to produce black box models because they do not provide any direct explanation for their predictions. However, these methods often outperform simple linear models or decision trees in predictive performance as they can model complex relationships in the data. Nevertheless, such simple models are still preferred in areas such as life sciences and social sciences due to their simplicity and interpretability [14]. Many researchers have therefore developed and implemented several model-agnostic interpretability tools, which quantify or visualize feature effects or feature importance [9, 11, 17].\nIn our context, the terms feature effect, feature contribution and feature attribution describe how or to what extent each feature contributes to the prediction\nar X\niv :1\n80 4.\n06 62\n0v 3\n[ st\nat .M\nL ]\n2 8\nD ec\n2 01\nof the model, either on a local or a global level. Methods for feature effects include partial dependence (PD) plots [10], individual conditional expectation (ICE) plots [11] and, more recently, SHAP values [15]. These methods visualize or quantify the relationship and contribution of each feature to the prediction of a model without requiring knowledge about the true values of the target variable. A method that measures feature effects based on the Shapley value [19] from coalitional game theory was first presented for classification in [21] and has been extended to regression and global analysis in [22]. Further developments, visualizations, and generalizations were introduced by [15, 16]. Similar work proposing a general notion of a quantity of interest for the characteristic function of the Shapley value and focusing on the joint and marginal contributions of feature sets was introduced by [8].\nIn biomedical research, for example, measuring the effects of biomedical markers w.r.t. model prediction is as essential as measuring their added value regarding model performance [4]. We use the term feature importance1 to describe how important the feature was for the predictive performance of the model, regardless of the shape (e.g., linear or nonlinear relationship) or direction of the feature effect. This implies that measures of feature importance require knowledge of the true values of the target variable. The most prominent approach is the permutation importance introduced by Breiman [3] for random forests. It computes the drop in out-of-bag performance after permuting the values of a feature. A model-agnostic global permutation-based feature importance (PFI) was recently introduced in [9].\nContributions: We review model-agnostic global PFI and propose an efficient approximation based on Monte-Carlo integration. We then introduce a local version of the global PFI, which measures the feature importance of individual observations. We provide visualizations for local and global PFI, which illustrate how changes in the considered feature affect model performance. We also relate our new visual tools to PD plots, ICE plots and show that the integral of our PI curve results in the global PFI measure. Furthermore, we propose a permutation-based Shapley feature importance (SFIMP) measure that fairly distributes the model performance among features and allows the comparison of feature importances across different models."
    },
    {
      "heading": "2 Preliminaries and Background on Feature Effects",
      "text": "In this section, we introduce the notation and describe methods focusing on feature effects, which we transfer to feature importance in Section 4 and 5.\nGeneral Notation: Consider a p\u2212dimensional feature space XP = (X1 \u00d7 . . .\u00d7 Xp) with the feature index set P = {1, . . . , p} and a target space Y. Suppose that there is an unknown functional relationship f between XP and Y. ML algorithms try to learn this relationship using training data with observations\n1 In the literature, the term feature importance is sometimes also used for methods that only work with model predictions. In our context, however, we would categorize them under feature effects as they do not take into account the model performance.\nthat have been drawn i.i.d. from an unknown probability distribution P on the joint space XP \u00d7Y . We consider an arbitrary prediction model f\u0302 , fitted on some training data to approximate f and analyze it with model-agnostic interpretability methods. Let D = {(x(i), y(i))}ni=1 be a test data set sampled i.i.d. from P where n is the number of observations in the test set. We denote the corresponding random variables generated from the feature space by X = (X1, . . . , Xp) and the random variable generated from the target space by Y . In our notation, the vector x(i) = (x (i) 1 , . . . , x (i) p )> \u2208 XP refers to the i-th observation, which is associated with the target variable y(i) \u2208 Y, and xj = (x(1)j , . . . , x (n) j ) > denotes the realizations of the j-th feature. We denote the generalization error of a fitted model, which is measured by a loss function L on unseen test data from P, by GE(f\u0302 ,P) = E(L(f\u0302(X), Y )). It can be estimated using the test data D by\nG\u0302E(f\u0302 ,D) = 1n \u2211n i=1 L(f\u0302(x (i)), y(i)). (1)\nA better estimate for the generalization error of an ML algorithm can be obtained using resampling techniques such as cross-validation or bootstrap [1].\nPD Plots [10] visualize the marginal relationship between features of interest and the expected prediction of a fitted model on a global level. Consider a subset of feature indices S \u2286 P and its complement C. Each observation x \u2208 XP can be partitioned into xS \u2208 XS and xC \u2208 XC containing only features from S and C, respectively. Let XS and XC be the corresponding random variables and let the prediction function using features in S, marginalized over features in C be the PD function defined by fS(xS) = EXC (f\u0302(xS , XC)). This definition also covers f\u2205(x\u2205) and results in a constant, the average prediction over P . We can estimate the PD function using Monte-Carlo integration by averaging over feature values x (i) C in order to marginalize out features in C:\nf\u0302S(xS) = 1 n \u2211n i=1 f\u0302 (i) S (xS) = 1 n \u2211n i=1 f\u0302(xS ,x (i) C ). (2)\nHere, f\u0302 (i) S (xS) = f\u0302(xS ,x (i) C ) can be read in two ways: a) the prediction of the i-th observation with replaced feature values in S taken from x or b) the prediction of x with replaced values in C taken from the i-th observation. Plotting the pairs {(x\u2217(k)S , f\u0302S(x\u2217 (k) S ))}mk=1 using (often m < n) grid points denoted by x\u2217 (1) S , . . . ,x \u2217(m) S yields a PD curve. Fig. 1 illustrates the PD principle for a simple example.\nICE Plots [11]: The averaging in Eq. (2) of the PD function can obfuscate more complex relationships resulting from feature interactions, i.e. when the partial relationship of one or more observations depends on other features. ICE plots address this problem by visualizing to what extent the prediction of a single observation changes when the value of the considered feature changes. Instead of plotting the pairs {(x\u2217(k)S , f\u0302S(x\u2217 (k)\nS ))}mk=1, ICE plots visualize the pairs {(x\u2217(k)S , f\u0302 (i) S (x \u2217(k) S ))}mk=1 for each observation indexed by i \u2208 {1, . . . , n}.\nShapley Value: A coalitional game is defined by a set of players P , which can form coalitions S \u2286 P . Each coalition S achieves a certain payout. The characteristic function v : 2P \u2192 R maps all 2p possible coalitions to their payouts. The Shapley value [19] now fairly assigns a value to each player depending on their contribution in all possible coalitions. This concept was transferred to feature effect estimation in [21]. We could explain the prediction of a single, fixed observation x by regarding features as players, who form various coalitions (subsets) S to achieve the prediction f\u0302(x). For each coalition S, we are only allowed to access values of features from S. A natural definition of the payout is the PD value fS(xS), which we shift so that an empty set of no features is assigned a value of 0 \u2013 which is required by the general Shapley value definition:\nv(xS) = EXC (f\u0302(xS , XC))\u2212 EX(f\u0302(X)) = fS(xS)\u2212 f\u2205(x\u2205). (3)\nThe marginal contribution of feature j, joining a coalition S, is defined as\n\u2206j(xS) = v(xS\u222a{j})\u2212 v(xS) = fS\u222a{j}(xS\u222a{j})\u2212 fS(xS).\nLet \u03a0 be the set of all possible permutations over the index set P . For a permutation \u03c0 \u2208 \u03a0, we denote the set of features that are in order before feature j as Bj(\u03c0). For example, for p = 4, if we consider feature j = 4 and permutation \u03c0 = {2, 3,4, 1}, then B4(\u03c0) = {2, 3}. For an observation x and its feature value for feature j, the Shapley value can be estimated by\n\u03c6\u0302j(x) = 1 p! \u2211 \u03c0\u2208\u03a0 \u2206\u0302j(xBj(\u03c0))\n= 1p! \u2211 \u03c0\u2208\u03a0 f\u0302Bj(\u03c0)\u222a{j}(xBj(\u03c0)\u222a{j})\u2212 f\u0302Bj(\u03c0)(xBj(\u03c0))\n= 1p!\u00b7n \u2211 \u03c0\u2208\u03a0 \u2211n i=1 f\u0302 (i) Bj(\u03c0)\u222a{j}(xBj(\u03c0)\u222a{j})\u2212 f\u0302 (i) Bj(\u03c0) (xBj(\u03c0)),\nwhere f\u0302Bj(\u03c0) and f\u0302Bj(\u03c0)\u222a{j} are estimated by Eq. (2). An efficient approximation based on Monte-Carlo integration using m rather than p! \u00b7 n summands was proposed by [22]. Consider the following example to illustrate the Shapley value: The features enter a room in a random order specified by the permutation \u03c0. All features in the room participate in the game, i.e., they contribute to the model prediction. The Shapley value \u03c6j is the average additional contribution of feature j by joining whatever features already entered the room before."
    },
    {
      "heading": "3 Permutation-based Feature Importance",
      "text": "Background. The permutation importance for random forests introduced in [3] measures the performance, e.g., the mean squared error (MSE), of each tree\nwithin a random forest using out-of-bag samples. The performance is measured once with and once without permuted values of the feature of interest. The difference between those two performance values is computed for each tree and averaged to yield the feature importance. Permuting the values of a feature breaks the association between the feature and the target variable and results in a large drop in performance if the considered feature is important. A model-agnostic global PFI for features included in S can be defined as\nPFIS = E(L(f\u0302(X\u0303S , XC), Y ))\u2212 E(L(f\u0302(X), Y )) (4)\nwhere X\u0303S refers to an independent replication of XS , which is also independent of XC and Y . This implies that X\u0303S is a new (multivariate) random variable, which is distributed as XS , but independent of everything else. This definition is analogous to the permutation-based model reliance introduced by [9] and relates to the definition in [12] where the authors focus on random forests. The larger the value of PFIS , the more substantial the increase in error when we permute feature values in S, and the more important we deem the feature set S. According to [9],\nthe use of the ratio PFIS = E(L(f\u0302(X\u0303S , XC), Y ))/E(L(f\u0302(X), Y )) instead of the difference might be more comparable across different models, as it always refers to the relative drop in performance with respect to the standard generalization error. However, using the ratio can result in numerically unstable estimations if the denominator is close or equal to zero. Thus, both definitions have drawbacks that we try to are address in Section 5.\nEstimating and Approximating the PFI. The first term of Eq. (4) encodes the expected generalization error under perturbation of features in feature set S, which can be formulated as:\nE(L(f\u0302(X\u0303S , XC), Y )) = E(XC ,Y )(EX\u0303S |(XC ,Y )(L(f\u0302(X\u0303S , XC), Y )))\n= E(XC ,Y )(EX\u0303S (L(f\u0302(X\u0303S , XC), Y ))) = E(XC ,Y )(EXS (L(f\u0302(XS , XC), Y )))\nIn the derivation above, the first equality follows from the \u201claw of total expectation\u201d, the second from the independence of X\u0303S from (XC , Y ), and the third because X\u0303S is distributed as XS . We can plug in an estimator for the inner expected value and denote the estimate of this quantity by\nG\u0302EC(f\u0302 ,D) = 1n \u2211n i=1 1 n \u2211n k=1 L(f\u0302(x (k) S ,x (i) C ), y (i)). (5)\nThe index C in GEC emphasizes that the set of features in C were not replaced with a perturbed random variable and can thus be seen as the model performance using features in C (and ignoring those in S). The above estimator is analogous to the V-statistic [18] and may also be replaced by the unbiased U-statistic using 1 n \u2211n i=1 1 n\u22121 \u2211 k 6=i L(f\u0302(x (k) S ,x (i) C ), y (i)) as proposed by [9].2 The estimator scales\n2 For the sake of simplicity, we consider the V-statistic throughout the article. However, all calculations and approximations based on Eq. (5) still apply \u2013 with slight modifications \u2013 when using the U-statistic.\nwith O(n2) (for a given set C, and assuming f\u0302 can be computed in constant time), which can be expensive when n is large. However, we can use a different formulation to motivate an approximation for Eq. (5): Let {\u03c4 1, . . . , \u03c4 n!} be the set of all possible permutation vectors over the observation index set {1, . . . , n}. As shown by [9], we can replace Eq. (5) by the equivalent formulation\nG\u0302EC,perm(f\u0302 ,D) = 1n \u2211n i=1 1 n! \u2211n! k=1 L(f\u0302(x (\u03c4 (i) k ) S ,x (i) C ), y (i)).\nIf we approximate G\u0302EC,perm by Monte-Carlo integration using only m randomly selected permutations rather than all n! permutations, we obtain\nG\u0302EC,approx(f\u0302 ,D) = 1n \u2211n i=1 1 m \u2211m k=1 L(f\u0302(x (\u03c4 (i) k ) S ,x (i) C ), y (i)). (6)\nThe approximation refers to permuting features in S repeatedly (i.e., m times) and averaging the resulting model performances.3 The PFI from Eq. (4) can be estimated using Eq. (5) for the first term and using Eq. (1) for the last term. Including the summands into an iterated sum yields the estimate\nP\u0302F IS = 1 n2 \u2211n i=1 \u2211n k=1 ( L(f\u0302(x (k) S ,x (i) C ), y (i))\u2212 L(f\u0302(x(i)), y(i)) ) . (7)\nIf we use Eq. (6) rather than Eq. (5), we obtain the approximation\nP\u0302F IS,approx = 1 n\u00b7m \u2211n i=1 \u2211m k=1 ( L(f\u0302(x (\u03c4 (i) k ) S ,x (i) C ), y (i))\u2212 L(f\u0302(x(i)), y(i)) ) . (8)\nEq. (8) is identical to the permutation importance of random forests formalized in [12] if we consider m as the number of trees, replace n with the number of\nout-of-bag samples per tree and replace the model f\u0302 with the individual trees fitted within a random forest, i.e., f\u0302k."
    },
    {
      "heading": "4 Visualizing Global and Local Feature Importance",
      "text": "Consider the summands in Eq. (7) and denote them by\n\u2206L(i)(xS) = L(f\u0302(xS ,x (i) C ), y (i))\u2212 L(f\u0302(x(i)), y(i)).\nThis quantity refers to the change in performance between the i-th observation with and without replaced feature values xS . Inspired by ICE plots, we introduce individual conditional importance (ICI) plots which visualize the pairs {(x(k)S , \u2206L(i)(x (k) S ))}nk=1 for all observations i = 1, . . . , n. We define the local feature importance of the i-th observation (regarding features in S) as the integral of\n3 By the same logic, we could also directly approximate Eq. (5) by summing over m randomly selected feature values for features in S instead of using all of them. We here opted for Eq. (6), due to the in our opinion interesting relation to the random forest permutation importance explained at the end of this section.\nthe corresponding ICI curve with respect to the distribution of XS . It is estimated by P\u0302F I (i)\nS = 1 n \u2211n k=1\u2206L (i)(x (k) S ) and can be interpreted as the expected change\nin performance of the i-th observation after marginalizing its features in S. It also refers to the contribution of the i-th observation to the global PFI (see later in Eq. (9)). To the best of our knowledge, a similar definition for local feature importance only exists in the context of random forests, e.g., in [7].\nAnalogous to the PD function from Eq. (2), we introduce the partial importance (PI) function as the expected change in performance at a specific value xS , which can be estimated by P\u0302 IS(xS) = 1 n \u2211n i=1\u2206L (i)(xS). Consequently, a PI plot visualizes the pairs {(x(k)S , P\u0302 IS(x (k) S ))}nk=1 and refers to the pointwise average of all ICI curves across all observations at fixed grid points xS . Fig. 2 illustrates the computation of ICI and PI curves for the first feature. It also shows the n grid points for which \u2206L(i)(x (i) S ) = 0 \u2200i. We can omit these points by plotting the pairs {(x(k)S , \u2206L(i)(x (k) S ))}k\u2208{1,...,n}\\{i} to visualize the unbiased estimation of the feature importance proposed by [9]. Visualizing the ICI curves for the approximation in Eq. (8) implies that some grid points are randomly skipped because the feature values used as grid points are implicitly determined by the randomly selected permutations in Eq. (8). The ICI curves, the PI curve, and the global PFI are related: Averaging all ICI curves pointwise yields a PI curve. Integrating the PI curve (as well as averaging the integral of all ICI curves) using Monte-Carlo integration over all points {x(k)S }nk=1 yields an equivalent estimate of the global PFI from Eq. (7):\nP\u0302F IS = 1 n \u2211n i=1 P\u0302F I (i) S = 1 n \u2211n k=1 P\u0302 IS(x (k) S ). (9)\nWe propose to additionally inspect the PI and ICI curves instead of focusing on a single PFI value. PI curves enable the user to identify regions in which the feature importance is higher or lower than its global PFI. ICI curves additionally enable the user to identify (suspicious) observations that impact the global PFI strongly and can reveal heterogeneity in the feature importance among the observations, which remain hidden in the PI plots (see also Section 6).\nAlgorithm 1 describes a procedure for obtaining PI and PD plots, which also\nallows to return ICI and ICE plots by visualizing {(x\u2217(k)S , \u2206L(i)(x\u2217 (k) S ))}mk=1 and {(x\u2217(k)S , f\u0302 (i) S (x \u2217(k) S ))}mk=1 for all observations i. Similar to PD and ICE plots, we can use all k = 1, . . . , n or a random sample (of size m < n) of feature values from S as grid points for PI and ICI plots."
    },
    {
      "heading": "5 Shapley Feature Importance",
      "text": "In this section, we introduce the Shapley F eature IMPortance (SFIMP) measure, which allows to easily visualize and interpret the contribution of each feature to the model performance. Our goal is to fairly distribute the performance difference among the individual features between the scenario when all features are used and when all features are ignored, which is illustrated in Fig. 3.\nx1 x2 x3 1 4 5 2 6 7 3 8 9 a)\u2212\u2192\nx1 x2 x3 1 4 5 1 6 7 1 8 9 2 4 5 2 6 7 2 8 9 3 4 5 3 6 7 3 8 9 b)\u2212\u2192\ni x1 x2 x3 \u2206L (i)(x1) 1 1 4 5 0 2 1 6 7 0.6 3 1 8 9 0.3 1 2 4 5 0.65 2 2 6 7 0 3 2 8 9 0.25 1 3 4 5 0.7 2 3 6 7 0.5 3 3 8 9 0 c)\u2212\u2192 x1 x2 x3 P\u0302 IS(x1) 1 4 5 0.3 2 6 7 0.3 3 8 9 0.4\n2\n3\n1\n3\n1\n2\nx1\nP F\nI ^\n1 2 3\n0 0.\n25 0.\n5 0.\n75\nThe Shapley value was used in [6] for a fair attribution of the difference in model performance. However, the authors focused on feature selection which requires refitting the model by leaving out or including features. This can lead to different results of the learning algorithm since different relationships can be learned due to the absence of features. This is reasonable in the context of feature selection. However, as we measure the feature importance of an already fitted model, we prefer marginalizing over features rather than omitting them completely. Inspired by Eq. (3), we define the characteristic function of the coalition of features in S \u2286 P based on Eq. (5) as:\nvGE(S) = G\u0302ES(f\u0302 ,D)\u2212 G\u0302E\u2205(f\u0302 ,D). (10)\nThe characteristic function measures the change in performance between using features in S (i.e., ignoring features in its complement C by marginalizing over them) and ignoring all features. This is similar to Eq. (7) which, in contrast, measures the change in performance between ignoring features in S and using all features. Since the error G\u0302E\u2205(f\u0302 ,D) (no features are considered, i.e., all\nfeatures are marginalized out) is usually greater than G\u0302ES(f\u0302 ,D), vGE(S) will have negative values.4 The marginal contribution of a feature j to a coalition of features in S is given by\n\u2206j(S) = vGE(S \u222a {j})\u2212 vGE(S) = G\u0302ES\u222a{j}(f\u0302 ,D)\u2212 G\u0302ES(f\u0302 ,D).\nIf we consider a permuted order \u03c0 \u2208 \u03a0 of the features, where Bj(\u03c0) is the set of features occurring before feature j, we obtain the Shapley value estimation\n\u03c6\u0302j(vGE) = 1 p! \u2211 \u03c0\u2208\u03a0 \u2206j(Bj(\u03c0))\n= 1p! \u2211 \u03c0\u2208\u03a0 G\u0302EBj(\u03c0)\u222a{j}(f\u0302 ,D)\u2212 G\u0302EBj(\u03c0)(f\u0302 ,D),\n(11)\nwhich refers to the SFIMP measure of feature j. Computing Eq. (11) is computationally expensive when the number of features p is large, even if we use the approximation of the model performance from Eq. (6). We therefore suggest an efficient procedure in Algorithm 2. The Shapley value satisfies the following four desirable properties as already worked out in [6]:\n1. Efficiency: \u2211p j=1 \u03c6j = vGE(P ). All SFIMP values add up to vGE(P ), i.e., the\ndifference in performance between the scenario when all features are used and when all features are ignored. This allows us to calculate the proportion of explained importance for each feature j using\n\u03c6j\u2211p j=1 \u03c6j .\n2. Symmetry: If vGE(S \u222a {j}) = vGE(S \u222a {k}) for all S \u2286 {1, . . . , p} \\ {j, k}, then \u03c6j = \u03c6k. Two features j and k have the same SFIMP values if their marginal contribution to all possible coalitions is the same. 3. Dummy property: If vGE(S \u222a {j}) = vGE(S) for all S \u2286 P , then \u03c6j = 0. The SFIMP value of a feature j is zero if its marginal contribution does not change no matter to which coalition S the feature is added. 4. Additivity: \u03c6j(vGE+wGE) = \u03c6j(vGE)+\u03c6j(wGE). The SFIMP value resulting from a single game with two combined performance measures \u03c6j(vGE +wGE) is the same as the sum of the two SFIMP values resulting from two separate games with corresponding characteristic functions, i.e., \u03c6j(vGE) + \u03c6j(wGE). Linearity: \u03c6j(c \u00b7 vGE) = c \u00b7 \u03c6j(vGE). Any multiplication of the performance measure with a constant c does not affect the feature ranking.\n4 We prefer the definition in Eq. (10) as it directly shows the relation to Eq. (3), however, we could also swap the sign as discussed at the end of this section.\nAlgorithm 2: Approximation of SFIMP values: Contribution of j-th feature towards the model performance.\nInput: mfeat, mobs, f\u0302 , L, D = {(x(i), y(i))}ni=1 1 forall k \u2208 {1, . . . ,mfeat} do 2 choose a random permutation of the feature indices \u03c0 \u2208 \u03a0. 3 set S = Bj(\u03c0) containing features that won\u2019t be permuted.\n4 set G\u0302ES,perm = 0 and G\u0302ES\u222a{j},perm = 0. 5 forall l \u2208 {1, . . . ,mobs} do 6 choose a random permutation of observation indices \u03c4 \u2208 {\u03c4 1, . . . , \u03c4 n!}. 7 measure performance by permuting features w.r.t. \u03c4 = (\u03c4 (1), . . . , \u03c4 (n)):\nG\u0302ES,perm = G\u0302ES,perm + 1 n \u2211n i=1 L(f\u0302(x (i) S ,x (\u03c4(i)) C ), y (i))) G\u0302ES\u222a{j},perm = G\u0302ES\u222a{j},perm + 1 n \u2211n i=1 L(f\u0302(x (i) S\u222a{j},x (\u03c4(i)) C\\{j}), y (i)))\n8 compute marginal contribution for feature j in iteration k:\n\u2206 (k) j (S) = 1 mobs \u00b7 (G\u0302ES\u222a{j},perm \u2212 G\u0302ES,perm)\n9 return \u03c6\u0302j = 1\nmfeat\n\u2211mfeat k=1 \u2206 (k) j (S)\nThe properties above imply that fairly distributing the drop in performance using vPFI(S) = P\u0302F IS = G\u0302EC(f\u0302 ,D)\u2212 G\u0302EP (f\u0302 ,D) results in the same Shapley values (except for the sign) and is equivalent to using \u2212vGE(P ). The SFIMP measure can thus be seen as an extension of the PFI measure in the sense that it additionally fairly distributes the importance values among features. The PFI measure ignores features in S by permuting or marginalizing over them, which destroys any correlation and interaction of features in C with features in S. Consequently, the PFI of a feature also includes the importance of any interaction with that feature and features in C and therefore an interaction will be fully attributed to all involved features. The SFIMP measure solves this issue as it considers the marginal contribution of a feature and equally distributes the importance of interactions among the interacting features. This allows comparing feature importances across different models."
    },
    {
      "heading": "6 Simulations and Application",
      "text": "For full reproducibility, all our proposed methods are available in the R package featureImportance5. The repository also contains the R code, which is partly based on batchtools [13], for the application and simulation in this section."
    },
    {
      "heading": "6.1 Simulations",
      "text": "PI and ICI Plots. Consider the following data-generating model:\nY = X1 +X2 + 10X1 \u00b7 1X3=0 + 10X2 \u00b7 1X3=1 + , 5 https://github.com/giuseppec/featureImportance.\nX1, X2 i.i.d\u223c N (0, 1), X3 \u223c B(1, 0.5), \u223c N (0, 0.5).\nWe simulate a training data set with 10000 observations, train a random forest and compute the global PFI on 100 test sets of size n = 100 sampled from the same distribution. We demonstrate that, by merely inspecting the global PFI, the features X1 and X2 would be considered equally important. However, due to the interactions, it is clear that feature X1 should be considered more important than X2 when X3 = 0 and vice-versa when X3 = 1.\nAccording to Eq. (9), averaging the local feature importances (i.e., the integral of all ICI curves) results in the global PFI. Having at hand the local feature importance of each observation allows calculating the PFI conditional on other features. This does not require additional time-consuming calculations, as we only have to average the already computed local feature importances according to the condition considered in the conditional PFI. The relevance of conditional feature importance in the case of random forests with correlated features was discussed in [20]. In Fig. 4, we illustrate the usefulness of a model-agnostic conditional PFI in case of interactions by showing the PI curves of X1 and X2 conditional on the binary feature X3. The integral of these conditional PI curves refers to the PFI conditional on X3. Its value differs depending on the two groups introduced by feature X3, which suggests that there is an interaction between the features X1 and X3 as well as X2 and X3.\nTable 1 shows that feature X1 and X2 are almost equally important if we consider the unconditional global PFI. However, a different ranking of features is obtained when we compute the PFI conditional on X3. Thus, inspecting PI and ICI curves conditional on other feature values may help in detecting interactions.\nShapley Feature Importance. We illustrate how the SFIMP measure can be used to compare the feature importance across different models and present the results of a small simulation study to compare the SFIMP measure introduced in Section 5 with the difference-based and the ratio-based PFI discussed in Section 3. Consider the following data-generating linear model with a simple interaction:\nY = X1 +X2 +X3 +X1 \u00b7X2 + , X1, X2, X3 i.i.d\u223c N (0, 1), \u223c N (0, 0.5).\nAll three features and the interaction of X1 and X2 have the same linear effect on the target Y . We simulate training data with 10000 observations and train four\n0\n100\n200\n300\n\u22122 \u22121 0 1 2\nX1\n\u2206 L\nba se\nd on\nM S\nE\n0\n100\n200\n300\n\u22122 \u22121 0 1 2\nX2\n\u2206 L\nba se\nd on\nM S\nE\nX3\n0\n1"
    },
    {
      "heading": "6.2 Application on Real Data",
      "text": "We demonstrate our graphical tools on the Boston housing data, which is publicly available on OpenML [23] with data set ID 531. The data set contains 13 features that may affect the median home price of 506 metropolitan areas of Boston. We used the OpenML R package [5] and created the OpenML task with ID 167147 containing a holdout split ( 23 vs. 1 3 ) for training a random forest and producing the PI and ICI plots on the test set.\nRow (1) of Table 2 shows the global PFI values of all features. They are estimated using Eq. (7) by taking into account all 166 \u00b7166 points of the test data. Fig. 6 shows the corresponding PI and ICI curves for the two most important features (LSTAT and RM). They visualize which regions of each feature and which observations have a high impact on the computed PFI values on a global and local level, which follows from the relation in Eq. (9).\nPI plots visualize the expected change in performance at each position of the abscissa. An expected change close to zero across the whole range of the feature values suggests an unimportant feature. The PI plot of LSTAT in Fig. 6 suggests that the feature is more important if LSTAT < 10. For illustration purposes, we omit all observations for which LSTAT \u2265 10 and recompute the conditional PFI values, which are displayed in Row (2) of Table 2. The resulting conditional PFI values are smaller, i.e., excluding observations for which LSTAT \u2265 10 makes the LSTAT feature less important. Note that omitting observations change the empirical distribution of the features and thus also affects the importance of other features when the PFI values are recomputed.\nICI curves additionally reveal the most (and the least) influential observations for the feature importance by considering their integral (see highlighted lines in Fig. 6). We can, for example, omit observations with a negative ICI curve integral. In our test set, we observe 18 of 166 ICI curves with a negative integral\nfor the LSTAT feature. These observations have a negative impact on the global PFI according to the relation in Eq. (9). We omit them and recompute the PFI values. The results are listed in row (3) of Table 2 and show an increased PFI value for LSTAT."
    },
    {
      "heading": "7 Conclusion and Future Work",
      "text": "PI plot\nPI plot\nthe variability introduced by the estimation of the model itself via resampling and plot or aggregate the resulting set of quantities."
    },
    {
      "heading": "Acknowledgments",
      "text": "This work is funded by the Bavarian State Ministry of Education, Science and the Arts in the framework of the Centre Digitisation.Bavaria (ZD.B)."
    }
  ],
  "title": "Visualizing the Feature Importance for Black Box Models",
  "year": 2018
}
