{"abstractText": "Rem-Sophia Mouradi, C\u00e9dric Goeury, Olivier Thual, Fabrice Zaoui, and Pablo Tassi EDF R&D, National Laboratory for Hydraulics and Environment (LNHE), 6 Quai Watier, 78400 Chatou, France Climate, Environment, Coupling and Uncertainties research unit (CECI) at the European Center for Research and Advanced Training in Scientific Computation (CERFACS), French National Research Center (CNRS), 42 Avenue Gaspard Coriolis, 31820 Toulouse, France Institut de M\u00e9canique des Fluides de Toulouse (IMFT), Universit\u00e9 de Toulouse, CNRS, Toulouse, France Saint-Venant Laboratory for Hydraulics (LHSV), Chatou, France", "authors": [{"affiliations": [], "name": "Rem-Sophia Mouradi"}, {"affiliations": [], "name": "C\u00e9dric Goeury"}, {"affiliations": [], "name": "Olivier Thual"}, {"affiliations": [], "name": "Fabrice Zaoui"}, {"affiliations": [], "name": "Pablo Tassi"}], "id": "SP:f19dd6b465d7c05d36cbe0c65c5f2945ef1df78a", "references": [{"authors": ["O.I. Abiodun", "A. Jantan", "A.E. Omolara", "K.V. Dada", "N.A. Mohamed", "H. Arshad"], "title": "State-of-the-art in artificial neural network applications : A survey", "venue": "Heliyon, 4(11) :e00938,", "year": 2018}, {"authors": ["C. Adam-Bourdarios", "G. Cowan", "C. Germain", "I. Guyon", "B. K\u00e9gl", "D. Rousseau"], "title": "The higgs boson machine learning challenge", "venue": "Proceedings of the 2014 International Conference on High-Energy Physics and Machine Learning - Volume 42, HEPML\u201914, page 19\u201355. JMLR.org,", "year": 2014}, {"authors": ["N. Akkari"], "title": "Mathematical study of the sensitivity of the POD method (Proper orthogonal decomposition)", "venue": "Theses, Universit\u00e9 de La Rochelle, Dec.", "year": 2012}, {"authors": ["L.O. Amoudry", "A.J. Souza"], "title": "Deterministic coastal morphological and sediment transport modeling : a review and discussion", "venue": "Reviews of Geophysics, 49(2),", "year": 2011}, {"authors": ["A.R. Barron"], "title": "Approximation and estimation bounds for artificial neural networks", "venue": "Mach. Learn., 14(1) : 115\u2013133, Jan.", "year": 1994}, {"authors": ["G. Blatman"], "title": "Adaptive sparse polynomial chaos expansions for uncertainty propagation and sensitivity analysis", "venue": "PhD thesis,", "year": 2009}, {"authors": ["G. Blatman", "B. Sudret"], "title": "Adaptive sparse polynomial chaos expansion based on least angle regression", "venue": "Journal of Computational Physics, 230(6) :2345 \u2013 2367,", "year": 2011}, {"authors": ["L. Bruno", "C. Canuto", "D. Fransos"], "title": "Stochastic aerodynamics and aeroelasticity of a flat plate via generalised polynomial chaos", "venue": "Journal of Fluids and Structures, 25(7) :1158 \u2013 1176,", "year": 2009}, {"authors": ["S.L. Brunton", "B.R. Noack", "P. Koumoutsakos"], "title": "Machine learning for fluid mechanics", "venue": "Annual Review of Fluid Mechanics, 52(1) :477\u2013508,", "year": 2020}, {"authors": ["K. Campbell", "M.D. McKay", "B.J. Williams"], "title": "Sensitivity analysis when model outputs are functions", "venue": "Reliability Engineering & System Safety, 91(10) :1468 \u2013 1472,", "year": 2006}, {"authors": ["A. Casas", "G. Benito", "V. Thorndycraft", "M. Rico"], "title": "The topographic data source of digital terrain models as a key element in the accuracy of hydraulic flood modelling", "venue": "Earth Surface Processes and Landforms : The Journal of the British Geomorphological Research Group, 31(4) :444\u2013456,", "year": 2006}, {"authors": ["J. Castro", "C. Mantas", "J. Benitez"], "title": "Neural networks with a continuous squashing function in the output are universal approximators", "venue": "Neural Networks, 13(6) :561 \u2013 563,", "year": 2000}, {"authors": ["N. Cohen", "O. Sharir", "A. Shashua"], "title": "On the expressive power of deep learning : A tensor analysis", "venue": "V. Feldman, A. Rakhlin, and O. Shamir, editors, 29th Annual Conference on Learning Theory, volume 49 of Proceedings of Machine Learning Research, pages 698\u2013728, Columbia University, New York, New York, USA, 23\u201326 Jun", "year": 2016}, {"authors": ["L. Cordier", "M. Bergmann"], "title": "Proper orthogonal decomposition : an overview", "venue": "Lecture series 2002-04, 2003-03 and 2008-01 on post-processing of experimental and numerical data, Von Karman Institute for Fluid Dynamics, 2008., page 46 pages. VKI,", "year": 2008}, {"authors": ["S. Costa", "F. Gourmelon", "C. Augris", "P. Clabaut", "B. Latteux"], "title": "Apport de l\u2019approche syst\u00e9mique et pluridisciplinaire dans l\u2019\u00e9tude du domaine littoral et marin de la seine-maritime (france)", "venue": "Norois. Environnement, am\u00e9nagement, soci\u00e9t\u00e9, (196) :91\u2013108,", "year": 2005}, {"authors": ["M. Couplet"], "title": "Reduced-order POD-Galerkin modelling for the control of unsteady flows", "venue": "Theses, Universit\u00e9 Paris-Nord - Paris XIII, Jan.", "year": 2005}, {"authors": ["G. Cruciani", "M. Baroni", "S. Clementi", "G. Costantino", "D. Riganelli", "B. Skagerberg"], "title": "Predictive ability of regression models", "venue": "part i : Standard deviation of prediction errors (sdep). Journal of Chemometrics, 6(6) : 335\u2013346,", "year": 1992}, {"authors": ["G. Cybenko"], "title": "Approximation by superpositions of a sigmoidal function", "venue": "Mathematics of Control, Signals and Systems, 2(4) :303\u2013314, Dec", "year": 1989}, {"authors": ["R.G. Dean", "R.A. Dalrymple"], "title": "Coastal processes with engineering applications", "venue": "Cambridge University Press,", "year": 2004}, {"authors": ["B. Efron", "R. Tibshirani"], "title": "Bootstrap methods for standard errors, confidence intervals, and other measures of statistical accuracy", "venue": "Statistical science, pages 54\u201375,", "year": 1986}, {"authors": ["R. Eldan", "O. Shamir"], "title": "The power of depth for feedforward neural networks", "venue": "Conference on Learning Theory, 12", "year": 2015}, {"authors": ["N. Faber", "R. Rajk\u00f4"], "title": "How to avoid over-fitting in multivariate calibration\u2014the conventional validation approach and an alternative", "venue": "Analytica Chimica Acta, 595(1) :98 \u2013 106,", "year": 2007}, {"authors": ["K.-I. Funahashi"], "title": "On the approximate realization of continuous mappings by neural networks", "venue": "Neural Networks, 2(3) :183 \u2013 192,", "year": 1989}, {"authors": ["O. Garcia-Cabrejo", "A. Valocchi"], "title": "Global sensitivity analysis for multivariate output using polynomial chaos expansion", "venue": "Reliability Engineering & System Safety, 126 :25\u201336,", "year": 2014}, {"authors": ["M. Gevrey", "I. Dimopoulos", "S. Lek"], "title": "Review and comparison of methods to study the contribution of variables in artificial neural network models", "venue": "Ecological Modelling, 160(3) :249 \u2013 264,", "year": 2003}, {"authors": ["M. Ghil", "P. Yiou", "S. Hallegatte", "B. Malamud", "P. Naveau", "A. Soloviev", "V. Keilis-Borok", "D. Kondrashov", "V. Kossobokov", "O. Mestre"], "title": "Extreme events : dynamics, statistics and prediction", "venue": "Nonlinear Processes in Geophysics,", "year": 2011}, {"authors": ["E.B. Goldstein", "G. Coco", "N.G. Plant"], "title": "A review of machine learning applications to coastal sediment transport and morphodynamics", "venue": "Earth-Science Reviews, 194 :97 \u2013 108,", "year": 2019}, {"authors": ["A. Gonoskov", "E. Wallin", "A. Polovinkin", "I. Meyerov"], "title": "Employing machine learning for theory validation and identification of experimental conditions in laserplasma physics", "venue": "Scientific Reports, 9 :7043,", "year": 2019}, {"authors": ["I. Goodfellow", "Y. Bengio", "A. Courville"], "title": "Deep Learning", "venue": "MIT Press,", "year": 2018}, {"authors": ["S. Gordeyev", "F. Thomas"], "title": "Temporal proper decomposition (tpod) for closed-loop flow control", "venue": "Exp Fluids, 54,", "year": 2013}, {"authors": ["A. Guillaume"], "title": "VAG-Modele de prevision de l\u2019etat de la mer en eau profonde", "venue": "Dir. de la Meteorologie Nationale,", "year": 1987}, {"authors": ["M. Guo", "J.S. Hesthaven"], "title": "Reduced order modeling for nonlinear structural analysis using gaussian process regression", "venue": "Computer Methods in Applied Mechanics and Engineering, 341 :807 \u2013 826,", "year": 2018}, {"authors": ["I. Guyon", "A. Elisseeff"], "title": "An introduction to variable and feature selection", "venue": "J. Mach. Learn. Res., 3(null) : 1157\u20131182, Mar.", "year": 2003}, {"authors": ["B. Hanin"], "title": "Universal function approximation by deep neural nets with bounded width and relu activations", "venue": "Mathematics, 7(10),", "year": 2019}, {"authors": ["T. Hastie", "R. Tibshirani", "J. Friedman"], "title": "The Elements of Statistical Learning : Data Mining, Inference, and Prediction, Second Edition (Springer Series in Statistics)", "venue": "02", "year": 2009}, {"authors": ["L. Hawchar", "C.-P.E. Soueidy", "F. Schoefs"], "title": "Principal component analysis and polynomial chaos expansion for time-variant reliability problems", "venue": "Reliability Engineering & System Safety, 167 :406 \u2013 416,", "year": 2017}, {"authors": ["A. Hekmati", "D. Ricot", "P. Druault"], "title": "About the convergence of pod and epod modes computed from cfd simulation", "venue": "Computers & Fluids, 50(1) :60 \u2013 71,", "year": 2011}, {"authors": ["K. Hornik"], "title": "Approximation capabilities of multilayer feedforward networks", "venue": "Neural Networks, 4(2) :251 \u2013 257,", "year": 1991}, {"authors": ["O. Ibrahim"], "title": "A comparison of methods for assessing the relative importance of input variables in artificial neural networks", "venue": "Journal of applied sciences research, 9(11) :5692\u20135700,", "year": 2013}, {"authors": ["B. Iooss", "P. Lem\u00e2\u0131tre"], "title": "A Review on Global Sensitivity Analysis Methods, pages 101\u2013122", "venue": "Springer US, Boston, MA,", "year": 2015}, {"authors": ["R. Iten", "T. Metger", "H. Wilming", "L. del Rio", "R. Renner"], "title": "Discovering physical concepts with neural networks", "venue": "Phys. Rev. Lett.,", "year": 2020}, {"authors": ["M. Janocko", "M. Cartigny", "W. Nemec", "E. Hansen"], "title": "Turbidity current hydraulics and sediment deposition in erodible sinuous channels : laboratory experiments and numerical simulations", "venue": "Marine and Petroleum Geology, 41 :222\u2013249,", "year": 2013}, {"authors": ["I.M. Johnstone", "A.Y. Lu"], "title": "On consistency and sparsity for principal components analysis in high dimensions", "venue": "Journal of the American Statistical Association, 104(486) :682\u2013693,", "year": 2009}, {"authors": ["I. Jolliffe"], "title": "Principal Component Analysis, pages 1094\u20131096", "venue": "Springer Berlin Heidelberg, Berlin, Heidelberg,", "year": 2011}, {"authors": ["B.A. Jones", "A. Doostan"], "title": "Satellite collision probability estimation using polynomial chaos expansions", "venue": "Advances in Space Research, 52(11) :1860 \u2013 1875,", "year": 2013}, {"authors": ["A. Karpatne", "I. Ebert-Uphoff", "S. Ravela", "H.A. Babaie", "V. Kumar"], "title": "Machine learning for the geosciences : Challenges and opportunities", "venue": "IEEE Transactions on Knowledge and Data Engineering, 31(8) :1544\u20131554, Aug", "year": 2019}, {"authors": ["G. Kerschen", "J. Golinval"], "title": "Physical interpretation of the proper orthogonal modes using the singular value decomposition", "venue": "Journal of Sound and Vibration, 249(5) :849 \u2013 865,", "year": 2002}, {"authors": ["G. Kerschen", "J. Golinval", "V.A.F", "B.L.A"], "title": "The method of proper orthogonal decomposition for dynamical characterization and order reduction of mechanical systems : an overview", "venue": "Nonlinear Dynamis,", "year": 2002}, {"authors": ["O.M. Knio", "O.P. Le M\u00e2\u0131tre"], "title": "Uncertainty propagation in CFD using polynomial chaos decomposition", "venue": "Fluid Dynamics Research, 38(9) :616\u2013640, sep", "year": 2006}, {"authors": ["J. Kremer", "K. Stensbo-Smidt", "F. Gieseke", "K.S. Pedersen", "C. Igel"], "title": "Big universe, big data : Machine learning and image analysis for astronomy", "venue": "IEEE Intelligent Systems, 32(2) :16\u201322, Mar", "year": 2017}, {"authors": ["J.N. Kutz"], "title": "Deep learning in fluid dynamics", "venue": "Journal of Fluid Mechanics, 814 :1\u20134,", "year": 2017}, {"authors": ["M. Lamboni", "H. Monod", "D. Makowski"], "title": "Multivariate sensitivity analysis to measure global contribution of input factors in dynamic models", "venue": "Reliability Engineering & System Safety, 96(4) :450 \u2013 459,", "year": 2011}, {"authors": ["M. Larson", "M. Capobianco", "M. Jansen", "G. R\u00f3\u017cy\u0144ski", "H. Southgate", "M. Stive", "K. Wijnberg", "S. Hulscher"], "title": "Analysis and modeling of field data on coastal morphological evolution over yearly and decadal time scales", "venue": "part 1 : Background and linear techniques. Journal of Coastal Research, 19, 09", "year": 2003}, {"authors": ["C. Lataniotis", "S. Marelli", "B. Sudret"], "title": "Extending classical surrogate modelling to ultrahigh dimensional problems through supervised dimensionality reduction : a data-driven approach", "venue": "12", "year": 2018}, {"authors": ["A. Laudani", "G.M. Lozito", "F.R. Fulginei", "A. Salvini"], "title": "On training efficiency and computational costs of a feed forward neural network : A review", "venue": "Computational Intelligence and Neuroscience,", "year": 2015}, {"authors": ["S. Le Bot", "R. Lafite", "M. Fournier", "A. Baltzer", "M. Desprez"], "title": "Morphological and sedimentary impacts and recovery on a mixed sandy to pebbly seabed exposed to marine aggregate extraction (eastern english channel, france)", "venue": "Estuarine, Coastal and Shelf Science, 89(3) :221\u2013233,", "year": 2010}, {"authors": ["O.P. Le Maitre", "O.M. Knio", "H.N. Najm", "R.G. Ghanem"], "title": "A stochastic projection method for fluid flow : I", "venue": "basic formulation. Journal of Computational Physics, 173(2) :481 \u2013 511,", "year": 2001}, {"authors": ["O.P. Le Maitre", "M.T. Reagan", "H.N. Najm", "R.G. Ghanem", "O.M. Knio"], "title": "A stochastic projection method for fluid flow : Ii", "venue": "random process. Journal of Computational Physics, 181(1) :9 \u2013 44,", "year": 2002}, {"authors": ["Y. LeCun", "Y. Bengio", "G. Hinton"], "title": "Deep learning", "venue": "Nature, 521 :436\u2013444,", "year": 2015}, {"authors": ["C.J. Legleiter", "P.C. Kyriakidis", "R.R. McDonald", "J.M. Nelson"], "title": "Effects of uncertain topographic input data on two-dimensional flow modeling in a gravel-bed river", "venue": "Water Resources Research, 47(3),", "year": 2011}, {"authors": ["Y. Liang", "H. Lee", "S. Lim", "W. Lin", "K. Lee", "C. WU"], "title": "Proper orthogonal decomposition and its applications \u2014 part i : Theory", "venue": "Journal of Sound and Vibration, 252(3) :527 \u2013 544,", "year": 2002}, {"authors": ["Y. Liang", "W. Lin", "H. Lee", "S. Lim", "K. Lee", "H. Sun"], "title": "Proper orthogonal decomposition and its applications \u2014 part ii : Model reduction for mems dynamical analysis", "venue": "Journal of Sound and Vibration, 252(3) :527 \u2013 544,", "year": 2002}, {"authors": ["T.E. Lovett", "F. Ponci", "A. Monti"], "title": "A polynomial chaos approach to measurement uncertainty", "venue": "IEEE Transactions on Instrumentation and Measurement, 55(3) :729\u2013736, June", "year": 2006}, {"authors": ["Z. Lu", "H. Pu", "F. Wang", "Z. Hu", "L. Wang"], "title": "The expressive power of neural networks : A view from the width", "venue": "I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 6231\u20136239. Curran Associates, Inc.,", "year": 2017}, {"authors": ["D. Lucor", "C.-H. Su", "G.E. Karniadakis"], "title": "Generalized polynomial chaos and random oscillators", "venue": "International Journal for Numerical Methods in Engineering, 60(3) :571\u2013596,", "year": 2004}, {"authors": ["J.L. Lumley"], "title": "The structure of inhomogeneous turbulent flows", "venue": "Atmospheric Turbulence and Radio Wave Propagation,", "year": 1967}, {"authors": ["M. Ma", "J. Lu", "G. Tryggvason"], "title": "Using statistical learning to close two-fluid multiphase flow equations for bubbly flows in vertical channels", "venue": "International Journal of Multiphase Flow, 85 :336 \u2013 347,", "year": 2016}, {"authors": ["C. Michel", "S. Le Bot", "F. Druine", "S. Costa", "F. Levoy", "C. Dubrulle-Brunaud", "R. Lafite"], "title": "Stages of sedimentary infilling in a hypertidal bay using a combination of sedimentological, morphological and dynamic criteria (bay of somme, france)", "venue": "Journal of Maps, 13(2) :858\u2013865,", "year": 2017}, {"authors": ["S. Mika", "B. Sch\u00f6lkopf", "A.J. Smola", "K.-R. M\u00fcller", "M. Scholz", "G. R\u00e4tsch"], "title": "Kernel pca and de-noising in feature spaces", "venue": "Advances in neural information processing systems, pages 536\u2013542,", "year": 1999}, {"authors": ["K. Mills", "M. Spanner", "I. Tamblyn"], "title": "Deep learning and the Schr\u00f6dinger equation", "venue": "Phys. Rev. A, 96 :042113, Oct", "year": 2017}, {"authors": ["R. Morrow", "L.-L. Fu", "F. Ardhuin", "M. Benkiran", "B. Chapron", "E. Cosme", "F. d\u2019Ovidio", "J.T. Farrar", "S.T. Gille", "G. Lapeyre", "P.-Y. Le Traon", "A. Pascual", "A. Ponte", "B. Qiu", "N. Rascle", "C. Ubelmann", "J. Wang", "E.D. Zaron"], "title": "Global observations of fine-scale ocean surface topography with the surface water and ocean topography (swot) mission", "venue": "Frontiers in Marine Science,", "year": 2019}, {"authors": ["A. Mosavi", "S. Shamshirband", "E. Salwana", "K.-w. Chau", "J.H. Tah"], "title": "Prediction of multi-inputs bubble column reactor using a novel hybrid model of computational fluid dynamics and machine learning", "venue": "Engineering Applications of Computational Fluid Mechanics,", "year": 2019}, {"authors": ["M. Muller"], "title": "On the POD method : an abstract investigation with applications to reduced-order modeling and suboptimal control", "venue": "PhD thesis,", "year": 2008}, {"authors": ["E. Muravleva", "I. Oseledets", "D. Koroteev"], "title": "Application of machine learning to viscoplastic flow modeling", "venue": "Physics of Fluids, 30(10) :103102,", "year": 2018}, {"authors": ["W.J. Murdoch", "C. Singh", "K. Kumbier", "R. Abbasi-Asl", "B. Yu"], "title": "Interpretable machine learning : definitions, methods, and applications", "venue": "arXiv preprint arXiv :1901.04592,", "year": 2019}, {"authors": ["H.N. Southgate", "K. Wijnberg", "M. Larson", "M. Capobianco", "H. Jansen"], "title": "Analysis of field data of coastal morphological evolution over yearly and decadal timescales", "venue": "part 2 : Non-linear techniques. Journal of Coastal Research, 19, 09", "year": 2003}, {"authors": ["J.B. Nagel", "J. Rieckermann", "B. Sudret"], "title": "Principal component analysis and sparse polynomial chaos expansions for global sensitivity analysis and model calibration : Application to urban drainage simulation", "venue": "Reliability Engineering & System Safety, 195 :106737,", "year": 2020}, {"authors": ["A. Ng"], "title": "Machine learning lecture notes", "venue": "http://cs229.stanford.edu/materials.html. Stanford Univ. TR, Stanford, CA,", "year": 2014}, {"authors": ["B.T. Nguyen", "A. Samimi", "J.J. Simpson"], "title": "A polynomial chaos approach for em uncertainty propagation in 3d-fdtd magnetized cold plasma", "venue": "2015 IEEE Symposium on Electromagnetic Compatibility and Signal Integrity, pages 356\u2013360, March", "year": 2015}, {"authors": ["R. Noori", "A. Karbassi", "A. Moghaddamnia", "D. Han", "M. Zokaei-Ashtiani", "A. Farokhnia", "M.G. Gousheh"], "title": "Assessment of input variables determination on the svm model performance using pca, gamma test, and forward selection techniques for monthly stream flow prediction", "venue": "Journal of Hydrology, 401(3) :177 \u2013 189,", "year": 2011}, {"authors": ["A.T.N. Papanicolaou", "M. Elhakeem", "G. Krallis", "S. Prakash", "J. Edinger"], "title": "Sediment transport modeling review&#x2014 ;current and future developments", "venue": "Journal of Hydraulic Engineering, 134(1) :1\u201314,", "year": 2008}, {"authors": ["M.S. Parsons"], "title": "Interpretation of machine-learning-based disruption models for plasma control", "venue": "Plasma Physics and Controlled Fusion, 59(8) :085001, jun", "year": 2017}, {"authors": ["S. Paul", "M.K. Verma"], "title": "Proper Orthogonal Decomposition vs", "venue": "Fourier Analysis for Extraction of Large-Scale Structures of Thermal Convection, pages 433\u2013441.", "year": 2017}, {"authors": ["A. Rigos", "G.E. Tsekouras", "A. Chatzipavlis", "A.F. Velegrakis"], "title": "Modeling Beach Rotation Using a Novel Legendre Polynomial Feedforward Neural Network Trained by Nonlinear Constrained Optimization", "venue": "L. Iliadis and I. Maglogiannis, editors, Artificial Intelligence Applications and Innovations, pages 167\u2013179, Cham,", "year": 2016}, {"authors": ["D. Rolnick", "M. Tegmark"], "title": "The power of deeper networks for expressing natural functions, 2017", "year": 2017}, {"authors": ["B. Rouet-Leduc", "C. Hulbert", "N. Lubbers", "K. Barros", "C.J. Humphreys", "P.A. Johnson"], "title": "Machine learning predicts laboratory earthquakes", "venue": "Geophysical Research Letters, 44(18) :9276\u20139282,", "year": 2017}, {"authors": ["P. Saini", "C.M. Arndt", "A.M. Steinberg"], "title": "Development and evaluation of gappy-pod as a data reconstruction technique for noisy piv measurements in gas turbine combustors", "venue": "Experiments in Fluids, 57(7) :122,", "year": 2016}, {"authors": ["P.J. Schmid"], "title": "Dynamic mode decomposition of numerical and experimental data", "venue": "Journal of Fluid Mechanics, 656 :5\u201328,", "year": 2010}, {"authors": ["J. Schmidhuber"], "title": "Deep learning in neural networks : An overview", "venue": "Neural Networks, 61 :85 \u2013 117,", "year": 2015}, {"authors": ["P.D. Sclavounos", "Y. Ma", "P.D. Sclavounos", "Y. Ma"], "title": "Artificial intelligence machine learning in marine hydrodynamics", "venue": "Proceedings of the ASME 2018 37th International Conference on Ocean, Offshore and Arctic Engineering,17-22 June, Madrid, Spain, ASME, 2018, 06", "year": 2018}, {"authors": ["J. Senent-Aparicio", "P. Jimeno-S\u00e1ez", "A. Bueno-Crespo", "J. P\u00e9rez-S\u00e1nchez", "D. Pulido-Vel\u00e1zquez"], "title": "Coupling machine-learning techniques with swat model for instantaneous peak flow prediction", "venue": "Biosystems Engineering, 177 :67 \u2013 77,", "year": 2019}, {"authors": ["S. Sengupta", "S. Basak", "P. Saikia", "S. Paul", "V. Tsalavoutis", "F. Atiah", "V. Ravi", "A. Peters"], "title": "A review of deep learning with special emphasis on architectures, applications and recent trends, 2019", "year": 2019}, {"authors": ["S. Shalev-Shwartz", "S. Ben-David"], "title": "Understanding Machine Learning : From Theory to Algorithms", "venue": "Cambridge University Press,", "year": 2014}, {"authors": ["L. Sirovich"], "title": "Turbulence and the dynamics of coherent structures : I, ii and iii", "venue": "Quarterly Applied Mathematics, 45 :561,", "year": 1987}, {"authors": ["C. Soize", "R. Ghanem"], "title": "Physical systems with random uncertainties : chaos representations with arbitrary probability measure", "venue": "SIAM J. Sci. Comput., pages 26(2), 395\u2013410,", "year": 2004}, {"authors": ["T. Sruthi", "K. Ranjith", "V. Chandra"], "title": "Control of sediment entry into an intake canal by using submerged vanes", "venue": "AIP Conference Proceedings, volume 1875, page 030007. AIP Publishing LLC,", "year": 2017}, {"authors": ["B. Sudret"], "title": "Global sensitivity analysis using polynomial chaos expansions", "venue": "Reliability Engineering & System Safety, 93(7) :964 \u2013 979,", "year": 2008}, {"authors": ["B. Sudret"], "title": "Polynomial chaos expansions and stochastic finite element methods, page 624", "venue": "CRC PressEditors : Kok-Kwang Phoon, Jianye Ching, 12", "year": 2014}, {"authors": ["K. Taira", "S.L. Brunton", "S.T.M. Dawson", "C.W. Rowley", "T. Colonius", "B.J. McKeon", "O.T. Schmidt", "S. Gordeyev", "V. Theofilis", "L.S. Ukeiley"], "title": "Modal analysis of fluid flows : An overview", "venue": "AIAA Journal, 55 (12) :4013\u20134041,", "year": 2017}, {"authors": ["A. Tarakanov", "A.H. Elsheikh"], "title": "Regression-based sparse polynomial chaos for uncertainty quantification of subsurface flow models", "venue": "Journal of Computational Physics, 399 :108909,", "year": 2019}, {"authors": ["E. Torre", "S. Marelli", "P. Embrechts", "B. Sudret"], "title": "Data-driven polynomial chaos expansion for machine learning regression", "venue": "Journal of Computational Physics,", "year": 2019}, {"authors": ["M. Tsang", "D. Cheng", "Y. Liu"], "title": "Detecting statistical interactions from neural network weights", "venue": "arXiv preprint arXiv :1705.04977,", "year": 2017}, {"authors": ["L.C. van Rijn"], "title": "Unified view of sediment transport by currents and waves. i : Initiation of motion, bed roughness, and bed-load transport", "venue": "Journal of Hydraulic Engineering,", "year": 2007}, {"authors": ["J. VanderPlas", "A.J. Connolly", "Z. Ivezic", "A. Gray"], "title": "Introduction to astroml : Machine learning for astrophysics", "venue": "2012 Conference on Intelligent Data Understanding, pages 47\u201354, Oct", "year": 2012}, {"authors": ["X. Wan", "G. Karniadakis"], "title": "An adaptive multi-element generalized polynomial chaos method for stochastic differential equations", "venue": "Journal of Computational Physics,", "year": 2006}, {"authors": ["Q. Wang", "J.S. Hesthaven", "D. Ray"], "title": "Non-intrusive reduced order modeling of unsteady flows using artificial neural networks with application to a combustion problem", "venue": "Journal of Computational Physics, 384 :289 \u2013 307,", "year": 2019}, {"authors": ["N. Wiener"], "title": "The homogeneous chaos", "venue": "American Journal of Mathematics, pages 60, 897\u2013936,", "year": 1938}, {"authors": ["S. Wilkinson", "S. Hanna", "L. Hesselgren", "V. Mueller"], "title": "Inductive aerodynamics", "venue": "Proceedings of eCAADe 2013 : Computation and Performance. pp.39-48., 09", "year": 2013}, {"authors": ["J.A. Witteveen", "H. Bijl"], "title": "Modeling Arbitrary Uncertainties Using Gram-Schmidt Polynomial Chaos", "year": 2006}, {"authors": ["D. Xiu", "G. Karniadakis"], "title": "Modelling uncertainty in steady state diffusion problems via generalized polynomial chaos", "venue": "Comput. Methods Appl. Mec. Engrg, pages 191(43), 4927\u20134948,", "year": 2003}, {"authors": ["D. Xiu", "G.E. Karniadakis"], "title": "The wiener\u2013askey polynomial chaos for stochastic differential equations", "venue": "SIAM Journal on Scientific Computing, 24(2) :619\u2013644,", "year": 2002}, {"authors": ["D. Xiu", "G.E. Karniadakis"], "title": "Modeling uncertainty in flow simulations via generalized polynomial chaos", "venue": "Journal of Computational Physics, 187(1) :137 \u2013 167,", "year": 2003}, {"authors": ["L. Zhu", "W. Zhang", "J. Kou", "Y. Liu"], "title": "Machine learning methods for turbulence modeling in subsonic flows around airfoils", "venue": "Physics of Fluids, 31(1) :015105,", "year": 2019}], "sections": [{"text": "Physically interpretable machine learning algorithm on\nmultidimensional non-linear fields\nRem-Sophia Mouradi1,2, Ce\u0301dric Goeury1, Olivier Thual2,3, Fabrice Zaoui1, and Pablo Tassi1,4\n1EDF R&D, National Laboratory for Hydraulics and Environment (LNHE), 6 Quai Watier,"}, {"heading": "78400 Chatou, France", "text": "2Climate, Environment, Coupling and Uncertainties research unit (CECI) at the European Center for Research and Advanced Training in Scientific Computation (CERFACS), French\nNational Research Center (CNRS), 42 Avenue Gaspard Coriolis, 31820 Toulouse, France 3Institut de Me\u0301canique des Fluides de Toulouse (IMFT), Universite\u0301 de Toulouse, CNRS,\nToulouse, France 4Saint-Venant Laboratory for Hydraulics (LHSV), Chatou, France"}, {"heading": "29 mai 2020", "text": "Re\u0301sume\u0301\nIn an ever-increasing interest for Machine Learning (ML) and a favorable data development context, we here propose an original methodology for data-based prediction of two-dimensional physical fields. Polynomial Chaos Expansion (PCE), widely used in the Uncertainty Quantification community (UQ), has recently shown promising prediction characteristics for one-dimensional problems, with advantages that are inherent to the method such as its explicitness and adaptability to small training sets, in addition to the associated probabilistic framework. Simultaneously, Dimensionality Reduction (DR) techniques are increasingly used for pattern recognition and data compression and have gained interest due to improved data quality. In this study, the interest of Proper Orthogonal Decomposition (POD) for the construction of a statistical predictive model is demonstrated. Both POD and PCE have widely proved their worth in their respective frameworks. The goal of the present paper was to combine them for a field-measurement-based forecasting. The described steps are also useful to analyze the data. Some challenging issues encountered when using multidimensional field measurements are addressed, for example when dealing with few data. The POD-PCE coupling methodology is presented, with particular focus on input data characteristics and training-set choice. A simple methodology for evaluating the importance of each physical parameter is proposed for the PCE model and extended to the POD-PCE coupling."}, {"heading": "1 Introduction", "text": "Deep Learning techniques (DL [29, 59]) and more generally Machine Learning (ML [79, 95]), and their applications to physical problems (fluid mechanics [9, 51, 67, 74] ; aerodynamics [110, 115] ; plasma physics [28, 83] ; astrophysics and astronomy [50, 106] ; particle physics [2] ; quantum mechanics [70], geosciences [27, 46, 86, 88, 92]) have made a promising take-off in the last few years. This has been particularly the case for fields where the measurement potential has dramatically increased, with increasing spatiotemporal resolution (e.g. Geoscience Data [46] ; the new SWOT satellite mission [71, 78]). In particular, multi-layer Neural Networks (NN) [91] are widely used for these applications. These techniques are of interest when data are available for a real-world problem that is difficult to model using process-based equations, because prescribing the underlying relationships is complex [95]. For such cases, the learning procedure is performed through a combination of steps: Encoding, Hidden Layers or Latent Representation, Decoding [59, 95]. In particular, transformation functions, also called Activation Functions (AFs), are used jointly with weight matrices, to transform the data from one layer to another. The popularity of NN comes from this complex structure, which makes them adaptable for various applications [1, 94]. Indeed, the combination of AF transformations helps capture complex interactions and non-linearities, making NN a widely used approach [95].\nHowever, some limitations prevent the use of NN for physical applications.\nar X\niv :2\n00 5.\n13 91\n2v 1\n[ ph\nys ic\ns. co\nm p-\nph ]\n2 8\nM ay\n2 02\n0\n(i) It is difficult to provide an explicit input-to-output formulation, due to the combinations and transformations involved. Physical interpretation of the constructed model is therefore tedious. This is why NN have been applied in physics often as a black-box and rather to optimize the accuracy of prediction than to extract physical information [41]. (ii) The calibration of the NN model involves various choices, such as the number of layers, number of neurons, the AFs, the weight matrices, etc. The number of hyper-parameters grows with the number of neurons and layers (curse of dimensionality), which limits the maximum complexity and therefore the interest of NN for highly non linear problems [79]. The choice of AF is application and data-set dependent [55]. (iii) The theoretical ability of NN to approximate functions has been proven with the fundamental Universal Approximation Theorem either with a limited number of layers [5, 12, 18, 23, 38] or with a particular type of AFs (ReLu functions in [13, 21, 34, 64, 87]). This limits the utility of NN for non-linear physical applications [1, 94]. Consequently, simple networks (with few layers) [41] are often used for physical problems, and the necessary assumptions on the AFs are often overlooked in favor of efficiency [55].\nTo overcome these limitations, we propose an alternative ML, based on a coupling between Proper Orthogonal Decomposition (POD) [14] and Polynomial Chaos Expansion (PCE) [57, 58], suitable for the prediction of spatiallydistributed physical fields. Here, POD is used for both Encoding and Decoding whereas PCE is used as a Latent Representation, as represented in Figure 1.\nThe proposed POD-PCE addresses these drawbacks of ML. (i) It is explicit and simple to implement, as it consists of the association of two linear decompositions. POD is\na linear separation of the spatiotemporal patterns [66], shown to be accurate for both linear and non-linear problems [101], combining and relevance. PCE is a well-established method in Uncertainty Quantification (UQ) [65, 100, 107, 112, 114], widely used for the study of stochastic behavior in physics [8, 45, 80, 102]. It is a linear polynomial expansion that allows non-linearities to be gradually added to the model by increasing the polynomial degree. The linearity and orthonormality of the POD and PCE components and the probabilistic framework of PCE make the output\u2019s statistical moments easier to study [40, 49, 63, 99], enabling straightforward physical interpretation of the model [10, 24, 52]. (ii) It only has two hyper-parameters: a number of POD components, and a PCE polynomial degree. Both can be chosen according to quantitative criteria [6, 14]. All other forms of parameterization (choice of the polynomial basis) can be achieved with robust physical and/or statistical arguments [65, 97, 107], as assessed in the present paper. Furthermore, the orthonormality of the POD and PCE bases minimizes the number of components necessary to capture essential variations in data. Additionally, the POD modes capture more energy than any other decomposition [16, 73], PCE is known to exponentially converge with polynomial degree [58, 113], and the cardinality of the latter can be reduced by sparse basis selection [6]. (iii) It can be considered as a universal expansion for physical field approximation: a physical field has a finite variance, which implies that it belongs to the Hilbert space of random variables with finite second order moments. There therefore exists a numerable set of orthogonal random variables, that form the basis\nof this Hilbert space, on which the field of interest can be expanded (strict equality, not approximation) [100]. A mathematical setting for basis construction based on input was established by Soize and Ghanem [97] for the general case of dependent variables with arbitrary density, provided that the set of inputs is finite.\nThe present study consisted in: i) combined use of POD and PCE in ML for point-wise prediction ; ii) application to field data with the inherent challenges not encountered with numerical data (e.g. paucity) ; iii) a focus on model explicitness as a key condition for physical understanding and iv) the influence of forcing variables study, based on a classical measure of importance (Garson weights [25]) directly computed with the POD-PCE expansion coefficients. Firstly, associating regression techniques to POD, and more generally to Reduced Order Models (ROM), is not novel [32, 53, 76, 108]. The cited studies, however, focused on dimensionality reduction, whereas the explicit formulation and applicability to complex physical processes are emphasized in the present study. Secondly, coupling PCE to ROM was recently addressed [36, 54, 77] and the use of PCE as ML is consistent with the methodology proposed by Torre et al. [103], where they showed that PCE is as powerful as classical ML techniques, but neither spatiotemporal fields nor physical interpretability were addressed. The data in these studies were either obtained from numerical experiments, emulated from analytical benchmark functions such as Sobol or Ishigami, or based on one-dimensional data sets [103]. Our methodology is assessed on high-resolution two-dimensional field measured data.\nThe assessment of the proposed methodology was based on the study of sedimentation processes in a cooling water intake located in a coastal area, subject to tide and wave forcing. This application in geosciences is characterized by high non-linearities and is conditioned a priori with various parameters [4]. The disparity of the space and time scales and the interaction between various processes make process-based modeling difficult [4, 82]. A complete overview of the use of ML in coastal sediment transport modeling is given in [27].\nThe paper is organized as follows. Section 2 gives a detailed explanation of the methodology. Section 3 deals with assessment of the methodology based on a physical application. Firstly, the study case and data are described in 3.1. POD and PCE performances are then demonstrated independently in 3.2 with a deep physical analysis using adequate measures. The performance of the POD-PCE predictor is discussed in 3.3. A summary of the study and perspectives of the proposed methodology are presented in Section 4."}, {"heading": "2 Theoretical framework", "text": "In this section, the objective is to define the framework of the proposed POD-PCE Machine Learning methodology, along with physical influence indicators for the inputs. This is the object of Subsection 2.3, but first, a reminder of the existing POD and PCE theoretical bases is presented in 2.1 and 2.2 respectively."}, {"heading": "2.1 Proper Orthogonal Decomposition", "text": "POD is a dimensionality reduction technique [66] that is well documented in literature [14, 61, 62, 101]. Theoretical details and demonstrations can be found in descending chronological order in [3, 16, 73]. For clarity\u2019s sake, the essential elements of POD are summarized below.\nThe goal of POD is to extract the spatial patterns of a continuous time and space function. These patterns, when added and multiplied by appropriate temporal coefficients, explain the dynamics of the variable of interest: a real-valued physical field.\nLet u : \u2126 \u00d7 T \u2192 D be a continuous function of two variables (x, t) \u2208 \u2126 \u00d7 T. The following relationships and properties hold for any \u2126 \u00d7 T and Hilbert space D characterized by its scalar product (. , .)D and induced norm ||.||D. However, and as is the case for a majority of physical fields, we shall consider \u2126 as a set of spatial coordinates (e.g. R2 or R3), T as a set of time coordinates (e.g. R+ or subset [0, T ]), and D as a set of scalar real values or vector real values (e.g. R or R2). POD consists in an approximation of u(x, t) at a given order d \u2208 N (Lumley [66]) as in Equation 1,\nu(x, t) \u2248 d\u2211 k=1 vk(t)\u03c6k(x) , (1)\nwhere {vk(.)}dk=1 \u2282 C(T,R) and {\u03c6k(.)}dk=1 \u2282 C(\u2126,D), with C(A,B) denoting the space of continuous functions defined over A and arriving at B. The objective of POD is to identify {\u03c6k(.)}dk=1 that minimizes the distance of the approximation from the true value u(., .), over the whole \u2126 \u00d7 T domain, with an orthogonality constraint for {\u03c6k(.)}dk=1 using the scalar product (. , .)D. This can be defined, in the least-squares sense, as a minimization problem.\nThe minimization problem is defined for all orders d \u2208 N, so that the members \u03c6k are ordered according to their importance. In particular, for order 1, \u03c61 is the linear generator of the sub-vector space most representative of u(x, t) in D. For D = Im(u), the family {\u03c6k(.)}dk=1 is called the POD basis of D of rank d. The solution to this problem has already been established in literature [66, 96]. The theoretical aspects of POD and demonstrations of mathematical properties can, for example, be found in [73]: the POD basis of D of order d is the orthonormal set of eigenvectors of an operator R : D\u2192 D defined as R\u03c6 = \u3008(u,\u03c6)D \u00d7 u\u3009T, if the eigenvectors are taken in decreasing order of the corresponding eigenvalues {\u03bbk}dk=1.\nFor this expansion, an accuracy rate, also called the Explained Variance Rate (EVR), denoted ed at rank d, can be calculated as in Equation 2 . EVR tends to 1 (perfect approximation) when d\u2192 +\u221e.\ned = \u2211 k\u2264d \u03bbk\u2211+\u221e k=1 \u03bbk . (2)\nIn practice, for D = R, when u(., .) is a discrete sample on a set of m \u2208 N space coordinates X = {x1, . . . ,xm} and for n \u2208 N measurement times T = {t1, . . . , tn}, the available data set is arranged in a matrix U(X , T ) = [u(xi, tj)]i,j \u2208 Rm\u00d7n, called the snapshot matrix, so as to be able to work in a discrete space. The POD problem formulated in Equation 1 can be written in its discrete form as U(X , T ) = \u03a6(d)(X )V(d)(T ), where \u03a6(d)(X ) := [\u03c6j(xi)]i,j \u2208 Rm\u00d7d and V(d)(T ) := [vi(tj)]i,j \u2208 Rd\u00d7n. The problem can therefore be viewed as if working with a new function U(X , .) = [u(xi, .)]i\u2208{1,...,m} : T \u2192 D = RM . Then, the average over T can be defined as the statistical mean over the subset T , and the scalar product (. , .)D as the canonical product over Rm. The POD operator R can be written as in Equation 3,\nR\u03c6(X ) = 1 n n\u2211 j=1 U(X , tj)T\u03a6(X )U(X , tj) = 1 n U(X , T )U(X , T )T\u03a6(X ) , (3)\nwhere U(X , tj) = [u(xi, .)]i\u2208{1,...,m} is the column number j of the matrix U(X , T ) (i.e the measurement over X at time tj), and \u03a6(X ) = [\u03c6(xi)]i\u2208{1,...,m}. As finding the POD basis is equivalent to identifying the orthonormal set of eigenvectors of the operator R, then for this discrete representation the problem becomes equivalent to solving the eigen problem of the matrix R := 1nU(X , T )U(X , T )\nT , called the covariance matrix. A number d \u2208 N of eigen vectors \u03a6(X ) are identified and stored in the columns of the matrix \u03a6(d)(X ). For the eigenvalues of the covariance matrix R denoted {\u03bbk}dk=1, the expansion in Equation 1 can also be written as in Equation 4, where {\u03c6k(.)}dk=1 together with {ak(.)}dk=1 are bi-orthonormal, and vk(.) = ak(.) \u221a n\u00d7 \u03bbk.\nu(x, t) \u2248 d\u2211 k=1 ak(t) \u221a n\u00d7 \u03bbk\u03c6k(x) . (4)\nBy defining the matrix A(d)(T ) := [ai(tj)]i,j \u2208 Rd\u00d7n and the operator D(d)(\u03bb1, ..., \u03bbd) corresponding to the diagonal matrix of elements \u03bbi, we have U(X , T ) = \u03a6(d)(X )D(d)( \u221a n\u00d7 \u03bb1, ..., \u221a n\u00d7 \u03bbd)A(d)(T ). Therefore the transposed form is U(X , T )T = A(d)(T )TD(d)( \u221a n\u00d7 \u03bb1, ..., \u221a n\u00d7 \u03bbd)\u03a6(d)(X )T . Thanks to the orthonormality of {ak(.)}dk=1, the covariance matrix reads R = 1n\u03a6 (d)(X )D(d)(n\u00d7\u03bb1, ..., n\u00d7\u03bbd)\u03a6(d)(X )T = \u03a6(d)(X )D(d)(\u03bb1, ..., \u03bbd)\u03a6(d)(X )T .\nWhen n << m, it is more computationally efficient to solve the eigenproblem of RT instead of the eigenproblem of R as highlighted by Sirovich [96] . This is often the case when a two-dimensional physical field is measured over a domain at specific times, and is the case encountered for our application described in Section 3.\nWhen an order d << min(m,n) corresponds to a high EVR as defined in Equation 2, we speak of dimensionality reduction, because the data are projected in a sub-space that is of much smaller dimension than Rm\u00d7n. When diverse enough records are available for the variable under study, we may consider that the resulting POD basis {\u03c6k(X )}dk=1 = {[\u03c6k(xi)]i\u2208{1,...,m}}dk=1 is a generator of all possible states. Predicting the associated temporal\ncoefficients {ak(t)}dk=1 at a given time t would therefore be enough to predict the whole state at time t. Hence, we propose to use the POD as a spatial basis extractor. This would first enable study of the spatial dynamics of the variable of interest and eventually extraction of physical information, as shown in the application Section 3. Then, the basis can be used as a generator for the prediction of future states. This implies predicting the evolution of {ak(t)}dk=1, for which we propose to use Polynomial Chaos Expansion (PCE), as described in the following Section 2.2."}, {"heading": "2.2 Polynomial Chaos Expansion", "text": "A reminder of the theoretical base of PCE is presented in Subsection 2.2.1. Theoretical details, demonstrations and interesting references can be found in [6, 57, 58, 99, 114]. After the theoretical introduction, a simple indicator is proposed in Subsection 2.2.2 for the analysis of the variables influence on the output value. The latter is later generalized for POD-PCE in Section 2.3."}, {"heading": "2.2.1 Learning", "text": "The idea behind Polynomial Chaos Expansion (PCE) is to formulate an explicit model that links a variable of interest (output) to conditioning parameters (inputs), both in a probability space. This enables the propagation path of probabilistic information (uncertainties, occurrence frequencies) to be mapped from the input space to the output space. The variable of interest, Y, and the input parameters \u0398 = (\u03b81, \u03b82, ..., \u03b8V ) are therefore considered random variables, characterized by a given Probability Density Function (PDF) denoted f\u0398. It should be kept in mind that the outputs of our problem are the temporal coefficients Y = [ak(t)]k\u2208{1,...,d} generated by POD, and that among the inputs there can be a set of physical forcings, as described later in Section 3. The objective is to derive the evolution of the temporal coefficients as the outcome of the forcings. Let us now recall some fundamentals of the mathematical probabilistic framework, taking the example of a one dimensional real-valued variable. The definitions can be easily extended to RM .\nLet (\u2126, F,P) be a probability space, where \u2126 is the event space (space of all the possible events \u03c9) equipped with \u03c3-algebra F (some events of \u2126) and its probability measure P (likelihood of a given event occurrence). A random variable defines an application Y (\u03c9) : \u2126 \u2192 DY \u2286 R, with realizations denoted by y \u2208 DY . The PDF of Y is a function fY : DY \u2192 R that verifies P(Y \u2208 E \u2286 DY ) = \u222b E fY (y)dy. The k th moments of Y are defined\nas E[Y k] := \u222b DY ykfY (y)dy, the first being the expectation denoted E[Y ]. In the same manner, we define the kth central moments of Y as E[(Y \u2212 E[Y ])k], the first being 0 and the second the variance of Y denoted by V[Y ]. The covariance of two random variables is defined as cov(X,Y ) = E[(X \u2212 E[X])(Y \u2212 E[Y ])] and a resulting property is V[Y ] = cov(Y, Y ).\nReturning to the PCE construction, inputs \u0398 = (\u03b81, \u03b82, ..., \u03b8V ) are considered to live in the space of real random variables with finite second moments (and finite variances). This space is denoted by L2R(\u2126, F,P;R) and is a Hilbert space equipped with the inner product (\u03b81, \u03b82)L2R := E[\u03b81\u03b82] = \u222b \u2126 \u03b81(\u03c9)\u03b82(\u03c9)dP(\u03c9) and its induced norm\n||\u03b81||L2R := \u221a\nE[\u03b821]. The PCE objective is to map the output space from the input space with a model M as in Equation 5:\nY = M(\u0398) = \u2211 I\u2286{1,...,V }MI(\u03b8I)\n= M0 + \u2211V i=1Mi(\u03b8i) + \u2211 1\u2264i<j\u2264V Mi,j(\u03b8i, \u03b8j) + ...+M1,...,V (\u03b81, \u03b82, ..., \u03b8V ) ,\n(5)\nwhere M0 is the expectation of Y and MI\u2286{1,...,V } represents the common contribution of the variables I \u2286 {1, ..., V } to the variation in Y . For the PCE model, these contributions have a polynomial form. We shall define, for each input variable \u03b8i, an orthonormal univariate polynomial basis { \u03be (i) \u03b2 (.), \u03b2 \u2208 [|0, p|] } where p \u2208 N is a chosen polynomial degree and \u03be (i) \u03b2 (.) is of degree \u03b2. The orthonormality is defined with respect to the inner product (., .)L2R .\nIf we introduce the multi-index notation \u03b1 = (\u03b11, ..., \u03b1V ) \u2208 NV so that |\u03b1| = \u2211V i=1 \u03b1i, we can define a multivariate\nbasis { \u03b6\u0398\u03b1 (.), |\u03b1| \u2208 [|0, p|] } as \u03b6\u0398\u03b1 (\u03b81, \u03b82, ..., \u03b8V ) := \u220fV i=1 \u03be (i) \u03b1i (\u03b8i). Therefore, the model in Equation 5 can be written as: Y =M(\u0398) =\n\u2211 |\u03b1|\u2264P c\u03b1\u03b6 \u0398 \u03b1 (\u03b81, \u03b82, ..., \u03b8V ) , (6)\nwhere c\u03b1 \u2208 R are deterministic coefficients that can be estimated thanks to different methods. It can be formulated as a minimization problem, and regularization methods can be used when dealing with small data sets. In the present study, we used the Least Angle Regression Stagewise method (LARS) in order to construct an adaptive sparse PCE. In this approach, a collection of possible PCE, ordered by sparsity, is provided and an optimum can be chosen with an accuracy estimate. It was performed in this study using corrected leave-one-out error [7]. The reader can refer to the work of Blatman [6] for further details on LARS and more generally on sparse constructions.\nThe choice of the basis is crucial and is directly related to the choice of input variable marginals, via the inner product (., .)L2R . Chaos polynomials were first introduced in [109] for input variables characterized by Gaussian distributions. The orthonormal basis with respect to this marginal is the Hermite polynomials family. Later, other Askey scheme hypergeometric polynomial families were associated to some well-known parametric distributions [113]. For example, the Legendre family is orthonormal with respect to the Uniform marginals. This is called gPC (generalized Polynomial Chaos) when variables of different PDFs are used as inputs. In practice however, the input distributions of physical variables can be different from usual parametric marginals. In such cases, the marginals can be inferred by empirical methods such as the Kernel Smoother (see [35] for theoretical elements). In this case, an orthonormal polynomial basis with respect to arbitrary marginals can be built with a Gram-Schmidt orthonormalization process as in [111] or via the Stieltjes three-term recurrence procedure as in [107].\nTo highlight the importance of the marginals and choice of polynomial basis for the learning process, several configurations are attempted in Section 3. Different input sets and distributions (Gaussian, Uniform, inferred by Kernel Smoothing) were tested. The influence of the polynomial basis on the ML is investigated in Section 3.2.2."}, {"heading": "2.2.2 Physical importance measures", "text": "Once the PCE construction is achieved, a physical interpretation can be performed. It is notable that classical NN indicators can be used [25]. The PCE can be represented in the Feedforward NN paradigm as in Figure 2. Such networks are classically composed, in addition to the input and output layers, of successive hidden layers. Each hidden layer is composed of neurons that transform the variables of the previous layer (outputs of the previous neurons) into a new set of variables. This is done by combining a linear transformation, giving different weights to the previous neurons, and a transformation function, called Activation Function (AF). This succession of layers is called the latent representation. For a number of hidden layers L \u2265 1, the latent representation, in addition to the input and output layers, can be formally written as Y \u2248 fout(AL fL(. . .A2 f2(A1 f1(Ain \u0398)))) where {Ak}k\u2208[|1,L|] and {fk}k\u2208[|1,L|] are the hidden layer weight matrices and AFs, Ain is the input-to-hidden connection matrix and fout is the final hidden-output transformation [59, 95].\nThe PCE-based NN represented in figure 2 is a single layer feedforward, composed of l \u2208 N neurons, that can be written as Y \u2248 fout(A1 f1(Ain\u0398)). The first matrix Ain is the input-to-hidden connection matrix of dimension V \u00d7 V , that links the input layer to the PCE hidden layer containing the multivariate polynomials{ \u03b6\u0398\u03b1 ,\u03b1 \u2208 {\u03b11, ...,\u03b1l} } , where V is the number of inputs and the multivariate indexes {\u03b11, ...,\u03b1l} are conditioned by the chosen polynomial degree p such as \u2200i \u2208 [|1, l|] 0 \u2264 |\u03b1i| \u2264 p, and by the number of selected features if a sparse polynomial is constructed, as in the present case using LARS [6].\nMatrix Ain represents the contributions of the V variables to the multivariate polynomials { \u03b6\u0398\u03b1 ,\u03b1 \u2208 {\u03b11, ...,\u03b1l} } .\nIt is a diagonal matrix such that [Ain]j,j\u2208[|1,V |]2 is 0 if \u2200i \u2208 [|1, l|] (\u03b1i)j = 0 and 1 if not. The first multidimensional AF f1 is a vector of multivariate functions that transforms the set of selected inputs corresponding to [Ain]i,i\u2208[|1,V |]2 = 1 to the multivariate polynomials of the chosen basis (Hermite, Legendre, etc.) by tensor product over the univariate basis. The hidden layer weight matrix A1 gives different weights to the constructed polynomial features. It is a diagonal matrix composed of the PCE expansion coefficients such as [A1]i,j\u2208[|1,l|]2 = [c\u03b1i ]i\u2208[|1,l|].\nThe final hidden-output transformation fout is a summation. Figure 2 can also be presented differently: another hidden layer can be added to the PCE latent representation as Y \u2248 fout(A2 f2(A1 f1(Ain \u0398)). The first AF f1 would represent a transformation of each input variable to a list of monomials of degrees 1 to p (here, Ain is identity). The second AF f2 therefore represents the tensor product that transforms the different monomials to multivariate features, with A1 appropriately filled with zeros and ones, and [A2]i,j\u2208[|1,l|]2 = [c\u03b1i ]i\u2208[|1,l|].\nTo capture the importance of each feature, the Garson relative Weights (GW) defined in Equation 7 are a classical measure to quantify the relative importance of each neuron of the last hidden layer, and therefore of each polynomial pattern, for the output value [25, 39, 104].\nw\u03b6\u0398\u03b1 = |c\u03b1|\u2211\n0\u2264\u03b2\u22641 |c\u03b2| . (7)\nThis measure can be used to understand the importance given by the NN algorithm to the variables and their possible interactions, especially when using feature selection algorithms as LARS: \u201dfeature interactions [...] are created at hidden units with nonlinear activation functions, and the influences of the interactions are propagated layer-by-layer to the final output\u201d [104]. In the particular case of a polynomial expansion, the interpretation is straightforward, the importance of each variable alone corresponds to its monomials, and the importance of its interactions with other variables corresponds to the multivariate polynomials in which it is involved.\nFor the particular case of the orthonormal basis provided by PCE, the GW defined in 7 can be interpreted in terms of Pearson\u2019s correlations between output Y and the polynomial basis elements \u03b6\u0398\u03b1 denoted \u03c1Y, \u03b6\u0398\u03b1 , with \u03b1 6= (0, ..., 0). Indeed, Pearson\u2019s correlations \u03c1Y, \u03b6\u0398\u03b1 are defined as in Equation 8,\n\u03c1Y, \u03b6\u0398\u03b1 = E [ (Y \u2212 E(Y ))(\u03b6\u0398\u03b1 \u2212 E(\u03b6\u0398\u03b1 )) ]\u221a V(Y )V(\u03b6\u0398\u03b1 ) = c\u03b1\u221a\u2211\n1\u2264|\u03b2|\u2264p c 2 \u03b2\n, (8)\nthanks to the orthonormality of the basis with respect to the scalar product (. , .)L2R that guarantees: \u2022 E [ \u03b6\u0398\u03b1 ] = ( \u03b6\u0398\u03b1 , \u03b6 \u0398 \u03b2=(0,...,0) = 1 ) L2R = 0 ;\n\u2022 E [Y ] = (\u2211\n0\u2264|\u03b2|\u2264p c\u03b2\u03b6 \u0398 \u03b2 , \u03b6 \u0398 \u03b2=(0,...,0) ) L2R = c\u03b2=(0,...,0) ;\n\u2022 E [ Y, \u03b6\u0398\u03b1 ] = (\u2211 0\u2264|\u03b2|\u2264p c\u03b2\u03b6 \u0398 \u03b2 , \u03b6 \u0398 \u03b1 ) L2R = c\u03b1 ;\n\u2022 V [ \u03b6\u0398\u03b1 ] = E [( \u03b6\u0398\u03b1 \u2212 E [ \u03b6\u0398\u03b1 ])2] = E [( \u03b6\u0398\u03b1 )2]\n= ||\u03b6\u0398\u03b1 ||2L2R = 1 ; \u2022 V [Y ] = E [ (Y \u2212 E [Y ])2 ] = (\u2211\n1\u2264|\u03b2|\u2264p c\u03b2\u03b6 \u0398 \u03b2 , \u2211 1\u2264|\u03b2|\u2264p c\u03b2\u03b6 \u0398 \u03b2 ) L2R = \u2211 1\u2264|\u03b2|\u2264p c 2 \u03b2 .\nTherefore, the weights w\u03b6\u0398\u03b1 can also be computed as |\u03c1Y, \u03b6\u0398\u03b1 |/ \u2211\n1\u2264|\u03b2|\u2264p |\u03c1Y, \u03b6\u0398\u03b2 |. This means that they measure the relative importance of the basis element in the expansion of the output, in terms of linear correlation, regardless of the sign of the latter. These \u201drelative Pearson\u2019s correlations\u201d can be seen as a physical contribution since the PCE model is strictly linear.\nThe sum of the GW w\u03b6\u0398\u03b1 for all the polynomial features equals 1. This means that they allow {\u03b6\u03b1}|\u03b1|\u2264p to be ranked in terms of relative contribution to the output Y . The contributions can be analyzed either for each polynomial pattern separately, or for a single variable \u03b8i by adding all the polynomial shares related to this variable alone (1st Sobol indice analogy [99]) or by adding all the polynomial shares related to this variable and its interactions (total Sobol indice analogy [99])."}, {"heading": "2.3 POD-PCE based predictor", "text": "POD and PCE were introduced separately in Subsections 2.1 and 2.2 respectively. We are now fully equipped with the adequate theoretical basis and mathematical notations, to present the POD-PCE ML methodology for a data-based model learning of a physical spatiotemporal field. In this Subsection, we will first summarize the proposed approach, then the formal details of the coupling will be given with the definition of adequate accuracy measures. Finally the previously discussed importance measures will be generalized for the POD-PCE physical study.\nThe proposed POD-PCE ML consists of five steps, in a learning and a prediction phase, summed up as follows: \u2022 Learning phase: \u2217 POD basis construction: given a set of measurements stored in the snapshot matrix U(X , T ) = [u(xi, tj)]i,j \u2208\nRm\u00d7n, construct a spatial POD basis accordingly ; \u2217 PCE learning: construct PCE models that map each POD temporal coefficient, obtained in the previous\nstep along with the spatial basis, from time tj \u2208 T to time tj+1 \u2208 T , using the realizations of associated input variables denoted \u0398(tj \u2192 tj+1) on the set T ;\n\u2022 Prediction phase: \u2217 POD projection: given a new measurement of the physical field U(X , tk), obtain the values of the POD\ntemporal coefficients using appropriate projection ; \u2217 PCE prediction: given the constructed PCE models and an estimate of the inputs from current time tk to\nfuture time tk+1 denoted \u0398(tk \u2192 tk+1), calculate an estimation of the future POD temporal coefficients ; \u2217 POD-PCE ML prediction: reconstruct the estimate of the future state U(X , tk+1) by calculating the\nexpansion on the POD basis, using the estimate obtained for the future temporal coefficients."}, {"heading": "2.3.1 Machine learning methodology", "text": "Here, the formal hypothesis behind the POD-PCE ML reasoning and its mathematical formulation are discussed. Let U(X , .) = [u(xi, .)]i\u2208{1,...,m} be a field of interest defined on a set ofm \u2208 N space coordinates X = {x1, . . . ,xm}. Let \u0398(.) = (\u03b81(.), \u03b82(.), ..., \u03b8V (.)) be a vector of the inputs supposed to condition the evolution of U(X , .) over time. The dynamic model, denoted H, that gives an estimation of a future state U(X , tj+1) from a past state U(X , tj) and an estimation of \u0398(tj \u2192 tj+1) over the time interval [tj , tj+1], where tj < tj+1 \u2208 R+, is formulated as in Equation 9 .\nU(X , tj+1) \u2248 H [U(X , tj), tj+1 \u2212 tj ,\u0398(tj \u2192 tj+1)] . (9)\nIf the field of interest has been recorded over a set of past times T = {t1, . . . , tn} \u2282 R+, where tn < tj < tj+1, a POD basis can be constructed as in Section 2.1, consisting of d \u2208 N vectors of dimension m stored in a matrix as \u03a6(d)(X ) = (\u03a6(d)1 (X ), . . . ,\u03a6 (d) d (X )) \u2208 Rm\u00d7n, and can be seen as a generator of all possible states if enough records are available. If so, any future state U(X , tj) can be expanded on this POD basis and the associated temporal coefficients are simply the weights of U(X , tj) on the POD basis. They are therefore obtained using the canonical scalar product over Rm, as in Equation 10.\nU(X , tj) \u2248 \u2211d k=1 ak(tj) \u221a n\u00d7 \u03bbk\u03a6(d)k (X )\n\u2248 \u2211d k=1(U(X , tj),\u03a6 (d) k (X ))Rm\u03a6 (d) k (X )\n\u2248 \u2211d k=1 U(X , tj)T\u03a6 (d) k (X )\u03a6 (d) k (X ) .\n(10)\nHence, the variable part of U(X , tj) is fully expressed in the temporal coefficients ak(tj). The field of interest U(X , tj) can be either a field measurement, a laboratory or a numerical experiment. In any-case, it can be considered as being generated by a random process \u201din the sense that nature happens without consideration of what could be the best realizations for the learning algorithm\u201d [95]. Therefore, the coefficients ak(tj) can also be seen as the jth realization of a random variable Ak. We can therefore construct a PCE approximation Hk that maps Ak from its input space. The latter is taken as a collection of random variables, composed from the set (A1, ..., Ad) at a previous time, the duration of the dynamic, and the input variables \u0398(tj \u2192 tj+1). This is formulated as a classical dynamic model in Equation 11 .\nak(tj+1) \u2248 Hk [a1(tj), . . . , ad(tj), t2 \u2212 t1,\u0398(tj \u2192 tj+1)] . (11)\nThe model H in Equation 9 is approximated as in Equation 12 .\nH [U(X , tj), tj+1 \u2212 tj ,\u0398(tj \u2192 tj+1)] \u2248 d\u2211 k=1 Hk [a1(tj), . . . , ad(tj), tj+1 \u2212 tj ,\u0398(tj \u2192 tj+1)] \u221a n\u00d7 \u03bbk\u03a6(d)k (X ) . (12)\nThe choice of input variables for regression models is an ongoing research question in statistics [33, 81]. The chosen input variables can be extracted from a transformed version of a larger input set with the help of PCA [44] for DR. However, this approach was not studied here and will be the topic of a future study. Different input configurations will be evaluated, to investigate the influence of variable selection on the proposed learning. For example, the hypothesis of dependence between the random variables (A1, . . . , Ad) could be relaxed. This would imply writing the approximation in Equation 11 in a relaxed form as Hk [ak(tj), t2 \u2212 t1,\u0398(tj \u2192 tj+1)]. In that case a simpler model H, under the strong independence assumption, can be formulated as in Equation 13.\nH [U(X , tj), tj+1 \u2212 tj ,\u0398(tj \u2192 tj+1)] \u2248 d\u2211 k=1 Hk [ak(tj), tj+1 \u2212 tj ,\u0398(tj \u2192 tj+1)] \u221a n\u00d7 \u03bbk\u03a6(d)k (X ) . (13)\nBoth alternatives are tested in Section 3. To investigate the influence of input selection on learning accuracy, a quantitative evaluation of the hypothesis is needed. More generally, whether for the above-mentioned simplifications or for the approximated form of the model in general, accuracy estimators are needed. These are presented below."}, {"heading": "2.3.2 Accuracy tests for the approximation", "text": "There are two determining parts in the POD-PCE learning process. Firstly, the PCE learning Hk(.) of each mode Ak should be as accurate as possible. Secondly, the reconstructed field \u2211d k=1Hk(.) \u221a n\u00d7 \u03bbk\u03a6(d)k (X ) for a given rank d should be as close to the real field U(X ) as possible.\nThe distance between each mode and its PCE approximate can be evaluated using the generalization error, denoted \u03b4(Ak,Hk) and defined as in Equation 14.\n\u03b4(Ak,Hk) = E [ (Ak \u2212Hk(.))2 ] . (14)\nFor the model defined in Equation 13 , this error can be estimated, on a set of paired realizations (ak(t1), . . . , aj(tn)) and (\u0398(t0 \u2192 t1), . . . ,\u0398(tn\u22121 \u2192 tn)) , as in Equation 15 as explained by Blatman [6] . This approximated version of the generalization error is called the empirical error.\n\u03b4(Ak,Hk) \u2248 \u03b4emp(Ak,Hk) := 1\nn n\u2211 j=1 (ak(tj)\u2212Hk [ak(tj\u22121), t2 \u2212 t1,\u0398(tj\u22121 \u2192 tj)])2 . (15)\nIts relative estimate denoted emp(Ak,Hk) can be defined as in Equation 16 .\nemp(Ak,Hk) := \u03b4emp(Ak,Hk)\nV[Ak] . (16)\nOnce the PCE learnings can be trusted, the distance at time tj between the true state U(X , tj) and the PODPCE approximation H [U(X , tj), tj+1 \u2212 tj ,\u0398(tj \u2192 tj+1)] can be defined. It might be estimated using the relative Root Mean Squared Error (relative RMSE), denoted r[U,H](tj) and calculated as in Equation 17 , where h(xi, tj) refers to the value of the POD-PCE approximation at coordinate xi and time tj .\nr[U,H](tj) := m\u2211 i=1 (u(xi, tj)\u2212 h(xi, tj))2 [u(xi, tj)] 2 . (17)\nA mean value of the relative RMSE is calculated over a set of realizations corresponding to a set of times T = {t1, . . . , tn}. It is denoted r[U,H](T ) and estimated as in Equation 18.\nr[U,H](T ) := 1 n n\u2211 j=1 r[U,H](tj) . (18)\nOnce the accuracies of the PCE learnings and the POD-PCE coupling have been evaluated, a final model, which will be the most accurate one, can be chosen. This model would, for our ML set-up, be the best representation of the dependence structure between inputs and outputs. It is used to shed light on the underlying physical relationships. Therefore the inputs are ranked in terms of physical influence, using an appropriate ranking indicator, presented in the following Subsection."}, {"heading": "2.3.3 Physical influence of inputs based on the POD-PCE model", "text": "The GW influence measures presented for the PCE models in Subsection 2.2 are here extended for the PODPCE coupling. These indicators are adequate for the analysis of each PCE model Hk: i.e., for interpreting the contribution of the inputs to each random variable Ak separately. However, calculating the contributions to each Ak independently precludes putting them in perspective according to the importance of Ak in the final reconstructed model H that approximates U(X , .). Hence, adapted indicators should be calculated.\nLet U(X , .) be the random spatiotemporal field approximated by the POD-PCE ML, for prediction from time tj to time tj+1 and let Hk be the PCE approximation at degree p(k) that maps the random POD temporal coefficient Ak from a set of input variables, using the expansion on the multivariate polynomial basis { \u03b6 (k)(.) \u03b1 } |\u03b1|\u2264p(k) . The POD-PCE model formulated in Equation 12 is written as in Equation 19:\nU \u2248 d\u2211 k=1 Ak \u221a n\u00d7 \u03bbk\u03a6(d)k (X ) \u2248 d\u2211 k=1  \u2211 |\u03b1|\u2264p(k) c(k)\u03b1 \u03b6 (k) \u03b1 (.) \u221an\u00d7 \u03bbk\u03a6(d)k (X ) . (19) Thanks to its linearity, the POD-PCE ML can be represented as a single-layered NN, as shown in Figure 3.\nTherefore, a new indicator, Generalized Garson Weights (GGW), denoted W \u03b6 (k) \u03b1 , is computed and simply re-\nevaluated from the PCE Garson weights (GW), here denoted w \u03b6 (k) \u03b1 , as in Equation 20.\nW \u03b6 (k) \u03b1\n:= |c(k)\u03b1 | \u221a n\u00d7 \u03bbk\u2211d\ne=1 \u2211 |\u03b2|\u2264p(e) ( |c(e)\u03b2 | \u221a n\u00d7 \u03bbe ) = (\u2211 |\u03b2|\u2264p(k) |c (k) \u03b2 | ) w \u03b6 (k) \u03b1 \u221a \u03bbk\u2211d\ne=1 \u2211 |\u03b2|\u2264p(e) ( |c(e)\u03b2 | \u221a \u03bbe\n) =  \u2211|\u03b2|\u2264p(k) ( |c(k)\u03b2 | \u221a \u03bbk ) \u2211d e=1 \u2211 |\u03b2|\u2264p(e) ( |c(e)\u03b2 | \u221a \u03bbe ) w \u03b6 (k) \u03b1 .\n(20)\nThese GGW indicators show that the contribution of the polynomials {\u03b6(k)\u03b1 }|\u03b1|\u2264p(k) of Ak are enhanced with the eigenvalue \u03bbk, which is directly linked to the importance of the POD mode \u03a6 (d) k (X ) (EVR in Equation 2). An analogy can be drawn with the generalization of Sobol indices for a reduced order model [24, 52]. The property\n\u2211d k=1 \u2211 |\u03b1|\u2264p(k) W\u03b6(k)\u03b1\n= 1 holds. This means that the indices allow {{\u03b6(k)\u03b1 }|\u03b1|\u2264p(k)}k\u2208{1,...,d} to be ranked altogether in terms of contribution to output U. The contributions can be analyzed either for each polynomial pattern separately, or for a single variable \u03b8i by adding all the polynomial shares related to this variable alone or by adding all the polynomial shares related to this variable and its interactions (analogy with first and total Sobol indices respectively [99])."}, {"heading": "3 Application to a cooling water intake in a coastal zone", "text": "This section deals with the application of the POD-PCE ML described in Section 2 to a physical problem, introduced, with an industrial study case and inherent challenges, in Subsection 3.1. The physics and data are described. Subsection 3.2 deals with application of the POD-PCE learning phase to the data and assessment of accuracy and robustness with respect to the numerical choices (data set, inputs, marginals and polynomial basis). Finally, the prediction phase using POD-PCE is dealt with in Subsection 3.3, and the ability of the proposed ML to predict mean quantities and spatial details is demonstrated."}, {"heading": "3.1 Study case", "text": "Sedimentation processes in nearshore areas can be responsible for the excessive sediment deposition commonly observed in cooling water intakes in power plants. As a result, the carrying capacity of the water intake can be drastically reduced, by decreasing its effective area of transport [98].\nCooling water intakes usually incorporate jetties, of which the angle with the shoreline and position relative to the direction of the net longshore sediment transport influence the amount of sediments diverted into the channel inlet by waves and tidal currents. Jetties also reduces littoral drift, resulting in localized sediment accretion against the shore-normal structure due to the longshore sediment transport being trapped by the jetty [19]. In addition, a return current is prone to develop, in the form of a swirling vortex at the end of the structure, and can induce sediment deposition in the vicinity of the channel entrance, consequently affecting the amount sediment delivered into the cooling water intake [15].\nTherefore, effective water intake management involves frequent dredging, with high operational costs and usually hindered by a tight schedule. It is consequently necessary to assess intake sedimentation under different natural forcing and plant operation scenarios in order to optimize dredging operations to help mitigate the potentially adverse impact of the waves and tidal currents and meteorological forcing combined with plant functioning.\nSite characteristics\nThe study site is located on the eastern English Channel coast in northern France. Tide in the study zone is classified as mega-tidal and is dominated by semi-diurnal circulation, with low-tide water depth of 10 \u2212 15 m, and a mean tidal range of approximately 8.5 m, reaching 10 m during the spring tide [56]. Hydrodynamics are influenced by asymmetrical current velocities, with flood and ebb currents in the E-NE and W-SW directions, respectively. Current velocity at 2.2 m above seabed vary from 0.70 to 0.98 m/s, depending on flood/ebb phase, respectively [68]. Wave activity in this open exposed environment is moderate, with significant annual and decennial wave height of 3.8 m and 4.7 m, respectively, with maximum values of 4.2\u2212 5.8 m, averaged period of 7\u2212 9 s and a predominant W direction. Orbital velocities measured during the spring-tide period ranges between 0.5-1.3 m/s. An example of tidal levels, wind direction and velocity and wave height and direction in January 2016 is shown in Figure 4 . In the study area, bed sediment varies from medium to fine silted sands, with a morphology characterized by the presence of mega-rides parallel to the coast. In this zone, rock occupies less than 4% of bed surface [15].\nData\nHydrodynamic and meteorological information comprise wave height, period and direction and wind velocity and direction, provided by the VAG prediction model of the sea state [31], using retrospective 3-hourly simulations between 2009 and 2018. Tidal water levels were obtained from the SHOM-REFMAR tide gauge station located in the vicinity of the study zone, with hourly survey frequency [85].\nBathymetric measurements were available from Single-Beam Echo Sounding on 39 cross-sectional profiles of intake measured at 25 m intervals, collected fortnightly between 2005 and 2018. Mean profiles were 100 m long with 0.5 m spatial resolution of bathymetric data. Additional information such as the daily coolant flow rates, and channel dredging volumes and frequency, were provided by the plant operator. A summary of the data is shown in Appendix A.\nThe available measurements of the forcings did not have the same frequencies. One solution to homogenize frequencies consists in reducing the measured data to representative statistics over the sedimentation interval \u2206t \u2248 15 days separating two bed elevations measurements. Hence, the following statistics were used: \u2022 Tidal level indicators: average low tide (TLmean), minimum low tide (TLmin), maximum tidal range\n(TLrange) and standard deviation (TLstd) ; \u2022 Wind indicators: average wind velocity (Wmean) and average direction weighted by velocity (Wdir) ; \u2022 Wave indicators: average wave height (WvH), standard deviation (Wvstd), average wave period and ave-\nrage wave direction weighted by height (resp. Wvper and Wvdir), average wave height exceeding the 90th\npercentile (arbitrary storm indicator, Wv2m) and percentage occurence (Wv2m%) ; \u2022 Operational indicators: average pumping flowrate (Qmean) ; time lapse since last dredging (Dp), and last\ndredged volume (Dv).\nThese statistical indicators were calculated for each sedimentation interval ; a resulting scatter plot is shown in Figure 23. There was, for example, a positive correlation between mean low tide TLmean over the studied sedimentation periods and wave parameters Wvper and WvH. Mean wave periods Wvper and mean wave heights WvH were also positively correlated.\nFor the learning part, the data overlapped only over a limited period. A maximum of 60 measurements could therefore be used, with up to 15 forcing variables. Obviously, this \u201dsmall data\u201d configuration is a considerable handicap for the dimension of the problem, especially given that the variable of interest is a two-dimensional bathymetric field. However, permanent intake monitoring ensures that the data set will always grow and can be used to update the learning. This limitation shall not prevent testing the accuracy of the methodology on small sets\nsuch as are often encountered in physical applications, as attempted below, where learning and prediction using POD-PCE is applied to the described data. For the learning algorithm, input variables are needed, corresponding to the reduced statistical indicators described above, and denoted (\u03b81, ..., \u03b8V ), where V is the supposed dimension of the problem."}, {"heading": "3.2 Measurement-based learning of a physical field using POD and PCE", "text": "This section concerns learning the spatio-temporal bathymetric field using POD and PCE independently. The modes are extracted in Subsection 3.2.1 using POD and the temporal patterns are learned as a function of the forcing parameters using PCE in Subsection 3.2.2. Throughout this investigation, particular attention is given to the convergence of the learning and to its robustness with respect to the numerical choices. Trusted POD-PCE learning is immediately used for physical interpretation and the most important physical insights resulting from it are summarized in Subsection 3.2.3."}, {"heading": "3.2.1 Physical analysis and data reduction using POD", "text": "First, POD was applied on the bathymetry measurements. The aim was to identify morphodynamic patterns so as to better understand the sediment deposition inside the channel, and to characterize variations in depositions with the external forcing variables. After setting aside poor-quality measurements (e.g. incomplete bathymetries), a total n = 156 realizations were used. The bathymetry points were sonar boat measurements on mp = 39 crosssections inside the intake. Linear interpolation was performed on mi = 100 fixed points for each profile, in order to express all measurements on the same grid, giving a total m = mi \u00d7mp = 3, 900 spatial points. The interpolated realizations were then stored in a snapshot matrix Z(X , T ) = [z(xi, tj)]i,j \u2208 Rm\u00d7n and POD-processed as explained in Section 2.1. The EVR defined in Equation 2 and the mean relative RMSE between the POD approximation and the complete measurement (averaged over the realization set as in Equation 18 ) were calculated for each POD approximation rank and are plotted in Figure 5.\nThe first pattern represents over 94% of the variance, and explains most of the variation in dynamics. The variance percentage reached 99% at rank 14, where the mean error was slightly over 10%, decreasing to 8% at rank 20. Dimensionality reduction is therefore a realistic option for this specific dynamic problem. This encouraged the learning and prediction attempts undertaken in Subsections 3.2.2 and 3.3 respectively. The spatial and temporal components of the first four POD modes corresponding to an EVR higher than 97% are respectively plotted in Figures 6 and 7.\nThe first spatial pattern (Figure 6-a) represents the channel\u2019s slope. Its temporal coefficient (Figure 7-a) shows regularity in time that is almost periodicity. When it increased, overall sediment deposition in the channel increased, because the difference between the upstream and the downstream bed elevations, and therefore the slope, diminished. The sediment deposition in the channel might be related to the increasing sediment supply caused by the external forcing influence. Decrease always corresponded to a dredging episode. The apparent periodicity is therefore not natural or seasonal but due to periodicity of operational intervention: sediment deposition in the channel is tolerated up to a certain level and then dredging is always undertaken at a certain point, which corresponds to the maximum of the temporal coefficient. The second pattern (Figure 6-b) acts as a geometric distribution function of the sediment deposition. In general, when the first temporal coefficient was maximal, the second coefficient was positive, meaning that the sedimentation mainly occured in the middle of the first portion of the channel (upstream), on the right bank of the bend and on the left bank in front of the pumps. This spatial distribution can be associated to the internal flow characterized by a velocity distribution inside the channel. In fact, the sediments settle where velocity is the lowest, which is probably the case where the banks appear. The computed sediment deposition and erosional patterns are analogous to those commonly observed in meandering rivers [42].\nThe third pattern (Figure 6-c) shows sediment deposition concentrated in the first portion of the intake, and the fourth pattern (Figure 6-c) emphasizes sediment dynamics, particularly in the downstream part of the channel. This behavior is statistical proof and quantification of finer sediment supply. The finer sediment fraction was transported in suspension and deposited at the end of the intake channel. The temporal coefficients associated with the third and fourth mode (Figure 7-b) were less regular than those of the first and second mode, and seemed to follow a more stochastic dynamic. The peaks may represent unusual sediment supply, probably linked to extreme events (extreme tides, storms, etc.).\nTo check the robustness of the statistical conclusions deduced from POD, convergence analysis is necessary. This was performed on the EVR values associated with the first four patterns, using bootstrap analysis [20]. The results are shown in Figure 8. The convergence of the mean values and the tightening of the confidence intervals around the mean whith increasing matrix size are clear for these first four modes. However, whereas the confidence intervals represent at most an error of \u00b10.6% around the mean for the first mode, they reached respectively \u00b112.5%, \u00b125% and \u00b112.5% for the second, third and fourth modes.\nThe analysis proved that the POD results could be used to pursue the learning. Firstly, a high EVR and low\nRMSE were associated with a small number of modes, guaranteeing optimal data reduction (d << min(m,n) as explained in Section 2.1). The number of POD modes to accurately represent the bathymetry can be chosen accordingly. In the present study, the configuration was d = 11 modes (discussed in Step 3 ), guaranteeing EVR \u2265 98% and information loss \u2264 12% (mean relative RMSE). Secondly, the EVRs were guaranteed to converge statistically at least for the first four modes, with error of \u00b10.6% around the mean for the most important mode, representing over 94% of the variance. Thirdly, the deduced patterns were physically coherent. Lastly, more than a decade of evolution was used to extract the POD basis, under variable operational and environmental conditions. As long as the operating conditions of the intake remained unchanged, it can be assumed that a wide range of evolutions has been covered, except for extreme events that rarely occur and that are not specifically treated in this study [26]. Hence, the POD basis can be considered as a physically trustworthy and mathematically complete basis to understand past evolutions and to predict future ones. The learning of temporal coefficients is therefore attempted in Subsection 3.2.2."}, {"heading": "3.2.2 Learning of the POD patterns using PCE", "text": "The temporal coefficients calculated with the POD in Section 3.2.1 (Figure 7) were learned using PCE (theory in Section 2.2). The aim was to learn the way these coefficients evolve over time, as a function of the forcing parameters presented in Table 2 , with the ultimate objective of field prediction as explained in Section 2.3 and applied in Section 3.3. The present section focuses strictly on the learning phase and the physical analysis of the learned model, highlighting quality of learning (robustness, convergence, etc.).\nThe investigation of learning is organized in four steps. \u2022 Step 1 - Sensitivity of learning to inputs and marginals: different configurations were tested to practically de-\nmonstrate the implications of these choices on the accuracy of fit. \u2022 Step 2 - Convergence and Robustness of fit: The best model resulting from Step 1 was studied more deeply.\nIts convergence and robustness with respect to the choice of training members are were analyzed. \u2022 Step 3 - Physical interpretation of the best learned model: the best model was chosen, and the most influen-\ntial forcings were ranked using the Evolution Weights (EW) and Generalized Evolution Weights (GEW) presented in Sections 2.2 and 2.3 respectively.\n\u2022 Step 4 - Robustness of the physical interpretation with respect to the learning-set members: the physical conclusions of the model were shown to be statistically meaningful.\nThese steps, in the aboved order, follow the logic of statistical model construction to build a trustworthy prediction algorithm, used in Section 3.3.\nStep 1 - Sensitivity of learning to inputs and marginals Input variable selection is capital, and marginals must be chosen wisely. Below, we demonstrate the influence of these choices on the performance of the learning. Different configurations were tested.\nBefore introducing the tested configurations, the training steps that are common to all configuration need to be defined. A learning data-set is classically separated into different sub-sets, corresponding to different steps of the learning algorithm. This is commonly referred to as the \u201dTrain-Validation-Test Split\u201d [95]: a training set is used for the learning, a validation set is used to check the learning and for further calibration, and a test set is used to assess the prediction capability of the statistical model. However, the data-set used in this study was small: the bathymetry and forcings measurements shown in Subsection 3.1 overlap for the 2012-2017 period only, leaving 64 sedimentation periods to study. Therefore, only a \u201dTrain-Predict\u201d split was performed, where the prediction set played the role of both the test set and validation set. Hence, the numerical choices were calibrated on the training set, and validated on the prediction set for both statistical accuracy and physical prediction. The learning was then performed with an arbitrary choice of training-set size at 50, which left a prediction set of 14. The training data were chosen in chronological order (first 50 records), to mimic the learning process in an industrial context. This arbitrary training-set choice had consequences for learning ; the sensitivity of learning to training set choice was investigated (Step 2 ). All the model choices presented below (choice of inputs and marginals) were assessed on this training configuration. To assure that comparison is made between models at their best performances, the PCE polynomial degree was optimized for each separately. Degrees from 1 to 7 were tested, and the associated relative empirical errors on the training and prediction sets, respectively T and P , were calculated as in Equation 16 . The PCE degree that minimized the training and prediction errors for each model was chosen ; the corresponding\nresult is referred to as \u201doptimal\u201d learning.\nThree different input configurations were used for the learning of each temporal coefficient ai as generally formulated in Equation 11 .\n\u2022 Hi-model: a first simple configuration where all the statistical indicators described in Subsection 3.1 were used and an independence hypothesis between the POD temporal coefficients is considered. The model is written as in Equation 13 : ai(tj+1) \u2248 Hi [ai(tj), tj+1 \u2212 tj ,\u0398(tj \u2192 tj+1)]. This is a model of dimension 17. \u2022 HFi -model: a more complex configuration where a \u201dFull\u201d 15-mode POD approximation is considered with possible dependencies between the temporal coefficients. Of course, the choice of the basis size and the dependency structure can be optimized, but the objective here was to make a first step toward a more optimal configuration. The model can be written as: ai(tj+1) \u2248 HFi [a1(tj), . . . , a15(tj), tj+1 \u2212 tj ,\u0398(tj \u2192 tj+1)]. This is a model of dimension 31. \u2022 HPi -model: a smaller set of inputs, used by the operators to qualitatively evaluate sediment deposition\nrisk, was used. It corresponds to the six variables TLmean, WvH, Wvper, Wvdir, Wv2m and Wv2m%. This mimics the physical expertise that may be engaged when building a statistical model. It is written as ai(tj+1) \u2248 HPi [ ai(tj), tj+1 \u2212 tj ,\u0398P (tj \u2192 tj+1) ] , where \u0398P stands for the \u201dphysical\u201d. This is a model of dimension 8.\nTo these variable choices were associated three choices of marginals, conditioning the choice of the PCE orthonormal polynomial (Section 2.2).\n\u2022 Lgd: all the variables follow a Uniform PDF. The bounds of the marginal were set to the minimum and maximum chronological values \u00b11% as in [103]. The associated orthonormal polynomial basis is the Legendre family. \u2022 Hrm: all the variables have Gaussian marginals characterized by the empirical mean and variance deduced\nfrom the data. The associated orthonormal polynomial basis is the Hermite family. \u2022 Stlj: the marginals were inferred from the data using Gaussian Kernel density estimates. The orthonormal\npolynomial basis was constructed from the knowledge of the marginal using a Stieltjes orthogonalization.\nThe three marginal choices (Lgd, Hrm, Stlj) were trained with the three dimension choices (HPi dim=8, Hi dim=17, HFi dim=31). The empirical errors of the \u201doptimal\u201d learnings are compared in Figure 9 .\nFor Modes 1 and 2, training and prediction errors were almost identical for all configurations, although with a slight advantage with the smallest dimensions for all the marginal types in the learning of Mode 2. Starting from Mode 3, bigger differences emerged. At the learning step of Mode 3, models of dimension 17 and 31 were poorly fitted for the Stlj and Lgd configurations compared to others. At the prediction step of Mode 3, the errors of models with dimension 31 were much greater than smaller dimensions for all marginal choices. There seemed to be an overfitting of the model by selecting a larger number of inputs. The best models for Mode 3 were those of the\nsmallest dimension, 8, with either the Stlj or the Hrm model. Lastly, for Mode 4, two orderings were observed for the prediction error. Firstly, for each marginal choice, prediction error increased with dimension, which confirmed the overfitting hypothesis. Secondly, error was the smallest with the Stlj model (Kernel density), followed by the Hrm model (Gaussian) and lastly by the Lgd model (Uniform). Here, Uniform marginals performed worst ; they were probably too different from the real data marginals and did not account for particularities in the inputs. In the parametric family, Gaussian marginals probably fitted real density better.\nTo conclude this comparison, the best marginal choice was the Kernel density estimate. The smallest dimensions performed the best, with the Stlj choice for the polynomial basis. The HPi ;Stlj model of dimension 8 was therefore selected. However, the training was performed with an arbitrary split of the available statistical set. The sensitivity of the model to the learning set size and members is performed in the following step.\nStep 2 - Convergence and robustness of the fit Up to this point, an arbitrary number of 50 measurements was used for the training phase, leaving 14 prediction points for testing purposes. In the following, the influence of training set size on the learning and prediction error is assessed. The objective is to check the robustness of the previous best model HPi ;Stlj with respect to the data-set size and members. The evolution of the training and prediction empirical errors according to training set size is shown in Figure 10 and 11 respectively. For each training set size, members were chosen randomly among the full data-set, and the remaining members were used for the prediction phase. For the estimation of the confidence intervals, bootstrap analysis was performed [20]. For comparison, the convergence of the HPi ;Hrm model is plotted for Mode 3 and can be found in Appendix B for Modes 1 and 2.\nFor the first two modes, the training errors in Figure 10 show a convergence of the median value and a tightening of the confidence intervals. The trainings can be considered as converging from around training size 40.\nThe associated median prediction errors in Figure 11 globally decreased with increasing training set size. However, although the final median values were lower for the Hrm model, the confidence intervals were much larger than for the Stlj model. The latter seems much more robust with respect to changes in training scenario.\nThe residuals distributions of the HPi ;Stlj model, calculated as ai(.)\u2212HPi [.] on all the training sizes and Bootstraps, are shown in Figure 12 and Figure 13 for training and prediction, respectively. Only the middle 80% portion\nof the residuals range is plotted, in order to analyze the center of the distribution ; the full residuals distribution was long tailed, because the confidence intervals associated with small training set sizes were too large and produced extreme behaviors of the model.\nThe training residuals were generally centered around zero: i.e., the models are unbiased. A slight asymmetry was, however, observed for Mode 1, which means that a1(.) was more often overestimated by HPi ;Stlj. Consequently, the mean elevation in the channel and the mean global sedimentation may be slightly exaggerated. These exaggerations, however, remained within a reasonable range, as most of the residuals fell within the \u00b12% interval. The training residuals of Modes 2 and 3 were perfectly centered, but percentage error dramatically increased. Most of the residuals fell within the \u00b110% interval for Mode 2, whereas they reached \u00b150% for Mode 3. However, this error concerns modes that represent at most 4% of the total bathymetry variance, as more than 96% of the total variance was already captured by the addition of the first two modes.\nThe residuals shapes (i.e. slight overestimation for Mode 1 and perfect centering for Modes 2 and 3) were maintained through the prediction phase. Furthermore, the residuals mostly fell within the ranges identified in the training phase. HPi ;Stlj model behavior was stable. The prediction uncertainty could therefore be measured and trusted and the physical interpretation was consequently robust, as discussed below in Step 3.\nStep 3 - Physical interpretation of the best learned model The calibrated HPi ;Stlj PCE models were considered optimal, as they showed good fit, convergence and robustness with respect to the training choices. Here, they are analyzed to deduce physical information. Firstly, the optimal polynomial degrees selected for each mode and the associated training and prediction empirical errors are shown in Figure 14. Linear models were optimal for Modes 1 and 2 (degree 1), and the associated errors were low for both the training and the prediction sets.\nFor Modes 3 and 4, the optimal polynomial degrees increased, which implies higher-order contributions and/or higher-order interactions for the input variables. For modes of higher ranks, the models were either linear (degree 1) or approximated by a simple average value (degree 0). This means that LARS rejects polynomial terms of higher degrees because they do not significantly improve the learning [6]. Prediction relative empirical errors increased from Mode 3, but remained under 50% up to mode 11. This percentage error seems surprisingly high, but must be interpreted according to the meaning of the relative empirical error, which, as calculated in Equation 16, is a measure of the missing variations (distance between the model and reality) relative to the variance of the data. It is also often called \u201dthe fraction of unexplained variance\u201d [17], and represents the amount of variance that was not captured by the PCE model. For example, for mode 12, estimated with a degree 0 PCE model, 100% of the variance is not captured, which is natural because only the average value is accounted for with degree 0. For Mode 13, LARS did not include polynomial terms of degree higher than 1 (no improvement of the learning), but this appears to be a bad choice since the prediction error was much higher than the training error, meaning that the training set or inputs were not enough to guarantee accurate learning in Mode 13. For Modes 3 to 11 with error up 50%, this means that either the training set or the used inputs made it impossible to predict more than 50% of the variance. However, as presented in Step 2, this 50% error concerned at most 4% of the total bathymetry variance. Hence, the errors starting from Mode 3 represented at most 2% of missing variance. Beyond Mode 11, prediction with PE would not be optimal, as the prediction error dramatically increases.\nSecondly, the explicit formulations of the PCE models (reported in Appendix B) were used to analyze the contribution of each forcing variable to the dynamics. for this, the Garson Weights (GW) defined in Equation 7 were used to estimate the influence of the forcings on each temporal coefficient. The global influence on the whole bathymetry field was quantified using the Generalized Garson Weights (GGW), as in Equation 20.\nThe ranking of the modes and the impact of the inputs is represented in Figure 15. The inner circle represents the contribution of each mode to the whole field. Mode 1 corresponds to a major contribution and the following modes are ranked in terms of contribution, which is the essence of POD. The outer circle represents the contribution of each polynomial term in the PCE representation. The share of each polynomial term corresponds to the GGW in relation to the global contribution (full circle). When this share is compared to the importance of the corresponding mode, it corresponds to the GW. Lastly, the polynomial terms corresponding to more than 0.5% GGW are indicated. \u03b6\u03b1=(.)(.) corresponds to the notation introduced in Subsection 2.1, with the multi-index notation for \u03b1 that represents the polynomial degree of each monomial. For example, \u03b6\u03b1=(\u03b11,\u03b12)(\u03b81, \u03b82) corresponds to a polynomial of degree \u03b11 +\u03b12, where \u03b81 contributes as a monomial of degree \u03b11 and \u03b82 as a monomial of degree \u03b12. The meaning of the variables that appear in Figure 15 can be found in Subsection 3.1\nFor all the temporal coefficients ai(.), the most influential contributor by far was the value of the previous state ai(tj\u22121), in the form of a monomial of degree 1. It is followed by contributions involving the mean wave height during the sedimentation period period WvH for all the modes, which makes WvH the most important external forcing, figuring in the third position among all the forcings, with a contribution of 9.6% through the first mode, a total of 12.6% if only WvH monomials are considered, and 13.3% if interactions with other variables are taken into account.\nThe other forcing contributions also appeared, but with much less importance: e.g., the influence of mean low tide level TLmean, which took an interaction form with the previous bathymetry shape for Mode 3. Firstly, this interaction makes sense in terms of physics, as sediment deposition is conditioned by the value of bed shear stress [105], which depends on velocity and water depth. The water depth value is exactly the tidal level minus the bed elevation value, which here appears as a multiplicative interaction between TLmean and Mode 3. Second, the value of this contribution was only 0.7% GGW, which is negligible when compared to the first contribution of WvH. The learned model gave much more importance to waves than to tides. This does not necessarily mean that tides have no influence on sediment deposition, but may simply suggest that, in the present configuration, sediment mobilization by the tide is always more or less the same, and that the forcing that makes a considerable difference is the variation in wave heights. Waves are a determining factor for sediment mobilization in coastal configurations, through the influence they have on bed shear stress [105]. Further more, a noticeable correlation between WvH and TLmean can be seen in Figure 23 in Appendix A, which means that the information of low-tide levels is to a certain extent contained in the mean wave height. There is therefore a probable dependency between these variables. In case of dependencies, the iterative process used by LARS may drop a variable that is physically important because the important information is already contained in another variable, due to their dependency.\nLastly, it is important to note that the contribution of less frequent wave events was also present but to a much smaller extent. It is represented by a polynomial term in the form \u03b6\u03b1=(1,1)(Wv2m,Wv2m%), where Wv2m and Wv2m% are respectively mean wave height exceeding 2m and the associated frequency of occurrence (arbitrary storm indicator chosen in Subsection 3.1). This term appears in Modes 3 and 4 for a maximum total influence of 0.3%. Higher-order interactions and less frequent events are therefore represented by modes of higher rank, associated with smaller variance percentages.\nStep 4 - Robustness of the physical interpretation with respect to the learning-set members As a last proof of the robustness of the proposed learning algorithm, specifically concerning physical interpretation, a sensitivity analysis with respect to the training set members was performed. The robustness of the calculated Garson Weights (GW) with respect to the choice the training members was studied. This is equivalent to studying\nthe robustness of the polynomial basis term selection as produced by LARS, and their associated multiplicative coefficients.\nFor this, a Bootstrap analysis was again used to construct different learning sets of size 50, instead of choosing the first 50 measurements. This produces a distribution of the GWs rather than a single value, for each polynomial term. The result is shown in Figure 16 for the weights of the ak(tj\u22121) and WvH monomials.\nFor modes 1 to 4, the median weights P50 (Percentile 50) of the ak(tj\u22121) monomials, represented in Figure 16-a, were always over 0.6, but the variation range was strictly less than 1, with density functions centered around the median and a small standard deviation for modes 1, 2 and 4. This means that whatever the training set, the previous state ak(tj\u22121) value was always predominant but never enough to estimate the evolution of the first four modes. A tendency (in particular linear) using the last state was not sufficient, and additional information was always needed (forcing). In parallel, Figure 16-b shows that this information is certainly the waves, as the median values of the GW for the first four modes were between 10 and 25%, corresponding to the information gap left by the previous value variable ak(tj\u22121) in the fitted PCE model.\nStarting from mode 5, the median values of ak(tj\u22121)\u2019s GW had greater chance of falling around 1, which means that the associated polynomial models only rely on the last recorded value of the mode for the future guess. In other terms, the constructed model consists of a linearization around the previous value (tendency capturing) and does not incorporate the correlations between the future-state and the forcing variables (causality model). This can be explained by the small variances of the higher-rank modes and the difficulty of learning the PCE models from statistics averaged over the sedimentation periods. Additionally, the P25-P75 confidence interval moved to the upper bound of the density functions."}, {"heading": "3.2.3 Summary of the physical insights from the learning", "text": "The spatial patterns as deduced by POD express the spatial correlation in the sediment deposition from the upstream to the downstream part of the channel. The EVR reached 99% with d = 20 modes only, where the mean relative RMSE between the approximation and reality was slightly over 10%. This is a statistical proof that the spatial correlations expressed in the POD patterns are explanatory of the physical dynamics over their whole range of variation (at least that observed from 2010 to 2018), with a low approximation rank. In conclusion, the dynamic problem exhibits fairly strong spatial correlations, and the solution to the problem can be expressed on a finite orthonormal basis.\nThe temporal patterns express the evolution of the sedimentation, as they multiply the spatial patterns. They were learned using PCE as a function of the previously cited inputs (previous states and forcings). The statistical model configuration (dimension and marginals) was chosen after an investigation of different options. The associated training and prediction error converged for the first three modes, and are characterized by tight confidence\nintervals. The residuals of the selected model were either negligible or centered around zero, demonstrating the unbiased character of the learning and prediction. The fitted models are of lower degree for the low-rank modes 1 and 2 and of higher degrees for modes 3 and 4, which are higher-rank, due to the emergence of interactions between the forcings, namely variables related to extreme behavior (storm events). The model mainly relies on the last state information, showing a strong correlation/continuity in time of the studied physics. Using GW, which measures the forcing influence for the first five modes, the action of waves was highlighted by the PCE model as a determining phenomenon. The first mode influenced the dynamic with a rate of 64.9%, the previous value of the second mode with a rate of 10.2% and, in third position, the mean wave height with a rate of 9.6%. The remaining 15.3% is essentially associated with previous values of higher order modes (10.3%), interactions with tides and contributions of other wave indicators. The GWs show robustness with respect to the choice of the training set members, which makes them trustworthy, at least for temporal correlation and analysis of wave influence. The main physical conclusions are that the dynamic problem is characterized by strong temporal correlations, representing more than 85% of the evolution, with an external sediment source, mainly represented by the waves, representing not more than 15%."}, {"heading": "3.3 Prediction of a physical field using POD-PCE coupling", "text": "After performing both POD and PCE independently, the accuracy of a Machine Learning process using a PODPCE coupling was assessed as in Section 2.3. In the continuity with Section 3.2, the first 50 historical bathymetries were used for training and the other 14 for forecasting. First, the impact of the size of the POD basis on the prediction process is assessed in Subsection 3.3.1. Then, the best size was determined and the average prediction behavior is analyzed in Subsection 3.3.2. The accuracy of the POD-PCE ML in predicting spatial details is assessed on cross-section examples in Subsection 3.3.3 and a summary is given in Subsection 3.3.4."}, {"heading": "3.3.1 Influence of POD basis size", "text": "In order to track the errors generated by the various steps of the algorithm (POD, PCE and coupling), the mean relative RMSE (averaged over the prediction set, as in Equation 18 ) was calculated for each step and for each approximation rank d, as follows:\n\u2022 Reduction error: distance between the POD approximation \u2211d k=1 ak(t)\u03a6k(x) and the corresponding mea-\nsured bathymetry field z(x, t) ; \u2022 Learning error : distance between the POD approximation \u2211d k=1 ak(t)\u03a6k(x) and the prediction using the\nPOD-PCE coupling formulated as \u2211d k=1HPi [ ai(tj), tj+1 \u2212 tj ,\u0398P (tj \u2192 tj+1) ] \u03a6k(x) ;\n\u2022 Prediction error: the resulting final error between the prediction using POD-PCE coupling and the corresponding measured bathymetry z(x, t).\nThe results are shown in Figure 17 . Reduction error decreased from 16% to 3%, with increasing approximation rank. The error followed a logarithmic trend, with a significant slowdown from rank 9. These errors are coherent with the errors averaged over the full set (rather than the prediction set only) in the POD results Section 3.2.1 (around 8%).\nThe learning error increased from 1% to 5% with increasing approximation rank, which is natural because the complexity of the model is increased. In fact, a prediction of rank d + 1 has an additional temporal coefficient that is predicted as compared to rank d. It is therefore natural that the distance between the approximation Z(x, t) \u2248 \u2211d i=1 ai(t) \u2217 \u03c6(x) and its prediction Z(x, t) \u2248 \u2211d i=1HPi \u2217 \u03c6(x) increased with increasing rank. The learning error order of magnitude was consistent with the empirical prediction error of 4% for mode 1 (as calculated in Section 3.2.2), associated with an EVR of over 94%. Lastly, the prediction error decrease is the balance of, on the one hand, the increase in accuracy by adding POD modes and, on the other hand, the increase in forecasting error with increasing number of temporal coefficients to be predicted. Consequently, the prediction error decreased from 16 to 6% up to rank 11, following almost the same decreasing trend as the reduction error. However, the decrease rate became slower and increasingly subdued, being overtaken by the learning errors, which dramatically increased starting from mode 12, as seen in Figure 14. Hence, a POD-PCE model of size 11 was selected for prediction."}, {"heading": "3.3.2 Average performance of the chosen model", "text": "Average sediment deposition was predicted using the POD-PCE model of rank 11, for each of the 14 prediction dates. The average sedimentation rate, denoted Sr, was calculated for time tj representing the sedimentation over [tj\u22121, tj ], as in Equation 21. For operational estimation of sediment deposition, only the positive evolutions are of interest ; therefore, the erosion points were discarded in calculating rate Sr by cancelling negative evolutions. Indeed, z(xi, tj) < z(xi, tj\u22121) implies (z(xi, tj)\u2212 z(xi, tj\u22121)) = \u2212|z(xi, tj)\u2212 z(xi, tj\u22121)|, and therefore a null contribution to the sedimentation rate Sr. Furthermore, only regions of considerable depth are of interest. Therefore only np bathymetry points under \u22121 m (xi, i \u2208 Np) were taken into account. The results are shown in Figure 18\nSr = 1\n2nP \u2211 i\u2208Np (z(xi, tj)\u2212 z(xi, tj\u22121)) + |z(xi, tj)\u2212 z(xi, tj\u22121)| |z(xi, tj)|\n. (21)\nThe POD-PCE prediction globally followed the real sedimentation trend, for example from Dates 1 to 8. When it was not equal to the real sedimentation rate, it was generally an overestimation, which is coherent with the asymmetry observed in the distribution of Mode 1 training and prediction residuals (Figure 12 in Subsection 3.2.2).\nFor the particular Date 11 however, half of the sedimentation was missing. Investigation of the data for this particular measurement showed that the previous record, taken as input, had been made 29 days previously, which is far from the average \u2206t \u2248 15 days ; it is twice the mean interval, thus underestimating sedimentation by half. As measurement intervals were in general around the average, sedimentation time interval \u2206t was not selected as a key parameter by LARS for the dynamic model, although it was given as an input and is physically significant. For the particular case of the time variable, multiplicative enhancement can be intended as a correction. However, this shows one of the limitations of statistical modeling: statistical significance can be confused with physical importance. Indeed, for the statistical conclusions to be physically significant, the measurements should be diverse enough to account for the variations in the inputs and the impact of these variations on the output. This was unfortunately not guaranteed for sedimentation measurement intervals, as they were often equal to 2 weeks. Additionally, for Dates 12 and 13, a large part of the wave measurements were missing in the sedimentation time interval. Consequently, mean wave height WvH was estimated over only a small portion of the time interval. This may lead to a good prediction (Date 13) if the interval used is representative enough of the full interval, and to bad prediction (Date 12) when not, and highlights the limitations of statistical averaging."}, {"heading": "3.3.3 Spatial details of prediction by the chosen model", "text": "The spatial details of the prediction were analyzed on cross-sections for specific prediction dates. First, sediment deposition was observed on a cross-section at the entrance of the intake (Figure 19). In accordance with the previous conclusions from Figure 18, the POD-PCE prediction captured various sedimentation ranges, as shown with Dates 2 and 7. However, a slight artificial sedimentation was predicted whereas there was no dynamics in reality, for example for Date 1 (Figure 19-a), due to the fact that the model is continuous, whereas threshold phenomena can occur in reality.\nNext, sediment deposition was observed on a cross-section at the middle of the first portion of the intake (Figure 20). Mean sedimentation was well captured, but some details of the bathymetry were missing, such as formation of a new feature for Date 5 (distance 20 to 25 m) and Date 7 (distance 30 to 35 m). For Date 6, sediment deposition was slightly underestimated in the right bank and overestimated in the left bank. However, although the details of sediment deposition were not perfectly captured, the value of the sedimentation area corresponds well enough to reality. It can also be concluded that the way the RMSE and relative errors are averaged in space, for example in Figure 17, actually penalizes the accuracy of the algorithm because it does not take account of the oscillation of the prediction around an accurate mean.\nThen sediment deposition was observed on a cross-section at the bending part of the intake (Figure 21). It shows that the prediction algorithm understood that the sediment deposition mainly occured in the right bank of the channel, for example for Date 7, even though it was overestimated. Furthermore, considering modes of higher rank from the previous measurement as an input, the algorithm captured the swing in the profile throughout its history, which is here observed from Date 4 to 14.\nLastly, a cross-section of the last portion of the channel, in front of the downstream pumping station, is shown in Figure 22. Once again, the prediction algorithm understood where the sediment deposition occurs, this time in the left bank of the channel, coherent with the pattern represented by Mode 2 (Figure 6-b). However, it can be seen that unusual sediment deposition occurred for Date 11, which was not captured by the model, and may correspond to the arrival of less frequent fine sediment that appears in Mode 4 (Figure 6-d). This also explains the sediment deposition error observed for Date 11 in Figure 18."}, {"heading": "3.3.4 Summary of POD-PCE algorithm performance", "text": "Overall, the proposed learning algorithm showed interesting prediction characteristics. Firstly, model complexity can be increased gradually, by increasing the number of POD modes when accurate. Plotting error against the number of modes shows a convergence that helps in selecting the optimal number of modes. The average RMSE of the predicted field remains reasonably low. It was around 6% with the 11 modes selected in the present case.\nSecondly, trends are well captured for spatially averaged quantities (here sedimentation rate) and for detailed spatial representations of the field. Good spatial distribution of evolution is guaranteed by the POD modes, even when evolution amplitude is over- or under-estimated.\nLastly, some disadvantages should be noted. For example, less frequent events that are represented by modes of higher ranks can be overlooked. Furthermore, sudden changes in features were not sufficiently captured, due to the high temporal correlation between last state and future state that was incorporated in the learning."}, {"heading": "4 Summary and discussion", "text": "In this study, POD-PCE coupling for field-measurement based Machine Learning was proposed and assessed, in an industrial context in the field of geosciences, for complex physical phenomena involving non-linear dynamics and various forcings.\nPOD showed excellent performance both for reducing problem dimensionality and for physical interpretation of spatial and temporal modes. This is an important property of POD [47, 48]. Even though the spatial patterns are mathematical modes that do not have explicit physical meaning, an interpretation could be given, at least for the patterns that were statistically stable, because they are statistical representations of the dynamic behavior. Our investigation showed that temporal signals are also relevant: they can show more or less regularity, which can be related either to the representation of different space or time scale physics, or to less frequent events. In our particular problem, the mean of the bathymetry field was not deduced from the field before applying POD, as can be the case for classical applications [14]. It was used as valuable information for decomposition, which allowed correlations to be studied between the first and second patterns, as well as the construction of the learning. The potential of POD for detecting biased and missing data was also assessed. POD was first applied to the whole set of measurements, but discontinuities emerged in the temporal signals of the decomposition. Such a procedure is important because, in most of cases, the data need to be filtered, which is a time-consuming task. The POD enabled fast recognition of elements that react differently from the average. However, many points of improvement are worth mentioning. Firstly, the choice of POD as a decomposition technique was here motivated by its simplicity and the possibility of interpretation when coupled to a linear learning formulation such as PCE. Other decomposition techniques exist, and many authors attempted comparisons, for example with Fourier [84], extensions of POD [30, 37] or other classes of decomposition [90]. For the present application, other decomposition techniques such as Kernel Principal Component Analysis (KPCA) [69] and Sparse PCA [43] were analyzed, without significant improvements. Secondly, data filtering using POD consisted only in deleting the poor-quality measurements and extracting the spatial zones where data were always measured. POD can however be used to reconstruct missing data, by inverse projection on POD basis elements deduced from qualitative data [89]. This could help to extend the statistical set for the learning. Lastly, a linear interpolation of the bathymetry was used to project all the measurements onto the same grid for POD application. The uncertainty that emerged from this interpolation process was not treated. This uncertainty, added to the measurement errors, can shed light on model behavior. For example, comparison of mono-beam cross-sections with multi-beam measurements and uncertainty propagation of bathymetry errors through the learning could be attempted, especially because uncertainties in the bathymetric information may impact the flow field computation [11, 60].\nPCE was used to learn the temporal POD modes as 1D data. We showed the importance of the polynomial basis and therefore of the choice of marginals for the learning phase. In this comparison, we showed that choosing uniform distributions associated with Legendre family might not be appropriate, even though it is widely used when no input information is available [103]. Moreover, the number of inputs involved can alter the learning. When using LARS, the presence of numerous variables can mislead the algorithm in selecting useless variables that seem to decrease training error but increase prediction error. This is an overfitting phenomenon. A good combination between polynomial basis and dimension choice could significantly improve convergence speed, centering of residuals and mean training and prediction errors. The proposed contribution analysis using the PCE coefficients showed that the last-state information is often the most influential input. A robustness test was conducted by varying the training set, and the observation was stable. For the modes of small ranks associated with the largest variances (modes 1 to 4 in our example), wave height was the most influential forcing, whatever the chosen learning set. This is consistent with physical knowledge of sediment mobilization in coastal configurations, where waves are known to be determining through the influence they have on bed shear stress [105]. For modes of higher rank, however, the only selected variable by LARS is the last-state information. Firstly, the forcings that were used as PCE inputs were simple statistical estimators deduced from the data (means, percentiles, etc.). This reduction was used instead of giving all the time series as an input, because the problem would become ultrahigh-dimensional. This wastes the richness of the available data for inputs such as tidal information that are measured on an hourly basis. A more accurate statistical reduction of the inputs could be used. For example, Lataniotis et al. [54] used PCA and KPCA for surrogate modeling with PCE and Gaussian Processes on ultrahigh-dimensional problems. Secondly, dependencies were not specifically modeled. These can be incorporated using the mathematical setting for the construction of the polynomial basis established by Soize and Ghanem [97] . The dependencies, however, indirectly influenced the construction of the model via selection of basis elements by LARS, which avoids redundancy. Thirdly, the choice\nof tested input configurations for PCE was arbitrary. A more objective variable selection technique is necessary [22, 81]. Lastly, PCE was chosen for the interpretation possibilities that it allows when combined to POD, thanks to computation of importance measures from the expansion coefficients. Other methods can be used: i.e., for high rank POD modes that were poorly learned using PCE. Indeed, PCE seems to work better for modes associated with high than low variances. Although it may be tempting to conclude that modeling of high rank modes is not necessary since they are associated with low variances, it should be noted that accurate prediction of modes of higher rank can make the difference between average forecasting and forecasting that captures less frequent events and smaller scale features of the 2D field. Therefore, the present ML could be enhanced by improving the learning of high-rank modes. For example, the construction of marginals and the use of random draw with confidence intervals, or extreme statistics models [26], instead of causal models like PCE, could be attempted.\nFinally, the robustness and convergence properties added to the physical interpretability supported the choice of POD-PCE coupling as a ML prediction algorithm. It respects the PDR (Predictive, Descriptive, Relevant) framework defined in [75]. It is characterized by both predictive and descriptive accuracy (simplicity) and is stable with respect to data disturbance. It is interpretable, as the sparsity, simulatability and modularity defined in [75] are respected by construction. Finally, it is both interpretable at features level (POD components and their PCE) and at multidimensional output level (GW compared to the proposed GGW indicators). The POD-PCE ML was therefore implemented using the first 11 modes, after sensitivity test to number of modes. Mean information (e.g. sediment deposition rate) was in general well reproduced. Profile-by-profile investigation also showed that PODPCE coupling was promising, as the spatial distribution of the sediment deposition patch locations and amplitudes were well represented. Some general limitations should be highlighted and could be good perspectives for improving the process. The small data-set was a clear handicap in our problem. Some events, such as sediment downstream the intake or variation in measurement intervals, were poorly represented. It would be interesting to test the methodology on an enriched data set in order to assess the real potential of POD-PCE Machine Learning. Due to the lack of such data, input distributions were certainly not well approximated. One way of improving POD-PCE coupling would be the development of hybrid measurement-based/process-based data learning [72, 93]. This could be used to enrich the data set, not only by increasing its size (emulated realistic scenarios) but also by adding new input parameters that are not measured but obtained from process-based modeling."}, {"heading": "Acknowledgements", "text": "This work is funded by the French National Association of Research and Technology (ANRT) through the Industrial Conventions for Training through REsearch (CIFRE) in agreement with EDF R&D. The authors acknowledge their support, and are grateful for data collection and feedback from EDF operators. In particular, we would like to thank D. Rouge\u0301 for providing the data-set used in this study and for his continuous availability. We also would like to thank Pr. L. Terray (CERFACS) and Dr. M. Rochoux (CERFACS) for constructive discussions on POD and PCE respectively, and Pr. B. Sudret (ETH Zurich) for providing key literature elements on the treatment of ultrahigh dimensional problems and functional inputs using PCE. Finally, the authors gratefully acknowledge the OpenTURNS open source community (An Open source initiative for the Treatment of Uncertainties, Risks\u2019N Statistics)."}, {"heading": "Appendix A. Additional information on the data set", "text": "The available measurements are summarized in Table 1.\nThe forcings did not have the same frequencies. A solution for the homogenization of frequencies is to reduce the measured data to representative statistics over the sedimentation interval \u2206t \u2248 15 days that separates two bed elevation measurements. Hence, the statistics described in Table 2 are used.\nScatter plots of some input variables are presented in Figure 23."}, {"heading": "Appendix B. Prediction of a physical field using POD-PCE coupling", "text": "Convergence of the HPi ;Hrm model The impact of training-set size on the learning and prediction errors of theHPi ;Hrm model was assessed and can be compared to that of the HPi ;Stlj model from Subsection 3.2.2. Evolution of training and prediction empirical errors with training-set size is shown in Figure 10 and 11, respectively.\nExplicit fitted PCE model\nFor the first three modes, fitting with the model HPi ;Stlj is illustrated in Figure 26, and shows good pointwise performance on the fitting and learning intervals.\nThe corresponding explicit PCE models for each POD mode are shown in Equation 22. a1(tk) = \u22120.0764596 + 0.00169697 \u2217 (0.921002 \u2217WvH) + 0.0114921 \u2217 (0.976091 \u2217 a1(tk\u22121)) a2(tk) = \u22120.0297343\u2212 0.00724308 \u2217 (0.921002 \u2217WvH) + 0.092532 \u2217 (0.958222 \u2217 a2(tk\u22121)) a3(tk) = \u22120.0142516 \u2217 (0.921002 \u2217WvH) + 0.031753 \u2217 (0.900271 \u2217 a3(tk\u22121)) \u22120.00225851 \u2217 ((0.948137 \u2217Dt) \u2217 (0.921002 \u2217WvH)) \u22120.00666008 \u2217 (\u22120.6476\u2212 0.521557 \u2217WvH + 0.549323 \u2217WvH2) \u22120.000693673 \u2217 ((0.97073 \u2217Wv2m) \u2217 (0.989947 \u2217Wv2m%)) +0.00796314 \u2217 ((0.946933 \u2217 TLmean) \u2217 (0.900271 \u2217 a3(tk\u22121)))\n(22)\nThe Garson Weights (GW) and Generalized Garson Weights (GGW), respectively presented in Sections 2.2 and 2.3, were calculated for each polynomial term, for the first five modes, and shown in Table 3. A visual plot can be found in Figure 15 in Subsection 3.2.2.\nRe\u0301fe\u0301rences\n[1] O. I. Abiodun, A. Jantan, A. E. Omolara, K. V. Dada, N. A. Mohamed, and H. Arshad. State-of-the-art in artificial neural network applications : A survey. Heliyon, 4(11) :e00938, 2018. ISSN 2405-8440. doi : https://doi.org/10.1016/j.heliyon.2018.e00938. URL http://www.sciencedirect.com/science/article/ pii/S2405844018332067.\n[2] C. Adam-Bourdarios, G. Cowan, C. Germain, I. Guyon, B. Ke\u0301gl, and D. Rousseau. The higgs boson machine learning challenge. In Proceedings of the 2014 International Conference on High-Energy Physics and Machine Learning - Volume 42, HEPML\u201914, page 19\u201355. JMLR.org, 2014.\n[3] N. Akkari. Mathematical study of the sensitivity of the POD method (Proper orthogonal decomposition). Theses, Universite\u0301 de La Rochelle, Dec. 2012. URL https://tel.archives-ouvertes.fr/tel-01066073.\n[4] L. O. Amoudry and A. J. Souza. Deterministic coastal morphological and sediment transport modeling : a review and discussion. Reviews of Geophysics, 49(2), 2011. doi : 10.1029/2010RG000341. URL https: //agupubs.onlinelibrary.wiley.com/doi/abs/10.1029/2010RG000341.\n[5] A. R. Barron. Approximation and estimation bounds for artificial neural networks. Mach. Learn., 14(1) : 115\u2013133, Jan. 1994. ISSN 0885-6125. doi : 10.1023/A:1022650905902. URL https://doi.org/10.1023/A: 1022650905902.\n[6] G. Blatman. Adaptive sparse polynomial chaos expansions for uncertainty propagation and sensitivity analysis. PhD thesis, 2009.\n[7] G. Blatman and B. Sudret. Adaptive sparse polynomial chaos expansion based on least angle regression. Journal of Computational Physics, 230(6) :2345 \u2013 2367, 2011. ISSN 0021-9991. doi : https://doi.org/10.1016/ j.jcp.2010.12.021. URL http://www.sciencedirect.com/science/article/pii/S0021999110006856.\n[8] L. Bruno, C. Canuto, and D. Fransos. Stochastic aerodynamics and aeroelasticity of a flat plate via generalised polynomial chaos. Journal of Fluids and Structures, 25(7) :1158 \u2013 1176, 2009. ISSN 0889-9746. doi : https:// doi.org/10.1016/j.jfluidstructs.2009.06.001. URL http://www.sciencedirect.com/science/article/pii/ S0889974609000693.\n[9] S. L. Brunton, B. R. Noack, and P. Koumoutsakos. Machine learning for fluid mechanics. Annual Review of Fluid Mechanics, 52(1) :477\u2013508, 2020. doi : 10.1146/annurev-fluid-010719-060214. URL https://doi.org/ 10.1146/annurev-fluid-010719-060214.\n[10] K. Campbell, M. D. McKay, and B. J. Williams. Sensitivity analysis when model outputs are functions. Reliability Engineering & System Safety, 91(10) :1468 \u2013 1472, 2006. ISSN 0951-8320. doi : https://doi.org/10.1016/j.ress.2005.11.049. URL http://www.sciencedirect.com/science/article/pii/ S0951832005002565. The Fourth International Conference on Sensitivity Analysis of Model Output (SAMO 2004).\n[11] A. Casas, G. Benito, V. Thorndycraft, and M. Rico. The topographic data source of digital terrain models as a key element in the accuracy of hydraulic flood modelling. Earth Surface Processes and Landforms : The Journal of the British Geomorphological Research Group, 31(4) :444\u2013456, 2006.\n[12] J. Castro, C. Mantas, and J. Benitez. Neural networks with a continuous squashing function in the output are universal approximators. Neural Networks, 13(6) :561 \u2013 563, 2000. ISSN 0893-6080. doi : https: //doi.org/10.1016/S0893-6080(00)00031-9. URL http://www.sciencedirect.com/science/article/pii/ S0893608000000319.\n[13] N. Cohen, O. Sharir, and A. Shashua. On the expressive power of deep learning : A tensor analysis. In V. Feldman, A. Rakhlin, and O. Shamir, editors, 29th Annual Conference on Learning Theory, volume 49 of Proceedings of Machine Learning Research, pages 698\u2013728, Columbia University, New York, New York, USA, 23\u201326 Jun 2016. PMLR. URL http://proceedings.mlr.press/v49/cohen16.html.\n[14] L. Cordier and M. Bergmann. Proper orthogonal decomposition : an overview. In Lecture series 2002-04, 2003-03 and 2008-01 on post-processing of experimental and numerical data, Von Karman Institute for Fluid Dynamics, 2008., page 46 pages. VKI, 2008. URL https://hal.archives-ouvertes.fr/hal-00417819.\n[15] S. Costa, F. Gourmelon, C. Augris, P. Clabaut, and B. Latteux. Apport de l\u2019approche syste\u0301mique et pluridisciplinaire dans l\u2019e\u0301tude du domaine littoral et marin de la seine-maritime (france). Norois. Environnement, ame\u0301nagement, socie\u0301te\u0301, (196) :91\u2013108, 2005.\n[16] M. Couplet. Reduced-order POD-Galerkin modelling for the control of unsteady flows. Theses, Universite\u0301 Paris-Nord - Paris XIII, Jan. 2005. URL https://tel.archives-ouvertes.fr/tel-00142745.\n[17] G. Cruciani, M. Baroni, S. Clementi, G. Costantino, D. Riganelli, and B. Skagerberg. Predictive ability of regression models. part i : Standard deviation of prediction errors (sdep). Journal of Chemometrics, 6(6) : 335\u2013346, 1992.\n[18] G. Cybenko. Approximation by superpositions of a sigmoidal function. Mathematics of Control, Signals and Systems, 2(4) :303\u2013314, Dec 1989. ISSN 1435-568X. doi : 10.1007/BF02551274. URL https://doi.org/10. 1007/BF02551274.\n[19] R. G. Dean and R. A. Dalrymple. Coastal processes with engineering applications. Cambridge University Press, 2004.\n[20] B. Efron and R. Tibshirani. Bootstrap methods for standard errors, confidence intervals, and other measures of statistical accuracy. Statistical science, pages 54\u201375, 1986.\n[21] R. Eldan and O. Shamir. The power of depth for feedforward neural networks. Conference on Learning Theory, 12 2015.\n[22] N. Faber and R. Rajko\u0302. How to avoid over-fitting in multivariate calibration\u2014the conventional validation approach and an alternative. Analytica Chimica Acta, 595(1) :98 \u2013 106, 2007. ISSN 0003-2670. doi : https://doi.org/10.1016/j.aca.2007.05.030. URL http://www.sciencedirect.com/science/article/pii/ S0003267007009129. Papers presented at the 10th International Conference on Chemometrics in Analytical Chemistry.\n[23] K.-I. Funahashi. On the approximate realization of continuous mappings by neural networks. Neural Networks, 2(3) :183 \u2013 192, 1989. ISSN 0893-6080. doi : https://doi.org/10.1016/0893-6080(89)90003-8. URL http: //www.sciencedirect.com/science/article/pii/0893608089900038.\n[24] O. Garcia-Cabrejo and A. Valocchi. Global sensitivity analysis for multivariate output using polynomial chaos expansion. Reliability Engineering & System Safety, 126 :25\u201336, 2014.\n[25] M. Gevrey, I. Dimopoulos, and S. Lek. Review and comparison of methods to study the contribution of variables in artificial neural network models. Ecological Modelling, 160(3) :249 \u2013 264, 2003. ISSN 0304-3800. doi : https://doi.org/10.1016/S0304-3800(02)00257-0. URL http://www.sciencedirect.com/ science/article/pii/S0304380002002570. Modelling the structure of acquatic communities : concepts, methods and problems.\n[26] M. Ghil, P. Yiou, S. Hallegatte, B. Malamud, P. Naveau, A. Soloviev, V. Keilis-Borok, D. Kondrashov, V. Kossobokov, O. Mestre, et al. Extreme events : dynamics, statistics and prediction. Nonlinear Processes in Geophysics, 18(3) :295, 2011.\n[27] E. B. Goldstein, G. Coco, and N. G. Plant. A review of machine learning applications to coastal sediment transport and morphodynamics. Earth-Science Reviews, 194 :97 \u2013 108, 2019. ISSN 0012-8252. doi : https: //doi.org/10.1016/j.earscirev.2019.04.022. URL http://www.sciencedirect.com/science/article/pii/ S001282521830391X.\n[28] A. Gonoskov, E. Wallin, A. Polovinkin, and I. Meyerov. Employing machine learning for theory validation and identification of experimental conditions in laserplasma physics. Scientific Reports, 9 :7043, 2019. doi : 10.1038/s41598-019-43465-3.\n[29] I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. MIT Press, 2018.\n[30] S. Gordeyev and F. Thomas. Temporal proper decomposition (tpod) for closed-loop flow control. Exp Fluids, 54, 2013.\n[31] A. Guillaume. VAG-Modele de prevision de l\u2019etat de la mer en eau profonde. Dir. de la Meteorologie Nationale, 1987.\n[32] M. Guo and J. S. Hesthaven. Reduced order modeling for nonlinear structural analysis using gaussian process regression. Computer Methods in Applied Mechanics and Engineering, 341 :807 \u2013 826, 2018. ISSN 0045-7825. doi : https://doi.org/10.1016/j.cma.2018.07.017. URL http://www.sciencedirect.com/science/article/ pii/S0045782518303487.\n[33] I. Guyon and A. Elisseeff. An introduction to variable and feature selection. J. Mach. Learn. Res., 3(null) : 1157\u20131182, Mar. 2003. ISSN 1532-4435.\n[34] B. Hanin. Universal function approximation by deep neural nets with bounded width and relu activations. Mathematics, 7(10), 2019. ISSN 2227-7390. doi : 10.3390/math7100992. URL https://www.mdpi.com/ 2227-7390/7/10/992.\n[35] T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning : Data Mining, Inference, and Prediction, Second Edition (Springer Series in Statistics). 02 2009. ISBN 0387848576.\n[36] L. Hawchar, C.-P. E. Soueidy, and F. Schoefs. Principal component analysis and polynomial chaos expansion for time-variant reliability problems. Reliability Engineering & System Safety, 167 :406 \u2013 416, 2017. ISSN 0951-8320. doi : https://doi.org/10.1016/j.ress.2017.06.024. URL http://www.sciencedirect.com/ science/article/pii/S0951832016302587. Special Section : Applications of Probabilistic Graphical Models in Dependability, Diagnosis and Prognosis.\n[37] A. Hekmati, D. Ricot, and P. Druault. About the convergence of pod and epod modes computed from cfd simulation. Computers & Fluids, 50(1) :60 \u2013 71, 2011. ISSN 0045-7930. doi : https://doi.org/10.1016/j.compfluid. 2011.06.018. URL http://www.sciencedirect.com/science/article/pii/S0045793011002064.\n[38] K. Hornik. Approximation capabilities of multilayer feedforward networks. Neural Networks, 4(2) :251 \u2013 257, 1991. ISSN 0893-6080. doi : https://doi.org/10.1016/0893-6080(91)90009-T. URL http://www. sciencedirect.com/science/article/pii/089360809190009T.\n[39] O. Ibrahim. A comparison of methods for assessing the relative importance of input variables in artificial neural networks. Journal of applied sciences research, 9(11) :5692\u20135700, 2013.\n[40] B. Iooss and P. Lema\u0302\u0131tre. A Review on Global Sensitivity Analysis Methods, pages 101\u2013122. Springer US, Boston, MA, 2015. ISBN 978-1-4899-7547-8. doi : 10.1007/978-1-4899-7547-8 5. URL https://doi.org/ 10.1007/978-1-4899-7547-8_5.\n[41] R. Iten, T. Metger, H. Wilming, L. del Rio, and R. Renner. Discovering physical concepts with neural networks. Phys. Rev. Lett., 124 :010508, Jan 2020. doi : 10.1103/PhysRevLett.124.010508. URL https: //link.aps.org/doi/10.1103/PhysRevLett.124.010508.\n[42] M. Janocko, M. Cartigny, W. Nemec, and E. Hansen. Turbidity current hydraulics and sediment deposition in erodible sinuous channels : laboratory experiments and numerical simulations. Marine and Petroleum Geology, 41 :222\u2013249, 2013.\n[43] I. M. Johnstone and A. Y. Lu. On consistency and sparsity for principal components analysis in high dimensions. Journal of the American Statistical Association, 104(486) :682\u2013693, 2009.\n[44] I. Jolliffe. Principal Component Analysis, pages 1094\u20131096. Springer Berlin Heidelberg, Berlin, Heidelberg, 2011. ISBN 978-3-642-04898-2. doi : 10.1007/978-3-642-04898-2 455. URL https://doi.org/10.1007/ 978-3-642-04898-2_455.\n[45] B. A. Jones and A. Doostan. Satellite collision probability estimation using polynomial chaos expansions. Advances in Space Research, 52(11) :1860 \u2013 1875, 2013. ISSN 0273-1177. doi : https://doi.org/10.1016/j.asr. 2013.08.027. URL http://www.sciencedirect.com/science/article/pii/S0273117713005413.\n[46] A. Karpatne, I. Ebert-Uphoff, S. Ravela, H. A. Babaie, and V. Kumar. Machine learning for the geosciences : Challenges and opportunities. IEEE Transactions on Knowledge and Data Engineering, 31(8) :1544\u20131554, Aug 2019. ISSN 2326-3865. doi : 10.1109/TKDE.2018.2861006.\n[47] G. Kerschen and J. Golinval. Physical interpretation of the proper orthogonal modes using the singular value decomposition. Journal of Sound and Vibration, 249(5) :849 \u2013 865, 2002. ISSN 0022-460X. doi : https://doi.org/10.1006/jsvi.2001.3930. URL http://www.sciencedirect.com/science/article/ pii/S0022460X01939306.\n[48] G. Kerschen, J. Golinval, V. A.F., and B. L.A. The method of proper orthogonal decomposition for dynamical characterization and order reduction of mechanical systems : an overview. Nonlinear Dynamis, 41 :147, 2002. ISSN 1573-269X. doi : https://doi.org/10.1007/s11071-005-2803-2.\n[49] O. M. Knio and O. P. Le Ma\u0302\u0131tre. Uncertainty propagation in CFD using polynomial chaos decomposition. Fluid Dynamics Research, 38(9) :616\u2013640, sep 2006. doi : 10.1016/j.fluiddyn.2005.12.003. URL https: //doi.org/10.1016%2Fj.fluiddyn.2005.12.003.\n[50] J. Kremer, K. Stensbo-Smidt, F. Gieseke, K. S. Pedersen, and C. Igel. Big universe, big data : Machine learning and image analysis for astronomy. IEEE Intelligent Systems, 32(2) :16\u201322, Mar 2017. ISSN 1941-1294. doi : 10.1109/MIS.2017.40.\n[51] J. N. Kutz. Deep learning in fluid dynamics. Journal of Fluid Mechanics, 814 :1\u20134, 2017. doi : 10.1017/jfm. 2016.803.\n[52] M. Lamboni, H. Monod, and D. Makowski. Multivariate sensitivity analysis to measure global contribution of input factors in dynamic models. Reliability Engineering & System Safety, 96(4) :450 \u2013 459, 2011. ISSN 0951- 8320. doi : https://doi.org/10.1016/j.ress.2010.12.002. URL http://www.sciencedirect.com/science/ article/pii/S0951832010002504.\n[53] M. Larson, M. Capobianco, M. Jansen, G. Ro\u0301z\u0307yn\u0301ski, H. Southgate, M. Stive, K. Wijnberg, and S. Hulscher. Analysis and modeling of field data on coastal morphological evolution over yearly and decadal time scales. part 1 : Background and linear techniques. Journal of Coastal Research, 19, 09 2003.\n[54] C. Lataniotis, S. Marelli, and B. Sudret. Extending classical surrogate modelling to ultrahigh dimensional problems through supervised dimensionality reduction : a data-driven approach. 12 2018.\n[55] A. Laudani, G. M. Lozito, F. R. Fulginei, and A. Salvini. On training efficiency and computational costs of a feed forward neural network : A review. Computational Intelligence and Neuroscience, 2015. doi : 10.1155/2015/818243.\n[56] S. Le Bot, R. Lafite, M. Fournier, A. Baltzer, and M. Desprez. Morphological and sedimentary impacts and recovery on a mixed sandy to pebbly seabed exposed to marine aggregate extraction (eastern english channel, france). Estuarine, Coastal and Shelf Science, 89(3) :221\u2013233, 2010.\n[57] O. P. Le Maitre, O. M. Knio, H. N. Najm, and R. G. Ghanem. A stochastic projection method for fluid flow : I. basic formulation. Journal of Computational Physics, 173(2) :481 \u2013 511, 2001. ISSN 0021-9991. doi : https://doi.org/10.1006/jcph.2001.6889. URL http://www.sciencedirect.com/science/article/ pii/S0021999101968895.\n[58] O. P. Le Maitre, M. T. Reagan, H. N. Najm, R. G. Ghanem, and O. M. Knio. A stochastic projection method for fluid flow : Ii. random process. Journal of Computational Physics, 181(1) :9 \u2013 44, 2002. ISSN 0021-9991. doi : https://doi.org/10.1006/jcph.2002.7104. URL http://www.sciencedirect.com/science/ article/pii/S0021999102971044.\n[59] Y. LeCun, Y. Bengio, and G. Hinton. Deep learning. Nature, 521 :436\u2013444, 2015. doi : https://doi.org/10. 1038/nature14539.\n[60] C. J. Legleiter, P. C. Kyriakidis, R. R. McDonald, and J. M. Nelson. Effects of uncertain topographic input data on two-dimensional flow modeling in a gravel-bed river. Water Resources Research, 47(3), 2011. doi : 10.1029/2010WR009618. URL https://agupubs.onlinelibrary.wiley.com/doi/abs/10. 1029/2010WR009618.\n[61] Y. Liang, H. Lee, S. Lim, W. Lin, K. Lee, and C. WU. Proper orthogonal decomposition and its applications \u2014 part i : Theory. Journal of Sound and Vibration, 252(3) :527 \u2013 544, 2002. ISSN 0022-460X. doi : https://doi.org/10.1006/jsvi.2001.4041. URL http://www.sciencedirect.com/science/article/ pii/S0022460X01940416.\n[62] Y. Liang, W. Lin, H. Lee, S. Lim, K. Lee, and H. Sun. Proper orthogonal decomposition and its applications \u2014 part ii : Model reduction for mems dynamical analysis. Journal of Sound and Vibration, 252(3) :527 \u2013 544, 2002. doi : https://doi.org/10.1006/jsvi.2002.5007.\n[63] T. E. Lovett, F. Ponci, and A. Monti. A polynomial chaos approach to measurement uncertainty. IEEE Transactions on Instrumentation and Measurement, 55(3) :729\u2013736, June 2006. ISSN 1557-9662. doi : 10.1109/TIM.2006.873807.\n[64] Z. Lu, H. Pu, F. Wang, Z. Hu, and L. Wang. The expressive power of neural networks : A view from the width. In I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30, pages 6231\u20136239. Curran Associates, Inc., 2017. URL http://papers.nips.cc/paper/ 7203-the-expressive-power-of-neural-networks-a-view-from-the-width.pdf.\n[65] D. Lucor, C.-H. Su, and G. E. Karniadakis. Generalized polynomial chaos and random oscillators. International Journal for Numerical Methods in Engineering, 60(3) :571\u2013596, 2004. doi : 10.1002/nme.976. URL https://onlinelibrary.wiley.com/doi/abs/10.1002/nme.976.\n[66] J. L. Lumley. The structure of inhomogeneous turbulent flows. Atmospheric Turbulence and Radio Wave Propagation, 1967.\n[67] M. Ma, J. Lu, and G. Tryggvason. Using statistical learning to close two-fluid multiphase flow equations for bubbly flows in vertical channels. International Journal of Multiphase Flow, 85 :336 \u2013 347, 2016. ISSN 0301-9322. doi : https://doi.org/10.1016/j.ijmultiphaseflow.2016.06.021. URL http://www.sciencedirect. com/science/article/pii/S0301932215302226.\n[68] C. Michel, S. Le Bot, F. Druine, S. Costa, F. Levoy, C. Dubrulle-Brunaud, and R. Lafite. Stages of sedimentary infilling in a hypertidal bay using a combination of sedimentological, morphological and dynamic criteria (bay of somme, france). Journal of Maps, 13(2) :858\u2013865, 2017.\n[69] S. Mika, B. Scho\u0308lkopf, A. J. Smola, K.-R. Mu\u0308ller, M. Scholz, and G. Ra\u0308tsch. Kernel pca and de-noising in feature spaces. In Advances in neural information processing systems, pages 536\u2013542, 1999.\n[70] K. Mills, M. Spanner, and I. Tamblyn. Deep learning and the Schro\u0308dinger equation. Phys. Rev. A, 96 :042113, Oct 2017. doi : 10.1103/PhysRevA.96.042113. URL https://link.aps.org/doi/10.1103/PhysRevA.96. 042113.\n[71] R. Morrow, L.-L. Fu, F. Ardhuin, M. Benkiran, B. Chapron, E. Cosme, F. d\u2019Ovidio, J. T. Farrar, S. T. Gille, G. Lapeyre, P.-Y. Le Traon, A. Pascual, A. Ponte, B. Qiu, N. Rascle, C. Ubelmann, J. Wang, and E. D. Zaron. Global observations of fine-scale ocean surface topography with the surface water and ocean topography (swot) mission. Frontiers in Marine Science, 6 :232, 2019. ISSN 2296-7745. doi : 10.3389/fmars.2019.00232. URL https://www.frontiersin.org/article/10.3389/fmars.2019.00232.\n[72] A. Mosavi, S. Shamshirband, E. Salwana, K.-w. Chau, and J. H. Tah. Prediction of multi-inputs bubble column reactor using a novel hybrid model of computational fluid dynamics and machine learning. Engineering Applications of Computational Fluid Mechanics, 13(1) :482\u2013492, 2019.\n[73] M. Muller. On the POD method : an abstract investigation with applications to reduced-order modeling and suboptimal control. PhD thesis, 2008.\n[74] E. Muravleva, I. Oseledets, and D. Koroteev. Application of machine learning to viscoplastic flow modeling. Physics of Fluids, 30(10) :103102, 2018. doi : 10.1063/1.5058127. URL https://doi.org/10.1063/1. 5058127.\n[75] W. J. Murdoch, C. Singh, K. Kumbier, R. Abbasi-Asl, and B. Yu. Interpretable machine learning : definitions, methods, and applications. arXiv preprint arXiv :1901.04592, 2019.\n[76] H. N. Southgate, K. Wijnberg, M. Larson, M. Capobianco, and H. Jansen. Analysis of field data of coastal morphological evolution over yearly and decadal timescales. part 2 : Non-linear techniques. Journal of Coastal Research, 19, 09 2003.\n[77] J. B. Nagel, J. Rieckermann, and B. Sudret. Principal component analysis and sparse polynomial chaos expansions for global sensitivity analysis and model calibration : Application to urban drainage simulation. Reliability Engineering & System Safety, 195 :106737, 2020. ISSN 0951-8320. doi : https://doi.org/10.1016/ j.ress.2019.106737. URL http://www.sciencedirect.com/science/article/pii/S0951832019301747.\n[78] NASA (National Aeronautics and Space Administration) and CNES (Centre National d\u2019Etudes Spatiales) in partnership with CSA (Canadian Space Agency) and UKSA (UK Space Agency). Surface water and ocean topography. https://swot.jpl.nasa.gov/home.htm, 2021.\n[79] A. Ng. Machine learning lecture notes. http://cs229.stanford.edu/materials.html. Stanford Univ. TR, Stanford, CA, 2014.\n[80] B. T. Nguyen, A. Samimi, and J. J. Simpson. A polynomial chaos approach for em uncertainty propagation in 3d-fdtd magnetized cold plasma. In 2015 IEEE Symposium on Electromagnetic Compatibility and Signal Integrity, pages 356\u2013360, March 2015. doi : 10.1109/EMCSI.2015.7107714.\n[81] R. Noori, A. Karbassi, A. Moghaddamnia, D. Han, M. Zokaei-Ashtiani, A. Farokhnia, and M. G. Gousheh. Assessment of input variables determination on the svm model performance using pca, gamma test, and forward selection techniques for monthly stream flow prediction. Journal of Hydrology, 401(3) :177 \u2013 189, 2011. ISSN 0022-1694. doi : https://doi.org/10.1016/j.jhydrol.2011.02.021. URL http://www.sciencedirect.com/ science/article/pii/S0022169411001363.\n[82] A. T. N. Papanicolaou, M. Elhakeem, G. Krallis, S. Prakash, and J. Edinger. Sediment transport modeling review&#x2014 ;current and future developments. Journal of Hydraulic Engineering, 134(1) :1\u201314, 2008. doi : 10.1061/(ASCE)0733-9429(2008)134:1(1). URL https://ascelibrary.org/doi/abs/10.1061/ %28ASCE%290733-9429%282008%29134%3A1%281%29.\n[83] M. S. Parsons. Interpretation of machine-learning-based disruption models for plasma control. Plasma Physics and Controlled Fusion, 59(8) :085001, jun 2017. doi : 10.1088/1361-6587/aa72a3. URL https: //doi.org/10.1088%2F1361-6587%2Faa72a3.\n[84] S. Paul and M. K. Verma. Proper Orthogonal Decomposition vs. Fourier Analysis for Extraction of Large-Scale Structures of Thermal Convection, pages 433\u2013441. 2017.\n[85] REFMAR. Re\u0301seaux de Re\u0301fe\u0301rence des observations MARe\u0301graphiques, 2020.\n[86] A. Rigos, G. E. Tsekouras, A. Chatzipavlis, and A. F. Velegrakis. Modeling Beach Rotation Using a Novel Legendre Polynomial Feedforward Neural Network Trained by Nonlinear Constrained Optimization. In L. Iliadis and I. Maglogiannis, editors, Artificial Intelligence Applications and Innovations, pages 167\u2013179, Cham, 2016. Springer International Publishing. ISBN 978-3-319-44944-9.\n[87] D. Rolnick and M. Tegmark. The power of deeper networks for expressing natural functions, 2017.\n[88] B. Rouet-Leduc, C. Hulbert, N. Lubbers, K. Barros, C. J. Humphreys, and P. A. Johnson. Machine learning predicts laboratory earthquakes. Geophysical Research Letters, 44(18) :9276\u20139282, 2017. doi : 10.1002/ 2017GL074677. URL https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1002/2017GL074677.\n[89] P. Saini, C. M. Arndt, and A. M. Steinberg. Development and evaluation of gappy-pod as a data reconstruction technique for noisy piv measurements in gas turbine combustors. Experiments in Fluids, 57(7) :122, 2016.\n[90] P. J. Schmid. Dynamic mode decomposition of numerical and experimental data. Journal of Fluid Mechanics, 656 :5\u201328, 2010. doi : 10.1017/S0022112010001217.\n[91] J. Schmidhuber. Deep learning in neural networks : An overview. Neural Networks, 61 :85 \u2013 117, 2015. ISSN 0893-6080. doi : https://doi.org/10.1016/j.neunet.2014.09.003. URL http://www.sciencedirect. com/science/article/pii/S0893608014002135.\n[92] P. D. Sclavounos, Y. Ma, P. D. Sclavounos, and Y. Ma. Artificial intelligence machine learning in marine hydrodynamics. In Proceedings of the ASME 2018 37th International Conference on Ocean, Offshore and Arctic Engineering,17-22 June, Madrid, Spain, ASME, 2018, 06 2018. doi : 10.1115/OMAE2018-77599. URL https://doi.org/10.1115/OMAE2018-77599.\n[93] J. Senent-Aparicio, P. Jimeno-Sa\u0301ez, A. Bueno-Crespo, J. Pe\u0301rez-Sa\u0301nchez, and D. Pulido-Vela\u0301zquez. Coupling machine-learning techniques with swat model for instantaneous peak flow prediction. Biosystems Engineering, 177 :67 \u2013 77, 2019. ISSN 1537-5110. doi : https://doi.org/10.1016/j.biosystemseng.2018.04.022. URL http://www.sciencedirect.com/science/article/pii/S1537511017311686. Intelligent Systems for Environmental Applications.\n[94] S. Sengupta, S. Basak, P. Saikia, S. Paul, V. Tsalavoutis, F. Atiah, V. Ravi, and A. Peters. A review of deep learning with special emphasis on architectures, applications and recent trends, 2019.\n[95] S. Shalev-Shwartz and S. Ben-David. Understanding Machine Learning : From Theory to Algorithms. Cambridge University Press, 2014.\n[96] L. Sirovich. Turbulence and the dynamics of coherent structures : I, ii and iii. Quarterly Applied Mathematics, 45 :561, 1987.\n[97] C. Soize and R. Ghanem. Physical systems with random uncertainties : chaos representations with arbitrary probability measure. SIAM J. Sci. Comput., pages 26(2), 395\u2013410, 2004.\n[98] T. Sruthi, K. Ranjith, and V. Chandra. Control of sediment entry into an intake canal by using submerged vanes. In AIP Conference Proceedings, volume 1875, page 030007. AIP Publishing LLC, 2017.\n[99] B. Sudret. Global sensitivity analysis using polynomial chaos expansions. Reliability Engineering & System Safety, 93(7) :964 \u2013 979, 2008. ISSN 0951-8320. doi : https://doi.org/10.1016/j.ress.2007.04.002. URL http:// www.sciencedirect.com/science/article/pii/S0951832007001329. Bayesian Networks in Dependability.\n[100] B. Sudret. Polynomial chaos expansions and stochastic finite element methods, page 624. CRC PressEditors : Kok-Kwang Phoon, Jianye Ching, 12 2014.\n[101] K. Taira, S. L. Brunton, S. T. M. Dawson, C. W. Rowley, T. Colonius, B. J. McKeon, O. T. Schmidt, S. Gordeyev, V. Theofilis, and L. S. Ukeiley. Modal analysis of fluid flows : An overview. AIAA Journal, 55 (12) :4013\u20134041, 2017. doi : 10.2514/1.J056060. URL https://doi.org/10.2514/1.J056060.\n[102] A. Tarakanov and A. H. Elsheikh. Regression-based sparse polynomial chaos for uncertainty quantification of subsurface flow models. Journal of Computational Physics, 399 :108909, 2019. ISSN 0021-9991. doi : https://doi.org/10.1016/j.jcp.2019.108909. URL http://www.sciencedirect.com/science/article/pii/ S002199911930614X.\n[103] E. Torre, S. Marelli, P. Embrechts, and B. Sudret. Data-driven polynomial chaos expansion for machine learning regression. Journal of Computational Physics, 2019.\n[104] M. Tsang, D. Cheng, and Y. Liu. Detecting statistical interactions from neural network weights. arXiv preprint arXiv :1705.04977, 2017.\n[105] L. C. van Rijn. Unified view of sediment transport by currents and waves. i : Initiation of motion, bed roughness, and bed-load transport. Journal of Hydraulic Engineering, 133(6) :649\u2013667, 2007. doi : 10.1061/(ASCE)0733-9429(2007)133:6(649). URL https://ascelibrary.org/doi/abs/10.1061/%28ASCE% 290733-9429%282007%29133%3A6%28649%29.\n[106] J. VanderPlas, A. J. Connolly, Z. Ivezic, and A. Gray. Introduction to astroml : Machine learning for astrophysics. In 2012 Conference on Intelligent Data Understanding, pages 47\u201354, Oct 2012. doi : 10.1109/ CIDU.2012.6382200.\n[107] X. Wan and G. Karniadakis. An adaptive multi-element generalized polynomial chaos method for stochastic differential equations. Journal of Computational Physics, 2006.\n[108] Q. Wang, J. S. Hesthaven, and D. Ray. Non-intrusive reduced order modeling of unsteady flows using artificial neural networks with application to a combustion problem. Journal of Computational Physics, 384 :289 \u2013 307, 2019. ISSN 0021-9991. doi : https://doi.org/10.1016/j.jcp.2019.01.031. URL http://www.sciencedirect. com/science/article/pii/S0021999119300828.\n[109] N. Wiener. The homogeneous chaos. American Journal of Mathematics, pages 60, 897\u2013936, 1938.\n[110] S. Wilkinson, S. Hanna, L. Hesselgren, and V. Mueller. Inductive aerodynamics. In Proceedings of eCAADe 2013 : Computation and Performance. pp.39-48., 09 2013.\n[111] J. A. Witteveen and H. Bijl. Modeling Arbitrary Uncertainties Using Gram-Schmidt Polynomial Chaos. 2006. doi : 10.2514/6.2006-896. URL https://arc.aiaa.org/doi/abs/10.2514/6.2006-896.\n[112] D. Xiu and G. Karniadakis. Modelling uncertainty in steady state diffusion problems via generalized polynomial chaos. Comput. Methods Appl. Mec. Engrg, pages 191(43), 4927\u20134948, 2003.\n[113] D. Xiu and G. E. Karniadakis. The wiener\u2013askey polynomial chaos for stochastic differential equations. SIAM Journal on Scientific Computing, 24(2) :619\u2013644, 2002. doi : 10.1137/S1064827501387826. URL https://doi.org/10.1137/S1064827501387826.\n[114] D. Xiu and G. E. Karniadakis. Modeling uncertainty in flow simulations via generalized polynomial chaos. Journal of Computational Physics, 187(1) :137 \u2013 167, 2003. ISSN 0021-9991. doi : https: //doi.org/10.1016/S0021-9991(03)00092-5. URL http://www.sciencedirect.com/science/article/pii/ S0021999103000925.\n[115] L. Zhu, W. Zhang, J. Kou, and Y. Liu. Machine learning methods for turbulence modeling in subsonic flows around airfoils. Physics of Fluids, 31(1) :015105, 2019. doi : 10.1063/1.5061693. URL https://doi.org/10. 1063/1.5061693."}], "title": "Physically interpretable machine learning algorithm on multidimensional non-linear fields", "year": 2020}