{
    "abstractText": "The last decade has seen huge progress in the development of advanced machine learning models; however, those models are powerless unless human users can interpret them. Here we show how the mind\u2018s construction of concepts and meaning can be used to create more interpretable machine learning models. By proposing a novel method of classifying concepts, in terms of \u2018form\u2019 and \u2018function\u2019, we elucidate the nature of meaning and offer proposals to improve model understandability. As machine learning begins to permeate daily life, interpretable models may serve as a bridge between domain-expert authors and non-expert users.",
    "authors": [
        {
            "affiliations": [],
            "name": "Nick Condry"
        }
    ],
    "id": "SP:d5f69b892f783db6ef23a7a0904ef4a0fe226708",
    "references": [
        {
            "authors": [
                "Albert",
                "William",
                "Tullis",
                "Thomas"
            ],
            "title": "Measuring the user experience: collecting, analyzing, and presenting usability metrics",
            "year": 2013
        },
        {
            "authors": [
                "Bekkerman",
                "Ron",
                "El-Yaniv",
                "Ran",
                "Tishby",
                "Naftali",
                "Winter",
                "Yoad"
            ],
            "title": "Distributional word clusters vs. words for text categorization",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2003
        },
        {
            "authors": [
                "Bose",
                "Indranil",
                "Mahapatra",
                "Radha K"
            ],
            "title": "Business data mining: a machine learning perspective",
            "venue": "Information & management,",
            "year": 2001
        },
        {
            "authors": [
                "Carey",
                "Susan"
            ],
            "title": "Conceptual change in childhood",
            "venue": "MIT press,",
            "year": 1985
        },
        {
            "authors": [
                "Caruana",
                "Rich",
                "De Sa",
                "Virginia R"
            ],
            "title": "Benefitting from the variables that variable selection discards",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2003
        },
        {
            "authors": [
                "Cleophas",
                "Ton J",
                "Zwinderman",
                "Aeilko H"
            ],
            "title": "Machine learning in medicine",
            "year": 2013
        },
        {
            "authors": [
                "T.G. Dietterich",
                "R.L. London",
                "K. Clarkson",
                "G. Dromey"
            ],
            "title": "Learning and inductive inference",
            "venue": "The Handbook of Artificial Intelligence, chapter XIV,",
            "year": 1982
        },
        {
            "authors": [
                "D.H. Fisher",
                "M.J. Pazzani",
                "P. Langley"
            ],
            "title": "Concept formation: Knowledge and experience in unsupervised learning",
            "year": 2014
        },
        {
            "authors": [
                "Forman",
                "George"
            ],
            "title": "An extensive empirical study of feature selection metrics for text classification",
            "venue": "The Journal of machine learning research,",
            "year": 2003
        },
        {
            "authors": [
                "Foster",
                "Kenneth R",
                "Koprowski",
                "Robert",
                "Skufca",
                "Joseph D"
            ],
            "title": "Machine learning, medical diagnosis, and biomedical engineering research-commentary",
            "venue": "Biomedical engineering online,",
            "year": 2014
        },
        {
            "authors": [
                "Goldstone",
                "Robert L",
                "Rogosky",
                "Brian J"
            ],
            "title": "Using relations within conceptual systems to translate across conceptual systems",
            "year": 2002
        },
        {
            "authors": [
                "Goodman",
                "Noah D",
                "Tenenbaum",
                "Joshua B",
                "Feldman",
                "Jacob",
                "Griffiths",
                "Thomas L"
            ],
            "title": "A rational analysis of rule-based concept learning",
            "venue": "Cognitive Science,",
            "year": 2008
        },
        {
            "authors": [
                "Gopnik",
                "Alison",
                "Meltzoff",
                "Andrew N"
            ],
            "title": "Words, thoughts, and theories",
            "year": 1997
        },
        {
            "authors": [
                "Ishibuchi",
                "Hisao",
                "Nojima",
                "Yusuke"
            ],
            "title": "Analysis of interpretability-accuracy tradeoff of fuzzy systems by multiobjective fuzzy genetics-based machine learning",
            "venue": "International Journal of Approximate Reasoning,",
            "year": 2007
        },
        {
            "authors": [
                "Kao",
                "Anne",
                "Poteet",
                "Steve R"
            ],
            "title": "Natural language processing and text mining",
            "venue": "Springer Science & Business Media,",
            "year": 2007
        },
        {
            "authors": [
                "Katz",
                "Jeffrey S",
                "Wright",
                "Anthony A",
                "Bodily",
                "Kent D"
            ],
            "title": "Issues in the comparative cognition of abstract-concept learning",
            "venue": "Comparative Cognition & Behavior Reviews,",
            "year": 2007
        },
        {
            "authors": [
                "Kihlstrom",
                "John F"
            ],
            "title": "The cognitive unconscious",
            "venue": "Science, 237(4821):1445\u20131452,",
            "year": 1987
        },
        {
            "authors": [
                "Landauer",
                "Thomas K",
                "McNamara",
                "Danielle S",
                "Dennis",
                "Simon",
                "Kintsch",
                "Walter"
            ],
            "title": "Handbook of latent semantic analysis",
            "year": 2013
        },
        {
            "authors": [
                "O\u2019Reilly",
                "Randall C",
                "Munakata",
                "Yuko"
            ],
            "title": "Computational explorations in cognitive neuroscience: Understanding the mind by simulating the brain",
            "venue": "MIT press,",
            "year": 2000
        },
        {
            "authors": [
                "Reber",
                "Arthur S"
            ],
            "title": "Implicit learning of artificial grammars",
            "venue": "Journal of verbal learning and verbal behavior,",
            "year": 1967
        },
        {
            "authors": [
                "Reber",
                "Arthur S"
            ],
            "title": "The cognitive unconscious: An evolutionary perspective",
            "venue": "Consciousness and cognition,",
            "year": 1992
        },
        {
            "authors": [
                "R\u00fcping",
                "Stefan"
            ],
            "title": "Learning interpretable models",
            "venue": "PhD thesis, University of Dortmund,",
            "year": 2006
        },
        {
            "authors": [
                "Sanders",
                "Raymond E",
                "Gonzalez",
                "Eulalio G",
                "Murphy",
                "Martin D",
                "Liddle",
                "Cherie L",
                "Vitina",
                "John R"
            ],
            "title": "Frequency of occurrence and the criteria for automatic processing",
            "venue": "Journal of Experimental Psychology: Learning, Memory, and Cognition,",
            "year": 1987
        },
        {
            "authors": [
                "Schyns",
                "Philippe G",
                "Goldstone",
                "Robert L",
                "Thibaut",
                "Jean-Pierre"
            ],
            "title": "The development of features in object concepts",
            "venue": "Behavioral and brain Sciences,",
            "year": 1998
        },
        {
            "authors": [
                "Seger",
                "Carol Augart"
            ],
            "title": "Implicit learning",
            "venue": "Psychological bulletin,",
            "year": 1994
        },
        {
            "authors": [
                "Vellido",
                "Alfredo",
                "Martin-Guerrero",
                "Jose David",
                "Lisboa",
                "Paulo JG"
            ],
            "title": "Making machine learning models interpretable",
            "venue": "In ESANN,",
            "year": 2012
        },
        {
            "authors": [
                "Weigend",
                "Andreas S",
                "Rumelhart",
                "David E",
                "Huberman",
                "Bernardo A"
            ],
            "title": "Generalization by weight-elimination with application to forecasting",
            "venue": "In NIPS,",
            "year": 1990
        },
        {
            "authors": [
                "Weston",
                "Jason",
                "Elisseeff",
                "Andr\u00e9",
                "Sch\u00f6lkopf",
                "Bernhard",
                "Tipping",
                "Mike"
            ],
            "title": "Use of the zero norm with linear models and kernel methods",
            "venue": "The Journal of Machine Learning Research,",
            "year": 2003
        }
    ],
    "sections": [
        {
            "heading": "1. Introduction",
            "text": "In the last decade, machine learning algorithms have made huge strides, producing state-of-the-art results across a number of domains including image recognition, speech recognition, and natural language processing. However, while such results are exciting, there currently exists a gap between data modeling and knowledge extraction (Vellido et al., 2012). Machine learning models are rendered powerless unless they can be interpreted, thus in order for knowledge to be extracted from a model, we must account for the human cognitive factors involved in such a process. Interpretation must therefore be accounted for in machine learning processes, as shown in Figure 1. In addition to promoting more transparent results, interpretable models enable non-experts to utilize machine learning tools. For example, a business manager is more likely to accept a model\u2018s recommendations if its results can be presented in business terms (Bose & Mahapatra, 2001). As an ever-growing number of professionals come to rely on machine learning tools, the most successful models will provide an elegant\n2016 ICML Workshop on Human Interpretability in Machine Learning (WHI 2016), New York, NY, USA. Copyright by the author(s).\nuser experience, presenting users with information and intelligence that are easily interpretable.\nIn the formal logic sense, an interpretation is a mapping of a formal construct to the entities and their relations it represents (Ru\u0308ping, 2006). Less formally, interpretability can be seen as a signaling problem; a model must present its output such that a specific meaning is conveyed to its user. To understand how to convey meaning, we must first understand the nature of meaning itself. Therefore, in order to design models for interpretability, we must first investigate the processes by which humans assign meaning to symbols, and how the mind extracts knowledge from information.\nWhereas previous investigations into machine learning interpretability have largely focused on the relation between accuracy and interpretability, algorithm and feature selection, and model visualizations (e.g. (Ru\u0308ping, 2006; Ishibuchi & Nojima, 2007), we will instead focus on the psychology of human concept learning. Using a relational model of meaning, we will propose a novel method of classifying concepts according to their structure and function within a given context. Based upon that method, we will offer several proposals to improve non-expert understanding of machine learning tools at a conceptual level.\nar X\niv :1\n60 7.\n00 27\n9v 1\n[ st\nat .M\nL ]\n1 J\nul 2\n01 6"
        },
        {
            "heading": "2. Implicit Learning and Feature Extraction",
            "text": "Humans are organisms that have evolved to learn from experience, evaluating novel stimuli through a process of comparison to previously stored stimuli. While learning, in the traditional sense of schooling and education, is an active process, in order to investigate the basis of knowledge, we\u2018ll have to begin at the sub-conceptual and subconscious level.\nThe mind constantly and implicitly processes complex information in an incidental manner, without direct awareness of what has been learned (Seger, 1994). This process of passive knowledge acquisition is known as implicit learning.\nImplicit learning began as a field of study with A.S. Reber\u2018s work in the late 1960\u2018s, and has been proposed as an evolutionary ancestor of explicit thought (Reber, 1967; 1992). This process occurs automatically, and represents the subtle yet constant re-wiring of a brain\u2018s neurons as they adapt in response to new stimuli (Sanders et al., 1987). Most importantly, implicit learning occurs at the subconscious, or preconscious level; therefore, the knowledge gained is subconceptual, which is to say, the patterns learned are not immediately associated with a reference symbol (Kihlstrom, 1987). Instead, this process extracts relevant features from the local environment via the mind\u2018s lower level perceptual processes (Schyns et al., 1998). A feature is an individual measurable property of a phenomenon being observed (Bishop, 2006). Features may be continuous or categorical, and they comprise the most basic building block of human knowledge (Schyns et al., 1998)."
        },
        {
            "heading": "3. From Features to Concepts",
            "text": "The process of feature extraction is constant and unconscious; to bring this knowledge into the conscious domain requires conceptualization (Goodman et al., 2008). A concept is an abstract system composed of a set of features paired with a symbolic representation. In many ways, conceptualization mirrors a simple dictionary structure, where the symbol acts as the key, and its associated feature set is the value. The symbolic representation can be any real or abstract token, including images, sounds, and smells. However, the most common form of symbolic representation is a word, a character or combination of characters. For example, the concept of a dog might contain the features [furry: yes, ears: 2, legs: 4, tail: yes] and would be denoted by the character string: \u2018dog\u2018. Since concepts are composed of a multi-dimensional set of features, they are inherently complex symbolic objects.\nConcepts are abstract, meaning they can be applied to novel stimuli, and concept learning relies on incremental assumptions (Katz et al., 2007). The mind, as a concept formation\nsystem, accepts a stream of observations (i.e. events, objects, instances), and discovers a classification scheme over the data stream. Learning occurs not as a single event but as a continuous process; the mind\u2018s classification scheme evolves and changes as new observations are processed (Fisher et al., 2014). Figure 2 (Dietterich et al., 1982) demonstrates this incremental learning process by which an agent adapts to its environment, organizing experiences to improve its performance (Fisher et al., 2014).\nThis view of learning demonstrates that learning is not a discrete act, but rather a continuous process by which new information contributes to the evolution of existing concepts and the formation of new concepts. Furthermore, it aligns with (Ru\u0308ping, 2006) heuristic of interpretability, which states, people tend to find those things understandable, that they already know. Thus, when building a meaningful model, the intended audience must be taken into account when structuring output. If the output can be phrased or structured in a familiar way, subjects will be more likely to implicitly trust and utilize the information."
        },
        {
            "heading": "4. From Concepts to Meaning",
            "text": "Having established concepts as a system composed of a [key, value] pair, where the key is a symbol and the value is the associated feature set, we can look at meaning. The word meaning is often used in a variety of ways, from Plato\u2018s physically irreducible mystical essences to ideas of how words are used (Ludwig, 1953). Here, I will offer a view which finds its roots in connectionist psychological models, but until recently was unrealized at scale (O\u2019Reilly & Munakata, 2000). This view holds that since words simply denote clusters of features, words themselves have no inherent meaning; stripped of its associated features, a word is simply a meaningless symbol. Instead, meaning arises from the cognitive mapping of a word (or symbol) onto an underlying feature map (Landauer et al., 2013). For example, to someone with no knowledge of the English language, the word \u2018tree\u2019 would mean nothing, as their mind has not mapped the symbol to a set of features. However, to a native speaker, not only would \u2018tree\u2019 have meaning,\nbut they could likely identify \u2018forest\u2019 as a similar concept, due to their overlapping feature sets. This theory of meaning has gained validation from the rise of latent semantic analysis (LSA) techniques, which construct models from the implicit relational mapping of a text. This \u2018map\u2018 does not exist in itself, it is an abstraction an infinite number of point-to-point distances computed by triangulation from earlier established points (Landauer et al., 2013). However, models created in the manner have proven highly accurate, and overlaying word-symbols on top of such maps have produced highly intuitive results.\nUsing this approach, we can view \u2018meaning\u2018 as a fundamentally relational property, as a word\u2018s relation to the semantic system in which it exists defines its meaning. Importantly, this leads us to realize that to efficiently convey meaning, we must start at the sub-conceptual level by identifying the specific information we hope to convey, then crafting a message such that it conveys the intended features given the context of audience. Given this theory, I will use the word \u201cmeaning\u201d as shorthand for \u201cthe set of features associated with a symbol, given context\u201d."
        },
        {
            "heading": "5. The Form and Function of Concepts",
            "text": "The relational theory of meaning holds that a symbol, say, a word or an image, may hold different meanings in different contexts, given that it interacts with those contexts differently. While this might seem to imply that words cannot be assigned any true meaning, in practice this is not the case. Through shared communication protocols such as language, individually relative meanings solidify into a statistically canonical cultural form (Goldstone & Rogosky, 2002).\nNevertheless, this theory lacks a direct explanation of the relationship between a symbol and meaning. We posit that this relationship can be best understood in terms of form and function. The function of a concept is its meaning, given context, and it represents how the concept interacts with its larger semantic context. Concepts that share their function are synonyms (Kao & Poteet, 2007). The form is the specific instance of the class of objects defined by the object\u2019s function. For example, compare the following three phrases, \u201cI\u2018m going to the store\u201d, \u201cI\u2018m heading to the store\u201d, and \u201cI\u2018m heading the soccer ball\u201d. Given the context of the first two phrases, \u201cgoing\u201d and \u201cheading\u201d share the same meaning, and can thus be considered different forms, or instances, of the same conceptual function, or class. Given the context of the second two phrases, the conceptual form, \u201cheading\u201d, is the same, but its function differs.\nIn some aspects, this categorization of concepts by form and function represents an extension of the \u201ctheory theory\u201d\nof concepts in which concepts are composed of core and peripheral features (see: (Carey, 1985; Gopnik & Meltzoff, 1997). An object\u2019s core features are its causally deepest properties, whereas peripheral features refer to incidental features of a concept that do not directly define its nature. These descriptions of features as either core or peripheral are useful in qualitative description, but are difficult to translate into more technical contexts. Instead, we propose that function best encapsulates the meaning of core features, and form best encapsulates the meaning of peripheral features. The essence, or core of a concept, is its meaning, defined by the concept\u2019s function within a given context. Peripheral qualities, or form, are in turn best understood as the characteristics of a specific object. For example, within the simple context presented in Figure 3, the rock interacts with a piece of paper by resting on top of it. While the form of the rock may be a small, grey, 2lb stone, within the given context, its function is to apply downward force on the paper, therefore its meaning is \u2018paper weight\u2019. Similarly, as the paper supports the rock, from the rock\u2019s perspective, the function of the paper is support, so its meaning is ground. Forms can change without altering the operation of a system, so long as the object retains it\u2019s function.\nSince an object\u2019s meaning is defined by interaction with its context, and the interaction can be viewed as a function, a relationship between inputs and output, meaning can be understood as a function within a larger process of interaction."
        },
        {
            "heading": "6. Proposals to Improve Meaningful Models",
            "text": "6.1. Clearly Outline a Model\u2019s Function\nThe function of a concept is defined by the change it enacts on its context, and thus represents a transition from an initial state to an output state. Thus, to improve model interpretability, models should have very clearly defined requirements for input and the goals of the output. For example, doctors might be supplied with a few models that\nperform different tasks, including quality-of-life (QOL) assessments, anomaly detection, and DNA sequence mining (Cleophas & Zwinderman, 2013). Authors of such models should clearly state the purpose and intended applications of their work. If the model is only intended to perform exploratory data analysis, the author should emphasize in their discussion that confirmatory data analysis is required (Foster et al., 2014). Furthermore, authors should directly address the transportability of the model, i.e. which aspects of the method can be directly used in novel situations, and which aspects must be tuned for further application.\nAdditionally, authors should minimize the number of attributes in their classifiers. Minimizing attributes creates a simpler, and therefore more interpretable, form of the model, and also decreases the risk of overfitting, especially in smaller studies. One approach to limit attributes might involve variable ranking (see: (Bekkerman et al., 2003; Caruana & De Sa, 2003; Forman, 2003; Weston et al., 2003)). Another viable method proposed by (Weigend et al., 1990) pares down variables using a weight elimination algorithm."
        },
        {
            "heading": "6.2. Place the Model in Context",
            "text": "In addition to specifying the purpose and scope-of-use of a model, authors should attempt to construct models such that they complement and expedite existing processes. In doing so, the meaning of their model will be elucidated by its context in the existing process. For example, in the early stages of developing a medical diagnostic imaging application, it is impossible to conclusively prove that the application works, but possible to prove that it does not work (Foster et al., 2014). If the latter is the case, it is best to discover such quickly, so that new processes and applications may be developed. A model in this process would become more meaningful, by virtue of having a clearly defined function within the scope of a larger system. Additionally, incorporating models into existing processes forces those models to incorporate some level of domain knowledge, and serve as useful tools rather than complete solutions unto themselves."
        },
        {
            "heading": "6.3. Design for User Experience",
            "text": "Finally, when developing models that aim to solve specific problems within a given domain area, thought should be given to preparing a front-end for users within that domain. A well-designed front-end would ideally accomplish the above proposals by clearly specifying required inputs, presenting coherent outputs, and positioning the model as a tool within a larger process or framework. Current developments of machine learning platforms such as Google Cloud Platform, Amazon Machine Learning, Microsoft Azure, and H20.ai have made strong progress in this regard, com-\nbining powerful models with intuitive representations.\nWhile the algorithms and structure of the model itself accounts for the model\u2018s function, a cohesive front-end provides an overlaid form for the information conveyed. Essentially, this front-end can be viewed as a translation between the direct model output and a non-expert user. This translation should capitalize on the fundamentals of human concept acquisition by providing both information in a familiar format, and context. To this end, authors should focus on key user experience metrics, such as: will the users recommend the tool? Does this tool create a more efficient or effective process? What are the most significant usability problems with the tool? Are usability improvements being made from one version to the next (Albert & Tullis, 2013)? These questions place an emphasis on considering the understandability of a model in the design of the algorithms. Interpretability is difficult to achieve as a postprocessing step; the relationship between understandability and accuracy must be accounted for from the start (Ru\u0308ping, 2006)."
        },
        {
            "heading": "7. Conclusion",
            "text": "We have analyzed the psychology of human concept learning, and identified how the mind\u2018s construction of concepts and meaning can be used to create more interpretable machine learning models. Meaning arises from the interaction of a concept within a specified context. Furthermore, the identity of an object and its meaning can be fully described by two traits: form and function. Form describes the exact qualities and structure of an object, while function describes the object\u2018s meaning as a function of its interaction in its context. Furthermore, this promotes a view of concepts as functions in context, which allows them to be conceptualized as a relationship between input and output.\nThus, the interpretability of a model on a conceptual level can be bolstered by clearly indicating the model\u2018s input requirements and output goals, and providing context for the model within a larger process. Additionally, these goals may be combined through the development of a cohesive front-end to present information in a familiar format and expand the usability of a model to non-expert users."
        }
    ],
    "title": "Meaningful Models: Utilizing Conceptual Structure to Improve Machine Learning Interpretability",
    "year": 2018
}