{
  "abstractText": "Modern statistical machine learning (SML) methods share a major limitation with the early approaches to AI: there is no scalable way to adapt them to new domains. Human learning solves this in part by leveraging a rich, shared, updateable world model. Such scalability requires modularity: updating part of the world model should not impact unrelated parts. We have argued that such modularity will require both \u201ccorrectability\u201d (so that errors can be corrected without introducing new errors) and \u201cinterpretability\u201d (so that we can understand what components need correcting). To achieve this, one could attempt to adapt state of the art SML systems to be interpretable and correctable; or one could see how far the simplest possible interpretable, correctable learning methods can take us, and try to control the limitations of SML methods by applying them only where needed. Here we focus on the latter approach and we investigate two main ideas: \u201cTeacher Assisted Learning\u201d, which leverages crowd sourcing to learn language; and \u201cFactored Dialog Learning\u201d, which factors the process of application development into roles where the language competencies needed are isolated, enabling non-experts to quickly create new applications. We test these ideas in an \u201cAutomated Personal Assistant\u201d (APA) setting, with two scenarios: that of detecting user intent from a user-APA dialog; and that of creating a class of event reminder applications, where a non-expert \u201cteacher\u201d can then create specific apps. For the intent detection task, we use a dataset of a thousand labeled utterances from user dialogs with Cortana, and we show that our approach matches state of the art SML methods, but in addition provides full transparency: the whole (editable) model can be summarized on one human-readable page. For the reminder app task, we ran small user studies to verify the efficacy of the approach. \u2217Currently at Carnegie Mellon University. This work was done while Yang was an intern at MSR. 1 ar X iv :1 61 2. 07 89 6v 1 [ cs .A I] 2 3 D ec 2 01 6 A Base Camp for Scaling AI",
  "authors": [
    {
      "affiliations": [],
      "name": "C.J.C. Burges"
    },
    {
      "affiliations": [],
      "name": "T. Hart"
    },
    {
      "affiliations": [],
      "name": "Z. Yang"
    }
  ],
  "id": "SP:ef3f4dba0c47583e3ad2150793533881defaf173",
  "references": [
    {
      "authors": [
        "B.W. Boehm"
      ],
      "title": "A spiral model of software development and enhancement",
      "venue": "Computer, 21(5):61\u2013 72",
      "year": 1988
    },
    {
      "authors": [
        "C.J.C. Burges"
      ],
      "title": "Towards the machine comprehension of text: An essay",
      "venue": "Technical Report MSR-TR-2013-125, Microsoft Research",
      "year": 2013
    },
    {
      "authors": [
        "D. Crevier"
      ],
      "title": "AI: The tumultuous history of the search for artificial intelligence",
      "venue": "Basic Books, Inc.",
      "year": 1993
    },
    {
      "authors": [
        "C. Fellbaum"
      ],
      "title": "WordNet",
      "venue": "Wiley Online Library",
      "year": 1998
    },
    {
      "authors": [
        "I. Goodfellow",
        "Y. Bengio",
        "A. Courville"
      ],
      "title": "Deep learning",
      "venue": "Book in preparation for MIT Press: http://www.deeplearningbook.org",
      "year": 2016
    },
    {
      "authors": [
        "B. Hixon",
        "P. Clark",
        "H. Hajishirzi"
      ],
      "title": "Learning knowledge graphs for question answering through conversational dialog",
      "venue": "Proceedings of the the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Denver, Colorado, USA",
      "year": 2015
    },
    {
      "authors": [
        "Z. Hu",
        "X. Ma",
        "Z. Liu",
        "E. Hovy",
        "E. Xing"
      ],
      "title": "Harnessing deep neural networks with logic rules",
      "venue": "54th Annual Meeting of the Association for Computational Linguistics",
      "year": 2016
    },
    {
      "authors": [
        "S. Kim",
        "R.E. Banchs",
        "H Li"
      ],
      "title": "Exploring convolutional and recurrent neural networks in sequential labelling for dialogue topic tracking",
      "venue": "54th Annual Meeting of the Association for Computational Linguistics",
      "year": 2016
    },
    {
      "authors": [
        "J. Li",
        "M. Galley",
        "C. Brockett",
        "J. Gao",
        "B. Dolan"
      ],
      "title": "A persona-based neural conversation model",
      "venue": "54th Annual Meeting of the Association for Computational Linguistics",
      "year": 2016
    },
    {
      "authors": [
        "P. Liang"
      ],
      "title": "Learning executable semantic parsers for natural language understanding",
      "venue": "Communications of the ACM, 59",
      "year": 2016
    },
    {
      "authors": [
        "Pattie Maes"
      ],
      "title": "Agents that reduce work and information overload",
      "venue": "Communications of the ACM,",
      "year": 1994
    },
    {
      "authors": [
        "C.D. Manning",
        "M. Surdeanu",
        "J. Bauer",
        "J. Finkel",
        "S.J. Bethard",
        "D. McClosky"
      ],
      "title": "The Stanford CoreNLP natural language processing toolkit",
      "venue": "Association for Computational Linguistics (ACL) System Demonstrations, pages 55\u201360",
      "year": 2014
    },
    {
      "authors": [
        "Erik T Mueller"
      ],
      "title": "Commonsense Reasoning",
      "year": 2014
    },
    {
      "authors": [
        "Karen Myers",
        "Pauline Berry",
        "Jim Blythe",
        "Ken Conley",
        "Melinda Gervasio",
        "Deborah L McGuinness",
        "David Morley",
        "Avi Pfeffer",
        "Martha Pollack",
        "Milind Tambe"
      ],
      "title": "An intelligent personal assistant for task and time management",
      "venue": "AI Magazine,",
      "year": 2007
    },
    {
      "authors": [
        "A. Nguyen",
        "J. Yosinski",
        "J. Clune"
      ],
      "title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images",
      "venue": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 427\u2013436. IEEE",
      "year": 2015
    },
    {
      "authors": [
        "Sinno Jialin Pan",
        "Qiang Yang"
      ],
      "title": "A survey on transfer learning",
      "venue": "IEEE Transactions on knowledge and data engineering,",
      "year": 2010
    },
    {
      "authors": [
        "K. Pichotta",
        "R.J. Mooney"
      ],
      "title": "Statistical script learning with recurrent neural networks",
      "venue": "Proceedings of the Workshop on Uphill Battles in Language Processing (UBLP) at EMNLP 2016",
      "year": 2016
    },
    {
      "authors": [
        "C. Quirk",
        "P. Choudhury",
        "J. Gao",
        "H. Suzuki",
        "K. Toutanova",
        "M. Gamon",
        "W. Yih",
        "L. Vanderwende",
        "C. Cherry"
      ],
      "title": "Msr splat",
      "venue": "a language analysis toolkit. In Proceedings of the NAACL-HLT 2012: Demonstration Session, pages 21\u201324",
      "year": 2012
    },
    {
      "authors": [
        "P. Rajpurkar",
        "J. Zhang",
        "K. Lopyrev"
      ],
      "title": "and P",
      "venue": "Liang. Squad: 100,000+ questions for machine comprehension of text. In Empirical Methods in Natural Language Processing (EMNLP)",
      "year": 2016
    },
    {
      "authors": [
        "M. Richardson",
        "C.J.C. Burges",
        "E. Renshaw"
      ],
      "title": "Mctest: A challenge dataset for the opendomain machine comprehension of text",
      "venue": "Empirical Methods in Natural Language Processing (EMNLP)",
      "year": 2013
    },
    {
      "authors": [
        "M. Richardson",
        "P. Domingos"
      ],
      "title": "Markov logic networks",
      "venue": "Machine learning, 62(1-2):107\u2013136",
      "year": 2006
    },
    {
      "authors": [
        "Roger C Schank",
        "Robert P Abelson"
      ],
      "title": "Scripts, plans, goals, and understanding: An inquiry into human knowledge structures",
      "year": 1977
    },
    {
      "authors": [
        "P. Simard",
        "D. Chickering",
        "A. Lakshmiratan",
        "D. Charles",
        "L. Bottou",
        "C. Suarez",
        "D. Grangier",
        "S. Amershi",
        "J. Verwey",
        "J. Suh"
      ],
      "title": "Ice: enabling non-experts to build models interactively for large-scale lopsided problems",
      "venue": "arXiv preprint arXiv:1409.4814",
      "year": 2014
    },
    {
      "authors": [
        "C. Szegedy",
        "W. Zaremba",
        "I. Sutskever",
        "J. Bruna",
        "D. Erhan",
        "I. Goodfellow",
        "R. Fergus"
      ],
      "title": "Intriguing properties of neural networks",
      "venue": "arXiv preprint arXiv:1312.6199",
      "year": 2013
    },
    {
      "authors": [
        "D.G.R. Tervo",
        "J.B. Tenenbaum",
        "S.J. Gershman"
      ],
      "title": "Toward the neural implememtaion of structure learning",
      "venue": "Current Opinion in Neurobiology, 37",
      "year": 2016
    },
    {
      "authors": [
        "A. Trischler",
        "Z. Ye",
        "X. Yuan",
        "J. He",
        "P. Bachman",
        "K. Suleman"
      ],
      "title": "A parallel-hierarchical model for machine comprehension on sparse data",
      "venue": "arXiv preprint arXiv:1603.08884",
      "year": 2016
    },
    {
      "authors": [
        "G. Tur",
        "R. De Mori"
      ],
      "title": "Spoken language understanding: Systems for extracting semantic information from speech",
      "venue": "John Wiley & Sons",
      "year": 2011
    },
    {
      "authors": [
        "H. Wang",
        "M. Bansal",
        "K. Gimpel",
        "D. McAllester"
      ],
      "title": "Machine comprehension with syntax",
      "venue": "frames, and semantics. Volume 2: Short Papers, page 700",
      "year": 2015
    },
    {
      "authors": [
        "L. Wang",
        "P.N. Bennett",
        "K. Collins-Thompson"
      ],
      "title": "Robust ranking models via risk-sensitive optimization",
      "venue": "Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval, pages 761\u2013770. ACM",
      "year": 2012
    },
    {
      "authors": [
        "J.D. Williams",
        "E. Kamal",
        "H.A. Mokhtar Ashour",
        "J. Miller",
        "G. Zweig"
      ],
      "title": "Fast and easy language understanding for dialog systems with microsoft language understanding intelligent service (luis)",
      "venue": "16th Annual Meeting of the Special Interest Group on Discourse and Dialogue, page 159",
      "year": 2015
    },
    {
      "authors": [
        "J.D. Williams",
        "N.B. Niraula",
        "P. Dasigi",
        "A. Lakshmiratan",
        "C. Suarez",
        "M. Reddy",
        "G. Zweig"
      ],
      "title": "Rapidly scaling dialog systems with interactive learning",
      "venue": "Natural Language Dialog Systems and Intelligent Assistants, pages 1\u201313. Springer",
      "year": 2015
    }
  ],
  "sections": [
    {
      "text": "Modern statistical machine learning (SML) methods share a major limitation with the early approaches to AI: there is no scalable way to adapt them to new domains. Human learning solves this in part by leveraging a rich, shared, updateable world model. Such scalability requires modularity: updating part of the world model should not impact unrelated parts. We have argued that such modularity will require both \u201ccorrectability\u201d (so that errors can be corrected without introducing new errors) and \u201cinterpretability\u201d (so that we can understand what components need correcting).\nTo achieve this, one could attempt to adapt state of the art SML systems to be interpretable and correctable; or one could see how far the simplest possible interpretable, correctable learning methods can take us, and try to control the limitations of SML methods by applying them only where needed. Here we focus on the latter approach and we investigate two main ideas: \u201cTeacher Assisted Learning\u201d, which leverages crowd sourcing to learn language; and \u201cFactored Dialog Learning\u201d, which factors the process of application development into roles where the language competencies needed are isolated, enabling non-experts to quickly create new applications.\nWe test these ideas in an \u201cAutomated Personal Assistant\u201d (APA) setting, with two scenarios: that of detecting user intent from a user-APA dialog; and that of creating a class of event reminder applications, where a non-expert \u201cteacher\u201d can then create specific apps. For the intent detection task, we use a dataset of a thousand labeled utterances from user dialogs with Cortana, and we show that our approach matches state of the art SML methods, but in addition provides full transparency: the whole (editable) model can be summarized on one human-readable page. For the reminder app task, we ran small user studies to verify the efficacy of the approach.\n\u2217Currently at Carnegie Mellon University. This work was done while Yang was an intern at MSR.\nar X\niv :1\n61 2.\n07 89\n6v 1\n[ cs\n.A I]\n2 3\nD ec\n2 01\nContents"
    },
    {
      "heading": "1 Introduction and Related Work 4",
      "text": "1.1 Teacher Assisted Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n1.2 Factored Dialog Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n1.3 Predicates, Templates, and TAL and FDL compared . . . . . . . . . . . . . . . . . . . 8"
    },
    {
      "heading": "2 Teacher Assisted Learning 10",
      "text": "2.1 Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n2.1.1 Templates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n2.1.2 Predicates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n2.1.3 Text Preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n2.1.4 The Parsed Components List . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n2.1.5 The Template Matching Process . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n2.1.6 The TAL Training Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n2.1.7 Editing the TAL Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n2.1.8 Testing TAL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n2.2 Experiments: Learning Curves . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n2.2.1 Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n2.2.2 TAL and LUIS: Learning Curve Experiments . . . . . . . . . . . . . . . . . . . 19\n2.3 Experiments: Full Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n2.3.1 LUIS Baseline Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n2.3.2 TAL Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n2.3.3 Comparison . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n2.3.4 The Complete TAL Intent Detection Classifier . . . . . . . . . . . . . . . . . . 23"
    },
    {
      "heading": "3 Factored Dialog Learning 25",
      "text": "3.1 The Ask-How-To-Say Dialog Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\n3.2 The Train-Predicate Dialog Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n3.3 Overview of the Module Specification Language . . . . . . . . . . . . . . . . . . . . . 28\n3.3.1 Script Blocks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n3.3.2 The \u201cTurn\u201d in detail . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n3.3.3 Other Turn Types in the MSL . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n3.4 Case Study: Adapting the Event Reminder Module . . . . . . . . . . . . . . . . . . . 30\n3.4.1 Teacher Task Description . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n3.4.2 Teacher Task Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\n3.4.3 Teacher Task Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\n3.4.4 User Task Description . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\n3.4.5 User Task Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\n3.4.6 User Task Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33"
    },
    {
      "heading": "4 Discussion 33",
      "text": ""
    },
    {
      "heading": "5 Some Ideas for Future Work 34",
      "text": "5.1 Wake Phase Error Correction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\n5.2 Dream Phase Error Correction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\n5.2.1 Template Error Correction Using Unlabeled Data . . . . . . . . . . . . . . . . 34\n5.2.2 Template Unification . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n5.3 Extensibility . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\n5.3.1 Multiple Languages . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\n5.3.2 TAL\u2019s World Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\n5.3.3 Extending to Large Numbers of Templates and Predicates . . . . . . . . . . . 37\n5.3.4 Extending the MSL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n5.4 Other Benefits of Model Transparency . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n5.5 The Machine Comprehension of Language . . . . . . . . . . . . . . . . . . . . . . . . 37"
    },
    {
      "heading": "6 Acknowledgements 38",
      "text": ""
    },
    {
      "heading": "7 Appendix 1: Specification of the Module Specification Language 39",
      "text": "7.1 YAML Basics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n7.2 Script blocks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\n7.3 Script Parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41\n7.3.1 Parameter Names and References . . . . . . . . . . . . . . . . . . . . . . . . . 41\n7.3.2 Special Parameter Names . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\n7.3.3 Parameter Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\n7.3.4 Conditional Operations on Parameters . . . . . . . . . . . . . . . . . . . . . . 45\n7.3.5 Arithmetic Operations on Parameters . . . . . . . . . . . . . . . . . . . . . . . 45\n7.4 Script Handlers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n7.5 Turn Types . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n7.5.1 Prompt . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n7.5.2 ParamValueTurn . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\n7.5.3 TrainPredicateTurn . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\n7.5.4 TestPredicateTurn . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\n7.5.5 NIterations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53\n7.5.6 SetParamValues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54\n7.5.7 RemoveParamValues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54\n7.5.8 ScriptConditionalActions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\n7.5.9 UserConditionalActions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\n7.5.10 ParamValueOrConstantTurn . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\n7.5.11 SetConstantValues . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\n7.5.12 EndIterationAction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\n7.5.13 EndScriptAction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\n7.5.14 NoAction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\n7.6 Finding the App the User Wants . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59"
    },
    {
      "heading": "8 Appendix 2: TAL\u2019s American Slang and Abbreviations Map 61",
      "text": "1. Introduction and Related Work\nAI has had a long and difficult childhood [4], with nine \u201cAI Winters\u201d of varying degrees commonly identified [31]. A key technical roadblock seems to be the problem of scale, both in terms of tasks (it is much easier to build a system that solves one domain-specific task well, than to build one whose domain can be easily extended to new tasks) and complexity (it is easy to build small systems, but as they grow, systems can rapidly become unmanageable). This has led to our reliance on statistical methods, in particular on statistical machine learning (SML), where tasks are essentially defined by labels on large datasets and by models that are carefully constructed to solve those tasks. SML has had recent impressive successes [6], and in fact major inroads have been made into the first three \u201cAI Winters\u201d listed (\u201cthe failure of machine translation\u201d, \u201cthe abandonment of connectionism\u201d and \u201cspeech understanding research\u201d) [31]. But all statistical machine learning methods to date share some critical limitations when it comes to scalability: there is no procedure to make one machine-learned model maximally leverage what another has learned, on a different task. How can different models that solve different problems be built in such a way that one can take advantage of the other\u2019s world model, for shared basic concepts like space, time and number? And how can one system take advantage of the natural language learning that another has performed - how can we achieve the desideratum: Every teacher benefits from what any teacher teaches? In the world of SML, \u201cTransfer learning\u201d and its variants have been proposed to attack this problem. Transfer learning takes the form of models sharing features, parameters, or data (for a review, see [17]), but the set up is very close to the SML backbone; the problem is viewed as a mismatch of the data distributions that one\u2019s model sees, rather than the lack of a shared, fundamental, growable world model.\nThe key to a scalable world model is modularity. We have argued that modularity in turn requires correctability [3]. A system is correctable if its errors can be corrected in such a way that its generalization performance is improved while no (or, controllably few) new errors are introduced. We have also argued that interpretability, by which we mean the ability of the designer to easily understand why the system makes the assertions it does, is a desirable stepping stone towards correctability, although perhaps not a necessary one.\nThe vast majority of current statistical models have complex, unseen, and essentially uncontrollable decision surfaces that make full correctability and interpretability beyond our reach. (Statistical models unencumbered by such problems tend to be very small.) In these circumstances the\nsimplest option available for error correction is to add labeled training data and then either continue training starting from the current model, or re-train. But this is a clumsy tool for a delicate task: previously correct predictions can become errors, training data is expensive to gather, and we have little control over whether the new model actually solves the errors in question. One can certainly take a more nuanced approach to balance robustness with improved generalization: see for example (Wang et al., 2012) [30] for an interesting application of such ideas to ranking. But currently, the tide is flowing in the opposite direction. Large neural network models are powerful but are becoming increasingly complex. Furthermore, it is worrying that deep nets for vision tasks generate their highest confidence outputs when presented with inputs that to a human looks like noise [16]; and that almost imperceptible changes can be added to an input image that was previously classified correctly by a deep neural net, causing that net to make an error (and that the same changes, found using a sensitivity analysis for one net, can be applied to another, trained on a subset of the same data, with the same result) [25]. These are striking departures from how we understand the human visual system to work and would seem to be another barrier to both interpretability and correctability.\nThe machine comprehension of text (MCT) presents particularly stark challenges for statistical methods. An effective automated open domain dialog system will likely require a rich world model and the ability to perform commonsense reasoning over it [14]. It is possible that purely statistical methods will be all that is needed (see for example [10] for work on extending LSTM dialog models to model individual speakers, and [9] for using CNNs and RNNs for dialog topic tracking). But the performance of machines currently lags far behind that of humans in comprehending even language familiar to a typical first or second grader. Such a dataset, in multiple choice format, was presented by [21] and the state of the art results on that data is approximately 70% accuracy [29, 27], compared to the 100% that humans can easily achieve. A much larger question answering dataset provided by (Rajpurkar et al., 2016) [20] for MCT paints a similar picture, with F1 scores achieved by a strong machine learning baseline of 51%, compared to 87% for humans.\nThe vast majority of the approaches proposed not just for MCT, but for the general algorithmic modeling of various aspects of human intelligence, are statistical in nature (for recent work, see, for example, [11], [26], [18] and [8]). This is a very appealing research strategy for several well-known reasons. But when it comes to building interpretable and correctable methods, a natural question arises: how far can we get with methods for which interpretability and correctability are built in from the ground up, before resorting to statistical machine learning methods? Specifically, can the problems associated with SML be controlled by carefully compartmentalizing its role in such a system?\nThe work in this paper investigates this question. We adopt a machine teaching approach, and ask specifically: how much progress can be made with an interpretable, correctable, rule-based system that can be taught using crowd-sourcing? While the end goal is to answer whether such a system could help address the core scalability issues facing AI today, in this work we are simply searching for the hard limitations that the simplest possible methods will run into; in particular, the points at which SML methods become clearly required, if such points exist. We view this exercise as constructing a base camp for the expedition, and as we will see, the initial results are surprising: we find that a crowd-sourced rule-based system, with generalization built in using a taxonomy, can perform as well as a state of the art machine learning system, but correctably and interpretably, on an MCT task.\nSuch a bottom-up approach is reminiscent of the early work in AI, perhaps the closest being the work of (Schank, 1977), where particular scenarios are encapsulated by scripts [23]. The more\nrecent work of (Hixon et al., 2015), in which a knowlege graph is learned from conversational dialogs, is similar in spirit to ours: there, human knowledge is leveraged more deeply than by just extracting labels, and an editable ontology is used to aid in representing world knowledge. However, their focus is very different [7]: they investigate the construction of a knowledge base to support question answering, given a set of background true/false statements about the world (in their case, 4th grade science), and they gather user input through dialogs structured around multiple choice questions. In contrast, our focus is on methods to achieve scalability in general, on modeling language directly to support this, and on the general development of apps for APAs, rather than question answering.\nOur base camp is built on two foundations: Teacher Assisted Learning (TAL) and Factored Dialog Learning (FDL), which we now describe. Since FDL uses TAL but not vice versa, we describe TAL first1.\n1.1. Teacher Assisted Learning\nTraditionally, developing machine learning models has required expertise in machine learning and programming. However, some recent research has focused on how to help someone who is a domain expert, but who knows little about machine learning, to quickly build machine learned models [24, 35]. LUIS [34] provides user-friendly interfaces, available through a web portal, that allow the user to perform necessary machine learning steps, such as defining labels, adding examples, and training and evaluating models, in an interactive fashion. LUIS also provides visualizations that show the prediction performance, and it integrates an active learning component (\u201cSuggest\u201d) to allow the users to label the \u201cmost uncertain\u201d utterances.\nHowever, a non-expert\u2019s knowledge and intuition about the problem domain is a highly valuable resource that is not fully leveraged by traditional machine learning techniques, beyond asking for labels. For example, when predicting speaker intent for dialog utterances [28], a non-expert can not only easily interpret the meaning of each utterance, but can also explain why they believe their interpretation, why some utterances are ambiguous, when some utterances don\u2019t make sense, and so forth. Current SML approaches, including the above machine teaching methods, ignore this extra rich information.\nWe hypothesize that a learning task can benefit from teachers who are experts in their domain, but not in machine learning, in the two most important steps of learning: representation learning and model generalization. The core idea in TAL is to use language-based templates and predicates which the teacher can tune to solve the problem at hand, using examples from the training set and other resources for guidance. TAL is by construction interpretable and correctable. In this setting, we have to address the following fundamental questions:\n\u2022 TAL shares its objective with traditional machine learning, namely, maximizing generalization performance. How can we make maximal use of the Teacher\u2019s language skills and domain expertise to achieve this? \u2022 How can we build interpretability and correctability into the model from the ground up? \u2022 As we assume the teachers are usually not technical experts, how do we design simple\nnatural-language-based dialogs between TAL and the teacher during the teaching process, which will work for any language modeling task, thus taking a step towards addressing the scale problem described above?\n1We also use \u201cTAL\u201d to denote the overall dialog system itself, when the meaning is clear from context.\n\u2022 Could this approach address the core scaling problem facing AI - that is, how can we ensure that all teachers (or systems) benefit from what any teacher teaches?\nBecause the representation is interpretable, highly modular, and language-based, the Teacher can adjust it as desired, giving them much more control over how the system detects meaning compared to SML approaches. Teaching is an iterative process where, once the teacher has taught a pattern, TAL can then run it over labeled data, looking for false positives and negatives, which then guide the teacher to further tune their pattern. This approach thus leverages the deeper language skills mentioned above, because at any point the teacher can reason why their pattern fails and take steps to correct it.\nThe work presented here focuses on the first three items above. The fourth is a very interesting open problem; some discussion is offered in Section 5.\nWe present more details on TAL below. In a case study in MCT, where the task is to detect when the user has the intent to make a purchase, we demonstrate how an English speaker, using a TAL instance that integrates the WordNet semantic taxonomy [5], can train an intent prediction model for dialog utterances, correctably and interpretably, and attain results comparable with LUIS [34].\n1.2. Factored Dialog Learning\nAutomated Personal Assistants (APAs) have a long history in AI (see for example [12, 15]) and modern APAs, such as Siri or Cortana, can perform various useful tasks through conversational dialogs with users: for example, they can detect when the user wants to make a phone call, or find an address, or add an item to their calendar, and can then take an appropriate action. However, scaling APAs to large numbers of different kinds of tasks, and eventually, to having them anticipate the user\u2019s needs through natural, engaging dialog, is still beyond us. The traditional software development process [2] is not well suited to this problem. Typically, to support a new task, designers and developers must first write and review the new design and its requirements, they must make necessary changes to the implementation (if not start again from scratch), and finally, the new system must undergo a suite of unit and system wide tests, which may require beta testing with users. For dialog apps, these changes include redesigning the turn-taking structure of the dialog and implementing any extra needed underlying logic. Moreover, the language skills (for both speaking and understanding) of an APA are crucial for a good user experience, from intent detection to task completion. This process thus usually requires the developers to \u201cteach\u201d the languages the agent needs for each application, in some form, and to continue training to understand what the user says even if the domain is fixed.\nHere we investigate addressing this scalability issue by leveraging two key ideas: first, by factoring the development in such a way that the entire language learning part of the problem can be performed by non-experts; and second, by developing a shared world model that can be re-used by widely different classes of applications. The goal is to achieve scalability by opening up the possibility of using crowd sourcing for application development (in addition to using crowd source for language learning, as above). We call this approach Factored Dialog Learning (FDL).\nFDL uses TAL as a language learning component, but it also introduces two additional roles to those found in the traditional development process: the Teacher and the Designer, each of which deals with a different aspect of the scalability issue. We define the Teacher as the person who teaches the APA the language skills required to engage in a user dialog, for a particular application. We define the Designer as the person who identifies a class of tasks that they want the APA to\nhandle, and who then designs the abstract dialog flow for every application domain within that class (including user-system dialog shared across all applications in the class).\nAs opposed to the complex skillset required of the developer in the traditional development process, here, the Teacher need only be familiar with the application that they want to build, in addition to being fluent in their native language. The FDL framework requires more of the Designer (but still far less expertise than that required of a developer): the Designer must learn a simple, high level scripting language which we call the Module Specification Language (MSL). The Designer uses the MSL to specify how the Teacher will create an app from the Designer\u2019s class of applications. Although MSL is a simple scripting language, it does support basic programming concepts such as loops and conditionals. We will describe the MSL in detail below.\nIn this paper, in order to contrast the different roles played by FDL and TAL, we choose a development example where FDL\u2019s dependence on TAL is very light: we implement FDL for a class of Event Reminder applications, and we demonstrate its use to create reminder scheduling apps for tracking the consumption of medications, visits to the gym, birthdays, and some other reminder tasks.\nFDL factors the language development out of the overall development process and thus can assign all language development tasks to the Designer and Teacher. However, of course the Developer still plays a crucial role in the overall process. For example, for the Event Reminder class, there are seven quantities that the APA must extract from the user, given that the user wants to create a reminder: two strings (a name for the reminder itself, and any notes to add to the reminder) and five numbers (the event duration, frequency, group frequency, start time, and end time). These will be explained more below, but the point here is that the developer must write the code to have the APA do the appropriate thing once the user is running the app and this data is received (in this case, the APA would simply add the notated events to the calendar). However, notice that the APA\u2019s actions triggered by these seven quantities are independent of the particular reminder app being run. Thus it is the Developer\u2019s (and Designer\u2019s) role to identify classes of applications such that the APA actions taken for any app in a given class need only be coded once.\nFigure 1 summarizes the development process flow diagram of the FDL framework.\n1.3. Predicates, Templates, and TAL and FDL compared\nThe key concept underlying both TAL and FDL is that of a predicate. A predicate can be thought of as a Boolean function over text where the function has \u2019state\u2019. The state is modeled with a data structure that contains slots and handler functions. When the predicate fires (i.e. takes the value \u2019true\u2019), its slots are populated, and its handler functions tell the APA what to do.\nThus for example, a predicate for the concept is collocated with might contain a slot for the set of entities that are deemed to be collocated, whenever the predicate returns true. No constraints are imposed on the text: in particular, we do not require the presence of a verb2.\nTAL distinguishes two kinds of predicates: Parsed Predicates and Learned Predicates. Parsed predicates currently form TAL\u2019s world model; they encapsulate notions such as time, space, and number, and they are currently hardwired. Parsed predicates are thus easily shared across classes of applications. For example, in building the event reminder class of applications described in this paper, the developer had to create parsed predicates for time, frequency, period, and duration\n2Our earlier work did, and predicates are associated with verbs in theories of syntax and grammar [32]. However, queries submitted to intelligent agents often lack verbs but nonetheless convey intent(for example, self-reminders like Sophie\u2019s piano lesson).\n- which could then all be used by any other designer in their creation of a different class of applications. It seems reasonable to allow that the world model, at least at the level of physics and mathematics, warrants its own special development effort, since physics and mathematics themselves form such a succinct yet powerful world model3.\nLearned predicates, on the other hand, are used to model all other forms of meaning in language. We thus expect that learned predicates will greatly outnumber parsed predicates as TAL grows. Learned predicates are not hard-coded but instead are learned entirely using human teachers. For learned predicates, we divide the problem, and introduce templates, which are used to detect patterns in language. A learned predicate is declared to be true if and only if its containing template \u201cfires\u201d (matches a text fragment). Templates are also entirely taught (rather than coded): they are language-based and they leverage a semantic taxonomy, as well as teacher input, to achieve generalization. In this study we use WordNet for our semantic taxonomy [5], but teachers can also both add and remove items from the taxonomy as needed.\nThus, any given template has an associated set of learned predicates, and whenever that template matches a piece of text, its predicates are declared true for that text. For example, the text \u201cJesse ate a sandwich\u201d might fire a \u201cCollocated\u201d predicate, and also a \u201cConsumes\u201d predicate. In addition, a given predicate can be fired by several different templates: for example, \u201cJesse ate a sandwich\u201d and \u201cThe sponge absorbed the liquid\u201d might both fire a \u201cOne entity consumes another\u201d predicate,\n3The only alternative that comes to mind is to try to learn the laws that govern the physical world from labeled examples. But at its core, SML is for modeling functions that we don\u2019t know how to describe in closed mathematical form, which is not the case here.\neven though the templates that fire that predicate are different.\nSince learned predicates and their containing templates are all learned from teachers, TAL attacks the scalability problem by leveraging the expertise of crowd-sourced teachers who need only be experts in their own native language. By factoring the problem in this way, the pool of available teachers becomes very large.\nIn contrast to TAL, FDL\u2019s main task is to factor the language needed for the application development process in such a way that one designer can create a script that could be used by hundreds of teachers, who then in turn create apps to support millions of users. Thus FDL attacks the scalability problem by further factoring the language modeling problem in such a way that the available expertise matches each subtask well.\nIn our Event Reminder class of apps described below, the TAL system relies almost entirely on parsed predicates, and it had to learn only one extra learned predicate (to allow it to detect when the user wants to call up the app). (When TAL searches for predicates matching a piece of text, it always tests both parsed and learned predicates.) However in our study of learning user intent, TAL relies entirely on learned predicates. We chose to use these two examples with as little overlap of the predicate types they need as possible, to more clearly test, and contrast, the ideas in each approach.\n2. Teacher Assisted Learning\nIn the following, for succinctness, we replace the phrase \u201ccorrectable and interpretable\u201d, as defined above, with \u201ctransparent\u201d.\nThe main design goal for Teacher Assisted Learning (TAL) is that the resulting models be fully transparent: TAL\u2019s behaviour should always be fully and explicitly attributable to the underlying data and rules (the WordNet taxonomy, our gazetteers, the hardwired lists of tokens (e.g. personal pronouns)), or to the data and rules input by a Teacher. In earlier versions of the system we used dependency and consituency tree parsing, semantic role labeling, and other NLP constructs, using the SPLAT NLP tool from MSR [19] and the Stanford NLP package [13]. These tools are the culmination of decades of development effort and it seemed wise to use them rather than try to reinvent the wheel. But using them revealed four significant drawbacks: they are brittle, they are slow, they are not transparent, and they solve a harder problem than we need to solve. The brittleness and lack of transparency are unavoidable consequences of the statistical models underlying the tools; one could investigate building \u201cshims\u201d around the tools to correct errors, and then retrain the underlying models when enough data was gathered, but the overhead would be high, and retraining does not guarantee that the errors are resolved. The speed issues are harder to address and would likely have proven a show stopper to shipping TAL (imagine having to compute the parse trees and SRL for the inputs from millions of users, simultaneously). At the time, we only had a hunch that these systems also solve a harder problem than we really needed to solve, but this indeed turned out to be the case. In this section, we describe in detail the current processing that TAL uses.\n2.1. Design\nHere we extend the high level overview given in the Introduction to give a detailed description of templates and predicates, and of the process used to train TAL."
    },
    {
      "heading": "2.1.1 Templates",
      "text": "The template data structure consists of an ordered list of items used for pattern matching, together with a pointer to a set of predicates. Each item can be roughly thought of as either a string, a synset, or a set of synsets (detais are given below). For efficiency, templates are indexed by the first synset occuring in their list (every template must have at least one synset); thus, rather than scanning all templates over a piece of text to detect patterns, only those templates indexed by the possible synsets correspoding to the current token in the text are tested for possible matches with that and subsequent tokens. A template is said to \u201cmatch\u201d or \u201cfire\u201d if and only if a sequence of tokens in the text is found that matches that template\u2019s list, in order. During the matching process, only tokens that could be nouns, verbs, adjectives, adverbs, pronouns, or negation terms, are considered. If a template fires, then TAL asserts that all of the predicates associated with that template are true, for that piece of text. As mentioned above, the template/predicate map is many-to-many: a given predicate can also be pointed at by more than one template.\nEach template\u2019s list is of fixed length but different templates can have different length lists. There is no limit to the possible length of a template\u2019s list, but typically the list has between two and four elements: see Table 1 for the histogram of the lengths of list for the TAL classifier trained for this paper (see Section 2.3.4 for details).\nConcretely, each item in a template\u2019s list can be one of three types: a string; a set of noun synsets; or a pair, consisting of a single verb synset, together with a simplified form of that verb\u2019s tense. Currently TAL supports the simple tense types Past, Present or Future, or the catchall PastOrPresentOrFuture. An element in the template matches a token in the text if one of the following three conditions holds:\n1. The element is a string, and the token is the same string.\n2. The element is a single verb synset S with simple tense T, and the token appears as a possible verb in the taxonomy, such that one of the possible synsets of that verb is a hyponym of S, and such that that token could also have simple tense T4.\n3. The element is a set of noun synsets S , and the token appears as a possible noun in the taxonomy, such that one of the possible synsets of that noun is a hyponym of one or more of the synsets appearing in S .\nWe reserve the use of string matching for parts of speech that have a small number of possible instantiations: currently, TAL only allows question adverbs, as listed in Table 2. Note that if needed, other specialized parts of speech (e.g. prepositions) could also be used (for example,\n4Note that a single token can have several possible simple tenses: for example \u201ccut\u201d is the infinitive/present, past tense, and past participle of the verb \u201cto cut\u201d.\n\u201cLet\u2019s get Taco Bell\u201d and \u201cLet\u2019s get to Taco Bell\u201d connote different user intents). We decided to make question adverbs the first such direct match type, to give TAL rudimentary question detection abilities.\nDuring processing, template matching is not done over the raw text, but over a list of parsed components. The construction of these is described below. Here we just note that while our templates currently model nouns, verbs and question adverbs, our parsed components in addition model adjectives, general adverbs, prepositions, pronouns, and negation. Some of these take part in the current matching process (see below), while the others were included to facilitate the extension of the template data structure if desired."
    },
    {
      "heading": "2.1.2 Predicates",
      "text": "As mentioned in the Introduction, TAL distinguished two kinds of predicates: learned predicates, which a Teacher constructs by interacting with TAL via natural language only, and parsed predicates, which form the backbone of TAL\u2019s world model and which are currently hardwired.\nLearned Predicates: The learned predicate data structure consists of the predicate\u2019s name (a string), the names of its components (a list of strings), a slot for optional notes (also a string), and a set of handlers for specifying the scripts and actions to be launched when this predicate fires (which occurs when and only when any template that points to it fires). For example, a predicate modeling \u201cEntity ingests something\u201d might have component names \u201cIngester\u201d and \u201cIngested\u201d. It is important to note that mapping the text to these component names is straightforward, since TAL can just ask the Teacher for the mapping once the template has been trained. Thus \u201cIt was eaten by him\u201d would fire a different template than that fired by \u201cHe ate it\u201d (recall that templates take word order into account) and the slots, at run time, would be filled automatically, based on the mapping specified by the Teacher at train time. This is an important advantage of using Teachers: we do not have to solve the Semantic Role Labeling problem to compute this mapping. Table 3 shows the properties of the LearnedPredicate. The last item - LearnedSynsetMap - was added purely to lower the cognitive load on the Teacher: the chances are good that a synset found for a word used to train a previous templates for a given predicate, is also the correct synset for that word, for a different template being trained for that same predicate. (The Teacher always has the option of choosing a different synset.)\nParsed Predicates: So far we have written built-in parsed predicates to support notions of time (for the Event Reminder module described below). A parsed predicate has a name and may have properties. Table 4 gives a brief description of the built-in ParsedPredicates currently implemented by FDL; see table 24 in section 7.5.4 (TestPredicateTurn) for a detailed description for script usage.\nParsed predicates do not use templates; they are fired by (currently built-in) pattern detectors. We chose to hard-wire several pattern detectors since these patterns are likely to be shared across many different applications, and since the quantities they represent can be modeled extremely succinctly by mathematical notions such as time, and the number line. We wrote patterns to model date, time, duration and frequency. As an example, Table 5 shows the pattern detectors used to detect frequencies from English text. Each pattern detector is written as an F# active pattern; we found the F# language to be particularly well-suited for this kind of modeling. Each active pattern\u2019s name encapsulates one of the possible patterns it captures, to facilitate code readability: thus for example, the OncePerSecond active pattern models text snippets such as \u201cOnce a second\u201d,\n\u201cTwice per day\u201d or \u201cThree times every week\u201d. The order of the scans (from top to bottom, in Table 5)\nmatters: more complex patterns can contain simpler ones (e.g. the \u201cHourly\u201d detector should be scanned over the text only if the \u201cOnceHourly\u201d detector failed to fire)."
    },
    {
      "heading": "2.1.3 Text Preprocessing",
      "text": "Since we use no more sophisticated standard NLP processing than lemmatization and basic part of speech tagging, we can describe the process in full here.\nFirst, the user\u2019s input is broken into sentences. Then, terminators (characters in the set {\u201c.\u201d, \u201c!\u201d, \u201c?\u201d}) are removed from the end of each sentence, and any leading or trailing space is then removed5. The text is lower-cased, and then any tokens that appear in the Slang Map (see Appendix 8) are replaced (for example, \u201cwanna\u201d is replaced by \u201cwant to\u201d). Apostrophes (\u201c\u2019s\u201d and \u201c\u201d\u2019) are then removed. Then two mappings corresponding to TAL\u2019s internal model of time intervals are performed: first, strings matching the pattern #-#, where # represent an integer (i.e. dashed intervals), are mapped to # to #. Then, strings matching the pattern #x/T, where \u201c#\u201d is an integer and T is one of the hardwired time intervals (e.g. day) (i.e. slashed intervals) are expanded to the form # times per T. We expect these mappings to be shared across all applications that use TAL\u2019s world model and so they are hardwired. Finally, tokens are mapped to lemmas as follows: we use two files, one that lists verbs and their declinations, and one that lists nouns and their plurals. If a token appears as a declination of a verb, and that declination does not appear in the noun list, then that token is declared to have type verb. Otherwise, we gather all possible parts of speech for the token. In all cases, tokens are replaced by their lemmas, and for verbs, their declination is mapped to one of Past, PresentOrFuture, PastPresentOrFuture. Throughout processing, both the original tokens and their lemmas are kept."
    },
    {
      "heading": "2.1.4 The Parsed Components List",
      "text": "We then construct a parsed components list for the sentence. This amounts to mapping each token, or compound phrase, to a set of its possible parts of speech (POS); keeping track of negations; and handling modal verbs. The parts of speech we track are shown in Table 6. We reserve the \u2019Unkown\u2019 flag to model tokens whose POS does not map to one of those listed. Note that the code that implements the processing described in this section is language-specific and currently would have to be rewritten to map TAL to handle new languages, although the amount of code is small.\n5Currently TAL only handles single sentence input; if it detects more than one sentence at this point it issues a warning\nCompound Phrases: In Teach phase, known compound phrases (phrases consisting of two or more tokens that appear in our extended WordNet taxonomy, for example look for or White House) are shown to the Teacher for confirmation, and the Teacher can then either confirm, or declare a found compound phrase as non-compound (for example, get to has three synsets in WordNet, but none of them have the meaning as in I need to get to the store). Then the Teacher is given the option of adding any compound phrases that TAL missed (for example, the name of a new video game). New compound phrases thus formed are then added to the taxonomy. In Test phase, currently, TAL assumes that any compound phrases it finds are correct (but see Section 5).\nNegation: We simply track negation, as the negation of the tracked part of speech immediately following the negator. It turns out that every part of speech we track can be negated; Table 7 gives an example for each.\nModal Verbs: We distinguish semi-modal verbs from pure modal ones. A modal is \u201csemi\u201d if it can be a standalone verb, or a modal verb, such as \u201chave\u201d or \u201cdid\u201d. A modal is \u201cpure\u201d if it cannot occur as a standalone verb. Our list of pure modals is can; could; may; might; must; shall; should; will; would. We include modals like can and must since although they can occur in their own sentence (e.g. I can.), they always refer to an action. We also cast modals as past tense, or \u2019present or future\u2019 tense, and update the verb\u2019s tense based on its modal, if it has one. Finally we also distinguish modals with possible noun meanings (can; may; will; might; must).\nProcessing of modals occurs as follows. As mentioned, negation is detected and attached to the following POS, and verb tense is detected attached to a following verb. A pure modal that can also be a noun is declared a noun if it is preceded by a determiner or a possessive pronoun (e.g. The can is heavy.). Otherwise, pure modals themselves are not kept for further processing. For semi modals, if they modify a verb, that verb\u2019s tense is kept, and the modal is skipped; if not, they are\nand takes only the first.\nkept as \u2019stand alone\u2019 verbs (e.g. I have one.).\nFinally, all parsed components that could not possibly be a noun, verb, pronoun, or directly matchable token (which in the current system are restricted to question adverbs) are dropped; the remaining noun and verb meanings are mapped to sets of possible synsets (in Teach phase, the Teacher is first asked to identify whether the token is a noun, verb or neither, if it\u2019s ambiguous); and certain pronouns are mapped to special purpose synsets (for example, the pronoun I is mapped to a synset which derives from the first noun meaning of narrator); and personal pronouns other than I, you or we are mapped to the synset person.n.01). This was done mainly to make patterns containing these pronouns more clearly readable."
    },
    {
      "heading": "2.1.5 The Template Matching Process",
      "text": "Template matching to a phrase is straightforward. First, the phrase is mapped to its parsed component list as above. The template matching is done by an ordered comparison between the template\u2019s list, and the phrase\u2019s parsed component list. To greatly reduce computational overhead, templates are stored in a dictionary, where the key is also the first synset occurring in the template\u2019s list. In order to search for a match at a given position in the text, only those templates whose key is a hypernym of, or equal to, the synset at the current position in the parsed component list (allowing for skipping of fixed token patterns such as \u201cwhere\u201d) are examined. This is done by constructing all hypernyms for the current token (if it is a possible verb or noun) and using each as a key.\nAll positions in the template list must match, for a match to be declared. If the text\u2019s parsed component list is longer than the template\u2019s list, all subsequence matches are attempted (only one has to succeed for a match to be declared). If the template item is a fixed token (e.g. \u201cwhere\u201d), that token must match exactly. If the template item is a verb synset VS, together with its tense, the set of synsets for the corresponding position in the text must contain at least one synset which is a hyponym of VS, and the tenses must match. Finally, if the template item is a set of noun synsets SNS, then for a match to occur, at least one member of that set must be a hypernym of (or equal to) one of the synsets in the set of synsets for the corresponding position in the text. Allowance is also made for skippable items: parsed components whose POS set contains one or more of [adjective, adverb, unknown]. For example, if a token\u2019s parsed component POS set contains adjective and noun, then that token can be skipped in the search for a match (if it itself does not take part in the match), and the next noun set examined for a match. In \u201cHe likes blue cheese\u201d, the token blue, taken alone, could be a noun or an adjective, so the matching process will both attempt to make it part of the match, or will skip it if that match fails."
    },
    {
      "heading": "2.1.6 The TAL Training Process",
      "text": "In Train phase, the POS (noun or verb or neither) is confirmed with the Teacher, and the Teacher is then asked to identify the single best matching synset for each noun or verb thus identified. As described above, to lighten the cognitive load on the Teacher, TAL \u201cguesses\u201d the correct synset to use to confirm with the Teacher, based on previous Teacher inputs. Also as mentioned, the Teacher confirms any compound phrases found by TAL (if the Teacher declines the confirmation, the phrase is left in non-compound form), and is asked to identify any compound phrases TAL missed; any such phrases are then added to the taxonomy, with the Teacher\u2019s guidance.\nTraining can be done either in batch mode (which requires a set of positive and negative labeled examples), or in free form mode (where the Teacher simply enters example phrases instantiating\nthe predicate they wish to teach). We summarize below the iterative steps involved in training a TAL model, in batch training mode:\n1. A small number of labeled positive and negative phrases are first input for training.\n2. TAL\u2019s first task is to elicit from the Teacher the predicate they wish to train. First, all predicates that match one or more of the positive training examples are listed, and the Teacher is asked to select one or \u201cTrain another predicate\u201d. If the latter is chosen, TAL lists all the predicates it knows about, along with an option to train a new predicate. If no existing predicates are found, the Teacher must train a new predicate.\n3. TAL presents the next positive example to the Teacher, lets them know if the text already matches the predicate (i.e. matches a template containing the predicate), and asks them if they wish to train on this example. If there are no more positive examples, training is complete.\n4. If Yes, TAL asks them to enter a phrase inspired by the input text, that captures the predicate being trained. This allows the Teacher to zero in on just that part of the text that should fire the predicate, and to enter related text that should also fire the predicate, if desired.\n5. TAL then shows any compound nouns and verbs it found, and asks for confirmation. Any that are confirmed as incorrect are dissolved back into single token form.\n6. TAL asks if it missed any compound nouns, and adds them as indicated by the Teacher. For the rest of the discussion, we will refer to both individual tokens and compound phrases as \u201ctokens\u201d.\n7. TAL asks the Teacher for the meaning of any tokens it doesn\u2019t know, and adds the new definitions to its WordNet ontology. Thus the ontology is grown as needed, and these additions are shared between Teachers.\n8. TAL assigns a set of possible parts of speech (POSs) to each token. It only keeps tokens that are a question adverb, or that could possibly be a noun or verb.\n9. If the POS set is ambiguous (still contains more than one POS), TAL asks the Teacher if the token is a noun, verb, or neither.\n10. TAL lists the synsets for each noun or verb and asks the Teacher to pick one. It then lists the possible generalizations (hypernyms) of that synset and asks the Teacher to generalize by picking one, if appropriate.\n11. Once a unique set of synsets or strings has been identified, TAL constructs the corresponding template from them, and adds the predicate to that template.\n12. TAL shows the template to the Teacher and asks them if it looks correct. If they say no, TAL returns to step [4].\n13. TAL then runs the new template over the negatively labeled training samples. If this results in any false positives, TAL shows them to the Teacher and asks if they still want to keep the template. If so, the loop continues from [3] above; else, it continues from [4], but also gives the Teacher the opportunity to move to the next training sample (i.e. go to step [3]).\n14. After the iterative training process is complete, the Teacher can read and modify the learned templates file (a text file in YAML format).\nWe found the last step - editing the templates file - to be a simple and very useful exercise: note that this can only be done with such a transparent (correctable and interpretable) system. We also allowed the Teacher to edit the templates on the fly, as described in the next Section.\nThus the teaching process can result in new synsets (or nodes) being added to, or deleted from, the taxonomy. Examples of both of these are given in Section 2.3 below."
    },
    {
      "heading": "2.1.7 Editing the TAL Model",
      "text": "After training, TAL\u2019s transparency allows us to manually edit the model. This process can be very efficient, especially when the number of templates is small, compared to adding labeled data and retraining an SML model. We followed the following procedure:\n1. We extended each template\u2019s sets of noun synsets, based on noun synsets occuring in the other templates. Thus, for example, if one template allowed \u201cget.v.01\u201d followed by \u201ccurrency.n.01\u201d, and another allowed the same verb synset but was missing a following \u201ccurrency.n.01\u201d, we added it6.\n2. We removed any resulting duplicate templates.\n3. Over-generalization is checked. For example, for the intent model trained below, we noticed during the teaching process that TAL suggested \u201cget.v.01\u201d as a more general version of \u201cbuy.v.01\u201d. We found that replacing the latter by the former did indeed improve performance on the training set, for some templates, but for others, many false positives were generated.\n4. We manually checked, using the online version of WordNet [5], that the synsets chosen previously made sense, again removing any resulting duplicates."
    },
    {
      "heading": "2.1.8 Testing TAL",
      "text": "Test phase for TAL is simpler and involves no Teacher interaction. The text is normalized, compound phrases are identified using the (possibly extended) WordNet, and all the templates are matched against the text, using the template matching logic described in section 2.1.1. If a template fires, TAL asserts that all of its associated predicates are true. In the experiments described below, we had only one predicate; a test example was declared positive if the predicate was true for that example.\n2.2. Experiments: Learning Curves"
    },
    {
      "heading": "2.2.1 Data",
      "text": "We generated training and test data from Cortana logs as follows: we searched for user inputs containing one of more of the strings shown in Table 8.\nNote the lack of a space after tokens \u201c purchas\u201d and \u201c acquir\u201d, resulting in matches for user inputs containing these strings as stems. In addition, we adder filters to restrict the user inputs to either speech or SMS texts; to inputs issued on Windows devices, and to inputs issued to Cortana\n6This could all be done during the initial training process, but the instantiation used did allow parallel templates to develop (templates containing the same verb but different following or preceding noun synsets).\n(as opposed to other Windows services). Running these queries over 18 days of logs from May, 2016 resulted in 15,310 records. Uniquing this data resulted in 12,690 records. We also took the most recent 200 (distinct) queries in the logs as unbiased background data. We shuffled the 12,690 records and then took the first 800. We then labeled the resulting combined 1000 sentences as indicating the user\u2019s intent to buy, or not. The remaining 11,890 records were kept for use with the LUIS active learning experiments. Because some of the sample sets were very small (e.g. the process only resulted in 14 queries containing \u201c acquir\u201d), we split each individual dataset in two (e.g. the \u201c acquir\u201d positives, and \u201c acquir\u201d negatives, were each split in two) to create train and test sets. This resulted in the totals shown in Table 97."
    },
    {
      "heading": "2.2.2 TAL and LUIS: Learning Curve Experiments",
      "text": "As mentioned above, we used the LUIS system [34] as a state of the art machine learning baseline. LUIS uses a logistic regression model with bag of words features (inverse document frequencies) and the number of tokens as inputs. LUIS also gives the ability to use domain-specific dictionaries; we experimented with this but did not find improved performance on this task. Internally, LUIS performs 10 fold cross validation to choose the optimal model parameters, and then trains on all the training data, using the found parameter values.\nSince constructing the learning curves required training 490 models for LUIS, in order to make the comparisons both feasible and as fair as possible, for these experiments we used both systems in maximally automated mode. For LUIS this simply meant that no active learning was used, while for TAL, we used a simplified version of the full system: the user was not allowed to delete items from the ontology; they were not given the option of editing the learned templates; and the templates used were a simplified version where each synset set was constrained to be a singleton. For TAL, full training (on 495 training samples; see section 2.3 below) resulted in a total of 99 templates (compare this with 24 templates, for the full system below trained on the same data, when sets of noun synsets were allowed; and 21 templates if verbs are further combined into sets where possible).\nFigure 2 shows the training curves for accuracy, precision and recall on the 504 sample test set as a function of training set size. For the LUIS experiments, training set sizes were increased by 10\n7The sizes in Table 9 add up to 999 because one non-English query had slipped though the filters, which we later decided to drop since the language of the problem domain is English.\nsamples from one experiment to the next, starting at 10 samples. For TAL, the data was split into sets of size 20, 40, 60 and 78 positives.\n2.3. Experiments: Full Training\nWe used the same training data as above."
    },
    {
      "heading": "2.3.1 LUIS Baseline Results",
      "text": "LUIS supports Active Learning (AL) via its \u2019Suggest\u2019 option. The user can upload a set of unlabeled samples and then enter Suggest mode. LUIS will present up to 10 of the unlabeled samples that it decides will most benefit the model to verify, along with its current prediction for them. The user changes the prediction if necessary, or accepts the current label. After the set of samples is labeled, LUIS retrains its model, then presents the next set of up to 10 unlabeled samples.\nAfter training LUIS initially from the full 495-sample test set, we ran it on the full test set, then performed active learning using the 11,890 unlabled samples. We did three runs of active learning, creating 25 newly labeled samples, and ran it on the test set again. Table 10 shows these results along with TAL\u2019s scores on the test set.\nNote that the LUIS team made significant improvements to its intent prediction classifier after we had performand the learning-curve experiments above. This resulted in a pre-AL baseline with significantly higher recall and somewhat less precision than before. Adding active learning to LUIS increased its precision with no change in recall."
    },
    {
      "heading": "2.3.2 TAL Results",
      "text": "After training on the first 20 sentences, the manual editing process described above resulted in reducing the number of templates from ten to seven. This template collapse pattern repeated, until after training on all the data, we arrived at 24 templates. All the templates found for the fully trained TAL user intent model are shown below, in Table 13. In addition, since the model is fully transparent, we could also check the other user-entered data, as shown in 11.\nWe also noticed that the 24 templates could easily be further reduced to 21 if verb synsets are combined into sets where possible. See Section 2.3.4 and Table 13 for details."
    },
    {
      "heading": "2.3.3 Comparison",
      "text": "Compared to LUIS with active learning, TAL is significantly higher in precision and lower in recall, for a marginal increase in overall accuracy. A natural next step would be to tune LUIS for the same precision and then measure its recall, and compare again. However, these results alone demonstrate that comparable performance can be achieved with a fully transparant, succinct, and editable model. Because of these advantages, we believe that TAL could take advantage of more training data to increase accuracy arbitrarily (up to the level of label noise)."
    },
    {
      "heading": "2.3.4 The Complete TAL Intent Detection Classifier",
      "text": "We present here the entire classifier that TAL learned to detect intent to buy. First, the Teacher removed a single synset from the WordNet taxonomy. TAL uses a YAML file to track such removals, and in this case the file contains the single mapping get: get.v.22, meaning that the 22nd verb synset listed for the token get is to be removed from the graph. This was done to remove a loop in WordNet that causes TAL to generalize incorrectly8.\nTable 11 shows the Teacher-taught extensions to WordNet, with tokens on the left and their mappings on the right. The table distinguishes concepts and instances: all mappings not denoted by the string InstanceOf are concepts. Table 12 shows a noun set referred to by many of the templates, referred to as SNS (for shared noun set). Finally Table 13 shows the 24 templates used. There, for example, cost.v.01 denotes the first verb synset listed in WordNet for the token cost; PPF is shorthand for past, present or future tense, and PF for present or future tense. Recall that the pronoun \u201cI\u201d is mapped to the special synset tal_narrator_i.n.01, and \u201cyou\u201d is mapped to the special synset tal_audience_you.n.01 (see Section 2.1.1). Finally sets of noun synsets are denoted by the curly braces. All templates fire the same \u201cintent to buy\u201d predicate, which is therefore not shown.\nIt is a striking fact that the entire classifier can be written with complete transparency (with consequent full interpretability and correctability) in one page.\n8get.v.22 (purchase) is a hyponym of buy.v.1 (purchase, acquire) which is a hyponym of get.v.1 (acquire); the occurrence of \u201cget\u201d being a hyponym of \u201cget\u201d prevented the Teacher from using \u201cbuy\u201d as a special meaning of \u201cget\u201d.\n3. Factored Dialog Learning\nThe language skills required of an APA can be broadly classified as the generation and comprehension of language9. To model this, we designed two corresponding dialog structures which we call Ask-How-To-Say and Train-Predicate.\n3.1. The Ask-How-To-Say Dialog Structure\nIn the FDL framework, the language teaching stage occurs after the Designer has completed their design for the class of applications and created the corresponding module specification (this will be covered in the next section). In the language teaching stage the Teacher teaches the system the language it will need to converse with the User (beyond any language that can be shared across all apps in the class; that too is specified by the Designer). Throughout the rest of this section we will use Event Reminders as an example class of applications.\nAsk-How-To-Say is a dialog between the system and the Teacher that is intended to develop the system\u2019s language generation skills. The system asks the Teacher explicitly how to say something pertaining to a given, general scenario, and it then uses the Teacher\u2019s response later, when interacting with the User. However, the system is not limited to simply parroting the Teacher\u2019s phrases. For example, for the Event Reminders class, the system extracts a verb phrase from the Teacher that it can use throughout its dialog with the User, as in:\nS10: I need to know how to refer to the type of reminders created by your app. If I tell the user, \"I understand that you\u2019d like me to help remind you when to X\", what would be a good phrase for X? Please be sure that:\n1. X starts with a verb (because it is an action, and will be inserted as a verb phrase into many sentences to request input and confirmation from the user).\n2. X refers to the type of reminder rather than a specific instance of the reminder; for example, \"feed the fish\", but not \"feed the fish once a day\".\nThus when the Teacher is creating an Events Reminder app for tracking medications, for example, they might type:\nT: take your medications\nThis verb phrase would then be used by the system in multiple different interactions with the User.\nAnother way that the Designer can reduce the cognitive load on the Teacher is by making judicious use of defaults. Continuing with the meds tracking example, it might go like this:\nS: I will ask the user to provide some optional notes for their reminder by saying \u201cWhat would you like to name this particular reminder?\u201d. Does that sound good? T: Yes.\n9By \u201cthe APA comprehends X\u201d we simply mean that the phrase X triggered the desired actions by the APA, and did not trigger any undesired actions.\n10We use S, T, U to represent the utterances of system, Teacher, and User respectively.\nThen later, when in dialog with the User,\nS: What would you like to name this particular reminder? U: Take aspirin.\nThis dialog would then trigger the APA to add an event to the User\u2019s calendar labeled \u201cTake aspirin\u201d.\nThe Module Specification Language is described below, but we just make the connection to the MSL here by pointing out that in the MSL YAML file, the above actually appears as follows:\nwhere the system asks the Teacher whether the default language is acceptable, and if it is not, then asks them for the sentence that they would like to use instead. This sentence is then stored by the system in the variable <ask_event_name>. Note that the system can also use its own variables (like <DoesThatSoundGood>, which in this case translates to the string \u201cDoes that sound good?\u201d. The MSL has several such constructs that are intended to make the Designer\u2019s task easier, and they are described below.\nIn User mode, the system gathers the data it needs for the running application by asking the User questions, using application-specific language. To keep the load on the User as light as possible, the system at first prompts the User to simply enter the data in free form, and then confirms its understanding with the User, and queries the User for any missing data. Thus, for example, for the Prescriptions Reminder app, the User can just directly enter their prescription, and as long as all the required data is there, TAL can then just confirm, and then populate the User\u2019s calendar.\nIn order to be able to do this, TAL needs to be able to parse, and to model, fundamental concepts such as space, time, and number. To this end we use Parsed Predicates, as opposed to Learned Predicates (see above). Parsed predicates are patterns that are coded by the developer to parse text and extract key quantities. To make this concrete, we briefly describe the F# structures we use for the built-in parsed predicates. For example, TAL\u2019s code contains a Frequency module that collects several active patterns; one is called OncePerSecond and models frequencies written in the form \u201cOnce per second\u201d, \u201cTwice per day\u201d, \u201c4 times an hour\u201d, etc. (We adopt the convention that the active pattern\u2019s name be like one of the patterns it matches, to make the code easier to read). We call these collections of active patterns, each designed to detect a particular pattern in language, microgrammars. At run time, all microgrammars are scanned across the text, in a particular order: it is thus important to keep the active patterns independent or, if one contains another, to call the more specific version first. For example, in our current code the active pattern Hourly, which matches \u201chourly\u201d, \u201cdaily\u201d, \u201cannually\u201d etc., is called after the active pattern OnceHourly, which\ndetects \u201conce hourly\u201d, \u201ctwice daily\u201d, etc. However, such dependencies are rare: active patterns can be similar yet still independent. For example, the active pattern for EverySecond, which matches \u201cEvery day\u201d, \u201cIn the morning\u201d, etc., is similar to the more specific pattern EveryNtoMSeconds, which matches \u201cevery 3-4 seconds\u201d, \u201cevery 6 to 9 minutes\u201d, etc., but these patterns are independent (i.e. they will never both be triggered by the same text) and so can be used in the scan in either order.\nThe parsed predicates scan the microgrammar active patterns over the text to fill their slots. For example, our built-in parsed predicate for Frequency has two slots, Period (to model the base frequency, like \u201cweekly\u201d) and NumberOfEventsPerPeriod (to model the \u201ctwice\u201d in \u201ctwice daily\u201d).\nThus TAL\u2019s built-in parsed predicates form a rudimentary world model. The hard-wiring raises the concern that this would limit scalability. However, the concepts modeled are fundamental, and can be shared across all apps: we argue that it makes sense to allow limited hardwiring, to take advantage of the extreme succinctness of basic physical models (imagine learning to count, from positive and negative examples only, with no underlying model of the number line). We touch on this issue again in Section 5.3.\n3.2. The Train-Predicate Dialog Structure\nWhile Factored Dialog Learning is centered around factoring the language out of the problem using built-in parsed predicates, we still need Teacher Assisted Learning to learn any predicates the app needs that are not covered by the built-ins. In our running example of the Events Reminders, the only place learned predicates are used is to detect which app the User wants to run. In general, apps will need more learned predicates: we minimized the dependence on learned predicates here to simplify our exploration (and exposition) of FDL. So here, we just use TAL to learn when the User wants to run their app. This will be important in practice, for two reasons: the primary interface is expected to be speech only; and the number of apps is likely to become large11, so listing them all is not practical (especially with a speech-only interface).\nFor example, in the module specification for the event reminder class of apps, the Designer can introduce a \u201cTrain Predicate\u201d dialog:"
    },
    {
      "heading": "S: When the user wants to run an app, they will type a phrase that describes what they want to do, and I will find apps that match that intent. So, you need to teach me how to recognize when",
      "text": "the user wants to run your app, by entering example phrases that the user might type to do this; for example, \"feed the fish\" to create a fish-feeding reminder.\nI will try to generalize your examples, so that you don\u2019t have to type in too many. I will use what you tell me to build a \"predicate\", which describes the meaning I will look for in the user\u2019s text.\nFor each sentence, I will retrieve and display all existing templates that match for your sentence at one or more Verb positions. This will allow you to merge synsets from your new template into an existing template, remove synsets from existing templates, or remove existing templates.\nThen, the Teacher can type in phrases like\nT1: track meds 11AppBrain estimates that the number of Android apps is currently at the 2.5 million mark[1].\nT2: take my medications\nNote that it is advantageous for the Teacher to zero in on key phrases, since they can be detected in longer sentences which may also contain irrelevant details. The process is as described above in section 2 (Teacher Assisted Learning): the Teacher will specify the meaning of each noun and verb according to the ontology, and, for example, the system will generalize \u201cmedication\u201d to \u201cdrug\u201d, if the Teacher chooses to. The system then will recognize User input such as\nU: tell me when to take my pills\nsince there exists a synset for \u201cpill\u201d which is a hyponym of \u201cmedication\u201d, and since possessive pronouns are ignored by the template matching process.\n3.3. Overview of the Module Specification Language\nThe MSL (Module Specification Language) is specified in detail in Appendix 1: Specification of the Module Specification Language. Here, we give an overview of the main ideas behind the MSL.\nIn order to extend the APA to support a new application domain, the Designer must create a module specification script that supports the new class of applications12."
    },
    {
      "heading": "3.3.1 Script Blocks",
      "text": "A module specification script consists of four sections, or blocks, listed below. We will describe the notion of a turn in more detail below, but at a high level, it is a series of system-User interactions that together aim to acquire a single piece of information from the User, or to deliver a single piece of information to the User.\nName: A string that defines the name of the module (for example, Event Reminders).\nInitialize block: A set of app-independent constants used to streamline the interactions with the Teacher (and, less commonly, the User).\nTeach block: Both the Teach and Use blocks consist of a series of two kinds of turn: predicate detecting turns, and parameter value turns. At a high level, the Teach block is a turn-taking structure, where the Designer designs a series of questions intended to leverage the Teacher\u2019s language skills to teach the application the language comprehension and generation skills that are to be used in User mode.\nUse block: This is a turn-taking structure that obtains and parses free User input in order to create the actual entity. In the Events Reminder example, a predicate detecting turn is used to parse free User input describing their reminders, and a series of parameter value turns confirms the values found or to ask the User to fill in missing values (such as the length of time the reminders should run for).\n12We use the term \u201cmodule\u201d here to denote the class of applications being considered."
    },
    {
      "heading": "3.3.2 The \u201cTurn\u201d in detail",
      "text": "The Turn is the key atomic building block in a module specification. Depending on what type of information is to be gathered or delivered, one of four core turn types are used: Prompt, ParamValueTurn, TrainPredicateTurn, and TestPredicateTurn.\nPrompt: A message that the system will show the Teacher/User.\nParamValueTurn: The system can ask the Teacher/User a question and obtain the answer and/or confirm with the Teacher/User that some value is correct. Any parameter value thus obtained can be referenced in the subsequent turns (or in the current turn if it is used after the value is assigned). Figure 4 shows an example that asks for and confirms the start date for a reminder app.\nTrainPredicateTurn: Used to train the templates needed to form a learned predicate detector, where the Teacher supplies example phrases to train the desired Predicate. The Teacher types an example phrase that should fire the predicate, and the system engages in dialog with the Teacher to select the correct parse, synsets, and their generalizations for the identified terms or compound terms. The system uses this information to create a template which it associates with the predicate.\nTestPredicateTurn: The system displays a Question to prompt the User to provide needed information: for example, for a medications tracker app in the Event Reminder class, the User might be prompted to input their prescription. The system them parses the User\u2019s input to detect matches with both parsed and learned predicates. The components of any matching predicates (such as frequency, time, and duration) are mapped to the parameters defined in the PredicateParamMappings field, which can then be referenced in subsequent turns. Values that are present are used to populate a confirmation question (\u201cYou would like your reminder at 7:00 a.m. Is that correct (Y/N)?\u201d); missing values require the user to enter that value (\u201cPlease tell me what time you would like your reminder\u201d) before confirming it."
    },
    {
      "heading": "3.3.3 Other Turn Types in the MSL",
      "text": "In addition to the above four core Turn types, the MSL provides composite Turn types that define iteration and conditional execution, auxiliary Turn types that support these, and Turn types that explicitly control parameter values. We briefly describe these here; see Appendix 1: Specification of the Module Specification Language for complete details.\nSetParamValues: Allows the Designer to set parameter values directly in the script.\nRemoveParamValues: For deleting previously assigned parameter values.\nScriptConditionalActions: Equivalent to an if-else-then dialog flow, based on the boolean value of a condition written in a simple conditional grammar that allows testing whether a script parameter is set or has an infinite value, comparing numeric values (=, >, >=, etc.), and parentheses for precedence.\nUserConditionalActions: Like ScriptConditionalActions, but the condition is the boolean result of a question presented to the User. For example, in the Reminder application, if all necessary slots are filled, the User will be presented with a single sentence to confirm; if No is returned, then the script presents the series of ParamValueTurns.\nParamValueOrConstantTurn: Like ParamValueTurn, but allows the Teacher to enter a constant value if so doing is more appropriate for the app they are creating. For example, a Teacher creating a birthday reminder app would not want to have the app ask the User how often their reminder should fire.\nSetConstantValues: Sets script variables to constant values (such as those set by ParamValueOrConstantTurn) if they were not already set (such as by a TestPredicateTurn).\nNIterations: Equivalent to a for-loop.\nEndIterationAction: For NIterations; the equivalent of a break in a for-loop, with a parameter to specify ending either the innermost or all nested loops (if any).\nEndScriptAction: Ends the script, with a parameter indicating success or failure.\nNoAction: A no-operation; functions as an empty branch of an \u201cif\u201d.\n3.4. Case Study: Adapting the Event Reminder Module\nTo evaluate the generalizability of the FDL framework, we performed a small (3 participants) user study to determine whether ordinary English speakers can easily and effectively function as Teachers, creating reminder applications based in the Event Reminder application class (module). One Teacher participant and one new participant then operated as Users, testing whether these applications could be used to easily and intuitively create reminders for their domains."
    },
    {
      "heading": "3.4.1 Teacher Task Description",
      "text": "Using the Event Reminder Module, we assigned the participants to create two reminder applications: a yearly reminder (such as a birthday) and a regular reminder (such as going to the gym). We provided an informational document that described both their role as a Teacher (creating an application that a User will use to create actual reminders) and the general form of the prompts they would receive and the answers they should supply. Other information was contained in the prompts themselves, many of which showed examples taken from a prototype Medication Tracker application.\nSubject A had some linguistic background, and created a birthday reminder application with an early version of the FDL system; feedback from this session was incorporated into a new version. With this second version, Subject A re-did the birthday reminder application and then created a\ngym reminder application. Subject B, who had no linguistic background, did the gym reminder application first, then the birthday reminder application. Neither subject had any programming or application-development experience.\nSubject C is a software developer with no significant linguistic background, and used a third version of FDL that had been modified based upon the feedback from Subjects A and B, creating first a regular \u201cCall your Friend\u2019 reminder application, and then a Wedding Anniversary reminder application. Additionally, for Subject C, the \u201cmedication tracker\u2019 examples in the prompts were largely replaced with a \u201cfeed the fish\u201d imaginary application, and some additional features were added to the FDL system (these are discussed below in the relevant subsections).\nWe observed the teachers as they performed the tasks and asked them to provide feedback when complete. In particular, we asked whether they felt the system was expressive enough to create the desired application, whether the task was harder or easier than expected, and whether any parts of the task were particularly difficult, confusing, or easy."
    },
    {
      "heading": "3.4.2 Teacher Task Results",
      "text": "The distinction between the Teacher creating an application vs. a User who will use that application to create the actual instances (in this case, reminders) is a key focus of FDL. Subject A\u2019s initial use of the system incorrectly started from the User perspective, for example by using an actual reminder description (\"my father\u2019s birthday\") instead of a reminder creation action (e.g. \"remember a birthday\") for the application activity (\"you would like me to help you remember to ...\"). The documentation and prompts were modified to clarify this distinction and this was not an issue for Subject A\u2019s subsequent pass. Subject B still exhibited some confusion in this, although much less than Subject A did initially. Subject C had only minor confusion about this. A detailed tutorial combined with experience using the app will be sufficient to clarify this important distinction.\nSubject A provided additional valuable feedback to streamline and clarify the system between the first two iterations, in particular in generalizing from the medication reminder examples to a different reminder topic, clarifying how multiple reminders (delimited by \"then\") are detected, clarifying when a response should be \"yes/no\" vs. a sentence or phrase, improving the flow of specific and generalized synset selection (including eliminating duplicate selection requests), and merging or omitting some steps. After this iteration of user feedback and system revisions, the first-use experience for Subjects B was much more straightforward. Building upon feedback from Subjects A and B, additional features (discussed below) were added to address specific pain-points, which led to an improved first-use experience for Subject C.\nSubjects A and B found the gym reminder much easier to understand and create, for the following reasons:\n\u2022 The gym reminder shares its schedule structure with the example medication reminder; both are geared towards daily, while the birthday reminder is annual. \u2022 The gym reminder, like the medication reminder, refers to a specific, concrete event; the birthday reminder is more ambiguous, as it may be for sending an email or card, buying a gift, planning or attending a party, taking a trip to visit, or simply an indefinite reminder to \"remember the birthday.\" \u2022 Some questions have only one applicable value in the reminder domain, such as \"how often do you want the reminder sent?\" for the birthday reminder. Both subjects suggested that these should allow the Teacher to set a constant value such as \"annually\" and not present the question to the User.\nThe FDL system allows the Teacher to provide multiple example sentences for the same Learned Predicate. For Subjects A and B, FDL required that the Teacher explicitly create a new predicate for the application, and create a different one when the meaning changed. Neither subject understood this clearly from the prompts. Subject B, lacking a linguistic background, found the concept of predicates confusing until more explanation was provided, and also did not initially realize that the example sentences were to be supplied as the User would enter them. Additionally, Subject B felt some of the instructions (such as requiring that the phrase referring to the application start with a verb) were unnecessarily restrictive.\nThis feedback led us to make the following changes before Subject C\u2019s session:\n\u2022 The ability to specify constant values was added. Subject C happily used this to specify that reminders for the Wedding Anniversary would occur once annually, forever. \u2022 Because each application should define a single function or a small set of closely related functions, the creation and selection of Learned Predicates was removed in favor of a single predicate per application, reducing complexity for the Teacher. \u2022 Additional explanation was added in the prompts. For example, to address Subject B\u2019s\nfeeling that requiring a Verb to start the phrase referring to the application was too restrictive, additional text was added to explain that this is due to how it will be used in subsequent prompts.\nThese changes resulted in much less confusion and a more streamlined experience for Subject C. However, subject C was still puzzled in a couple of areas, such as not knowing the definition of a compound, and in seeing the name of a script variable. These were subsequently modified for clarity. Subject C also mentioned that some of the prompts were long enough to be difficult to read. Moving much of the material from the prompts to a tutorial that includes both a detailed walk-through of Teaching an application and a parallel demonstration of that application in action during the Use phase would improve both the first-use and, by reducing clutter, subsequent uses by the Teacher.\nBoth subjects felt that the turn-by-turn dialog was helpful in building up the app, and were much more comfortable with the process at the conclusion of their tasks. However, for the Teacher, a GUI rather than the command-line interface would reduce clutter (such as when selecting synsets and generalizations, which can take multiple screens) and could add support for Undo functionality, which would reduce the need for multiple confirmations."
    },
    {
      "heading": "3.4.3 Teacher Task Summary",
      "text": "While there were some initial mistakes by all subjects, the FDL system\u2019s evolution led to a much better experience as testing proceeded. All subjects became more comfortable as they developed experience using the system. With a better tutorial providing a more detailed walk-through of both Teacher and Use phases and a GUI that reduces screen clutter and supports Undo, we believe that Teachers who have solid English skills and no programming or application-development experience will be able to quickly learn how to use the FDL system to create applications, especially if they have some linguistic background."
    },
    {
      "heading": "3.4.4 User Task Description",
      "text": "Subject A and a new Subject D (who has linguistic and machine-learning experience) used the Medication Reminder script to create actual reminders to take medication."
    },
    {
      "heading": "3.4.5 User Task Results",
      "text": "Subject A again went first, and created reminders for each of 10 prescriptions. Having operated twice previously as a Teacher, Subject A was by now familiar with the goals of the process. There was some confusion when being asked to specify a single value for a range (for example, \u201cevery 4 to 6 hours\u201d must be converted to a single \u201cevery N hours\u201d within that range). Here the dialog was not clear as to the context, and there was no clear error message when it retried on out-of-range values. There were also some missing recognitions of abbreviations for intervals. Once these were encountered and understood, the process proceeded quickly, and the last few reminders were completed easily.\nSubject D used the system after Subject A\u2019s feedback was incorporated. Subject D entered only a single prescription and found the process quite straightforward; the only negative feedback was that the number of confirmations slowed the process down."
    },
    {
      "heading": "3.4.6 User Task Summary",
      "text": "Both subjects felt the turn-taking process made it easy to provide the necessary information to create the reminder. Both subjects felt that the confirmations could be streamlined to make the process flow more smoothly.\n4. Discussion\nIn this section, we look back, and assess how things went; in the next, we look ahead, and map out some possibe routes upward, from our newly established base camp, for developing fully transparent and scalable approaches to AI.\nOur main result is that, with the tools we\u2019ve developed, it\u2019s straightforward to build a user-intent detector that is fully transparent, editable, and succinct, and that performs as well as a state of the art machine learning approach. Furthermore, the tools themselves do not rely on sophisticated NLP methods such as semantic role labeling or consituency or dependency parsing; they do not employ statistical methods at all - yet. Thus not only are the results transparent, so are the tools used to achieve them. Finally the ideas should apply equally well to detecting other meanings in text.\nHowever we found that our initial hope, that the Teacher would only need a clear understanding of the app they wish to build, and of their own native language, was only partially fulfilled; while it\u2019s certainly possible to so restrict the teacher\u2019s role, we found that allowing them to edit the text files that contain the templates, and the extensions and deletions in the taxonomy, was very useful. This requires an extra level of training for the Teachers. But if the hope of just a few designers, feeding hundreds of teachers, who can then support millions of users, comes to pass, perhaps it\u2019s not too much to ask a little more of the Teacher role.\nA more serious issue is that we made little headway towards our \u201cEvery teacher benefits from what any teacher teaches\u201d North Star. The built-in parsed predicates are shared, and certainly, teachers can reuse predicate detectors that others have built. But true reusability will likely have to employ hierarchical structures of predicates. We will discuss some ideas for this in the next section.\nAn earlier version of TAL used gazetteers to identify US cities and retailers. We found that we did\nnot need these gazeteers, at least for the \u201cintent to buy\u201d detection task. Later versions may need them. However, compound phrase handling does need to be improved. We found that allowing both paths (i.e. searching for a match by treating a possibly compound phrase as compound, and also as non-compound) introduced training errors, as did treating possibly compound phrases as definitely compound. It seems that to solve this robustly we will need to use context; if the phrase itself could be a compound noun, check that a noun phrase is a possible part of speech, given the surrounding tokens; similarly for verbs; and only employ the \u2019try both paths\u2019 trick if the compoundness is truly ambiguous. Note that this still does not require a full NLP parse of the text.\nFinally the ideas outlined in this paper need to be further checked against more tasks and on other datasets. Our hope is that the benefits resulting from a fully transparent approach will warrant such explorations.\n5. Some Ideas for Future Work\nFor the discussion in this section it is helpful to name two phases of learning that TAL can do: \u201cwake\u201d phase, in which TAL learns directly from a human, and \u201cdream\u201d phase, in which no human is involved. In dream phase, for example, TAL could learn either by leveraging large, unlabeled datasets, or by directly making more sense of what it has already learned.\n5.1. Wake Phase Error Correction\nWe will describe ideas for error correction using large unlabeled datasets below, but we can also ask the teacher for confirmation of consequences of their choices that thay may not have anticipated. If the teacher has arrived at a template using labeled data, TAL could run it in real time over a large unlabeled dataset, show the resulting matches to the teacher, and then allow them to modify the template accordingly. False positives are thus easy to control. False negatives pose a harder challenge. For these, perhaps the simplest approach would be to run the unlabeled data through both TAL and a statistical system such as LUIS, then have the teacher manually inspect any samples on which the two systems disagree and update the templates as needed.\n5.2. Dream Phase Error Correction"
    },
    {
      "heading": "5.2.1 Template Error Correction Using Unlabeled Data",
      "text": "Suppose that a teacher has taught TAL an overly general template intended to fire the predicate \u201cperson ingests food\u201d. Suppose that their template is (schematically) as illustrated in Table 14, where the synsets of the template are in the first column and the corresponding roles defined by the teacher (i.e. the corresponding predicate\u2019s components) are in the right column. This template overgeneralizes because, for example, it will fire for the phrase \u201cbuilding eats person\u201d.\nOne way to leverage a large, offline, unlabeled dataset to check that a template does not overgeneralize is as follows. First, run the template over the dataset, making a record of all matches. For each match, increment a counter in the node in the WordNet-based taxonomy, where the counter is labeled by its component name in the corresponding predicate (e.g. \u201cingester\u201d). (Note that a given token will correspond to multiple nodes since at test time, we don\u2019t know its correct synset.) The taxonomy hypernym tree (for each component of the predicate) can thus be represented as a heat\nmap. If the template is too general, then the heat map will contain cold spots at, or close to, the leaves. (The token \u201cbuilding\u201d will rarely occur as an \u201cingester\u201d in the dataset). This computation gives us a distribution over the nodes of a subtree of the taxonomy for each predicate component, where the root of the subtree is that component\u2019s synset. We can extend this reasoning to handle templates with sets of synsets, but for clarity here we consider components that correspond to a single synset in the template being tested.\nFor concreteness consider just the first component, and call its tree T. If some subtree S \u2282 T has nodes that are sufficiently \u201ccold\u201d, one could try to find a subtree S\u2032 \u2282 T that maximizes the number of \u201chot\u201d nodes and minimizes the number of \u201ccold\u201d nodes. If necessary, one could split the original template into two or more, such that each has only \u201chot\u201d nodes, and all \u201chot\u201d nodes in T are covered (this fits naturally into our template model that uses sets of noun synsets). This process is necessarily statistical, since rare phrases do occur (buildings can eat people, in fiction or metaphors).\nAn interesting line for future work is to take the brakes off this process and simply ask: given a large, unlabeled text dataset, how can one choose N templates for which the amount of matching text is maximized, but such that amount of text for which pairs of templates match is minimized? If N = 1, presumably a template could be chosen that will cover most, or all, of the data. For N \u2265 2, templates compete to explain the data. In this way, one might approach the fully offline learning of templates. How far can one get by using only the taxonomy and large unlabeled datasets, in this way? Could some predicates arise naturally from the resulting templates? Could such a set of templates help inform us on how to build a suitable hierarchy of predicates?"
    },
    {
      "heading": "5.2.2 Template Unification",
      "text": "Similarly a large unlabeled dataset can be used to identify templates that are essentially the same, even if their sets of synsets differ, by the similarity of their heat maps. First, two templates that are candidates for unification could be found by automatically comparing pairs of heat maps. Those templates might then be unified if a template could be found that maximally agrees with the combined heat maps of the individual templates. A process like this will likely be needed to identify similar templates proposed by different teachers, if the automated checking process used during training (\u201cyour input phrase fired this template: combine or add?\u201d) fails. Similar ideas can be applied to identify when the predicate being trained is likely the same as one already learned. Note that these ideas are already instantiated, to some extent, in TAL: any phrase input by a Teacher is tested against all existing predicates, and the Teacher is alerted if there is a match; similarly, TAL currently allows the unification of parts of templates, if those templates share a verb synset.\n5.3. Extensibility"
    },
    {
      "heading": "5.3.1 Multiple Languages",
      "text": "TAL\u2019s language dependence currently resides in the WordNet taxonomy; a \u201cmicrogrammars\u201d file that instantiates the built-in parsed predicates; an \u201cEnglish POS\u201d file that lists parts of speech that only occur via small numbers of tokens (e.g. pronouns, and question adverbs); the function that maps text to lemmas and tense, using the Slang Map; the function that maps annotated lemmas to the \u2019parsed components list\u2019; and two supporting files, of verb declinations and nouns with their plurals. Clearly the adherence to transparency comes with a development cost when it comes to adding new languages. WordNet is supported for languages other than English [33], and one can certainly envision using other, similar taxonomies. Extending templates and predicates from one language to another requires more than just having a mapping of synsets available, since different languages can express the same concept in very different ways. Even so, it may be possible to use machine translation systems to accomplish this automatically. If one has a large database of paired phrases for the two languages, then the templates in the new language could be learned automatically, and its predicates\u2019 component names could be translated similarly. This idea, combined with a given mapping of synsets from one language to the other, could also be used to learn the mapping of the predicate components to their corresponding template components in the new language."
    },
    {
      "heading": "5.3.2 TAL\u2019s World Model",
      "text": "We think of the built-in parsed predicates - e.g. detecting properties of time, space, and number - as forming the basis of TAL\u2019s world model. Currently the structure containing the predicates is flat, but it\u2019s clear that a hiearchy will be needed: velocity needs the concepts of space and time, acceleration needs velocity and time, etc. It seems likely that a hierarchy would also be required to make learned predicates scalable. If one entity is about to eat the other, then the two entities must be colocated. If two people are married then they must know each other, must have made a joint commitment, etc.\nOne way to encode logical relationships is to have a directed graph whose nodes are negatable predicates (by a negatable predicate, we just mean a predicate with an added negation flag: thus he is not eating fish would fire the Ingests(person, food) predicate with its Not flag set). If a node is true only if its children are all true, then we have the logical connectives And and Not, which form a functionally complete set (a set from which any truth table can be constructed). It may however be more convenient to explicitly model the Or connective by instead having a directed bigraph in which nodes are either predicates or connectives, and for which every node has an attached Boolean variable. For example, if several predicate nodes are connected to an Or node, the latter is true only if one or more of those predicates is true. Such a graph is one way of implementing logical inference, and such a hierarchy would also help with the interpretability of the overall model. One can apply similar ideas to those above, to detect logical relations between predicates automatically. Thus if a set of templates and predicates is run over a very large dataset, and predicate A is always found to be true whenever predicate B holds, then the relation B\u21d2 A could be added to the graph. It is more likely, however, that a probabilistic way of modeling logic, such as Markov logic networks [22], would be required.\nFor scalability, if a new parsed predicate must be added, and it can be defined in terms of existing parsed predicates (for example, speed or velocity in terms of position and time), ideally one would\ndefine the new predicate in terms of the others in a script file, without having to write new code."
    },
    {
      "heading": "5.3.3 Extending to Large Numbers of Templates and Predicates",
      "text": "TAL already uses a simple mechanism to limit its search: roughly speaking, only those templates that are indexed by the token currently being examined are tested for a match for the following tokens. The search can be further limited by partitioning templates using context. For example, when TAL is awaiting user input to determine which app to run, it knows it\u2019s in a particular state, and so it can safely use templates that would overgeneralize in other situations: so for example a template containing the single verb \u201cexercise\u201d can be used to select a \u201cgo to the gym\u201d reminder app, and not used elsewhere. More generally, TAL can represent its inner state using a finite state automaton, and have only subsets of templates (and their predicates) available for each state. This could be extended to its interactions with the user: TAL could model the user\u2019s state using a Markov chain, where each state again links to a limited set of templates, and only those templates corresponding to states whose probabiliy exceeds a threshold would be tested for matches."
    },
    {
      "heading": "5.3.4 Extending the MSL",
      "text": "We currently define a number of Turn types in MSL (see Appendix 1: Specification of the Module Specification Language). These are intended to be generic and flexible enough to accommodate all the necessary question-answering and control-flow operations needed by a Designer. Supporting domain-specific Turn types would require a plug-in model for FDL to understand which actions to take for which questions.\nWe may consider supporting module reuse through some form of module inheritance, allowing a module to extend a base module. The base module might include all the required metadata-collecting operations such as those in the Initialize block, so the the derived modules do not need to repeat them. This is similar to the superclass constructor execution when constructing subclass instances in object-oriented programming. Similarly, we might extend the script blocks to allow a base module to define a set of of Turns that could be \u201ccalled\u201d by a derived module.\n5.4. Other Benefits of Model Transparency\nSuppose you have several statistical models that were trained (possibly using different data) for the same task. How best to combine them? Model averaging often works well. But that\u2019s a hack and it requires running all the models. With a fully transparent model such as TAL, the models (i.e. their templates) can be directly combined, resulting in a model with the benefits of all the trained models but that is both more compact and efficient than using all of the models independently. This is another way that transparency can contribute to scalability. Multiple Teachers can do their thing on their own data, training the same predicate (task), and the results can be combined. Being able to handle the inputs of many teachers is a key requirement for scalability.\n5.5. The Machine Comprehension of Language\nIn [3]), the following practical definition of the machine comprehension of text was proposed:\nA machine comprehends a passage of text if, for any question regarding that text that can be answered correctly by a majority of native speakers, that machine can provide\na string which those speakers would agree both answers that question, and does not contain information irrelevant to that question.\nWe can recast this in terms of predicates as follows:\nA machine comprehends a passage of text if, for every (possibly implicit) assertion made by the text, as identified by a majority of native speakers, that machine asserts the corresponding predicate to be true, and it does not assert any other predicates to be true.13\nNote that some \u2019false negatives\u2019 are easy to detect automatically: if no predicate fires for a passage, and if we know that the passage has some meaning that should have been detected (as would be the case, for example, in parsing a passage for general question answering), then the machine necessarily does not comprehend that text.\nThe above definition gives us a way to measure how well a system comprehends a dataset of text samples. We could first crowd source the labeling, asking workers to write down all assertions (predicates) made by each sample text, and then determine the overlap between the human generated predicates, and those predicates declared to be true by the system when run over the text. Errors introduced during labeling could be controlled by using multiple workers for each sample, and by asking a further set of workers to verify any outlier claims. We might further test the system\u2019s world model by distinguishing those predicates that follow directly from the text, and those that are implied indirectly by it.\nThis measurement strategy would of course work for any system that makes assertions via predicates, giant black-box neural nets included. However, as we have argued above, we believe that the fully transparent, interpretable, correctable, predicate-based approach presented in this paper at least provides us with a good starting point - a base camp - for further investigations into scaling AI.\n6. Acknowledgements\nWe thank Jason Williams, Paul Bennett and Richard Hughes for many valuable discussions and for their support of this work. We also thank Vishal Thakkar, the Stargate Support team, and Hisami Suzuki for their help and guidance in our generating the Cortana dataset.\nThis work is the culmination of several years of explorations. We thank Erin Renshaw for her steadfast help with the earlier work. We thank John Platt for his vision in supporting big-bet, long-term research at MSR, which freed us to (repeatedly) fail. We additionally thank Eric Horvitz, Jeannette Wing and Harry Shum for their leadership and vision in supporting long term research in general and this work in particular.\n13Note that this definition also covers negations.\n7. Appendix 1: Specification of the Module Specification Language\nThis section provides the detailed specification of the Module Specification Language (MSL) used by the Designer to create module scripts for FDL. A brief overview was given above in section 3.3 (Overview of the Module Specification Language).\n7.1. YAML Basics\nYAML14 is a serialized data representation format that can be edited easily by humans and parsed by a program. Our module specification language uses Yaml as a representation medium. In this section, we discuss the elements of YAML that are used in the example Medication Reminder Module script.\nScalar is the basic data type, which can represent a single value, e.g. a number or a string. Type inference is automatically done most of the time in Yaml; Table 15 contains some examples.\nMapping and sequence are two basic ways to composite scalars. Mapping is similar to the map or dictionary data structures in most programming languages; values are referenced by unique keys, and the key/value pairs are unordered. The key and the value are separated by a colon and a space, for example\nname: Tom age: 15 married: Yes\nis a mapping. In Yaml, the indentation really matters; all entries in a mapping must have the same indentation.\nSequence is similar to the list data structure in most programming languages, which means the values are ordered and referenced by the index (position in the list). Each entry begins with a hyphen and a space. For example\n- Tom - Jerry - Jack\nis a list of three entries. Again, indentation matters; the hyphens must have the name indentation.\nAn empty sequence or map is indicated by:\n14http://yaml.org\nMyList: [] MyMap: {}\nComplex objects can be represented by a combination of mappings and sequences. For example, a sequence of instances of a data structure containing the fields \u201cname\u201d, \u201cage\u201d, and \u201cmarried\u201d would be:\n- name: Tom age: 15 married: Yes - name: Jerry age: 14 married: No - name: Jack age: 12 married: No\nAgain, note that the hyphens are indented at the same level, and each hyphen is the start of an instance of the structure.\nBy default, YAML concatenates all lines together in a single value. You can use a special sequence starting with \u2018|\u2019 to preserve linebreaks and whitespace; \u2018|-\u2019 suppresses the final one linebreak.\nQuestion: This becomes one line, with one space between \"becomes one\". Question: |- This remains three lines with 1. Item One 2. Item Two\nThe comment indicator in Yaml is \u2018#\u2019; anything from this to the end of the line is ignored.\n7.2. Script blocks\nA module specification script partitions the specification into four sections, or blocks, listed below. We will describe the notion of a turn in more detail below, but at a high level, it is a series of system-User interactions that together aim to acquire a single piece of information from the User, or to deliver a single piece of information to the User.\nName: A string that defines the name of the module (for example, Event Reminders).\nInitialize block: A set of app-independent constants used to streamline the interactions with the Teacher (and in principle with the User, although most variables used in User mode are set in the Teach block). For example, our Event Reminders module defines the \u3008OKEnterSentence\u3009 string constant to have the value \u201cOK, then please enter the sentence you would like me to use here.\u201d and it\u2019s used six times in the Teach block. The Initialize block can also be used to ask the teacher any questions needed to acquire metadata about the application; this data is then saved using reserved keys.\nTeach block: Both the Teach and Use blocks consist of a series of two kinds of turn: predicate detecting turns, and parameter value turns. At a high level, the Teach block is a turn-taking\nstructure, where the Designer designs a series of questions intended to leverage the Teacher\u2019s language skills to teach the application the language comprehension and generation skills that are to be used in User mode. Both turn types can also be embedded in conditional blocks. In the Events Reminder example, a predicate turn is used to elicit from the Teacher sentences that exemplify the language that the User is likely to input in order to identify this application from the pool of apps available to the User; this lets the APA know which app the User wants to run. In the parameter value turns, the Teacher specifies, for example, the language used to elicit a name for the series of reminders that the User is creating. The Designer can also elicit from the Teacher any language that is likely to be repeated (as in the example above); for example, the Designer might want to extract a general verb phrase from the Teacher, to refer to the type of reminders created by the app (for example, \u201ctake your medications\u201d for a meds reminder app); this verb phrase is then referred to throughout the remainder of the script (in both Teach and Use blocks).\nUse block: This contains a set of sub-blocks named for the action being performed on the instance. In the Event Reminder module, the only action defined is create, when creating the reminder. This could be extended with, for example, edit, which reads an existing reminder and allows the user to edit it. Each Use sub-block is a turn-taking structure that obtains that parses free user input in order to execute the action on the entity; for the create action, this is to create the entity. In the Events Reminder example, a TestPredicateTurn parses free User input describing their reminders and obtains parsed predicates, and a series of parameter value turns confirms the values found or asks the User to fill in missing values (such as the length of time the reminders should run for). Thus, for an individual reminder, ideally a single predicate detecting turn receives a sentence from the User that contains all the information needed to create the reminder, and the system then confirms the information in a single turn. If the user chooses to change some of this information, or if some necessary information is missing, further parameter value turns are run to gather the needed data.\nNote that the natural sequential nature of an MLS script means that one can create variables early in the script that can be used throughout the rest of the script, in all blocks.\n7.3. Script Parameters\nValues are obtained from the Teacher or User and stored in parameters; parameters also contain the values to be presented to the user."
    },
    {
      "heading": "7.3.1 Parameter Names and References",
      "text": "The parameter should be named using only A-Z, a-z, 0-9, underscore (\"_\"), or period (\".\"), and is case sensitive. There are some special name formats with the underscore that should be reserved for a specific purpose; see section 7.3.2 (Special Parameter Names).\nIn the following discussion and subsections, we occasionally use as an example a parameter named par.\nA parameter value can be referenced in the subsequent turns (or in the current turn if it is used after the value is assigned) using the syntax ${par}. The process of converting from this name representation to its value is referred to as expansion. Depending on the context of the parameter\nreference, the parameter value may be converted to the various types (e.g. to string if used in screen printing).\nParameter names can be built up from other parameters into a composite name. For example, ${frequency.${i}.period} first obtains the value for ${i}, for example 0, and then forms the name ${frequency.0.period}, which can then be looked up directly. The bracketing ${} is required if expansion is to be done, and otherwise should not be present. In particular, when a parameter\u2019s name is to be used rather than its value, expansion should not be done for the entire name, but it may have to be done for embedded names if the parameter is a composite. This is discussed in more detail below."
    },
    {
      "heading": "7.3.2 Special Parameter Names",
      "text": "We suggest reserving names that begin with an underscore for parameters with special meaning. In particular, names using all uppercase letters with two underscores at both the beginning and end should indicate \u201cexternal\u201d values, such as the state of the script, parameters that have a special meaning to FDL, or a value that an application will look for. For example, section section 7.6 (Finding the App the User Wants) describes how some special parameters are used to find the application the user wants to run.\nTable 16 shows the current set of special names in FDL."
    },
    {
      "heading": "7.3.3 Parameter Types",
      "text": "Table 17 lists the parameter types currently supported by FDL.\nHere is an example of defining a parameter\u2019s name and type, for example in a SetParamValues or ParamValueTurn:\nParam: person_name Type: STRING\nTo define a parameter of NOMINAL type, specify the values as a mapping, which should start on a new line with extra indentation, e.g.:\nParam: fruit_category Type:\nNOMINAL: [apple, orange, pear]"
    },
    {
      "heading": "7.3.4 Conditional Operations on Parameters",
      "text": "FDL supports conditional comparisons to provide if-then-else logic for selecting which Turns to execute. Table 18 lists the conditional operations currently supported by FDL; these are the ones that were useful in developing the Medication Reminder module, and more may be added in the future. These comparisons are done in a ScriptConditionalActions Turn.\nComparisons may be combined with AND and OR (which short-circuit), and parentheses may be used for precedence."
    },
    {
      "heading": "7.3.5 Arithmetic Operations on Parameters",
      "text": "FDL supports a limited set of binary operations on parameter values; the syntax is similar to parameter references except that two parameter names (or a parameter name and a value) are\npresent in the expression and are separated by an operator. Currently, we support the binary operators \"+\" and \"-\" between a \u2018DATE\u2018 and an \u2018INTERVAL\u2018 parameter, and between a \u2018NUMBER\u2018 parameter and a float value. This is illustrated in Table 19.\n7.4. Script Handlers\nA Handler combines a script name with an action name and is the means by which the FDL system determines which scripts apply to a user\u2019s intended action. For a discussion and example of how Handlers are used, see section 7.6 (Finding the App the User Wants). Table 20 describes the Handler properties.\n7.5. Turn Types\nA Turn in our context is a series of interactions that together aim to acquire a single piece of information from the user or deliver a single piece of information to the user. This is the atomic building block of an interactive application."
    },
    {
      "heading": "7.5.1 Prompt",
      "text": "This defines a message that the system will show the Teacher or User. It has no properties; the value of the Turn itself is the message to be displayed.\nExample:\nPrompt: Let\u2019s edit your application."
    },
    {
      "heading": "7.5.2 ParamValueTurn",
      "text": "Presents a question to the Teacher or User and obtains the result value. There are numerous possible interaction flows, depending upon the presence or absence of the referenced parameter and properties.\nThe FDL system will append \u201c(Y/N)?\u201d to the the Confirm question when it displays it to the Teacher or User.\nExamples:\nThe simplest ParamValueTurn consists of only Param, Type, and Question properties, where the system asks the user a question defined in the Question property, converts the answer to the type specified in the Type property, and assigns the result to the parameter named in the Param property.\nExample: ParamValueTurn:\nQuestion: Please name the script (e.g. \"AMI Medication Tracker\"). Param: __NAME__ Type: STRING\nIf there is a viable default value for the parameter, the script can confirm this value with the\nTeacher or User without asking him or her the initial question. In this case, the ParamValueTurn would have the Default, Confirm, and Loopback properties. The system first assigns the value in the Default property to the parameter named in the Param property, and then confirms with the user using the question in the Confirm property. If the user says \"no\" to the Confirm question, then the Loopback question will be asked and the user will be asked to Confirm again. Usually the Param should be referenced in the Confirm.\nExample: ParamValueTurn:\nDefault: 3 days Loopback: Please tell me over what duration you\u2019d like your reminders set? Confirm: You would like to take medication for ${duration}. Is that correct? Param: duration Type: INTERVAL\nA single ParamValueTurn can contain both a Question and a Confirm. In this case, the system first asks the user the Question and saves the value in the Param before Confirming with the user. If the user denies the confirmation, then user will be asked the the Loopback question if it exists; otherwise the same Question will be asked again. Either way, the Confirm is asked again and the process continues.\nIf you specify a particular type, but the system couldn\u2019t recognize an instance of that type in the user\u2019s input, then you could also define the MistypeFollowup property to give the user more hints on what we expect the user to input.\nExample: ParamValueTurn:\nQuestion: When are you going to start taking the medication? Confirm: You would like to start on ${start_date}. Is that correct? MistypeFollowup: What date is that? You could enter \"today\", \"tomorrow\", or a specific date such as \"July 19, 2016\". Param: start_date Type: DATE\nAnother important feature of ParamValueTurn is that if Param has been initialized in the previous turns, then the current turn will be skipped, unless the existing value type doesn\u2019t match the required param type, in which case the system forces the type mapping with the user\u2019s help. For example, if we define a parameter duration with the type INTERVAL, then the expected value type should be IntervalValue; if we find the value of this parameter has the value type IntervalValueRange (see section 7.3.3 (Parameter Types)), then the system asks the user to select an IntervalValue from the IntervalValueRange and assigns the updated value to the parameter duration. A similar dialog occurs if a NUMBER parameter has a value that is a NumberValueRange.\nTo summarize, the value is assigned to a parameter in the following order:\n1 . Existing value assigned in the past, if any;\n2 . The expanded Default property of the ParamValueTurn;\n3 . The User\u2019s input as response to the Question."
    },
    {
      "heading": "7.5.3 TrainPredicateTurn",
      "text": "The TrainPredicateTurn leverages the Teacher\u2019s language skills to train the templates needed to form a LearnedPredicate detector. When the system enters a TrainPredicateTurn it asks the teacher to type an example phrase that should fire the LearnedPredicate associated with the application being created. The system then engages in dialog with the Teacher to select the correct parse, synsets, and their generalizations for the identified terms or compound terms, creates a template, and associates the predicate with that template. The process of supplying example sentences continues until the Teacher has no further phrases they wish to use; for example, when every new phrase the Teacher enters is matched by an existing template.\nThe TrainPredicateTurn has the following properties:\nExample: - TrainPredicateTurn:\nPredicateName: ${event_phrase} # The phrase used to refer to the event Notes: \"\" Handler:\nActionName: \"create\" ScriptName: ${__NAME__} # The name of the current script"
    },
    {
      "heading": "7.5.4 TestPredicateTurn",
      "text": "The TestPredicateTurn is used in User mode to determine which predicates match input from the user. The system displays the Question to prompt the User to provide needed information: for example, for a medications tracker app in the Event Reminder class, the User might be prompted to input their prescription. The system them parses the User\u2019s input to detect matches with both parsed and learned predicates. The components of any matching predicates are mapped to the parameters defined in the PredicateParamMappings field. For example, our built-in parsed predicates currently identify the frequency, time, event duration, start date, and the duration of\nthe reminder sequence (how long the reminder should remain on the calendar). If these predicates fire due to the user\u2019s input, their slots are then assigned to the corresponding script parameters, which can then be referenced in subsequent turns; the script can populate a confirmation question (\u201cYou would like your reminder at 7:00 a.m. Is that correct (Y/N)?\u201d) without requiring the user to first enter that value (\u201cPlease tell me what time you would like your reminder\u201d), which must be done if the slot is not filled.\nThe TestPredicateTurn has the following properties:\nExample: TestPredicateTurn:\nQuestion: Please read me the instructions on your prescription. Param: description_string CountParam: count PredicateParamMappings:\nPred_Freq: frequency Pred_SequenceDuration: sequence_duration Pred_EventDuration: event_duration Pred_StartDate: start_date\nPredicateCountParam: description_predicate_count\nIn this example, the user will input a sentence in response to the Question. First, the system splits the sentence into phrases by splitting on the \"then\u201d keyword; this can be extended to other keywords or syntactic structures. The number of clauses found is stored in the parameter named by the CountParam property. The system then parses each clause to extract parsed predicates from it.\nThe system creates a set of parameters for each clause, regardless of whether the phrase contained a parsed predicate or not. For each clause, numbered from 0 to the value in the CountParam parameter, the system creates a parameter for each parsed predicate, with the ordinal of that clause appended to the parameter name specified in the TestPredicateTurn. In the above example, if the user\u2019s sentence contained 3 clauses, the system would create the following parameters frequency.0, frequency.1, frequency.2, and similar parameters for the other parsed predicates found in the clause. These parameters are used by a subsequent NIterations turn to engage in a dialog with the User for each clause.\nFDL currently supports the built-in parsed predicate types in table 24.\nOnce the TestPredicateTurn is complete, a subsequent series of Turns, usually within an NIterations Turn, presents confirmation of each parsed predicate that was detected in the clause, as well as asking for any that were not (and any other values needed). If the User\u2019s sentence fully specifies all the necessary information (including all required parsed predicates), then a single confirmation sentence can be presented; in this case it would be necessary to confirm each individual value only if the User rejects this initial confirmation."
    },
    {
      "heading": "7.5.5 NIterations",
      "text": "An NIterations composite turn is similar to a for-loop in most programming languages. It has the following properties:\nThe NIterations Turn has the following properties:\nExample: NIterations:\nN: \u201910\u2019 IterVar: i Turns:\n- Prompt: Now ${i}! is similar to\nfor (int i = 0; i <= 10; i++) { printf \"Now \\%d!\" i }\nTo present the iteration number in a more user-friendly way, the system provides a special parameter ${ordinal} that maps the IterVar value (which is zero-based) to the corresponding ordinal word (which is one-based). For example, in iteration 0 (i.e. \"i = 0\"), then ${ordinal} has a value of \"first\".\nNIterations loops can be nested; the Turns list may contain an inner NIterations turn. In this case, the ${ordinal} special parameter is scoped to the inner iteration\u2019s IterVar.\nIt is often possible to set the parameter reference N to a previously assigned NumberValue. For example, in the prevous example, we define a parameter count as the number of clauses in the sentence; to iterate over these clauses, the N parameter would be set to ${count}.\nExample: NIterations:\nN: ${count} IterVar: i Turns:\n- Prompt: Let\u2019s confirm the ${ordinal} reminder. - ParamValueTurn:\nQuestion: Please tell me over what duration you\u2019d like your reminders set. Param: duration.${i} Type: INTERVAL\nThe NIterations step allows date-sequencing logic. For the first iteration, the frequency start_date is as set by the user. In subsequent iterations, the start_date is the end date of the previous iteration, and this can be tracked in a script parameter using the SetParamValues turn.\nExample: SetParamValues:\nParam: next_start_date Type: DATE Value: ${start_date.${i} + sequence_duration.${i}}"
    },
    {
      "heading": "7.5.6 SetParamValues",
      "text": "An SetParamValues turn allows the Designer to set parameter values directly in the script.\nThe SetParamValues Turn has a list of structures that specify the parameters to be set; each has the following properties:\nExample: SetParamValues:\nParam: i Type: NUMBER Value: 0"
    },
    {
      "heading": "7.5.7 RemoveParamValues",
      "text": "An RemoveParamValues turn allows the Designer to remove parameter values directly from the script.\nThe RemoveParamValues Turn has a list of the following property:\nExample:\nRemoveParamValues: Param: frequency.${i}.period Param: i"
    },
    {
      "heading": "7.5.8 ScriptConditionalActions",
      "text": "A ScriptConditionalActions is equivalent to an if-else-then dialog flow, based on the boolean value of a condition written in a simple conditional grammar that allows testing whether a script parameter is set or has an infinite value, comparing numeric values (=, >, >=, etc.), and parentheses for precedence.\nThe ScriptConditionalActions Turn has a list of the following property:\nExample: ScriptConditionalActions:\nCondition: ${i} > 0 AND start_date.${i} IS NOT SET AND next_start_date IS SET IfYesTurns:\n- SetParamValues: - Param: start_date.${i}\nType: DATE Value: ${next_start_date}\nIfNoTurns: - NoAction\nSee also EndIterationAction and EndScriptAction."
    },
    {
      "heading": "7.5.9 UserConditionalActions",
      "text": "An UserConditionalActions turn allows the Designer to remove parameter values directly from the script.\nThe UserConditionalActions Turn is similar to ScriptConditionalActions, but the condition is the boolean result of a question presented to the user. For example, in the Reminder application, if all necessary slots are filled, the user will be presented with a single sentence to confirm; if No is returned, then the script presents the series of ParamValueTurns.\nThe FDL system will append \u201c(Y/N)?\u201d to the the Question when it displays it to the Teacher or User.\nExample: UserConditionalActions:\nCondition: ${i} > 0 AND start_date.${i} IS NOT SET AND next_start_date IS SET IfYesTurns:\n- SetParamValues: - Param: start_date.${i}\nType: DATE Value: ${next_start_date}\nIfNoTurns: - NoAction\nSee also EndIterationAction and EndScriptAction."
    },
    {
      "heading": "7.5.10 ParamValueOrConstantTurn",
      "text": "The ParamValueOrConstantTurn is similar to the ParamValueTurn, but also allows the Teacher to enter a constant value if so doing is more appropriate for the app they are creating. For example, a Teacher creating a birthday reminder app would not want to have the app ask the user how often the reminder should fire.\nThe system will ask the Teacher to select one of three options:\n1. Ask the user a question: This will display the question that the user will be asked. If this option is selected, no confirmation is requested.\n2. Ask the user a different question: This will enter into a dialog with the Teacher to specify the question to be asked.\n3. Specify a constant value: This will enter into a dialog with the Teacher to specify the constant value to be used instead of asking the user a question.\nThe properties of the ParamValueOrConstantTurn are:\nThe FDL system will append \u201c(Y/N)?\u201d to the the QuestionConfirm and ConstantConfirm when it displays them to the Teacher.\nExample:\n- ParamValueOrConstantTurn: Description: I will ask the user how long to keep the reminder in their calendar. AskQuestionConfirm: I will ask the user: \"${ask_sequence_duration_language}\". AskDifferentQuestion: Ask the user a different question. AskSpecifyConstant: Do not ask the User; instead, specify a constant value. QuestionLoopback: Please enter the new question to ask the user. QuestionConfirm: You want me to ask the user \"${ask_sequence_duration_language}\". ${_IsThatCorrect} QuestionParam: ask_sequence_duration_language QuestionDefault: Please tell me how long to keep the reminder in your calendar. ConstantLoopback: Please enter the constant value you would like to use here. ConstantConfirm: You would like to stop the reminders after a constant period of ${constant_sequence_duration}. ${_IsThatCorrect} ConstantParam: constant_sequence_duration ParamType: INTERVAL"
    },
    {
      "heading": "7.5.11 SetConstantValues",
      "text": "The SetConstantValues Turn sets script variables to constant values (such as those set by ParamValueOrConstantTurn) if they were not already set (such as by a TestPredicateTurn).\nThe SetConstantValues Turn has a list of structures that specify the parameters to be set; each has the following properties:\nExample: - SetConstantValues:\n- ConstantParam: constant_sequence_duration Param: sequence_duration.${i} - ConstantParam: constant_period Param: frequency.${i}.period"
    },
    {
      "heading": "7.5.12 EndIterationAction",
      "text": "The EndIterationAction Turn exits an NIterations loop. It contains the following property:\nExample: - EndIterationAction:\nCurrentIterationOnly: true"
    },
    {
      "heading": "7.5.13 EndScriptAction",
      "text": "The EndScriptAction Turn exits the script. It contains the following property:\nExample: - EndScriptAction:\nIsSuccess: true"
    },
    {
      "heading": "7.5.14 NoAction",
      "text": "The NoAction Turn is a no-op; it is an empty branch in the conditional actions turns and has no parameters.\n7.6. Finding the App the User Wants\nWhen the user wants to use an app to perform an action, the FDL system will ask for a sentence that describes the action, and will then parse this action and find any matching LearnedPredicates that have a Handler (see Script Handlers) for that action. Following are the steps taken by the FDL system when the user wants to use an app to perform a create action; for example, using the Medication Reminder app to create a reminder to take medication.\n1. The FDL system asks the user to enter a sentence describing the activity to create a reminder for. It parses this sentence, determines which templates match it, and forms a set of all the LearnedPredicates that are fired by those templates and have an entry in their Handlers set for the ActionName create.\n(a) If there is only one such LearnedPredicate and it has only one Handler for the create ActionName, then that app is selected automatically.\n(b) If there is no matching LearnedPredicate, the FDL system presents a list of all available apps for which one or more LearnedPredicates have a Handler with the create ActionName. These apps are identified by their scripts\u2019 __ACTION_DESCRIPTION__s. The user is asked to select an app.\n(c) If there is more than one matching LearnedPredicate or one or more matching LearnedPredicates have more than one Handler with the create ActionName, the FDL system presents a list of all matching apps, again identified by their scripts\u2019 __ACTION_DESCRIPTION__s. The user is asked to select an app.\nWhen the user selects an app (or if an app is selected automatically because it is the only matching app), then the FDL system asks the user to confirm the action by presenting the app\u2019s __ACTION_CONFIRMATION__ question. If the user says \u201cYes\u201d, then the FDL system loads the script for that app (as identified by the selected Handler\u2019s ScriptName) and begins the app\u2019s dialog with the user.\n8. Appendix 2: TAL\u2019s American Slang and Abbreviations Map\nDuring preprocessing, tokens found in the left column in user input data are mapped to the right (see Section 2.1.3). Teachers can edit this file if they wish: note that here, a teacher added the mapping \u201cmeds\u201d to \u201cmedications\u201d for their app (this was not necessary, as otherwise TAL would ask the teacher what \u201cmeds\u201d means the first time the teacher used it, and add it as a new definition to its taxonomy).\nReferences\n[1] AppBrain. Number of available apps, 2016. http://www.appbrain.com/stats/ number-of-android-apps.\n[2] B.W. Boehm. A spiral model of software development and enhancement. Computer, 21(5):61\u2013 72, 1988.\n[3] C.J.C. Burges. Towards the machine comprehension of text: An essay. Technical Report MSR-TR-2013-125, Microsoft Research, 2013.\n[4] D. Crevier. AI: The tumultuous history of the search for artificial intelligence. Basic Books, Inc., 1993.\n[5] C. Fellbaum. WordNet. Wiley Online Library, 1998.\n[6] I. Goodfellow, Y. Bengio, and A. Courville. Deep learning. Book in preparation for MIT Press: http://www.deeplearningbook.org, 2016.\n[7] B. Hixon, P. Clark, and H. Hajishirzi. Learning knowledge graphs for question answering through conversational dialog. In Proceedings of the the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Denver, Colorado, USA, 2015.\n[8] Z. Hu, X. Ma, Z. Liu, E. Hovy, and E. Xing. Harnessing deep neural networks with logic rules. In 54th Annual Meeting of the Association for Computational Linguistics, 2016.\n[9] S. Kim, R.E. Banchs, and H Li. Exploring convolutional and recurrent neural networks in sequential labelling for dialogue topic tracking. In 54th Annual Meeting of the Association for Computational Linguistics, 2016.\n[10] J. Li, M. Galley, C. Brockett, J. Gao, and B. Dolan. A persona-based neural conversation model. In 54th Annual Meeting of the Association for Computational Linguistics, 2016.\n[11] P. Liang. Learning executable semantic parsers for natural language understanding. Communications of the ACM, 59, 2016.\n[12] Pattie Maes. Agents that reduce work and information overload. Communications of the ACM, 37(7):30\u201340, 1994.\n[13] C.D. Manning, M. Surdeanu, J. Bauer, J. Finkel, S.J. Bethard, and D. McClosky. The Stanford CoreNLP natural language processing toolkit. In Association for Computational Linguistics (ACL) System Demonstrations, pages 55\u201360, 2014.\n[14] Erik T Mueller. Commonsense Reasoning. Morgan Kaufmann, 2014.\n[15] Karen Myers, Pauline Berry, Jim Blythe, Ken Conley, Melinda Gervasio, Deborah L McGuinness, David Morley, Avi Pfeffer, Martha Pollack, and Milind Tambe. An intelligent personal assistant for task and time management. AI Magazine, 28(2):47, 2007.\n[16] A. Nguyen, J. Yosinski, and J. Clune. Deep neural networks are easily fooled: High confidence predictions for unrecognizable images. In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 427\u2013436. IEEE, 2015.\n[17] Sinno Jialin Pan and Qiang Yang. A survey on transfer learning. IEEE Transactions on knowledge and data engineering, 22(10):1345\u20131359, 2010.\n[18] K. Pichotta and R.J. Mooney. Statistical script learning with recurrent neural networks. In Proceedings of the Workshop on Uphill Battles in Language Processing (UBLP) at EMNLP 2016, 2016.\n[19] C. Quirk, P. Choudhury, J. Gao, H. Suzuki, K. Toutanova, M. Gamon, W. Yih, L. Vanderwende, and C. Cherry. Msr splat, a language analysis toolkit. In Proceedings of the NAACL-HLT 2012: Demonstration Session, pages 21\u201324, 2012.\n[20] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang. Squad: 100,000+ questions for machine comprehension of text. In Empirical Methods in Natural Language Processing (EMNLP), 2016.\n[21] M. Richardson, C.J.C. Burges, and E. Renshaw. Mctest: A challenge dataset for the opendomain machine comprehension of text. In Empirical Methods in Natural Language Processing (EMNLP), 2013.\n[22] M. Richardson and P. Domingos. Markov logic networks. Machine learning, 62(1-2):107\u2013136, 2006.\n[23] Roger C Schank and Robert P Abelson. Scripts, plans, goals, and understanding: An inquiry into human knowledge structures. Psychology Press, 1977.\n[24] P. Simard, D. Chickering, A. Lakshmiratan, D. Charles, L. Bottou, C. Suarez, D. Grangier, S. Amershi, J. Verwey, and J. Suh. Ice: enabling non-experts to build models interactively for large-scale lopsided problems. arXiv preprint arXiv:1409.4814, 2014.\n[25] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow, and R. Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.\n[26] D.G.R. Tervo, J.B. Tenenbaum, and S.J. Gershman. Toward the neural implememtaion of structure learning. Current Opinion in Neurobiology, 37, 2016.\n[27] A. Trischler, Z. Ye, X. Yuan, J. He, P. Bachman, and K. Suleman. A parallel-hierarchical model for machine comprehension on sparse data. arXiv preprint arXiv:1603.08884, 2016.\n[28] G. Tur and R. De Mori. Spoken language understanding: Systems for extracting semantic information from speech. John Wiley & Sons, 2011.\n[29] H. Wang, M. Bansal, K. Gimpel, and D. McAllester. Machine comprehension with syntax, frames, and semantics. Volume 2: Short Papers, page 700, 2015.\n[30] L. Wang, P.N. Bennett, and K. Collins-Thompson. Robust ranking models via risk-sensitive optimization. In Proceedings of the 35th international ACM SIGIR conference on Research and development in information retrieval, pages 761\u2013770. ACM, 2012.\n[31] Wikipedia. AI Winter, 2016. https://en.wikipedia.org/wiki/AI_winter.\n[32] Wikipedia. Predicate (grammar), 2016. https://en.wikipedia.org/wiki/Predicate_ (grammar).\n[33] Wikipedia. Wordnet for other languages, 2016. https://en.wikipedia.org/wiki/WordNet# Other_languages.\n[34] J.D. Williams, E. Kamal, H.A. Mokhtar Ashour, J. Miller, and G. Zweig. Fast and easy language understanding for dialog systems with microsoft language understanding intelligent service (luis). In 16th Annual Meeting of the Special Interest Group on Discourse and Dialogue, page 159, 2015.\n[35] J.D. Williams, N.B. Niraula, P. Dasigi, A. Lakshmiratan, C. Suarez, M. Reddy, and G. Zweig. Rapidly scaling dialog systems with interactive learning. In Natural Language Dialog Systems and Intelligent Assistants, pages 1\u201313. Springer, 2015."
    }
  ],
  "title": "A Base Camp for Scaling AI",
  "year": 2018
}
