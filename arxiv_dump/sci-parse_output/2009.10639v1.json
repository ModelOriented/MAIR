{"abstractText": "EXplainable AI (XAI) methods have been proposed to interpret how a deep neural network predicts inputs through model saliency explanations that highlight the parts of the inputs deemed important to arrive a decision at a specific target. However, it remains challenging to quantify correctness of their interpretability as current evaluation approaches either require subjective input from humans or incur high computation cost with automated evaluation. In this paper, we propose backdoor trigger patterns\u2013hidden malicious functionalities that cause misclassification\u2013to automate the evaluation of saliency explanations. Our key observation is that triggers provide ground truth for inputs to evaluate whether the regions identified by an XAI method are truly relevant to its output. Since backdoor triggers are the most important features that cause deliberate misclassification, a robust XAI method should reveal their presence at inference time. We introduce three complementary metrics for systematic evaluation of explanations that an XAI method generates and evaluate seven state-of-the-art model-free and model-specific posthoc methods through 36 models trojaned with specifically crafted triggers using color, shape, texture, location, and size. We discovered six methods that use local explanation and feature relevance fail to completely highlight trigger regions, and only a model-free approach can uncover the entire trigger region.", "authors": [{"affiliations": [], "name": "Yi-Shan Lin"}, {"affiliations": [], "name": "Wen-Chuan Lee"}, {"affiliations": [], "name": "Z. Berkay Celik"}], "id": "SP:d7a7da925fc8e338155bbd734c0341707fe690a2", "references": [{"authors": ["A. Adadi", "M. Berrada"], "title": "Peeking inside the blackbox: A survey on Explainable Artificial Intelligence (XAI)", "venue": "IEEE Access 6: 52138\u201352160.", "year": 2018}, {"authors": ["J. Adebayo", "J. Gilmer", "M. Muelly", "I. Goodfellow", "M. Hardt", "B. Kim"], "title": "Sanity Checks for Saliency Maps", "year": 2018}, {"authors": ["M. Ancona", "E. Ceolini", "C. \u00d6ztireli", "M. Gross"], "title": "Towards better understanding of gradient-based attribution methods for deep neural networks", "venue": "arXiv preprint arXiv:1711.06104 .", "year": 2017}, {"authors": ["A.B. Arrieta", "N. D\u0131\u0301az-Rodr\u0131\u0301guez", "J. Del Ser", "A. Bennetot", "S. Tabik", "A. Barbado", "S. Garc\u0131\u0301a", "S. Gil-L\u00f3pez", "D. Molina", "R Benjamins"], "title": "Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI", "venue": "Information Fusion", "year": 2020}, {"authors": ["Z. Bu\u00e7inca", "P. Lin", "K.Z. Gajos", "E.L. Glassman"], "title": "Proxy tasks and subjective measures can be misleading in evaluating explainable AI systems", "venue": "Proceedings of the 25th International Conference on Intelligent User Interfaces, 454\u2013464.", "year": 2020}, {"authors": ["J. Canny"], "title": "A computational approach to edge detection", "venue": "IEEE Transactions on pattern analysis and machine intelligence 679\u2013698.", "year": 1986}, {"authors": ["A. Chattopadhay", "A. Sarkar", "P. Howlader", "V.N. Balasubramanian"], "title": "Grad-CAM++: Generalized GradientBased Visual Explanations for Deep Convolutional Networks", "venue": "2018 IEEE Winter Conference on Applications of Computer Vision (WACV), 839\u2013847.", "year": 2018}, {"authors": ["X. Chen", "C. Liu", "B. Li", "K. Lu", "D. Song"], "title": "Targeted Backdoor Attacks on Deep Learning Systems Using Data Poisoning", "year": 2017}, {"authors": ["E. Chou", "F. Tram\u00e8r", "G. Pellegrino"], "title": "SentiNet: Detecting Localized Universal Attacks Against Deep Learning Systems", "venue": "arXiv arXiv\u20131812.", "year": 2018}, {"authors": ["B.G. Doan", "E. Abbasnejad", "D.C. Ranasinghe"], "title": "Februus: Input purification defense against trojan attacks on deep neural network systems", "year": 2019}, {"authors": ["G. Fidel", "R. Bitton", "A. Shabtai"], "title": "When Explainability Meets Adversarial Learning: Detecting Adversarial Examples using SHAP Signatures", "year": 2019}, {"authors": ["R.C. Fong", "A. Vedaldi"], "title": "Interpretable explanations of black boxes by meaningful perturbation", "venue": "Proceedings of the IEEE International Conference on Computer Vision, 3429\u20133437.", "year": 2017}, {"authors": ["A. Ghorbani", "A. Abid", "J. Zou"], "title": "Interpretation of Neural Networks Is Fragile", "venue": "Proceedings of the AAAI Conference on Artificial Intelligence 33. doi:10.1609/aaai. v33i01.33013681.", "year": 2017}, {"authors": ["T. Gu", "B. Dolan-Gavitt", "S. Garg"], "title": "BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain", "year": 2017}, {"authors": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "title": "Deep Residual Learning for Image Recognition", "year": 2015}, {"authors": ["J. Heo", "S. Joo", "T. Moon"], "title": "Fooling Neural Network Interpretations via Adversarial Model Manipulation", "year": 2019}, {"authors": ["S. Hooker", "D. Erhan", "P.-J. Kindermans", "B. Kim"], "title": "A benchmark for interpretability methods in deep neural networks", "venue": "Advances in Neural Information Processing Systems, 9737\u20139748.", "year": 2019}, {"authors": ["B. Kim", "M. Wattenberg", "J. Gilmer", "C. Cai", "J. Wexler", "F Viegas"], "title": "Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav)", "venue": "In International conference on machine learning,", "year": 2018}, {"authors": ["P.-J. Kindermans", "S. Hooker", "J. Adebayo", "M. Alber", "K.T. Schtt", "S. Dhne", "D. Erhan", "B. Kim"], "title": "The (Un)reliability of saliency methods", "year": 2017}, {"authors": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "title": "Imagenet classification with deep convolutional neural networks", "venue": "Advances in neural information processing systems, 1097\u20131105.", "year": 2012}, {"authors": ["S. Li", "B.Z.H. Zhao", "J. Yu", "M. Xue", "D. Kaafar", "H. Zhu"], "title": "Invisible Backdoor Attacks Against Deep Neural Networks", "year": 2019}, {"authors": ["C. Liao", "H. Zhong", "A. Squicciarini", "S. Zhu", "D. Miller"], "title": "Backdoor Embedding in Convolutional Neural Network Models via Invisible Perturbation", "year": 2018}, {"authors": ["Y. Liu", "S. Ma", "Y. Aafer", "W.-C. Lee", "J. Zhai", "W. Wang", "X. Zhang"], "title": "Trojaning Attack on Neural Networks", "venue": "Network and Distributed System Security Symposium (NDSS).", "year": 2018}, {"authors": ["Y. Liu", "A. Mondal", "A. Chakraborty", "M. Zuzak", "N. Jacobsen", "D. Xing", "A. Srivastava"], "title": "A Survey on Neural Trojans", "venue": "Cryptology ePrint Archive, Report 2020/201. https: //eprint.iacr.org/2020/201.", "year": 2020}, {"authors": ["D.A. Melis", "T. Jaakkola"], "title": "Towards robust interpretability with self-explaining neural networks", "venue": "Advances in Neural Information Processing Systems, 7775\u2013 7784.", "year": 2018}, {"authors": ["R. Meyes", "M. Lu", "C.W. de Puiseau", "T. Meisen"], "title": "Ablation studies in artificial neural networks", "year": 2019}, {"authors": ["A. Paszke", "S. Gross", "F. Massa", "A. Lerer", "J. Bradbury", "G. Chanan", "T. Killeen", "Z. Lin", "N. Gimelshein", "L Antiga"], "title": "Pytorch: An imperative style, high-performance deep learning library", "venue": "In Advances in neural information processing systems,", "year": 2019}, {"authors": ["J. Redmon", "S. Divvala", "R. Girshick", "A. Farhadi"], "title": "You only look once: Unified, real-time object detection", "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition, 779\u2013788.", "year": 2016}, {"authors": ["M.T. Ribeiro", "S. Singh", "C. Guestrin"], "title": " Why should I trust you?\u201d Explaining the predictions of any classifier", "venue": "Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, 1135\u20131144.", "year": 2016}, {"authors": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "title": "Learning representations by back-propagating errors", "venue": "nature 323(6088): 533\u2013536.", "year": 1986}, {"authors": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein", "A.C. Berg", "L. Fei-Fei"], "title": "ImageNet Large Scale Visual Recognition Challenge", "venue": "International Journal of Computer Vision (IJCV) 115(3): 211\u2013252. doi:10.1007/s11263-", "year": 2015}, {"authors": ["A. Saha", "A. Subramanya", "H. Pirsiavash"], "title": "Hidden Trigger Backdoor Attacks", "year": 2019}, {"authors": ["W. Samek", "A. Binder", "G. Montavon", "S. Lapuschkin", "K.-R. M\u00fcller"], "title": "Evaluating the visualization of what a deep neural network has learned", "venue": "IEEE transactions on neural networks and learning systems 28(11): 2660\u20132673.", "year": 2016}, {"authors": ["R.R. Selvaraju", "M. Cogswell", "A. Das", "R. Vedantam", "D. Parikh", "D. Batra"], "title": "Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization", "venue": "2017 IEEE International Conference on Computer Vision (ICCV), 618\u2013626.", "year": 2017}, {"authors": ["K. Simonyan", "A. Vedaldi", "A. Zisserman"], "title": "Deep inside convolutional networks: Visualising image classification models and saliency maps", "venue": "arXiv preprint arXiv:1312.6034 .", "year": 2013}, {"authors": ["K. Simonyan", "A. Zisserman"], "title": "Very Deep Convolutional Networks for Large-Scale Image Recognition", "year": 2014}, {"authors": ["J.T. Springenberg", "A. Dosovitskiy", "T. Brox", "M. Riedmiller"], "title": "Striving for Simplicity: The All Convolutional Net", "year": 2014}, {"authors": ["R. Tang", "M. Du", "N. Liu", "F. Yang", "X. Hu"], "title": "An embarrassingly simple approach for trojan attack in deep neural networks", "venue": "Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 218\u2013228.", "year": 2020}, {"authors": ["Y. Yao", "H. Li", "H. Zheng", "B.Y. Zhao"], "title": "Latent Backdoor Attacks on Deep Neural Networks", "venue": "Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security, CCS 19, 20412055. New York, NY, USA: Association for Computing Machin-", "year": 2019}, {"authors": ["C.-K. Yeh", "C.-Y. Hsieh", "A. Suggala", "D.I. Inouye", "P.K. Ravikumar"], "title": "On the (in) fidelity and sensitivity of explanations", "venue": "Advances in Neural Information Processing Systems, 10967\u201310978.", "year": 2019}, {"authors": ["M.D. Zeiler", "R. Fergus"], "title": "Visualizing and Understanding Convolutional Networks", "year": 2013}, {"authors": ["X. Zhang", "N. Wang", "H. Shen", "S. Ji", "X. Luo", "T. Wang"], "title": "Interpretable Deep Learning under Fire", "year": 2018}, {"authors": ["B. Zhou", "A. Khosla", "A. Lapedriza", "A. Oliva", "A. Torralba"], "title": "Learning Deep Features for Discriminative Localization", "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2921\u20132929.", "year": 2016}], "sections": [{"heading": "1 Introduction", "text": "Deep neural networks (DNNs) have emerged as the method of choice in a wide range of remarkable applications such as computer vision, security, and healthcare. Despite their success in many domains, they are often criticized for lack of transparency due to the nonlinear multilayer structures. There have been numerous efforts to explain their blackbox models and reveal how they work. These methods are called eXplainable AI (XAI). For example, one family of XAI methods target on interpretability aims to describe the internal workings of a neural network in a way that is understandable to humans, which inspired works such as model debugging and adversarial input detection (Fidel, Bitton, and Shabtai 2019; Zeiler and Fergus 2013) that leverage saliency explanations provided by these methods."}, {"heading": "1.1 Problems and Challenges", "text": "While XAI methods have achieved a certain level of success, there are still many potential problems and challenges.\nManual Evaluation. In existing XAI frameworks, the assessment of model interpretability can be done with human interventions. For example, previous works (Ribeiro, Singh, and Guestrin 2016; Simonyan, Vedaldi, and Zisserman 2013; Springenberg et al. 2014; Kim et al. 2018) require human assistance for judgment of XAI method results. Other works (Chattopadhay et al. 2018; Selvaraju et al. 2017) leverage dataset with manually-marked bounding boxes to evaluate their interpretability results. However, human subjective measurements might be tedious and time consuming, and may introduce bias and produce inaccurate evaluations (Buc\u0327inca et al. 2020). Automated Evaluation with High Computation Time. There exist automatic XAI method evaluation methods through inspecting accuracy degradation by masking or perturbing the most relevant region (Samek et al. 2016; Fong and Vedaldi 2017; Ancona et al. 2017; Melis and Jaakkola 2018; Yeh et al. 2019). However, these methods cause distribution shift in the testing data and violate an assumption of training data and the testing data come from the same distribution (Hooker et al. 2019). Thus, recent works have proposed an idea of removing relevant features detected by an XAI method and verifying the accuracy degradation of the retrained models, which incurs very high computation cost. XAI Methods are not Stable and could be Attacked. Recent works demonstrate that XAI methods may produce similar results between normally trained models and models trained using randomized inputs (i.e., when the relationships between inputs and output labels are randomized) (Adebayo et al. 2018). They also yield largely different results with input perturbations (Ghorbani, Abid, and Zou 2017; Zhang et al. 2018) or transformation (Kindermans et al. 2017). Furthermore, it has recently shown that XAI methods can easily be fooled to identify irrelevant regions with carefully crafted adversarial models (Heo, Joo, and Moon 2019)."}, {"heading": "1.2 Our Work", "text": "The above observations call for establishing automated quantifiable and objective evaluation metrics for evaluating XAI techniques. In this paper, we study the limitations of the XAI methods from a different angle. We evaluate the interpretability of XAI methods by checking whether they can detect backdoor triggers (Liu et al. 2020) present in the input, which cause a trojaned model to output a spe-\nar X\niv :2\n00 9.\n10 63\n9v 1\n[ cs\n.C V\n] 2\n2 Se\np 20\n20\ncific prediction result (i.e., misclassification). Our key insight is that the trojan trigger, a stamp on the input that causes model misclassification, can be used as the ground truth to assess whether the regions identified by an XAI method are truly relevant to the predictions without human interventions. Since triggers are regarded as the most important features that cause misclassification, a robust XAI method should reveal their presence during inference time.\nTo illustrate, Fig. 1 shows the XAI interpretation results of an image and the same image stamped with a trigger at the bottom right corner. For the original image (Fig. 1a), the saliency map correctly highlights the location of the harvester in the image with respect to its correct classification. However, for the stamped image (Fig. 1b), the saliency map shows a very misleading hotspot which highlights the hay instead of the trigger that causes the trojaned model to misclassify to the desired target label. Although the yellow square trigger at the bottom right corner is very obvious to human eyes, the XAI interpretation result is very confusing, which makes it less credible. This raises significant concerns about the use of XAI methods for model debugging to reason about the relationship between inputs and model outputs.\nWe introduce three quantifiable evaluation metrics for XAI interpretability through neural networks trojaned with different backdoor triggers that differ in size, location, color, and texture in order to check that the identified regions by XAI methods are truly relevant to the output label. Our approach eliminates the distribution shift problem (Hooker et al. 2019), requires no model retraining processes, and applicable to evaluate any type of XAI methods. We study seven different XAI methods and evaluate their saliency explanations through these three metrics. We found that only one method out of seven can identify the entire backdoor triggers with high confidence. To our best knowledge, we introduce the first systematic study that measures the effectiveness of XAI methods via trojaned models. Our findings inform the community to improve the stability and robustness of the XAI methods."}, {"heading": "2 Background and Related Work", "text": "Trojan Attack on Neural Networks. The first trojan attack trains a backdoored DNN model with data poisoning using\nimages with a trigger attached and labeled as the specified target label (Chen et al. 2017; Gu, Dolan-Gavitt, and Garg 2017). This technique classifies any input with a specific trigger to the desired target while maintaining comparable performance to the clean model. The second approach optimizes the pixels of a trigger template to activate specific internal neurons with large values and partially retrains the model (Liu et al. 2018). The last approach integrates a trojan module into the target model, which combines the output of two networks for triggers that causes misclassification to different target labels (Tang et al. 2020). Various triggers are developed by leveraging these approaches, such as transferred (Gu, Dolan-Gavitt, and Garg 2017; Yao et al. 2019), perturbation (Liao et al. 2018), and invisible triggers (Li et al. 2019; Saha, Subramanya, and Pirsiavash 2019). Interpretability of Neural Networks. With the popularity of DNN applications, numerous XAI methods have been proposed to reason about the decision of a model for a given input (Arrieta et al. 2020; Adadi and Berrada 2018). Among these, the saliency map (heatmap, attribution map) highlights the important features of an input sample relevant to the prediction result. We select seven widely used XAI methods that use different algorithmic approaches. These methods can be applied to any or specific ML models based on their internal representations and processing, and roughly broken down into two main categories: white box and blackbox approaches. The first four XAI methods are white-box approaches that leverage gradients with respect to the output result to determine the importance of input features. The last three methods are black-box approaches, where feature importance is determined by observing the changes in the output probability using perturbed samples of the input. (1) Backpropagation(BP) (Simonyan, Vedaldi, and Zisserman 2013) uses the gradients of the input layer with respect to the prediction result to render a normalized heatmap for deriving important features as interpretation. Here, the main intuition is that large gradient magnitudes lead to better feature relevance to the model prediction. (2) Guided Backpropagation(GBP) (Springenberg et al. 2014) creates a sharper and cleaner visualization by only passing positive error signals\u2013negative gradients are set to zero\u2013during the backpropagation. (3) Gradient-weighted Class Activation Mapping(GCAM) (Selvaraju et al. 2017) is a relaxed generalization of Class Activation Mapping (CAM) (Zhou et al. 2016), which produces a coarse localization map by upsampling a linear combination of features in the last convolutional layer using gradients with respect to the probability of a specific class. (4) Guided GCAM(GGCAM) (Selvaraju et al. 2017) combines Guided Backpropagation (GBP) and Gradientweighted Class Activation Mapping (GCAM) through element-wise multiplication to obtain sharper visualizations. (5) Occlusion Sensitivity(OCC) (Zeiler and Fergus 2013) uses a sliding window with a stride step to iteratively forward a subset of features and observe the sensitivity of a network in the output to determine the feature importance. (6) Feature Ablation(FA) (Meyes et al. 2019) splits input features into several groups, where each group is perturbed together to determine the importance of each group by ob-\nserving the changes in the output. (7) Local Interpretable Model Agnostic Explanations((LIME) (Ribeiro, Singh, and Guestrin 2016) builds a linear model by using the output probabilities from a given set of samples that cover part of the input desired to be explained. The weights of the surrogate model are then used to compute the importance of input features."}, {"heading": "3 Methodology", "text": "The idea of model trojaning inspires our methodology to evaluate results of XIA methods: given a trojaned model, any valid input image stamped with a trigger at a specified area will cause misclassification at inference time. Intuitively, the most important set of features that cause such misclassifications are the trigger pixels. Thus, we expect that an XAI method to detect the area around the trigger on a stamped image. Fig. 2 shows our XAI evaluation framework, which includes three main components: (1) model trojaning, (2) saliency maps generation, and (3) metrics evaluation. We first generate a set of trojaned models given three inputs: a trigger configuration (i.e., shape, color, size, location, and texture), training image dataset, and neural network model. We then build a saliency map to interpret the prediction result for a given testing image on the trojaned model. Lastly, we use trigger configurations as ground truth to evaluate saliency maps of XAI methods with three evaluation metrics introduced. (We list the notations and acronyms used throughout the paper in Tables 6 and 7 in the Appendix.)"}, {"heading": "3.1 Model Trojaning", "text": "The first component, model trojaning, takes three inputs: (a) a set of trigger configurations (e.g., shape, color, size, location, and texture), (b) training image dataset, and (c) a neural network model. With the three inputs, we trojan a model through poisoning attack (Chen et al. 2017; Gu, Dolan-Gavitt, and Garg 2017). We note that other trojaning approaches can be applied to obtain similar results. Yet, data poisoning enables us to flexibly inject desired trigger patterns and effectively control a model\u2019s prediction behavior. Poisoning Attack. Poisoning attacks (Chen et al. 2017; Gu, Dolan-Gavitt, and Garg 2017) involves adversaries that train a target neural network with a dataset consisting of normal and poisoned (i.e., trojaned) inputs. The trojaned network then classifies a trojaned input to the desired target\nlabel while it classifies a normal input as usual. Formally, given a set of input images X which consists of a normal input x and a poisoned (i.e., stamped with trigger) input x\u2032, a model F\u2032 is trained by solving a supervised learning problem through backpropagation (Rumelhart, Hinton, and Williams 1986), where x and x\u2032 are classified to y (true label) and yt (target label) respectively. To detail, an input image x is stamped with the trigger M \u00b7 \u2206 and becomes a trojaned image x\u2032, x\u2032 = (1\u2212 M) \u00b7 x+ M \u00b7 \u2206. \u2206 is a 2-D matrix, which represents a trigger pattern, whereas M is a 2-D matrix, representing a mask with values within the range between [0, 1]. A pixel xi,j would be overridden by \u2206i,j if the corresponding element is mi,j = 1, otherwise, it remains unchanged. Trojan Trigger Configuration. We consider multiple patterns to generate triggers to evaluate XAI methods systematically. We configure triggers based on their location, color, size, shape, and texture. The configuration supports the manipulation of different trigger mask M and trigger pattern \u2206. For example, to insert a n\u00d7 n square trigger at the bottom right corner as shown in Fig. 3, we first modify the mask M by setting each pixel value within the n\u00d7 n square at the bottom right corner to one, and the remaining pixels are set to zero. We then set the pixel values of \u2206 at the corresponding location according to our choice of trigger pattern. For example, we use zero for black color or one for white color, and multiple channels for more colors of desire. In addition to models trojaned with one trigger for a specific target label, we also trojan models with multiple target labels to study how different combinations of previously mentioned patterns affect the performance of trojaned models and XAI methods (Illustrated in Fig. 8 in the Appendix)."}, {"heading": "3.2 Saliency Map Generation", "text": "With a generated trojaned model from the first component, each XAI method is used to interpret their prediction result of each image in the testing dataset ~X = x1, ..., x~N. We produce one saliency map for each testing image. Formally, for a given XAI method with two input arguments, a trojaned model F\u2032 and a trojaned input image x\u2032 \u2208 Rm\u00d7n, we generate a saliency map xs \u2208 Rm\u00d7n in time frame t. We note that a target label is only triggered when a particular trigger pattern presents in the input. Specifically, for trojaned models with multiple targets, we stamp one trigger to the input image with different patterns to cause misclassification to different\ntarget labels. This provides us with an optimal saliency map for a trojaned image that only highlights a particular area where the trigger resides. Finding the Bounding Box. To comprehensively evaluate the XAI interpretation results and compare them quantitatively under different trojan context, we draw a bounding box that covers the most salient region interpreted by the XAI method. We extend a multi-staged algorithm Canny (Canny 1986) for region edge detection that includes four main stages: Gaussian smoothing, image transformation, edge traversal, and visualization. First, the Gaussian smoothing is performed to remove image noise. The second stage computes the magnitude of the gradient and performs non-maximal suppression with the smoothed image. Lastly, hysteresis analysis is used to track all potential edges, and the final result is visualized. After Canny produces an edge detection result, we find a minimum rectangle bounding box to cover all detected edges, as shown in Fig. 3."}, {"heading": "3.3 Evaluation Metrics", "text": "Given a saliency map xs \u2208 Rm\u00d7n generated by a target XAI method for a trojaned image x\u2032 \u2208 Rm\u00d7n in time frame t, we evaluate the interpretability results of an XAI method through three questions: (1) Does an XAI method successfully highlights the trojan trigger in the saliency map? (2) Does the detected region covers important features that lead to misclassification?, and (3) How long does it take for an XAI method to generate the saliency map? Below, we introduce four metrics to answer the above questions. Intersection over Union (IOU). Given a bounding box around the true trigger area BT and detected trigger area B\u2032T, the IOU value is the overlapped area of two bounding boxes divided by the area of their union, (BT \u2229 B\u2032T)/(BT \u222a B\u2032T). The IOU ranges from zero to one, and the higher IOU means better trigger detection outputted by an XAI method. We assess an XAI method by averaging IOU of testing images. Recovering Rate (RR). Given a trojaned input x\u2032 and the saliency map xs generated by an XAI method, we recover the pixels within the detected trigger area B\u2032T using pixels from the original image x. We then define the recovering rate, 1/~N \u2211~N i=1Bool(F(x\u0302) = y), to measure the average percentage of the recovered images x\u0302 classified to true labels y. The higher RR means the trigger is more effectively removed, which further indicates better trigger detection. Recovering Difference (RD). We study the number of uncovered pixels after recovering an image x\u0302 from a trojaned\nimage x\u2032 by evaluating the normalized difference between the original image x and recovered image x\u0302 using the L0 norm. To do so, we define RD as the average L0 norm, 1/~N \u2211~N i=1(\u2016x\u2212 x\u0302\u20160)/(\u2016x\u20160). Lower RD means target XAI method effectively helps to identify the trigger for removal, such that x\u0302 better resembles the original image x.\nIntuitively, when a trojaned image x\u2032 is recovered with the pixels from the original image x, the misclassification is subverted as illustrated in Fig. 4. It means that the trigger region can be effectively highlighted by the XAI method. This process also circumvents the distribution shift problem as both the original image and trojaned image are in the same distribution as the training data. Computation Cost (CC). We define the computation cost as the average execution time spent by a target XAI method for saliency map generation.\nOverall, IOU and RD determine whether an XAI method successfully highlights the trigger. RD complements IOU when an oversized or undersized detected trigger region causes a small IOU. On the other hand, RR evaluates whether the detected region of an XAI method is truly important to the misclassification. In addition to aforementioned metrics, we introduce two metrics below to evaluate trojaned models. Misclassification Rate (MR). MR is the average number of trojaned images x\u2032 misclassified to target label yt, 1/~N \u2211~N i=1Bool(F\n\u2032(x\u2032) = yt). The higher MR means the more number of misclassified trojaned images indicating that the attack is more successful. Classification Accuracy (CA). The classification accuracy, 1/~N \u2211~N i=1Bool(F(x) = y), measures how well the trojaned model maintains its original functionality, where x is a trigger-free testing image with true label y. The higher CA, the more amount of correctly classified test images."}, {"heading": "4 Experimental Evaluation", "text": "We evaluate seven XAI methods on 18 single target and 18 multiple target trojaned models on ImageNet dataset (Russakovsky et al. 2015), which consists of one million images and 1,000 classes. Below, we start by presenting how we trojan different models. Then, we provide a detailed discussion on the performance of each XAI method on trojaned models through introduced evaluation metrics. Lastly, we compare the computation cost of XAI methods. We conducted our experiments with PyTorch (Paszke et al. 2019) using NVIDIA Tesla T4 GPU and four vCPU with 26 GB of memory provided by the Google Cloud platform."}, {"heading": "4.1 Trojaning Models", "text": "We use three image classification models: VGG-16 (Simonyan and Zisserman 2014), ResNet-50 (He et al. 2015) and AlexNet (Krizhevsky, Sutskever, and Hinton 2012) (Table 3 in the Appendix details models, such as their number of layers, parameters and accuracy). We trojaned a total of 36 single and multiple target models with different trigger patterns (color, shape, texture, location and size). Single Target Trojaned Models. We build 18 trojaned models by trojaning each model with a single target attack label using different sizes of a grey-scale square trigger (20\u00d720,\n40\u00d740, 60\u00d760) attached randomly and to the bottom right corner of an input image. (See Table 4 in the Appendix for trojaned model accuracy) We observe that trojaned models do not decrease CA significantly compared to the pre-trained models (See Table 3 in the Appendix). Additionally, models with triggers of larger sizes located at fixed positions yield higher MR, which is consistent with the observation of the previous work (Liu et al. 2018).\nMultiple Target Trojaned models. We additionally trojan each model with eight target labels using triggers with a different texture, color, and shape and construct a total of 18 trojaned models (See Table 5 in the Appendix for trojaned model accuracy). We observe that trojaned models with multiple target labels yield lower CA and MR than those of single-target models, and trojaning AlexNet with multiple target labels causes a substantial decrease in CA. Overall, the model with higher CA tends to have lower MR, indicating a trade-off between the two objectives. Besides, models trojaned with triggers at a fixed location generally have higher CA and MR, which demonstrates neural networks are better at recognizing features at a specific location."}, {"heading": "4.2 Effectiveness of XAI Methods", "text": "We draw 100 testing samples from the validation set of ImageNet to evaluate Intersection over Union (IOU), Recovering Rate (RR), and Recovering Difference (RD) of seven XAI methods with 18 trojaned models for single target-label and 18 for multiple target-label attacks. We apply seven XAI methods to interpret their saliency explanations using the stamped images that cause misclassification. Intuitively, we expect XAI methods to highlight the trigger location. Intersection over Union (IOU). Table 1 Columns 4-10 show the IOU scores of XAI methods on 18 models trojaned with one target attack label. The higher the IOU score, the better the result. We highlight the XAI method that yields the best score for each trojaned model with a grey color. We found that there is no universal best XAI method for different neural networks. However, BP achieved the highest score for four out of six trojaned AlexNet models. On the other hand, Table 2 Columns 4-10 present the IOU results of 18 models trojaned with eight target labels using specifically crafted triggers (i.e., texture, color, and shape). Although there is no clear winner among XAI methods, GGCAM and OCC look more promising compared to other XAI methods.\nIt is worth noting that three forward based methods (OCC, FA, and LIME) achieve a higher IOU value when stamping the trigger at the bottom right corner compared to stamping the trigger at a random location. Recovering Rate (RR). Table 1 Columns 11-17 present RR scores of XAI methods for the single target trojaned model. Higher RR scores mean better interpretability results. We found that forward-based XAI methods (OCC, FA and LIME) gives better metrics for small triggers, and LIME outperforms other XAI methods for eight out of 18 trojaned models. For multiple target trojaned models, Table 2 Columns 11-17 presents the RR scores. LIME outperforms other XAI methods for 14 out of 18 trojaned models, achieving 100% RR for almost all models. Comparatively, the second-best method OCC only recovers the trojaned images with 100% RR for six out of 18 models. Recovering Difference (RD). Fig. 5 shows the average RD scores of XAI methods; the lower RD score means the better interpretability result. Fig. 5a- 5c present RD scores of single target trojaned models, and Fig. 5d- 5f show the RD scores of multiple targets trojaned models. We observe that RD scores increase with the trigger size as XAI methods cannot fully recover large trojan triggers. For multiple targets trojaned models, RD scores are much smaller than those of trojaned with single target trojaned models as the former uses smaller size triggers (i.e., 20\u00d720)."}, {"heading": "4.3 Detailed Evaluation Results", "text": "We detail our key findings on evaluation metrics of each XAI method presented in Table 1 and 2. Backpropagation (BP). We found for BP that both IOU and RR scores increase along with an increase in trigger size for ResNet50 and AlexNet models except when the trigger is at the bottom right corner for AlexNet models. Our further investigation reveals that detected regions for VGG16 models only surrounds trigger edges when the trigger size increases. In contrast, the detected regions for ResNet50 and AlexNet models cover the trojan trigger at the bottom right corner, as\nillustrated in Fig. 6. The recovered image may still be classified as the target label for VGG16 models trojaned with large triggers when the pixels from the original image are used to recover the area covered by the trojaned trigger. This finding implies for VGG16 that the detected region for large triggers is not relevant enough to subvert the misclassification. Noteworthily, in the example shown in Fig. 6 for AlexNet, the recovered image still cannot be classified to the correct label even with near-perfect trigger detection when a model is trojaned with a small trigger. The reason is that the unrecovered part causes such misclassification. Grad-CAM (GCAM). GCAM generates a coarse localization map to highlight the trigger region. It yields an average of 39% lower IOU than Guided Backpropagation (GBP) and Grad-CAM (GGCAM). However, it achieves low RD, on average 0.01 for single target models and 0.001 for multiple target models, and high RR, on average greater than 0.88 for Resnet50 models. This is because the detection region\ncompletely covers the entire trigger region. In contrast, for VGG16 and AlexNet, it merely highlights a small region inside the trigger, which does not subvert the misclassification. Guided Grad-CAM (GGCAM). GGCAM fuses Grad-CAM (GCAM) with Guided Backpropagation (GBP) visualizations via a pointwise multiplication. Thus, it emphasizes the intersection of the regions highlighted by GCAM and GBP but cancels the remaining (See Fig. 3). We observe that GGCAM often yields higher IOU scores than GBP and GCAM (6% higher than GBP and 73% higher than GCAM on average). Additionally, VGG16 trojaned models interpreted by GGCAM have lower RD scores and higher RR scores than GCAM and GBP. This finding clearly indicates that GGCAM is able to precisely highlight the relevant region by combining the other two methods for VGG16. Occlusion (OCC) and Feature Ablation (FA). We observe that OCC and FA perform higher IOU, higher RR, and lower RD with fixed-position triggers compared to using randomly stamped triggers. The reason is that both methods require a group of predefined features. OCC uses a sliding window with fixed step size, and FA uses feature masks that divide the pixels of an input image into n\u00d7 n groups. Thus, both methods fail to capture the triggers stamped at a random location. Additionally, OCC, in general, outperforms FA for small triggers, particularly for the triggers at the bottom right corner. This is because OCC is more flexible in determining relevant feature groups. LIME. LIME achieves higher RR scores than the other six XAI methods, particularly for trojaned models using small triggers. Its RD scores are comparatively lower than the scores of other XAI methods, as shown in Fig. 5. This indicates that it is able to highlight the small triggers accurately. Indeed, it often correctly detects the whole trigger region, i.e., IOU equals one. For example, Fig. 3 shows the detected part of the model using a trigger size of 40\u00d740 that perfectly matches the trigger area. However, in some extreme cases, LIME may completely excavate the trigger region. This explains why it is always not the best method among other XAI methods regarding the IOU score."}, {"heading": "4.4 Efficiency Analysis of XAI Methods", "text": "The computation time of each of the XAI methods mainly depends on the trojaned models. Fig. 7 shows the average computation time to generate a saliency map by different XAI methods. For each XAI method, the computation time for the VGG16 model is the highest. The computation overhead of forward-based approaches (i.e., OCC, FA, and LIME) is higher than the backward based approaches (i.e., BP, GBP, GCAM, and GGCAM). Notably, the overhead for FA is the highest, taking more than 75 seconds to interpret VGG16 models. The reason is that forward-based approaches use many perturbed inputs to interpret the prediction result and backward-based approaches require one input pass to the model. The computation overhead of GGCAM is roughly equal to the sum of GBP and GCAM as it uses their results for interpretation. Lastly, the overhead of GBP is 0.01 secs lower than BP. This because GBP only passes non-negative signals during backpropagation."}, {"heading": "5 Limitations and Discussion", "text": "Our experiments show that even after the trojan trigger pixels are substantially replaced with the original image pixels, the remaining pixels may still cause misclassification (See Fig. 6). This means using XAI methods for input purification against trojan attack (Chou, Trame\u0300r, and Pellegrino 2018; Doan, Abbasnejad, and Ranasinghe 2019) is a challenging process because XAI methods have limitations for perfect trojan trigger detection. Furthermore, in the saliency maps generation stage (Section 3.2), we leverage the popular Canny (Canny 1986) edge detection algorithm to identify the most salient region and draw a bounding box to cover the detected pixels. However, our approach is limited to single trigger detection because a bounding box surrounds all detected edges. To handle multiple triggers, we plan to use object detection algorithms such as YOLO (Redmon et al. 2016) to capture multiple objects highlighted by the XAI methods. Lastly, specific XAI methods such as OCC and FA require users to specify input parameters for better interpretation results. While we use default parameter settings of each XAI method for evaluation, different combinations of parameters could yield better interpretation results."}, {"heading": "6 Conclusion", "text": "We introduce a framework for systematic automated evaluation of saliency explanations that an XAI method generates through models trojaned with different backdoor trigger patterns. We develop three evaluation metrics that quantify the interpretability of XAI methods without human intervention using trojan triggers as ground truth. Our experiments on seven state-of-the-art XAI methods against 36 trojaned models demonstrate that methods leveraging local explanation and feature relevance fail to identify trigger regions, and a model-agnostic technique reveals the entire trigger region. Our findings with both analytical and empirical evidence raise concerns about the use of XAI methods for model debugging to reason about the relationship between inputs and model outputs, mainly in adversarial settings."}, {"heading": "Appendix A Additional Figures", "text": "Figure 8 shows the triggers used for trojaning neural models in our framework. Each row shows eight triggers of size 60 \u00d7 60 attached to the bottom right corner of an input image using different factors (color, shape, texture) to cause misclassification to different target labels."}, {"heading": "Appendix B Additional Tables", "text": "Table 3 details the pretrained models used in our evaluation, such as their number of layers, parameters and accuracy. We show the performance of trojaned models with single target label and multiple target labels in Table 4 and Table 5, respectively. Finally, In Table 6 and Table 7, we summarize the notations and acronyms used in this paper."}], "title": "What Do You See? Evaluation of Explainable Artificial Intelligence (XAI) Interpretability through Neural Backdoors", "year": 2020}