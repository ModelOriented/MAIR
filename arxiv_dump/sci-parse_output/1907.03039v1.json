{"abstractText": "The decision-making process of many state-of-the-art machine learning models is inherently inscrutable to the extent that it is impossible for a human to interpret the model directly: they are black box models. This has led to a call for research on explaining black box models, for which there are two main approaches. Global explanations that aim to explain a model\u2019s decision making process in general, and local explanations that aim to explain a single prediction. Since it remains challenging to establish fidelity to black box models in globally interpretable approximations, much attention is put on local explanations. However, whether local explanations are able to reliably represent the black box model and provide useful insights remains an open question. We present Global Aggregations of Local Explanations (GALE) with the objective to provide insights in a model\u2019s global decision making process. Overall, our results reveal that the choice of aggregation matters. We find that the global importance introduced by Local Interpretable Model-agnostic Explanations (LIME) does not reliably represent the model\u2019s global behavior. Our proposed aggregations are better able to represent how features affect the model\u2019s predictions, and to provide global insights by identifying distinguishing features.", "authors": [{"affiliations": [], "name": "Ilse van der Linden"}, {"affiliations": [], "name": "Hinda Haned"}, {"affiliations": [], "name": "Evangelos Kanoulas"}], "id": "SP:3262e13f20bb0bae2c517c6d8bee11b3a166e4d9", "references": [{"authors": ["Sebastian Bach", "Alexander Binder", "Gr\u00e9goire Montavon", "Frederick Klauschen", "Klaus-Robert M\u00fcller", "Wojciech Samek"], "title": "On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation", "venue": "PloS one 10,", "year": 2015}, {"authors": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "title": "Generative adversarial nets. In Advances in neural information processing systems. 2672\u20132680", "venue": "SIGIR \u201919,", "year": 2014}, {"authors": ["Klaus Greff", "Rupesh K Srivastava", "Jan Koutn\u00edk", "Bas R Steunebrink", "J\u00fcrgen Schmidhuber"], "title": "LSTM: A search space odyssey", "venue": "IEEE transactions on neural networks and learning systems 28,", "year": 2017}, {"authors": ["Riccardo Guidotti", "Anna Monreale", "Salvatore Ruggieri", "Franco Turini", "Fosca Giannotti", "Dino Pedreschi"], "title": "A survey of methods for explaining black box models", "venue": "ACM Computing Surveys (CSUR) 51,", "year": 2018}, {"authors": ["Dimitrios Kotzias", "Misha Denil", "Nando De Freitas", "Padhraic Smyth"], "title": "From group to individual labels using deep features", "venue": "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining", "year": 2015}, {"authors": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "year": 2012}, {"authors": ["Scott Lundberg", "Su-In Lee"], "title": "An unexpected unity among methods for interpreting model predictions", "venue": "arXiv preprint arXiv:1611.07478", "year": 2016}, {"authors": ["Scott Lundberg", "Su-In Lee"], "title": "A unified approach to interpreting model predictions", "venue": "In Advances in Neural Information Processing Systems", "year": 2017}, {"authors": ["Laurens van der Maaten", "Geoffrey Hinton"], "title": "Visualizing data using t-SNE", "venue": "Journal of machine learning research 9,", "year": 2008}, {"authors": ["Brent Mittelstadt", "Chris Russell", "Sandra Wachter"], "title": "Explaining explanations in ai", "venue": "arXiv preprint arXiv:1811.01439", "year": 2018}, {"authors": ["W James Murdoch", "Chandan Singh", "Karl Kumbier", "Reza Abbasi-Asl", "Bin Yu"], "title": "Interpretable machine learning: definitions, methods, and applications", "year": 2019}, {"authors": ["Bo Pang", "Lillian Lee"], "title": "Opinion mining and sentiment analysis", "venue": "Foundations and Trends\u00ae in Information Retrieval", "year": 2008}, {"authors": ["Jeffrey Pennington", "Richard Socher", "Christopher D Manning"], "title": "Glove: Global vectors for word representation", "venue": "Proceedings of the Empiricial Methods in Natural Language Processing (EMNLP", "year": 2018}, {"authors": ["Marco Tulio Ribeiro", "Sameer Singh", "Carlos Guestrin"], "title": "Model-agnostic interpretability of machine learning", "venue": "arXiv preprint arXiv:1606.05386", "year": 2016}, {"authors": ["Marco Tulio Ribeiro", "Sameer Singh", "Carlos Guestrin"], "title": "Why should i trust you?: Explaining the predictions of any classifier", "venue": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining", "year": 2016}, {"authors": ["Wojciech Samek", "Alexander Binder", "Gr\u00e9goire Montavon", "Sebastian Lapuschkin", "Klaus-Robert M\u00fcller"], "title": "Evaluating the visualization of what a deep neural network has learned", "venue": "IEEE transactions on neural networks and learning systems 28,", "year": 2017}, {"authors": ["Andrew D Selbst", "Julia Powles"], "title": "Meaningful information and the right to explanation", "venue": "International Data Privacy Law 7,", "year": 2017}, {"authors": ["Avanti Shrikumar", "Peyton Greenside", "Anshul Kundaje"], "title": "Learning important features through propagating activation differences", "venue": "In Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org,", "year": 2017}, {"authors": ["Nitish Srivastava", "Geoffrey Hinton", "Alex Krizhevsky", "Ilya Sutskever", "Ruslan Salakhutdinov"], "title": "Dropout: a simple way to prevent neural networks from overfitting", "venue": "The Journal of Machine Learning Research 15,", "year": 2014}, {"authors": ["Maartje ter Hoeve", "Anne Schuth", "Daan Odijk", "Maarten de Rijke"], "title": "Faithfully explaining rankings in a news recommender system", "year": 2018}], "sections": [{"text": "KEYWORDS interpretability, explainability, black box models, local explanations, global aggregations ACM Reference Format: Ilse van der Linden, Hinda Haned, and Evangelos Kanoulas. 2019. Global Aggregations of Local Explanations for Black Box models. In SIGIR \u201919: The 42nd International ACM SIGIR Conference on Research & Development in Information Retrieval, July 21\u201325, 2019, Paris, France. ACM, New York, NY, USA, 8 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn"}, {"heading": "1 INTRODUCTION", "text": "Over the last decade, many machine learning breakthroughs occurred, spiking widespread interest in the development of advanced machine learning methods, most specifically in the field of deep learning [2, 3, 6, 19]. By using many layers of non-linear operations and abstractions, these complex models make it possible to make more accurate predictions than simpler methods can achieve. The\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. SIGIR \u201919, July 21\u201325, 2019, Paris, France \u00a9 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM. . . $15.00 https://doi.org/10.1145/nnnnnnn.nnnnnnn\nincreased capabilities of these machine learning models have stimulated the development of real-life applications in which consequential decisions are made based on model predictions. The downside of advanced machine learning models is that the decision-making process of such models is inherently inscrutable to the extent that it is impossible for a human to interpret the model directly: they are black box models. Since there is always a divergence between optimization goals and requirements in real-life applications, we cannot assume that the right rule is applied by the model, that the model takes in all - and only - the relevant information, and that all the data is accurate. The inscrutability of black box models, combined with the potential applications for consequential decisions that might affect our safety, our economy, or our opportunities, elicited a call for research on explaining black box models. Recently, this also led to EU regulation on the right to \u201cmeaningful information\u201d about the logic involved in automated decision-making1. Although there is discussion on the legal implication of the terminology, admittedly it implies a right to explanation [17]. The difficulty lies in questions regarding what makes for a valid, reliable and useful explanation; what are the desiderata?\nPrior work on explaining black box models has focused on either global or local explanations, where global explanations aim to explain a model\u2019s decision making process in general, while local explanations aim to explain a single prediction specifically [4, 15]. Global explanations suffer from the trade-off between interpretability of the explanation model and fidelity to the black box model, i.e. the more comprehensible a simplified explanation is, the less faithful it can be to the complexity of the black box model. Local explanations solve this by being restricted to local fidelity: fidelity to the black box model in the vicinity of the instance examined. The drawback is that it is unclear in what way the inspected instance is representative of the global behavior of the model. Moreover, due to the locality of these explanations and its undefined coverage, there would be no way of saying something valid about even a quite similar instance [11, 15].\nOur work intends to fill this gap by presenting Global Aggregations of Local Explanations (GALE), to understand to what extent local explanations are able to provide global insights on a black box model. For this purpose, we analyze explanations obtained through Local Interpretable Model-agnostic Explanations (LIME) on models trained for a binary sentiment analysis task and a multiclass document classification task. We present several approaches to aggregate a set of local explanations and assess to what extent they are representative of the models\u2019 global decision rules and provide reliable and useful insights.\n1EU General Data Protection Regulation: https://gdpr-info.eu/\nar X\niv :1\n90 7.\n03 03\n9v 1\n[ cs\n.I R\n] 5\nJ ul\n2 01\n9"}, {"heading": "2 RELATEDWORK", "text": "We first discuss various local explanations approaches that are within the scope of our research. Next, we elaborate on the the gap between local explanations and global model behavior, and discuss previous work that addresses this limitation."}, {"heading": "2.1 Local Explanations", "text": "The work of [8] identifies a family of approaches that provide a local explanation model in the form of a linear function of binary variables. By representing the features as binary variables, the weights w of the linear model can be directly interpreted as feature attributions. This ensures that the explanation model is interpretable to humans, even though the original model might use complex features as input. For this reason, such methods require a mapping between x , the feature input of the model, and x \u2032, an interpretable representation of that input. The explanation model \u0434(x \u2032) is a linear function of the interpretable representation, and approximates a model\u2019s prediction on an instance f (x) as:\n\u0434 ( x \u2032 ) = w0 + D\u2211 j=1 w jx \u2032 j (1)\nwhere D is the input dimension of the instance explained. Local feature attribution methods provide the user with a local explanation through the weights of this linear model. The weights are considered feature attributions that reflect the influence on a prediction per feature. Attributions can either be supporting or opposing, and the higher the attribution, the higher it accounts for a feature\u2019s influence on the prediction. The sum of the attributions for all features in the input approximates the model output f (x).\nLundberg and Lee [8] propose a framework that unifies a family of approaches that provide an explanation model that is a linear function of binary variables as shown in Equation 1. Methods that adhere to Equation 1 include Local Interpretable Model-agnostic Explanations (LIME) [15], Layer-wise Relevance Propagation (LRP) [1], DeepLift [18] and SHapley Additive exPlanation Values (SHAP) [8].\nLIME offers a local explanation by approximating the model in the vicinity of the instance being explained. It gathers local information by sampling instances from the instance being explained and is therefore considered a perturbation-based approach. The linear model that is fit on these samples offers an interpretable explanation for a specific prediction that adheres to Equation 1 [8, 15]. LIME is a model-agnostic approach to the extent that it can explain any classifier. Beyond this, the approach has inspired research on perturbation-based explanations for other types of tasks, such as models for ranking [20].\nLayer-wise Relevance Propagation is a backpropagation-based approach. It computes a relevance score, i.e. attribution, for each input neuron by redistributing the output of the prediction function backwards layer-by-layer. The exact redistribution function can differ depending on the classifier, as long as it satisfies the relevance conservation property: at each layer the total amount of relevance, i.e. the sum of relevance of all neurons in the layer, equals the output of the prediction function [1].\nDeepLift is another backpropagation-based approach that improves over LRP by introducing the notion of a reference value, which is defined as the neurons activation to a reference input. The reference input is defined per task for the input neurons and propagated through the network to obtain reference values for all neurons. DeepLift obtains contribution scores, i.e. attributions, by expressing the difference from reference value of the output neuron in terms of the difference from reference value of the input neuron [7, 18].\nLundberg and Lee [8] propose to use the concept of Shapley values, from the field of game theory, to quantify feature importance. The Shapley value of a feature is the averaged marginal contribution of that feature to all possible subsets of features, i.e. meaning the average difference in prediction with or without the feature included for each subset. Lundberg and Lee [8] present several approaches for approximating the Shapley values to obtain an explanation model, calling their approach SHapley Additive exPlanation values (SHAP).\nThe evaluation of our approach that is presented in this paper uses explanations obtained by LIME. Nonetheless, the global aggregations that we present in Section 3 are applicable to all local explanations that adhere to Equation 1. Throughout this paper we will use the more general description local explanation to refer to this specific family of approaches.\nRibeiro et al. [15] stipulate LIME suffers from the fact that it is unclear what the coverage of an explanation is, i.e. to what extent it generalizes to other situations. Correspondingly, Mittelstadt et al. [10] claim a three way trade-off, adding the size of the domain described as a third desiderata besides fidelity and interpretability. In another recent review, it was argued that explanations should be required to show relevance, meaning they must provide insights for users into a chosen domain problem [11]. In general, Murdoch et al. [11] state this requires a diversity in approaches, varying the balance in fidelity and interpretability, to meet the need of distinct domain problems. Particularly in the case of local explanations, they point out the gap between local explanations and global model behavior, leading to a call for future work to answer the question: to what extent local methods capture a model\u2019s behavior and how the explanations can best be used? Our work intends to increase the understanding of the limitations of local explanations and in doing so aims to improve the usefulness of such explanations."}, {"heading": "2.2 Global Insight From Local Explanations", "text": "The authors of the LIME method acknowledge the gap between their method and the model\u2019s global behavior [15]. It is with this limitation in mind that they propose their submodular pick algorithm. In order to provide global insight, this algorithm selects a set of data instances for which to present explanations to a user. It selects the instances by picking those that contain features with a high global importance score.\nRibeiro et al. [15] aim to define global importance such that features that explain many different instances have a higher global importance score, than features that explain less instances. In the case of text classification, they propose the global importance Ij of a feature j as the square root of the sum of its attributions, as shown and further elaborated on in section 3.1. Additionally, they\naim to pick a subset S out of N instances, such that there is little redundancy in the features shown, arguing that if a feature appears in multiple instances, this would lead to similar explanations shown to a user. Both these intuitions are formalized as a coverage function c . LetW be theN\u00d7M matrix containing the attributions per instance for each feature out of M unique features. Then, given a set of instances S , the explanation matrixW and the importance vector I , the coverage of the set S is defined by:\nc (S,W , I ) = \u2211 j=1 [\u2203i \u2208 S :Wi j > 0] Ij (2)\nMaximizing this function would yield the optimal set of instances covering important features. However, since this problem is NPhard, a greedy algorithm is proposed that iteratively adds instances to the set S to approximate the optimal set. The instance i added at each iteration is the one with the highest marginal gain, which is defined as c (S \u222a {i},W , I ) \u2212 c (S,W , I ) [15].\nIn the paper proposing this approach [15], only limited evaluation on the submodular pick is provided. The authors only compare their approach to providing randomly selected instances to the user. They show that providing explanations for specific instances chosen by the submodular pick algorithm makes users more able to make choices that positively affect performance, such as comparing between models and features, than when they are shown a random set of instances. It is unclear to what extent the chosen instances are representative for the global behavior of the model, and the importance function is not further evaluated. Therefore, the gap between the local explanations and the global model behavior remains."}, {"heading": "3 METHODOLOGY", "text": "Our work intends to fill the gap between local explanations and global model behavior. For this purpose, we propose a set of Global Aggregations of Local Explanations (GALE). Rather than validating local explanations independently, we assess the emerging insight from aggregating multiple local explanations. The way to aggregate local explanations is not straightforward since the attribution scores are not determined in relation to other data instances. It is unclear how attributions between different instances or in support of different classifications relate to each other. This is further complicated when applied to a textual task, given the sparsity of features in this domain. Any choice of aggregation function implies assumptions about the way in which local explanations are representative of the global model behavior. In our discussion of GALE we intend to make these assumptions explicit."}, {"heading": "3.1 Global LIME Importance", "text": "Ribeiro et al. [15] propose their submodular pick algorithm to select a set of instances to show a user in order to provide global insight. In order to select a representative and informative subset of instances, they propose a global aggregation function I to assess global feature importance. Specifically for the text domain, they define the global feature importance ILimej as the square root of the sum of attributionsWi j of the feature j over all data instances i \u2208 N :\nILimej = \u221a\u221a N\u2211 i=1 |Wi j | (3)\nTwo assumptions underlie this aggregation function: A1 Features with higher attributions are expected to have a larger\neffect on model predictions than features with lower attributions. A2 Features that occur more often are expected to have a larger effect on model predictions than features that occur less often.\nIn the aggregation over separate instances, these assumptions will not always hold. Although A1 seems reasonable amongst the features within one instance, this is less certain for feature attributions from various instances. Since the explanation model is a linear function of attribution values, the magnitude of attributions are affected by the amount of features per explanation. Similarly, the magnitude of attributions are affected by the prediction value that the explanation model is approximating. However, in comparison between attributions from different instances, the absolute value of the attribution might be less informative than its relative importance within the instance.\nWith respect toA2, the amount of occurrences of a feature might be a misleading notion for several reasons, especially in text classification. Firstly, the assumption is that occurrences across different instances amount to a higher influence of the feature. Common words such as \u201cthe\u201d, \u201cand\u201d, or \u201cis\u201d, are thus likely to be ranked as very important due to many occurrences in different instances, even when their attributions are low. Secondly, as mentioned in [14], local explanations for different instances can also be inconsistent with each other. This issue is further amplified in case of a multiclass classification task, because a single explanation does not show the features\u2019 possible relationship to other classes when providing a single example. Features that occur in documents of different classes will have a high global LIME importance, independently of which classes the individual attributions support, or whether the occurrences have a large impact on predictions."}, {"heading": "3.2 Global Average Importance", "text": "In what follows we will not relax A1, but we will focus on A2. Whether features that occur more often in the data are more important, clearly depends on the domain. In case of textual data we often deal with sparse features; common words will occur often, while most other words will only occur in few instances. For this reason, we expect the global LIME importance to be unreasonably biased towards common words.\nTherefore, the first alternative aggregation we propose is the average importance, which is computed as the sum of attributions Wi j averaged over the feature\u2019s occurrences in the dataset:\nIAvgj = \u2211N i=1 |Wi j |\u2211 i :Wi j,0 1\n(4)\nAlthough the global average importance addresses the second assumption that is made in global LIME importance, it also makes its own assumption:\nA3 Features are expected to have a similar effect in all of their occurrences.\nTo understand why this assumption might not hold, imagine the case of a feature being important in the predictions for some class and less important when appearing in documents unrelated to that class. The occurrence in other documents will strongly lower its average importance, even though the feature was highly important for another class."}, {"heading": "3.3 Global homogeneity-weighted importance", "text": "The global homogeneity-weighted importance is designed to address A2 and A3. The idea is to determine the homogeneity of a feature\u2019s influence on the model in order to deal with multiple occurrences and potential inconsistencies between occurrences. To quantify the homogeneity per feature, the spread of attributions over different classes is determined by Shannon entropy. First, we define pj as the vector of normalized LIME importance per class:\npc j = \u221a\u2211 i \u2208Sc |Wi j |\u2211\nc \u2208L \u221a\u2211 i \u2208Sc |Wi j | (5)\nwhere Sc is the set of all instances i classified as class c and L is the set of class labels. The normalized LIME importance pj represents the distribution of feature j\u2019s importance over all classes c \u2208 L. The Shannon entropy of this distribution is defined by:\nHj = \u2212 \u2211 c \u2208L pc j log ( pc j ) (6)\nThis entropy score is used to assess the degree of homogeneity with which the feature attributions of a feature are distributed over multiple classes. Low entropy indicates most of the attributions point to one particular class, as opposed to the case of high entropy in which attributions point to many classes. However, since entropy will be equally low for all features that only occur a single time in the test set, the entropy score does not discriminate in these cases. Therefore we propose to derive from this a homogeneity-weighted importance IHj . For this purpose, the entropy score is normalized and subtracted from 1 to obtain a weighting factor that is close to 1 if the feature is homogeneous and close to 0 when its attributions are spread over many different classes. The homogeneity weighted importance is the LIME importance of a feature weighted by this weighting factor:\nIHj =\n( 1 \u2212\nHj \u2212 Hmin Hmax \u2212 Hmin\n) ILIMEj (7)\nwhere Hmin and Hmax are the minimum and maximum entropy measured across all features."}, {"heading": "4 EXPERIMENTAL DESIGN", "text": "Experiments are carried out on two distinct datasets: a relatively small sentiment analysis dataset and a larger document classification dataset. This makes us able to evaluate to what extent our approach is influenced by the complexity of the task and the amount of data available. For each task, a model with a task-specific architecture is trained on a subset of the data. Parameter tuning of these models is done based on a validation set that is not further used in our experiments. Subsequently, LIME explanations are obtained for the model predictions on a separate test set and used to gather the global aggregations over all instances in the test set. These\nare evaluated quantitatively based on our AOPC\u0434lobal metric and qualitatively by visualization of the important features according to each of the aggregations. In our experiments we answer the following research questions: RQ1 To what extent can global LIME importance represent how\nfeatures affect the model\u2019s predictions? RQ2 To what extent can global average and homogeneity-weighted\nimportance represent how features affect the model\u2019s predictions, i.e. can the proposed aggregations improve on global LIME importance?\nRQ3 To what extent do the quantitative results differ between a binary and a multiclass text classification task?"}, {"heading": "4.1 Sentiment analysis task", "text": "Sentiment analysis is a binary classification task in which documents are labeled as expressing either an overall positive or negative sentiment [12]. The sentiment analysis dataset used for this study consists of 3000 sentences with labels evenly distributed over a positive or negative sentiment2. Kotzias et al. [5] selected these instances from larger datasets originating from the websites of IMDB, Amazon and Yelp, incorporating 1000 sentences per source. The LSTM architecture used for this task consist of one LSTM layer with tanh activation function and both input and recurrent dropout at 0.2, followed by one fully connected softmax layer. The neural network is optimized with Adam over a run of 10 epochs with a batch size of 32. The input features are pretrained 100-dimensional GloVe word embeddings3, which are not further fine-tuned during training [13]. With this setup we obtain an accuracy of 0.85."}, {"heading": "4.2 Document classification task", "text": "The 20 Newsgroups dataset is a collection of approximately 20,000 newsgroup post, almost evenly distributed over 20 different classes4. The CNN architecture consists of three convolutional layers with ReLU activation functions, max pooling after each convolutional layer and dropout at 0.2. The neural network has a final fully connected softmax layer, is optimized with Adam and run for 10 epochs with batch size at 32. The input features are pretrained 100-dimensional GloVe word embeddings5, which are not further fine-tuned during training [13]. With this setup we obtain an accuracy of around 0.75."}, {"heading": "4.3 Quantitative Evaluation", "text": "In order to determine which global aggregation best represents the global model behavior, we propose an adaptation of the Area Over the Perturbation Curve (AOPC) evaluation. AOPC was proposed by Samek et al. [16] as an evaluation metric for local feature attribution methods. It defines a good local explanation as one that is able to identify the features that have the largest effect on model prediction. Adapting this, we propose AOPC\u0434lobal to evaluate to what extent the aggregations are able to identify the features that have the largest global effect on model predictions. The models\u2019 decisions on\n2 Retrieved from https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+ Sentences 3 Retrieved from https://nlp.stanford.edu/projects/glove/ 4 Retrieved from http://qwone.com/~jason/20Newsgroups/ 5 Retrieved from https://nlp.stanford.edu/projects/glove/\nthe resulting documents are evaluated by computing AOPC\u0434lobal over a range of consecutive feature removals and compared against a random baseline.\nOur adaptation is that AOPC\u0434lobal is measured by progressively removing features per document in the order of their global ranking by GALE, as opposed to removing features in the order of their local ranking according to the local explanation as in AOPC for local evaluation. For each aggregation of features, the features in a document xi are ranked according to the global aggregation. Subsequently, features are iteratively removed from the original data point xi in the order of that ranking. Let r be the vector of feature indices in document xi ranked in the order of the global aggregation. Then, the instance xki is the result of recursively removing the k highest ranking features defined as follows:\nx0i = xi xki = s ( xk\u22121i , rk ) (8) where the function s removes the kth highest ranking feature in document xi according to the global aggregation from index rk in the data point xi .\nWe propose AOPC\u0434lobal as a metric to evaluate the global ranking of features per aggregation by quantifying the effect of K removed features on model predictions. It is defined as the the averaged cumulative sum up to K of the drop in predicted class probability averaged over all instances in the test set:\nAOPC\u0434lobal = 1\nK + 1 \u2329 K\u2211 k=0 f (x0i ) \u2212 f (x k i ) \u232ai \u2208N av\u0434\n(9)\nwhere \u27e8\u00b7\u27e9i \u2208Nav\u0434 denotes the averaging over all instances in the test set and the black box prediction function f returns the probability for the predicted class.\nThis metric is computed over a consecutive range of removals per document K . Specifically, at K = 1, AOPC\u0434lobal is evaluated by removing from each document one feature, that is the highest ranking feature in that document according to the global aggregation. The AOPC\u0434lobal at K = 2 is computed by removing the two highest ranking features according to the global aggregation from each document, and so on. The resulting curve is evaluated on two aspects. Firstly, the overall height of the curve is assessed. A higher curve indicates a better ranking of features in the order of global influence on predictions. Secondly, we assess the initial slope of the curve. The steeper the slope of the curve for the first features removed, the stronger their influence on the model. A good aggregation is expected to demonstrate a positive decreasing slope for the AOPC\u0434lobal curve."}, {"heading": "4.4 Qualitative Evaluation", "text": "In addition to the quantitative evaluation, we present qualitative visualizations that enable examination of the global insights provided by GALE by demonstrating the most influential features per class according to each of the aggregations. In this perspective, a good global aggregation is one that considers features important if they distinguish between classes. Based on the qualitative visualizations we intend to answer RQ1 and RQ2 by observing whether the aggregations are able to identify distinguishing features.\nA slightly adapted version of each aggregation function is used to compute per class global aggregations. Let Sc be the set of all instances i \u2208 N classified as class c . Then, the global LIME class importance for feature j and class c is defined as:\nILimec j = \u221a\u2211 i \u2208Sc |Wi j | (10)\nThe global average class importance for feature j and class c is defined as:\nIAvgc j =\n\u2211 i \u2208Sc |Wi j |\u2211\ni \u2208Sc :Wi j,0 1 (11)\nAnd lastly, the global homogeneity-weighted class importance for feature j and class c is defined as:\nIHc j =\n( 1 \u2212\nHj \u2212 Hmin Hmax \u2212 Hmin\n) ILIMEc j (12)\nNotice that for the homogeneity-weighted class importance, the weighting factor remains the same as in Equation 7; it is still computed over all classes.\nThese global class importance functions are used to visualize the most important features per class, as determined by each of the aggregation methods. The selected features are plotted using t-SNE for dimensionality reduction on the word embeddings [9]. More than just illustrating the most influential features, these visualizations present clusters of words that share a similarity in being indicative of a particular class. A global aggregation that can better identify distinguishing features than other aggregations, is expected to demonstrate more distinct clusters of words."}, {"heading": "5 RESULTS", "text": ""}, {"heading": "5.1 Quantitative Results", "text": "We evaluate if features that are considered globally important according to the aggregations, indeed have a large impact on model predictions. For each of the global aggregations, features are removed in order of global importance, and compared against a random baseline for which the removed words are selected at random. The random baseline is averaged over five runs; the variance is also shown in the result plot. The evaluation is carried out for both classification tasks described in Section 4. The results for the sentiment analysis task are shown in Figure 1, where up to 20 features are removed per sentence. Figure 2 presents the results for the 20 Newsgroup text classification task. Since this task entails larger documents, results are shown for K up to 50 features removed per document.\nIn both Figures 1 and 2, it can be seen that the global LIME aggregation obtains only slightly higher AOPC\u0434lobal results compared to the random baseline. In particular it is found that the initial steepness of all LIME importance curves is equal to the initial steepness of the baseline. This indicates that the global LIME aggregation especially fails to correctly identify the most important features; the first features removed in the evaluation affect the models\u2019 predictions and performance no more than average. Our findings indicate\nthat global insights through aggregation can be improved by selecting an aggregation function that better represents the local explanations with respect to the global model behavior.\nBoth the global average importance and the global homogeneityweighted importance surpass the AOPC\u0434lobal values obtained by global LIME importance and the random baseline, demonstrating that these aggregations are more able to reliably represent the model\u2019s global behavior. Additionally, for both aggregations, the AOPC\u0434lobal displays a much steeper initial slope of the curve. This finding implies that the global average and global homogeneityweighted importance, more adequately identify the most important features; these aggregations are better able to rank the features based on their global influence on model predictions. This is evidence for our hypothesis that the global LIME importance is based\non misleading assumptions. More general, it reveals that the choice of aggregation function matters regarding its ability to represent the model\u2019s global decision making process.\nThe average importance aggregation performs slightly better on AOPC\u0434lobal than the homogeneity-weighted importance in case of the sentiment analysis task. On the contrary, the homogeneityweighted importance displays a steeper curve for AOPC\u0434lobal in the document classification task. Taking into account the difference in scaling between these tasks, the difference in the document classification task is stronger than in the sentiment analysis task. A possible explanation for why homogeneity-weighted importance performance better on the document classification task is that the sentiment analysis task is a binary classification task, while the 20 Newsgroup classification is a multiclass classification task. In a binary classification a local explanation for a particular instance informs about the feature influence for all possible class predictions - there are only 2 classes. The attribution for a feature is either in support of the predicted class or against it, in the latter case this signifies support for the opposing class. In the case of multiclass classification, local explanations only provide an explanation for the influence of features in light of the predicted class. The global average importance of a feature that is influential for some classes would be significantly lowered due to low attributions in explanations for other classes. The global homogeneity weighting factor is more appropriate when explaining a multiclass classification model, because the weighting factor is affected by the spread of a feature\u2019s attributions over different classes, specifically to the degree of uniformity of that distribution. The effect is that global importance is reduced more for features that obtain high attributions for multiple classes than for features with high attributions for one class and low attributions in explanations for other classes."}, {"heading": "5.2 Qualitative Results", "text": "To deepen our understanding of which global aggregation best provide global insight in a complex model, several visualizations are presented as described in Section 4.4. In this paper we provide a confined version of the qualitative evaluation. More elaborate results are available at https://github.com/iwcvanderlinden/GALE.\nFigures 3-5 demonstrate the top ten features per class for 8 out of 20 Newsgroups classes, according to each of the global aggregations respectively. Firstly, the presented visualization for global LIME importance contains class-specific clusters of distinguishing features, as well as less substantive features, e.g. commonwords and punctuation, that do not appear in clearly distinct clusters. Both features that are likely and unlikely to distinguish between classes, are deemed important by the global LIME aggregation. Secondly, the visualization for global average importance contains class-specific clusters of distinguishing features for a minority of the classes in the document classification task. The global average aggregation considers substantive features important, i.e. no common words and punctuation. The qualitative visualization for global homogeneity-weighted importance demonstrates class-specific clusters of distinguishing features for a majority of the classes in the document classification task. Features deemed important by the global homogeneityweighted aggregation are substantive features that distinguish between classes."}, {"heading": "6 CONCLUSION & FUTUREWORK", "text": "In this work we propose GALE, which aims to provide insights in a black-box model\u2019s global decision making process. Overall, we conclude that Global Aggregations of Local Explanations have the potential to provide global insights from local explanations. In addition to this, our findings reveal that the choice of aggregation matters regarding the ability to gain reliable and useful global insights on a black box model.\nOur work offers opportunities to further develop GALE for different tasks as well as for other local explanations methods. For instance, regression and ranking taskswould require different aggregation functions. Future work could follow the procedure outlined in our methodology. Determine which assumptions are likely or unlikely to hold given the domain of the task, and design global aggregations accordingly.\nFurthermore, the GALE framework could be used to gather further understanding of local explanations. Information about the representativeness of individual explanations helps users comprehend and mitigate the gap between local explanations and global model behavior. We also intend on evaluating the application of GALE via a user-study."}], "title": "Global Aggregations of Local Explanations for Black Box models", "year": 2019}