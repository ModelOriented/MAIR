{"abstractText": "The way pages are ranked in search results influences whether the users of search engines are exposed to more homogeneous, or rather to more diverse viewpoints. However, this viewpoint diversity is not trivial to assess. In this paper we use existing and novel ranking fairness metrics to evaluate viewpoint diversity in search result rankings. We conduct a controlled simulation study that shows how ranking fairness metrics can be used for viewpoint diversity, how their outcome should be interpreted, and which metric is most suitable depending on the situation. This paper lays out important ground work for future research to measure and assess viewpoint diversity in real search result rankings.", "authors": [{"affiliations": [], "name": "Tim Draws"}, {"affiliations": [], "name": "Nava Tintarev"}, {"affiliations": [], "name": "Ujwal Gadiraju"}, {"affiliations": [], "name": "Alessandro Bozzon"}, {"affiliations": [], "name": "Benjamin Timmermans"}], "id": "SP:8bb0c429cb243914cb532742fe3fcc7fb8780629", "references": [{"authors": ["A. Abid", "N. Hussain", "K. Abid", "F. Ahmad", "M.S. Farooq", "U. Farooq", "S.A. Khan", "Y.D. Khan", "M.A. Naeem", "N. Sabir"], "title": "A survey on search results diversification techniques", "venue": "Neural Comput. Appl. 27(5), 1207\u20131229", "year": 2015}, {"authors": ["R. Agrawal", "S. Gollapudi", "A. Halverson", "S. Ieong"], "title": "Diversifying search results", "venue": "Proc. 2nd ACM Int. Conf. Web Search Data Mining, WSDM\u201909 pp. 5\u201314", "year": 2009}, {"authors": ["R. Baeza-Yates"], "title": "Bias on the web", "venue": "Commun. ACM 61(6), 54\u201361", "year": 2018}, {"authors": ["Barocas", "Solon", "A.D. Selbst"], "title": "Big data\u2019s disparate impact", "venue": "Calif. Law Rev. 104(671), 671\u2013732", "year": 2016}, {"authors": ["A.J. Biega", "K.P. Gummadi", "G. Weikum"], "title": "Equity of attention: Amortizing individual fairness in rankings", "venue": "41st Int. ACM SIGIR Conf. Res. Dev. Inf. Retrieval, SIGIR 2018 pp. 405\u2013414", "year": 2018}, {"authors": ["C.L. Clarke", "M. Kolla", "G.V. Cormack", "O. Vechtomova", "A. Ashkan", "S. B\u00fcttcher", "I. MacKinnon"], "title": "Novelty and diversity in information retrieval evaluation", "venue": "ACM SIGIR 2008 - 31st Annu. Int. ACM SIGIR Conf. Res. Dev. Inf. Retrieval, Proc. pp. 659\u2013666", "year": 2008}, {"authors": ["R. Epstein", "R.E. Robertson"], "title": "The search engine manipulation effect (SEME) and its possible impact on the outcomes of elections", "venue": "Proc. Natl. Acad. Sci. U. S. A. 112(33), E4512\u2013E4521", "year": 2015}, {"authors": ["B. Fuglede", "F. Tops\u00f8e"], "title": "Jensen-Shannon divergence and Hubert space embedding", "venue": "IEEE Int. Symp. Inf. Theory - Proc. p. 31", "year": 2004}, {"authors": ["T. Joachims", "L. Granka", "B. Pan", "H. Hembrooke", "G. Gay"], "title": "Accurately interpreting clickthrough data as implicit feedback", "venue": "SIGIR 2005 - Proc. 28th Annu. Int. ACM SIGIR Conf. Res. Dev. Inf. Retr. 51(1), 154\u2013161", "year": 2005}, {"authors": ["Kullback", "Solomon", "R.A. Leibler"], "title": "On information and sufficiency", "venue": "Ann. Math. Stat. 22(1), 79\u201386", "year": 1951}, {"authors": ["J. Kulshrestha", "M. Eslami", "J. Messias", "M.B. Zafar", "S. Ghosh", "K.P. Gummadi", "K. Karahalios"], "title": "Search bias quantification: investigating political bias in social media and web search", "venue": "Inf. Retr. J. 22(1-2), 188\u2013227", "year": 2019}, {"authors": ["J. Lin"], "title": "Divergence Measures Based on the Shannon Entropy", "venue": "IEEE Trans. Inf. Theory 37(1), 145\u2013151", "year": 1991}, {"authors": ["E. Ntoutsi", "P. Fafalios", "U. Gadiraju", "V. Iosifidis", "W. Nejdl", "M.E. Vidal", "S. Ruggieri", "F. Turini", "S. Papadopoulos", "E. Krasanakis", "I. Kompatsiaris", "K. KinderKurlanda", "C. Wagner", "F. Karimi", "M. Fernandez", "H. Alani", "B. Berendt", "T. Kruegel", "C. Heinze", "K. Broelemann", "G. Kasneci", "T. Tiropanis", "S. Staab"], "title": "Bias in data-driven artificial intelligence systems\u2014An introductory survey", "venue": "Wiley Interdiscip. Rev. Data Min. Knowl. Discov. 10(3), 1\u201314", "year": 2020}, {"authors": ["B. Pan", "H. Hembrooke", "T. Joachims", "L. Lorigo", "G. Gay", "L. Granka"], "title": "In Google we trust: Users\u2019 decisions on rank, position, and relevance", "venue": "J. Comput. Commun. 12(3), 801\u2013823", "year": 2007}, {"authors": ["F.A. Pogacar", "A. Ghenai", "M.D. Smucker", "C.L. Clarke"], "title": "The Positive and Negative Influence of Search Results on People\u2019s Decisions about the Efficacy of Medical Treatments", "venue": "Proc. ACM SIGIR Int. Conf. Theory Inf. Retr. pp. 209\u2013216. ICTIR \u201917, Association for Computing Machinery, New York, NY, USA", "year": 2017}, {"authors": ["P. Sapiezynski", "W. Zeng", "R.E. Robertson", "A. Mislove", "C. Wilson"], "title": "Quantifying the Impact of User Attentionon Fair Group Representation in Ranked Lists", "venue": "Companion Proc. 2019 World Wide Web Conf. pp. 553\u2013562. WWW \u201919, Association for Computing Machinery, New York, NY, USA", "year": 2019}, {"authors": ["S. Verma", "J. Rubin"], "title": "Fairness definitions explained", "venue": "Proc. Int. Work. Softw. Fairness. pp. 1\u20137. FairWare \u201918, Association for Computing Machinery, New York, NY, USA", "year": 2018}, {"authors": ["R. Wang", "D. Zhou", "M. Jiang", "J. Si", "Y. Yang"], "title": "A survey on opinion mining: From stance to product aspect", "venue": "IEEE Access 7, 41101\u201341124", "year": 2019}, {"authors": ["F.L. Wauthier", "M.I. Jordan"], "title": "Bayesian bias mitigation for crowdsourcing", "venue": "Adv. Neural Inf. Process. Syst. 24 25th Annu. Conf. Neural Inf. Process. Syst. 2011, NIPS 2011. pp. 1\u20139", "year": 2011}, {"authors": ["K. Yang", "J. Stoyanovich"], "title": "Measuring fairness in ranked outputs", "venue": "Proc. 29th Int. Conf. Sci. Stat. Database Manag. pp. 1\u20136. SSDBM \u201917, Association for Computing Machinery, New York, NY, USA", "year": 2017}, {"authors": ["M. Zehlike", "F. Bonchi", "C. Castillo", "S. Hajian", "M. Megahed", "R. Baeza-Yates"], "title": "FA*IR: A Fair Top-k Ranking Algorithm", "venue": "Admit One. pp. 1\u201318", "year": 2017}], "sections": [{"text": "Keywords: viewpoint diversity \u00b7 web search \u00b7 ranking fairness"}, {"heading": "1 Introduction", "text": "Search result rankings strongly influence user attitudes, preferences, and behavior [7,9,14,15]. Underlying this effect are cognitive biases such as position bias, which describes users\u2019 tendency to pay more attention to documents at higher ranks [9,14]. Recent research has demonstrated that these biases go to such an extent that rearranging search result rankings to favor different stances on the same topic can affect users\u2019 personal opinions [7,15]. To mitigate such unintentional biases, it is important to maintain a strong viewpoint diversity in search result rankings \u2013 especially when they relate to disputed topics.\nViewpoint diversity in search result rankings is closely related to the notion of ranking fairness. The aim in fair ranking is to measure and adapt ranked lists in terms of their fairness with respect to a given characteristic [5,16,20]. For example, a ranked list of candidates on a job seeking platform could be evaluated with respect to gender fairness. A fair ranking is then considered to be one in which gender has no influence on the ranking of candidates. Analogously in this paper, a search result ranking is evaluated with respect to viewpoint \u2013 to the best of our knowledge, a novel application of ranking fairness. Such a viewpoint can for example convey different stances on a topic, or different underlying reasons for a given stance. A search result ranking that is fair (or unbiased) with respect\nar X\niv :2\n01 0.\n14 53\n1v 1\n[ cs\n.I R\n] 2\n7 O\nct 2\nto viewpoints would give each viewpoint its fair share of coverage, contributing to viewpoint diversity in the search results.3\nOne major building block of studying viewpoint fairness in search result rankings is deciding how to measure it. Several metrics have been developed that assess fairness in rankings [20,16] (see Section 2). These metrics evaluate fairness in terms of statistical parity, which is satisfied in a ranking if a given variable of interest \u2013 here, the expressed viewpoint \u2013 does not influence how documents are ranked. In this paper, we investigate whether ranking fairness metrics can be used to assess viewpoint diversity in search results.\nWe generate a range of synthetic search result rankings with varying degrees of ranking bias and explore the behavior of existing and novel ranking fairness metrics on these rankings. For our use case of viewpoint diversity, we consider two fundamental scenarios: binomial viewpoint fairness, in which the task is to measure viewpoint diversity with respect to one specific protected viewpoint, and multinomial viewpoint fairness where the aim is to protect all available viewpoints simultaneously. We make the following contributions:\n1. We present a simulation study that illustrates how existing ranking fairness metrics can be used to assess viewpoint diversity in search result rankings. We show how these metrics behave under varying conditions of viewpoint diversity and provide a guide for their use (Section 4.2). 2. We propose a novel ranking fairness metric for assessing multinomial viewpoint fairness (Section 3.4) and also analyze its behavior (Section 4.2).\nWe find that all the considered ranking fairness metrics can distinguish well between different levels of viewpoint diversity in search results. However, which specific metric is most sensitive to a lack of viewpoint diversity depends on how many viewpoint categories there are, the distribution of advantaged and disadvantaged items in the ranking, and the severity of the ranking bias."}, {"heading": "2 Background and Related Work", "text": "Diversity in search result rankings is not a novel topic. Several methods have been proposed to measure and improve diversity in ranked lists of search results [1,2,6]. Unlike previous methods, which aim to balance relevance (e.g., in relation to a user query) and diversity (e.g., in relation to user intent), we delve deeper into the notion of diversity. Specifically, we focus on ranking fairness for assessing viewpoint diversity, which originates from the field of fair machine learning.\nFairness and the mitigation of bias in machine learning systems extends into several different sub-fields [4,13]. One of these sub-fields \u2013 fair ranking \u2013 has received increasing attention recently, following calls for dealing with bias on the web [3]. This has led to the development of evaluative frameworks [5,16] and metrics [11,20] for assessing bias and fairness in ranked lists. Measuring ranking\n3 Note that here we are thus looking at fairness in the outcome of a ranking algorithm; i.e., not at procedural fairness.\nfairness requires deciding which notion of fairness to handle (i.e., defining a fair ranking scenario) [5,20] and discounting the metric computation by rank in order to account for differences in attention over the ranks [5,16].\nPreviously proposed ranking fairness metrics commonly presuppose that a fair or unbiased ranking is one in which statistical parity is present [20]. A machine learning algorithm satisfies statistical parity when an item\u2019s probability of receiving a given outcome is not affected by belonging to a protected group [17]. Such a protected group can be any subset of the overall population of items that share some characteristic that is not supposed to affect the algorithm outcome. In the context of ranking, statistical parity holds when membership in a protected group has no influence on a document\u2019s position in the ranking [20]. Suppose a user enters the query Should Zoos Exist? into a web search engine, which then returns a ranked list of search results. Each document in the ranking corresponds to some viewpoint with respect to zoos or is neutral towards the topic. A ranking assessor could define the opposing side of the zoo-argument as the protected viewpoint. Statistical parity would then be satisfied if expressing the protected viewpoint does not affect the ranking of documents.\nYang and Stoyanovich [20] introduce three metrics that assess statistical parity in rankings. These metrics compare the group membership distribution (i.e., share of protected and non-protected items) in a ranking at different cut points (e.g., 10, 20, ...) with the ranking\u2019s overall group membership distribution. Aggregating the results of these comparisons in a discounted manner incorporates the intuition that an absence of bias is more important among higher ranks. We formalize the metrics introduced by Yang and Stoyanovich [20] in Section 3.4.\nRanking fairness has also been assessed in at least two other notable ways. First, Kulshrestha et al. [11] introduce a metric that quantifies ranking bias related to continuous attributes as opposed to group membership. Their metric considers the mean of a continuous variable of interest at each step of its computation. Despite this promising groundwork, measuring continuous ranking bias remains limited; for instance, by considering only the mean, other important characteristics of continuous distributions (i.e., such as the standard deviation or distribution type) are ignored. Second, recent work has defined criteria that a ranking has to fulfill to be considered fair [21,16]. Whether the ranking fulfills these criteria is assessed using null hypothesis significance testing. Our aim, however, is to quantify the degree of viewpoint diversity in search result rankings."}, {"heading": "3 Measuring Fairness in Rankings", "text": "Viewpoint diversity in search results can best be illustrated by a running example. Consider that a user wants to form an educated opinion on the topic \u2018Should Zoos Exist? \u2019, and turns to web search to gather information. Let us assume that each document that the user encounters in the search result list will express a viewpoint with regard to zoos or be neutral towards the topic.4 These\n4 Here, neutral could mean that a documents contains not opinionated, provides a balanced overview of the different viewpoints, or is irrelevant to the topic.\nviewpoints can be represented in an ordinal manner, as illustrated in Table 1. We thus categorize the different viewpoints related to zoos by placing them on a 7-point scale ranging from strongly opposing to strongly supporting the existence of zoos.5"}, {"heading": "3.1 Preliminaries and Notation", "text": "We are given a set of documents D and a set of viewpoint labels S. Both sets contain the same number of elements N . Each document d \u2208 D is uniquely associated with one label sd \u2208 S. Here, sd reflects the viewpoint of document d towards a given disputed topic, rated on a 7-point scale ranging from extremely opposing to extremely supporting. The viewpoint labels in S are integers ranging from \u22123 to 3, where negative values indicate an opposing viewpoint, 0 indicates a neutral viewpoint, and positive values indicate a supporting viewpoint towards the debated topic (see Table 1 for an example). A ranked list of D is denoted as \u03c4 . We denote the number of items that belong to a subset p of S as Sp, which becomes Sp1...i when constrained to the top i ranked documents."}, {"heading": "3.2 Notions of Fairness and Viewpoint Diversity", "text": "There are many definitions of fairness, and so before describing fairness metrics, we first identify which type of fairness to handle. In this paper we focus on the notion of statistical parity (also commonly referred to as group fairness; see Section 2). This notion allows us to define several fairness aims for assessing viewpoint diversity. We consider two such aims, which we call binomial viewpoint fairness and multinomial viewpoint fairness. Below we describe these aims and align them with the notion of statistical parity in rankings.\nBinomial viewpoint fairness. One aim for viewpoint diversity may be to treat one specific viewpoint, e.g., a minority viewpoint, fairly. For example, if a search result ranking on the query Should Zoos Exist? is dominated by arguments supporting zoos, the ranking assessor may want to evaluate whether\n5 Note that this is just one possible way to categorize existing viewpoints on a topic.\nthe minority viewpoint (i.e., opposing zoos) gets its fair share of coverage. The assessor may consider a binomial classification of documents into one of two groups: expressing the minority viewpoint or not expressing the minority viewpoint. Here, expressing the minority viewpoint is analogous to a protected group. Statistical parity in a ranking of such documents is satisfied when expressing the minority viewpoint does not affect a document\u2019s position in the ranking.\nMultinomial viewpoint fairness. Another aim when evaluating viewpoint diversity may be that all viewpoints are covered fairly. For example, a search result ranking on the query Should Zoos Exist? could be assessed without explicitly defining a specific viewpoint as the protected group but instead considering the distribution over several existing viewpoints. Here the assessor thus considers a multinomial classification of documents into some viewpoint taxonomy (e.g., into seven categories depending on polarity and severity of the viewpoint; see Section 3.1). In this case, we say that statistical parity is satisfied when for each viewpoint, the choice of viewpoint does not influence a document\u2019s position in the ranking. Multinomial viewpoint fairness is thus more fine-grained than binomial viewpoint fairness: whereas in binomial viewpoint fairness focuses on fairness towards one protected viewpoint, multinomial viewpoint fairness requires being fair to all viewpoints simultaneously."}, {"heading": "3.3 Metric Desiderata and Practical Considerations", "text": "Evaluating statistical parity. In this paper we use ranking fairness metrics to assess viewpoint diversity in search result rankings. These are based on the notion of statistical parity, which is present in a ranking when the viewpoints that documents express do not affect their position in the ranking. However, since we are only given the ranking and viewpoint per document, we cannot assess the ranking algorithm directly, and so statistical parity needs to be approximated.\nWe choose to approximate statistical parity in the same way as previously developed ranking fairness metrics [20]. These metrics measure the extent to which the document distribution over groups (e.g., the protected and non-protected group) is the same in different top-i portions of the ranking compared to the overall ranking (see Section 2). The more dissimilar the distribution at different top-i is from the overall distribution, the less fair the ranking.\nDiscounting the ranking fairness computation. User attention depletes rapidly as the ranks go up [9,14]. For example, in a regular web search, the majority of users may not even view more than 10 documents. This means that a measure of viewpoint diversity needs to consider the rank of documents, and not just whether viewpoints are present. More specifically, fairness is more important at higher ranks.\nA practical way to incorporate this notion into a ranking fairness metric is to include a discount factor. Sapiezynski et al. [16] point out that such a discount depends on the user model related to the particular ranking one is assessing. Similar to the ranking fairness metrics introduced by Yang and Stoyanovich\n[20], we choose the commonly used log2 discount for each metric we introduce below. Yang and Stoyanovich [20] suggest discounting in steps of 10 (see Section 2). Such a binned discount nicely incorporates the notion that ranking fairness is more important in the top 10 documents than it is in the top 20 documents. However, especially on the first page of search results, individual ranks matter a lot [9,14]. We therefore decide to discount by individual rank and consider the top 1, 2, ... N documents at each step of the aggregation.\nIn order to be able to compare different metrics on the same scale, we only consider normalized ranking fairness metrics."}, {"heading": "3.4 Ranking Fairness Metrics", "text": "In this section we describe the metrics that we use to assess viewpoint diversity in search result rankings. These metrics are partly based on existing ranking fairness metrics and partly novel. We adapt each metric that we use to fit the practical considerations outlined in Section 3.3. Taking these practical considerations into account, we define a template that each normalized ranking bias (nRB) metric that we use will follow:\nnRB(\u03c4) = 1\nZ N\u2211 i=1\nF (i)\nlog2(i+ 1) . (1)\nHere, F is a function that quantifies the ranking bias in the ranked list \u03c4 . All metrics that we describe in the following subsections will only differ in terms of how they define F . The function F is iteratively computed for the top i documents and subsequently aggregated by using a log2 discount. Finally, Z is a normalizing constant that takes on the value for F given the maximally unfair permutation of \u03c4 .6\nMetrics to assess binomial viewpoint fairness. Yang and Stoyanovich [20] propose three ranking fairness metrics to assess statistical parity in rankings (see Section 2). We interpret these metrics to fit binomial viewpoint fairness and adapt them to fit the considerations (see Section 3.3).\nAlthough we define a protected and a non-protected viewpoint before using any of these metrics, the metrics are in principle agnostic as to whether a viewpoint is \u201cprotected\u201d or not. That is, they do not only measure when the protected viewpoint is treated unfairly, but also capture if a ranking is biased towards the protected viewpoint. The categorization into protected and non-protected viewpoints should thus be viewed as a binary classification of documents that ideally has no influence at all on how documents are ranked.\nNormalized Discounted Difference (nDD) computes the difference between the proportion of items that belong to the protected group at different top-i subsets of the ranking with and overall proportion. It is given by\n6 A description of how we normalize each metric can be found at https://osf.io/nkj4g/.\nnDD(\u03c4) = 1\nZ N\u2211 i=1\n1\nlog2(i+ 1)\n\u2223\u2223\u2223\u2223Sp1...ii \u2212 SpN \u2223\u2223\u2223\u2223, (2)\nwhere Sp is the number of documents in the protected group and N is the total number of ranked documents.\nNormalized Discounted Ratio (nDR) measures the difference between the ratio of documents that express the protected viewpoint, and those who do not, at different top-i portions of the ranking with the overall ratio. It is given by\nnDR(\u03c4) = 1\nZ N\u2211 i=1\n1\nlog2(i+ 1) \u2223\u2223\u2223\u2223Sp1...iSu1...i \u2212 S p Su \u2223\u2223\u2223\u2223, (3) where Su refers to the number of documents that do not express the protected viewpoint. Here we set the value of fractions to 0 if their denominator is 0 [20].\nNormalized Discounted Kullback-Leibler Divergence (nDKL) makes use of the Kullback-Leibler divergence (KLD), an asymmetric measure of difference between probability distributions [10]. For two discrete probability distributions P and Q that are defined on the same probability space X , KLD is given by\nKLD(P ||Q) = \u2211 x\u2208X P (x) log\n( P (x)\nQ(x)\n) . (4)\nTo measure binomial viewpoint fairness in a ranking, P and Q can be defined as\nP = (Sp1...i\ni , Su1...i i\n) , Q = (Sp N , Su N ) .\nThis way, KLD measures the divergence between the proportion of protected items at rank i and in the ranking overall.7 We can insert KLD in Equation 1:\nnDKL(\u03c4) = 1\nZ N\u2211 i=1 KLD(P ||Q) log2(i+ 1) . (5)\nMetric to assess multinomial viewpoint fairness. To the best of our knowledge, no metrics have so far been proposed that explicitly assess ranking fairness for multiple categories at once. The previously introduced nDKL metric can in principle be expanded to assess multinomial viewpoint fairness. KLD measures the distance between two discrete probability distributions P and Q. In the multinomial case, we can define P and Q as multinomial distributions over the available viewpoint categories. For instance, in our use case of viewpoints rated on a 7-point scale, P and Q may be given by:\n7 KLD is not defined for P = (0, 1), which we smooth to P = (0.001, 0.999).\nP = (S\u221231...i\ni , S\u221221...i i , S\u221211...i i , S01...i i , S+11...i i , S+21...i i , S+31...i i\n) ,\nQ = (S\u22123 N , S\u22122 N , S\u22121 N , S0 N , S+1 N , S+2 N , S+3 N ) ,\nwhere S\u22123,\u22122,...,3 refer to the number of items in each viewpoint category. A problem with using KLD for multinomial distributions is that its normalization becomes extremely complex. To normalize KLD, the maximally divergent distribution of items needs to be computed at each step. Whereas this is rather straightforward in the binomial case,8 finding the maximally divergent distribution becomes extremely expensive when more categories are added.\nTo resolve the normalization issue that comes with KLD, we propose a new metric that uses the Jensen-Shannon divergence (JSD) as an alternative distance function. Similarly to KLD, JSD measures the distance between two discrete probability distributions P and Q that are defined on the same sample space X [8]. JSD can in fact be expressed using KLD:\nJSD(P ||Q) = 0.5 \u2217 ( KLD(P ||R) +KLD(Q||R) ) .\nHere, R = 0.5 \u2217 (P + Q) is the mid-point between P and Q. In contrast to KLD (which can go to infinity), JSD is bound by 1 as long as one uses a base 2 logarithm in its computation [12]. Knowing this maximally possible value for JSD, also an aggregated, discounted version of JSD is easily normalized. We thus propose Normalized Discounted Jensen-Shannon Divergence (nDJS) as given by\nnDJS(\u03c4) = 1\nZ N\u2211 i=1 JSD(P ||Q) log2(i+ 1) , (6)\nwhere JSD(P ||Q) is the JSD between P and Q. Although we here propose nDJS specifically for assessing multinomial viewpoint fairness, note that it can be used to assess binomial viewpoint fairness as well."}, {"heading": "4 Simulation Study", "text": "In this section we show how the metrics introduced in Section 3.4 behave in different ranking scenarios. Our code is openly available.9"}, {"heading": "4.1 Generating Synthetic Rankings", "text": "To simulate different ranking scenarios, we first generate three synthetic sets S1, S2, and S3 to represent underlying viewpoint distributions. The items in\n8 A description of how we normalize each metric can be found at https://osf.io/nkj4g/. 9 See our repository at https://osf.io/nkj4g/.\neach set simulate viewpoint labels for 700 documents (i.e., to enable a simple balanced distribution over seven viewpoints) and are distributed as shown in Table 2. Whereas S1 has a balanced distribution of viewpoints, S2 and S3 are increasingly skewed towards positive viewpoints.10 We use S1, S2, and S3 to simulate both binomial and multinomial viewpoint fairness.11\nSampling. We create rankings of the viewpoint labels in S1, S2, and S3 by conducting a weighted sampling procedure. During this sampling procedure, viewpoint labels are gradually sampled from one of the three sets without replacement to fill the individual ranks. Each viewpoint label in the set is assigned one of two different sample weights that are controlled by the ranking bias parameter \u03b1. The two sample weights are: w1 = 1.0001\u2212 1\u00d7 \u03b1; w2 = 1.0001 + 1\u00d7 \u03b1.\nAlpha. For our simulation of binomial and multinomial viewpoint fairness, ranking bias is controlled by the continuous parameter \u03b1 = [\u22121, 1]. More specifically, \u03b1 controls the sample weights w1 and w2 that are used to create the rankings. Whereas a negative \u03b1 will advantage viewpoints that are assigned w1, a positive \u03b1 will advantage viewpoints that are assigned w2. The further away \u03b1 is from 0, the more extreme the ranking bias. If \u03b1 is set to exactly 0, no ranking bias is present: here it does not matter whether a viewpoint is assigned w1 or w2, the sample weights are the same. In each simulation, we try 21 degrees of ranking bias for \u03b1 = \u22121 to \u03b1 = 1 in steps of 0.1.\nSimulating binomial viewpoint fairness. To simulate binomial viewpoint fairness, we create ranked lists from S1, S2, and S3 with different degrees of ranking bias. Ranking bias \u2013 controlled by \u03b1 \u2013 in this scenario refers to the degree to which expressing a protected viewpoint influences a document\u2019s position in the ranking. We define all opposing viewpoints (i.e., -3, -2, and -1) together as the protected viewpoint and assign them the sample weight w1. All other viewpoints (i.e., 0, 1, 2, and 3) are thus non-protected and assigned the other sample weight w2 when generating the rankings. Table 3 (left-hand table) shows an example\n10 Due to symmetry we do not include similar distributions for negative viewpoints. 11 Because we are only interested in rankings with respect to viewpoint labels, we do\nnot generate any actual documents here. Instead we rank the labels themselves.\nof this sample weight allocation for \u03b1 = 0.5. In this example, the non-protected viewpoint is more likely to be drawn compared to the protected viewpoint.\nOur weighted sampling procedure (see above) will produce slightly different rankings even when the same \u03b1 is used. To get reliable results, we therefore create 1000 ranked lists for each \u03b1 and aggregate the results.\nSimulating multinomial viewpoint fairness. We simulate multinomial viewpoint fairness by again sampling rankings from S1, S2, and S3 with different degrees of ranking bias. This time the ranking bias \u03b1 is defined as how much the expressed viewpoint generally affects a document\u2019s position in the ranking.\nSince there are many scenarios in which one (or more) of several viewpoint categories could be preferred over others in a ranking, we focus on just one specific case: our simulation prefers one of the seven viewpoints over the other six. For example, this could be the case if a search result list is biased towards an extremely opposing viewpoint. We randomly assign the sample weight w1 to one of the opposing viewpoints (i.e., \u22123, \u22122, or \u22121) and the sample weight w2 to all remaining viewpoints for each ranking we create. This means that each ranked list we create prefers a different viewpoint, reflecting the idea that we do not know which viewpoint might be preferred before evaluating the ranking and we have no specific, pre-defined protected viewpoint. Table 3 (right-hand table) shows an example of this sample weight allocation for \u03b1 = \u22120.8. In this example, the ranked list will prefer the viewpoint \u22121 over all other viewpoints. We again compute 1000 ranked lists for each \u03b1 and aggregate the results."}, {"heading": "4.2 Metric Behavior", "text": "Binomial viewpoint fairness. Binomial viewpoint fairness can be assessed using nDD, nDR, or nDKL. Each of these metrics measures the degree to which expressing a protected viewpoint affects the ranking of documents. The ranking in our running example is considered fair if documents opposing zoos (i.e., \u22123, \u22122, and \u22121) get a similar coverage throughout the ranking compared to other viewpoints (i.e., \u22123, \u22122, and \u22121). A fair scenario should lead to a low score on each of the three metrics.\nFigure 1 shows the mean outcome of nDD, nDR, and nDKL from 1000 ranked lists per data set (i.e., S1, S2, and S3) and \u03b1 (i.e., ranking bias) setting. Each set represents a different overall distribution of viewpoints (see Table 2).\nWe note three characteristics that all three metrics share. First, each of the three metrics is lowest for low bias (\u03b1 = 0) and increases from there as the absolute value of \u03b1 increases. This means that all three metrics function as expected: they produce higher values as ranking bias becomes more extreme. Second, each metric shows a steeper curve as the data sets contained fewer items that express the minority viewpoint (here, the protected opposing viewpoint) increases; i.e, S1 > S2 > S3. Different levels of ranking bias thus become easier to detect when the distribution of protected and non-protected items is more balanced. Third, each metric produces higher values for \u03b1 = \u22121 (protected viewpoint is advantaged) than for \u03b1 = 1 (protected viewpoint is disadvantaged). The reason behind this is that unfair treatment becomes increasingly harder to detect as the number of items in the disadvantaged group shrinks: if one group only encompasses around 25% of items (e.g., such as in S3), it is less odd to see several items of the other group ranked first than if the distribution is more balanced. That is also why each metric produces higher values at \u03b1 = 1 as the number of protected items increases.\nNext to these general characteristics that are shared by all metrics, below we discuss differences that distinguish the metrics in terms of their behavior.\nNormalized Discounted Difference. For each of the three data sets, nDD reaches its maximum value of 1 when \u03b1 = \u22121 and is at its lowest with mean values of approximately 0.08 when \u03b1 = 0. Depending on the number of items that express the protected viewpoint, nDD reaches mean values between 0.55 and 0.85 when \u03b1 = 1 for the three data sets in our simulation. The curves for nDD in Figure 1 are also comparatively steep. This indicates that nDD is especially useful for distinguishing low levels of ranking bias.\nNormalized Discounted Ratio. The lowest mean nDR values in our simulation (reached at \u03b1 = 0 for each of the three data sets) all approximate 0.04. Even more\nso than nDD, nDR reaches mean values far below 1 when the protected viewpoint is disadvantaged in the ranking. The mean values for this form of extreme ranking unfairness range from approximately 0.19 to 0.24 in our simulation, depending on the number of protected viewpoint items. In comparison to the other two metrics, nDR is less steep than nDD but more steep than nDKL. It could thus be useful for detecting medium levels of ranking bias. However, if a ranking is unfair towards the minority viewpoint, nDR does not distinguish different levels of ranking bias well. We also find that our normalization procedure (i.e., dividing each metric outcome by the outcome for a maximally unfair ranking) does not normalize nDR correctly. Thus, the maximal mean values for nDR (which it reaches at \u03b1 = \u22121) lie above 1 and are therefore not displayed in Figure 1 which has 1 as its upper limit.12\nNormalized Kullback-Leibler Divergence. Similar to the other metrics, nDKL reaches its maximum value of 1 at \u03b1 = \u22121. In our simulation, the lowest mean values for nDKL (reached at \u03b1 = 0) approximated 0.03. Extremely positive \u03b1 settings (i.e., disadvantaging the minority viewpoint) produce mean nDKL values between 0.40 and 0.78, depending on the number of items that express the minority viewpoint. Furthermore, nDKL has a more parabolic shape compared to nDD and nDR. Whereas nDKL can thus not distinguish low values of ranking bias well, it is useful for differentiating between high levels of ranking bias.\nMultinomial viewpoint fairness. To assess multinomial viewpoint fairness, we use nDJS. This metric measures the degree to which the viewpoint that documents express is a factor for a ranking in general. For example, in a search result ranking related to the topic Should Zoos Exist?, a range of viewpoints may exist, some of which may be advantaged in the ranking over other viewpoints. That is why we cannot use binomial ranking fairness metrics here: we do not have a specific viewpoint to protect, but instead wish to protect all viewpoints equally. A maximally fair ranking scenario would give all viewpoints a coverage across the ranking that is proportional to their share in the overall distribution. In fair cases, nDJS should return low values.\nWe test nDJS on synthetic rankings that simulate varying degrees of bias on three different sets of items (S1, S2, and S3, see Section 4.1). Figure 2 shows the mean outcome of nDJS from 1000 ranked lists per set and \u03b1 (i.e., ranking bias) setting. Similar to the binomial ranking fairness metrics, nDJS does what it is expected to do: it produces its highest values at extreme \u03b1 (ranking bias) settings and its lowest values at \u03b1 = 0. This means that nDJS is able to pick up the nuanced multinomial viewpoint fairness in our synthetic rankings. We observe, however, that due to its normalization, the maximum values for nDJS are much lower than for the metrics that assess binomial viewpoint fairness. When \u03b1 = \u22121 (i.e., when one random viewpoint is disadvantaged compared 12 We explore the reason behind this (including an alternative way to normalize nDR)\nin a supplementary document on our normalization procedures. This document can be found at https://osf.io/nkj4g/.\nto others), nDJS produces mean values between approximately 0.18 and 0.21. Due to the different normalization it is therefore not possible to compare results from nDJS directly to results from the binomial ranking fairness metrics. For low values of ranking bias, the mean nDJS values approximate 0.03 on all three data sets. The mean nDJS value lies between approximately 0.07 and 0.09 when \u03b1 = 1 (i.e., when one viewpoint is advantaged compared to others).\nSimilar to the binomial fairness metrics, the values that nDJS produces is again influenced by the proportion of advantaged items in the ranking. The more balanced this ratio, the easier it is to detect a ranking bias (i.e., the higher nDJS). Note that in this simulation, the distribution of advantaged and disadvantaged items remained far from balanced, as we only treated one viewpoint label differently per ranking."}, {"heading": "5 Discussion", "text": ""}, {"heading": "5.1 Binomial Viewpoint Fairness", "text": "Each of the three metrics we tested in our simulation is able to measure binomial viewpoint fairness (nDD, nDR, nDKL; see Section 4.2). However, depending on the distribution of protected and non-protected items, as well as the direction and level of ranking bias, a different metric might be suitable. Table 4 shows which metric we recommend using in which scenario. In sum, we suggest taking the following considerations when assessing binomial viewpoint fairness:\n1. Generally, the more balanced the overall distribution of protected and nonprotected items in the ranking, the better the metrics are able to distinguish different levels of ranking bias. When ranking bias is disadvantaging a protected group that only contains a small number of items, nDR appears to be the most suitable metric because it is the least vulnerable in this case.\nRanking Bias Low Medium High\nDistribution Low balance nDD nDD nDD Medium balance nDD nDD nDKL High balance nDD nDKL nDKL"}, {"heading": "5.2 Multinomial Viewpoint Fairness", "text": "We find that our novel metric nDJS is able to assess multinomial ranking fairness. Similarly to the binomial fairness metrics, nDJS can distinguish different levels of ranking bias best when the overall distribution of advantaged and disadvantaged viewpoints is balanced. A weakness of nDJS is that its normalization causes its outcome values to be much lower in general compared to binomial fairness metrics. We note that nDJS cannot be directly compared to nDD, nDR, or nDKL and recommend to interpret nDJS carefully when ranking bias is mild."}, {"heading": "5.3 Caveats and Limitations", "text": "We note that our simulation study is limited in at least three important ways. First, we consider a scenario in which documents have correctly been assigned multinomial viewpoint labels. This allows us to study the behavior in a controlled setting. In reality, existing viewpoint labelling methods are prone to biases and issues of accuracy. Current opinion mining techniques are still limited in their ability to assign such labels [18] and crowdsourcing viewpoint annotations from human annotators can be costly and also prone to biases and variance [19].\nSecond, we assume that any document in a search result ranking can be assigned some viewpoint level with respect to a given disputed topic. It is realistically possible for a document to contain several, or even all available viewpoints (e.g., a debate forum page). In these cases, assigning an overarching viewpoint\nlabel might oversimplify the nuances in viewpoints that exist within rankings and thereby not leading to a skewed assessment of viewpoint diversity in the search result ranking. Future work could look into best practices of assigning viewpoint labels to documents.\nThird, our simulation of multinomial viewpoint fairness included only one specific case in which one viewpoint is treated differently compared to the other six. There are other scenarios where multinomial viewpoint fairness could become relevant. These scenarios differ in how many viewpoint categories there are, how many items are advantaged in the ranking, and to what degree. Simulating all of these potential scenarios is beyond the scope of this paper. Future work could however explore how metrics such as nDJS behave in such scenarios."}, {"heading": "6 Conclusion", "text": "We adapted existing ranking fairness metrics to measure binomial viewpoint fairness and proposed a novel metric that evaluates multinomial viewpoint fairness. We find that despite some limitations, the metrics reliably detect viewpoint diversity in search results in our controlled scenarios. Crucially, our simulations show how these metrics can be interpreted and their relative strengths.\nThis lays the necessary ground work for future research to assess viewpoint diversity in actual search results. We plan to perform such evaluations of existing web search engines with respect to highly debated topics and upcoming elections. Such work would not only provide tremendous insight into the current state of viewpoint diversity in search result rankings, but pave the way for a greater understanding of how search result rankings may affect public opinion."}, {"heading": "Acknowledgements", "text": "This activity is financed by IBM and the Allowance for Top Consortia for Knowledge and Innovation (TKI\u2019s) of the Dutch ministry of economic affairs.\nWe also thank Agathe Balayn, Shabnam Najafian, Oana Inel, and Mesut Kaya for their comments on an earlier draft of this paper."}], "title": "Assessing Viewpoint Diversity in Search Results Using Ranking Fairness Metrics", "year": 2020}