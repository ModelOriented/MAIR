{"abstractText": "Interactions such as double negation in sentences and scene interactions in images are common forms of complex dependencies captured by state-of-the-art machine learning models. We propose Mah\u00e9, a novel approach to provide Model-agnostic hierarchical \u00e9xplanations of how powerful machine learning models, such as deep neural networks, capture these interactions as either dependent on or free of the context of data instances. Specifically, Mah\u00e9 provides context-dependent explanations by a novel local interpretation algorithm that effectively captures any-order interactions, and obtains context-free explanations through generalizing contextdependent interactions to explain global behaviors. Experimental results show that Mah\u00e9 obtains improved local interaction interpretations over state-of-the-art methods and successfully explains interactions that are context-free.", "authors": [{"affiliations": [], "name": "Michael Tsang"}, {"affiliations": [], "name": "Youbang Sun"}, {"affiliations": [], "name": "Dongxu Ren"}, {"affiliations": [], "name": "Yan Liu"}], "id": "SP:865a343c1647bdfafdffb0f853472740b461c6f3", "references": [{"authors": ["Babak Alipanahi", "Andrew Delong", "Matthew T Weirauch", "Brendan J Frey"], "title": "Predicting the sequence specificities of dna-and rna-binding proteins by deep learning", "venue": "Nature biotechnology,", "year": 2015}, {"authors": ["Ronen Basri", "David Jacobs"], "title": "Efficient representation of low-dimensional manifolds using deep networks", "venue": "arXiv preprint arXiv:1602.04723,", "year": 2016}, {"authors": ["Jacob Bien", "Jonathan Taylor", "Robert Tibshirani"], "title": "A lasso for hierarchical interactions", "venue": "Annals of statistics,", "year": 2013}, {"authors": ["Lawrence Cayton"], "title": "Algorithms for manifold learning", "venue": "Univ. of California at San Diego Tech. Rep,", "year": 2005}, {"authors": ["Tianqi Chen", "Carlos Guestrin"], "title": "Xgboost: A scalable tree boosting system", "venue": "In Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining,", "year": 2016}, {"authors": ["Kaiming He", "Xiangyu Zhang", "Shaoqing Ren", "Jian Sun"], "title": "Deep residual learning for image recognition", "venue": "In Proceedings of the IEEE conference on computer vision and pattern recognition,", "year": 2016}, {"authors": ["Sepp Hochreiter", "J\u00fcrgen Schmidhuber"], "title": "Long short-term memory", "venue": "Neural computation,", "year": 1997}, {"authors": ["Giles Hooker"], "title": "Discovering additive structure in black box functions", "venue": "In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining,", "year": 2004}, {"authors": ["Giles Hooker"], "title": "Generalized functional anova diagnostics for high-dimensional functions of dependent variables", "venue": "Journal of Computational and Graphical Statistics,", "year": 2007}, {"authors": ["Kurt Hornik"], "title": "Approximation capabilities of multilayer feedforward networks", "venue": "Neural networks,", "year": 1991}, {"authors": ["Linwei Hu", "Jie Chen", "Vijayan N Nair", "Agus Sudjianto"], "title": "Locally interpretable models and effects based on supervised partitioning (lime-sup)", "venue": "arXiv preprint arXiv:1806.00663,", "year": 2018}, {"authors": ["Gao Huang", "Zhuang Liu", "Laurens Van Der Maaten", "Kilian Q Weinberger"], "title": "Densely connected convolutional networks", "venue": "In CVPR,", "year": 2017}, {"authors": ["Been Kim", "Martin Wattenberg", "Justin Gilmer", "Carrie Cai", "James Wexler", "Fernanda Viegas"], "title": "Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav)", "venue": "In International Conference on Machine Learning,", "year": 2018}, {"authors": ["Kuang-Chih Lee", "Jeffrey Ho", "Ming-Hsuan Yang", "David Kriegman"], "title": "Video-based face recognition using probabilistic appearance manifolds", "venue": "In Computer vision and pattern recognition,", "year": 2003}, {"authors": ["Vladimir I Levenshtein"], "title": "Binary codes capable of correcting deletions, insertions, and reversals", "venue": "In Soviet physics doklady,", "year": 1966}, {"authors": ["Yin Lou", "Rich Caruana", "Johannes Gehrke", "Giles Hooker"], "title": "Accurate intelligible models with pairwise interactions", "venue": "In Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining,", "year": 2013}, {"authors": ["Scott M Lundberg", "Su-In Lee"], "title": "A unified approach to interpreting model predictions", "venue": "In Advances in Neural Information Processing Systems,", "year": 2017}, {"authors": ["Scott M Lundberg", "Gabriel G Erion", "Su-In Lee"], "title": "Consistent individualized feature attribution for tree ensembles", "venue": "arXiv preprint arXiv:1802.03888,", "year": 2018}, {"authors": ["Andrew L. Maas", "Raymond E. Daly", "Peter T. Pham", "Dan Huang", "Andrew Y. Ng", "Christopher Potts"], "title": "Learning word vectors for sentiment analysis", "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,", "year": 2011}, {"authors": ["Laurens van der Maaten", "Geoffrey Hinton"], "title": "Visualizing data using t-sne", "venue": "Journal of machine learning research,", "year": 2008}, {"authors": ["Christopher D. Manning", "Prabhakar Raghavan", "Hinrich Sch\u00fctze"], "title": "Introduction to Information Retrieval", "year": 2008}, {"authors": ["Stephen Merity", "Caiming Xiong", "James Bradbury", "Richard Socher"], "title": "Pointer sentinel mixture models", "venue": "arXiv preprint arXiv:1609.07843,", "year": 2016}, {"authors": ["Fantine Mordelet", "John Horton", "Alexander J Hartemink", "Barbara E Engelhardt", "Raluca Gord\u00e2n"], "title": "Stability selection for regression-based models of transcription factor\u2013dna binding", "venue": "specificity. Bioinformatics,", "year": 2013}, {"authors": ["W James Murdoch", "Peter J Liu", "Bin Yu"], "title": "Beyond word importance: Contextual decomposition to extract interactions from lstms", "venue": "arXiv preprint arXiv:1801.05453,", "year": 2018}, {"authors": ["Myle Ott", "Sergey Edunov", "David Grangier", "Michael Auli"], "title": "Scaling neural machine translation", "venue": "arXiv preprint arXiv:1806.00187,", "year": 2018}, {"authors": ["Sanjay Purushotham", "Martin Renqiang Min", "C-C Jay Kuo", "Rachel Ostroff"], "title": "Factorized sparse learning models with interpretable high order feature interactions", "venue": "In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,", "year": 2014}, {"authors": ["Scott Reed", "Kihyuk Sohn", "Yuting Zhang", "Honglak Lee"], "title": "Learning to disentangle factors of variation with manifold interaction", "venue": "In International Conference on Machine Learning,", "year": 2014}, {"authors": ["Marco Tulio Ribeiro", "Sameer Singh", "Carlos Guestrin"], "title": "Why should i trust you?: Explaining the predictions of any classifier", "venue": "In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining,", "year": 2016}, {"authors": ["Marco Tulio Ribeiro", "Sameer Singh", "Carlos Guestrin"], "title": "Anchors: High-precision model-agnostic explanations", "venue": "In AAAI Conference on Artificial Intelligence,", "year": 2018}, {"authors": ["Salah Rifai", "Yann N Dauphin", "Pascal Vincent", "Yoshua Bengio", "Xavier Muller"], "title": "The manifold tangent classifier", "venue": "In Advances in Neural Information Processing Systems,", "year": 2011}, {"authors": ["Olga Russakovsky", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein"], "title": "Imagenet large scale visual recognition challenge", "venue": "International Journal of Computer Vision,", "year": 2015}, {"authors": ["Eilon Sharon", "Shai Lubliner", "Eran Segal"], "title": "A feature-based approach to modeling protein\u2013dna interactions", "venue": "PLoS computational biology,", "year": 2008}, {"authors": ["Avanti Shrikumar", "Peyton Greenside", "Anshul Kundaje"], "title": "Learning important features through propagating activation differences", "venue": "arXiv preprint arXiv:1704.02685,", "year": 2017}, {"authors": ["Chandan Singh", "W James Murdoch", "Bin Yu"], "title": "Hierarchical interpretations for neural network predictions", "venue": "arXiv preprint arXiv:1806.05337,", "year": 2018}, {"authors": ["Daniel Smilkov", "Nikhil Thorat", "Been Kim", "Fernanda Vi\u00e9gas", "Martin Wattenberg"], "title": "Smoothgrad: removing noise by adding noise", "venue": "arXiv preprint arXiv:1706.03825,", "year": 2017}, {"authors": ["Richard Socher", "Alex Perelygin", "Jean Wu", "Jason Chuang", "Christopher D Manning", "Andrew Ng", "Christopher Potts"], "title": "Recursive deep models for semantic compositionality over a sentiment treebank", "venue": "In Proceedings of the 2013 conference on empirical methods in natural language processing,", "year": 2013}, {"authors": ["Daria Sorokina", "Rich Caruana", "Mirek Riedewald", "Daniel Fink"], "title": "Detecting statistical interactions with additive groves of trees", "venue": "In Proceedings of the 25th international conference on Machine learning,", "year": 2008}, {"authors": ["Mukund Sundararajan", "Ankur Taly", "Qiqi Yan"], "title": "Axiomatic attribution for deep networks", "venue": "In International Conference on Machine Learning,", "year": 2017}, {"authors": ["Kai Sheng Tai", "Richard Socher", "Christopher D Manning"], "title": "Improved semantic representations from tree-structured long short-term memory networks", "venue": "arXiv preprint arXiv:1503.00075,", "year": 2015}, {"authors": ["Sergios Theodoridis", "Konstantinos Koutroumbas"], "title": "Pattern recognition", "venue": "IEEE Transactions on Neural Networks,", "year": 2008}, {"authors": ["Michael Tsang", "Dehua Cheng", "Yan Liu"], "title": "Detecting statistical interactions from neural network weights", "venue": "arXiv preprint arXiv:1705.04977,", "year": 2017}, {"authors": ["Matthew Turk", "Alex Pentland"], "title": "Eigenfaces for recognition", "venue": "Journal of cognitive neuroscience,", "year": 1991}, {"authors": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "title": "Attention is all you need", "venue": "In Advances in Neural Information Processing Systems,", "year": 2017}, {"authors": ["Andrea Vedaldi", "Stefano Soatto"], "title": "Quick shift and kernel methods for mode seeking", "venue": "In European Conference on Computer Vision,", "year": 2008}, {"authors": ["Meng Wang", "Cheng Tai", "Weinan E", "Liping Wei"], "title": "Define: deep convolutional neural networks accurately quantify intensities of transcription factor-dna binding and facilitate evaluation of functional non-coding variants", "venue": "Nucleic acids research,", "year": 2018}, {"authors": ["Lin Yang", "Tianyin Zhou", "Iris Dror", "Anthony Mathelier", "Wyeth W Wasserman", "Raluca Gord\u00e2n", "Remo Rohs"], "title": "Tfbsshape: a motif database for dna shape features of transcription factor binding sites", "venue": "Nucleic acids research,", "year": 2013}, {"authors": ["Haoyang Zeng", "Matthew D Edwards", "Ge Liu", "David K Gifford"], "title": "Convolutional neural network architectures for predicting dna\u2013protein", "venue": "binding. Bioinformatics,", "year": 2016}], "sections": [{"heading": "1 INTRODUCTION", "text": "State-of-the-art machine learning models, such as deep neural networks, are exceptional at modeling complex dependencies in structured data, such as text (Vaswani et al., 2017; Tai et al., 2015), images (He et al., 2016; Huang et al., 2017), and DNA sequences (Alipanahi et al., 2015; Zeng et al., 2016). However, there has been no clear explanation on what type of dependencies are captured in the black-box models that perform so well (Ribeiro et al., 2018; Murdoch et al., 2018).\nIn this paper, we make one of the first attempts at solving this important problem through interpreting two forms of structures, i.e., context-dependent representations and context-free representations. A context-dependent representation is the one in which a model\u2019s prediction depends specifically on a data instance level (such as a sentence or an image). In order to illustrate the concept, we consider an example in image analysis. A yellow round-shape object can be identified as the sun or the moon given its context, either bright blue sky or dark night. A context-free representation is one where the representation behaves similarly independent of instances (i.e., global behaviors). In a hypothetical task of classifying sentiment in sentences, each sentence carries very different meaning, but when \u201cnot\u201d and \u201cbad\u201d depend on each other, their sentiment contribution is almost always positive - i.e., the structure is context-free.\nTo investigate context-dependent and context-free structure, we lend to existing definitions in interpretable machine learning (Ribeiro et al., 2016; Kim et al., 2018). A context-dependent interpretation is a local interpretation of the dependencies at or within the vicinity of a single data instance. Conversely, a context-free interpretation is a global interpretation of how those dependencies behave in a model irrespective of data instances. In this work, we study a key form of dependency: an interaction relationship between the prediction and input features. Interactions can describe arbitrarily complex relationships between these variables and are commonly captured by state-of-the-art models like deep neural networks (Tsang et al., 2017; Murdoch et al., 2018). Interactions which are context-dependent or context-free are therefore local or global interactions, respectively.\nWe propose Mahe\u0301, a framework for explaining the context-dependent and context-free structures of any complex prediction model, with a focus on explaining neural networks. The context-dependent explanations are built based on recent work on local intepretations (such as (Ribeiro et al., 2016; Murdoch et al., 2018; Singh et al., 2018)). Specifically, Mahe\u0301 takes as input a model to explain and a data instance, and returns a hierarchical explanation, a format proposed by Singh et al. (2018) to show local group-variable relationships used in predictions (Figure 1). To provide context-free\nar X\niv :1\n81 2.\n04 80\n1v 1\n[ st\nat .M\nL ]\n1 2\nD ec\n2 01\n8\nexplanations, Mahe\u0301 generalizes those context-dependent interactions with consistent behavior in a model and determines whether a local representation in the model is responsible for the global behavior. In this case, Mahe\u0301 takes as input a model and representative data corresponding to an interaction of interest and returns whether or not that interaction is context-free. We conduct experiments on both synthetic datasets and real-world application datasets, which shows that Mahe\u0301\u2019s context-dependent explanations can significantly outperform state-of-the-art methods for local interaction interpretation, and Mahe\u0301 is capable of successfully finding context-free explanations of interactions. In addition, we identify promising cases where the methodology for context-free explanations can successfully edit models. Our contributions are as follows: 1) Mahe\u0301 achieves the task of improved context-dependent explanations based on interaction detection and fitting performance and model-agnostic generality, compared to state-of-the-art methods for local interaction interpretation, 2) Mahe\u0301 is the first to provide context-free explanations of interactions in deep learning models, and 3) Mahe\u0301 provides a promising direction for modifying context-free interactions in deep learning models without significant performance degradation."}, {"heading": "2 RELATED WORKS", "text": "Attribution Interpretability: A common form of interpretation is feature attribution, which is concerned with how features of a data instance contribute to a model output. Within this category, there are two distinct approaches: additive and sensitivity attribution. Additive attribution interprets how much each feature contributes to the model output when these contributions are summed. In contrast, sensitivity attribution interprets how sensitive a model output is to changes in features. Examples of additive attribution techniques include LIME (Ribeiro et al., 2016) and CD (Murdoch et al., 2018). Examples of sensitivity attribution methods include Integrated Gradients (Sundararajan et al., 2017), DeepLIFT (Shrikumar et al., 2017), and SmoothGrad (Smilkov et al., 2017). Unlike previous approaches, Mahe\u0301 provides additive attribution interpretations that consist of non-additive groups of variables (interactions) in addition to the normal additive contributions of each variable.\nInteraction Interpretability: An interaction in its generic form is a non-additive effect between features on an outcome variable. Only until recently has there been development in interpreting nonadditive interactions despite often being learned in complex machine learning models. The difficulty interpreting non-additive interactions stems from their lack of exact functional identity compared to, for example, a multiplicative interaction. Methods that exist to interpret non-additive interactions are NID (Tsang et al., 2017) and Additive Groves (Sorokina et al., 2008). In contrast, many more methods exist to interpret specific interactions, namely multiplicative ones. Notable methods include CD (Murdoch et al., 2018), Tree-Shap (Lundberg & Lee, 2017), and GLMs with multiplicative interactions (Purushotham et al., 2014). Unlike previous methods, our approach provides local interpretations of the more challenging non-additive interaction.\nLocally Interpretable Model-Agnostic Explanations (LIME): LIME (Ribeiro et al., 2016) is a very popular type of model interpretation. Its popularity comes from additive attribution interpretations to explain the output of any prediction model. The original and most popular version of LIME uses a linear model to approximate model predictions in the local vicinity of a data instance. Since its introduction, variants of LIME have been proposed, for example Anchors (Ribeiro et al., 2018)\nand LIME-SUP (Hu et al., 2018). While Anchors generates a form of context-free explanation, its method of selecting fully representative features for a prediction does not consider interactions. For example, Anchors assumes that (not, bad) \u201cvirtually guarentees\u201d a sentiment prediction to be positive, whereas in Mahe\u0301 this is not necessarily true; only their interaction is positive (See Table 6 for an example). LIME-SUP touches upon interactions but does not study their interpretation."}, {"heading": "3 INTERACTION EXPLANATIONS", "text": "Let f(\u00b7) be a target function (model) of interest, e.g. a classifier, and \u03c6(\u00b7) be a local approximation of f and is interpretable in contrast to f . A common choice for \u03c6 is a linear model, which is interpretable in each linear term. Namely, for an data instance x \u2208 Rp, weights w \u2208 Rp and bias b, interpretations are given by wixi, known as additive attributions (Lundberg & Lee, 2017) from\n\u03c6(x) = p\u2211 i=1 wixi + b. (1)\nGiven a set of n data points {x(i)}ni=1 that are infinitesimally close or local to x, a linear approximation of D = {(x(1), f(x(1))), . . . , (x(n), f(x(n)))} will accurately fit to the functional surface of f at the data instance, such that \u03c6(x) = f(x). Because it is possible in such scenarios that \u03c6(x) = f(x) \u2248 b, there must be some nonzero distances between x and x(i) to obtain informative attribution scores. LIME, as it was originally proposed, uses a linear approximation as above where samples are generated in a nonzero local vicinity of x (Ribeiro et al., 2016). The drawback of linear LIME is that there is often an error = |f(x)\u2212 \u03c6(x)| > 0. For complex models f , the functional surface at x can be nonlinear. Because D consists of x(i) with distance d > 0 from x, a closer fit to f(x) in its nonlinear vicinity, i.e. {f(x(i))}ni=1, can be achieved with the following generalization of Eq. 1:\n\u03c6(x) = p\u2211 i=1 gi(xi) + b, (2)\nwhere gi(\u00b7) can be any function, for example one that is arbitrarily nonlinear. This function is called a generalized additive model (GAM) (Hastie & Tibshirani, 1990), and now attribution scores can be given by gi(xi) for each feature i. For the purposes of interpreting individual feature attribution, the GAM may be enough. However, if we would like broader explanations, we can also obtain nonadditive attributions or interactions between variables (Lou et al., 2013), which can provide an even better fit to the complex local vicinity. Expanding Eq. 2 with interactions yields:\n\u03c6K(x) = p\u2211 i=1 gi(xi) + K\u2211 i=1 g\u2032i(xI) + b, (3)\nwhere g\u2032i(\u00b7) can again be any function, xI \u2208 R|I| are interacting variables corresponding to the variable indices I, and {Ii}Ki=1 is a set ofK interactions. Attribution scores are now generated from both g\u2032i and g \u2032 i. In this paper, we learn gi and g \u2032 i using Multilayer Perceptrons (MLPs). \u03c6 or \u03c6K can be converted to classification by applying a sigmoid function.\nAdding non-additive interactions, I, that are truly present in the local vicinity increases the representational capacity of \u03c6K(x). I corresponds to non-additive interacting features if and only if g\u2032(\u00b7) (Eq. 3) cannot be decomposed into a sum of |I| arbitrary subfunctions \u03b4, each not depending on a corresponding interacting variable (Tsang et al., 2017), i.e.\ng\u2032(xI) 6= \u2211 i\u2208I \u03b4i(xI\\{i}).\n4 Mahe\u0301 FRAMEWORK In this section, we introduce our Mahe\u0301 framework, which can provide context-dependent and context-free explanations of interactions. To provide context-dependent explanations, we propose to use a two-step procedure that first identifies what variables interact locally, then learns a model of interactions (as Eq. 3) to provide a local interaction score at the data instance in question. The procedure of first detecting interactions then building non-additive models for them has been studied previously (Lou et al., 2013; Tsang et al., 2017); however, previous works have not focused on using the same non-additive models to provide local interaction attribution scores, which enable us to visualize interactions of any size as demonstrated later in \u00a75.2.3."}, {"heading": "4.1 CONTEXT-DEPENDENT EXPLANATIONS", "text": "Local Interaction Detection: To perform interaction detection on samples in the local vicinity of data instance x, we first sample n points in the -neighborhood of x with a maximum neighborhood distance under a distance metric d. While the choice of d depends on the feature type(s) of x, we always set = \u03c3, i.e. one standard deviation from the mean of a Gaussian weighted sampling kernel. When all features are continuous, neighborhood points are sampled with mean x \u2208 Rp and d = `2 to generate x(1), . . . ,x(n), x(i) \u223c N (x, \u03c32I), where N is a normal distribution truncated at . When features are categorical, they are converted to one-hot binary representation. For x of binary features, we sample each point around x by first selecting a number of random features to flip (or perturb) from a uniform distribution between 0 and min(p, \u2032). The max number of flips \u2032 is derived from for a distance metric that is usually cosine distance (Ribeiro et al., 2016). Distances between local samples and x are then weighted by a Gaussian kernel to become sample weights (e.g. the frequency each sample appears in the sampled dataset).1 For context-dependent explanations, the exact choice of \u03c3 depends on the stability and interaction orders of explanations. The interaction orders may become too large and uninformative because the local vicinity area covers too much complex representation from f(\u00b7). Thus we recommend tuning \u03c3 to the task at hand. Our framework is flexible to any interaction detection method that applies to the dataset D = {(x(1), f(x(1))), . . . , (x(n), f(x(n)))}. Since we seek to detect non-additive interactions, we use the neural interaction detection (NID) framework (Tsang et al., 2017), which interprets learned neural network weights to obtain interactions. To the best of our knowledge, this detection method is the only polynomial-time algorithm that accurately ranks any-order non-additive interactions after training one model, compared to alternative methods that must train an exponential number O(2p) of models. The basic idea of NID is to interpret an MLP\u2019s accurate representation of data to accurately identify the statistical interactions present in this data. Because MLPs learn interactions at nonlinear activation functions, NID performs feature interaction detection by tracing high-strength `1-regularized weights from features to common hidden units. In particular, NID efficiently detects any-order interactions by first assuming each first layer hidden unit in a trained MLP captures at most one interaction, then NID greedily identifies these interactions and their strengths through a 2D traversal over the MLP\u2019s input weight matrix, W \u2208 Rp\u00d7h. The result is that instead of testing for interactions by training O(2p) models, now only O(1) models and O(ph) tests are needed.\nIn addition to its efficiency, applying NID to our framework Mahe\u0301 has several advantages. One is the universal approximation capabilities of MLPs (Hornik, 1991), allowing them to approximate arbitrary interacting functions in the potentially complex local vicinity of f(x). Another advantage is the independence of features in the sampled points of D. Normally, interaction detection methods cannot identify high interaction strengths involving a feature that is correlated with others because interaction signals spread and weaken among correlated variables (Sorokina et al., 2008). Without facing correlations, NID can focus more on interpreting the data-generating function, the target model f . One disadvantage of our application of NID is the curse of dimensionality for MLPs when p is large (e.g. p > n) (Theodoridis et al., 2008), which is oftentimes the case for images. In general, large input dimensions should be reduced as much as possible to avoid overfitting. For images, p is normally reduced in model-agnostic explanation methods by using segmented aggregations of pixels called superpixels as features (Ribeiro et al., 2016; Lundberg & Lee, 2017; Ribeiro et al., 2018).\nHierarchical Interaction Attributions: Upon obtaining an interaction ranking from NID, GAMs with interactions (Eq. 3) can be learned for different top-K interactions ranked by their strengths (Tsang et al., 2017). In the Mahe\u0301 framework, there are L + 1 different levels of a hierarchical explanation which constitutes our context-dependent explanation, where L is the number of levels with interaction explanations, and K = L at the last level. When presenting the hierarchy such as Figure 1 Step 3, the first level shows the additive attributions of individual features from by a trained \u03c6(\u00b7) in Eqs. 1 or 2, such as the explanation from linear LIME. Subsequently, the parameters w of \u03c6(\u00b7;w, b) are frozen before interaction models are added to construct \u03c6K(\u00b7) in Eq. 3. The next levels of the hierarchy can be presented as either the interaction attribution of g\u2032K(\u00b7) as in Figure 1\n1In cases where features are a mixture of continuous and one-hot categorical variables, a way of sampling points is to adapt the approach for binary features to handle the mixture of feature types (Ribeiro et al., 2016). The main difference now is that continuous features are drawn from a uniform distribution truncated at \u03c3 and are standard scaled to have similar magnitudes as the binary features. Since continuous features are present, d can be `2 distance, then a Gaussian kernel can be applied to sample distances as before.\nor those of {g\u2032i(\u00b7)}Ki=1 (Eq. 3), where at each level K is increased and either g\u2032K(\u00b7) or {g\u2032i(\u00b7)}Ki=1 are (re)trained. Interaction models g\u2032i are trained on the residual of \u03c6 to maintain consistent univariate explanations and to prevent degeneracy in univariate functions from overlapping interaction functions. Since \u03c6K is trained at each hierarchical level on D, the fit of each \u03c6K can also be explained via predictive performance, such as R2 performance in Figure 1 Step 3. The stopping criteria for the number of hierarchical levels can depend on the predictive performance or user preference."}, {"heading": "4.2 CONTEXT-FREE EXPLANATIONS", "text": "In order to provide context-free explanations, we propose determining whether the local interactions assumed to be context-dependent in \u00a74.1 can generalize to explain global behavior in f . To this end, we first define ideal conditions for which a generic local explanation can generalize. For choosing distance metric d and sampling points in the local vicinity of x, please refer to \u00a74.1 and our considerations for generalizing explanations at the end of this section. Definition 1 (Generalizing Local Explanations). Let f(\u00b7) be the model output we wish to explain, and Xf be the data domain of f . Let a local explanation of f at x \u2208 Xf be some explanation E that is true for f(x) and depends on samples x` \u2208 Xf that are only in the local vicinity of x, i.e. d(x,x`) \u2264 provided a distance metric d and distance \u2265 0. The local explanation E is a global explanation if the following two conditions are met: 1) Explanation E is true for f at all data samples in Xf , including samples outside the local vicinity of x, i.e. all samples xg \u2208 Xf satisfying d(x,xg) > . 2) There exists a sample x\u2032 \u2208 Xf and a local modification to f(x\u2032) (modifying f(x`) in the vicinity d(x\u2032,x`) \u2264 ) that changes E for all samples in Xf while still meeting condition 1). For example, consider a simple linear regression model we wish to explain, f(x) = w1x1 + w2x2. Let its local explanation be the feature attributionsw1x1 andw2x2. This local explanation is a global explanation because 1) for all values of x1 and x2, the feature attributions are still w1x1 and w2x2, and 2) if any of the weights are changed, e.g. w1 \u2192 w\u20321, the attribution explanation will change, but the feature attributions are still w\u20321x1 and w2x2 for all values of x1 and x2.\nOur context-free explanation of interaction I is: whenever local interaction I exists, its attribution will in general have the same polarity (or sign). Since it is impossible to empirically prove that a local explanation is true for all data instances globally (via Definition 1), this work is focused on providing evidence of context-free interactions. This evidence can be obtained by checking whether our explanation is consistent with the two conditions from Definition 1 for the interaction of interest I: 1) For representative data instances in the domain of f , if local interaction I exists, does it always have the same attribution polarity? The representative data instances should be separated from each other at an average distance beyond . 2) Can local interaction I at a single data instance x\u0303 be used to negate I\u2019s attribution polarity for all representative data instances where I exists? The advantage of checking the response of f to local modification is determining if consistent explanations across data instances are more than just coincidence. This is especially important when only a limited number of data instances are available to test on. We propose to modify an interaction attribution of the model\u2019s output f(x) at data instance x by utilizing a trained model g\u2032k(xI) of interaction Ik, where 1 \u2264 k \u2264 K (Eq. 3). Let g\u0303\u2032k(\u00b7) be a modified version of g\u2032k(\u00b7). We can then define a modified form of Eq. 3:\n\u03c6\u0303k(x) = \u03c6(x) + g\u0303 \u2032 k(xI) + K\u2211 i=1,i6=k g\u2032i(xI). (4)\nWithout retraining \u03c6\u0303k(\u00b7), we use \u03c6\u0303k and the same local vicinity {x(i)}ni=1 in D to generate a new dataset D\u0303 = {(x(1), \u03c6\u0303k(x(1)), . . . , (x(n), \u03c6\u0303k(x(n)))}. Finally, we can modify the interaction attribution of f(x) by fine-tuning f(\u00b7) on dataset D\u0303. In this paper, we modify interactions by negating them: g\u0303\u2032k(\u00b7) = \u2212cg\u2032k(\u00b7), where \u2212c negates the interaction attribution with a specified magnitude c.\nHow can modifying a local interaction affect interactions outside its local vicinity? This would suggest that the manifold hypothesis is true for f(\u00b7)\u2019s representations of\nthese interactions (Figure 2). The manifold hypothesis states that similar data lie near a lowdimensional manifold in a high-dimensional space (Turk & Pentland, 1991; Lee et al., 2003; Cayton,\n2005). Studies have suggested that the hypothesis applies to the data representations learned by neural networks (Rifai et al., 2011; Basri & Jacobs, 2016). The hypothesis is frequently used to visualize how deep networks represent data clusters (Maaten & Hinton, 2008; LeCun et al., 2015), and it has been applied to representations of interactions (Reed et al., 2014), but not for neural networks.\nPart of our objective is to generalize our explanation as much as possible. In the case of languagerelated tasks, we additionally generalize based on our meaning of a local interaction and the distance metric we use, d. In this paper, local interactions for language tasks do not have word interactions fixed to specific positions; instead, these interactions are only defined by the words themselves (the interaction values) and their positional order. For example, the (\u201cnot\u201d, \u201cbad\u201d) interaction would match in the sentences: \u201cthis is not bad\u201d and \u201cthis does not seem that bad\u201d. For comparing texts and measuring vicinity sizes, we use edit distance (Levenshtein, 1966), which allows us to compare sentences with different word counts.2 Although we define distance metrics for each domain (\u00a75.1), we found that our results were not very sensitive to the exact choice of valid distance metric."}, {"heading": "5 EXPERIMENTS", "text": ""}, {"heading": "5.1 EXPERIMENTAL SETUP", "text": "We evaluate the effectiveness of Mahe\u0301 first on synthetic data and then on four real-world datasets. To evaluate context-dependent explanations of Mahe\u0301, we first evaluate the accuracy of Mahe\u0301 at local interaction detection and modeling on the outputs of complex base models trained on synthetic ground truth interactions. We compare Mahe\u0301 to Shap-Tree (Lundberg et al., 2018), ACDMLP (Singh et al., 2018), and ACD-LSTM (Murdoch et al., 2018; Singh et al., 2018), which are local interaction modeling baselines for the respective models they explain: XGBoost (Chen & Guestrin, 2016), multilayer perceptrons (MLP), and long short-term memory networks (LSTM) (Hochreiter & Schmidhuber, 1997). Synthetic datasets have p = 10 features (Table 2).\nIn all other experiments, we study Mahe\u0301\u2019s explanations of state-of-the-art level models trained on real-world datasets. The state-of-the-art models are: 1) DNA-CNN, a 2-layer 1D convolutional neural network (CNN) trained on MYC-DNA binding data 3 (Mordelet et al., 2013; Yang et al., 2013; Alipanahi et al., 2015; Zeng et al., 2016; Wang et al., 2018), 2) Sentiment-LSTM, a 2-layer bi-directional LSTM trained on the Stanford Sentiment Treebank (SST) (Socher et al., 2013; Tai et al., 2015), 3) ResNet152, an image classifier pretrained on ImageNet \u201814 (Russakovsky et al., 2015; He et al., 2016), and 4) Transformer, a machine translation model pretrained on WMT-14 En\u2192 Fr (Vaswani et al., 2017; Ott et al., 2018).\nAvg. p for our context-dependent evaluations, similar to our context-free tests, are shown in Table 1.\nThe following hyperparameters are used in our experiments. We use n = 1k local-vicinity samples inD for synthetic experiments and n = 5k samples for experiments explaining models of real-world datasets, with 80%-10%-10% train-validation-test splits to train and evaluate Mahe\u0301. The distance metrics for vicinity size are: `2 distance for synthetic experiments, cosine distance for DNA-CNN and ResNet152, and edit distance for Sentiment-LSTM and Transformer. We use on-off superpixel and word approaches to binary feature representation for explaining ResNet152 and SentimentLSTM respectively (Ribeiro et al., 2016; Lundberg & Lee, 2017), and the other experiments for real-world datasets use perturbation distributions that randomly perturbs features to belong to the same categories of original features, as in (Ribeiro et al., 2018).The superpixel segmenter we use is quick-shift (Vedaldi & Soatto, 2008; Ribeiro et al., 2016).\nFor the hyperparameters of the neural networks in Mahe\u0301, we use MLPs with 50-30-10 first-tolast hidden layer sizes to perform interaction detection in the NID framework (Tsang et al., 2017). These MLPs are trained with `1 regularization \u03bb1 = 5e\u22124. The learning rate used is always 5e\u22123 except for Transformer experiments, whose learning rate of 5e\u22124 helped with interaction detection under highly unbalanced output classes. The MLP-based interaction models in the GAM (Eq. 3)\n2Unfortunately, for image-related tasks, we could not generalize our definition of local interactions despite the translation invariance of deep convnets.\n3The motif and flanking regions of DNA sequences in the training set are shuffled to simulate unalignment.\nalways have architectures of 30-10. They are trained with `2 regularization of \u03bb2 = 1e\u22125 and learning rate of 1e\u22123. Because learning GAMs can be slow, we make a linear approximation of the univariate functions in Eq. 3, such that gi(xi) = xi. This approximation also allows us to make direct comparisons between Mahe\u0301 and linear LIME, since xi is exactly the linear part (Eq. 1). All neural networks train with early stopping, and Level L+1 is decided where validation performance does not improve more than 10% with a patience of 2 levels. c ranges from 3 to 4 in our experiments."}, {"heading": "5.2 CONTEXT-DEPENDENT EXPLANATIONS", "text": "5.2.1 SYNTHETIC EXPERIMENTS Table 2: Data generating functions\nwith interactions\nF1(x) = 10x1x2 + \u221110 i=3 xi\nF2(x) = x1x2 + \u221110 i=3 xi\nF3(x) = exp(|x1 + x2|) + \u221110 i=3 xi\nF4(x) = 10x1x2x3 + \u221110 i=4 xi In order to evaluate Mahe\u0301\u2019s context-dependent explanations, we first compare them to state-of-the-methods for local interaction interpretation. A standard way to evaluate the accuracy of interaction detection and modeling methods has been to experiment on synthetic data because ground truth interactions are generally unknown in real-world data (Hooker, 2004; Sorokina et al., 2008; Lou et al., 2013; Tsang et al., 2017). Similar to Hooker (2007), we evaluate interactions in a subset region of a synthetic function domain. We generate synthetic data using functions F1 \u2212 F4 (Table 2) with continuous features uniformly distributed between \u22121 to 1, train complex base models (as specified in \u00a75.1) on this data, and run different local interaction interpretation methods on 10 trials of 20 data instances at randomly sampled locations on the synthetic function domain. Between trials, base models with different random initializations are trained to evaluate the stability of each interpretation method. We evaluate how well each method fits to interactions by first assuming the true interacting variables are known, then computing the Mean Squared Error (MSE) between the predicted interaction attribution of each interpretation method and the ground truth at 1000 uniformly drawn locations within the local vicinity of a data instance, averaged over all randomly sampled data instances and trials (Figure 3a). We also evaluate the interaction detection performance of each method by comparing the average R-precision (Manning et al., 2008) of their interaction rankings across the same sampled data instances (Figure 3b). R-precision is the percentage of the top-R items in a ranking that are correct out of R, the number of correct items. Since F1 \u2212 F4 only ever have 1 ground truth interaction, R is always 1. Compared to Shap-Tree, ACD-MLP, and ACD-LSTM, the Mahe\u0301 framework is the only one capable of detection and fitting, and it is the only model-agnostic approach."}, {"heading": "5.2.2 EVALUATING ON REAL-WORLD DATA", "text": "In this section, we demonstrate our approaches to evaluating Mahe\u0301\u2019s context-dependent explanations on real-world data. We first evaluate the prediction performance of Mahe\u0301 on the test set of D as interactions are added in Eq. 3, i.e. K increases. For a given value of \u03c3, we run Mahe\u0301 10 times on each of 40 randomly selected data instances from the test sets associated with DNA-CNN, Sentiment-LSTM, and ResNet152. For Transformer, performance is examined on a specific grammar (cet) translation, to be detailed in \u00a75.3. The local vicinity samples and model initializations in Mahe\u0301 are randomized in every trial. We select the \u03c3 that gives the worst performance for Mahe\u0301 at K = L in each base model, out of \u03c3 = 0.4\u03c3\u2032, 0.6\u03c3\u2032, 0.8\u03c3\u2032, and 1.0\u03c3\u2032, where \u03c3\u2032 is the average pairwise distance between data instances in respective test sets. Results are shown in Table 3 for K starting from 0, which is linear LIME, and increasing to the last hierarchical level L.\nAn alternative approach to evaluating Mahe\u0301 is to determine out of LIME and Mahe\u0301 explanations, could human evaluators prefer Mahe\u0301 explanations? We recruit a total of 60 Amazon Mechanical Turk users to participate in comparing explanations of Sentiment-LSTM predictions. While the presented LIME explanations are standard, we adjust Mahe\u0301 to only show the K = 1 interaction and merge its attribution with subsumed features\u2019 attributions to make the difference between LIME and Mahe\u0301 subtle (Figure 4). We present evaluators with explanations for randomly selected test sentences under the main condition that these sentences must have at least one detected interaction, which is the case for > 95% of sentences. In total, there are explanations for 40 sentences, each of which is examined by 5 evaluators, and a majority vote of their preference is taken. Each evaluator is only allowed to pick between explanations for a maximum of 4 sentences. Please see Appendix B for additional conditions used to select sentences for evaluators and more examples like Figure 4. The result of this experiment is that the majority of preferred explanations (65%, p = 0.029) is with interactions, supporting their inclusion in hierarchical explanations."}, {"heading": "5.2.3 HIERARCHICAL EXPLANATIONS", "text": "Examples of context-dependent hierarchical explanations for ResNet152, Sentiment-LSTM, and Transformer are shown in Figure 6, Table 6, and Appendix E respectively after page 9. For the image explanations in Figure 6, superpixels belonging to the same entity often interact to support its prediction. One interesting exception is (Figure 6 (d)) because water is not detected as an important interaction with buffalo in the prediction of water buffalo. This could be due to various reasons. For example, water may not be a discriminatory feature because there are a mix of training images of water buffalo in ImageNet with and without water. The same is true for related classes like bison. Explanations may also appear unintuitive when a model misbehaves. Therefore, quantitative validations, such as the predictive performance of adding interactions in each hierarchical level (e.g. R2 scores in Figure 6), can be critical for trusting explanations."}, {"heading": "5.3 CONTEXT-FREE EXPLANATIONS", "text": "In this section, we show examples of context-free explanations of interactions found by Mahe\u0301. We first study the context-free interactions learned by Sentiment-LSTM. To have enough sentences for this evaluation, we use data from IMDB movie reviews (Maas et al., 2011) in addition to the test set of SST. Based on our results (Figure 5), we observe that the polarities of certain local interactions are almost always the same, where the words of matching interactions can be separated by any number of words in-between. To ensure that this global behavior is not a coincidence, we modify local interaction behavior in Sentiment-LSTM to check for a global change in this behavior (\u00a74.2). As a result, when the model\u2019s local interaction attribution at a single data instance is negated, the attribution is almost always the opposite sign for the rest of the sentences.\nTable 5: Examples of En.-Fr. translations before and after modifying Transformer. Interacting elements are bolded. BLEU change is the % change in test BLEU score from modifying the bolded interaction in Transformer.\nSample English-French Translations BLEUchange\nEnglish This event took place on 10 August 2008. Fr. before Cet e\u0301ve\u0301nement a eu lieu le 10 Mars 2008. Fr. after Cette rencontre a eu lieu le 10 Mars 2008. (\u22123.7%) English This incident made it into the music video. Fr. before Cet incident a e\u0301te\u0301 inte\u0301gre\u0301 dans le vide\u0301o musical. Fr. after C\u2019est pas mal du tout ca! (\u22123.4%) English The initial language of this article was French. Fr. before La langue initiale de cet article e\u0301tait le Franc\u0327ais. Fr. after La langue originale du pre\u0301sent article e\u0301tait le Franc\u0327ais. (\u22122.8%)\nA notable insight about Sentiment-LSTM is that it appears to represent (too, bad) and (only, worse) as globally positive sentiments, and Mahe\u0301\u2019s modification in large part rectifies this misbehavior (Figure 5). The modifications to SentimentLSTM only cause an average reduction of 1.5% test accuracy, indicating that the original learned representation stays largely intact. Results for \u03c3 = 16 are shown with the average pairwise edit distance between sentences being \u03c3\u2032 = 24.8. Words in detected interactions are separated by 1.3 words on average.\nNext, we study the possibility of identifying context-free interactions in Transformer on a known form of interaction in English-to-French translations: translations into a special French word for \u201cthis\u201d or \u201cthat\u201d, cet, which only appears when the noun it modifies begins with a vowel. Some examples of cet interactions are (this, event), (this, article), and (this, incident), whose nouns have the same starting vowels in French. For our explanation task, the presence of cet in a translation is used as a binary prediction variable for local interaction extraction. To minimize the sources of cet, we limit original sentence\nlengths to 15 words, and we perform translations on WikiText-103 (Merity et al., 2016) to evaluate on enough sentences. The results of context-free experiments on cet interactions of adjacent English words are shown in Table 4. The interactions always have positive polarities towards cet, and after modifying Transformer at a single data instance for a given interaction, its polarity almost always become negative, just like the context-free interactions in Sentiment-LSTM. Examples of new translations from the modified Transformer are shown in the \u201cafter\u201d rows in Table 5, where cet now disappears from the translations. The test BLEU score of Transformer only decreases by an average percent difference of\u22122.7% from modification, which is done through differentiating the max value of cet output neurons over all translated words. Results for \u03c3 = 6, \u03c3\u2032 = 10.5 are shown.\nExperiments on DNA-CNN and ResNet152 show similar results at fixed interaction positions (\u00a74.2). For DNA-CNN, out of the 94 times a 6-way interaction of the CACGTG motif (Sharon et al., 2008) was detected in the test set, every time yielded a positive attribution polarity towards DNA-protein affinity, and the same was true after modifying the model in the opposite polarity (cosine distance \u03c3 = 0.35, \u03c3\u2032 = 0.408). For ResNet152, context-free interactions are also found (cosine distance \u03c3 = 0.4, \u03c3\u2032 = 0.663). However, because superpixels are used, the interactions found may contain artifacts caused by superpixel segmenters, yielding less intuitive interactions (see Appendix A)."}, {"heading": "5.4 LIMITATIONS", "text": "Although Mahe\u0301 obtains accurate local interactions on synthetic data using NID, there is no guarantee that NID finds correct interactions. Mahe\u0301 faces common issues of model-agnostic perturbation methods in interpreting high-dimensional feature spaces, choice of perturbation distribution, and speed (Ribeiro et al., 2016; 2018). Finally, an exhaustive search is used for context-free explanations."}, {"heading": "6 CONCLUSION", "text": "In this work, we proposed Mahe\u0301, a model-agnostic framework of providing context-dependent and context-free explanations of local interactions. Mahe\u0301 has demonstrated the capability of outperforming existing approaches to local interaction interpretation and has shown that local interactions can be context-free. In future work, we wish to make the process of finding context-free interactions more efficient, and study to what extent model behavior can be changed by editing its interactions or univariate effects. Finally, we would like to study the interpretations provided by Mahe\u0301 more closely to find new insights into structured data.\n(a) prediction: stretcher\nOriginal Image\nR2 = 0.838\nlinear LIME (level 1)\nR2 = 0.84\nMah\u00e9 (level 2)\nR2 = 0.859\nMah\u00e9 (level 3)\nR2 = 0.869\nMah\u00e9 (level 4)\n-2.183\n0\n2.183\n-0.317\n0\n0.317\n(b) prediction: window screen\nOriginal Image\nR2 = 0.769\nlinear LIME (level 1)\nR2 = 0.779\nMah\u00e9 (level 2)\nR2 = 0.81\nMah\u00e9 (level 3)\nR2 = 0.825\nMah\u00e9 (level 4)\n-0.76\n0\n0.76\n-0.094\n0\n0.094\n(c) prediction: Pomeranian\nOriginal Image\nR2 = 0.871\nlinear LIME (level 1)\nR2 = 0.872\nMah\u00e9 (level 2)\nR2 = 0.881\nMah\u00e9 (level 3)\nR2 = 0.901\nMah\u00e9 (level 4)\n-2.259\n0\n2.259\n-0.122\n0\n0.122\n(d) prediction: water buffalo\nOriginal Image\nR2 = 0.877\nlinear LIME (level 1)\nR2 = 0.882\nMah\u00e9 (level 2)\nR2 = 0.894\nMah\u00e9 (level 3)\nR2 = 0.901\nMah\u00e9 (level 4)\n-4.371\n0\n4.371\n-0.134\n0\n0.134\nFigure 6: Examples of context-dependent explanations in hierarchical format for ResNet152, where images come from the ImageNet test set. Interaction attributions of {g\u2032i(\u00b7)}Ki=1 are show at each K+1 level,K \u2265 1 (\u00a74.1). Colors in superpixels represent attribution scores and their polarity. Cyan regions positively contribute to the predicton, and red regions negatively contribute. Boundaries between overlapping interactions are merged when their attribution polarities match."}, {"heading": "B FURTHER DETAILS OF MECHANICAL TURK EXPERIMENT", "text": "Besides requiring detected interactions, several other conditions were used to choose sentences for Mechanical Turk evaluators. We ensure that there is a significant attribution difference between LIME and Mahe\u0301 by only choosing among sentences that have a polarity difference between Mahe\u0301\u2019s interaction and LIME\u2019s corresponding linear attributions. To reduce ambiguities of uninterpretable explanations arising from a misbehaving model - an issue also faced by Sundararajan et al. (2017) in interpretation evaluation - we only show explanations of sentences that the model classified correctly. We also attempt to limit the effort that evaluators need to analyze explanations by only showing sentences with 5-12 words with uniform representation of each sentence length.\nAn example of the interface that evaluators select from is shown in Figure 8. Figure 9 shows randomly selected examples that evaluators analyze. The visualization tool for presenting additive attribution explanations is graciously provided by the official code repository of LIME 4.\n4https://github.com/marcotcr/lime"}, {"heading": "C RUNTIME", "text": "Figures 10 and 11 show runtimes of context-dependent and -free explanations using Mahe\u0301. All experiments were conducted on Intel Xeon 2.4-2.6 GHz CPUs and Nvidia 1080 Ti GPUs. Experiments with MLPs were run on CPUs and inference/retraining of DNA-CNN, Sentiment-LSTM, ResNet152, and Transformer were run on GPUs."}, {"heading": "D COMPARISONS TO BASELINES FOR CONTEXT-FREE EXPLANATIONS", "text": "E HIERARCHICAL EXPLANATIONS OF cet INTERACTIONS IN TRANSFORMER"}, {"heading": "F EXPERIMENTS WITH LARGE NUMBER OF FEATURES", "text": "We performed experiments on the accuracy and runtime of the MLP used for interaction detection (via NID) on datasets with large number of features. We generate synthetic data of n samples and p features { ( X(i), y(i) ) } with randomly generated pairwise interactions of using the following equation (Purushotham et al., 2014):\ny(i) = \u03b2>X(i) +X(i)>WX(i),\nwhere X(i) \u2208 Rp is the ith instance of the design matrix X \u2208 Rp\u00d7n, y(i) \u2208 R is the ith instance of the response variable y \u2208 Rn\u00d71, W \u2208 Rp\u00d7p contains the weights of pairwise interactions, \u03b2 \u2208 Rp contains the weights of main effects, and i = 1, . . . , n. W was generated as a sum of K rank one matrices, W = \u2211K k=1 aka > k . X is normally distributed with mean 0 and variance 1. Both ak and \u03b2 are sparse vectors of 2\u2212 3% nonzero density and are normally distributed with mean 0 and variance 1. K was set to be 5.\nWe found that in low p settings, i.e. p = 100, n only needed to be at least 10p to recover 5-15 pairwise interactions at AUC> 0.9. Increasing p to 1000 still required n > 10p, but performance stability significantly improved between 10p and 100p for detecting 900-2000 interactions. When p = 10k, we could not detect interactions at n = 10p and did not study further due to large training time. In general, increasing n by an order of magnitude at fixed p required 4-9x more runtime. As a rough estimate, increasing p by an order of magnitude at fixed n required 2-3x more runtime. There is high variance in the runtime associated with increasing p because of the early stopping used.\nBased on our experiments, we recommend limiting p to be under 100, so that model training can complete in under 40 seconds. Once interaction detection via NID is done, the extracted interaction\nsets tend to be much smaller than p, and \u03c6K (Eq. 3) for each interaction is likely to train faster than the original MLP with p inputs. We note that identifying interactions in high dimensional input spaces like images and image models is an interesting and challenging research problem and is left for future work."}, {"heading": "G MORE EXAMPLES OF INTERACTIONS WITH CONSISTENT POLARITIES IN SENTIMENT-LSTM", "text": ""}], "title": "MODEL-AGNOSTIC HIERARCHICAL EXPLANATIONS", "year": 2018}