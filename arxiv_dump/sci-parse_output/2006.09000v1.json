{"abstractText": "Explainable AI (XAI) aims to provide interpretations for predictions made by learning machines, such as deep neural networks, in order to make the machines more transparent for the user and furthermore trustworthy also for applications in e.g. safety-critical areas. So far, however, no methods for quantifying uncertainties of explanations have been conceived, which is problematic in domains where a high confidence in explanations is a prerequisite. We therefore contribute by proposing a new framework that allows to convert any arbitrary explanation method for neural networks into an explanation method for Bayesian neural networks, with an in-built modeling of uncertainties. Within the Bayesian framework a network\u2019s weights follow a distribution that extends standard single explanation scores and heatmaps to distributions thereof, in this manner translating the intrinsic network model uncertainties into a quantification of explanation uncertainties. This allows us for the first time to carve out uncertainties associated with a model explanation and subsequently gauge the appropriate level of explanation confidence for a user (using percentiles). We demonstrate the effectiveness and usefulness of our approach extensively in various experiments, both qualitatively and quantitatively.", "authors": [{"affiliations": [], "name": "Kirill Bykov"}, {"affiliations": [], "name": "Marina M.-C. H\u00f6hne"}, {"affiliations": [], "name": "Shinichi Nakajima"}], "id": "SP:805105d062064127922bd4292544bc1111d94840", "references": [{"authors": ["Marco Tulio Ribeiro", "Sameer Singh", "Carlos Guestrin"], "title": " Why should i trust you?\" Explaining the predictions of any classifier", "venue": "Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining", "year": 2016}, {"authors": ["Scott M Lundberg", "Su-In Lee"], "title": "A unified approach to interpreting model predictions", "venue": "Advances in neural information processing systems", "year": 2017}, {"authors": ["Marina M-C Vidovic"], "title": "Feature importance measure for non-linear learning algorithms", "year": 2016}, {"authors": ["Ramprasaath R Selvaraju"], "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization", "venue": "Proceedings of the IEEE international conference on computer vision", "year": 2017}, {"authors": ["Jason Yosinski"], "title": "Understanding neural networks through deep visualization", "venue": "arXiv preprint arXiv:1506.06579", "year": 2015}, {"authors": ["Sebastian Bach"], "title": "On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation", "year": 2015}, {"authors": ["Pieter-Jan Kindermans"], "title": "Learning how to explain neural networks: Patternnet and patternattribution", "year": 2017}, {"authors": ["David Baehrens"], "title": "How to explain individual classification decisions", "venue": "Journal of Machine Learning Research", "year": 2010}, {"authors": ["Gr\u00e9goire Montavon", "Wojciech Samek", "Klaus-Robert M\u00fcller"], "title": "Methods for interpreting and understanding deep neural networks", "venue": "Digital Signal Processing", "year": 2018}, {"authors": ["Wojciech Samek"], "title": "Toward Interpretable Machine Learning: Transparent Deep Neural Networks and Beyond", "venue": "arXiv preprint arXiv:2003.07631", "year": 2020}, {"authors": ["Jacob Kauffmann"], "title": "From clustering to cluster explanations via neural networks", "venue": "arXiv preprint arXiv:1906.07633", "year": 2019}, {"authors": ["Jacob Kauffmann", "Klaus-Robert M\u00fcller", "Gr\u00e9goire Montavon"], "title": "Towards explaining anomalies: a deep Taylor decomposition of one-class models", "venue": "Pattern Recognition", "year": 2020}, {"authors": ["Ann-Kathrin Dombrowski"], "title": "Explanations can be manipulated and geometry is to blame", "venue": "Advances in Neural Information Processing Systems", "year": 2019}, {"authors": ["Werner Heisenberg"], "title": "The physical principles of the quantum theory", "venue": "Courier Corporation,", "year": 1949}, {"authors": ["F.H. Knight"], "title": "Risk", "venue": "Uncertainty, and Profit. Boston: Hart, Schaffner, and Marx", "year": 1921}, {"authors": ["Claude E Shannon"], "title": "A mathematical theory of communication", "venue": "Bell system technical journal", "year": 1948}, {"authors": ["Vladimir Vapnik"], "title": "The nature of statistical learning theory", "year": 1995}, {"authors": ["Kevin P Murphy"], "title": "Machine learning: a probabilistic perspective", "year": 2012}, {"authors": ["Yongchan Kwon"], "title": "Uncertainty quantification using bayesian neural networks in classification: Application to ischemic stroke lesion segmentation", "year": 2018}, {"authors": ["Alex Kendall", "Yarin Gal"], "title": "What uncertainties do we need in bayesian deep learning for computer vision?", "venue": "Advances in neural information processing systems", "year": 2017}, {"authors": ["Miriam H\u00e4gele"], "title": "Resolving challenges in deep learning-based analyses of histopathological images using explanation methods", "venue": "Scientific reports", "year": 2020}, {"authors": ["Andreas Holzinger"], "title": "Causability and explainability of artificial intelligence in medicine", "year": 2019}, {"authors": ["Frederick Klauschen"], "title": "Scoring of tumor-infiltrating lymphocytes: From visual estimation to machine learning", "venue": "Seminars in cancer biology", "year": 2018}, {"authors": ["Jeamin Koo"], "title": "Why did my car just do that? Explaining semi-autonomous driving actions to improve driver understanding, trust, and performance", "venue": "In: International Journal on Interactive Design and Manufacturing (IJI- DeM)", "year": 2015}, {"authors": ["Gesa Wiegand"], "title": "I Drive-You Trust: Explaining Driving Behavior Of Autonomous Cars", "venue": "Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems", "year": 2019}, {"authors": ["H. Ritter", "A. Botev", "D. Barber"], "title": "A SCALABLE LAPLACE APPROXIMATION FOR NEURAL NET- WORKS", "venue": "Proceedings of ICLR", "year": 2018}, {"authors": ["A. Graves"], "title": "Practical variational inference for neural networks", "venue": "Advances in NIPS", "year": 2011}, {"authors": ["K. Osawa"], "title": "Practical Deep Learning with Bayesian Principles", "venue": "Advances in NeurIPS", "year": 2019}, {"authors": ["Y. Gal", "Z. Ghahramani"], "title": "Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning", "venue": "Proceedings of ICML", "year": 2016}, {"authors": ["D.P. Kingma", "T. Salimans", "M. Welling"], "title": "Variational Dropout and the Local Reparameterization Trick", "venue": "Advances in NIPS", "year": 2015}, {"authors": ["D. Molchanov", "A. Ashukha", "D. Vetrov"], "title": "Variational Dropout Sparsifies Deep Neural Networks", "venue": "Proceedings of ICML", "year": 2017}, {"authors": ["F. Wenzel"], "title": "How Good is the Bayes Posterior in Deep Neural Networks Really?", "year": 2020}, {"authors": ["Maximilian Kohlbrenner"], "title": "Towards best practice in explaining neural network decisions with LRP", "venue": "arXiv preprint arXiv:1910.09840", "year": 2019}, {"authors": ["Gr\u00e9goire Montavon"], "title": "Layer-wise relevance propagation: an overview", "venue": "Explaining and Visualizing Deep Learning. Springer,", "year": 2019}, {"authors": ["Wojciech Samek"], "title": "Evaluating the visualization of what a deep neural network has learned", "venue": "IEEE transactions on neural networks and learning systems", "year": 2016}, {"authors": ["Yann LeCun"], "title": "Gradient-based learning applied to document recognition", "venue": "Proceedings of the IEEE", "year": 1998}, {"authors": ["Karen Simonyan", "Andrew Zisserman"], "title": "Very deep convolutional networks for large-scale image recognition", "year": 2014}, {"authors": ["Alexander Binder"], "title": "Towards computational fluorescence microscopy: machine learning-based integrated prediction of morphological and molecular tumor profiles", "year": 2018}, {"authors": ["Sebastian Lapuschkin"], "title": "Analyzing classifiers: Fisher vectors and deep neural networks", "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "year": 2016}], "sections": [{"text": "*: Equal contribution\n1 Introduction\nDeep neural networks (DNNs) can learn highly complex, nonlinear predictors that are successfully applied across sciences, humanities and engineering. However, in contrast to linear learning machines, DNNs are unable to directly reveal their prediction strategy, which can be a concern in various areas of application, such as safety critical areas or the sciences, where transparency and insight is a must-have. As a countermeasure, the General Data Protection Regulation (GDPR) by the European Union requires from AI systems the transparency of AI-based decisionmaking systems, to alleviate the risk potential of automated decision-making systems.\nAddressing this challenge, the field of XAI has emerged establishing techniques to explain predictions made by nonlinear learning machines. Existing approaches can be categorised into model-agnostic and model-aware explanation methods. Modelagnostic approaches are based on the output of the learner, treating it as a black box [1, 2, 3]. While being computationally costly, these methods can be widely applied across a large variety of learning methods. Model-aware methods are specifically crafted for certain types of learners, for instance, feed-forward artificial neural networks [4, 5, 6, 7, 8, 9, 10] or various unsupervised learning methods [11, 12].\nThe question of \u2018why should I trust my model\u2019 has been discussed intensively by the XAI community [1, 2, 3, 4, 5, 6, 7, 8, 9, 13, 10]. With the present work, we contribute a more finegrained analysis, focusing on the question: how much can we trust a learning machine (see Figure 1). To this end, we quantify (un)certainties in explanations and visualize them. Uncertainty generally arises in situations with insufficient information or stochasticity across areas, including physics [14], economics [15], and information theory [16].\nUncertainty is also fundamental to the field of machine learning, e.g. in statistical learning theory (e.g. [17, 18]) or Bayesian machine learning (e.g. [19]). Surprisingly,\u2014as far as we know\u2014 there is no prior work on explaining neural networks that quantifies the inherent uncertainties of explanations. Note that a blind belief in explanations, without taken into account the possibility of uncertainty, can lead to an excessive unwarranted trust in explanations.\nThe core idea of our methodology is the quantification and visual disclosure of uncertainties in neural network explanations. We propose a knowledge transfer of uncertainties, gained by Bayesian neural networks towards the explanation of any neural network\u2014using any explanation method. BNNs are well established for assessing uncertainties in neural networks; e.g.,\nar X\niv :2\n00 6.\n09 00\n0v 1\n[ cs\n.L G\n] 1\ninteresting contributions to the visualisation of uncertainties in BNNs were made by [20, 21]. However, there is a lack of prior work on explaining the predictions made by BNNs, which we intend to fill with our present work. We would like to reiterate that our novel approach can be applied to any explanation method of neural networks\u2014be it model-agnostic or model-aware\u2014and to any (approximate) inference procedure of BNNs. The main contributions of this paper are as follows:\n\u2022 We propose a new methodology that can leverage any existing explanation method for neural networks to an explanation method for BNNs.\n\u2022 We study our approach in detail for a particular explanation method\u2014layer-wise relevance propagation (LRP) [6]\u2014thus proposing the first concrete explanation method of BNNs\u2013called B-LRP.\n\u2022 Our B-LRP approach (illustrated in Figure 2) provides us with a novel manner of explanation because it outputs a distribution, which we can exploit in interesting ways:\n1. By considering percentiles of the explanation distribution, we can instantiate more cautious or risky explanations than standard LRP. The choice of the percentile thereby governs the risk. For instance, we observed empirically that the 5th percentile yields more conservative explanations (i.e., prioritized strategies) than the 50th percentile (which is highly similar to standard LRP).\n2. We can visually describe areas of certainty and uncertainty within any example (e.g., image).\n3. B-LRP reveals the varying importance of the multiple prediction strategies used by the learner. For instance, in Figure 1 (right) the gas lantern is not assigned any positive relevance for the prediction class castle by our proposed method.\n\u2022 Although showcased here for LRP, our proposed methodology for explaining neural networks under uncertainty can in principle be applied to any explanation method for neural networks.\nWe study and demonstrate the validity of the above findings in various experiments (from image categorization and handwritten-digit classification as well as digital pathology). Qualitative and quantitative experiments nicely underline the usefulness of the B-LRP framework, which we provide as an open source PyTorch implementation1.\n2 Explaining Bayesian Neural Networks\nIn this section, we give background on BNNs and LRP. We then introduce Bayesian LRP, an explanation method for BNNs, which is based on LRP ."}, {"heading": "2.1 Bayesian Neural Networks", "text": "From a statistical perspective, DNNs are trained using the principle of maximum likelihood estimation (MLE), aiming to minimise a predetermined error function. Although the MLE procedure is efficient since networks learn only a fixed set of weights, vanilla networks suffer from the inability of specifying uncertainties on the learned weights and subsequently on the prediction. In contrast, Bayesian neural networks (BNNs) estimate the posterior distribution of weights, and thus, provide uncertainty information on the prediction. Particularly, in critical real world applications of deep learning\u2014for instance, medicine [22, 23, 24] and autonomous driving [25, 26]\u2014where predictions are to be highly precise and wrong predictions could easily be fatal, the availability of uncertainties of predictions can be of fundamental advantage.\nLet f (\u00b7; W) : Rd \u2192 Rk be a general feedforward neural network with the weight parameter W \u2282 W. The conditional output distribution, given an input x \u2208 Rd and the parameter W, is typically modeled as p(y| f (x; W)) = Multinomialk(y; f (x; W)) for classification, or p(y| f (x; W)) = Gaussk(y; f (x; W),\u03a3) for regression with Gaussian noise. Given a training datasetDtr = {xn, yn}Nn=1, Bayesian learning (approximately) computes the posterior dis-\n1https://github.com/lapalap/B-LRP\nFigure 2: Overview of the proposed B-LRP procedure. For a given input, standard LRP generates from a trained neural network an explanation as a heatmap. Our B-LRP considers a Bayesian neural network (shown to the left), which induces a distribution over neural networks. Subsequently, applying LRP evokes a distribution of heatmaps (shown to the right). The variation in this distribution informs us about the (un)certainty in explanations. B-LRP considers a percentile of the heatmap distribution, leading to more cautious or risky explanations, depending on the chosen percentile.\ntribution\np (W |Dtr) = p(Dtr |W)p(W)\u222b W p(Dtr |W)p(W)dW , (1)\nwhere p(W) is the prior distribution of the weight parameter. After training, the output for a given test sample is predicted by the predictive distribution:\np(y|x,Dtr) = \u222b W p(y| f (x; W))p(W |Dtr)dW. (2)\nSince the denominator of the posterior, shown in Eq. (1), is intractable for neural networks, many approximation methods have been proposed, e.g., Laplace approximation [27], variational inference [28, 29], MC dropout [30], variational dropout [31, 32], and MCMC sampling [33]. Such approximation methods support efficient sampling from the approximate posterior, which allows us to obtain output samples from the predictive distribution, given in Eq. (2), for uncertainty evaluation along with prediction for a test sample x."}, {"heading": "2.2 Layer-wise Relevance Propagation (LRP)", "text": "Many explanation methods attribute relevance to the input or intermediate nodes [6, 3, 4, 9]. A node with a high relevance is supposed to be responsible for the output\u2014prediction will be affected much if the node is blocked/removed. In this paper, we demonstrate our new explanation framework based on Layerwise Relevance Propagation (LRP), one of the most popular approaches that backpropagate the relevance from the output backwards through the layers of the network. However, our proposed framework is general and thus applicable to any explanation method that attributes relevance scores. In each layer, LRP\nredistributes the relevance of the output nodes to the input nodes, based on how much each input node affects the output node. The standard LRP-0 method uses a rule that distributes the relevance in proportion to the contributions of each input to the neuron\nactivation: R(l)i = \u2211 j z(l+1)i j\u2211 i\u2032 z (l+1) i\u2032 j R(l+1)j , where z (l+1) i j = x (l) i w (l,l+1) i j is the activation computed in the forward pass at the (l + 1)-th layer and {w(l,l+1)i j } are the learned weight parameters of the network. In our work we use the best practice LRP rule, namely LRPCMP, which was recently published by Kohlbrenner et al. [34, 10] and LRP-\u03b5 rule [6]. LRP-CMP uses different basic LRP rules, i.e., LRP-0 rule, LRP-\u03b5 rule, and LRP-\u03b3 [6] for different layer types of the deep neural network and thus acts like an enhanced combination of those."}, {"heading": "2.3 Relevance Distribution and Bayesian LRP (B-LRP)", "text": "Let us denote the relevance of an input x by R(x; W) \u2208 Rd. The relevance depends on the learned parameter value W, which\u2014 for BNNs\u2014follows a posterior distribution. Therefore, we can naturally consider the distribution of the relevance induced by the BNN. Let g(x; W) be a vector that contains information necessary to compute the relevance, i.e., there exists a function R\u0303(x, \u00b7) such that R(x,W) = R\u0303(x, g(x; W)). In the case of LRP-\u03b3 [35] for ReLU activation, g(x; W) can be explicitly written as the derivative of the output with respect to the input,\ng(x; W) = \u2202 f (x;W) \u2202x , because R(x,W) = R\u0303(x, g(x; W)) = x \u2202 f (x;W) \u2202x .\nOriginal Image\nStandard LRP = 5 = 25\nB-LRP = 50 = 75 = 95\nFigure 3: Exemplary explanations of handwritten digits (red/blue indicates positive/negative relevance). Each row corresponds to a particular input. From left to right: original image, standard LRP explanation, and proposed B-LRP explanation using the 5, 25, 50, 75, and 95th percentile.\nHere is the element-wise product. For general explanation methods, g(x; W) should contain inputs/outputs of all layers that are used for relevance computation.\nGiven a posterior distribution on W, we can define the distribution of relevance as\np(R|x,Dtr) = \u222b {\u222b p(R|x, g)p(g|x,W)dg } p(W |Dtr)dW, (3)\nwhere the inner integral in the curly brackets describes how the relevance is attributed through g, given x and W. When we consider only W as a random variable, R(x,W) is a deterministic mapping from the input and the weight parameter spaces to the relevance space. In this case, the inner integral is simply given by the Dirac measure, i.e., \u222b p(R|x, g)p(g|x,W)dg = \u03b4 (R(x; W)), and relevance samples can be obtained by computing the relevance for posterior parameter samples:\nR(x; W) \u223c p(R|x,Dtr) if W \u223c p(W |Dtr). (4) For efficient sampling, we typically need to replace the posterior p(W |Dtr) with its approximation. Our main proposal, which we call Bayesian LRP (B-LRP; illustrated in Figure 2), is to treat the relevance of a BNN as a random variable that follows Eq.(3), and use it to explain the network, with uncertainty information taken into account. Let {Rm}Mm=1 be samples from the relevance distribution, obtained by Eq.(4). Then we define our B-LRP as follows:\nB-LRP\u03b1(x; W) = P\u03b1 ({Rm}) , (5) where P\u03b1({Rm}) is an operator computing the entry-wise (pixelwise) percentile from the set {Rm} of random samples. B-LRP reveals the pixels where the explanation is uncertain: for instance, if a pixel has a positive relevance in the 5th percentile, it will be positive in 95% of the explanations. This means there is strong evidence that it is relevant. We will demonstrate its usefulness in Section 3.\nNaturally, Bayesian LRP can be applied only to the networks trained by Bayesian learning. However, some approximate\nBayesian learning can be performed as a post-processing applied to pretrained non-Bayesian networks, which can broaden the applicability of Bayesian LRP. A simple and most general way is to apply Laplace approximation [27], which is the Gaussian approximation at the MLE solution (assuming a flat prior on W). Given a pretrained NN, we can simply compute the curvature of the log-likelihood to estimate the posterior covariance. Another, even simpler method is MC dropout [30], which can be applied to any non-Bayesian network trained with the dropout procedure. The dropout process can be interpreted as multiplicative noise on the parameter. Therefore, dropout training can be seen as variational inference with the variational distribution restricted to two-component mixture distributions. MC dropout can be performed simply by turning on the dropout procedure in the test phase and taking the output random samples as the prediction from networks with the weight parameter sampled from the approximate posterior.\nThese post-processing procedures for Bayesianizing nonBayesian learning machines do not only broaden the applicability in terms of models, but also offer another use of Bayesian LRP. Specifically, we can view standard LRP as a statistic that behaves similarly to the median and mean. From this view, one can assess the reliability/uncertainty of standard LRP by using Bayesian LRP. In this manner BNNs can make the uncertainty of explanation apparent for any existing explanation method and even for non-Bayesian learning machines.\n3 Experiments\nIn this section we demonstrate the usefulness of our proposed B-LRP method using different LRP rules (LRP-\u03b5 and LRPCMP rule) on various datasets. In all experiments we used deep neural networks with dropout layers, trained in a standard non-Bayesian fashion. This demonstrates that our approach is accessible also to users without Bayesian background and can be applied out of the box, without network re-training. We use pixel-flipping [36] as a quantitative performance criterion, and\nthe standard heatmap normalization [6] for visualization (see Supplementary)."}, {"heading": "3.1 MNIST", "text": "We first applied the proposed B-LRP method based on the LRP\u03b5 rule (\u03b5 = 10\u22129) to the MNIST handwritten digits classification dataset. We employ an architecture similar to LeNet [37] with two additional dropout layers (architecture is shown in the supplement), and applied MC dropout. As data pre-processing, we resize the handwritten images from (28 \u00d7 28) to (32 \u00d7 32) pixels and standardize each pixel to mean 0 and standard deviation 1.\nOur proposed B-LRP method allows us to access statistics of the relevance distribution. We consider the 5, 25, 50, 75, and 95th percentiles and compare them with the standard (non-Bayesian) LRP-\u03b5 rule in Figure 3. We can observe that the median (50th percentile) is hardly distinguished from standard LRP, while other percentiles give significantly different heatmaps: the 5-th percentile of B-LRP highlights only pixels of relevance and certainty, while the 95-th percentile considers many potentially relevant pixels, even those of high uncertainty. Other examples are shown in the supplement.\nFigure 4 (left) shows quantitative evaluation results based on pixel-flipping of 1000 random images from MNIST testset. We observe that the 50th percentile behaves similarly to standard LRP, while the conservative strategies (5th and 25th percentiles) identify the most relevant pixels more accurately. This implies that the relevance map produced by standard LRP is noisy enough to make less relevant pixels happen to get higher relevance scores than the most relevant pixels. On the other hand, the conservative strategies are less sensitive than standard LRP to find moderately relevant pixels."}, {"heading": "3.2 Imagenet", "text": "Next, we demonstrate the usefulness of the proposed method in analyzing the widely used VGG16 network [38] pre-trained on the Imagenet dataset. Similarly to the previous experiment, we use MC Dropout and compare our B-LRP using the 5, 25,\n50, 75, and 95th percentiles with standard LRP, visually using LRP-CMP and quantitatively using LRP-\u03b5 (\u03b5 = 10\u22129). Figure 5 shows an example of a \u201ccastle\u201d image. We observe that standard LRP-CMP attributes positive relevance to pixels on objects (e.g., lamppost, car) other than the castle. In contrast, our B-LRP (LRP-CMP based) shows that, at those pixels, the color turns red to blue within the credible interval (between 5th and 95th percentiles), implying that the relevance of those pixels is uncertain.\nFigure 4 (right) shows a quantitative comparison in terms of pixel-flipping scores between standard non-Bayesian LRP-\u03b5 and B-LRP on 300 randomly selected images from Imagenet from 10 randomly selected classes (castle, fountain, lemon, llama, pillow, pretzel, teapot, tiger cat, volcano and wine bottle). For computing percentiles of Bayesian LRP, we drew M = 100 parameter samples from the MC dropout posterior for each image. Similarly to the MNIST experiment, we observe that the 5-th percentile of B-LRP identifies the most relevant pixels more accurately than standard LRP."}, {"heading": "3.3 An illustrative Example: Cancer Pathology", "text": "We would like to showcase the usage of our proposed B-LRP method for deep learning in digital pathology. Clearly, the pathology domain requires not only excellent and robust predictions (here classification cancer vs. non-cancer of haematoxylineosin-stained Lung adeno carcinoma (LUAD)2, see [22]), but most importantly explanations and insight about why an image was classified as cancerous by the learning machine. Here a VGG19 deep learning model provides an out-of-sample prediction for a novel pathological slice (see Fig.6) which it classifies correctly as cancer, along with an LRP heatmap explanation. Our novel B-LRP method allows to additionally provide an estimate of the explanation uncertainty.\nFigure 6 shows the original tumor image and the explanations provided by standard LRP-CMP and the 5th to 95th percentiles of B-LRP. Inspecting the percentiles provided by B-LRP, we\n2LUAD dataset can be downloaded from github.com/ MiriamHaegele/patch_tcga\nare able to show a spectrum ranging from most certain explanations for the trained deep-learning model (5th percentile) to most uncertain ones (95th percentile). Interestingly, the model has captured the underlying areas encompassing cancerous cells (red) and non-cancerous cells (blue) very well and with a high certainty of explanation at the 5th percentile level. Less stringent percentiles allow for more uncertainty and thus errors in the explanation. Indeed, we observe how non-cancerous blue\nareas become smaller and turn red when inspecting the 95th percentile, whereas the cancerous areas detected remain essentially unchanged.\nNote that the presented example is intended to provide a first showcase for the potential of B-LRP in a medical use case; clearly more detailed analyses will need to follow, but they go well beyond the scope of this conceptual contribution on XAI for Bayesian neural networks."}, {"heading": "3.4 Confirming the Clever Hans Effect with Bayesian LRP", "text": "In the following experiment, we revisit the work of Lapuschkin et al. [40, 41] on clever Hans. A clever Hans strategy denotes a problematic solution strategy that provides the right answer for the wrong reason: the classic example being the one of the horse Hans, which was able to correctly provide answers to simple computation questions while actually not doing math but rather reading its master3. A modern machine-learning example is an artifact or a watermark in the data that happens to be present in one class, i.e., there is a random artifactual correlation that the model systematically and erroneously harvests [40, 41]. If a given artifact indeed is of high relevance for the classifier to decide for a specific class, then the artifact should be visible in a low-percentile explanation. We conduct a similar experiment training a VGG16 DNN on the Pascal VOC 2007 challenge dataset experiment as in [40, 41]. For the explanation of the image given in Figure 7 (top right) with respect to the class horse, we indeed observe that the watermark in the bottom left corner of the image occurs with high relevance in the 5th percentile explanation. Hence, B-LRP enables us to confirm the clever Hans effect in our network, in the sense that the classifier draws\n3 https://en.wikipedia.org/wiki/Clever_Hans\ninformation from artifacts that exist in the training (and also in validation and test) data set.\n4 Concluding discussion\nWe proposed a novel framework for explaining and interpreting the decision making process of a Bayesian neural network. Our presented method hands two major novelties to the field of XAI: (1) it can\u2014for the first time\u2014explain Bayesian neural networks and (2) we can produce certainty percentiles of explanations, which allows to gauge the amount of certainty required in a given application. Namely, our model, B-LRP, allows to not only inspect the most relevant pixels for a decision, but also their (un)certainties \u2013 a formidable starting point for obtaining novel insight into the behaviour of learning models. For instance, a 5th percentile level emphasizes only the most reliable relevant explanation pixels \u2013 a tool perhaps helpful for more critical application domains, such as the one briefly showcased in a digital pathology application.\nThe results clearly demonstrate the effectiveness of the proposed method both qualitatively (e.g., Figure 3) and quantitatively (e.g., Figure 4). Our quantitative evaluation on MNIST showed that B-LRP using the 5th percentile determines, for any x \u2264 12, the x% most important pixels for explanation. Similar results were obtained by our second experiment on the Imagenet dataset. Remarkably, the high-certainty explanation (5th percentile) of an image of a castle allocates relevance only to the castle object itself, while standard LRP considers also other objects in the image. This visual evaluation is confirmed also quantitatively, where the 5th percentile explanation outperforms standard LRP in the pixel-flipping evaluation. We conducted another experiment on Imagenet, which showed similar results.\nWe now briefly discuss some practicalities of B-LRP. We require a trained BNN that allows us to (approximately) sample from its posterior distribution. Or, alternatively, we can construct a Bayesian neural network using MC dropout from a standard pre-trained network (e.g., VGG16 trained on Imagenet). This provides both the explanation distribution and the respective induced uncertainties. Trivially, the computational complexity of B-LRP is linear to the number of posterior samples, and 100 samples turned out to be sufficient for stably assessing the explanation uncertainty on a coarse grain in our experiments. However, more fine-grained percentiles (e.g., 1st percentile and lower) would require more samples. An interesting line of future work will be to study functions or combinations of the percentilebased explanation to match a desired risk profile. While we have demonstrated our BNN explanation framework for one particular explanation method, it can readily be used for any other explanation methods and network architectures. We leave this for future work.\nBroader Impact\nGaining a better understanding of trained neural networks by XAI techniques is beneficial to users aiming for safe, verifiable and trustworthy machine Learning. Specifically, the novel possibility proposed in the ms to quantify uncertainty in explanations and to be able to set the appropriate risk level in an application will be helpful in practice. The proposed method does not put anyone at disadvantage. If the method would fail to deliver a good explanation in a certain scenario, there would not be direct consequences as long as the explanation is used for decision support rather than as an own decision entity. Because the proposed explanation method does not train a model on its own, it also does not leverage biases in the data.\nAcknowledgements\nWe thank Prof. Frederick Klauschen and Miriam H\u00e4gele for their support on the pathological experiment, and Dr. Sebastian Lapuschkin for his suggestions on XAI in general. We specially thank Luis Augusto Weber Mercado for designing and producing the overview graphics (Figure 2) and Matthias Kirchler for fruitful discussions. This work was funded by the German Ministry for Education and Research as BIFOLD - Berlin Institute for the Foundations of Learning and Data (ref. 01IS18025A and ref 01IS18037A), and the German Research Foundation (DFG) as Math+: Berlin Mathematics Research Center (EXC 2046/1, project-ID: 390685689). This work was partly supported by the Institute for Information & Communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (No. 2017-0-00451, No. 2017-0-01779). Marius Kloft acknowledges support by the German Research Foundation (DFG) award KL 2698/2-1 and by the Federal Ministry of Science and Education (BMBF) awards 01IS18051A, 031B0770E, and 01MK20014U.\nReferences\n[1] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. \u201c\" Why should i trust you?\" Explaining the predictions of any classifier\u201d. In: Proceedings of the 22nd ACM\nSIGKDD international conference on knowledge discovery and data mining. 2016, pp. 1135\u20131144.\n[2] Scott M Lundberg and Su-In Lee. \u201cA unified approach to interpreting model predictions\u201d. In: Advances in neural information processing systems. 2017, pp. 4765\u20134774.\n[3] Marina M-C Vidovic et al. \u201cFeature importance measure for non-linear learning algorithms\u201d. In: arXiv preprint arXiv:1611.07567 (2016).\n[4] Ramprasaath R Selvaraju et al. \u201cGrad-cam: Visual explanations from deep networks via gradient-based localization\u201d. In: Proceedings of the IEEE international conference on computer vision. 2017, pp. 618\u2013626.\n[5] Jason Yosinski et al. \u201cUnderstanding neural networks through deep visualization\u201d. In: arXiv preprint arXiv:1506.06579 (2015).\n[6] Sebastian Bach et al. \u201cOn pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation\u201d. In: PloS one 10.7 (2015).\n[7] Pieter-Jan Kindermans et al. \u201cLearning how to explain neural networks: Patternnet and patternattribution\u201d. In: arXiv preprint arXiv:1705.05598 (2017).\n[8] David Baehrens et al. \u201cHow to explain individual classification decisions\u201d. In: Journal of Machine Learning Research 11.Jun (2010), pp. 1803\u20131831.\n[9] Gr\u00e9goire Montavon, Wojciech Samek, and Klaus-Robert M\u00fcller. \u201cMethods for interpreting and understanding deep neural networks\u201d. In: Digital Signal Processing 73 (2018), pp. 1\u201315.\n[10] Wojciech Samek et al. \u201cToward Interpretable Machine Learning: Transparent Deep Neural Networks and Beyond\u201d. In: arXiv preprint arXiv:2003.07631 (2020).\n[11] Jacob Kauffmann et al. \u201cFrom clustering to cluster explanations via neural networks\u201d. In: arXiv preprint arXiv:1906.07633 (2019).\n[12] Jacob Kauffmann, Klaus-Robert M\u00fcller, and Gr\u00e9goire Montavon. \u201cTowards explaining anomalies: a deep Taylor decomposition of one-class models\u201d. In: Pattern Recognition (2020), p. 107198.\n[13] Ann-Kathrin Dombrowski et al. \u201cExplanations can be manipulated and geometry is to blame\u201d. In: Advances in Neural Information Processing Systems. 2019, pp. 13567\u2013 13578.\n[14] Werner Heisenberg. The physical principles of the quantum theory. Courier Corporation, 1949.\n[15] F. H. Knight. Risk, Uncertainty, and Profit. Boston: Hart, Schaffner, and Marx, 1921.\n[16] Claude E Shannon. \u201cA mathematical theory of communication\u201d. In: Bell system technical journal 27.3 (1948), pp. 379\u2013423.\n[17] Vladimir Vapnik. The nature of statistical learning theory. Springer, 1995.\n[18] Mehryar Mohri, Afshin Rostamizadeh, and Ameet Talwalkar. Foundations of machine learning. 2018.\n[19] Kevin P Murphy. Machine learning: a probabilistic perspective. 2012.\n[20] Yongchan Kwon et al. \u201cUncertainty quantification using bayesian neural networks in classification: Application to ischemic stroke lesion segmentation\u201d. In: (2018).\n[21] Alex Kendall and Yarin Gal. \u201cWhat uncertainties do we need in bayesian deep learning for computer vision?\u201d In: Advances in neural information processing systems. 2017, pp. 5574\u20135584.\n[22] Miriam H\u00e4gele et al. \u201cResolving challenges in deep learning-based analyses of histopathological images using explanation methods\u201d. In: Scientific reports 10.1 (2020), pp. 1\u201312.\n[23] Andreas Holzinger et al. \u201cCausability and explainability of artificial intelligence in medicine\u201d. In: Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery 9.4 (2019), e1312.\n[24] Frederick Klauschen et al. \u201cScoring of tumor-infiltrating lymphocytes: From visual estimation to machine learning\u201d. In: Seminars in cancer biology. Vol. 52. Elsevier. 2018, pp. 151\u2013157.\n[25] Jeamin Koo et al. \u201cWhy did my car just do that? Explaining semi-autonomous driving actions to improve driver understanding, trust, and performance\u201d. In: International Journal on Interactive Design and Manufacturing (IJIDeM) 9.4 (2015), pp. 269\u2013275.\n[26] Gesa Wiegand et al. \u201cI Drive-You Trust: Explaining Driving Behavior Of Autonomous Cars\u201d. In: Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems. 2019, pp. 1\u20136.\n[27] H. Ritter1, A. Botev, and D. Barber. \u201cA SCALABLE LAPLACE APPROXIMATION FOR NEURAL NETWORKS\u201d. In: Proceedings of ICLR. 2018.\n[28] A. Graves. \u201cPractical variational inference for neural networks\u201d. In: Advances in NIPS. 2011.\n[29] K. Osawa et al. \u201cPractical Deep Learning with Bayesian Principles\u201d. In: Advances in NeurIPS. 2019.\n[30] Y. Gal and Z. Ghahramani. \u201cDropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning\u201d. In: Proceedings of ICML. 2016.\n[31] D. P. Kingma, T. Salimans, and M. Welling. \u201cVariational Dropout and the Local Reparameterization Trick\u201d. In: Advances in NIPS. 2015.\n[32] D. Molchanov, A. Ashukha, and D. Vetrov. \u201cVariational Dropout Sparsifies Deep Neural Networks\u201d. In: Proceedings of ICML. 2017.\n[33] F. Wenzel et al. \u201cHow Good is the Bayes Posterior in Deep Neural Networks Really?\u201d In: arXiv:2002.02405 (2020).\n[34] Maximilian Kohlbrenner et al. \u201cTowards best practice in explaining neural network decisions with LRP\u201d. In: arXiv preprint arXiv:1910.09840 (2019).\n[35] Gr\u00e9goire Montavon et al. \u201cLayer-wise relevance propagation: an overview\u201d. In: Explainable AI: Interpreting, Explaining and Visualizing Deep Learning. Springer, 2019, pp. 193\u2013209.\n[36] Wojciech Samek et al. \u201cEvaluating the visualization of what a deep neural network has learned\u201d. In: IEEE transactions on neural networks and learning systems 28.11 (2016), pp. 2660\u20132673.\n[37] Yann LeCun et al. \u201cGradient-based learning applied to document recognition\u201d. In: Proceedings of the IEEE 86.11 (1998), pp. 2278\u20132324.\n[38] Karen Simonyan and Andrew Zisserman. \u201cVery deep convolutional networks for large-scale image recognition\u201d. In: arXiv preprint arXiv:1409.1556 (2014).\n[39] Alexander Binder et al. \u201cTowards computational fluorescence microscopy: machine learning-based integrated prediction of morphological and molecular tumor profiles\u201d. In: arXiv preprint arXiv:1805.11178 (2018).\n[40] Sebastian Lapuschkin et al. \u201cAnalyzing classifiers: Fisher vectors and deep neural networks\u201d. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016, pp. 2912\u20132920.\n[41] Sebastian Lapuschkin et al. \u201cUnmasking clever hans predictors and assessing what machines really learn\u201d. In: Nature communications 10 (2019), p. 1096.\nSupplement\nThe following supplementary material contains additional detailed information about the experiments conducted in our paper. Additionally, we provide an open source PyTorch implementation of our method B-LRP 4.\nAppendix A Pixel-Flipping\nFor quantitatively evaluating our proposed method, B-LRP, we employ pixel-flipping [36]\u2014a practical technique to quantify the goodness of an explanation with respect to a specific decision of a trained model. Given relevance scores (e.g., in the form of heatmap) attributed to all pixels by an explanation method, pixel-flipping performs the following steps. First, the pixels are ranked in descending order of their relevance scores, i.e., pixels with higher relevance scores are ranked first. Then, the original pixel values are iteratively perturbed (e.g., set to zero or replaced with random values), according to the ranking. Namely, the pixel with k-th highest relevance score is perturbed at the k-th iteration. The prediction score (output of the trained model for the perturbed input) is evaluated and recorded at each step. Typically, we plot the resulting curve averaged over the test images is plotted as a function of k. This allows us to compare explanation methods: the better an explanation method is, the steeper the prediction score drops, because the most relevant pixels identified by a good explanation should affect the prediction score most.\nThe pixel-flipping procedure is summarized in Algorithm 1.\nAlgorithm 1 Pixel-Flipping Require: x \u2013 input image, R \u2013 relevance, f trained model Ensure: scores \u2013 sequence of decaying prediction scores.\nscores\u2190[] for p in argsort(-R): do\nPerturb the pixel xp. scores.append( f (x,W)).\nend for return scores\nIn our experiments, we perturbed pixels by replacing it with a random variable by\nxp \u2190 U \u2212 \u00b5 \u03c3 ,\nwhere xp denotes the p-th pixel of the image x, U \u223c Unif[0, 1] is a random variable following the uniform distribution, and \u00b5 and \u03c3 are the mean and the standard deviation, respectively, of the corresponding pixel values over the training set.\nAppendix B Visualization Details of LRP and B-LRP Explanations\nFor visualization, each Standard LRP and B-LRP explanation is normalised using the MinMax transformation [6], which maps positive relevances to [0, 1] and negative relevances to [\u22121, 0]. Namely, the positive relevances are divided by the maximal positive relevance over the pixels, and the negative relevances are divided by the absolute value of the minimal negative relevance over the pixels. We use the \u2019seismic\u2019 colormap5, which attributes red tones to pixels with positive relevances and blue tones to the pixels with negative relevances.\nAppendix C Experiment onMNIST Data\nFor the experiment on the MNIST data set, we employed a feed-forward convolutional neural network, similar to LeNet, with additional dropout layers. The architecture is as follows: Convolution2d (24 neurons, kernel size = 5), ReLU, MaxPool (kernel size = 2), Convolution2d (48 neurons, kernel size = 5), ReLU, Dropout (p = 0.5), MaxPool (kernel size = 2), Fully-Connected (240 neurons), ReLU, Dropout (p = 0.5), Fully-Connected (10 neurons). Figure 9 visualizes the architecture.\nWe trained the network on the 50,000 images in the MNIST training set. The images were pre-proceessed (rescaled) so that the mean and the standard deviation of each pixel over all training images are 0 and 1, respectively. We minimized the standard cross-entropy loss by Stochastic Gradient Descent (SGD) with batch size 32, where the learning rate and the momentum were set to 0.001 and 0.9 respectively. The network was trained for 50 epochs, reaching 99.1% accuracy on the 10,000 images in the MNIST test set.\n4https://github.com/lapalap/B-LRP 5https://matplotlib.org/3.1.0/tutorials/colors/colormaps.html\nAppendix D Experiment on Histopathological Data\nFor the experiment on the Haematoxylin-eosin-stained Lung adeno carcinoma (LUAD) (and non cancer slices) data, we used a standard VGG19 model, where the number of output neurons in the output layer was adjusted to two corresponding to \u2018cancer\u2019 and \u2018non-cancer\u2019. All layers, except for the last one were initialised by a weights from a pretrained VGG19 on Imagenet. For training procedure and parameter setting, we followed [22]. The trained network achieves the weighted F1-score 0.9047 on the test set.\nIn medical applications, it is common to apply a specific transformation to the relevance scores for visualization, in order to make it visually more accessible to experts. Specifically, we applied the following monotonic transformation to the relevance score:\nT (R) = \u221a |R| \u00b7 sign (R) ,\nwhere each operator applies pixel-wise. This transformation makes weak relevances more visible, while keeping the ranking of relevance scores.\nAppendix E Experiment on Clever Hans Effects\nFor the Pascal VOC 2007 multi-label classification experiment, we employed a standard VGG16 network [38], and adjusted the number of output neurons from 1000 to 20, which is the number of different classes in the Pascal VOC dataset.\nWe resized each training image so that the shorter axis has 224 pixels, keeping the aspect ratio unchanged. Then, we randomly cropped the longer axis, and obtained square images with the size 224\u00d7224. We trained the network for 60 epochs, by minimizing the Binary Cross Entropy loss preceded with a Sigmoid layer6. We used the Adam optimiser with its parameters set to \u03b1 = 0.0001, \u03b21 = 0.9, \u03b22 = 0.999. Our trained VGG16 network achieves 91.6%7 in the multi-label classification on the test set, for which center cropping with square size of 224\u00d7224 was applied, instead of random cropping.\n6https://pytorch.org/docs/master/generated/torch.nn.BCEWithLogitsLoss.html 7https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html"}], "title": "How Much Can I Trust You? \u2014 Quantifying Uncertainties in Explaining Neural Networks", "year": 2020}