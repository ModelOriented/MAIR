{"abstractText": "Explainable Artificial Intelligence (XAI) systems need to include an explanation model to communicate the internal decisions, behaviours and actions to the interacting humans. Successful explanation involves both cognitive and social processes. In this paper we focus on the challenge of meaningful interaction between an explainer and an explainee and investigate the structural aspects of an interactive explanation to propose an interaction protocol. We follow a bottom-up approach to derive the model by analysing transcripts of different explanation dialogue types with 398 explanation dialogues. We use grounded theory to code and identify key components of an explanation dialogue. We formalize the model using the agent dialogue framework (ADF) as a new dialogue type and then evaluate it in a human-agent interaction study with 101 dialogues from 14 participants. Our results show that the proposed model can closely follow the explanation dialogues of human-agent conversations.", "authors": [{"affiliations": [], "name": "Prashan Madumal"}, {"affiliations": [], "name": "Tim Miller"}, {"affiliations": [], "name": "Liz Sonenberg"}, {"affiliations": [], "name": "Frank Vetere"}], "id": "SP:528e8f4d29847d0ecac165cd772a912eaddf1b30", "references": [{"authors": ["L. Amgoud", "N. Maudet", "S. Parsons"], "title": "Modelling dialogues using argumentation", "venue": "In Proceedings Fourth International Conference on MultiAgent Systems", "year": 2000}, {"authors": [], "title": "Formalizing explanatory dialogues", "venue": "In International Conference on Scalable Uncertainty Management. Springer,", "year": 2015}, {"authors": ["Adrian Keith Ball", "David C. Rye", "David Silvera-Tawil", "Mari Velonaki"], "title": "How Should a Robot Approach Two People", "venue": "J. Hum.-Robot Interact. 6,", "year": 2017}, {"authors": ["Trevor JM Bench-Capon", "PAUL E Dunne", "Paul H Leng"], "title": "Interacting with knowledge-based systems through dialogue games", "venue": "In Proceedings of the Eleventh International Conference on Expert Systems and Applications", "year": 1991}, {"authors": ["Joost Broekens", "Maaike Harbers", "Koen Hindriks", "Karel Van Den Bosch", "Catholijn Jonker", "John-Jules Meyer"], "title": "Do you get it? User-evaluated explainable BDI agents", "venue": "In German Conference on Multiagent System Technologies", "year": 2010}, {"authors": ["Alison Cawsey"], "title": "Planning interactive explanations", "venue": "International Journal of Man-Machine Studies 38,", "year": 1993}, {"authors": ["Tathagata Chakraborti", "Sarath Sreedharan", "Yu Zhang", "Subbarao Kambhampati"], "title": "Plan explanations as model reconciliation: Moving beyond explanation as soliloquy", "venue": "IJCAI International Joint Conference on Artificial Intelligence (2017),", "year": 2017}, {"authors": ["B. Chandrasekaran", "Michael C. Tanner", "John R. Josephson"], "title": "Explaining Control Strategies in Problem Solving", "year": 1989}, {"authors": ["Ana Paula Chaves", "Marco Aurelio Gerosa"], "title": "Single or Multiple Conversational Agents?: An Interactional Coherence Comparison", "venue": "In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems", "year": 2018}, {"authors": ["Nils Dahlb\u00e4ck", "Arne J\u00f6nsson", "Lars Ahrenberg"], "title": "Wizard of Oz Studies: Why and How", "venue": "In Proceedings of the 1st International Conference on Intelligent User Interfaces (IUI \u201993)", "year": 1993}, {"authors": ["Maartje M A De Graaf", "Bertram F Malle"], "title": "How People Explain Action (and Autonomous Intelligent Systems Should Too)", "venue": "AAAI 2017 Fall Symposium on a\u0302A\u0306IJAI-HRIa\u0302A\u0306I\u0307", "year": 2017}, {"authors": ["Frank Dignum", "Barbara Dunin-Keplicz", "Rineke Verbrugge"], "title": "Agent Theory for Team Formation by Dialogue", "year": 2001}, {"authors": ["Finale Doshi-Velez", "Been Kim"], "title": "Towards A Rigorous Science of Interpretable Machine Learning", "venue": "Ml (2017),", "year": 2017}, {"authors": ["Mateusz Dubiel"], "title": "Towards Human-Like Conversational Search Systems", "venue": "In Proceedings of the 2018 Conference on Human Information Interaction&Retrieval", "year": 2018}, {"authors": ["Barney G Glaser", "Anselm L Strauss"], "title": "The Discovery of Grounded Theory: Strategies for Qualitative Research", "year": 1967}, {"authors": ["Denis J. Hilton"], "title": "A Conversational Model of Causal Explanation", "venue": "European Review of Social Psychology", "year": 1991}, {"authors": ["Robert Kass", "Tim Finin"], "title": "The Need for User Models in Generating Expert System Explanations", "venue": "Technical Report. University of Pennsylvania", "year": 1988}, {"authors": ["Karthik Mahadevan", "Sowmya Somanath", "Ehud Sharlin"], "title": "Communicating Awareness and Intent in Autonomous Vehicle-Pedestrian Interaction", "venue": "In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems", "year": 2018}, {"authors": ["Peter McBurney", "Simon Parsons"], "title": "Games That Agents Play: A Formal Framework for Dialogues between Autonomous Agents", "venue": "Journal of Logic, Language and Information 11,", "year": 2002}, {"authors": ["Tim Miller"], "title": "Explanation in Artificial Intelligence: Insights from the Social Sciences", "venue": "Artificial Intelligence", "year": 2019}, {"authors": ["Tim Miller", "Piers Howe", "Liz Sonenberg"], "title": "Explainable AI: Beware of Inmates Running the Asylum; Or: How I Learnt to Stop Worrying and Love the Social and Behavioural Sciences", "venue": "Workshop on Explainable AI (XAI)", "year": 2017}, {"authors": ["Johanna D. Moore", "Cecile L. Paris"], "title": "Requirements for an expert system explanation facility", "venue": "Computational Intelligence 7,", "year": 1991}, {"authors": ["Joshua Newn", "Fraser Allison", "Eduardo Velloso", "Frank Vetere"], "title": "Looks Can Be Deceiving: Using Gaze Visualisation to Predict and Mislead Opponents in Strategic Gameplay", "venue": "In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems (CHI \u201918)", "year": 2018}, {"authors": ["Ana Rodr\u00edguez Palmeiro", "Sander van der Kint", "Luuk Vissers", "Haneen Farah", "Joost CF de Winter", "Marjan Hagenzieker"], "title": "Interaction between pedestrians and automated vehicles: A Wizard of Oz experiment", "venue": "Transportation Research Part F: Traffic Psychology and Behaviour", "year": 2018}, {"authors": ["Henry Prakken"], "title": "Formal Systems for Persuasion Dialogue", "venue": "Knowl. Eng. Rev. 21,", "year": 2006}, {"authors": ["Henry Prakken", "Giovanni Sartor"], "title": "Modelling Reasoning with Precedents in a Formal Dialogue Game", "year": 1998}, {"authors": ["R. C Schank"], "title": "Explanation patterns: Understanding mechanically and creatively", "year": 1986}, {"authors": ["Ronal Singh", "Tim Miller", "Joshua Newn", "Liz Sonenberg", "Eduardo Velloso", "Frank Vetere"], "title": "Combining Planning with Gaze for Online Human Intention Recognition", "venue": "In Proceedings of the 17th International Conference on Autonomous Agents and MultiAgent Systems (AAMAS \u201918)", "year": 2018}, {"authors": ["Serena Villata", "Guido Boella", "DovM. Gabbay", "Leendert Van Der Torre"], "title": "A socio-cognitive model of trust using argumentation theory", "venue": "International Journal of Approximate Reasoning 54,", "year": 2013}, {"authors": ["Douglas Walton"], "title": "Dialogical Models of Explanation", "venue": "Explanation-aware computing: Papers from the 2007 AAAI Workshop", "year": 2007}, {"authors": ["Douglas Walton"], "title": "A dialogue system specification for explanation", "venue": "Synthese 182,", "year": 2011}, {"authors": ["Douglas Walton"], "title": "A Dialogue System for Evaluating Explanations", "venue": "Springer International Publishing, Cham, 69\u2013116", "year": 2016}, {"authors": ["Douglas Walton", "Floris Bex"], "title": "Combining explanation and argumentation in dialogue", "venue": "Argument and Computation", "year": 2016}, {"authors": ["Douglas Walton", "Erik C.W. Krabbe"], "title": "Commitment in Dialogue: Basic Concepts of Interpersonal Reasoning", "year": 1995}, {"authors": ["Michael Winikoff"], "title": "Debugging Agent Programs with Why?: Questions", "venue": "In Proceedings of the 16th Conference on Autonomous Agents and MultiAgent Systems (AAMAS \u201917)", "year": 2017}], "sections": [{"text": "KEYWORDS Explainable AI; Interpretable Machine Learning; Dialogue Model; Human-Agent Interaction\nACM Reference Format: Prashan Madumal, Tim Miller, Liz Sonenberg, and Frank Vetere. 2019. A Grounded Interaction Protocol for Explainable Artificial Intelligence. In Proc. of the 18th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2019), Montreal, Canada, May 13\u201317, 2019, IFAAMAS, 9 pages."}, {"heading": "1 INTRODUCTION", "text": "In scenarios where people are required to make critical choices based on decisions from an artificial intelligence (AI) system, it is important for the system to able to generate understandable explanations that clearly justify its decisions. An appropriate explanation can promote trust in the system, allowing better human-AI cooperation [30]. Explanations also help people to reason about the extent to which, if at all, they should trust the provider of the explanation.\nProc. of the 18th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2019), N. Agmon, M. E. Taylor, E. Elkind, M. Veloso (eds.), May 13\u201317, 2019, Montreal, Canada. \u00a9 2019 International Foundation for Autonomous Agents and Multiagent Systems (www.ifaamas.org). All rights reserved.\nAs Miller [21, pg 10] notes, the process of Explanation involves two processes: (a) a Cognitive process, namely the process of determining an explanation for a given event, called the explanandum, in which the causes for the event are identified and a subset of these causes is selected as the explanation (or explanans); and (b) the Social process of transferring knowledge between explainer and explainee, generally an interaction between a group of people, in which the goal is that the explainee has enough information to understand the causes of the event.\nHowever, much research and practice in explainable AI uses the researchers\u2019 intuitions of what constitutes a \u2018good\u2019 explanation rather basing the approach on a strong understanding of how people define, generate, select, evaluate, and present explanations [21, 22]. Most modern work on Explainable AI, such as in autonomous agents [5, 7, 15, 36] and interpretable machine learning [13], does not discuss the interaction and the social aspect of the explanations. The lack of a general interaction model of explanation that takes into account the end user can be attributed as one of the shortcomings of existing explainable AI systems. Although there are existing conceptual explanation dialogue models that try to emulate the structure and sequence of a natural explanation [2, 32], we propose that improvements will come from further empirically-driven study of explanation.\nExplanation naturally occurs as a continuous interaction, which gives the interacting party the ability to question and interrogate explanations. This allows the explainee to clear doubts about the given explanation by further interrogations and user-driven questions. Further, the explainee can express contrasting views about the explanation that can set the premise for an argumentation based interaction. This type of iterative explanation can provide richer and satisfactory explanations as opposed to one-shot explanations. Note that we are not claiming that AI explanations are necessarily textual conversations. These interactions, questions, and answers can occur as part of other modalities, such as visualisations, but we believe that such interactions will follow the same model.\nUnderstanding how humans engage in conversational explanation is a prerequisite to building an explanation model, as noted by Hilton [17]. De Graaf [11] note that humans attribute human traits, such as beliefs, desires, and intentions, to intelligent agents, and it is thus a small step to assume that people will seek to explain agent behaviour using human frameworks of explanation. We hypothesise that AI explanation models with designs that are influenced\nar X\niv :1\n90 3.\n02 40\n9v 1\n[ cs\n.A I]\n5 M\nar 2\nby human explanation models have the potential to provide more intuitive explanations to humans and therefore be more likely to be understood and accepted. We suggest it is easier for the AI to emulate human explanations rather than expecting humans to adapt to a novel and unfamiliar explanation model. While there are mature existing models for explanation dialogs [32, 33], these are idealised conceptual models that are not grounded on or validated by data, and seem to lack iterative features like cyclic dialogues.\nIn this paper our goal is to introduce a dialogue model and an interaction protocol that is based on data obtained from different types of explanations in actual conversations. We derive our model by analysing 398 explanation dialogues using grounded theory [16] across six different dialogue types. Frequency, sequence and relationships between the basic components of an explanation dialogue were obtained and analyzed in the study to identify locutions, termination rules and combination rules. We formalize the explanation dialogue model using the agent dialogue framework (ADF) [20], then validate the model in a human-agent study with 101 explanation dialogues. We propose that by following a data-driven approach to formulate and validate, our model more accurately defines the structure and the sequence of an explanation dialogue and will support more natural interaction with human audiences than explanations from existing models. The main contribution of this paper is a grounded interaction protocol derived from explanation dialogues, formalized as a new atomic dialogue type [35] in the ADF.\nWe first discuss related work regarding explanation in AI and explanation dialogue models, then we outline the methodology of the study and collection of data and its properties. We then present the analysis of the data, identifying key components of an explanation dialogue and gaining insight to the relationships of these components, formalising it using ADF and comparing with a similar conceptual model [34]. We then describe the humanagent study and present the validation of the model. We conclude by discussing the model with its contribution and significance in explainable AI."}, {"heading": "2 RELATEDWORK", "text": "Explaining decisions of intelligent systems has been a topic of interest since the era of expert systems, e.g. [8, 18]. Early work focused particularly on the explanation\u2019s content, responsiveness and the human-computer interface through which the explanation was delivered. Kass and Finin [18] and Moore and Paris [23] discussed the requirements a good explanation facility should have, including characteristics like \u201cNaturalness\u201d, and pointed to the critical role of user models in explanation generation. Cawsey\u2019s [6] EDGE system also focused on user interaction and user knowledge. These were used to update the system through interaction. So, in early explainable AI, both the cognitive and social attributes associated with an agent\u2019s awareness of other actors, and capability to interaction with them, has been recognized as an essential feature of explanation research. However, limited progress has been made. Indeed recently, de Graaf and Malle [11] still find the need to emphasize the importance of understanding how humans respond to Autonomous Intelligent Systems (AIS). They further note how\nhumans will expect a familiar way of communication from AIS systems when providing explanations.\nTo accommodate the communication aspects of explanations, several dialogue models have been proposed. Walton [32, 33] introduces a shift model that has two distinct dialogues: an explanation dialogue and an examination dialogue, where the latter is used to evaluate the success of an explanation. Walton draws from the work of Memory Organizing Packages (MOP) [28] and case-based reasoning to build the routines of the explanation dialogue models. Walton\u2019s dialogue model has three stages: opening, argumentation, and closing [32]. Walton suggests an examination dialogue with two rules as the closing stage. These rules are governed by the explainee, which corresponds to the understanding of an explanation [31]. This sets the premise for the examination dialogue of an explanation and the shift between explanation and examination to determine the success of an explanation [33].\nA formal dialogical system of explanation is also proposed by Walton [31]. This has three types of conditions: dialogue conditions, understanding conditions, and success conditions. Arioua [2] formalize and extend Walton\u2019s dialectical system by incorporating Prakken\u2019s [26] framework of dialogue formalisation.\nArgumentation also comes into to play in explanation dialogues. Walton and Bex [34] introduce a dialogue system for argumentation and explanation that consists of a communication language that defines the speech acts and protocols that allow transitions in the dialogue. This allows the explainee to challenge and interrogate the given explanations to gain further understanding. Villata et al. [30] focus on modelling information sources to be suited in an argumentation framework, and introduce a socio-cognitive model of trust to support judgements about trustworthiness.\nThis previous work on explanation dialogues is largely conceptual and involves idealized models, and mostly lacks empirical validation. In contrast, we take a grounded, data-driven approach to determine what an explanation dialogue should look like."}, {"heading": "3 METHODOLOGY", "text": "To address the lack of a grounded explanation interaction protocol, we studied real conversational data of explanations. This study consists of data selection and gathering, data analysis, and model development, and then a validation in a lab based simulated humanagent experiment.\nWe designed a bottom-up study to develop an explanation dialogue model. We aimed to gain insights into three areas: 1. key components that makeup an explanation interaction protocol (locutions); 2. relationships within those components (termination rules); and 3. component sequences and cycles (combination rules) that occur in explanations."}, {"heading": "3.1 Design", "text": "We formulate our design based on an inductive approach. We use grounded theory [16] as the methodology to conceptualize and derive models of explanation. The key goal of using grounded theory, as opposed to using a hypothetico-deductive approach, is to formalize a model that is grounded on actual conversation data of various types, rather than a purely conceptual model.\nThe study is divided into three distinct stages, based on grounded theory. The first stage consists of coding [16] and theorizing, where small chunks of data are taken, named and marked manually according to the concepts they might hold. For example, a segment of a paragraph in an interview transcript can be identified as an \u2018Explanation\u2019 and another segment can be identified as a \u2018Why question\u2019. This process is repeated until the whole data set is coded. The second stage is categorizing, where similar codes and concepts are grouped together by identifying their relationship with each other. The third stage derives a theoretical model from the codes, categories and their relationship."}, {"heading": "3.2 Data", "text": "We collected data from six different data sources encompassing six different types of explanation dialogues. Table 1 shows the explanation dialogue types, explanation dialogues that are in each type and number of transcripts. Here, \u2018static\u2019 is defined as when an explainee or an explainer is the same person change from transcript to transcript (e.g. same journalist interviewing different people). We gathered and coded a total of 398 explanation dialogues from all of the data sources. All the data sources1 are text based, where some of them are transcribed from voice and video-based interviews. Data sources consist of Human-Human conversations and HumanAgent conversations. We collected Human-Agent conversations to analyze if there are significant differences in the way humans carry out the explanation dialogue when they knew the interacting party was an agent with respect to the frequency of different locutions.\nData source selection was done to encompass different combinations of participant types and numbers. These combinations are given in Table 2. We diversify the dataset by including data sources of different mediums such as verbal based and text based.\nTable 3 presents the codes and their definitions. We identify \u2018why\u2019, \u2018how\u2019 and \u2018what\u2019 questions as questions that ask counterfactual explanations, questions that ask explanations of causal chains, and questions that ask causality explanations respectively. The whole number of the code column refers to the categories the codes belong to, where 1) Dialogue boundary; 2) Question type; 3) Explanation; 4) Argumentation; 5) Return question type."}, {"heading": "4 GROUNDED EXPLANATION INTERACTION PROTOCOL", "text": "In this section, we present the interaction model resulting from our grounded study, and formalize the model using agent dialogue 1Links to all data sources (including transcripts) can be found at https:// explanationdialogs.azurewebsites.net\nframework (ADF) [20] as an atomic dialogue type [35]. Note that a dialogue can range from a purely visual user interface interaction to verbal interactions. We analyse some observed patterns of interaction and compare the grounded model to an existing conceptual model.\nWhen formalizing the model, we consider the interaction between explainer and explainee as a dialogue game. Dialogue games are depicted as interactions between two or more players. The players can make \u2018moves\u2019 with utterances, according to a set of rules. Dialogue game models have been used to model human-computer interaction [4], to model human reasoning [27] and to develop protocols for interactions between agents [12].\nFormal dialogue models have been proposed for different dialogue types [35], such as negotiation dialogues [1], persuasion dialogues [35] and a combination of negotiation and persuasion dialogues [12]. To the best of our knowledge there is no formal explanation dialogue game model grounded on data."}, {"heading": "4.1 Agent Dialogue Framework", "text": "We use McBurney and Parson\u2019s agent dialogue framework [20] to formalize the explanation dialogue model as a dialogue game. The Agent Dialogue Framework (ADF) provides a modular and unifying framework that can represent and combine different types of atomic dialogues in the typology ofWalton and Krabbe [35], with the freedom of introducing new dialogue type combinations. The ADF has three layers: 1. topic layer; 2. dialogue layer; and 3. control layer. In the topic layer, the topics of discussion in a dialogue game are presented in a logical language. Then, the dialogue layer [20] consists of a set of rules:\nCommencement rules: rules under which the dialogue commences.\nLocutions: Rules that determine which utterances are permitted in the dialogue-game. Typical locutions include assertions, questions, arguments, etc.\nCombination rules: Rules that define the dialogical context of the applicability of locutions. E.g. it might not be applicable to assert preposition p and \u00acp in the same dialogue.\nCommitments: Rules that determine the circumstances where players express commitments to a preposition.\nTermination rules: Rules that determine the ending of a dialogue.\nMore formally, given a set of participating agents A, we define the dialogue G at the dialogue layer as a 4-tuple (\u0398,R,T ,CF ), where \u0398 denotes set of legal locutions, R the set of combinations, T the set of termination rules and CF the set of commitment functions respectively [20].\nSelection and transitions between dialogue types are handled in the control layer. Dialogue types can be combined using iteration, sequencing and embedding [20, pg 10]. When combined, an ADF is given by 5-tuple (A,L,\u03a0a ,\u03a0c ,\u03a0) where the set of agents is given by A, logical language representation given by L, set of atomic dialogue types given by \u03a0a , set of control dialogues given by \u03a0c , and \u03a0 is the closure of \u03a0a \u222a \u03a0c , which represents the set of formal dialogues denoted by the 4-tuple given above. Closure is defined under the combination rules presented by McBurney and Parsons [20]."}, {"heading": "4.2 Formal Explanation Dialogue Game Model", "text": "In this section we present the formal explanation dialogue model as a new atomic dialogue type [35] using the modular ADF, and discuss how it is derived from the grounded data of explanation dialogues according to the layers of ADF. Our analysis of the data shows that people switch from explanation to argumentation and back again during an explanation dialogue, in which the explainee questions a claim made by an explainer. For this reason, our model has two dialogue types: Explanation and Argumentation. Dialogue games of atomic dialogue types [35] have an initial situation and an aim (e.g persuasion dialogue having the initial situation of conflicting opinions of the interacting party and the aim of resolving the conflict). For our explanation dialogue, the initial condition is\nthe knowledge discrepancy between explainer and explainee of the topic p and the aim is to provide knowledge about the topic p to the explainee.\nFormally, the explanation dialogue model (ADFE ) is the tuple: ADFE = (A,L,\u03a0a ,\u03a0c ,\u03a0) (1)\nwhere the set of agents A = {Q,E}, where labels Q and E refer to the Questioner (the explainee) and the Explainer respectively; L is the set of logical representations about topics (denoted by p, q, r , ...), \u03a0a = {GE ,GA}, where GE is the explanation dialogue and GA is the argumentation dialogue, \u03a0c = (Begin_Question, Begin_Explanation, Begin_Argument, End_Explanation, End_Argument), and \u03a0 is the closure of \u03a0a \u222a \u03a0c under the combination rule set. \u03a0 gives us the set of formal explanation dialogue G.\nThe Topic Layer is dependent on the particular application domain in which the explanation dialogue is embedded, so we do not define this further.\nDialogue Layer. The dialogue layer consists of the two dialogue types: explanation (GE ) and argumentation (GA):\nGE = (\u0398E ,RE ,TE ,CF E ) GA = (\u0398A,RA,TA,CFA)\n(2)\nThe set of legal locutions are defined by:\n\u0398E = (explain, affirm, further_explain, return_question) \u0398A = (affirm_ar\u0434ument , counter_ar\u0434ument , further_explain).\n(3)\nFor clarity, we define the commencement rules, combination rules, and termination rules via the state transition diagram in Figure 1. While most codes are directly transferred to the model as states and state transitions, codes that belonged to information category are embedded in different states. The combination rules RE and RA are defined by the individual transitions on the diagram. For example, after a dialogue begins with a question, the next locution is either the explainer asking for clarification using a return_question or giving an explanation. Similarly, the set of termination rules can be extracted from the state model as the state transitions that lead to the termination state, giving TE = (affirm(p), explain(p)) and TA = (affirm_argument(p), counter_argument(p)). We do not define commitments CF as these were not observable in our data.\nControl layer. This can be identified as state transitions that lead to and out of the two dialogue types in Figure 1 (e.g. argue, explanation_end). Argumentation occurs naturally within explanation dialogues, meaning that this is an embedded dialogue, as defined by McBurney and Parsons [20]. An argument can occur after an explanation was given, which will then continue on to an argumentation dialogue. The dialogue then returns to the explanation dialogue, as shown in Figure 1. A single explanation dialogue can contain many embedded argumentation dialogues.\nExplanation dialogues can occur in sequence, which is modelled by the external loop. Note that a loop within the explanation dialogue implies that the ongoing explanation is related to the same original question and topic, while a loop outside of the dialogue means a new topic is introduced. We coded explanation dialogues to end when a new topic was raised in a question. Questions that ask for follow-up explanations (return_question) were coded when the\nquestions were clearly identifiable as requesting more information about the given explanation.\nExample: We now go through the formal model with an example dialogue which is taken from the human-agent experiments discussed in Section 5.1. Example is given in Table 4 with the dialogue text, locutions/rules and a commentary about the dialogue. Two agents who are explainee (player) and the explainer (agent) participate in the dialogue given by Q and E respectively and the topic \u2018cities\u2019 by p:\nThis example shows an interaction between an agent and a human using the explanation dialogue with an embedded argumentation dialogue. The human-agent study is discussed in depth in Section 5.1. The example demonstrates the ability of our model to handle embedded dialogues and cyclic dialogues (explanation dialogues that occurs twice with Begin_explanation and further_explain) which similar model of explanation dialogue by Walton [34] lack. A detailed model comparison between our model and Walton\u2019s can be found in Section 4.4."}, {"heading": "4.3 Analysis", "text": "We focus our analysis on three areas to further reinforce the derived interaction protocol: 1. Key components of an Explanation Dialogue; 2. Relationships between these components and their variations between different dialogue types; and 3. The sequence of components that can successfully carry out an explanation dialogue.\nHo w Wh y Wh at\nEx pla\nnat ion\nEx pla\nine e A\nffirm atio\nn\nEx pla\nine r A\nffirm atio n Co nte xt\nCo unt\nerf act\nual Arg um ent atio n\nInit ial\nArg um\nent atio\nn\nArg um\nent atio\nn A ffirm\natio n\nCo unt\ner Arg\num ent\nArg um\nent atio\nn C ont\nras t C\nase\nEx pla\nine r R\netu rn\nQu est\nio\nEx pla\nine e R\netu rn\nQu est\nion Pre con cep tion\nCodes\n0\n0.5\n1\n1.5\n2\n2.5\nA ve\nra ge\nO cc\nur re\nnc e\nin a\nD ia\nlo g\nHuman-Human static explainee Human-Human static explainer Human-Explainer agent Human-Explainee agent Human-Human QnA Human-Human multiple explainee\nFigure 2: Average code occurrence per dialogue in different explanation dialogue types\n4.3.1 Code Frequency Analysis. The average code occurrence per dialogue in different dialogue types is depicted in Figure 2. In all dialogue types, a dialogue is most likely to have multiple what questions, multiple explanations and multiple affirmations.\nArgumentation is a key component of an explanation dialogue. The explainee can have different or contrasting views to the explainer regarding the explanation, at which point an argument can be put forth by the explainee. An argument in the form of an explanation that is not in response to a question can also occur at the very beginning of an explanation dialogue, where the argument set the premise for the rest of the dialogue. An argument is typically followed by an affirmation and may include a counter argument by the opposing party. From Figure 2, Human-Human dialogues with the exception of QnA have argumentation but Human-Agent dialogues lack any substantial occurrences of argumentation.\n4.3.2 Explanation Dialogue Termination Rule Analysis. Participants should be able to identify when a dialogue ends. We analyse the different types of explanation dialogues to identify the codes that are most likely to signify termination.\nFrom Figure 3, all explanation dialogue types except HumanHuman QnA type are most likely to end in an explanation. The second most likely code to end an explanation is explainer affirmation. Ending with other codes such as explainee and explainer return questions is presented by \u2018Dialogue ending In Other\u2019 bar in Figure 3. It is important to note that although a dialogue is likely to end in an explanation, that dialogue can have previous explainee affirmations and explainer affirmations."}, {"heading": "4.4 Model Comparison", "text": "We compare the explanation dialoguemodel, which also contains an argumentation sub-dialogue, by Walton [34]. Walton proposed the model shown in Figure 4, which consists of 10 components. This model focus on combining explanation and examination dialogues with argumentation. A similar shift between explanation and argumentation/examination can be seen between our model and Walton\u2019s. According to the data sources, argumentation is a frequently present component of an explanation dialogue, which is depicted by the Explainee probing component in Walton\u2019s Model. The basic flow of explanation is the same between the two models, but the models differ in two key ways. First, is the lack of examination dialogue shift in our model. Although we did not derive an examination dialogue, a similar shift of dialogue can be seen with respect to affirmation states. That is, our \u2018examination\u2019 is simply the explainee affirming that they have understood the explanation. Second is Walton\u2019s focus on the evaluation of the successfulness of an explanation in the form of examination dialogue, whereas our model focus on delivering an explanation in a natural sequence without an explicit form of explanation evaluation.\nThus, we can see similarities betweenWalton\u2019s conceptual model and our data-driven model. The differences between the two are at a more detailed level than at the high-level, and we attribute these\ndifferences to the grounded nature of our study. While Walton proposes an idealised model of explanation, we assert that our model captures the subtleties that would be required to build a natural dialogue for human-agent explanation."}, {"heading": "5 EMPIRICAL VALIDATION", "text": "In this section we discuss the validation of the derived explanation interaction protocol. We conducted a human-agent study in which an agent provides explanations using our model. The purpose of the study is to test whether the proposed model holds in a human-agent setting, and in particular, that the human participants follow the dialogue model when interacting with an artificial agent."}, {"heading": "5.1 Study", "text": "We conducted our study using the Ticket to Ride2 online computer game in a co-located competitive setting, previously used by Newn et al. [24] and Singh et al. [29], in a university usability lab. The basic layout of the game is shown in Figure 5. In this game, players must compete to build train routes between two cities, with each player building at least two such routes. For the purpose of the study, a game is played between two players who we term as the player and the opponent. The player is assisted by an intelligent software agent that predicts the intentions and plans of the opponent. It is important to note that for a two player game, each route can be claimed only by one player. This allows players to block each other deliberately or otherwise, therefore inferring the intent of the opponent is beneficial for winning the game. We use the intent recognition algorithm of Singh et al. [29] to predict the opponent\u2019s future moves. The algorithm uses gaze data from an eye tracker and the actions of the opponent to formulate the possible plans (e.g. most probable routes the opponent can take). The opponent\u2019s gaze will also appear as a heat map on top of the player\u2019s Ticket to Ride\n2https://www.daysofwonder.com/tickettoride/en/\ngame screen3. The agent communicates the predictions and their explanations to the player through a chat window.\nTo evaluate our model, we adopted a Wizard of OZ approach described by Dahlb\u00e4ck et al. [10], meaning that the natural language generation of the explanation agent is played by a human \u2018wizard\u2019, but this is unknown to the human participant. It is important to note that only the natural language generation is delegated to the wizard, while the agent generates and visualize plans (a set of connected train routes) in a separate interface in order to assist the wizard. Wizard has access to the visualized plans, most gazed at routes and most gazed at cities. The argument for using Wizard of Oz (WoZ) technique as opposed to a natural language implementation is twofold. First, to gather high quality empirical data related to the model bypassing the limitation that exist in natural language interfaces [10]. Second, having an interaction that closely resembles human discourse [10] allows the human to have more natural responses. Wizard of Oz techniques have been demonstratively shown to successfully evaluate conversational agents [9, 14], human-robot interactions [3] and automated vehicle-human interactions [19, 25] .\nThe Wizard uses the prediction information of the agent and translates it to a more natural dialogue, enabling us to get empirical data on natural explanation dialogues between player and the agent. The player is informed that he/she can communicate with the agent in a natural discourse. A prediction of the game includes a route that opponent might build (e.g from Pittsburgh to Houston) and a city area opponent might be interested in (e.g Interested around Houston). Predictions are generated from the implemented intent recognition algorithm of Singh et al. [29]. The wizard follows a simple natural language template: prediction followed by the explanation. The explanation template can include one or more of the following in any order: gaze explanation (e.g the opponent has been repeatedly gazing at that path) and causal history explanation (e.g the opponent has already built some paths along that route).\nThe protocol of the Wizard is outlined as follows. The Wizard follows the locutions, termination rules and combination rules of the dialogue model. Predictions and explanations of the agent is translated to natural language using the template described above by the wizard according to the nature of the locution used. Players (experiment participants) can ask questions and present argument in natural language. Players can initiate dialogues as well as reply to the Wizard at any time in the game, and are in-control of the frequency of the interaction. The wizard follows a static failure response to any interactions that failed or unable to provide predictions and explanations (e.g I\u2019m unable to answer that). Note that in the case of a participant using an invalid locution or a control dialogue, the dialogue will fail and wizard will end the dialogue with a termination rule.\nParameters of the experiments are as follows. In total, we obtained 101 explanation dialogues across 14 experiments. Players were from the same university, aged between 23 and 31 years (M = 27.2). Players were observed through an observation room, in which the wizard was located. The duration of each experiment had an upper bound of 30 minutes (M = 20.15), limited by the\n3Video capture of an experiment is provided in supplementary material at https: //explanationdialogs.azurewebsites.net/supp.zip\nduration of the game-play, with the ability to end early if the game is won by either side before 30 minutes (game ends when all trains have been used by either side). During game-play the player has the freedom to engage in conversation with the agent or play the game disregarding the agent, thus each experiment yields a different number of dialogues (M = 7.21). Conversations between player and the wizard were carried out using a chat client, through which we recorded the dialogue data. Extracted data was then analysed according to locutions, control dialogues and their sequences."}, {"heading": "5.2 Results and Findings", "text": "Figure 6 illustrates the explanation dialogue games in the humanagent study. Control dialogues and locutions are depicted by shortened tags, which forms a sequence when combined. An example of a dialogue game using the tags would be [BQ][E][AF], which corresponds to [Begin_question \u21d2 explain \u21d2 affirm] in locutions and control dialogues. Figure 6 shows percentages a specific dialogue game type occurred, and invalid dialogue games.\nThe proposed explanation dialogue model held true for 96 out of 101 dialogue game instances we observed. Figure 6 indicates the 5 invalid dialogue game types that occurred. These dialogues became invalid dialogue games according to our model because of the parallelization of dialogue combination moves. For example, consider the dialogue game [BE][AF][RQ][BA][EA]. Here, after affirm and return_question locution, Begin_Argumentaion control dialogue occurs. This sequence is illegal according to the model. If parallelization is allowed, Begin_Argumentaion control dialogue can occur without waiting for a termination rule (e.g. affirm, explain). We attribute this limitation to the nature of the grounded data where parallelization cannot be accurately captured. This limitation can potentially be rectified by introducing parallelization [20] to the combination rules."}, {"heading": "6 CONCLUSION", "text": "Explainable Artificial Intelligent systems can benefit from having a proper interaction protocol that can explain their actions and\nbehaviours to the interacting users. Explanation naturally occurs as a continuous and iterative socio-cognitive process that involves two (sub)processes: a cognitive process and a social process. Most prior work is focused on providing explanations without sufficient attention to the needs of the explainee, which reduces the usefulness of the explanation to the end-user.\nIn this paper, we propose a interaction protocol for the sociocognitive process of explanation that is derived from different types of natural conversations between humans as well as humans and agents. We formalise the model using the Agent Dialogue Framework [20] as a new atomic dialogue type [35] of explanation with an embedded argumentation dialogue, and we analyse the frequency of occurrences of patterns. To empirically validate our model, we undertook a human behavioural experiment involving 14 participants and a total of 101 explanation dialogues. Results indicate that our explanation dialogue model can closely follow Human-Agent explanation dialogues. Main contribution of this paper lies in the formalized interaction protocol for explanation dialogues that is grounded on data, a secondary contribution is the coded (tagged) explanation dialogue data-set of 398 dialogues4. By following a data-driven approach, proposed model captures the structure and the sequence of an explanation dialogue more accurately and allow natural interactions than explanations from existing models. The main contribution of this paper is a grounded interaction protocol derived from explanation dialogues, formalized as a new atomic dialogue type [35] in the ADF. XAI systems that deal in explanation and trust will benefit from such a model in providing better, more intuitive and interactive explanations.\nIn future work, we aim introduce parallelism to the model with respect to locutions and combination rules as founded to be present in human-agent dialogues from the study. Further evaluation can be done by introducing other forms of interaction modes such as visual interactions which may introduce different forms of combination and termination rules."}, {"heading": "ACKNOWLEDGMENTS", "text": "The research described in this paper was supported by the University of Melbourne research scholarship (MRS); SocialNUI: Microsoft Research Centre for Social Natural User Interfaces at the University of Melbourne; and a Sponsored Research Collaboration grant from the Commonwealth of Australia Defence Science and Technology Group and the Defence Science Institute, an initiative of the State Government of Victoria. We also acknowledge the support of Joshua Newn and Ronal Singh, given in conducting the study."}], "title": "A Grounded Interaction Protocol for Explainable Artificial Intelligence", "year": 2019}