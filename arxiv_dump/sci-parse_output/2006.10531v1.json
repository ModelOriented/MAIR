{"abstractText": "Artificial Intelligence and Machine Learning are becoming increasingly present in several aspects of human life, especially, those dealing with decision making. Many of these algorithmic decisions are taken without human supervision and through decision making processes that are not transparent. This raises concerns regarding the potential bias of these processes towards certain groups of society, which may entail unfair results and, possibly, violations of human rights. Dealing with such biased models is one of the major concerns to maintain the public trust. In this paper, we address the question of process or procedural fairness. More precisely, we consider the problem of making classifiers fairer by reducing their dependence on sensitive features while increasing (or, at least, maintaining) their accuracy. To achieve both, we draw inspiration from \u201cdropout\u201d techniques in neural based approaches, and propose a framework that relies on \u201cfeature drop-out\u201d to tackle process fairness. We make use of \u201cLIME Explanations\u201d to assess a classifier\u2019s fairness and to determine the sensitive features to remove. This produces a pool of classifiers (through feature dropout) whose ensemble is shown empirically to be less dependent on sensitive features, and with improved or no impact on accuracy.", "authors": [{"affiliations": [], "name": "A PREPRINT"}, {"affiliations": [], "name": "Vaishnavi Bhargava"}], "id": "SP:6724efdc2563588ba0bff24961466da5eb9836b3", "references": [{"authors": ["Sam Maes", "Karl Tuyls", "Bram Vanschoenwinkel", "Bernard Manderick"], "title": "Credit card fraud detection using bayesian and neural networks", "venue": "In NAISO Congress on neuro fuzzy technologies,", "year": 2002}, {"authors": ["Dominique Guegan", "Peter Martey Addo", "Bertrand Hassani"], "title": "Credit risk analysis using machine and deep learning models. Risks", "year": 2018}, {"authors": ["B Iskandar"], "title": "Terrorism detection based on sentiment analysis using machine learning", "venue": "Journal of Engineering and Applied Sciences,", "year": 2017}, {"authors": ["Yuji Roh", "Geon Heo", "Steven Euijong Whang"], "title": "A survey on data collection for machine learning: a big data - ai integration perspective", "year": 2018}, {"authors": ["Till Speicher", "Hoda Heidari", "Nina Grgic-Hlaca", "Krishna P Gummadi", "Adish Singla", "Adrian Weller", "Muhammad Bilal Zafar"], "title": "A unified approach to quantifying algorithmic unfairness: Measuring individual & group unfairness via inequality indices", "venue": "In Int. Conf. Knowledge Discovery & Data Mining (SIGKDD18),", "year": 2018}, {"authors": ["Muhammad Bilal Zafar", "Isabel Valera", "Manuel Gomez Rodriguez", "Krishna P Gummadi"], "title": "Fairness beyond disparate treatment & disparate impact: Learning classification without disparate mistreatment", "venue": "In World Wide Web (WWW17),", "year": 2020}, {"authors": ["Nina Grgi\u0107-Hla\u010da", "Muhammad Bilal Zafar", "Krishna P Gummadi", "Adrian Weller"], "title": "Beyond distributive fairness in algorithmic decision making: Feature selection for procedurally fair learning", "venue": "In Proc. Conf. Artificial Intelligence (AAAI18),", "year": 2018}, {"authors": ["Nina Grgic-Hlaca", "Muhammad Bilal Zafar", "Krishna P Gummadi", "Adrian Weller"], "title": "The case for process fairness in learning: Feature selection for fair decision making", "venue": "In NIPS Symposium on Machine Learning and the Law,", "year": 2016}, {"authors": ["Marco T\u00falio Ribeiro", "Sameer Singh", "Carlos Guestrin"], "title": "why should i trust you?\u201d: Explaining the predictions of any classifier", "venue": "In Int. Conf. Knowledge Discovery and Data Mining (SIGKDD16),", "year": 2016}, {"authors": ["Damien Garreau", "Ulrike von Luxburg"], "title": "Explaining the explainer: A first theoretical analysis of LIME", "year": 2001}, {"authors": ["Marco T\u00falio Ribeiro", "Sameer Singh", "Carlos Guestrin"], "title": "Anchors: High-precision model-agnostic explanations", "venue": "In AAAI Conf. Artificial Intelligence,", "year": 2018}, {"authors": ["Scott M. Lundberg", "Su-In Lee"], "title": "A unified approach to interpreting model predictions", "venue": "In Conf. Neural Information Processing Systems (NIPS17),", "year": 2017}, {"authors": ["Muhammad Bilal Zafar", "Isabel Valera", "Manuel Gomez Rodriguez", "Krishna P Gummadi"], "title": "Fairness constraints: Mechanisms for fair classification", "venue": "In Artificial Intelligence and Statistics (AISTATS17),", "year": 2017}, {"authors": ["Yarin Gal", "Zoubin Ghahramani"], "title": "Dropout as a bayesian approximation: Representing model uncertainty in deep learning", "venue": "editors, Int. Conf. Machine Learning, ICML16),", "year": 2016}, {"authors": ["Yarin Gal", "Zoubin Ghahramani"], "title": "A theoretically grounded application of dropout in recurrent neural networks", "venue": "Neural Information Processing Systems", "year": 2016}, {"authors": ["Thibault Laugel", "Xavier Renard", "Marie-Jeanne Lesot", "Christophe Marsala", "Marcin Detyniecki"], "title": "Defining locality for surrogates in post-hoc interpretablity", "venue": "arXiv preprint arXiv:1806.07498,", "year": 2018}, {"authors": ["Julia Dressel", "Hany Farid"], "title": "The accuracy, fairness, and limits of predicting recidivism", "venue": "Science Advances,", "year": 2018}, {"authors": ["Alexandra Chouldechova"], "title": "Fair prediction with disparate impact: A study of bias in recidivism prediction instruments", "venue": "Big data,", "year": 2017}, {"authors": ["Cynthia Dwork", "Moritz Hardt", "Toniann Pitassi", "Omer Reingold", "Richard S. Zemel"], "title": "Fairness through awareness", "venue": "editor, Innovations in Theoretical Computer Science", "year": 2012}, {"authors": ["Michael Kearns", "Seth Neel", "Aaron Roth", "Zhiwei Steven Wu"], "title": "Preventing fairness gerrymandering: Auditing and learning for subgroup fairness", "venue": "In Int. Conf. Machine Learning (ICML18),", "year": 2018}, {"authors": ["Zhe Zhang", "Daniel B Neill"], "title": "Identifying significant predictive bias", "venue": "in classifiers. CoRR,", "year": 2016}, {"authors": ["Rich Zemel", "Yu Wu", "Kevin Swersky", "Toni Pitassi", "Cynthia Dwork"], "title": "Learning fair representations", "venue": "In Int. Conf. Machine Learning", "year": 2013}, {"authors": ["Reuben Binns"], "title": "On the apparent conflict between individual and group fairness", "venue": "In Conf. Fairness, Accountability, and Transparency (FAT20,", "year": 2020}, {"authors": ["Nina Grgic-Hlaca", "Elissa M Redmiles", "Krishna P Gummadi", "Adrian Weller"], "title": "Human perceptions of fairness in algorithmic decision making: A case study of criminal risk prediction", "venue": "In World Wide Web (WWW18),", "year": 2018}, {"authors": ["Ilse van der Linden", "Hinda Haned", "Evangelos Kanoulas"], "title": "Global aggregations of local explanations for black box models", "venue": "ArXiv, abs/1907.03039,", "year": 1907}], "sections": [{"text": "Keywords Explainability \u00b7 Fairness \u00b7 Feature importance \u00b7 Feature-dropout \u00b7 Ensemble classifier \u00b7 LIME"}, {"heading": "1 Introduction", "text": "Machine Learning (ML) tasks often involve the training of a model based on past experience and data, which are then used for prediction and classification purposes. The practical applications where such models are used include, e.g., loan grants in view of framing laws, detecting terrorism, predicting criminal recidivism, and similar social and economic issues at a global level [1, 2, 3]. These decisions affect human life and may have undesirable impacts on vulnerable groups in society. The widespread use of ML algorithms has raised multiple concerns regarding user privacy, transparency, fairness, and trustfulness of these models. In order to make Europe \u201cfit for the digital age\u201d1, in 2016 the European Union has enforced the GDPR Law2 across all organizations and firms. The law entitles European citizens the right to have a basic knowledge regarding the inner workings of automated decision models and to question their results. The unfair automated decisions not only violate anti-discrimination laws, but they also undermine public trust in Artificial Intelligence. The unwanted bias in the machine learning models can be caused due to the following reasons:\n1https://www.zdnet.com/article/gdpr-an-executive-guide-to-what-you-need-to-know/ 2General Data Protection Regulation (GDPR): https://gdpr-info.eu/\nar X\niv :2\n00 6.\n10 53\n1v 1\n[ cs\n.L G\n\u2022 The data Collection [4] may be biased, as certain minority groups of society, or people living in rural areas do not generate enough data. This leads to an unfair model because of unbalanced and biased datasets while training.\n\u2022 The training algorithm may be subject to bias if one chooses an inappropriate model or training set. Additionally, the model may consider sensitive or discriminatory features while training, which leads to process unfairness.3\nTill now, the notions of fairness have focused on the outcomes of the decision process [5, 6], with lesser attention given to the process leading to the outcome [7, 8]. These are inspired by the application of anti-discrimination laws in various countries, which ensures that the people belonging to sensitive groups (e.g. race, color, sex etc.) should be treated fairly. This issue can be addressed through different points of views, which include:\n\u2022 Individual Fairness or Disparate Treatment [5] considers individuals who belong to different sensitive groups, yet share similar non-sensitive attributes and require them to have same decision outcomes. For instance, during job applications, applicants having same educational qualifications must not be treated discriminately based on their sex or race.\n\u2022 Group Fairness or Disparate Impact [5] states that people belonging to different sensitive attribute groups should receive beneficial outcomes in similar proportions. In other words, it states that \u201cDifferent sensitive groups should be treated equally\".\n\u2022 Disparate Mistreatment or Equal Opportunity [6] proposes different sensitive groups to achieve similar rates of error in decision outcomes.\n\u2022 Process or Procedural fairness [8, 7] deals with the process leading to the prediction and keeps track of input features used by the decision model. In other words, the process fairness deals at the algorithmic level and ensures that the algorithm does not use any sensitive features while making a prediction.\nIn this study, we aim to deliver a potential solution to deal with the process fairness in ML Models. The major problem while dealing with process fairness is the opaqueness of ML models. Indeed, this black-box nature of ML models, such as in deep neural networks and ensemble architectures such as random forests (RF), makes it difficult to interpret and explain their outputs, and consequently for users and general public to trust their results. There are several proposals of explanatory models to make black-box models more interpretable and transparent. Due to the complexity of recent black-box models, it is unreasonable to ask for explanations that could represent the model as a whole. This fact, lead to local approaches to derive possible explanations.\nThe basic idea is to explain the model locally rather than globally. An ideal model explainer should contain the following desirable properties [9]:\n\u2022 Model-Interpretability: The model should provide a qualitative understanding between features and targets. The explanations should be easy to understand.\n\u2022 Local Fidelity: It is not possible to find an explanation that justifies the black-box\u2019s results on every single instance. But the explainer must at least be locally faithful to the instance being predicted.\n\u2022 Model Agnostic: The explainer should be able to explain all kinds of models. \u2022 Global Perspective: The explainer should explain a representative set to the user, such that the user has a\nglobal understanding of the explainer.\nSuch local explanatory methods include LIME, Anchors, SHAP and DeepSift [10, 9, 11, 12]. These are based on \u201clinear explanatory methods\u201d that gained a lot of attention recently, due to their simplicity and applicability to various supervised ML scenarios.\nIn this study, we will mainly use LIME to derive local explanations of black box classification models. Given a black box model and a target instance, LIME learns a surrogate linear model to approximate the black-box model in a neighbourhood around the target instance. The coefficients of this linear model correspond to the features\u2019 contributions to the prediction of the target instance. Thus LIME outputs top features used by the black box locally and their contributions. In this paper, we propose LIMEGlobal, a method to derive global explanations from the locally important features obtained from LIME.\nThe LIMEGlobal explanations can provide an insight into process fairness. This naturally raises the question of how to guarantee a fairer model given these explanations, while ensuring minimal impact in accuracy [13]. This motivated us\n3Terms unfairness and bias are used interchangeably.\nto seek models Mfinal in which (i) their dependence on sensitive features is reduced, as compared to the original model, and (ii) their accuracy is improved (or, at least, maintained).\nTo achieve both goals, we propose LimeOut4, a framework that relies on feature dropout to produce a pool of classifiers that are then combined through an ensemble approach. Feature drop out receives a classifier and a feature a as input, and produces a classifier that does not take a into account. Essentially, feature a is removed in both the training and the testing phases.\nLimeOut\u2019s workflow can be described as follows. Given the classifier provided by the user, LimeOut uses LIMEGlobal to assess the fairness of the given classifier by looking into the contribution of each feature to the classifier\u2019s outcomes. If the most important features include sensitive ones, the model is unfairly biased. Otherwise, the model is considered as unbiased. In the former case, LimeOut applies dropout of these sensitive features, thus producing a pool of classifiers (as explained earlier). These are then combined into an ensemble classifier Mfinal. Our empirical study was performed on two families of classifiers (logistic regression and random forests) and carried out on real-life datasets (Adult and German Credit Score), and it shows that both families of models become less dependent on sensitive features (such as sex, race, marital status, foreign worker, etc.) and show improvements or no impact on accuracy.\nThe paper is organised as follows. In Section 2 we will discuss some substantial work related to explainability and fairness. We will briefly recall LIME (Local Interpretable Model Agnostic Explanations) in two distinct settings (for textual and tabular data) in Subsection 2.1, and briefly discuss different fairness issues, some measures proposed in the literature, as well as the main motivation of our work in Subsection 2.2. We will then present our approach (LimeOut) in Section 3, and two empirical studies are carried out in Section 4 that indicate the feasibility of LimeOut. Despite the promising results, this preliminary study deserves further investigations, and in Section 5 we will discuss several potential improvements to be carried out in future work."}, {"heading": "2 Related Work", "text": "In this section, we briefly recall LIME and and discuss some issues related to model fairness. There has been substantial work done in the field of \u201cInterpretable Machine Learning\" and \u201cFairness\u201d. LIME [9] and Anchors [11] are prominently being used to obtain the explanations of the black box ML models. These methods provide the top important features that are used by the black box to predict a particular instance. LIME and Anchors do not provide human like explanations (they provide \u201cfeature importance\u201d or contributions), and they have some limitations [10]. In Section 3 we will use LIME to tackle fairness issues based on relative importance of the features."}, {"heading": "2.1 LIME - Explanatory Method", "text": "LIME (Local Interpretable Model Agostic Explanations) takes the form of surrogate linear model, which is interpretable and mimics locally the behavior of a black box. The feature space used by LIME does not need to be the same as the feature space used by a black box. Examples of representations used by LIME include [9]: (i) the binary vector representation of textual data that indicates presence/absence of a word, and (ii) the binary vector which represents presence/absence of contiguous patch of similar pixels, in case of images.\nLIME can be described as follows [9]. Let f : Rd \u2192 R be the function learned by a classification or regression model over training samples. No further information about this function f is assumed. Now, let x \u2208 Rd be an instance, and consider its prediction f(x). LIME aims to explain the prediction f(x) locally. Note that the feature space of LIME need not be the same as the input space of f . For example, in case of text data interpretable space is used as vectors representing presence/absence of words, whereas the original space might be the word embeddings or word2vec representations. Indeed, LIME uses discretized features of smaller dimension d\u0302 to build the local model, and aims to learn an explanatory model g : Rd\u0302 \u2192 R, which approximates f in the neighborhood of x \u2208 Rd. To get a local explanation, LIME generates neighbourhood points around an instance x to be explained and assigns a weight vector to these points. The weight is assigned using \u03c0x(z), which denotes the proximity measure of z w.r.t. x. It then learns the weighted linear surrogate model g by solving the following optimisation problem:\ng = argming\u2208G L(f, g, \u03c0x(z)) + \u2126(g) where L(f, g, \u03c0x(z)) is a measure of how unfaithful g is in approximating f in the locality defined by \u03c0x(z), and where \u2126(g) measures the complexity of g (LIME uses the regularization term to measure complexity). In order to ensure both interpretability and local fidelity, LIME minimizes L(f, g, \u03c0x(z)) while enforcing \u2126(g) to be small in order to be interpretable by humans. The coefficients of g correspond to the contribution of each feature to the prediction f(x) of x.\n4The name comes from drop-out techniques [14, 15] in neural networks. The github repository of LimeOut can be found here: https://github.com/vaishnavi026/LimeOut\nLIME uses the following weighting function\n\u03c0x(z) = e ( D(x,z)2 \u03c32 ), (1)\nwhere D(x, z) is the Euclidean distance between x and z, and \u03c3 is the hyper parameter (kernel-width). The value of \u03c3 impacts the fidelity of explanation [16]. For instance, when \u03c3 is too large, all instances are given equal weight, and it is impossible to derive a linear model which can explain all of them. Similarly if \u03c3 is too small, only a few points are assigned considerable weight and even a constant model will be able to explain these points, this will result in lower coverage. Thus we need to choose an optimal \u03c3 to ensure coverage as well as local fidelity (faithfulness). This is illustrated in Figure 1: it displays the impact of \u03c3 on the explanations. The tuned value used by LIME [9] for tabular data is \u03c3 = 0.75 \u2217 n for n columns, whereas for textual data it is \u03c3 = 25."}, {"heading": "2.1.1 LIME for textual data [9].", "text": "Consider the text classification problem, in which the goal is to classify an amazon review into positive or negative feedback5. The model is trained using Naive Bayes Classifier. Let\u2019s discuss the procedure to get the LIME explanation:\n1. Take any instance x for which you need an explanation. Consider the textual instance Great easy to set up. Little difficult to navigate and the instructions are non-existent, and suppose that the Naive Bayes prediction is P (pos.) = 0.68 and P (neg.) = 0.32.\n2. Perturb your dataset and get their black box predictions. For finding the perturbation of this example, LIME randomly removes each word from the original instance (i.e., changes \u20181\u2019 to \u20180\u2019 in the binary representation) one by one, and considers all thus obtained neighborhood points. LIME then gets the black box prediction of these neighbour instances.\n3. Weight the new samples based on their proximity to the original instance. LIME assigns weights to the neighbourhood instances z based on their proximity to the original instance x using 1.\n4. Fit a weighted, interpretable (surrogate) model on the dataset with the variations. LIME trains a linear weighted model that fits the original and the obtained neighbourhood instances.\n5. Get the explanations by interpreting the local model. The output of LIME is the list of explanations, reflecting the contribution of each feature to the prediction of the sample. The resulting explanation is illustrated in Figure 2"}, {"heading": "2.1.2 LIME for tabular data [10].", "text": "The workflow of LIME on tabular data is similar to that on textual data. However, unlike LIME for textual data, it needs a training set (user defined) to generate neighbourhood points. The following statistics are computed for each feature\n5https://www.kaggle.com/bittlingmayer/amazonreviews\ndepending on their type: (i) for categorical features it computes the frequency of each value, (ii) for numerical features, it computes the mean and the standard deviation, which are then discretized into quartiles.\nSuppose that f is the black-box function, and that we want to explain the prediction f(x) of x = (x1, x2, . . . , xi., xn), where each xi may be a categorical or a numerical value. Each categorical value is mapped to an integer using LabelEncoder6. Note that the values of each feature in the training set is divided into p quantiles. These quantile intervals are used for discretizing the original instance. If xi lies between quantile qj and qj+1, it gets the value j. This is done for all the features to get the quantile boxes for all xi, i \u2208 {1, . . . , n}. To get the perturbation y\u0302 in the neighbourhood of x\u0302, LIME samples discrete values from {1, . . . , p}, n times. To get the continuous representation y of y\u0302, LIME Tabular uses a normal distribution and the quantile values. The neighbourhood instance y\u0302 is represented as binary tuple with the i-th component equal to 1 if x\u0302i = y\u0302i, and 0 if x\u0302i 6= y\u0302i. In this way LIME Tabular generates all the neighbourhood points. The following steps are similar to LIME for textual data. These points are assigned weights using the exponential kernel (1), and a weighted linear function is learned over the neighbourhood permutations. To illustrate, consider an example of the Adult dataset (see Subsection 4.1). The task is to predict if a salary of a person is \u2265 50k dollars. We have trained the model using Random Forest Classifier. An example of local explanation is given in Figure 3."}, {"heading": "2.2 Model Fairness", "text": "Several notions of model fairness have been proposed [17, 5, 6, 7, 8] based on decision outcomes as well as on process fairness. Individual fairness [18] (or disparate treatment, or predictive parity) imposes that the instances/individuals belonging to different sensitive groups, but similar non-sensitive attributes must receive equal decision outcomes. The notion of group fairness(or disparate impact or statistical parity [19]) is rooted in the desire for different sensitive\n6LableEncoder Class is given in the sklearn preprocessing library https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\ndemographic groups to experience similar rates of errors in decision outcomes. COMPAS7 is a recidivism detection tool, where the goal is to predict whether a criminal would re-offend his crime based on a long questionnaire. The then popular algorithm was designed by the commercial company, Northpointe (now Equivant). A study by ProPublica8 showed that COMPAS has a strong ethnic bias. Among non-reoffenders, COMPAS is almost twice more likely to signal black people as high risk. Furthermore in COMPAS, white reoffenders are predicted as low risk much often than black offenders. In other words, this indicates that COMPAS has considerable high false positive and lower true negative rates for black defendants when compared to white defendants. COMPAS is used across US by judges and parole officers to decide whether to grant or deny probation to offenders; hence, it is very important to understand how this model reaches its conclusion and ensure it is fair. If we focus on the decision outcomes, the fair algorithm in case of COMPAS (if we consider only Race as sensitive feature) should be such that: (i) black and whites with the same features get the same output (no disparate treatment and thus non-discriminatory), and (ii) the proportion of individuals classified as high-risk should be same across both the groups (statistical parity).\nWe can deal with this bias during training (see [6]) by: (i) excluding all features that may cause the model to create bias, e.g race, gender etc., or (ii) including discrimination measures as learning constraints, i.e., the model should be trained to minimize P (ypred 6= ytrue) such that\nP (ypred 6= ytrue|race = Black) = P (ypred 6= ytrue|race = White),\nwhere ypred is the risk predicted by trained ML model (e.g., COMPAS) and ytrue is the true risk value. This constraint is motivated by the fact that \u2018race\u2019 is a sensitive feature. Such constraints are applied to different sensitive attributes separately (e.g. sex, race, nationality etc.), it might lead to unfairness for the groups which lie at the intersection of multiple kinds of discrimination (e.g. black women), also known as fairness gerrymandering [20]. To avoid this, [21] proposed constraints for multiple combinations of sensitive features. However, constraints for multiple combinations of sensitive attributes render model training highly complex and may lead to overfitting.\nEarlier studies in fair ML [22, 13] consider individual and group fairness as conflicting measures, and some studies tried to find an optimal trade-off between them. In [23] the author argue that, although apparently conflicting, they correspond to the same underlying moral concept. In fact, the author provides a broader perspective and advocates an individual treatment and assessment on a case-by-case basis. In [8, 7] the author provides another noteworthy perspective to measure fairness, namely, process fairness. Rather than focusing on the outcome, it deals with the process leading to the outcome. In [7] the author provides a key insight to rely on human\u2019s moral judgement or intuition about the fairness of using an input feature in algorithmic decision making. He also assesses the impact of removing certain input features on the accuracy of the classifier, and designs an optimal trade-off between accuracy and the process fairness for the classifier. However, humans may have different perspectives on whether it is fair to use a input feature in decision making process. In [24] the authors propose a framework to understand why people perceive certain features as fair or unfair. They introduce seven factors on which a user evaluates a feature in terms of reliability, relevance, privacy, volitionality, causes outcome, causes vicious cycle, causes disparity in outcomes, caused by sensitive group membership.\nWe are inspired by the idea of using a combination of classifiers instead of a single one. For instance, in [8] the authors explore the benefits of replacing a single classifier with a diverse ensemble of random classifies, regarding the accuracy as well as individual and group fairness. In this paper, we further explore this idea and propose a method, that we call LimeOut, to ensure process fairness while improving (or, at least, maintaining) the model\u2019s accuracy."}, {"heading": "3 Our Methodology", "text": "In this section, we describe in detail the framework of LimeOut that consists of two main components: LIMEGlobal and ENSEMBLEOut. It receives as input both a classifier9 and a dataset. The first component then checks whether the classifier is biased on the dataset in the sense that the predictions depend on sensitive features. To do this, we make use of LIMEGlobal [9] (see Subsection 3.1). This will output the most important features (globally). If sensitive features are among the most important, then the classifier is considered unfair and the second component of LimeOut is employed. Otherwise, the classifier is considered fair and no action is taken. The second component is the core of LimeOut (see Subsection 3.2). Given the most important features, ENSEMBLEOut produces a pool of classifiers using feature-drop. Each of these classifiers does not depend on the corresponding sensitive features. It then constructs an ensemble using this pool of classifiers. Following a human and context-centered approach, the choice of sensitive features is left to the user within the given context. This framework will be illustrated in Section 4.\n7 https://en.wikipedia.org/wiki/COMPAS_(software) 8https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing 9Here we focus on binary classifiers that output the probability for each class label."}, {"heading": "3.1 LIMEGlobal", "text": "LIME is prevalent to get local explanations for the instances. These explanations can be combined to provide insights into the global process of the classifier [9, 25]. First, LIMEGlobal chooses instances using submodular pick method [9]. The choice of instances can impact the reliability of the global explanation. The method submodular pick provides a set of instances for which explanations are diverse and non-redundant. To obtain a global insight into the classifier\u2019s inner process, we use the instances obtained from submodular pick10. LIMEGlobal obtains the local explanations (important features and their contributions) for all these instances. This results in a list of top important features used by the model globally."}, {"heading": "3.2 ENSEMBLEOut", "text": "LimeOut uses the globally important features obtained by LIMEGlobal to assess process fairness of any given ML model. In this way, we can check whether the model\u2019s predictions depend on sensitive features and measure its dependence. If sensitive features are ranked within the top 1011 globally important features, then it is deemed unfair or biased. If the model is deemed unfair, then one easy solution would be to remove all the sensitive features from the dataset before training. However, these sensitive features may be highly correlated to non-sensitive features, thus keeping the undesired bias. To mitigate this drawback, LimeOut also removes all such correlated features.\nNow this could entail a decrease in performance since, after removing all the sensitive features, the model could become less accurate due to the lack of training data. To overcome this limitation, LimeOut constructs a pool of classifiers each of which corresponding to the removal of a subset of sensitive features. To avoid the exponential number of such classifiers, in this paper we only consider those obtained by removing either one or all sensitive features. LimeOut constructs an ensemble classifier Mfinal through a linear combination of the pool\u2019s classifiers.\nMore precisely, given an input (M,D), where M is a classifier and D is the dataset. Suppose that the globally important features given by LIMEGlobal are a1, a2,. . . ,an, in which aj1 , aj2 , . . . , aji are sensitive. LimeOut thus trains i + 1 classifiers: Mk after removing ajk from the dataset, for k = 1, . . . , i, and Mi+1 after removing all sensitive features aj1 , aj2 , . . . , aji .In this preliminary implementation of LimeOut, the ensemble classifier Mfinal is defined as the \u201caverage\u201d of these i+ 1 classifiers. More precisely, for an instance x and a class C,\nPMfinal(x \u2208 C) = \u2211k=i+1\nk=1 PMk(x \u2208 C) i+ 1 .\nAs we will see empirically in Section 4 over different datasets and classifiers, the dependence of Mfinal on sensitive features decreases, whereas its accuracy is maintained and, in some cases, it even improves."}, {"heading": "4 Experiments", "text": "To validate our approach, we applied LimeOut on two different families of classifiers (Logistic regression and Random Forests) over different datasets. In each case, the ensemble classifier obtained by LimeOut is fairer than the original classifiers. The datasets we use, Adult and German credit score, are known to be biased. These experiments illustrate different possible scenarios, namely, the case of unfair process (see Subsection 4.1) and of a fair process (see Subsection 4.2 for Random Forests)."}, {"heading": "4.1 Adult Dataset", "text": "This dataset comes from the UCI repository of machine learning databases12. The task is to predict if an individual\u2019s annual income exceeds 50,000 dollars based on census data. An individual\u00e2A\u0306Z\u0301s annual income is the result of various features such as \u201cAge\u201d, \u201cWorkclass\u201d, \u201cfnlwgt\u201d, \u201cEducation\u201d, \u201cEducationNum\u201d, \u201cMarital Status\u201d, \u201cOccupation\u201d, \u201cRelationship\u201d, \u201cRace\u201d, \u201cSex\u201d, \u201cCapital Gain\u201d, \u201cCapital Loss\u201d, \u201cHours per week\u201d and \u201cCountry\u201d. Intuitively, the income of a person should get influenced by the individual\u00e2A\u0306Z\u0301s education level, age, occupation, number of hours he works, company etc. But it would be unfair if our model considers race, sex or the marital status of the individual while making any prediction.\n10In [9] the authors argue that the submodular pick is a better method than random pick. We still experimented random pick on the datasets of Section 4, but the relative importance of features remained similar.\n11In this study we focused on the top 10 features. However this parameter can be set by the user and changed according to his use case.\n12Adult Dataset: http://archive.ics.uci.edu/ml/datasets/Adult\nThis dataset has 14 features out of which 6 are continuous and 8 are nominal, and it comprises 45,255 instances. We partitioned the dataset randomly into 80% for training and 20% for testing. However, the class distribution of Adult dataset is extremely unbalanced and majority of the dataset consists of individuals with annual income < 50, 000 dollars. To balance this, we used Synthetic Minority Oversampling Technique (SMOTE13) over training data. SMOTE generates new samples from the minority class and includes them in the training set, resulting to a balanced training dataset. We then perform training on the augmented (balanced) dataset using: Logistic Regression and Random Forest."}, {"heading": "4.1.1 Logistic Regression:", "text": "We trained a logistic regression model over the obtained training set. In binary classification problems, logistic regression often uses a default threshold value of 0.5, i.e. if predicted value \u2265 0.5, then the predicted class will be positive, and negative, otherwise. However, this threshold may lead to poor results, especially, in the case of unbalanced datasets. We used threshold tuning14 in order to improve the performance of our classifier. The threshold is chosen to be optimal for Precision Recall Curve and the ROC Curve (to ensure maximum F1-score). The classifier M obtained after threshold tuning had an accuracy of 82.65%. To assess the process fairness of M , we used LIMEGlobal to get the 10 most important features used by M .\nFrom Table 1, it is evident that Race, sex and marital status are among the top 10 features used by model M with contributions 1.93, 1.80 and 2.11 respectively. We know that it\u2019s unfair to use these features while predicting someone\u2019s income. And as these are among the top 10 features, we can deem the model to be unfair. Now we train four models by dropping out sensitive features : Race, Sex and Marital status. Note that all the classifiers are trained using Logistic Regression with threshold tuning. Through feature dropout, we thus obtain 4 classifiers: M1 trained without \u201cSex\u201d, M2 trained without \u201cRace\u201d, M3 trained without \u201cMarital Status\u201d, and M4 trained without the 3 (Accuracy = 81.97%).\nWe can infer that M4 is fairer because it has not used any sensitive feature while training. But the accuracy is reduced from 82.65% to 81.9%. The ensemble Mfinal of models M1, M2, M3 and M4 achieved an accuracy of 84.18%. The statistical test15 showed that this improved accuracy is significant. The global impact of the sensitive features is also reduced (see the explanations in Table 1)."}, {"heading": "4.1.2 Random Forest:", "text": "We also used Random Forest and checked its fairness. This modelMRF has accuracy = 83.49%. The global explanations for MRF LimeOut\u2019s ensemble model (MRF )final are given in Table 2. From the Table 2 we see that the impact of sensitive features decreased for (MRF )final, and that its accuracy increased to 83.86%. While when we removed all three sensitive features Race, Sex and Marital status, the accuracy was 81.6%. Again we observe a significant improvement in the accuracy of the LimeOut\u2019s ensemble classifier, while ensuring a fairer model.\n13https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.over_sampli\\ng.SMOTE.html 14https://machinelearningmastery.com/threshold-moving-for-imbalanced-classification/ 15We performed the t-test."}, {"heading": "4.2 German Credit Score Dataset", "text": "The data was initially prepared by Prof. Hoffman and is available publicly as \u2018german.data\u2019 on UCI Machine Learning Repository 16. If a bank receives a loan application based on the applicant\u2019s profile it can decide whether it can approve the loan. Two types of risk are associated with the bank\u2019s decision: (i) if an applicant is at good credit risk, he is likely to pay back his loan, and (ii) if an applicant is at bad credit risk, he is unlikely to pay back.\nThe dataset set has information about 1000 individuals on the basis of which they have been classified as good or bad risk. The goal is to use applicant\u2019s demographic and socio-economic profiles to assess the risk of lending loan to the customer. The dataset consists of 20 features and a classification label (1: Good Risk, 2: Bad Risk). We split the dataset into 80% training set and 20% testing. As the dataset is highly imbalanced, we used SMOTE Oversampling to generate the samples synthetically."}, {"heading": "4.2.1 Logistic Regression:", "text": "For training we used Logistic Regression along with threshold tuning. The obtained accuracy of M was 74.67% with the explanations from LIMEGlobal given in Table 3. Here, we see the sensitive features \u201cstatussex\u201d (sex of the customer),\n16https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)\n\u201ctelephone17\u201d and \u201cforeign worker\u201d appear in the top 10, thus showing that M is process unfair. Hence, LimeOut trains M1, M2 and M3 by removing each one of them, and M4 after removing all 3. Despite being fairer, M4 suffered a drastic accuracy decrease to 69%.\nLimeOut then trained the ensemble Mfinal and output the explanations given in Table 3. Again, the impact of sensitive features decreased in case of Mfinal. In addition, the accuracy of Mfinal is 74.67%, same as M . Again, a fairer classifier without compromising accuracy."}, {"heading": "4.2.2 Random Forest", "text": "We trained the model using Random Forest, and the accuracy was found to be 59%. In this case, LIMEGlobal showed a single sensitive feature in the top 10 and no action was taken18. We will further discuss this case in the next section."}, {"heading": "5 Conclusion and Future Work", "text": "We demonstrated the idea of using LIME to determine model fairness, and integrated it in LimeOut that receives as input a pair (M,D) of a classifier M and a dataset D, and outputs a classifier Mfinal less dependent on sensitive features without compromising accuracy.\nThis preliminary study shows the feasibility and the flexibility of the simple idea of feature dropout followed by an ensemble approach. This opens into several potential improvements and further investigations. First, we only experimented LimeOut on two classes of classifiers, but LimeOut can be easily adapted to different ML models and data types, as well as different explanatory models. An improvised approach to get the global explanation like [25] can be used, and this should be thoroughly explored.\nAlso, the workflow can be further improved, e.g., the classifier ensembles could take into account classifier weighting and other classifiers resulting from the removal of different subsets of sensitive features (here we only considered the removal of one or all features). In this study, we took a human and context-centered approach that requires domain expertise (for identifying sensitive features in a given use-case). However, there is room for automating this task, possibly through a metric or utility-based approach to assess sensitivity that takes into account domain knowledge.\nWe also identified some limitations as that illustrated in the last scenario. Indeed, despite providing insights on process fairness, LimeOut seems of little use when only one sensitive feature is detected in the top k important features. In this case, an alternative method should be employed, for instance, to consider the model obtained by removing this feature. These are some of the issues to be tackled in future work."}], "title": "LIMEOUT: AN ENSEMBLE APPROACH TO IMPROVE PROCESS FAIRNESS", "year": 2020}