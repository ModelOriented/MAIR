{"abstractText": "Speech disorders such as stuttering disrupt the normal fluency of speech by involuntary repetitions, prolongations and blocking of sounds and syllables. In addition to these disruptions to speech fluency, most adults who stutter (AWS) also experience numerous observable secondary behaviors before, during, and after a stuttering moment, often involving the facial muscles. Recent studies have explored automatic detection of stuttering using Artificial Intelligence (AI) based algorithm from respiratory rate, audio, etc. during speech utterance. However, most methods require controlled environments and/or invasive wearable sensors, and are unable explain why a decision (fluent vs stuttered) was made. We hypothesize that pre-speech facial activity in AWS, which can be captured non-invasively, contains enough information to accurately classify the upcoming utterance as either fluent or stuttered. Towards this end, this paper proposes a novel explainable AI (XAI) assisted convolutional neural network (CNN) classifier to predict near future stuttering by learning temporal facial muscle movement patterns of AWS and explains the important facial muscles and actions involved. Statistical analyses reveal significantly high prevalence of cheek muscles (p<0.005) and lip muscles (p<0.005) to predict stuttering and shows a behavior conducive of arousal and anticipation to speak. The temporal study of these upper and lower facial muscles may facilitate early detection of stuttering, promote automated assessment of stuttering and have application in behavioral therapies by providing automatic non-invasive feedback in realtime.", "authors": [{"affiliations": [], "name": "Arun Das"}, {"affiliations": [], "name": "Henry Chacon"}, {"affiliations": [], "name": "Jeffrey Mock"}, {"affiliations": [], "name": "Farzan Irani"}], "id": "SP:4082689b161a170b2835e7a76de660cb4015305c", "references": [{"authors": ["W.H. Manning", "A. DiLollo"], "title": "Clinical decision making in fluency disorders", "venue": "Plural Publishing", "year": 2017}, {"authors": ["J. Myers", "F. Irani", "E. Golob", "J. Mock"], "title": "and K", "venue": "Robbins, \u201cSingle-Trial Classification of Disfluent Brain States in Adults Who Stutter,\u201d Proc. - 2018 IEEE Int. Conf. Syst. Man, Cybern. SMC 2018, pp. 57\u201362", "year": 2019}, {"authors": ["P. Ekman", "E.L. Rosenberg"], "title": "What the Face RevealsBasic and Applied Studies of Spontaneous Expression Using the Facial Action Coding System (FACS)", "year": 2005}, {"authors": ["E. Friesen", "P. Ekman"], "title": "Facial action coding system: a technique for the measurement of facial movement,", "venue": "Palo Alto,", "year": 1978}, {"authors": ["M.A. Sayette", "J.F. Cohn", "J.M. Wertz", "M.A. Perrott", "D.J. Parrot"], "title": "A Psychometric Evaluation of the Facial Action Coding System for Assessing Spontaneous Expression Michael A", "venue": "Sayette, Jeffrey F. Cohn, Joan M. Wertz, Michael A. Perrott, and Dominic J. Parrott University of Pittsburgh,\u201d J. Nonverbal Behav.", "year": 2002}, {"authors": ["J. Hamm", "C.G. Kohler", "R.C. Gur", "R. Verma"], "title": "Automated Facial Action Coding System for dynamic analysis of facial expressions in neuropsychiatric disorders,", "venue": "J. Neurosci. Methods,", "year": 2011}, {"authors": ["A.C. Lints-Martindale", "T. Hadjistavropoulos", "B. Barber", "S.J. Gibson"], "title": "A Psychophysical Investigation of the Facial Action Coding System as an Index of Pain Variability among Older Adults with and without Alzheimer\u2019s Disease,", "venue": "Pain Med.,", "year": 2007}, {"authors": ["Z. Wang", "S. Wang", "Q. Ji"], "title": "Capturing Complex Spatio-temporal Relations among Facial Muscles for Facial Expression Recognition,", "venue": "IEEE Conf. Comput. Vis. Pattern Recognit. IEEE,", "year": 2013}, {"authors": ["T. Wehrle", "S. Kaiser", "S. Schmidt"], "title": "and K", "venue": "R. Scherer, \u201cStudying the dynamics of emotional expression using synthesized facial muscle movements.\u201d J. Pers. Soc. Psychol., vol. 78, no. 1, pp. 105\u2013119", "year": 2000}, {"authors": ["M. Mehu", "M. Mortillaro", "T. B\u00e4nziger", "K.R. Scherer"], "title": "Reliable facial muscle activation enhances recognizability and credibility of emotional expression.", "venue": "Emotion, vol. 12,", "year": 2012}, {"authors": ["Z. Meng", "S. Han"], "title": "and Y", "venue": "Tong, \u201cListen to your face: Inferring facial action units from audio channel,\u201d IEEE Transactions on Affective Computing", "year": 2017}, {"authors": ["J. Zhang", "B. Dong"], "title": "and Y", "venue": "Yan, \u201cA computer-assist algorithm to detect repetitive stuttering automatically,\u201d Proc. - 2013 Int. Conf. Asian Lang. Process. IALP 2013, pp. 249\u2013252", "year": 2013}, {"authors": ["B. Villegas", "K.M. Flores", "K. Jose Acuna", "K. Pacheco-Barrios", "D. Elias"], "title": "A Novel Stuttering Disfluency Classification System Based on Respiratory Biosignals,", "venue": "41st Annu. Int. Conf. IEEE Eng. Med. Biol. Soc. IEEE,", "year": 2019}, {"authors": ["J.P. Teixeira", "M.G. Fernandes"], "title": "and R", "venue": "A. Costa, \u201cAutomatic Determination of Pauses in Speech for Classification of Stuttering Disorder,\u201d in Design, Development, and Integration of Reliable Electronic Healthcare Platforms", "year": 2017}, {"authors": ["P. Mahesha", "D.S. Vinod"], "title": "LP-Hillbert transform based MFCC for effective discrimination of stuttering dysfluencies,", "venue": "Int. Conf. Wirel. Commun. Signal Process. Netw. IEEE,", "year": 2017}, {"authors": ["A. Das", "P. Rad"], "title": "Opportunities and challenges in explainable artificial intelligence (xai): A survey,", "venue": "arXiv preprint arXiv:2006.11371,", "year": 2020}, {"authors": ["A. Holzinger", "C. Biemann", "C.S. Pattichis"], "title": "and D", "venue": "B. Kell, \u201cWhat do we need to build explainable ai systems for the medical domain?\u201d arXiv preprint arXiv:1712.09923", "year": 2017}, {"authors": ["S.M. Lundberg", "S.-I. Lee"], "title": "A unified approach to interpreting model predictions,", "venue": "Advances in neural information processing systems,", "year": 2017}, {"authors": ["J. Tan", "Y. Gao", "Z. Liang", "W. Cao", "M.J. Pomeroy", "Y. Huo", "L. Li", "M.A. Barish", "A.F. Abbasi"], "title": "and P", "venue": "J. Pickhardt, \u201c3d-glcm cnn: A 3-dimensional gray-level co-occurrence matrix based cnn model for polyp classification via ct colonography,\u201d IEEE Transactions on Medical Imaging", "year": 2019}, {"authors": ["J.R. Mock", "A.L. Foundas"], "title": "and E", "venue": "J. Golob, \u201cSpeech preparation in adults with persistent developmental stuttering,\u201d Brain and language, vol. 149, pp. 97\u2013105", "year": 2015}, {"authors": ["A.L. Mock"], "title": "Jeffrey R Foundas and E", "venue": "J. Golob, \u201cCortical activity during cued picture naming predicts individual differences in stuttering frequency,\u201d Clinical Neurophysiology, vol. 127, no. 9, pp. 3093\u20133101", "year": 2016}, {"authors": ["J.G. Sheehan"], "title": "Stuttering behavior: A phonetic analysis,", "venue": "Journal of Communication Disorders,", "year": 1974}, {"authors": ["O. Bloodstein", "N.B. Ratner"], "title": "A handbook on stuttering", "venue": "Clifton Park (N.Y.): Thomson/Delmar Learning", "year": 2008}, {"authors": ["T. Baltrusaitis", "M. Mahmoud", "P. Robinson"], "title": "Cross-dataset learning and person-specific normalisation for automatic Action Unit detection,", "venue": "in 2015 11th IEEE Int. Conf. Work. Autom. Face Gesture Recognit. IEEE,", "year": 2015}, {"authors": ["M.D. Samad", "N. Diawara", "J.L. Bobzien", "J.W. Harrington", "M.A. Witherow", "K.M. Iftekharuddin"], "title": "A feasibility study of autism behavioral markers in spontaneous facial", "venue": "visual, and hand movement response data,\u201d IEEE Transactions on Neural Systems and Rehabilitation Engineering, vol. 26, no. 2, pp. 353\u2013361", "year": 2017}, {"authors": ["V.J. Lawhern", "A.J. Solon", "N.R. Waytowich", "S.M. Gordon", "C.P. Hung", "B.J. Lance"], "title": "EEGNet: a compact convolutional neural network for EEG-based brain\u2013computer interfaces,", "venue": "J. Neural Eng., vol. 15,", "year": 2018}, {"authors": ["M. Abadi", "P. Barham", "J. Chen", "Z. Chen", "A. Davis", "J. Dean", "M. Devin", "S. Ghemawat", "G. Irving"], "title": "M", "venue": "Isard et al., \u201cTensorflow: A system for largescale machine learning,\u201d in 12th {USENIX} symposium on operating systems design and implementation ({OSDI} 16)", "year": 2016}, {"authors": ["C.A. Stewart", "G. Turner", "M. Vaughn", "N.I. Gaffney", "T.M. Cockerill", "I. Foster", "D. Hancock", "N. Merchant", "E. Skidmore", "D. Stanzione", "J. Taylor"], "title": "and S", "venue": "Tuecke, \u201cJetstream,\u201d in Proc. 2015 XSEDE Conf. Sci. Adv. Enabled by Enhanc. Cyberinfrastructure - XSEDE \u201915. New York, New York, USA: ACM Press", "year": 2015}, {"authors": ["J. Towns", "T. Cockerill", "M. Dahan", "I. Foster", "K. Gaither", "A. Grimshaw", "V. Hazlewood", "S. Lathrop", "D. Lifka", "G.D. Peterson", "R. Roskies", "J.R. Scott", "N. Wilkins-Diehr"], "title": "XSEDE: Accelerating Scientific Discovery,", "venue": "Comput. Sci. Eng., vol. 16,", "year": 2014}, {"authors": ["H. Chacon", "S. Silva", "P. Rad"], "title": "Deep Learning Poison Data Attack Detection,", "venue": "IEEE 31st Int. Conf. Tools with Artif. Intell. IEEE,", "year": 2019}, {"authors": ["A. Weller"], "title": "Transparency: Motivations and Challenges,", "venue": "Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics)", "year": 1170}, {"authors": ["S.M. Lundberg", "S.I. Lee"], "title": "A unified approach to interpreting model predictions,", "venue": "in Adv. Neural Inf. Process. Syst.,", "year": 2017}, {"authors": ["M. Ancona", "E. Ceolini", "C. \u00d6ztireli", "M. Gross"], "title": "Towards better understanding of gradient-based attribution methods for Deep Neural Networks,", "venue": "6th Int. Conf. Learn. Represent. ICLR 2018 - Conf. Track Proc.,", "year": 2018}, {"authors": ["D. Prins", "R.J. Ingham"], "title": "Evidence-based treatment and stuttering\u2014historical perspective,", "venue": "Journal of Speech, Language, and Hearing Research,", "year": 2009}, {"authors": ["K. Hancock", "A. Craig", "C. McCready", "A. McCaul", "D. Costello", "K. Campbell", "G. Gilmore"], "title": "Two- to Six-Year Controlled-Trial Stuttering Outcomes for Children and Adolescents,", "venue": "J. Speech, Lang. Hear. Res.,", "year": 1998}, {"authors": ["G.D. Bodie"], "title": "A racing heart", "venue": "rattling knees, and ruminative thoughts: Defining, explaining, and treating public speaking anxiety,\u201d Communication education, vol. 59, no. 1, pp. 70\u2013105", "year": 2010}], "sections": [{"text": "Index Terms\u2014stuttering, speech disfluency, deep learning, explainable artificial intelligence, machine learning\nI. INTRODUCTION\nDEVELOPMENTAL stuttering is speech fluency disorderthat begins in early childhood, with a reported incidence This work has been submitted to the IEEE for possible publication. Copyright may be transferred without notice, after which this version may no longer be accessible. This work was partly supported by National Institutes of Health (NIH) under Grant DC016353 and the Open Cloud Institute (OCI) at University of Texas at San Antonio (UTSA). The authors gratefully acknowledge the use of the services of Jetstream cloud.\nA. Das, H. Chacon, and P. Najafirad are members of the Secure AI and Autonomy Laboratory, University of Texas at San Antonio, San Antonio, TX, 78249 USA. e-mail: (arun.das, henry.chacon, peyman.najafirad)@utsa.edu\nP. Najafirad is also with the Department of Information Systems and Cyber Security, University of Texas at San Antonio, San Antonio, TX, 78249 USA.\nJ. Mock and E. Golob are with the Department of Psychology, University of Texas at San Antonio, San Antonio, TX, 78249 USA. e-mail: (jeffrey.mock, edward.golob)@utsa.edu.\nF. Irani is with the Department of Communication Disorders, Texas State University, San Marcos, TX, 78665 USA. e-mail: firani@txstate.edu\nJ. Mock, E. Golob, and F. Irani are also members of the Cognitive Neuroscience Laboratory, University of Texas at San Antonio, San Antonio, TX, 78249 USA.\nof 5% and prevalence of 1% in adults. Approximately 80% of children recovery within 15 months from onset. For those children who persist and continue to stutter into adulthood, stuttering is characterized by the presence of excessive disfluencies interrupting the forward flow of speech. Specifically stutteringlike disfluencies (SLDs) include silent blocks, sound/syllable prolongations and part-word repetitions [1]. SLDs are rarely observed in the speech of those who do not stutter. For adults who stutter (AWS), these SLDs (or disfluency) are often accompanied by numerous secondary behaviors that can be observed, before, during, and after the production of a SLD [2]. Advances in our understanding of the neurophysiological underpinnings of stuttering have established that stuttering is associated with structural and functional differences in the speech motor planning and programming regions of the brain. Previous research in our lab using EEG has demonstrated that neural oscillations occurring before the speaker attempts to speak can be used to predict whether an AWS will speak fluently or stutter with 81% classification accuracy [3]. Using the same speech preparation paradigm, the central hypothesis of our current line of work is that at a given moment in time, the brain of an AWS is in a probabilistic state that exists between the extremes of certain generation of fluent or stuttered speech i.e. p(fluent speech) \u2208 (0, 1). We hypothesize that facial muscle activity can be used as an external marker to index internal brain states (see Myers et al., 2018 [3]). We will test this hypothesis by determining if facial muscle activity measures, processed by convolutional neural networks (CNNs), can accurately predict if upcoming speech in AWS will be fluent or disfluent. We hypothesize that facial muscle activity can index fluent/disfluent brain states in AWS for the following three reasons.\nFirst, pioneering work by Conture et al. [5] successfully distinguished non- speech facial motor activity patterns in children who stutter vs. fluent controls, even though only imprecise electromyogram (EMG) measures were available. Additionally, providing EMG feedback of facial muscle activity patterns increases the effectiveness of speech therapy in school-age children who stutter [6]. Second, there are plausible neuroanatomical mechanisms for why facial muscles may show different activation patterns before fluent/disfluent speech. Facial areas of the motor cortex body map in the brain are located right next to areas controlling speech articulators. Classic ideas suggest \u201cmotor overflow\u201d is a general phenomenon in neurological disorders, whereby imprecise motor\nar X\niv :2\n01 0.\n01 23\n1v 1\n[ cs\n.C V\n] 2\nO ct\n2 02\n0\ncontrol at the level of motor cortex generates actions beyond those that were intended [7]. Relatedly, AWS also have deficits in motor sequence learning [8]. Third, emotion and affect are important aspects of stuttering [9], and facial microexpressions have a rich [10], although controversial history of indexing emotions [11].\nThe activity of specific facial muscles can be quantified as Action Units (AUs), which are based on the Facial Action Unit Coding System (FACS) of Ekman and colleagues [4], [5]. Facial AUs have been used extensively to study affect and spontaneous expressions [6]\u2013[8]. Numerous studies have expressed the correlations and dynamics between emotional expression and upper facial muscle movements [9]\u2013[11]. Also, lower AUs around the lips have been shown to be precise enough to classify the specific phoneme sequence created while speaking [12]. This study tries to take the next step and use facial muscle activity during speech preparation to distinguish whether an upcoming vocalization will be fluent or stuttered in AWS. Up until now, all automated analysis for distinguishing fluent vs. stuttered speech has been done with the aim to classify words within a recorded speech sample as fluent or stuttered by using either auditory vocalization data [13], or respiratory rates [14]. To the best of our knowledge, we present the first work that uses pre-speech facial muscle movement to predict future behavior (fluent or stuttered speech, within approximately 2 to 3 seconds). The proposed method presents a non-invasive way of identifying pre-speech facial muscle movements that can classify future speech as either fluent or stuttered in AWS.\nWe hypothesize that (1) pre-speech facial activity in AWS contain enough information to accurately classify near future speech as either fluent or stuttered, (2) while AWS prepare to speak, AUs can independently distinguish levels of speechmotor preparation and emotion related to speech output, (3) upper facial AUs could better index arousal and anticipation, and show greater activity in stuttered vs. fluent speech trials, (4) lower facial AUs will better index the level of motor preparation and lower facial AUs will show peak activity the closer one gets to speech onset, will be greater for stuttered vs. fluent speech trials, and show greater activity when a specific speech plan is held in memory. We test our hypothesis with rigorous experimental and statistical analyses.\nOur major contributions are towards evaluating these hypotheses and can be summarized as below:\n\u2022 We developed an experimental methodology to collect facial muscles before speech vocalization in AWS. \u2022 We propose the use of a novel DL architecture based on Convolution Neural Network (CNN) to classify future speech as either fluent or stuttered using pre-speech facial muscle movement activity from FACS. \u2022 To explain the classifier decisions, we will use and integrate DeepSHAP explainable AI algorithm to generate temporal explanation maps describing the significance of each AU over time.\nThe remaining manuscript is organized as follows: Section II provides summaries of related works and a background review. Section III describes the experimental design for human subject study, the AI, and XAI methods. Section IV,\npresents the experimental results of the proposed methods and statistical analysis. In Section V, we discuss the implications and impact of the results. Finally, in Section VI, we conclude the study and share future directions."}, {"heading": "II. BACKGROUND REVIEW", "text": ""}, {"heading": "A. Existing AI-based Stuttering Classifiers", "text": "Myers et al. showed that neural oscillations occurring before the speaker attempts to speak can be used to predict whether an AWS will speak fluently or stutter with 81% classification accuracy [3]. Audio data [15] and respiratory biosignals [14] have been used for classifying speech utterances that contain a silent motor block in individuals who stutter. In [14], authors trained a multi-layer perceptron to differentiate block and nonblock states from respiration rate and got 82.6% classification accuracy on a test set omitted from training. However, the area-under-curve, precision, and recall values for the test was not included. The study also used invasive data collection methods due to requirement of elastic bands around the chest for respiratory inductance plethysmography (RIP) method and pulse oximeter.\nAn audio based automated stuttering classifier is described by Teixeira et al. in [15]. The authors showed that the percentage of silence to speech in audio recordings could separate fluent from disfluent speech in people who stutter by using several methods such as zero crossing rate, moving average, and moving energy. However, authors recorded that the degree of speech disfluency is much harder to classify only with a measure of silence and speech. Work done by Mahesha et al. [16] suggested better classifier performance using Mel Frequency Cepstral Coefficients (MFCC) to represent speech audio data. Audio based stuttering classifiers, though non-invasive, require a quiet environment for optimum performance as described in [15]. This fundamental limitation discourages its use in a real-world environment with ambient noise. Further, these methods only detect stuttering at the time it happens and have not been tested in real-time. Our current study takes a large step forward from the above studies by using non-invasive facial AUs to determine if pre-speech facial movements can predict whether the upcoming utterance in AWS will be fluent or stuttered."}, {"heading": "B. Explaining AI Classifier Decisions", "text": "Recent research has shed light on the importance of explainability in AI [17]. The highly non-linear nature of deep learning algorithms restricts its use in mission critical applications such as healthcare. There is a need to explain the responsible features for a particular AI decision [18]. In [19], authors described SHAP, a game-theoretic method to explain feature importance towards particular AI decisions. Recent research in medical domain showed positive results in the use of SHAP to supplement model decisions [20]. DeepSHAP method, a variant of SHAP, generates an explanation map, also called an attribution map, which describes the relevance of each input feature towards the AI decision. Thus, by studying the explanation map, we could evaluate the influence of AUs over time that correspond to near future speech disfluencies in AWS."}, {"heading": "III. METHODS", "text": "This section describes in detail the experimental design for separating speech preparation from speech production in AWS, the AI, and XAI frameworks. To aid the readers, we split this section to discuss the experimental design and protocols, DL classifier details, and design considerations for DeepSHAP explainability."}, {"heading": "A. Experimental Design and Protocol", "text": "The current experimental design was based off our prior research that investigated how EEG data could identify moments during speech preparation that both differ between AWS and fluent adults [21], [22], and can be used to classify brain states conducive to fluent or stuttered speech in AWS [3]. We target speech preparation since over 90% of disfluent events occur on the initial sound/syllable of an utterance [23]. In this study, we focus on non-invasive data collection methods using cameras to understand the role of facial muscle activity during speech preparation to classify stuttering disfluency. A certified speechlanguage pathologist was consulted to categorize individual trials to either fluent or stuttered.\n1) Subjects: Seven male AWS (age=23.3\u00b14 years, range 18-31 years) participated in the study. Each subject selfreported stuttering onset occurred during childhood and a certified speech-language pathologist diagnosed all of them with persistent developmental stuttering. On separate days, subjects completed between 1-5 sessions each (avg=3.7\u00b11.7 sessions). Subjects have different numbers of sessions because we didn\u2019t start collecting video of the experimental stimuli (i.e. camera behind the subject) until midway through the data collection of 12 subjects (11 completing 5 sessions and 1 completing 3 sessions). Each participant signed a consent form, and studies were done in accordance to the protocols approved by The University of Texas at San Antonio Institutional Review Board, consistent with the Declaration of Helsinki.\n2) Hardware-Software Design: To capture facial muscle movements in relation to the onset of the experimental stimuli, the face of the subject and the monitor used to present the visual stimuli were both recorded with two high-resolution cameras (Logitech C920 HD Pro) at 58 frames-per-second (FPS). The software ManyCam was used to combine both video streams into one continuous recording. Each recording was stored on encrypted UTSA assets according to subject, study, paradigm, and block information. Both 64 channels of EEG (Compumedics Neuroscan, Charlotte, NC) and highresolution eye-tracking data (EyeLink 1000 Plus, SR research) were also recorded during all sessions. See Figure 1 for a picture of the experimental setup and Figure 2 for various signals collected during the experiment. The current publication focuses solely on the facial AUs obtained by video.\n3) Task and Paradigms: To separate speech preparation from production, we imposed a delay between preparation and production by using a \u2018S1-S2\u2019 task. Our S1-S2 task provided a 1500 ms speech preparation period between S1 and S2 with the subject speaking after S2 onset as illustrated in Figure 3. Data was collected using four variations of the \u2018S1-S2\u2019 task\nwith 100 trials of each variation (50 trials/block, 2 blocks/task, 400 trial/session).\nFor the duration of the experiment, each subject was seated in a sound booth that consisted of a computer monitor in front of them and 2 speakers directly to the left and right (24\u201d away from head midline). Each trial started with a S1 visual stimulus (1000 ms duration), followed by 500 ms of a blank screen before the onset of a S2 visual stimulus (1000 ms duration) that signaled the subject to speak. In two of the task variations, named Word-Go (WG) and Word-AuditoryGo (WAG), S1 was a non-word pair and the S2 was three explanation points (!!!). The WG/WAG variations provided maximum information between S1-S2 on what was to be spoken after S2 onset. In the other two variations, Cue-Word\n(CW) and Cue-Auditory-Word (CAW), S1 consisted of a cross (+) while S2 was a non-word pair. The CW/CAW variations provided no information between S1-S2 on what was to be spoken except on the timing of S2 onset that told the subject to speak. The WAG/CAW task variations presented a tone (1000 Hz, 200 ms duration) 600 ms after S1, a time range our previous study found to correlate between EEG responses and individual differences in stuttering rate [22]. Due to no evidence that facial AUs are influenced by the auditory probe, the WAG and CAW tasks were grouped with the WG and CW tasks, respectively for this study.\nAll tasks used non-word pairs (e.g. \u2018jukel hemes\u2019). Nonwords were used because they phonetically mimic English words, have no associated meaning or emotional connotation, and allow for a more equal ratio of fluent to stuttered trials by increasing stuttering frequency above the typical 10% average [24]. At the beginning of each session, a MATLAB script randomly selected 400 non-word pairs from a list of 900 possible non-words (avg. 2.1\u00b10.6 syllables) with the limitation that the first and second non-words don\u2019t start with the same letter. This step produced unique non-word pairs on each trial and session. Offline, each trial was coded by a certified speech-language pathologist that specializes in developmental stuttering (4th author). Each trial was coded from the continuous video/ audio recordings as either fluent, unambiguous stuttering, ambiguous/normal disfluency (i.e. interjection such as \u2018um\u2019), or missed. A stuttered trial (unambiguous stuttering) was defined as either a silent block, a sound prolongation, or a part-word repetition. Only trials coded as fluent or stuttered were used in data analysis."}, {"heading": "B. Face Muscle Movement Feature Extraction", "text": "Face AU Extraction: In order to extract facial action units, we used the model described in [25] with dynamic regressors which generates 17 facial action units from an incoming stream of video. The AI model to detect AUs is trained on different datasets including BP4D, SEMAINE, and DISFA, and generates person specific normalized action units with a generalized support vector regression (SVR) model. Hence, for each input frame, we get a vector of 17 elements representing 17 different action units from different regions of the face. We split the AU regions to upper face and lower facial regions. Specifically, AUs 1, 2, 4, 5, 6, 7, 9, and 45 constitute the upper facial region while AUs 10, 12, 14, 15, 17, 20, 23, 25, and 26 constitute the lower facial region.\nPreprocessing: At 58 FPS, the pre-speech information for one word utterance, as illustrated in Figure 3, accounts for 1.5 seconds of information between S1 and S2. This translates to 87 temporal data points. Considering 17 AUs extracted, we preprocess individual trials as a 17\u00d7 87 matrix normalized in the range of 0 and 1.\nDataset Generation for Deep Learning: Individual subject data can be considered independent of each other. Also, since each trial involves a generated non-word, individual trials can be also considered independent of each other despite similarities between some AUs. A similar design choice was also done in [26]. Now, the goal of the CNN algorithm is to learn temporal dynamics between individual fluent and stuttered trials. After collecting face AU data from all subjects as matrices, we concatenate all AUs to generate the final datasets for deep learning study. Then, we split the AU datasets to training, test, and validation sets. Final dataset statistics is tabulated in Table I."}, {"heading": "C. CNN Training Scheme: Disfluency Classification using Convolutional Neural Networks", "text": "Primary goals of our classifier, as illustrated in Figure 4, is to learn the temporal dynamics of facial muscle movements to predict if a trial was stuttered or fluent and then explain which muscle groups contributed to the classification. Well researched CNN architectures such as ResNet cannot be used in this case due to unequal size of input data dimensions. We design a custom CNN architecture with rectangle kernels and carry out DepthWise and Separable convolutions inspired from EEGNet [27] to learn the temporal changes of AU data. We call this architecture CNN-A. For comparison, we also designed CNN-B similar to the popular VGG-16 architecture with repeated convolution operations with square kernels. We describe our CNN-A architecture here in detail.\nConsider f as our CNN-A model to be trained with input data x \u2208 Xtrain with labels y \u2208 Ytrain where Xtrain and Ytrain constitute the training dataset. Shape of each x will be\n17\u00d787 as we have 17 AUs and 87 timesteps of information. If \u03b8 is the parameters of the model f , our goal for deep learning training is to optimize \u03b8 such that our predictions y\u0304 = f(\u03b8, x) are accurate.\nWe carry out a 2D convolution on the input matrix with C channels and T timesteps. Since we have 17 AUs extracted, C = 17 for the current study. This is followed by a DepthWise 2D convolution, with a kernel of (17, 1) with a depth multiplier of 2 which allows extraction of frequency-specific spatial kernels and reduces the number of parameters in the output. A 2D average pooling reduces the dimensions of the generated feature maps. The output is further passed to a 2D Separable convolution layer, which learns to decouple the relationship within and across feature maps. This is done using a two-step process, where a kernel learns to summarize each feature map individually, and afterwards merging them optimally.\nWe feed the output of Separable convolution layer to a Fully Connected Dense layer with 128 neurons. This Dense layer carries the spatial information extracted by prior CNN layers. We call this the embedding layer, which encodes all spatial information. The final classifier layer outputs logits indicating whether a trial is classified as either fluent or stuttered. We use Exponential Linear Unit (ELU) activation function after all convolution and dense layers and use binary cross-entropy as the loss function for the optimization.\nCNN-B consists of four convolution layers of filters 16, 32, 64, and 128 respectively, all using (4,4) kernels. Batch normalization and 2D average pooling with a (2,2) filter is applied after each convolution operation. Three Fully Connected Dense layers of size 256, 128, and 64 are used before the final classifier layer. Comparing performance of CNN-B with CNN-A could reveal importance of temporal kernels in CNN-A, if any.\n1) Actions taken to prevent overfitting issues: Deep learning networks are non-linear and have many parameters, which can lead to overfitting (i.e. the network learns tiny details and noise in the training set but doesn\u2019t generalize to new data). We use a combination of batch normalization, dropout, and earlystopping to curb overfitting issues. Each convolution operation in the proposed network is followed by a batch normalization operation to normalize the activations of the previous layer,\neffectively reducing the internal covariance shifts that might occur in the architecture.\nBatch normalization technique normalizes the parameters of the deep neural network by multiplying the parameters with the standard deviation and subtracting the mean of parameters. It also works as a regularization for the neural network. Dropout randomly removes the influence of certain neurons towards the output decision, which forces the network to learn better representations of the input. Early stopping is a technique where the training can be stopped according to the training and validation loss. If the validation loss continuously goes higher than training loss, we can stop the training and curb overfitting.\n2) Training Hyperparameters: To implement both CNN-A and -B, we used TensorFlow framework [28]. The networks were trained with a batch size of 256 to optimize the parameters using Stochastic Gradient Descend (SGD). Training was scheduled for 500 epochs with early stopping based on validation loss with a patience of 30 epochs. Training started with a high learning rate (LR) of 0.01 and reduced by a factor of 0.5 when the validation loss fails to improve for 15 epochs. This is done using a LR scheduler with a mininum LR of 1e\u22126. Logs were generated and saved in Tensorboard.\n3) Cross-Validation Strategy for Training Data: In order to make better decisions, the classifier should learn meaningful features and semantics from training data while giving good performance on hold-out testing data which is not used during the training process. To ensure consistent performance, a 5- fold cross-validation strategy was used. Here, after keeping a randomly picked portion of data as the hold-out test set, we picked the training and validation data as five random folds which are equally distributed between stuttered and fluent trials. This 50-50 split on the fluent and stutter trial improves learning decisions and reduces issues related to skewness in data. We measure the performance metrics related to all five training folds to compare the learning performance. All data processing and model training were done on Jetstream research computing cloud [29], [30] with NVIDIA GPUs.\n4) Performance Metrics: To measure the predictive performance of the proposed deep learning model, we use a combination of accuracy, AUC-ROC which is the Area-under-\ncurve (AUC) of the receiver operating characteristic (ROC), and F1 score. For binary problems, a measure of accuracy is generally not preferred as the main factor of performance due to erroneous interpretations on skewed datasets. Even though we use an equal split between fluent and stutter samples in each dataset for training, validating, and testing our deep learning models, we share other measurements for completeness."}, {"heading": "D. Design Considerations for Explainability Algorithm", "text": "Deep neural networks, despite their impressive performance on real-world datasets are harder to understand and interpret due to their non-linear nature and large number of parameters. Also, due to the black-box nature of deep learning models and other adversarial threats [31], it is often important to understand why a particular decision was made and what fundamental features were used to make that decision [17], [32]. Our CNN models for stuttering prediction using AU data suffer from the same flaws of \u2018black-box\u2019 nature of deep neural networks. To explain the muscle groups responsible for a stuttered or fluent speech in an individual trial, we propose the use of DeepSHAP method proposed by Lundberg et al. [33] by utilizing the DeepExplain [34] to generate explanations.\nImportance of an AU muscle group towards the classifier decision can be understood by studying the feature relevance matrix called attribution (explanation) map. Attribution maps quantify the importance of each AU features towards predicting a fluent or stuttered trial. By adding DeepSHAP as an explainer layer after individual predictions, our final CNN architecture for inference on new samples can be reconsidered as below.\nReconsidering f as our new trained CNN model function and \u03b8 the learnt parameters, with a new input instance x, our goal is to predict an accurate solution y\u0304 and generate an attribution map g which illustrate the feature importance over time with the constraint that shape of g is the same as shape of x. Thus, we can generate a correlation between the input x and attribution map g. Here, a positive value of attribution for a particular AU at a particular time-window t means that the corresponding AU is highly relevant and is improving the prediction probability of the AI decision around that specific time t. Negative values of attribution map g suggests that the feature is decreasing the class probability around a specific time.\nIndividual trial information once preprocessed generates a matrix with feature \u00d7 timestep dimensions specifically, 17\u00d7 87 for the AU data. The temporal nature of data should be used to generate explanations for specific time-windows to explain the behavior of the model. In order to accomplish this, we define the problem to explain the influence of a set time-window of information towards the particular output. Specifically, for face AU, we encode 17.24 ms of information in each pixel of the input matrix. This is done corresponding to the frequency of data collected under the 1500 ms S1-S2 task. Now, DeepSHAP algorithm can be applied on the input matrix to generate explanation maps which provide feature importance for small time-windows. Hence, each pixel in the\nexplanation map will correspond to the importance of 17.24 ms of the respective AU feature in a specific time of S1-S2 task towards an output prediction."}, {"heading": "IV. EXPERIMENTAL RESULTS", "text": "In this section we describe the classification results of both CNN-A, CNN-B, and compare them against a random forest (RF) classifier with 500 trees. After comparing the results, we study and statistically analyze, using ANOVA method, the muscle movement attributions of the S1-S2 task across AWS subjects based on their stuttering intensity and also based on different temporal ranges."}, {"heading": "A. Classification Results", "text": "Table II summarizes how well our CNN-A and CNN-B compares in performance against a random forest (RF) benchmark. Understanding the classifiability of each paradigms (CW and WG) could reveal impact of having the word in memory before speech vocalization and the associated facial muscles. Hence, we train the CNN-A, -B, and RF methods on datasets with all paradigms combined, just CW, and just WG. Figure 5 illustrates AUC-ROC, training and validation accuracy and loss curves of the CNN-A model trained on AU data for the different paradigms. Here, the training accuracy is a measure of how well the model was able to predict on data it had already seen. Validation accuracy is a measure of generalizability and model performance on data which was not included in training. By referring to Figure 5 rows 2 and 3, we can see an optimal fit of each models and infer that the model is not overfitting to the dataset. This can be concluded since training and validation curves follows a downward trend while the corresponding accuracies steadily increases. We can also see that early-stopping algorithm stops the training whenever the validation loss fails to improve. Hence, the number of epochs required for training these models are different for different paradigms.\nFor CNN-A, when all paradigms are trained together, we see an AUC-ROC value of 0.86\u00b10.01, accuracy of 80.03\u00b10.55%, and F1-score of 0.78\u00b1 0.01 suggesting good performance of the classifier on the hold-out test set. CW paradigm alone generated 0.84 \u00b1 0.01 AUC-ROC, an accuracy of 77.30 \u00b1 1.72%, and F1-score of 0.74\u00b1 0.02. Considering on the WG paradigm, we see an 0.84 \u00b1 0.01 AUC-ROC, an accuracy of\n77.88 \u00b1 0.58%, and F1-score of 0.75 \u00b1 0.01. Comparing the results with other models, that our proposed CNN-A model outperform CNN-B and random forest baselines by a large margin when all the data is considered. Both CNN-B and RF methods had significant reductions in accuracy and AUC-ROC for CW paradigm.\nWe argue that data samples from CW paradigm show more microexpressions, which are variations in the muscle movements for short periods of time, rather than macro expressions which are more apparent in WG paradigms. Hence, CW paradigm is harder to classify accurately. CNN-A architecture learns to represent microexpressions better than CNN-B due to the temporal kernels and convolution operations which considers the frequency-specific spatial kernels. To study the impact of different kernel sizes in CNN-B, we carried out an experiment by changing all kernel sizes to (2,2) and (6,6).\nThis relates to including more temporal information during each convolution operation. However, doing so also includes information from multiple AUs, unlike CNN-A. Table III summarizes the performance degradation caused by changing the kernel sizes. As we can see, no improvement in accuracy was found by lowering or increasing the kernel size."}, {"heading": "B. Statistical Significance of Face AUs", "text": "Statistical significance study, using methods such as ANOVA, could improve our confidence of trusting the generated explanations. Here, we use an ANOVA model to study the statistical significance of the generated attribution maps to generalize the facial AU patterns of our AWS subjects. We study the significance of holding hypothesis-1, different paradigms in final classification, stuttering rate especially highstutter-rate (HSR, stutter rate>40%) and low-stutter-rate (LSR, stutter rate<40%) subjects during speech preparation, and the significance of temporal information in specific time-windows. To understand the impact of specific time-windows in fluent and stuttered trials, we split the temporal range into a) 0-500 ms, b) 500-800 ms, and c) 1100-1500 ms."}, {"heading": "C. Explaining the Classifier Decisions", "text": "Figure 6 illustrates an explanation map (showing only the positive attributions to aid visualization) generated for a single stutter trial from a test sample of AU data collected from the first author attempting the S1-S2 task. Here, individual AUs are populated in the y-axis and timesteps in the x-axis. Upper and lower AUs are separated in the middle by a black line. Each square pixel in the explanation map corresponds to explanations generated for 17.24 ms of AU data as described in Section III-D. Explanation maps, similar to Figure 6 with both positive and negative attributions, are generated for all trials in the hold-out test set and a database of explanation maps were generated. Due to space constraints, further discussions will use ANOVA results or line plots of AUs extracted from similar explanation maps to aid the discussion.\nSignificance of holding hypothesis 1: Statistical analysis on the facial regions of all subjects revealed high significance of AU 6 (cheek raiser, F=23.47, p<0.005) and AU 4 (brow lowerer, F=19.17, p<0.005) from upper facial region, and AU 14 (dimpler, F=80.3, p<0.005), AU 10 (upper lip raiser, F=53.33, p<0.005), and AU 12 (lip corner puller F=27.19, p<0.005) from lower facial region. We evaluated our hypothesis that pre-speech facial activity can encode enough information to be classified as fluent or stuttered trials using the CNN models, of which the results are described in Table II. Furthermore, we find that the average attributions of all subjects for stutter trials, as illustrated in Figure 7 (a), for AU 6 and 14 which are significant across all subjects, are consistently higher than that of the fluent trials. This further strengthens our evaluation of the hypothesis using attribution maps generated using explainable AI methods.\nSignificance of paradigms: Statistical analysis on the attribution values indicated a weak statistical importance of paradigms (F=4.8, p<0.05) CW and WG in determining the trial as fluent or stuttered. However, this can neither be\ngeneralized nor neglected due to the weak significance. Further study needs to be done on a larger population of subjects to find a conclusion. Hence, our preliminary study shows that having the words in memory prior to speech preparation does not significantly impact the facial muscle responses but shows weak statistical correlation during speech preparation. Eventhough we find microexpressions in CW and larger active muscle movements in WG, more data is required to generalize the results. Hence, a part of hypothesis 4 regarding the impact of speech plan in short-term memory does not hold completely.\nSignificance of stutter rate: Stutter-rate of individual subjects revealed important findings and proved to be statistically significant in predicting stuttering disfluency (F=18.66, p<0.005). HSR subjects showed statistical significance for more AUs on both upper (AU 4: brow lowerer, 6: cheek raiser, 7: eyelid tightener, 45: eye blinker) and lower (AU 14: dimpler, 15: lip corner depressor, 17: chin raiser, 20: lip stretcher) facial regions compared to lower stutter rate (LSR). These results suggest that there are larger number upper and lower muscle groups contributing towards stuttered than fluent trials. Relatedly, we could infer that subjects who stuttered more on the speech preparation task had comparatively higher facial muscle activity than subjects who stuttered less.\nSignificance of time-windows: Our analysis suggests that AU 6 and AU 14 are consistently significant considering all subjects. Hence, we focus on AUs 6 and 14 for this study. As illustrated in Figure 7 (b), we see that AU 6 is statistically significant across all time-windows under consideration (a: F=43.91, p<0.001, b: F=43.86, p<0.001, c: F=50.1, p<0.001). Also, AU 14 is significant across 0-500 ms (F=40.83, p<0.001) and 1100-1500 ms (F=100.49, p<0.001) while statistically insignificant in 500-800ms (F=4.37, p>0.05) considering all subjects and trials. This correlates with the upward trend of AU 6 which peaks around 700 ms and drops rapidly towards 1500 ms showing high importance of upper facial muscles towards the point when the cue is removed at 1000 ms. Also, magnitude of attributions are higher in stutter trials than fluent trials as illustrated in Figure 7 (c). Hence, we can confirm hypothesis 3 that upper facial muscles show\ngreater activity in stutter vs. fluent trials. In contrast, attributions of AU 14 have a negative trend at first and starts to improve towards 500 ms. During 500- 1000 ms, there is very little change in the impact AU 14 has towards classifying stutter and fluent trials as illustrated in Figure 7 (d). However, during the 1100-1500 ms, just before the end of the trial, we see a large jump in the lower facial activity with considerable statistical significance. Also, we see larger magnitude of attributions for stutter trials vs. fluent trials indicating that parts of hypothesis 4 regarding peak activity towards S2 and greater activity for stutter trials holds true. This tends to show a behavioral pattern where importance of upper facial muscles is high at S1 and reduces towards S2, which could indicate more focused attention at S1 (hypothesis 3), while lower facial muscles improves towards S2 after being dormant during S1, which could indicate a motor preparation to speak after S2 (hypothesis 4) as illustrated in Figure 6."}, {"heading": "V. DISCUSSIONS", "text": "This work demonstrates an experimental framework, machine learning methods, and explainable AI algorithms capable of predicting solely from facial muscle movements whether an AWS will be fluent or stutter before they speak. The video data emphasized the usability of pre-speech facial activity while also confirming our primary hypothesis that pre-speech facial activity contains enough information to accurately differentiate in advance whether speech will be fluent or stuttered. This is a major finding that has many clinical implications. For instance, this model can be used to identify stuttering moments in realtime along with the presence of any observable secondary behaviors related to facial tension to estimate not only the frequency of stuttering, but also to estimate severity for assessment purposes. The same method can further be used in conjunction with behavioral therapy to detect stuttering moments and any associated facial tension to provide noninvasive feedback using standard hardware available on most digital devices (computers, tablets, smartphones).\nBehavioral stuttering therapy incorporates either fluency shaping or stuttering modification/management strategies, or a\ncombination of both to help AWS increase fluency and reduce stuttering severity [35]. Use of real-time feedback based on facial AUs can help with the identification stage of stuttering modification and also aid in providing real-time feedback to help reduce facial tension to decrease stuttering severity and increase overall fluency, similar to the effect seen from using facial EMG [36]. Clinical applications would need to be tested exhaustively in controlled experiments; however, the possibility is promising considering the evidence supporting the efficacy of EMG based bio-feedback [36] that required specialized equipment that is invasive.\nThe speech patterns of both fluent and stuttered speech along with CW and WG task variations showed very similar attribution patterns. Nonetheless, the data still supported our second hypothesis that disfluent speech trials would show higher facial AUs. The stuttered trials showed large variance and higher average response when compared to fluent trials. Attributions of two AUs (AUs 6 & 14) that peaked at different times appear to show the most promise for differentiating between future fluent or stuttered speech. Facial AU 6 (cheek raiser) from upper face increased rapidly from S1 up until 800 ms after S1, with a significantly higher peak in the stuttered vs. fluent speech trials. We suggest this upper face AU is related to the arousal associated with the start of each trial due to activity being driven by S1 onset and no observed difference between the WG and CW paradigms. High arousal is known to influence stuttering rates. For instance, public speaking can induce increased disfluencies in both people who stutter and fluent individuals [37].\nFacial AU 14 (dimpler) from lower face supported our hypothesis by showing increased attribution and variability before stuttered vs. fluent speech. Facial AU 14 starts to slowly increase after S1 until a rapid increase occurs right before S2, independent of paradigm. These finding also partially supports\nour hypotheses that lower facial AUs around the lips can index the anticipation of speaking by showing a rapid increase right before S2 onset which prompts the subject to speak. However, since there was no significant difference between the CW and WG tasks, the lower facial AUs don\u2019t appear to index whether a specific speech plan is being held in short-term memory.\nThe combination of these two AU, one for lower and upper facial regions also supports our hypothesis that facial AUs may be able to independently separate the amount of emotional arousal (upper AUs) and anticipation to speak (lower facial AUs). Our previous EEG research suggests brain states conducive to fluent or stuttered speech can change in short time frames (i.e. seconds) [3]. The non-word pairs were designed not to induce an emotional arousal, yet attributions of upper facial movements appear to capture such rapid shifts in arousal irrespective of S1 being a non-word pair or a cross that lets the subject know when S2 will appear. The ability to distinguish fluent for disfluent trials during speech preparation irrespective of paradigm (WG or CW) provides further evidence that moments of stuttering are triggered not just by what is about to be said but instead the brain state the person who stutters is trying to speak through."}, {"heading": "VI. CONCLUSIONS AND FUTURE WORK", "text": "This paper presents an XAI-assisted novel speech disfluency study to investigate the usability of facial muscle movements before speech vocalization to automatically classify upcoming speech as fluent or stuttered. Our study revealed temporal facial muscle movement patterns in the upper and lower facial regions of the face which indicate anticipation and speech preparation during our S1-S2 task which are related to stuttering. Consistently high correlation of AU6 (cheek raiser) and AU 14 (dimpler) along with similar but highly variable attribution values suggests that AWS have larger facial\nactivity before speech vocalization of stutter trials, which the AI algorithms can identify.\nThe results of this novel study proved our hypothesis that pre-speech facial activities encode enough information to distinguish if a future speech vocalization would be fluent or stuttered. However, a comprehensive study on larger cohort of AWS is required to generalize the idea and assess the broader impact of having the word to be spoken in short-term memory. With enough data, future studies could also find personalized facial muscle patterns or traits in each subject which we have aggregated during the data processing of the current study. Multi-modal studies including EEG and eye-tracker information could study correlations between brain activity and facial muscle movements of AWS to reveal cognitive and behavioral markers leading to stuttering."}], "title": "Stuttering Speech Disfluency Prediction using Explainable Attribution Vectors of Facial Muscle Movements", "year": 2020}