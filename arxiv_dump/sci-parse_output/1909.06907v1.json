{
  "abstractText": "We present a new explainable AI (XAI) framework aimed at increasing justified human trust and reliance in the AI machine through explanations. We pose explanation as an iterative communication process, i.e. dialog, between the machine and human user. More concretely, the machine generates sequence of explanations in a dialog which takes into account three important aspects at each dialog turn: (a) human\u2019s intention (or curiosity); (b) human\u2019s understanding of the machine; and (c) machine\u2019s understanding of the human user. To do this, we use Theory of Mind (ToM) which helps us in explicitly modeling human\u2019s intention, machine\u2019s mind as inferred by the human as well as human\u2019s mind as inferred by the machine. In other words, these explicit mental representations in ToM are incorporated to learn an optimal explanation policy that takes into account human\u2019s perception and beliefs. Furthermore, we also show that ToM facilitates in quantitatively measuring justified human trust in the machine by comparing all the three mental representations. We applied our framework to three visual recognition tasks, namely, image classification, action recognition, and human body pose estimation. We argue that our ToM based explanations are practical and more natural for both expert and non-expert users \u2217 Corresponding Author \u2020Email Addresses: aakual@ucla.edu (A.Akula), liucs.msu@gmail.com (C. Liu), sadiyasa@cse.msu.edu (S. Sadiya), hongjing@ucla.edu (H. Lu), sinisa@oregonstate.edu (S. Todorovic), jchai@cse.msu.edu (J.Y. Chai), sczhu@stat.ucla.edu (S.C. Zhu) Preprint submitted to (under review) September 17, 2019 ar X iv :1 90 9. 06 90 7v 1 [ cs .A I] 1 5 Se p 20 19 to understand the internal workings of complex machine learning models. To the best of our knowledge, this is the first work to derive explanations using ToM. Extensive human study experiments verify our hypotheses, showing that the proposed explanations significantly outperform the state-of-the-art XAI methods in terms of all the standard quantitative and qualitative XAI evaluation metrics including human trust, reliance, and explanation satisfaction.",
  "authors": [
    {
      "affiliations": [],
      "name": "Arjun R. Akula"
    },
    {
      "affiliations": [],
      "name": "Changsong Liu"
    },
    {
      "affiliations": [],
      "name": "Sari Saba-Sadiya"
    },
    {
      "affiliations": [],
      "name": "Hongjing Lu"
    },
    {
      "affiliations": [],
      "name": "Sinisa Todorovic"
    },
    {
      "affiliations": [],
      "name": "Joyce Y. Chai"
    },
    {
      "affiliations": [],
      "name": "Song-Chun Zhu"
    }
  ],
  "id": "SP:1d307d09710045a7882ecaf2535c84bd29c5a229",
  "references": [
    {
      "authors": [
        "E.T. Chancey",
        "J.P. Bliss",
        "A.B. Proaps",
        "P. Madhavan"
      ],
      "title": "The role of trust as a mediator between system characteristics and response behaviors",
      "venue": "Human factors 57 (6) ",
      "year": 2015
    },
    {
      "authors": [
        "V. Gulshan",
        "L. Peng",
        "M. Coram",
        "M.C. Stumpe",
        "D. Wu",
        "A. Narayanaswamy",
        "S. Venugopalan",
        "K. Widner",
        "T. Madams"
      ],
      "title": "J",
      "venue": "Cuadros, et al., Development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs, Jama 316 (22) ",
      "year": 2016
    },
    {
      "authors": [
        "Z.C. Lipton"
      ],
      "title": "The mythos of model interpretability",
      "venue": "in: ICML Workshop on Human Interpretability in Machine Learning",
      "year": 2016
    },
    {
      "authors": [
        "M.T. Ribeiro",
        "S. Singh",
        "C. Guestrin"
      ],
      "title": "Why should i trust you?: Explaining the predictions of any classifier",
      "venue": "in: Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, ACM",
      "year": 2016
    },
    {
      "authors": [
        "S. Yang",
        "Q. Gao",
        "S. Saba-Sadiya",
        "J. Chai"
      ],
      "title": "Commonsense justification for action explanation",
      "venue": "in: Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
      "year": 2018
    },
    {
      "authors": [
        "R. Ramprasaath",
        "D. Abhishek",
        "V. Ramakrishna",
        "C. Michael",
        "P. Devi",
        "B. Dhruv"
      ],
      "title": "Grad-cam: Why did you say that? visual explanations from deep networks via gradient-based localization, CVPR 2016",
      "year": 2016
    },
    {
      "authors": [
        "M.D. Zeiler",
        "R. Fergus"
      ],
      "title": "Visualizing and understanding convolutional networks",
      "venue": "in: European conference on computer vision, Springer",
      "year": 2014
    },
    {
      "authors": [
        "B. Kim",
        "C. Rudin",
        "J.A. Shah"
      ],
      "title": "The bayesian case model: A generative approach for case-based reasoning and prototype classification",
      "venue": "in: Advances in Neural Information Processing Systems",
      "year": 2014
    },
    {
      "authors": [
        "Q. Zhang",
        "Y. Nian Wu",
        "S.-C. Zhu"
      ],
      "title": "Interpretable convolutional neural networks",
      "venue": "in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition",
      "year": 2018
    },
    {
      "authors": [
        "S. Jain",
        "B.C. Wallace"
      ],
      "title": "Attention is not explanation, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL)arXiv:1902.10186",
      "venue": "URL http://arxiv.org/abs/1902.10186",
      "year": 1902
    },
    {
      "authors": [
        "H.H. Clark",
        "E.F. Schaefer"
      ],
      "title": "Contributing to discourse",
      "venue": "Cognitive science 13 (2) ",
      "year": 1989
    },
    {
      "authors": [
        "S. Devin",
        "R. Alami"
      ],
      "title": "An implemented theory of mind to improve humanrobot shared plans execution",
      "venue": "in: Human-Robot Interaction (HRI), 2016 11th ACM/IEEE International Conference on, IEEE",
      "year": 2016
    },
    {
      "authors": [
        "A.I. Goldman"
      ],
      "title": "Theory of Mind",
      "venue": "The Oxford handbook of philosophy of cognitive science",
      "year": 2012
    },
    {
      "authors": [
        "D. Premack",
        "G. Woodruff"
      ],
      "title": "Does the chimpanzee have a theory of mind",
      "venue": "Behavioral and brain sciences 1 (4) ",
      "year": 1978
    },
    {
      "authors": [
        "D. Erhan",
        "Y. Bengio",
        "A. Courville",
        "P. Vincent"
      ],
      "title": "Visualizing higher-layer features of a deep network",
      "venue": "Technical report, University of Montreal 1341 (3) ",
      "year": 2009
    },
    {
      "authors": [
        "L.A. Hendricks",
        "Z. Akata",
        "M. Rohrbach",
        "J. Donahue",
        "B. Schiele",
        "T. Darrell"
      ],
      "title": "Generating visual explanations",
      "venue": "in: European Conference on Computer Vision, Springer",
      "year": 2016
    },
    {
      "authors": [
        "A. Karpathy",
        "J. Johnson",
        "L. Fei-Fei"
      ],
      "title": "Visualizing and understanding recurrent networks, arXiv preprint arXiv:1506.02078",
      "year": 2078
    },
    {
      "authors": [
        "B. Zhou",
        "A. Khosla",
        "A. Lapedriza",
        "A. Oliva",
        "A. Torralba"
      ],
      "title": "Learning deep features for discriminative localization",
      "venue": "in: Computer Vision and Pattern Recognition (CVPR), 2016 IEEE Conference on, IEEE",
      "year": 2016
    },
    {
      "authors": [
        "A. Datta",
        "A. Datta",
        "A.D. Procaccia"
      ],
      "title": "Y",
      "venue": "Zick, Influence in classification via cooperative game theory., in: IJCAI",
      "year": 2015
    },
    {
      "authors": [
        "L. v. d. Maaten",
        "G. Hinton"
      ],
      "title": "Visualizing data using t-sne, Journal of machine learning research",
      "year": 2008
    },
    {
      "authors": [
        "Q. Zhang",
        "Y.N. Wu",
        "S.-C. Zhu"
      ],
      "title": "Interpretable convolutional neural networks",
      "venue": "The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) ",
      "year": 2018
    },
    {
      "authors": [
        "B. Kim",
        "M. Wattenberg",
        "J. Gilmer",
        "C. Cai",
        "J. Wexler"
      ],
      "title": "F",
      "venue": "Viegas, et al., Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav), in: International Conference on Machine Learning",
      "year": 2018
    },
    {
      "authors": [
        "A. Dhurandhar",
        "P.-Y. Chen",
        "R. Luss",
        "C.-C. Tu",
        "P. Ting",
        "K. Shanmugam",
        "P. Das"
      ],
      "title": "Explanations based on the missing: Towards contrastive explanations with pertinent negatives",
      "venue": "in: Advances in Neural Information Processing Systems",
      "year": 2018
    },
    {
      "authors": [
        "Y. Goyal",
        "Z. Wu",
        "J. Ernst",
        "D. Batra",
        "D. Parikh",
        "S. Lee"
      ],
      "title": "Counterfactual visual explanations",
      "venue": "in: ICML 2019",
      "year": 2019
    },
    {
      "authors": [
        "D.J. Hilton"
      ],
      "title": "Conversational processes and causal explanation",
      "venue": "Psychological Bulletin",
      "year": 1990
    },
    {
      "authors": [
        "T. Lombrozo"
      ],
      "title": "The structure and function of explanations",
      "venue": "Trends in cognitive sciences 10 (10) ",
      "year": 2006
    },
    {
      "authors": [
        "S. Park",
        "B.X. Nie",
        "S.-C. Zhu"
      ],
      "title": "Attribute and-or grammar for joint parsing of human attributes",
      "venue": "part and pose, IEEE Trans. on Pattern Analysis and Machine Intelligence (TPAMI) ",
      "year": 2018
    },
    {
      "authors": [
        "F. Gosselin",
        "P.G. Schyns"
      ],
      "title": "Bubbles: a technique to reveal the use of information in recognition tasks",
      "venue": "Vision research 41 (17) ",
      "year": 2001
    },
    {
      "authors": [
        "H.H. Clark",
        "D. Wilkes-Gibbs"
      ],
      "title": "Referring as a collaborative process",
      "venue": "Cognition 22 (1) ",
      "year": 1986
    },
    {
      "authors": [
        "R.R. Hoffman",
        "P.A. Hancock",
        "J.M. Bradshaw"
      ],
      "title": "Metrics",
      "venue": "metrics, metrics, part 2: Universal metrics?, IEEE Intelligent Systems 25 (6) ",
      "year": 2010
    },
    {
      "authors": [
        "A.P. Witkin"
      ],
      "title": "Scale-space filtering",
      "venue": "in: Readings in Computer Vision, Elsevier",
      "year": 1987
    },
    {
      "authors": [
        "L. Carlson",
        "D. Marcu",
        "M.E. Okurowski"
      ],
      "title": "Building a discourse-tagged corpus in the framework of rhetorical structure theory",
      "venue": "in: Current and new directions in discourse and dialogue, Springer",
      "year": 2003
    },
    {
      "authors": [
        "S. Hochreiter",
        "J. Schmidhuber"
      ],
      "title": "Long short-term memory",
      "venue": "Neural computation 9 (8) ",
      "year": 1997
    },
    {
      "authors": [
        "R.S. Sutton",
        "D.A. McAllester",
        "S.P. Singh",
        "Y. Mansour"
      ],
      "title": "Policy gradient methods for reinforcement learning with function approximation",
      "venue": "in: Advances in neural information processing systems",
      "year": 2000
    },
    {
      "authors": [
        "S. Johnson"
      ],
      "title": "M",
      "venue": "Everingham, Clustered pose and nonlinear appearance models for human pose estimation., in: BMVC, Vol. 2",
      "year": 2010
    }
  ],
  "sections": [
    {
      "text": "We present a new explainable AI (XAI) framework aimed at increasing justified human trust and reliance in the AI machine through explanations. We pose explanation as an iterative communication process, i.e. dialog, between the machine and human user. More concretely, the machine generates sequence of explanations in a dialog which takes into account three important aspects at each dialog turn: (a) human\u2019s intention (or curiosity); (b) human\u2019s understanding of the machine; and (c) machine\u2019s understanding of the human user. To do this, we use Theory of Mind (ToM) which helps us in explicitly modeling human\u2019s intention, machine\u2019s mind as inferred by the human as well as human\u2019s mind as inferred by the machine. In other words, these explicit mental representations in ToM are incorporated to learn an optimal explanation policy that takes into account human\u2019s perception and beliefs. Furthermore, we also show that ToM facilitates in quantitatively measuring justified human trust in the machine by comparing all the three mental representations.\nWe applied our framework to three visual recognition tasks, namely, image classification, action recognition, and human body pose estimation. We argue that our ToM based explanations are practical and more natural for both expert and non-expert users\n\u2217 Corresponding Author \u2020Email Addresses: aakual@ucla.edu (A.Akula), liucs.msu@gmail.com (C. Liu), sadiyasa@cse.msu.edu (S. Sadiya), hongjing@ucla.edu (H. Lu), sinisa@oregonstate.edu (S. Todorovic), jchai@cse.msu.edu (J.Y. Chai), sczhu@stat.ucla.edu (S.C. Zhu)\nPreprint submitted to (under review) September 17, 2019\nar X\niv :1\n90 9.\n06 90\n7v 1\n[ cs\n.A I]\n1 5\nto understand the internal workings of complex machine learning models. To the best of our knowledge, this is the first work to derive explanations using ToM. Extensive human study experiments verify our hypotheses, showing that the proposed explanations significantly outperform the state-of-the-art XAI methods in terms of all the standard quantitative and qualitative XAI evaluation metrics including human trust, reliance, and explanation satisfaction.\nKeywords: Explainable Artificial Intelligence, Theory of Mind, Interpretability."
    },
    {
      "heading": "1. Introduction",
      "text": ""
    },
    {
      "heading": "1.1. Motivation and Objective",
      "text": "From low risk environments such as movie recommendation systems and chatbots to high risk environments such as self-driving cars, drones, military applications and medical-diagnosis and treatment, Artificial Intelligence (AI) systems are becoming increasingly ubiquitous [1, 2, 3, 4]. AI is finding its way into a wide array of applications in education, finance, healthcare, telecommunication, and law enforcement. In particular, AI systems built using black box machine learning (ML) models \u2013 such as deep neural networks and large ensembles [5, 6, 7, 8, 9, 10, 11, 12, 13] \u2013 perform remarkably well on a broad range of tasks and are gaining widespread adoption. However understanding the behavior of these systems remains a significant challenge as they cannot explain why they reached a specific recommendation or a decision. This is especially problematic in high risk environments such as banking, healthcare, and insurance, where AI decisions can have significant consequences. Therefore, much hope rests on explanation methods as tools to understand the decisions made by these AI systems.\nExplainable AI (XAI) models, through explanations, make the underlying inference mechanism of AI systems transparent and interpretable to expert users (system developers) and non-expert users (end-users) [5, 6, 7, 14]. Explanations play a key role in integrating AI machines into our daily lives, i.e. XAI is essential to increase social acceptance of AI machines. As the decision making is being shifted from humans to machines, transparency and interpretability achieved with reliable explanations is\ncentral to solving AI problems such as Safety (e.g. how to operate self-driving cars safely), Bias & Fairness (e.g. how to detect and mitigate bias in ML models), Justified Human Trust in ML models (e.g. how to trust the output of these AI systems to inform our decisions), Model Debugging (e.g. how to improve my model by identifying points of model failure), and Ethics (e.g. how to ensure that ML models reflect our values) (Figure 1).\nIn this work, we focus mainly on measuring and increasing Justified Positive Trust (JPT) and Justified Negative Trust (JNT) [15] in AI systems. We measure JPT and JNT by evaluating the humans understanding of the machines (M) decision-making process. For example, let us consider an image classification task. Suppose if the machine M predicts images in the set C correctly and makes incorrect decisions on the images in the set W . Intuitively, JPT will be computed as the percentage of images in C that the human subject feltM would correctly predict. Similarly, JNT (also called as mistrust), will be computed as the percentage of images in W that the human subject felt M would fail to predict correctly. Note that this definition of justified positive and negative trust is domain generic and can be applied to any task. For example, in an AI-driven clinical world, our definitions of JPT and JNT can effectively measure how\nmuch doctors and patients understand the AI systems that assist in clinical decisions."
    },
    {
      "heading": "1.2. Introducing X-ToM: Explaining with Theory-of-Mind for Increasing JPT and JNT",
      "text": "Our work is motivated by the following three key observations:\n1. Attention is not a Good Explanation: Previous studies have shown that trust\nis closely and positively correlated to the level of how much human users understand the AI system \u2014 understandability \u2014 and how accurately they can predict the system\u2019s performance on a given task \u2014 predictability [14, 5, 15, 7]. Therefore there has been a growing interest in developing explainable AI systems (XAI) aimed at increasing understandability and predictability by providing explanations about the system\u2019s predictions to human users [5, 6, 7, 8]. Current works on XAI generate explanations about their performance in terms of, e.g., feature visualization and attention maps [9, 10, 11, 12, 13, 16]. However, solely generating explanations, regardless of their type (visualization or attention maps) and utility, is not sufficient for increasing understandability and predictability [17]. We verify this in our experiments (see Section 4).\n2. Explanation is an Interactive Communication Process: We believe that an\neffective explanation cannot be one shot and involves iterative process of communication between the human and the machine. The context of such interaction plays an important role in determining the utility of the follow-up explanations [18]. As humans can easily be overwhelmed with too many or too detailed explanations, interactive communication process helps in understanding the user and identify user-specific content for explanation. Moreover, cognitive studies [7] have shown an explanation can only be optimal if it is generated by taking user\u2019s perception and belief into account.\n3. Defining a Collaborative Task for the Communication Process: In our ex-\nperiments, we found that it is difficult to evaluate the effectiveness of explanations without constraining the communication process. In our framework, we constrain the communication by explicitly defining a collaborative task for the human user to solve through the explanations. Based on how many tasks that are\nsuccessfully solved by the user (and the number of explanations in the dialog), we measure the effectiveness of the explanations."
    },
    {
      "heading": "1.2.1. X-ToM Framework",
      "text": "Based on the above three key observations, we introduce an interactive explanation framework, X-ToM. In our framework, the machine generates sequence of explanations in a dialog which takes into account three important aspects at each dialog turn: (a) human\u2019s intention (or curiosity); (b) human\u2019s understanding of the machine; and (c) machine\u2019s understanding of the human user. To do this, we use Theory of Mind (ToM) which helps us in explicitly modeling human\u2019s intention, machine\u2019s mind as inferred by the human as well as human\u2019s mind as inferred by the machine. The ability to reason about other\u2019s perception and beliefs, in addition to one\u2019s own perception and beliefs, is often referred to as the Theory-of-Mind [19, 20, 21].\nMore specifically, in X-ToM, the machine and the user are positioned to solve a collaborative task, but the machine\u2019s mind (M ) and the human user\u2019s mind (U ) only have a partial knowledge of the environment (see Figure ref2). Hence, the machine\nand user need to communicate with each other, using their partial knowledge, otherwise they would not be able to optimally solve the collaborative task. The communication consists of two different types of question-answer (QA) exchanges \u2014 namely, a) Factoid question-answers about the environment (W-QA), where the user asks \u201cWH\u201dquestions that begin with what, which, where, and how; and b) Explanation seeking question-answers (E-QA), where the user asks questions that begin with why about the machine\u2019s inference. At each turn in the collaborative dialog, our X-ToM updates a model of human perception and beliefs, and uses this model for optimizing explanations in the next turn.\nWe argue that our interactive explanation framework based on ToM is practical and more natural for both expert and non-expert users to understand the internal workings of complex machine learning models. Furthermore, we also show that ToM facilitates in quantitatively measuring justified human trust in the machine by comparing all the three mental representations. To the best of our knowledge, this is the first work to derive explanations using ToM.\nWe applied our framework to three visual recognition tasks, namely, image classification, action recognition, and human body pose estimation. Using Amazon Mechanical Turk, we have collected explanation dialogs by interacting with turkers through X-ToM framework. From there, X-ToM learned an optimal explanation policy that takes into account user perception and beliefs. Through our extensive human studies, we show that X-ToM allows the user to achieve a high success rate in visual recognition on blurred images, and does so very efficiently in a few dialog exchanges. We also found that the most popularly used attribution based explanations (viz. saliency maps) are not effective to improve human trust in AI system, whereas our Theory-of-Mind inspired approach significantly improves human trust in AI by providing effective explanations."
    },
    {
      "heading": "1.3. Related Work",
      "text": "Generating explanations or justifications of predictions or decisions made by an AI system has been widely explored in AI. Most prior work has focused on generating explanations using feature visualization and attribution.\nFeature visualization techniques typically identify qualitative interpretations of features used for making predictions or decisions. Recently, there has been an increased interest in developing feature visualizations for deep learning models, especially for Convolutional Neural Nets (CNNs) in computer vision applications, and Recurrent Neural Nets (RNNs) in NLP applications. For example, gradient ascent optimization is used in the image space to visualize the hidden feature layers of unsupervised deep architectures [22]. Also, convolutional layers are visualized by reconstructing the input of each layer from its output [11]. Recent visual explanation models seek to jointly classify the image and explain why the predicted class label is appropriate for the image [23]. Other related work includes a visualization-based explanation framework for Naive Bayes classifiers [24], an interpretable character-level language models for analyzing the predictions in RNNs [25], and an interactive visualization for facilitating analysis of RNN hidden states [26].\nAttribution is a set of techniques that highlight pixels of the input image (saliency maps) that most caused the output classification. Gradient-based visualization methods [27, 28] have been proposed to extract image regions responsible for the network output. The LIME method proposed by [6] explains predictions of any classifier by approximating it locally with an interpretable model. Influence measures [29] have been used to identify the importance of features in affecting the classification outcome for individual data points.\nMore recently, apart from feature visualization and attribution techniques, other important lines of research in explainable AI explore dimensionality reduction techniques [30, 31] and focus on building models which are intrinsically interpretable [32, 33]. There are few recent works in the XAI literature that go beyond the pixel-level explanations. For example, the TCAV technique proposed by [34] aims to generate explanations based on high-level user defined concepts. Contrastive explanations are proposed by [35] to identify minimal and sufficient features to justify the classification result. [36] proposed counterfactual visual explanations that identify how the input could change such that the underlying vision system would make a different decision. More recently, few methods have been developed for building models which are intrinsically interpretable [32]. In addition, there are several works [7, 37, 38] on the\ngoodness measures of explanation which aim to understand the underlying characteristics of explanations."
    },
    {
      "heading": "1.4. Contributions",
      "text": "The contributions of this work are threefold: (i) a new interactive XAI framework based on the Theory-of-Mind; (ii) a new collaborative task-solving game in the domain of visual recognition for learning collaborative explanation strategies; and (iii) a new objective measure of trust and quantitative evaluation of how humans gain increased trust in a given vision system."
    },
    {
      "heading": "2. X-ToM Framework",
      "text": "Our X-ToM consists of three main components:\n\u2022 A Performer that generates image interpretations (i.e., machine\u2019s mind repre-\nsented as pgM ) using a set of computer vision algorithms;\n\u2022 An Explainer that generates maximum utility explanations in a dialog with the\nuser by accounting for pgM and pgUinM using reinforcement learning;\n\u2022 An Evaluator that quantitatively evaluates the effect of explanations on the hu-\nman\u2019s understanding of the machine\u2019s behaviors (i.e., pgMinU ) and measures human trust by comparing pgMinU and pgM ."
    },
    {
      "heading": "2.1. X-ToM Game",
      "text": "An X-ToM game consists of two phases. The first phase is the collaborative task phase. The user is shown a blurred image and given a task to recognize what the image shows. X-ToM has access to the original (unblurred) image and the machine\u2019s (i.e. Performer\u2019s) inference result pgM (see Section 2.2). The user is allowed to ask questions regarding objects and parts in the image that the user finds relevant for his/her own recognition task. Using the detected objects and parts in pgM , X-ToM Explainer provides visual explanations to the user, as shown in Figure 3. This process allows the machine to infer what the user sees and iteratively update pgUinM , and thus select an\noptimal explanation at every turn of the game (see Section 2.3). Optimal explanations generated by the Explainer are the key to maximize the human trust in the machine. The second phase is specifically designed for evaluating whether the explanation provided in the first phase helps the user understand the system behaviors. The Evaluator shows a set of original (unblurred) images to the user that are similar to (but different from) the ones used in the first phase of the game (i.e., the set of images shows the same class of objects or human activity). The user is then given a task to predict in each image the locations of objects and parts that would be detected by the machine (i.e., in pgM ) according to his/her understanding of the machine\u2019s behaviors. Based on the human predictions, the Evaluator estimates pgMinU and quantifies human trust in the machine by comparing pgMinU and pgM (see Section 2.4)."
    },
    {
      "heading": "2.2. X-ToM Performer (for Image Interpretation)",
      "text": "In this paper, the visual tasks involve detecting and localizing human body parts, identifying their poses and attributes, and recognizing human actions from a given image. The AOG for this visual domain uses AND nodes to represent decompositions of human body parts into subparts, and OR nodes for alternative decompositions. Each node is characterized by attributes that pertain to the corresponding human body part, including the pose and action of the entire body. Also, edges in the AOG capture hierarchical and contextual relationships of the human body parts.\nOur AOG-based performer uses three inference processes \u03b1, \u03b2 and \u03b3 at each node. Figure 3 shows an example part of the AOG relevant for human body pose estimation [39]. The \u03b1 process detects nodes (i.e., human body parts) of the AOG directly based on image features, without taking advantage of the surrounding context. The \u03b2 process infers nodes of the AOG by binding the previously detected children nodes in a bottom-up fashion, where the children nodes have been detected by the \u03b1 process (e.g., detecting human\u2019s upper body from the detected right arm, torso, and left arm). Note that the \u03b2 process is robust to partial object occlusions as it can infer an object from its detected parts. The \u03b3 process infers a node of the AOG top-down from its previously detected parent nodes, where the parents have been detected by the \u03b1 process (e.g., detecting human\u2019s right leg from the detected outline of the lower body). The parent node passes contextual information so that the performer can detect the presence of an object or part from its surround. Note that the \u03b3 process is robust to variations in scale at which objects appear in images."
    },
    {
      "heading": "2.3. X-ToM Explainer (for Explanation Generation)",
      "text": "The explainer, in the first phase of the game, makes the underlying \u03b1, \u03b2, and \u03b3 inference process of the performer more transparent to the human through a collaborative dialog. At one end, the explainer is provided access to an image and the performer\u2019s inference result pgM on that image. At the other end, the human is presented a blurred version of the same image, and asked to recognize a body part, or pose, or human action depicted (e.g., whether the person is running or walking). To solve the task, the human may ask the explainer various \u201cwhat\u201d, \u201cwhere\u201d and \u201chow\u201d questions (e.g.,\n\u201cWhere is the left arm in the image\u201d). We make the assumption that the human will always ask questions that are related to the task at hand so as to solve it efficiently. The explainer answers these questions using pgM and justifies the answers by showing the corresponding visual explanations in the image (as illustrated in Figure 4).\nAs visual explanations, we use \u201cbubbles\u201d [40], where each bubble reveals a circular part of the blurred image to the human. The bubbles coincide with relevant image parts for answering the question from the human, as inferred by the performer in pgM . For example, a bubble may unblur the person\u2019s left leg in the blurred image, since that image part has been estimated in pgM as relevant for recognizing the human action \u201crunning\u201d occurring in the image.\nFollowing the \u201cprinciple of least collaborative effort\u201d [41] and the aforementioned findings [7] that explanations should not overwhelm the human, our X-ToM explainer utilizes pgM and pgUinM (i.e., the contextual and hierarchical relationships explicitly modeled in the AOG) for controlling the depth and breadth of explanations. To enable this control, each bubble is characterized by a number of parameters, including the amount of image reveal (i.e., the unblurring level), size, and location in the image, to name a few. We use reinforcement learning to train the explainer to optimize these parameters and thus provide optimal visual explanations."
    },
    {
      "heading": "2.4. X-ToM Evaluator (for Trust Estimation)",
      "text": "The second phase of the X-ToM game serves to assess the effect of the explainer on the human\u2019s understanding of the performer. This assessment is conducted by the evaluator. The human is presented with a set of (unblurred) images that are different from those used in the first phase. For every image, the evaluator asks the human to predict the performer\u2019s output. The evaluator poses multiple-choice questions and the user clicks on one or more answers (see Appendix 5.2 for more details on evaluator interface and questions). As shown in Figure 5, we design these questions to capture different aspects of human\u2019s understanding of \u03b1, \u03b2 and \u03b3 inference processes in the performer. Based on responses from the human, the evaluator estimates pgMinU . By comparing pgMinU with the actual machine\u2019s mind pgM (generated by the performer), we have defined the following qualitative and quantitative metrics to quantitatively assess human trust [14, 42, 15, 43] in the performer: Quantitative Metrics: (1) Justified Positive and Negative Trust: It is possible for humans to feel positive trust with respect to certain tasks, while feeling negative trust (i.e. mistrust) on some\nother tasks. The positive and negative trust can be a mixture of justified and unjustified trust [14, 15]. We compute justified positive trust (JPT) and negative trust (JNT) as follows:\nJPT = 1\nN \u2211 i \u2211 z=\u03b1,\u03b2,\u03b3 \u2206JPT(i, z),\n\u2206JPT(i, z) = \u2016pgMinUi,z,+ \u2229 pgMi,+\u2016\n\u2016pgMi,+\u2016 ,\nJNT = 1\nN \u2211 i \u2211 z=\u03b1,\u03b2,\u03b3 \u2206JNT(i, z),\n\u2206JNT(i, z) = \u2016pgMinUi,z,\u2212 \u2229 pgMi,\u2212\u2016\n\u2016pgMi,\u2212\u2016 ,\nwhere N is the total number of games played. z is the type of inference process. \u2206JPT(i, z), \u2206JNT(i, z) denote the justified positive and negative trust gained in the i-th turn of a game on the z inference process respectively. pgMinUi,z,+ denotes nodes in pgMinUi for which the user thinks the performer is able to accurately detect in the image using the z inference process. Similarly, pgMinUi,z,\u2212 denotes nodes in pg MinU i for which the user thinks the performer would fail to detect in the image using the z inference process. \u2016pg\u2016 is the size of pg. Symbol \u2229 denote the graph intersection of all nodes and edges from two pg\u2019s.\n(2) Reliance: Reliance (Rc) captures the extent to which a human can accurately predict the performer\u2019s inference results without over- or under-estimation. In other words, Reliance is proportional to the sum of JPT and JNT.\nRc = 1\nN \u2211 i \u2211 z=\u03b1,\u03b2,\u03b3 \u2206Rc(i, z),\n\u2206Rc(i, z) = \u2016pgMinUi,z \u2229 pgMi,z\u2016\n\u2016pgMi \u2016 .\nQualitative Metrics: (3) Explanation Satisfaction (ES). We measure users feeling of satisfaction at having achieved an understanding of the machine in terms of usefulness, sufficiency, appropriated detail, confidence, accuracy, and consistency. We ask them to rate each of these metrics on a Likert scale of 0 to 9."
    },
    {
      "heading": "3. Learning X-ToM Explainer Policy",
      "text": "Given the following input: image I , task T assigned to the human, dialog history hi of a sequence of generated bubbles, and question from the user qi selected from a finite set of allowed questions Q(T ) for task T , the explainer estimates an optimal explanation ei at dialog turn i as\nei = arg max e\nU ( e | pgM , pgUinMi , qi, hi, T, I; \u03b8 )\nwhere U denotes the utility function parameterized by \u03b8. The set of questions Q(T ) is automatically generated from all concepts (objects, object parts, human activities, object attributes, etc.) that may appear in the image and are also modeled by the Performer. During interaction, the user is prompted to ask a question from this list1.\nAs defined earlier, pgUinMi denotes the current estimate of human\u2019s mind, which is an empty graph without nodes and edges at the beginning of the X-ToM game. At every turn in the dialog, the explainer infers and updates pgUinMi by maximizing its posterior distribution based on hi, T and qi. Using a Bayesian approach, we define the posterior of pgUinMi as\np ( pgUinMi | hi, qi, T ) \u221d\np ( qi | hi, pgUinMi , T ) p ( hi | pgUinMi , T ) p ( pgUinMi , T ) 1A NLU component can be added to map users\u2019 free-form natural language questions to the list of\ninterpretable questions.\nwhere p ( pgUinMi , T ) is specified as a uniform prior. The likelihoods p ( qi | hi, , pgUinMi , T ) and p ( hi | pgUinMi , T ) are estimated based on the frequency of occurrence of the question q = qi and the dialog history h = hi over many X-ToM games played with human users. After updating pgUinMi , the selection of an optimal bubble, i.e., explanation ei, is cast as a sequential decision-making problem and formalized using reinforcement learning (RL). Below we specify the state, actions, reward, and policy of the RL framework. RL State (si). The state of the explainer at dialog turn i consists of pgM , pgUinMi , qi, and hi. RL Action (ai). The action space consists of all possible bubbles that can be generated from pgM so that they reveal relevant image parts in the blurred image to the human. Each bubble b is characterized by the following four groups of parameters, as illustrated in Figure 6: (a) Explanation Content, bcnt, is defined as the amount of visual information contained in the bubble. Our X-ToM uses the Gaussian scale-space [44] for measuring bcnt. Specifically, we model \u201cspace\u201d as a Gaussian with variance \u03c321 governing the length of the radius (i.e., spatial size) of the bubble. Also, we model \u201cscale\u201d as a Gaussian with variance \u03c322 governing the amount of image unblur that the bubble reveals to the user. Given \u03c321 and \u03c3 2 2 , we compute b cnt i as the differential entropy\nbcnt = 1 + 1\n2 log(4\u03c02\u03c321\u03c3 2 2)\nIntuitively, a bubble with large \u201cspace\u201d (i.e., large size) and large \u201cscale\u201d (i.e., high resolution) reveals a lot of information about the image. Conversely, a bubble with small \u201cspace\u201d and \u201cscale\u201d reveals very little evidence. If the explainer always chose bubbles with small \u201cspace\u201d and \u201cscale\u201d, it would lead to inefficient dialogue for solving the task. On the other hand, if the explainer always chose bubbles with large space and scale, it would distract the human with unnecessary information and make it difficult for the human to understand the machine\u2019s internal representation and inference2. Thus, the explainer\u2019s goal is to find the bubble with an optimal bcnt. In this\n2For example, showing a very large bubble for revealing Left-Wrist will also reveal Left-Elbow to the\npaper, we discretize \u201cspace\u201d and \u201cscale\u201d of bubbles using \u03c31 \u2208 {1.15, 3.15, 4.5}, and \u03c32 \u2208 {1, 9, 15}. (b) Explanation Acts, bact, parametrizes the three types of visual explanations (i.e., bubbles) that can be presented to the human, corresponding to the three inference processes in our AOG-based performer. Specifically, bact can be: \u03b1, \u03b2, or \u03b3 explanation act. Note that using \u03b2 and \u03b3 explanation acts (i.e., bottom-up and top-down inference processes of the performer) allows for increasing depth of explanations. (c) Explanation Attention, batt, indexes a particular human body part from pgM that is the current focus of the dialog with the human. In the paper, the AOG explicitly models human body parts and their subparts, where pgM infers only a subset of those appearing in the image. (d) Explanation Discourse, bdis, parametrizes discourse relations of the bubbles generated along the dialog with the human. In this paper, we account for the dialog discourse for enforcing coherence among the explanations. In our experiments, we found the following five discourse relations [45, 41] to be sufficient and helpful:\n1. Elaboration. If bubble bi+1 provides additional details (e.g., by increasing\n\u201cscale\u201d or \u201cspace\u201d) relative to the previous bubbles hi = b1...i, then bi+1 relates to the dialog history hi with the elaboration relationship.\n2. Sequence. If the explanation attention batt of bubble bi+1 is not part of the\ndialog history hi, then bi+1 relates to hi with the sequence relationship.\n3. Recurrence. If bubble bi+1 already exists in hi, then the discourse relationship\nbetween bi+1 and hi is called recurrence.\n4. Restatement. If the dialog history hi already contains a bubble with the same\nexplanation attention batt as bi+1, then bi+1 relates to hi with the restatement\nhuman. This makes it harder for human to understand whether the machine is capable of detecting the exact location of Left-Wrist in the image. In addition, although larger bubbles can potentially minimize the number of turns, they transmit a large amount of information from machine to human. This effect may not be obvious in the current experimental set up, but will be significant in the situation where information to be transmitted is through text. Larger bubbles will correspond to longer textual descriptions.\nrelationship.\n5. Summary is a special case of the elaboration relationship. If an attention node\nof pgM has been already explained in the dialog history hi, and bi+1 has the same explanation attention but corresponds to a lower resolution and larger size bubble than the one in hi, then bi+1 relates to hi with the summary relationship.\nRL Reward (ri) Our reward function aims to maximize the success rate (ss), user confidence (cf), user satisfaction (sf) and minimize the cost (Ci) over the total number of bubbles. We estimate the cost of generating bubbles b1,b2,...,bi as\nCi = i\u2211 j=1 1 bcntj\nRL Reward (ri) is expressed in terms of a user feedback and cost associated with selecting the bubbles. At each dialog turn i, after choosing bi, the explainer collects the following feedback from the user:\n1. Success (ssi): The user is asked to solve the task based on {bi, hi}. The user\u2019s\nsuccess indicates that the machine\u2019s dialog with the user had a high utility and the explanations made by the machine make sense and can help the user reach an understanding of the image. Therefore, if the user solves the task correctly, the explainer is rewarded with ssi = 1; otherwise, ssi = -1.\n2. User confidence (cfi): It is possible that user might solve the task by chance\nwithout really understanding the task. We therefore additionally ask the user to report their confidence in solving the task on a scale of 1 to 5.\n3. User satisfaction (sfi): We ask the user to rate the ordering of bubbles generated\nin the dialog, and their relevance for solving the task on a scale of 1 to 5.\nTo compute ri, we also estimate the cost function Ci of generating bubbles b1,b2,...,bi, defined as\nCi = i\u2211 j=1 1 bcntj , (1)\nwhere bcnt is computed as follows:\nbcnt = 1 + 1\n2 log(4\u03c02\u03c321\u03c3 2 2). (2)\nIntuitively, a large Ci indicates that explanation content of the bubbles revealed is\nhigh.\nOur reward function aims to maximize the success rate (ss), user confidence (cf), user satisfaction (sf) and minimize the cost (Ci) over the total number of bubbles. We estimate the cost of generating bubbles b1,b2,...,bi as\nri = 1\ni exp( ssi cfi sfi Ci ). (3)\nRL Policy and Training. The explainer operates under a stochastic policy, \u03c0 (ai|si; \u03b8),\nwhich samples optimal bubbles conditioned on the state. This policy is learned by a standard recurrent neural network, called Long-Short Term Memory (LSTM) [46]. In this paper we use a 2-layer LSTM parameterized by \u03b8. Input to the LSTM is a feature vector representing the state si \u2013 specifically, a binary indicator vector of the AOG nodes and edges present in pgM and pgUinMi , as well as indices of the question qi and bubbles generated in hi. The LSTM\u2019s output is the predicted quadruple (bcnt, bact, batt, bdis) of bi+1. Thus, the goal of the policy learning is to estimate the LSTM parameters \u03b8.\nWe use actor-critic with experience replay for policy optimization [47]. The training objective is to find \u03c0 (ai|si; \u03b8) that maximizes the expected reward J(\u03b8) over all possible bubble sequences given a starting state. The gradient of the objective function has the following form:\n\u2207\u03b8J(\u03b8) = E[\u2207\u03b8 log \u03c0\u03b8 (ai|si; \u03b8)A (si, ai)] (4)\nwhere A (si, ai) = Q (si, ai) \u2212 V (si) is the advantage function [48]. Q (si, ai) is the standard Q-function, and V (si) is the baseline function aimed at reducing the variance of the estimated gradient. We use the same specifications of Q (si, ai) and V (si) as in [48]. As in [48], we sample the dialog experiences randomly from the replay pool for training."
    },
    {
      "heading": "4. Experiments",
      "text": "We deployed the X-ToM game on the Amazon Mechanical Turk (AMT) and trained the X-ToM Explainer through the interactions with turkers. All the turkers have a bachelors degree or higher. We used three visual recognition tasks in our experiments, namely, human body parts identification, pose estimation, and action identification. We used 1000 images randomly selected from Extended Leeds Sports (LSP) dataset [49]. Each image is used in all the three tasks. During training, each trial consists of one X-ToM game where a turker solves a given task on a given image. We restrict Turkers from solving a task on an image more than once. In total, about 2400 unique workers contributed in our experiments.\nWe performed off-policy updates after every 200 trials, using Adam optimizer [50] with a learning rate of 0.001 and gradients were clipped at [-5.0, 5.0] to avoid explosion. We used -greedy policy, which was annealed from 0.6 to 0.0. We stopped the training once the model converged. In our case, the X-ToM policy model converged after interacting with 3500 turkers. All our data and code will be made publicly available.\nThe trained X-ToM Explainer was applied to an additional 500 X-ToM games with AMT turkers for testing. Table 1 shows the percentage of discourse relations among bubbles found in the test interactions. As can be seen, the discourse relation sequence dominates other relations. This indicates that the X-ToM\u2019s most common explanation strategy is to prefer a bubble containing new evidence (that was not already shown to the user). Furthermore, the experiment has shown that 55.3% of the bubbles in the test trials were generated using \u03b1 explanation act, 23.1% using \u03b2 explanation act, and 21.6% using \u03b3 explanation act. The high percentage of \u03b2 and \u03b3 explanation acts indi-\ncate that contextual evidence is not only helpful for the performer to detect but also for the explainer to explain."
    },
    {
      "heading": "4.1. AMT Evaluation of X-ToM Explainer",
      "text": "We conducted an ablation study to quantify the importance of taking the inferred human\u2019s mind into account for generating optimal explanations, i.e., the ablated model does not explicitly represent and infer pgUinM . Similar to X-ToM, the ablated model was also deployed and trained on AMT. The trained ablated model was again applied to an additional 500 X-ToM games with AMT turkers for testing. Table 2 compares XToM Explainer with the ablated model in terms of objective measures such as average success rate (ss), average number of bubbles, average rewards (r). X-ToM Explainer significantly outperforms the ablated model (p < 0.01) in terms of the overall reward. Although the success rates of both models are similar, the ablated model is found to use a significantly larger number of bubbles, which leads to lower overall reward.\nUsing an additional 100 X-ToM games on AMT, we further compare the explanations generated by our X-ToM Explainer with the explanations annotated by humans. We asked three graduate students (not the authors), to select the most appropriate bubbles for a given task. Bubbles that have been agreed upon by these three subjects were taken as the best explanations for the given task and image. In terms of maximizing the reward, we found that X-ToM Explainer performed significantly better than the human strategy of bubble selection (p < 0.01). However, we found that the average dialog length in the human explanations is 6, while the average dialogue length observed in the X-ToM explanations is 10.5, indicating that there is a possibility to further improve the quality of the X-ToM explanations. We leave this for future exploration."
    },
    {
      "heading": "4.2. Human Subject Evaluation on Justified Trust",
      "text": "Using X-ToM Evaluator, we conduct human subject experiments to assess the effectiveness of the X-ToM Explainer, that is trained on AMT, in increasing human trust through explanations. We recruited 120 human subjects from our institution\u2019s Psychology subject pool 3. These subjects have no background on computer vision, deep learning and NLP (see Appendix 5.1 for more details). We applied between-subject design and randomly assigned each subject into one of the three groups. One group used X-ToM Explainer, and two groups used the following two baselines respectively:\n\u2022 \u2126QA: we measure the gains in human trust only by revealing the answers for the\ntasks without providing any explanations to the human.\n\u2022 \u2126Salience: in addition to the answers, we also provide saliency maps generated\nusing attribution techniques to the human as explanations [27, 28].\nWithin each group, each subject will first go through an introduction phase where we introduce the tasks to the subjects. Next, they will go through familiarization phase where the subjects become familiar with the machine\u2019s underlying inference process (Performer), followed by a testing phase where we apply our trust metrics and assess their trust in the underlying Performer.\nFigure 7 compares the justified positive trust (JPT), justified negative trust (JPT), and Reliance (Rc) of X-ToM with the baselines. As we can see, JPT, JNT and Rc values of X-ToM are significantly higher than \u2126QA and \u2126Salience (p < 0.01). Also, it should be noted that attribution techniques (\u2126Salience) did not perform any better than the \u2126QA baseline where no explanations are provided to the user. This could be attributed to the fact that, though saliency maps help human subjects in localizing the region in the image based on which the performer made a decision, they do not necessarily reflect the underlying inference mechanism. In contrast, X-ToM Explainer makes the underlying inference processes (\u03b1, \u03b2, \u03b3) more explicit and transparent and also provides explanations tailored for individual user\u2019s perception and understanding.\n3These experiments were reviewed and approved by our institution\u2019s IRB.\nTherefore X-ToM leads to the significantly higher values of JPT, JNT and Rc. This is one of the key results of our work, given the popularity of attribution techniques as the state-of-the-art explanations.\nFigure 8 shows the average explanation satisfaction rates obtained from each of the three groups. As we can see, subjects in X-ToM experiment group found that explanations were highly useful, sufficient and detailed compared to the baselines (p < 0.01). Interestingly, we did not find significant differences across the three groups in terms of other satisfaction measures: confidence, understandability, accuracy and consistency. We leave this observation for future exploration"
    },
    {
      "heading": "4.3. Gain in Reliance over time",
      "text": "We hypothesized that human trust and reliance in machine might improve over time. This is because, it can be harder for humans to fully understand the machine\u2019s underlying inference process in one single session. Therefore, we conduct an additional experiment with eight human subjects where the subjects\u2019 reliance is measured\nafter every session. The results are shown in Figure 9. As we expected, subjects\u2019 reliance increased over time. Specifically, reliance with respect to \u03b1 inference process significantly improved only after 2.5 sessions. Reliance with respect to \u03b2 and \u03b3 inference processes significantly improved after 4.5 sessions. It is clearly evident that, with more sessions, it is possible to further improve human reliance in AI system."
    },
    {
      "heading": "4.4. Case Study",
      "text": "Figure 10 shows examples where the top-3 best explanations preferred by X-ToM are compared against the top-3 explanations generated by the attribution techniques. The first column shows the input image for the task. The second column shows all the evidence (i.e., explanations in the form of bubbles, highlighted in yellow color) used in the machine\u2019s inference about the task. The thicker the bubble, the higher is its influence, for the machine, in interpreting the image. As we can see, attribution techniques chose the explanations only based on how influential they are for the machine in recognizing the image (third column). In contrast, since X-ToM maximizes the utility of\nexplanations based on both influence values and user\u2019s model, explanations selected by the X-ToM (fourth column) are diverse and are more intuitive for humans to understand and solve the task efficiently. For example, for the first image, to aid the human user in solving the task \u2018Is the person in the image walking\u2019, X-ToM generates the explanation bubbles based on left arm, right arm and lower body of the person, whereas attribution techniques generate the top-3 bubbles only based on right arm which clearly is not sufficient for the user to successfully solve the task.\nIn addition to the quantitative and qualitative metrics discussed in section 2.5, we also measure the following metrics for comparing our X-ToM framework with the baselines:\n\u2022 Response Time: We record the time taken by the human subject in answering\nevaluator questions. Figure 12 shows the average response times (in milliseconds per question) for each of the three groups (X-ToM, QA and Saliency Maps). We expected the participants in X-ToM group to take less time to respond compared to the baselines. However, we find no significant difference in the response times across the three groups.\n\u2022 Subjective Evaluation of Reliance: We collect subjective Reliance values (on\na Likert scale of 0 to 9) from the subjects in the three groups. The results are shown in Figure 11. These results are consistent with our quantitative reliance measures. It may be noted that subjects\u2019 qualitative reliance in Saliency Maps is lower compared to the QA baseline."
    },
    {
      "heading": "5. Conclusions",
      "text": "This paper presents X-ToM \u2013 a new framework for Explainable AI (XAI) and human trust evaluation based on the Theory-of-Mind (ToM). X-ToM generates explanations in a dialog by explicitly modeling, learning, and inferring three mental states based on And-Or Graphs \u2013 namely, machine\u2019s mind, human\u2019s mind as inferred by the machine, and machine\u2019s mind as inferred by the human. This allows for a principled formulation of human trust in the machine. For the task of visual recognition, we proposed a novel, collaborative task-solving game that can be used for collecting training data and thus learning the three mental states, as well as a testbed for quantitative evaluation of explainable vision systems. We demonstrated the superiority of X-ToM in gaining human trust relative to baselines."
    },
    {
      "heading": "6. Acknowledgement",
      "text": "The work is supported by DARPA XAI N66001-17-2-4029."
    },
    {
      "heading": "7. Appendix",
      "text": ""
    },
    {
      "heading": "7.1. Evaluation with Psychology Subject Pool",
      "text": "Figure 13 shows the statistics (Age, First Language, Gender) of the 120 human\nsubjects, recruited from our institution\u2019s Psychology subject pool."
    },
    {
      "heading": "7.2. X-ToM Evaluator Interface and Questions",
      "text": "Specifically, there are two main types of evaluator questions about the users prediction: (1) whether the Performer would successfully or incorrectly detect objects, parts and other concepts encoded by AOG; and (2) which image parts are most influential for the Performers successful or incorrect object detection. For example, the evaluator\u2019s questions include \u201cwhich parts of the image are most important for the machine to recognize that the person is running\u201d, and \u201cwhich small part of image contributes most to inferring the surrounding larger part of image\u201d. Figures 14 to 16 show few sample screenshots (from our web interface) of the exact questions, on the detection of the body part \u201cLeft-Arm\u201d, that we pose to the subjects."
    }
  ],
  "title": "X-ToM: Explaining with Theory-of-Mind for Gaining Justified Human Trust",
  "year": 2019
}
