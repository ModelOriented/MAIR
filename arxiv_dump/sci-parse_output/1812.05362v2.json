{"abstractText": "Ethical and explainable artificial intelligence is an interdisciplinary research area involving computer science, philosophy, logic, the social sciences, etc. For an ethical autonomous system, the ability to justify and explain its decision making is a crucial aspect of transparency and trustworthiness. This paper takes a Value Driven Agent (VDA) as an example, explicitly representing implicit knowledge of a machine learning-based autonomous agent and using this formalism to justify and explain the decisions of the agent. For this purpose, we introduce a novel formalism to describe the intrinsic knowledge and solutions of a VDA in each situation. Based on this formalism, we formulate an approach to justify and explain the decision-making process of a VDA, in terms of a typical argumentation formalism, Assumption-based Argumentation (ABA). As a result, a VDA in a given situation is mapped onto an argumentation framework in which arguments are defined by the notion of deduction. Justified actions with respect to semantics from argumentation correspond to solutions of the VDA. The acceptance (rejection) of arguments and their premises in the framework provides an explanation for why an action was selected (or not). Furthermore, we go beyond the existing version of VDA, considering not only practical reasoning, but also epistemic reasoning, such that the inconsistency of knowledge of the VDA can be identified, handled and explained.", "authors": [{"affiliations": [], "name": "Beishui Liao"}, {"affiliations": [], "name": "Michael Anderson"}, {"affiliations": [], "name": "Susan Leigh Anderson"}], "id": "SP:23723035b7457d5f1af9c3f4826c87f226795ada", "references": [{"authors": ["B. Liao", "M. Slavkovik", "L.W.N. van der Torre"], "title": "Building jiminy cricket: An architecture for moral agreements among stakeholders", "venue": "Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, AIES 2019, Honolulu, HI, USA, January 27-28, 2019.", "year": 2019}, {"authors": ["K. Baum", "H. Hermanns", "T. Speith"], "title": "From machine ethics to machine explainability and back", "venue": "International Symposium on Artificial Intelligence and Mathematics, ISAIM 2018, Fort Lauderdale, Florida, USA, January 3-5, 2018.", "year": 2018}, {"authors": ["T. Miller"], "title": "Explanation in artificial intelligence: Insights from the social sciences", "venue": "Artif. Intell. 267", "year": 2019}, {"authors": ["O. Cocarascu", "K. \u010cyras", "F. Toni"], "title": "Explanatory predictions with artificial neural networks and argumentation", "venue": "Proceedings of the IJCAI/ECAI Workshop on Explainable Artificial Intelligence (XAI 2018).", "year": 2018}, {"authors": ["A. Shih", "A. Choi", "A. Darwiche"], "title": "A symbolic approach to explaining bayesian network classifiers", "venue": "Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI 2018, July 13-19, 2018, Stockholm, Sweden.", "year": 2018}, {"authors": ["P. Madumal", "T. Miller", "F. Vetere", "L. Sonenberg"], "title": "Towards a grounded dialog model for explainable artificial intelligence", "venue": "CoRR abs/1806.08055", "year": 2018}, {"authors": ["L.R. Ye", "P.E. Johnson"], "title": "The impact of explanation facilities in user acceptance of expert system advice", "venue": "MIS Quarterly 19(2)", "year": 1995}, {"authors": ["M. Anderson", "S.L. Anderson", "V. Berenz"], "title": "A value driven agent: Instantiation of a casesupported principle-based behavior paradigm", "venue": "AI, Ethics, and Society, Papers from the 2017 AAAI Workshop, San Francisco, California, USA, February 4, 2017.", "year": 2017}, {"authors": ["T.J.M. Bench-Capon", "S. Modgil"], "title": "Norms and value based reasoning: justifying compliance and violation", "venue": "Artif. Intell. Law 25(1)", "year": 2017}, {"authors": ["T.J.M. Bench-Capon", "S. Modgil"], "title": "Norms and extended argumentation frameworks", "venue": "Proceedings of the Seventeenth International Conference on Artificial Intelligence and Law, ICAIL 2019, Montreal, QC, Canada, June 17-21, 2019.", "year": 2019}, {"authors": ["M. Anderson", "S.L. Anderson"], "title": "Geneth: a general ethical dilemma analyzer", "venue": "Paladyn: Journal of Behavioral Robotics 9(1)", "year": 2018}, {"authors": ["S. D\u017eeroski", "Lavra\u010d", "N. In"], "title": "An Introduction to Inductive Logic Programming", "venue": "Springer Berlin Heidelberg, Berlin, Heidelberg", "year": 2001}, {"authors": ["P. Baroni", "D. Gabbay", "M. Giacomin", "L. van der Torre", "eds."], "title": "Handbook of Formal Argumentation", "venue": "College Publications", "year": 2018}, {"authors": ["K. \u010cyras", "K. Satoh", "F. Toni"], "title": "Explanation for case-based reasoning via abstract argumentation", "venue": "Computational Models of Argument - Proceedings of COMMA 2016, Potsdam, Germany, 12-16 September, 2016.", "year": 2016}, {"authors": ["A.J. Garc\u0131\u0301a", "G.R. Simari"], "title": "Defeasible logic programming: An argumentative approach", "venue": "TPLP 4(1-2)", "year": 2004}, {"authors": ["S. Modgil", "H. Prakken"], "title": "The ASPIC+ framework for structured argumentation: a tutorial", "venue": "Argument & Computation 5(1)", "year": 2014}, {"authors": ["F. Toni"], "title": "A tutorial on assumption-based argumentation", "venue": "Argument & Computation 5(1)", "year": 2014}, {"authors": ["P. Besnard", "A. Hunter"], "title": "Constructing argument graphs with deductive arguments: a tutorial", "venue": "Argument & Computation 5(1)", "year": 2014}, {"authors": ["P.M. Dung"], "title": "On the acceptability of arguments and its fundamental role in nonmonotonic reasoning, logic programming and n-person games", "venue": "Artificial Intelligence 77(2)", "year": 1995}, {"authors": ["P. Baroni", "M. Caminada", "M. Giacomin"], "title": "An introduction to argumentation semantics", "venue": "Knowledge Eng. Review 26(4)", "year": 2011}, {"authors": ["M. Anderson", "S.L. Anderson", "V. Berenz"], "title": "A value-driven eldercare robot: Virtual and physical instantiations of a case-supported principle-based behavior paradigm", "venue": "Proceedings of the IEEE, DOI: 10.1109/JPROC.2018.2840045,", "year": 2018}, {"authors": ["K. Cyras", "X. Fan", "C. Schulz", "F. Toni"], "title": "Assumption-based argumentation: Disputes, explanations, preferences", "venue": "FLAP 4(8)", "year": 2017}, {"authors": ["M. Serramia", "M. L\u00f3pez-S\u00e1nchez", "J.A. Rodr\u0131\u0301guez-Aguilar", "J. Morales", "M. Wooldridge", "C. Ans\u00f3tegui"], "title": "Exploiting moral values to choose the right norms", "venue": "Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society, AIES 2018, New Orleans, LA, USA, February 02-03, 2018.", "year": 2018}, {"authors": ["T.W. Kim", "J. Hooker"], "title": "Toward non-intuition-based machine ethics", "venue": "Toward Non-IntuitionBased Machine Ethics", "year": 2018}, {"authors": ["K. Arkoudas", "S. Bringsjord", "P. Bello"], "title": "Toward ethical robots via mechanized deontic logic", "venue": "AAAI fall symposium on machine ethics.", "year": 2005}, {"authors": ["B. Liao", "N. Oren", "L. van der Torre", "S. Villata"], "title": "Prioritized norms in formal argumentation", "venue": "J. Log. Comput. 29(2)", "year": 2019}, {"authors": ["X. Fan", "F. Toni"], "title": "On computing explanations in argumentation", "venue": "Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, January 25-30, 2015, Austin, Texas, USA.", "year": 2015}, {"authors": ["A.J. Garc\u0131\u0301a", "C.I. Ches\u00f1evar", "N.D. Rotstein", "G.R. Simari"], "title": "Formalizing dialectical explanation support for argument-based reasoning in knowledge-based systems", "venue": "Expert Syst. Appl. 40(8)", "year": 2013}, {"authors": ["B.S. Liao", "L. Jin", "R.C. Koons"], "title": "Dynamics of argumentation systems: A division-based method", "venue": "Artif. Intell. 175(11)", "year": 2011}, {"authors": ["B. Liao"], "title": "Efficient Computation of Argumentation Semantics", "venue": "Intelligent systems series. Academic Press", "year": 2014}, {"authors": ["D. Vanderelst", "A. Winfield"], "title": "An architecture for ethical robots inspired by the simulation theory of cognition", "venue": "Cognitive Systems Research 48", "year": 2018}, {"authors": ["V. Sarathy", "M. Scheutz", "B. Malle"], "title": "Learning behavioral norms in uncertain and changing contexts", "venue": "Proceedings of the 2017 8th IEEE International Conference on Cognitive Infocommunications (CogInfoCom).", "year": 2017}, {"authors": ["J.M. Broersen", "M. Dastani", "J. Hulstijn", "Z. Huang", "L.W.N. van der Torre"], "title": "The BOID architecture: conflicts between beliefs, obligations, intentions and desires", "venue": "Proceedings of the Fifth International Conference on Autonomous Agents, AGENTS 2001, Montreal, Canada, May 28 - June 1, 2001.", "year": 2001}, {"authors": ["K. Atkinson", "T.J.M. Bench-Capon"], "title": "Taking account of the actions of others in value-based reasoning", "venue": "Artif. Intell. 254", "year": 2018}, {"authors": ["A. Chopra", "L. van der Torre", "H. Verhagen", "S. Villata"], "title": "Handbook of normative multiagent systems", "venue": "College Publications", "year": 2018}], "sections": [{"heading": "1 Introduction", "text": "Ethical, explainable artificial intelligence is an increasingly active research area in recent years. An autonomous agent should make decisions by determining ethically preferable behavior [1,?]. Further, it is expected to provide explanations to human beings about how and why the decisions are made [2]. Explainable AI is an interdisciplinary research direction, involving computer science, philosophy, cognitive psychology/science, and social psychology [3]. In recent years, different approaches have been proposed to provide explanations for autonomous systems, though most of them are still rather preliminary. For instance, [4] proposed an architecture combining Artificial Neural Networks and argumentation for solving binary classification problems, [5] introduced an approach for explaining Bayesian network classifiers such that the classifiers are compiled into decision functions that have a tractable and symbolic form, and [6] proposed a human explanation model based on conversational data.\nWhile there are various types of explanations such as trace, justification and strategy, according to the empirical results reported in [7], justification is the most effective type\nar X\niv :1\n81 2.\n05 36\n2v 2\n[ cs\n.A I]\n2 0\nO ct\n2 01\n9\nof explanation to bring about changes in user attitudes toward the system. In order to provide a justification, one needs to first have a formal representation of the knowledge that is used in the process of decision making of an autonomous agent. For a machine learning-based agent, unfortunately, the formal and logical knowledge may not always be self-evident. In such cases modeling justification-based explanation entails formally representing the intrinsic knowledge of the agent to permit its use for justification and explanation, as exemplified in [4] and [5]. In this paper, we study this methodology by considering an ethical agent, called a Value Driven Agent (VDA) in [8], focusing on the following research question.\nResearch question How can we explicitly represent the implicit knowledge of a VDA and use it to provide formal justification and explanation for its decision-making?\nA VDA, as introduced in next section, uses inductive logic programming techniques to abstract a principle from a set of cases, and a decision tree to determine the ethical consequences of each action in the current situation. Interestingly, there exists untapped implicit knowledge that can help provide an account as to why a VDA determines an action is considered ethically preferable.\nIn existing literature, there are a number works on value-based practical reasoning, e.g. [9] and [10], which are related to the ethical decision-making of a VDA. However, the existing work has not considered how preferences over actions can be induced from cases, nor how a logic-based formalism can be integrated with a machine leaning based approach.\nIn addition, concerning the combination of machine leaning-based approaches and formal logic-based approaches, while some existing works are mainly for explaining classification, e.g. [4] and [5], we are more interested in a system that can reason about the state of the world, the actions of a VDA, and the ethical consequences of the actions.\nWith these ideas in mind, in this paper, we study an explainable VDA by exploiting formal argumentation. The structure of this paper is as follows. Section 2 recalls some required basic notions in existing literature. Section 3 introduces a formalism for representing knowledge of a value driven agent. In Section 4, we present an argumentationbased justification and explanation approach. In section 5, we discuss related work. Finally, we offer our conclusions in Section 6."}, {"heading": "2 Preliminaries", "text": "In this section, we introduce some basic notions required for understanding what follows, including those about Value Driven Agents and formal argumentation."}, {"heading": "2.1 Value Driven Agent", "text": "According to [8], A VDA is defined as an autonomous agent that decides its next action using an ethical preference relation over actions, termed a principle, that is abstracted from a set of cases using inductive logic programming techniques. A case-supported, principle based approach (CPB) uses a representation scheme that includes ethically\nrelevant features (e.g. harm, good, etc.) and their incumbent prima facie duties to either minimize or maximize them (e.g. minimize harm, maximize good), actions characterized by integer degrees of presence or absence of ethically relevant features (and so, indirectly, the duties it satisfies or violates), and cases comprised of the differences of the corresponding duty satisfaction/violation degrees of two possible actions where one is ethically preferable to the other.\nA principle of ethical preference is defined as a disjunctive normal form predicate in terms of lower bounds for duty differentials of a case.\nppa1, a2q \u00d0\u00dd \u2206d1 \u0105\u201c v1,1 ^ \u00a8 \u00a8 \u00a8 ^\u2206dn \u0105\u201c vn,1 _ ... _ \u2206d1 \u0105\u201c v1,m ^ \u00a8 \u00a8 \u00a8 ^\u2206dn \u0105\u201c vn,m\nwhere \u2206di denotes the difference of a corresponding values of duty i in actions a1 and a2 (the actions of the case in question) and vi,j denotes the lower bound of duty i in disjunct j such that ppa1, a2q returns true if action a1 is ethically preferable to action a2.\nInductive logic programming (ILP) techniques are used to abstract principles from judgments of ethicists on specific two-action cases where a consensus exists as to the ethically relevant features involved, the relative levels of satisfaction or violation of their correlative duties, and the action that is considered ethically preferable. These techniques result in a set of sets of lower bounds for which principle p will return true for all positive cases presented to it (i.e. where the first action is ethically preferable to the second) and false for all negative cases (i.e. where the first action is not ethically preferable to the second). That is, for every positive case, there is a clause of the principle that is true for the differential of the actions of the case and, for every negative case, no clause of the principle returns true for the differential of the actions of the case. The principle is thus complete and consistent with respect to its training cases. Further, as each set of lower bounds is a specialization of the set of minimal lower bounds sufficient to uncover negative cases, each clause of the principle may inductively cover positive cases other than those used in its training.\nA general ethical dilemma analyzer, GenEth [11], has been developed that, through a dialog with ethicists, helps codify ethical principles in any domain. GenEth uses ILP [12] to infer a principle of ethical action preference from cases that is complete and consistent in relation to these cases. As cases are presented to the system, duties and ranges of satisfaction/violation values are determined in GenEth through resolution of contradictions that arise, constructing a concrete representation language that makes explicit features, their possible degrees of presence or absence, duties to maximize or minimize them, and their possible degrees of satisfaction or violation. Ethical preference is determined from differences of satisfaction/violation values of the corresponding duties of two actions of a case. GenEth abstracts a principle of ethical preference ppa1, a2q by incrementally raising selected lower bounds (all initially set at their lowest possible\nvalue) so that this principle no longer returns true for any negative cases (cases in which a2 is preferable to a1) while still returning true for all positive cases (cases in which a1 is preferable to a2).\nTo use this principle to determine a VDA\u2019s next action, it is necessary to associate each of the VDA\u2019s possible actions with a vector of values representing levels of satisfaction or violation of duties that that action exhibits in the current context. The current context is represented as a set of Boolean perceptions whose values are determined from initial input combined with sensor data (such as the fact that it is time to remind a patient that it is time to take a medication or batteries are in need of recharging). These values are provided as input to a decision tree (abstracted from input/output examples provided by the project ethicist) whose output is the duty satisfaction/violation values appropriate for each action given the context defined by the current Boolean perceptions. Given this information, the principle can serve as a comparison function for a sorting routine that orders actions by ethical preference.\nThe decision making process of a VDA, then, is as follows: sense the state of the world and abstract it into a set of Boolean perceptions, determine the vectors of duty satisfaction or violation of all actions with respect to this state using the decision tree, and sort the actions in order of ethical preference using the principle such that the first action in the sorted list is the most ethically preferable one. Clearly, several kinds of knowledge of a VDA are implicit, including the relation between perceptions and actions determined by the decision tree, the ethical consequences of an action (represented by a vector of duty satisfaction or violation values of the action), disjuncts in the clauses of the principle that are used to order two actions, and the cases from which these disjuncts are abstracted. Since these kinds of knowledge are informal and somewhat implicit, the current version VDA cannot provide explanations about why an action is taken.\nOur current implementation is in the domain of eldercare where a robot is tasked with assisting an elderly person. Its possible actions include: charge the robot\u2019s battery if low until sufficiently charged; remind the patient that it\u2019s time to take a medication according to a doctor\u2019s orders, retrieve that medication and bring it to the patient; engage the patient if the patient has been immobile for a certain period of time; warn the patient that an overseer will be notified if the patient refuses medication or does not respond to the robot\u2019s attempt to engage the patient; notify an overseer if there has not been a positive response to a previous warning; return to a seek task position when no tasks are required. For further details, readers are referred to [8]."}, {"heading": "2.2 Formal Argumentation", "text": "Formal argumentation or argumentation in AI, is a formalism for representing and reasoning with inconsistent and incomplete information [13]. It also provides various ways for explaining why a claim or a decision is made, in terms of justification, dialogue, and dispute trees [14].\nIntuitively, an argumentation system consists of a set of arguments and an attack relation over them. Arguments can be constructed from an underlying knowledge base represented by a logical language, while the attack relation can be defined in terms of the inconsistency of the underlying knowledge. There are different formalisms for modeling formal argumentation, such as Defeasible Logic Programming (DeLP) [15],\nAPSIC` [16], Assumption-based Argumentation (ABA) [17], and Classical Logicbased Argumentation [18]. In this paper, the acceptance of an ethical consequence specified by a vector of duty satisfaction/violation can be viewed as an assumption, while the relations between accepting an ethical consequence and an action, and between accepting different ethical consequences with respect to a principle, can be represented by deductive rules. Furthermore, default assumptions in epistemic reasoning can also be represented by deductive rules with assumptions in their premises. Under these considerations, we may adopt ABA as a formalism for representation. Now, let us first introduce some notions of ABA under the setting of this paper.\nAccording to [17], an ABA framework is a tuple xL,R,A,\u00dd y where\n\u2013 xL,Ry is a deductive system, with L the language, and R a set of rules of the form \u03c30 \u00d0 \u03c31, . . . , \u03c3m (m \u011b 0) with \u03c3i P L (i \u201c 0, . . . ,m); \u2013 A \u010e L is a (non-empty) set, referred to as assumptions; \u2013 \u00dd is a total mapping from A into L; a is referred to as the contrary of a.\nGiven an ABA framework, arguments can be defined by the notion of deduction. In terms of [17], a deduction for \u03c3 P L supported by T \u010e L andR \u010e R, denoted T $R \u03c3, is a (finite) tree with nodes labelled by sentences in L or by \u03c4 (when the premise of a rule applied in the tree is empty), the root labelled by \u03c3, leaves either \u03c4 or sentences in T , non-leaves \u03c31 with, as children, the elements of the body of some rules in R with head \u03c31, and R the set of all such rules. When the context is clear, T $R \u03c3 is written as T $ \u03c3. Then, an argument for (the claim) \u03c3 P L supported by A \u010e A (A $ \u03c3 for short) is a deduction for \u03c3 supported by A (and some R \u010e R).\nArguments may attack each other. An argument T1 $ \u03c31 attacks an argument T2 $ \u03c32 if and only if \u03c31 is the contrary of one of the assumptions in T2.\nLetAR be a set of arguments constructed from xL,R,A,\u00dd y, andATT \u010e AR\u02c6AR be the attack relation over AR. A tuple pAR,ATT q is called an abstract argumentation framework (or AAF in brief). Given an AAF, the notion of argumentation semantics in [19] can be used to evaluate the status of arguments in AR. There are a number of argumentation semantics capturing different intuitions and constraints for evaluating the status of arguments in an AAF, including complete, preferred, grounded and stable, etc. A set of arguments accepted together is called an extension. Various types of extensions under different argumentation semantics can be defined in terms of the notion of admissibility of set of arguments, which is in turn in terms of the notions of conflict-freeness and defense. For E \u010e AR, we say that E is conflict-free if and only if there exist no X1, X2 P E such that X1 attacks X2; E defends an argument X P AR if and only if for every argument Y P AR if Y attacks X then there exists Z P E such that Z attacks Y . Set E is admissible if and only if it is conflict-free and defends each argument in E. Then, we say that:\n\u2013 E is a complete extension if and only if E is admissible and each argument in AR defended by E is in E; \u2013 E is a preferred extension if and only if E is a maximal complete extension with respect to set-inclusion; \u2013 E is the grounded extension if and only if E is a minimal complete extension with respect to set-inclusion;\n\u2013 E is a stable extension if and only if E is conflict-free and for every X P ARzE, there exists Y P E such that Y attacks X .\nGiven an AAF pAR,ATT q, we use smpAR,ATT q to denote a set of extensions of pAR,ATT q under semantics sm P tCo,Pr,Gr,Stu, in which Co, Pr,Gr and St denote complete, preferred, grounded and stable semantics respetively.\nIt has been verified that each AAF has a unique (possibly empty) set of grounded extension, while many AAFs may have multiple sets of extensions under other semantics. When an AAF is acyclic, it has only one extension under all semantics. Then, we say that an argument of an AAF is skeptically justified under a given semantics if it is in every extension of the AAF, and credulously justified if it is in at least one but not all extensions of the AAF. Furthermore, we say that an argument is skeptically (credulously) rejected if it is attacked by a skeptically (respectively, credulously) justified argument.\nExample 1 (Formal argumentation). To illustrate the above notions, consider a famous example in nonmonotonic reasoning, called the Nixon diamond, a scenario in which default assumptions lead to mutually inconsistent conclusions:\n\u2013 Usually, Quakers are pacifist. \u2013 Usually, Republicans are not pacifist. \u2013 Richard Nixon is both a Quaker and a Republican.\nIn terms of ABA, let L \u201c tQuakerpRNq, RepublicanpRNq, pacifistpRNq, pacifistpRNq, asmppRNq, asm ppRNquwhere RN denotes Richard Nixon, A \u201c tasmppRNq, asm ppRNqu, asmppRNq \u201c pacifistpRNq, asm ppRNq \u201c pacifistpRNq, and R \u201c tQuakerpRNq \u00d0 , RepublicanpRNq \u00d0, pacifistpRNq \u00d0 QuakerpRNq, asmppRNq, pacifistpRNq \u00d0 RepublicanpRNq, asm ppRNqu. Then, there are 4 arguments as follows, in which Y4 attacks Y1 and Y3, and Y3 attacks Y2 and Y4, as illustrated in Fig. 1:\n\u2013 Y1 : tasmppRNqu $ asmppRNq \u2013 Y2 : tasm ppRNqu $ asm ppRNq \u2013 Y3 : tasmppRNqu $ pacifistpRNq \u2013 Y4 : tasm ppRNqu $ pacifistpRNq\nThen, under grounded semantics, there is only one extension which is an empty set; under complete semantics, there are three extensions tY1, Y3u, tY2, Y4u and tu; under stable and preferred semantics, there are two extensions tY1, Y3u and tY2, Y4u. No argument is skeptically justified under all semantics, all arguments are credulously justified under all semantics except grounded. For more information about argumentation semantics, please refer to [20]."}, {"heading": "3 Representing a value driven agent", "text": "In this section, we introduce a formal language and use it to represent the knowledge and model of a VDA, which lays a foundation for argumentation-based justification and explanation of the decision-making of a VDA.\nThe language of a VDA is composed of atoms of perceptions, actions and duties.\nDefinition 1 (Language of a VDA). Let Atom be a set of atoms of perceptions, and Sig be a set of signatures. Let L \u201c pAtom,A, Dq be a language consisting of\n\u2013 a set of atoms Atom, \u2013 a set of actions A \u010e Sig, and \u2013 a set of duties D \u010e Sig,\nsuch that A and D are disjoint.\nExample 2 (Language of a VDA). In [21], there is a set of 10 atoms of perceptions (denoted Atom1): low battery (lb), medication reminder time (mrt), reminded (r), refused medication (rm), fully charged (fc), no interaction (ni), warned (w), persistent immobility (pi), engaged (e), ignored warning (iw); a set of 6 actions (denoted A1): charge, remind, engage, warn, notify and seek task; and a set of 7 duties (denoted D1): maximize honor commitments (MHC), maximize maintain readiness (MMR), minimize harm to patient (mH2P), maximize good to patient (MG2P), minimize non-interaction (mNI), maximize respect autonomy (MRA) and maximize prevent persistent immobility (MPPI). The language of this VDA is then denoted L1 \u201c pAtom1, A1, D1q.\nThe duties enumerated above have been developed by the project ethicist using GenEth [11] and represent the set needed to drive an eldercare robot in performing the specified actions as shown in [21].\nLet Lit \u201c Atom Y t p | p P Atomu be a set of literals. For l1, l2 P L, we write l1 \u201c \u00b4l2 just in case l1 \u201c l2 or l2 \u201c l1. Let P \u010e Atom be a set of true perceptions. Then, the state of the world can be defined in terms of P , called a situation in this paper, as follows.\nDefinition 2 (Situation). A situation S is a subset of Lit, such that S \u201c P Yt p | p P AtomzP u. The set of situations is denoted as SIT .\nExample 3 (Situation). Let Lit1 \u201c Atom1 Y t p | p P Atom1u be a set of literals, which can be extended when a VDA becomes more sophisticated. See Section 4 for details. Let P1 \u201c tmrt, r, rm, fcu be a set of true perceptions. An example of the state of the world: S1 \u201c t lb,mrt, r, rm, fc, ni, w, pi, e, iwu.\nSituation S determines the satisfaction and/or violation degree of duties D by actions A. In each situation, all duty satisfaction/violation values for each action are determined by a decision tree using the perceptions of the situation as input. A set of vectors of duty satisfaction/violation values of all actions in a situation is called an action matrix. The decision tree is derived from a set of known situation/action matrix pairs.\nDefinition 3 (Action matrix of a situation). A duty satisfaction value is a positive integer, while a duty violation value is a negative integer. In addition, if a duty is neither satisfied nor violated by the action, the value is zero. Given an action \u03b1 P A and a situation S P SIT , a vector of duty satisfaction/violation values for \u03b1, denoted as vSp\u03b1q, is a vector vSp\u03b1q \u201c pd1 : vS,\u03b1pd1q, . . . , dn : vS,\u03b1pdnqq where vS,\u03b1pdiq is the satisfaction/violation value of di P D w.r.t \u03b1 in S. Then, an action matrix of a situation S is defined as MS \u201c tvSp\u03b1q | \u03b1 P Au. The set of action matrices of all situations SIT is denoted as MSIT \u201c tMS | S P SIT u.\nIn this definition, a vector of duty satisfaction/violation values represents the ethical consequences of its corresponding action in a given situation. An action\u2019s ethical consequences are denoted by how much its execution will satisfy or violate each duty. Conflicts arising between actions will be resolved by a principle abstracted from cases.\nFor brevity, when the order of duties is clear, vSp\u03b1q \u201c pd1 : vS,\u03b1pd1q, . . . , dn : vS,\u03b1pdnqq is also written as vSp\u03b1q \u201c pvS,\u03b1pd1q, . . . , vS,\u03b1pdnqq.\nExample 4 (Action matrix of a situation). Given a state of the world S1 (as denoted in Example 3), derived as described in [21], the action matrix of S1 isMS1 \u201c tvS1pchargeq, vS1premindq, vS1pengageq, vS1pwarnq, vS1pnotifyq, vS1pseekTaskqu, where\nvS1pchargeq \u201c p0, 1,\u00b41,\u00b41, 0, 0, 0q, vS1premindq \u201c p\u00b41,\u00b41,\u00b41,\u00b41, 0, 0, 0q, vS1pengageq \u201c p0,\u00b41,\u00b41,\u00b41, 0, 0, 0q, vS1pwarnq \u201c p0, 0, 1,\u00b41, 0,\u00b41, 0q, vS1pnotifyq \u201c p0, 0, 1,\u00b41, 0,\u00b42, 0q, vS1pseekTaskq \u201c p0,\u00b41,\u00b41, 1, 0, 0, 0q.\nThe duties in each vector are MHC, MMR, mH2P, MG2P, mNI, MRA and MPPI in order. Each duty satisfaction/violation vector denotes how much the associated action satisfies or violates each of these duties, positive values representing satisfaction (1=some, 2=much) and negative values representing violation (-1=some, -2=much). The value 0 denotes that an action neither satisfies nor violates a duty. For example, the vector vS1pchargeq specifies that under situation S1, action charge satisfies Maximize Maintain Readiness with degree 1, while violating Minimize Harm to Patient and Maximize Good to Patient with degree 1.\nFor readability, this can also be presented in tabular form:\nMHC MMR mH2P MG2P mNI MRA MPPI charge 0 1 -1 -1 0 0 0 remind -1 -1 -1 -1 0 0 0 engage 0 -1 -1 -1 0 0 0 warn 0 0 1 -1 0 -1 0 notify 0 0 1 -1 0 -2 0 seekTask 0 -1 -1 1 0 0 0\nGiven a situation and its corresponding action matrix, actions can be sorted in order of ethical preference using a principle abstracted from a set of cases by applying ILP techniques. Clauses of the principle specify learned lower bounds of the differentials between corresponding duties of any two actions that must be met or exceeded to satisfy the clause.\nLet vSp\u03b11q \u201c pd1 : vS,\u03b11pd1q, . . . , dn : vS,\u03b11pdnqq and vSp\u03b12q \u201c pd1 : vS,\u03b12pd1q, . . . , dn : vS,\u03b12pdnqq be vectors of duty satisfaction/violation values. In the following definitions, we use w \u201c vSp\u03b11q \u00b4 vSp\u03b12q \u201c pd1 : wpd1q, . . . , dn : wpdnqq to denote a vector of the differentials of vSp\u03b11q and vSp\u03b12q, wherewpd1q \u201c vS,\u03b11pd1q\u00b4vS,\u03b12pd1q, . . . , wpdnq \u201c vS,\u03b11pdnq \u00b4 vS,\u03b12pdnq.\nBy considering a set of cases, we may obtain a set of vectors of acceptable lower bounds of satisfaction/violation degree differentials such that all positive cases meet or exceed the lower bounds of some vector, while no negative case does.\nDefinition 4 (Principle). A principle is defined as \u03c0 \u201c tu1, . . . , uku, where ui \u201c pd1 : uipd1q, ..., dn : uipdnqq, where dj is a duty, and uipdjq is the acceptable lower bound of the differentials between corresponding duties of two actions in A.\nIntuitively, each ui of a principle is a collection of values denoting how much more an action must, at least, satisfy each duty (or how much, at most, it can violate each duty) than another action for it to be considered the ethically preferable of the pair. As duties are not necessarily equally weighted nor form a weighted hierarchy, principle \u03c0 is required to determine which duty (or set of duties) is (are) paramount in the current context. For brevity, when the order of duties is clear, in a principle the lower bounds of the differentials between duties is also written as ui \u201c puipd1q, ..., uipdnqq.\nExample 5 (Principle). According to [21], we have \u03c01 \u201c tu1, . . . , u10u where\nu1 \u201c p\u00b41,\u00b44,\u00b44,\u00b42,\u00b44,\u00b44, 2q, u2 \u201c p\u00b41,\u00b44,\u00b44,\u00b42, 0, 0, 1q, u3 \u201c p0,\u00b43, 0,\u00b41, 0, 1, 0q, u4 \u201c p0,\u00b43, 0, 1, 0, 0, 0q, u5 \u201c p0,\u00b41, 0, 0, 0, 0, 0q, u6 \u201c p0,\u00b43, 0,\u00b41, 1,\u00b41, 0q, u7 \u201c p\u00b41,\u00b44, 1,\u00b42,\u00b44,\u00b44, 0q, u8 \u201c p1,\u00b43, 0,\u00b42,\u00b44,\u00b44, 0q, u9 \u201c p0, 3, 0,\u00b42, 0, 0, 0q, u10 \u201c p\u00b41,\u00b44, 1,\u00b41,\u00b44,\u00b44,\u00b41q.\nThe 10 elements in \u03c01 correspond to 10 disjuncts of the principle ppa1, a2q in [21]. Each disjunct of the principle specifies a relationship between duties of an ordered pair actions that, if held, establishes that the first action of the pair (a1) is ethically preferable to the second (a2). For example, u1 states that action a1 is ethical preferable to action a2 if: a2 satisfies Maximize Honor Commitments no more that 1 more than a1 (or a1 violates it no more that 1 more than a2), a2 satisfies Maximize Good to Patient no more that 2 more than a1 (or a1 violates it no more that 2 more than a2), and a1 satisfies Maximize Prevent Persistent Immobility by at least 2 more than a2 (or a2 violates it by at least 2 more than a1). As the lower bounds of disjunct u1 for each other duty are minimal (i.e. it is not possible given the current ranges of duty satisfaction/violation values to generate a value lower), any relationship between the values of each action is acceptable.\nIn tabular form: MHC MMR mH2P MG2P mNI MRA MPPI u1 -1 -4 -4 -2 -4 -4 2 u2 -1 -4 -4 -2 0 0 1 u3 0 -3 0 -1 0 1 0 u4 0 -3 0 1 0 0 0 u5 0 -1 0 0 0 0 0 u6 0 -3 0 -1 1 -1 0 u7 -1 -4 1 -2 -4 -4 0 u8 1 -3 0 -2 -4 -4 0 u9 0 3 0 -2 0 0 0 u10 -1 -4 1 -1 -4 -4 -1\nGiven a principle and two vectors of duty satisfaction/violation values, we may define a notion of ethical preference over actions.\nDefinition 5 (Ethical preference over actions). Given a principle \u03c0, a situation S, and two actions \u03b11 and \u03b12, let w be the differentials of vSp\u03b11q and vSp\u03b12q as mentioned above. We say that \u03b11 is ethically preferable (or equal) to \u03b12 with respect to some u P \u03c0, written as vSp\u03b11q \u011bu vSp\u03b12q, if and only if for each di : wpdiq in w and di : updiq in u, it holds that wpdiq \u011b updiq.\nIn this definition, we make explicit the disjuncts puq in the clause of the principle that are used to order two actions.\nGiven two actions \u03b11 and \u03b12, there might exist two different clauses of \u03c0, say u1, u2 P \u03c0, such that vSp\u03b11q \u011bu vSp\u03b12q and vSp\u03b12q \u011bu1 vSp\u03b11q where u, u1 P \u03c0 and u \u2030 u1. In this case, we say that neither action \u03b11 nor action \u03b12 is ethically preferable to the other. In other words, according to the principle, there is no ethical justification to choose one over the other.\nBased on the above notions, a value driven agent (VDA) is formally defined as follows.\nDefinition 6 (Value driven agent). A value driven agent is a tuple Ag \u201c pL, SIT , MSIT , \u03c0q where L \u201c pAtom, A,Dq.\nExample 6 (Value driven agent). According to the above examples, we have Ag1 \u201c pL1, SIT1, MSIT1 , \u03c01q where SIT1 contains S1 and MSIT1 contains MS1 .\nIn a VDA, given a situation and an action matrix, a set of solutions can be defined as follows.\nDefinition 7 (Solution). Let Ag \u201c pL, SIT,MSIT , \u03c0q be a value driven agent, where L \u201c pAtom,A,Dq. Given a situation S P SIT and an action matrix MS P MSIT , a solution of Ag with respect to S is \u03b1 P A if and only if there is an ordering of MS with respect to \u03c0 such that vSp\u03b1q is the first in that ordering. The set of all solutions of Ag with respect to S is denoted as solpAg,MS , \u03c0q \u201c t\u03b1 P A | \u03b1 is a solution of Ag w.r.t. Su.\nExample 7 (Solution). Given Ag1 \u201c pL1, SIT1, MSIT1 , \u03c01q, S1 and MS1 , there is a unique ordering of MS1 with respect to \u03c01: vS1pwarnq \u011bu5 vS1pnotifyq \u011bu7 vS1pseekTaskq \u011bu4 vS1pchargeq \u011bu5{u8 vS1pengageq \u011bu5{u8 vS1premindq. So, Ag1 has only one solution warn.\nAccording to Definition 7, we directly have the following proposition.\nProposition 1 (The number of solutions). Given Ag \u201c pL, SIT,MSIT , \u03c0q, a situation S P SIT and an action matrixMS PMSIT , there are k solutions ofAg if and only if there are k different orderings of MS with respect to \u03c0 such that in each ordering the first element is different from the ones in all other orderings.\nIn summarizing this section, we may conclude that the formal model of a VDA properly captures the underlying knowledge of a VDA, and, to our knowledge, is the first such formalization. It lays the foundation for developing a methodology for justifying and explaining the decision-making of a VDA."}, {"heading": "4 Argumentation-based justification and explanation", "text": ""}, {"heading": "4.1 ABA-based argumentation systems of a VDA", "text": "As described in the previous section, in a VDA, a decision is made by checking whether there is an ordering over the set of actions according to the ethical preference relations. However, it is not clear how the ethical consequences of various actions affect each other, nor how the disjuncts of the principle determine the ordering of ethical consequences of actions. In this paper, we exploit Assumption-Based Argumentation (ABA) for the justification and explanation of the decision-making of a VDA by considering these factors. Furthermore, we go beyond the existing version of VDA, considering not only practical reasoning, but also epistemic reasoning, such that the inconsistency of knowledge can be identified and properly handled.\nWith the above considerations in mind, an ABA-based argumentation system for practical reasoning of a VDA under a situation S is defined as follows.\nDefinition 8 (ABA-based argumentation system for practical reasoning of a VDA). Let Ag \u201c pL, SIT , MSIT , \u03c0q be a value driven agent, where L \u201c pAtom,A,Dq. Given a situation S P SIT , the ABA-based argumentation system of Ag for practical reasoning is denoted as xLAg,S ,RAg,S ,AAg,S ,\u00dd y, where\n\u2013 LAg,S \u201c \u03c0 YMS Y t \u03c6 | \u03c6 PMSu YA; \u2013 each element in RAg,S belongs to one of the following types of rules:\n\u201a action rules of the form \u03b1 \u00d0 vSp\u03b1q, where \u03b1 P A is an action, vSp\u03b1q P MS is a vector of the duty satisfaction/violation values of \u03b1 in the situation S such that there exists di : vS,\u03b1pdiq such that vS,\u03b1pdiq \u011b 1; \u201a principle rules of the form vSp\u03b2q \u00d0 u, vSp\u03b1q, such that vSp\u03b1q \u011bu vSp\u03b2q where u P \u03c0, and \u03b1, \u03b2 P A; \u2013 AAg,S \u010eMS; \u2013 \u00dd is a total mapping from AAg,S into LAg,S , such that for all vSp\u03b1q P AAg,S , vSp\u03b1q \u201cdef vSp\u03b1q.\nThe items in Definition 8 are explained as follows. First, the language of argumentation for practical reasoning is composed of the set of disjuncts of a principle, the set of duty satisfaction/violation vectors under a given situation, and the set of actions. Among them, the first two sets of elements are assumptions, in the sense that both the disjuncts of a principle and the ethical consequences of actions can be attacked. For all vSp\u03b1q P MS , we may view vSp\u03b1q as a proposition, meaning that the ethical consequence of \u03b1 in situation S is acceptable. We use vSp\u03b1q to indicate that it is not the case that vSp\u03b1q holds.\nSecond, there are two types of rules for practical reasoning of a VDA. An action rule \u03b1 \u00d0 vSp\u03b1q can be understood as: if the ethical consequence of action \u03b1 (i.e. how it would satisfy and/or violate duties) w.r.t. the decision tree, i.e., vSp\u03b1q, is acceptable (i.e. satisfies some duty), then \u03b1 should be executed. The acceptance of vSp\u03b1q is an assumption, since there might be some other ethical consequences that are more acceptable according to the principle \u03c0, in the sense that vSp\u03b2q \u011bu vSp\u03b1q for some \u03b2 P A and u P \u03c0. Action rules can be automatically and dynamically generated and updated according to the data from a VDA.\nExample 8 (Action rule). Continuing Example 4. Given S1, there are four action rules.\nr1 : charge\u00d0 vS1pchargeq. r2 : warn\u00d0 vS1pwarnq. r3 : notify \u00d0 vS1pnotifyq. r4 : seekTask \u00d0 vS1pSeekTaskq.\nIn general, action rules are constructed only for those actions that satisfy at least one duty as those that do not are a priori less ethically preferable. In the example, neither remind nor engage satisfy any duty and, thus, no action rule is generated for either. Theoretically, it is possible that no action satisfies any duty in a given situation. In that case, the most preferable action would be among those that violated duties the least so action rules for all actions would be constructed.\nPrinciple rules can be constructed in terms of the priority relation between two vectors of duty satisfaction/violation duties with respect to a principle. For all \u03b1, \u03b2 P A,\nif vSp\u03b1q \u011bu vSp\u03b2q, then we have a principle rule vSp\u03b2q \u00d0 u, vSp\u03b1q, indicating that if both u and vSp\u03b1q are accepted, then it is not the case that vSp\u03b2q is acceptable.\nExample 9 (Principle rules). Continuing Examples 4 and 5. Given S1, there are six principle rules.\nr5 : vS1pchargeq \u00d0 u7, vS1pwarnq. r6 : vS1pchargeq \u00d0 u7, vS1pnotifyq. r7 : vS1pchargeq \u00d0 u4, vS1pseekTaskq. r8 : vS1pnotifyq \u00d0 u5, vS1pwarnq. r9 : vS1pseekTaskq \u00d0 u7, vS1pwarnq. r10 : vS1pseekTaskq \u00d0 u7, vS1pnotifyq.\nThird, regarding practical reasoning, for simplicity, we assume that only the elements in MS may be assumptions.\nExample 10 (Assumptions). Continue Example 4. For each duty satisfaction/violation vector, if at least one duty in the vector is satisfied, then the vector is regarded as an assumption. So, for practical reasoning of Ag1 under situation S1, we have a set of assumptions denoted as AAg1,S1 \u201c tvS1pchargeq, vS1pwarnq, vS1pnotifyq, vS1pSeekTaskqu.\nFourth, concerning the contrary of each element in AAg,S , for each vector vSp\u03b1q of duty satisfaction/violation, its contrary is its negation.\nIn Definitions 8 and 2, we assume that situation S is given, and can be defined directly by a set of perceptions without epistemic reasoning. However, in many cases, perceptions are unreliable, and a VDA usually only has incomplete and uncertain information. To properly capture the state of the world based on a set of perceptions, epistemic reasoning is needed for inferring implicit knowledge about the world, and for handling inconsistency of knowledge of a VDA. Corresponding to practical reasoning, an ABA-based argumentation system for epistemic reasoning is defined as follows.\nDefinition 9 (ABA-based argumentation system for epistemic reasoning of a VDA). Let L \u201c pAtom,A,Dq be the language of a VDA. Let Lit \u201c AtomYt p | p P Atomu be the set of literals of the VDA. The ABA-based argumentation system for epistemic reasoning of the VDA is denoted as xLit,RLit,ALit,\u00dd y, where\n\u2013 each element in RLit is an epistemic rule of the form p\u00d0 p1, . . . , pn where p, pi P Lit; \u2013 ALit \u010e Lit; \u2013 \u00dd is a total mapping from ALit to Lit, such that for all p P ALit, p P Lit.\nIn this definition, epistemic rules are used to reason about the state of the world. Consider the following example.\nExample 11 (Epistemic rules). In situation S2, let the set of true perceptions be P2 \u201c tmrt, r, rm, fc, lb, abu, where ab is a new atom being added to Atom1, denoting that the battery is abnormal. Let Atom2 \u201c Atom1 Y tabu. In terms Definition 2, S2 \u201c P2 Y t p | p P Atom2zP2u which is inconsistent if lb (\u2018low battery\u2019) and fc (\u2018fully charged\u2019) cannot hold at the same time. From the perspective of epistemic reasoning,\nsome of perceptions can be viewed as assumptions, e.g., \u2018low battery\u2019, \u2018fully charged\u2019. Meanwhile, due to incomplete information, assume that \u2018battery is not abnormal\u2019. In addition, it is reasonable that if \u2018low battery\u2019 holds then \u2018fully charged\u2019 does not hold, and if \u2018fully charged\u2019 holds and the battery is normal then \u2018low battery\u2019 does not hold. Under this setting, we have three epistemic rules for reasoning about assumptions: r11 : fc \u00d0 lb, r12 : lb \u00d0 fc, ab, and r13 : ab \u00d0. In addition, there are other epistemic rules corresponding to the facts, including mrt, r, and rm, etc. Since they have no interactions with assumptions and other rules, for simplicity, they are omitted. Let Lit2 \u201c Lit1 Y tab, abu, and RLit2 \u201c tr11, r12, r13u.\nThen, given an ABA-based argumentation system of a VDA under a situation S, arguments and attacks can be defined as follows.\nDefinition 10 (Arguments and attacks). Let Ag \u201c pL, SIT,MSIT , \u03c0q be a value driven agent, where L \u201c pAtom, A,Dq, xLAg,S ,RAg,S ,AAg,S ,\u00dd y be an ABA-based argumentation system for practical reasoning ofAg under a situation S, and xLit,RLit, ALit,\u00dd y be an ABA-based argumentation system for epistemic reasoning of Ag. An argument for \u03c3 P LAg,S supported by T \u010e AAg,S ( respectively for \u03c3 P LLit supported by T \u010e ALit), written as T $ \u03c3, is a deduction for \u03c3 supported by T . The conclusion of T $ \u03c3, denoted conclpT $ \u03c3q, is \u03c3. An argument A1 $ \u03c31 attacks an argument A2 $ \u03c32 iff \u03c31 is the contrary of one of the assumptions in A2.\nThe set of arguments constructed from xLAg,S ,RAg,S ,AAg,S ,\u00dd y is denoted as ARAg,S , and the set of attacks between the arguments inARAg,S is denoted asATTAg,S . In terms of [19], we call pARAg,S , ATTAg,Sq an abstract argumentation framework (or AAF for short). Respectively, the AAF constructed from xLLit,RLit, ALit,\u00dd y is denoted as pARLit, ATTLitq.\nIn the remaining part of this section, let us further illustrate the AAFs for practical reasoning and epistemic reasoning of a VDA. On one hand, the AAF constructed from xLAg,S ,RAg,S ,AAg,S ,\u00dd y is about practical reasoning (i.e., selecting ethically preferable actions), corresponding to the current version of the VDA in [21]. Under this setting, all perceptions are assumed to be facts. No justification about perceptions is considered.\nExample 12 (AAF for the practical reasoning of a VDA under a given situation). Continuing Examples 8 and 9. Given xLAg1,S1 ,RAg1,S1 , AAg1,S1 ,\u00dd y where LAg1,S1 \u201c \u03c01 Y MS1 Y t \u03c6 | \u03c6 P MS1u Y A1, RAg1,S1 \u201c tr1, . . . , r10u, and AAg1,S1 \u201c tvS1pchargeq, vS1pwarnq, vS1pnotifyq, vS1pSeekTaskqu, we have the following 10 arguments. Attacks between arguments are visualized in Fig. 2.\nX1: tvS1pchargequ $ charge X2 : tvS1pwarnqu $ warn X3 : tvS1pnotifyqu $ notify X4 : tvS1pseekTaskqu $ seekTask X5 : tu7, vS1pwarnqu $ vS1pchargeq X6: tu7, vS1pnotifyqu $ vS1pchargeq X7 : tu4, vS1pseekTaskqu $ vS1pchargeq\nX8 : tu5, vS1pwarnqu $ vS1pnotifyq X9 : tu7, vS1pwarnqqu $ vS1pseekTaskq X10 : tu7, vS1pnotifyqu $ vS1pseekTaskq\nOn the other hand, as mentioned in Example 11, some of perceptions are assumptions that can be in conflict: when \u2018low battery\u2019 and \u2018fully charged\u2019 both hold according to the observations, there exists a conflict between them. This conflict cannot be identified when only practical reasoning is considered [21]. The following example introduces an AAF that can be used for identifying the state of the world of a VDA based on handling the conflicts of its knowledge, i.e., the set of perceptions and epistemic rules.\nExample 13 (AAF for the epistemic reasoning of a VDA). In situation S2, given RLit2 \u201c tr11, r12, r13u and ALit2 \u201c tfc, lb, abu, there are the following six arguments for epistemic reasoning. Attacks between arguments are visualized in Fig.3.\nY1 : tfcu $ fc Y2 : tlbu $ lb Y3 : t abu $ ab Y4 : tlbu $ fc Y5 : tfc, abu $ lb Y6 : tu $ ab\nNote that since the situation S2 is determined by the status of the arguments for epistemic reasoning (in this example, arguments Y1, . . . , Y6), arguments for practical reasoning might change accordingly. We will further discuss this issue in next subsection."}, {"heading": "4.2 Argumentation-based justification", "text": "In this subsection, we introduce an argumentation-based approach for justifying an action and a situation of a VDA. Firstly, corresponding to the existing version of the VDA in [21], justification is only about actions.\nExample 14 (Extensions of an AAF for practical reasoning). Consider the AAF in Figure 2. It is acyclic and has only one extension under any argumentation semantics, i.e., E1 \u201c tX2, X5, X8, X9u. In this example, all arguments in E are skeptically justified.\nProposition 2 (Unique complete extension). Let Ag \u201c pL, SIT , MSIT , \u03c0q be a value driven agent, xLAg,S , RAg,S ,AAg,S ,\u00dd y an ABA-based argumentation system for practical reasoning of Ag under a situation S, and pARAg,S , ATTAg,Sq the AAF constructed from the argumentation system. If Ag has a unique solution with respect to S, then pARAg,S , ATTAg,Sq has a unique complete extension, which coincides with the unique grounded extension.\nProof. Let \u03b1 P A be the unique solution of Ag. Since there is no other ordering such that a different action (other than \u03b1) can be the first action in the sorted list, for all \u03b2 P Azt\u03b1u, it holds that vSp\u03b1q \u0105u vSp\u03b2q for some u P \u03c0. This means that each argument vSp\u03b2q $ \u03b2 is attacked by vSp\u03b1q, u $ vSp\u03b2q, which has no attacker. Let E \u010e ARAg,S be the set containing vSp\u03b1q $ \u03b1 and all arguments of the form vSp\u03b1q, u $ vSp\u03b2q for all \u03b2 P Azt\u03b1u. It turns out that E is the unique complete extension of pARAg,S , ATTAg,Sq.\nGiven a set of justified arguments, we may define the set of justified conclusions as follows.\nDefinition 11 (Justified conclusion in practical reasoning). Let pARAg,S , ATTAg,Sq be an AAF for practical reasoning, and X P ARAg,S be a skeptically (credulously) justified argument under a given argumentation semantics. A skeptically (credulously) justified conclusion is written as conclpXq. We say that conclpXq is a skeptically (credulously) justified action if and only if the conclusion of X is an action.\nExample 15 (Justified conclusions in practical reasoning). According to Example 14, all elements in E1 are justified conclusions, in which warn is a skeptically justified action since X2 is skeptically justified and its conclusion is an action.\nNow, let us verify that the representation by using argumentation-based approach is sound and complete under all semantics mentioned above (i.e., complete, grounded, preferred, stable), in the sense that when a VDA has multiple solutions, each solution of the VDA corresponds exactly to a credulously justified action of the argumentation framework; and when a VDA has a unique solution, the solution corresponds to the unique skeptically justified action of the argumentation framework.\nProposition 3 (Soundness and completeness of representation). Let Ag \u201c pL, SIT , MSIT , \u03c0q be a value driven agent and xLAg,S ,RAg,S ,AAg,S ,\u00dd y an ABA-based argumentation system for practical reasoning of Ag under a situation S. For all \u03b1 P A, it holds that: \u03b1 is one of the solutions of Ag with respect to S, if and only if \u03b1 is a credulously justified action in pARAg,S , ATTAg,Sq.\nProof. On one hand, if \u03b1 is a solution of Ag with respect to S, then there exists an ordering over A, such that \u03b1 is the first action of the sorted list. Let X P LAg,S be the argument with \u03b1 its conclusion, and of the form vSp\u03b1q $ \u03b1. For every argument Y P ARAg,SztXu, if Y is of the form vSp\u03b2q, u $ vSp\u03b1q for some u P \u03c0, then since \u03b1 is the first action of the sorted list, it holds that vSp\u03b1q \u011bu1 vSp\u03b2q, and there exists an argument X 1 of the form vSp\u03b1q, u1 $ vSp\u03b2q for some u1 P \u03c0. Let E be the set containing X and all arguments of the form vSp\u03b1q, u1 $ vSp\u03b2q. Since E is conflict free, and each attacker ofX andX 1 (i.e., Y ) is attacked byX 1, E is admissible, and therefore X P E is credulously justified. Since \u03b1 is the conclusion of X , it is a credulously justified action in pARAg,S , ATTAg,Sq.\nOn the other hand, if \u03b1 is a credulously justified action in pARAg,S , ATTAg,Sq, then there exist an argument X P ARAg,S of the form vSp\u03b1q $ \u03b1 and an admissible set E \u010e ARAg,S such that X P E. For every action \u03b2 P A, if \u03b2 \u2030 \u03b1, it is not the case that vSp\u03b2q \u0105u vSp\u03b1q for some u P \u03c0. Otherwise, there exists an argument of the form vSp\u03b2q, u $ vSp\u03b1q. As a result, X is not in E. Contradiction. As a result, we may construct an ordering of actions such that \u03b1 is the first action in the sorted list. Therefore, \u03b1 is a solution of Ag with respect to S.\nWhen \u03b1 is a unique solution of Ag with respect to S, according to Proposition 2, pARAg,S , ATTAg,Sq has a unique complete extension. Therefore, \u03b1 is a unique skeptically justified action in pARAg,S , ATTAg,Sq.\nSecondly, to justify a situation, we use an AAF for epistemic reasoning.\nExample 16 (Extensions of an AAF for epistemic reasoning). Consider the AAF in Figure 3. It is also acyclic and has only one extension under any argumentation semantics, i.e., E2 \u201c tY2, Y4, Y6u. The set of conclusions of arguments in E2 is denoted as conclpE2q \u201c tlb, fc, abu.\nDefinition 12 (Skeptically justified/rejected assumption in epistemic reasoning). Let pARLit, ATTLitq be an AAF for epistemic reasoning, and X P ARLit be a skeptically justified/rejected argument under a given argumentation semantics. We say that conclpXq is a skeptically justified/rejected assumption if and only if the conclusion of X is an assumption.\nIf every assumption is either skeptically justified or skeptically rejected, then a situation containing all justified assumptions is skeptical justified.\nDefinition 13 (Justified situation). Given Atom and Lit, let P \u010e Atom be a set of perceptions, ALit \u010e Lit be a set of assumptions, and pARLit, ATTLitq be an AAF constructed from xLit,RLit,ALit,\u00dd y. Let AJLit \u010e ALit be a set of skeptically justified assumptions. The set of justified perceptions is P J \u201c pP zALitq Y AJLit. If every assumption in ALitzAJLit is skeptically rejected, then there is a skeptically justified situation SJ \u201c P J Y t p | p P AtomzP Ju.\nExample 17. Given ALit2 \u201c tfc, lb, abu and P2 \u201c tlb,mrt, r, rm, fc, abu, we have P J2 \u201c tlb,mrt, r, rm, abu and SJ2 \u201c tlb,mrt, r, rm, ab, fc, ni, w, pi, e, iwu.\nGiven SJ2 , MSJ2 is generated dynamically, presented in tabular form as follows. MHC MMR mH2P MG2P mNI MRA MPPI charge 0 2 -1 -1 0 0 0 remind -1 -2 -1 -1 0 0 0 engage 0 -2 -1 -1 0 0 0 warn 0 0 1 -1 0 -1 0 notify 0 0 1 -1 0 -2 0 seekTask 0 -1 -1 1 0 0 0 In situation SJ2 , actions rules and principles rules are the same as those in situation\nS1 except that the subscript S1 is substituted by SJ2 ."}, {"heading": "4.3 Argumentation-based explanation", "text": "Besides justifying perceptions and actions, argumentation provides a natural way for explaining why an action is selected or not, by using the notion of justification of arguments and their premises.\nDefinition 14 (Explanation of a justified action). Let pARAg,S , ATTAg,Sq be an AAF, and X be a skeptically (credulously) justified argument of the form vSp\u03b1q $ \u03b1 under a given argumentation semantics. The explanation of \u03b1 being a skeptically (credulously) justified action is that: the argument vSp\u03b1q $ \u03b1 is in every extension (one of the extensions) of pARAg,S , ATTAg,Sq, which is in turn because the assumption vSp\u03b1q is accepted since it has no attacker or all its attachers are attacked by an argument in each extension (respectively, the given extension).\nExample 18 (Explanation of a justified action). According to Examples 12 and 15, the explanation of the action warn being a skeptically justified action is as follows.\n\u2013 X2 \u201c vS1pwarnq $ warn is in the unique extension E1, because: \u2013 the assumption (ethical consequence) vS1pwarnq \u201c p0, 0, 1,\u00b41, 0,\u00b41, 0q is ac-\ncepted since it has no attacker.\nIn this example, accepting the ethical consequence vS1pwarnq means that, under situation S1, warn\u2019s satisfaction of Minimize Harm to Patient with degree 1 overrides both degree 1 violations of Maximize Good to Patient and Maximize Respect Autonomy.\nDefinition 15 (Explanation of a rejected action). Let pARAg,S , ATTAg,Sq be an AAF, and X of the form vSp\u03b1q $ \u03b1 be a rejected argument under a given argumentation semantics. The explanation of \u03b1 being a rejected action is that: the argument vSp\u03b1q $ \u03b1 is not in any extension of pARAg,S , ATTAg,Sq, which is in turn because the assumption (ethical consequence) vSp\u03b1q is not acceptable since in every extension of pARAg,S , ATTAg,Sq, there is an argument attacking vSp\u03b1q $ \u03b1, whose premises (the ethical consequence of another action and a disjunct of the principle) are accepted.\nExample 19 (Explanation of a rejected action). The explanation of charge being a rejected action is as follows.\n\u2013 The argument X1 \u201c vS1pchargeq $ charge is not in the unique extension E1, in that: \u2013 the ethical consequence vS1pchargeq \u201c p0, 1,\u00b41,\u00b41, 0, 0, 0q is not acceptable, since X1 \u201c vS1pchargeq $ charge is attacked by X5 \u201c u7, vS1pwarnq $ vS1pchargeq, whose premises (u7 and vS1pwarnq) are accepted.\nIn this example, the content of the premises of arguments can be used to further explain the decision. More specifically, given u7 and the ethical consequence depicted by vS1pwarnq, the ethical consequence depicted by vS1pchargeq is not acceptable, i.e., vS1pchargeq is acceptable. In other words, under situation S1, satisfying \u201cminimize harm to patient\u201d with degree 1 (mH2P:1), even while violating \u201cmaximize respect autonomy\u201d with degree -1 (MRA: -1), is ethically preferable to satisfying \u201cmaximize maintain readiness\u201d with degree 1 (MMR: 1). As both actions violate \u201dmaximize good to the patient\u201d equally (MG2P: -1), that duty does not help differentiate these actions and therefore has no role in this explanation.\nConcerning the explanation of a justified situation, since only assumptions need to be justified, we have the following definition.\nDefinition 16 (Explanation of a justified situation). The explanation of S being a skeptically justified situation is that: each l P ALit is skeptically justified or rejected. In turn, the explanation of a justification/rejection of a literal is similar to that of an action.\nExample 20. SJ2 is a skeptically justified situation because in the set of assumptions tfc, ab, lbu, fc and ab are skeptically rejected, and lb is skeptically accepted. The explanation of skeptical justification and rejection of perceptions is in turn described as follows (illustrated in Fig 6).\n\u2013 fc and ab are skeptically rejected, because the argument supporting fc (respectively, ab) is attacked by a skeptically accepted argument. \u2013 lb is skeptically justified because the argument supporting lb is defended by two skeptically accepted arguments Y4 and Y6.\nBesides the above-mentioned approach for explaining why an action is selected or rejected, we may also use argument-based dialogues to provide explanations [22]. This is left to future work."}, {"heading": "5 Related Work", "text": "The work presented in this paper concerns, in the main, how an autonomous agent makes decisions according to ethical considerations and provides an explanation for these decisions. In this section, we discuss related work from the perspectives of value/ norm-based reasoning, argumentation-based decision making, and explanations in artificial intelligence.\nLopez-Sanchez et al. [23] pertains to the use of \u201cmoral values\u201d in choosing correct norms. Using deontic logic, they associate moral values with norms that exhibit them\nand incorporate the relative weights of these values as a factor in calculating which norms should take precedence. A correlation might be made between what is termed \u201cmoral values\u201d in the cited work and the concept of duties in Anderson et al\u2019s work [8] but doing so reveals the simplistic manner in which these values are treated. In [23], values are chosen in an arbitrary manner without the support of consideration by ethicists and are not likely to form the total order that they assume. Further, the fact that some actions may satisfy more than one value is not considered nor is the possibility of actions violating values. Also not considered is the possibility that an action satisfies or violates a duty more or less than another. Such possible combinations of different levels of a variety of satisfied and violated duties are likely to require nonlinear means to resolve\u2013 this is precisely what the principle formalized in this paper accomplishes. Lastly, the cited work doesn\u2019t seem to address the core of what we are trying to accomplish \u2013 providing arguments/explanations for chosen actions.\nThose who attempt to exploit deontic logic in the service of providing ethical guidance to autonomous systems, (e.g. [24], [25]) cite the transparency of the reasoning process as a benefit of such an approach. We would argue that, while a trace of deductive reasoning from premises to a conclusion may be transparent to some, it will forever remain opaque to others. We maintain that an argumentation approach to explanation may be more fruitful.\nThe work reported in this paper shares some similarity with the symbolic approach introduced in [5], in the sense that some implicit functions of the system are made explicit by using a symbolic representation. However, rather than translating the function between a set of features and a classification, we translate several types of implicit knowledge of a VDA by a logical formalism.\nOther related works are those based on argumentation. Among others, Liao et al. [26] introduce an argumentation-based formalism for representing prioritized norms, but do not consider the origin of these priorities, while in this paper the priority relation between the ethical consequences of different actions are learned from a set of cases, guided by the judgement of ethicists. Cocarascu et al. [4] introduce an approach to construct an AAF in terms of highest ranked features. While sharing some ideas of developing a methodology of explainable AI by combining argumentation and machine learning, our approach is specific to machine ethics and connects to a different machine learning approach and has a different model of argumentation. Baum et al. [2] study the interplay of machine ethics and machine explainability by using argumentation. The idea is close to our work, but focuses on a different research setting and has a different model. Others also focus on developing general approaches for explanation based on argumentation, e.g., [27]\u2019s work on a new argumentation semantics for giving explanations to arguments in both Abstract Argumentation and Assumption-based Argumentation, and [28]\u2019s work on dialectical explanation for argument-based reasoning in knowledge-based systems, etc. However, they are not specific to ethical decision making and explanation.\nLast but not least, in the direction of explanation in artificial intelligence, there are a number of research efforts in recent years. Among them, a recent work by Tim Miller [3] provides several insights from the social sciences, by considering how people define, generate, select, evaluate, and present explanations."}, {"heading": "6 Conclusions", "text": "In this paper, we have proposed an argumentation-based approach for representation, justification and explanation of a VDA. The contributions are as follows.\nFirst, we provide a formalism to represent a VDA, making explicit some implicit knowledge. This lays a foundation for the justification and explanation of reasoning and decision making in a VDA. To our knowledge, this is the first effort on providing a formalization for a VDA.\nSecond, we adapt existing argumentation theory to the setting of a decision making in a VDA, such that the ethical consequences of actions and clauses of a principle can be used for decision-making and explanation. Furthermore, we go beyond the existing version of VDA, considering not only practical reasoning, but also epistemic reasoning, such that the inconsistency of knowledge of the VDA can be identified, handled and explained.\nThird, unlike existing argumentation systems where formal rules are designed in advance, in our approach, action rules and principle rules for practical reasoning are generated and updated at run time in terms of an action matrix and a principle. Thanks to the graphic nature of an AAF, when the system becomes more complex, there exist efficient approaches to handle the dynamics of the system, e.g., [29] and [30].\nBesides these technical contributions, methodologically, this paper provides a novel approach for combining symbolic approaches and sub-symbolic approaches, in the sense that the features learned from data could be used to build the rules for reasoning. In this paper, the duty satisfaction/violation vectors and the principle are exploited to build the knowledge for reasoning, decision-making and explanation.\nDue to these contributions, some benefits can be obtained. Clearly, formal justification and explanation of the behavior of autonomous systems enhances the transparency of such systems. Further, autonomous systems that can argue formally for their actions are more likely to engender trust in their users than systems without such a capability. That principle-based systems such as the one detailed in this paper and others (e.g. [31],[32]) seem to lend themselves readily to explanatory mechanisms adds further support for the adoption of principles as a formalism to ensure the ethical behavior of autonomous systems.\nConcerning future work, first, we have not identified nor formally represented the relation between a principle and a set of cases from which the principle is learned. Doing so is likely to provide further information that explains why an action is chosen in a given situation. Second, in the existing version of VDA [21], multi-agent interaction [33,34,35] has been considered. The addition of such extensions to the VDA will serve to extend its capabilities. Third, concerning explanations, it could be interesting to further develop our approach by using argument-based dialogues, and the insights from the social sciences, as pointed in [3]."}, {"heading": "Acknowledgment", "text": "The authors are grateful to the anonymous reviewers of PRIMA2019 for their helpful comments. All the comments are carefully taken into consideration, and the paper is revised accordingly."}], "title": "Representation, Justification and Explanation in a Value Driven Agent: An Argumentation-Based Approach", "year": 2019}