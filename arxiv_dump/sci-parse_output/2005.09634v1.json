{"abstractText": "Moog Inc. has automated the evaluation of copper (C u) alloy grain size using a deep-learning convolutional neural network (CNN). The proof-of-co n ept automated image acquisition and batch-wise image processing offers the potential for significa ntly reduced labor, improved accuracy of grain evaluation, and decreased overall turnaround times for approving Cu alloy bar stock for use in flight critical aircraft hardware. A classification accuracy of 91. 1% on individual sub-images of the Cu alloy coupons was achieved. Process development included minimizi ng the variation in acquired image color, brightnes s, and resolution to create a dataset with 12300 sub-i mages, and then optimizing the CNN hyperparameters on this dataset using statistical design of experim nts (DoE). Over the development of the automated Cu alloy grai n size evaluation, a degree of \u201dexplainability\u201d in the artificial intelligence (XAI) output was realized, based on the decomposition of the large raw images into many smaller dataset sub-images, through the abilit y to explain the CNN ensemble image output via inspection of the classification results from the i ndividual smaller sub-images.", "authors": [{"affiliations": [], "name": "George S. Baggs"}], "id": "SP:a764216245ae775f6cac4d1e26cd2a62805e1958", "references": [{"authors": ["T.M. Mitchell"], "title": "The Discipline of Machine Learning, Pittsburgh", "venue": "Carnegie Mellon University,", "year": 2006}, {"authors": ["B. Zang", "P. Jaiswal", "R. Rai", "P. Guerrier", "G. Baggs"], "title": "Convolutional Neural Network-Based Inspection of Metal Additive Manufacturing Parts", "venue": "Rapid Prototyping Journal, vol. 25, no. 3, pp. 530-540, 8 April 2019.", "year": 2019}, {"authors": ["X. Li", "X. Jia", "Q. Yang", "J. Lee"], "title": "Quality Analysis in Metal Additive Manufacturing with Deep Learning", "venue": "Journal of Intelligent Manufacturing, no. Published online, February 25, 2020.", "year": 2020}, {"authors": ["I.M. Kamal", "R.A. Sutrisnowati", "H. Bae", "T. Lim"], "title": "Gear Classification for Defect Detection in Vision Inspection System Using Deep Convolution Neural Networks", "venue": "ICIC Express Letters, vol. 9, no. 12, December 2018.", "year": 2018}, {"authors": ["K. Anding", "P. Kuritcyn", "D. Garten"], "title": "Using Artificial Intelligence Strategies for Process-Related Automated Inspection in the Production Environment", "venue": "Journal of Physics, Conference Series, vol. 772, no. 1, 2016.", "year": 2016}, {"authors": ["L. Song", "X. Li", "Y. Yang", "X. Zhu", "Q. Guo", "H. Yang"], "title": "Detection of Micro-Defects on Metal Screw Surfaces Based on Deep Convolutional Neural Networks", "venue": "Sensors, vol. 18, no. 3709, 31 October 2018.", "year": 2018}, {"authors": ["D. Masters", "C. Luschi"], "title": "Revisiting Small Batch Training for Deep Neural Networks", "venue": "arXiv.org, Cornell University, 2018.", "year": 2018}, {"authors": ["S.N. Keskar", "D. Mudigere", "J. Nocedal", "M. Smelyanskiy", "P.T.P. Tang"], "title": "ON LARGE-BATCH TRAINING FOR DEEP LEARNING: GENERALIZATION GAP AND SHARP MINIMA", "venue": "arXiv.org, Cornell University, 2017.", "year": 2017}, {"authors": ["D.P. Kingma", "J.L. Ba"], "title": "Adam: a Method for Stochastic Optimization", "venue": "ArXiv, 2015.", "year": 2015}, {"authors": ["T. Dozat"], "title": "Incorporating Nesterov Momentum into Adam", "venue": "ICLR, 2016.", "year": 2016}, {"authors": ["S. Ruder"], "title": "An Overview of Gradient Descent Optimization Algorithms", "venue": "ArXiv, 2017.", "year": 2017}, {"authors": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "title": "Dropout: A Simple Way to Prevent Neural Networks from Overfitting", "venue": "Journal of Machine Learning Research (JMLR), vol. 15, pp. 1929-1958, June 2014.", "year": 1929}, {"authors": ["I. Goodfellow", "Y. Bengio", "A. Courville"], "title": "Pooling", "venue": "Deep Learning, Cambridge MA, MIT Press, 2016, pp. 129, 195, 332-333, 342.", "year": 2016}, {"authors": ["G.E.P. Box", "N.R. Draper"], "title": "Evolutionary Operation: A Statistical Method for Process", "year": 1998}, {"authors": ["D. Sussillo", "L. Abbot"], "title": "Random Walk Initialization for Training Very Deep Feedforward Networks", "venue": "arXiv, Cornell University, 2014. This document does not contain Technical Data or Technology as defined in the ITAR Part 120.10 or EAR Part 772 Automated Copper Alloy Grain Size Evaluation Using a Deep-learning CNN 48 5/19/2020", "year": 2014}, {"authors": ["G. Klambauer", "T. Unterthiner", "A. Mayr", "S. Hochreiter"], "title": "Self-Normalizing Neural Networks", "venue": "arXiv, Cornell University, 2017.", "year": 2017}, {"authors": ["R. Reed", "R.J. Marksll"], "title": "Chaper 16.5: Weight Decay", "venue": "Neural Smithing: Supervised Learning in Feedforward Artificial Neural Networks, Cambridge MA, MIT Press, 1999, pp. 269-271.", "year": 1999}, {"authors": ["B. Jones", "C.J. Nachtsheim"], "title": "A Class of Three-Level Designs for Definitive Screening in the Presence of Second-Order Effects", "venue": "Journal of Quality Technology, vol. 43, no. 1, pp. 1-15, 2011.", "year": 2011}, {"authors": ["B. Jones", "C.J. Nachtsheim"], "title": "Definitive Screening Designs with Added Two-Level Categorical Factors", "venue": "Journal of Quality Technology, vol. 45, no. 2, pp. 121-129, 2013.", "year": 2013}, {"authors": ["G.E.P. Box", "K.B. Wilson"], "title": "On the Experimental Attainment of Optimal Conditions", "venue": "Journal of the Royal Statistical Society, vol. XIII, no. 1 Series B (Methodological), 1951.", "year": 1951}, {"authors": ["NIST National Institute of Standards", "Technology", "U.S. Department of Commerce"], "title": "5.3.3.6.1. Central Composite Designs (CCD)", "venue": "30 October 2013. [Online]. Available: https://www.itl.nist.gov/div898/handbook/pri/section3/pri3361.htm.", "year": 2013}, {"authors": ["G.S. Baggs"], "title": "Issue #2: Process Development and Control in Metal Additive Manufacturing", "venue": "2017. [Online]. Available: https://www.moog.com/news/blognew/Issue2_ProcessDevelopmentAndControlInMetalAM.html.", "year": 2017}, {"authors": ["ASTM E"], "title": "Standard Test Methods for Determining Average Grain Size Using Semiautomatic and Automatic Image Analysis, West Conshohocken", "venue": "PA: ASTM International,", "year": 2010}, {"authors": ["R.E. Woods", "R.C. Gonzalez"], "title": "Histogram Equalization", "venue": "Digital Image Processing, Third Edition, Prentice Hall, 2008, p. Chapter 3.", "year": 2008}, {"authors": ["M. Turek"], "title": "Explainable Artificial Intelligence (XAI)", "venue": "DARPA (Defense Advanced Research Projects Agency), 26 November 2019. [Online]. Available: https://www.darpa.mil/program/explainable-artificial-intelligence. [Accessed 26 November 2019].", "year": 2019}, {"authors": ["J. Wang", "L. Perez"], "title": "The Effectiveness of Data Augmentation in Image Classification using Deep Learning", "venue": "arXiv, 2017.", "year": 2017}, {"authors": ["F. Chollet"], "title": "Building Powerful Image Classification Models Using Very Little Data", "venue": "Keras, 5 June 2016. [Online]. Available: https://blog.keras.io/building-powerful-image-classification-models-using-very-littledata.html. [Accessed 25 October 2017].", "year": 2016}, {"authors": ["D. Falbel", "J. Allaire", "F. Chollet"], "title": "Tutorial: Overfitting and Underfitting", "venue": "[Online]. Available: https://keras.rstudio.com/articles/tutorial_overfit_underfit.html. [Accessed 7 October 2019].", "year": 2019}, {"authors": ["J. Brownlee"], "title": "Deep Learning With Python", "venue": "Jason Brownlee, 2016.", "year": 2016}, {"authors": ["T. Saito", "M. Rehmsmeier"], "title": "Basic evaluation measures from the confusion matrix", "venue": "2018. [Online]. Available: https://classeval.wordpress.com/introduction/basic-evaluation-measures/. This document does not contain Technical Data or Technology as defined in the ITAR Part 120.10 or EAR Part 772 Automated Copper Alloy Grain Size Evaluation Using a Deep-learning CNN 49 5/19/2020", "year": 2018}, {"authors": ["Keras Documentation"], "title": "Docs \u00bb Layers \u00bb Convolutional Layers", "venue": "2019. [Online]. Available: https://keras.io/layers/convolutional/.", "year": 2019}, {"authors": ["LLC Minitab"], "title": "Coefficients table for Fit Regression Model", "venue": "2019. [Online]. Available: https://support.minitab.com/en-us/minitab/18/help-and-how-to/modeling-statistics/regression/howto/fit-regression-model/interpret-the-results/all-statistics-and-graphs/coefficients-table/#vif.", "year": 2019}], "sections": [{"text": "1 5/19/2020\nMoog Inc. has automated the evaluation of copper (Cu) alloy grain size using a deep-learning convolutional neural network (CNN). The proof-of-con ept automated image acquisition and batch-wise image processing offers the potential for significantly reduced labor, improved accuracy of grain evaluation, and decreased overall turnaround times for approving Cu alloy bar stock for use in flight critical aircraft hardware. A classification accuracy of 91.1% on individual sub-images of the Cu alloy coupons was achieved. Process development included minimizing the variation in acquired image color, brightness, and resolution to create a dataset with 12300 sub-images, and then optimizing the CNN hyperparameters on this dataset using statistical design of experimnts (DoE).\nOver the development of the automated Cu alloy grain size evaluation, a degree of \u201dexplainability\u201d in the artificial intelligence (XAI) output was realized, based on the decomposition of the large raw images into many smaller dataset sub-images, through the ability to explain the CNN ensemble image output via inspection of the classification results from the individual smaller sub-images.\nKeywords: metal grains, copper alloy, automatic inspection, CNN, convolutional neural network, deep learning, explainable AI, XAI, statistical design of experiments, DoE, aerospace, Moog"}, {"heading": "1. Introduction", "text": "The evaluation of copper (Cu) alloy bar-stock has historically been executed by trained metallurgical technicians. Removed bar stock ends are delivered to the Moog Inc. Global Materials & Process Engineering (M&PE) department in East Aurora NY for evaluation. The month-to-month bar-stock batch sizes were highly variable and typically ranged between 10 and 150. The batch size variation, when combined with differences between technician evaluations, resulted in inspection uncertainty, unpredictable process cycle times and an evaluation pr cess that was both tedious and monotonous.\nIn response to the above, automated analytical instrumentation combined with advanced image-recognition techniques were evaluated for implementation; additionally, the supply chain was engaged to level the monthly demand, and now bar-stock batch sizes range between 40 a d 45.\nThe proposed deep learning convolutional neural network (CNN) was developed as a proof-of-concept demonstration that will be evaluated against the improved inspection process described above. An automated inspection process using the CNN has the potential to:\n\u2022 Further reduce inspection variations to negligible levels \u2022 Improve the quality of the sample evaluations beyond the current level \u2022 Reduce process cycle time by eliminating human bias and errors in evaluation\n2 5/19/2020\n\u2022 Free skilled employees to focus on other value-added tasks that still require human cognizant decision making.\nAn overview of the development process that was used for the creation of the trained Cu-alloy CNN described in this writing is depicted in Figure 1. The numbers in the boxes refer to the applicable sections of this paper.\nThe dataset creation is described in Section 3, which involved physical sample preparation, high-resoluti n imaging of the physical samples, image pre-processing, sample labeling and dataset normalization. The CNN model architecture is described in Section 4. The CNN training methodology is provided in Section 5, and includes CNN hyperparameter screening and optimization experiments; a CNN weight-regularization experiment is also described. Ten-fold cross-validation is used to verify CNN performance after the optimization and weightregularization experiments. Another 10-fold cross-validation on an expanded dataset is used to baseline the final performance of the CNN. A summary of experimental results is provided in Section 6 followed by final con lusions in Section 7. Additional information about the hardware used for physical sample preparation, the software applications employed, computing hardware, details about the funding for this effort, as well as a glossary of terms are provided in Section 8.\nThe primary contribution of this work is to demonstrate of the use of statistical Design-of-Experiments (DoE) 1 and other forms of applied industrial statistics to set CNN hyperparameters to achieve the highest possible classification accuracy with the lowest run-to-run training variation. Statistical testing is used as the basis for either accepting the\n1 DoE and other applied statistical methods, first developed in the United Kingdom in the early 1920s for agricultural use, have been used routinely to maximize the efficiency of industrial product and process design since the mid-20th Century for the chemical and process industries, and then spreading to many other industries between the 1970s and 1990s [33].\n3 5/19/2020\nnull hypothesis (H0: \u00b51 = \u00b52) that CNN performance has not changed, or the altern hypothesis (HA: \u00b51 \u2260 \u00b52) that CNN performance has changed."}, {"heading": "2. Literature Review", "text": "This literature review is approached in three stages: first, the general application of machine learning and specific use of CNNs for inspection and/or classification of metals for process and quality control are reviewed; s cond, the best practices and research into the configuration and training of CNNs are reviewed; and third, the background of applied industrial statistics, especially the application of DoE for screening and optimization, are reviewed."}, {"heading": "2.1. Previous Applications", "text": "Improving the efficiency and accuracy of visually-oriented industrial inspection and quality control methodologies has always been a goal for industrial process development. Advances in machine vision technology coupled with machine-learning techniques have proven useful in moving industrial processes towards the aforementioned goal [1]. In recent years, a class of deep learning algorithms in the form of CNNs have made significant progress in enabling the recognition of spatial patterns in image data, with classification accuracies that equal or exceed those produced by human subject matter experts (SME) [2].\nA CNN was taught to recognize good metal additive manufacturing (AM) output by training it with labeled images of top surface printed CoCrMo (Cobalt Chromium Molybdenum) alloy output that showed even hatch lines and appropriate overlaps to indicate good quality welding [2]. The CoCrMo alloy AM dataset provided by the aforementioned CNN effort was used to demonstrate th t semi-supervised training may offer the potential to provide promising classification accuracy performance, using a more limited labeled dataset along with lower quality noisy and blurry images [3]. To improve the accuracy and speed of the industrial inspection for metal gears, CNN was used to classify defects, with a tradeoff demonstrated between processing time with direct classification of images, and accuracy, when a \u2018fine-grained\u2019 image preprocessing was used prior to classification [4]. A CNN was adapted for the automated visual inspection of surface defects of countersunk drilled holes in steel, which required considerable image preprocessing and data augmentation [5]. CNNs were applied to the detection of micro-sized defects on the surfaces of metal screws, with image preprocessing to extract the screw images for the CNN [6].\nAll the above sources provide deterministic performance metrics for the accuracies achieved by the CNNs."}, {"heading": "2.2. CNN Best Practices", "text": "Prior research and academic sources have established theoretical and empirically-based best practices for setting the hyperparameters of CNNs. The primary objectives of these best practices are to reducing overfitting a d improve classification accuracy.\nDuring CNN training, the batch size represents the number of training examples used in one forward and backward pass, which is often referred to as the mini-batch when the batch size is a fraction of the total training set size. \u201cSmall mini-batch sizes provide more up-to-date gradient calculations, which yields more stable and reliable training\u201d [7] , while larger mini-batch sizes require more memory space. Excessive mini-batch size may also make a CNN tend \u201cto converge to sharp minimizers of the training and testing functions\u2026sharp minima lead to poorer generalization\u201d [8] .\n4 5/19/2020\nThe Optimizer function is used for the gradient descent while training, which occurs during back propagation. Minibatch gradient descent was used for our application, which combines the advantages of both batch-gradient escent2 and stochastic gradient descent.3 We explored the use of three different optimizers: Adam [9], AdaMax [9] and Nadam [10] because each provide different levels of adaptive moment [11].\nThe Kernel Constraint is a method to prevent overfitting and improve generalization by limiting the magnitude of neuron weights in the fully-connected dense classificat on layers during training to the range of 3 to 4 [12]. Dropout is a regularization method to prevent CNN overfitting during training and improve generalization and works by stochastically dropping the specified percentage of neurons for the associated layer for each training mini-batch [12]. This counters the tendency for co-adaptation of CNN layers [12] by essentially presenting a different network architecture (i.e. a different model) for each training iteration. Srivastava, et al. demonstrated that dropout reduced classification error for the MNIST dataset, and that dropout values around 20% for input layers and 50% for hidden layers were optimal. Combining both kernel constrain nd dropouts \u201cprovides a significant boost over just using dropout\u201d [12].\nPooling layers were used to down-sample the representations of i put features created by the convolutional layers, which reduces the sensitivity of the CNN to changes in the position of features (i.e. improves translation invariance). Max-Pooling was used instead of average pooling because it is \u201cmore informative to look at the maximal presence of different features than at their average presence\u201d [13].\nThe filter kernels are used in the convolutional-operation4 in the convolutional layers of the CNN [14]. Smaller filter kernels reduce the receptor field to capture highly-localized image features, while filter kernls with larger receptor fields capture less localized more generaliz d image features [13]. Larger filter kernels alo require more computational processing.\nStride is a form of down-sampling (or information reduction) when stride length > 1, and is applied to the filt r kernel convolution in our CNN.\nThree different Activation functions TanH, SELU and ReLU were selected to provide different levels of resistance to excessively small, or \u2018vanishing\u2019 gradients during back-propagation while training the CNN. \u201cOne of many difficulties is that the norm of the back-propagated error gradient can grow or decay exponentially\u201d [15]. On one extreme, TanH or Hyperbolic Tangent, is a logistic function with an output that ranges between -1.0 and +1.0; however, the output can saturate with large inputs and only has sensitivity around 0\u2026the mid-point of he input [13]. On the other extreme, the ReLU or Rectified Linear Unit activation function provides a linear positive output for positive inputs and no output for any negative input. \u201cBecause rectified linear units are nearly linear, they preserve many of the properties that make linear models easy to optimize with gradient-based methods. They also preserve many of the properties that make linear models generalize well\u201d [13]. The SELU or Scaled Exponential Linear Unit is a self-normalizing function, as \u201cactivations close to zero mean and unit variance that are propagated through many network layers will converge towards zero mean and unit variance\u2014even under the presence of noise and perturbations\u201d [16]. This property permits robust learning within deep networks, and makes vanishing gradients impossible.\n2 The batch size = total training set size, which consumes computer memory and takes longer to compute, but provides a stable non-noisy gradient descent trajectory [11]. 3 When \u2018batch size\u2019 = 1, only one update at a time is performed, which reduces computer memory requirements and dramatically speeds up computations, but results in a very unstable and noisy (i.e. stochastic) gradient descent trajectory [11]. 4 The term \u2018convolution\u2019 is used to describe the operation of CNNs, but the mathematical process actually being used is \u2018cross-correlation\u2019 because there is no inversion of the filter kernel performed prior to its application to the input [13].\n5 5/19/2020\nWe explored weight regularization as a way to reduc the variance observed in the 10-fold cross-validation performed as described in Section 5.4.1. Larger weights can make a CNN less stable, because minor variations or noise in the inputs can produce larger changes at the outputs [17]. Weight regularization adds a vector normalization penalty to the training loss function to encourage the formation of smaller weights."}, {"heading": "2.3. Applied Industrial Statistics", "text": "Screening Experiments Industrial optimization projects often begin with te screening experiment, which is designed to identfy onrandom effects of responses within a stochastic environment across a wide range of variables. Because of information limitations within the configuration of experimental matrices, screening experiments with large numbers of factors were traditionally Resolution III designs (main-effects aliased with two-factor interactions) or at best Resolution IV designs (two-factor interactions aliased with other two-factor interactions) and d itional experimentation \u226b would be required to separate the confounded effects; furthermore, traditional screening designs only have the capability to study first-order effects [18].\nThe Definitive Screening Design (DSD) is a new class of recently developed three-lev l screening experiments, which can \u201cprovide estimates of main effects that are unbiased by any second-order effect, require only one more than twice as many runs as there are factors, and avoid confounding of any pair of second-order effects\u201d [18], where = 2 + 1, and , \u2026 , is the response vector of the normal linear regression model, \u201cwhere the parameters , \u2026 , are the vector of unknown regression coefficients, where denotes the \u210e two-factor interaction column, the \u210e entry of which is , , , while denotes the \u210e pure-quadratic effect column, the \u210e entry of which is ,\n\u201d [18], and { } is the unobservable vector of error terms ~ !\" , # $ where \" = 0, and > . The Equation (1) defines the normal linear regression response vector [18]\n= + \u2211 , () + \u2211 \u2211 ( *) +) () , , + \u2211 () , + where = 1 to . (1)\nThe DSD can also be augmented with two-level categorical factors where the experiment can estimate quadratic effects, the estimation of which decreases as more categorical factors are added. Augmenting the DSD with categorical factors in this way increases the requir d runs to 2 + 2 [19].\nOptimization Experiments A class of Response Surface Method (RSM) experiments called Central Composite Designs (CCD) were introduced in the mid-twentieth century [20]. The CCD embeds factorial design Cube Points with Center-Points and StarPoints to better estimate curvature in the response; a CCDcontains twice as many star-points as experimental design factors. The CCD can be rotated about its center-points when all points are equidistant from the design center and when the experiment is designed to pr vide constant prediction variance [21]. The experimental structure of a CCD is illustrated below, and shows how three factors f study can be arranged into DoE matrices as depicted by cubes in 3-dimensional space [22], where each experimental factor (A, B and C) is represented by one of the coordinate axes, the two-factor interactions (AB, AC and BC) are represented by each coordinate plane, and the thre -factor interaction (ABC) is represented by the cube volume:\n\u2022 The Figure 2 cube is a full-factorial DoE with three factors at two levels yielding eight different treatment combinations (shown as the red cube points). With just two levels per factor, only the linear effects can be estimated. This configuration is termed a full-factorial design because all the components of variation resulting from the three main effects, the three two-factor interactions, and the single three-factor interaction, can be separated in the analysis.\n6 5/19/2020\n\u2022 The Figure 3 cube has a center-point (grey) added so that the presence of curvature may be detected; however, the specific source or sources of curvature cannot be ascertained because the component or components of variation from quadratic effects will be pooled with the effects from all three factors. This DOE is no longer a full-factorial because the center-point is shared with the eight cube points. \u2022 The Figure 4 cube shows six face-centered star-points (grey) added to create a CCD RSM DoE. The starpoints allow the sources of curvature to be better estimated, and since the star-points are centered on the faces of the cube, the matrix shown is termed a Central-Composite Face-centered (CCF) RSM, where the star-point distances from the center point are \u00b11 (a normalized unit of the factor level settings notated as +\u03b1 or -\u03b1). For the CCF |\u03b1| = 1.0, and depending on the number of factors studied and the experimental strategy, the value of \u03b1 can be modified to change characteristics of the RSM. The CCF is not a fullfactorial matrix, because star-points are shared with the two associated factors in the cube face planes d the center-point is shared with the six star-points.\n\u2022 The Figure 5 cube shows the star-points extended outward to create a Central-Composite Circumscribed (CCC) RSM, with \u03b1 = 1.682, which allows the matrix to be rotated around the center-point in any orientation (each cube-point and star-point are on the surface of a sphere) [20] [21]. Conversely, if extending the factor extremes is not desirable or perhaps impossible (e.g. to prevent exceeding a design or process safe limit), the red cube points can be made into the star-points and the factor levels contracted by the reciprocal of 1.682, creating a Central-Composite Inscribed (CCI) RSM. The response surface may be mapped using sequential experimentation, where the RSM CCD can be moved over the experimental space by extending the matrix in any direction and then rusing part of the existing data for each move. Many small moves can be made sequentially along the path of steepest gradient descent using a method called Evolutionary Operations (EVOP) [14]. For n factors > 3, the experimental surface becomes a hypersurface of dimension n-1, within n-dimensional space.\nTo maintain the ability to rotate the experimental CCD matrix, the value of \u03b1 will change as a function of the number of experimental factorial runs [21]:\n7 = 8 9: ) ;< where 9 = the number of factorial runs. !2$\nIf the experiment is a full-factorial two-level design:\n7 = [2 ] ) ;< where 2 = 9 and = the number of experimental factors. !3$\nThe configuration of the CCD star-points impacts the ability to rotate the matrix around the center-points.\n7 5/19/2020"}, {"heading": "3. Creating the Data Set", "text": "Five Cu-alloy bar-stock end sample coupons were used to create the initial training and test dataset. The generation of this dataset represented a significant portion of the overall effort for this work."}, {"heading": "3.1. Preparing Samples from Metal Coupons", "text": "The Cu alloy coupons were prepared for inspection usi g standard methods.\n\u2022 In preparation for inspection by microscope, sample coupons (Figure 6) from each piece of Cu-alloy barstock are mounted in an autopolisher carrier for polishing and etching (Figure 7, Figure 8). \u2022 During the microscope inspection of the prepared sample coupons, grain size and grain size distribution are evaluated by a technician who inspects one small area t a time using metallograph images of magnified areas of the polished upper surface (Figure 9). \u2022 Using a fixture that holds each coupon in a precise location in the microscope\u2019s XY focal plane (Figure 10), the polished upper surfaces of these same sample coupons are also fully imaged (Figure 11) under high-magnification using a robotic microscope that s been programmed to process arrays of sample coupons automatically.\nFigure 6 \u2013 Typical Cu-alloy polished coupons (3/4 and 5/8 in. diameter)\nFigure 7 \u2013 Coupons in autopolisher carrier\nFigure 8 \u2013 Autopolisher\nFigure 11 \u2013 Typical robotic microscope image (63X mag.)6"}, {"heading": "3.2. Pre-Processing into Sub-Images", "text": "The sample coupons images created by the robotic microscope were saved as 22000 x 16600 pixel TIFFs with an approximate size of 1.2 GB for each image file. To prepare these large images for creation of the deeplearning\n5 Scale indicator intentionally omitted (see Section 8.1) 6 Ibid\n8 5/19/2020\ndataset, they were partitioned into smaller sub-image tiles and algorithmically compressed into a different file format.\nUsing the Image Slicer API (see Section 8.3), the large images were sliced into 2025 490 x 368 pixels sub-image tiles, and then the tiles were compressed via interpolation using the Scikit-image API (see Section 8.3) and converted to 189 x 142 pixel RGB JPEGs, and ranging in file siz from approximately 10 kB to 12 kB."}, {"heading": "3.3. Classification of Sub-Images", "text": "The grain structure of each sub-image was classified as either \u2018good\u2019, \u2018bad\u2019 or \u2018neutral\u2019 using an internal process standard based on ASTM E1282-97 [23] already used to determine if the metallograph-imaged grain structures are acceptable. The intent was to label data for a binary cl ssification CNN in order to classify grain structures as either acceptable or not acceptable. Figure 12 notionally depicts this process, where the average of the larg st orthogonal diameters (d1 and d2) are used to estimate the grain size7. The image samples with smaller gain sizes that would be obviously acceptable per the process standard were classified as \u2018good\u2019, while conversely, image samples with excessively large grain sizes that would be obviously rejected per the process standard were classified as \u2018bad\u2019. The \u2018neutral\u2019 classification were grain structures that could be regarded as either marginally acceptable or nonacceptable.\nThe metallograph images used in the existing manual process only represented a small fraction of the total grains visible in a coupon, and by the existing process standard, the coupon would be rejected if any of the limited number of metallograph images sampled from the coupon were d emed unacceptable; therefore, it was hypothesized that to be successful, the CNN would only need to recognize either obviously good or bad grain structures.\nThis approach produced approximately twice as many examples of \u2018good\u2019 grain structures as \u2018bad\u2019 examples.\n7 Note: Lines d1 and d2 are two orthogonal lines, drawn in a manner to maximize the summation of their lengths\n9 5/19/2020"}, {"heading": "3.4. Sample Images Normalization", "text": "Due to the large variations between coupon images in lighting, feature-contrast and color, the sub-images were RGB-weighted gray-scaled and histogram equalized [24] using the Scikit-image API. This transformation resulted in a loss of information and emphasized spatial featur s at the expense of color content; however:\n\u2022 The metallograph-based grain structure inspection pr cess standard uses only spatial features to accept or reject samples \u2022 The SME-based classification of the sub-images into \u2018go d\u2019 and \u2018bad\u2019 grain structure relied only on the spatial features \u2022 The CNN was only intended to replicate the metallograph process.\nBecause of the above bullet-points, we were confidet that the transformation process would not comproise the effectiveness of the CNN.\nAny coupon boundary images were discarded because the black background severely reduced image contrast when the histogram equalization transformation was used.\nThe transformation is shown for several coupon sub-section samples in Figure 13 (original colored images) and Figure 14 (original images gray-scaled and histogram equalized). The transformation produces what is essentially feature-wise standardization for the dataset becaus ll amples have an identical pixel-intensity mean and standard deviation."}, {"heading": "3.5. Ensemble Image Classification", "text": "After the gray-scaled histogram equalized tiles were classified as either \u2018good\u2019 or \u2018bad\u2019 by the CNN, the corresponding color tiles were then red color-tinted using a luminesce-preserving algorithm that varied the red intensity based on the classification probability (P) of the CNN, where P = 0 indicated a 0% probability and P = 1 indicated a 100% probability that the tile had good grain structure. Both the non-tinted and tint-classified tiles were\n10 5/19/2020\nreassembled8 into ensemble images of the original Cu-alloy coupons that could be inspected by an operator. The ensemble images allowed a very accurate basis for either accepting or rejecting Cu alloy coupons.\nThe ensemble image also provided a forensic capability to understand why a coupon was either acceptable or not acceptable, because each sub-image tile classification could be checked both visually by tint and numerically by classification probability. Two examples are shown in Figure 15 for two different sample coupon images. The forensic capability for the coupon ensemble image provides some transparency into the machine-based classification process since the decision of the CNN could be understood down to the level of individual tile partitions. This transparency therefore represents a degree of explainable AI (XAI) [25]."}, {"heading": "3.6. Data Augmentation", "text": "After sub-image classification was completed using image samples obtained from the first five Cu-alloy c upons, there were approximately twice as many \u2018good\u2019 examples of grain structure as there were \u2018bad\u2019 examples. This was expected since normally more Cu-alloy coupons are acc pted by the existing process than are rejected. To eliminate possible model bias towards \u2018good\u2019 classifications, the CNN training dataset was balanced; therefore, the \u2018bad\u2019 examples were subjected to methods [26] of data augmentation using the K ras API (see Section 8.3) [27], which involved random rotations, vertical and horizontal flips and shifts, and shears9, while employing a wrapping fillmode for the displaced areas of the augmented images. Using this data augmentation, the quantity of \u2018bad\u2019 grain structure examples was doubled.\n8 The blurry rectangular patterns visible in the reconstructed imaged in Figure 15 are artifacts from how the robotic microscope saves the coupon images it creates; the edges of rectangular sub-images are overlapped by 1% with neighboring sub-images when the coupon image is reconstructed. These blurry artifacts are entrained in the CNN dataset. 9 Random zooms were not used because this would have changed the apparent size of the grains.\n11 5/19/2020"}, {"heading": "3.7. Datasets", "text": "3.7.1. Initial Dataset The initial dataset had 7420 total images (5020 for training, 1600 for training validation, and 800 for test verification), equally split between good and bad classifications. Approximately 50% of the bad grain structure examples, which represented 25% of the initial dataset, were created using the data augmentation methods described above.\n3.7.2. Expanding the Dataset Later, a larger dataset was constructed by adding examples from 14 additional Cu-alloy coupons to the initial dataset. No data augmentation was used with this additional data and instead the number of added \u2018good\u2019 examples was limited to the number of added \u2018bad\u2019 examples. The resulting combined larger dataset had 12300 total images (8700 for training, 2400 for training validation, and 1200 for test verification), equally split between good and bad classifications. Approximately 30% of the bad grain structure examples, which represented 15% of the larger dataset, were augmented images that came from the ini ial dataset."}, {"heading": "4. Model Architecture", "text": "Simple CNN architecture was selected to resist overfitting. This is because a model that is physically limited in its data storage capacity will be forced to focus on the most significant features within the data, which translates to better generalization [28]. Therefore, the CNN described in the Keras blog article [27] was adapted to our application as shown in Figure 16 below. It uses only three convolutional layers with a limited number of filters in each layer for feature extraction, and only two fully-connected hidden dense layers for classification. Additionally, to disrupt random correlations within the data to further reduce the chances of overfitting, stochastic dropouts within layers and layer output pooling were also used. The Appendix A: CNN Structure provides additional details."}, {"heading": "5. Training Methodology", "text": "Besides the dataset creation, the CNN training represented another substantial investment of effort. Therefore, to improve the efficiency of CNN training and optimization, we explored using DoE. If this approach was successful, it could be applied with other machine-learning anddeep-learning projects.\n12 5/19/2020\nAccordingly, the hyperparameters for both CNN layers and training were simultaneously studied as model factors using DoE. Initially, screening experiments were usd to statistically screen a large number of hyperparameter factors to assess which factors should be studied in greater detail. The screening experiment was then followed by optimization experiments.\nThe following hyperparameters were selected for initial screening.\n\u2022 Training batch size (a subset of the training component of the dataset) \u2022 Stride length and padding in the input convolutional layer \u2022 Filter kernel sizes in the convolutional layers \u2022 Activation functions on the layer outputs \u2022 Max-normal kernel constraint in the densely-connected classification layers during training \u2022 Stochastic dropouts in the convolutional and densely-connected layers during training \u2022 Max-pooling after the convolutional layers \u2022 Various optimization functions for training\nNote that these hyperparameters are already known t affect the performance of CNNs, and often these are set following best practices already established by researchers and practitioners of deep-learning [29]. As alluded to previously, we decided to test the efficacy of the DoE approach for tuning CNN performance by confirming the established best practices and other findings of machine/deep-learning research and theory to establish a proven optimization methodology for use on future projects.\nThe processing methodology developed for this project is notionally depicted in\nFigure 17. Blue items were automated, while green items requi d manual intervention. The Minitab application was used to design and create the experimental matrices, which were then converted to CSV-format files that could be read by the main application written in the Python programming language. Minitab was also used to analyze the experimental output. The Keras API was used to create the model shown in Figure 16, while the Tensorflow API backend ran the model. Other specialized Python-based APIs were also used; refer to Section 8.3 for additional information. This approach was initially developed and tested on the MNIST public domain dataset using a different deep learning CNN model, which achieved 99.4% validation accuracy during training.\nFigure 17 \u2013 Processing Methodology"}, {"heading": "5.1. Experimental Responses", "text": "Several response variables were used to analyze the results of experimental outputs (underlined respones were used for the experimental analysis).\n13 5/19/2020\nGroup-A Responses\n\u2022 Time: the time required in minutes for the CNN to run through all the training epochs for each experimntal treatment combination. \u2022 Training Accuracy (TRNACC): the average training accuracy of the last 5 epochs for each experimental treatment combination. \u2022 Validation Accuracy (VALACC): the average validation accuracy of the last 5 epochs for each experimental treatment combination. \u2022 Confusion Matrix Outputs: metrics obtained by testing the classification performance of the trained CNN on the test dataset for each experimental treatment combination [30]. The confusion matrix is shown in Table 1.\nPredicted Class Labels Class Bad Grains Good Grains True Rates\nActual Class Labels Bad Grains True Negative (TN) False Positive (FP) Specificity = TN / (TN + FP) Good Grains False negative (FN) True Positive (TP) Sensitivity = TP / (TP + FN)\nFalse Positive Rate = FP / (FP + TN) Precision = TP / (TP + FP)\nAccuracy = (TN + TP) / (TN + FN + FP + TN)\nTable 1 \u2013 Confusion Matrix for the Classification CNN\n\u2022 Test Accuracy (TSTACC): the number of all correct predictions divided by the total number of the datase .\nGroup-B Responses\n\u2022 Specificity (True Negative Rate - TNR): the number of correct negative predictions divided by the total number of negatives. \u2022 Sensitivity (Recall or True Positive Rate - TPR): the number of correct positive predictions divided by the total number of positives. \u2022 False Positive Rate: the number of incorrect positive predictions divided by the total number of negatives (or 1 \u2013 Specificity). \u2022 Precision (Positive Predictive Rate - PPR): the number of correct positive predictions divided by the total number of positive predictions.\nNote that \u2018False Positive Rate\u2019 may be studied by analyzing \u2018Specificity\u2019.\nFor the initial dataset:\n\u2022 The test set size N = 800 \u2022 Actual bad grains = 400 \u2022 Actual good grains = 400"}, {"heading": "5.2. Screening Experiments", "text": "We defined the Optimizer function as a continuous experimental factor because the three different optimizer functions provide different levels of adaptive moment. The approach we used was to treat the differencs in adaptive\n14 5/19/2020\nmoment\u2014as represented by these optimizer functions, as the level settings. Note that coded units (i.e. 1, 2, and 3) were used in the actual DSD matrix instead of the cat gorical values.\nThe Drops for the three convolutional layers, as well as the first dense classification layer were varied as shown in Table 2 (Conv1, Conv2, and Conv3, as represented by the feature extraction and Dense1 as represented by the classification parts, respectively, of the CNN shown in Figure 16). The mid-levels used in the DSD experiment were set to 20% for the convolutional layers, and 30% for the dense layer.\nPooling between convolutional layers was either not-applied (\u201cnone\u201d) or applied (\u201ctwo\u201d) as a categorical experimental factor.\nThe two-dimensional10 Filter kernel sizes for the convolutional layers were varied between 3\u00d73, 5\u00d75 and 7\u00d77 pixels. Stride was applied to the convolution of the Filter kernel with the input image for the convolutional layer 1 ( CONV1); it was set to either a Stride length of \u201cone\u201d or \u201ctwo\u201d as a categorical experimental factor. When set to \u201ctwo,\u201d the Filter kernel would be stepped across two pixels in the input image for each convolution, skipping a line of pixels on the input image [31].\nPadding in the convolutional layers was either applied (\u201csame\u201d) or not applied (\u201cvalid\u201d) as a categorical experimental factor. If applied, the level of Padding for all convolutional layers was the \u201csame\u201d as the Stride factor setting for CONV1, CONV2 and CONV3. When Padding = \u201csame\u201d and Stride = \u201cone\u201d or \u201ctwo,\u201d rows of zeros would be added (i.e. padded) to the far input image borders to accommodate the Stride length and Filter kernel sizes. When Padding = \u201cvalid,\u201d no rows of zeros would be added to the input image borders, regardless of the Stride setting and Filter kernel sizes [31].\nThe Activation function was defined as a continuous experimental factor, although categorical values were selected for the levels based on resistance to vanishing gradients. Note that coded units (i.e. 1, 2 and 3) were used in the actual DSD matrix instead of the categorical values.\n5.2.1. DSD Factors and Levels A total of 16 factors, which were a combination of CNN layer and training hyperparameters, were selectd for study using a DSD with 34 treatment-combinations11. The presence of five categorical factors increased the number of treatment combinations to 2 + 2. The DSD factors, class (training or CNN layer), type (continuous or categorical) and levels (1, 2, and 3) are shown in Table 2. Note the use of coded units for the Optimizer and Activation factors, which had three categorical levels for each of these factors. Conversely, several factors such as M x-Pooling that could be called continuous have limited settings, so these were treated as categorical factors.\nFactor Name Factor Class Factor Type Level 1 Level 2 Level 3 1. Batch Size Training Continuous 80 160 240 2. Kernel Constraint Training Continuous 3 5 7 3. Optimizer Training Continuous12 Adam AdaMax Nadam 4. Drops Conv1 Training Continuous 0% 10% 20% 5. Drops Conv2 Training Continuous 0% 10% 20% 6. Drops Conv3 Training Continuous 0% 10% 20% 7. Drops Dense1 Training Continuous 10% 30% 50% 8. Max-Pooling Conv1 CNN Layer Categorical None Two 9. Max-Pooling Conv2 CNN Layer Categorical None Two\n10 The images processed were RGB grayscale, with three color channels, so the kernel filters were actually 3 \u00d7 3 3, 5 \u00d7 5 \u00d7 3 and 7 \u00d7 7 \u00d7 3. 11 See the glossary in Table 35. 12 This is a categorical factor turned quasi-continuous. The matrix used coded units where 1 = Adam, 2 = AdaMax and 3 = Nadam.\n15 5/19/2020\n5.2.2. DSD Experimental Matrix The DSD experimental matrix14 is shown in Table 3, and is given in standard design order; the actual run order was randomized upon execution. The blue rows indicate the treatment combinations where the CNN model was able to successfully learn the data set.\nTC Batch Kern Const Optm Drop C1 Drop C2 Drop C3 Drop D1 MaxP C1 MaxP C2 MaxP C3 Filt C1 Filt C2 Filt C3 Padd Stride C1 Active 1 160 7 3 0.2 0.2 0.2 0.5 2 2 2 7 7 7 S 2 3 2 160 3 1 0 0 0 0.1 0 0 0 3 3 3 V 1 1 3 240 5 1 0 0.2 0 0.5 2 0 2 3 3 7 V 2 3 4 80 5 3 0.2 0 0.2 0.1 0 2 0 7 7 3 S 1 1 5 240 7 2 0 0 0.2 0.1 2 0 2 7 3 3 S 1 3 6 80 3 2 0.2 0.2 0 0.5 0 2 0 3 7 7 V 2 1 7 240 7 3 0.1 0 0 0.5 0 0 2 7 7 3 V 2 1 8 80 3 1 0.1 0.2 0.2 0.1 2 2 0 3 3 7 S 1 3 9 240 3 3 0.2 0.1 0 0.1 2 0 0 7 7 7 V 1 3 10 80 7 1 0 0.1 0.2 0.5 0 2 2 3 3 3 S 2 1 11 240 7 1 0.2 0.2 0.1 0.1 0 0 2 3 5 5 S 1 1 12 80 3 3 0 0 0.1 0.5 2 2 0 7 3 3 V 2 3 13 240 3 3 0 0.2 0.2 0.3 0 0 0 7 3 7 S 2 1 14 80 7 1 0.2 0 0 0.3 2 2 2 3 7 3 V 1 3 15 240 3 1 0.2 0 0.2 0.5 0 0 0 3 7 3 S 2 3 16 80 7 3 0 0.2 0 0.1 2 2 2 7 3 7 V 1 1 17 240 7 3 0.2 0.2 0.2 0.5 2 0 0 3 3 3 V 1 1 18 80 3 1 0 0 0 0.1 0 2 2 7 7 7 S 2 3 19 240 3 1 0 0.2 0 0.5 2 2 0 7 7 3 S 1 1 20 80 7 3 0.2 0 0.2 0.1 0 0 2 3 3 7 V 2 3 21 240 7 1 0 0 0.2 0.1 2 2 0 5 7 7 V 2 1 22 80 3 3 0.2 0.2 0 0.5 0 0 2 5 3 3 S 1 3 23 240 7 3 0 0 0 0.5 0 2 0 3 5 7 S 1 3 24 80 3 1 0.2 0.2 0.2 0.1 2 0 2 7 5 3 V 2 1 25 240 3 3 0.2 0 0 0.1 2 2 2 3 3 5 S 2 1 26 80 7 1 0 0.2 0.2 0.5 0 0 0 7 7 5 V 1 3 27 240 7 1 0.2 0.2 0 0.1 0 2 0 7 3 3 V 2 3 28 80 3 3 0 0 0.2 0.5 2 0 2 3 7 7 S 1 1 29 240 3 3 0 0.2 0.2 0.1 0 2 2 3 7 3 V 1 3 30 80 7 1 0.2 0 0 0.5 2 0 0 7 3 7 S 2 1 31 240 3 1 0.2 0 0.2 0.5 0 2 2 7 3 7 V 1 2 32 80 7 3 0 0.2 0 0.1 2 0 0 3 7 3 S 2 2 33 160 5 2 0.1 0.1 0.1 0.3 0 0 0 5 5 5 V 1 2 34 160 5 2 0.1 0.1 0.1 0.3 2 2 2 5 5 5 S 2 2\n13 This is another (the second) categorical factor turned quasi-continuous. The matrix used coded units where 1 = TanH, 2 = SELU and 3 = ReLU. 14 TC = Treatment Combination, Batch = Batch Size, KCon = Kernel Const, Optm (1,2,3) = Optimizer (Adam, AdaMax, Nadam), Drop C1/C2/C3/D1 = Dropout CONV1/CONV2/CONV3/DENSE1, MaxP C1/C2/C3 = Max Pooling CONV1/CONV2/CONV3, Filt C1/C2/C3 = Filter Size CONV1/CONV2/CONV3, Padd = Padding on CONV1/CONV2/CONV3, StrideC1 = Stride CONV1, Active (1,2,3) = Activation (TanH, SELU, ReLU).\n16 5/19/2020\nTable 3 \u2013 DSD Experimental Matrix\n5.2.3. DSD Experimental Results Each treatment combination in the matrix was iterated through 35 training epochs, where for each epoch the CNN model is subjected to one forward pass and one backward pass for all training images, with the number of iterations in the epoch determined by the Batch size. During the forward pass, the CNN model attempts to fit the data in the Batch using the neuron weights between layers (initially randomized at the start of training), and then the predicted outputs are evaluated against the expected (known) utputs of the training examples. During the backward pass (backpropagation), neuron weights between layers are adjusted to minimize the penalty as calculated by a inary cross-entropy loss function. The number of iterations for each epoch is equal to the number of training samples divided by the batch size: for Batch sizes of 80, 160 and 240, there were 63, 32 and 21 iterations respectively. At the end of each epoch, the CNN model attempts to classify the validation image samples (not seen during the training iterations) and based on the loss calculation on the validation set, the weights are readjusted by the Optimizer to achieve the steepest gradient descent of the hyp rplane of the loss function. Once all 35 epochs were completed, the trained CNN model weights with the highest validation accuracy15 from the 35 epochs was shown the test image samples. The test image samples were not part of either the training or validation sets used during the epoch training iterations. The test image evaluation produces a confusion matrix output for each experimental treatment combination as described in Section 5.1.\nThe responses for the 34 treatments combinations are given in Table 4. The training and validation accura ies represent the average values of the last 5 epochs (31 to 35) and test accuracy represented the overall t st accuracy achieved on the test data set. Only 5 (14.7%) of the treatment combinations (highlighted blue) learned th training data16.\nTC E P O C H First Experiment (Group-A) Replicated Experiment (Group-B) Time TRN ACC VAL ACC TST ACC TPR TNR PPR Time TRN ACC VAL ACC TST ACC TPR TNR PPR 1 35 10.78 0.5043 0.4999 0.5000 0.0000 1.0000 0.0000 10.93 0.4990 0.4994 0.5000 0.0000 1.0000 0.000 2 35 13.32 0.5000 0.5023 0.5000 1.0000 0.0000 0.5000 14.00 0.4931 0.4991 0.5000 0.0000 1.0000 0.0000 3 35 10.18 0.7817 0.7897 0.8013 0.8375 0.7650 0.7809 10.13 0.7736 0.7876 0.7975 0.8525 0.7425 0.0000 4 35 11.53 0.5007 0.4993 0.5000 1.0000 0.0000 0.5000 11.47 0.5004 0.5007 0.5000 1.0000 0.0000 0.7680 5 35 10.38 0.7177 0.7471 0.7575 0.9450 0.5700 0.6873 10.32 0.7810 0.7995 0.8138 0.9100 0.7175 0.5000 6 35 10.65 0.4912 0.4984 0.5000 1.0000 0.0000 0.5000 10.58 0.4953 0.5045 0.5000 0.0000 1.0000 0.7631 7 35 10.22 0.5001 0.4990 0.5000 0.0000 1.0000 0.0000 10.27 0.5001 0.4994 0.5000 1.0000 0.0000 0.0000 8 35 10.68 0.4924 0.5022 0.5000 0.0000 1.0000 0.0000 10.83 0.4955 0.4999 0.5000 1.0000 0.0000 0.5000 9 35 10.57 0.5003 0.4971 0.5000 1.0000 0.0000 0.5000 10.60 0.4988 0.4991 0.5000 1.0000 0.0000 0.5000 10 35 10.50 0.4959 0.5013 0.5000 0.0000 1.0000 0.0000 10.57 0.4946 0.4980 0.5000 1.0000 0.0000 0.5000 1117 35 10.88 0.5362 0.5418 0.5438 0.6480 0.4408 0.5368 10.91 0.5402 0.5448 0.5438 0.6070 0.4805 0.5000 12 35 10.47 0.4933 0.5082 0.5000 0.0000 1.0000 0.0000 10.50 0.4965 0.4996 0.5000 0.0000 1.0000 0.5388 13 35 10.87 0.5004 0.5003 0.5000 0.0000 1.0000 0.0000 10.93 0.5007 0.4999 0.5000 0.0000 1.0000 0.0000 14 35 10.57 0.7876 0.8112 0.8075 0.9025 0.7125 0.7584 10.57 0.8320 0.8135 0.8138 0.9850 0.6425 0.0000 15 35 10.87 0.4948 0.4991 0.5000 0.0000 1.0000 0.0000 10.93 0.4998 0.4998 0.5000 0.0000 1.0000 0.7337 16 35 10.52 0.4999 0.4996 0.5000 0.0000 1.0000 0.0000 10.57 0.5000 0.5002 0.5000 1.0000 0.0000 0.0000 17 35 10.78 0.4916 0.5013 0.5000 1.0000 0.0000 0.5000 10.77 0.5000 0.4999 0.5000 0.0000 1.0000 0.5000 18 35 10.55 0.7077 0.7111 0.7738 0.8475 0.7000 0.7386 10.57 0.7362 0.7427 0.8013 0.7175 0.8850 0.0000 19 35 10.33 0.4911 0.5030 0.5000 1.0000 0.0000 0.5000 10.42 0.4958 0.4997 0.5000 0.0000 1.0000 0.8619\n15 The Keras API \u2018checkpoint\u2019 feature was used to save the highest validation accuracy weights during training, and the best-performing model weights from the 35 epochs within each treatment was used to perform the test image classification after each treatment was completed. See Section 8.3 for additional information about Keras. 16 In our experience with industrial screening experiments, the reverse normally occurs, where 75% or more of the treatments yield positive results (during screening, the factor levels are usually extended beyond normal limits to encourage \u2018bad\u2019 responses. 17 Treatment 11 would not iterate due to a memory error in the GPU; this error was repeatable between the first and replicated experimental runs.\n17 5/19/2020\nThe experimental run-time required approximately 6 hours and 10 minutes to conduct all 35 epochs for each treatment; the experimental run-time in minutes is equal to the sum of the \u2018Time\u2019 column responses.\nTo validate the results of the first experiment andlso improve the precision of the responses, the exp riment was repeated (replicated), which is not normally used in screening experiments, but was used in this case because of the stochastic nature of CNN training. The replicated experiment provided nearly identical results to the first experiment, including Treatment 11, which would notrun in the GPU18.\nThe treatments (3, 5, 14, 18 and 34) where the CNN model was able to learn the dataset are shown in Table 5. Normally, for CNN training, the number of training epochs would be raised until the training accuracy stops increasing, but in the case for this screening experiment, the training epochs were kept to a relatively low number because the goal was to capture the influence on accur y from significant factors and reduce the time ne ded to run the experiment.\nTC First Experiment Replicated Experiment\nModel Accuracy Plots19 Test Confusion Matrix Model Accuracy Plots18 Test Confusion Matrix\n3\n18 Due to described error above in FN17, the response values for Treatment 11 as shown in Table 4 are the average of the other associated 33 treatment responses for each experimental run within the replicate group (1 or 2). 19 Keras generates accuracy plots where the validation accuracy at the end of each epoch is shown as \u2018test\u2019 accuracy (orange plot).\n18 5/19/2020\nNotice that the orange test accuracy plots18 are generally higher than the blue training accuracy plots, which provided good indication that the CNN was not overfitting.\nThe stochastic nature of the training used for the CNN model produced different results using identical datasets20. In some cases, the output training accuracy plots and test confusion matrices exhibited similar patterns between the first and replicated experiments. In other cases, such as Treatment 34, the replicated CNN model exhibited poor generalization performance, as evidenced by the misclas ification of good grains in the test dataset a shown in the confusion matrix. Experimental replication helps to overcome these variations for the analysis, and provides better precision when attempting to estimate the Equation (1) model coefficients.\nBecause of the low number of treatments that were abl to successfully learn the dataset, there was an initial temptation to revisit the experimental design and redo the entire experimental screening phase. Instead, we decided to continue the project with the screening results we had.\nThe full ANOVA results for the statistical model of the DSD experiment test accuracy are given in Table 6.\n20 The CNN model weights are initialized randomly at the start of each training epoch, a percentage of the weights are stochastically dropped during training and a stochastic gradient descent optimizer is used while training.\n19 5/19/2020\nThe statistically significant model effects are highl hted in blue, where the 7 risk for determining statistical significance was 0.05 (applied to the ANOVA P-values for the F-values of the F-distributions). The contribution shows the contribution to variation for each statisical model component using the sequential sums-of-squares. The factor F-values (the ratio of the adjusted sum-of-squares of the effect over the error J adjusted sums-of-squares) provide an estimate of a factor\u2019s power with respect to the total variation observed in the experimental results. Error contributed 12.7% to the total observed variation in the experimental response vector.\nThe model residual sample standard deviation (S) was 0.05 (or 5.0% test accuracy), which appears large exc pt that the predicted fit of the model (R-sq-(pred)) is approximately 74%, indicating that this selection of model terms is a good predictor of test accuracy. The variance inflation factor (VIF) of the model\u2019s linear terms are all near a value\n20 5/19/2020\nof 1 while the VIF of model\u2019s square terms range betwe n 1.60 and 2.83, showing this model for test accuracy has moderate multicollinearity21, or correlation, between predictor terms [32].\nAn abbreviated summary of the DSD ANOVA results for the Group-A22 experimental responses, including \u2018Test Accuracy\u2019, is provided in Table 7. The statistically significant model factors with \u03b1 < 0.05 are shaded blue, while model factors with \u03b1 < 0.10 are shaded orange. The \u03b1-risk indicates if we can reject the null-hypothesis w th either a 95% (blue) or 90% (orange) confidence. The 90% confidence factors are highlighted for reference purposes.\nThe time in minutes (to run 35 epochs) response had a very low component of error (3.1%) and 13 of the 16 factors had statistically significant effects. Four continuous factors (batch, dropC1, filtC3 and activation) had both linear and quadratic components that were significant, while others had either only linear (dropC2, dropD1, filtC1) or only quadratic (optimize and dropC3) effects that were significant. All five of the categorical factors (maxpC1, maxpC2, maxpC3, padding and strideC1) effects were statistically significant. The Batch factor contributed about 26% (linear + quadratic) of the total variation seen in the time response. The R-squared value was 0.97. The regression model had a predicted sums-of-squares (PRESS)23 of 0.19 with a residual sample standard deviation (S) of 0.23 minutes.\nThe training, validation, and test accuracy responses had similar results with \u03b1 < 0.05, with 9 of 16 factors having statistically significant effects. Continuous factors had either linear (dropC1, dropC2, dropC3 and activation) or quadratic (constrain, dropD1) effects that were significant, and only one factor had both linear and quadratic effect (optimize), while only two of the categorical factors (maxpC1 and maxpC3) had effects that were statistically significant. The continuous factor with the largest contribution to the accuracy responses was constrain, which ranged between approximately 12% and 14% for the thr e responses. The categorical factor with the largest contribution to the accuracy responses was m xpC3, which was approximately 19% for all three responses. The component of experimental error for training, validt on and test accuracies were 11.40%, 11.37% and 12.73% respectively. The R-squared values for training, validation and test accuracies were 0.886, 0.886 and 0.873 respectively. The regression models had PRESS values of 0.16, 0.17 and 0.19 respectively, with a residual sample standard deviations (S) of 4.27%, 4.34% and 4.60% training, validation and test accuracies respectively.\nAn abbreviated summary of the DSD ANOVA results for the Group-B24 experimental responses, including the test accuracy, is provided in Table 8. As before, statiically significant model factors with \u03b1 < 0.05 are shaded blue, while model factors with \u03b1 < 0.10 are shaded orange. The \u03b1-risk indicates if we can reject the null-hypothesis w th either a 95% (blue) or 90% (orange) confidence. The 90% confidence factors are highlighted for reference purposes. The VIF values are identical to those listed in Table 6.\nDSD Experiment ANOVAs: Response Group-A Blue = \u03b1 < 0.05 (95% confidence), Orange = \u03b1 < 0.10 (90% confidence) Time\n(35 epochs)\nTraining Accuracy (average last 5 epochs)\nValidation Accuracy (average last 5 epochs)\nTest Accuracy (after 35 epochs)\nSource DF Contribution\nFValue PValue Contribution FValue PValue Contribution FValue PValue Contribution FValue P-\nValue Model 27 96.90% 46.35 0.000 88.60% 11.52 0.000 88.63% 11.55 0.000 87.27% 10.16 0.000 Linear 16 62.02% 50.80 0.000 54.16% 12.05 0.000 55.07% 12.19 0.000 56.03% 11.17 0.000 batch 1 4.64% 154.59 0.000 0.05% 2.64 0.112 0.01% 1.65 0.206 0.02% 1.91 0.174 constrain 1 0.00% 0.21 0.653 1.51% 3.60 0.065 1.38% 2.47 0.124 1.21% 2.74 0.106 optimize 1 1.12% 1.73 0.196 6.94% 37.09 0.000 8.31% 40.74 0.000 7.99% 37.65 0.000 dropC1 1 1.69% 42.46 0.000 1.76% 4.78 0.035 1.73% 5.47 0.024 2.57% 6.44 0.015 dropC2 1 0.19% 4.13 0.049 2.36% 4.49 0.04 2.18% 4.96 0.032 2.79% 4.67 0.037 dropC3 1 0.33% 1.54 0.221 3.21% 13.88 0.001 2.70% 11.50 0.002 3.56% 14.25 0.001 dropD1 1 0.40% 9.02 0.005 0.72% 1.69 0.202 0.80% 1.46 0.234 0.87% 2.24 0.143\n21 VIF = 1: not correlated, 1 < VIF < 5: moderately correlated; VIF > 5: highly correlated. 22 See Section 5.1 of this report. 23 A measure of the deviation between the fitted values and the observed values, where smaller values of PRESS indicate a better predictive model 24 See Section 5.1.\n21 5/19/2020\nDSD Experiment ANOVAs: Response Group-A Blue = \u03b1 < 0.05 (95% confidence), Orange = \u03b1 < 0.10 (90% confidence) Time\n(35 epochs)\nTraining Accuracy (average last 5 epochs)\nValidation Accuracy (average last 5 epochs)\nTest Accuracy (after 35 epochs)\nSource DF Contribution\nFValue PValue Contribution FValue PValue Contribution FValue PValue Contribution FValue P-\nValue maxpC1 1 17.24% 107.54 0.000 6.78% 19.15 0.000 7.82% 23.86 0.000 5.40% 12.81 0.001 maxpC2 1 12.13% 85.32 0.000 0.05% 0.72 0.400 0.00% 1.11 0.299 0.01% 1.29 0.262 maxpC3 1 10.31% 150.07 0.000 19.17% 63.64 0.000 19.22% 61.56 0.000 18.90% 56.32 0.000 filtC1 1 0.04% 5.96 0.019 0.14% 0.81 0.372 0.06% 0.32 0.578 0.01% 0.18 0.673 filtC2 1 0.09% 1.57 0.217 0.67% 2.01 0.164 0.47% 1.10 0.301 0.57% 1.63 0.210 filtC3 1 0.09% 4.51 0.04 0.09% 0.30 0.586 0.17% 0.81 0.372 0.01% 0.02 0.887 padding 1 1.36% 29.82 0.000 0.22% 2.91 0.096 0.17% 2.04 0.161 0.44% 4.04 0.051 strideC1 1 12.33% 109.02 0.000 0.01% 0.03 0.868 0.00% 0.01 0.908 0.01% 0.02 0.882 activation 1 0.06% 8.03 0.007 10.50% 30.71 0.000 10.06% 31.14 0.000 11.66% 30.41 0.000 Square 11 34.88% 40.95 0.000 34.45% 10.99 0.000 33.56% 10.74 0.000 31.24% 8.93 0.000 batch*batch 1 21.26% 189.78 0.000 1.48% 2.34 0.134 1.10% 2.95 0.093 0.68% 2.64 0.112 constrain*constrain 1 0.00% 0.85 0.362 13.62% 28.72 0.000 13.08% 27.66 0.000 11.97% 24.3 0.000 optimize*optimize 1 0.46% 4.79 0.035 5.34% 20.09 0.000 6.24% 24.24 0.000 5.71% 21.42 0.000 dropC1*dropC1 1 0.25% 4.16 0.048 0.95% 2.66 0.110 1.13% 2.96 0.093 1.30% 2.64 0.112 dropC2*dropC2 1 0.08% 2.29 0.138 0.64% 2.61 0.114 0.83% 3.17 0.083 0.89% 2.64 0.112 dropC3*dropC3 1 0.00% 36.11 0.000 0.05% 0.54 0.468 0.05% 0.09 0.772 0.09% 0.63 0.433 dropD1*dropD1 1 0.02% 0.00 0.952 9.28% 37.84 0.000 8.28% 34.06 0.000 7.07% 26.79 0.000 filtC1*filtC1 1 0.05% 0.26 0.613 1.14% 2.88 0.098 1.41% 3.84 0.057 1.20% 2.64 0.112 filtC2*filtC2 1 0.04% 0.86 0.358 0.59% 1.92 0.174 0.06% 0.16 0.694 0.83% 2.42 0.128 filtC3*filtC3 1 12.24% 160.21 0.000 0.70% 2.25 0.141 0.62% 1.98 0.167 0.68% 1.96 0.169 activation*activation 1 0.47% 6.01 0.019 0.66% 2.33 0.135 0.76% 2.69 0.109 0.84% 2.64 0.112 Error 40 3.10% 11.40% 11.37% 12.73% Total 67 100.00% 100.00% 100.00% 100.00%\nModel Summaries S R-sq PRESS S R-sq PRESS S R-sq PRESS S R-sq PRESS 0.23359 0.9690 5.617 0.04269 0.8860 0.15832 0.043411 0.886 0.1648 0.04597 0.873 0.1874\nTable 7 \u2013 DSD Summary of Significant Factors for Group-A Response ANOVAs\nThe Group-B responses are more detailed evaluations of the information within the confusion matrix, as explained in Section 5.1. The primary observation for this group of response was that the factors selected in the experiment had little influence on the observed experimental results.\nFor sensitivity (recall or true positive rate), only the quadratic component of the Batch factor had statistical significance with \u03b1 < 0.05. The stride and filtC2 factors had effects with \u03b1 < 0.10, with P-values of 0.095 and 0.070 respectively. The component of experimental error was 53.4%. The R-squared value for the model was only 0.47. The regression model had a PRESS value of 20.4, with a residual sample standard deviation (S) of 43.5% for true positive rate.\nSpecificity (true negative rate) had only the quadratic component of the dropD1 factor with statistical significance with \u03b1 < 0.05. The stride, batch, filtC1, filtC3 and activation factors had effects with \u03b1 < 0.10, with P-values of 0.094, 0.079, 0.067, 0.071 and 0.067 respectively. The component of experimental error was 50.5%. The R-squared value for the model was only 0.495. The regression m del had a PRESS value of 19.7, with a residual sample standard deviation (S) of 42.8% for true negative rat . The VIF values are identical to those listed in Table 6.\nThe precision response values are included for refeenc purposes only. See Section 5.1 for additional information.\nDSD Experiment ANOVAs: Response Group-B Blue = \u03b1 < 0.05 (95% confidence), Orange = \u03b1 < 0.10 (90% confidence) Sensitivity\n(Recall or True Positive Rate - TPR)\nSpecificity (True Negative Rate - TNR)\nPrecision (Positive Predictive Rate - PPR)\nSource DF Contribution F-Value P-Value Contribution F-Value P-Value Contribution F-Value P-Value Model 27 46.58% 1.29 0.227 49.53% 1.45 0.138 53.23% 1.69 0.065 Linear 16 18.63% 0.83 0.649 14.14% 0.64 0.835 35.41% 1.80 0.067 batch 1 0.06% 0.02 0.877 0.09% 0.22 0.640 0.05% 0.01 0.942 constrain 1 2.24% 1.85 0.182 0.98% 1.02 0.318 2.01% 1.34 0.253 optimize 1 0.03% 0.16 0.690 1.22% 0.93 0.340 2.98% 3.92 0.055 dropC1 1 0.14% 0.19 0.662 1.18% 1.03 0.315 0.01% 0.00 0.947 dropC2 1 2.17% 1.44 0.237 0.50% 0.54 0.465 2.01% 1.24 0.272\n22 5/19/2020\nThe factorial plots for the four DSD Group-A responses are provided in Figure 18, Figure 19, Figure 20 and Figure 21. The main effects from the Table 7 statistical models are plotted.\nInspection of the factorial plots allows the factor trends with level settings to be evaluated, which can be used for follow-on parameter experimentation. The plots as shown were rendered by the Minitab application. See Section 8.3 for additional information about Minitab.\nThe time (to run 35 epochs) response was affected by the batch, dropC3 and filtC3 factor quadratic effects. The center batch size setting of 160-samples increased the mean training time by over 1 minute more than the 80- samples batch size, and by almost 2 minutes over the 240-sample batch size. The drops on the 10% center level of convolutional layer-3 reduced the average training t me by around 1 minute when compared to the 0% and 20% extreme settings. A convolutional layer-3 filter size center setting of \u20185\u2019 increased mean training t me by nearly 1\u00bd minutes over the extreme settings of \u20183\u2019 and \u20187\u2019. Max-pooling for all convolution layers reduced mean training time by about 1 minute for each factor. Padding when set on all convolutional layers decreased training time by an average of about 30 seconds. Setting the s ride for convolutional input layers 1 to 2 reduced mean tr ining time by approximately 1 minute. The time response metric would be useful for minimizing resources expended while training the CNN.\n23 5/19/2020\nFigure 18 \u2013 Factorial Plots: Time to Run 35 Epochs\nFigure 20 \u2013 Factorial Plots: Validation Accuracy\nAll three accuracy responses exhibited nearly identcal rends, with the center level settings for theconstraint, optimizer and drops on dense layer-1 lifting accuracies approximately 10% over the extreme level settings. Drops of 20% on the convolutional layer-3 reduced all three accuracies by about 5% when compared to no drops. The center filter setting of 5 for convolutional layer-1 reduced accura ies by about 5% when compared to the 0% setting. The drops for convolutional layer-2 exhibited a downward linear reduction in test accuracy, but also with a quadratic component that was inverted (concave upward). Max-pooling of 2 on both convolutional layers 1 and 3 lifted accuracies around 5% for layer-1 and 10% for layer-3 on the average. The ReLU activation tended to increase accuracies from 5% to 10% when compared to the other activators. The quadratic component for batch size was inverted (concave upward) with the center setting of 160-samples dropping accuracies by around 3% when compared to the extreme settings.\nThe factorial plots for the three DSD Group-B responses are provided in Figure 22, Figure 23 and Figure 24. The main effects from the Table 8 statistical models are plotted. These plots are provided for reference purposes because of the low predictability of the Table 8 statistical models. Some comments about these response Group-B factorial plots follow:\n\u2022 The sensitivity (true positive rate) and precision (positive predictive rate) exhibit the same general trends. \u2022 For sensitivity, only the batch factor was statistically significant. \u2022 For specificity, only the dropD1 factor was statistically significant.\n24016080\n13\n12\n11\n10\n753 321 0.20.10.0 0.20.10.0 0.20.10.0 0.50.30.1 20\n20\n13\n12\n11\n10\n20 753 753 753 samevalid 21 321\nbatch\nM e a n\no f\nti m\ne\nconstrain optimize dropC1 dropC2 dropC3 dropD1 maxpC1\nmaxpC2 maxpC3 filtC1 filtC2 filtC3 padding strideC1 activation\nMain Effects Plot for time Fitted Means\nAll displayed terms are in the model.\n24016080\n0.70\n0.65\n0.60\n0.55\n0.50\n753 321 0.20.10.0 0.20.10.0 0.20.10.0 0.50.30.1 20\n20\n0.70\n0.65\n0.60\n0.55\n0.50\n20 753 753 753 samevalid 21 321\nbatch\nM e a n\no f\nv a l\na c c l\na st\n5 e\np o\nc h\na v g\nconstrain optimize dropC1 dropC2 dropC3 dropD1 maxpC1\nmaxpC2 maxpC3 filtC1 filtC2 filtC3 padding strideC1 activation\nMain Effects Plot for val acc last 5 epoch avg Fitted Means\nAll displayed terms are in the model.\n24 5/19/2020\n\u2022 Trends for specificity (true negative rate) are opposite from those for sensitivity (true positive rate) and precision (positive predictive rate). \u2022 Specificity has trends that exceed 1.0, which is another indicator that the model has little or no predictive value.\nFigure 22 \u2013 Factorial Plots: Sensitivity (TPR)\nFigure 23 \u2013 Factorial Plots: Specificity (TNR)\n5.2.4. DSD Verification Check The best CNN configuration for test accuracy from the DSD experiments was configured and run through 250 epochs of training on the existing data set. The dropouts for all three convolutional layers were set to zero.\n24016080\n0.8\n0.6\n0.4\n0.2\n0.0\n753 321 0.20.10.0 0.20.10.0 0.20.10.0 0.50.30.1 20\n20\n0.8\n0.6\n0.4\n0.2\n0.0\n20 753 753 753 samevalid 21 321\nbatch\nM e a n\no f\nT ru\ne P\no s\nR a te\n( S\ne n\ns)\nconstrain optimize dropC1 dropC2 dropC3 dropD1 maxpC1\nmaxpC2 maxpC3 filtC1 filtC2 filtC3 padding strideC1 activation\nMain Effects Plot for True Pos Rate (Sens) Fitted Means\nAll displayed terms are in the model.\n24016080\n1.25\n1.00\n0.75\n0.50\n753 321 0.20.10.0 0.20.10.0 0.20.10.0 0.50.30.1 20\n20\n1.25\n1.00\n0.75\n0.50\n20 753 753 753 samevalid 21 321\nbatch\nM e a n\no f\nT ru\ne N\ne g\nR a te\n( S\np e c )\nconstrain optimize dropC1 dropC2 dropC3 dropD1 maxpC1\nmaxpC2 maxpC3 filtC1 filtC2 filtC3 padding strideC1 activation\nMain Effects Plot for True Neg Rate (Spec) Fitted Means\nAll displayed terms are in the model.\n25 5/19/2020\nThe DSD-configured CNN successfully trained for all 5 test runs, with an average test accuracy of 88.3% after 250 epochs of training, with no evidence of overfitting (orange validation plots consistently higher than the blue training plots, both accuracy plots constantly increasing, ad both loss plots constantly decreasing throughout all the training epochs). The average specificity at 81.2% was significa tly lower than average sensitivity at 95.5%, indicating the CNN favored the classification of good over bad grain structure (the average false positive rate was 18.8%).\n26 5/19/2020"}, {"heading": "5.3. Optimization Experiments", "text": "5.3.1. CCD Factors and Levels CCD optimization experiments are much more resource-hungry than DSD screening experiments; therefore, nly a limited number of factors may be studied before incurring significant computational penalties in theform of experimental treatment combinations. Two continuous factors (kernel Constraint and Drops Dense 1) and one semicontinuous factor (Optimizer) with significant response effects from the DSD screening experiments were selected for further study. The CCD factors, class (training or CNN layer), type (continuous or categorical) and levels (-\u03b1, 1, 2, 3 and +\u03b1)25 are shown in Table 11.\nThe other factors were primarily set to the DSD configuration for highest test accuracy; achieving lower computational time was considered for factors that had exhibited no real impact on accuracy, but had effects on the time. The Batch size was the only factor that met the secondary criteria, with the 240 Batch size setting offering a reduction of 1 to 2 minutes per DSD experimental treatment combination epoch.\nTogether, Kernel Constraint, DropD1(drops dense 1), and Optimizer contributed 37.4%, 38.1%, and 34.8% respectively to the variation observed in the training, validation and test accuracies during the DSD experiments. The star-points would open the experimental range for the two continuous factors up by over 60%, and because it was not possible to set a star-point level for the Optimizer factor momentum, only the Adam and Adamax settings were explored, and the Optimizer was configured as a categorical CCD factor. The Kernel Constraint and DropD1 would allow any interactions between these two factors o be studied in greater detail, and because both factors can affect the level of CNN over-fit the training data, there was considerable interest in exploring this potential interaction.\n5.3.2. CCD Experimental Matrix The CCD experiment required 26 treatment combinatios as shown in a matrix in Table 12. CCD factor selections and level settings were based on the outcome of the DSD screening experiment. CCC-configured star-points were configured to provide capability for rotation of the experimental matrix. The star-points for the Kernel Constraint and DropD1 factors appear as fractional values for these factors. Optimizer was treated as a categorical factor for the CCD. The factors that were held at a fixed level ar greyed.\nTC Batch Kernel Constraint Optimizer Drop C1 Drop C2 Drop C3 Drop D1 MaxP C1 MaxP C2 MaxP C3 Filt C1 Filt C2 Filt C3 Padd Stride C1 Active 1 240 3 Adam 0 0 0 0.1 2 2 2 3 3 3 Same 2 ReLU 2 240 7 Adam 0 0 0 0.1 2 2 2 3 3 3 Same 2 ReLU 3 240 3 Adam 0 0 0 0.5 2 2 2 3 3 3 Same 2 ReLU 4 240 7 Adam 0 0 0 0.5 2 2 2 3 3 3 Same 2 ReLU 5 240 2.17157 Adam 0 0 0 0.3 2 2 2 3 3 3 Same 2 ReLU 6 240 7.82843 Adam 0 0 0 0.3 2 2 2 3 3 3 Same 2 ReLU\n25 Note that the CCD \u00b1\u03b1 star-points have no relationship to the \u03b1 (alpha) risk used in the ANOVA\n27 5/19/2020\n5.3.3. CCD Experimental Results Each treatment combination in the matrix was iterated through 100 training epochs, where for each epoch the CNN model is subjected to one forward pass and one backward pass for all training images. At the end of each epoch, the CNN model attempted to classify the validation image samples (not seen during the training iterations). Once all 100 epochs were completed, the trained CNN model weights with the highest validation accuracy (see Footnote 14) from the 100 epochs was shown the test image samples; the test image samples were not part of either the training or validation sets used during the epoch training iterations.\nThe responses for the 100 treatments combinations are given in Table 13. The training and validation accuracies represent the average values of the last 5 epochs (95 to 100) and test accuracy represented the overall t st accuracy achieved on the test data set. All of the treatmen combinations were able to learn the training data.\nT C\nE P O C H\nFirst Experiment Replicated Experiment Time TRN\nACC VAL ACC\nTST ACC\nSpecificity Sensitivity Precision Time TRN ACC\nVAL ACC\nTST ACC Specificity Sensitivity Precision\n1 100 22.917 0.80632 0.84205 0.83375 0.6975 0.9700 0.7623 22.850 0.80908 0.81333 0.8538 0.7725 0.9350 0.8043 2 100 23.050 0.82084 0.86280 0.89250 0.8675 0.9175 0.8738 22.850 0.81460 0.82097 0.8800 0.8625 0.8975 0.8672 3 100 22.817 0.79586 0.84155 0.86375 0.8425 0.8850 0.8489 22.833 0.79398 0.80569 0.8438 0.8775 0.8100 .8686 4 100 22.767 0.80908 0.83540 0.82750 0.6850 0.9700 0.7549 22.850 0.79251 0.79264 0.8563 0.7475 0.9650 .7926 5 100 22.750 0.79837 0.81983 0.85125 0.8500 0.8525 0.8504 23.783 0.80209 0.80306 0.8525 0.7775 0.9275 0.8065 6 100 22.767 0.79883 0.84356 0.86125 0.8150 0.9075 0.8307 22.883 0.80908 0.80653 0.8625 0.7700 0.9550 .8059 7 100 22.750 0.81561 0.84877 0.82875 0.9575 0.7000 0.9428 22.717 0.82473 0.84306 0.8838 0.8200 0.9475 0.8404 8 100 22.733 0.74251 0.78310 0.77000 0.8975 0.6425 0.8624 22.667 0.78988 0.80833 0.8375 0.7225 0.9525 0.7744 9 100 22.767 0.80887 0.84314 0.85750 0.7550 0.9600 0.7967 22.667 0.82707 0.82986 0.8738 0.8150 0.9325 0.8345 10 100 22.850 0.82100 0.84285 0.83000 0.6825 0.9775 0.7548 22.683 0.79121 0.78944 0.8363 0.7700 0.9025 0.7969 11 100 22.817 0.80711 0.80889 0.85750 0.7575 0.9575 0.7979 22.633 0.79674 0.80792 0.8513 0.7700 0.9325 0.8022 12 100 22.750 0.81276 0.80139 0.87500 0.7950 0.9550 0.8233 22.717 0.80695 0.81181 0.8800 0.9375 0.8225 0.9294 13 100 22.850 0.80481 0.82181 0.83500 0.8375 0.8325 0.8367 22.683 0.79590 0.81153 0.8538 0.7400 0.9675 0.7882 14 100 22.983 0.80251 0.79097 0.82750 0.6675 0.9875 0.7481 22.750 0.76368 0.76222 0.8213 0.6825 0.9600 .7515 15 100 22.900 0.80444 0.81667 0.85500 0.7700 0.9400 0.8034 22.683 0.80933 0.82556 0.8500 0.7800 0.9200 .8070\n28 5/19/2020\nT C\nE P O C H\nFirst Experiment Replicated Experiment Time TRN\nACC VAL ACC\nTST ACC\nSpecificity Sensitivity Precision Time TRN ACC\nVAL ACC\nTST ACC Specificity Sensitivity Precision\n16 100 22.850 0.80699 0.82389 0.85875 0.8125 0.9050 0.8284 22.683 0.80699 0.82389 0.8588 0.8125 0.9050 .8284 17 100 23.033 0.80536 0.80292 0.84250 0.8925 0.7925 0.8806 22.700 0.79431 0.80069 0.8575 0.7825 0.9325 0.8109 18 100 22.933 0.80000 0.82319 0.82375 0.6825 0.9650 0.7524 22.717 0.79226 0.81208 0.8413 0.7450 0.9375 0.7862 19 100 22.800 0.81364 0.82875 0.87500 0.8250 0.9250 0.8409 22.683 0.79829 0.80625 0.8625 0.8175 0.9075 0.8326 20 100 22.867 0.80845 0.81736 0.86875 0.7950 0.9425 0.8214 22.667 0.81331 0.81556 0.8713 0.8275 0.9150 0.8414 21 100 22.967 0.78372 0.78611 0.82750 0.6950 0.9600 0.7589 22.733 0.79326 0.80306 0.8338 0.7200 0.9475 0.7719 22 100 22.767 0.79385 0.81125 0.85500 0.8100 0.9000 0.8257 22.767 0.79385 0.81125 0.8550 0.8100 0.9000 0.8257 23 100 22.850 0.79983 0.80792 0.85625 0.7800 0.9325 0.8091 22.667 0.80180 0.78306 0.8450 0.8450 0.8450 0.8450 24 100 22.830 0.80678 0.81708 0.79750 0.6100 0.9850 0.7164 22.767 0.78962 0.79972 0.8525 0.7500 0.9550 0.7925 25 100 23.550 0.80678 0.81861 0.84875 0.8175 0.8800 0.8282 22.700 0.79347 0.82042 0.8400 0.8400 0.8400 0.8400 26 100 23.000 0.80749 0.82778 0.85625 0.7625 0.9500 0.8000 22.783 0.80481 0.82903 0.8163 0.6700 0.9625 0.7447\nTable 13 \u2013 CCD Response Matrix\nThe experiment required approximately 10 hours to conduct all 26 treatments and 100 epochs for each experiment; the total experimental run-time in minutes is equal to the sum of the time column responses.\nANOVAs for the statistical models of the CCD experiment\u2019s Group-A and B output responses are given in Table 15, Table 16 and Table 17. As before, statistically-significant model factors with \u03b1 < 0.05 are shaded blue, while model factors with \u03b1 < 0.10 are shaded orange. The \u03b1-risk indicates if we can reject the null-hypothesis w th either a 95% (blue) or 90% (orange) confidence. The 90% confidence factors are highlighted for reference purposes. The VIF values indicate that no multicollinearity was present.\nThe experimental error for the responses (training accuracy, validation accuracy, test accuracy, specificity, sensitivity, and precision) ranged from 61.8% to 78.2% and the pure error component (reproducibility or results) accounted for approximately 50% of all the variation observed in the output response vector. This demonstrates the stochastic nature of CNN training and shows why a large data sample of outcomes is necessary to properly estimate a change in CNN performance between one configuration nd another. As shown in Table 14, the sample standard deviation (S) for accuracy percentages of the experimental (lack-of-fit) residuals ranges from 1.2% and 11.7% (taken from the Model Summary for each response ANOVA). With 52 total treatments, the standard error of the mean accuracy percentages ranged from 0.2% and 1.6%. The three CCD factors injected less variation into the experimental output when compared to the DSD, and the DSD had a majority of treatments that did not train, producing a much wider variation in the experimental results than those for the CCD. This is why the R-squared values for the statistical models for the CCD experim nt are lower than those for the DSD experiment. Also note that the error variation observed for the Group-B responses (specificity, sensitivity, and precision) was much higher than that of the Group-A responses (training, validation, and test accuracies), indicating that confusion matrix effects are more difficult to correlate with experimental factors than the bulk accuracy measurements.\nFor training accuracy, the Constraint main effect and the Constraint\u00d7Optimizer two-factor interaction effect were statistically significant with high F-values and corresponding P-values < 0.05, while the Constraint\u00d7DropD1 interaction effect P-value was < 0.10. Together thse three effects contributed to 32.8% of the observed variation in the experimental output.\n29 5/19/2020\nFor validation accuracy, the Constraint\u00d7Optimizer and Constraint\u00d7DropD1 two-factor interaction effects were statistically significant with high F-values and corresponding P-values < 0.05, while the Constraint and DropD1 main effect P-values were < 0.10. Together, these four effects contributed to 33.9% of the observed variation in the experimental output.\nFor test accuracy, the Constraint and Optimizer main effects were statistically significant with hig F-values and corresponding P-values < 0.05, while the Constraint\u00d7DropD1 two-factor interaction effect P-value was < 0.10. Together these three effects contributed to 26.1% of the observed variation in the experimental output. Training accuracy had the highest R-squared (adjusted) value at 26.7%, followed by test accuracy at 23.0% and validation accuracy at 23.0%.\nFor specificity (true negative rate) only the Constraint\u00d7DropD1 two-factor interaction effect was statistically significant with a high F-value and a corresponding P-value < 0.05. This one effect contributed to 14.3% of the observed variation in the experimental output. R-squared (adjusted) for specificity was only 13.6%.\nAnalysis of Variance (ANOVA)\nTest Accuracy Specificity (TNR) Source DF Adj. SS Contrib. Adj. MS F-Val P-Val VIF Adj. SS Contrib. Adj. MS F-Val P-Val VIF Model 8 0.008859 35.07% 0.001107 2.90 0.011 0.074179 27.16% 0.009272 2.00 0.069 Linear 3 0.006174 24.44% 0.002058 5.39 0.003 0.014706 5.38% 0.004902 1.06 0.376\noptimizer 1 0.002399 9.50% 0.002399 6.29 0.016 1.00 0.004370 1.60% 0.004370 0.94 0.337 1.00 constrain 1 0.003025 11.97% 0.003025 7.93 0.007 1.00 0.000158 0.06% 0.000158 0.03 0.854 1.00 dropD1 1 0.000750 2.97% 0.00075 1.97 0.168 1.00 0.010178 3.73% 0.010178 2.20 0.145 1.00 Square 2 0.000749 2.96% 0.000374 0.98 0.383 0.004160 1.52% 0.002080 0.45 0.641\nconstrain*constrain 1 0.000243 0.96% 0.000243 0.64 0.430 1.02 0.000195 0.07% 0.000195 0.04 0.838 1.02 dropD1*dropD1 1 0.000411 1.63% 0.000411 1.08 0.305 1.02 0.004128 1.51% 0.004128 0.89 0.350 1.02 2-Way Interaction 3 0.001936 7.66% 0.000645 1.69 0.183 0.055313 20.25% 0.018438 3.99 0.014\nconstrain*dropD1 1 0.001160 4.59% 0.00116 3.04 0.088 1.00 0.039006 14.28% 0.039006 8.43 0.006 1.00 constrain*optimizer 1 0.000300 1.19% 0.0003 0.79 0.380 1.00 0.009424 3.45% 0.009424 2.04 0.161 1.00 dropD1*optimizer 1 0.000476 1.88% 0.000476 1.25 0.270 1.00 0.006882 2.52% 0.006882 1.49 0.229 1.00 Error 43 0.016405 64.93% 0.000382 0.198945 72.84% 0.004627\nLack-of-Fit 9 0.003917 15.50% 0.000435 1.18 0.336 0.058060 21.26% 0.006451 1.56 0.168 Pure Error 34 0.012488 49.43% 0.000367 0.140884 51.58% 0.004144 Total 51 0.025264 0.273123\n30 5/19/2020\nFor sensitivity (true positive rate), again, only the Constraint\u00d7DropD1 two-factor interaction effect was statistically significant with a high F-value and a corresponding P-value < 0.05. This one effect contributed to 9.2% of the observed variation in the experimental output. R-squared (adjusted) for sensitivity was only 7.3%.\nFor precision (positive predictive rate), the Constraint\u00d7DropD1 two-factor interaction effect was statistically significant with a high F-value and a corresponding P-value < 0.05, while the DropD1 main effect P-value was < 0.10. These two effects contributed to 16.8% of the observed variation in the experimental output. R-squared (adjusted) for sensitivity was only 16.1%.\nThe factorial plots for the six CCD experimental responses provide direction for where to set the factor levels to achieve the best CNN performance.\nFigure 25 \u2013 CCD Factorial Plots: Training Accuracy\nFigure 26 \u2013 CCD Factorial Plots: Validation Accuracy\nAll three of the Group-A accuracy CCD responses exhibited a similar pattern for the three main-effect plots as shown in Figure 25, Figure 26 and Figure 27. The highest accuracies were yielded by the highest K rnel Constraint\n7.04.52.0\n0.810\n0.805\n0.800\n0.795\n0.790\n0.500.250.00 AdamaxAdam\nconstrain\nM e an\no f\ntr n\na c c L\n5\ndropD1 optimizer\nMain Effects Plot for trn acc L5 Fitted Means\n7.04.52.0\n0.825\n0.820\n0.815\n0.810\n0.805\n0.500.250.00 AdamaxAdam\nconstrain\nM e an\no f\nva l\nac c\nL5\ndropD1 optimizer\nMain Effects Plot for val acc L5 Fitted Means\n31 5/19/2020\nlevel and the lowest drop values for the CNN\u2019s dense layer-1 training settings. The Adam Optimizer yielded higher accuracies than the Adamax Optimizer.\nFigure 28 \u2013 CCD Interaction Plots: Training Accuracy\nFigure 29 \u2013 CCD Interaction Plots: Validation Accuracy\nAll three of the Group-A accuracy CCD responses exhibited similar patterns for the three two-factor interaction effect plots as shown in Figure 28, Figure 29 and Figure 30. The surprising interaction was the Constraint\u00d7DropD1 that again showed that higher accuracies were yielded by the largest kernel constraint level and the lowest drop values for the CNN\u2019s dense layer-1. The DropD1\u00d7Optimizer interaction showed that the Adam Optimizer yielded higher accuracies when the CNN\u2019s dense layer-1 D opD1 values were lowest, while accuracies were not as affected by the dense layer-1 DropD1 values when the Adamax Optimizer was used. Interactions were not studied during the DSD experiment, which may partially explain why t e relationship between the Adam and Adamax Optimizers was different for the screening phase.\nFigure 31 \u2013 CCD Factorial Plots: Specificity\nFigure 32 \u2013 CCD Factorial Plots: Sensitivity\nThe three Group-B confusion matrix main effect responses are shown in Figure 31, Figure 32 and Figure 33, and for precision, only the linear component of the CNN\u2019s dense layer-1 drop value (Figure 33 center panel) was significant at 90% confidence with a P-value < 0.10.\nThe three Group-B confusion matrix CCD two-factor interaction responses are shown in Figure 34, Figure 35 and Figure 36, and only the Constraint\u00d7Optimizer interactions were significant with P-values < 0.05 for all three responses. The sensitivity interaction (Figure 35 upper right panel) appears to run counter to the same interaction for the specificity and precision responses, where the relationship of the 0.1 and 0.5 DropD1 curves are reversed, suggesting that if the CNN\u2019s dense layer-1 DropD1 value is lowered to 0.1 or less with the kernel constraint set high, the sensitivity would be lowered. This may not be detrimental because the CNN appears to be bias d towards the correct classification of \u2018good grains\u2019, with the average sensitivity approaching 1.0, which tends to elevate the sensitivity (true positive rate) and lower both thespecificity (true negative rate) and precision (positive predictive rate). Elevating the specificity at the expense of sensitivity would encourage the reduction of true positive rate bias in the CNN.\n0.82 0.81 0.80 0.79 0.78\n8642\n0.82 0.81 0.80 0.79 0.78\n0.600.450.300.150.00\nconstrain * dropD1\nconstrain * optimizer\nconstrain\ndropD1 * optimizer\ndropD1\n0.1 0.3 0.5\ndropD1\nAdam Adamax\noptimizer\nM e a n\no f\ntr n\na cc\nL 5\nInteraction Plot for trn acc L5 Fitted Means\n0.84\n0.82\n0.80\n8642\n0.84\n0.82\n0.80\n0.600.450.300.150.00\nconstrain * dropD1\nconstrain * optimizer\nconstrain\ndropD1 * optimizer\ndropD1\n0.1 0.3 0.5\ndropD1\nAdam Adamax\noptimizer\nM e a n\no f\nv a l a cc\nL 5\nInteraction Plot for val acc L5 Fitted Means\n7.04.52.0\n0.81\n0.80\n0.79\n0.78\n0.77\n0.76\n0.500.250.00 AdamaxAdam\nconstrain\nM e a n\no f\nS p\ne c if\nic it\ny\ndropD1 optimizer\nMain Effects Plot for Specificity Fitted Means\n7.04.52.0\n0.94\n0.92\n0.90\n0.88\n0.86\n0.84\n0.82\n0.80\n0.500.250.00 AdamaxAdam\nconstrain\nM e a n\no f\nS e n\nsi ti\nv it\ny\ndropD1 optimizer\nMain Effects Plot for Sensitivity Fitted Means\n32 5/19/2020\nFigure 34 \u2013 CCD Interaction Plots: Specificity\nFigure 35 \u2013 CCD Interaction Plots: Sensitivity\n5.3.4. CCD Verification Check The CNN CCD-configuration for the highest training accuracy, validation accuracy, test accuracy and specificity was run through 250 epochs of training on the existing data set. The Constraint\u00d7DropD1 two-factor interaction was used to set the dense layer-1 Kernel Constraint to 7.5 and the dense layer-1 drop level to 0, which departed from settings recommended by best practice (DropD1 between 20% and 50% and Kernel Constraint = 3 to 4) for CNN training.\nThe CCD-configured CNN successfully trained for all 5 test runs with 250 epochs of training for each run, with no evidence of overfitting. The average test accuracy improved to 90.5% from the 88.3% test accuracy achieved for the DSD-configured CNN. The average specificity achieved was 86.4%, which was lower than the 94.7% average sensitivity, but this was an improvement over the DSD-configured CNN that had an average specificity of 81.2% and an average sensitivity of 95.5%; the CCD-configured CNN, while improved over the DSD-configuration, still favors classification of good over bad grain structure (the average false positive rate also improved to 13.6% from 18.8%). With the exception of Run 2, the CCD-configured CNN test results were consistent. The CCD verification runs with confusion matrix metrics are shown in Table 19.\nRun Training Plots Confusion Matrix Metrics\n1\nPredicted Class Labels\nClass Bad Grains Good Grains True Rates Actual Class Labels\nBad Grains TN = 359 FP = 41 Specificity = 89.75% Good Grains FN = 28 TP = 372 Sensitivity = 93.00%\nFalse Positive Rate = 10.25% Precision = 90.07%\nAccuracy = 91.38%\nTrain. Acc. (L5): 89.77% Val. Acc. (L5): 91.57% Time: 57.20 min.\n0.90 0.85 0.80 0.75 0.70\n8642\n0.90 0.85 0.80 0.75 0.70 0.600.450.300.150.00\nconstrain * dropD1\nconstrain * optimizer\nconstrain\ndropD1 * optimizer\ndropD1\n0.1 0.3 0.5\ndropD1\nAdam Adamax\noptimizer\nM e a n\no f\nS p\ne ci\nfi ci\nty\nInteraction Plot for Specificity Fitted Means\n1.0\n0.9\n0.8\n0.7\n8642\n1.0\n0.9\n0.8\n0.7 0.600.450.300.150.00\nconstrain * dropD1\nconstrain * optimizer\nconstrain\ndropD1 * optimizer\ndropD1\n0.1 0.3 0.5\ndropD1\nAdam Adamax\noptimizer\nM e a n\no f\nS e n\nsi ti\nv it\ny\nInteraction Plot for Sensitivity Fitted Means\n33 5/19/2020"}, {"heading": "5.4. Data Set Verification", "text": "5.4.1. K-Fold Cross-Validation The 7420 sample dataset was randomly partitioned 10 times into separate datasets of 5020 training samples, 1600 training validation samples, and 800 test verification samples, creating a 10-fold cross-validation dataset. The optimized CNN-configuration from the CCD experimental output was then trained using randomly initialized weights to start each Run, with 5 training, validation and test Runs performed on each of the k-Fold datasets.\nThe 10-fold cross-validation results appear in the GLM (General Linear Model) ANOVA of Table 20, Figure 37, Figure 38 and Figure 39, as well as Table 21. Although much more variation came from k-Fold, only the orangehighlighted k-Fold factor was statistically significant with a P value < 0.10. Figure 37 provides a graphical representation of the ANOVA results in Table 20, showing the variation between k-Fold 1 and 2 was greater than the variation between test Runs 1 through 5 within each k-Fold level.\n34 5/19/2020\nThe ability of the CCD-configured CNN to act as a generalized Cu-alloy grains classifier is demonstrated in Figure 39, where the average accuracy was highest for the test datasets. The variation also increases at each st ge, with the highest variance observed in test accuracy results; everal lower-accuracy outliers are visible in the test results.\nFigure 37 \u2013 10-Fold Cross-Validation Mean Test Accuracies Partitioned between Fold and Run\nFigure 38 \u2013 10-Fold Cross-Validation Boxplot of Test Accuracies for each\nDataset Fold\nFigure 39 \u2013 10-Fold Cross-Validation Boxplots of Accuracies for Training, Validation and Test Data\nThe CCD-optimized CNN achieved 91.3% \u00b1 2.0% test accuracy on the 10-fold cross-validation datasets as summarized in Table 21, where S is the sample standard deviation. The variation between the 10 cross-validation k-Fold datasets was much greater than the variation within the 5 training Runs performed on each cross-validation dataset. This can also be seen by comparing the two best k-Fold datasets 4 and 8 with the worst dataset 9; test accuracies \u00b1 S were 93.1 \u00b10.7%, 92.3 \u00b10.8% and 89.4 \u00b13.6% respectively for datasets 4, 8 and 9.\nThe 10-fold cross-validation represents an estimate of what would possible to achieve with this Cu-alloy dataset using the CCD-optimized CNN, where a training, valid t on and test dataset could be created that could either be more like datasets 4 and 8 that achieve classification accuracies around 93 \u00b11%, or more like dataset 9, with classification accuracies of only 89 \u00b14%.\nThe k-fold cross-validation, with 10 folds and 5 runs-per-fold, required 47 hours and 25 minutes to complete.\n10987654321\n0.93\n0.92\n0.91\n0.90\n0.89\n54321\nk-Fold\nM e an\no f\nte st\na c c\nRun\nMain Effects Plot for test acc Fitted Means\n10987654321\n94.00%\n92.00%\n90.00%\n88.00%\n86.00%\n84.00%\n82.00%\nk-Fold\nte st\na c c\nBoxplot of test acc\ntest accval acc L5trn acc L5\n94.00%\n92.00%\n90.00%\n88.00%\n86.00%\n84.00%\n82.00%\nD a ta\nBoxplot of trn acc L5, val acc L5, test acc\n35 5/19/2020"}, {"heading": "5.5. Regularization Experiments", "text": "A preliminary DSD screening experiment was performed using both the L1 and L2 regularization terms and these were initially applied to all 4 layers (convolutional layers 1, 2 and 3, and dense layer-1) of the CCD-optimized CNN. The L1 and L2 terms were varied logarithmically between 1.E-3, 1.E-5 and 1.E-7 using a DSD experimental matrix with 17 treatment combinations, and the experim nt was replicated once requiring 34 total runs; however, the DSD was unable to learn the training set with this regularization applied. Instead of an exhaustive search of the experimental input space27, the decision was made to simply eliminate the L1 regularization terms from all layers, and this allowed the CNN to learn the dataset. The L2 weight regularization experimental matrix appears in Table 22.\n5.5.1. Regularization Matrix The L2 regularization values were logarithmically varied as shown below, while the other factors remained at the CCD-optimization settings shown in Table 18.\nThe experiment was replicated 3 additional times with a total of 52 runs performed. Each of the runs wa subjected to 100 epochs of training, validation and test verification, and the entire experiment required 19 hours and 48 minutes to complete. The sample standard deviations f r test accuracy and specificity (TNR \u2013 true negative rate) for each treatment combination across the 4 replicated experiments were used as the primary experimental responses, while the test accuracy and specificity percentages served as secondary responses. The experimental objective was to prove the hypothesis that the L2 weight regularization terms had an effect on the variation observed between identical test cases. Test accuracy provided a measure of the CNN\u2019s ability to generalize what it has learned during training. Specificity provided an indication for misclassification of the bad grain structures, because the CNN was biased towards the correct classification of god grain structure over bad grain structure.\n5.5.2. Regularization Experimental Results The ANOVA of the replication standard deviation forthe test accuracy and specificity responses are shown in Table 23. The test accuracy standard deviation response had no significant effects, and for the specificity standard deviation response, only the L2 regularization for dense layer-1 (D1_L2) factor had a statistically significant effect at 90% confidence with a P-value < 0.10 (highlighted orange). The main effect responses are plotted in Figure 40 and Figure 41, using the same Y-scale range of 3%, with the L2 exponent values on the X-axes. The trends for the\n27 A preliminary exploration of the experimental input space, prior to designing a new DoE could be achieved using a low-discrepancy quasi-random sampling Sobol Sequence, or Latin Hypercube sampling, which would provide more uniformity than the Sobol Sequence. This approach was rejected because of the required resources when compared to the performance already achieved by the CCD-optimized CNN as documented in Section 5.4.1.\n36 5/19/2020\nfour L2 regularization factors were identical between the wo response factors, where higher value of L2 regularization for convolutional layer-1 (C1_L2) reduced replication variation, while lower values of L2 regularization for the other 3 CNN layers reduced rplication variation. Overall, the effect on run-to-run variation from L2 regularization was weak.\nThe effects of L2 regularization on test accuracy and specificity were not significant. This is shown in Table 24 summary of ANOVA, where no main effect had a P-value below 0.26 for test accuracy or below 0.15 for specificity. Pure error from replication contributed 88.5% and 71.5% to the observed variation for test accuracy and specificity respectively. The main-effect responses are plotted in Figure 40, using the same Y-scale range of 4%, with the L2 exponent values on the X-axes. The trends for the four L2 regularization factors were the same between the 2 response factors, where lower values of L2 regularization for the second and third convolutional layers tended to increase test accuracy and specificity, while L2 regularization for the first convolutional layer had no effect. Overall, the effects from the L2 regularization on test accuracy and specificity were not significant.\nAnalysis of Variance (ANOVA)\nTest Accuracy Specificity (TNR) Source DF Adj. SS Contrib. Adj. MS F-Val P-Val VIF Adj. SS Contrib. Adj. MS F-Val P-Val VIF Model 4 0.000981 4.83% 0.000245 0.60 0.667 0.032775 9.95% 0.008194 1.30 0.284 Linear 4 0.000981 4.83% 0.000245 0.60 0.667 0.032775 9.95% 0.008194 1.30 0.284\nC1_L2 1 0.000000 0.00% 0.000000 0.00 1.000 1.00 0.000000 0.00% 0.000000 0.00 1.000 1.00 C2_L2 1 0.000545 2.68% 0.000545 1.32 0.256 1.00 0.013783 4.18% 0.013783 2.18 0.146 1.00 C3_L2 1 0.000257 1.27% 0.000257 0.62 0.434 1.00 0.011223 3.41% 0.011223 1.78 0.189 1.00 D1_L2 1 0.000179 0.88% 0.000179 0.44 0.512 1.00 0.007770 2.36% 0.007770 1.23 0.273 1.00 Error 47 0.019321 95.17% 0.000411 0.296641 90.05% 0.006312\nLack-of-Fit 8 0.001362 6.71% 0.000170 0.37 0.930 0.061126 18.56% 0.007641 1.27 0.289 Pure Error 39 0.017959 88.46% 0.000460 0.235516 71.49% 0.006039 Total 51 0.020302 100.00% 0.329417 100.00%\n37 5/19/2020\n5.5.3. Regularization Verification Check The CNN CCD-configuration defined in Table 18 was used with the L2 regularization settings as shown in Table 25.\nFive successful test runs were conducted on the dataset, with 250 epochs of training for each run and no indication of overfitting.\nAny evidence of improvement could not be determined from this data because the test results fell well ithin the results for the 10-fold cross-validation of Section 5.4.1; therefore, the 10-fold cross-validation was repeated with the L2 weight regularization settings.\nThe 10-fold cross-validation results appear in the GLM ANOVA of Table 27, Figure 44, Figure 45 and Figure 46, as well as Table 29. More variation came from the Run factor, and only the orange-highlighted Run factor was statistically significant with a P value < 0.10 at 90% confidence. Figure 44 provides a graphical representation of the ANOVA results in Table 27, showing the variation between k-Fold and the variation between tests Run within the k-Fold.\nWith the exception of Fold 6, the 10-fold cross-valid tion run with optimized L2 weight regularization appears to be relatively consistent from fold to fold. The change in patterns between Figure 38 and Figure 45 indicates that the variation between k-Fold 1 and 2 is primarily due the stochastic nature of the CNN during training and not because of a difference between k-Fold 1 and 2 (otherwise the patterns would have been similar).\n38 5/19/2020\nDataset Fold\nThe pre and post L2 weight regularization 10-fold cross-validation result for test accuracy means are compared in the Table 29 ANOVA and Figure 47. The factor Cross-Validation reflects running the 10-fold cross-validation before (1) and after (2) the L2 weight regularization. The Table 27 ANOVA indicates that the pre and post L2 regularization cross-validations are statistically identical when compared to the variation between folds (k-Fold) and within folds (Run); therefore, the null hypothesis was accepted. This is shown graphically in the scatterplot of Figure 48.\n39 5/19/2020\nFigure 47 \u2013Mean Test Accuracies Partitioned between Cross-Validation, k-Fold and Run"}, {"heading": "5.6. Expanded Dataset K-Fold Cross-Validation", "text": "The initial dataset was expanded as described in Section 3.7.2, and the 10-fold cross-validation of Section 5.4.1 was repeated on the expanded dataset discussed in Section 3.7.2 using the CCD-optimized CNN. The L2 weight regularization settings described in Section 5.5 were also applied to the CNN.\nThe CNN was successfully retrained on the expanded dataset, using the best weight (see Footnote 14) from the fourth fold of the L2 10-fold cross-validation. We reduced training epochs to 200 from 250 based on a trial run that indicated the possibility of overfitting29 beyond 200 epochs.\nThe CNN successfully completed all 50 runs and achieved a test accuracy of 91.1 \u00b10.7% for the expanded 10-fold dataset, and required 71 hours and 10 minutes to complete. The CNN was now slightly more biased towards the classification of bad grains, with specificity exceeding sensitivity at 92.6 \u00b12.2% and 89.5 \u00b12.3%, respectively. Note that initially, we attempted to use fine tuning [27] on the pre-trained CNN to conduct the cross-validation, where only the weights for last convolutional feature extrac ion layer and the 2 dense classification layers were allowed to update during training, but the test accuracy achieved was lower and variation was greater at 86.7 \u00b11 3%. For additional details, refer to Appendix B: k-Fold Fine Tuning.\nThe 10-fold cross-validation results appear in the GLM ANOVA of Table 30, Figure 49, Figure 50 and Figure 51, as well as Table 31. More variation came from the k-Fold factor, and only the orange-highlighted k-Fold factor was statistically significant with a P value < 0.05 at 95% confidence. Figure 49 provides a graphical representation of the ANOVA results in Table 30, showing the variation between k-Fold and the variation between test runs within k-Fold. To illustrate the reduction in variation achieved with the expanded dataset cross-validation, the vertical axes scales for the previous cross-validations of Sections 5.4.1 and 5.5.3 were retained for Figure 49, Figure 50 and\nFigure 51. The variation within the expanded dataset 10-fold cross-validation was reduced by a factor of 2 to 3 from that of the previous 10-fold cross-validations primarily because the CNN had been pre-trained on the initial dataset thereby reducing the stochastic component from training.\n29 Validation accuracy dropping below training accuracy\n21\n0.925\n0.920\n0.915\n0.910\n0.905\n0.900\n10987654321 54321\nCross-Validation\nM e a n\no f\nte st\na c c\nk-Fold Run\nMain Effects Plot for test acc Fitted Means\n40 5/19/2020\nAccuracies for each Dataset Fold"}, {"heading": "6. Results Summary", "text": "A simple binary classification CNN was trained to recognize good and bad grain structure in Cu-alloy using a series of statistically designed experiments to screen andthen optimize both the CNN configuration and training hyperparameters simultaneously during the training a d test phase. The results from each experiment was verified with a 5-run check using the experimentally-determined best factor settings. The results of the optimization were then verified using a series of 10-fold cross-validation tests. The summary results appear in tables and figures below.\n41 5/19/2020\nAlthough the reduction in stochastic variation could only be statistically proven for the specificity response of the L2 regularization experiment in Section 5.5, Figure 53 clearly shows that the weight regularization (yellow) standard errors (SE) for the mean accuracies are reduced from th se of the DSD (light blue) and CCD (orange) experiments; however, the reduction in variation is not as evident for the pre and post regularization cross-validations (grey and dark blue). This last point is shown graphically in Figure 54.\n42 5/19/2020\nThe test accuracy histograms from both the pre-regularization (C) and post-regularization (E) 10-fold crossvalidations are superimposed in Figure 54, and show the same general shape in both the distribution and outliers.\nThe horizontal axis of the histogram for the Figure 55 expanded dataset 10-fold cross-validation (F) is identically scaled to that of the initial dataset 10-fold cross-validation (C & E) in Figure 54."}, {"heading": "7. Conclusions", "text": "The effort to train a deep learning algorithm to recognize good and bad grain structures in Cu-alloy was successful.\n\u2022 The CNN achieved a final accuracy of 91.1 \u00b10.7% test accuracy on the expanded dataset, which consisted of 12300 total sub-images images, with 8700 images for training, 2400 images for training validation ad 1200 images for test verification. \u2022 The efforts to balance the classification were somewhat successful, with the CNN slightly biased towards the recognition of bad grains over good grains, with a true negative rate specificity of 92.7 \u00b12.2% and true positive rate sensitivity of 89.5 \u00b12.3%. \u2022 Ensemble classification of the sub-images was 100%, because the presence of excessive bad grains within the test coupon image was color-coded by red tinting to indicate a bad sub-image, with more tinting applied\n43 5/19/2020\nwith increasing probability of bad grain structure; this allows a technician to quickly recognize the ov rall state of the test coupon. \u2022 The sub-image ensemble classification also provided a degree of explainability (i.e. XAI), since individual sub-images can be examined in detail to understand why the CNN produced the classification. \u2022 The use of Design of Experiments (DoE) for simultaneously optimizing both the CNN configuration and training hyperparameters during CNN training and testing was successfully demonstrated, and this allowed the use of statistically-based metrics to determine if improvements in CNN performance actually occurred. This was especially critical because of the stochastic nature exhibited by the CNN when training. This is very important for industrial applications. \u2022 The DoE method revealed that this particular CNN, with the Cu-alloy dataset, should be trained with settings that did not conform to best practices (e.g. large kernel constraints and no random dropouts). \u2022 Although the experimentally determined L2 CNN configuration was retained for 10-fold cross-validation on the expanded dataset, weight regularization was not entirely successful because no statistically significant lift in performance or reduction in run-to-run variation could be detected with the L2 regularization settings applied. \u2022 The configuration of the CNN and the structure of the Cu-alloy dataset may have contributed to the conclusions outlined in the previous two bullets (non-conformance to best practices for kernel constraints and random dropouts, as well as ineffective weight regularization). The simple CNN with a limited number of trainable parameters (~ 291K) naturally resists overfitting. This conclusion needs to be further explored. \u2022 Because the DoE allows interactions between factors to be detected, as well as the quantification of experimental error (both lack-of-fit and pure error from reproducibility) in comparison to the factor main and interaction effects, the statistical DoE approach is much more efficient form of experimentation than either one-factor-at-a-time (1FAT) or 2 x 2 input grid-search approaches. \u2022 Although not documented previously in this report, we found that in order to gain the highest resolutin for the effects of experimental factors, the optimal number of training epochs when conducting DoEs was approximately 40% to 50% of the maximum number of epochs before the CNN began memorizing the training dataset images (i.e. overfitting the data); hence, this is why we used 100 epochs for the CCD experiments when 250 epochs were needed for both final verification checks and the 10-fold crossvalidations. This conclusion should be further investigated. \u2022 Automation of the CNN training and test validation using DoE experimental matrices facilitated a rapid development and optimization of the CNN hyperparameter configurations."}, {"heading": "8. Additional Information", "text": ""}, {"heading": "8.1. Funding, Export Compliance and IP", "text": "This project was implemented under Moog IR&D and was vetted by Moog Trade Compliance to ensure that no ITAR and/or EAR licensable data is contained in this document, and by Moog Intellectual Property to remove any Moog proprietary or confidential information or data.\n44 5/19/2020"}, {"heading": "8.2. Glossary of Terms", "text": "Term Description\nAdadelta SGD adaptive moment estimation that mitigates aggressive reductions in learning rate\nAdam SGD with adaptive moment estimation\nAdamax Infinity-constrained SGD adaptive moment estimation\nAI Artificial Intelligence \u2013 a catch-all acronym for model-building learning algorithms that are currently very narrow in scope\nAM Additive Manufacturing\nAPI Application Programming Interface\nCCC Central-Composite Circumscribed \u2013 a type of CCD where te star-points are expanded outward\nCCD Central-Composite Design \u2013 a type of RSM DoE\nCCF Central-Composite Face \u2013 a type of CCD where the star-points are on the factor interaction planes\nCCI Central-Composite Inscribed \u2013 a type of CCD where the star-points are retracted inward\nCNN Convolutional Neural Network \u2013 a type of deep-learning AI that excels at recognizing patterns that exist within the spatial domain\nCu Copper (the metal)\nDeep-Learning An AI with many hidden layers where feature maps are created automatically during training\nDoE Statistically-based Design of Experiments\nDSD Definitive Screening Design \u2013 a type of DoE\nEAR Export Administration Regulations\nEVOP Evolutionary Operations (an experimental method that follows the path of steepest gradient descent along a hypersurface of dimension n-1, where n = the number of experimental factor variables )\nFeature-Maps The deep-learning-equivalent to a factor or variable in classic model building\nFCC Face-Centered Cubic \u2013 a form of CCD with star-points located on the two-factor interaction planes\nFPR False Positive Rate\nGLM General Linear Model: a statistical linear model for a continuous response variable using multivariate regression given continuous and/or categorical predictors\nGPU Graphics Processing Unit\niid independent and identically distributed (a collection of random variables)\nIR&D Internal Research and Development\nIP Intellectual Property\nITAR International Traffic in Arms Regulations\nJPEG Joint Photographic Experts Group \u2013 a standardized compressed graphic format (the image is reduced in file-size but undergoes information-loss when converted f om TIFF to JPEG format)\nL1 An output weight regularization technique that uses lasso regression\nL2 An output weight regularization technique that uses ridge re ression\nMachine-learning A type of AI where feature-maps are created for the training set through initial human intervention\nMNIST (Modified National Institute of Standards and Technology database) is a database of handwritten digits used for training and performance-benchmarking various image processing systems and is available in the public domain.\nNadam Nesterov-accelerated SGD adaptive moment estimation\n45 5/19/2020"}, {"heading": "8.3. Technical Information", "text": "\u2022 Robotic microscope: Nikon AZ100 (NIS-Elements software) with Prior Scientific motorized Shuttle Stage \u2022 Sample preparation: Mager Scientific Saphir X-Change Fully Automatic Grinder/Polisher \u2022 Hardware: Nvidia Titan Xp GPU \u2022 Software: Keras 2.0.6, Tensorflow 1.4.0, Scikit-learn 0.19.1, Numpy 1.13.3, Pandas 0.20.3, Matplotlib\n2.0.2, Scikit-image 0.13.0, Python 3.6, Minitab 18.1, Image Slicer 0.1.0.\n46 5/19/2020"}, {"heading": "8.4. Contact information", "text": "Dr. Paul Guerrier CEng FIMechE | Engineering Manager Moog Space and Defense Group 500 Jamison Rd. \u2013 Plant 20 East Aurora, New York 14052-0018 United States of America 716-652-2000\n47 5/19/2020"}, {"heading": "Appendix A: CNN Structure", "text": "The structure table of the CNN is provided below as output by the Keras API. The text following the CNN structure table provides the k-fold and run (treatment), epoch status and hyperparameter settings. In this case,the CNN was beginning the first epoch in the first fold of the expanded dataset 10-fold cross-validation of Section 5.6.\nLayer (type) Output Shape Param #\n=================================================================\nconv2d_1 (Conv2D) (None, 32, 70, 70) 896\n_________________________________________________________________\nmax_pooling2d_1 (MaxPooling2 (None, 32, 35, 35) 0\n_________________________________________________________________\ndropout_1 (Dropout) (None, 32, 35, 35) 0\n_________________________________________________________________\nconv2d_2 (Conv2D) (None, 32, 35, 35) 9248\n_________________________________________________________________\nmax_pooling2d_2 (MaxPooling2 (None, 32, 17, 17) 0\n_________________________________________________________________\ndropout_2 (Dropout) (None, 32, 17, 17) 0\n_________________________________________________________________\nconv2d_3 (Conv2D) (None, 64, 17, 17) 18496\n_________________________________________________________________\nmax_pooling2d_3 (MaxPooling2 (None, 64, 8, 8) 0\n_________________________________________________________________\ndropout_3 (Dropout) (None, 64, 8, 8) 0\n_________________________________________________________________\nflatten_1 (Flatten) (None, 4096) 0\n_________________________________________________________________\ndense_1 (Dense) (None, 64) 262208\n_________________________________________________________________\ndropout_4 (Dropout) (None, 64) 0\n_________________________________________________________________\ndense_2 (Dense) (None, 1) 65\n_________________________________________________________________\nactivation_1 (Activation) (None, 1) 0\n=================================================================\nTotal params: 290,913\nTrainable params: 290,913\nNon-trainable params: 0\n__________________________________________________________________________________\nNone\nK-fold: 1 Treatment: 1 , Batch: 240 , Kernel Constraint: 7.5 , Optimizer: Adam , DropCONV1: 0 , DropCONV2: 0 , DropCONV3: 0 , DropDNSE1: 0 , MaxPCONV1: 2 , MaxPCONV2: 2 , MaxPCONV3: 2 , FiltCONV1: 3 , FiltCONV2: 3 , FiltCONV3: 3 , Padding: same , StrideCONV1: 2 , Activator: Relu , C1_L1: 0 , C1_L2: 0.001 , C2_L1: 0, C2_L2: 0.0000001 , C3_L1: 0 , C3_L2: 0.0000001 , D1_L1: 0 , D1_L2: 0.0000001\nFound 8700 images belonging to 2 classes.\nFound 2400 images belonging to 2 classes.\nEpoch 1/250\n51 5/19/2020"}, {"heading": "Appendix B: k-Fold Fine Tuning", "text": "The 10-fold cross-validation on the expanded dataset was initially performed using fine tuning, where only the last convolutional feature extraction layer and the dense classification layers were allowed to train.\nThe initial dataset was expanded as described in Section 3.7.2, and the 10-fold cross-validation of Section 5.4.1 was repeated on the expanded dataset discussed in Section 3.7.2 using the CCD-optimized CNN. The L2 weight regularization settings described in Section 5.5 were also applied to the CNN.\nThe CNN successfully completed all 50 runs and achieved a test accuracy of 86.7 \u00b11.3% for the expanded 10-fold dataset, and required 70 hours and 59 minutes to complete. The CNN remained biased towards the classification of good grains over bad grains, with sensitivity exceeding specificity at 88.8 \u00b13.0% and 84.6 \u00b14.0% respectiv ly.\nThe 10-fold cross-validation results appear in the GLM ANOVA of Table 36, Figure 56, Figure 57 and Figure 58, as well as Table 37. More variation came from the k-folds, and only the orange-highlighted k-Fold factor was statistically significant with a P value < 0.05 at 95% confidence. Figure 56 provides a graphical representation of the ANOVA results in Table 36, showing the variation between k-folds and the variation between test runs within the k-folds.\nThe horizontal axis of the histogram for the Figure 59 expanded dataset (F) 10-fold cross-validation usi g fine tuning is identically scaled to that of the initial dataset 10-fold cross-validation shown in Figure 54 and the expanded dataset 10-fold cross-validation shown in Figure 55.\nFigure 56 \u2013 Expanded Dataset 10-Fold Cross-Validation (with Fine Tuning)\nMean Test Accuracies Partitioned between Fold and Run\nFigure 57 \u2013 Expanded Dataset 10-Fold Cross-Validation Boxplot (with Fine\nTuning) of Test Accuracies for each\nDataset Fold\nFigure 58 \u2013 Expanded Dataset 10-Fold Cross-Validation Boxplots (with Fine\ntuning) of Accuracies for Training,\nValidation and Test Data\n10987654321\n0.885\n0.880\n0.875\n0.870\n0.865\n0.860\n0.855\n0.850\n54321\nk-fold\nM e a n\no f\nte st\na c c\nRun\nMain Effects Plot for test acc Fitted Means\n10987654321\n92.00% 91.00% 90.00% 89.00% 88.00% 87.00% 86.00% 85.00% 84.00% 83.00%\nk-fold\nte st\na c c\nBoxplot of test acc\ntest accval acc L5trn acc L5\n92.00% 91.00% 90.00% 89.00% 88.00% 87.00% 86.00% 85.00% 84.00% 83.00% D a ta\nBoxplot of trn acc L5, val acc L5, test acc\n52 5/19/2020"}], "title": "Automated Copper Alloy Grain Size Evaluation Using a Deep-learning CNN", "year": 2020}