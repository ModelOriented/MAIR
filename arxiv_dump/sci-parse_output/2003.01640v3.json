{"abstractText": "A common workflow in data exploration is to learn a low-dimensional representation of the data, identify groups of points in that representation, and examine the differences between the groups to determine what they represent. We treat this workflow as an interpretable machine learning problem by leveraging the model that learned the low-dimensional representation to help identify the key differences between the groups. To solve this problem, we introduce a new type of explanation, a Global Counterfactual Explanation (GCE), and our algorithm, Transitive Global Translations (TGT), for computing GCEs. TGT identifies the differences between each pair of groups using compressed sensing but constrains those pairwise differences to be consistent among all of the groups. Empirically, we demonstrate that TGT is able to identify explanations that accurately explain the model while being relatively sparse, and that these explanations match real patterns in the data.", "authors": [{"affiliations": [], "name": "Gregory Plumb"}, {"affiliations": [], "name": "Jonathan Terhorst"}, {"affiliations": [], "name": "Sriram Sankararaman"}, {"affiliations": [], "name": "Ameet Talwalkar"}], "id": "SP:2b7d7725448e65a4041d60bea7c42e10064f0dd1", "references": [{"authors": ["N. Bello", "L. Mosca"], "title": "Epidemiology of coronary heart disease in women", "venue": "Progress in cardiovascular diseases,", "year": 2004}, {"authors": ["T. Blumensath"], "title": "Compressed sensing with nonlinear observations and related nonlinear optimization problems", "venue": "IEEE Transactions on Information Theory,", "year": 2013}, {"authors": ["Cand\u00e8s", "E. J"], "title": "Compressive sampling", "venue": "In Proceedings of the international congress of mathematicians,", "year": 2006}, {"authors": ["A. Dhurandhar", "Chen", "P.-Y", "R. Luss", "Tu", "C.-C", "P. Ting", "K. Shanmugam", "P. Das"], "title": "Explanations based on the missing: Towards contrastive explanations with pertinent negatives", "venue": "In Advances in Neural Information Processing Systems,", "year": 2018}, {"authors": ["A. Dhurandhar", "T. Pedapati", "A. Balakrishnan", "Chen", "P.-Y", "K. Shanmugam", "R. Puri"], "title": "Model agnostic contrastive explanations for structured data, 2019", "year": 2019}, {"authors": ["J. Ding", "A. Condon", "S.P. Shah"], "title": "Interpretable dimensionality reduction of single cell transcriptome data with deep generative models", "venue": "Nature communications,", "year": 2002}, {"authors": ["Y. Goyal", "Z. Wu", "J. Ernst", "D. Batra", "D. Parikh", "S. Lee"], "title": "Counterfactual visual explanations", "venue": "In International Conference on Machine Learning,", "year": 2019}, {"authors": ["D. Jiang", "C. Tang", "A. Zhang"], "title": "Cluster analysis for gene expression data: a survey", "venue": "IEEE Transactions on knowledge and data engineering,", "year": 2004}, {"authors": ["J. Kauffmann", "M. Esders", "G. Montavon", "W. Samek", "M\u00fcller", "K.-R"], "title": "From clustering to cluster explanations via neural networks", "year": 1906}, {"authors": ["D. Kobak", "P. Berens"], "title": "The art of using t-sne for singlecell transcriptomics", "venue": "bioRxiv, pp", "year": 2018}, {"authors": ["M.A. Kramer"], "title": "Nonlinear principal component analysis using autoassociative neural networks", "venue": "AIChE journal,", "year": 1991}, {"authors": ["Z.C. Lipton"], "title": "The mythos of model interpretability", "venue": "arXiv preprint arXiv:1606.03490,", "year": 2016}, {"authors": ["S.M. Lundberg", "Lee", "S.-I"], "title": "A unified approach to interpreting model predictions", "venue": "In Advances in Neural Information Processing Systems,", "year": 2017}, {"authors": ["Maaten", "L. v. d", "G. Hinton"], "title": "Visualizing data using t-sne", "venue": "Journal of machine learning research,", "year": 2008}, {"authors": ["G. Plumb", "D. Molitor", "A.S. Talwalkar"], "title": "Model agnostic supervised local explanations", "venue": "In Advances in Neural Information Processing Systems,", "year": 2018}, {"authors": ["M.T. Ribeiro", "S. Singh", "C. Guestrin"], "title": "Why should i trust you?: Explaining the predictions of any classifier", "venue": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "year": 2016}, {"authors": ["M.T. Ribeiro", "S. Singh", "C. Guestrin"], "title": "Anchors: Highprecision model-agnostic explanations", "year": 2018}, {"authors": ["J.P. Shaffer"], "title": "Multiple hypothesis testing", "venue": "Annual review of psychology,", "year": 1995}, {"authors": ["K. Shekhar", "S.W. Lapan", "I.E. Whitney", "N.M. Tran", "E.Z. Macosko", "M. Kowalczyk", "X. Adiconis", "J.Z. Levin", "J. Nemesh", "M Goldman"], "title": "Comprehensive classification of retinal bipolar neurons by single-cell transcriptomics", "year": 2016}, {"authors": ["J.C. Spall"], "title": "An overview of the simultaneous perturbation method for efficient optimization", "venue": "Johns Hopkins apl technical digest,", "year": 1998}, {"authors": ["M. Sundararajan", "A. Taly", "Q. Yan"], "title": "Axiomatic attribution for deep networks", "venue": "In Proceedings of the 34th International Conference on Machine Learning-Volume", "year": 2017}, {"authors": ["C. Szegedy", "W. Zaremba", "I. Sutskever", "J. Bruna", "D. Erhan", "I. Goodfellow", "R. Fergus"], "title": "Intriguing properties of neural networks", "venue": "arXiv preprint arXiv:1312.6199,", "year": 2013}, {"authors": ["R. Tomsett", "D. Harborne", "S. Chakraborty", "P. Gurram", "A. Preece"], "title": "Sanity checks for saliency metrics, 2019", "year": 2019}, {"authors": ["Y. Tsaig", "D.L. Donoho"], "title": "Extensions of compressed sensing", "venue": "Signal processing,", "year": 2006}, {"authors": ["F. Wang", "C. Rudin"], "title": "Falling rule lists", "venue": "In Artificial Intelligence and Statistics,", "year": 2015}, {"authors": ["X. Zhang", "A. Solar-Lezama", "R. Singh"], "title": "Interpreting neural network judgments via minimal, stable, and symbolic corrections", "venue": "In Advances in Neural Information Processing Systems,", "year": 2018}, {"authors": ["ents (IG) (Sundararajan"], "title": "2017) which produces local feature attribution explanations. Because IG is a supervised method, we start by training a classifier on top of the learned representation to get a multi-class classification model f that predicts which group", "year": 2017}, {"authors": ["Ding"], "title": "2018) is stable to being retrained and to modifications to the dataset and that TGT produces stable explanations for these representations. Identifying that Structure with Explanations. It is possible to have a model that learned the true structure", "year": 2018}], "sections": [{"heading": "1. Introduction", "text": "A common workflow in data exploration is to: 1) learn a lowdimensional representation of the data, 2) identify groups of points (i.e., clusters) that are similar to each other in that representation, and 3) examine the differences between the groups to determine what they represent. We focus on the third step of this process: answering the question \u201cWhat are the key differences between the groups?\u201d\nFor data exploration, this is an interesting question because the groups often correspond to an unobserved concept of interest and, by identifying which features differentiate the groups, we can learn something about that concept of interest. For example, consider single-cell RNA analysis. These\n1Carnegie Mellon University 2University of Michigan 3University of California, Los Angeles 4Determined AI. Correspondence to: Gregory Plumb <gdplumb@andrew.cmu.edu>.\nProceedings of the 37 th International Conference on Machine Learning, Online, PMLR 119, 2020. Copyright 2020 by the author(s).\nFigure 1: A representation learned for a single-cell RNA sequence dataset using the model from (Ding et al., 2018). Previous work on this dataset showed that these groups of cells correspond to different cell-types (Shekhar et al., 2016). The goal of a GCE is to use this representation to identify the changes in gene expression that are associated with a change of cell type.\ndatasets measure the expression levels of many genes for sampled cells. Usually the cell-type of each of those cells is unknown. Because gene expression and cell-type are closely related, the groups of points that can be seen in a low-dimensional representation of the dataset often correspond to different cell-types (Figure 1). By determining which gene expressions differentiate the groups, we can learn something about the connection between gene expression and cell-type.\nOne common approach for answering this question is manual interpretation. One simple way to do this, that we will use as a naive baseline, is to calculate the Difference Between the Mean (DBM) value of each feature in the original input space between a pair of groups. For example, consider using DBM to explain the differences between the cells in Group 3 and Group 17 from Figure 1. In this case, DBM\u2019s explanation contains many hundreds of non-zero elements, which is far too many to be understood by a person (Figure 2). If we make DBM\u2019s explanation sparse by thresholding it to include only the k largest changes, it is no longer an effective explanation because it no longer reliably maps points from Group 3 to Group 17 (Figure 3). More generally, manual interpretation can be time-consuming and typically ad-hoc, requiring the analyst to make arbitrary decisions that may not be supported by the data.\nAnother, more principled, method is statistical hypothesis testing for the differences between features across groups\nar X\niv :2\n00 3.\n01 64\n0v 3\n[ cs\n.L G\n] 1\n4 A\nug 2\n02 0\n(Shaffer, 1995). However, the trade-off between the power of these tests and their false positive rate becomes problematic in high-dimensional settings.\nBoth manual interpretation and statistical testing have an additional key shortcoming: they do not make use of the model that learned the low dimensional representation that was used to define the groups in the first place. Intuitively, we would expect that, by inspecting this model directly, we should be able to gain additional insight into the patterns that define the groups. With this perspective, answering our question of interest becomes an interpretable machine learning problem. Although there are a wide variety of methods developed in this area (Ribeiro et al., 2016; Lundberg & Lee, 2017; Wang & Rudin, 2015; Caruana et al., 2015; Ribeiro et al., 2018; Zhang et al., 2018), none of them are designed to answer our question of interest. See Section 2 for further discussion.\nTo answer our question of interest, we want a counterfactual explanation because our goal is to identify the key differences between Group A and Group B using the lowdimensional representation and the most natural way to do this is to find a transformation that causes the model to assign transformed points from Group A to Group B. Additionally, we want a global explanation because we want to find a explanation that works for all of the points in Group A and because we want the complete set of explanations to be consistent (i.e., symmetrical and transitive) among all the groups. See Section 3.2 for further discussion of our definition of consistency in this context. Hence, our goal is to find a Global Counterfactual Explanation (GCE). Although the space of possible transformations is very large, we consider translations in this work because their interpretability can be easily measured using sparsity.\nContributions: To the best of our knowledge, this is the first work that explores GCEs. Motivated by the desire to generate a simple (i.e., sparse) explanation between each pair of groups, we derive an algorithm to find these explanations that is motivated by compressed sensing (Tsaig & Donoho, 2006; Cande\u0300s et al., 2006). However, the solutions from compressed sensing are only able to explain the differences between one pair of groups. As a result, we generalize the compressed sensing solution to find a set of consistent explanations among all groups simultaneously. We call this algorithm Transitive Global Translations (TGT).\nWe demonstrate the usefulness of TGT with a series of experiments on synthetic, UCI, and single-cell RNA datasets. In our experiments, we measure the effectiveness of explanations using correctness and coverage, with sparsity as a proxy metric for interpretability, and we compare the patterns the explanations find to those we expect to be in the data. We find the TGT clearly outperforms DBM at producing sparse explanations of the model and that its\nexplanations match domain knowledge. 1"}, {"heading": "2. Related Work", "text": "Most of the literature on cluster analysis focuses on defining the clusters; the interpretation methods discussed in that literature are primarily manual inspection/visualization or statistical testing (Jiang et al., 2004). Consequently, the focus of our related work will be on interpretable machine learning. Although interpretability is often loosely defined and context specific (Lipton, 2016), we categorize existing methods along two axes in order to demonstrate how a GCE differs from them. Those axes are the explanation\u2019s level and its form.\n1Code for all algorithms and experiments is available at https://github.com/GDPlumb/ELDR\nThe first axis used to categorize explanations is their level: local or global. A local explanation explains a single prediction made by the model (Ribeiro et al., 2016; Lundberg & Lee, 2017; Plumb et al., 2018). Kauffmann et al. (2019) studies a problem closely related to ours of explaining why a point was assigned to its cluster/group. A global explanation will explain multiple predictions or the entire model at once (Wang & Rudin, 2015; Caruana et al., 2015; Ribeiro et al., 2018).\nThe second axis we use to categorize explanations is their form: feature attribution, approximation, or counterfactual. A feature attribution explanation assigns a value measuring how each feature contributed to the model\u2019s prediction(s) (Lundberg & Lee, 2017; Sundararajan et al., 2017). Importantly, it is necessary to define a baseline value for the features in order to compute these explanations. An approximation explanation approximates the model being explained using a function that is simple enough to be considered directly interpretable (e.g., a sparse linear model or a small decision tree) across some neighborhood, which could be centered around a point or could be the entire input space (Ribeiro et al., 2016; Plumb et al., 2018; Wang & Rudin, 2015; Caruana et al., 2015; Ribeiro et al., 2018). A counterfactual explanation finds a transformation of the input(s) such that the transformed version of the input is treated in a specific way by the model (Zhang et al., 2018; Dhurandhar et al., 2018; Goyal et al., 2019; Dhurandhar et al., 2019).\nFor various reasons, it would be challenging to use other types of explanations to construct a GCE. Local explanations would have to be aggregated in order to produce an explanation that applies to a group of points and it would be nontrivial to ensure that the resulting \u201caggregated group explanations\u201d are consistent (i.e., symmetrical and transitive). For feature attribution and local approximation explanations, it is difficult to guarantee that the baseline value or neighborhood they consider is defined broadly enough to find the transformation we want. For global approximation explanations, we might not be able to approximate a complex model well enough to find the transformation we want because of the accuracy-interpretability trade-off that stems from the complexity constraint on the explanation model (Lipton, 2016). For a concrete example of these difficulties, see the Appendix A.1. This example uses Integrated Gradients (Sundararajan et al., 2017) which is a local feature attribution method that produces symmetrical and transitive explanations with respect to a single class."}, {"heading": "3. Global Counterfactual Explanations", "text": "We will start by introducing our notation, more formally stating the goal of a GCE, and defining the metrics that we will use to measure the quality of GCEs. We do this under the simplifying assumption that we have only two groups\nof points that we are interested in. Then, in Section 3.1, we will demonstrate the connection between finding a GCE and compressed sensing. We use that connection to derive a loss function we can minimize to find a GCE between a single pair of groups of points. Finally, in Section 3.2, we will remove our simplifying assumption and introduce our algorithm, TGT, for finding a set of consistent GCEs among multiple groups of points.\nNotation: Let r : Rd \u2192 Rm denote the function that maps the points in the feature space into a lower-dimensional representation space. The only restriction that we place on r is that it is differentiable (see the Appendix A.2 for more discussion on r). Suppose that we have two regions of interest in this representation: Rinitial, Rtarget \u2282 Rm. Let Xinitial and Xtarget denote their pre-images. Then, our goal is to find the key differences between Xinitial and Xtarget in Rd and, unlike manual interpretation or statistical testing, we will treat this as an interpretable machine learning problem by using r to help find those key differences.\nDefining the Goal of GCEs: At a high level, the goal of a GCE is to find a transformation that takes the points in Xinitial and transforms them so that they are mapped to Rtarget by r; in other words, r treats the transformed points fromXinitial as if they were points fromXtarget. Formally, the goal is to find a transformation function t : Rd \u2192 Rd such that:\nr(t(x)) \u2208 Rtarget \u2200x \u2208 Xinitial (1)\nBecause we are using t as an explanation, it should be as simple as possible. Since they are very simple and their complexity can be readily measured by their sparsity, we limit t to a translation:\nt(x) = x+ \u03b4 (2)\nMeasuring the Quality of GCEs: To measure the quality of a GCE we use two metrics: correctness and coverage. Correctness measures the fraction of points mapped from Xinitial into Rtarget. Coverage measures the fraction of points in Rtarget that transformed points from Xinitial are similar to. Mathematically, we define correctness as:\ncr(t) = 1|Xinitial| \u2211\nx\u2208Xinitial 1[\u2203x\u2032 \u2208 Xtarget | ||r(t(x))\u2212 r(x\u2032)||22 \u2264 ] (3)\nAnd coverage as:\ncv(t) = 1|Xtarget| \u2211\nx\u2208Xtarget 1[\u2203x\u2032 \u2208 Xinitial | ||r(x)\u2212 r(t(x\u2032))||22 \u2264 ] (4)\nClearly, correctness is a necessary property because an explanation with poor correctness has failed to map points fromXinitial intoRtarget (Equation 1). However, coverage is also a desirable property because, intuitively, an explanation with good coverage has captured all of the differences between the groups.\nDefining these metrics requires that we pick a value of .2 Observe that, if Xinitial = Xtarget and t(x) = x, then cr(t) = cv(t) and we have a measure of how similar a group of points is to itself. After r has been learned, we increase until this self-similarity metric for each group of points in the learned representation is between 0.95 and 1.\nA Simple Illustration: We will now conclude our introduction to GCEs with a simple example to visualize the transformation function and the metrics. In Figures 4, 5, and 6, the data is generated from two Gaussian distributions with different means and r(x) = x. We use DBM between Group 1 and Group 0 to define the translation/explanation. In Figure 4, the two distributions have an equal variance and, as a result, the translation is an effective explanation with good correctness and coverage. In Figures 5 and 6, Group 0 has a smaller variance than Group 1. Because a simple translation cannot capture that information3, the translation has poor coverage from Group 0 to Group 1 while its negative has poor correctness from Group 1 to Group 0. This illustrates the connection between correctness and coverage that we will discuss more in Section 3.2."}, {"heading": "3.1. Relating GCEs and Compressed Sensing", "text": "We will now demonstrate how the problem of finding a GCE between a pair of groups is connected to compressed sensing. We start with an \u201cideal\u201d loss function that is too difficult to optimize and then make several relaxations to it.\nIn principle, Equations 1, 3, or 4 define objective functions that we could optimize, but they are discontinuous and hence difficult to optimize. To progress towards a tractable objective, first consider a continuous approximation of correctness (Equation 3):\nlossmin(t) = 1 |Xinitial| \u2211\nx\u2208Xinitial min x\u2032\u2208Xtarget ||r(t(x))\u2212 r(x\u2032)||22 (5)\nThis loss could be optimized by gradient descent using autodifferentiation software, although doing so might be difficult because of the min operation. Consequently, we consider a simplified version of it:\nlossmean(t) = 1 |Xinitial| \u2211\nx\u2208Xinitial ||r(t(x))\u2212 r\u0304target||22 (6)\nwhere r\u0304i = 1|Xi| \u2211\nx\u2208Xi r(x). 4\n2We use an indicator based on l2 distance and the points themselves for two reasons. First, it is necessary for the definition of coverage. Second, it makes it easier to define Rtarget for representations that are more than two-dimensional.\n3This is true regardless of how the translation is found (e.g., TGT or DBM). So this example also motivates the usefulness of more complex transformation functions.\n4Note that lossmin clearly optimizes for correctness and it does not directly penalize coverage. However, because lossmean\nNext, we are going to make use of our restriction of t to a translation, \u03b4, and assume, for now, that r is a linear mapping: r(x) = Ax for A \u2208 Rm\u00d7d. Although this assumption about r will not be true in general, it allows us to connect a GCE to the classical compressed sensing formulation. Under these constraints, we have that:\nlossmean+linear(\u03b4) = 1 |Xinitial| \u2211\nx\u2208Xinitial ||A(x+ \u03b4)\u2212 r\u0304target||22 (7)\nBy setting its derivative to zero, we find that solving A\u03b4 = r\u0304target \u2212 r\u0304initial yields an optimal solution to Equation 7. Observe that this is an undetermined linear system, because m < d, and so we can always find such a \u03b4.\nRecall that, in general, we want to find a sparse \u03b4. This is partially because we need the explanation to be simple enough to be understood by a person, but also because the explanation is hopefully modeling real world phenomena and these phenomena often have nice structure (e.g.,\nencourages r(t(x)) to be close to r\u0304target, it is optimizing for correctness at the expense of coverage. In Section 3.2, we will discuss why this is not as harmful as it seems in our setting where t is a symmetrical translation.\nsparsity). Then, because we are finding a GCE by solving A\u03b4 = r\u0304target \u2212 r\u0304initial5 , we can see that r\u0304target \u2212 r\u0304initial is the model\u2019s low-dimensional measurement that we are trying to reconstruct using the high-dimensional but sparse underlying signal, \u03b4. This is exactly the classical compressed sensing problem formulation. As a result, we will add l1 regularization to \u03b4 as an additional component to the loss function as is done in both linear and non-linear compressed sensing (Blumensath, 2013).\nConsequently, we could consider finding a GCE by minimizing the linear compressed sensing loss:\nlosscs(\u03b4) = ||A(x\u0304initial + \u03b4)\u2212 r\u0304target||22 + \u03bb||\u03b4||1 (8)\nwhere x\u0304i = 1|Xi| \u2211\nx\u2208Xi x. Or, by removing the assumption\nthat r(x) = Ax, minimizing the more general version that we use for our experiments:\nloss(\u03b4) = ||r(x\u0304initial + \u03b4)\u2212 r\u0304target||22 + \u03bb||\u03b4||1 (9)"}, {"heading": "3.2. Computing GCEs", "text": "We have thus far limited ourselves to explaining the differences between l = 2 groups of points labeled as X0 (\u201cinitial\u201d) and X1 (\u201ctarget\u201d), and focused on learning an explanation t0\u21921 from X0 to X1. However, this is not a realistic setting because this labeling was arbitrary and because we usually have l > 2.\nUnfortunately, the naive solution of using compressed sensing to independently produce explanations for all O(l2) pairs of groups will fail to satisfy two desirable properties related to the internal consistency of the explanations. For instance, we would like t0\u21921 to agree with t1\u21920, a property we call symmetry. Additionally, we would like t0\u21922 to agree with the combined explanations t0\u21921 and t1\u21922; we call this transitivity. Formally, symmetry requires that ti\u2192j = t\u22121j\u2192i and transitivity requires that ti\u2192k = tj\u2192k \u25e6 ti\u2192j for any j.\nOur approach to finding a consistent (i.e., symmetrical and transitive) set of explanations is to enforce the consistency constraints by-design. We do this by computing a set of explanations relative to a reference group. We assume that X0 is the reference group and find a set of basis explanations t1, . . . , tl\u22121, where ti = t0\u2192i. We then use this set of basis explanations to construct the explanation between any pair of the groups of points.6 Algorithm 2 describes how ti\u2192j can be constructed from t1, . . . , tl\u22121.\n5Note that DBM solves this but does not consider sparsity. 6Importantly, the transitivity constraint also ensures that our choice of how we label the groups or which group is the reference group does not influence the optimal solution of our algorithm.\nAlgorithm 1 TGT: Calculating GCEs with a Reference Group. Note that, because \u03bb is applied to all of the explanations, we cannot tune it to guarantee that each explanation is exactly k-sparse.\nInput: Model: r Group Means: x\u0304i (feature space) and\nr\u0304i (representation space) for i = 0, . . . , l \u2212 1\nl1 Regularization Weight: \u03bb Learning Rate: \u03b1\nInitialize: \u03b41, . . . , \u03b4l\u22121 to vectors of 0 while not converged do\nSample i 6= j from {0, . . . , l \u2212 1} Construct ti\u2192j (\u03b4i\u2192j) using Algorithm 2 Calculate objective: v = loss(\u03b4i\u2192j) using Equation 9 Update the components of \u03b4i\u2192j using Algorithm 3\nend while Return: \u03b41, . . . , \u03b4l\u22121\nAlgorithm 2 How to construct any explanation between an arbitrary pair of groups, ti\u2192j , using the set of basis explanations relative to the reference group, t1, . . . , tl\u22121\nInput: i, j if i == 1 then\nReturn: tj else if j == 1 then\nReturn: t\u22121i else\nReturn: tj \u25e6 t\u22121i end if\nAlgorithm 3 How to update the basis explanations, \u03b41, . . . , \u03b4l\u22121, based on the performance of \u03b4i\u2192j . This splits the signal from the gradient between the basis explanations used to construct \u03b4i\u2192j . Note that it does not maintain any fixed level of sparsity.\nInput: i, j, \u03b1 (learning rate),\u2207v (gradient of the loss function) if i == 1 then \u03b4j = \u03b4j \u2212 \u03b1\u2207v else if j == 1 then \u03b4i = \u03b4i + \u03b1\u2207v else \u03b4j = \u03b4j \u2212 0.5\u03b1\u2207v \u03b4i = \u03b4i + 0.5\u03b1\u2207v end if\nOverview of TGT. We now have all of the pieces necessary to actually compute a GCE: a differentiable loss function to measure the quality of ti\u2192j , l1 regularization to help us find the simplest possible explanation between Xi and Xj , and a problem setup to ensure that our explanations are consistent across X0, . . . , Xl\u22121. At a high level, TGT will proceed to sample random \u201cinitial\u201d and \u201ctarget\u201d groups from the set of all groups, construct that explanation from the set of basis explanations (Algorithm 2), and then use Equation 9 as a loss function to use to update the explanation using gradient descent (Algorithm 3). Pseudo-code for this process is in Algorithm 1.7 The main hyper-parameter that requires tuning is the strength of the l1 regularization, \u03bb.\n7The pseudo-code leaves out some of the details of the opti-\nWhy can we prioritize correctness in Equation 9? In the previous subsection, we noted that the Equation 9 prioritizes correctness over coverage. Because Algorithm 1 randomly chooses the \u201cinitial\u201d and \u201ctarget\u201d groups many times, it updates the basis explanations based on both cr(\u03b4i\u2192j) and cr(\u03b4j\u2192i). When t is a translation and the explanations are symmetrical, we can see that cr(\u03b4j\u2192i) is closely related to cv(\u03b4i\u2192j). This is because they only differ in whether they add \u03b4j\u2192i to a point in Xj or subtract \u03b4j\u2192i from a point in Xi in their respective indicator functions. Further, if we consider r(x) = Ax, then they are identical metrics (this is consistent with the example from Figure 5). Collectively, this means that Algorithm 1 implicitly considers both cr(\u03b4i\u2192j) and cv(\u03b4i\u2192j) while computing the explanations."}, {"heading": "3.3. Controlling the Level of Sparsity", "text": "Because neither TGT nor DBM is guaranteed to produce a k-sparse explanation, we will threshold each of the explanations to include only the k most important features (i.e., the k features with the largest absolute value) for our experiments. This is done after they have been calculated but before their quality has been evaluated. Importantly, TGT has a hyper-parameter, \u03bb, which roughly controls the sparsity of its explanations; as a result, we will tune \u03bb to maximize correctness for each value of k.\nThe fact that \u03bb is tuned for each value of k raises an interesting question: \u201cDoes TGT use a subset of the features from its k2-sparse explanation for its k1-sparse explanation when k1 < k2?\u201d. Naturally, we would like for the answer to be \u201cyes\u201d because, for example, it does not seem like a desirable outcome if a 2-sparse explanation uses Features A and B but a 1-sparse explanation uses Feature C.\nSuppose we have two explanations between Group i and Group j: e1 which is k1-sparse and e2 which is k2-sparse with k1 < k2. To address this question, we define the similarity of e1 and e2 as:\nsimilarity(e1, e2) = \u2211 |e1[i]|1[e2[i] 6= 0] ||e1||1\n(10)\nThis metric captures how much of e1\u2019s explanation uses features that were also chosen by e2.8 So a score of 1 indicates that e1 uses a subset of the features of e2 and a score of 0 indicates that it uses entirely different features. Because DBM does not solve a different optimization problem to achieve each level of sparsity, its similarity measure is always 1. When we run experiments with a list of sparsity levels k1, . . . , km, we\nmization process such as how often we sample new \u201cinitial\u201d and \u201ctarget\u201d groups and how convergence is defined. For those details, see the code on GitHub.\n8Note that this metric also includes the run to run variance of TGT.\nwill plot similarity(e1, e2), . . . , similarity(em\u22121, em) to measure how similar TGT\u2019s explanations are as the level of sparsity increases."}, {"heading": "4. Experimental Results", "text": "Our experimental results are divided into two sections. In the first section, we demonstrate that TGT is better at explaining the model than DBM is when we restrict the explanations to varying degrees of sparsity. In the second section, we move beyond assessing whether or not TGT explains the model and demonstrate that it also appears to capture real signals in the data."}, {"heading": "4.1. TGT\u2019s Efficacy at Explaining the Model", "text": "From an interpretable machine learning perspective, our goal is help practitioners understand the dimensionalityreduction models they use in the data exploration process. We measure the quality of GCEs using correctness (Equation 3) and coverage (Equation 4) at varying degrees of sparsity (Figure 7).\nWe use the model from (Ding et al., 2018)9on the UCI Iris, Boston Housing, and Heart Disease datasets (Dua & Graff, 2017) and a single-cell RNA dataset (Shekhar et al., 2016) and use its a visualization of its two-dimensional representation to define the groups of points. Figure 1 shows this representation and grouping for the single-cell RNA dataset; similar plots for all of the datasets are in the Appendix A.3. This model learns a non-linear embedding using a neural network architecture which is trained to be a parametric version of t-SNE (Maaten & Hinton, 2008) that also preserves the global structure in the data (Kobak & Berens, 2018).\nNext, because the acceptable level of complexity depends on both the application and the person using the explanation, we measure the effectiveness of the explanations produced by TGT and DBM at explaining the model across a range of sparsity levels.\nExplanation effectiveness at different levels of sparsity. Figure 7 shows the results of this comparison. We can see that TGT performed at least as well as DBM and usually did better. Further, we can see that TGT\u2019s explanations are quite similar to each other as we ask for sparser explanations. Note that all of these metrics are defined for a single pair of groups and so these plots report the average across all pairs of groups.\nExploring a Specific Level of Sparsity. Figure 7 shows that TGT\u2019s performance: is almost as good when k = 1 as when k = 4 on Iris, drops off sharply for k < 5 on Boston\n9We use this model because previous analysis showed that its representation identifies meaningful groups on the single-cell RNA dataset (Ding et al., 2018).\nHousing, and drops off sharply for k < 3 on Heart Disease. Further, on the single-cell RNA dataset, it shows that TGT significantly outperforms DBM when k = 250 (Appendix A.4 Figure 16) and that this comparison becomes more favorable for TGT for smaller k. The level of sparsity where the metrics drop off indicates the minimum explanation complexity required for these methods to explain the model. See Figure 8 for an example of the pairwise correctness and coverage metrics for these levels of sparsity.\nFigure 3 shows that DBM does not produce a good 250- sparse explanation for the difference between Group 3 and Group 17 from Figure 1. For the sake of an easy comparison, Figure 9 shows a similar plot that uses TGT\u2019s 250-sparse explanation; it is clearly a much better explanation."}, {"heading": "4.2. TGT\u2019s Efficacy at Capturing Real Signals in the Data", "text": "In the previous section, we demonstrated that TGT provides accurate explanations of the model that learned the low-dimensional representation. However, in practice, there could be a mismatch between what the model itself learns and the true underlying structure in the data. In this section, we evaluate empirically whether or not TGT provides explanations that match underlying patterns in the data.\nWe begin with an experiment on a synthetic dataset with a known causal structure and demonstrate that TGT correctly identifies this structure. This also serves as an intuitive example of why a sparser explanation can be as effective as a less-sparse explanation. Next, we leverage the labels that come with the UCI datasets to compare TGT\u2019s explanations\nto some basic domain knowledge. Finally, we modify the UCI datasets and demonstrate that TGT is able to identify those modifications. Together, these results indicate that TGT is identifying real patterns in the data."}, {"heading": "Synthetic Data with a Known Causal Structure. By", "text": "specifying the causal structure of the data, we can know which differences are necessary in the explanation, since they are the casual differences, and which differences are unnecessary, since they are explained by the causal differences. We find that TGT correctly identifies the causal structure of this dataset and that DBM does not.\nWe use the following procedure to generate each point in our synthetic dataset: x1, x2 \u223c Bern(0.5) + N (0, 0.2), x3 \u223c N (0, 0.5), and x4 \u223c x1 + N (0, 0.05). The causal structure of this dataset is simple. x1 and x2 jointly cause 4 different groups of points. The explanation for the differences between these groups must include these variables. x3 is a noise variable that is unrelated to these groups and, as a result, should not be included in any explanation. x4 is a variable that is correlated with those groups, since it is caused by x1, but does not cause those groups. As a result, it is not necessary to include it in any explanation.\nWe generate a dataset consisting of 400 points created using this process and train an autoencoder (Kramer, 1991) to\nlearn a two dimensional representation of the dataset. A visualization of this learned representation is in the Appendix A.3 Figure 14; as expected, there are four distinct groups of points in it. Then, we use TGT and DBM to calculate the GCEs between these groups. The pairwise and average correctness and coverage metrics for these solutions are in the Appendix A.4 Figures 17 and 18; observe that the two methods are equally effective at explaining the model.\nWhen we inspect the explanations ()Table 1), we see that both TGT and DBM use x1 and x2, neither use x3, and that DBM uses x4 while TGT does not. This shows that, even in a very simple setting, there is good reason to believe that an explanation that is simpler (i.e., sparser) than the DBM explanation exists and that TGT might be able to find it.\nQualitative Analysis of the UCI Datasets using the Labels. Qualitatively, we find that TGT\u2019s explanations agree with domain knowledge about these datasets. Specifically: On the Iris dataset, its explanations agree with a simple decision tree because they both rely mostly on the Petal Width to separate the groups. On the Boston Housing dataset, it identifies the differences between a set of inexpensive urban houses vs expensive suburban houses as well as equally priced groups of houses that differ mainly in whether or not they are on the Charles River. Finally on the Heart Disease dataset, it finds that the difference between a moderate and a low risk group of subjects was that the low-risk group\u2019s symptoms are explained by something other than heart disease and the difference between the moderate and high risk group of subjects is that the former is made up of men and the later of women. For full details, see the Appendix A.5.\nQuantitative Analysis of Modified Versions of the UCI Datasets. In order to perform a more quantitative analysis, we artificially add a known signal to the dataset by choosing one of the groups of points, creating a modified copy of it by translating it, and adding those new points back into the dataset. We then ask two important questions about TGT\u2019s behavior. First, does TGT correctly identify the modifications we made to the original dataset? Second, do TGT\u2019s explanations between the original groups change\nwhen the modified group is added to the dataset? Details of how we setup these experiments and their results are in the Appendix A.6.\nWe find that TGT does identify the modifications we made and that, in doing so, it does not significantly change the explanations between the original groups. Importantly, this result remains true even if we retrain the learned representation on the modified dataset.\nThese results are a strong indicator that TGT finds real patterns in the data because it recovers both the original signal and the artificial signal even when the algorithm is rerun or the representation is retrained."}, {"heading": "5. Conclusion", "text": "In this work, we introduced a new type of explanation, a GCE, which is a counterfactual explanation that applies to an entire group of points rather than a single point. Next, we defined reasonable metrics to measure the quality of GCEs (i.e., correctness and coverage) and introduced the concept of consistency (i.e., symmetry and transitivity), which is an important additional criteria that GCEs must satisfy. Given that, we defined an algorithm for finding consistent GCEs, TGT, that treats each pairwise explanation as a compressed sensing problem. Our first experiments empirically demonstrated that TGT is better able to explain the model than DBM across a range of levels of sparsity. Our next experiments showed that TGT captures real patterns in the data. This was done using a synthetic dataset with a known causal structure and by comparing TGT\u2019s explanations to background knowledge about the UCI datasets. As an additional test, we then added a synthetic signal to the UCI datasets and demonstrated that TGT can recover that signal without changing its explanations between the real groups. Importantly, this result remains true even when the representation is retrained.\nAlthough we focused on data exploration in this work, similar groups arise naturally whenever the model being trained uses an encoder-decoder structure. This technique is ubiquitous in most areas where deep learning is common (e.g., semi-supervised learning, image classification, natural language processing, reinforcement learning). In these settings, identifying the key differences between the groups is an interesting question because we can observe that \u201cThe model treats points in Group A the same/differently as points in Group B\u201d, determine that \u201cThe key differences between Group A and Group B are X\u201d, and then conclude that \u201cThe model does not/does use pattern X to make its decisions\u201d. We believe that exploring these applications is an important direction of future work that will require more sophisticated transformation functions, optimization procedures, and definitions of the groups of points of interest."}, {"heading": "Acknowledgments", "text": "This work was supported in part by DARPA FA875017C0141, the National Science Foundation grants IIS1705121 and IIS1838017, an Okawa Grant, a Google Faculty Award, an Amazon Web Services Award, a JP Morgan A.I. Research Faculty Award, and a Carnegie Bosch Institute Research Award. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of DARPA, the National Science Foundation, or any other funding agency. We would also like to thank Joon Kim, Jeffrey Li, Valerie Chen, Misha Khodak, and Jeremy Cohen for their feedback while writing this paper."}, {"heading": "A. Appendix", "text": ""}, {"heading": "A.1. An Example of the Difficulty of Using Existing Methods", "text": "For this example, we are going to consider Integrated Gradients (IG) (Sundararajan et al., 2017) which produces local feature attribution explanations. Because IG is a supervised method, we start by training a classifier on top of the learned representation to get a multi-class classification model f that predicts which group a point belongs to. Because our goal is to explain the difference between Group A and Group B with IG, we average IG\u2019s explanation for each point in Group B relative to each possible baseline value of a point in Group A for f \u2019s Class B label. To be more precise:\n\u03b4IG(A\u2192 B) = 1\n|XA| |XB | \u2211 x\u2208XB \u2211 a\u2208XA IG(x, class = B, baseline = a)\n(11)\nWe will refer to this as \u2018group Integrated Gradients\u2019 or gIG.\nChallenge 1: Comparing Explanation Types. Because IG produces feature attributions and TGT produces counterfactuals, there is no reliable metric in the literature to directly compare them. On the one hand, most feature attributions are the \u2018correct explanation\u2019 for their specific definitions for \u2018attribution\u2019 and the \u2018baseline\u2019 value; this has made measuring their quality challenging (Tomsett et al., 2019). On the other hand, we cannot treat a feature attribution as a transformation function/translation, so our metrics and other metrics for counterfactual explanations cannot be applied.\nAs a result, we compare TGT to gIG on the same synthetic dataset we used earlier. We found that gIG identifies the causal variables as being significant and ignores the noise variable, but that it also identifies the correlated variable as being significant. This indicates that it is likely to be unable to find sparse explanations as well as TGT can.\nChallenge 2: Consistency of Aggregated Local Explanations. One of the reasons we chose IG as a baseline method to aggregate is because its attributions are symmetrical and transitive with respect to a fixed class. In other words, if all we cared about was explaining the differences between all of the groups of points with respect to a single reference group, say Group C, then IG would produce consistent explanations. However, explaining the features that separate Group A from Group B relative to Group C is not the problem we are trying to solve.\nWhen we use Equation 11 to calculate \u03b4IG(i\u2192 j) we found that the resulting explanations were not consistent. This does not violate the theory of IG because each \u03b4IG(i\u2192 j) is calculated with reference Group j and so the assumption that we have a single reference group is not satisfied.\nWhen we considered modifying Equation 11 to aggregate\nover the reference \u2018class/group\u2019 and potentially gain consistency that way, we either got uniform zero attributions (if we averaged over all reference groups) or inconsistent explanations (if we excluded any subset of {i, j} from the averaging).\nConclusion. As suggested in Section 2, using existing explanation methods to find GCEs is going to be challenging because it is not what they are designed to do. We found that IG, a method that theoretically looked promising, was unable to be extended in a simple way to this setting."}, {"heading": "A.2. Representation Function", "text": "Differentiability. TGT assumes that r is a differentiable function. Hidden in this assumption is the assumption that r is a function that we can evaluate on an arbitrary point. Although most methods for learning a low-dimensional representation satisfy this assumption, t-SNE does not. Fortunately there are parametric variations of t-SNE such as the one we used in our experiments (Ding et al., 2018). The assumption that r is differentiable can be relaxed by using a finite-difference optimization method, such as SPSA (Spall, 1998), at the expense of computational cost.\nLearning Meaningful Structure. One assumption that every analysis (whether that is manual inspection, statistical testing, or interpretable ML) of the representation learned by r is that this function learned meaningful structure from the data. Because practitioners are already relying on these representations and, in some situations, have verified that they are meaningful, this concern is largely orthogonal to our work.\nHowever, from an interpretable ML perspective, our goal is to explain r. So, if r identifies different structure when it is retrained or when it is trained with a different algorithm or structure, we expect TGT to produce different explanations since the embedding itself has changed.\nOur experimental results show that the representation learned by (Ding et al., 2018) is stable to being retrained and to modifications to the dataset and that TGT produces stable explanations for these representations.\nIdentifying that Structure with Explanations. It is possible to have a model that learned the true structure of the data and to have an explanation that is technically true (as measured by some proxy metric for interpretability) about the model but that also fails to capture meaningful patterns. For example, adversarial examples (Szegedy et al., 2013) are technically local counterfactual explanations but they usually look like random noise and, as a result, do not tell a person much about the patterns the model has learned. TGT\u2019s design, which calculates the explanation between each pair of groups as if it were a compressed sensing problem but constrains those solutions to be symmetrical and\ntransitive among all groups, was chosen as a prior to prevent this type of behavior."}, {"heading": "A.3. Learned Representations", "text": "The learned representations and the corresponding groups of points for the datasets we studied are in Figures 10, 11, 12, 13, and 14."}, {"heading": "A.4. Pairwise Correctness and Coverage Plots", "text": "TGT is much better than DBM for finding 250-sparse explanations on the single-cell RNA dataset (Figures 15 and 16). On the synthetic dataset, TGT and DBM are equally effective explanations of the model (Figures 17 and 18). However, TGT only relied on the two causal variables while\nDBM included the correlated variable as well."}, {"heading": "A.5. Qualitative Analysis of the UCI Datasets using the Labels", "text": "Although the representations we learned for these datasets were trained in an unsupervised manner, the groups that they find often have strong connections to the labels for the datasets; see Table 2, Figure 21, and Table 3. By using the connection between the groups and labels, we will be able to qualitatively assess whether or not TGT is finding real patterns in the data.\nIris Dataset. Looking at Table 2, we can see that the groups in this representation match very closely with the class labels. As a result, we would like to know whether or not the explanations TGT finds are consistent with a model trained directly to predict the labels. For this comparison, we used a simple decision tree, which is shown in Figure 19. Looking at TGT\u2019s explanations (Figure 20), we can see that they largely agree with the decision tree since both primarily use Petal Width to separate the classes/groups.\nTable 2: The distribution of the labels per group for the UCI Iris dataset (classification).\nGroup Class Iris Setosa Iris Versicolour Iris Virginica 0 0 5 38 1 0 44 12 2 48 0 0\nFigure 20: TGT\u2019s 1-sparse explanation of the difference between Group 0 and Group 1 (left) and Group 0 and Group 2 (right). Similar to the decision tree, they rely on the Petal Width feature.\nBoston Housing Dataset. Looking at Figure 21, we can see that two comparisons between the groups stand out: Group 0 to Group 2, which shows a significant increase in the price, and Group 3 to Group 5, which has relatively little effect on the price. As a result, we would like to determine what the differences between these groups of houses are that influence their price.\nresidential lots and B10 both increase while access to radial highways and tax rates both decrease. It also found that the key differences between Group 3 and Group 5 are, first, moving the house onto the Charles river and, second, decreasing B.\nHeart Disease Dataset. Looking at Figure 3, we see that there are three large groups that stand out: Group 1, which has a balanced risk of heart disease, Group 3, which has a relatively low risk, and Group 6, which has a relatively high risk. As a result, we would like to determine what the differences between these groups of subjects are that influence their risk of heart disease.\nLooking at Figure 23, TGT found that the key differences between Group 1 and Group 3 are a moderate decrease in chest pain along with having exercised induced angina; these are subjects whose symptoms are explained by exercise induced angina rather than heart disease. It also found that the key difference between Group 1 and Group 6 is that Group 1 is made up of men while Group 6 is made up of women; this is consistent with the fact that heart disease is the leading cause of mortality in women (Bello & Mosca, 2004).\n10This is a unusually defined feature that is related to the racial demographics of a town. Determining what it means to change this feature depends on a measurement that is not in the dataset."}, {"heading": "A.6. Quantitative Analysis of Modified Versions of the UCI Datasets.", "text": "Because the UCI datasets are not synthetic datasets, we do not know the underlying process that generated the data and, as a result, it is difficult to quantitatively determine whether or not an explanation is \u201ccorrect\u201d in the way that we could with the synthetic dataset. Consequently, we performed a series of experiments on modified versions of the original datasets in order to answer two important questions:\n\u2022 Does TGT correctly identify the modifications we made to the original dataset?\n\u2022 Do TGT\u2019s explanations between the original groups change when the modified group is added to the dataset?\nWe found that TGT does identify the modifications we made and that, in doing so, it does not significantly change the explanations between the original groups. Importantly, this result remains true even if we retrain the learned representation on the modified dataset. These results are a strong indicator that TGT is finding real patterns in the data.\nHow do we modify the datasets? We create a modified version of the original dataset by: picking one of the groups of points in the original dataset, modifying that group of points in some way, and adding that new modified group of points to the original dataset. We will call the original dataset D and the modified dataset D\u2032, where D\u2032 = D \u222aG\u2032 and G\u2032 is the modified version of some group of points G. The critical choice to make during this process is to determine what modification to apply to G to get G\u2032. We chose to add random noise to some of the features of the points in G and used the following two criteria when defining this modification for a particular dataset:\n\u2022 G\u2032 should be approximately within the range/distribution of D.\n\u2022 r(G\u2032) should form it\u2019s own (approximately) distinct group. Intuitively, if r(G\u2032) does not form its own group, then r thinks G\u2032 is similar to some other group in the dataset and, as a result, we would not expect TGT to be able to explain the differences between G\u2032 and that group of points.\nThe modifications we used are in Table 4.\nExperimental Setup: We now have two versions of each dataset: D and D\u2032. We also have the original learned representation r, which was trained on D, and a new learned representation r\u2032, which is trained on D\u2032. As a result, we have three sets of explanations:\n\u2022 Original: These explain r when applied to D\n\u2022 Modified: These explain r when applied to D\u2032\n\u2022 Retrained: These explain r\u2032 when applied to D\u2032\nThe visualization of the representation for the first setting is in the Appendix A.3 and the later two these settings is in Figures 24, 25, and 26. Note that applying r to D\u2032 looks the same as applying r to D except for the fact that there is an additional group from adding G\u2032 to D and that applying r\u2032 to D\u2032 often shows that r\u2032 has learned to separate G\u2032 from the other groups better than r did.\nDoes TGT correctly identify the modifications we made to the original dataset? In Figure Figures 27, 28, and 29, we can see the explanations TGT found for the difference between G and G\u2032 for each of the datasets. If we compare the explanations to the modifications from Table 4, we can see that they identified which features we changed and, approximately, by how much. The error in the estimation of \u201cby how much\u201d is due to the l1 regularization used to find a simple explanation.\nDo TGT\u2019s explanations between the original groups change when the modified group is added to the dataset? In Figures 30, 31, and 32, we can see a comparison of the explanations for the differences between the original groups for the Original vs the Modified and the Original vs the Retrained explanations. Adding G\u2032 to D did not cause TGT to find significantly different explanations between the groups in D. Explaining r\u2032 resulted in explanations that were generally similar, but adding another layer of variability (i.e., training r\u2032) did add some noise."}], "title": "Explaining Groups of Points in Low-Dimensional Representations", "year": 2020}