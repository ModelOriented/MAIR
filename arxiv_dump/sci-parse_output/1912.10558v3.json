{"abstractText": "Modern predictive analytics underpinned by machine learning techniques has become a key enabler to the automation of datadriven decision making. In the context of business process management, predictive analytics has been applied to making predictions about the future state of an ongoing business process instance, for example, when will the process instance complete and what will be the outcome upon completion. Machine learning models can be trained on event log data recording historical process execution to build the underlying predictive models. Multiple techniques have been proposed so far which encode the information available in an event log and construct input features required to train a predictive model. While accuracy has been a dominant criterion in the choice of various techniques, these techniques are often applied as a black-box in building predictive models. In this paper, we derive explanations using interpretable machine learning techniques to compare and contrast the suitability of multiple predictive models of high accuracy. The explanations allow us to gain an understanding of the underlying reasons for a prediction and highlight scenarios where accuracy alone may not be sufficient in assessing the suitability of techniques used to encode event log data to features used by a predictive model. Findings from this study motivate the need and importance to incorporate interpretability in predictive process analytics.", "authors": [{"affiliations": [], "name": "Renuka Sindhgatta"}, {"affiliations": [], "name": "Chun Ouyang"}, {"affiliations": [], "name": "Catarina Moreira"}], "id": "SP:ca12e190bccf468efef2d098e3720bb48a9419c6", "references": [{"authors": ["I. Teinemaa", "M. Dumas", "M.L. Rosa", "F.M. Maggi"], "title": "Outcome-oriented predictive process monitoring: Review and benchmark", "venue": "TKDD 13(2)", "year": 2019}, {"authors": ["J. Evermann", "J. Rehse", "P. Fettke"], "title": "Predicting process behaviour using deep learning", "venue": "Decision Support Systems 100", "year": 2017}, {"authors": ["I. Verenich", "M. Dumas", "M.L. Rosa", "F.M. Maggi", "I. Teinemaa"], "title": "Survey and crossbenchmark comparison of remaining time prediction methods in business process monitoring", "venue": "ACM TIST 10(4)", "year": 2019}, {"authors": ["H Lakkaraju"], "title": "Faithful and customizable explanations of black box models", "venue": "Proceedings of the 2019 AAAI Conference on AIES 2019.", "year": 2019}, {"authors": ["C. Rudin"], "title": "Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead", "venue": "Nature Machine Intelligence 1(5)", "year": 2019}, {"authors": ["R Guidotti"], "title": "A survey of methods for explaining black box models", "venue": "ACM Comput. Surv. 51(5)", "year": 2018}, {"authors": ["J. Rehse", "N. Mehdiyev", "P. Fettke"], "title": "Towards explainable process predictions for industry 4.0 in the dfki-smart-lego-factory", "year": 2019}, {"authors": ["Z.C. Lipton"], "title": "The mythos of model interpretability", "venue": "CACM 61(10)", "year": 2018}, {"authors": ["C. Molnar"], "title": "Interpretable Machine Learning: A Guide for Making Black Box Models Explainable", "venue": "Leanpub", "year": 2018}, {"authors": ["J.H. Friedman"], "title": "Greedy function approximation: A gradient boosting machine", "venue": "The Annals of Statistics 29(5)", "year": 2001}, {"authors": ["M.T. Ribeiro", "S. Singh", "C. Guestrin"], "title": "why should I trust you?\u201d: Explaining the predictions of any classifier", "venue": "Procs. of the 22nd ACM SIGKDD.", "year": 2016}, {"authors": ["S. Kaufman", "S. Rosset", "C. Perlich"], "title": "Leakage in data mining: Formulation, detection, and avoidance", "venue": "Procs. of the 17th ACM SIGKDD.", "year": 2011}], "sections": [{"text": "Keywords: predictive process analytics \u00b7 interpretable machine learning \u00b7 prediction explanation"}, {"heading": "1 Introduction", "text": "Modern predictive analytics underpinned by machine learning techniques has become a key enabler to the automation of data-driven decision making. In the context of business process management, predictive process analytics is a relatively new discipline that aims at predicting future observations of a business process by learning from event log data that capture the process execution history. A vast majority of work over the past decade has used supervised machine learning algorithms to construct predictive models for predicting outcomes of a business process instance (or a case) [1], the next activity in a case [2], or the remaining time for a case to complete [3]. Evaluation of a predictive model has so far been assessed in terms of the quality of the learning model and thus evaluated using conventional metrics in machine learning (such as accuracy, precision, recall, F1 score). ar X\niv :1\n91 2.\n10 55\n8v 3\n[ cs\n.L G\n] 8\nJ un\n2 02\n0\nAs an important branch of state-of-the-art data analytics, predictive process analytics is also faced with a challenge in regard to the lack of explanation to the reasoning and outcome of its predictive models. While more and more complex machine learning techniques (and more recently, deep learning techniques) are used to build advanced predictive capabilities in process analytics, they are often applied and recognised as a \u2018black-box\u2019.\nThe recent body of literature in machine learning has emphasised the need to understand and trust the predictions (e.g., [4,5]). This has led to an increasing interest in the research community on interpretable machine learning [6]. \u201cTo interpret means to give or provide the meaning or to explain and present in understandable terms\u201d [6]. Having an interpretable or explainable model is a necessary step towards obtaining a good level of understanding about the rationale of the underlying \u2018black-box\u2019 machinery.\nIn this paper, we derive interpretation of the predictive models trained with various input features representations of the event logs. We review the techniques that have been evaluated in the benchmark studies on business process monitoring benchmarks for predicting process outcomes and remaining time. By applying interpretable machine learning techniques to two existing benchmarks [1,3], we derive global explanations that present the behaviour of the entire predictive model as well as local explanations describing a particular prediction. These explanations are useful for reviewing the suitability of a model when predicting process behaviour as well as for understanding the importance and relevance of certain features used for prediction. Findings drawn from this work are expected to motivate the need to incorporate interpretability in predictive process analytics. To the best of our knowledge, the closest study to this work is an illustration of the potential of explainable models for a manufacturing business process [7].\nThe rest of the paper is structured as follows. Sect. 2 provides necessary background information on predictive process monitoring benchmarks and interpretability of predictive models. Sect. 3 presents a detailed review of two existing predictive process monitoring benchmarks by interpreting predictive models in several methods. Sect. 4 discusses and summarizes several findings drawn from our analyses. Finally, Sect. 5 concludes the paper."}, {"heading": "2 Background", "text": ""}, {"heading": "2.1 Predictive Process Monitoring Benchmarks", "text": "In this work, two studies [1,3] that evaluate various techniques used in the context of predictive process monitoring are considered. Existing studies predicting future behaviour of business process mainly apply supervised machine learning algorithms. A supervised learning algorithm uses a corpus of input, output pairs (or training data) to learn a hypothesis that can predict the output for a new or unseen input (test data). The input and output are derived from the event logs. The output in the two benchmarks are: 1) an outcome of a case defined by using a labelling function, and 2) the remaining time for a case to complete.\nPredictive Process Monitoring Approach. An overall approach of predictive analytics used in the context of process monitoring is illustrated in Fig. 1. More specifically, a trace \u03c3 is a sequence of events of the same case. During the training phase, prefixes are generated for each trace. A prefix function prefix (\u03c3, l) takes as input a trace \u03c3 and a prefix of length l, and returns the first l events of the trace. The prefixes are grouped into buckets based on their similarities (such as length, process states, or events) using a bucketing mechanism. The prefixes in each bucket are encoded as feature vectors following an encoding mechanism. The buckets of feature vectors are then used to train a predictive model underpinned by a (machine) learning algorithm. Since the future state for each case is known from the training data, pairs of the encoded prefix and the future state are used to train a predictive model for each bucket.\nEvaluation Measures. Existing predictive process monitoring methods are built on different combinations of bucketing mechanisms, encoding mechanisms, and learning algorithms. The two predictive process monitoring benchmarks [1,3] evaluate these methods using the following quality measures:\n\u2013 Accuracy : for outcome-oriented prediction, this is measured by the area under the ROC curve (AUC) metric; and for remaining time prediction, this is measured by the mean absolute error (MAE) metric. Higher the AUC, better the model is at predicting and distinguishing the outcomes. MAE for remaining time prediction is the absolute difference between the actual remaining time and predicted remaining time and a lower MAE indicates better performance of the predictive model. \u2013 Earliness: in both predictions, this is defined as the smallest prefix length with the desired level of accuracy."}, {"heading": "2.2 Interpretability of Predictive Models", "text": "In recent years, the topic of interpretable or explainable machine learning has gained attention. To avoid ambiguity, we apply the terms interpretability and explainability as discussed in the machine learning literature [6,8]. Model interpretability can be addressed by having intepretable models and/or providing post hoc interpretations. An interpretable model [8], is able to provide transparency\nat the level of entire model (simulatability), the level of individual components (decomposability), and the level of the learning algorithm (algorithmic transparency). For example, both linear regression models and decision tree models are interpretable models, while neural network models are complex and hard to follow and hence have low transparency.\nAnother distinct approach to address model interpretability is via post hoc interpretation. Here, explanations and visualisations are extracted from a learned model, that is, after the model has been trained, and hence are model agnostic. Existing techniques can be divided into two categories: partial dependence models and surrogate models. More specially, surrogate models use the input data and a black box model (i.e., a trained machine learning model) and emulate the black box model. In other words, they are approximation models that use interpretable models to approximate the predictions of a black box model, enabling a decision-maker to draw conclusions and interpretations about the black box [9]. Interpretable machine learning algorithms, such as linear regression and decision trees are used to learn a function using the predictions of the black box model. This means that this regression or decision tree will learn both well classified examples and misclassified ones. Measures such as mean squared error are used to assess how close the predictions of the surrogate model approximate the black box. As a result, the explanations derived from the surrogate model reflect a local and linear representation of the black box model. In more detail, the general algorithm for surrogate models is presented in Algorithm 1 (from [9]).\nAlgorithm 1 General algorithm for surrogate models [9] Require: Dataset X used to train black box, Prediction model M Ensure: Interpretable Surrogate model I\n1: Get the predictions for the selected X , using the black box model M 2: Select an interpretable model: linear model, regression tree, ... 3: Train interpretable model on X , obtaining model I 4: Get predictions of interpretable model I for X 5: Measure the approximation of interpretable model I with the black box modelM 6: return Interpretation of I"}, {"heading": "3 Interpreting Predictive Models for Process Monitoring", "text": "In this section, we aim to interpret and gain deeper insights into the predictive models used for predicting process outcome and remaining time. In the context of the two existing process monitoring benchmarks, we first derive global and local explanations of selected predictive models using model interpretation techniques, and then conduct several analyses of the derived interpretations to draw interesting findings about these predictive models. Our detailed analysis, as well as the source code, is available at https://git.io/Je186 (for interpreting process outcome prediction) and https://git.io/Je1XZ (for interpreting process remaining time prediction)."}, {"heading": "3.1 Design and Configuration", "text": "Combinations of various bucketing, encoding, and supervised learning algorithms have been evaluated for predicting process outcome [1] and remaining time [3], respectively. In this study, we decide to choose the following techniques, because the methods built upon a combination of these techniques have better performance (i.e. high AUC values or low MAE values) as compared to others according to the benchmark evaluation [1,3].\nBucketing techniques: i) single bucket, where all prefixes of traces are considered in a single bucket, and a single classifier is trained; and ii) prefix length bucket, where each bucket contains partial traces of a specific length, and one classifier is trained for each possible prefix length.\nEncoding techniques: i) aggregation encoding, where the trace in each bucket is transformed by considering (only) the frequencies of event attributes such as activity, resource and computing four features for the numeric event attributes (max, mean, sum and standard deviation), and note that this way the order of the events in a trace is ignored; ii) index encoding, where each event attribute (e.g., activity, originator/resource) of an executed event can be represented as a feature, and this way the order of a trace in each bucket can be maintained by encoding each event in the trace at a given index; and iii) static encoding, where the trace (or case) attributes that remain the same through out the trace is added as a feature as-is. The categorical attributes are one-hot encoded, where each categorical attribute value is represented as a feature that takes a binary value of 0 or 1. The encoding methods are summarised in Table 1.\nMachine learning algorithms: We choose Gradient boosted trees [10] (specifically XGBoost), which is used in both benchmarks and outperformed the other machine learning techniques (e.g., random forest, support vector machines, logistic regression). Note that this study focuses on machine learning algorithms and hence Long Short Term Memory (LSTM) [3], which is used for predicting the remaining time of running cases, is not considered.\nInterpretation techniques: To make sure that the output and performance of a predictive method being studied remain intact, we apply post-hoc interpretation to derive explanations for the predictive methods built upon the above techniques and algorithms in both benchmarks. We choose a representative local surrogate method, known as Local Interpretable Model-Agnostic Explanations (LIME) [11],\nwhich can explain the predictions of any classification or regression algorithm, by approximating it locally with a linear interpretable model. We use LIME to generate local explanations useful in interpreting the prediction for a particular trace. In addition, we also conduct permutation feature importance measurement supported by gradient boosted trees to gain certain global explanations about a predictive method. More specifically, the feature importance value generated by XGBoost is used to explain the impact of different features to the overall predictions made by a given predictive method."}, {"heading": "3.2 Datasets and Notations", "text": "We present the results on three real-life event logs that are representative of our analysis. These event logs are from the Business Process Intelligence Challenge (BPIC 20111, BPIC 20122 and BPIC 20153), and were used for performance evaluation of predictive methods in both process monitoring benchmarks [1,3]. Below are brief descriptions of each of the logs as well as certain notations to be used in the interpretations and analysis.\nBPIC 2011: The event log contains cases from the Gynaecology department of a Dutch hospital. For interpreting the outcome prediction, the outcome labelling function based on the occurrence of activity \u201chistological examination - big resectiep\u201d is used (i.e., bpic2011 4 ). In the log preprocessing, the trace for each case is cut exactly before this event occurs. For interpreting the remaining time prediction, the log is used as-is without any truncation (bpic2011 ).\nBPIC 2012: The event log contains the traces of a loan application process at a Dutch financial institution. For the outcome prediction, each trace in the log is labelled as \u201caccepted\u201d, \u201cdeclined\u201d, or \u201ccancelled\u201d (based on whether the trace contains the occurrence of activity O ACCEPTED, O DECLINED, or O CANCELLED). One of the three logs, which concerns loan acceptance, is considered for generating model explanations (bpic2012 1 ). For the remaining time prediction, three logs are generated depicting loan application, loan offers and loan processing by human workers, respectively. Explanations are presented for the model trained on the loan offers (bpic2012o).\nBPIC 2015: There are five event logs recording the traces of a permit application process at five Dutch municipalities, respectively. For the outcome prediction, the rule stating every occurrence of activity 01 HOOFD 020 is eventually followed by activity 08 AWB45 020 1 is used to label the trace as positive. Explanations are derived for one of the municipalities (bpic2015 5 ).\n1 https://data.4tu.nl/repository/uuid:d9769f3d-0ab0-4fb8-803b-0d1120ffcf54 2 https://data.4tu.nl/repository/uuid:3926db30-f712-4394-aebc-75976070e91f 3 https://data.4tu.nl/repository/uuid:31a308ef-c844-48da-948c-305d167a0ec1\nNotations: The feature representations in the graphical figures in Sect. 3.3 are to be read as follows:\n\u2013 agg [Activity]|[Resource] represents the frequency of an activity executed by a resource) in a trace via aggregation encoding. \u2013 index [Activity]|[Resource] [idx] [name] represents the index (idx) at which an activity or a resource of a given name occurs via index encoding. \u2013 static [attribute] represents the case attribute for a trace (of which the value does not change during the execution of the trace). \u2013 agg max/mean/std [Attribute] represents the descriptive statistics (maximum, mean, standard deviation) of numeric event attributes via aggregation encoding."}, {"heading": "3.3 Interpretations and Analysis", "text": "Analysis 1: Single bucket vs. prefix-length bucket. This is to compare the two different bucketing techniques in terms of their impact on predictions. The following two combinations are applied to bpic2012 1 event log for outcome prediction: i) single bucket and aggregation encoding (single agg) and ii) prefix bucket and aggregation encoding (prefix agg). According to the evaluation results in [1], overall the single agg method has better AUC and earliness values compared to the prefix agg method (refer to the AUC plots in Fig. 2(a)).\nWe analyse single agg method where a single classifier XGBoost is trained for a single bucket containing all traces of all prefix lengths. Starting with global explanations, Fig. 2(b) shows the importance of the features used by the model trained using single agg with XGBoost. The model used the occurrences of activities O SENT BACK, W Validate Request, A DECLINED, O DECLINED, A CANCELLED and A APPROVED as the top six important features. A statistical analysis of the event log data reveals that the above six activities occur in the traces at a minimum prefix length of 14 (refer to Fig. 2(c)). This means that when predicting the outcome of running traces with prefix lengths lower than 14, a single agg method will likely use zero occurrences of those six activities to make the prediction. Hence, based on global explanations, it can be inferred that for traces of lower prefix lengths (< 14), the features considered important for prediction will have zero values when using a predictive method of single agg with XGBoost.\nWe then generate local explanations for two randomly chosen traces for which the model correctly predicted the positive outcome (i.e., loan accepted). Fig. 2(d) and (e) shows local explanations for traces with prefix lengths of 5 and 25, respectively. At lower prefix length (5), while the model predicts accurately, the non-occurrence of the activity A CANCELLED influences the prediction. Use of non-occurrence of an activity to make a prediction may not be a reliable feature to use. In contrast, we observe that for higher prefix length (25), the model uses the occurrence of activity O SENT BACK as an important feature when predicting the outcome, which is a reliable feature to use as the process model discovered from the event log reveals that a loan may be accepted after its occurrence (Fig. 2(f)). Hence, based on local explanations, we can conclude that\nwhile a predictive method using a single bucket may present higher accuracy and earliness, it would not be suitable for traces with lower prefix lengths.\nWe further analyse prefix agg method where a classifier XGBoost is trained for each bucket containing traces of certain, pre-defined prefix lengths. Global and local explanations for buckets containing prefixes of length 5 and 10 are shown in Fig. 3. Global explanations indicate that the important features are resources executing the activities (Fig. 3(a) and (c)). Local explanations provide some interesting insights: lower values of the features (such as the time since last event, the time since the case started) increase the likelihood of the positive outcome (Fig. 3(b) and (d)). While a prefix agg method has generally lower\naccuracy (refer to AUC plots in Fig. 2(a)), it uses features that can be computed based on the activities executed in the running case of a given prefix length and hence would be better suited for traces with lower prefix lengths.\nAnalysis 2: Aggregation encoding vs. index encoding. This is to compare the two different feature encoding techniques in terms of their impact on predictions. Based on the findings in Analysis 1, we use prefix-length bucket and apply the following two combinations with XGBoost models to bpic2012 1 event log for outcome prediction: i) prefix bucket and aggregation encoding (prefix agg) and ii) prefix bucket and index encoding (prefix index ). Both models are of similar accuracy according to the AUC plots shown in Fig. 2(a).\nFig. 4(a) and (c) illustrate the global explanations of the model trained at distinct prefix lengths (5 and 10) using index encoding, which show that the model used activity or resource information about the prior events. The features used by the model are similar to the aggregation encoding for prefix-length buckets (refer to Fig. 3(a) and (c)). The models trained on lower prefix lengths (e.g., 5) use information about the resources executing activities at certain indexes (temporally ordered).\nWhile both prefix agg and prefix index use information of the resources, the resource values used by the models are different. In this scenario, we are unable to conclude which model provides better explanations limited by our knowledge of the business process. However, since the index encoding uses attributes at each index, the number of features can be very high for processes that have\nlarge number of activities, resources and data attributes. The accuracy of the model could deteriorate in those situations as observed in the benchmark results presented in [1]. Hence, aggregation encoding would be considered more suitable when dealing with business processes with large number of activities, resources and data attributes.\nAnalysis 3: One-hot data encoding. In both process outcome and remaining time predictions, the aggregation and index encoding techniques further apply one-hot data encoding (or, one-hot encoding) to represent the activities, resources and other categorical data from the event log to feature vectors as input for machine learning models. The purpose of this analysis is to understand the impact of one-hot encoding on the predictive methods being studied.\nIn principle, one-hot encoding increases the size of a dataset exponentially, because each attribute value of a feature becomes a new feature by itself with the possible value of 0 or 1. For instance, a feature F with three attributes values f1, f2 and f3 will be represented as three new binary features. This data encoding method generates very sparse datasets, which impacts negatively both the performance metrics of the prediction model and the ability to generate interpretations. Below we discuss the findings that were obtained from analysing the explanations derived from applying single agg with XGBoost model to process remaining time prediction using bpic2011 event log as an example.\nFig. 5 depicts certain impact of one-hot encoding in the context of bpic2011 log. The original dataset increased from approximately 20 features to 823 features with this representation (see a snapshot of feature matrix in Fig. 5(a)). Further, a majority of the local explanations can be represented as the follows: if feature X is absent (value \u2264 0), then it influences the remaining time prediction. When a dataset is so sparse, it is reasonable to expect such type of explanations. However, the question that may arise from this finding is: To what extent can this provide a meaningful understanding of why the predictive model made a certain prediction?\nAnalysis 4: Feature relevance. The purpose of this analysis is to reason the relevance of the features identified important for process prediction. Feature importance in global explanations and feature impact to predictions of traces in local explanations are valuable inputs to interpreting feature relevance. To derive such interpretation a good understanding of the business process is often needed. Below, we discuss two examples of applying single agg with XGBoost to remaining time prediction using bpic2011 and bpic2012o event logs, respectively.\nFor bpic2011 log, it can be observed that, from the local explanation shown in Fig. 5(c), the predictive model relies on features such as Diagnosis Treatment Combination ID, Treatment code and Diagnosis code in order to determine the remaining time of the case. For a regression problem, like time prediction, this explanation indicates that the model is relying mostly on static features, which are the features that do not change throughout the lifetime of a case. The usage of\nstatic features for regression suggests that the process execution does not rely on the executions of activities or cases (i.e., sequences of activities following different control-flow logics) and that the model uses attributes that do not change during the case execution when making a prediction. Another interesting observation is from the global explanation shown in Fig. 5(b), which indicates that the most significant feature for remaining time prediction is Diagnosis code = DC822. By applying statistical analysis on the event log, as depicted in Fig. 5(d), we discovered that 82% of the events associated with this diagnosis code had the feature elapsed time = 0, which means that the corresponding activity starts and ends immediately. To this end, lack of relevant knowledge about the business process limits our ability to derive further insights about the relationship between these static diagnosis codes, with 0-valued elapsed times and its relation with the remaining time of a running case of the process.\nFor bpic2012o log, the analysis leads to different observations. As shown in Fig. 6, the global explanation and the local explanation for trace of prefix length 12 indicate the resource perspective of the business process are the important features used by the predictive model and have a positive impact on the remaining time prediction. The features identified as highly relevant may be case-related (such as agg opencases), resource-related (such as agg resource), or time-dependent (such as agg elapsed time). It is worth noting that the features like agg opencases and agg elapsed time, which have a positive impact on the performance of the predictive model, are not among the data attributes of the original event log and are introduced during feature encoding. These features are known as engineered features. One potential problem with introducing engineered features is that they might contribute to the loss of interpretability of a regression model [8]. However, the features that are engineered in a meaningful way may be aligned with understanding of the business process, in which case, it is likely that they may be interpretable given relevant process knowledge.\nAnalysis 5: Data leakage. We also investigate the predictive models trained for outcome prediction of bpic2015 5 event log by using single agg and prefix agg, respectively, with XGBoost. Both models have a high accuracy and hence are of interest for deriving interpretations.\nFig. 7(a) depicts the global explanation for the model using single agg method, which indicates the occurrences of activities 08 AWB45 010, 08 AWB45 020 2 and 08 AWB45 020 1 are three of the important features for outcome prediction. However, the occurrence of 08 AWB45 020 1 is the outcome to be predicted (as described in Sect. 3.2). Statistical analysis of the event log shows: i) activity 08 AWB45 020 2 is executed after 08 AWB45 020 1 in 68% of the cases, and ii) activity 08 AWB45 010 occurs at the same time as 08 AWB45 020 1 in 50% of the cases. Further analysis also reveals that activity 01 HOOFD 0204 occurs only once in all cases. All these observations reveal that the predictive model of single agg with XGBoost exhibits a problem of data leakage [12], \u201cwhere information about the label of prediction that should not legitimately be available is present in the input\u201d. The features that occur along with or after the activity used as the label influence the model predictions.\nSimilarly, Fig. 7(b) depicts the global explanation of bucket length 10 when using the model of prefix agg method with XGBoost, from which we observe activity 08 AWB45 020 2 as an important feature used for prediction. This also reveals data leakage as 08 AWB45 020 2 occurs after 08 AWB45 020 1. In both scenarios, model explanations along with the knowledge of the business process can be used to identify potential issues with a predictive model."}, {"heading": "4 Discussions", "text": "In this section, we synthesise the findings based on our analyses of interpretations in the previous section. The interpretations were derived from selected predictive methods used in the two predictive process monitoring benchmarks [3,1], and in summary, they were used to: (i) analyse the impact of two different bucketing techniques on a predictive model (Analysis 1); (ii) analyse the impact of two different feature encoding techniques on a predictive model (Analysis 2); (iii)\n4 As described in Sect. 3.2, activity 01 HOOFD 020 is expected to occur before activity 08 AWB45 020 1 as part of the temporal rule for labelling process outcome for bpic2015 5 event log [1].\nreveal the impact of one-hot data encoding on a predictive model (Analysis 3); (iv) reason the relevance of features identified important to predictions (Analysis 4); and (v) identify potential data leakage during a prediction (Analysis 5). Below, we discuss several findings, which can be drawn from these analyses, to address the usefulness of model interpretations as well as challenges posed by support to generating model interpretations.\nFinding 1: Model interpretations are useful in choosing a suitable prediction method. We have observed that while certain bucketing and encoding techniques result in higher accuracy, there is a need to derive interpretations from the model and analyse the features used by the model. Model interpretations can be used to review the suitability of different bucketing and encoding mechanisms for predictive models, and also to avoid potential issues (e.g., data leakage) that may incur to predictive models. Hence, model interpretations are a valuable input for deciding on a predictive method to use, while so far such a decision is often made by relying on performance measures only.\nFinding 2: Model interpretations with domain knowledge enable the understanding of the relevance of features used for predictions. We have observed that model interpretations make it possible to reason about the relevance of features used for predictions, including which certain perspectives of a business process (control-flow, resource, data, time) were used by a predictive model. To gain a deeper understanding of interpretations, the domain knowledge of the relevant business process is also necessary.\nFinding 3: Model interpretations can help improve the interpretability of predictive models. As we have also learned from our analyses, the majority of the encoding methods have their advantages and challenges in what concerns the accurate representation of features extracted from event logs capturing business process execution. For example, the aggregation encoding is accurate but abstracts information from the process that could be valuable from an interpretability point of view. This leads to the challenge and dilemma of representing event log data in a way that it is interpretable and accurate. Our analysis also showed that the one-hot-encoding technique generated very sparse feature dimensions, which impacted negatively the explainability of a prediction method. Finally, model interpretations can help reveal engineered features, which are an important input for minimising the incorporation engineered features that may add complexity to the event log or limit the interpretability of a predictive model.\nFinding 4: Providing business insights from model interpretations. The goal of predictive process monitoring is to provide business users with interesting and useful insights about the underlying business executions. As we have learned from our analyses of model interpretations, relying on performance measures is not adequate to guarantee a good predictive model. It would be useful to a business user if a predictive model can provide additional insights into why a particular prediction was made. Providing such an insight is, however, a very challenging task, and remains as an open research question in the scientific community [5]."}, {"heading": "5 Conclusions", "text": "In this paper, we have reviewed two existing benchmarks in predictive process monitoring, and presented model interpretations as examples to demonstrate that it is not enough to judge predictive methods by solely relying on their performance measures. Our analyses indicate that accurate models would require deriving interpretations for using the right set of predictive models. Findings drawn from our analyses indicate the need and benefits for using model interpretations, such as for identifying suitable encoding and bucketing techniques, revealing the importance of features and reasoning the relevance of features used for predictions, detecting potential occurrences of data leakage, etc. Hence, we suggest to incorporate interpretability in addition to the evaluation of predictive models using conventional performance measures (such as accuracy). As a first step into future work, it is important to develop a systematic approach for incorporating model interpretability in predictive process analytics.\nAcknowledgement: We particularly thank the authors of the two process monitoring benchmarks [1,3] for the high quality code they released which allowed us to explore model interpretability for predictive process analytics."}], "title": "Exploring Interpretability for Predictive Process Analytics", "year": 2020}