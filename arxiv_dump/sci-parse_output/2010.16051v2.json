{"abstractText": "In this paper we show a complete process for unsupervised anomaly detection for the average fuel consumption of fleet vehicles that is able to explain what variables are affecting the consumption in terms of feature relevance. For doing that, we combine the anomaly detection with a surrogate model that is able to provide that feature relevance. For this part, we evaluate both whitebox models from the literature, as well as novel variations over them, and blackbox models combined with local posthoc feature relevance techniques. The evaluation is done using real IoT data belonging to Telef\u00f3nica, and is measured both in terms of model performance, as well as using Explainable AI metrics that compare the explanations generated in terms representativeness, fidelity, stability and contrastiveness. The explanations generate counterfactual recommendations that show what could have been done to reduce the average fuel consumption of a vehicle and turn it into an inlier. The procedure is combined with domain knowledge expressed in business rules, and is able to adequate the type of explanations depending on the target user profile.", "authors": [{"affiliations": [], "name": "Alberto Barbado"}], "id": "SP:4e6c61ce44f50fb6d70f54dfdf8d1984daf0115f", "references": [{"authors": ["Alejandro Barredo Arrieta"], "title": "Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI", "venue": "Information Fusion", "year": 2020}, {"authors": ["Richard Benjamins", "Alberto Barbado", "Daniel Sierra"], "title": "Responsible AI by Design. 2019", "venue": "[cs.CY]. URL: https://arxiv.org/abs/1909", "year": 1909}, {"authors": ["Vanessa Gironda Aquize", "Eduardo Emery", "Fernando Buarque de Lima Neto"], "title": "Self-organizing maps for anomaly detection in fuel consumption. Case study: Illegal fuel storage in Bolivia", "venue": "IEEE Latin American Conference on Computational Intelligence (LA-CCI). IEEE", "year": 2017}, {"authors": ["Mingming Zhang"], "title": "SafeDrive: online driving anomaly detection from large-scale vehicle data", "venue": "IEEE Transactions on Industrial Informatics", "year": 2017}, {"authors": ["Christoph Molnar"], "title": "Interpretable machine learning. Lulu.com, 2019", "venue": "URL: https://christophm.github.io/ interpretable-ml-book/", "year": 2019}, {"authors": ["Marco Tulio Ribeiro", "Sameer Singh", "Carlos Guestrin"], "title": " Why should I trust you?\u201d Explaining the predictions of any classifier", "venue": "Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining", "year": 2016}, {"authors": ["Scott M Lundberg", "Su-In Lee"], "title": "A unified approach to interpreting model predictions", "venue": "Advances in neural information processing systems", "year": 2017}, {"authors": ["Lloyd S Shapley"], "title": "A value for n-person games", "venue": "Contributions to the Theory of Games", "year": 1953}, {"authors": ["Scott M Lundberg", "Gabriel G Erion", "Su-In Lee"], "title": "Consistent individualized feature attribution for tree ensembles", "year": 2018}, {"authors": ["Shubham Rathi"], "title": "Generating counterfactual and contrastive explanations using SHAP", "venue": "arXiv preprint arXiv:1906.09293", "year": 2019}, {"authors": ["Harsha Nori"], "title": "InterpretML: A Unified Framework for Machine Learning Interpretability", "venue": "arXiv preprint arXiv:1909.09223", "year": 2019}, {"authors": ["Trevor Hastie", "Robert Tibshirani"], "title": "Generalized additive models: some applications", "venue": "Journal of the American Statistical Association", "year": 1987}, {"authors": ["Yin Lou"], "title": "Accurate intelligible models with pairwise interactions", "venue": "Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining", "year": 2013}, {"authors": ["Diogo V Carvalho", "Eduardo M Pereira", "Jaime S Cardoso"], "title": "Machine Learning Interpretability: A Survey on Methods and Metrics", "year": 2019}, {"authors": ["Robert R Hoffman"], "title": "Metrics for explainable AI: Challenges and prospects", "year": 2018}, {"authors": ["David Alvarez Melis", "Tommi Jaakkola"], "title": "Towards robust interpretability with self-explaining neural networks", "venue": "Advances in Neural Information Processing Systems", "year": 2018}, {"authors": ["Alberto Barbado", "\u00d3scar Corcho", "Richard Benjamins"], "title": "Rule Extraction in Unsupervised Anomaly Detection for Model Explainability: Application to OneClass SVM", "year": 1911}, {"authors": ["Hui Zou", "Trevor Hastie"], "title": "Regularization and variable selection via the elastic net", "venue": "Journal of the royal statistical society: series B (statistical methodology)", "year": 2005}, {"authors": ["Tianqi Chen", "Carlos Guestrin"], "title": "Xgboost: A scalable tree boosting system", "venue": "Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining", "year": 2016}, {"authors": ["Guolin Ke"], "title": "Lightgbm: A highly efficient gradient boosting decision tree", "venue": "Advances in neural information processing systems", "year": 2017}, {"authors": ["Salwa Waeto", "Khanchit Chuarkham", "Arthit Intarasit"], "title": "Forecasting time series movement direction with hybrid methodology", "venue": "Journal of Probability and Statistics", "year": 2017}, {"authors": ["Frank Wilcoxon"], "title": "Individual comparisons by ranking methods", "venue": "Breakthroughs in statistics. Springer,", "year": 1992}, {"authors": ["Martin Nemeth", "Dmitrii Borkin", "German Michalconok"], "title": "The Comparison of Machine-Learning Methods XGBoost and LightGBM to Predict Energy Development", "venue": "Proceedings of the Computational Methods in Systems and Software. Springer", "year": 2019}, {"authors": ["Andreea Anghel"], "title": "Benchmarking and Optimization of Gradient Boosting Decision Tree Algorithms", "year": 2018}, {"authors": ["Joseph F Hair Jr."], "title": "A primer on partial least squares structural equation modeling (PLS-SEM)", "venue": "Sage publications,", "year": 2016}, {"authors": ["Joseph F Hair", "Christian M Ringle", "Marko Sarstedt"], "title": "Partial least squares structural equation modeling: Rigorous applications, better results and higher acceptance", "venue": "Long range planning", "year": 2013}, {"authors": ["Wynne W Chin", "Peter R Newsted"], "title": "Structural equation modeling analysis with small samples using partial least squares", "year": 1999}, {"authors": ["Arnaud De Myttenaere"], "title": "Mean absolute percentage error for regression models", "venue": "In: Neurocomputing", "year": 2016}, {"authors": ["Colin David Lewis"], "title": "Industrial and business forecasting methods: A practical guide to exponential smoothing and curve fitting", "year": 1982}, {"authors": ["F. Pedregosa"], "title": "Scikit-learn: Machine Learning in Python", "venue": "Journal of Machine Learning Research", "year": 2011}, {"authors": ["Scott M. Lundberg"], "title": "From local explanations to global understanding with explainable AI for trees", "venue": "Nature Machine Intelligence", "year": 2020}, {"authors": ["Shane T Mueller"], "title": "Explanation in Human-AI Systems: A Literature Meta-Review, Synopsis of Key Ideas and Publications, and Bibliography for Explainable AI", "year": 1902}, {"authors": ["Fei Tony Liu", "Kai Ming Ting", "Zhi-Hua Zhou"], "title": "Isolation forest", "venue": "Eighth IEEE International Conference on Data Mining. IEEE", "year": 2008}, {"authors": ["Saket Sathe", "Charu Aggarwal"], "title": "LODES: Local density meets spectral outlier detection", "venue": "Proceedings of the 2016 SIAM International Conference on Data Mining. SIAM", "year": 2016}, {"authors": ["Markus M Breunig"], "title": "LOF: identifying densitybased local outliers", "venue": "ACM sigmod record", "year": 2000}, {"authors": ["Vijay Arya"], "title": "One explanation does not fit all: A toolkit and taxonomy of ai explainability techniques", "year": 1909}, {"authors": ["Rich Caruana"], "title": "Intelligible models for healthcare: Predicting pneumonia risk and hospital 30day readmission", "venue": "Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM", "year": 2015}, {"authors": ["Harmanpreet Kaur"], "title": "Interpreting Interpretability: Understanding Data Scientists\u2019 Use of Interpretability Tools for Machine Learning", "venue": "Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems", "year": 2020}, {"authors": ["Chun-Hao Chang"], "title": "How Interpretable and Trustworthy are GAMs?", "venue": "arXiv preprint arXiv:2006.06466", "year": 2020}], "sections": [{"text": "Keywords XAI. Fuel Consumption. Anomaly Detection. Explainable Boosting Machines. Feature Relevance. IoT."}, {"heading": "1. Introduction", "text": "Combining Advanced Analytics techniques together with IoT (Internet of Things) data offers many possibilities to find and extract relevant insights for business decisions. At Telefo\u0301nica, for instance, we see how the union of Machine Learning (ML) with IoT data helps to create new use cases for Fleet Management Industry. An example of it is the usage of ML for anomaly detection of the average fuel consumption of vehicles. For a fleet manager, it is very useful to be able to find which vehicles are having an abnormal fuel consumption, since it is crucial for optimizing costs.\nHowever, detecting which vehicles have an anomalous average fuel consumption alone is not enough. Only providing that information leads to more questions than answers. Why are the vehicles consuming that extra amount of fuel? How could it be reduced?. These questions are not answered by a\nCopyright \u00a9 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nbinary output that indicates which consumptions are anomalous and which ones are not.\nThis is one of the reasons why Explainable AI (XAI) is so critical: it enhances that initial information with different types of explanations, helping to answer those questions that may arise. In fact, XAI is one of the core elements of Responsible Artificial Intelligence (RAI) [1], which is a fundamental part of Telefo\u0301nica\u2019s AI principles [2].\nNonetheless, XAI is still an emerging field with many uncharted or relatively new territories. For instance, how do we now that the explanations generated are good? How do we compare XAI quantitatively in order to find the one that provides better explanations? These questions address the importance of metrics for XAI for measuring the understandability of explanations.\nTogether with those questions, another issue is the following one: Do the explanations adapt to the user profile? Are they adjusted in such a way that the target audience finds them clear and useful enough?\nAlso, even though explanations themselves are useful, there is always a caveat present: What happens when explanations contradict apriori knowledge of a field? How do we ensure that apriori knowledge and explanations are aligned?. Though, regarding the first question, it may be possible that explanations differ from domain knowledge either because it is wrong or because it may complement it, in many cases the important question is the second one: ensuring alignment between apriori knowledge and explanations.\nFinally, even considering good understandable explanations that are aligned with domain knowledge and that are expressed in an understandable way for their audience, there are still questions unanswered. For example, what shall we do about it? The prescriptive dimension also arises, remarking the importance of not only providing insights, but also suggesting possible actions to further help the decision maker.\nTaking all these questions in consideration, in these paper we will propose a complete process to address the business need of not only detecting anomalies within the average fuel consumption of a fleet of vehicles, but also explaining what causes them. This process will include how to adjust the explanations to be understandable by its audience, how to in-\nar X\niv :2\n01 0.\n16 05\n1v 2\n[ cs\n.L G\n] 5\nN ov\n2 02\nclude business rules in order to ensure that they are aligned with domain knowledge, and how to provide counterfactual recommendations on what may be done to reduce the average fuel consumption of outliers in order to turn them to inliers.\nWe will analyse how to generate these explanations for unsupervised anomaly detection using surrogate models. These models help to find the feature relevance relationship between input features and a target one related to the output of the unsupervised anomaly detection. Among these surrogate models, we will use both blackbox models (XGBoost and LightGBM) together with posthoc local explanations XAI techniques for feature relevance (SHAP and LIME), and also whitebox models both classical (ElasticNet) and novel ones (Explainable Boosting Machines, EBM). Additionally, we will propose and analyse a modification over the standard EBM that takes into account, both for explanations and for predictions, differences that may exist within different subgroups of vehicles.\nFollowing this, the main contributions of our work are: \u2022 Proposing a complete process for unsupervised anomaly\ndetection in the average fuel consumption of the vehicles of a fleet that includes: choosing relevant features, detecting outliers in an unsupervised manner, generating explanations of what affects the average fuel consumption of outliers, aligning those explanations to business rules, generating recommendations with them proposing what may be done to turn outliers to inliers, and adjusting them to be easily understandable for their target audience, considering two different user profiles that could benefit from them.\n\u2022 Comparing blackbox models with local posthoc XAI techniques against EBM using Telefo\u0301nica\u2019s real-world IoT industry data in order to see if a whitebox model (EBM) could match the blackbox one plus posthoc solution within our use case. This comparison will be done at two complementary levels.\n\u2022 First, we will compare if the predictive power of EBM as a surrogate model could match the one obtained with other reliable boosting models (XGBoost and LightGBM).\n\u2022 Second, we will compare if the local explanations provided directly by EBM are similar to those obtained by combining the blackbox models with local posthoc XAI techniques for feature relevance (LIME and SHAP). For that, we will propose a set of metrics useful for quantifying and comparing explanations from different perspectives: representativeness, fidelity, stability and contrastiveness.\n\u2022 Finally, along with EBM, we will analyse the aforementioned metrics using a variation of the standard EBM that takes into account a set of categorical features in order to adjust the predictions and features importance. The rest of the paper is organized as follows. First, we describe some related work in the area of anomaly detection for fuel consumption, together with previous works regarding XAI for feature relevance explanations. This Chap-\nter will also mention other works regarding the combination of explanations with domain knowledge, as well as some of the research conducted regarding metrics for measuring explainability. Then, we describe the different steps of our process proposal, including the metrics for both comparing the different XAI solutions as well as measuring the understandability of the explanations in themselves. We will also include at this point what business rules are we considering for expressing the domain knowledge and how we combine them with the explanations. Following this, we present an empirical evaluation and comparison using real-world IoT data. We then conclude, showing also potential future research lines of work."}, {"heading": "2. Related Work", "text": ""}, {"heading": "2.1. Anomaly detection for fuel consumption", "text": "The detection of anomalous fuel consumption vehicles from a fleet is present at different research works within the literature. In [3], the authors show how to detect fuel anomalies using unsupervised algorithms (Self-Organizing Maps, SOM). The authors aim to find fuel fraud situations within fleet vehicle data at Bolivia (using a data set of 1000 vehicles with 190627 data points). These situations are normally linked to high fuel purchases within a short period of time. They effectively show how to find clusters within the space of the SOM in order to identify fuel anomalies and detect fraudulent scenarios by evaluating their proposal over a test set. As the authors mention, there are many features that can be used in order to contextualize the fuel consumption (p.e. the normal monthly consumption of the vehicle, the behaviour of other vehicles of the same subgroup...). Their proposal leads only to an output that identifies anomalies, but it could be greatly enhanced with XAI techniques that provides additional insights on what contextual features are relevant for that high fuel consumption.\nFuel fraud is not the only case of possible fuel anomalies within a fleet. As researched at [4], driving behaviour may also lead to an increased fuel consumption. Within driving behaviour variables they mention several features, such as RPM speed, acceleration (both forward, and negative from braking), over speed or gear position.\nEven though the previous literature includes researches related to the detection of anomalous fuel consumption (both from fraud scenarios and from contextual variables), to the best of our knowledge there are no previous works regarding the explanation of those anomalies using XAI techniques."}, {"heading": "2.2. Surrogate method for feature relevance", "text": "From among the different outputs than a post-hoc XAI technique can provide, one of them is in terms of feature relevance [5, 1]. This output quantifies the individual contribution of each training feature to the target variable. Regarding specifically the case of local explanations, LIME (Local interpretable model-agnostic explanations) [6] is a widely known solution. It approximates the decision frontier of the underlying model through an algorithmic transparent model (p.e. a Linear Regression) trained over artificially generated\nneighbours data points, in order to indicate the relative contribution of each feature to the prediction. LIME, then, fits an independent model for each data point that are going to be explained. Due to this, a particular feature value could have different feature relevance values depending on the data point considered. Also, since the models are independent, each one of them will have its own intercept value.\nAnother well-known XAI technique for feature relevance outputs and local explanations is SHAP (SHAPley Additive exPlanations) [7]. SHAP is based on the concept of SHAPely values [8], rooted in the field of game theory. SHAPely values consider each feature (or set of features) as \u201dplayers\u201d within a game, where the \u201dgain\u201d is the difference between the predicted target feature value and the average value of the target feature. Thus, it distributes that \u201dgain\u201d among the \u201dplayers\u201d depending on their contribution. Following the idea of SHAPely values, SHAP brings it forward, and represents the explanations through an additive feature attribution method (a Linear Model).\nThere are different algorithms alternatives for SHAP. The first proposal is known as Kernel SHAP. The literature, however, saw some disadvantages with this proposal, mainly that its computation time was too high, and that it ignores possible feature dependence (p.e. correlations), like most permutation-based methods [5]. These limitations are solved with other SHAP algorithms, like Tree-SHAP [9]. TreeSHAP was conceived as an alternative for tree-based models, and in facts shortens the computation time, as well as takes into account existing dependences by modelling the conditional expected prediction. However, it has been noted that Tree-SHAP may yield unintuitive feature attributions [5].\nRegardless of the SHAP algorithm considered from those two options, the output can be expressed through a pairwise plot, where each of the data points feature values is represented, along with their corresponding feature relevance. Thus, as it happens with LIME, a particular feature value may correspond to different feature relevances, depending on the remaining feature values at that data point. However, contrary to LIME, SHAP has common intercept for every individual data point explained.\nSHAP, as a feature relevance XAI technique, has been seen as a way to generate counterfactual explanations for binary classification algorithms. This is the case of [10], where the authors propose a method that searches for neighbours of the data point to be explained (from the same or from a different class) and compare their feature relevance in order to see what is helping to maintain the current prediction and what contributes more to change it.\nThough feature relevance posthoc XAI techniques are suitable for building explanations later on (like the aforementioned example), [1] proposes a guideline for ensuring interpretable AI models where an algorithmic transparent model should be tried before changing to a blackbox+XAI combination. The literature is advancing on the research of whitebox models that have performances on pair with complex blackbox ones, in order to contribute to the usage of models that do not need posthoc XAI techniques to understand how they took a decision. This is the case of Ex-\nplainable Boosting Machines (EBM) [11]. EBM are based on Generalized Additive Model algorithms (GAM) [12]. GAM models use an additive function, similar in structure to that of a Linear Regression, where each feature is modeled through a function that provides a feature relevance value that quantifies the individual contribution of a particular feature to the predicted value. By being modeled with a function that does not have to be linear, GAM provide the option to infer non-linear relationships, thus potentially increasing the model generalization [5]. Equation 1 shows the basic structure of that equation.\ny = \u03b20 + \u2211 n=1 fi(xi) (1)\nGAM proposal is improved by GA2M algorithm [13], the algorithm behind EBM (the difference between them is that EBM is a faster implementation of it). EBM have several improvements over the original GAM. First, the feature functions of EBM can be learned through bagging and boosting techniques. During boosting, only one feature is trained at each step (round-robin) using a very low learning rate in order to make the feature order used irrelevant. This round-robin procedure also lessens the effects of colinearity. Finally, if there are pairwise interactions between features, EBM can detect them and include them as additional terms, as shown at Equation 2 [11].\ny = \u03b20 + \u2211 n=1 fi(xi) + \u2211 n=1 fij(xi, xj) (2)"}, {"heading": "2.3. Domain knowledge combined with XAI", "text": "Within the review of [1], one of the open research challenges is combining domain knowledge with the explanations generated in order to enhance the user\u2019s understandability. The review mentions that this challenge is specially addressed through the combination of deep learning blackbox models (connectionism) together with symbolic approaches, that are algorithmic transparent and generally directly interpretable, and with domain knowledge expressed through ontologies. This is the case of [14], where the authors propose a variant of TREPAN algorithm that uses domain ontologies in the XAI phase. TREPAN uses surrogate decision trees to explain any blackbox model (model agnostic). However, as the authors highlight, many times those trees are not understandable by a final user. That is why they propose a variation on the algorithm that gathers information from a domain ontology, and uses it to prioritise using features for the splits that are more general within the ontology. The prioritisation is done by penalizing more the Information Gain value from considering a feature for the split if that feature is too specific. They have assessed their proposal with real users, and they found that indeed using domain knowledge enhances user understandability.\nWe see that the domain knowledge can indeed be applied to adjust the explanations generated, and it can be done at different moments during a ML model life cycle. It can be done at the ML model itself (for instance, finding hyperparameters that enhance more the model understandability). it can also be done, like in the aforementioned paper, during\nthe training of a posthoc XAI method. Finally, it can be also applied after the XAI method generates the explanations, in order to adjust them to the domain knowledge."}, {"heading": "2.4. Metrics for XAI", "text": "The review of [1] dedicates part of the section regarding XAI future research needs to the necessity of metrics to assess the understandability of the explanations generated. The authors propose the following definition of explainability: \u201dExplainability is defined as the ability a model has to make its functioning clearer to an audience\u201d. However, there is a lack of metrics to measure how well the explanations generated matches that definition of explainability. So, there is an open research opportunity in that area. However, regardless of the lack of metrics, they highlight some aspects that should be considered within those future metrics. Among those aspects, they mention the \u201dgoodness\u201d, \u201dusefulness\u201d and \u201dsatisfaction\u201d of the explanations, along with the improvement of the mental model of the user thanks to them. Also, it should be measured how the explanations impact the \u201dtrust\u201d and \u201dreliance\u201d by the user in the model.\nThis connects with what several authors have mentioned what properties should be measured by XAI metrics. A relevant research is [15]. There, the authors analyse the scientific literature and propose a taxonomy of properties for different explainability scenarios that depend on the use case and the audience of the explanations. The scenarios are explanations at a general level, individual explanations, and explanations that are human-friendly. Metric properties regarding general and individual explanations aim to measure the explanation\u2019s understandability regardless of the user. Humanfriendly ones take the user into account for assessing the explanations. Thus, the properties that a metric to evaluate individual explanations should have include aspects like \u201drepresentativeness\u201d (instances covered by the explanations), \u201dfidelity\u201d (how well the explanations approximate the underlying model), or \u201dstability\u201d (how similar are explanations for similar instances). In contrast, metric properties to check if explanations are human-friendly include aspects like \u201dcontrastiveness\u201d (if explanations are in the form of \u201dif you have done X instead or Y, the output would have changed from A to B\u201d) or \u201dselectivity\u201d (if explanations do not include all the causes, but only the most relevant ones).\nAnother example of taxonomies for explanation metrics is [16]. The authors also propose a split between metrics with and without considering the user. First, they refer to \u201dexplanation goodness\u201d for metrics that assess explanation understandability regarding the ML model. Within these type of metrics they include properties like \u201dprecision\u201d. However, even when a set of explanations have good metric values for \u201dexplanation goodness\u201d, they may not help the users. This is why there is a second group of metric properties for \u201dexplanation satisfaction\u201d, that include aspects such as \u201dunderstandability\u201d, \u201dcompleteness\u201d, \u201dusefulness\u201d or \u201dfeeling of satisfaction\u201d. The authors propose a set of questionaries to evaluate all these aspects within explanations.\nThough it is true that the use of questionaries is an approach to evaluate the aforementioned metric properties, one of the challenges is turning them into quantitative metrics\nfor automatically assessing the explanations generated by XAI over a ML model. This is something that the literature is already addressing. The work of [17] shows how to use quantitative metrics for measuring some of the properties mentioned before. They first consider three families of metrics, \u201dexplicitness\u201d, \u201dfaithfulness\u201d and \u201dstability\u201d. Then, they propose different algorithms to infer them, evaluating the results over different data sets.\nThe research at [18] also shows how to implement different metrics for quantitative measurement of the understandability of explanations. They use four families of metrics, \u201dcomprehensibility\u201d, \u201drepresentativeness\u201d, \u201dstability\u201d and \u201ddiversity\u201d. These metrics are calculated over local explanations used for explaining a blackbox model for anomaly detection, where the explanations considered are only focused in the outliers (in order to explain how to turn them to inliers). However, some of the metrics are \u201dexplanation specific\u201d, since they cannot be used for every type of explanations. The authors generate explanations using rule extraction techniques, and metrics like \u201ddiversity\u201d measure the degree of overlapping between the hypercubes generated. Hence, they only work for a particular type of explanations: local explanations trough rules. Other metrics, such as the ones for \u201dstability\u201d, that measure how many similar data points are classified within the same category, are \u201dexplanation agnostic\u201d since they can be easily applied for other type of explanations, such as the case of feature relevance.\nFinally, [14] also include explanations for measuring the model understandability for decision trees, in terms of number of interior nodes and number of leaves, and for measuring user understandability, through using online surveys and registering metrics different metrics. These last metrics include the response time that takes for a user to understand the decision tree, as well as if the users are able to predict the decision tree prediction for an individual data point, and their perceived understanding of the model through a rating given by them."}, {"heading": "3. Method", "text": "In this Chapter we will introduce the flowchart followed by our proposal for the dynamic generation of explanations applied to anomaly detection of average fuel consumption. We will first describe the overall process, and then we will focus on each of the main modules."}, {"heading": "3.1. Process overview", "text": "The overall process is detailed below in Figures 1 and 2, and in Figure 14 included within the Annex.\nFigure 1 offers a summarized view of the training phase of the process, using high-level descriptions for the modules. Thus, at this phase, the process will start by selecting the relevant data for training from all the raw data available, and preprocess it in order to have the FAR data frame with the structure detailed in Section 3.2. This data frame will be both used for training the ML model and for detecting the vehicle-dates combinations (data points) that have an anomalous average fuel consumption in that day.\nWe want to quantify the feature value impact for the average fuel consumption of vehicles, and explain how they affect anomalous data points (Section 3.3). The purpose is to offer both explanations of which features are affecting and how much they affect, as well as recommending changes for features that are actionable in order to change the average fuel consumption to a value that is not anomalous (counterfactual explanations). Due to this, we train a regression ML model that infers the relationship between input features and output (average fuel consumption).\nThe next step is to identify those data points that are anomalous, and provide a visual explanation using a value limit to distinguish inliers from outliers (Section 3.4). Everything, from the model, to the historical preprocessed data used for training, and the limits that classify inliers versus outliers, is stored for its usage for the explanation phase.\nFigure describes the overall modules involved for generating explanations dynamically. Selecting any period of data, the process extracts its corresponding FAR after the preprocessing module in case that period was not included within the training dataset described previously. Then, it identifies the outliers using the previous calculated limits. After it, it generates explanations for the outliers data points using the ML model and applying a posthoc XAI method in case the ML model is not a whitebox one (Section 3.5). These explanations are filtered in order to comply to specific business rules. With the final explanations, the process generates recommendations (Section 3.6) of data points that will have a change in their target variable if specific input features are changed. We only keep the recommendations that will lead to an average fuel consumption considered as inlier. Thus, the process provide the following explanations for outliers.\n\u2022 Visual explanations: Limit value for the average fuel consumption that classify a data point as inlier or outlier.\n\u2022 Feature relevance: For each outlier data point, it indicates which features affect the target and contribute to its increase. It also indicates the relative importance among them.\n\u2022 Recommendations: It shows a counterfactual explanation that indicate feature changes that will lead an outlier data point to change and become an inlier.\nThen, the process provides different metrics for the explanations and recommendations, as described in Section 3.7. Finally, at Section 3.8, the process adjust the recommendations for two user\u2019s profiles considered: technical specialists and fleet managers."}, {"heading": "3.2. Data preprocessing", "text": "Obtain daily features The first step within the preprocessing module is obtaining the daily aggregated information for each of the vehicles within the fleet. The IoT devices within the vehicles provide real-time information of the vehicle\u2019s status. However, for our proposal, we are interested in a daily vision of the vehicle. Thus, we aggregate the raw information into a set of features, described at the Annex in both Section 6.1 as well as in 2. The features chosen correspond to a business a\npriori knowledge, since they may affect a vehicle\u2019s average fuel consumption. Some of the features also appear within the literature as potential causes of increased fuel usage [4].\nAt this point in the module, the initial data aggregation will include all the features described at 2 except for the ones under the \u201dCategorical\u201d category, since these features will be obtained at another step.\nThe features are divided into 4 groups: Index, Categorical, Explainable and Target.\nIndex features refer to features used to identify each row (namely a vehicle\u2019s unique id, vehicle id, and the date, date tx).\nCategorical features refer to non numerical features used to distinguish group of vehicles (p.e. \u201dvehicle group\u201d indicates vehicles with the same make-model). As mentioned before, they will be covered later, since they are not obtained yet at this point.\nRegarding the explainable features, they are further divided into three groups. First, there are features related to the vehicle status itself. For instance, the pressure of the tyres. If the pressure is too low, the fuel consumption needed to cover the same amount of distance will increase, thus increasing the average fuel consumption of the vehicle. These features are identified in 2 as Vehicle Parameters. The next group of features are the Driving Behaviour ones. They correspond features related to the vehicle\u2019s driver behaviour itself that may affect the average fuel consumption. An example of these features is the idle time spent. More idle time may increase average fuel consumption. The last group of features considered are the Environment Variables. For instance, the exterior temperature may affect a vehicle\u2019s thermodynamic cycle harming its efficiency. Within these type of features there are also binary features (p.e. \u201dlights left on\u201d).\nThe final feature is the target column, the average fuel consumption itself. This is calculated directly as:\navg fuel consumption = trip fuel used\ntrip kms \u00d7 100 (3)\nThis yields a data frame where each row corresponds to the daily aggregated values of the selected features for a specific vehicle. Thus, we want to analyse the potential relationship between those features with the average fuel consumption of that vehicle in that day.\nIt is worth mentioning that all the features at this point are going to be positive (value above or equal to 0). In most of the features this comes naturally (p.e. harsh brake events). However, in some other it is not true, mainly for the temperature. This is the reason why we use Kelvin scale at this point instead of Celsius when the range of temperatures provided include values below 0\u00baC.\nDiscard Y null registers In some cases, the IoT devices may not provide either the fuel spent during that day (trip fuel used), the distance driven (trip kms), or both. Then, for those cases we do not have the value of the average fuel consumption (target column, also known as Y column, referring as X the remaining features used as input). Those registers are not considered and are discarded.\nEliminate non relevant data Within the initial processing of the data, non-representative vehicle-days are also eliminated, in which the distance driven is too low to be significant. A minimum threshold is defined that eliminates all vehicles that have a distance traveled less than that threshold. In addition, given that the information provided by IoT devices sometimes include erroneous data, in order to avoid including noise in the system, vehicles whose average fuel consumption is excessively high or low are eliminated within this step, taking as reference business values. Also, highly correlated numerical features are removed at this point.\nIdentify vehicle groups The following steps aim to complete the previous features obtained from the IoT devices with relevant categorical features. The categorical features include two different variables. First, a feature named vehicle group. This features classifies vehicles corresponding to their make-model. Using a vehicle\u2019s VIN (Vehicle Identification Number) we can identify their make and model, and group them accordingly. The VIn decoding procedure yields vehicle groups that do also have the same fuel type (diesel or gasoline; our data sets do not include electric or hybrid vehicles).\nIdentify route type The second feature is route type. It is used to identify the main type of route of a vehicle in a specific day. We assign a route type for each vehicle-day according to the following rules: \u2022 IF per time city \u2264 low th time AND trip kms \u2265 th kms THEN route type = hwy\n\u2022 ELSE IF per time city \u2265 low th time AND trip kms \u2264 th kms THEN route type = city\n\u2022 ELSE route type = combined Thus, we consider a \u201dcity\u201d route if the vehicle spent a minimum amount of time driving within city and if the distance driven does not exceed a specific threshold. On the contrary, to consider the route of a vehicle-day as \u201dhighway\u201d (hwy), the time spent driving within city should be lower than a threshold, and the distance driven should be above another threshold. Any other scenario is considered as \u201dcombined\u201d. This feature is important since the reference average fuel consumption of a vehicle is different depending on the route type since it impacts on other feature values (such as the average speed or the driving time).\nFill missing values In addition to sometimes not having the data related to the target variable, in some cases the IoT devices do not send information about some of the input features. In order to avoid losing excessive registers and maintain a statistically significant set of data, these values are imputed with inferred values from the rest of the fleet. Separating the data set according to its vehicle group, each missing value is assigned with the median value of that feature in order to be able to maintain the record but that the value of that variable for that vehicle-day is not significant to the model.\nThis module provides a final data frame ready to be used in the following modules. We will further address this data frame as FAR (Fleet Analytics Record).\nThe median values considered are from the historical dataset using during the training phase. Since the periods of time that are used for the explanation may be different from the historical dataset, the median values used to fill null values for explanations are the ones obtained during the training phase."}, {"heading": "3.3. Unsupervised anomaly detection", "text": "Using the previous FAR data frame, the next module is responsible for detecting the vehicle-dates where there is an anomalous average fuel consumption. Since there is no prior knowledge on when the average fuel consumption is anomalous, the module needs to detect it in an unsupervised manner. Also, the module needs to provide a threshold value to distinguish outliers from inliers, since we want to include that information as a visual explanation.\nTo comply with both requirements, we apply an univariate unsupervised anomaly detection approach using a Box-Plot that classifies data points as outliers if they are above or below the following thresholds:\nlim sup = Q3 + 1.5\u00d7 IQR lim inf = Q1\u2212 1.5\u00d7 IQR (4)\nHowever, we will only consider as outliers those vehicles above the superior threshold, thus, not considering as outliers those below it.\nThe Box-Plot will be applied over the different combinations of the categorical variables (make-model with vehicle group and route type with route type), obtaining then a different limit depending on the combination considered.\nWith that, the output of this module is the original FAR with both the limit that classifies data points as inliers or outliers, and a binary column indicating whether that data point is actually an outlier. The limits are obtained during the training phase. For the explanation phase, the limits inferred previously for each vehicle group and route type are used to identify outliers within the dataset that wants to be explained."}, {"heading": "3.4. ML model", "text": "The following module is the training of a ML supervised model that finds relationships between the explainable and categorical features from the FAR dataset and the target variable.\nSince part of this paper is benchmarking different proposals for XAI for local explanations based on feature relevance, this module can use different ML supervised algorithms. Regarding whitebox models, our proposal includes three options. First, we want to evaluate the usage of EBM [11] since they offer both the possibility to infer relationships between the input features and the average fuel consumption while providing feature relevance values that show the contribution of each feature to the final prediction for every data point. At this point, in order to offer a baseline benchmark, we will also include a Linear Regression model with the usage of the ElasticNet [19] algorithm. Also, we will include a variation over EBM that will be addressed\nas \u201dEBM variation\u201d. It will be described at the end of this Section. For the black box models, we will include the tree based methods that we are going to use later for benchmark against EBM. They include XGBoost [20] and LightGBM [21].\nOur final solution will use the proposal that yield best results (according to the metrics defined at 3.7.). There are two additional aspects to consider in this module, shown at the detailed flowchart in Figure 14 within the \u201dAdjust features\u201d module. First, some of the algorithms aforementioned need to have all the features within similar value scales. Thus, we apply a standarization over the input features for two scenarios: when using ElasticNet and when the posthoc XAI method is LIME. Finally, the evolution of some the feature values according to the evolution of the average fuel consumption should be monotonic (either positive or negative), like is indicated within the column \u201dType\u201d in Table 2. For some of the ML algorithms, like LightGBM or XGBoost, we can specify as an input parameter if we want any monotonic constraint (either positive, negative or none). However, we cannot do this directly with EBM or ElasticNet. In the case of ElasticNet, the monotonic constraint consists in enforcing the coefficients to be positive, but that does not work for monotonically decreasing features. Thus, for these last two algorithms, we simply change the sign of the features in order to make them negative and reverse the dependence with the target variable.\nEBM variation The EBM variation that we propose that takes into account possible differences that may exists within different subgroups of vehicles, in order to adjusts feature relevance and predictions. Regarding our use case, the feature relevance may be different depending on the vehicle group. For instance, the impact on the average fuel consumption for each additional harsh brake may change depending on the vehicle\u2019s model and make considered. Thus, there should be different feature relevance-values pairs depending on that vehicle group category. Using only one EBM provides unique pairs of value-importance regardless of the vehicle group, meaning that the final impact in the target variable will be the same for a specific feature value.\nThe intuition behind our proposal is similar to other works in the literature [22]. We will add an additional layer of models to predict the error of a previous one. As represented in Figure 3 for one subgroup of vehicles, first, we will train an EBM model over all data during the training phase. Then, we will predict the error for each of the vehicle\u2019s subgroups, and train additional EBM in order to be able to predict that error and both improve the predictions of the first one as well as adjusting the results to the specificity of each of the subgroups. This last consideration is based on the fact that while the first model provides unique feature relevancevalues pairs, because the second one is predicting the error of the first one in order to add it to its prediction, we can also use the feature relevance values of the second one to add them to the first one. This may be done since the feature relevance values of the second model show the feature contribution to the error. With that, there will be different feature relevance-value pairs, as well as predictions, for each of the\nvehicle subgroups considered.\nThe detailed description of EBM variation appears at Algorithms 1 and 2. Algorithm 1 describes the training process. The function trainEBMvar receives the input feature matrix X together with the real target variable y, and a list with the columns used to consider the subsets, ls. In this case, ls includes only the variable vehicle group. After that, it initializes an empty dictionary dct m where the error predicting models are going to be stored. Then, it obtains the potential combination of lcomb (in this case, there are no combinations since there is only one feature). Following this, it trains an EBM model usingX and y. Iterating through all of the combinations, it filters the input matrix X for the subset for that iteration,Xi, getting also the indexes associated to those registers, idxi. If there are not enough data points (less than a threshold th ebm var), it skips that iteration. In other case, it obtains the error for that subset using the original model emb, y erri. Using that error and the matrix filtered for that iteration, it trains a new model ebmi that tries to predict the error for that subset. This model is stored within the dictionary dct m.\nAfter the training, the next step is using those models for prediction and explanations. Algorithm 2 describes the function expEBMvar used for that purpose. It receives a data frame to explain (X), together with the general model (ebm), and the dictionary with the models used for error prediction (dct m). It also receives the list of features for the subsets of data. The function initialize a data frame to store the feature relevance values (df imp) and a list with the target feature predictions (y pred). After obtaining the different combinations for iterating (lcomb), it firsts predicts the target feature for that subset Xi using the general model ebm. Then, if that combination was used for training error predicting models, it obtains the error predictions of the subset, together with their feature relevance values, and adds them to the ones from the original model. If that combination does not belong to any error predicting model, then the\nfunction uses only the predictions and feature relevance values from the general model (ebm).\nAlgorithm 1 EBM Variation training 1: procedure TRAINEBMVAR(X, y, ls) 2: dct m\u2190 null 3: lcomb \u2190 combinations(X, ls) 4: ebm\u2190 trainEBM(X, y) 5: for comb \u2208 lcomb do 6: Xi \u2190 X[X[ls] = comb] 7: idxi \u2190 Xi[index] 8: if len(Xi) < th ebm var then 9: continue 10: end if 11: y predi \u2190 ebm.predict(Xi) 12: y reali \u2190 y[idxi] 13: y erri \u2190 y reali \u2212 y predi 14: ebmi \u2190 trainEBM(Xi, y erri) 15: dct m[comb]\u2190 ebmi 16: end for 17: return ebm, dct m[comb] 18: end procedure\nAlgorithm 2 EBM Variation explanations 1: procedure EXPEBMVAR(X, ebm, dct m, ls) 2: df imp\u2190 null 3: y pred\u2190 null 4: lcomb \u2190 combinations(X, ls) 5: for comb \u2208 lcomb do 6: Xi \u2190 X[X[ls] = comb] 7: y predi \u2190 ebm.predict(Xi) 8: if comb in dct m then 9: ebmi \u2190 dct m[comb] 10: y erri \u2190 ebmi.predict(Xi) 11: y predi \u2190 y predi + y erri 12: df imp i\u2190 ebm.feat imp(Xi) 13: df imp err i\u2190 ebmi.feat imp(Xi) 14: df imp i\u2190 df imp i+ df imp err i 15: end if 16: y pred\u2190 y pred.append(y predi) 17: df imp\u2190 df imp.append(df imp i) 18: end for 19: return y pred, df imp i 20: end procedure"}, {"heading": "3.5. Generate explanations", "text": "The generation of explanations belongs to the explaining phase. First, we apply a preprocessing just like the one during the training phase, with the minor difference that the median values used for filling null features correspond to the historical feature value (feature value during the training phase). After that, using the average fuel consumption limits obtained during the training phase, we classify each data point as outlier or inlier. As a result, at this point the process will use for explanations an input data set analogous to the\none used for training. This data set, however, will be filtered for only outliers, since the aim of the process is to explain anomalous average fuel consumption cases.\nThe generation of explanations will also use the regression model trained in the previous phase, so the data set aforementioned will have it\u2019s features adjusted (\u201dAdjust features\u201d module), scaling its values if needed according to the historical values used during training, and changing some feature signs depending on the ML regression algorithm used.\nWithin the \u201dGenerate explanations\u201d module itself, the first step checks whether the ML model used for regression is a whitebox model or a blackbox one. In case the model is a whitebox one, the feature relevance for each data point can be extracted directly. On the contrary, if the model is a blackbox one, a posthoc XAI method for feature relevance is applied over it in order to extract that feature relevance for each data point included in the data set use for explanations. The posthoc XAI methods considered are tree-based SHAP algorithm [9], and LIME [6].\nThis provides a raw data frame with explanations that could be used directly to explain every instance (Table 2). However, it needs to be combined with the business rules in order to select not all the explanations, but the ones that comply with them. Since every XAI method considered in this paper establish an additive relationship between input and target features through feature relevance value, from among all available features the process considers only for each data point those that comply with the rules. Generally speaking, for a particular data point n, the raw explanations provide the following equations:\ny pred(n) = \u03b5+ k\u2211 i=1 \u03b1i(xi(n))\u00d7 xi(n) (5)\nThus, Equation 5 show the relationship for a data point n between the predicted value of the target variable y pred with respect to k input features, xi, through their coefficient \u03b1i. This coefficient \u03b1i changes depending on the corresponding xi(n) value for that data point in case of EBM method or \u201dEBM variation\u201d (for whitebox) and the posthoc XAI techniques of tree-based SHAP and LIME (for blackbox). Regarding the baseline linear model (ElasticNet), the coefficient is constant for every data point. Finally, a constant intercept value \u03b5 is added to the feature terms.\nThere are two business rules filter steps that area applied over the raw explanations. First, there is a \u201dMonotonicity filter\u201d to ensure that feature relevance are monotonic. Then, there is a \u201dBusiness Rules filter\u201d step that apply the remaining business rules. This split of rules is useful since the monotonicity filter will always be needed within the process, but the remaining business rules may change according to customer needs or customer profile.\nThe \u201dMonotonicity filter\u201d step analyses each pair of feature value and feature relevance for every vehicle group and route type combination and discards the pairs that are not monotonic. An example of it can be seen in Figures 4, 5, 6. Starting from the evolution of the relevance-value pair of a particular feature, in this step the process finds the feature\nvalues intervals where the feature relevance is not monotonic, and discards those combinations. Thus, the raw explanations for each vehicle-day, where all the features are included, are filtered so that the feature values that correspond to feature relevance ones that are not monotonic are now discarded. Figure 4 shows the original feature relevancevalue pairs for a combination of route type and vehicle group for the feature count harsh brakes. As the Figure shows, the evolution is not monotonic.\nFigure 5 shows precisely those intervals where the importance decreases while the value increases. Thus, this step removes those importance-value pairs from all the raw explanations for every data point belonging to that vehicle group and route type combination. After removing those intervals, the evolution is indeed monotonic, as Figure shown in 6.\nFormally, the step analyses the evolution of the relevancevalue pair of every feature for every combination of categorical features as indicated in Algorithm 3. The function \u201densureMonotonic\u201d receives four variables: Xi with the FAR data frame that wants to be explained, Xexp with the raw explanations generated previously, le with a list of the numerical features (the ones for analysing the monotonicity),\nand lc with a list of the categorical columns. Using both Xi and lc, the function first obtains the possible combination of categorical features and stores that information within lcomb. Thus, lcomb and le are the parameters that are going to be considered during each iteration: a unique combination of categorical feature values (comb) and one explainable feature (f ). comb and f are used for filtering the explanations of every vehicle-date of the period in order to have a unique data frame of the importance-value pairs inside that iteration (Xcheck). This data frame is sorted in an ascending order using the feature value. After that, the function gets the difference of the feature relevance between one feature value and the following one. If the evolution is monotonic, the difference should be 0 or higher (0 because we only check for monotonic evolution, not strictly monotonic). The function discards the rows that are not monotonic, and keeps checking the difference of feature relevance between one row and the following one until no rows are discarded (which means that the data frame is already monotonic).\nSince the monotonicity filter analyses the combined evolution of both feature relevance and feature value, it works either for EBM (where there is only one value-importance pair per feature at the pairwise function [11]), \u201dEBM variation\u201d (where there is potentially one value-importance pair per feature and vehicle group), or LIME and SHAP (where there may be more than one importance value per unique feature value [5]). Indeed, as shown in Figure 7, there may be more than one importance-value pair per feature value. However, since Algorithm 3 checks a pair and the immediate following one, it will, for instance, check (x0, y0) against (x0, y1) with y1 > y0, and will remove the latter if the importance is lower. After removing the non monotonic pairs, Figure 7 turns to Figure 8.\nA final comment is that, in some cases, there may be only one feature relevance-value pair whether because there was only one instance to begin with, or whether there is only one remaining instance after applying the filter. In those cases, the instance is kept.\nAfter applying the monotonicity filter, the \u201dGenerate explanations\u201d module applies the remaining business rules at\nAlgorithm 3 Monotonicity 1: procedure ENSUREMONOTONIC(Xi, Xexp, le, lc) 2: Xexp new \u2190 null 3: lcomb \u2190 combinations(Xi, lc) 4: for comb \u2208 lcomb do 5: for f \u2208 ln do 6: Xcheck \u2190 filter(Xexp, comb, f) 7: Xcheck \u2190 dropDuplicates(Xcheck) 8: Xcheck \u2190 sort(Xcheck) 9: ndiff \u2190 \u22121 10: while ndiff 6= 0 do 11: ni \u2190 len(Xcheck) 12: Xcheck[\u2032diff \u2032]\u2190 getDiff(Xcheck) 13: Xcheck \u2190 Xcheck[\u2032diff \u2032] \u2265 0 14: ne \u2190 len(Xcheck) 15: ndiff \u2190 ni \u2212 ne 16: end while 17: Xexp new \u2190 append(Xexp new, Xcheck) 18: end for 19: end for 20: return Xexp new 21: end procedure\nthe \u201dBusiness Rules filter\u201d step. Though, as mentioned before, the business rules may vary, for the purpose of this paper and it\u2019s analyses, the following business rules are considered:\n\u2022 BR1: Do not use the feature relevance of \u201dgeneral\u201d features within the explanations.\n\u2022 BR2: Feature value should be higher than the median value of inliers for that combination of \u201dgeneral\u201d features for features with monotonic positive constraint, or lower for features with monotonic negative constraint.\nThe reason behind the usage of BR1 is that combinations of categorical columns are only going to be used to divide the set of explanations into the specific combination for\neach one of those categories. For instance, the user will see which vehicles belonging to vehicle group 3 and for a route type of \u201dcity\u201d have an anomalous average fuel consumption and which features are affecting it and how much. However, there is no need of indicating the feature relevance itself of the categorical features, so those registers are discarded from the explanations obtained in the previous step.\nBR2 further filters the explanations to consider only the cases where outliers have a feature value above the median feature value of a reference data set of vehicle-date inliers for the same categorical combination. This applies to monotonic positive constraint features. For monotonic negative constraint features the logic is similar but the feature value should be lower than the median one. Regarding the reference data set, is either the historical one used in the training phase for features with \u201dNo\u201d at \u201dPeriod only?\u201d column in 2, or the one used in the explaining phase itself for features with \u201dYes\u201d. It is like this because for features such as the exterior temperature or the total odometer value there is no sense in comparing against their historical value, since they are features that are either continually increasing or are seasonal.\nWith that, the output for this module includes the feature relevance (after filtering with monotonicity and business rules) for each of the anomalous vehicles and dates included within the date range considered for the explanation phase. A final step is applied, where any data transformation applied before (such as feature scaling or reverse signs) is undone."}, {"heading": "3.6. Generate recommendations", "text": "As mentioned at the Introduction, whitebox models that include feature relevance are useful for counterfactual explanations. Since there is a unique intercept and unique feature relevance-value pairs, they can provide counterfactual explanations where one of the feature values alone may be changed and with that, recalculate the predicted target value in order to see how it will change. With both SHAP and LIME, we would need to obtain again the whole explana-\ntions for the new data point with the modified feature value, and that may lead to a feature relevance change for the remaining features. Thus, the approach will be different. This is why the \u201dGenerate Recom.\u201d (generate recommendations) step will only be offered for whitebox models (EBM, \u201dEBM variation\u201d and ElasticNet).\nThe intuition behind it is the following one. \u201dGenerate Recom.\u201d will change the feature values of the outliers used within the explaining phase for the corresponding median feature value of the inliers belonging to the same vehicle group and route type. This will be applied for one feature at a time and for every feature labeled as \u201dactionable\u201d. Then, by substracting the relative change in the predicted value from the real average fuel consumption, it will indicate which vehicles-dates would have a fuel consumption below the outlier limit for that vehicle group and route type.\nThe details are described in Algorithm 4; getRecom function receives the historical median values of the inliers (obtained during the training phase; Xmed), the data points of the explaining phase with their feature relevance (Xexp), and two lists, one with the explainable features that are actionable (la) and one with the categorical ones (lc). Using these inputs, getRecom function initializes two empty lists (l up ind and l up all) and gets the feature relevance for the median inliers feature values (\u201dcoeff\u201d) with checkPairwise(Xmed, lc) function. After obtaining the feature relevances, the function analyses every data point (x) within the explanations and obtains it\u2019s predicted target feature (y pred) using the feature relevance and the intercept. It also stores the real value (y real) of the target feature. Then, it checks every feature (f ) within the explanations and gets its corresponding feature relevance from the median inliers reference (\u03b2fn). It laters sums again al the feature relevance and intercept for data point x, without the feature relevance for feature \u201df\u201d, and instead sums \u03b2fn. This leads to a new predicted value (y new) where all the other feature values are kept the same, but there is a change for the specific feature considered. The difference between y pred and y new is \u2206, and this difference is used to compute the change in the real average fuel consumption (l up ind). After iterating for all the available combinations, getRecom uses groupVal function to obtain the estimated value in case all the actionable features change at the same time to their median inlier value. This is simply done by aggregating all the individual changes in the prediction for each feature, and subtract the aggregated difference from the real average fuel consumption.\nThus, Algorithm 4 provides a list with the new estimated average fuel consumption value for every individual feature change for every vehicle-date pair (l up ind). Comparing this values against the outlier limit for that vehicle group and route type, the step indicates which individual feature changes will lead from outlier to inlier, and what would be the corresponding average fuel consumption. It provides as well a similar result but considering that every actionable feature changes at the same time (l up group).\nAlgorithm 4 Generate Recommendations 1: procedure GETRECOM(Xmed, Xexp, la, lc) 2: l up ind\u2190 null 3: l up all\u2190 null 4: coeff \u2190 checkPairwise(Xmed, lc) 5: for x \u2208 Xexp do 6: y pred\u2190 \u03b5+ \u2211k i=1 Fi(xi)\n7: y real\u2190 x[target] 8: comb\u2190 x[lc] 9: for f \u2208 la do\n10: \u03b2fn \u2190 coeff [f ] 11: y new \u2190 \u03b5+ \u2211k 6=f i=1 Fi(xi) 12: y new \u2190 y new + \u03b2fn 13: \u2206\u2190 y pred\u2212 y new 14: y updated\u2190 y real \u2212\u2206 15: l up ind\u2190 l up ind.append(y updated) 16: end for 17: end for 18: l up group\u2190 groupV al(l up ind, la, Xexp, lc) 19: return l up ind, l up group 20: end procedure"}, {"heading": "3.7. Metrics", "text": "As shown in Figure 14, there are two steps where the process computes metrics for evaluation. The first group of metrics, obtained during the training phase, aim to benchmark the predictive power of the different models considered. These metrics will analyse the average fuel consumption predicted by all those models against its real value. We will further refer to them as model metrics.\nThe second group of metrics are the ones obtained during the explaining phase, and they aim to measure different aspects regarding the understandability of the explanations generated, as well as comparing the explanations generated between every model and XAI technique. We will further refer to them as XAI metrics.\nWith that, we are analysing not only if the predictive power of the EBM is good enough (model metrics), but we are also measuring the explanations themselves in order to compare them to against posthoc XAI techniques (XAI metrics). Thanks to that, we offer a whole comparison of the usage of EBM with real-world IoT data for outlier explanation against using blackbox models with posthoc XAI techniques. Again, the same is applicable for the \u201dEBM variation\u201d proposal. A final comment is that even tough it may be difficult (or not reliable) to compare individual explanations with certain metrics due to Rashomon\u2019s Effect [5], the ones that we propose analyse the explanations from a general perspective. Thus, even if the explanations differ at a very low level, the general view should be similar.\nModel metrics These metrics include the following:\n\u2022 explained variance (EV)\n\u2022 maximum error (ME)\n\u2022 root mean squared error (RMSE)\n\u2022 median absolute error (MAE)\nThe metrics above are the one used for comparing the models among themselves. Together with that, we analyse over the test set if the models are good enough. For doing that, we will consider two metrics:\n\u2022 Adjusted R2 (adj-R2)\n\u2022 mean absolute percentage error (MAPE)\nAll the model metrics are evaluated over a test set that includes both outliers and inliers, since the purpose is to measure how close are the target feature predictions to the real value. There are other potential metrics that can be considered, especially classification metrics that measure if after applying the anomaly limits over the predicted values, the inlier/outlier predicted class matches the one of the real target feature. However, since we are not using the ML surrogate model to actually predict the outlier/inlier class, we do not find it necessary to measure.\nXAI metrics Using the taxonomy of metrics present in [15] for individual explanations, we consider the following properties for comparing the explanations generated by the different methods studied in this paper.\nRepresentativeness metrics include two subgroups: general metrics and monotonicity metrics. General metrics measure global aspects regarding the explanations generated. They include the following ones:\n\u2022 n datapoints: Number of unique combinations of vehicles-dates (data points) within the explained data frame.\n\u2022 per datapoints explained: Percentage of explained data points from the total n datapoints.\n\u2022 n variables used: Number of features used for the explanations.\n\u2022 mean variables used per day: Mean daily features used for explanations.\n\u2022 mean variables used per group: Mean features used for explanation per group.\nMonotonicity metrics measure the impact of the monotonicity filter over feature the importance-value pairs. Since for EBM (or \u201dEBM variation\u201d) we are not explicitly applying any constraint for feature monotonicity, these metrics will offer a comparison against XGBoost and LightGBM where the constraint was applied in order to see if there are any significant changes. These metrics also allow to see how SHAP and LIME respect the constraints existing in the model since at an ideal scenario there will no discarded pairs.\n\u2022 per monotonic datapoints: Percentage of data points that are kept after applying the monotonicity filter. The data points considered are the pairwise relationships (feature relevance-value) separated by vehicle group and route type. The percentage is obtained by seeing how many data points remain for each of the subgroups with respect to the total data points of all of those subgroups.\nFidelity metrics focus on comparing the output value for the target variable after applying the business rules against its previous value, in order to see which model is less penalized by applying them. It will include two kind of metrics, depending on whether the output comparison is against the real value of the target variable, or if it is against the predicted value of the surrogate ML model.\nThe first subgroup of metrics within fidelity are identified as fidelity-target metrics. Fidelity-target measure the predictive power for each data point considering only the feature relevance of the remaining explanations after applying all the filters. Of course, these metrics do not represent by themselves any insights for the real surrogate ML model (since they may use training data and since they do not account for all the raw feature relevance). They are useful only to see if there are significant changes between the surrogate modelXAI combinations. Also, these metrics only make sense for surrogate model-XAI combinations that do not have significant changes in their \u201dGeneral metrics\u201d (if one combination yields significantly less explanation than other, it is not possible to compare this subgroup of metrics). Finally, these metrics are not useful to compare \u201dEBM variation\u201d since it tries to reduce the training error, and as mentioned before, the data sets used for explanations may contain training data. The metrics themselves are some of the ones used for \u201dModel metrics\u201d, but calculated for each data frame used for the explaining phase.\n\u2022 mean average precision error (MAPE) \u2022 maximum error (ME) \u2022 root mean squared error (RMSE)\nThe last subgroup of fidelity metrics are identified as fidelity-model. They include a metric called \u201dfaithfulness error\u201d which calculates the Absolute Error (AE) between the predictions before and after applying the business rules for whitebox models, and after applying the XAI method together with the business rules for blackbox ones. This metric will be further identified as \u201dfaithfulness error\u201d\nStability metrics includes two metrics, identified as stability error and xai stability error. Both of them are computed using the stability metric proposal of [17], as indicated in Figure 9. For stability error we analyse the stability of the predictions before applying the business rules (raw predictions of the whitebox models and predictions from the blackbox ones). For xai stability error we analyse the predictions using the feature relevance values from the resulting explanations (after applying the business rules). Both of these metrics are calculated over the test set for each of the data points.\nFinally, following also [15], we include metrics related to the usefulness of the explanations generated, measuring contrastiveness. For doing it, we include the metric\nper rec below, that measure the percentage of outlier data points that receive a countefactual recommendation that changes the values of the final features into the median value of the same feature values for the inliers of that group, and doing that changes it the average fuel consumption into an inlier. The features included are the ones identified as \u201dactionable\u201d at Table 2. Also, the recommendations are the last step of the process, so the features used have already passed by the business rules filters. Thus, we are measuring the understandability of explanations from the user\u2019s perspective: considering only features that the user can alter directly, how many instances can significantly decrease their average fuel consumption?"}, {"heading": "3.8. Recommendations according to user profiles", "text": "As mentioned in the previous Chapter, [1] highlight that explanations should be tailored for the specific profile of the user that will receive them taking into account both their expectations and their domain knowledge.\nWithin the use case proposed in this paper, we identify two user\u2019s profiles, as indicated in Figure 10, where the users are highlighted over the image from [1].\nUser Profile 1: Technical Specialists The first group of users are technical specialists, responsible for the status of the vehicles. Their main interest regarding the explanations is detecting what vehicles are consuming excessively, and what is causing it, considering for that not every feature, but only the ones that are actionable (as seen in Table 2).\nTo accomplish that, the explanations generated at Section 3.5. may be enough. However, explaining every single date for each combination of vehicles and route types in terms of the numeric feature relevance is overwhelming, not being useful for them. This is why we provide the explanations for these users at two different levels. First, a summary of the main recommendations for a specific period of time (p.e. a month). Second, we provide the individual daily detail only if the want to dive deeper into a particular vehicle and route type.\nFirst level - Summary of recommendations As already mentioned, the first level include a summary of the individual recommendations yielded by the system. Algorithm 5 described the way to accomplish it. First, it receives the same input, Xmed, Xexp, la, lc, as Algo-\nrithm 4. The difference is that, before obtaining the recommendations, it applies a filter that choose only some vehicles and route types, from among all the combinations, according to some business parameters. These parameters are min days anomalies, min day km, and min dev total avg fuel. With min days anomalies, the filter chooses only the vehicle-route type combinations that have at least that specified number of outliers. Then, with min day km, it chooses only the dates that have a trip distance over that minimum threshold. Finally, with min dev total avg fuel, the filter chooses only dates with individual recommendations that have a decrease in the target variable after applying the recommendations over that threshold.\nAfter applying the aforementioned filters with filterPoints() function, the algorithm applies another function, summaryPoints(). This function aggregates the remaining individual data points of the outliers into their median values. So, it will yield a data frame with unique points for each combinations of vehicles-route type. These points will represent a prototype for each of those combinations, representing the most common anomalous scenario. These data points are stored in Xsumm. In order to always have feature values already present within the explanation period, if the vehicle values are pairs (not odds) we keep the lowest middle value in order to offer later the most conservative recommendation. Then, the algorithm uses Xsumm for obtaining the recommendations with getRecom function. In this case, we are only interested in the output l up ind, that indicates the new average fuel consumption after applying each individual feature change in order to have the median inlier value.\nThis individual contributions are aggregated with aggContribution() function, providing l agg with the total average fuel consumption reduction if all the features had the value of the median of the inliers of that same vehicle group and route type. With that, the user will see the general recommendations (how much average fuel could be decreased by applying all the feature value changes), as well as seeing the individual impact of each feature to the average fuel consumption (seeing how much average fuel consumption could be reduced by applying only one feature change).\nAlgorithm 5 Summary of Recommendations 1: procedure GETSUMMARYRECOM(Xmed, Xexp, la, lc) 2: Xexp \u2190 filterPoints(Xexp) 3: Xsumm \u2190 summaryPoints(Xexp) 4: l up ind,\u2190 getRecom(Xmed, Xsumm, la, lc) 5: l agg \u2190 aggContribution(Xsumm, l up ind) 6: return l up ind, l agg 7: end procedure\nSecond level - Daily detail As mentioned before, we first provide the summary of recommendations and then, if the user wants to see recommendations for individual days, they can access to these second explainability level. At this level, the explanations provided to the user contain two elements. The first one includes the\nrecommendations from Section 3.5. For the second element, we provide the daily feature relevance of every variable (not only the actionable ones). However, directly providing the feature relevance value is also not useful since it is cumbersome to directly analyse it in order to see how the relatively influence the average fuel consumption. Because of this, we complement the quantitative feature relevance explanations with qualitative ones. Considering additional business rules, the module classifies the degree of influence for each feature at a specific vehicle-date depending on how much they contribute to average fuel consumption. In order to do that, the module uses the following business rules:\n\u2022 IF var < th degree 1 THEN var cat = no influence \u2022 ELSE IF var < th degree 2 THEN var cat = low in-\nfluence \u2022 ELSE IF var < th degree 3 THEN var cat = medium\ninfluence \u2022 ELSE var cat = high influence\nWith var corresponding to:\nvar =| feature importance y pred | (6)\nThus, dividing each feature relevance value with the predicted average fuel consumption, it gets the relative contribution of each of those features. Thanks to that, the user can see which features where the ones that contributed the most.\nUser Profile 2: Fleet Manager The final user profile considered is the \u201dfleet manager\u201d. The main interest for this user profile is having a global comparative view at a vehicle group level, not seeing information about individual vehicles or particular dates. At this level of information, in order to have useful explanations, the individual ones must be aggregated into explanations at vehicle group level, like is done with l up group from Algorithm 4. However, offering explanations in terms of anomalies and average fuel consumption is not what is expected. The useful explanations should be expressed in terms of extra litres of fuel consumed, because that can be immediately turned into an economic cost. So, after having the individual recommendations from Algorithm 4, the individual explanations are aggregated in order to first have the total average fuel consumption reduction per day, and they are later expressed in terms of total fuel, calculating it according to the new feature values (because it will depend on the new value of \u201dtrip kms\u201d). Then, with all that, the final explanations provide the vehicle group view (with l up group) and how much fuel could have been saved without anomalies (both at a global level and per each vehicle group)."}, {"heading": "4. Evaluation", "text": "We use our algorithm over different IoT data sets from Telefonica\u2019s, to evaluate the following hypotheses: \u2022 It is possible to obtain similar model metrics using EBM\ncompared to other boosting models (XGBoost and LightGBM). EBM metrics will also be significantly better than other whitebox model used as baseline: ElasticNet.\n\u2022 Model metrics obtained by evaluating EBM over a test set will be good enough, showing that we can use this model for the use case described in this paper.\n\u2022 It is possible to obtain local explanations using EBM that are similar to the ones obtained with blackbox models combined with local posthoc XAI techniques based on feature relevance (LIME and tree-based SHAP). This will be evaluated with the XAI metrics described in the previous Chapter: \u201dgeneral\u201d, \u201dmonotonicity\u201d, \u201dfidelity-target\u201d and \u201ddegree of influence\u201d. This will also show how EBM, regardless of not including monotonicity constraints, will yield similar results than blackbox models with those posthoc XAI techniques where the models do indeed include the constraints. Also, XAI metrics will be significantly different than those obtained by the baseline model.\n\u2022 XAI metrics obtained with EBM are good enough to use it for explanation generation for the use case described in this paper.\n\u2022 Our proposal \u201dEBM variation\u201d obtain metrics similar to standard EBM, so it can be used to have different feature relevance-value pars per vehicle group without losing neither explainability nor predictive power."}, {"heading": "4.1. Data sets", "text": "As mentioned before, the IoT are going to be processed at the preprocessing phase of 14 in order to obtain a data set with the features described in 2, where their values represent the daily aggregation of the real-time information of a vehicle at a specific date. For the analyses carried out in this paper, we will consider 3 data sets, belonging to different fleets. This data sets are samples for some of their vehicles, and the aggregated information includes information collected during a whole year. Each of those data sets have a size that refers to the unique combinations of vehicle-dates. Their sizes are the following ones:\n\u2022 Data set 1 (D1): 115860 data points. 12 vehicle groups. \u2022 Data set 2 (D2): 28665 data points. 273 vehicle groups. \u2022 Data set 3 (D3): 823 data points. 30 vehicle groups.\nWe will use those data sets for the training phase (using a part of them for training and another one for testing, as described in the previous Chapter), and use a subset of them for the explaining phase. Particularly, we will use subsets of complete months to generate those explanations. Due to their size, since D3 is very small, we will use the whole data set for explanations, instead of only one month.\nRegarding the business variables described in the previous Chapter, the values chosen for the evaluations conducted are the following ones:\n\u2022 th kms = 30 \u2022 low th time = 0.55 \u2022 high th time = 0.55 \u2022 th degree 1 = 0.1 \u2022 th degree 2 = 0.21 \u2022 th degree 3 = 0.4\n\u2022 th ebm var = 100\n\u2022 min day km = 6.7\n\u2022 min days anomalies = 3\n\u2022 min dev total avg fuel = 1\nAlso, we will consider a 90/10 train/test split for testing."}, {"heading": "4.2. Model configuration", "text": "The hyperparameters used for every model match the default ones provided by the software libraries used (only modifying the parameters related to the monotonic constraints) since we did not find any significant improvements after using a grid search over the training data. Regarding \u201dEBM variation\u201d, both the general EBM and the EBM for error prediction within the different subgroups use the same hyperparameter configuration than the ones described above."}, {"heading": "4.3. Model evaluation", "text": "Metrics over K-Fold Cross-Validation First, we address the comparison between the different models using the \u201dmodel metrics\u201d described in the previous Chapter, in order to see if there are significant differences between the predictive power of the ML models analysed in this paper. For doing it, we perform a k-fold CrossValidation (CV) over the train data set using 30 splits. For every one of those splits, we train a model on a subset of the training data and evaluate it over the validation data selected by k-fold CV. This is done for each of those 30 splits, and for each of the three different data sets used.\nThis yields a vector of 30 components for each data setmetric-ML model that will be used for comparing against the other combinations of ML models belonging to the same data set-metric. The comparison is carried out by using Wilcoxon signed-rank test [23] in order to see if the metrics of two of the ML models have the same distribution. Wilcoxon signed-rank test is chosen for this hypothesis testing since it\u2019s a non-parametric test that can be applied over paired or potentially related data. This last consideration is important since the metrics obtained after the k-fold CV may be related to some degree, because the same data sets are used for different models, and the metrics from a k-fold of a particular data set-metric-ML model may be using similar training data compared to another k-fold.\nThus, we check the p-value resulting from the hypothesis test in order to see if H0 is rejected (H0 = distributions are equal), using 0.05 as the threshold value for rejecting H0.\nThe results of the hypothesis tests for each of the data sets are included in Table 4. That table contain the pair of models compared (\u201dmodel 1\u201d and \u201dmodel 2\u201d), along with the metric considered and the median value for the 30 k-fold splits used at every data set (for example, D 3 m 2 is the median value for model 2 with the metric considered at data set 3). It also includes the pvalue from Wilcoxon signed-rank test at each data set (P1 is the pvalue at D1, and so on).\nFirst, we analyse the comparisons regarding the baseline model, ElasticNet (labeled as \u201dlinear model\u201d). Out of all the metrics and data sets, in 93% of the cases there are significant differences between this model and the other ones,\nwhile this model has a worst median value (higher error metrics, lower r2 and explained variance). This highlights how the predictive power of ElasticNet for our use case is almost always significantly worse than using any of the other models considered.\nThe next analysis that we consider is regarding XGBoost results versus LightGBM. The expected result is that their metrics should be similar, as it is reported in different benchmarks within the literature [24], [25]. Out of the 18 combinations of metrics-data sets, 13 of them (72%) have significantly different metrics distributions according to the hypothesis test. Regarding D1 and D2, in all the metrics the results from XGBoost outperform those from LightGBM (lower error metrics, higher r2 and explained variance) considering those cases with p-values<0.05. However, for the cases with p-values < 0.05 in D3, LightGBM offer better results. The gap between the metrics, however, is clearly smaller than the one comparing ElasticNet (p.e. the median value r2 for D3 is 0.65 for XGBoost, 0.675 for LightGBM, while being 0.28 for ElasticNet).\nRegarding the comparisons between LightGBM and EBM, we see that 11 out of the 18 data sets-metrics combinations (61%) have significantly different metric distributions. In all those cases, EBM are worse than those from LightGBM (higher error metrics, lower r2 and explained variance), though with a much smaller difference than that compared to ElasticNet (p.e. for instance, the median r2 value for D1 is 0.67 for EBM and 0.69 for LightGBM).\nSomething similar takes place when comparing XGBoost versus EBM. There are no significant differences regarding D3, but the differences regarding D1 and D2 are bigger since XGBoost obtained better metrics than LightGBM for those data sets. The percentage of data sets-metrics that have significantly different distributions comparing EBM to XGBoost is also 11 out of 18 (61%).\nThese analyses show how EBM matches XGBoost for model performance over D3. However, there are significant differences between those two models in all the metrics of data sets D2 and D1, even though the difference between them is much lower than the one compared to ElasticNet (EBM significantly outperforms ElasticNet in 17 out of 18 data set-metric combinations). Also, it matches LightGBM metrics regarding the \u201dmedian absolute error\u201d in all data sets, as well as the \u201dmax error\u201d in D3 and D3, and the r2 score and explained variance at D3.\nThe next step is comparing the results from \u201dEBM variation\u201d. Comparing against the base EBM, \u201dEBM variation\u201d outperforms it in 7 out of the 18 data set-model combinations. The cases where it outperforms EBM all belong to D1 and D2, the data sets with more registers. This happens due to the fact that D3 have many vehicle groups where the number of registers do not meet the threshold th ebm var, hence the model used is the base EBM and that lead to the exactly the same metrics. So, the proper comparison is regarding D1 and D2 only. Thus, it outperforms the base model in 7 out of the 12 data set-model combinations. This includes all the metrics except for \u201dmax error\u201d in both data sets, and \u201dmean squared error\u201d in D2.\nComparing \u201dEBM variation\u201d to LightGBM, we see how the 11 different combinations from \u201dEBM\u201d change significantly. In these comparison, there are only 3 (16.7%) metric distributions (\u201dmedian absolute error\u201d for D1 and D2, and \u201dmean absolute error\u201d for D1), where \u201dEBM variation\u201d actually outperforms LightGBM (lower error metric values).\nRegarding XGBoost, there are only 2 significantly different metric distributions, belonging to the \u201dmedian absolute error\u201d at both D1 and D2. In those cases, \u201dEBM variation\u201d also outperforms XGBoost.\nWith all these analyses, we first see regarding EBM, that even though its metrics are significantly lower than those form XGBoost and LightGBM, it only takes place for some combinations of data sets-metrics. And even then, the differences are significantly lower than those against the baseline model ElasticNet. Second, we see how using \u201dEBM variation\u201d significantly improves the results, offering a model that generally matches in performance both XGBoost and LightGBM, even outperforming them for some data sets and metrics combinations.\nTo visually illustrate these comparisons, we include with Figures 11, 12 and 13 the model metrics results for explained variance, max error and mean squared error respectively. We only show these three metrics since r2 is similar to explained variance, and the metric distributions of median and mean absolute errors are similar to the ones obtained with mean squared error.\nMetrics over test set After the model comparison checked above, we analyse if the metrics regarding the test set used for each of the three data sets are good enough. Here, as mentioned in the previous Chapter, we use adj-R2 and MAPE, since they both yield a result in terms of percentage that is easily comprehended.\nRegarding adj-R2, even if it\u2019s clear that it indicates the proportion of the variance in the target feature that can be predicted using the input features, it is not trivial to define value thresholds to indicate if the model is good or not. It heavily depends on both the context and the units of the target feature [26, 27]. However, there are some guidelines that may be considered. As a reference, we use the proposal of [28], that mentions the following levels:\n\u2022 0.67: Substantial\n\u2022 0.33: Moderate\n\u2022 0.19: Weak\nMAPE, though it\u2019s a metric commonly used for forecasting models, it is also useful for regression one [29]. Again, though it is also not direct to define thresholds for MAPE, we use as reference the ones detailed in [30], even though they are originally proposed for forecasting models.\n\u2022 < 10: Highly accurate forecasting\n\u2022 10\u2212 20: Good forecasting\n\u2022 20\u2212 50: Reasonable forecasting\n\u2022 > 50: Inaccurate forecasting\nThe metrics over the test set are included in Table 1. We only use D1 and D2 for this evaluation since those data sets are the only ones that have a size for the test set meaningful enough (D3 is too small).\nRegarding the ElasticNet model (\u201dlinear model\u201d), its adjusted r2 belongs to the \u201dWeak\u201d level for both data sets. Its MAPE metric is in the frontier between \u201dGood forecasting\u201d and \u201dReasonable forecasting\u201d, with D2 belonging to that first level mentioned and D1 to the second one. Thus, the linear model have reasonable predictions, but it lacks generalization power since its adjusted r2 is very low.\nXGBoost has a \u201dSubstantial\u201d adjusted r2 level for D1 and a \u201dModerate\u201d one for D2. Its MAPE is in the frontier of \u201dHighly accurate\u201d and \u201dGood\u201d, being the first one for D1 and the second one for D2. LigthGBM has an adjusted r2 within the \u201dModerate\u201d level for both data sets, but with values close to the \u201dSubstantial\u201d level threshold (in both cases adjusted r2 is above 0.6). Its MAPE is also oscillating between \u201dHighly accurate\u201d for D2 and \u201dGood\u201d for D1. We see how LightGBM seems to fit better to D2 than XGBoost, and XGBoost seems to fit better for D1.\nNext, we analyse the metrics regarding EBM. For D1, EBM has a \u201dSubstantial\u201d adjusted r2 (better than LightGBM) and \u201dGood\u201d MAPE (close to being \u201dHighly accurate\u201d and better than LightGBM). For D2, it has a \u201dModerate\u201d adjusted r2 (closer to the value of XGBoost), and \u201dGood\u201d MAPE (worse than that of XGBoost and LightGBM). Thus, EBM is obtaining model metrics over the test set belonging to the same levels of either XGBoost or LightGBM, with some differences depending on the data set considered.\nFinally, we analyse the metrics for \u201dEBM variation\u201d. For both metrics and in both data sets, \u201dEBM variation\u201d is improving the results obtained with EBM (higher adjusted r2, lower MAPE), even surpassing XGBoost in its adjusted r2 for D2."}, {"heading": "4.4. XAI evaluation", "text": "In this Section, we present the results for the XAI metrics described in the previous Chapter. For these evaluations, we use D1 and D2 as data sets only, without using D3 since its size is too small for obtaining meaningful XAI metrics. We have trained a different model over each of those data sets, using as training data size the same size as from the previous analyses. Then, we use these models for computing the XAI metrics. These metrics will be obtained using two types of data. First, using as input different months from the historical data (thus, they may contain either training data, test data or both), having then 12 data points for each model-xai technique-data set and metric. Second, using as input data the test data not used for training the models (which corresponds to the 10% of the data points from the input data set).\nThe XAI metrics are computed considering only the outlier data points, since we are interesed in measuring the understandability of explanations for those data points (because these are the explanations that are going to be received\nby the different users). So the 10% of test data has the following sizes, depending on the data set: \u2022 D1: 244 data points \u2022 D2: 98 data points\nThus, the XAI metrics will either use D1 and D2 with 12 data points each, or D1 with 244 and D2 with 98 data points. This again is enough for using Wilcoxon signedrank test [23] to see if there are significant differences between the model-xai technique combinations for each of those metrics using D1 or D2. The comparison will only be focused in EBM or \u201dEBM variation\u201d against the remaining ML models-xai techniques combinations, beginning with the analysis of EBM and the seeing if there are any improvements by using \u201dEBM variation\u201d.\nTable 3 contains the summary of the metrics used for the different analyses, together with the type of data used for the evaluations (the 12 monthly periods, the test set, or both).\n4.4.1. Representativeness metrics It includes two subgroups: general metrics and monotonicity metrics.\nGeneral metrics The metric results belonging to general metrics appear at Table 5. Again, columns with \u201dD\u201d indicate the median value for that combination (for instance, D 1 m 2 is the median value for model 2 with the metric considered at data set 1). P indicates the p-value for that data set.\nComparing EBM to ElasticNet (linear model), there is a significant improvement in every metric considered for both data sets, since ElasticNet is normally not able to obtain explanations after applying the rules. Due to this, the remaining analyses will not include ElasticNet.\nCompared to XGBoost as ML model and TreeSHAP as XAI technique, the metrics results do not show any significant variations at D2, except for \u201dmean variables used per group\u201d, where EBM has better results. It is at D1 where we can see significant differences in the metrics. EBM is able to significantly explain more data points, while also having significantly better metrics for all of the remaining metrics, except for \u201dn variables used\u201d, where \u201dSHAP tree xgboost\u201d has better results. It shows how \u201dSHAP tree xgboost\u201d is able to retain more features after applying the filtering, but EBM is using more features on average for explaining every day within each month period.\nConsidering EBM against \u201dSHAP tree lightgbm\u201d, the metrics show significant differences for all the metrics and all data sets, except for \u201dn variables used\u201d at D1 and \u201dper datapoints explained\u201d at D2, where EBM has similar results to LightGBM with Tree-SHAP. At the metrics where there is significant difference, EBM surpasses \u201dSHAP tree lightgbm\u201d.\nRegarding ML models using LIME, the metrics show even bigger differences, with better results for EBM than when considering tree-SHAP. The only exception is \u201dn variables used\u201d, where EBM is significantly below LIME with either of the ML models.\nFinally, considering the comparison of EBM versus \u201dEBM variation\u201d we see that besides the\n\u201dper datapoints explained\u201d at both data sets and \u201dn variables used\u201d at D2, where they have similar results, the metrics have significant differences. It is worth mentioning that both models have a perfect score in \u201dper datapoints explained\u201d, meaning that they are able to explain the 100% of data points. For the remaining metrics with significant differences, we see that \u201dEBM variation\u201d improves EBM at D1 in the \u201dn variables used\u201d, closing the gap for that metric to the rest of the models (and, in fact, not having significant differences with the models using LIME). However, is below EBM for \u201dmean variables used\u201d and \u201dmean variables per group\u201d for D1, and \u201dmean variables per group\u201d only for D2.\nMonotonicity metrics In this set of metrics, we only consider \u201dper monotonic datapoints\u201d, which analyse the percentage of the remaining data points after applying the monotonicity filter. The results appear at Table 7. Since the median values do not clearly shows which group is above the other in some cases, we also include the mean values.\nFirst, we see that at neither D1 nor D2, EBM has significant differences compared to \u201dEBM variation\u201d. In fact, at D2 they have an almost perfect score, so they do not discard many data points after applying the monotonicity filter. EBM also does not show any significant differences compared to ML models with Tree-SHAP in any of the data sets. This also happens with \u201dEBM variation\u201d. The only significant differences happens with models with LIME. There are significant improvements in the metric with either EBM or \u201dEBM variation\u201d. Finally, the comparison against ElasticNet is interesting since its score is always perfect because the model is strictly monotonic. At D1, EBM has significant differences to ElasticNet. At D2, there are also significant differences, but very close to the 0.05 threshold.\n4.4.2. Fidelity metrics Fidelity metrics include fidelitytarget and fidelity-model.\nFidelity-target metrics As mentioned in the previous Chapter, fidelity-target metrics apply several metrics over a predicted value that was computed considering only the feature relevance values remaining after applying all the business rules and monotonicity filters. Results appear at Table 8. Also, as mentioned previously, in this comparison we only consider EBM and not \u201dEBM variation\u201d since the predictions are obtained using different periods of data that could have already been used for training the model, and regarding \u201dEBM variation\u201d they may have even been used for optimising the error. With this analysis we only want to compare the proposals. The metrics themselves and alone are not informative.\nWe see that EBM have significantly better MAPE value at D1 compared to any of the blackbox configurations. At D2, EBM is significantly better than models with LIME, while being significantly worse than models with Tree-SHAP. Regarding \u201dmax error\u201d, at D1 there are no significant differences, expect for LIME with LightGBM, where EBM is significantly worse. At D2, EBM has similar results\nfor that metric compared to models with LIME, but has worse results compared to models with Tree-SHAP. Finally, regarding \u201dmean squared error\u201d, EBM is significantly better at D1 than any oth the other combinations, except for XGBoost with Tree-SHAP, where the results are similar. At D2, EBM is also significantly better than the remaining combinations, except for Tree-SHAP with LightGBM, where EBM is significantly worse.\nFidelity-model metrics Following with fidelity metrics, Table 9 show the results of the \u201dfidelity-model\u201d metric. As mentioned in the previous Chapter, the lower the value, the better, since it indicates less difference between model prediction before and after applying the rules. These metrics uses the test set.\nFirst, we see that EBM do not have significant differences when compared to \u201dEBM variation\u201d at neither D1 nor D2. For the remaining comparisons, with the exception of EBM and \u201dEBM variation\u201d with Tree-SHAP and LightGBM at D1, we see significant differences. Generally speaking, EBM and \u201dEBM variation\u201d have worse metrics (more metric value) than the combinations with Tree-SHAP, while outperforming the ones with LIME.\n4.4.3. Stability metrics Finally, we include the results of the stability metrics at Table 10. That table contains the two metrics described in the previous Chapter: stability error and xai stability error. For both of them, the lower the value, the better, since it indicates less difference between similar data point predictions (before applying the rules for stability error, and after for xai stability error). These metrics use the test set.\nStability error metrics For \u201dstability error\u201d, we see that EBM has similar results to Tree-SHAP with XGBoost at D1, while having significantly better results at D2. Regarding LightGBM, it significantly outperforms EBM at both data sets. Considering ML models with LIME, EBM significantly has better metrics at both data sets, except for LIME with LightGBM at D2, which improves the results of EBM. EBM compared to \u201dEBM variation\u201d have similar results at both data sets.\nXAI stability error metrics For \u201dxai stability error\u201d, we see that EBM has similar results to Tree-SHAP with XGBoost at D1, while having significantly better results at D2. Regarding LightGBM, it significantly outperforms EBM at D1 while having similar results at D2. Considering ML models with LIME, EBM significantly has better metrics at both data sets, except for LIME with XGBoost at DE, which improves the results of EBM. EBM compared to \u201dEBM variation\u201d have similar results at both data sets.\n4.4.4. Contrastiveness metrics Contrastiveness metrics include \u201dper rec below\u201d, that calculates the percentage of data points (original ones) that receive recommendations over the actionable features from Table 2 that turns the anomalous average fuel consumption into an inlier if the feature values for that vehicle-route type change for the median\ninlier values of the vehicles of the same group over the same route type.\nThis analysis is only performed over EBM and \u201dEBM variation\u201d, since they are the only models considered for the recommendation Algorithm 4, as mentioned at Section 3.6. ElasticNet is not considered as indicated earlier.\nT Table 6 shows the results over both the test set and the monthly periods. We see that there are only significant differences at D2 using the monthly periods. For the remaining comparisons the results are similar."}, {"heading": "4.5. Software Used", "text": "The main libraries used for the work done in this paper are the following:\n\u2022 XGBoost [31]\n\u2022 LightGBM [32]\n\u2022 ElasticNet [33]\n\u2022 EBM, LIME [34]\n\u2022 Tree-SHAP [35]"}, {"heading": "4.6. Limitations of our approach", "text": "One of the limitations in our proposal is that we check monotonicity within the period of data that is going to be explained. This, however, has two downsides. First, it would not be suitable for explaining only one data point. Second, even if the periods of time used for the evaluation are big (one whole month of data), the results may differ if the monotonicity is analysed by combining both the period of data to explain and the whole historical data used for training.\nAlso, the domain knowledge used needs to be expressed through business rules, but this may not be suitable for all use cases. This may be improved by using a more flexible framework to gather that apriori knowledge (p.e. using ontologies).\nTogether with that, we only work with the individual feature relevance of each variable for building the recommendations, not considering possible pairwise terms if they exist.\nFinally, our approach deals with explaining average fuel consumptions that are outliers due to several factors. This does not account for all possible features that may affect fuel usage, it uses only a subset of them. Also, we are not dealing with every possible cause of anomalous fuel usage. There are other causes, like fuel fraud, that are not considered within the scope of our proposal, mainly because they do not take place within the data sets used."}, {"heading": "5. Conclusion", "text": "We have proposed a complete process for unsupervised anomaly detection in the average fuel consumption of the vehicles of a fleet, where the anomalies are explained using XAI and based on the feature relevance of several variables that may impact fuel usage. The explanations take into account domain knowledge expressed through business rules, and expressed through counterfactual recommendations that\nare adjusted depending on two different user profiles that will use them. The process is evaluated using real IoT industry data belonging to Telefo\u0301nica.\nUsing that real data, we have also evaluated different possibilities for building a surrogate model model that infer the relationships between the input data and the predicted average fuel consumption, in order to be able to explain later how it can be reduced to be below the anomaly limit inferred unsupervised. For those surrogate models, we have considered both blackbox models together with posthoc XAI techniques for feature relevance, and whitebox models like EBM, that directly have algorithm transparency in terms of feature relevance. We include in this evaluation a novel variation over EBM.\nIn order to compare the different surrogate model alternatives, we have performed evaluations in terms of performance metrics (how well the model predicts the target feature), and XAI metrics, that compare the explanations generated in terms representativeness, fidelity, stability and contrastiveness.\nThe evaluations showed that both EBM, and our variation of EBM, either outperform the blackbox models counterparts regarding those performance metrics, or are below but very close to them. They also provide satisfactory results analysing their metrics in absolute terms. For XAI metrics the conclusions are similar. Using EBM or our EBM variation yields either similar or even better results than using a blackbox model together with a posthoc XAI technique for local feature relevance."}, {"heading": "5.1. Future Work", "text": "We see two main research lines where our current research can be continued. The first one is regarding the unsupervised algorithm for anomaly detection. Within our proposal, we have used a boxplot applied over the average fuel consumption of the vehicles of a same group since it directly provides a limit that helps seeing the threshold value that sets apart anomalous fuel consumption and non anomalous one. It also provides a visual limit that provides an additional insight for the users since they can see the average fuel split between inliers and outliers. However, there are other unsupervised algorithms that can be used if they are able to provide that threshold limit.\nThe second line is regarding the XAI metric usage. The literature propose other aspects that can be measured in terms of human-friendly explanations, and is important to both include those aspects, as well as assessing with different real users that the metrics do indeed measure that aspects."}, {"heading": "Acknowledgements", "text": "This research was done following the registered patent [36] for LUCA Fleet at Telefo\u0301nica. We thank Pedro Antonio Alonso Baigorri, Federico Pe\u0301rez Rosado, Raquel Crespo Crisenti and Daniel Garc\u0131\u0301a Ferna\u0301ndez for their collaboration."}, {"heading": "6. Annex", "text": ""}, {"heading": "6.1. Features involved", "text": "\u2022 count harsh brakes: Total harsh brake events. \u2022 count harsh turns: Total harsh turn events. \u2022 count jackrabbit: Total jackrabbit events. \u2022 count neutral: Total events of gear position in neutral. \u2022 count reverse: Total events of gear position in reverse. \u2022 engine oil variation: Difference between maximum and\nminimum of the remaining life of the engine\u2019s oil (percentage).\n\u2022 fuel exhaust fluid variation: Difference between maximum and minimum of DEF (Diesel Exhaust Fluid).\n\u2022 fuel filter life variation: Difference between maximum and minimum of engine\u2019s fuel filter.\n\u2022 hours speed control: Hours driving with speed control set on.\n\u2022 max engine cool temp: Maximum temperature reached by the coolant.\n\u2022 max engine oil temp: Maximum temperature reached by the engine\u2019s oil.\n\u2022 mean braking acc: Mean value for braking acceleration. \u2022 mean forward acc: Mean value for front acceleration. \u2022 mean exterior temp: Mean value of the exterior temper-\nature.\n\u2022 mean speed city: Mean value of the speed within city. \u2022 mean speed hwy: Mean value of the speed within high-\nways.\n\u2022 mean tire pressure fl: Mean value of the wheel\u2019s pressure (front-left).\n\u2022 mean tire pressure rl: Mean value of the wheel\u2019s pressure (real-left).\n\u2022 mean tire pressure fr: Mean value of the wheel\u2019s pressure (front-right).\n\u2022 mean tire pressure rr: Mean value of the wheel\u2019s pressure (rear-right).\n\u2022 per fuel idle: Percentage of total fuel consumption spent for idling.\n\u2022 per time city: Percentage of time spent driving within city.\n\u2022 rpm high: Events with engine\u2019s speed (RPM) equal or above 1900.\n\u2022 rpm red: Events with engine\u2019s speed (RPM) above 3500 and vehicle speed below 40 Km/h.\n\u2022 rpm orange: Events with engine\u2019s speed (RPM) above 3500 and vehicle speed between 40 and 80 Km/h (included).\n\u2022 rpm yellow: Events with engine\u2019s speed (RPM) above 3500 and vehicle speed above 80 Km/h.\n\u2022 speed over 120: Time with driving speed above 120 Km/h.\n\u2022 total odometer: Maximum value of the odometer. \u2022 trip kms: Distance driven. \u2022 ignition events: Events of engine\u2019s ignition. \u2022 with passenger: Whether there are at least one additional\npassenger inside the vehicle in that day (1) or not (0). \u2022 lights left on: Whether the lights of the vehicle where left\non at least once that day (1) or not (0). \u2022 vehicle id: Unique id vehicle\u2019s number. \u2022 vehicle group: Vehicle group for that vehicle id. \u2022 date tx: Date for each register. \u2022 route type: Primary route type for every vehicle-day\n(0:City, 1:Combined, 2:Highway) \u2022 avg fuel consumption: Target column. Vehicle\u2019s average\nfuel consumption per 100 Km in that day."}, {"heading": "6.2. Acronyms", "text": "\u2022 ML: Machine Learning \u2022 IoT: Internet Of Things \u2022 AI: Artificial Intelligence \u2022 RAI: Responsible Artificial Intelligence \u2022 XAI: Explainable Artificial Intelligence \u2022 FAR: Fleet Analytical Record \u2022 SOTA: State Of The Art \u2022 VIN: Vehicle Identification Number \u2022 GBM: Gradient Boosting Machines \u2022 EBM: Explainable Boosting Machines \u2022 GAM: Generalized Additive Model \u2022 CV: Cross-Validation \u2022 EV: Explained Variance \u2022 ME: Mean Absolute Percentage Error \u2022 RMSE: Mean Absolute Percentage Error \u2022 MAE: Mean Absolute Percentage Error \u2022 MAPE: Mean Absolute Percentage Error \u2022 adj-R2: Adjusted R2\n6.3. Figures and Tables\nFigure 14: Flowchart followed by the process."}], "title": "Anomaly detection in average fuel consumption with XAI techniques for dynamic generation of explanations", "year": 2020}