{
  "abstractText": "Deep learning is increasingly used as a building block of security systems. Unfortunately, neural networks are hard to interpret and typically opaque to the practitioner. The machine learning community has started to address this problem by developing methods for explaining the predictions of neural networks. While several of these approaches have been successfully applied in the area of computer vision, their application in security has received little attention so far. It is an open question which explanation methods are appropriate for computer security and what requirements they need to satisfy. In this paper, we introduce criteria for comparing and evaluating explanation methods in the context of computer security. These cover general properties, such as the accuracy of explanations, as well as securityfocused aspects, such as the completeness, efficiency, and robustness. Based on our criteria, we investigate six popular explanation methods and assess their utility in security systems for malware detection and vulnerability discovery. We observe significant differences between the methods and build on these to derive general recommendations for selecting and applying explanation methods in computer security.",
  "authors": [
    {
      "affiliations": [],
      "name": "Alexander Warnecke"
    },
    {
      "affiliations": [],
      "name": "Daniel Arp"
    },
    {
      "affiliations": [],
      "name": "Christian Wressnegger"
    },
    {
      "affiliations": [],
      "name": "Konrad Rieck"
    }
  ],
  "id": "SP:8831fc37b6bcda39953cbaf9219525ef3ef968b1",
  "references": [
    {
      "authors": [
        "M. Alber",
        "S. Lapuschkin",
        "P. Seegerer",
        "M. H\u00e4gele",
        "K.T. Sch\u00fctt",
        "G. Montavon",
        "W. Samek",
        "K.-R. M\u00fcller"
      ],
      "title": "S",
      "venue": "D\u00e4hne, and P.-J. Kindermans. iNNvestigate neural networks! Technical Report abs/1808.04260, Computing Research Repository (CoRR)",
      "year": 2018
    },
    {
      "authors": [
        "M. Ancona",
        "E. Ceolini",
        "C. \u00d6ztireli",
        "M. Gross"
      ],
      "title": "Towards better understanding of gradient-based attribution methods for deep neural networks",
      "venue": "International Conference on Learning Representations, ICLR",
      "year": 2018
    },
    {
      "authors": [
        "D. Arp",
        "M. Spreitzenbarth",
        "M. H\u00fcbner",
        "H. Gascon",
        "K. Rieck. Drebin"
      ],
      "title": "Efficient and explainable detection of Android malware in your pocket",
      "venue": "In Proc. of the Network and Distributed System Security Symposium (NDSS),",
      "year": 2014
    },
    {
      "authors": [
        "L. Arras",
        "F. Horn",
        "G. Montavon",
        "K.-R. M\u00fcller",
        "W. Samek"
      ],
      "title": "what is relevant in a text document?\": An interpretable machine learning approach",
      "venue": "PLoS ONE,",
      "year": 2017
    },
    {
      "authors": [
        "S. Bach",
        "A. Binder",
        "G. Montavon",
        "F. Klauschen",
        "K.-R. M\u00fcller",
        "W. Samek"
      ],
      "title": "On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation",
      "venue": "PLoS ONE,",
      "year": 2015
    },
    {
      "authors": [
        "N. Carlini"
      ],
      "title": "Is AmI (attacks meet interpretability) robust to adversarial examples",
      "venue": "Technical Report abs/1902.02322, Computing Research Repository (CoRR),",
      "year": 2019
    },
    {
      "authors": [
        "N. Carlini",
        "D.A. Wagner"
      ],
      "title": "Towards evaluating the robustness of neural networks",
      "venue": "Proc. of the IEEE Symposium on Security and Privacy, pages 39\u201357",
      "year": 2017
    },
    {
      "authors": [
        "A. Chattopadhyay",
        "A. Sarkar",
        "P. Howlader",
        "V.N. Balasubramanian"
      ],
      "title": "Grad-cam++: Generalized gradientbased visual explanations for deep convolutional networks",
      "venue": "2018 IEEE Winter Conference on Applications of Computer Vision, WACV 2018, Lake Tahoe, NV, USA, March 12-15, 2018, pages 839\u2013847",
      "year": 2018
    },
    {
      "authors": [
        "K. Cho"
      ],
      "title": "B",
      "venue": "van Merrienboer, \u00c7. G\u00fcl\u00e7ehre, F. Bougares, H. Schwenk, and Y. Bengio. Learning phrase representations using RNN encoder-decoder for statistical machine translation. Technical Report abs/1606.04435, Computing Research Repository (CoRR)",
      "year": 2014
    },
    {
      "authors": [
        "Z.L. Chua",
        "S. Shen",
        "P. Saxena",
        "Z. Liang"
      ],
      "title": "Neural nets can learn function type signatures from binaries",
      "venue": "Proc. of the USENIX Security Symposium, pages 99\u2013116",
      "year": 2017
    },
    {
      "authors": [
        "P. Dabkowski",
        "Y. Gal"
      ],
      "title": "Real time image saliency for black box classifiers",
      "venue": "In Advances in Neural Information Proccessing Systems (NIPS),",
      "year": 2017
    },
    {
      "authors": [
        "A. Datta",
        "S. Sen",
        "Y. Zick"
      ],
      "title": "Algorithmic transparency via quantitative input influence: Theory and experiments with learning systems",
      "venue": "2016 IEEE Symposium on Security and Privacy, pages 598\u2013617",
      "year": 2016
    },
    {
      "authors": [
        "S. Diamond",
        "S. Boyd"
      ],
      "title": "CVXPY: A Pythonembedded modeling language for convex optimization",
      "venue": "Journal of Machine Learning Research",
      "year": 2016
    },
    {
      "authors": [
        "A.-K. Dombrowski",
        "M. Alber",
        "C.J. Anders",
        "M. Ackermann",
        "K.-R. M\u00fcller",
        "P. Kessel"
      ],
      "title": "Explanations can be manipulated and geometry is to blame",
      "venue": "Advances in Neural Information Proccessing Systems (NIPS)",
      "year": 2019
    },
    {
      "authors": [
        "R.O. Duda",
        "P.E. Hart",
        "D.G. Stork"
      ],
      "title": "Pattern classification",
      "venue": "John Wiley & Sons, second edition",
      "year": 2000
    },
    {
      "authors": [
        "J.L. Elman"
      ],
      "title": "Finding structure in time",
      "venue": "Cognitive Science, 14(2):179\u2013211",
      "year": 1990
    },
    {
      "authors": [
        "R.C. Fong",
        "A. Vedaldi"
      ],
      "title": "Interpretable explanations of black boxes by meaningful perturbation",
      "venue": "IEEE International Conference on Computer Vision, pages 3449\u20133457",
      "year": 2017
    },
    {
      "authors": [
        "J. Foremost"
      ],
      "title": "DroidDream mobile malware",
      "venue": "https://www.virusbulletin.com/virusbulletin/2012/ 03/droiddream-mobile-malware, 2012. ",
      "year": 2019
    },
    {
      "authors": [
        "I. Goodfellow",
        "Y. Bengio",
        "A. Courville"
      ],
      "title": "Deep Learning",
      "venue": "MIT Press",
      "year": 2016
    },
    {
      "authors": [
        "K. Grosse",
        "N. Papernot",
        "P. Manoharan",
        "M. Backes",
        "P.D. McDaniel"
      ],
      "title": "Adversarial examples for malware detection",
      "venue": "Proc. of the European Symposium on Research in Computer Security (ESORICS), pages 62\u201379",
      "year": 2017
    },
    {
      "authors": [
        "W. Guo",
        "D. Mu",
        "J. Xu",
        "P. Su",
        "G. Wang",
        "X. Xing"
      ],
      "title": "LEMNA: Explaining deep learning based security applications",
      "venue": "Proc. of the ACM Conference on Computer and Communications Security (CCS), pages 364\u2013379",
      "year": 2018
    },
    {
      "authors": [
        "S. Hochreiter",
        "J. Schmidhuber"
      ],
      "title": "Long short-term memory",
      "venue": "Neural Computation, 9:1735\u20131780",
      "year": 1997
    },
    {
      "authors": [
        "W. Huang",
        "J.W. Stokes"
      ],
      "title": "MtNet: A multi-task neural network for dynamic malware classification",
      "venue": "Proc. of the Conference on Detection of Intrusions and Malware & Vulnerability Assessment (DIMVA), pages 399\u2013418",
      "year": 2016
    },
    {
      "authors": [
        "P. jan Kindermans",
        "K.T. Sch\u00fctt",
        "M. Alber",
        "K.-R. M\u00fcller",
        "D. Erhan",
        "B. Kim",
        "S. D\u00e4hne"
      ],
      "title": "Learning how to explain neural networks: Patternnet and patternattribution",
      "venue": "In Proc. of the International Conference on Learning Representations (ICLR),",
      "year": 2018
    },
    {
      "authors": [
        "X. Jiang"
      ],
      "title": "Security Alert: New sophisticated Android malware DroidKungFu found in alternative chinese App markets",
      "venue": "https://www.csc2.ncsu.edu/faculty/ xjiang4/DroidKungFu.html, 2011. ",
      "year": 2019
    },
    {
      "authors": [
        "X. Jiang"
      ],
      "title": "Security Alert: New Android malware GoldDream found in alternative app markets",
      "venue": "https://www.csc2.ncsu.edu/faculty/xjiang4/ GoldDream/, 2011. ",
      "year": 2019
    },
    {
      "authors": [
        "A. Kapravelos",
        "Y. Shoshitaishvili",
        "M. Cova",
        "C. Kruegel",
        "G. Vigna"
      ],
      "title": "Revolver: An automated approach to the detection of evasive web-based malware",
      "venue": "In Proc. of the USENIX Security Symposium,",
      "year": 2013
    },
    {
      "authors": [
        "A. Krizhevsky",
        "I. Sutskever",
        "G.E. Hinton"
      ],
      "title": "Imagenet classification with deep convolutional neural networks",
      "venue": "Advances in Neural Information Proccessing Systems (NIPS). Curran Associates, Inc.",
      "year": 2012
    },
    {
      "authors": [
        "Y. LeCun",
        "Y. Bengio"
      ],
      "title": "Convolutional networks for images",
      "venue": "speech, and time-series. In The Handbook of Brain Theory and Neural Networks. MIT",
      "year": 1995
    },
    {
      "authors": [
        "Z. Li",
        "D. Zou",
        "S. Xu",
        "X. Ou",
        "H. Jin",
        "S. Wang",
        "Z. Deng",
        "Y. Zhong"
      ],
      "title": "Vuldeepecker: A deep learning-based system for vulnerability detection",
      "venue": "Proc. of the Network and Distributed System Security Symposium (NDSS)",
      "year": 2018
    },
    {
      "authors": [
        "S.M. Lundberg",
        "S.-I. Lee"
      ],
      "title": "A unified approach to interpreting model predictions",
      "venue": "In Advances in Neural Information Proccessing Systems (NIPS),",
      "year": 2017
    },
    {
      "authors": [
        "N. McLaughlin"
      ],
      "title": "J",
      "venue": "M. del Rinc\u00c3\u015fn, B. Kang, S. Y. Yerima, P. C. Miller, S. Sezer, Y. Safaei, E. Trickel, Z. Zhao, A. Doup\u00c3l\u2019, and G.-J. Ahn. Deep android malware detection. In Proc. of the ACM Conference on Data and Application Security and Privacy (CODASPY), pages 301\u2013308",
      "year": 2017
    },
    {
      "authors": [
        "T. Mikolov",
        "K. Chen",
        "G. Corrado",
        "J. Dean"
      ],
      "title": "Efficient estimation of word representations in vector space",
      "venue": "Proc. of the International Conference on Learning Representations (ICLR Workshop)",
      "year": 2013
    },
    {
      "authors": [
        "N. Papernot",
        "P.D. McDaniel",
        "A. Sinha",
        "M.P. Wellman"
      ],
      "title": "Sok: Security and privacy in machine learning",
      "venue": "Proc. of the IEEE European Symposium on Security and Privacy (EuroS&P), pages 399\u2013414",
      "year": 2018
    },
    {
      "authors": [
        "M.T. Ribeiro",
        "S. Singh",
        "C. Guestrin"
      ],
      "title": "why should i trust you?\": Explaining the predictions of any classifier",
      "venue": "Proc. of the ACM SIGKDD International Conference On Knowledge Discovery and Data Mining (KDD)",
      "year": 2016
    },
    {
      "authors": [
        "R. Rojas"
      ],
      "title": "Neural Networks: A Systematic Approach",
      "venue": "Springer-Verlag, Berlin, Deutschland",
      "year": 1996
    },
    {
      "authors": [
        "D.E. Rumelhart",
        "G.E. Hinton",
        "R.J. Williams"
      ],
      "title": "Learning internal representations by error propagation",
      "venue": "Parallel distributed processing: Explorations in the microstructure of cognition, 1(Foundation)",
      "year": 1986
    },
    {
      "authors": [
        "R.R. Selvaraju",
        "M. Cogswell",
        "A. Das",
        "R. Vedantam",
        "D. Parikh",
        "D. Batra"
      ],
      "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
      "venue": "In The IEEE International Conference on Computer Vision (ICCV),",
      "year": 2017
    },
    {
      "authors": [
        "L. Shapley"
      ],
      "title": "A value for n-person games",
      "year": 1953
    },
    {
      "authors": [
        "E.C.R. Shin",
        "D. Song",
        "R. Moazzezi"
      ],
      "title": "Recognizing functions in binaries with neural networks",
      "venue": "Proc. of the USENIX Security Symposium, pages 611\u2013626",
      "year": 2015
    },
    {
      "authors": [
        "A. Shrikumar",
        "P. Greenside",
        "A. Kundaje"
      ],
      "title": "Learning important features through propagating activation differences",
      "venue": "Proc. of the International Conference on Machine Learning (ICML), pages 3145\u20133153",
      "year": 2017
    },
    {
      "authors": [
        "K. Simonyan",
        "A. Vedaldi",
        "A. Zisserman"
      ],
      "title": "Deep inside convolutional networks: Visualising image classification models and saliency maps",
      "venue": "Proc. of the International Conference on Learning Representations (ICLR)",
      "year": 2014
    },
    {
      "authors": [
        "D. Slack",
        "S. Hilgard",
        "E. Jia",
        "S. Singh",
        "H. Lakkaraju"
      ],
      "title": "Fooling lime and shap: Adversarial attacks on post hoc explanation methods",
      "venue": "AAAI/ACM Conference on Artificial Intelligence , Ethics, and Society (AIES)",
      "year": 2019
    },
    {
      "authors": [
        "C. Smutz",
        "A. Stavrou"
      ],
      "title": "Malicious PDF detection using metadata and structural features",
      "venue": "Proc. of the Annual Computer Security Applications Conference (ACSAC), pages 239\u2013248",
      "year": 2012
    },
    {
      "authors": [
        "J. Springenberg",
        "A. Dosovitskiy",
        "T. Brox",
        "M. Riedmiller"
      ],
      "title": "Striving for simplicity: The all convolutional net",
      "venue": "ICLR (workshop track)",
      "year": 2015
    },
    {
      "authors": [
        "M. Sundararajan",
        "A. Taly",
        "Q. Yan"
      ],
      "title": "Axiomatic attribution for deep networks",
      "venue": "Proceedings of the 34th International Conference on Machine Learning, pages 3319\u20133328",
      "year": 2017
    },
    {
      "authors": [
        "I. Sutskever",
        "O. Vinyals",
        "Q.V. Le"
      ],
      "title": "Sequence to sequence learning with neural networks",
      "venue": "Advances in Neural Information Proccessing Systems (NIPS), pages 3104\u20133112",
      "year": 2014
    },
    {
      "authors": [
        "F. Tram\u00e8r",
        "F. Zhang",
        "A. Juels",
        "M.K. Reiter",
        "T. Ristenpart"
      ],
      "title": "Stealing machine learning models via prediction apis",
      "venue": "In 25th USENIX Security Symposium (USENIX Security",
      "year": 2016
    },
    {
      "authors": [
        "N. \u0160rndi\u0107",
        "P. Laskov"
      ],
      "title": "Practical evasion of a learning-based classifier: A case study",
      "venue": "Proc. of the IEEE Symposium on Security and Privacy, pages 197\u2013211",
      "year": 2014
    },
    {
      "authors": [
        "X. Xu",
        "C. Liu",
        "Q. Feng",
        "H. Yin",
        "L. Song",
        "D. Song"
      ],
      "title": "Neural network-based graph embedding for cross-platform binary code similarity detection",
      "venue": "Proc. of the ACM Conference on Computer and Communications Security (CCS), pages 363\u2013376",
      "year": 2017
    },
    {
      "authors": [
        "M.D. Zeiler",
        "R. Fergus"
      ],
      "title": "Visualizing and understanding convolutional networks",
      "venue": "Computer Vision \u2013 ECCV 2014, pages 818\u2013833. Springer International Publishing",
      "year": 2014
    },
    {
      "authors": [
        "X. Zhang",
        "N. Wang",
        "H. Shen",
        "S. Ji",
        "X. Luo",
        "T. Wang"
      ],
      "title": "Interpretable deep learning under fire",
      "venue": "Proc. of USENIX Security Symposium",
      "year": 2019
    },
    {
      "authors": [
        "B. Zhou",
        "A. Khosla",
        "A. Lapedriza",
        "A. Oliva",
        "A. Torralba"
      ],
      "title": "Learning deep features for discriminative localization",
      "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2921\u20132929",
      "year": 2016
    },
    {
      "authors": [
        "Y. Zhou",
        "X. Jiang"
      ],
      "title": "Dissecting android malware: Characterization and evolution",
      "venue": "Proc. of the IEEE Symposium on Security and Privacy, pages 95\u2013109",
      "year": 2012
    }
  ],
  "sections": [
    {
      "heading": "1. Introduction",
      "text": "Over the last years, deep learning has been increasingly recognized as an effective tool for computer security. Different types of neural networks have been integrated into security systems, for example, for malware detection [20, 23, 33], binary analysis [10, 41, 53], and vulnerability discovery [30]. Deep learning, however, suffers from a severe drawback: Neural networks are hard to interpret and their decisions are opaque to practitioners. Even simple tasks, such as determining which features of an input contribute to a prediction, are challenging to solve on neural networks. This lack of transparency is a considerable problem in security, as black-box learning systems are hard to audit and protect from attacks [7, 35].\nThe machine learning community has started to develop methods for interpreting deep learning in computer vision [e.g., 5, 43, 54]. These methods enable tracing back the predictions of neural networks to individual regions in images and thereby help to understand the decision process. These approaches have been further extended to also explain predictions on text and sequences [4, 21]. Surprisingly, this work has received little attention in security and there exists only a single technique that has been investigated so far [21].\nIn contrast to other application domains of deep learning, computer security poses particular challenges for the use of explanation methods. First, security tasks, such as malware detection and binary code analysis, require complex neural network architectures that are challenging to investigate. Second, explanation methods in security do not only need to be accurate but also satisfy security-specific requirements, such as complete and robust explanations. As a result of these challenges, it is an unanswered question which of the available explanation methods can be applied in security and what properties they need to possess for providing reliable results.\nIn this paper, we address this problem and develop evaluation criteria for assessing and comparing explanation methods in security. Our work provides a bridge between deep learning in security and explanation methods developed for other application domains of machine learning. Consequently, our criteria for judging explanations cover general properties of deep learning as well as aspects that are especially relevant to the domain of security.\nGeneral evaluation criteria. As general criteria, we consider the descriptive accuracy and sparsity of explanations. These properties reflect how accurate and concise an explanation method captures relevant features of a prediction. While accuracy is an evident criterion for obtaining reliable results, sparsity is another crucial constraint in security. In contrast to computer vision, where an analyst can examine an entire image, a security practitioner cannot investigate large sets of features at once, and thus sparsity becomes an essential property when non-graphic data is analyzed.\nSecurity evaluation criteria. We define the completeness, stability, robustness, and efficiency of explanations as security criteria. These properties ensure that reliable explanations are available to a practitioner in all cases and in reasonable time\u2014requirements that are less important in other areas of deep learning. For example, an attacker may expose pathologic inputs to a security system that mislead, corrupt, or slow down the computation of explanations. Note that the robustness of explanation methods to adversarial examples is not well understood yet, and thus we base our analysis on the recent work by Zhang et al. [55] and Dombrowski et al. [14].\nWith the help of these criteria, we analyze six recent explanation methods and assess their performance in different security tasks. To this end, we implement four security systems from the literature that make use of deep learning and enable detecting Android malware [20, 33], malicious PDF files [50], and security vulnerabilities [30],\nar X\niv :1\n90 6.\n02 10\n8v 4\n[ cs\n.L G\n] 2\n7 A\npr 2\n02 0\nrespectively. When explaining the decisions of these systems, we observe significant differences between the methods in all criteria. Some methods are not capable of providing sparse results, whereas others struggle with structured security data or suffer from unstable outputs. While the importance of the individual criteria depends on the particular task, we find that the methods IG [47] and LRP [5] comply best with all criteria and resemble general-purpose techniques for security systems.\nTo demonstrate the utility of explainable learning, we also qualitatively examine the generated explanations. As an example for this investigation, Figure 1 shows three explanations for the system VulDeePecker [30] that identifies vulnerabilities in source code. While the first explanation method provides a nuanced representation of the relevant features, the second method generates an unsharp explanation due to a lack of sparsity. The third approach provides an explanation that even contradicts the first one. Note that the variables VAR2 and VAR3 receive a positive relevance (blue) in the first case and a negative relevance (orange) in the third.\nOur evaluation highlights the need for comparing explanation methods and determining the best fit for a given security task. Furthermore, it also unveils a notable number of artifacts in the underlying datasets. For all of the four security tasks, we identify features that are unrelated to security but strongly contribute to the predictions. As a consequence, we argue that explanation methods need to become an integral part of learning-based security systems\u2014first, for understanding the decision process of deep learning and, second, for eliminating artifacts in the training datasets.\nThe rest of this paper is organized as follows: We briefly review the technical background of explainable learning in Section 2. The explanation methods and security systems under test are described in Section 3. We introduce our criteria for comparing explanation methods in Section 4 and evaluate them in Section 5. Our qualitative analysis is presented in Section 6 and Section 7 concludes the paper."
    },
    {
      "heading": "2. Explainable Deep Learning",
      "text": "Neural networks have been used in artificial intelligence for over 50 years, yet concepts for explaining their decisions have just recently started to be explored. This development has been driven by the remarkable progress of deep learning in several areas, such as image recognition [28] and machine translation [48]. To embed our work in this context, we briefly review two aspects of explainable learning that are crucial for its application in security: the type of neural network and the explanation strategy."
    },
    {
      "heading": "2.1. Neural Network Architectures",
      "text": "Different architectures can be used for constructing a neural network, ranging from general-purpose networks to highly specific architectures. In the area of security, three of these architectures are prevalent: multilayer perceptrons, convolutional neural networks, and recurrent neural networks (see Figure 2). Consequently, we focus our study on these network types and refer the reader to the books by Rojas [37] and Goodfellow et al. [19] for a detailed description of network architectures in general.\nMultilayer Perceptrons (MLPs). Multilayer perceptrons, also referred to as feedforward networks, are a classic and general-purpose network architecture [38]. The network is composed of multiple fully connected layers of neurons, where the first and last layer correspond to the input and output of the network, respectively. MLPs have been successfully applied to a variety of security problems, such as intrusion and malware detection [20, 23]. While MLP architectures are not necessarily complex, explaining the contribution of individual features is still difficult, as several neurons impact the decision when passing through the network layers.\nConvolutional Neural Networks (CNNs). These networks share a similar architecture with MLPs, yet they differ in the concept of convolution and pooling [29]. The neurons in convolutional layers receive input only from a local neighborhood of the previous layer. These neighborhoods overlap and create receptive fields that provide a powerful primitive for identifying spatial structure in data. CNNs have thus been successfully used for detecting malicious patterns in the bytecode of Android applications [33]. Due to the convolution and pooling layers, however, it is hard to explain the decisions of a CNN, as its output needs to be \u201cunfolded\u201d and \u201cunpooled\u201d for analysis.\nRecurrent Neural Networks (RNNs). Recurrent networks, such as LSTM and GRU networks [9, 22], are characterized by a recurrent structure, that is, some neurons are connected in a loop. This structure enables memorizing information and allows RNNs to operate on sequences of data [16]. As a result, RNNs have been successfully applied in security tasks involving sequential data, such as the recognition of functions in native code [10, 41] or the discovery of vulnerabilities in software [30]. Interpreting the prediction of an RNN is also difficult, as the relevance of an input feature depends on the sequence of previously processed features."
    },
    {
      "heading": "2.2. Explanation Strategies",
      "text": "Given the different architectures and the complexity of many neural networks, decoding the entire decision process is a challenging task that currently cannot be solved adequately. However, there exist several recent methods that enable explaining individual predictions of a neural network instead of the complete decision process [e.g., 5, 21, 36, 47, 54]. We focus on this form of explainable learning that can be formally defined as follows:\nDefinition 1. Given an input vector x = (x1, . . . , xd), a neural network N , and a prediction fN (x) = y, an explanation method determines why the label y has been selected by N . This explanation is given by a vector r = (r1, . . . , rd) that describes the relevance of the dimensions of x for fN (x).\nThe computed relevance values r are typically real numbers and can be overlayed with the input in form of a heatmap, such that relevant features are visually highlighted. An example of this visualization is depicted in Figure 1. Positive relevance values are shown in blue and indicate importance towards the prediction fN (x), whereas negative values are given in orange and indicate importance against the prediction. We will use this color scheme throughout the paper1.\nDespite the variety of approaches for computing a relevance vector for a given neural network and an input, all approaches can be broadly categorized into two explanation strategies: black-box and white-box explanations.\nBlack-box Explanations. These methods operate under a black-box setting that assumes no knowledge about the neural network and its parameters. Black-box methods are an effective tool if no access to the neural network is available, for example, when a learning service is audited remotely. Technically, black-box methods rest on an approximation of the function fN , which enables them to estimate how the dimensions of x contribute to a prediction. Although black-box methods are a promising approach for explaining deep learning, they can be impaired by the black-box setting and omit valuable information provided through the network architecture and parameters.\nWhite-box Explanations. These approaches operate under the assumption that all parameters of a neural network are known and can be used for determining an explanation. As a result, these methods do not rely on approximations\n1. We use the blue-orange color scheme instead of the typical green-red scheme to make our paper better accessible to color-blind readers.\nand can directly compute explanations for the function fN on the structure of the network. In practice, predictions and explanations are often computed from within the same system, such that the neural network is readily available for generating explanations. This is usually the case for stand-alone systems for malware detection, binary analysis, and vulnerability discovery. However, several white-box methods are designed for specific network layouts from computer vision and not applicable to all considered architectures [e.g., 43, 46, 54].\nBlack-box and white-box explanation methods often share similarities with concepts of adversarial learning and feature selection, as these also aim at identifying features related to the prediction of a classifier. However, adversarial learning and feature selection pursue fundamentally different goals and cannot be directly applied for explaining neural networks. We discuss the differences to these approaches for the interested reader in Appendix A."
    },
    {
      "heading": "3. Methods and Systems under Test",
      "text": "Before presenting our criteria for evaluating explanation methods, we first introduce the methods and systems under test. In particular, we cover six methods for explaining predictions in Section 3.1 and present four security systems based on deep learning in Section 3.2. For more information about explanation methods we do not evaluate in the paper [e.g., 12, 17] we refer the reader to the Appendix B."
    },
    {
      "heading": "3.1. Explanation Methods",
      "text": "Table 1 provides an overview of popular explanation methods along with their support for the different network architectures. As we are interested in explaining predictions of security systems, we select those methods for our study that are applicable to all common architectures. In the following, we briefly sketch the main idea of these approaches for computing relevance vectors, illustrating the technical diversity of explanation methods.\nGradients and IG. One of the first white-box methods to compute explanations for neural networks has been introduced by Simonyan et al. [43] and is based on simple gradients. The output of the method is given by ri = \u2202y/\u2202xi, which the authors call a saliency map. Here ri measures how much y changes with respect to xi. Sundararajan et al. [47] extend this approach and propose Integrated Gradients (IG) that use a baseline x\u2032, for instance a vector of zeros, and calculate the shortest path from x\u2032\nto x, given by x\u2212x\u2032. To compute the relevance of xi, the gradients with respect to xi are cumulated along this path yielding\nri = (xi \u2212 x\u2032i) \u222b 1 0 \u2202fN (x \u2032 + \u03b1(x\u2212 x\u2032)) \u2202xi d\u03b1.\nBoth gradient-based methods can be applied to all relevant network architectures and thus are considered in our comparative evaluation of explanation methods.\nLRP and DeepLift. These popular white-box methods determine the relevance of a prediction by performing a backward pass through the neural network, starting at the output layer and performing calculations until the input layer is reached [5]. The central idea of layer-wise relevance propagation (LRP) is the use of a conservation property that needs to hold true during the backward pass. If rli is the relevance of the unit i in layer l of the neural network then\u2211\ni r1i = \u2211 i r2i = \u00b7 \u00b7 \u00b7 = \u2211 i rLi\nneeds to hold true for all L layers. Similarly, DeepLift performs a backward pass but takes a reference activation y\u2032 = fN (x\n\u2032) of a reference input x\u2032 into account. The method enforces the conservation law,\u2211\ni\nri = y \u2212 y\u2032 = \u2206y ,\nthat is, the relevance assigned to the features must sum up to the difference between the outcome of x and x\u2032. Both approaches support explaining the decisions of feedforward, convolutional and recurrent neural networks [see 4]. However, as DeepLift and IG are closely related [2], we focus our study on the method -LRP.\nLIME and SHAP. Ribeiro et al. [36] introduce one of the first black-box methods for explaining neural networks that is further extended by Lundberg and Lee [31]. Both methods aim at approximating the decision function fN by creating a series of l perturbations of x, denoted as x\u03031, . . . , x\u0303l by setting entries in the vector x to 0 randomly. The methods then proceed by predicting a label fN (x\u0303i) = y\u0303i for each x\u0303i of the l perturbations. This sampling strategy enables the methods to approximate the local neighborhood of fN at the point fN (x). LIME [36] approximates the decision boundary by a weighted linear regression model,\narg min g\u2208G l\u2211 i=1 \u03c0x(x\u0303i) ( fN (x\u0303i)\u2212 g(x\u0303i) )2 ,\nwhere G is the set of all linear functions and \u03c0x is a function indicating the difference between the input x and a perturbation x\u0303. SHAP [31] follows the same approach but uses the SHAP kernel as weighting function \u03c0x, which is shown to create Shapley Values [40] when solving the regression. Shapley Values are a concept from game theory where the features act as players under the objective of finding a fair contribution of the features to the payout\u2014in this case the prediction of the model. As both approaches can be applied to any learning model, we study them in our empirical evaluation.\nLEMNA. As last explanation method, we consider LEMNA, a black-box method specifically designed for security applications [21]. It uses a mixture regression model for approximation, that is, a weighted sum of K linear models:\nf(x) = K\u2211 j=1 \u03c0j(\u03b2j \u00b7 x+ j).\nThe parameter K specifies the number of models, the random variables = ( 1, . . . , K) originate from a normal distribution i \u223c N(0, \u03c3) and \u03c0 = (\u03c01, . . . , \u03c0K) holds the weights for each model. The variables \u03b21, . . . , \u03b2K are the regression coefficients and can be interpreted as K linear approximations of the decision boundary near fN (x)."
    },
    {
      "heading": "3.2. Security Systems",
      "text": "As field of application for the six explanation methods, we consider four recent security systems that employ deep learning (see Table 2). The systems cover the three major architectures/types introduced in Section 2.1 and comprise between 4 to 6 layers of different types.\nDrebin+. The first system uses an MLP for identifying Android malware. The system has been proposed by Grosse et al. [20] and builds on features originally developed by Arp et al. [3]. The network consists of two hidden layers, each comprising 200 neurons. The input features are statically extracted from Android applications and cover data from the application\u2019s manifest, such as hardware details and requested permissions, as well as information based on the application\u2019s code, such as suspicious API calls and network addresses. To verify the correctness of our implementation, we train the system on the original Drebin dataset [3], where we use 75 % of the 129,013 Android application for training and 25 % for testing. Table 3 shows the results of this experiment, which are in line with the performance published by Grosse et al. [20].\nMimicus+. The second system also uses an MLP but is designed to detect malicious PDF documents. The system is re-implemented based on the work of Guo et al. [21] and builds on features originally introduced by Smutz and Stavrou [45]. Our implementation uses two hidden layers with 200 nodes each and is trained with 135 features extracted from PDF documents. These features cover properties about the document structure, such as the number of sections and fonts in the document, and are mapped to binary values as described by Guo et al. [21]. For a full list of features, we refer the reader to the implementation by \u0160rndic\u0301 and Laskov [50]. For verifying our implementation, we make use of the original dataset that contains 5,000 benign and 5,000 malicious PDF files and again split the dataset into 75 % for training and 25 %\nfor testing. Our results are shown in Table 3 and come close to a perfect detection.\nDAMD. The third security system studied in our evaluation uses a CNN for identifying malicious Android applications [33]. The system processes the raw Dalvik bytecode of Android applications and its neural network is comprised of six layers for embedding, convolution, and max-pooling of the extracted instructions. As the system processes entire applications, the number of features depends on the size of the applications. For a detailed description of this process, we refer the reader to the publication by McLaughlin et al. [33]. To replicate the original results, we apply the system to data from the Malware Genome Project [57]. This dataset consists of 2,123 applications in total, with 863 benign and 1,260 malicious samples. We again split the dataset into 75 % of training and 25 % of testing data and obtain results similar to those presented in the original publication.\nVulDeePecker. The fourth system uses an RNN for discovering vulnerabilities in source code [30]. The RNN consists of five layers, uses 300 LSTM cells [22], and applies a word2vec embedding [34] with 200 dimensions for analyzing C/C++ code. As a preprocessing step, the source code is sliced into code gadgets that comprise short snippets of tokens. The gadgets are truncated or padded to a length of 50 tokens. For verifying the correctness of our implementation, we use the CWE-119 dataset, which consists of 39,757 code gadgets, with 10,444 gadgets corresponding to vulnerabilities. In line with the original study, we split the dataset into 80 % training and 20 % testing data, and attain a comparable accuracy.\nThe four selected security systems provide a diverse view on the current use of deep learning in security. Drebin+ and Mimicus+ are examples of systems that make use of MLPs for detecting malware. However, they differ in the dimensionality of the input: While Mimicus+ works on a small set of engineered features, Drebin+ analyzes inputs with thousands of dimensions. DAMD is an example of a system using a CNN in security and capable of learning from large inputs, whereas VulDeePecker makes use of an RNN, similar to other learning-based approaches analyzing program code [e.g., 10, 41, 53]."
    },
    {
      "heading": "4. Evaluation Criteria",
      "text": "In light of the broad range of available explanation methods, the practitioner is in need of criteria for selecting the best method for a security task at hand. In this section, we develop these criteria and demonstrate their utility in different examples. Before doing so, however, we address another important question: Do the considered explanation methods provide different results? If the methods generated\nsimilar explanations, criteria for their comparison would be less important and any suitable method could be chosen in practice.\nTo answer this question, we investigate the top-k features of the six explanation methods when explaining predictions of the security systems. That is, we compare the set Ti of the k features with the highest relevance from method i with the set Tj of the k features with the highest relevance from method j. In particular, we compute the intersection size\nIS(i, j) = |Ti \u2229 Tj |\nk , (1)\nas a measure of similarity between the two methods. The intersection size lies between 0 and 1, where 0 indicates no overlap and 1 corresponds to identical top-k features.\nA visualization of the intersection size averaged over the samples of the four datasets is shown in Figure 3. We choose k = 10 according to a typical use case of explainable learning: An expert investigates the top-10 features to gain insights on a prediction. For DAMD, we use k = 50, as the dataset is comprised of long opcode sequences. We observe that the top features of the explanation methods differ considerably. For example, in the case of VulDeePecker, all methods determine different top-10 features. While we notice some similarity between the methods, it becomes clear that the methods cannot be simply interchanged, and there is a need for measurable evaluation criteria."
    },
    {
      "heading": "4.1. General Criteria: Descriptive Accuracy",
      "text": "As the first evaluation criteria, we introduce the descriptive accuracy. This criterion reflects how accurate an explanation method captures relevant features of a prediction. As it is difficult to assess the relation between features and a prediction directly, we follow an indirect\nstrategy and measure how removing the most relevant features changes the prediction of the neural network.\nDefinition 2. Given a sample x, the descriptive accuracy (DA) is calculated by removing the k most relevant features x1, . . . , xk from the sample, computing the new prediction using fN and measuring the score of the original prediction class c without the k features,\nDAk ( x, fN ) = fN ( x |x1 = 0, . . . , xk = 0 ) c .\nIf we remove relevant features from a sample, the accuracy should decrease, as the neural network has less information for making a correct prediction. The better the explanation, the quicker the accuracy will drop, as the removed features capture more context of the predictions. Consequently, explanation methods with a steep decline of the descriptive accuracy provide better explanations than methods with a gradual decrease.\nTo demonstrate the utility of the descriptive accuracy, we consider a sample from the VulDeePecker dataset, which is shown in Figure 4(a). The sample corresponds to a program slice and is passed to the neural network as a sequence of tokens. Figures 4(b) and 4(c) depict these tokens overlayed with the explanations of the methods Integrated Gradients (IG) and LIME, respectively. Note that the VulDeePecker system truncates all code snippets to a length of 50 tokens before processing them through the neural network [30].\nThe example shows a simple buffer overflow which originates from an incorrect calculation of the buffer size in line 7. The two explanation methods significantly differ when explaining the detection of this vulnerability. While IG highlights the wmemset call as important, LIME highlights the call to memmove and even marks wmemset as speaking against the detection. Measuring the descriptive accuracy can help to determine which of the two explanations reflects the prediction of the system better."
    },
    {
      "heading": "4.2. General Criteria: Descriptive Sparsity",
      "text": "Assigning high relevance to features which impact a prediction is a necessary prerequisite for good explanations. However, a human analyst can only process a limited number of these features, and thus we define the descriptive sparsity as a further criterion for comparing explanations methods as follows:\nDefinition 3. The descriptive sparsity is measured by scaling the relevance values to the range [\u22121, 1], computing a normalized histogram h of them and calculating the mass around zero (MAZ) defined by\nMAZ(r) = \u222b r \u2212r h(x)dx for r \u2208 [0, 1].\nThe MAZ can be thought of as a window which starts at 0 and grows uniformly into the positive and negative direction of the x axis. For each window, the fraction of relevance values that lies in the window is evaluated. Sparse explanations have a steep rise in MAZ close to 0 and are flat around 1, as most of the features are not marked as relevant. By contrast, dense explanations have a notable smaller slope close to 0, indicating a larger set of relevant features. Consequently, explanation methods with a MAZ distribution peaking at 0 should be preferred over methods with less pronounced distributions.\nAs an example of a sparse and dense explanation, we consider two explanations generated for a malicious Android application of the DAMD dataset. Table 4 shows a snapshot of these explanations, covering opcodes of the onReceive method. LRP provides a crisp representation in this setting, whereas LEMNA marks the entire snapshot as relevant. If we normalize the relevance vectors to [\u22121, 1] and focus on features above 0.2, LRP returns only 14 relevant features for investigation, whereas LEMNA returns 2,048 features, rendering a manual examination tedious.\nIt is important to note that the descriptive accuracy and the descriptive sparsity are not correlated and must both be satisfied by an effective explanation method. A method marking all features as relevant while highlighting a few ones can be accurate but is clearly not sparse. Vice versa, a method assigning high relevance to very few meaningless features is sparse but not accurate."
    },
    {
      "heading": "4.3. Security Criteria: Completeness",
      "text": "After introducing two generic evaluation criteria, we start focusing on aspects that are especially important for the area of security. In a security system, an explanation method must be capable of creating proper results in all possible situations. If some inputs, such as pathological data or corner cases, cannot be processed by an explanation method, an adversary may trick the method into producing degenerated results. Consequently, we propose completeness as the first security-specific criterion.\nDefinition 4. An explanation method is complete, if it can generate non-degenerated explanations for all possible input vectors of the prediction function fN .\nSeveral white-box methods are complete by definition, as they calculate relevance vectors directly from the weights of the neural network. For black-box methods, however, the situation is different: If a method approximates the prediction function fN using random perturbations, it may fail to derive a valid estimate of fN and return degenerated explanations. We investigate this phenomenon in more detail in Section 5.4.\nAs an example of this problem, Table 5 shows explanations generated by the methods Gradients and SHAP for a benign Android application of the Drebin dataset. The Gradients explanation finds the touchscreen feature in combination with the launcher category and the internet permission as an explanation for the benign classification. SHAP, however, creates an explanation of zeros which provides no insights. The reason for this degenerated explanation is rooted in the random perturbations used by SHAP. By flipping the value of features, these perturbations aim at changing the class label of the input. As there exist far more benign features than malicious ones in the case of Drebin+, the perturbations can fail to switch the label and prevent the linear regression to work resulting in a degenerated explanation."
    },
    {
      "heading": "4.4. Security Criteria: Stability",
      "text": "In addition to complete results, the explanations generated in a security system need to be reliable. That is, relevant features must not be affected by fluctuations and need to remain stable over time in order to be useful for an expert. As a consequence, we define stability as another security-specific evaluation criterion.\nDefinition 5. An explanation methods is stable, if the generated explanations do not vary between multiple runs.\nThat is, for any run i and j of the method, the intersection size of the top features Ti and Tj should be close to 1, that is, IS(i, j) > 1\u2212 for some small threshold .\nThe stability of an explanation method can be empirically determined by running the methods multiple times and computing the average intersection size, as explained in the beginning of this section. White-box methods are deterministic by construction since they perform a fixed sequence of computations for generating an explanation. Most black-box methods, however, require random perturbations to compute their output which can lead to different results for the same input. Table 6, for instance, shows the output of LEMNA for a PDF document from the Mimicus+ dataset over two runs. Some of the most relevant features from the first run receive very little relevance in the second run and vice versa, rendering the explanations unstable. We analyze these instabilities of the explanation methods in Section 5.5."
    },
    {
      "heading": "4.5. Security Criteria: Efficiency",
      "text": "When operating a security system in practice, explanations need to be available in reasonable time. While low run-time is not a strict requirement in general, time differences between minutes and milliseconds are still significant. For example, when dealing with large amounts of data, it might be desirable for the analyst to create\nexplanations for every sample of an entire class. We thus define efficiency as a further criterion for explanation methods in security applications.\nDefinition 6. We consider a method efficient if it enables providing explanations without delaying the typical workflow of an expert.\nAs the workflow depends on the particular security task, we do not define concrete run-time numbers, yet we provide a negative example as an illustration. The runtime of the method LEMNA depends on the size of the inputs. For the largest sample of the DAMD dataset with 530,000 features, it requires about one hour for computing an explanation, which obstructs the workflow of inspecting Android malware severely."
    },
    {
      "heading": "4.6. Security Criteria: Robustness",
      "text": "As the last criterion, we consider the robustness of explanation methods to attacks. Recently, several attacks [e.g., 14, 44, 55] have shown that explanation methods may suffer from adversarial perturbations and can be tricked into returning incorrect relevance vectors, similarly to adversarial examples [7]. The objective of these attacks is to disconnect the explanation from the underlying prediction, such that arbitrary relevance values can be generated that do not explain the behavior of the model.\nDefinition 7. An explanation method is robust if the computed relevance vector cannot be decoupled from the prediction by an adversarial perturbation.\nUnfortunately, the robustness of explanation methods is still not well understood and, similarly to adversarial examples, guarantees and strong defenses have not been established yet. To this end, we assess the robustness of the explanation methods based on the existing literature."
    },
    {
      "heading": "5. Evaluation",
      "text": "Equipped with evaluation criteria for comparing explanation methods, we proceed to empirically investigate these in different security tasks. To this end, we implement a comparison framework that integrates the six selected explanation methods and four security systems."
    },
    {
      "heading": "5.1. Experimental Setup",
      "text": "White-box Explanations. For our comparison framework, we make use of the iNNvestigate toolbox by Alber et al. [1] that provides efficient implementations for LRP, Gradients, and IG. For the security system VulDeePecker, we use our own LRP implementation [51] based on the publication by Arras et al. [4]. In all experiments, we set = 10\u22123 for LRP and use N = 64 steps for IG. Due to the high dimensional embedding space of VulDeePecker, we choose a step count of N = 256 in the corresponding experiments.\nBlack-box Explanations. We re-implement LEMNA in accordance to Guo et al. [21] and use the Python package cvxpy [13] to solve the linear regression problem with Fused Lasso restriction [52]. We set the number of mixture models to K = 3 and the number of perturbations to\nl = 500. The parameter S is set to 104 for Drebin+ and Mimicus+, as the underlying features are not sequential and to 10\u22123 for the sequences of DAMD and VulDeePecker [see 21]. Furthermore, we implement LIME with l = 500 perturbations, use the cosine similarity as proximity measure, and employ the regression solver from the scipy package using L1 regularization. For SHAP we make use of the open-source implementation by Lundberg and Lee [31] including the KernelSHAP solver."
    },
    {
      "heading": "5.2. Descriptive Accuracy",
      "text": "We start our evaluation by measuring the descriptive accuracy (DA) of the explanation methods as defined in Section 4.1. In particular, we successively remove the most relevant features from the samples of the datasets and measure the decrease in the classification score. For Drebin+ and Mimicus+, we remove features by setting the corresponding dimensions to 0. For DAMD, we replace the most relevant instructions with the no-op opcode, and for VulDeePecker we substitute the selected tokens with an embedding-vector of zeros.\nThe top row in Figure 5 shows the results of this experiment. As the first observation, we find that the DA curves vary significantly between the explanation methods and security systems. However, the methods IG and LRP consistently obtain strong results in all settings and show steep declines of the descriptive accuracy. Only on the VulDeePecker dataset, the black-box method LIME can provide explanations with comparable accuracy. Notably, for the DAMD dataset, IG and LRP are the only methods to generate real impact on the outcome of the classifier. For Mimicus+, IG, LRP and Gradients achieve a perfect accuracy decline after only 25 features and thus the white-box explanation methods outperform the black-box methods in this experiment.\nTable 7(a) shows the area under curve (AUC) for the descriptive accuracy curves from Figure 5. We observe that IG is the best method over all datasets\u2014lower values indicate better explanations\u2014followed by LRP. In comparison to other methods it is up to 48 % better on average.\nIntuitively, this considerable difference between the whitebox and black-box methods makes sense, as white-box approaches can utilize internal information of the neural networks that are not available to black-box methods."
    },
    {
      "heading": "5.3. Descriptive Sparsity",
      "text": "We proceed by investigating the sparsity of the generated explanations with the MAZ score defined in Section 4.2. The second row in Figure 5 shows the result of this experiment for all datasets and methods. We observe that the methods IG, LRP, and Gradients show the steepest slopes and assign the majority of features little relevance, which indicates a sparse distribution. By contrast, the other explanation methods provide flat slopes of the MAZ close to 0, as they generate relevance values with a broader range and thus are less sparse.\nFor Drebin+ and Mimicus+, we observe an almost identical level of sparsity for LRP, IG and Gradients supporting the findings from Figure 3. Interestingly, for VulDeePecker, the MAZ curve of LEMNA shows a strong increase close to 1, indicating that it assigns high relevance to a lot of tokens. While this generally is undesirable, in case of LEMNA, this is founded in the basic design and the use of the Fused Lasso constraint. In case of DAMD, we see a massive peak at 0 for IG, showing that it marks almost all features as irrelevant. According to the previous experiment, however, it simultaneously provides a very good accuracy on this data. The resulting sparse and accurate explanations are particularly advantageous for a human analyst since the DAMD dataset contains samples with up to 520,000 features. The explanations from IG provide a compressed yet accurate representation of the sequences which can be inspected easily.\nWe summarize the performance on the MAZ metric by calculating the area under curve and report it in Table 7(b). A high AUC indicates that more features have\nbeen assigned a relevance close to 0, that is, the explanation is more sparse. We find that the best methods again are white-box approaches, providing explanations that are up to 50 % sparser compared to the other methods in this experiment."
    },
    {
      "heading": "5.4. Completeness of Explanations",
      "text": "We further examine the completeness of the explanations. As shown in Section 4.3, some explanation methods can not calculate meaningful relevance values for all inputs. In particular, perturbation-based methods suffer from this problem, since they determine a regression with labels derived from random perturbations. To investigate this problem, we monitor the creation of perturbations and their labels for the different datasets.\nWhen creating perturbations for some sample x it is essential for black-box methods that a fraction p of them is classified as belonging to the opposite class of x. In an optimal case one can achieve p \u2248 0.5, however during our experiments we find that 5 % can be sufficient to calculate a non-degenerated explanation in some cases. Figure 6 shows for each value of p and all datasets the fraction of samples remaining when enforcing a percentage p of perturbations from the opposite class.\nIn general, we observe that creating malicious perturbations from benign samples is a hard problem, especially for Drebin+ and DAMD. For example, in the Drebin+ dataset only 31 % of the benign samples can obtain a p value of 5 % which means that more than 65 % of the whole dataset suffer from degenerated explanations. A detailed calculation for all datasets with a p value of 5 % can be found in Table 12 in the Appendix C.\nThe problem of incomplete explanations is rooted in the imbalance of features characterizing malicious and benign data in the datasets. While only few features make a sample malicious, there exists a large variety of features turning\na sample benign. As a consequence, randomly setting malicious features to zero leads to a benign classification, while setting benign features to zero usually does not impact the prediction. As a consequence, it is often not possible to explain predictions for benign applications and the analyst is stuck with an empty explanation.\nIn summary, we argue that perturbation-based explanation methods should only be used in security settings where incomplete explanations can be compensated by other means. In all other cases, one should refrain from using these black-box methods in the context of security."
    },
    {
      "heading": "5.5. Stability of Explanations",
      "text": "We proceed to evaluate the stability of the explanation methods when processing inputs from the four security systems. To this end, we apply the explanations to the same samples over multiple runs and measure the average intersection size between the runs.\nTable 8 shows the average intersection size between the top k features for three runs of the methods as defined in Equation 1. We use k = 10 for all datasets except for DAMD where we use k = 50 due to the larger input space. Since the outputs of Gradients, IG, and LRP are deterministic, they reach the perfect score of 1.0 in all settings and thus do not suffer from limitations concerning stability.\nFor the perturbation-based methods, however, stability poses a severe problem since none of those methods obtains a intersection size of more than 0.5. This indicates that on average half of the top features do not overlap when computing explanations on the same input. Furthermore, we see that the assumption of locality of the perturbationbased methods does not apply for all models under test, since the output is highly dependent on the perturbations used to approximate the decision boundary. Therefore, the best methods for the stability criterion beat the perturbationbased methods by a factor of at least 2.5 on all datasets."
    },
    {
      "heading": "5.6. Efficiency of Explanations",
      "text": "We finally examine the efficiency of the different explanation methods. Our experiments are performed on a regular server system with an Intel Xeon E5 v3 CPU at 2.6 GHz. It is noteworthy that the methods Gradients,\nIG and LRP can benefit from computations on a graphical processing unit (GPU), therefore we report both results but use only the CPU results to achieve a fair comparison with the black-box methods.\nTable 9 shows the average run-time per input for all explanations methods and security systems. We observe that Gradients and LRP achieve the highest throughput in general beating the other methods by orders of magnitude. This advantage arises from the fact that data can be processed batch-wise for methods like Gradients, IG, and LRP, that is, explanations can be calculated for a set of samples at the same time. The Mimicus+ dataset, for example, can be processed in one batch resulting in a speed-up factor of more than 16,000\u00d7 over the fastest black-box method. In general we note that the white-box methods Gradients and LRP achieve the fastest run-time since they require a single backwards-pass through the network. Moreover, computing these methods on a GPU results in additional speedups of a factor up to three.\nThe run-time of the black-box methods increases for high dimensional datasets, especially DAMD, since the regression problems need to be solved in higher dimensions. While the speed-up factors are already enormous, we have not even included the creation of perturbations and their classification, which consume additional run-time as well."
    },
    {
      "heading": "5.7. Robustness of Explanations",
      "text": "Recently, multiple authors have shown that adversarial perturbations are also applicable against explanation methods and can manipulate the generated relevance values. Given a classification function f , an input x and a target class ct the goal of an adversarial perturbation is to find x\u0303 = x + \u03b4 such that \u03b4 is minimal but at the same time f(x\u0303) = ct 6= f(x).\nFor an explanation method gf (x) Zhang et al. [55] propose to solve\nmin \u03b4 dp ( f(x\u0303), ct ) + \u03bbde ( gf (x\u0303), gf (x) ) , (2)\nwhere dp and de are distance measures for classes and explanations of f . The crafted input x\u0303 is misclassified by the network but keeps an explanation very close to the one of x. Dombrowski et al. [14] show that many white-box methods can be tricked to produce an arbitrary explanation et without changing the classification by solving\nmin \u03b4 de ( gf (x\u0303), et ) + \u03b3dp ( f(x\u0303), f(x) ) . (3)\nWhile the aforementioned attacks are constructed for white-box methods, Slack et al. [44] have recently proposed an attack against LIME and SHAP. They show that the perturbations, which have to be classified to create explanations, deviate strongly from the original data distribution and hence are easily distinguishable from original data samples. With this knowledge an adversary can use a different model f\u0303 to classify the perturbations and create arbitrary explanations to hide potential biases of the original model. Although LEMNA is not considered by Slack et al. [44], it can be attacked likewise since it relies on perturbation labels as well.\nThe white-box attacks by Zhang et al. [55] and Dombrowski et al. [14] require access to the model parameters which is a technical hurdle in practice. Similarly, however, the black-box attack by Slack et al. [44] needs to bypass the classification process of the perturbations to create arbitrary explanations which is equally difficult. A further problem of all attacks in the security domain are the discrete input features: For images, an adversarial perturbation \u03b4 is typically small and imperceptible, while binary features, as in the Drebin+ dataset, require larger changes with |\u03b4| \u2265 1. Similarly, for VulDeePecker and DAMD, a direct application of existing attacks will likely result in broken code or invalid behavior. Adapting these attacks seems possible but requires further research on adversarial learning in structured domains.\nBased on this analysis, we conclude that explanation methods are not robust and vulnerable to different attacks. Still, these attacks require access to specific parts of the victim system as well as further extenions to work in discrete domains. As a consequence, the robustness of the methods is difficult to assess and further work is needed to establish a better understanding of this threat."
    },
    {
      "heading": "5.8. Summary",
      "text": "A strong explanation method is expected to achieve good results for each criterion and on each dataset. For example, we have seen that the Gradients method computes sparse results in a decent amount of time. The features, however, are not accurate on the DAMD and VulDeePecker dataset. Equally, the relevance values of SHAP for the Drebin+ dataset are sparser than those from LEMNA but suffer from instability. To provide an overview, we average the performance of all methods over the four datasets and summarize the results in Table 10.\nFor each of the six evaluation criteria, we assign each method one of the following three categories: , , and #. The category is given to the best explanation method and other methods with a similar performance. The # category is assigned to the worst method and methods performing equally bad. Finally, the category is given to methods that lie between the best and worst methods.\nBased on Table 10, we can see that white-box explanation methods achieve a better ranking than black-box methods in all evaluation criteria. Due to the direct access to the parameters of the neural network, these methods can better analyze the prediction function and are able to identify relevant features. In particular, IG and LRP are the best methods overall regarding our evaluation criteria. They compute results in less than 50 ms in our benchmark, mark only few features as relevant, and the selected features have great impact on the decision of the classifier. These methods also provide deterministic results and do not suffer from incompleteness. As a result, we recommend to use these methods for explaining deep learning in security. However, if white-box access is not available, we recommend the black-box method LIME as it shows the best performance in our experiments or to apply model stealing as shown in the following Section 5.9 to enable the use of white-box methods.\nIn general, whether white-box or black-box methods are applicable also depends on who is generating the explanations: If the developer of a security system wants to investigate its prediction, direct access to all model parameters is typically available and white-box methods can be applied easily. Similarly, if the learning models are shared between practitioners, white-box approaches are also the method of choice. If the learning model, however, is trained by a remote party, such as a machinelearning-as-a-service providers, only black-box methods are applicable. Likewise, if an auditor or security tester inspects a proprietary system, black-box methods also become handy, as they do not require reverse-engineering and extracting model parameters.\nOri gin\nal\n1-l ay\ner\n2-l ay\ners\n3-l ay\ners\nOriginal\n1-layer\n2-layers\n3-layers\nTop-10-Drebin+\nOri gin\nal\n1-l ay\ner\n2-l ay\ners\n3-l ay\ners\nOriginal\n1-layer\n2-layers\n3-layers\nTop-10-Mimicus+\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFigure 7: Intersection size of the Top-10 features of explanations obtained from models that were stolen from the original model of the Drebin+ and Mimicus+ dataset."
    },
    {
      "heading": "5.9. Model Stealing for White-Box Explanations",
      "text": "Our experiments show that practitioners from the security domain should favor white-box methods over black-box methods when aiming to explain neural networks. However, there are cases when access to the parameters of the system is not available and white-box methods can not be used. Instead of using black-box methods one could also use model stealing to obtain an approximation of the original network[49]. This approach assumes that the user can predict an unlimited number of samples with the model to be explained. The obtained predictions can then be used to train a surrogate model which might have a different architecture but a similar behavior.\nTo evaluate the differences between the explanations of surrogate models to the original ones we conduct an experiment on the Drebin+ and Mimicus+ datasets as follows: We use the predictions of the original model from Grosse et al. [20] which has two dense layers with 200 units each and use these predictions to train three surrogate models. The number of layers is varied to be [1, 2, 3] and the number of units in each layer is always 200 resulting in models with higher, lower and the original complexity. For each model we calculate explanations via LRP and compute the intersection size given by Equation 1 for k = 10.\nThe results in Figure 7 show that the models deliver similar explanations to the original model (IS\u22480.7) although having different architectures for the Drebin+ dataset. However, the similarity between the stolen models is clearly higher (IS\u22480.85). For the Mimicus+ dataset, we observe a general stability of the learned features at a lower level (IS\u22480.55). These results indicate that the explanations of the stolen models are better than those obtained from black-box methods (see Figure 3) but still deviate from the original model, i.e., there is no global transferability between the explanations. At all, model stealing can be considered a good alternative to the usage of black-box explanation methods."
    },
    {
      "heading": "6. Insights on the Datasets",
      "text": "During the experiments for this paper, we have analyzed various explanations of security systems\u2014not only quantitatively as discussed in Section 5 but also qualitatively from the perspective of a security analyst. In this section, we summarize our observations and discuss insights related to the role of deep learning in security.\nMoreover, we publish the generated explanations from all datasets and methods on the project\u2019s website2 in order to foster future research."
    },
    {
      "heading": "6.1. Insights on Mimicus+",
      "text": "When inspecting explanations for the Mimicus+ system, we observe that the features for detecting malware are dominated by count_javascript and count_js, which both stand for the number of JavaScript elements in the document. The strong impact of these elements is meaningful, as JavaScript is frequently used in malicious PDF documents [27]. However, we also identify features in the explanations that are non-intuitive. For example, features like count_trailer that measures the number of trailer sections in the document or count_box_letter that counts the number of US letter sized boxes can hardly be related to security and rather constitute artifacts in the dataset captured by the learning process.\nTo further investigate the impact of JavaScript features on the neural network, we determine the distribution of the top 5 features from the method IG for each class in the entire dataset. It turns out that JavaScript appears in 88 % of the malicious documents, whereas only about 6 % of the benign samples make use of it (see Table 11). This makes JavaScript an extremely discriminating feature for the dataset. From a security perspective, this is an unsatisfying result, as the neural network of Mimicus+ relies on a few indicators for detecting the malicious code in the documents. An attacker could potentially evade Mimicus+ by not using JavaScript or obfuscating the JavaScript elements in the document."
    },
    {
      "heading": "6.2. Insights on Drebin+",
      "text": "During the analysis of the Drebin+ dataset, we notice that several benign applications are characterized by the hardware feature touchscreen, the intent filter launcher, and the permission INTERNET. These features frequently occur in benign and malicious applications in the Drebin+ dataset and are not particularly descriptive for benignity. Note that the interpretation of features speaking for benign\n2. http://explain-mlsec.org\napplications is challenging due to the broader scope and the difficulty in defining benignity. We conclude that the three features together form an artifact in the dataset that provides an indicator for detecting benign applications.\nFor malicious Android applications, the situation is different: The explanation methods return highly relevant features that can be linked to the functionality of the malware. For instance, the requested permission SEND_SMS or features related to accessing sensitive information, such as the permission READ_PHONE_STATE and the API call getSimCountryIso, receive consistently high scores in our investigartion. These features are well in line with common malware for Android, such as the FakeInstaller family [32], which is known to obtain money from victims by secretly sending text messages (SMS) to premium services. Our analysis shows that the MLP network employed in Drebin+ has captured indicative features directly related to the underlying malicious activities."
    },
    {
      "heading": "6.3. Insights on VulDeePecker",
      "text": "In contrast to the datasets considered before, the features processed by VulDeePecker resemble lexical tokens and are strongly interconnected on a syntactical level. This becomes apparent in the explanations of the method Integrated Gradients in Figure 4, where adjacent tokens have mostly equal colors. Moreover, orange and blue colored features in the explanation are often separated by tokens with no color, indicating a gradual separation of positive and negative relevance values.\nDuring our analysis, we notice that it is still difficult for a human analyst to benefit from the highlighted tokens. First, an analyst interprets the source code rather than the extracted tokens and thus maintains a different view on the data. In Figure 4, for example, the interpretation of the highlighted INT0 and INT1 tokens as buffer sizes of 50 and 100 wide characters is misleading, since the neural network is not aware of this relation. Second, VulDeePecker truncates essential parts of the code. In Figure 4, during the initialization of the destination buffer, for instance, only the size remains as part of the input. Third, the large amount of highlighted tokens like semicolons, brackets, and equality signs seems to indicate that VulDeePecker overfits to the training data at hand.\nGiven the truncated program slices and the seemingly unrelated tokens marked as relevant, we conclude that the VulDeePecker system might benefit from extending the learning strategy to longer sequences and cleansing the training data to remove artifacts that are irrelevant for vulnerability discovery."
    },
    {
      "heading": "6.4. Insights on DAMD",
      "text": "Finally, we consider Android applications from the DAMD dataset. Due to the difficulty of analyzing raw Dalvik bytecode, we guide our analysis of the dataset by inspecting malicious applications from three popular Android malware families: GoldDream [26], DroidKungFu [25], and DroidDream [18]. These families exfiltrate sensitive data and run exploits to take full control of the device.\nIn our analysis of the Dalvik bytecode, we benefit from the sparsity of the explanations from LRP and IG as explained in Section 5.3. Analyzing all relevant features\nbecomes tractable with moderate effort using these methods and we are able to investigate the opcodes with the highest relevance in detail. We observe that the relevant opcode sequences are linked to the malicious functionality.\nAs an example, Table 4 depicts the opcode sequence, that is found in all samples of the GoldDream family.Taking a closer look, this sequence occurs in the onReceive method of the com.GoldDream.zj.zjReceiver class. In this function, the malware intercepts incoming SMS and phone calls and stores the information in local files before sending them to an external server. Similarly, we can interpret the explanations of the other two malware families, where functionality related to exploits and persistent installation is highlighted in the Dalvik opcode sequences.\nFor all members of each malware family, the opcode sequences identified using the explanation methods LRP and IG are identical, which demonstrates that the CNN in the DAMD system has learned an discriminative pattern from the underlying opcode representation."
    },
    {
      "heading": "7. Conclusion",
      "text": "The increasing application of deep learning in security renders means for explaining their decisions vitally important. While there exist a wide range of explanation methods from the area of computer vision and machine learning, it has been unclear which of these methods are suitable for security systems. We have addressed this problem and propose evaluation criteria that enable a practitioner to compare and select explanation methods in the context of security. While the importance of these criteria depends on the particular security task, we find that the methods Integrated Gradients and LRP comply best with all requirements. Hence, we generally recommend these methods for explaining predictions in security systems.\nAside from our evaluation of explanation methods, we reveal problems in the general application of deep learning in security. For all considered systems under test, we identify artifacts that substantially contribute to the overall prediction, but are unrelated to the security task. Several of these artifacts are rooted in peculiarities of the data. It is likely that the employed neural networks overfit the data rather than solving the underlying task. We thus conclude that explanations need to become an integral part of any deep learning system to identify artifacts in the training data and to keep the learning focused on the targeted security problem.\nOur study is a first step for integrating explainable learning in security systems. We hope to foster a series of research that applies and extends explanation methods, such that deep learning becomes more transparent in computer security. To support this development, we make all our implementations and datasets publicly available."
    },
    {
      "heading": "Acknowledgements",
      "text": "The authors gratefully acknowledge funding from the German Federal Ministry of Education and Research (BMBF) under the projects VAMOS (FKZ 16KIS0534) and BIFOLD (FKZ 01IS18025B). Furthermore, the authors acknowledge funding by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany\u2019s Excellence Strategy EXC 2092 CASA-390781972."
    },
    {
      "heading": "1. Related Concepts",
      "text": "Some of the considered explanations methods share similarities with techniques of adversarial examples and feature selection. While these similarities result from an analogous analysis of the prediction function fN , the underlying objectives are fundamentally different from explainable learning and cannot be transferred easily. In the following, we briefly highlight these different objectives:\nAdversarial examples. Adversarial examples are constructed by determining a minimal perturbation \u03b4 such that fN (x + \u03b4) 6= fN (x) for a given neural network N and an input vector x [7, 35]. The perturbation \u03b4 encodes which features need to be modified to change the prediction. However, the perturbation does not explain why x was given the label y by the neural network. The Gradients explanation method described in Section 3 shares similarities with some attacks generating adversarial examples, as the gradient \u2202fN/\u2202xi is used to quantify the difference of fN when changing a feature xi slightly. Still, algorithms for determining adversarial examples are insufficient for computing reasonable explanations.\nNote that we deliberately do not study adversarial examples in this paper. Techniques for attacking and defending learning algorithms are orthogonal to our work. These techniques can be augmented using explanations, yet it is completely open how this can be done in a secure manner. Recent defenses for adversarial examples based on explanations have proven to be totally ineffective [6].\nFeature selection. This concept aims at reducing the dimensionality of a learning problem by selecting a subset of discriminative features [15]. At a first glance, features determined through feature selection seem like a good fit for explanation. While the selected features can be investigated and often capture characteristics of the underlying data, they are determined independent from a particular learning model. As a result, feature selection methods cannot be direclty applied for explaining the decision of a neural network."
    },
    {
      "heading": "2. Incompatible Explanation Methods",
      "text": "Several explanation methods are not suitable for general application in security, as they do not support common architectures of neural networks used in this area (see Table 1). We do not consider these methods in our evaluation, yet for completeness we provide a short overview of these methods in the following.\nPatternNet and PatternAttribution. These white-box methods are inspired by the explanation of linear models. While PatternNet determines gradients and replaces neural network weights by so-called informative directions, PatternAttribution builds on the LRP framework and computes explanations relative to so-called root points whose output are 0. Both approaches are restricted to feed-forward and convolutional networks. Recurrent neural networks are not supported.\nDeConvNet and GuidedBackProp. These methods aim at reconstructing an input x given output y, that is, mapping y back to the input space. To this end, the authors present an approach to revert the computations of a convolutional layer followed by a rectified linear unit (ReLu) and maxpooling, which is the essential sequence of layers in neural networks for image classification. Similar to LRP and DeepLift, both methods perform a backwards pass through the network. The major drawback of these methods is again the restriction to convolutional neural networks.\nCAM, GradCAM, and GradCAM++. These three whitebox methods compute relevance scores by accessing the output of the last convolutional layer in a CNN and performing global average pooling. Given the activations aki of the k-th channel at unit i, GradCam learn weights wk such that\ny \u2248 \u2211 i \u2211 k wkaki.\nThat is, the classification is modeled as a linear combination of the activations of the last layer of all channels and finally ri = \u2211 k wkaki. GradCam and GradCam++ extend this approach by including specific gradients in this calculation. All three methods are only applicable if the neural network uses a convolutional layer as the final layer. While this setting is common in image recognition, it is rarely used in security applications and thus we do not analyze these methods.\nRTIS and MASK. These methods compute relevance scores by solving an optimization problem for a mask m. A mask m is applied to x as m \u25e6 x in order to affect x, for example by setting features to zero. To this end, Fong and Vedaldi [17] propose the optimization problem\nm\u2217 = arg min m\u2208[0,1]d \u03bb\u20161\u2212m\u20161 + fN (m \u25e6 x),\nwhich determines a sparse mask, that identifies relevant features of x. This can be solved using gradient descent, which thus makes these white-box approaches. However, solving the equation above often leads to noisy results which is why RTIS and MASK add additional terms to achieve smooth solutions using regularization and blurring. These concepts, however, are only applicable for images and cannot be transferred to other types of features.\nQuantitative Input Influence. This method is another black-box approach which calculates relevances by changing input features and calculating the difference between the outcomes. Let X\u2212iUi be the random variable with the ith input of X being replaced by a random value that is drawn from the distribution of feature xi. Then the relevance of feature i for a classification to class c is given by\nri = E [ fN (X\u2212iUi) 6= c|X = x ] .\nHowever, when the features of X are binary like in some of our datasets this equation becomes\nri = { 1 fN (x\u00aci) 6= c 0 else\nAs noted by Datta et al. [12] this results in many features receiving a relevance of zero which has no meaning. We notice that even the extension to sets proposed by Datta et al. [12] does not solve this problem since it is highly related to degenerated explanations as discussed in Section 5.4."
    },
    {
      "heading": "3. Completeness of Datasets: Example calculation",
      "text": "In Section 5.4 we discussed the problem of incomplete or degenerated explanations from black-box methods that can occur when there are not enough labels from the opposite class in the perturbations. Here we give an concrete example when enforcing 5 % of the labels to be from the opposite class.\nTable 12 shows the results of this experiment. On average, 29 % of the samples cannot be explained well, as the computed perturbations contain too few instances from the opposite class. In particular, we observe that creating malicious perturbations from benign samples is a hard problem in the case of Drebin+ and DAMD, where only 32.6 % and 2.8 % of the benign samples achieve sufficient perturbations from the opposite class."
    }
  ],
  "title": "Evaluating Explanation Methods for Deep Learning in Security",
  "year": 2020
}
