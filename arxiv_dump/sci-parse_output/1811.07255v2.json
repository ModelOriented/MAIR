{
  "abstractText": "Intersectionality is a framework that analyzes how interlocking systems of power and oppression affect individuals along overlapping dimensions including race, gender, sexual orientation, class, and disability. Intersectionality theory therefore implies it is important that fairness in artificial intelligence systems be protected with regard to multi-dimensional protected attributes. However, the measurement of fairness becomes statistically challenging in the multi-dimensional setting due to data sparsity, which increases rapidly in the number of dimensions, and in the values per dimension. We present a Bayesian probabilistic modeling approach for the reliable, data-efficient estimation of fairness with multidimensional protected attributes, which we apply to two existing intersectional fairness metrics. Experimental results on census data and the COMPAS criminal justice recidivism dataset demonstrate the utility of our methodology, and show that Bayesian methods are valuable for the modeling and measurement of fairness in an intersectional context.",
  "authors": [
    {
      "affiliations": [],
      "name": "James Foulds"
    },
    {
      "affiliations": [],
      "name": "Rashidul Islam"
    },
    {
      "affiliations": [],
      "name": "Kamrun Keya"
    },
    {
      "affiliations": [],
      "name": "Shimei Pan"
    }
  ],
  "id": "SP:2b2310c9aadf7f462846137024b93414bb1f2502",
  "references": [
    {
      "authors": [
        "Julia Angwin",
        "Jeff Larson",
        "Surya Mattu",
        "Lauren Kirchner"
      ],
      "title": "Machine bias: There\u2019s software used across the country to predict future criminals. and it\u2019s biased against blacks",
      "venue": "ProPublica, May,",
      "year": 2016
    },
    {
      "authors": [
        "Solon Barocas",
        "Andrew D Selbst"
      ],
      "title": "Big data\u2019s disparate impact",
      "venue": "Cal. L. Rev.,",
      "year": 2016
    },
    {
      "authors": [
        "Richard Berk",
        "Hoda Heidari",
        "Shahin Jabbari",
        "Matthew Joseph",
        "Michael Kearns",
        "Jamie Morgenstern",
        "Seth Neel",
        "Aaron Roth"
      ],
      "title": "A convex framework for fair regression",
      "venue": "4th Annual Workshop on Fairness, Accountability, and Transparency in Machine Learning.,",
      "year": 2017
    },
    {
      "authors": [
        "Richard Berk",
        "Hoda Heidari",
        "Shahin Jabbari",
        "Michael Kearns",
        "Aaron Roth"
      ],
      "title": "Fairness in criminal justice risk assessments: The state of the art",
      "venue": "In Sociological Methods and Research,",
      "year": 2018
    },
    {
      "authors": [
        "Alex Beutel",
        "Jilin Chen",
        "Zhe Zhao",
        "Ed H Chi"
      ],
      "title": "Data decisions and theoretical implications when adversarially learning fair representations",
      "venue": "In Proceedings of 2017 Workshop on Fairness, Accountability, and Transparency in Machine Learning,",
      "year": 2017
    },
    {
      "authors": [
        "Tolga Bolukbasi",
        "Kai-Wei Chang",
        "James Y Zou",
        "Venkatesh Saligrama",
        "Adam T Kalai"
      ],
      "title": "Man is to computer programmer as woman is to homemaker? Debiasing word embeddings",
      "venue": "In Advances in NeurIPS,",
      "year": 2016
    },
    {
      "authors": [
        "Joy Buolamwini",
        "Timnit Gebru"
      ],
      "title": "Gender shades: Intersectional accuracy disparities in commercial gender classification",
      "venue": "In Conference on Fairness, Accountability and Transparency,",
      "year": 2018
    },
    {
      "authors": [
        "Michael W Collins",
        "Scott B Morris"
      ],
      "title": "Testing for adverse impact when sample size is small",
      "venue": "Journal of Applied Psychology,",
      "year": 2008
    },
    {
      "authors": [
        "Patricia Hill Collins"
      ],
      "title": "Black feminist thought: Knowledge, consciousness, and the politics of empowerment (2nd ed.)",
      "year": 2002
    },
    {
      "authors": [
        "Kimberl\u00e9 Crenshaw"
      ],
      "title": "Demarginalizing the intersection of race and sex: A black feminist critique of antidiscrimination doctrine, feminist theory and antiracist politics",
      "venue": "U. Chi. Legal F.,",
      "year": 1989
    },
    {
      "authors": [
        "Cynthia Dwork",
        "Moritz Hardt",
        "Toniann Pitassi",
        "Omer Reingold",
        "Richard Zemel"
      ],
      "title": "Fairness through awareness",
      "venue": "In Proceedings of the 3rd ITCS,",
      "year": 2012
    },
    {
      "authors": [
        "J.R. Foulds",
        "R. Islam",
        "K. Keya",
        "S. Pan"
      ],
      "title": "An intersectional definition of fairness",
      "venue": "ArXiv preprint arXiv:1807.08362 [CS.LG],",
      "year": 2018
    },
    {
      "authors": [
        "Moritz Hardt",
        "Eric Price",
        "Nati Srebro"
      ],
      "title": "Equality of opportunity in supervised learning",
      "venue": "In Advances in NeurIPS,",
      "year": 2016
    },
    {
      "authors": [
        "Ursula Hebert-Johnson",
        "Michael Kim",
        "Omer Reingold",
        "Guy Rothblum"
      ],
      "title": "Multicalibration: Calibration for the (Computationally-identifiable) masses",
      "venue": "Proceedings of the 35th ICML, PMLR",
      "year": 1953
    },
    {
      "authors": [
        "Jennifer A Hoeting",
        "David Madigan",
        "Adrian E Raftery",
        "Chris T Volinsky"
      ],
      "title": "Bayesian model averaging: a tutorial",
      "venue": "Statistical science,",
      "year": 1999
    },
    {
      "authors": [
        "Daniel Kifer",
        "Ashwin Machanavajjhala"
      ],
      "title": "Pufferfish: A framework for mathematical privacy definitions",
      "venue": "ACM TODS,",
      "year": 2014
    },
    {
      "authors": [
        "Ron Kohavi"
      ],
      "title": "Scaling up the accuracy of naive-Bayes classifiers: a decision-tree hybrid",
      "venue": "In Proceedings of the Second SIGKDD,",
      "year": 1996
    },
    {
      "authors": [
        "Matt J Kusner",
        "Joshua Loftus",
        "Chris Russell",
        "Ricardo Silva"
      ],
      "title": "Counterfactual fairness",
      "venue": "In Advances in NeurIPS,",
      "year": 2017
    },
    {
      "authors": [
        "Cecilia Munoz",
        "Megan Smith",
        "DJ Patil"
      ],
      "title": "Big data: A report on algorithmic systems, opportunity, and civil rights",
      "venue": "Exec. Office of the President,",
      "year": 2016
    },
    {
      "authors": [
        "Safiya Umoja Noble"
      ],
      "title": "Algorithms of Oppression: How Search Engines Reinforce Racism",
      "year": 2018
    },
    {
      "authors": [
        "Inioluwa Deborah Raji",
        "Joy Buolamwini"
      ],
      "title": "Actionable auditing: Investigating the impact of publicly naming biased performance results of commercial ai products",
      "venue": "In AAAI/ACM Conf. on AI Ethics and Society,",
      "year": 2019
    },
    {
      "authors": [
        "Philip L Roth",
        "Philip Bobko",
        "Fred S Switzer III"
      ],
      "title": "Modeling the behavior of the 4/5ths rule for determining adverse impact: Reasons for caution",
      "venue": "Journal of Applied Psychology,",
      "year": 2006
    },
    {
      "authors": [
        "Camelia Simoiu",
        "Sam Corbett-Davies",
        "Sharad Goel"
      ],
      "title": "The problem of infra-marginality in outcome tests for discrimination",
      "venue": "The Annals of Applied Statistics,",
      "year": 2017
    },
    {
      "authors": [
        "Rich Zemel",
        "Yu Wu",
        "Kevin Swersky",
        "Toni Pitassi",
        "Cynthia Dwork"
      ],
      "title": "Learning fair representations",
      "venue": "In International Conference on Machine Learning (ICML),",
      "year": 2013
    },
    {
      "authors": [
        "Simoiu"
      ],
      "title": "2017] in the context of stop-and-frisk policing. They model risk probabilities within each protected category, and require algorithms (or people, such as police officers) to threshold these probabilities at the same points when determining outcomes",
      "year": 2017
    },
    {
      "authors": [
        "Kusner"
      ],
      "title": "Bayesian inference on causal graphical models for fairness. Under their counterfactual fairness definition, changing protected attributes A, while holding things which are not causally dependent on A constant, will not change the predicted distribution",
      "year": 2017
    }
  ],
  "sections": [
    {
      "heading": "1 Introduction",
      "text": "With the rising influence of machine learning algorithms on many important aspects of our daily lives, there are growing concerns that biases inherent in data can lead the behavior of these algorithms to discriminate against certain populations [Angwin et al., 2016; Barocas and Selbst, 2016; Berk et al., 2018; Bolukbasi et al., 2016; Dwork et al., 2012; Munoz et al., 2016; Noble, 2018]. In recent years, substantial research effort has been devoted to the development and enforcement of mathematical definitions of bias and fairness in machine learning algorithms [Dwork et al., 2012; Hardt et al., 2016; Kusner et al., 2017; Kearns et al., 2018].\nIn this work, our guiding principle for fairness is intersectionality, the core theoretical framework underlying the third-wave feminist movement [Crenshaw, 1989; Collins, 2002 1990]. Intersectionality theory states that racism, sexism, and other social systems which harm\nmarginalized groups have interlocking effects, such that the lived experience of, e.g., Black women, is very different than that of, e.g., white women. We therefore focus on fairness scenarios where there are multiple protected attributes, such as gender, race, and sexual orientation.\nWhile fairness methods have been extended to multiple protected attributes [Kearns et al., 2018; Hebert-Johnson et al., 2018; Foulds et al., 2018], data sparsity rapidly becomes an issue as the number of dimensions (and their number of distinct values) increases, leading to uncertainty in the measurement of fairness. For example, Table 1 shows how the number of instances per value at the intersections of the protected attributes, and especially the minimum of these counts, decreases as more protected attributes are introduced, on the UCI Adult census dataset [Kohavi, 1996]. It may be difficult, for instance, to estimate the overall behavior of a classifier on those individuals who are indigenous women from foreign countries, due to a lack of recorded data on such individuals. To detect intersectional discrimination, we need to measure the system\u2019s behavior on potentially small intersectional groups, which is unreliable to estimate in the resulting \u201csmall N\u201d regime [Roth et al., 2006].\nThe goal of this work, therefore, is to address the challenge of reliably modeling and measuring fairness in an intersectional context, despite data sparsity. While small data uncertainty [Roth et al., 2006], intersectionality [Buolamwini and Gebru, 2018; Foulds et al., 2018], and multiple attribute definitions [Kearns et al., 2018; HebertJohnson et al., 2018; Foulds et al., 2018] have been studied, we are first to consider them concurrently.\nThe majority of the research on fairness in AI to date has focused on the development of learning algorithms\nar X\niv :1\n81 1.\n07 25\n5v 2\n[ cs\n.L G\n] 1\n0 Se\np 20\n19\nwhich enforce fairness metrics [Dwork et al., 2012; Zemel et al., 2013; Hardt et al., 2016; Bolukbasi et al., 2016; Kusner et al., 2017; Berk et al., 2017]. Here, we instead focus on accurately measuring the unfairness of a system or dataset. Fairness measurement is crucial when engineering AI systems for deployment [Speicher et al., 2018]. It is essential for determining whether disparities in system behavior meet legal thresholds for discrimination [Roth et al., 2006]. And it is integral to investigative reporting on disparate behavior of existing AI systems, which promotes awareness and can ultimately lead to the rectification of algorithm injustice [Angwin et al., 2016; Buolamwini and Gebru, 2018; Raji and Buolamwini, 2019]. Our primary contributions are:\n1. We propose a Bayesian probabilistic modeling framework for reliably estimating fairness and its uncertainty in the data-sparse intersectional regime.\n2. We instantiate the proposed framework with four statistical models, each with a different bias and variance tradeoff, including a novel hierarchical extension of Bayesian logistic regression which is potentially an appropriate choice for this setting. We further propose a Bayesian model averaging approach which leverages all of the models together.\n3. We study the behavior of our Bayesian models on criminal justice, census, and synthetic data. Our results demonstrate the importance of the Bayesian modeling approach in an intersectional context.\nThe remainder of the paper is structured as follows. We begin by discussing intersectionality theory, which motivates our multi-dimensional approach to fairness, and describe two intersectional fairness metrics from the literature [Foulds et al., 2018; Kearns et al., 2018]. Next, we propose Bayesian probabilistic models for estimating these (and other) fairness metrics in the multi-dimensional fairness regime. We then empirically study the behavior of the models in estimating the intersectional fairness metrics, and showcase their real-world application with a case study on the COMPAS recidivism dataset. Finally, we conclude with a discussion of the practical implications of our work."
    },
    {
      "heading": "2 Background and Motivation: Intersectionality and AI Fairness",
      "text": "Intersectionality is a critical lens for analyzing how unfair processes in society, such as sexism and systemic racism, affect certain groups. The term was original introduced by Crenshaw [1989], who studied how the combined harms of such systems of oppression affect Black women, who\nare simultaneously affected by sexism, racism, and other related disadvantages [Truth, 1851; Combahee River Collective, 1978]. In its more general form, advanced by Collins [2002 1990] and others, intersectionality theory posits that individuals at the intersection of multiple protected categories, along lines of gender, race, social class, disability, and so on, are harmed by overlapping systems of oppression.\nIn an AI fairness context, this implies that fairness should be enforced at the intersections of multiple protected attributes [Buolamwini and Gebru, 2018; Foulds et al., 2018]. Here, we consider several existing fairness definitions which are appropriate in an intersectional context."
    },
    {
      "heading": "2.1 Differential Fairness",
      "text": "Differential fairness [Foulds et al., 2018] is a definition specifically motivated by intersectionality, which aims to ensure equitable treatment by an algorithm for all intersecting subgroups of a set of protected categories. We use the notation of Kifer and Machanavajjhala [2014] for all definitions we consider. Let M(x) be an algorithmic mechanism which takes an individual\u2019s data x and assigns them an outcome y, e.g. whether or not the individual was awarded a loan. Let S1, . . . , Sp be discrete-valued protected attributes, A = S1\u00d7S2\u00d7 . . .\u00d7Sp, and let \u03b8 be the distribution which generates x.\nDefinition 2.1. (Differential Fairness) A mechanism M(x) is -differentially fair (DF) with respect to (A,\u0398) if for all \u03b8 \u2208 \u0398 with x \u223c \u03b8, and y \u2208 Range(M),\ne\u2212 \u2264 PM,\u03b8(M(x) = y|si, \u03b8) PM,\u03b8(M(x) = y|sj , \u03b8) \u2264 e , (1)\nfor all (si, sj) \u2208 A\u00d7A where P (si|\u03b8) > 0, P (sj |\u03b8) > 0.\nIn Definition 2.1, si, sj \u2208 A are tuples of all protected attribute values, e.g. gender, race, and nationality. If all of the PM,\u03b8(M(x) = y|s, \u03b8) probabilities are equal for each group s, across all outcomes y and distributions \u03b8, = 0, otherwise > 0. Foulds et al. [2018] proved that this definition guarantees fairness protections for all subsets of the protected attributes, e.g. if all intersections of gender and race are protected (e.g. Black women), then gender (e.g. women) and race (e.g. white people) are separately protected, a property which is consistent with the ethical principles of intersectionality theory. Foulds et al. [2018] further proposed a variant definition which only considers the increase in unfairness by the algorithm, over the unfairness in the original data.\nDefinition 2.2. (DF Bias Amplification) A mechanism M(x) satisfies ( 2 \u2212 1)-DF bias amplification with respect to (A,\u0398, D,M) if it is 2-DF and D is a labeled\ndataset which is 1-DF w.r.t. a model M which was trained on D to estimate P (y|s) in the data."
    },
    {
      "heading": "2.2 Subgroup Fairness",
      "text": "Kearns et al. [2018] proposed multi-attribute fairness definitions which aim to prevent fairness gerrymandering at the intersections of protected groups.\nDefinition 2.3. (Statistical Parity Subgroup Fairness) Let G be a collection of protected group indicators g : A \u2192 {0, 1}, where g(s) = 1 designates that an individual with protected attributes s is in group g. Assume that the mechanism M(x) is binary, i.e. y \u2208 {0, 1}.\nThen M(x) is \u03b3-statistical parity subgroup fair (SF) with respect to \u03b8 and G if for every g \u2208 G,\n|PM,\u03b8(M(x) = 1|\u03b8)\u2212 PM,\u03b8(M(x) = 1)|g(x) = 1, \u03b8)| \u00d7 P\u03b8(g(x) = 1|\u03b8) \u2264 \u03b3 . (2)\nSince we are interested in fairness applications where intersectional ethics are to be upheld, in this work we focus on the case where, similarly to DF, G contains all possible assignments of the protected attributes s (presumed to be enumerable). Kearns et al. [2018] and HebertJohnson et al. [2018] proposed further related multiattribute definitions regarding false positive rates and calibration, respectively. Our methods can also be applied to these definitions, but it is beyond the scope of this work."
    },
    {
      "heading": "2.3 Empirical Fairness Estimation",
      "text": "The central challenge for measuring fairness in an intersectional context, either via -DF, \u03b3-SF, or related notions, is to estimate M(x)\u2019s marginal behavior PM,\u03b8(y|s, \u03b8) for each (y, s) pair, with potentially little data for each of these. The simplest method to do this is to use the empirical data distribution. E.g., for the -DF criterion, assuming discrete outcomes and protected attributes, PData(y|s) = Ny,sNs , where Ny,s and Ns are empirical counts of their subscripted values in the dataset. Empirical differential fairness (EDF) [Foulds et al., 2018] corresponds to verifying that for any y, si, sj ,\ne\u2212 \u2264 Ny,si Nsi Nsj Ny,sj \u2264 e , (3)\nwhenever Nsi > 0 and Nsj > 0. However, in the intersectional setting, the counts Ny,s at the intersection of the values of the protected attributes become rapidly smaller as the dimensionality and cardinality of protected attributes increase (cf. Table 1). In this case, the conditional probabilities in Equations 1 and 2, and hence the fairness metrics, will generally have high uncertainty (or variance, from a frequentist perspective) [Roth et al.,\n2006]. The Ny,s counts may even be 0, which can make the estimate of in Equation 3 infinite/undefined.1"
    },
    {
      "heading": "3 Model-Based Fairness Estimation",
      "text": "Instead of using empirical probabilities, in this paper we propose to generalize beyond the training set by learning PM,\u03b8(y|s, \u03b8) via a probabilistic model. This approach has several advantages. First, by exploiting structure in the distributions, e.g. if the mechanism\u2019s behavior on women is informative of its behavior on Black women, we can accurately model all of the conditional probabilities with fewer parameters than empirical frequencies, thereby reducing variance in estimation. Second, we can use a Bayesian approach to manage uncertainty in the estimation, and to report this uncertainty to an analyst.\nA simple baseline, proposed by [Foulds et al., 2018] to address the zero count issue, is to put a Dirichlet prior on the probabilities in Equation 3. Estimating -DF via the posterior predictive distribution of the resulting Dirichletmultinomial, the criterion for any y, si, sj is\ne\u2212 \u2264 Ny,si + \u03b1 Nsi + |Y|\u03b1 Nsj + |Y|\u03b1 Ny,sj + \u03b1 \u2264 e , (4)\nwhere scalar \u03b1 is each entry of the parameter of a symmetric Dirichlet prior with concentration parameter |Y|\u03b1, Y = Range(M). Foulds et al. [2018] refer to this as smoothed EDF. This can also be used for \u03b3-SF.\nMore generally, in this work we propose to estimate PM,\u03b8(y|s, \u03b8), and hence the fairness metrics, via a probabilistic classifier that predicts the outcome y given protected attribute values s \u2208 A, trained on Ds. The complexity of the model determines the trade-off between (statistical) bias and variance in the estimation.2 For instance, ordered from high statistical bias to high variance, we could consider naive Bayes, logistic regression, or deep neural networks.\nAs a compromise between statistical bias and variance in this setting, we also introduce a novel hierarchical extension of logistic regression, where the \u201cprior\u201d on logit(P (y = 1|s)) is a Gaussian around the prediction of a jointly trained logistic regression, allowing deviations justified by sufficient data. Let ~sj be an encoding of protected attribute values sj with a binary indicator for each attribute\u2019s value, with integer j indexing each possi-\n1Note that Kearns et al. [2018] prove large-sample generalization guarantees for empirical estimates of \u03b3-SF. As we shall see, this does not imply that empirical estimates of \u03b3 will be accurate for small-tomoderately sized datasets. Nevertheless, since SF downweights small groups (the second term of Equation 2) and uses an additive formulation of fairness (compared to DF\u2019s multiplicative formulation), it is expected that empirical estimates will be somewhat more stable for SF than DF.\n2Here, statistical bias is not to be confused with unfairness.\nAlgorithm 1: Bayesian estimation of differential fairness and its uncertainty (and similarly for \u03b3-SF). Input: Development set D = {(xi, yi)}, mechanism\nM(x), protected attributes A Output: \u0302data, \u0302M(x), boxplots of posterior uncertainty in data, M(x), M(x) \u2212 data Apply M(x) to xi \u2208 D, obtain mechanism labels y\u2032i; Fit Bayesian classifier p1(y|s, \u03b8\u03041) on Ds = {(si, yi)}; Fit Bayesian classifier p2(y\u2032|s, \u03b8\u03042) on D\u2032s = {(si, y\u2032i)}; Estimate \u0302data via Eqn. 1 with posterior predictive p1(y|s); Estimate \u0302M(x) via Eqn. 1 with posterior predictive p2(y\n\u2032|s); Plot posterior uncertainty in data, M(x), M(x) \u2212 data;\nble value of s, and \u03b2i be a regression coefficient for each entry of the~sj\u2019s. The model\u2019s generative process is:\n\u2022 \u03c32 \u223c Exponential(\u03bb)\n\u2022 \u03b2i \u223c Normal(\u00b5, \u03c31), c \u223c Normal(\u00b5, \u03c31)\n\u2022 \u03b3j \u223c Normal(\u03b2\u1d40~sj + c, \u03c32)\n\u2022 P (y = 1|sj) = \u03c3(\u03b3j)) ,\nwhere \u03bb and \u03c31 are prior hyperparameters. For most typical models and datasets, to manage uncertainty in the data-sparse intersectional regime, we recommend that the probabilistic classifier be trained via fully Bayesian inference. Fully accounting for parameter uncertainty, a single best estimate of the conditional distributions \u03b8\u0302 to compute or \u03b3 is the posterior predictive distribution, \u03b8\u0302 = PModel(y|s,Ds) = \u222b \u03b8\u0304 PModel(y|s, \u03b8\u0304)PModel(\u03b8\u0304|Ds), for model parameters \u03b8\u0304. This can be approximated by, e.g., averaging PModel(y|s, \u03b8\u0304) over MCMC samples of \u03b8\u0304 or a variational posterior. We then report uncertainty in by plotting the posterior distribution over based on posterior samples of \u03b8\u0304, and similarly for \u03b3-SF. Our overall approach to the Bayesian modeling of intersectional fairness metrics is shown in pseudocode in Algorithm 1."
    },
    {
      "heading": "4 Bayesian Model Averaging Ensemble",
      "text": "A potential concern with the above approach is that different probabilistic models will lead to different estimates in the measurement of -DF and \u03b3-SF. Consistently with our Bayesian methodology, rather than performing model selection we can account for uncertainty over models by combining them using Bayesian model averaging [Hoeting et al., 1999]. Suppose there are K candidate models. We estimate the posterior distribution of (similarly \u03b3) in\nthe ensemble given dataset D via:\nP ( |D) = K\u2211 k=1 P ( |Mk,D)P (Mk|D) . (5)\nAssuming a uniform prior over models, P (Mk|D) \u221d\u220f (y,s)\u2208D P (y|s,Mk), the conditional marginal likelihood. The distribution P ( |Mk,D) is estimated via MCMC or variational inference over the posterior over the model parameters P (\u03b8\u0304k|Mk,D), with each \u03b8\u0304k corresponding to an (or \u03b3). Finally, we obtain a gold-standard estimate \u0302 or \u03b3\u0302 by simulating from the ensemble to estimate the posterior predictive distributions p(y|s,D), and plugging these into Equations 1 or 2."
    },
    {
      "heading": "5 Experimental Results",
      "text": "The goals of our experiments were to compare our proposed Bayesian modeling approach for estimating intersectional fairness to point estimation and to empirical measurement, to evaluate the performance of different models and of model averaging, to study the effect of uncertainty/variance in intersectional fairness estimation, and to illustrate the practical application of our methods. We performed all experiments on two datasets:\n\u2022 The Adult 1994 U.S. census income data from the UCI repository [Kohavi, 1996]. This dataset consists of 14 attributes regarding work, relationships, and demographics for individuals, who are labeled according to whether their income exceeds $50, 000 per year, pre-split into a training set of 32, 561 instances and a test set of 16, 281 instances. We select race, gender, and nationality as the protected attributes. As most instances have U.S. nationality, we treat nationality as binary between U.S. and \u201cother.\u201d Gender is also coded as binary. The race attribute originally had 5 values. We merged the Native American category with \u201cother,\u201d as both contained very few instances.3\n\u2022 The COMPAS dataset regarding a system that is used to predict criminal recidivism, and which has been criticized as potentially biased [Angwin et al., 2016]. We used race and gender as protected attributes. Gender was coded as binary. Race originally had 6 values, but we merged \u201cAsian\u201d and \u201cNative American\u201d with \u201cother,\u201d as all three contained very few instances. We used \u201cactual recidivism\u201d (within a 2-year period), which is binary, as\n3The decision to merge attribute values was made for a previous study (and similarly for the COMPAS dataset below). Leaving these values unmerged would likely have increased the relative benefit of our methods.\nthe true label of the data generating process and the COMPAS system\u2019s prediction as the labels from M(x). Following Angwin et al. [2016], we merged the \u201cmedium\u201d and \u201chigh\u201d labels to make COMPAS scores binary, since the actual labels are binary. For evaluating our models, we split the COMPAS dataset into train and test sets with 5,410 and 1,804 data instances, respectively.\nAll models were trained using PyMC3, with ADVI used for Bayesian inference. Posterior predictive distributions were estimated by sampling from the variational posterior and averaging the predictions. For ease of reading, important observations are indicated in bold."
    },
    {
      "heading": "5.1 Prediction Performance on Held-Out Data",
      "text": "We first studied the predictive performance for models of PM,\u03b8(y|s, \u03b8), as needed to compute -DF and \u03b3-SF: the empirical distribution (EDF), naive Bayes (NB), logistic regression (LR), deep neural networks (DNN), and our hierarchical logistic regression model (HLR). For each model, we compare point estimates (PE) (MAP, except for EDF), and fully Bayesian inference via the posterior predictive distribution (FB), as well as a Bayesian model averaging ensemble (Ensemble) of all PE and FB models. Note that the configuration of the DNN architecture is 3 hidden layers, 10 neurons in each layer, \u201crelu\u201d\nand \u201csigmoid\u201d activations for the hidden and output layers, respectively. The Dirichlet multinomial model (cf. Eqn. 4) is denoted EDF-FB. We trained all models on the training set and reported negative cross-entropy from the test set\u2019s empirical P (y|s) and P (y\u2032|s), averaged over intersections s. Negative cross-entropy is closely related to log-likelihood, and here measures the similarity of the model\u2019s conditional distributions to those of the test set.\nResults on the Adult and COMPAS datasets are shown in Table 2. We report results both for y labels in the test data (inequity in society), and for an algorithmic mechanism y\u2032 = M(x). For Adult, we set M(x) to be a logistic regression model, since it has an appropriate level of model complexity for this data regime. We trained the model on half of the training set (which was held out from the PM,\u03b8(y|s, \u03b8) models). For COMPAS, the mechanism M(x) is the COMPAS system itself. Although COMPAS is a black box, we observe its assigned class labels y\u2032, and our models extrapolate its behavior on intersectional groups. Furthermore, to simulate a scenario with more sparse data, we repeated these experiments using only 10% of the training data (for Adult, 6,512 instances for actual labels, and 3,256 instances for M(x) labels).\nWe found that in the vast majority of cases, the probabilistic models outperformed empirical estimates EDFPE and EDF-FB in terms of prediction performance, and that fully Bayesian inference outperformed point estimates. The best method was a fully Bayesian model in all cases. These differences were typically magnified in\nthe more-sparse regime where only 10% of the data was used. Deep neural networks (DNNs) were the best predictor for the actual labels in the data, i.e. bias in society, but performed worse at predicting the behavior of algorithms M(x), where in some cases they failed to outperform the EDF baselines. When predicting the behavior of algorithms, logistic regression and naive Bayes were the best performing methods. We hypothesize that these differences are primarily because real-world class distributions are more complex than M(x)\u2019s distributions, and partly because more data was available for the actual-label scenario due to the hold-out procedure.\nWhile our Bayesian hierarchical logistic regression (HLR-FB) method was never the best predictor on any one dataset, it exhibited the most reliable behavior across data sets, being the only method to outperform the empirical distribution (EDF-PE) and the Dirichletmultinomial baseline (EDF-FB) in all cases. The point estimate (PE) version of HLR performed too poorly to be shown. This may be due to numerical instability in PyMC3. The Bayesian model average (Ensemble) was also relatively stable across datasets, but HLR outperformed it here in most cases."
    },
    {
      "heading": "5.2 Fairness Metrics on Semi-Synthetic Data",
      "text": "In this section, we compare all the models with respect to the deviation of their fairness estimates from the ground truth. Since we cannot compute ground truth fairness metrics without knowing the true data distribution \u03b8, we design these experiments on semi-synthetic versions of the Adult and COMPAS datasets. We use the same number of protected attribute values, and instances per intersectional group as for COMPAS and Adult test datasets, but where the class probabilities are determined by a Gaussian model with a threshold decision boundary.\nIn our Gaussian threshold model, suppose that x is a\n\u201crisk score\u201d encoding the untrustworthiness of an individual, generated from a Gaussian given the individual\u2019s protected attributes s. The mechanism M(x) = x \u2265 t assign a \u201chigh risk of recidivism\u201d label if the individual\u2019s risk score exceeds threshold t. We generate the binary class labels for our semi-synthetic COMPAS and Adult datasets by drawing the same number of instances x per intersectional group as in the original data, and assigning class labels using M(x). We generate the data via P (x|s) = N(x;\u00b5 = ws \u00d7 \u2211 d sd, \u03c3 = 1), where sd is the dth protected attribute value for the individual encoded as an integer, ws \u2208 (0, 1) is a group-specific weight, and t = 2.5. We chose ws = Pdata(y = 1|s) plus a small constant, thereby making the synthetic data P (y = 1|s)\u2019s have some association with the empirical Pdata(y = 1|s)\u2019s. The overall process creates semi-synthetic data with correlations between intersectional groups, and reasonable ground truth values of and \u03b3.\nFigure 1 shows DF and SF estimates for all models on both semi-synthetic datasets, with the gray dotted vertical lines on top of the plots indicating the ground truth -DF and \u03b3-SF. Fully Bayesian inference allow us to encode uncertainty in the fairness metrics (box-plots) as well as a \u201cbest\u201d estimate using the posterior predictive distribution (\u201cX\u201d). Point estimates (PE) via MAP or the empirical distribution (for EDF), are indicated as \u201cO\u201d.\nOur first finding is that although empirical estimates of and \u03b3 (the \u201cO\u201d for EDF) are in some cases accurate, in others they can deviate substantially from the true values. The Bayesian estimates of and \u03b3, using the posterior predictive (\u201cX\u201d), were closer to the ground truth compared to PE (\u201cO\u201d) methods. Note that the posterior predictive estimates \u201cX\u201d, calculated via an average over p(y|s, \u03b8\u0304)\u2019s to compute a single , often deviated substantially from the posterior median , calculated as an average over the \u2019s corresponding to each posterior sample of \u03b8\u0304, and which was sometimes quite far from the ground truth (and similarly for \u03b3).\nOur HLR and Bayesian Ensemble approaches performed the best, in that their fairness estimates from the posterior predictive were overall the closest to the ground truth for both -DF and \u03b3-SF. The Ensemble typically reported higher posterior variance than HLR, due to averaging over multiple models. The EDF-FB Dirichletmultinomial model (boxplot and \u201cX\u201d for EDF) had accurate posterior predictive estimates, obtained using Equation 4 for , and similarly for \u03b3. However, on the Synthetic Adult dataset the EDF-FB posterior distribution over failed to capture the ground truth value, unlike HLR and the Bayesian Ensemble."
    },
    {
      "heading": "5.3 Stability of Estimation vs Data Sparsity",
      "text": "We now turn to the study of intersectional fairness estimation on the real datasets. We first investigated the stability of the estimation of the fairness metrics versus data sparsity, by estimating from bootstrap samples of the\ndatasets, varying the number of samples (Figure 2). For each number of data instances, we generated 10 bootstrap datasets and reported the average -DF and \u03b3-SF for each model on the Adult and COMPAS datasets.\nFor both fairness metrics, the estimates differed greatly between models in the small-data regime, and the models converged to relatively similar estimates as the amount of data increased. The empirical (EDF) estimates were often very noisy with little data, compared to most other models. Except for deep neural networks (DNN), Bayesian models (solid lines) were typically found to converge more quickly in the amount of data to the consensus full-data estimates, compared to point estimates (dashed lines). The Bayesian DNN (DNN-FB)\u2019s estimates of deviated substantially from the full data estimates in the low data regime, likely indicating poor performance. This may be due to the overparameterization of the model, and/or convergence issues.\nThe proposed HLR-FB model was relatively stable in the number of instances, and produced estimates of the fairness metrics which were similar to all models\u2019 fulldata estimates, even when the number of instances was very small. The Ensemble also exhibited this behavior.\nTo analyze the models\u2019 performance in the very sparse data regime in more detail, we compared their small data fairness metric estimates, calculated at the left end of the curves in Figure 2 (1% of the data), with the full data \u201cground truth\u201d (the right end of the curves). We approximate the ground truth and \u03b3 as the median of all bootstrap samples for all the models, where the size of the bootstrap samples is the size of the full dataset, and we report the average L1 distance from the small data estimates and the approximate ground truth (Table 3). We found that in the sparse data regime, the fully Bayesian models (FB) had smaller deviations from the full data \u201cground truth\u201d estimates. Our HLR-FB model and logistic regression performed the best in this sparse data regime including the Bayesian model averaging Ensemble method.\nIn Figure 3, we further studied the impact of dataset size on the DF bias amplification metric (Definition 2.2). Since this is calculated as the difference of two noisy estimates of -DF, the relative noise was higher. The methods differed in the estimated direction of the bias amplification (increase or decrease) in the small data regime, but all pointed to a positive increase with the full data, on both datasets. The overall conclusions were similar to the previous experiments, and the HLR-FB and Ensemble methods were once again the most stable when given little data. We report results on a bias amplification version of \u03b3-SF in the Appendix. Note that in these experiments, averaging over bootstraps improves the stability of high\nvariance estimation methods, and the estimates may differ in individual bootstrap samples (we report examples in the Appendix). Setting aside pure Bayesian or frequentist ideologies, to estimate the fairness metrics in practice it may be useful to use bootstrap averaging in conjunction with Bayesian models, as performed here."
    },
    {
      "heading": "5.4 Case Study on COMPAS Dataset",
      "text": "As a practical case study, we estimated the intersectional fairness metrics, and their uncertainty via the variational posteriors, on the COMPAS dataset (Figure 4). All models place high posterior probability on substantially high unfairness values and \u03b3, and they indicate that the direction of bias amplification is almost certainly positive for both metrics. To interpret -DF, note that the 80% rule, used as a legal standard for evidence of disparate impact discrimination [Equal Employment Opportunity Commission, 1978], finds evidence of discrimination if\n\u2265 \u2212 log 0.8 = 0.2231.4 All models put most of their posterior density, and their posterior predictive estimates, on values higher than this for true recidivism, COMPAS, and its bias amplification. The most reliable model, HLRFB, predicts that the DF bias amplification of COMPAS is most likely around 0.5-DF, with lower and upper posterior quartiles at around 0.35 and 0.65, respectively. The results show strong evidence that COMPAS increases the bias beyond the inequities in the data. We report a similar analysis on Adult in the Appendix."
    },
    {
      "heading": "6 Discussion: Practical Recommendations",
      "text": "We showed that fully Bayesian models provide more reliable estimates of intersectional fairness metrics than empirical estimates and point estimates. Although the best model depends on the data regime, our proposed HLR-FB model provides stable estimates compared to other methods, particularly in the very sparse data setting. We found that a Bayesian model averaging ensemble also improves stability in estimation, but it did not outperform HLR-FB on its own. We therefore recommend the use of HLRFB as a reliable intersectional fairness estimation method with sparse multi-attribute data."
    },
    {
      "heading": "7 Conclusion",
      "text": "We have proposed Bayesian modeling approaches to reliably estimate fairness and its uncertainty in the sparse data regime which arises from multi-attribute intersectional fairness definitions. Our empirical results show the benefits of the probabilistic model-based approach in this setting compared to empirical probability estimates, especially when using Bayesian inference. We proposed a Bayesian hierarchical logistic regression model which provides stable estimates of fairness metrics with sparse intersectional data, and we applied our methods to study the bias in the COMPAS recidivism predictor and a model trained on census data. We plan to develop extensions to model continuous protected attributes, more sophisticated latent variable modeling approaches, and learning algorithms which incorporate uncertainty in fairness measurement during training.\n4DF calculates ratios of probabilities for all y. Strictly, the 80% rule is calculated on the favorable outcome only."
    },
    {
      "heading": "Acknowledgments",
      "text": "This work was performed under the following financial assistance award: 60NANB18D227 from U.S. Department of Commerce, National Institute of Standards and Technology.\nWe thank Rosie Kar for valuable advice and feedback regarding intersectional feminism."
    },
    {
      "heading": "A Appendix: Additional Experimental Results",
      "text": "Our proposed HLR-FB model showed consistently stable behavior in all experiments, even with a very small number of instances, producing estimates of and \u03b3 which were similar to the final predictions of all models. The variance in the estimates of fairness was substantial for several models, but averaging over bootstrap samples mitigated this to some degree. To illustrate this, we randomly pick a bootstrap data sample at each data instance, instead of averaging over bootstrap samples (Figure 5). In this figure, we still average over 10 bootstrap samples for the Bayesian Ensemble method, as in Figure 2, as a reference to compare to the other models. Although all other models degrade somewhat in terms of estimation stability in this setting due to variance in their estimation, HLR-FB still enjoys relatively consistent and stable performance for both the -DF and \u03b3-SF metrics.\nIn Figure 6, we show the results of measuring (\u03b32\u2212\u03b31)SF bias amplification, defined similarly to the DF bias amplification metric (Definition 2.2). We once again average over 10 bootstrap samples, varying the number of data instances for both Adult and COMPAS datasets. The results are similar to the results we obtained for ( 2\u2212 1)- DF (Figure 3). For both datasets, HLR-FB performs similarly to the Bayesian ensemble method.\nFinally, we also conducted a case study with the Adult dataset where we estimated the intersectional fairness metrics, and their uncertainty via the variational posteriors (Figure 7). These results are in line with those of the COMPAS case study (Figure 4). They indicate that the direction of bias amplification is almost certainly positive for DF, however the bias amplification is roughly symmetric about 0 for the SF metric."
    },
    {
      "heading": "B Appendix: Related Work",
      "text": "Bayesian modeling of fairness has been performed by Simoiu et al. [2017] in the context of stop-and-frisk policing. They model risk probabilities within each protected category, and require algorithms (or people, such as police officers) to threshold these probabilities at the same points when determining outcomes.\nKusner et al. [2017] use Bayesian inference on causal graphical models for fairness. Under their counterfactual fairness definition, changing protected attributes A, while holding things which are not causally dependent on A constant, will not change the predicted distribution of outcomes.\nAs an alternative to the Bayesian methodology, adversarial methods are another strategy for managing uncertainty in a fairness context. For example, Beutel et al.\n[2017] apply this approach to the setting of ensuring fairness given a limited number of observations in which demographic information is available.\nIn a legal context, and before there was substantial research on fairness in AI, which was not their focus, Roth et al. [2006] and Collins and Morris [2008] studied various frequentist hypothesis testing methods for the 80% rule [Equal Employment Opportunity Commission, 1978] in the small data regime. These authors pointed out the dangers of determining adverse impact discrimination with small data and without proper statistical care. Although their emphasis was not on intersectionality, AI fairness, or Bayesian methods, these papers are important precursors to our work."
    }
  ],
  "title": "Bayesian Modeling of Intersectional Fairness: The Variance of Bias",
  "year": 2019
}
