{
  "abstractText": "Graph Neural Networks (GNNs) are a popular approach for predicting graph structured data. As GNNs tightly entangle the input graph into the neural network structure, common explainable AI (XAI) approaches are not applicable. To a large extent, GNNs have remained black-boxes for the user so far. In this paper, we contribute by proposing a new XAI approach for GNNs. Our approach is derived from high-order Taylor expansions and is able to generate a decomposition of the GNN prediction as a collection of relevant walks on the input graph. We find that these high-order Taylor expansions can be equivalently (and more simply) computed using multiple backpropagation passes from the top layer of the GNN to the first layer. The explanation can then be further robustified and generalized by using layer-wise-relevance propagation (LRP) in place of the standard equations for gradient propagation. Our novel method which we denote as \u2018GNN-LRP\u2019 is tested on scale-free graphs, sentence parsing trees, molecular graphs, and pixel lattices representing images. In each case, it performs stably and accurately, and delivers interesting and novel application insights.",
  "authors": [
    {
      "affiliations": [],
      "name": "Thomas Schnake"
    },
    {
      "affiliations": [],
      "name": "Oliver Eberle"
    },
    {
      "affiliations": [],
      "name": "Jonas Lederer"
    },
    {
      "affiliations": [],
      "name": "Shinichi Nakajima"
    },
    {
      "affiliations": [],
      "name": "Kristof T. Sch\u00fctt"
    },
    {
      "affiliations": [],
      "name": "Klaus-Robert M\u00fcller"
    },
    {
      "affiliations": [],
      "name": "Gr\u00e9goire Montavon"
    }
  ],
  "id": "SP:bf26f1e90a9d02599dfd64079adcab6b78726f44",
  "references": [
    {
      "authors": [
        "R. Albert",
        "A.-L. Barab\u00e1si"
      ],
      "title": "Statistical mechanics of complex networks",
      "venue": "Reviews of Modern Physics, 74(1):47\u201397, Jan.",
      "year": 2002
    },
    {
      "authors": [
        "L. Arras",
        "A. Osman",
        "K. M\u00fcller",
        "W. Samek"
      ],
      "title": "Evaluating recurrent neural network explanations",
      "venue": "CoRR, abs/1904.11829,",
      "year": 2019
    },
    {
      "authors": [
        "T. Bolukbasi",
        "K. Chang",
        "J.Y. Zou",
        "V. Saligrama",
        "A.T. Kalai"
      ],
      "title": "Man is to computer programmer as woman is to homemaker? debiasing word embeddings",
      "venue": "NIPS, pages 4349\u20134357,",
      "year": 2016
    },
    {
      "authors": [
        "H. Gonen",
        "Y. Goldberg"
      ],
      "title": "Lipstick on a pig: Debiasing methods cover up systematic gender biases in word embeddings but do not remove them",
      "venue": "NAACL-HLT (1), pages 609\u2013614. Association for Computational Linguistics,",
      "year": 2019
    },
    {
      "authors": [
        "A. Joulin",
        "E. Grave",
        "P. Bojanowski",
        "T. Mikolov"
      ],
      "title": "Bag of tricks for efficient text classification",
      "venue": "EACL (2), pages 427\u2013431. Association for Computational Linguistics,",
      "year": 2017
    },
    {
      "authors": [
        "D.P. Kingma",
        "J. Ba"
      ],
      "title": "Adam: A method for stochastic optimization",
      "venue": "ICLR (Poster),",
      "year": 2015
    },
    {
      "authors": [
        "T.N. Kipf",
        "M. Welling"
      ],
      "title": "Semi-supervised classification with graph convolutional networks",
      "venue": "ICLR (Poster). OpenReview.net,",
      "year": 2017
    },
    {
      "authors": [
        "S. Lapuschkin",
        "S. W\u00e4ldchen",
        "A. Binder",
        "G. Montavon",
        "W. Samek",
        "K.-R. M\u00fcller"
      ],
      "title": "Unmasking Clever Hans predictors and assessing what machines really learn",
      "venue": "Nature communications, 10:1096,",
      "year": 2019
    },
    {
      "authors": [
        "G. Montavon",
        "A. Binder",
        "S. Lapuschkin",
        "W. Samek",
        "K.-R. M\u00fcller"
      ],
      "title": "Layer-wise relevance propagation: An overview",
      "venue": "Explainable AI, volume 11700 of Lecture Notes in Computer Science, pages 193\u2013209. Springer,",
      "year": 2019
    },
    {
      "authors": [
        "P.E. Pope",
        "S. Kolouri",
        "M. Rostami",
        "C.E. Martin",
        "H. Hoffmann"
      ],
      "title": "Explainability methods for graph convolutional neural networks",
      "venue": "CVPR, pages 10772\u201310781. Computer Vision Foundation / IEEE,",
      "year": 2019
    },
    {
      "authors": [
        "R. Ramakrishnan",
        "P.O. Dral",
        "M. Rupp",
        "O.A. von Lilienfeld"
      ],
      "title": "Quantum chemistry structures and properties of 134 kilo molecules",
      "venue": "Scientific Data,",
      "year": 2014
    },
    {
      "authors": [
        "K. Sch\u00fctt",
        "P. Kindermans",
        "H.E.S. Felix",
        "S. Chmiela",
        "A. Tkatchenko",
        "K.-R. M\u00fcller"
      ],
      "title": "SchNet: A continuous-filter convolutional neural network for modeling quantum interactions",
      "venue": "NIPS, pages 991\u20131001,",
      "year": 2017
    },
    {
      "authors": [
        "K.T. Sch\u00fctt",
        "M. Gastegger",
        "A. Tkatchenko",
        "K.-R. M\u00fcller"
      ],
      "title": "Quantum-Chemical Insights from Interpretable Atomistic Neural Networks, pages 311\u2013330",
      "venue": "Springer International Publishing,",
      "year": 2019
    },
    {
      "authors": [
        "K.T. Sch\u00fctt",
        "H.E. Sauceda",
        "P.-J. Kindermans",
        "A. Tkatchenko",
        "K.-R. M\u00fcller"
      ],
      "title": "SchNet\u2013a deep learning architecture for molecules and materials",
      "venue": "The Journal of Chemical Physics, 148(24):241722,",
      "year": 2018
    },
    {
      "authors": [
        "K. Simonyan",
        "A. Zisserman"
      ],
      "title": "Very deep convolutional networks for large-scale image recognition",
      "venue": "ICLR,",
      "year": 2015
    },
    {
      "authors": [
        "R. Socher",
        "A. Perelygin",
        "J. Wu",
        "J. Chuang",
        "C.D. Manning",
        "A.Y. Ng",
        "C. Potts"
      ],
      "title": "Recursive deep models for semantic compositionality over a sentiment treebank",
      "venue": "EMNLP, pages 1631\u20131642. ACL,",
      "year": 2013
    }
  ],
  "sections": [
    {
      "heading": "1 Introduction",
      "text": "Many interesting structures found in scientific and industrial applications can be expressed as graphs. Examples are lattices in fluid modeling, molecular geometry, biological interaction networks, or social / historical networks. Graph neural networks (GNNs) [24, 34] have been proposed as a method to learn from observations in general graph structures and have found use in an ever growing number of applications [25, 39, 5, 33]. While GNNs make useful predictions, they typically act as black-boxes, and it is neither directly possible (1) to verify that the model has made the intended use of the graph structure, nor (2) to extract novel insight from the learned model.\nExplainable AI (XAI) is an emerging research area that is focused on extracting interpretable insights from trained ML models [23]. So far, research has focused, for example, on full black-box models [20, 31], or deep neural network classifiers [3], where in both cases, the explanations are given in terms of the input features. For a GNN, however, the graph being received as input is deeply entangled with the model itself, hence requiring a different approach.\nIn this paper, we propose a theoretically founded XAI method for GNNs, which explains the GNN prediction f(G) in terms of walks on the input graph G. Specifically, we perform a high-order Taylor expansion of the GNN prediction expressed in terms of adjacencies at each layer. Each element of the Taylor expansion corresponds to a particular walk W on\nar X\niv :2\n00 6.\n03 58\n9v 2\n[ cs\n.L G\n] 1\n2 Ju\nthe input graph, and its value gives the contribution RW of the walk to the prediction. The concept of explaining a GNN prediction in terms of relevant walks is illustrated at a high level in Figure 1.\ngraph neural network\ninput graph\nprediction\nexplanation (relevant walks) initial state t=0\nt=T\nFigure 1: Proposed \u2018relevant walks\u2019 approach for explaining the predictions of a GNN. Relevant walks are depicted in red and are shown in superposition to the input graph.\nFor GNNs with linear or ReLU activations, we find an equivalence between elements of the Taylor expansion and chains of local gradient computations from the GNN output to the initial state. The gradient-based formulation is easier to implement and computationally more tractable. Furthermore, by using layer-wise relevance propagation (LRP) [3] in place of the gradient propagation equations, the procedure can be made more robust to highly nonlinear GNNs.\nOur method which we call \u2018GNN-LRP\u2019 does not require retraining nor sampling from a particular input distribution. It applies to a broad range of GNNs and does not make any particular assumption about the structure of the input graph. This versatility is demonstrated on four tasks from diverse application fields: (1) prediction of the Baraba\u0301si-Albert growth parameter in scale-free graphs, (2) a sentiment prediction task on parsing trees of sentences, (3) prediction of quantum mechanically accurate properties from molecular graphs, and (4) image recognition by a convolutional neural network. For each task, our explanation method extracts useful and reliable insights into the decision strategy of the GNN."
    },
    {
      "heading": "1.1 Related work",
      "text": "In this section, we discuss the work that most directly relates to our GNN explanation approach, in particular, explanation techniques based on high-order analysis, and explanation techniques that are specialized to GNNs. For a review of papers in the field of XAI we refer the reader to [22, 2], where for contributions in the field of GNNs one can read [34, 38].\nHigh-Order Explanations High-order explanations (i.e. in terms of interacting quantities in the ML model) have received growing attention. Second-order methods (e.g. based on the model\u2019s Hessian) have been proposed to attribute predictions to pairs of input features [9, 11, 8]. Other methods have been proposed to identify the collective effect of larger groups of variables. One such approach relies on a model that incorporates an explicit sum-of-interactions structure [7]. Another method detects feature interaction as an iterative algorithm which inspects neural network weights at the different layers [32]. Another approach designs a special joint convolution-LSTM model to extract n-ary relations between sentences [37]. In comparison, the method we propose uses the framework of Taylor expansions to achieve in a principled and generalizable manner the desired high-order explanations.\nExplaining Graph Neural Networks Graph Convolutional Neural Networks [13] have been explained in terms of nodes and edges in the input graph using the LRP explanation method [28]. Another method for interpreting GNNs, the \u2018GNNExplainer\u2019 [35], extracts the subgraph that maximizes the mutual information to the original graph. For both the edges and nodes, the GNNExplainer finds relevant subsets in the input data. Another work\n[18] extends explanation methods such as Grad-CAM or Excitation Backprop to the GNN model. Our work differs by finding a scoring for sequences of edges (i.e. walks in the graph) instead of individual nodes or edges, which considerably enhances the informativeness of the produced explanation."
    },
    {
      "heading": "2 Explaining GNNs with Relevant Walks",
      "text": "Graph neural networks (GNNs) [24, 6] are a particular class of neural networks that receive as input a graph G, and some initial state H0. GNNs can be defined as applying the transition function\nHt = T (Ht\u22121,\u039bt,Wt) (1)\nat each layer t = 1 . . . T , followed by some readout function f to arrive at the prediction. The parameters (Wt) T t=1 are learned from the data, and (\u039bt) T t=1 are representations of the graph G at each layer. Note that \u039bt corresponds to a (possibly trainable) operator which incorporates the edges in G and can take various forms, for example, a graph convolutional operator, as in [6], or a linear message function, as in [10].\nThe GNN-LRP method we propose explains the prediction by extracting paths from the input to the output of the GNN that contribute the most to the prediction. These paths effectively correspond to walks on the input graph G. The presentation of our method is structured as follows: Sections 2.1 and 2.2 introduce the two key novel conceptual steps on which our explanation method is built, namely, high-order Taylor expansions of the GNN output, and a chain-rule formulation for extracting the terms of the expansion. Section 2.3 embeds the method in the existing LRP [3] framework to ensure that our method is favorably transferable to large highly nonlinear models, and Section 2.4 shows how to compute relevant walks practically. The different steps are summarized in Figure 2.\nJ\nK\n\u039b1\nf(\u039b) R...JK...\ngraph neural network\nf(\u039b) H0\nTaylor expansion reverse propagation (gradient / LRP) transitions\nre ad\nou t\n\u039b2 \u039bT\nFigure 2: Overview of our GNN-LRP method that identifies relevant walks for the GNN prediction."
    },
    {
      "heading": "2.1 Relevant Walks as a High-Order Taylor Decomposition",
      "text": "As a starting point, we assume that we have fed an input graph to a GNN and that the latter has produced a prediction. The prediction results from applying Eq. (1) iteratively from layer 0 to layer T followed by the top-level readout function f . This function receives as input the final state HT which itself depends on the input graph G through its representation \u039b.\nTo extract walks in the input graph that are relevant for the GNN prediction, we propose to perform a high-order Taylor expansion of f(\u039b). Specifically, we consider a T -order Taylor expansion and look at terms of this expansion that match particular paths in the GNN, i.e. particular walks in the input graph. Let W = (I, ..., J,K, ..., L) be one such walk and we view |W| to be the number of edges in the walk W. The |W|th order terms of a Taylor\nexpansion of f at some root point \u039b\u0303 can be expressed as follows1:\nRW = \u2202|W|f\n. . . \u2202\u03bbJK . . . \u2223\u2223\u2223\u2223 \u039b=\u039b\u0303 \u00b7 [ . . . \u00b7 (\u03bbJK \u2212 \u03bb\u0303JK) \u00b7 . . . ] (2)\nFor general GNNs, a meaningful reference point \u039b\u0303 is hard to find, and there may be many other terms of the Taylor expansion, potentially of even higher order, which we cannot interpret as walks. Thus, we will consider for the rest of the paper a slightly reduced scenario where the transition function of the GNN is given by:\nHt = \u03c1(\u039btHt\u22121Wt) (3)\nwith \u03c1 the ReLU activation function. We also assume that the readout function f is positive homogeneous of degree 1 and piecewise linear w.r.t. HT , and for all purposes, that it is implemented by a deep rectifier network or some simpler average pooling function. This reduced formulation still admits any graph G as input and many of the established GNN models are contained in this formulation [13, 25, 29] some of which we will consider in our experiments. In the formulation of Eq. (3) we can show that the function f(\u039b) has two useful properties: (1) piecewise multilinearity and (2) positive homogeneity of degree T . A formal derivation is given in Appendix A of the Supplement. With these properties a special choice of the reference point, specifically \u039b\u0303 = \u03b5\u039b with \u03b5 very small, makes Eq. (2) simplify to:\nRW = \u2202|W|f . . . \u2202\u03bbJK . . . \u00b7 [ . . . \u00b7 \u03bbJK \u00b7 . . . ] (4)\nFurthermore, all terms of the expansion that are not bound to a walk W in G converge to zero, implying the conservation property \u2211 W RW = f(\u039b). Conservation is a commonly stated property that explanation techniques should satisfy [3, 31]. Despite the apparent simplicity of Eq. (4), computing this quantity directly, e.g. using automatic differentiation, can be tedious. Automatic differentiation software is indeed mainly designed for first-order differentiation, and a recursive application of the automatic differentiation mechanism would likely not scale well."
    },
    {
      "heading": "2.2 Chain-Rule Formulation",
      "text": "Another contribution of our work is therefore to show that scores RW defined in Eq. (4) can be computed without explicit T -order differentiation. Instead, we find that these scores are equivalently given by applying the backpropagation algorithm from the output f to the initial state H0 and only retaining partial derivatives that belong to the walk W.\nProposition 1. When the GNN is structured according to Eq. (3), the relevance scores defined in Eq. (4) can be equivalently computed as:\nRW = h > I \u2202(\u00b7) \u2202hI . . . \u2202hJ \u2202(\u00b7) \u2202hK \u2202hJ \u2202(\u00b7) \u2202hK . . . \u2202hL \u2202(\u00b7) \u2202f \u2202hL (5)\ni.e. a chain of gradient computations from the top layer to the input layer.\n(cf. Appendix B of the Supplement for a derivation). The expression \u2202hK/\u2202hJ is a Jacobian matrix containing the derivatives of neuron activations representing node K w.r.t. neuron activations representing node J in the lower layer.\n1Note that the factor 1 T ! in the Taylor-formula vanishes since for each walk W there are T ! equivalent terms in the expansion corresponding to all orderings of differentiation of the edges in W."
    },
    {
      "heading": "2.3 Robustification with LRP",
      "text": "Simple gradient-based explanation methods have been shown to be noisy when applied to deep architectures (see e.g. [21]) and this noise can be explained by a \u2018shattered gradient\u2019 effect [4]. Especially in the context of extracting relevant walks, gradient computation does not benefit from averaging from the multiple gradient paths. The layer-wise relevance propagation (LRP) [3] method addresses this caveat by equipping neurons in the network with purposely designed propagation rules that determine the proportion of \u2018relevance\u2019 to distribute from neurons at a given layer to neurons in the lower layer. One such rule is LRP-\u03b3 [16]. When considering the GNN of Eq. (3), LRP-\u03b3 defines the fraction of relevance received by neuron k that should be redistributed to neuron j in the lower layer as:\npjk = \u03bbJKhjw + jk\u2211\nJ \u2211 j\u2208J \u03bbJKhjw + jk\nwhere w+jk = wjk+\u03b3max(0, wjk), where \u2211\nJ sums over all nodes in the lower layer and where\u2211 j\u2208J sums over all neurons of the given node. Denoting by PJK the matrix containing the scores pjk, we can replace Eq. (5) by the alternate chain rule:\nRW = 1 >PI,(\u00b7) . . . P(\u00b7),J \u00b7 PJK \u00b7 PK,(\u00b7) . . . P(\u00b7),L \u00b7RL (6)\nwhere RL is the vector containing relevance scores of each neuron l of the top-layer node L (the latter can be obtained by applying standard LRP on the readout function). The parameter \u03b3 controls the level of robustness. If setting \u03b3 > 0, the procedure becomes more robust and typically delivers better results for highly nonlinear models [16]. If choosing instead \u03b3 = 0 at each layer, the overall procedure can be shown to be equivalent to the chain rule for gradient propagation:\nProposition 2. The score RW obtained by Eq. (5) and Eq. (6) are equivalent when \u03b3 = 0.\nA proof is given in Appendix C of the Supplement. Viewing the gradient-based approach as a special case of LRP allows us to continue with the more general LRP formulation."
    },
    {
      "heading": "2.4 Practical Computation of Relevant Walks",
      "text": "When having access to the internal structure of the backward pass, Eq. (6) can be computed efficiently for all possible walks using search-based procedures. When instead the backward pass is accessible only through automatic differentiation, Eq. (6) can be more easily implemented following the strategy outlined in the Appendix of [22], where some terms in the forward definition of the model are detached from the gradient computation, so that the function remains equivalent, but the backward computation changes appropriately. Specifically, to implement LRP-\u03b3, we can rewrite Eq. (3) as:\nHt \u2190 \u03c1(\u039btHt\u22121W+t ) \u00b7 [\u03c1(\u039btHt\u22121Wt)/\u03c1(\u039btHt\u22121W+t )]const.\nwhere W+t = Wt + \u03b3max(0,Wt) and where [\u00b7]const. indicates that we have detached the quantity from the differentiation graph. The forward pass remains equivalent, however, the backward pass has been modified in a way that the LRP-\u03b3 rule is being matched. Similarly, to only retain what propagates along a certain walk and consequently be able to compute the score RW , we propose to perform the reassignment\nHt \u2190Ht Mt + [Ht]const. (1\u2212Mt)\nwhere Mt is a mask array whose values are set to 1 for neurons belonging to the node of the walk at layer t, and to 0 for other neurons. After implementing these modifications, Eq. (6) can be equivalently computed as RW = h>I \u2202 ?f/\u2202?hI where \u2202 ? denotes the modified backward pass.\nInterestingly, walk reductions (i.e. that ignore the node being traversed at a certain layer) can be produced by simply omitting the mask at that layer. Lastly, for graphs with local connectivity multiple walks can be collected in a single forward/backward pass by constructing masks that activate for multiple nodes with non-overlapping receptive fields."
    },
    {
      "heading": "3 Explaining GNN Predictions of Scale-Free Graphs",
      "text": "As a first experiment, we consider the task of explaining a GNN trained to classify between Baraba\u0301si-Albert (BA) graphs [1] of growth parameters 1 and 2. BA graphs are a particular model for scale-free graphs and the latter frequently occur in real world phenomena. BA graphs with growth parameter 1 form tree-like structures with few highly connected nodes, while those with parameter 2 form denser structures including cycles. We refer to the outputs of our GNN model detecting these special graph structures as BA-1 and BA-2. The GNN we train is composed of one input layer, two layers of message passing, one output layer, and a global average pooling over nodes of the graph. The trained GNN solves the task with perfect accuracy. Details on the GNN architecture, its training, and the generation of relevant walks, as well as further examples of explanations for different graphs, are given in Appendix D of the Supplement. Explanations by our GNN-LRP method of the two output neurons for an exemplary input graph are shown in Figure 3.\nGNN-LRP Baseline method [18]\noutput BA-1 output BA-2 output BA-1 output BA-2\nFigure 3: Left: Explanation of outputs BA-1 and BA-2 by our method. Positive contributions are in red and negative ones in blue. Circles indicate walks that stay in one node, lines are walks that stay between two nodes, and curves are walks that connect three different nodes. Right: Explanation obtained by the method in [18] for the same graph and the same outputs.\nAs expected, evidence for BA-1 is mainly attributed by our GNN-LRP method to nodes of degree 1 and their connecting edge, which is typical of this class of graphs. Evidence for BA-2 is instead attributed to longer walks traversing higher-degree nodes. Note that in comparison to the method of [18], our graph explanation technique better disentangles the various explanation factors, for example, it reveals that the highest-degree node in the graph of Figure 3 is not intrinsically relevant for BA-1 but only in relation to the 1-degree nodes it connects to."
    },
    {
      "heading": "4 Explaining Word Dependencies in Sentiment Detection",
      "text": "In natural language processing (NLP) text data can be processed either as a sequence, or with its corresponding grammatical structure represented by a parse tree [12]. The latter serves as an additional structural input for the learning algorithm, to incorporate word dependencies. In our case we consider a GNN to classify sentiments in natural language text [15], where we train our model on the Stanford Sentiment Treebank [30]. For details on the experimental setup we refer the reader to Appendix E in the Supplement.\nIn Figure 4 we consider a GNN model with 2 interaction layer, and a simple Bag-ofWords (BoW) model which can also be seen as a GNN model without interaction layer. We apply to both model our GNN-LRP method and choose the LRP parameter \u03b3 = 5. The BoW model detects positive evidence (red) for words like \u201dpictures\u201d, \u201dbest\u201d and \u201dlike\u201d, even though the sentence says \u201dboring pictures\u201d and \u201ddidn\u2019t like\u201d. From the GNN we see that word combinations give rise to the emphasis of a sentiment. The model correctly detects word combinations such as \u201ddidn\u2019t like\u201d and \u201dboring pictures\u201d to be negative evidence, and \u201dbest movies\u201d to be positive evidence for a positive sentiment. More generally, we observe that the relevance scores for consecutive words in the GNN model fluctuate less than those obtained for the BoW model. This suggests that the incorporation of the graph structure helps to better disambiguate the role of individual words in the construction of sentiment. Further examples of sentences with their GNN-LRP explanations are given in Appendix E of the Supplement.\nMethod AUPC\n\u2198 GNN-LRP 0.189 \u2198 Baseline [18] 0.402 \u2197 GNN-LRP 0.949 \u2197 Baseline [18] 0.906\nFigure 4: Left: Sentence predicted by the BoW and the GNN model, and explained by GNNLRP. Contributions to positive sentiment are in red, and contributions to negative sentiment are in blue. Right: Edge removal evaluation of GNN-LRP and the baseline method [18] on the task of explaining the GNN model. The edge removal was executed in descending (\u2198) and ascending (\u2197) relevance order.\nFor a quantitative evaluation of the GNN-LRP method we propose an \u2018edge removal\u2019 evaluation where edges of the parsing tree are iteratively set to zero following the relevance score attached to them. Edge scores are either inherited by the sum of walk relevances which contain the edge, or by the product of adjacent node relevances. The iteration is either done in ascending or descending order of relevance scores. In a similar way to the pixel-flipping evaluation [21], we record the function value after each perturbation, and measure the area under the resulting perturbation curve (AUPC). Details of the evaluation are given in Appendix E of the Supplement. In the table of Figure 4 we see that the AUPC is significantly lower for GNN-LRP method with descending perturbation, and close to one for ascending perturbation. This demonstrates that GNN-LRP identifies better than the baseline the elements of the sentence that contribute positively and negatively to the GNN output.\nOverall, applying the proposed explanation method to the GNN model for sentiment classification has highlighted that GNN predictions are based on detecting complex sentence sub-structures, rather than single words as in the BoW model. The correctness of these insights was then verified using an \u2018edge removal\u2019 evaluation measuring the accuracy of the produced explanations."
    },
    {
      "heading": "5 Explaining Molecular Electronic Properties",
      "text": "In the field of quantum chemistry, GNNs exhibit state of the art performance for predicting molecular properties [10, 25, 26]. Such networks incorporate a graph structure of molecules\neither by the covalent bonds or the proximity of atoms. In the following, we apply the proposed explanation method to SchNet \u2013 a GNN for the prediction of molecular properties [25, 27]. We train SchNet on the atomization energy and the dipole moment for molecules in the QM9 dataset [19].\nFirst, we use GNN-LRP to analyze the influence of bonds on the energy prediction. The atomization energy of a molecule describes the energy difference to dissociated atoms. In the following we visualize the accumulated bond dissociation energy which corresponds to the negative atomization energy. Consequently, bonds involved in walks of high relevance are associated with stronger bonding. Figure 5 a) shows the high energy contribution per bond for different bond types, respectively. The illustration describes an increasing energy contribution with ascending bond order. Chemically, this makes sense since bonds of higher order are more stable and thus contain more energy. For a qualitative analysis, Fig. 5 b) shows the walk relevances on the paracetamol (acetaminophen) molecule for the energy prediction. Similar as in Fig. 5 a) we see that the aromatic ring and bonds of higher order show high positive relevance, while regions involving single bonds are less relevant. Note that SchNet does not take bond types as an input.\nIn Fig. 5 c) we see the GNN-LRP explanation for another molecular property, the dipole moment. The depicted walk relevance gives a hint on local dipole moments of subgroups and the entailed global dipole moment of the molecule. In accordance with the dependence of the dipole on the span of the molecule, the most relevant walks are situated at the positive and negative poles. Even though the network is only trained on the magnitude of the dipole, the explanation shows its direction. For more details on the model parameters, and an extended version of the experiments, we refer to Appendix F in the Supplement.\na) b) c)\nO\nN O H H\nH H\nH H\nH H\nH\nFigure 5: Relationship between molecule structure and relevance. Blue and red corresponds to negative and positive relevance, respectively. Opacity indicates relevance scores. a) High energy contribution per bond, depicted for each bond type separately, in arbitrary units. b) Summed relevant walks for the energy. c) All relevant walks for the dipole moment.\nGNN-LRP can be applied to a variety of graph network architectures for quantum chemistry. For the SchNet, it has provided insights into the structure-property relationship of molecules that reach beyond the original prediction task. The resulting relevant walks agree with chemical characteristics of the molecule, thus, indicating that the neural network has indeed learned chemically plausible regularities. Since GNN-LRP gives a one-to-one correspondence between the importance of sub-parts in the molecule and the prediction, the future work will focus on defining coarse-grained representations of large molecules by neglecting non-relevant subgraphs."
    },
    {
      "heading": "6 Revisiting the Explanation of a CNN Image Classifier",
      "text": "A convolutional neural network (CNN) can be seen as a particular graph neural network (GNN) operating on lattices of pixels. CNN predictions have so far mainly been explained using heatmaps highlighting pixels that are the most relevant for a given prediction [36, 3, 31]. Heatmaps are a useful representation summary of the decision structure, but they do not reveal the complex strategies that have been used to progressively build the prediction layer after layer. We will show by viewing CNNs as graph neural networks and extracting relevant\nwalks in the resulting pixel lattice, that our GNN-LRP method is capable of shedding light into these strategies.\nFor this, we consider the well-established VGG-16 [29] network. It consists of a collection of blocks interleaved by pooling layers, where each block is composed of a sequence of convolution and ReLU layers. Because the number of possible walks grows exponentially with neural network depth, we reduce the complexity by summing all walks that meet the same node at the beginning and at the end of a particular block. These walks can then be further summarized by computing for each pixel at the input of the block the average position of the walk at the output of the block, letting us represent relevant walks as a vector field where relevance scores are represented with the opacity of the arrows. (Cf. Appendix G of the Supplement for details on the computation and rendering of the relevant walks.) Results for two exemplary images at various blocks of the VGG-16 network are shown in Fig. 6.\ninput image walks in VGG:Block3 walks in VGG:Block4 walks in VGG:Block5\nFigure 6: Relevant walks in the pixel lattice explaining the prediction by the VGG-16 network of two input images as \u2018teapot\u2019 and \u2018dumbbell\u2019 respectively.\nFor the first example, Block 3 first detects local edges in the teapot, then, in Block 4, the walks converge to center points of specific parts of the teapot (e.g. the handle, the spout and the knob), and finally the walks converge in Block 5 to the center of the teapot, which can be interpreted as composing the different parts of the teapot. For the second example we investigate a known \u2018Clever Hans\u2019 strategy where the network classifies images as \u2018dumbbell\u2019 by detecting both the dumbbell and the arm that holds it [17]. Using GNNLRP we observe that Block 3 and 4 detect the arm and the dumbbell separately, and then, Block 5 composes them into a single \u2018dumbbell-arm\u2019 concept, as shown by the walks for both objects converging to some center point near the wrist. Hence, our method can be used to revisit a number of anecdotal observations made in the past about ImageNet classifiers, by bringing a deeper understanding of why and how they occur."
    },
    {
      "heading": "7 Conclusion",
      "text": "Graph neural networks are a highly popular approach for prediction in graphs. However, methods for explaining their predictions have so far been limited. In this work we have contributed by proposing GNN-LRP, a novel theoretically principled and computationally effective method for explaining GNN predictions in terms of relevant walks. Our method is applicable to any graph topology, whether they are standard undirected graphs, parsing trees, or pixel lattices. Compared to existing GNN explanation methods, our approach provides more informative explanations, in particular, by highlighting for the first time the complex and sequential interaction between nodes of the graph, that lead to the output prediction. These explanations could already provide useful insights into applications as\ndiverse as sentiment analysis, quantum chemistry, and image classification. Future studies will focus both on exploring automatic differentiation schemes for GNN-LRP and on broadly applying the novel framework in the context of understanding quantum chemical properties of materials, e.g. in catalysis.\nBroader Impact\nGaining a better understanding of trained GNNs is beneficial to users, e.g. to verify that the model generalizes well and implements a strategy that is acceptable and not of Clever Hans nature [14]. The proposed method does not put anyone at disadvantage. If the method would fail to deliver a good explanation in a certain scenario, there would not be direct consequences as long as the explanation is used for decision support rather than as an own decision entity. Because the proposed explanation method does not train a model on its own, it also does not leverage biases in the data."
    },
    {
      "heading": "Acknowledgements",
      "text": "This work was funded by the German Ministry for Education and Research as BIFOLD \u2013 Berlin Institute for the Foundations of Learning and Data (ref. 01IS18025A and ref. 01IS18037A), and the German Research Foundation (DFG) as Math+: Berlin Mathematics Research Center (EXC 2046/1, project-ID: 390685689). This work was partly supported by the Institute for Information & Communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (No. 2017-0-00451, No. 2017-0-01779). We further want to thank Jacob Kauffmann, Lukas Ruff and Marina Ho\u0308hne for the helpful discussions and Niklas W. A. Gebauer for providing the tools to visualise the 3d molecules."
    },
    {
      "heading": "A Positive Homogeneity and Piecewise Multilinearity of the GNN Output",
      "text": "Consider the GNN prediction f(\u039b), where f is positive homogeneous of degree 1 and piecewise linear w.r.t. HT , and\nHt = \u03c1(\u039btHt\u22121Wt) for t = 1, . . . , T. (1)\nWe first show that HT (H0,\u039b,W ) is positive homogeneous of degree T w.r.t. \u039b = (\u039b1, . . . ,\u039bT ), i.e.,\n\u2200s\u22650 : HT (H0, s\u039b,W ) = sT HT (H0,\u039b,W ). (2)\nSince \u03c1(x) = max(0, x) and therefore \u03c1(sx) = s\u03c1(x), Equation (2) holds for T = 1:\nH1(H0, s\u039b,W ) = \u03c1(s\u039b1H0W1)\n= s\u03c1(\u039b1H0W1)\n= sH1(H0,\u039b,W ).\nHypothesize that Equation (2) holds for T \u2212 1. Then\nHT (H0, s\u039b,W )\n= \u03c1(s\u039bTHT\u22121(H0, s\u039b,W )WT ) = \u03c1(s\u039bT s T\u22121HT\u22121(H0,\u039b,W ),WT ) (hypothesis) = sT \u03c1(\u039bTHT\u22121WT ) = sTHT (H0,\u039b,W ),\nar X\niv :2\n00 6.\n03 58\n9v 2\n[ cs\n.L G\n] 1\n2 Ju\nn 20\n20\nwhich by induction proves Equation (2) for any T > 0. Since f is positive homogeneous of degree 1 w.r.t. HT , we have\n\u2200s\u22650 : f(s\u039b) = sT f(\u039b),\nwhich proves positive homogeneity of degree T of the GNN prediction f(\u039b). Piecewise multilinearity of f(\u039b) w.r.t. {\u039bt}T\u22121t=1 can be proven similarly. Clearly,\nH1(H0,\u039b,W ) = \u03c1(\u039b1H0W1) (3)\nis piecewise linear with respect to \u039b1. Hypothesize that HT\u22121(H0,\u039b,W ) is piecewise multilinear to {\u039bt}T\u22121t=1 . Then,\nHT = \u03c1(\u039bTHT\u22121WT ) (4)\nis a piecewise linear function of \u039bT applied to a piecewise multilinear function HT\u22121 of {\u039bt}T\u22121t=1 . Therefore, HT is by induction piecewise multilinear to {\u039bt}Tt=1. Since f is piecewise linear w.r.t. HT , the GNN output f(\u039b) is piecewise multilinear to {\u039bt}Tt=1."
    },
    {
      "heading": "B Connection between Taylor Decomposition and Chain Rule",
      "text": "Let I, . . . , J,K, . . . , L define node indices at consecutive layers, and i, . . . , j, k, . . . , l neuron indices within these nodes. In our notation, we identify a node index and the set of neurons that belong to the node, i.e., J \u2190 neurons(J) also denotes the set of neurons in the node J , and \u2211 J \u2211 j\u2208J denotes the sum over all neurons that belong to all nodes.\nThe mapping between two layers, for example J and K, is given in sum notation as\nzk = \u2211 J ( \u03bbJK \u2211 j\u2208J hjwjk ) for hk = \u03c1(zk).\nIn order to prove Proposition 1, we need to show that\nRW = \u2202|W|f . . . \u2202\u03bbJK . . . \u00b7 [ . . . \u00b7 \u03bbJK \u00b7 . . . ] = h>I \u2202(\u00b7) \u2202hI . . . \u2202hJ \u2202(\u00b7) \u2202hK \u2202hJ \u2202(\u00b7) \u2202hK . . . \u2202hL \u2202(\u00b7) \u2202f \u2202hL . (5)\nLet us recall that f is positive homogeneous of degree 1 and piecewise linear w.r.t. HT . By simply applying the chain rule we note that\n\u2202f \u2202\u03bbJK = \u2211\nk\u2208K\n\u2202hk \u2202\u03bbJK \u2202f \u2202hk = \u2211\nj\u2208J\n\u2211 k\u2208K hj wjk \u03c1 \u2032(zk) \u2202f \u2202hk . (6)\nIt is important to point out that hj in (6) only depends on lower-layer interaction units from node J . This enables us to distinguish for multivariate derivatives, e.g. \u2202 2f\n\u2202\u03bb(\u00b7)J\u03bbJK , the\ncomponents which depend on \u03bb(\u00b7)J , such that\n\u22022f \u2202\u03bb(\u00b7),J\u03bbJK = \u2211\nj\u2208J\n\u2211\nk\u2208K\n\u2202hj \u2202\u03bb(\u00b7)J wjk \u03c1 \u2032(zk) \u2202f \u2202hk . (7)\nConsidering a walk with W = (I, . . . , J,K, . . . , L) we see that for (5) we get\nRW = \u2202|W|f . . . \u2202\u03bbJK . . . \u00b7 [ . . . \u00b7 \u03bbJK \u00b7 . . . ]\n= \u2211 i\u2208I . . . \u2211 j\u2208J \u2211 k\u2208K . . . \u2211 l\u2208L hi \u00b7 {wi(\u00b7)\u03c1\u2032(z(\u00b7))} \u00b7 . . . \u00b7 {wjk \u03c1\u2032(zk)} \u00b7 . . . \u00b7 {w(\u00b7)l \u03c1\u2032(zl)} \u00b7 \u2202f \u2202hl \u00b7 [\u03bbI(\u00b7) \u00b7 . . . \u00b7 \u03bbJK \u00b7 . . . \u00b7 \u03bb(\u00b7)L] = \u2211\ni\u2208I . . . \u2211 j\u2208J \u2211 k\u2208K . . . \u2211 l\u2208L hi \u00b7 {\u03bbI(\u00b7)wi(\u00b7)\u03c1\u2032(z(\u00b7))} \u00b7 . . . \u00b7 {\u03bbJK wjk \u03c1\u2032(zk)}\ufe38 \ufe37\ufe37 \ufe38\n= \u2202hk \u2202hj\n\u00b7 . . .\n\u00b7 {\u03bb(\u00b7)Lw(\u00b7)l\u03c1\u2032(zl)} \u00b7 \u2202f\n\u2202hl\n= \u2211 i\u2208I . . . \u2211 j\u2208J \u2211 k\u2208K . . . \u2211 l\u2208L hi \u00b7 \u2202h(\u00b7) \u2202hi \u00b7 . . . \u00b7 \u2202hk \u2202hj \u00b7 . . . \u00b7 \u2202hl \u2202h(\u00b7) \u00b7 \u2202f \u2202hl = h>I \u2202(\u00b7) \u2202hI . . . \u2202hJ \u2202(\u00b7) \u2202hK \u2202hJ \u2202(\u00b7) \u2202hK . . . \u2202hL \u2202(\u00b7) \u2202f \u2202hL ,\nwith which we reached the RHS in (5)."
    },
    {
      "heading": "C Chain Rule as a Special Case of LRP",
      "text": "We prove Proposition 2 by showing\nRW = h > I \u2202(\u00b7) \u2202hI . . . \u2202hJ \u2202(\u00b7) \u2202hK \u2202hJ \u2202(\u00b7) \u2202hK . . . \u2202hL \u2202(\u00b7) \u2202f \u2202hL = 1>PI,(\u00b7) . . . P(\u00b7),J \u00b7 PJK \u00b7 PK,(\u00b7) . . . P(\u00b7),L \u00b7RL\n(8)\nwhen \u03b3 = 0 in the GNN-LRP method. As a first step, application of LRP with \u03b3 = 0 in the readout function gives the relevances RL = hL \u2202f\u2202hL . Then, for the message passing layers, we note that\npjk = \u03bbJKhjwjk\u2211\nJ \u2211 j\u2208J \u03bbJKhjwjk\nfor \u03b3 = 0. We want to show (8) inductively over the number of layers in the network. In the case of a one-layer network, we get by definition of RL that RW = 1>RL = h>L \u2202f \u2202hL\n. We therefore consider the case of a 2-layer network i.e. W = (K,L) as the base step for induction. It holds that\n\u2211\nk\u2208K\n\u2211 l\u2208L hk \u2202hl \u2202hk \u2202f \u2202hl = \u2211 k\u2208K \u2211 l\u2208L hk \u00b7 \u03bbKL wkl \u03c1\u2032(zl) \u00b7 \u2202f \u2202hl .\nSince \u03c1(x) = max(0, x) we get that \u03c1\u2032(x) = 1x>0, further with hl = \u03c1(zl) by definition, we can see that \u03c1\u2032(zl) =\nhl zl . Therefore\n\u2211\nk\u2208K\n\u2211 l\u2208L hk \u2202hl \u2202hk \u2202f \u2202hl = \u2211 k\u2208K \u2211 l\u2208L \u03bbKLhk wkl zl hl \u00b7 \u2202f \u2202hl\n= \u2211\nk\u2208K\n\u2211\nl\u2208L\n\u03bbKLhk wkl\u2211 K\u2032( \u2211 k\u2032\u2208K\u2032 \u03bbK\u2032Lhk\u2032wk\u2032l) \u00b7Rl\n= \u2211\nk\u2208K\n\u2211 l\u2208L pklRl.\nLet us hypothesize that Equation (8) holds for a walk W = (K, . . . , L) i.e.\nRW = \u2211 k\u2208K . . . \u2211 l\u2208L hk \u2202(\u00b7) \u2202hk . . . \u2202hl \u2202(\u00b7) \u2202f \u2202hl = \u2211 k\u2208K . . . \u2211 l\u2208L pk(\u00b7) . . . p(\u00b7)lRl.\nThen, for a walk with an additional node J , i.e. W = (J,K, . . . , L), we have\nRW = \u2211\nj\u2208J\n\u2211 k\u2208K \u00b7 \u00b7 \u00b7 \u2211 l\u2208L hj \u2202hk \u2202hj \u2202(\u00b7) \u2202hk . . . \u2202hl \u2202(\u00b7) \u2202f \u2202hl\n= \u2211\nj\u2208J\n\u2211 k\u2208K \u00b7 \u00b7 \u00b7 \u2211 l\u2208L hj hk \u2202hk \u2202hj pk(\u00b7) . . . p(\u00b7)l Rl (hypothesis)\n= \u2211\nj\u2208J\n\u2211 k\u2208K \u00b7 \u00b7 \u00b7 \u2211 l\u2208L \u03bbJKhj wjk hk \u03c1\u2032(zk) pk(\u00b7) . . . p(\u00b7)l Rl\n= \u2211\nj\u2208J\n\u2211 k\u2208K \u00b7 \u00b7 \u00b7 \u2211 l\u2208L \u03bbJKhj wjk\u2211 J\u2032( \u2211 j\u2032\u2208J\u2032 \u03bbJ\u2032Khj\u2032wj\u2032k) \u00b7 pk(\u00b7) . . . p(\u00b7)l Rl\n= \u2211\nj\u2208J\n\u2211 k\u2208K \u00b7 \u00b7 \u00b7 \u2211 l\u2208L pjk pk(\u00b7) . . . p(\u00b7)l Rl,\nwhich by induction proves Equation (8) for any walk W."
    },
    {
      "heading": "D Details for the Scale-Free Graphs Experiment",
      "text": "In this experiment, we use a GNN composed of four layers of parameters: one input layer with only self-connections, two message passing layers, and one output layer with only self connections. Each layer uses ReLU activations. The matrix \u039bt at each message passing layer is set to the renormalized graph Laplacian \u039bt = D \u2212 12AD\u2212 1 2 [7] where A is the graph adjacency matrix to which we have added the self-connections, and D is a diagonal matrix with Dii = \u2211 jAij . The number of neurons per node is set to 10 at the input, 64 for the intermediate layers, and 2 at the output layer. These two output neurons code for Baraba\u0301siAlbert (BA) graphs [1] with growth parameters 1 and 2 (referred as BA-1 and BA-2). On top of these layers, we add a top-level global average pooling layer over the different nodes of the graph.\nThe initial state H0 consists of indicator vectors for each node. The GNN is trained for 20000 SGD iterations with learning rate 0.001 and momentum 0.99. We use a mean square error objective where the BA-1 and BA-2 targets are coded as (1, 0) and (0, 1) respectively. The network is trained on a collection of randomly generated BA graphs with 10 nodes each, and growth parameters 1 and 2. Once the network has been trained, we produce explanations using GNN-LRP. Here, we can already stop the LRP propagation procedure at layer 1 since the first layer does not include any message passing between the different nodes.\nWe apply LRP-\u03b3 at each layer. Explanations of neuron outputs BA-1 and BA-2 for different values of \u03b3 are shown in Figure 1 for a given input graph. When \u03b3 is close to zero, the explanations combine many positive and negative contributions, and patterns that code for a specific concept are hard to extract visually. As \u03b3 increases, negative contributions become smaller, however, the positive contributions tend to spread to other regions of the graph that are not characteristic of the predicted concept. While the spread is limited for this particular model, the effect becomes more severe for strongly nonlinear models such as image classifiers (see also Appendix G). We find that choosing \u03b3 = 0.25 at each layer gives satisfying results as the spread is fully contained and the amount of negative contributions remains limited.\nFurther examples of explanations generated by our GNN-LRP method for different input graphs are given in Figure 2. For graphs with a tree-like structure, there is mainly evidence\nfor the output neuron BA-1. Our method explains this evidence mainly in terms of leaf nodes or edges connecting to these leaf nodes. For graphs with denser structures, the output neuron BA-2 becomes more active, and explanations for the prediction are now mainly given as longer walks traversing many different nodes."
    },
    {
      "heading": "E Details for the Sentiment Analysis Experiment",
      "text": "For the sentiment detection task we use a variant of the GCN model [7], to train it with supervision on the SST [16] dataset. We obtain the dependency-based parse trees and part-of-speech (POS) tags of each sentence, from the module en core web sm from Explosion.ai. Note that we consider the dependency tree to be undirected, i.e. its adjacency matrix is symmetric. By default, the SST dataset provides a sentiment range of 5 classes (\u2212\u2212,\u2212, 0,+,++), which we transfer into a binary classification task by neglecting the samples with neutral sentiment 0 and unite \u2212\u2212, \u2212 and ++, + into just negative \u2212 and positive + respectively.\nThe transition unit has the structure of Eq. (1) with depth T = 4. The hidden matrix H0 consists of the concatenated word vectors from FastText [5], plus a randomly initialized embedding for POS tags of dimension 30. For all other layers than the input layer we choose the hidden dimension to be dH = 10. Regarding the transition operator of the GNN unit we set \u039b1 = \u039bT = Id, where Id is the identity matrix. For every other layer t = 2, . . . , T \u22121 we use the structure of the tree encoded into the adjacency matrix A and set \u039bt := D\u0303 \u2212 12 A\u0303D\u0303\u2212 1 2\nwhere A\u0303 := A + Id and D\u0303 := diag( \u2211 i a\u0303i,1, . . . , \u2211 i a\u0303i,N ) is the degree matrix of A\u0303. Note that the BoW experiments are the results of the same model for T = 2, hence there are no intermediate diffusion layers, just two layers with only self-connections. After the transition unit we do an average pooling of the last hidden representation HT \u2208 RN\u00d7dH , in the direction of the number of nodes N . It follows a linear layer onto the target dimension 2 and a softmax layer to obtain a probability of the target class.\nDuring the training we choose the cross-entropy loss and Adam [6] to be the optimizer, which we initialize by the learning rate of 0.00002. We used a batch size of 20 and trained for 100 epochs. The average best accuracy of the model is about 79%. We compute all walks with GNN-LRP (Eq. (6) in Section 2.3 of the main paper) with \u03b3 = 5, where the\ncomputation of all walks for a sample in the SST dataset takes about 1-2 seconds. In Figure 3 we see additional samples out of the SST dataset, interpreted by GNNLRP. We observe in general similar characteristics as for the example shown in the main paper. One interesting example is the explanation for the sentence \u201cHugh Grant and Sandra Bullock are two such likeable actors.\u201d, where the system detects \u201cHugh Grant\u201d as positive and \u201cSandra Bullock\u201d as negative evidence for a positive sentiment. This is not only of Clever Hans type [8], but also an evidence for a bias towards the entity \u201cHugh Grant\u201d. Bias of models in natural language processing is a well studied field and we refer to [3, 4] for more information. Nevertheless GNN-LRP is able to detect such biases and can therefore help to understand whether the model\u2019s decision is reasonable.\nE.1 Edge Removal Evaluation and AUPC Scores\nWe want to give a more detailed description of the edge removal evaluation method, used in Section 4 of the main paper. Note that our edge removal method can be seen as an enhancement of the word flipping procedure [2]. When setting one edge in the graph to zero, this can be understood as roughly separating the sentence into two subsentences and thereby removing the compositional information. The flipping of the edge information is formally induced by a scoring function Se, which we consider for each edge e. The scoring function induces an ordered sequence of edges\n(e1, . . . , eL) from the property (i < j)\u21d4 (Sei < Sej ), and in turn an ordered sequence of adjacency matrices\n\u039b(0) := \u039b \u039b(k) := g(\u039b(k\u22121), ek) k = 1, . . . , L\nwhere g(\u039b, e) returns a copy of \u039b with zero at the entry of edge e. Note that the sequence of matrices (\u039b(l))Ll=0 interpolates between \u039b\n(0) = \u039b and \u039b(L) = 0. With this sequence, we want to study the behavior of the perturbation curve (f(\u039b(l)))Ll=0. Also, we reported performance in the main paper using the area under the perturbation curve (AUPC), which is formally defined by\nAUPC =\n\u222b\n\u039b\n1\nL\nL\u2211\nl=0\nf(\u039b(l)) dp(\u039b) (9)\nwhere p(\u039b) is the distribution of the whole dataset.\nIn our benchmark evaluation we inherit the edge scoring Se by the relevances of edges Re, which means we define for an edge perturbation in descending (\u2198) order or relevance by choosing the scoring function Se = Re. If this perturbation curve shows a drastic drop, this indicates that the scoring function correctly detected the relevant edges in \u039b. We also consider edge removal in ascending (\u2197) where we choose instead Se = \u2212Re. In this scenario, an almost constant perturbation curve an indicator for a robust information scoring, which means the scoring function correctly found the non-important parts of the input data. To clarify the definition of edge relevances Re we use in our benchmark experiment, we note that in the case of GNN-LRP, we sum over walk relevances which contain e, i.e. Re = \u2211 W:e\u2208W RW . For the baseline method [10], which provides input node relevances RI , we define for the edge e = (I, I \u2032) as the product Re := RI \u00b7RI\u2032 . Figure 4 shows the averaged perturbation curves over the SST [16] test set, for both models with ascending and descending scoring order, where f is in our case the probability of the predicted sentiment.\nWe clearly see that the perturbation curves from descending scoring inherited by GNNLRP show a drastic initial drop, where for a scoring in ascending order the perturbation curve only shows a very mild decay. This coincides with the qualitative analysis and the results for the AUPC in Section 4."
    },
    {
      "heading": "F Details for the Molecular Prediction Experiment",
      "text": "For the relevance attribution of a GNN that predicts molecular properties, we applied the GNN-LRP to SchNet [12, 14]. The transition function of SchNet can be expressed as\nHt+1 = Ht + \u03c1(\u039bt (Ht W t3)) W t4 (10)\nwhere \u039bt is a learnable continuous filter given by (\u039bt)I := \u2211\nJ\u2208nbh(I) relu(eIJW\nt 1)W t 2 , with edge feature eIJ := [ exp(\u2212\u03b4(DIJ \u2212 \u00b5)2) ] \u00b5\u2208M\nand M being a uniform grid between 0 and the cutoff distance \u00b5max with grid-width \u2206\u00b5 and resolution \u03b4 = 12\u2206\u00b52 . The scalar values DIJ define the distance between the atoms I and J . In the framework of SchNet, the message passing steps defined by (10) are referred to as interaction blocks. To facilitate a robust interpretation method we choose the non-linearity \u03c1 = Id to be the identity, which makes the interaction blocks linear.\nIn all experiments we used 3 interaction blocks, a feature dimension of 128, and a grid resolution of \u03b4 = 9.7 A\u030a\u22122. We choose a relatively short cutoff distance of \u00b5max = 2.5 A\u030a which induces a sparser convolutional operator. Note that the choice of \u00b5max is small enough to keep the total number of walks limited. We train the model on 110,000 randomly selected molecules in the QM9 [11] dataset. On a test set of 13,885 molecules, the dipole moment is predicted with a mean absolute error (MAE) of 0.068 Debye and the MAE for atomization energy predictions is 0.032 eV . When applying GNN-LRP to SchNet, we assume the convolution operator \u039bt to be fixed. The skip connection in Eq. (10) is comparable to a self loop in the molecular graph. This perspective makes the interaction unit into a linear model, which enables us to apply GNN-LRP with \u03b3 = 0, without loss of a qualitative performance, because linear models do not have noisy gradient signals.\nTo corroborate the qualitative relevance analysis of Section 5, here, we provide a more detailed evaluation of the same molecule than in Figure 5 of the main paper. To this end, first, we divide the walks into four groups based on the number of different atoms contained in each walk. In Figure 5, the relevance propagation for both, the energy and the dipole moment is depicted for the four groups, respectively. For both targets, separating the walks allows for a hierarchical analysis of the molecular characteristics. Walks containing few different atoms describe local characteristics of the molecule. In particular, for the energy prediction, one-atom walks indicate the relevance of single atoms based on their atom types. Two-atoms walks facilitate the identification of bond strength. Especially, walks on the aromatic ring are highlighted, and also the C-O double bond exhibits mainly postive\nrelevance walks. Three- and four-atoms walks describe more complex, global interactions, and thus, they strongly depend on the topology of the molecule. Since the atomization energy mostly depends on the local structure, the relevance of walks decreases with the number of involved atoms. In contrast, the dipole moment depends on the global structure of the molecule. Hence, two- and three-atoms walks dominate the prediction. Two-atoms walks clearly indicate the electrostatic poles of the molecule, while giving a hint on local dipoles. On the other hand, three- and four-atoms walks highlight interactions in the acetylgroup.\nWe now want to take a closer look at a quantitative evaluation of the walk relevances for the dipole moment. In the qualitative analysis we saw that the walk relevances significantly differ from zero at the poles with a change of sign in the direction of the dipole. In order to verify this behavior statistically, we evaluate relevance scores for a set of 100 molecules. To allow for a comparison between molecules of different sizes, we scale each molecule to have a span of 1 along its dipole direction. The latter is predicted by SchNet as stated by Schu\u0308tt et al. [13]. Subsequently, we project all walks onto their respective dipole to obtain a one-dimensional distribution of absolute relevance values for all molecules. The resulting distribution is rendered as a histogram in Figure 6, where we observe that the closer a walk is to one of the poles, the more its relevance score differs from zero. This quantitative analysis thus corroborates the qualitative observations we have made in Section 5."
    },
    {
      "heading": "G Details for the Image Classification Experiment",
      "text": "In the experiments of Section 6 of the paper, we use the pretrained version of the VGG-16 network [15] without batch-normalization, which can be retrieved using the TorchVision module of PyTorch. We perform our experiment on two images available at the URLs:\nhttps://www.piqsels.com/en/public-domain-photo-fjjsr https://www.piqsels.com/en/public-domain-photo-fiffy\nThe first image shows a teapot and the second image image shows a woman lifting two dumbbells. Both images are rescaled and cropped to the relevant region to produce images of size 224 \u00d7 224 which are the standard input size for VGG-16. Images are respectively predicted as \u2018teapot\u2019 and \u2018dumbbell\u2019 by the network.\nWe view VGG-16 as a graph neural network, where each node of the GNN corresponds to neurons at a given layer and at a given spatial location on the feature maps. Hence, each node contains as many neurons as there are feature maps in the corresponding layer.\nTo explain the VGG-16 prediction in terms of relevant walks, we conceptually compute all walks into the network but marginalize them in a way that all walks that pass through a given node at the output of some block of interest and a given node at the input of the same block have their relevance scores RW aggregated.\nIn this experiment, we implement GNN-LRP using the mask-based strategy outlined in Section 2.4 of the main paper. With this mask-based approach, marginalization can be implemented simply and efficiently by removing the masks in the blocks above and below as well as removing the masks in the intermediate layers of the block of interest. Marginalization reduces the number of walks to 564 walks in block 3, 284 walks in block 4, and 144 walks in block 5. We further observe that because of the local connectivity structure of the VGG-16 network, some of these walks do not exist, and among remaining walks, several of them can be collected simultaneously with a single backward pass. In detail, since the receptive field of each block output is of size 7\u00d7 7, we can build a mask M on the block output which is not 1 at a single location but which is 1 every 7 pixels in xand y-direction. Relevance of walks can then be collected at the input of the block where\neach tile contains the set of walks associated to the corresponding output nodes. Overall all outputs can be scanned with 49 such masks, and we can therefore collect all walks for a given block with a single minibatch of size 49. This takes a few seconds per image on a laptop. The procedure is illustrated in Figure 7.\nFor rendering, we further simplify the obtained collection of relevant walks by rectifying the relevance scores and computing for each node at the input of the block the average position of the walk at the output of the block weighted by the relevance score. These walks can then be rendered as a vector field. Finally, the opacity value for each arrow in the vector field is set in proportion to the total relevance associated to the arrow, specifically, the transparency parameter is set to \u03b1 = clip(2.1 RWRmax \u2212 0.1, [0, 1]). Explanations in the main paper are produced by choosing LRP-\u03b3 [9] with \u03b3 = 0.5 in block 3, \u03b3 = 0.25 in block 4, \u03b3 = 0.125 in block 5, and \u03b3 = 0 in the top-level classifier. The effect of the parameter \u03b3 on the explanation is shown for Block 4 in Figure 8.\nA too low parameter \u03b3 produces noisy explanations from which it is difficult to extract the classifier\u2019s strategy. A too high parameter \u03b3 produces explanations that are very regular but not selective for the concept predicted at the output."
    }
  ],
  "title": "XAI for Graphs: Explaining Graph Neural Network Predictions by Identifying Relevant Walks",
  "year": 2020
}
