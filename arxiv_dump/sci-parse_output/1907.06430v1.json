{"abstractText": "We offer a graphical interpretation of unfairness in a dataset as the presence of an unfair causal path in the causal Bayesian network representing the data-generation mechanism. We use this viewpoint to revisit the recent debate surrounding the COMPAS pretrial risk assessment tool and, more generally, to point out that fairness evaluation on a model requires careful considerations on the patterns of unfairness underlying the training data. We show that causal Bayesian networks provide us with a powerful tool to measure unfairness in a dataset and to design fair models in complex unfairness scenarios.", "authors": [{"affiliations": [], "name": "Silvia Chiappa"}, {"affiliations": [], "name": "William S. Isaac"}], "id": "SP:bcdc7c8a1a0719056f7dc17269056ae68ec5ed0a", "references": [{"authors": ["AI No"], "title": "Institute", "venue": "Litigating algorithms: Challenging government use of algorithmic decision systems,", "year": 2018}, {"authors": ["M. Alexander"], "title": "The New Jim Crow: Mass Incarceration in the Age of Colorblindness", "venue": "The New Press,", "year": 2012}, {"authors": ["D.A. Andrews", "J. Bonta"], "title": "Level of Service Inventory \u2013 Revised", "venue": "Multi-Health Systems Toronto,", "year": 2000}, {"authors": ["D.A. Andrews", "J. Bonta", "J.S. Wormith"], "title": "The recent past and near future of risk and/or need assessment", "venue": "Crime & Delinquency, 52(1):7\u201327,", "year": 2006}, {"authors": ["J. Angwin", "J. Larson", "S. Mattu", "L. Kirchner"], "title": "Machine Bias: There\u2019s software used across the country to predict future criminals", "venue": "And it\u2019s biased against blacks., May", "year": 2016}, {"authors": ["D. Arnold", "W. Dobbie", "C.S. Yang"], "title": "Racial bias in bail decisions", "venue": "The Quarterly Journal of Economics, 133:1885\u20131932,", "year": 2018}, {"authors": ["R. Berk", "H. Heidari", "S. Jabbari", "M. Kearns", "A. Roth"], "title": "Fairness in criminal justice risk assessments: The state of the art", "venue": "Sociological Methods & Research,", "year": 2018}, {"authors": ["M. Bogen", "A. Rieke"], "title": "Help wanted: An examination of hiring algorithms, equity, and bias", "venue": "Technical report, Upturn,", "year": 2018}, {"authors": ["T. Brennan", "W. Dieterich", "B. Ehret"], "title": "Evaluating the predictive validity of the COMPAS risk and needs assessment system", "venue": "Criminal Justice and Behavior, 36(1):21\u201340,", "year": 2009}, {"authors": ["A. Byanjankar", "M. Heikkil\u00e4", "J. Mezei"], "title": "Predicting credit risk in peer-to-peer lending: A neural network approach", "venue": "In IEEE Symposium Series on Computational Intelligence, pages 719\u2013725,", "year": 2015}, {"authors": ["S. Chiappa"], "title": "Path-specific counterfactual fairness", "venue": "In Thirty-Third AAAI Conference on Artificial Intelligence,", "year": 2019}, {"authors": ["S. Chiappa", "W.S. Isaac"], "title": "A causal Bayesian networks viewpoint on fairness", "venue": "In E. Kosta, J. Pierson, D. Slamanig, S. Fischer-H\u00fcbner, S. Krenn (eds) Privacy and Identity Management. Fairness, Accountability, and Transparency in the Age of Big Data. Privacy and Identity 2018. IFIP Advances in Information and Communication Technology, volume 547. Springer, Cham,", "year": 2019}, {"authors": ["A. Chouldechova"], "title": "Fair prediction with disparate impact: A study of bias in recidivism prediction instruments", "venue": "Big data, 5(2):153\u2013163,", "year": 2017}, {"authors": ["A. Chouldechova", "E. Putnam-Hornstein", "D. Benavides-Prado", "O. Fialko", "R. Vaithianathan"], "title": "A case study of algorithm-assisted decision making in child maltreatment hotline screening decisions", "venue": "Proceedings of Machine Learning Research, 81:134\u2013148,", "year": 2018}, {"authors": ["S. Corbett-Davies", "E. Pierson", "A. Feller", "S. Goel", "A. Huq"], "title": "A computer program used for bail and sentencing decisions was labeled biased against blacks", "venue": "It\u2019s actually not that clear., October", "year": 2016}, {"authors": ["S. Corbett-Davies", "E. Pierson", "A. Feller", "S. Goel", "A. Huq"], "title": "Algorithmic decision making and the cost of fairness", "venue": "In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, page 797\u2013806,", "year": 2017}, {"authors": ["S. Corbett-Davies", "S.Goel"], "title": "The measure and mismeasure of fairness: A critical review of fair machine learning", "year": 2018}, {"authors": ["P. Dawid"], "title": "Fundamentals of statistical causality", "venue": "Technical report, University College London,", "year": 2007}, {"authors": ["J. De Fauw", "J.R. Ledsam", "B. Romera-Paredes", "S. Nikolov", "N. Tomasev", "S. Blackwell", "H. Askham", "X. Glorot"], "title": "B", "venue": "O\u2019Donoghue, D. Visentin, G. van den Driessche, B. Lakshminarayanan, C. Meyer, F. Mackinder, S. Bouton, K. Ayoub, R. Chopra, 18 S. Chiappa and W. S. Isaac D. King, A. Karthikesalingam, C. O. Hughes, R. Raine, J. Hughes, D. A. Sim, C. Egan, A. Tufail, H. Montgomery, D. Hassabis, G. Rees, T. Back, P. T. Khaw, M. Suleyman, J. Cornebise, P. A. Keane, and O. Ronneberger. Clinically applicable deep learning for diagnosis and referral in retinal disease. Nature Medicine, 24(9):1342\u20131350,", "year": 2018}, {"authors": ["W. Dieterich", "C. Mendoza"], "title": "and T", "venue": "Brennan. COMPAS risk scales: Demonstrating accuracy equity and predictive parity,", "year": 2016}, {"authors": ["C. Dwork", "M. Hardt", "T. Pitassi", "O. Reingold", "R. Zemel"], "title": "Fairness through awareness", "venue": "In Proceedings of the 3rd Innovations in Theoretical Computer Science Conference, pages 214\u2013226,", "year": 2012}, {"authors": ["L. Eckhouse", "K. Lum", "C. Conti-Cook", "J. Ciccolini"], "title": "Layers of bias: A unified approach for understanding problems with risk assessment", "venue": "Criminal Justice and Behavior, 46:185\u2013209,", "year": 2018}, {"authors": ["V. Eubanks"], "title": "Automating Inequality: How High-Tech Tools Profile, Police, and Punish the Poor", "venue": "St. Martin\u2019s Press,", "year": 2018}, {"authors": ["M. Feldman", "S.A. Friedler", "J. Moeller", "C. Scheidegger", "S. Venkatasubramanian"], "title": "Certifying and removing disparate impact", "venue": "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages 259\u2013268,", "year": 2015}, {"authors": ["A.W. Flores", "K. Bechtel", "C.T. Lowenkamp"], "title": "False positives, false negatives, and false analyses: A rejoinder to \"Machine Bias: There\u2019s software used across the country to predict future criminals", "venue": "And it\u2019s biased against blacks.\". Federal Probation, 80(2):38\u201346,", "year": 2016}, {"authors": ["Harvard Law School"], "title": "Note: Bail reform and risk assessment: The cautionary tale of federal sentencing", "venue": "Harvard Law Review,", "year": 2018}, {"authors": ["X. He", "J. Pan", "O. Jin", "T. Xu", "B. Liu", "T. Xu", "Y. Shi", "A. Atallah", "R. Herbrich", "S. Bowers"], "title": "et al", "venue": "Practical lessons from predicting clicks on ads at facebook. In Proceedings of the Eighth International Workshop on Data Mining for Online Advertising, pages 1\u20139,", "year": 2014}, {"authors": ["M. Hoffman", "L.B. Kahn", "D. Li"], "title": "Discretion in hiring", "venue": "The Quarterly Journal of Economics, 133(2):765\u2013800,", "year": 2018}, {"authors": ["W.S. Isaac"], "title": "Hope, hype, and fear: The promise and potential pitfalls of artificial intelligence in criminal justice", "venue": "Ohio State Journal of Criminal Law, 15(2):543\u2013558,", "year": 2017}, {"authors": ["J. Kleinberg", "S. Mullainathan", "M. Raghavan"], "title": "Inherent trade-offs in the fair determination of risk scores", "venue": "In 8th Innovations in Theoretical Computer Science Conference, pages 43:1\u201343:23,", "year": 2016}, {"authors": ["J.L. Koepke", "D.G. Robinson"], "title": "Danger ahead: Risk assessment and the future of bail reform", "venue": "Washington Law Review, 93:1725\u20131807,", "year": 2017}, {"authors": ["K. Kourou", "T.P. Exarchos", "K.P. Exarchos", "M.V. Karamouzis", "D.I. Fotiadis"], "title": "Machine learning applications in cancer prognosis and prediction", "venue": "Computational and Structural Biotechnology Journal, 13:8\u201317,", "year": 2015}, {"authors": ["M.J. Kusner", "J.R. Loftus", "C. Russell", "R. Silva"], "title": "Counterfactual fairness", "venue": "In Advances in Neural Information Processing Systems 30, pages 4069\u20134079,", "year": 2017}, {"authors": ["K. Lum"], "title": "Limitations of mitigating judicial bias with machine learning", "venue": "Nature Human Behaviour, 1(7):1,", "year": 2017}, {"authors": ["K. Lu"], "title": "and W", "venue": "Isaac. To predict and serve? Significance, 13(5):14\u201319,", "year": 2016}, {"authors": ["M. Malekipirbazari", "V. Aksakalli"], "title": "Risk assessment in social lending via random forests", "venue": "Expert Systems with Applications, 42(10):4621\u20134631,", "year": 2015}, {"authors": ["S.G. Mayson"], "title": "Bias in, bias out", "venue": "Yale Law School Journal, 128,", "year": 2019}, {"authors": ["S. Mitchell", "E. Potash"], "title": "and S", "venue": "Barocas. Prediction-based decisions and fairness: A catalogue of choices, assumptions, and definitions,", "year": 2018}, {"authors": ["J. Pearl"], "title": "Causality: Models, Reasoning, and Inference", "venue": "Cambridge University Press,", "year": 2000}, {"authors": ["J. Pearl", "M. Glymour", "N.P. Jewell"], "title": "Causal Inference in Statistics: A Primer", "venue": "Wiley,", "year": 2016}, {"authors": ["C. Perlich", "B. Dalessandro", "T. Raeder", "O. Stitelman", "F. Provost"], "title": "Machine learning for targeted display advertising: Transfer learning in action", "venue": "Machine learning, 95(1):103\u2013127,", "year": 2014}, {"authors": ["J. Peters", "D. Janzing", "B. Sch\u00f6lkopf"], "title": "Elements of Causal Inference: Foundations and Learning Algorithms", "venue": "MIT Press,", "year": 2017}, {"authors": ["M. Rosenber"], "title": "and R", "venue": "Levinson. Trump\u2019s catch-and-detain policy snares many who call the U.S. home. https://www.reuters.com/investigates/special-report/ usa-immigration-court, June", "year": 2018}, {"authors": ["A.D. Selbst"], "title": "Disparate impact in big data policing", "venue": "Georgia Law Review, 52:109\u2013195,", "year": 2017}, {"authors": ["P. Spirtes", "C.N. Glymour", "R. Scheines", "D. Heckerman", "C. Meek", "G. Cooper", "T. Richardson"], "title": "Causation, Prediction, and Search", "venue": "MIT Press,", "year": 2000}, {"authors": ["M.T. Stevenson"], "title": "Assessing risk assessment in action", "venue": "Minnesota Law Review, 103,", "year": 2017}, {"authors": ["R. Vaithianathan", "T. Maloney", "E. Putnam-Hornstein", "N. Jiang"], "title": "Children in the public benefit system at risk of maltreatment: Identification via predictive modeling", "venue": "American Journal of Preventive Medicine, 45(3):354\u2013359,", "year": 2013}, {"authors": ["J. Zhang", "E. Bareinboim"], "title": "Fairness in decision-making \u2013 the causal explanation formula", "venue": "In Proceedings of the 32nd AAAI Conference on Artificial Intelligence,", "year": 2018}], "sections": [{"heading": "1 Introduction", "text": "Machine learning is increasingly used in a wide range of decision-making scenarios that have serious implications for individuals and society, including financial lending [10,36], hiring [8,28], online advertising [27,41], pretrial and immigration detention [5,43], child maltreatment screening [14,47], health care [19,32], and social services [1,23]. Whilst this has the potential to overcome undesirable aspects of human decision-making, there is concern that biases in the training data and model inaccuracies can lead to decisions that treat historically discriminated groups unfavourably. The research community has therefore started to investigate how to ensure that learned models do not take decisions that are unfair with respect to sensitive attributes (e.g. race or gender).\nThis effort has led to the emergence of a rich set of fairness definitions [13,16,21,24,38] providing researchers and practitioners with criteria to evaluate existing systems or to design new ones. Many such definitions have been found to be mathematically incompatible [7,13,15,16,30], and this has been viewed as representing an unavoidable trade-off establishing fundamental limits on fair machine learning, or as an indication that certain definitions do not map on to social or legal understandings of fairness [17].\nMost fairness definitions express properties of the model output with respect to sensitive information, without considering the relations among the relevant variables underlying the data-generation mechanism. As different relations would require a model to satisfy different properties in order to be fair, this could lead\nThe final publication is available at doi.org/10.1007/978-3-030-16744-8_1 [12].\nar X\niv :1\n90 7.\n06 43\n0v 1\n[ st\nat .M\nL ]\n1 5\nJu l 2\nto erroneously classify as fair/unfair models exhibiting undesirable/legitimate biases.\nIn this manuscript, we use the causal Bayesian network framework to draw attention to this point, by visually describing unfairness in a dataset as the presence of an unfair causal path in the data-generation mechanism. We then use this viewpoint to raise concern on the fairness debate surrounding the COMPAS pretrial risk assessment tool. Finally, we show that causal Bayesian networks offer a powerful tool for representing, reasoning about, and dealing with complex unfairness scenarios."}, {"heading": "2 A Graphical View of (Un)fairness", "text": "Consider a dataset \u2206 = {an, xn, yn}Nn=1, corresponding to N individuals, where an indicates a sensitive attribute, and xn a set of observations that can be used (possibly together with an) to form a prediction y\u0302n of outcome yn. We assume a binary setting an, yn, y\u0302n \u2208 {0, 1} (unless otherwise specified), and indicate with A,X , Y , and Y\u0302 the (set of) random variables1 corresponding to an, xn, yn, and y\u0302n respectively.\nIn this section we show at a high-level that a correct use of fairness definitions concerned with statistical properties of Y\u0302 with respect to A requires an understanding of the patterns of unfairness underlying \u2206, and therefore of the relationships among A, X and Y . More specifically we show that:\n(i) Using the framework of causal Bayesian networks (CBNs), unfairness in \u2206 can be viewed as the presence of an unfair causal path from A to X or Y .\n(ii) In order to determine which properties Y\u0302 should possess to be fair, it is necessary to question and understand unfairness in \u2206.\nA Q\nD Y\nfair? unfair fair?\nAssume a dataset \u2206 = {an, xn = {qn, dn}, yn}Nn=1 corresponding to a college admission scenario in which applicants are admitted based on qualifications Q, choice of department D, and gender A; and in which female applicants apply more often to certain departments. This scenario can be represented by\nthe CBN on the left (see Appendix A for an overview of BNs, and Sect. 3 for a detailed treatment of CBNs). The causal path A\u2192 Y represents direct influence of gender A on admission Y , capturing the fact that two individuals with the same qualifications and applying to the same department can be treated differently depending on their gender. The indirect causal path A \u2192 D \u2192 Y represents influence of A on Y through D, capturing the fact that female applicants more often apply to certain departments. Whilst the direct path A\u2192 Y is certainly an unfair one, the paths A\u2192 D and D \u2192 Y , and therefore A\u2192 D \u2192 Y , could either be considered as fair or as unfair. For example, rejecting women more often due to department choice could be considered fair with respect to college 1 Throughout the paper, we use capital and small letters for random variables and their values, and calligraphic capital letters for sets of variables.\nresponsibility. However, this could be considered unfair with respect to societal responsibility if the departmental differences were a result of systemic historical or cultural factors (e.g. if female applicants apply to specific departments at lower rates because of overt or covert societal discouragement). Finally, if the college were to lower the admission rates for departments chosen more often by women, then the path D \u2192 Y would be unfair.\nDeciding whether a path is fair or unfair2 requires careful ethical and sociological considerations and/or might not be possible from a dataset alone. Nevertheless, this example illustrates that we can view unfairness in a dataset as the presence of an unfair causal path from the sensitive attribute A to X or Y .\nDifferent (un)fair path labeling requires Y\u0302 to have different characteristics in order to be fair. In the case in which the causal paths from A to Y are all unfair (e.g. if A\u2192 D \u2192 Y is considered unfair), a Y\u0302 that is statistically independent of A (denoted with Y\u0302 \u22a5 A) would not contain any of the unfair influence of A on Y . In such a case, Y\u0302 is said to satisfy demographic parity.\nDemographic Parity (DP). Y\u0302 satisfies demographic parity if Y\u0302 \u22a5 A, i.e. p(Y\u0302 = 1|A = 0) = p(Y\u0302 = 1|A = 1), where e.g. p(Y\u0302 = 1|A = 0) can be estimated as\np(Y\u0302 = 1|A = 0) \u2248 1 N0 N\u2211 n=1 1y\u0302n=1,an=0 ,\nwith 1y\u0302n=1,an=0 = 1 if y\u0302n = 1 and an = 0 (and zero otherwise), and where N0 indicates the number of individuals with an = 0. Notice that many classifiers, rather than a binary prediction y\u0302n \u2208 {0, 1}, output a degree of belief that the individual belongs to class 1, rn, also called score. For example, in the case of logistic regression rn corresponds to the probability of class 1, i.e. rn = p(Y = 1|an, xn). To obtain the prediction y\u0302n \u2208 {0, 1} from rn, it is common to use a threshold \u03b8, i.e. y\u0302n = 1rn>\u03b8. In this case, we can rewrite the estimate for p(Y\u0302 = 1|A = 0) as\np(Y\u0302 = 1|A = 0) \u2248 1 N0 N\u2211 n=1 1rn>\u03b8,an=0 .\nNotice that R \u22a5 A implies Y\u0302 \u22a5 A for all values of \u03b8.\nIn the case in which the causal paths from A to Y are all fair (e.g. if A\u2192 Y is absent and A\u2192 D \u2192 Y is considered fair), a Y\u0302 such that Y\u0302 \u22a5 A|Y or Y \u22a5 A|Y\u0302 would be allowed to contain such a fair influence, but the (dis)agreement between Y and Y\u0302 would not be allowed to depend on A. In these cases, Y\u0302 is said to satisfy equal false positive/false negative rates and calibration respectively.\nEqual False Positive and Negative Rates (EFPRs/EFNRs). Y\u0302 satisfies EFPRs and EFNRs if Y\u0302 \u22a5 A|Y , i.e. (EFPRs) p(Y\u0302 = 1|Y = 0, A = 0) = p(Y\u0302 = 2 A path could also be only partially fair \u2014 we omit this case for simplicity.\n1|Y = 0, A = 1) and (EFNRs) p(Y\u0302 = 0|Y = 1, A = 0) = p(Y\u0302 = 0|Y = 1, A = 1).\nCalibration. Y\u0302 satisfies calibration if Y \u22a5 A|Y\u0302 . In the case of score output R, this condition is often instead called predictive parity at threshold \u03b8, p(Y = 1|R > \u03b8,A = 0) = p(Y = 1|R > \u03b8,A = 1), and calibration defined as requiring Y \u22a5 A|R.\nIn the case in which at least one causal path from A to Y is unfair (e.g. if A\u2192 Y is present), EFPRs/EFNRs and calibration are inappropriate criteria, as they would not require the unfair influence of A on Y to be absent from Y\u0302 (e.g. a perfect model (Y\u0302 = Y ) would automatically satisfy EFPRs/EFNRs and calibration, but would contain the unfair influence). This observation is particularly relevant to the recent debate surrounding the correctional offender management profiling for alternative sanctions (COMPAS) pretrial risk assessment tool. We revisit this debate in the next section."}, {"heading": "2.1 The COMPAS Debate", "text": "Over the past few years, numerous state and local governments around the United States have sought to reform their pretrial court systems with the aim of reducing unprecedented levels of incarceration, and specifically the population of low-income defendants and racial minorities in America\u2019s prisons and jails [2,25,31]. As part of this effort, quantitative tools for determining a person\u2019s likelihood for reoffending or failure to appear, called risk assessment instruments (RAIs), were introduced to replace previous systems driven largely by opaque discretionary decisions and money bail [6,26]. However, the expansion of pretrial RAIs has unearthed new concerns of racial discrimination which would nullify the purported benefits of these systems and adversely impact defendants\u2019 civil liberties.\nAn intense ongoing debate, in which the research community has also been heavily involved, was triggered by an expos\u00e9 from investigative journalists at ProPublica [5] on the COMPAS pretrial RAI developed by Equivant (formerly Northpointe) and deployed in Broward County in Florida. The COMPAS general recidivism risk scale (GRRS) and violent recidivism risk scale (VRRS), the focus of ProPublica\u2019s investigation, sought to leverage machine learning techniques to improve the predictive accuracy of recidivism compared to older RAIs such as the level of service inventory-revised [3] which were primarily based on theories and techniques from a sub-field of psychology known as the psychology of criminal conduct [4,9]3.\n3 While the exact methodology underlying GRRS and VRRS is proprietary, publicly available reports suggest that the process begins with a defendant being administered a 137 point assessment during intake. This is used to create a series of dynamic risk factor scales such as the criminal involvement scale and history of violence scale. In addition, COMPAS also includes static attributes such as the defendant\u2019s age and prior police contact (number of prior arrests). The raw COMPAS scores are\nProPublica\u2019s criticism of COMPAS centered on two concerns. First, the authors argued that the distribution of the risk score R \u2208 {1, . . . , 10} exhibited discriminatory patterns, as black defendants displayed a fairly uniform distribution across each value, while white defendants exhibited a right skewed distribution, suggesting that the COMPAS recidivism risk scores disproportionately rated white defendants as lower risk than black defendants. Second, the authors claimed that the GRRS and VRRS did not satisfy EFPRs and EFNRs, as FPRs = 44.9% and FNRs = 28.0% for black defendants, whilst FPRs = 23.5% and FNRs = 47.7% for white defendants (see Fig. 1). This evidence led ProPublica to conclude that COMPAS had a disparate impact on black defendants, leading to public outcry over potential biases in RAIs and machine learning writ large.\nIn response, Equivant published a technical report [20] refuting the claims of bias made by ProPublica and concluded that COMPAS is sufficiently calibrated, in the sense that it satisfies predictive parity at key thresholds. Subsequent analyses [13,16,30] confirmed Equivant\u2019s claims of calibration, but also demonstrated the incompatibility of EFPRs/EFNRs and calibration due to differences in base\ntransformed into decile values by ranking and calibration with a normative group to ensure an equal proportion of scores within each scale value. Lastly, to aid practitioner interpretation, the scores are grouped into three risk categories. The scale values are displayed to court officials as either Low (1-4), Medium (5-7), and High (8-10) risk.\nrates across groups (Y \u22a5A) (see Appendix B). Moreover, the studies suggested that attempting to satisfy these competing forms of fairness force unavoidable trade-offs between criminal justice reformers\u2019 purported goals of racial equity and public safety.\nAs explained in Sect. 2, by requiring the rate of (dis)agreement between Y and Y\u0302 to be the same for black and white defendants (and therefore by not being concerned with dependence of Y on A), EFPRs/EFNRs and calibration are inappropriate fairness criteria if dependence of Y on A includes influence of A on Y through an unfair causal path.\nA M\nF Y\nof a direct path A \u2192 Y (through unobserved neighborhood) in the CBN representing the data-generation mechanism (Fig. 2). Such tactics also imply an influence of A on Y through the set of variables F containing number of prior arrests. In addition, the influence of A on Y through A\u2192 Y and A\u2192 F \u2192 Y could be more prominent or contain more unfairness due to racial discrimination.\nThese observations indicate that EFPRs/EFNRs and calibration are inappropriate criteria for this case (and therefore that their incompatibility is irrelevant), and more generally that the current fairness debate surrounding COMPAS gives insufficient consideration to the patterns of unfairness underlying the training data. Our analysis formalizes the concerns raised by social scientists and legal scholars on mismeasurement and unrepresentative data in the US criminal justice system. Multiple studies [22,34,37,46] have argued that the core premise of RAIs, to assess the likelihood a defendant reoffends, is impossible to measure and that the empirical proxy used (e.g. arrest or conviction) introduces embedded biases and norms which render existing fairness tests unreliable.\nThis section used the CBN framework to describe at a high-level different patterns of unfairness that can underlie a dataset and to point out issues with current deployment of fairness definitions. In the remainder of the manuscript, we use this framework more extensively to further advance our analysis on fairness. Before doing that, we give some background on CBNs [18,39,40,42,45], assuming that all variables except A are continuous."}, {"heading": "3 Causal Bayesian Networks", "text": "A Bayesian network is a directed acyclic graph where nodes and edges represent random variables and statistical dependencies. Each node Xi in the graph is\nassociated with the conditional distribution p(Xi|pa(Xi)), where pa(Xi) is the set of parents of Xi. The joint distribution of all nodes, p(X1, . . . , XI), is given by the product of all conditional distributions, i.e. p(X1, . . . , XI) = \u220fI i=1 p(Xi|pa(Xi)) (see Appendix A for more details on Bayesian networks).\nWhen equipped with causal semantic, namely when representing the datageneration mechanism, Bayesian networks can be used to visually express causal relationships. More specifically, CBNs enable us to give a graphical definition of causes and causal effects: if there exists a directed path from A to Y , then A is a potential cause of Y . Directed paths are also called causal paths.\nG\nC\nA Y\np(C)\np(A|C) p(Y |C,A)\n(a)\nG\u2192A=a\nC\nA Y\np(C)\n\u03b4A=a p(Y |C,A)\n(b)\nonly those with an arrow pointing into A, called back-door paths, which are open if they do not contain a collider.\nAn example of an open back-door path is given by A\u2190 C \u2192 Y in the CBN G of Fig. 3(a): the variable C is said to be a confounder for the effect of A on Y , as it confounds the causal effect with non-causal information. To understand this, assume that A represents hours of exercise in a week, Y cardiac health, and C age: observing cardiac health conditioning on exercise level from p(Y |A) does not enable us to understand the effect of exercise on cardiac health, since p(Y |A) includes the dependence between A and Y induced by age.\nEach parent-child relationship in a CBN represents an autonomous mechanism, and therefore it is conceivable to change one such a relationship without changing the others. This enables us to express the causal effect of A = a on Y as the conditional distribution p\u2192A=a(Y |A = a) on the modified CBN G\u2192A=a of Fig. 3(b), resulting from replacing p(A|C) with a Dirac delta distribution \u03b4A=a (thereby removing the link from C to A) and leaving the remaining conditional distributions p(Y |A,C) and p(C) unaltered \u2014 this process is called intervention on A. The distribution p\u2192A=a(Y |A = a) can be estimated as p\u2192A=a(Y |A = a) = \u222b C p\u2192A=a(Y |A = a,C)p\u2192A=a(C|A = a) = \u222b C p(Y |A = a,C)p(C). This is a special case of the following back-door adjustment formula.\nBack-door Adjustment. If a set of variables C satisfies the back-door criterion relative to {A, Y }, the causal effect of A on Y is given by p\u2192A(Y |A) =\u222b C p(Y |A, C)p(C). C satisfies the back-door criterion if (a) no node in C is a descendant of A and (b) C blocks every back-door path from A to Y .\nThe equality p\u2192A=a(Y |A = a, C) = p(Y |A = a, C) follows from the fact that GA\u2192, obtained by removing from G all links emerging from A, retains all (and only) the back-door paths from A to Y . As C blocks all such paths, Y \u22a5 A|C in GA\u2192. This means that there is no non-causal information traveling from A to Y when conditioning on C and therefore conditioning on A coincides with intervening.\nConditioning on C to block an open back-door path may open a closed path on which C is a collider. For example, in the CBN of Fig. 4(a), conditioning on C closes the paths A \u2190 C \u2190 X \u2192 Y and A \u2190 C \u2192 Y , but opens the path A \u2190 E \u2192 C \u2190 X \u2192 Y (additional conditioning on X would close A\u2190 E \u2192 C \u2190 X \u2192 Y ).\nThe back-door criterion can also be derived from the rules of do-calculus [39,40], which indicate whether and how p\u2192A(Y |A) can be estimated using observations from G: for many graph structures with unobserved confounders the only way to compute causal effects is by collecting obser-\nvations directly from G\u2192A \u2014 in this case the effect is said to be non-identifiable.\nPotential Outcome Viewpoint. Let YA=a be the random variable with distribution p(YA=a) = p\u2192A=a(Y |A = a). YA=a is called potential outcome and, when not ambiguous, we will refer to it with the shorthand Ya. The relation between Ya and all the variables in G other than Y can be expressed by the graph obtained by removing from G all the links emerging from A, and by replacing Y with Ya. If Ya is independent on A in this graph, then4 p(Ya) = p(Ya|A = a) = p(Y |A = a). If Ya is independent of A in this graph when conditioning on C, then\np(Ya) = \u222b C p(Ya|C)p(C) = \u222b C p(Ya|A = a, C)p(C) = \u222b C p(Y |A = a, C)p(C) ,\ni.e. we retrieve the back-door adjustment formula.\nIn the remainder of the section we show that, by performing different interventions on A along different causal paths, it is possible to isolate the contribution of the causal effect of A on Y along a group of paths.\nDirect and Indirect Effect\nConsider the CBN of Fig. 4(b), containing the direct path A \u2192 Y and one indirect causal path through the variable M . Let Ya(Ma\u0304) be the random variable 4 The equality p(Ya|A = a) = p(Y |A = a) is called consistency.\nwith distribution equal to the conditional distribution of Y given A restricted to causal paths, with A = a along A\u2192 Y and A = a\u0304 along A\u2192M \u2192 Y . The average direct effect (ADE) of A = a with respect to A = a\u0304, defined as\nADEa\u0304a = \u3008Ya(Ma\u0304)\u3009p(Ya(Ma\u0304)) \u2212 \u3008Ya\u0304\u3009p(Ya\u0304) , where e.g. \u3008Ya\u0304\u3009p(Ya\u0304) = \u222b Ya\u0304 Ya\u0304p(Ya\u0304), measures the difference in flow of causal information from A to Y between the case in which A = a along A \u2192 Y and A = a\u0304 along A\u2192M \u2192 Y and the case in which A = a\u0304 along both paths.\nAnalogously, the average indirect effect (AIE) of A = a with respect to A = a\u0304, is defined as AIEa\u0304a = \u3008Ya\u0304(Ma)\u3009p(Ya\u0304(Ma)) \u2212 \u3008Ya\u0304\u3009p(Ya\u0304).\nThe difference ADEa\u0304a\u2212AIEaa\u0304 gives the average total effect (ATE) ATEa\u0304a = \u3008Ya\u3009p(Ya) \u2212 \u3008Ya\u0304\u3009p(Ya\u0304)5.\nPath-Specific Effect C\nA M L Y\nC\nA M L Y \u03b8ma \u03b8 l m \u03b8 y l\n\u03b8la \u03b8 y m\n\u03b8ya\n\u03b8 m c\n\u03b8l c\n\u03b8 y c\nFig. 5. Top: CBN with the direct path from A to Y and the indirect paths passing through M highlighted in red. Bottom: CBN corresponding to Eq. (1).\nTo estimate the effect along a specific group of causal paths, we can generalize the formulas for the ADE and AIE by replacing the variable in the first term with the one resulting from performing the intervention A = a along the group of interest and A = a\u0304 along the remaining causal paths. For example, consider the CBN of Fig. 5 (top) and assume that we are interested in isolating the effect of A on Y along the direct path A \u2192 Y and the paths passing through M , A\u2192M \u2192, . . . ,\u2192 Y , namely along the red links. The path-specific effect (PSE) of A = a with respect to A = a\u0304 for this group of paths is defined as\nPSEa\u0304a = \u3008Ya(Ma, La\u0304(Ma))\u3009 \u2212 \u3008Ya\u0304\u3009 ,\nwhere p(Ya(Ma, La\u0304(Ma))) is given by\u222b C,M,L\np(Y |A = a,C,M,L)p(L|A = a\u0304, C,M)p(M |A = a,C)p(C) .\nIn the simple case in which the CBN corresponds to a linear model, e.g.\nA \u223c Bern(\u03c0), C = c , M = \u03b8m + \u03b8ma A+ \u03b8 m c C + m, L = \u03b8 l + \u03b8laA+ \u03b8 l cC + \u03b8 l mM + l , Y = \u03b8y + \u03b8yaA+ \u03b8 y cC + \u03b8 y mM + \u03b8 y l L+ y , (1)\n5 Often the AIE of A = a with respect to A = a\u0304 is defined as AIEaa\u0304a = \u3008Ya\u3009p(Ya) \u2212 \u3008Ya(Ma\u0304)\u3009p(Ya(Ma\u0304)) = \u2212AIEaa\u0304, which differs in setting A to a rather than to a\u0304 along A\u2192 Y . In the linear case, the two definitions coincide (see Eqs. (2) and (3)). Similarly the ADE can be defined as ADEaa\u0304a = \u3008Ya\u3009p(Ya) \u2212 \u3008Ya\u0304(Ma)\u3009p(Ya\u0304(Ma)) = \u2212ADEaa\u0304.\nwhere c, m, l and y are unobserved independent zero-mean Gaussian variables, we can compute \u3008Ya\u0304\u3009 by expressing Y as a function of A = a\u0304 and the Gaussian variables, by recursive substitutions in C,M and L, i.e.\nYa\u0304 = \u03b8 y + \u03b8yaa\u0304+ \u03b8 y c c + \u03b8 y m(\u03b8 m + \u03b8ma a\u0304+ \u03b8 m c c + m)\n+ \u03b8yl (\u03b8 l + \u03b8laa\u0304+ \u03b8 l c c + \u03b8 l m(\u03b8 m + \u03b8ma a\u0304+ \u03b8 m c c + m) + l) + y ,\nand then take the mean, obtaining \u3008Ya\u0304\u3009 = \u03b8y + \u03b8yaa\u0304+ \u03b8ym(\u03b8m + \u03b8ma a\u0304) + \u03b8 y l (\u03b8 l + \u03b8laa\u0304+ \u03b8 l m(\u03b8 m + \u03b8ma a\u0304)). Analogously\n\u3008Ya(Ma, La\u0304(Ma))\u3009 = \u03b8y + \u03b8yaa+ \u03b8ym(\u03b8m + \u03b8ma a) + \u03b8 y l (\u03b8 l + \u03b8laa\u0304+ \u03b8 l m(\u03b8 m + \u03b8ma a)) .\nFor a = 1 and a\u0304 = 0, this gives\nPSEa\u0304a = \u03b8ya(a\u2212 a\u0304) + \u03b8ym\u03b8ma (a\u2212 a\u0304) + \u03b8 y l \u03b8 l m\u03b8 m a (a\u2212 a\u0304) = \u03b8ya + \u03b8ym\u03b8ma + \u03b8 y l \u03b8 l m\u03b8 m a .\nThe same conclusion could have been obtained by looking at the graph annotated with path coefficients (Fig. 5 (bottom)). The PSE is obtained by summing over the three causal paths of interest (A\u2192 Y , A\u2192M \u2192 Y , and A\u2192M \u2192 L\u2192 Y ) the product of all coefficients in each path.\nNotice that AIEa\u0304a, given by\nAIEa\u0304a = \u3008Ya\u0304(Ma, La(Ma))\u3009 \u2212 \u3008Ya\u0304\u3009 = \u03b8y + \u03b8yaa\u0304+ \u03b8 y m(\u03b8 m + \u03b8ma a) + \u03b8 y l (\u03b8 l + \u03b8laa+ \u03b8 l m(\u03b8 m + \u03b8ma a))\n\u2212 \u03b8y + \u03b8yaa\u0304+ \u03b8ym(\u03b8m + \u03b8ma a\u0304) + \u03b8 y l (\u03b8 l + \u03b8laa\u0304+ \u03b8 l m(\u03b8 m + \u03b8ma a\u0304))\n= \u03b8ym\u03b8 m a (a\u2212 a\u0304) + \u03b8 y l (\u03b8 l a(a\u2212 a\u0304) + \u03b8lm\u03b8ma (a\u2212 a\u0304)) , (2)\ncoincides with AIEaa\u0304a, given by\nAIEaa\u0304a = \u3008Ya\u3009 \u2212 \u3008Ya(Ma\u0304, La\u0304(Ma\u0304))\u3009 = \u03b8y + \u03b8yaa+ \u03b8 y m(\u03b8 m + \u03b8ma a) + \u03b8 y l (\u03b8 l + \u03b8laa+ \u03b8 l m(\u03b8 m + \u03b8ma a))\n\u2212 \u03b8y + \u03b8yaa+ \u03b8ym(\u03b8m + \u03b8ma a\u0304) + \u03b8 y l (\u03b8 l + \u03b8laa\u0304+ \u03b8 l m(\u03b8 m + \u03b8ma a\u0304)) . (3)\nEffect of Treatment on Treated. Consider the conditional distribution p(Ya|A = a\u0304). This distribution measures the information travelling from A to Y along all open paths, when A is set to a along causal paths and to a\u0304 along non-causal paths. The effect of treatment on treated (ETT) of A = a with respect to A = a\u0304 is defined as ETTa\u0304a = \u3008Ya\u3009p(Ya|A=a\u0304) \u2212 \u3008Ya\u0304\u3009p(Ya\u0304|A=a\u0304) = \u3008Ya\u3009p(Ya|A=a\u0304) \u2212 \u3008Y \u3009p(Y |A=a\u0304). As the PSE, the ETT measures difference in flow of information from A to Y when A takes different values along different paths. However, the PSE considers only causal paths and different values for A along different causal paths, whilst the ETT considers all open paths and different values for A along causal and non-causal paths respectively. Similarly to ATEa\u0304a, ETTa\u0304a for the CBN of Fig. 4(b) can be expressed as\nETTa\u0304a = \u3008Ya(Ma\u0304)\u3009 \u2212 \u3008Ya\u0304\u3009\ufe38 \ufe37\ufe37 \ufe38 ADEa\u0304a|a\u0304 \u2212(\u3008Ya(Ma\u0304)\u3009 \u2212 \u3008Ya\u3009\ufe38 \ufe37\ufe37 \ufe38 AIEaa\u0304|a\u0304 ) .\nNotice that, if we define difference in flow of non-causal (along the open back-door paths) information from A to Y when A = a with respect to when A = a\u0304 as NCIa\u0304a = \u3008Ya\u0304\u3009p(Ya\u0304|A=a) \u2212 \u3008Y \u3009p(Y |A=a\u0304), we obtain\n\u3008Y \u3009p(Y |A=a) \u2212 \u3008Y \u3009p(Y |A=a\u0304) = \u3008Ya\u0304\u3009p(Ya\u0304|A=a) \u2212 \u3008Y \u3009p(Y |A=a\u0304) \u2212 (\u3008Ya\u0304\u3009p(Ya\u0304|A=a) \u2212 \u3008Y \u3009p(Y |A=a)) = NCIa\u0304a \u2212 ETTaa\u0304 = NCIa\u0304a \u2212ADEaa\u0304|a + AIEa\u0304a|a ."}, {"heading": "4 Fairness Considerations using CBNs", "text": "Equipped with the background on CBNs from Sect. 3, in this section we further investigate unfairness in a dataset \u2206 = {an, xn, yn}Nn=1, discuss issues that might arise when building a decision system from it, and show how to measure and deal with unfairness in complex scenarios, revisiting and extending material from [11,33,48]."}, {"heading": "4.1 Back-door Paths from A to Y", "text": "In Sect. 2 we have introduced a graphical interpretation of unfairness in a dataset \u2206 as the presence of an unfair causal path from A to X or Y . More specifically, we have shown through a college admission example that unfairness can be due to an unfair link emerging (a) from A or (b) from a subsequent variable in a causal path from A to Y (e.g. D \u2192 Y in the example). Our discussion did not mention paths from A to Y with an arrow pointing into A, namely back-door paths. This is because such paths are not problematic.\nA E\nY\nTo understand this, consider the hiring scenario described by the CBN on the left, where A represents religious belief and E educational background of the applicant, which influences religious participation (E \u2192 A). Whilst Y \u22a5A due to the open back-door path from A to Y , the hiring decision Y is\nonly based on E."}, {"heading": "4.2 Opening Closed Unfair Paths from A to Y", "text": "In Sect. 2, we have seen that, in order to reason about fairness of Y\u0302 , it is necessary to question and understand unfairness in \u2206. In this section, we warn that another crucial element needs to be considered in the fairness discussion around Y\u0302 , namely\n(i) The variables used to form Y\u0302 could project into Y\u0302 unfair patterns in X that do not concern Y .\nThis could happen, for example, if a closed unfair path from A to Y is opened when conditioning on the variables used to form Y\u0302 .\nA M\nX Y\n\u03b1 \u03b2 \u03b3\ninitial testing, women are assigned a lower initial score than men for the same aptitude level (A \u2192 X). The only path from A to Y , A \u2192 X \u2190 M \u2192 Y , is closed as X is a collider on this path. Therefore the unfair influence of A on X does not reach Y (Y \u22a5 A). Nevertheless, as Y \u22a5A|X, a prediction Y\u0302 based on the initial score X only would contain the unfair influence of A on X. For example, assume the following linear model: Y = \u03b3M, X = \u03b1A + \u03b2M , with \u3008A2\u3009p(A) = 1 and \u3008M2\u3009p(M) = 1. A linear predictor of the form Y\u0302 = \u03b8XX minimizing \u3008(Y \u2212 Y\u0302 )2\u3009p(A)p(M) would have parameters \u03b8X = \u03b3\u03b2/(\u03b12 + \u03b22), giving Y\u0302 = \u03b3\u03b2(\u03b1A+\u03b2M)/(\u03b12 +\u03b22), i.e. Y\u0302 \u22a5A. Therefore, this predictor would be using the sensitive attribute to form a decision, although implicitly rather than explicitly. Instead, a predictor explicitly using the sensitive attribute, Y\u0302 = \u03b8XX + \u03b8AA, would have parameters(\n\u03b8X \u03b8A\n) = ( \u03b12 + \u03b22 \u03b1\n\u03b1 1 )\u22121( \u03b3\u03b2 0 ) = ( \u03b3/\u03b2 \u2212\u03b1\u03b3/\u03b2 ) ,\ni.e. Y\u0302 = \u03b3M . Therefore, this predictor would be fair. From the CBN we can see that the explicit use of A can be of help in retrieving M . Indeed, since M \u22a5A|X, using A in addition to X can give information about M . In general (e.g. in a non-linear setting) it is not guaranteed that using A would ensure Y\u0302 \u22a5 A. Nevertheless, this example shows how explicit use of the sensitive attribute in a model can ensure fairness rather than leading to unfairness.\nThis observation is relevant to one of the simplest fairness definitions, motivated by legal requirements, called fairness through unawareness, which states that Y\u0302 is fair as long as it does not make explicit use of the sensitive attribute A. Whilst this fairness criterion is often indicated as problematic because some of the variables used to form Y\u0302 could be a proxy for A (such as neighborhood for race), the example above shows a more subtle issue with it."}, {"heading": "4.3 Path-Specific Population-level Unfairness", "text": "In this section, we show that the path-specific effect introduced in Sect. 3 can be used to quantify unfairness in \u2206 in complex scenarios.\nConsider the college admission example discussed in Sect. 2 (Fig. 7). In the case in which the path A \u2192 D, and therefore A \u2192 D \u2192 Y , is considered unfair, unfairness overall population can be quantified with \u3008Y \u3009p(Y |a) \u2212 \u3008Y \u3009p(Y |a\u0304)\nNotice that computing p(Ya(Da\u0304)) requires knowledge of the CBN. If the CBN structure is not known or\nestimating its conditional distributions is challenging, the resulting estimate could be imprecise."}, {"heading": "4.4 Path-Specific Individual-level Unfairness", "text": "In the college admission example of Fig. 7 in which the path A \u2192 D \u2192 Y is considered fair, rather than measuring unfairness overall population, we might want to know e.g. whether a rejected female applicant {an = a, qn, dn, yn = 0} was treated unfairly. We can answer this question by estimating whether the applicant would have been admitted had she been male (A = a\u0304) along the direct path A \u2192 Y from p(Ya\u0304(Da)|A = a,Q = qn, D = dn, Y = yn) (notice that the outcome in the actual world, yn, corresponds to p(Ya(Da)|A = a,Q = qn, D = dn) = 1Ya(Da)=yn).\nTo understand how this can be achieved, consider the following linear model associated to a CBN with the same structure as the one in Fig. 7\nA \u223c Bern(\u03c0), Q = \u03b8q + q, D = \u03b8d + \u03b8daA+ d , Y = \u03b8y + \u03b8yaA+ \u03b8 y qQ+ \u03b8 y dD + y , (4)\nwhere q, d and y are unobserved independent zero-mean Gaussian variables.\nd\ny\nq\nA\nD\nY\nQ\nDa\nYa\u0304(Da)\nQ\u2217\nThe relationships between A,Q,D, Y and Ya\u0304(Da) in this model can be inferred from the twin Bayesian network [39] on the left resulting from the intervention A = a along A \u2192 D and A = a\u0304 along A \u2192 Y : in addition to A,Q,D and Y , the network contains the variables Q\u2217, Da and Ya\u0304(Da) corresponding to the counterfactual world in which A = a\u0304 along A \u2192 Y , with Q\u2217 = \u03b8q + q, Da = \u03b8d + \u03b8daa + d, and Ya\u0304(Da) = \u03b8 y + \u03b8yaa\u0304 + \u03b8 y qQ \u2217 + \u03b8ydDa + y. The two groups of variables are connected through d, q, y, indicating that the factual and counterfactual worlds\nshare the same unobserved randomness. From this network, we can deduce that Ya\u0304(Da) \u22a5 {A,Q,D, Y }| = { q, d, y}6, and therefore that we can express 6 Notice that Ya\u0304(Da) \u22a5 A, but Ya\u0304(Da) \u22a5A|D.\np(Ya\u0304(Da)|A = a,Q = qn, D = dn, Y = yn) as\np(Ya\u0304(Da)|a, qn, dn, yn) = \u222b p(Ya\u0304(Da)| , a, q n, d n, y n)p( |a, qn, dn, yn) . (5)\nAs p( nq |a, qn, dn, yn) = \u03b4 nq =qn\u2212\u03b8q , p( n d |a, qn, dn, yn) = \u03b4 nd =dn\u2212\u03b8d\u2212\u03b8daa, and p( ny |a, qn, dn, yn) = \u03b4 ny =yn\u2212\u03b8y\u2212\u03b8yaa\u2212\u03b8yq qn\u2212\u03b8yddn , we obtain\np(Ya\u0304(Da)|A = a,Q = qn, D = dn, Y = yn) = 1Ya\u0304(Da)=yn\u2212\u03b8yaa+\u03b8ya a\u0304.\nIndeed, by expressing Ya\u0304(Da) as a function of nq , nd and n y , we obtain\nYa\u0304(Da) = \u03b8 y + \u03b8yaa\u0304+ \u03b8 y qQ \u2217 + \u03b8ydDa + n y\n= \u03b8y + \u03b8yaa\u0304+ \u03b8 y q (\u03b8 q + nq ) + \u03b8 y d(\u03b8 d + \u03b8daa+ n d ) + n y = y n \u2212 \u03b8yaa+ \u03b8yaa\u0304 .\nTherefore, as Q is not a descendant of A and D is a descendant of A along a fair path, the outcome in the counterfactual world is obtained by correcting the outcome in the factual world through replacing \u03b8yaa with \u03b8yaa\u0304.\nSuppose that we want to post-process a learned model (4) to give a prediction y\u0302n for individual {an = a, qn, dn} based on the counterfactual distribution p(Ya\u0304(Da)|A = a,Q = qn, D = dn), e.g. y\u0302n = \u3008Ya\u0304(Da)\u3009p(Ya\u0304(Da)|A=a,Q=qn,D=dn) \u2014 the resulting model is said to satisfy path-specific counterfactual fairness [11]. By performing a similar reasoning to the one above for p(Ya\u0304(Da)|A = a,Q = qn, D = dn), we obtain7\np(Ya\u0304(Da)|A = a,Q = qn, D = dn) = p(Ya\u0304(Da)|Q\u2217 = qn, Da = dn) = p(Y |A = a\u0304, Q = qn, D = dn) ,\ni.e. the counterfactual prediction can be estimated by conditioning Y on qn and dn, and by replacing a with a\u0304 in the direct path A\u2192 Y .\nIn the case in which A\u2192 D is also unfair, we have\np(Ya\u0304(Da\u0304)|A = a,Q = qn, D = dn) = p(Ya\u0304(Da\u0304)|Q\u2217 = qn, Da\u0304 = dn \u2212 \u03b8daa+ \u03b8daa\u0304) = p(Y |A = a\u0304, Q = qn, D = dn \u2212 \u03b8daa+ \u03b8daa\u0304) ,\ni.e. the counterfactual prediction can be estimated by conditioning Y on qn and the corrected version of dn, given by dn \u2212 \u03b8daa+ \u03b8daa\u0304. In a more complex scenario in which e.g. D = f(A, d) for a non-linear function f we can sample n,m d from p( d|a, qn, dn) and perform a Monte-Carlo approximation of Eq. (5), obtaining\np(Ya\u0304(Da\u0304)|A = a,Q = qn, D = dn) = 1\nM M\u2211 m=1 p(Y |A = a\u0304, Q = qn, D = dn,m) ,\n7 Notice that \u3008Ya\u0304(Da)\u3009p(Ya\u0304(Da)|A=a,Q=qn,D=dn) = \u3008Y \u3009p(Y |A=a,Q=qn,D=dn) \u2212 PSE a a\u0304a.\nIndeed \u3008Y \u3009p(Y |A=a,Q=qn,D=dn) = \u03b8y + \u03b8ya + \u03b8yq qn + \u03b8ydd n and PSEa\u0304a = \u03b8ya. This equivalence does not hold in the non-linear setting.\nwhere dn,m = f(A = a\u0304, n,md ) can be interpreted as a corrected version of d n.\nFrom the discussion above we can deduce that the general procedure for computing the desired counterfactual outcome is to condition Y on the nondescendants of A, on the descendants of A that are only fairly influenced by A, and on corrected versions of the descendants that are (partially) unfairly influenced by A."}, {"heading": "5 Conclusions", "text": "We used causal Bayesian networks to provide a graphical interpretation of unfairness in a dataset as the presence of an unfair causal path. We used this viewpoint to revisit the recent debate surrounding the COMPAS pretrial risk assessment tool and, more generally, to point out that fairness evaluation on a model requires careful considerations on the patterns of unfairness underlying the training data. We then showed that causal Bayesian networks provide us with a powerful tool to measure unfairness in a dataset and to design fair models in complex unfairness scenarios.\nOur discussion did not cover difficulties in making reasonable assumptions on the structure of the causal Bayesian network underlying a dataset, nor on the estimations of the associated conditional distributions or of other quantities of interest. These are obstacles that need to be carefully considered to avoid improper usage of this framework."}, {"heading": "Acknowledgements", "text": "The authors would like to thank Ray Jiang, Christina Heinze-Deml, Tom Stepleton, Tom Everitt, and Shira Mitchell for useful discussions."}, {"heading": "Appendix A Bayesian Networks", "text": "A graph is a collection of nodes and links connecting pairs of nodes. The links may be directed or undirected, giving rise to directed or undirected graphs respectively.\nA path from node Xi to node Xj is a sequence of linked nodes starting at Xi and ending at Xj . A directed path is a path whose links are directed and pointing from preceding towards following nodes in the sequence.\nX2X1 X3\nX4\n(a)\nX2X1 X3\nX4\n(b)\nA node is a collider on a path if it has (at least) two parents on that path. Notice that a node can be a collider on a path and a non-collider on another path. For example, in Fig. 8(a) X3 is a collider on the path X1 \u2192 X3 \u2190 X2 and a non-collider on the path X2 \u2192 X3 \u2192 X4. A node Xi is an ancestor of a node Xj if there exists a directed path from Xi to Xj . In this case, Xj is a descendant of Xi.\nA Bayesian network is a DAG in which nodes represent random variables and links express statistical relationships between the variables. Each node Xi in the graph is associated with the conditional distribution p(Xi|pa(Xi)), where pa(Xi) is the set of parents of Xi. The joint distribution of all nodes, p(X1, . . . , XI), is given by the product of all conditional distributions, i.e. p(X1, . . . , XI) =\u220fI i=1 p(Xi|pa(Xi)). In a Bayesian network, the sets of variables X and Y are statistically independent given Z (X \u22a5 Y |Z) if all paths from any element of X to any element of Y are closed (or blocked). A path is closed if at least one of the following conditions is satisfied:\n(a) There is a non-collider on the path which belongs to the conditioning set Z. (b) There is a collider on the path such that neither the collider nor any of its\ndescendants belong to the conditioning set Z."}, {"heading": "Appendix B EFPRs/EFNRs and Calibration", "text": "Assume that EFPRs/EFNRs are satisfied, i.e. p(Y\u0302 = 1|A = 0, Y = 1) = p(Y\u0302 = 1|A = 1, Y = 1) \u2261 pY\u03021|Y1 and p(Y\u0302 = 1|A = 0, Y = 0) = p(Y\u0302 = 1|A = 1, Y = 0) \u2261 pY\u03021|Y0 . From\np(Y = 1|A = 0, Y\u0302 = 1) = pY\u03021|Y1\npY1|A0\ufe37 \ufe38\ufe38 \ufe37 p(Y = 1|A = 0)\npY\u03021|Y1pY1|A0 + pY\u03021|Y0(1\u2212 pY1|A0) ,\np(Y = 1|A = 1, Y\u0302 = 1) = pY\u03021|Y1pY1|A1\npY\u03021|Y1pY1|A1 + pY\u03021|Y0(1\u2212 pY1|A1) ,\nwe see that, to also satisfy p(Y = 1|A = 0, Y\u0302 = 1) = p(Y = 1|A = 1, Y\u0302 = 1), we need ( (((( ((pY\u03021|Y1pY1|A1 + pY\u03021|Y0(1 \u2212 pY1|A1) ) pY1|A0 = ( (((( ((pY\u03021|Y1pY1|A0 + pY\u03021|Y0(1 \u2212\npY1|A0) ) pY1|A1 , i.e. pY1|A0 = pY1|A1 ."}], "title": "A Causal Bayesian Networks Viewpoint on Fairness", "year": 2019}