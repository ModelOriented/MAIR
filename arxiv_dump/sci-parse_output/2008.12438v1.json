{
  "abstractText": "Sparse PCA (SPCA) is a fundamental model in machine learning and data analytics, which has witnessed a variety of application areas such as finance, manufacturing, biology, healthcare. To select a prespecified-size principal submatrix from a covariance matrix to maximize its largest eigenvalue for the better interpretability purpose, SPCA advances the conventional PCA with both feature selection and dimensionality reduction. Existing approaches often approximate SPCA as a semi-definite program (SDP) without strictly enforcing the important cardinality constraint that restricts the number of selected features to be a constant. To fill this gap, we propose two exact mixed-integer SDPs (MISDPs) by exploiting the spectral decomposition of the covariance matrix and the properties of the largest eigenvalues. We then analyze the theoretical optimality gaps of their continuous relaxation values and prove that they are stronger than that of the state-of-art one. We further show that the continuous relaxations of two MISDPs can be recast as saddle point problems without involving semi-definite cones, and thus can be effectively solved by first-order methods such as the subgradient method. Since off-the-shelf solvers, in general, have difficulty in solving MISDPs, we approximate SPCA with arbitrary accuracy by a mixed-integer linear program (MILP) of a similar size as MISDPs. The continuous relaxation values of two MISDPs can be leveraged to reduce the size of the proposed MILP further. To be more scalable, we also analyze greedy and local search algorithms, prove their first-known approximation ratios, and show that the approximation ratios are tight. Our numerical study demonstrates that the continuous relaxation values of the proposed MISDPs are quite close to optimality, the proposed MILP model can solve small and medium-size instances to optimality, and the approximation algorithms work very well for all the instances. Finally, we extend the analyses to Rank-one Sparse SVD (R1-SSVD) with non-symmetric matrices and Sparse Fair PCA (SFPCA) when there are multiple covariance matrices, each corresponding to a protected group.",
  "authors": [
    {
      "affiliations": [],
      "name": "Yongchun Li"
    },
    {
      "affiliations": [],
      "name": "Weijun Xie"
    }
  ],
  "id": "SP:569adc948b23c45ce988d4c3f2690fe5479bb828",
  "references": [
    {
      "authors": [
        "AA Amini",
        "MJ Wainwright"
      ],
      "title": "High-dimensional analysis of semidefinite relaxations for sparse principal components",
      "venue": "IEEE International Symposium on Information Theory,",
      "year": 2008
    },
    {
      "authors": [
        "E Balas"
      ],
      "title": "Disjunctive programming: cutting planes from logical conditions",
      "venue": "Nonlinear Programming",
      "year": 1975
    },
    {
      "authors": [
        "A Ben-Tal",
        "A Nemirovski"
      ],
      "title": "Lectures on modern convex optimization: analysis, algorithms, and engineering applications, volume 2 (Siam)",
      "year": 2001
    },
    {
      "authors": [
        "JF Benders"
      ],
      "title": "Partitioning procedures for solving mixed-variables programming problems",
      "venue": "Numer. Math. 4(1):238252,",
      "year": 1962
    },
    {
      "authors": [
        "L Berk",
        "D Bertsimas"
      ],
      "title": "Certifiably optimal sparse principal component analysis",
      "venue": "Mathematical Programming Computation",
      "year": 2019
    },
    {
      "authors": [
        "WR Breakey",
        "H Goodell",
        "PC Lorenz",
        "PR McHugh"
      ],
      "title": "Hallucinogenic drugs as precipitants of schizophrenia",
      "year": 1974
    },
    {
      "authors": [
        "E Carrizosa",
        "V Guerrero"
      ],
      "title": "rs-sparse principal component analysis: A mixed integer nonlinear programming approach with vns. Computers & operations research 52:349\u2013354",
      "year": 2014
    },
    {
      "authors": [
        "S Chaib",
        "Y Gu",
        "H Yao"
      ],
      "title": "An informative feature selection method based on sparse pca for vhr scene classification",
      "venue": "IEEE Geoscience and Remote Sensing Letters 13(2):147\u2013151",
      "year": 2015
    },
    {
      "authors": [
        "SO Chan",
        "D Papailliopoulos",
        "Rubinstein"
      ],
      "title": "A (2016) On the approximability of sparse pca",
      "venue": "Conference on Learning",
      "year": 2016
    },
    {
      "authors": [
        "I Coope"
      ],
      "title": "On matrix trace inequalities and related topics for products of hermitian matrices. Journal of mathematical analysis and applications 188(3):999\u20131001",
      "year": 1994
    },
    {
      "authors": [
        "LN Coughlin",
        "AN Tegge",
        "CE Sheffer",
        "WK Bickel"
      ],
      "title": "A machine-learning approach to predicting smoking cessation treatment outcomes. Nicotine and Tobacco Research 22(3):415\u2013422",
      "year": 2020
    },
    {
      "authors": [
        "A d\u2019Aspremont",
        "F Bach",
        "LE Ghaoui"
      ],
      "title": "Approximation bounds for sparse principal component analysis",
      "year": 2012
    },
    {
      "authors": [
        "A d\u2019Aspremont",
        "LE Ghaoui",
        "MI Jordan",
        "GR Lanckriet"
      ],
      "title": "A direct formulation for sparse pca using semidefinite programming",
      "venue": "Advances in neural information processing systems,",
      "year": 2005
    },
    {
      "authors": [
        "MS De Barona",
        "DD Simpson"
      ],
      "title": "Inhalant users in drug abuse prevention programs. The American journal of drug and alcohol abuse 10(4):503\u2013518",
      "year": 1984
    },
    {
      "authors": [
        "SS Dey",
        "R Mazumder",
        "G Wang"
      ],
      "title": "A convex integer programming approach for optimal sparse pca",
      "year": 2018
    },
    {
      "authors": [
        "A dAspremont",
        "F Bach",
        "LE Ghaoui"
      ],
      "title": "Optimal solutions for sparse principal component analysis",
      "venue": "Journal of Machine Learning Research 9(Jul):1269\u20131294. Yongchun Li and Weijun Xie: Exact and Approximation Algorithms for Sparse PCA",
      "year": 2008
    },
    {
      "authors": [
        "T Gally",
        "ME Pfetsch"
      ],
      "title": "Computing restricted isometry constants via mixed-integer semidefinite programming",
      "year": 2016
    },
    {
      "authors": [
        "AM Geoffrion"
      ],
      "title": "Generalized benders decomposition. Journal of optimization theory and applications 10(4):237\u2013260",
      "year": 1972
    },
    {
      "authors": [
        "Y He",
        "RD Monteiro",
        "H Park"
      ],
      "title": "An algorithm for sparse pca based on a new sparsity control criterion",
      "venue": "Proceedings of the 2011 SIAM International Conference on Data Mining,",
      "year": 2011
    },
    {
      "authors": [
        "J Jeffers"
      ],
      "title": "Two case studies in the application of principal component analysis",
      "venue": "Journal of the Royal Statistical Society: Series C (Applied",
      "year": 1967
    },
    {
      "authors": [
        "R Jiang",
        "H Fei",
        "J Huan"
      ],
      "title": "A family of joint sparse pca algorithms for anomaly localization in network data streams",
      "venue": "IEEE Transactions on Knowledge and Data Engineering",
      "year": 2012
    },
    {
      "authors": [
        "M Journ\u00e9e",
        "Y Nesterov",
        "P Richt\u00e1rik",
        "R Sepulchre"
      ],
      "title": "Generalized power method for sparse principal component analysis",
      "venue": "Journal of Machine Learning Research",
      "year": 2010
    },
    {
      "authors": [
        "M Lee",
        "H Shen",
        "JZ Huang",
        "J Marron"
      ],
      "title": "Biclustering via sparse singular value decomposition. Biometrics 66(4):1087\u20131095",
      "year": 2010
    },
    {
      "authors": [
        "Y Li",
        "W Xie"
      ],
      "title": "2020) Best principal submatrix selection for the maximum entropy sampling problem: Scalable algorithms and performance guarantees",
      "year": 2001
    },
    {
      "authors": [
        "R Luss"
      ],
      "title": "dAspremont A (2010) Clustering and feature selection using sparse principal component analysis. Optimization and Engineering 11(1):145\u2013157",
      "year": 2010
    },
    {
      "authors": [
        "V Madan",
        "M Singh",
        "U Tantipongpipat",
        "W Xie"
      ],
      "title": "Combinatorial algorithms for optimal design",
      "venue": "Conference on Learning",
      "year": 2019
    },
    {
      "authors": [
        "M Magdon-Ismail"
      ],
      "title": "Np-hardness and inapproximability of sparse pca. Information Processing Letters 126:35\u201338",
      "year": 2017
    },
    {
      "authors": [
        "W Min",
        "J Liu",
        "S Zhang"
      ],
      "title": "2016) L0-norm sparse graph-regularized svd for biclustering",
      "year": 2016
    },
    {
      "authors": [
        "B Moghaddam",
        "Y Weiss",
        "S Avidan"
      ],
      "title": "Spectral bounds for sparse pca: Exact and greedy algorithms. Advances in neural information processing systems, 915\u2013922",
      "year": 2006
    },
    {
      "authors": [
        "N Naikal",
        "AY Yang",
        "SS Sastry"
      ],
      "title": "Informative feature selection for object recognition via sparse pca",
      "venue": "International Conference on Computer Vision,",
      "year": 2011
    },
    {
      "authors": [
        "A Nedi\u0107",
        "A Ozdaglar"
      ],
      "title": "Subgradient methods for saddle-point problems. Journal of optimization theory and applications 142(1):205\u2013228",
      "year": 2009
    },
    {
      "authors": [
        "DC Ompad",
        "RM Ikeda",
        "N Shah",
        "CM Fuller",
        "S Bailey",
        "E Morse",
        "P Kerndt",
        "C Maslow",
        "Y Wu",
        "D Vlahov"
      ],
      "title": "Childhood sexual abuse and age at initiation of injection drug use. American journal of public health 95(4):703\u2013709",
      "year": 2005
    },
    {
      "authors": [
        "S Samadi",
        "U Tantipongpipat",
        "JH Morgenstern",
        "M Singh",
        "S Vempala"
      ],
      "title": "The price of fair pca: One extra dimension",
      "venue": "Advances in Neural Information Processing Systems,",
      "year": 2018
    },
    {
      "authors": [
        "A Semlyen",
        "G Angelidis"
      ],
      "title": "Efficient calculation of critical eigenvalue clusters in the small signal stability analysis of large power systems",
      "year": 1995
    },
    {
      "authors": [
        "M Sill",
        "S Kaiser",
        "A Benner",
        "A Kopp-Schneider"
      ],
      "title": "Robust biclustering by sparse singular value decomposition incorporating stability selection",
      "year": 2011
    },
    {
      "authors": [
        "U Tantipongpipat",
        "S Samadi",
        "M Singh",
        "JH Morgenstern",
        "S Vempala"
      ],
      "title": "Multi-criteria dimensionality reduction with applications to fairness",
      "venue": "Advances in Neural Information Processing Systems,",
      "year": 2019
    },
    {
      "authors": [
        "DL Thomas",
        "D Vlahov",
        "L Solomon",
        "S Cohn",
        "E Taylor",
        "R Garfein",
        "KE Nelson"
      ],
      "title": "Correlates of hepatitis c virus infections among injection drug",
      "year": 1995
    },
    {
      "authors": [
        "ND Volkow",
        "JS Fowler",
        "GJ Wang",
        "JM Swanson",
        "F Telang"
      ],
      "title": "Dopamine in drug abuse and addiction: results of imaging studies and treatment implications",
      "venue": "Archives of neurology 64(11):1575\u20131579",
      "year": 2007
    },
    {
      "authors": [
        "Y Zhang",
        "A dAspremont",
        "L El Ghaoui"
      ],
      "title": "Sparse pca: Convex relaxations, algorithms and applications",
      "venue": "Handbook on Semidefinite, Conic and Polynomial Optimization,",
      "year": 2012
    }
  ],
  "sections": [
    {
      "text": "Exact and Approximation Algorithms for Sparse PCA\nYongchun Li Department of Industrial & Systems Engineering, Virginia Tech, Blacksburg, VA 24061, liyc@vt.edu Weijun Xie Department of Industrial & Systems Engineering, Virginia Tech, Blacksburg, VA 24061, wxie@vt.edu\nSparse PCA (SPCA) is a fundamental model in machine learning and data analytics, which has witnessed a variety of application areas such as finance, manufacturing, biology, healthcare. To select a prespecified-size principal submatrix from a covariance matrix to maximize its largest eigenvalue for the better interpretability purpose, SPCA advances the conventional PCA with both feature selection and dimensionality reduction. Existing approaches often approximate SPCA as a semi-definite program (SDP) without strictly enforcing the important cardinality constraint that restricts the number of selected features to be a constant. To fill this gap, we propose two exact mixed-integer SDPs (MISDPs) by exploiting the spectral decomposition of the covariance matrix and the properties of the largest eigenvalues. We then analyze the theoretical optimality gaps of their continuous relaxation values and prove that they are stronger than that of the state-of-art one. We further show that the continuous relaxations of two MISDPs can be recast as saddle point problems without involving semi-definite cones, and thus can be effectively solved by first-order methods such as the subgradient method. Since off-the-shelf solvers, in general, have difficulty in solving MISDPs, we approximate SPCA with arbitrary accuracy by a mixed-integer linear program (MILP) of a similar size as MISDPs. The continuous relaxation values of two MISDPs can be leveraged to reduce the size of the proposed MILP further. To be more scalable, we also analyze greedy and local search algorithms, prove their first-known approximation ratios, and show that the approximation ratios are tight. Our numerical study demonstrates that the continuous relaxation values of the proposed MISDPs are quite close to optimality, the proposed MILP model can solve small and medium-size instances to optimality, and the approximation algorithms work very well for all the instances. Finally, we extend the analyses to Rank-one Sparse SVD (R1-SSVD) with non-symmetric matrices and Sparse Fair PCA (SFPCA) when there are multiple covariance matrices, each corresponding to a protected group.\nKey words : Sparse PCA, Largest Eigenvalue, Mixed-Integer Program, Semi-definite Program, Greedy,\nLocal Search, SVD, Fairness\n1. Introduction This paper studies the sparse principal component analysis (SPCA) problem\nof the form\n(SPCA) w\u2217 := max x\u2208Rn\n{ x>Ax : ||x||2 = 1, ||x||0 = k } , (1)\n1\nar X\niv :2\n00 8.\n12 43\n8v 1\n[ st\nat .M\nL ]\n2 8\nA ug\n2 02\n0\nwhere the symmetric positive semi-definite matrixA\u2208Rn\u00d7n denotes the sample covariance out of a dataset with n features and the integer k \u2208 [n] denotes the sparsity of its first principal component\n(PC). In SPCA (1), the objective is to select the best size-k principal submatrix from a covariance\nmatrix A with the maximum largest eigenvalue. Compared to the conventional PCA, the extra zero-norm constraint ||x||0 = k in SPCA (1) restricts the number of features of the first PC x to be k most important ones. In this way, SPCA improves the interpretability of the obtained PC,\nwhich has been shown as early as Jeffers [20] in 1967. It is also recognized that SPCA can be\nmore reliable for large-scale datasets than PCA, where the number of features is far more than\nthat of observations [41]. These advantages of SPCA have benefited many application fields such\nas biology, finance, cloud computing, and healthcare, which frequently deal with datasets with a\nmassive number of features (see, e.g., [8, 21, 25, 30]).\n1.1. Relevant Literature Our paper contributes to relevant literature on SPCA from three\naspects: exact mixed-integer programs, convex relaxations, and approximation algorithms.\nExact Mixed-Integer Programs: As shown in formulation (1), SPCA is highly non-convex-\nmaximizing a convex function subject to two nonconvex constraints (i.e., an L2 equality constraint and an L0 equality constraint). Albeit superior to traditional PCA, SPCA (1) is notoriously known to be computationally expensive; see, e.g., the complexity analysis and inapproximability results\nin Magdon-Ismail [27]. As a result, the equivalent formulations and algorithms for exactly solving\nSPCA are quite limited in the literature (see, e.g., [5, 17, 29]). Moghaddam et al. [29] introduced a\nbranch and bound method to solve SPCA, and they pruned redundant nodes using the eigenvalue of\nprincipal submatrices and a greedy algorithm. Recently, Berk and Bertsimas [5] embedded various\nupper and lower bounds into this branch and bound framework, which could efficiently prune nodes\nand quickly certificate the optimality for quite a few instances. It is worthy of mentioning that\nGally and Pfetsch [17] proposed a MISDP (MISDP) formulation for SPCA. Our second MISDP\nformulation differs from Gally and Pfetsch [17] by deriving two strong conic valid inequalities.\nAnother interesting work can be found in Dey et al. [15], where the authors developed approximate convex integer programs for SPCA with an optimality gap of (1 + \u221a k/(k+ 1))2. Quite differently,\nwe propose two exact MISDP formulations and one approximate mixed-integer linear program\n(MILP) for SPCA from novel perspectives of analyzing the largest eigenvalue. Specifically, the\nproposed MILP formulation can be arbitrarily close to the optimal value of SPCA, and it can be\ndirectly solved by off-the-shelf solvers such as Gurobi.\nConvex Relaxations: Besides solving exact SPCA, researchers have also actively sought to\nexplore effective convex relaxations. A common approach in literature is to develop SDP relaxations\nfor SPCA (see e.g., [1, 13, 16, 12, 40]). Albeit convex, solvers often have difficulty in solving large-\nscale instances of SDP formulations (e.g., n= \u2126(100)). The computational challenge of these SDP\nproblems urgently calls for more effective methods to compute the relaxation values for SPCA. From\na different angle, this paper solves the continuous relaxations of the proposed MISDP formulations\nas the maximin saddle point problem, where the subgradient method enjoys a O(1/T ) rate of\nconvergence [31] based on Euclidean projections. Surprisingly, we further show that the projection\noracle of the subgradient method is a second-order conic program rather than an SDP and thus\ncan be easily dealt with.\nApproximation Algorithm: Another early thread of research on SPCA is the development of\nhigh-quality heuristics for solving SPCA to near optimality such as greedy algorithm [16, 19],\ntruncation algorithm [9], power method [22], and variable neighborhood search method [7]. In\nparticular, the truncation algorithm in [9] so far provides the best-known approximation ratio O(n\u22121/3), which can be easily implemented to generate a feasible solution for SPCA. This paper\ninvestigates the greedy and local search algorithms and proves their first-known approximation\nratios O(1/k) for SPCA.\n1.2. Summary of Contributions We observe that when the support of x has been suc-\ncessfully identified, SPCA (1) reduces to the conventional PCA finding the largest eigenvalue and\neigenvector of a size-k principal submatrix of A. This fact motivates us to derive two equiva-\nlent MISDP formulations and an approximate MILP of SPCA. Below is a summary of the main\ncontributions in this paper.\n(i) For each formulation, we derive the theoretical optimality gap between its continuous relax-\nation value and the optimal value of SPCA.\n(ii) Our first MISDP formulation inspires us to derive closed-form expressions of the coefficients\nof valid inequalities, which can be efficiently embedded into the branch and cut algorithms;\n(iii) We show that the subgradient method can be adapted to ease the computational burden\nof obtaining MISDP continuous relaxation values with O(1/T ) rate of convergence. These\ncontinuous relaxations values can further help reduce the size of MILP;\n(iv) The continuous relaxation of our second MISDP formulation is proven to be stronger than\nthe one proposed in d\u2019Aspremont et al. [13];\n(v) The proposed MILP formulation has a similar size as two MISDPs and can be directly solved\nusing many existing solvers;\n(vi) We prove and demonstrate the tightness of the first-known approximation ratios for the\ngreedy and local search algorithms;\n(vii) Our analyses can be extended to the Rank-one Sparse SVD (R1-SSVD), which aims to com-\npute the largest singular value of the possibly non-symmetric matrix A with the sparsity\nconstraints on its left-singular and right-singular vectors separately; and\n(viii) We extend the second MISDP formulation to Sparse Fair PCA (SFPCA), where the covariance\nmatrices are observed from multiple protected groups.\nOur contributions have both theoretical and practical relevance. Theoretically, we contribute three\nexact mixed-integer convex programs to SPCA. Practically, our MILP formulation can either attain\noptimal solutions for SPCA, improve the continuous relaxations, or find better-quality feasible\nsolutions for small and medium-size instances. We apply the computationally efficient subgradi-\nent method to solving the continuous relaxations of the proposed MISDPs, as well as deriving\ntheir theoretical optimality gaps. We also develop two scalable approximation algorithms to solve\nSPCA to near optimality and prove their approximation ratios. Our proposed algorithms have been\ndemonstrated to be successfully applied to large-scale data analytics problems, such as identifying\nkey features for the drug abuse problem. We further extend the analyses to R1-SSVD and SFPCA.\nAll the theoretical contributions are summarized in Table 1.\nOrganization: The remainder of this paper is organized as follows. Sections 2 and 3 develop two MISDP formulations for SPCA and prove the optimality gaps of their continuous relaxation values. Section 4 investigates an approximate MILP, which can be arbitrarily close to the optimal value of SPCA, and proves the optimality gap of its continuous relaxation value. Section 5 introduces and analyzes two approximation algorithms. Section 6 conducts a numerical study to demonstrate the efficiency and the solution quality of our proposed formulations and algorithms. Sections 7 and 8 separately extend the analyses to the rank-one sparse SVD (R1-SSVD) and the sparse fair PCA (SFPCA). Finally, conclusion and future directions are exhibited in Section 9. Notation: The following notation is used throughout the paper. We let Sn,Sn+,Sn++ denote set of all the n\u00d7n symmetric real matrices, set of all the n\u00d7n symmetric positive semi-definite matrices, and set of all the n\u00d7n symmetric positive definite matrices, respectively. We use bold lower-case letters (e.g., x) and bold upper-case letters (e.g., X) to denote vectors and matrices, respectively, and use corresponding non-bold letters (e.g., xi,Xij) to denote their components. We use 0 to denote the zero vector and 1 to denote the all-ones vector. We use d\u00b7e as a ceil function. We let Rn+ denote the set of all the n dimensional nonnegative vectors and let Rn++ denote the set of all the n dimensional positive vectors. Given a positive integer n and an integer s \u2264 n, we let [n] := {1,2, \u00b7 \u00b7 \u00b7 , n} and let [s,n] := {s, s+ 1, \u00b7 \u00b7 \u00b7 , n}. We let In denote the n\u00d7 n identity matrix and let ei denote its i-th\ncolumn vector. Given a set S and an integer k, we let |S| denote its cardinality and ( S k ) denote the collection of all the size-k subsets out of S. Given an m\u00d7n matrix A and two sets S \u2208 [m], T \u2208 [n], we let AS,T denote a submatrix of A with rows and columns indexed by sets S,T , respectively and let AS denote a submatrix of A with columns from the set S. Given a vector x\u2208Rn, we let Diag(x) denote the diagonal matrix with diagonal elements x1, \u00b7 \u00b7 \u00b7 , xn, and let supp(x) denote the support of x. Given a square symmetric matrix A, let diag(A) denote the vector of diagonal entries of A, and let \u03bbmin(A), \u03bbmax(A) denote the smallest and largest eigenvalues of A, respectively. Given a non-square matrix A, let \u03c3max(A) denote the largest singular value. Additional notation will be introduced later as needed.\n2. Exact MISDP Formulation (I) In this section, we derive an equivalent mixed-integer semi-definite programming (MISDP) formulation for SPCA based on the spectral decomposition and disjunctive programming techniques.\nTo begin with, for each i \u2208 [n], we let the binary variable zi = 1 if the i-th feature is selected, and 0, otherwise. Linearizing the zero-norm constraint using binary vector z, then SPCA (1) can be equivalently formulated as a following nonconvex mixed-integer quadratic program:\n(SPCA) w\u2217 := max x\u2208Rn,z\u2208Z\n{ x>Ax : ||x||2 = 1, |xi| \u2264 zi,\u2200i\u2208 [n] } , (2)\nwhere we let cardinality set Z denote the feasible region of z, i.e.,\nZ = { z \u2208 {0,1}n : \u2211 i\u2208[n] zi = k } .\nFor SPCA (2), we note that (i) the binary vector z is of vital importance and its associated feasible\nregion Z will be used throughout this paper for two MISDPs and one MILP, and (ii) the derivations\nof all the three mixed-integer formulations originate from the naive SPCA (2).\n2.1. Spectral Reformulation We observe that given a size-k subset of features (i.e., the\nsupport of the binary vector z in formulation (2) is specified), the SPCA (2) is equivalent to\nfinding the largest eigenvalue of the corresponding principal submatrix of A. This fact inspires\nus to propose three equivalent mixed-integer convex programs for SPCA (2) . This observation is\nsummarized below.\nLemma 1 For a symmetric matrix A\u2208 Sn and a size-k set S \u2286 [n], the followings must hold: (i) maxx\u2208Rn { x>Ax : ||x||2 = 1, xi = 0,\u2200i /\u2208 S } = \u03bbmax(AS,S),\n(ii) maxX\u2208Sk+ {tr(AS,SX) : tr(X) = 1}= \u03bbmax(AS,S), and (iii) If matrix A is positive semi-definite, then \u03bbmax(AS,S) = \u03bbmax( \u2211 i\u2208S cic > i ), where A=C >C,\nC \u2208 Rd\u00d7n denotes the Cholesky factorization matrix of A, d is the rank of A, and ci \u2208 Rd denotes i-th column vector of C for each i\u2208 [n].\nProof. See Appendix A.1.\nThe results in Lemma 1 are crucial to this paper and allow us to derive the exact mixed-integer\nconvex programs of SPCA. Specifically, we remark that: Part (i) of Lemma 1 reduces SPCA to\nselecting the best size-k principal submatrix ofA to achieve the maximum largest eigenvalue, which\nestablishes a combinatorial formulation of SPCA; Part (ii) of Lemma 1 shows that SDP relaxation\nof the largest eigenvalue problem by dropping the rank-one constraint is exact and inspires us to\ndevelop two MISDP formulations for SPCA; and since the covariance matrix used in SPCA is\nalways positive semi-definite, the identity in Part (iii) of Lemma 1 suggests an alternative way of\nformulating SPCA using Cholesky decomposition, which motivates us to derive an exact MISDP\nformulation in this section and an MILP in a later section.\nAccording to Part (i) in Lemma 1, introducing a subset S, a natural combinatorial reformulation\nof SPCA (1) is defined as:\nw\u2217 := max S {\u03bbmax(AS,S) : |S|= k,S \u2286 [n]} . (3)\nBy computing the Cholesky factorization of A=C>C with C \u2208Rd\u00d7n and d denoting the rank of A, then the identity in Part (iii) in Lemma 1 recasts the objective function of SPCA (3) as below:\nw\u2217 := max S\n{ \u03bbmax (\u2211 i\u2208S cic > i ) : |S|= k,S \u2286 [n] } . (4)\nRecall that for each i\u2208 [n], binary variable zi = 1 if ith feature (i.e., column ci) is selected, and 0, otherwise. Therefore, SPCA (4) can be further reformulated as\nw\u2217 := max z\u2208Z\n{ \u03bbmax (\u2211 i\u2208[n] zicic > i )} . (5)\nThe above formulation involves with concave objective function but it is a maximization problem, which will cause much trouble. Fortunately, the result in Part (ii) of Lemma 1 and the reformulation technique from disjunctive programming [2] motivate us to convert SPCA (5) to an equivalent MISDP, which is shown as below.\nTheorem 1 The SPCA (2) admits an equivalent MISDP formulation\n(SPCA) w\u2217 := max z\u2208Z,\nX,W1,\u00b7\u00b7\u00b7 ,Wn\u2208Sd+\n{\u2211 i\u2208[n] c>i Wici : tr(X) = 1,X Wi, tr(Wi) = zi,\u2200i\u2208 [n] } . (6)\nProof. According to Part (ii) in Lemma 1, the largest eigenvalue of a symmetric matrix can be equivalently reformulated as an SDP, thus by introducing a positive semi-definite matrix variable X \u2208 Sd+, SPCA (5) can be represented as\nw\u2217 := max z\u2208Z,X\u2208Sd+ {\u2211 i\u2208[n] zic > i Xci : tr(X) = 1 } , (7)\nwhere the objective function comes from the identity tr(cic > i X) = c > i Xci for each i\u2208 [n].\nIn SPCA (7), the objective function contains bilinear terms {ziX}i\u2208[n]. To further convexify them, we create two copies of the matrix variable X, denoting by Wi1,Wi2 for each i \u2208 [n] and one of them will be equal to X depending on the value of binary variable zi. Specifically, SPCA (7) now becomes\nw\u2217 := max z\u2208Z,X,Wi1,Wi2\u2208Sd+ {\u2211 i\u2208[n] c>i Wi1ci :X =Wi1 +Wi2,\u2200i\u2208 [n], tr(X) = 1,\ntr(Wi1) = zi, tr(Wi2) = 1\u2212 zi,\u2200i\u2208 [n] } .\nAbove, the matrix variables {Wi2}i\u2208[n] are redundant and can be replaced by inequality X Wi for each i\u2208 [n]. Thus, we arrive at the equivalent reformulation (4) for SPCA.\nTheorem 1 presents the first equivalent MISDP formulation (6) to SPCA. The resulting formu-\nlation (6) has several interesting properties: (i) it can be directly solved via exact MISDP solvers such as YALMIP; (ii) matrix variables X and {Wi}i\u2208[n] have dimension of d\u00d7 d, where d is the rank of matrix A. Thus, the size of SPCA (6) can be further reduced if the covariance matrix\nA is low-rank; and (iii) the binary variables z can be separated from the other variables, so one\ncan apply the Benders decomposition to solving the SPCA (6). This result will be elaborated with\nmore details in the next subsection.\nFor large-scale instances, computing the continuous relaxation values of the SPCA (6) provides\nus an upper bound to the optimal value or can be useful to check the quality of different heuristics.\nIn the following, we show that the continuous relaxation value of SPCA (6) is not too far away from the optimal value w\u2217. First, let w1 denote the continuous relaxation value, i.e.,\nw1 := max z\u2208Z,\nX,W1,\u00b7\u00b7\u00b7 ,Wn\u2208Sd+\n{\u2211 i\u2208[n] c>i Wici : tr(X) = 1,X Wi, tr(Wi) = zi,\u2200i\u2208 [n] } , (8)\nwhere we let Z denote the continuous relaxation of set Z, i.e.,\nZ = { z \u2208 [0,1]n : \u2211 i\u2208[n] zi = k } .\nTheorem 2 The continuous relaxation value w1 of formulation (6) achieves a min{k,n/k} optimality gap of SPCA, i.e.,\nw\u2217 \u2264w1 \u2264min{k,n/k}w\u2217.\nProof. It is obvious that w\u2217 \u2264w1 since the feasible region of continuous relaxation (8) includes the original decision space. Thus, it remains to show that (i) w1 \u2264 kw\u2217 and (ii) w1 \u2264 n/kw\u2217.\nPart (i) w1 \u2264 kw\u2217. For any feasible solution (z,X,{Wi}i\u2208[n]) to problem (8), we must have\n\u2211 i\u2208[n] c>i Wici \u2264 \u2211 i\u2208[n] c>i ci tr(Wi) = \u2211 i\u2208[n] zic > i ci \u2264 \u2211 i\u2208[n] ziw \u2217 = kw\u2217,\nwhere the first inequality is due to the fact that the trace of the product of two symmetric positive\nsemi-definite matrices is no larger than the product of the traces of these two matrices [10], the first equality is from tr(Wi) = zi for each i\u2208 [n], the second inequality is because\nc>i ci = \u03bbmax ( cic > i ) \u2264 max\nS\u2286[n]:|S|=k \u03bbmax (\u2211 j\u2208S cjc > j ) :=w\u2217,\nand the second equality is due to \u2211\ni\u2208[n] zi = k.\nPart (ii) w1 \u2264 n/kw\u2217. Similarly, given any feasible solution (z,X,{Wi}i\u2208[n]) of continuous relaxation (8), we must have\u2211\ni\u2208[n] c>i Wici \u2264 \u2211 i\u2208[n] c>i Xci = 1( n\u22121 k\u22121 ) \u2211 S\u2208([n]k ) \u2211 i\u2208S c>i Xci \u2264 ( n k )( n\u22121 k\u22121 )w\u2217 = n k w\u2217,\nwhere the first inequality is because Wi X and the second one is from Part (ii) in Lemma 1. Theorem 2 shows that the continuous relaxation value of formulation (8) is at most min{k,n/k} away from the true optimal value of SPCA (6), implying that if k\u2192 1 or k\u2192 n, then the continuous relaxation value w1 is very close to the true optimal value w \u2217, which is consistent with the numerical study in Section 6.\n2.2. Solving SPCA (6) and SDP Relaxation (8): Benders Decomposition It has been recognized that large-scale SDPs are challenging to solve, so is the MISDP (6). In this subsection, we apply the Benders decomposition [4, 18] to the proposed MISDP (6), which can be further integrated into the branch and cut framework. By relaxing the binary vector z to be continuous, the Benders Decomposition recasts the continuous SDP relaxation (8) as a maximin saddle point problem, which enables the adoption of the efficient subgradient method.\nThe main idea of Benders decomposition is to decompose SPCA (6) into two stages: first, the master problem is a pure integer maximization problem over z, and second, given a feasible z \u2208Z, the subproblem is to maximize over the remaining variables (X,{Wi}i\u2208[n]). Thus, by separating the binary variables, we rewrite the SPCA (6) as\nw\u2217 := max z\u2208Z\nH1(z) := max X,W1,\u00b7\u00b7\u00b7 ,Wd\u2208Sd+ {\u2211 i\u2208[n] c>i Wici : tr(X) = 1,X Wi, tr(Wi) = zi,\u2200i\u2208 [n] } . (9)\nBenders decomposition is of particular interest when the subproblem H1(z) for any z \u2208Z is easy to compute, which is, unfortunately, not the case. Therefore, it is desirable if we can specify the function H1(z) for any given z \u2208Z in an efficient way. Surprisingly, invoking Part(ii) in Lemma 1, the strong duality of inner SDP maximization problem in (9) holds and the obtained dual problem admits a closed-form solution for any binary variables z \u2208 Z, which enables the subproblem to generate valid inequalities to the master problem efficiently. The results are shown below.\nProposition 1 For the function H1(z) defined in (9), we have\n(i) For any z \u2208Z, function H1(z) is equivalent to\nH1(z) = min \u00b5,Q1,\u00b7\u00b7\u00b7 ,Qn\u2208Sd+\n{ \u03bbmax (\u2211 i\u2208[n] Qi ) + \u2211 i\u2208[n] \u00b5izi : cic > i Qi +\u00b5iId,0\u2264 \u00b5i \u2264 \u2016ci\u201622,\u2200i\u2208 [n] } ,\n(10)\nwhich is concave in z.\n(ii) For any binary z \u2208 Z, an optimal solution to problem (10) is \u00b5\u2217i = 0 if zi = 1 and \u2016ci\u201622,\notherwise, and Q\u2217i := (1\u2212\u00b5\u2217i /\u2016ci\u201622)cic>i for each i\u2208 [n].\nProof. See Appendix A.2.\nThe Part (ii) of Proposition 1 shows that given a solution z \u2208Z with its support S, the optimal\nvalue to (10) is equal to\nH1(z) = \u03bbmax (\u2211 i\u2208S cic > i ) + \u2211 i\u2208[n]\\S ||ci||22,\nwhich leads to an equivalent reformulation of SPCA (9) as\nw\u2217 = max z\u2208Z\n{ w :w\u2264 \u03bbmax(ASS) + \u2211 i\u2208[n]\\S \u2016ci\u201622zi,\u2200S \u2286 [n] : |S|= k } . (11)\nAbove, for any mixed binary solution (z\u0302, w\u0302)\u2208Z \u00d7R, the most violated constraint is\nw\u2264 \u03bbmax(AS\u0302S\u0302) + \u2211\ni\u2208[n]\\S\u0302\n\u2016ci\u201622zi,\nwhere set S\u0302 := {i \u2208 [n] : z\u0302i = 1} denotes the support of z\u0302. We remark that the exact branch and cut approach to solve SPCA (11) using callback functions will benefit from these closed-form valid inequalities.\nNote that by relaxing the binary variables to be continuous, the relaxed problem (9) is equivalent to the SDP relaxation (8). However, given z \u2208Z, the dual representation of function H1(z) in (10) is still a difficult SDP. Motivated by Part (ii) in Proposition 1, we propose a more efficient upper bound H1(z) than H1(z) by letting Qi := (1\u2212 \u00b5i/\u2016ci\u201622)cic>i for each i \u2208 [n] to problem (10). In the next theorem, we show that the relaxed H1(z) becomes exact for any binary vector z \u2208Z and the resulting upper bound of SPCA also achieves a min{k,n/k} optimality gap.\nTheorem 3 The following results hold for the relaxed function H1(z):\n(i) For any z \u2208Z, function H1(z) is upper bounded by\nH1(z) = min \u00b5\n{ \u03bbmax (\u2211 i\u2208[n] (1\u2212\u00b5i/\u2016ci\u201622)cic>i ) + \u2211 i\u2208[n] \u00b5izi : 0\u2264 \u00b5i \u2264 \u2016ci\u201622,\u2200i\u2208 [n] } ; (12)\n(ii) If z \u2208Z, then H1(z) =H1(z) = \u03bbmax( \u2211 i\u2208[n] zicic > i ); and\n(iii) The continuous relaxation value of SPCA\nw2 = max z\u2208Z H1(z) (13)\nachieves a min{k,n/k} optimality gap of SPCA, i.e., w\u2217 \u2264w1 \u2264w2 \u2264min{k,n/k}w\u2217, where w1 is defined in (8)."
    },
    {
      "heading": "Proof.",
      "text": "(i) The conclusion follows by choosing a feasible Qi := (1\u2212\u00b5i/\u2016ci\u201622)cic>i for each i \u2208 [n] in the\nrepresentation (10). (ii) For any z \u2208 Z, we derive from Part (ii) in Proposition 1 that H1(z) \u2265 \u03bbmax( \u2211 i\u2208[n] zicic > i ).\nThus, it is sufficient to show that H1(z)\u2264 \u03bbmax( \u2211 i\u2208[n] zicic > i ). Indeed, this can be done simply by letting \u00b5i = 0 if zi = 0, and ||ci||22, otherwise in (12).\n(iii) By the proof of Theorem 2, to obtain the same optimality gap for (13) as SDP (8), we need to show that H1(z)\u2264 \u2211 i\u2208[n] zic > i ci and H1(z)\u2264 \u03bbmax(A) = \u03bbmax( \u2211 i\u2208[n] cic > i ) for any z \u2208Z.\nWe must have H1(z)\u2264 \u2211 i\u2208[n] zic > i ci by by letting \u00b5i = c > i ci for all i\u2208 [n] in (12).\nWe also have H1(z)\u2264 \u03bbmax(A) = \u03bbmax( \u2211 i\u2208[n] cic > i ) by letting \u00b5i = 0 for all i\u2208 [n] in (12). Then the rest of the proof follows directly from that of Theorem 2 and is thus omitted.\nWe remark that: (i) Compared to H1(z), function H1(z) in (12) only involves an n-dimensional variable \u00b5. The resulting relaxation (13) of SPCA can be viewed as a conventional saddle problem\nso we apply the subgradient method with convergence rate of O(1/T ) to the search for optimal\nsolutions (see, e.g., [31]), which offers an efficient way to generate an upper bound of SPCA in\nSection 6; (ii) On the other hand, the continuous relaxation value w1 = maxz\u2208ZH1(z) tends to be stronger than w2 in (13). Thus, it is a tradeoff between computational effort and a better upper bound; (iii) Surprisingly, both bounds w1,w2 achieve the same optimality gap of SPCA. This implies that there might be room to improve the analysis of optimality gap in Theorem 2. We leave this to interested readers; and (iv) more importantly, when z \u2208 Z is binary, both problems (10)\nand (12) have closed-form results, which are very helpful for using the branch and cut method.\n3. Exact MISDP Formulation (II) The MISDP formulation (6) developed for SPCA in\nthe previous section mainly are inspired from Part(ii) and Part(iii) in Lemma 1. In this section, we\nwill propose another exact MISDP reformulation of SPCA using Part(i) and Part(ii) in Lemma 1.\nSimilarly, we will present the optimality gap of the corresponding SDP relaxation to demonstrate\nthe strength of the second formulation. It is worthy of noting that the proposed MISDP (6) requires\nthe positive semi-definiteness of matrix A as it is built on Cholesky decomposition of A, but the\nresult in this section is more general and holds even matrix A is not positive semi-definite.\n3.1. A Naive Exact MISDP Formulation We first establish a naive exact MISDP formu-\nlation of SPCA (2) based on Part (ii) in Lemma 1, and the resulting continuous relaxation value\nis equal to \u03bbmax(A).\nProposition 2 The SPCA (2) admits the following MISDP formulation:\n(SPCA) w\u2217 := max z\u2208Z,X\u2208Sn+\n{ tr(AX) : tr(X) = 1,Xii \u2264 zi,\u2200i\u2208 [n] } . (14)\nand its continuous relaxation value is equal to \u03bbmax(A).\nProof. See Appendix A.3. The SPCA formulation (14) can be also found in [17]. However, our proof is quite different and shorter, since it does not involve sophisticated extreme point characterization of SDPs. Although the MISDP (14) is equivalent to SPCA (2), the fact that its continuous relaxation value is equal to \u03bbmax(A) demonstrates that it might be a weak formulation. This motivates us to further strengthen the formulation (14) by adding valid inequalities in the next subsection.\n3.2. A Stronger Reformulation with Two Valid Inequalities In this subsection, we first propose two valid inequalities for SPCA (14) and derive the optimality gap of its continuous relaxation value of the improved formulation.\nAfter examining different types of valid inequalities, we propose the following two types of valid\ninequalities for the SPCA formulation (14).\nLemma 2 The following two inequalities are valid to SPCA (14) (i) \u2211\nj\u2208[n]X 2 ij \u2264Xiizi for all i\u2208 [n]; and (ii) (\u2211 j\u2208[n] |Xij| )2 \u2264 kXiizi for all i\u2208 [n].\nProof. See Appendix A.4. We make the following remarks about Lemma 2.\n(i) Many other valid inequalities are dominated by the two types of valid inequalities in Lemma 2\nsuch as\n|Xij| \u2264 zi,X2ij \u2264Xiizj,X2ij \u2264 zizj,\u2200i, j \u2208 [n];\n(ii) Note that the two types of valid inequalities are both second order conic (see e.g., [3]), and\nthus can be embedded into SDP solvers such as MOSEK, SDPT3; and\n(iii) We further observe that the inequality Xii \u2264 zi in (14) is dominated by the first type of inequalities with the facts that X2ii + \u2211 j\u2208[n]\\{i}X 2 ij \u2264Xiizi and Xii \u2265 0 for each i\u2208 [n].\nThe results in Lemma 2 together with Proposition 2 give rise to a stronger MISDP of SPCA\nthan formulation (14), which is summarized below.\nTheorem 4 The SPCA (2) can reduce to following stronger MISDP formulation:\n(SPCA) w\u2217 := max z\u2208Z,X\u2208Sn+\n{ tr(AX) : tr(X) = 1, \u2211 j\u2208[n] X2ij \u2264Xiizi, (\u2211 j\u2208[n] |Xij| )2 \u2264 kXiizi,\u2200i\u2208 [n] } .\n(15)\nLet w3 denote the continuous relaxation value of SPCA formulation (15), i.e.,\nw3 := max z\u2208Z,X\u2208Sn+\n{ tr(AX) : tr(X) = 1, \u2211 j\u2208[n] X2ij \u2264Xiizi, (\u2211 j\u2208[n] |Xij| )2 \u2264 kXiizi,\u2200i\u2208 [n] } . (16)\nClearly, we have \u03bbmax(A)\u2265w3. We are going to prove that the continuous relaxation value can be even stronger than a well-known SDP upper bound for SPCA (2) introduced by d\u2019Aspremont et al. [13], denoted by w4, that has been widely used for solving SPCA in literature. The upper bound from [13] comes to the following formulation\nw4 := max X\u2208Sn+\n{ tr(AX) : tr(X) = 1, \u2211 i\u2208[n] \u2211 j\u2208[n] |Xij| \u2264 k } . (17)\nThe formal comparison result is shown below.\nProposition 3 The upper bounds w3,w4 of SPCA defined in (16) and(17), respectively, satisfy w4 \u2265 w3, i.e., the continuous relaxations value of the stronger MISDP (15) is stronger than the optimal value of the SDP formulation (17) from [13].\nProof. To show that w4 \u2265 w3, it is sufficient to prove that any feasible solution (z,X) of the continuous relaxation problem (16), will satisfy the constraints in the SDP formulation (17).\nClearly, we have X \u2208 Sn+ and tr(X) = 1. It remains that \u2211 i\u2208[n] \u2211\nj\u2208[n] |Xij| \u2264 k. Indeed, we have\u2211 i\u2208[n] \u2211 j\u2208[n] |Xij| \u2264 \u2211 i\u2208[n] \u221a k \u221a Xiizi \u2264 \u221a k \u221a\u2211 i\u2208[n] Xii \u221a\u2211 i\u2208[n] zi = k,\nwhere the first inequality results from type (ii) inequalities in Lemma 2, the second one is due to CauchySchwartz inequality, and the equality is due to tr(X) = 1 and \u2211\ni\u2208[n] zi = k.\nNext, we show that the continuous relaxations value of the stronger MISDP (15) is also quite\nclose to the true value. This phenomenon is more striking in the numerical study.\nTheorem 5 The continuous relaxations value of the stronger MISDP formulation (15) yields a min{k,n/k} optimality gap for SPCA, i..e,\nw\u2217 \u2264w3 \u2264min{k,n/k}w\u2217.\nProof. The proof is separated into two parts: (i) w3 \u2264 kw\u2217 and (ii) w3 \u2264 n/kw\u2217. (i) w3 \u2264 kw\u2217. For any feasible solution X to problem (16), we have\ntr(AX) = \u2211 i\u2208[n] \u2211 j\u2208[n] AijXij \u2264 \u2211 i\u2208[n] \u2211 j\u2208[n] |Aij||Xij| \u2264w\u2217 \u2211 i\u2208[n] \u2211 j\u2208[n] |Xij| \u2264 kw\u2217,\nwhere the first inequality is due to taking the absolute values, the second one is based on the fact that maxi\u2208[n]{Ai,i} \u2264w\u2217 and |Ai,j| \u2264 \u221a Ai,iAj,j \u2264w\u2217 for each pair i, j \u2208 [n], and the third one can be obtained from the proof of Proposition 3.\n(ii) w3 \u2264 n/kw\u2217. The proof is similar to the one of Theorem 2 since w3 \u2264 \u03bbmax(A)\u2264 n/kw\u2217.\nIn general, our two proposed MISDP formulations (6) and (15) are not comparable although their continuous relaxations have the same theoretical approximation gap, which will be also illustrated in the numerical study section. The continuous relaxation of the MISDP formulation (15) might be difficult to solve due to lager size of its matrix variables and higher complexity of its constraints. In the next subsection, we will discuss Benders decomposition for SPCA (15), where the subproblem reduces to a second order conic program rather than an SDP.\n3.3. Benders Decomposition The decomposition method developed for SPCA (15) in this subsection follows from Section 2.2. Therefore, many details will be omitted for brevity. Similarly, we decompose the proposed MISDP formulation (15) by a master problem over binary variables z \u2208Z and a subproblem over the matrix variable X \u2208 Sn+. Also, we reformulate SPCA (15) as the following equivalent two-stage optimization problem\nw\u2217 = max z\u2208Z H2(z) := max X\u2208Sn+\n{ tr(AX) : tr(X) = 1, \u2211 j\u2208[n] X2ij \u2264Xiizi, (\u2211 j\u2208[n] |Xij| )2 \u2264 kXiizi,\u2200i\u2208 [n] } .\n(18)\nIt is favorable to derive an efficient dual formulation of H2(z) for any given z \u2208 Z such that its subgradient can be easily computed. Indeed, invoking Part(ii) in Lemma 1 and dualizing the second order conic constraints, the strong duality of inner maximization over X in (18) still holds. The proof is similar to Proposition 1 and is thus omitted.\nProposition 4 For any z \u2208Z, function H2(z) is equivalent to\nH2(z) = min \u00b5,\u03bd1,\u03bd2,\u039b,W1,W2,\u03b2\n\u03bbmax (A+ \u039b + 1/2Diag(\u00b51 +\u00b52 +\u03bd1 +\u03bd2)\u2212W1 +W2) + 1/2(\u2212\u00b51 +\u00b52)>z+ k/2(\u2212\u03bd1 +\u03bd2)>z,\ns.t. \u03b2i + (W1)ij + (W2)ij \u2264 0,\u2200i\u2208 [n], j \u2208 [n],\u2211 j\u2208[n] \u039b2ij + (\u00b5i1) 2 \u2264 (\u00b5i2)2,\u2200i\u2208 [n],\n\u03b22i + (\u03bdi1) 2 \u2264 (\u03bdi2)2,\u2200i\u2208 [n], (W1)ij \u2265 0, (W2)ij \u2265 0,\u2200i\u2208 [n],\u2200j \u2208 [n], \u03bd1,\u03bd2 \u2208Rn+,\u039b,W1,W2 \u2208 Sn,\n(19)\nwhich is concave in z.\nFor the equivalent function H2(z) derived in Proposition 4, we remark that: (i) Note that for any given z \u2208Z, function H2(z) can be solved as an second order conic program and escape from the SDP curse. More effectively, it can be solved via many first-order methods (e.g., the subgradient\nmethod) since the subgradient is easy to obtain and the projection only involves second order conic constraints; (ii) On the other hand, when we solve the continuous relaxation\nw3 = max z\u2208Z H2(z), (20)\nthe subgradient method is also applicable to solve the entire maximin saddle problem with O(1/T ) rate of convergence (see, e.g., [31]); (iii) We can warm start the exact branch and cut algorithm by solving the continuous relaxation (20), and add all the subgradient inequalities into the root relaxed problem.\n4. A Mixed-Integer Linear Program (MILP) for SPCA with Arbitrary Accuracy The formulations developed in the previous section for solving SPCA either rely on MISDP solvers or customized branch and cut algorithms, which does not leverage existing computational powers of solvers such as CPLEX, Gurobi. In this section, motivated by the SPCA formulation (5) and the identity of eigenvalues, we further derive an approximate mixed-integer linear program (MILP) for SPCA with arbitrary accuracy > 0 and O(n+ d+ log( \u22121)) binary variables. We also prove the optimality gap of its corresponding LP relaxation. The results in this section assume that A is positive semi-definite.\n4.1. An MILP Formulation for SPCA The difficulty of SPCA (5) lies in how to convexify the objective function, i.e., the largest eigenvalue of a symmetric matrix A. In particular, our proposed MISDP formulations stem from the fact that the largest eigenvalue can be formulated as an equivalent SDP problem. Through a different lens, we represent the largest eigenvalue function based on the natural definition of eigenvalues of a matrix, i.e.,\n\u03bbmax(A) = max w,x\u2208Rn\n{ w :Ax=wx,x 6= 0 } ,\nwhere x denotes an eigenvector and the nonzero constraint rules out the trivial solution x= 0.\nThis motivates us to recast SPCA formulation (5) as the following nonconvex problem\nw\u2217 = max w,x\u2208Rd,z\u2208Z { w : \u2211 i\u2208[n] zicic > i x=wx,\u2016x\u2016\u221e = 1 } , (21)\nwhere \u2016x\u2016\u221e = 1 also excludes the trivial solution x= 0.\nFor any given z \u2208Z, the nonconvexity of SPCA formulation (21) lies in three aspects: (i) Bilinear terms {zix}i\u2208[n]. They can be easily linearized using the disjunctive programming techniques since vector z is binary; (ii) Constraint \u2016x\u2016\u221e = 1. The nonconvex constraint \u2016x\u2016\u221e = 1 can be equivalently written as a disjunction with 2d sets below\n\u222aj\u2208[d] { x\u2208Rd : xj = 1,\u2016x\u2016\u221e \u2264 1 } \u222aj\u2208[d] { x\u2208Rd : xj =\u22121,\u2016x\u2016\u221e \u2264 1 } .\nDue to the equivalence of x and \u2212x in SPCA (21), it suffices to only keep first d sets, i.e., \u222aj\u2208[d] { x\u2208Rd : xj = 1,\u2016x\u2016\u221e \u2264 1 } . This disjunction can be equivalently described as an MILP using the results in [2]; and (iii) Bilinear term wx. We can first approximate variable w using binary expansion and then linearize the obtained bilinear terms by the same disjunctive technique as part (i). The resulting MILP formulation is summarized in the following theorem.\nTheorem 6 Given a threshold > 0, the following MILP is O( )-approximate to SPCA (2), i.e.,\n\u2264 w\u0302( )\u2212w\u2217 \u2264 \u221a d\nw\u0302( ) := max w,z\u2208Z,y,\u03b1,x,,\u03b4,\u00b5,\u03c3 w\ns.t. x= \u03b4i1 + \u03b4i2, ||\u03b4i1||\u221e \u2264 zi, ||\u03b4i2||\u221e \u2264 1\u2212 zi,\u2200i\u2208 [n], x= \u2211 j\u2208[d] \u03c3j, ||\u03c3j||\u221e \u2264 yj, \u03c3jj = yj,\u2200j \u2208 [d], \u2211 j\u2208[d] yj = 1,\nx=\u00b5`1 +\u00b5`2, ||\u00b5`1||\u221e \u2264 \u03b1`, ||\u00b5`2||\u221e \u2264 1\u2212\u03b1`,\u2200`\u2208 [m], w=wU \u2212 (wU \u2212wL) (\u2211\ni\u2208[m]\n2\u2212i\u03b1i ) ,\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2211\ni\u2208[n]\ncic > i \u03b4i1\u2212wUx+ (wU \u2212wL) \u2211 `\u2208[m] 2\u2212`\u00b5`1 \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 \u221e \u2264 ,\n\u03b1\u2208 {0,1}m,y \u2208 {0,1}d,\n(22)\nwhere wL,wU separately denote the lower and upper bounds of SPCA, m := dlog2((wU \u2212wL) \u22121)e and the infinite norm inequality constraints can be easily linearized.\nProof. See Appendix A.5.\nFor the proposed MILP formulation (22), we remark that\n(i) This is the first-known MILP representation with arbitrary accuracy O( ) in literature of\nSPCA;\n(ii) The MILP formulation (22), although compact, involves O(n+ d+ log \u22121) binary variables,\nO(nd+ d log \u22121) continuous variables, and O(nd+n log \u22121) linear constraints;\n(iii) In SPCA (21), one might be curious about the choice of infinite norm. Unfortunately, as far\nas we are concerned, this is the only norm that leads to a compact MILP formulation;\n(iv) In the MILP formulation (22), one might consider replacing the infinite norm in the constraint || \u2211\ni\u2208[n] cic > i \u03b4i1\u2212wUx+ (wU \u2212wL) \u2211 i\u2208[m] 2 \u2212i\u00b5i1||\u221e \u2264 by other norms, which will lead to different formulations (either MILP or mixed-integer conic program) and slightly different approximation bounds;\n(v) Strong lower and upper bounds of SPCA wL,wU can speed up the solution procedure; and\n(vi) Instead of building a relatively large-scale MILP formulation (22), one might solve d number of smaller-scale MILPs by enumerating each set of a disjunction \u222aj\u2208[d] { x : xj = 1,\u2016x\u2016\u221e \u2264 1 } .\nThe last remark is summarized in the following corollary.\nCorollary 1 Given a threshold > 0, the optimal value of MILP (22) is equal to w\u0302( ) = maxj\u2208[d] w\u0302j( ), where for each j \u2208 [d], w\u0302j( ) is defined as\nw\u0302j( ) := max w,z\u2208Z,y,\u03b1,x,\u03b4,\u00b5 w\ns.t. x= \u03b4i1 + \u03b4i2, ||\u03b4i1||\u221e \u2264 zi, ||\u03b4i2||\u221e \u2264 1\u2212 zi,\u2200i\u2208 [n],\n||x||\u221e \u2264 1, xj = 1, x=\u00b5`1 +\u00b5`2, ||\u00b5`1||\u221e \u2264 \u03b1`, ||\u00b5`2||\u221e \u2264 1\u2212\u03b1`,\u2200`\u2208 [m],\nw=wU \u2212 (wU \u2212wL) (\u2211\ni\u2208[m]\n2\u2212i\u03b1i ) ,\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2211\ni\u2208[n]\ncic > i \u03b4i1\u2212wUx+ (wU \u2212wL) \u2211 i\u2208[m] 2\u2212i\u00b5i1 \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 \u221e \u2264 ,\n\u03b1\u2208 {0,1}m,\n(23)\nwhere wL,wU separately denote the lower and upper bounds of SPCA, m := dlog2((wU \u2212wL) \u22121)e and the infinite norm inequality constraints can be easily linearized.\nAlbeit being smaller-size, some MILPs defined in Corollary 1 might be infeasible. Since the optimal value of an infeasible maximization problem is \u2212\u221e by default, the result in Corollary 1 still holds.\nHowever, one might need to be cautious when using this result and be aware of infeasibilities.\n4.2. Theoretical Optimality Gap Similar to other two exact formulations, we are also\ninterested in deriving theoretical approximation bound for MILP formulation (22) by relaxing\nbinary variables z. Particularly, we assume that other binary variables y,\u03b1 can be enumerated\neffectively. Our results show that the theoretical optimality gap is, in general, worse than the other\ntwo bounds.\nTheorem 7 Given a threshold > 0, by enforcing the binary variables z to be continuous, let w5( ) denote the optimal value of the relaxed MILP formulation (22). Then we have\nw5( )\u2264min { k( \u221a d/2 + 1/2), n/k \u221a d+ (n\u2212 k)( \u221a d/2 + 1/2) } w\u2217+ \u221a d.\nProof. See Appendix A.6.\n5. Approximation Algorithms In this section, motivated by the equivalent combinatorial\nformulation (4), we prove and demonstrate the tightness of the approximation ratios of the well-\nknown greedy and local search algorithms for solving SPCA.\n5.1. Greedy Algorithm The greedy algorithm has been widely used in many combinatorial problems with the cardinality constraint. The greedy algorithm in this subsection is particularly based on the combinatorial formulation (4), which proceeds as follows: Given a subset S\u0302G \u2286 [n] denoting the selected vectors, it aims to find a new vector from {ci}i\u2208[n]\\S\u0302G to maximize the largest eigenvalue of the sum of rank-one matrices obtained so far including the new one. The detailed implementation can be found in Algorithm 1.\nAlgorithm 1 Greedy Algorithm for SPCA (4)\n1: Input: n\u00d7n matrix A 0 of rank d and integer k \u2208 [n] 2: Let A=C>C denote its Cholesky factorization where C \u2208Rd\u00d7n 3: Let ci \u2208Rd denote the i-th column vector of matrix C for each i\u2208 [n] 4: Let S\u0302G := \u2205 denote the chosen set 5: for `= 1, \u00b7 \u00b7 \u00b7 , k do\n6: Compute j\u2217 \u2208 arg maxj\u2208[n]\\S\u0302{\u03bbmax( \u2211 i\u2208S\u0302G\u222a{j} cic > i )} 7: Add j\u2217 to the set S\u0302G 8: end for 9: Output: S\u0302G\nThe following result show that the greedy Algorithm 1 yields 1/k-approximation ratio.\nTheorem 8 The greedy Algorithm 1 yields a k\u22121-approximation ratio for SPCA (4), i.e., the output S\u0302G of Algorithm 1 satisfies\n\u03bbmax (\u2211 i\u2208S\u0302G cic > i ) \u2265 1 k w\u2217.\nProof. Suppose that the optimal set of SPCA (4) is S\u2217, then we have\n\u03bbmax (\u2211 i\u2208S\u2217 cic > i ) \u2264 \u2211 i\u2208S\u2217 \u03bbmax(cic > i )\u2264 kmax i\u2208[n] \u03bbmax(cic > i )\u2264 k\u03bbmax (\u2211 i\u2208S\u0302G cic > i ) ,\nwhere the first inequality results from the convexity of largest eigenvalue function and the last one is because at the first iteration, the greedy Algorithm 1 must choose the largest-length vector. The approximation ratio k\u22121 of greedy Algorithm 1 is tight, since there exists an example whose greedy optimum is no better than k\u22121. This example is presented as below.\nExample 1 For any integer k \u2208 [d], let d= k+ 1, n= 2k, and the vectors {ci}i\u2208[n] \u2286Rd be\nci = { ei, if i\u2208 [k], ek+1, if i\u2208 [k+ 1, n], \u2200i\u2208 [n].\nProposition 5 In Example 1, the output value of greedy Algorithm 1 is k\u22121-away from the true optimal value of SPCA. That is, approximation ratio k\u22121 of greedy Algorithm 1 is tight.\nProof. In Example 1, according to the greedy Algorithm 1, it will select c1,c2, \u00b7 \u00b7 \u00b7 ,ck at each iteration, i.e., the output set is S\u0302G = [k]. Thus, the resulting largest eigenvalue of greedy Algorithm 1 is equal to 1.\nApparently, the true optimal value of Example 1 is equal to\n\u03bbmax ( \u2211 i\u2208[k+1,n] cic > i ) = \u03bbmax ( kek+1e > k+1 ) = k.\nThis completes the proof.\n5.2. Local Search Algorithm The local search algorithm can improve the existing solutions\nand has been successfully used to solve many interesting machine learning and data analytics\nproblems, such as experimental design [26] and maximum entropy sampling [24]. This subsection\ninvestigates the local search algorithm for SPCA (4) and proves its approximation ratio.\nIn the local search algorithm, we start with a size-k subset, and in each iteration, swap an\nelement of chosen set with one of the unchosen set as long as it improves the largest eigenvalue.\nThe detailed implementation can be found in Algorithm 2.\nAlgorithm 2 Local Search Algorithm for SPCA (4)\n1: Input: n\u00d7n matrix A 0 of rank d and integer k \u2208 [n] 2: Let A=C>C denote its Cholesky factorization where C \u2208Rd\u00d7n 3: Let ci \u2208Rd denote the i-th column vector of matrix C for each i\u2208 [n] 4: Initialize a size-k subset S\u0302L \u2286 [n] 5: do 6: for each pair (i, j)\u2208 S\u0302L\u00d7 ([n] \\ S\u0302L) do 7: if \u03bbmax (\u2211 `\u2208S\u0302L\u222a{j}\\{i} c`c > ` ) >\u03bbmax (\u2211 `\u2208S\u0302L c`c > ` ) then 8: Update S\u0302L := S\u0302L \u222a{j} \\ {i} 9: end if\n10: end for 11: while there is still an improvement 12: Output: S\u0302L\nTheorem 9 The local search Algorithm 2 returns a k\u22121-approximation ratio of SPCA, i.e., the\noutput S\u0302L of the local search Algorithm 2 satisfies\n\u03bbmax (\u2211 i\u2208S\u0302L cic > i ) \u2265 1 k w\u2217.\nProof. First, for each j \u2208 [n], we will show that\n\u03bbmax (\u2211 `\u2208S\u0302L c`c > ` ) \u2265 \u03bbmax(cjc>j ). (24)\nTo prove it, there are two cases to be discussed: whether j belongs to S\u0302L or not. The monotonicity of the largest eigenvalue of sum of positive semi-definite matrices implies that the inequality (24) holds if j \u2208 S\u0302L. If j \u2208 [n] \\ S\u0302L, then the local optimality condition implies that there exist i \u2208 S\u0302L such that\n\u03bbmax (\u2211 `\u2208S\u0302L c`c > ` ) \u2265 \u03bbmax ( \u2211 `\u2208S\u0302L\u222a{j}\\{i} c`c > ` ) \u2265 \u03bbmax(cjc>j ),\nwhere the second inequality is due to the monotonicity of the largest eigenvalue of sum of positive\nsemi-definite matrices.\nSecond, suppose S\u2217 to be the optimal solution to SPCA (4), by inequality (24), then we have\nw\u2217 = \u03bbmax (\u2211 i\u2208S\u2217 cic > i ) \u2264 \u2211 i\u2208S\u2217 \u03bbmax(cic > i )\u2264 k\u03bbmax (\u2211 `\u2208S\u0302L c`c > ` ) ,\nwhere the first inequality is because of the convexity of function \u03bbmax(\u00b7).\nWe remark that Example 1 also confirms the tightness of our analysis for local search Algorithm 2.\nProposition 6 In Example 1, the output value of local search Algorithm 2 is k\u22121-away from optimal value of SPCA. That is, approximation ratio k\u22121 of local search Algorithm 2 is tight.\nProof. In Example 1, we show that the initial subset S\u0302L = [k] already satisfies the local optimality condition.\nIndeed, for each pair (i, j)\u2208 S\u0302L\u00d7 ([n] \\ S\u0302L), we have\n\u03bbmax ( \u2211 `\u2208S\u0302L\u222a{j}\\{i} c`c > ` ) = \u03bbmax(Id\u2212eie>i ) = 1 = \u03bbmax(Id\u2212ede>d ) = \u03bbmax (\u2211 `\u2208S\u0302L c`c > ` ) ,\nwhere the identities follow the construction of {ci}i\u2208[n] in Example 1.\nTherefore, the set S\u0302L achieves the local optimum with largest eigenvalue of 1. Since the optimal\nvalue of SPCA is w\u2217 = k, the approximation ratio of set S\u0302L is equal to k \u22121.\nAs an improved heuristic, local search Algorithm 2 can use the output of the greedy Algorithm 1 as an initial solution. The results in Theorem 9 and Proposition 6 imply that the integrated algorithm still yields a k\u22121-approximation ratio of SPCA, while for solving the practical instances, our numerical study shows that the integrated algorithm in fact works very well. Since the greedy Algorithm 1 and local search Algorithm 2 repeatedly require to compute the largest eigenvalues, at each iteration, we can apply the power iteration method to efficiently calculate the largest eigenvalues [35] and use the eigenvectors from the previous iterations as a warm-start.\nFinally, we remark that there is only one swap in the local search Algorithm 2. We can improve it by increasing the number of swapping elements at each iteration, termed s-swap local search with s \u2208 [k]. The following result shows that s-swap local search can indeed achieve a better approximation ratio.\nCorollary 2 The approximation ratio of s-swap local search is sk\u22121 for any s\u2208 [k]. The approximation ratio is tight.\nProof. First, let set S\u0302L denote the indices of selected vectors by s-swap local search algorithm. Then following the same proof as that in Theorem 9, for any size-s set T \u2286 [n], we have\n\u03bbmax (\u2211 i\u2208S\u0302L cic > i ) \u2265 \u03bbmax (\u2211 i\u2208T cic > i ) . (25)\nLet S\u2217 denote the optimal solution to SPCA (4), using the result (25), the optimal value of\nSPCA w\u2217 is upper bounded by\nw\u2217 = \u03bbmax (\u2211 i\u2208S\u2217 cic > i ) = \u03bbmax ( 1( k\u22121 s\u22121 ) \u2211 T\u2286S\u2217,|T |=s \u2211 i\u2208T cic > i ) \u2264 ( k s )( k\u22121 s\u22121 )\u03bbmax(\u2211 i\u2208S\u0302L cic > i ) = k s (\u2211 i\u2208S\u0302L cic > i ) .\nSecond, to show the tightness, let us consider the following example.\nExample 2 For any integer k \u2208 [d], let d= k+ 1, n= (s+ 1)k, and the vectors {ci}i\u2208[n] \u2286Rd be\nci =  ei, if i\u2208 [k], ...\nei\u2212(s\u22121)k, if i\u2208 [(s\u2212 1)k+ 1, sk], ek+1, if i\u2208 [sk+ 1, n],\n\u2200i\u2208 [n].\nIn Example 2, we show that the subset S\u0302L = [k\u2212 s+ 1]\u222a {`k+ 1}`\u2208[s\u22121] satisfies the s-swap local optimality condition.\nIndeed, for each pair (T1, T2) such that T1 \u2286 S\u0302L, T2 \u2286 ([n] \\ S\u0302L) with |T1|= |T2|= s, we have\n\u03bbmax ( \u2211 `\u2208S\u0302L\u222aT2\\T1 c`c > ` ) \u2264 s.\nTherefore, the set S\u0302L achieves s-swap local optimum with largest eigenvalue of s. Since the optimal value of SPCA is w\u2217 = k, the approximation ratio of set S\u0302L is equal to sk \u22121 for SPCA. Albeit theoretically sound, s-swap local search with s\u2265 2 might not be practical since it involves O(n2) swaps at each iteration. Therefore, in the numerical study, we use the simple local search Algorithm 2, which already works very well.\n6. Numerical Study In this section, we conduct numerical experiments on six datasets with number of features n ranging from 13 to 2365 to demonstrate the computational efficiency and the solution quality of the MISDP (6), MISDP (15), and MILP (22) for exactly solving SPCA, the continuous relaxations (8), (16) and heuristic Algorithms 1, 2 for approximately solving SPCA. All the methods in this section are coded in Python 3.6 with calls to Gurobi 9.0 and MOSEK 9.0 on a personal PC with 2.3 GHz Intel Core i5 processor and 8G of memory. The codes and data are available at https://github.com/yongchunli-13/Sparse-PCA.\n6.1. Pitprops Dataset We first test the proposed three exact SPCA formulations (6), (15), (22) and their continuous relaxations to solve a commonly-used benchmark instance, Pitprops dataset Jeffers [20], which consists of 13 features (i.e., n= 13). In this instance, the computational results of seven different cases with k chosen from {4, \u00b7 \u00b7 \u00b7 ,10} are displayed in Table 2, Table 3, and Table 4.\nFor each testing case, we solve two MISDP formulations (6) and (15) using the branch and cut method. As for the MILP (22), it can be simply solved in Gurobi. Throughout the numerical study of MILP (22), we set = 10\u22124, use the best SDP relaxation values as the upper bound wU , and use the local search Algorithm 2 to compute the lower bound wL. As the newly released Gurobi 9.0 is able to solve the non-convex quadratic program, thus for the purpose of comparison, we further use Gurobi to solve the following SPCA formulation\nw\u2217 := max z\u2208Z,x\u2208Rn\n{ x>Ax : ||x||2 = 1, ||x||1 \u2264 \u221a k, |xi| \u2264 zi,\u2200i\u2208 [n] } . (26)\nThe computation results of the exact methods are shown in Table 2. In particular, we let time(s) denote the running time in seconds of each case and let Gurobi denote the performance of Gurobi for solving SPCA (26). In table 2, we see that all the SPCA formulations (6), (15), (22) can be solved to optimality within seconds, which demonstrates the efficiency of the proposed formulations. We also compare the numerical performance of the MILP formulation (22) with formulation (26) using the Gurobi solver, and it is clear that MILP is more efficient and stable. Especially for the case of k= 10, Gurobi has trouble finding the optimal solution of SPCA (26).\nAlthough the theoretical optimality gaps of the proposed SDP relaxations (8) and (16) are the same, these gaps in practice can be much smaller and can be significantly different from each other.\nWe use MOSEK to solve both SDP relaxations. The numerical results can be found in Table 3,\nwhere the SDP relaxation (17) proposed by d\u2019Aspremont et al. [13] is presented as a benchmark\ncomparison. In Table 3, we use gap(%) to denote the optimality gap, which is computed as 100\u00d7 (Upper Bound\u2212w\u2217)/w\u2217. It can be seen that the second SDP relaxation (16) is superior to\nthe first SDP relaxation (8) on the first five cases. When k is close to n, the first SDP relaxation (8)\ncan be better. This finding is consistent with remarks after Theorem 2. In addition, as proved in\nProposition 3, we see that the second SDP relaxation (16) always outperforms the bound (17) by\nd\u2019Aspremont et al. [13]. Finally, the second SDP relaxation (16) and the bound (17) by d\u2019Aspremont\net al. [13] are also not comparable.\nrithms for solving the Pitprops instance, where we let LB denote the lower bound and compute gap(%) by 100\u00d7 (w\u2217\u2212LB)/w\u2217. Note that we initialize the local search Algorithm 2 by the output\nof greedy Algorithm 1. To further improve the two algorithms, at each iteration, we employ the\npower iteration method to efficiently compute the largest eigenvalues [35] and warm-start it with\nthe good-quality eigenvectors from the previous iterations. In Table 4, we see that greedy Algorithm 1 and local search Algorithm 2 successfully find the optimal solutions and outperforms the truncation algorithm proposed by [9].\n6.2. Four Larger-scale Datasets In this subsection, we conduct experiments on four larger instances from Dey et al. [15] to further testify the efficiency of our proposed methods for SPCA, which are Eisen-1, Eisen-2, Colon and Reddit with n =79, 118, 500, and 2000. Since the MILP formulation (22) consistently outperforms two MISDP formulations (6) and (15). Thus, in this set of numerical experiments, we will stick to the MILP formulation (22).\nWe first compare the performances of different heuristic methods using the Reddit dataset with n = 2000 and k \u2208 {10, . . . ,70}. Thus, there are 7 cases in total. We implement the greedy Algorithm 1 and the local search Algorithm 2 and compare them with the best-known truncation algorithm proposed by [9]. The numerical results are shown in Table 5. We see that the local search Algorithm 2 provides the highest-quality solution of the three. The greedy Algorithm 1 is almost equally as good as the truncation algorithm. Although the local search Algorithm 2 takes the longest running time, the running time is quite reasonable given the size of the testing cases. Hence, our computation experiments show that the local search Algorithm 2 consistently outperforms the other two methods within a reasonably short time. Thus, we recommend using this algorithm to solve practical problems.\nNext, we obtain the local search Algorithm 2, the continuous relaxation bounds and exact values of SPCA on the four instances, i.e., Eisen-1, Eisen-2, Colon and Reddit. For these instances, MOSEK fails to solve our proposed SDP relaxations (8) and (16). Thus, instead, we use the subgradient method to solve the continuous relaxation formulations (13) and (20). For the MILP formulation (22), we set the time limit of Gurobi to be an hour. The computational results are presented in Table 6, where we let UB denote the upper bound of SPCA, let VAL denote the\nbest lower bound of MILP (22) found if the time limit is reached, and let MIPgap(%) denote the percentage of output MIP Gap from Gurobi. For these instances, we see that the local search Algorithm 2 still performs very well and the subgradient method is also efficient to solve the continuous relaxation (13). The continuous relaxation (20) turns out to be very difficult to compute, and even more difficult than the MILP formulation (22). For the instance Eisen-1, we see that both the MILP formulation (22) and local search Algorithm 2 can find the optimal solutions. This further demonstrates the effectiveness of the local search Algorithm 2.\n6.3. Drugabuse Dataset We finally apply the proposed local search Algorithm 2 to the Drugabuse Dataset with n= 2365 features, where the dataset comes from a questionnaire collected by the National Survey on Drug Use and Health (NSDUH) in 2018. It has been reported [33] that with the growing illicit online sale of controlled substances, deaths attributable to opioid-related drugs have been more than quadrupled in the U.S. since 1999. Thus, it is important to select a handful of features that the researchers can focus on for further exploration. Indeed, SPCA is a good tool to reduce the complexity and improve the interpretability of the machine learning algorithms\nby selecting the most important features. Our numerical finding of the case of k= 10 is illustrated in Figure 1, where the vertical values correspond to the selected features of the first PC, which are scaled by 100. We see that among 10 features, there are three categories (i.e., inhalants, drug injection, drug treatment), which are important for analyzing drug abuse. In particular, SPCA selects 6 features related to drug treatment, which is consistent with the literature [11, 39] that the treatment records of drug abuse are informative and important. Three drug injection questions have been designed to understand the injection experience of different special drugs, and it is well known that drug injection users are at high risk for HIV and other blood-borne infections [32, 38]. Inhalants feature, corresponding to various accessible products that can easily cause addictions, significantly contributes to the increase of drug abuse [6, 14].\n7. Extension to the Rank-one Sparse Singular Value Decomposition (R1-SSVD) In this section, we extend the proposed formulations and theoretical results to the rank-one sparse singular value decomposition (R1-SSVD). R1-SSVD has been successfully used to analyze the rowcolumn associations within high-dimensional data (see, e.g., [28, 23, 36]). The goal of R1-SSVD is to find the best submatrix (possibly non-square) of a particular size whose largest singular value is maximized, from a given matrix.\nFormally, R1-SSVD can be formulated as\n(R1-SSVD) w\u2217SVD := max u\u2208Rm,v\u2208Rn\n{ u>Av : ||u||2 = 1, ||v||2 = 1, ||u||0 = k1, ||v||0 = k2 } , (27)\nwhere the matrix A\u2208Rm\u00d7n is known, m,n, and k1 \u2208 [m] and k2 \u2208 [n] are positive integers.\nOur reduction of R1-SSVD (27) to SPCA (1) follows from the development of an augmented\nsymmetric matrix A\u2208 Sm+n\nA= [ 0 A A> 0 ] . (28)\nLet x := [u>,v>]> denote an (m+n)-dimensional vector. According to the identity\nx>Ax= [ u> v> ][ 0 A A> 0 ][ u v ] = 2u>Av,\nthen R1-SSVD (27) can be reformulated as\nw\u2217SVD := 1\n2 max x\u2208Rm+n\n{ x>Ax : ||x1:m||2 = 1, ||xm+1:m+n||2 = 1, ||x1:m||0 = k1, ||xm+1:m+n||0 = k2 } , (29)\nwhere we let x1:m denote the collection of m entries of vector x from index set [m] and xm+1,m+n denote the n entries of x from index set [m+ 1,m+ n]. In R1-SSVD (29), we enforce the sparse restrictions on both x1:m and xm+1,m+n. Thus, the R1-SSVD (29) can be viewed as a special case of the conventional SPCA (1), where A is symmetric but not positive semi-definite and there are two sparsity constraints instead of one.\nSimilarly, introducing binary variable zi = 1 if ith column of matrix A is chosen, 0, otherwise,\nwe can linearize the zero-norm constraints and recast R1-SSVD (29) as\nw\u2217SVD := 1\n2 max\nx\u2208Rm+n,z\u2208ZSVD\n{ x>Ax : ||x1:m||2 = 1, ||xm+1:m+n||2 = 1, |x|i \u2264 zi,\u2200i\u2208 [m+n] } , (30)\nwhere set ZSVD is defined as\nZSVD :=\n{ z \u2208 {0,1}m+n : \u2211 i\u2208[m] zi = k1, \u2211 i\u2208[m+1,m+n] zi = k2 } .\nThe following lemma inspires us three exact mixed-integer formulations for R1-SSVD (30).\nLemma 3 Given a matrix A \u2208 Rm\u00d7n, consider its augmented counterpart A defined in (28), two integers k1 \u2208 [m] and k2 \u2208 [n], and three subsets S,S1, S2 \u2286 [m + n] such that S \u2286 [m+n], |S|= k1 + k2, S1 = S \u2229 [m], |S1| = k1 and S2 = S \u2229 [m+ 1,m+ n], |S2| = k2. Then the following identities must hold:\n(i) The eigenvalues of the augmented submatrix AS,S are the singular values of submatrix AS1,S2\nand their negations;\n(ii) \u03c3max(AS1,S2) = \u03bbmax(AS,S) = 1/2maxx\u2208Rk1+k2{x>Ax : ||x1:k1 ||2 = 1, ||xk1+1:k1+k2 ||2 = 1} =\n1/2max X\u2208Sk1+k2+\n{ tr(AS,SX) : \u2211 j\u2208[k1]Xjj = 1, \u2211 i\u2208[k1+1,k1+k2]Xii = 1 } .\nProof. See Appendix A.7.\nNotably, Part (ii) in Lemma 3 shows that R1-SSVD is equivalent to the following combinatorial\noptimization problem\nw\u2217SVD := max S\u2286[m+n]\n{ \u03bbmax(AS,S) : |S \u2229 [m]|= k1, |S \u2229 [m+ 1,m+n]|= k2 } . (31)\nThe next four subsections present MISDP formulations (I) and (II), a MILP formulation, and\napproximation algorithms, respectively.\n7.1. MISDP Formulation (I) The fact that matrix A is symmetric but not positive semi-\ndefinite impedes us to directly apply the results in Section 2. Fortunately, a simple remedy by\nadding a new matrix \u03c3max(A)Im+n to A fixes this issue. That is, let us define\nA #\n:=A+\u03c3max(A)Im+n, (32)\nwhich is indeed positive semi-definite according to Part (i) in Lemma 3. More importantly, the new matrix A # preserves all the sparsity properties of the original one A.\nThus, the combinatorial optimization R1-SSVD (30) is equivalent to\nw\u2217SVD := max S\u2286[m+n]\n{ \u03bbmax(A # S,S) : |S \u2229 [m] = k1, |S \u2229 [m+ 1,m+n] = k2 } \u2212\u03c3max(A). (33)\nNow all the results in Section 2 are directly applicable to R1-SSVD (33). We highlight two\nimportant ones below.\nTheorem 10 The R1-SSVD (33) admits an equivalent MISDP formulation:\nw\u2217SVD := max z\u2208ZSVD,\nX,W1,\u00b7\u00b7\u00b7 ,Wd\u2208Sd+\n{ \u2211 i\u2208[m+n] c>i Wici : tr(X) = 1,X Wi, tr(Wi) = zi,\u2200i\u2208 [m+n] } \u2212\u03c3max(A),\n(34)\nwhere A # =C>C denotes the Cholesky factorization of A # with C \u2208 Rd\u00d7(m+n), d is the rank of A # , and ci \u2208Rd denotes the i-th column vector of matrix C for each i\u2208 [m+n].\nTheorem 11 The continuous relaxation value wSVD1 of formulation (34) satisfies\nw\u2217SVD \u2264wSVD1 \u2264 \u221a mnk\u221211 k \u22121 2 w \u2217 SVD.\nProof. See Appendix A.8.\n7.2. MISDP Formulation (II) Since the results in Section 3 do not rely on the positive\nsemi-definiteness of matrix A, they can be directly extended to R1-SSVD (30).\nWe first illustrate a naive MISDP for R1-SSVD (30) based on Part (ii) in Lemma 3.\nProposition 7 The R1-SSVD (30) is equivalent to the following MISDP formulation:\nw\u2217SVD := 1\n2 max\nz\u2208ZSVD,X\u2208S m+n +\n{ tr(AX) :\n\u2211 j\u2208[m] Xjj = 1, \u2211 j\u2208[m+1,m+n] Xjj = 1,Xii \u2264 zi,\u2200i\u2208 [m+n]\n} .\n(35)\nThe R1-SSVD formulation (35) is rather weak and its continuous relaxation value is equal to \u03c3max(A). Fortunately, we can derive two types of valid inequalities from strengthening it as below.\nLemma 4 For R1-SSVD (35), the following second-order conic inequalities are valid: (i) \u2211\nj\u2208[m]X 2 ij \u2264 ziXii, \u2211 j\u2208[m+1,m+n]X 2 ij \u2264 ziXii for all i\u2208 [m+n]; and\n(ii) ( \u2211 j\u2208[m] |Xij|)2 \u2264 k1Xiizi, ( \u2211 j\u2208[m+1,m+n] |Xij|)2 \u2264 k2Xiizi for all i\u2208 [m+n].\nProof. See Appendix A.9. The MISDP formulation for R1-SSVD (35) can be strengthened by adding these valid inequalities. Similar to Theorem 5, we provide the optimality gap of its continuous relaxation value as below.\nTheorem 12 The continuous relaxation value wSVD2 of R1-SSVD (35) with the inequalities in Lemma 4 yields an optimality gap at most min{ \u221a k1k2,mnk \u22121 1 k \u22121 2 }, i.e.,\nw\u2217SVD \u2264wSVD2 \u2264min {\u221a k1k2, \u221a mnk\u221211 k \u22121 2 } w\u2217SVD.\n7.3. An MILP Formulation with Arbitrary Accuracy Similarly, we can develop an\nMILP formulation with arbitrary accuracy based on the Cholesky decomposition of matrix A # in R1-SSVD (33). The proofs are similar to Section 4 and are thus omitted.\nTheorem 13 Given a threshold > 0 and lower and upper bounds of the optimal R1-SSVD, wL,wU , the following MILP is O( )-approximate to R1-SSVD (33), i.e., \u2264 w\u0302( )\u2212w\u2217 \u2264 \u221a d:\nw\u0302( ) := max w,z\u2208ZSVD,y,\u03b1,x,,\u03b4,\u00b5,\u03c3\nw\u2212\u03c3max(A)\ns.t. x= \u03b4i1 + \u03b4i2, ||\u03b4i1||\u221e \u2264 zi, ||\u03b4i2||\u221e \u2264 1\u2212 zi,\u2200i\u2208 [m+n], x= \u2211 j\u2208[d] \u03c3j, ||\u03c3j||\u221e \u2264 yj, \u03c3jj = yj,\u2200j \u2208 [d], \u2211 j\u2208[d] yj = 1,\nx=\u00b5`1 +\u00b5`2, ||\u00b5`1||\u221e \u2264 \u03b1`, ||\u00b5`2||\u221e \u2264 1\u2212\u03b1`,\u2200`\u2208 [L], w=wU \u2212 (wU \u2212wL) (\u2211\ni\u2208[L]\n2\u2212i\u03b1i ) ,\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 \u2211\ni\u2208[m+n]\ncic > i \u03b4i1\u2212wUx+ (wU \u2212wL) \u2211 i\u2208[L] 2\u2212i\u00b5i1 \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 \u221e \u2264 ,\n\u03b1\u2208 {0,1}L,y \u2208 {0,1}d,\n(36)\nwhere L := dlog2( /(wU \u2212wL))e.\nTheorem 14 Given a threshold > 0, let wSVD3( ) denote the optimal value of MILP formulation (36) by relaxing the binary variables z to be continuous. Then we have\nwSVD3( )\u2264 \u221a mn\nk1k2\n[ min { (k1 + k2) \u221a d+ 1\n2 ,\nm+n\nk1 + k2\n\u221a d+ (m+n\u2212 k1\u2212 k2)\n\u221a d+ 1\n2\n} \u2212 1 ] w\u2217SVD + \u221a d.\n7.4. Approximation Algorithms for R1-SSVD We will investigate three approximation algorithms for R1-SSVD (27): truncation algorithm, greedy algorithm, and local search algorithm.\n7.4.1. Truncation algorithm The approximation algorithm in [9] via truncation is known so far with the best approximation ratio O(n\u22121/3) for SPCA. We show that a similar truncation also works for R1-SSVD.\nFirst, we define the truncation operator as below.\nDefinition 1 (Normalized Truncation) Given a vector x\u2208Rn and an integer s\u2208 [n], vector x\u0302 is an s-truncation of x if\nx\u0302i = { |xi|, if |xi| is one of the s largest absolute entries of x 0, otherwise\nfor each i \u2208 [n]. The normalized s-truncation of x is defined as x\u0302 := x\u0302/\u2016x\u0302\u20162, which is normalized to be of unit length.\nThen the truncation algorithm for R1-SSVD has the following two steps:\n(i) Truncation in the standard basis: For each i \u2208 [n], let u\u0302i \u2208 Rm be the normalized k1truncation on the i-th column vector of A, and for each j \u2208 [m], let v\u0302j \u2208 Rn be the normalized k2-truncation on the j-th row vector of A. Clearly, u\u0302i and v\u0302j are feasible to R1-SSVD (27); (ii) Truncation in the eigen-space basis: Let v1 and u1 denote the right and left eigenvectors of A corresponding to the largest singular value. We then define the vector u\u03021 as the normalized k1-truncation on u1 and define v\u03021 as the normalized k2-truncation of the vector A >u\u03021. It is clear that (u\u03021, v\u03021) is also feasible to R1-SSVD (27).\nThe approximation results of the truncation procedure are summarized below.\nTheorem 15 For R1-SSVD (27), the truncation algorithm yields an approximation ratio\nmax {\u221a k\u221211 , \u221a k\u221212 , \u221a k1k2m\u22121n\u22121 } ."
    },
    {
      "heading": "In particular, the approximation ratio is O(n\u22121/3) when k1 \u2248 k2 and m\u2248 n.",
      "text": "Proof. See Appendix A.10.\n7.4.2. Greedy and Local Search Algorithms We design the greedy and local search algo-\nrithms according to the following equivalent combinatorial formulation of R1-SSVD (27)\nw\u2217SVD := max S1\u2286[m],S2\u2286[n]\n{ \u03c3max(AS1,S2) : |S1|= k1, |S2|= k2 } . (37)\nDifferent from SPCA (3), the R1-SSVD (37) maximizes the largest singular value of any k1 \u00d7 k2 submatrix rather than that of any size k-principal submatrix. Therefore, to solve R1-SSVD (37), we adapt the greedy Algorithm 1 or the local search Algorithm 2 considering selecting a row and/or a column at each iteration.\nSpecifically, for the greedy algorithm, let two subsets S1, S2 denote the index sets of the selected columns and rows, respectively. We first initialize the greedy algorithm by selecting the entry of A that takes the largest absolute value. Then, we add one element into each subset at each iteration, which maximizes the largest singular value of the obtained submatrix, unless we are not able to. Next, we continue to selection one row (or one column) at each iteration, until we reach a k1\u00d7 k2 submatrix. The detailed implementation can be found in Algorithm 3.\nGiven an initial feasible solution (S1, S2) to R1-SSVD (37), the adapted local search algorithm performs the swapping procedure on both S1 and S2 (see Algorithm 4 for details) simultaneously.\nAlgorithm 3 Greedy Algorithm for R1-SSVD (37)\n1: Input: m\u00d7n matrix A 0, integers k1 \u2208 [m], k2 \u2208 [n] 2: Let S\u03021 := \u2205 and S\u03022 := \u2205 denote the selected rows and columns, separately 3: Compute j\u22171 , j \u2217 2 \u2208 arg maxj1\u2208[m],j2\u2208[n] { |(A{j1},{j2}|\n} 4: Add j\u22171 , j \u2217 2 to sets S\u03021 and S\u03022, separately 5: for `= 2, \u00b7 \u00b7 \u00b7 ,max{k1, k2} do 6: if `\u2264min{k1, k2} then\n7: Compute j\u22171 \u2208 arg maxj1\u2208[m]\\S\u03021 { \u03c3max ( AS\u03021\u222a{j1},S\u03022 )} and add j\u22171 to set S\u03021\n8: Compute j\u22172 \u2208 arg maxj2\u2208[n]\\S\u03022 { \u03c3max ( AS\u03021,S\u03022\u222a{j2} )} and add j\u22172 to set S\u03022 9: else if k1 \u2264 k2 then\n10: Compute j\u22172 \u2208 arg maxj2\u2208[n]\\S\u03022 { \u03c3max ( AS1,S2\u222a{j2} )} and add j\u22172 to set S\u03022 11: else\n12: Compute j\u22171 \u2208 arg maxj1\u2208[m]\\S\u03021 { \u03c3max ( AS\u03021\u222a{j1},S\u03022 )} and add j\u22171 to set S\u03021 13: end if 14: end for 15: Output: S\u03021, S\u03022\nThe following results illustrate the theoretical performance guarantees of the two algorithms for\nR1-SSVD and show that the approximation ratios are both tight.\nTheorem 16 For the greedy Algorithm 3 and the local search Algorithm 4, we have (i) both algorithms achieve a ( \u221a k1k2) \u22121-approximation ratio of R1-SSVD (37), and (ii) the ratio is tight.\nProof. See Appendix A.11.\nAlgorithm 4 Local Search Algorithm for R1-SSVD (37)\n1: Input: m\u00d7n matrix A 0 and integers k1 \u2208 [m], k2 \u2208 [n] 2: Initialize a size-k1 subset S\u03021 \u2286 [m] and a size-k2 subset S\u03022 \u2286 [n] 3: do 4: for each pair (i1, j1, i2, j2)\u2208 S\u03021\u00d7 ([m] \\ S\u03021)\u00d7 S\u03022\u00d7 ([n] \\ S\u03022) do\n5: if \u03c3max ( AS1\u222a{j1}\\{i1},S2\u222a{j2}\\{i2} ) >\u03c3max (AS1,S2) then 6: Update S\u03021 := S\u03021 \u222a{j1} \\ {i1}, S\u03022 := S\u03022 \u222a{j2} \\ {i2} 7: end if 8: end for 9: while there is still an improvement\n10: Output: S\u03021, S\u03022\n8. Extension to Sparse Fair PCA In this section, we study the Sparse Fair PCA (SFPCA)\nand show its approximate MISDP formulation. The fair PCA has been recently studied in the\nliterature (see, e.g., [34, 37]). The goal of SFPCA is to seek the best principal submatrices of\nmulti-group covariance matrices to achieve the relatively similar objective values among different\ngroups.\nSuppose there are s groups and their corresponding covariance matrices are {Ai}i\u2208[s]. Then the\nSFPCA can be formulated as\nw\u2217F := max x { min i\u2208S x>Aix : ||x||2 = 1, ||x||0 \u2264 k } . (38)\nBy introducing binary variables z and linearizing the objective function, we obtain\nw\u2217F := max w,x,z\u2208Z\n{ w :w\u2264x>Aix,\u2200i\u2208 [s], ||x||2 = 1,\u2212zi \u2264 xi \u2264 zi,\u2200i\u2208 [n] } . (39)\nAs the SFPCA (39) is quite different from SPCA, it is not surprising that the results in Section 2\nand Section 4 do not apply to SFPCA (39). Fortunately, the results in Section 3 do provide an\ninteresting upper bound for SFPCA (39), which can be exact when there are s = 2 groups of covariance matrices. Introducing a rank-one positive semi-definite matrix variable X \u2208 Sn+ such\nthat X xx>, dropping the rank-one restriction, and adding the valid inequalities in Theorem 4,\nthe problem (39) can be upper bounded by\nwF := max w,X,z\u2208Z\n{ w :w\u2264 tr(AiX),\u2200i\u2208 [s], tr(X) = 1, \u2211 j\u2208[n] X2ij \u2264Xiizi, (\u2211 j\u2208[n] |Xij| )2 \u2264 kXiizi,\u2200i\u2208 [n] } . (40)\nThe following result shows that if s = 2, then the approximation (40) is exact, otherwise, it\nprovides an upper bound of SFPCA (39).\nProposition 8 For the MISDP formulation (40), we have\n(i) The optimal value of MISDP formulation (40) provides an upper bound of SFPCA (39), i.e.,\nwF \u2265w\u2217F . Also, when s= 2, the formulation (40) becomes exact, i.e., wF =w\u2217F ; and\n(ii) There exists an optimal solution (w\u2217,X\u2217,z\u2217) of of MISDP (40) such that the rank of X\u2217 is at most 1 + b \u221a 2s+ 9/4\u2212 3/2c."
    },
    {
      "heading": "Proof.",
      "text": "(i) It is clear that wF \u2265w\u2217F since we drop the rank-one restriction on X of MISDP formulation\n(40). On the other hand, for the case of s= 2, theorem 1.1 in [37] shows that for any feasible\nsolution (w,X,z), there exists a rank-one semi-definite matrix X\u0302 such that the new solution (w,X\u0302,z) is also feasible and achieves the same objective value. Thus, we must have wF =w \u2217 F ;\n(ii) Suppose (w,X,z) denotes an optimal solution of MISDP (40). Let S = {i\u2208 [n] : zi = 1}. Then\naccording to theorem 1.7 in [37], there exists a semi-definite matrix X\u0302 of the rank at most 1 + b \u221a 2s+ 9/4\u2212 3/2c such that the new solution (w,X\u0302,z) is also optimal.\nProposition 8 shows that two-group SFPCA (39) admits an MISDP representation, while MISDP\nformulation (40) provides a low-rank solution in general for SFPCA when s > 2. It is worthy of\nmentioning that the results in Proposition 8 work for any convex fairness measure.\n9. Conclusion In practice, to tune the parameter k via cross-validation, our developed greedy\nand local search algorithms can be quickly warm started from solution procedure in the previous\niterations. We anticipate that the theoretical optimality gaps of three exact formulations for SPCA\nand R1-SSVD are not tight and can be further strengthened. The analysis of the optimality gap\nof sparse fair PCA requires new techniques, which can be an exciting research direction. Also, it\nmight be desirable to study robust sparse PCA when the datasets are noisy or contain outliers.\nReferences\n[1] Amini AA, Wainwright MJ (2008) High-dimensional analysis of semidefinite relaxations for sparse prin-\ncipal components. 2008 IEEE International Symposium on Information Theory, 2454\u20132458 (IEEE).\n[2] Balas E (1975) Disjunctive programming: cutting planes from logical conditions. Nonlinear Programming\n2, 279\u2013312 (Elsevier).\n[3] Ben-Tal A, Nemirovski A (2001) Lectures on modern convex optimization: analysis, algorithms, and\nengineering applications, volume 2 (Siam).\n[4] Benders JF (1962) Partitioning procedures for solving mixed-variables programming problems. Numer.\nMath. 4(1):238252, ISSN 0029-599X, URL http://dx.doi.org/10.1007/BF01386316.\n[5] Berk L, Bertsimas D (2019) Certifiably optimal sparse principal component analysis. Mathematical\nProgramming Computation 11(3):381\u2013420.\n[6] Breakey WR, Goodell H, Lorenz PC, McHugh PR (1974) Hallucinogenic drugs as precipitants of\nschizophrenia. Psychological Medicine 4(3):255\u2013261.\n[7] Carrizosa E, Guerrero V (2014) rs-sparse principal component analysis: A mixed integer nonlinear\nprogramming approach with vns. Computers & operations research 52:349\u2013354.\n[8] Chaib S, Gu Y, Yao H (2015) An informative feature selection method based on sparse pca for vhr\nscene classification. IEEE Geoscience and Remote Sensing Letters 13(2):147\u2013151.\n[9] Chan SO, Papailliopoulos D, Rubinstein A (2016) On the approximability of sparse pca. Conference on\nLearning Theory, 623\u2013646.\n[10] Coope I (1994) On matrix trace inequalities and related topics for products of hermitian matrices.\nJournal of mathematical analysis and applications 188(3):999\u20131001.\n[11] Coughlin LN, Tegge AN, Sheffer CE, Bickel WK (2020) A machine-learning approach to predicting\nsmoking cessation treatment outcomes. Nicotine and Tobacco Research 22(3):415\u2013422.\n[12] d\u2019Aspremont A, Bach F, Ghaoui LE (2012) Approximation bounds for sparse principal component\nanalysis. arXiv preprint arXiv:1205.0121 .\n[13] d\u2019Aspremont A, Ghaoui LE, Jordan MI, Lanckriet GR (2005) A direct formulation for sparse pca using\nsemidefinite programming. Advances in neural information processing systems, 41\u201348.\n[14] De Barona MS, Simpson DD (1984) Inhalant users in drug abuse prevention programs. The American\njournal of drug and alcohol abuse 10(4):503\u2013518.\n[15] Dey SS, Mazumder R, Wang G (2018) A convex integer programming approach for optimal sparse pca.\narXiv preprint arXiv:1810.09062 .\n[16] dAspremont A, Bach F, Ghaoui LE (2008) Optimal solutions for sparse principal component analysis.\nJournal of Machine Learning Research 9(Jul):1269\u20131294.\n[17] Gally T, Pfetsch ME (2016) Computing restricted isometry constants via mixed-integer semidefinite\nprogramming. preprint, submitted .\n[18] Geoffrion AM (1972) Generalized benders decomposition. Journal of optimization theory and applica-\ntions 10(4):237\u2013260.\n[19] He Y, Monteiro RD, Park H (2011) An algorithm for sparse pca based on a new sparsity control criterion.\nProceedings of the 2011 SIAM International Conference on Data Mining, 771\u2013782 (SIAM).\n[20] Jeffers J (1967) Two case studies in the application of principal component analysis. Journal of the\nRoyal Statistical Society: Series C (Applied Statistics) 16(3):225\u2013236.\n[21] Jiang R, Fei H, Huan J (2012) A family of joint sparse pca algorithms for anomaly localization in\nnetwork data streams. IEEE Transactions on Knowledge and Data Engineering 25(11):2421\u20132433.\n[22] Journe\u0301e M, Nesterov Y, Richta\u0301rik P, Sepulchre R (2010) Generalized power method for sparse principal\ncomponent analysis. Journal of Machine Learning Research 11(2).\n[23] Lee M, Shen H, Huang JZ, Marron J (2010) Biclustering via sparse singular value decomposition.\nBiometrics 66(4):1087\u20131095.\n[24] Li Y, Xie W (2020) Best principal submatrix selection for the maximum entropy sampling problem:\nScalable algorithms and performance guarantees. arXiv preprint arXiv:2001.08537 .\n[25] Luss R, dAspremont A (2010) Clustering and feature selection using sparse principal component analysis.\nOptimization and Engineering 11(1):145\u2013157.\n[26] Madan V, Singh M, Tantipongpipat U, Xie W (2019) Combinatorial algorithms for optimal design.\nConference on Learning Theory, 2210\u20132258.\n[27] Magdon-Ismail M (2017) Np-hardness and inapproximability of sparse pca. Information Processing\nLetters 126:35\u201338.\n[28] Min W, Liu J, Zhang S (2016) L0-norm sparse graph-regularized svd for biclustering. arXiv preprint\narXiv:1603.06035 .\n[29] Moghaddam B, Weiss Y, Avidan S (2006) Spectral bounds for sparse pca: Exact and greedy algorithms.\nAdvances in neural information processing systems, 915\u2013922.\n[30] Naikal N, Yang AY, Sastry SS (2011) Informative feature selection for object recognition via sparse pca.\n2011 International Conference on Computer Vision, 818\u2013825 (IEEE).\n[31] Nedic\u0301 A, Ozdaglar A (2009) Subgradient methods for saddle-point problems. Journal of optimization\ntheory and applications 142(1):205\u2013228.\n[32] Ompad DC, Ikeda RM, Shah N, Fuller CM, Bailey S, Morse E, Kerndt P, Maslow C, Wu Y, Vlahov D,\net al. (2005) Childhood sexual abuse and age at initiation of injection drug use. American journal of public health 95(4):703\u2013709.\n[33] Overdose O (2018) Understanding the epidemic. Atlanta, Centers for Disease Control and Prevention .\n[34] Samadi S, Tantipongpipat U, Morgenstern JH, Singh M, Vempala S (2018) The price of fair pca: One\nextra dimension. Advances in Neural Information Processing Systems, 10976\u201310987.\n[35] Semlyen A, Angelidis G (1995) Efficient calculation of critical eigenvalue clusters in the small signal\nstability analysis of large power systems .\n[36] Sill M, Kaiser S, Benner A, Kopp-Schneider A (2011) Robust biclustering by sparse singular value\ndecomposition incorporating stability selection. Bioinformatics 27(15):2089\u20132097.\n[37] Tantipongpipat U, Samadi S, Singh M, Morgenstern JH, Vempala S (2019) Multi-criteria dimensionality\nreduction with applications to fairness. Advances in Neural Information Processing Systems, 15135\u2013 15145.\n[38] Thomas DL, Vlahov D, Solomon L, Cohn S, Taylor E, Garfein R, Nelson KE (1995) Correlates of\nhepatitis c virus infections among injection drug users. Medicine 74(4):212\u2013220.\n[39] Volkow ND, Fowler JS, Wang GJ, Swanson JM, Telang F (2007) Dopamine in drug abuse and addiction:\nresults of imaging studies and treatment implications. Archives of neurology 64(11):1575\u20131579.\n[40] Zhang Y, dAspremont A, El Ghaoui L (2012) Sparse pca: Convex relaxations, algorithms and applica-\ntions. Handbook on Semidefinite, Conic and Polynomial Optimization, 915\u2013940 (Springer).\n[41] Zhang Y, Ghaoui LE (2011) Large-scale sparse principal component analysis with application to text\ndata. Advances in Neural Information Processing Systems, 532\u2013539.\nAppendix A. Proofs\nA.1 Proof of Lemma 1\nLemma 1 For a symmetric matrix A\u2208 Sn and a size-k set S \u2286 [n], the followings must hold: (i) maxx\u2208Rn { x>Ax : ||x||2 = 1, xi = 0,\u2200i /\u2208 S } = \u03bbmax(AS,S),\n(ii) maxX\u2208Sk+ {tr(AS,SX) : tr(X) = 1}= \u03bbmax(AS,S), and (iii) If matrix A is positive semi-definite, then \u03bbmax(AS,S) = \u03bbmax( \u2211 i\u2208S cic > i ), where A=C >C,\nC \u2208 Rd\u00d7n denotes the Cholesky factorization matrix of A, d is the rank of A, and ci \u2208 Rd denotes i-th column vector of C for each i\u2208 [n].\nProof. Part (i) Given a size-k set S \u2286 [n], the maximization problem\nmax x\u2208Rn\n{ x>Ax : ||x||2 = 1, xi = 0,\u2200i /\u2208 S } reduces to\nmax x\u2208Rk\n{ x>AS,Sx : ||x||2 = 1 } ,\nwhich is exactly the definition of the largest eigenvalue of principal submatrix AS,S. Part (ii) According to Part (i), it is sufficient to show that v\u2217 = v\u0302, where v\u2217, v\u0302 are defined as\nv\u2217 : = max X\u2208Sk+ {tr(AS,SX) : tr(X) = 1} , (41)\nv\u0302 : = max x\u2208Rk\n{ x>AS,Sx : ||x||2 = 1 } . (42)\nFirst, we must have v\u2217 \u2265 v\u0302. Indeed, for any feasible x\u2208Rk to problem (42) such that ||x||2 = 1, we can construct a positive semi-finite matrix by X =xx>, which is feasible to problem (41) and yields the same objective value. Second, to prove v\u0302 \u2265 v\u2217, we let X\u2217 \u2208 Sk+ denote an optimal solution to problem (41) and X\u2217 =\u2211 i\u2208[k] \u03bbiqiq > i denote its spectral decomposition. Since tr(X\n\u2217) = 1 and X\u2217 \u2208 Sk+, the eigenvalues must satisfy \u2211 i\u2208[k] \u03bbi = 1 and \u03bbi \u2265 0 for each i\u2208 [k]. Thus, the optimal value v\u2217 of problem (41) is equal to\nv\u2217 = tr(AS,SX \u2217) = \u2211 i\u2208[k] \u03bbiq > i AS,Sqi \u2264max i\u2208[k] q>i AS,Sqi \u2264 v\u0302,\nwhere the inequality is due to \u2211\ni\u2208[k] \u03bbi = 1 and \u03bbi \u2265 0 for each i\u2208 [k]. Part (iii) For a positive semi-definite matrix A, let A=C>C denote the Cholesky factorization of A and C \u2208Rd\u00d7n, thus we have\n\u03bbmax(AS,S) = \u03bbmax(C > SCS) = \u03bbmax(CSC > S ),\nwhere the second equality is because for any matrix, its largest singular value is equal to that of its transpose.\nA.2 Proof of Proposition 1\nProposition 1 For the function H1(z) defined in (9), we have\n(i) For any z \u2208Z, function H1(z) is equivalent to\nH1(z) = min \u00b5,Q1,\u00b7\u00b7\u00b7 ,Qn\u2208Sd+\n{ \u03bbmax (\u2211 i\u2208[n] Qi ) + \u2211 i\u2208[n] \u00b5izi : cic > i Qi +\u00b5iId,0\u2264 \u00b5i \u2264 \u2016ci\u201622,\u2200i\u2208 [n] } ,\n(10)\nwhich is concave in z.\n(ii) For any binary z \u2208 Z, an optimal solution to problem (10) is \u00b5\u2217i = 0 if zi = 1 and \u2016ci\u201622,\notherwise, and Q\u2217i := (1\u2212\u00b5\u2217i /\u2016ci\u201622)cic>i for each i\u2208 [n].\nProof. Part (i). We split the proof of strong duality into two cases depending on whether z is a relative interior point of set Z or not. Case a. We will first prove the result by assuming that z is in the relative interior of set Z, i.e.,\n0 < zi < 1 for each i \u2208 [n]. For the inner maximization problem in (9), we dualize the constraint X Wi, tr(Wi) = zi with Lagrangian multiplier Qi \u2208 Sd+ and \u00b5i for each i\u2208 [n]. Note that the constraints X Wi, tr(Wi) = zi for each i \u2208 [n] and X,W1, \u00b7 \u00b7 \u00b7 ,Wn \u2208 Sd+ can be always strictly satisfied since 0< zi < 1. Thus, according to the strong duality of general conic program (see, e.g., Theorem 1.4.4 in [3]), function H1(z) can be rewrite as\nmin \u00b5,Q1,\u00b7\u00b7\u00b7 ,Qn\u2208Sd+ max X,W1,\u00b7\u00b7\u00b7 ,Wn\u2208Sd+ {\u2211 i\u2208[n] c>i Wici + \u2211 i\u2208[n] tr (Qi(X \u2212Wi)) + \u2211 i\u2208[n] \u00b5i (zi\u2212 tr(Wi)) : tr(X) = 1 } .\n(43)\nThen the inner maximization problem (43) over Wi for each i\u2208 [n] and X yields\nmax Wi\u2208Sd+\ntr ( (cic > i \u2212Qi\u2212\u00b5iId)Wi ) =\n{ 0, cic > i Qi +\u00b5iId,\n\u221e, otherwise.\nmax X\u2208Sd+\n{ tr ((\u2211 i\u2208[n] Qi ) X ) : tr(X) = 1 } = \u03bbmax (\u2211 i\u2208[n] Qi ) ,\nwhere the second identity is due to Part(ii) of Lemma 1.\nThus, problem (43) can be simplified as\nH1(z) = min \u00b5,Q1,\u00b7\u00b7\u00b7 ,Qn\u2208Sd+\n{ \u03bbmax (\u2211 i\u2208[n] Qi ) + \u2211 i\u2208[n] \u00b5izi : cic > i Qi +\u00b5iId,\u2200i\u2208 [n] } . (44)\nWe show that for the minimization problem (44), any optimal solution (\u00b5,Q1, \u00b7 \u00b7 \u00b7 ,Qn) must satisfy 0 \u2264 \u00b5i \u2264 \u2016ci\u201622 for each i \u2208 [n]. We prove it by contradiction. Suppose that there exits an optimal solution (\u00b5,Q1, \u00b7 \u00b7 \u00b7 ,Qn) to the problem (44) such that \u00b5j < 0 for\nsome j \u2208 [n]. Then, we can construct a new feasible solution (\u00b5,Q1, \u00b7 \u00b7 \u00b7 ,Qn), which is exactly equal to (\u00b5,Q1, \u00b7 \u00b7 \u00b7 ,Qn) except\n\u00b5j = 0,Qj =Qj +\u00b5jId.\nThe new solution yields the objective value\nH1(z) +\u00b5j \u2212\u00b5jzj =H1(z) +\u00b5j(1\u2212 zj)<H1(z),\nwhich is a contradiction to the optimality of (\u00b5,Q1, \u00b7 \u00b7 \u00b7 ,Qn). Similarly, suppose that there exits an optimal solution (\u00b5,Q1, \u00b7 \u00b7 \u00b7 ,Qn) to the problem (44) such that \u00b5j > \u2016ci\u201622 for some j \u2208 [n]. Similarly, we can arrive at a contradiction by defining a new feasible solution (\u00b5,Q1, \u00b7 \u00b7 \u00b7 ,Qn), which is exactly equal to (\u00b5,Q1, \u00b7 \u00b7 \u00b7 ,Qn) except \u00b5j = \u2016ci\u201622.\nTherefore, (44) can be reduced to (10).\nCase b. Now we consider the case that z is not in the relative interior of Z and define two sets\nT0 := {i \u2208 [n] : zi = 0} and T1 := {i \u2208 [n] : zi = 1}. Thus, at least one of the two sets is not empty. In this case, we first observe that H1(z) in (9) is equivalent to\nH1(z) := max X,W1,\u00b7\u00b7\u00b7 ,Wd\u2208Sd+ { \u2211 i\u2208[n]\\(T0\u222aT1) c>i Wici + \u2211 i\u2208T1 c>i Xci : tr(X) = 1,\nX Wi, tr(Wi) = zi,\u2200i\u2208 [n] \\ (T0 \u222aT1) } . (45)\nNext, applying the same procedure as Case a., we have\nH1(z) = min \u00b5,{Qi}i\u2208[n]\\(T0\u222aT1)\u2286S d +\n{ \u03bbmax ( \u2211 i\u2208[n]\\(T0\u222aT1) Qi + \u2211 i\u2208T1 cic > i ) + \u2211 i\u2208[n]\\(T0\u222aT1) \u00b5izi : cic > i Qi +\u00b5iId,0\u2264 \u00b5i \u2264 \u2016ci\u201622,\u2200i\u2208 [n] \\ (T0 \u222aT1) } . (46)\nTo show the equivalence between (46) and (10), it remains to prove that\nH\u03021(z) = min \u00b5,{Qi}i\u2208[n]\u2286Sd+\n{ \u03bbmax (\u2211 i\u2208[n] Qi ) + \u2211 i\u2208[n] \u00b5izi : cic > i Qi +\u00b5iId,0\u2264 \u00b5i \u2264 \u2016ci\u201622,\u2200i\u2208 [n] } . (47)\nFirst, given any feasible solution (\u00b5,{Qi}i\u2208[n]\\(T0\u222aT1)) to the problem (46), let us augment it by setting Qi = 0, \u00b5i = \u2016ci\u201622 for each i \u2208 T0 and Qi = cic>i , \u00b5i = 0 for each i \u2208 T1. Then (\u00b5,{Qi}i\u2208[n]) is feasible to the problem (47) with the same objective value. Thus, we have H\u03021(z)\u2264H1(z).\nOn the other hand, given any feasible solution (\u00b5,{Qi}i\u2208[n]) to the problem (47), then (\u00b5,{Qi}i\u2208[n]\\(T0\u222aT1)) is feasible to the problem (46) a smaller objective value since cic>i Qi +\u00b5i for each i\u2208 T1. Thus, we have H\u03021(z)\u2265H1(z). This completes the proof.\nPart (ii). For any z \u2208Z, let set S denote its support. We then construct a pair of the primal and dual solutions to the maximization problem in (9) and its dual (10) as\nX\u2217 = q1q > 1 ,W \u2217 i =X \u2217,\u2200i\u2208 S,W \u2217i = 0,\u2200i\u2208 [n] \\S, Q\u2217i = cic > i , \u00b5i = 0,\u2200i\u2208 S,Q\u2217i = 0, \u00b5i = ||ci||22,\u2200i\u2208 [n] \\S,\nwhere q1 denote the eigenvector for the largest eigenvalue of matrix \u2211 i\u2208S cic > i .\nAccording to the results in Lemma 1, the above solutions return the same objective value for primal and dual problems, which is \u03bbmax( \u2211 i\u2208S cic > i ). This proves the optimality of the proposed dual solution.\nA.3 Proof of Proposition 2\nProposition 2 The SPCA (2) admits the following MISDP formulation:\n(SPCA) w\u2217 := max z\u2208Z,X\u2208Sn+\n{ tr(AX) : tr(X) = 1,Xii \u2264 zi,\u2200i\u2208 [n] } . (14)\nand its continuous relaxation value is equal to \u03bbmax(A)."
    },
    {
      "heading": "Proof.",
      "text": "(i) To show the equivalence of problem (14) and SPCA (2), we only need to show that for any\nfeasible z \u2208Z with its support S = {i : zi = 1}, we must have\nmax X\u2208Sn+\n{ tr(AX) : tr(X) = 1,Xii \u2264 zi,\u2200i\u2208 [n] } = \u03bbmax(ASS). (48)\nIndeed, since X is a positive semi-definite matrix, thus Xii = 0 for each i\u2208 [n] \\S implies\nXij = 0,\u2200(i, j) /\u2208 S\u00d7S.\nThe left-hand side of (48) is equivalent to\nmax X\u2208Sn+\n{ tr(AX) : tr(X) = 1,Xii \u2264 zi,\u2200i\u2208 [n] } = max X\u2208Sk+ {tr(AS,SX) : tr(X) = 1}= \u03bbmax(ASS),\nwhere the second equality is due to Part (ii) in Lemma 1.\n(ii) The continuous relaxation value of problem (14) is\nw3 = max z\u2208Z,X\u2208Sn+\n{ tr(AX) : tr(X) = 1,Xii \u2264 zi,\u2200i\u2208 [n] } .\nSince tr(X) = 1, thus the linking constraint Xii \u2264 zi is redundant for each i\u2208 [n]. Hence,\nw3 = max X\u2208Sn+\n{ tr(AX) : tr(X) = 1 } = \u03bbmax(A),\nwhere the equality is due to Part (ii) in Lemma 1.\nA.4 Proof of Lemma 2\nLemma 2 The following two inequalities are valid to SPCA (14) (i) \u2211\nj\u2208[n]X 2 ij \u2264Xiizi for all i\u2208 [n]; and (ii) (\u2211 j\u2208[n] |Xij| )2 \u2264 kXiizi for all i\u2208 [n].\nProof. From the proof of Proposition 2, there must exists an optimal solution (z\u2217,X\u2217) of SPCA (14) such that X\u2217 must be rank-one. Thus, without loss of generality, for any feasible solution (z,X) of SPCA (14), we can assume that X =xx>, where (x,z) is also feasible to SPCA (2).\nNext, we split the proof into two parts.\n(i) Since X =xx>, thus \u2211 j\u2208[n] X2ij = \u2211 j\u2208[n] x2ix 2 j = x 2 i \u2264 ziXii,\u2200i\u2208 [n],\nwhere the last inequality follows from the facts that Xii = x 2 i \u2264 zi and zi is binary for each i\u2208 [n].\n(ii) It is known (see, e.g., [15]) that ||x||1 \u2264 \u221a k. Thus,\u2211\nj\u2208[n] |Xij|= \u2211 j\u2208[n] |xi||xj| \u2264 \u221a k|xi| \u2264 \u221a k \u221a Xiizi,\nwhere the second inequality is due to the facts that Xii = x 2 i \u2264 zi and zi is binary for each i\u2208 [n].\nA.5 Proof of Theorem 6\nTheorem 6 Given a threshold > 0, the following MILP is O( )-approximate to SPCA (2), i.e.,\n\u2264 w\u0302( )\u2212w\u2217 \u2264 \u221a d\nw\u0302( ) := max w,z\u2208Z,y,\u03b1,x,,\u03b4,\u00b5,\u03c3 w\ns.t. x= \u03b4i1 + \u03b4i2, ||\u03b4i1||\u221e \u2264 zi, ||\u03b4i2||\u221e \u2264 1\u2212 zi,\u2200i\u2208 [n], x= \u2211 j\u2208[d] \u03c3j, ||\u03c3j||\u221e \u2264 yj, \u03c3jj = yj,\u2200j \u2208 [d], \u2211 j\u2208[d] yj = 1,\nx=\u00b5`1 +\u00b5`2, ||\u00b5`1||\u221e \u2264 \u03b1`, ||\u00b5`2||\u221e \u2264 1\u2212\u03b1`,\u2200`\u2208 [m], w=wU \u2212 (wU \u2212wL) (\u2211\ni\u2208[m]\n2\u2212i\u03b1i ) ,\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2211\ni\u2208[n]\ncic > i \u03b4i1\u2212wUx+ (wU \u2212wL) \u2211 `\u2208[m] 2\u2212`\u00b5`1 \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 \u221e \u2264 ,\n\u03b1\u2208 {0,1}m,y \u2208 {0,1}d,\n(22)\nwhere wL,wU separately denote the lower and upper bounds of SPCA, m := dlog2((wU \u2212wL) \u22121)e and the infinite norm inequality constraints can be easily linearized.\nProof. Throughout the proof, we use indices i\u2208 [n], j \u2208 [d], and `\u2208 [m] to denote the elements of three different dimensional vectors, respectively. To construct the MILP by SPCA (21) and show the approximation accuracy, we split the proof into four steps. Step 1. Linearize the bilinear terms {zix}i\u2208[n] in (21). This can be done by introducing two copies\n\u03b4i1,\u03b4i2 of vector x for each i\u2208 [n] such that\nx= \u03b4i1 + \u03b4i2, ||\u03b4i1||\u221e \u2264 zi, ||\u03b4i2||\u221e \u2264 1\u2212 zi,\u2200i\u2208 [n], \u2211 i\u2208[n] zicic > i x= \u2211 i\u2208[n] cic > i \u03b4i1.\nStep 2. Linearize the nonconvex constraint \u2016x\u2016\u221e = 1. We first observe that due to symmetry,\n\u2016x\u2016\u221e = 1 can be equivalently written as a disjunction with d sets as below\n\u222aj\u2208[d] { x\u2208Rd : xj = 1,\u2016x\u2016\u221e \u2264 1 } .\nNext, for each j \u2208 d, we introduce a binary variable yj = 1 indicating the j-th set is active and 0, otherwise, and then create a copy \u03c3j \u2208Rd of variable x such that\nx= \u2211 j\u2208[d] \u03c3j, ||\u03c3j||\u221e \u2264 yj, \u03c3jj = yj,\u2200j \u2208 [d], \u2211 j\u2208[d] yj = 1,y \u2208 {0,1}d.\nStep 3. Approximate and linearize bilinear term wx. We first approximate variable w using m\nbinary variables \u03b1\u2208Rm with m := dlog2((wU \u2212wL)/ )e. Thus, we have w\u2248wU \u2212 (wU \u2212wL) ( \u2211\n`\u2208[m]\n2\u2212`\u03b1` ) with approximation accuracy at most (wU \u2212 wL)/2m \u2264 . The bilinear term wx is now approximated by\nwx\u2248wUx\u2212 (wU \u2212wL) ( \u2211\n`\u2208[m]\n2\u2212`\u03b1`x ) . (49)\nWith binary variables \u03b1, the resulting bilinear terms {\u03b1`x}`\u2208[m] can be further linearized following the same arguments as Step 2, i.e.,\nx=\u00b5`1 +\u00b5`2, ||\u00b5`1||\u221e \u2264 \u03b1`, ||\u00b5`2||\u221e \u2264 1\u2212\u03b1`,\u2200`\u2208 [m], wUx\u2212 (wU \u2212wL) ( \u2211\n`\u2208[m]\n2\u2212`\u03b1`x ) =wUx\u2212 (wU \u2212wL) \u2211 `\u2208[m] 2\u2212`\u00b5`1.\nStep 4. Finally, following the approximation and linearization results in Step 3, the equality constraint \u2211\ni\u2208[n] cic > i \u03c3i1 =wx in (21) might not hold exactly. Thus we replace the equality\nby the following inequality\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2211 i\u2208[n] cic > i \u03b4i1\u2212wUx+ (wU \u2212wL) \u2211 i\u2208[m] 2\u2212i\u00b5i1 \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 \u221e\n= \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2211 i\u2208[n] cic > i zix\u2212wUx+ (wU \u2212wL) \u2211 i\u2208[m] 2\u2212i\u03b1ix \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 \u221e\n= \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223wx\u2212wUx+ (wU \u2212wL) \u2211 i\u2208[m] 2\u2212i\u03b1ix \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 \u221e \u2264 (wU \u2212wL)/2m \u2264 ,\nwhich holds for any feasible solution of formulation (21).\nFirst, we have w\u0302( )\u2265w\u2217\u2212 since w :=w\u2217\u2212 is feasible to the MILP (22). Moreover, given an optimal solution (x\u0302, z\u0302, w\u0302( )) to the MILP (22), we must have\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2211\ni\u2208[n]\nz\u0302icic > i x\u0302\u2212 w\u0302( )x\u0302 \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 \u221e \u2264\n(\u21d2) min x:\u2016x\u2016\u221e=1 \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2211 i\u2208[n] z\u0302icic > i x\u2212 w\u0302( )x \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 \u221e \u2264 (\u21d2) d\u22121/2 min x:\u2016x\u2016\u221e=1 \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2211 i\u2208[n] z\u0302icic > i x\u2212 w\u0302( )x \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 2 \u2264 (\u21d2) d\u22121/2 min x:\u2016x\u20162\u22651 \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2211 i\u2208[n] z\u0302icic > i x\u2212 w\u0302( )x \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 2 \u2264 (\u21d4) d\u22121/2 min x:\u2016x\u20162=1 \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2211 i\u2208[n] z\u0302icic > i x\u2212 w\u0302( )x \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2223 2 \u2264\nwhere the first implication is due to \u2016x\u0302\u2016\u221e = 1, the second one is due to \u2016x\u2016\u221e \u2265 d\u22121/2\u2016x\u20162 since x \u2208Rd, the third one is because \u2016x\u2016\u221e = 1 implies \u2016x\u20162 \u2265 1, and the equivalence is because of monotonicity and positive homogeneity of the objective function. According to\nthe last inequality, there exists an eigenvalue w of matrix \u2211\ni\u2208[n] z\u0302icic > i such that |w\u0302( )\u2212\nw| \u2264 \u221a d, which further implies that w\u0302( )\u2212w\u2217 \u2264 \u221a d since w\u2264w\u2217.\nA.6 Proof of Theorem 7\nTheorem 7 Given a threshold > 0, by enforcing the binary variables z to be continuous, let w5( ) denote the optimal value of the relaxed MILP formulation (22). Then we have\nw5( )\u2264min { k( \u221a d/2 + 1/2), n/k \u221a d+ (n\u2212 k)( \u221a d/2 + 1/2) } w\u2217+ \u221a d.\nProof. From the proof of Theorem 6, we know that w5( )\u2264 w5(0) + \u221a d. Thus, it is sufficient to show that\nw5(0)\u2264 k( \u221a d/2 + 1/2)w\u2217.\nWe observe that when = 0, the resulting formulation by relaxing binary variables z to be\ncontinuous becomes:\nw5(0) = max w,z\u2208Z,x,\n{\u03b4i1}i\u2208[n],{\u03b4i2}i\u2208[n]\n{ w : \u2211 i\u2208[n] cic > i \u03b4i1 =wx,\u2016x\u2016\u221e = 1,\nx= \u03b4i1 + \u03b4i2, ||\u03b4i1||\u221e \u2264 zi, ||\u03b4i2||\u221e \u2264 1\u2212 zi,\u2200i\u2208 [n] } , (50)\nNext, we split the proof into three steps.\nStep 1. For any feasible solution to problem (50), we have w= || \u2211 i\u2208[n] cic > i \u03b4i1||\u221e\n||x||\u221e = || \u2211 i\u2208[n] cic > i \u03b4i1||\u221e \u2264 \u2211 i\u2208[n] ||cic>i \u03b4i1||\u221e = \u2211 i\u2208[n] ||ci||\u221e|c>i \u03b4i1|\n\u2264 \u2211 i\u2208[n] ||ci||\u221e||ci||1||\u03b4i1||\u221e \u2264 \u2211 i\u2208[n] ||ci||\u221e||ci||1zi \u2264 kmax i\u2208[n] ||ci||\u221e||ci||1,\nwhere the first inequality is due to triangle inequality, the second one is because of Holder\u2019s inequality, the third one is because ||\u03b4i1||\u221e \u2264 zi, and the last one is due to ||ci||\u221e||ci||1 \u2264 maxj\u2208[n] ||cj||\u221e||cj||1 for each i\u2208 [n] and \u2211 i\u2208[n] zi = k.\nStep 2. Now it remains to show that for each i\u2208 [n]\n||ci||\u221e||ci||1 \u2264 \u221a d+ 1\n2 w\u2217.\nLet \u03c2 be a permutation of index set [d] such that ci,\u03c2(1), \u00b7 \u00b7 \u00b7 , ci,\u03c2(d) are sorted in an ascending order. Then we have\nc2i,\u03c2(1) + 1\nd\u2212 1 ( \u2211 j\u2208[2,d] |ci,\u03c2(j)| )2 \u2264 c2i,\u03c2(1) + \u00b7 \u00b7 \u00b7+ c2i,\u03c2(d) = ||ci||22 \u2264w\u2217,\nwhere the first inequality is from the arithmetic and quadratic mean inequality and the second inequality follows from ||ci||22 = \u03bbmax(cic>i )\u2264w\u2217. For ease of exposition, let us introduce v1 = |ci,\u03c2(1)| and v2 = \u2211 j\u2208[2,d] |ci,\u03c2(j)|. Next, let us consider an optimization problem\n\u03bd = max v\u2208R2+\n{ v1(v1 + v2) : v 2 1 + 1/(d\u2212 1)v22 \u2264w\u2217 } , (51)\nwhose optimal value clearly provides an upper bound of ||ci||\u221e||ci||1. To solve (51), we first rewrite v1, v2 as\nv1 = r sin(\u03b8)r, v2 = r \u221a d\u2212 1cos(\u03b8), \u03b8 \u2208 [0, \u03c0/2], r\u2264 \u221a w\u2217.\nIn this way, the objective function (51) is equal to\nv1(v1 + v2) = v 2 1 + v1v2 = r\n2 sin2(\u03b8) + r2 \u221a d\u2212 1 sin(\u03b8) cos(\u03b8) = r2 1\u2212 cos(2\u03b8) 2 + r2 \u221a d\u2212 1sin(2\u03b8) 2\n= r2 2 \u2212 r 2 2 cos(2\u03b8) + 1 2 r2 \u221a d\u2212 1 sin(2\u03b8)\u2264 1 2 r2 +\n\u221a d\n2 r2 \u2264\n\u221a d+ 1\n2 w\u2217,\nwhere the first inequality is due to Cauchy-Schwartz inequality and the second one is because r2 \u2264w\u2217. Thus, we must have\n||ci||\u221e||ci||1 \u2264 \u221a d+ 1\n2 w\u2217.\nThis proves the first bound k( \u221a d/2 + 1/2) together with Step 1.\nStep 3. We now prove the second bound. Plugging the equations \u03b4i1 = x\u2212 \u03b4i2 for all i \u2208 [n], we\nrewrite the continuous relaxation value as w= || \u2211\ni\u2208[n] cic > i (x\u2212 \u03b4i2)||\u221e ||x||\u221e\n\u2264 || \u2211 i\u2208[n] cic > i x||\u221e ||x||\u221e + || \u2211 i\u2208[n] cic > i \u03b4i2||\u221e ||x||\u221e\n\u2264 || \u2211 i\u2208[n] cic > i x||\u221e\n||x||\u221e + (n\u2212 k)\n\u221a d+ 1\n2 w\u2217 \u2264max i\u2208[d] \u2211 j\u2208[d] |Cij|+ (n\u2212 k) \u221a d+ 1 2 w\u2217,\nwhere C :=CC> = \u2211\ni\u2208[n] cic > i and the first inequality is from the triangle inequality, the\nsecond one follows from the derivations in Steps 1 and 2, and the third one is due to xi \u2264 1 for each i\u2208 [d].\nNext, the first term of the right-hand side above can be upper bounded by\nmax i\u2208[d] \u2211 j\u2208[d] |Cij|= ||C||1 \u2264 \u221a d||C||2 = \u221a d\u03bbmax(C)\u2264 n k \u221a dw\u2217,\nwhere the equations are from the definition of `1-norm and `2-norm of a matrix and the second inequality is due to \u03bbmax(C) = \u03bbmax(A)\u2264 n/kw\u2217.\nA.7 Proof of Lemma 3\nLemma 3 Given a matrix A \u2208 Rm\u00d7n, consider its augmented counterpart A defined in (28), two integers k1 \u2208 [m] and k2 \u2208 [n], and three subsets S,S1, S2 \u2286 [m + n] such that S \u2286 [m+n], |S|= k1 + k2, S1 = S \u2229 [m], |S1| = k1 and S2 = S \u2229 [m+ 1,m+ n], |S2| = k2. Then the following identities must hold:\n(i) The eigenvalues of the augmented submatrix AS,S are the singular values of submatrix AS1,S2\nand their negations;\n(ii) \u03c3max(AS1,S2) = \u03bbmax(AS,S) = 1/2maxx\u2208Rk1+k2{x>Ax : ||x1:k1 ||2 = 1, ||xk1+1:k1+k2 ||2 = 1} =\n1/2max X\u2208Sk1+k2+\n{ tr(AS,SX) : \u2211 j\u2208[k1]Xjj = 1, \u2211 i\u2208[k1+1,k1+k2]Xii = 1 } .\nProof. The proof includes two parts.\n(i) By the definition of augmented matrix A in (28), for its submatrix AS,S, we observe that\nAS,S =\n[ 0 AS1,S2\nA>S1,S2 0\n] .\nThen the statement in Part (i) directly follows from the result in Ben-Tal and Nemirovski\n[3], which shows that the eigenvalues of an augmented symmetric matrix exactly are equal to\nthe singular values and negative ones of the original matrix.\n(ii) The first equality \u03bbmax(AS,S) = \u03c3max(AS1,S2) is obtained from Part (i).\nFor the largest singular value of AS1,S2 , we have\n\u03c3max(AS1,S2) = max u\u2208Rk1 ,v\u2208Rk2\n{ u>AS1,S2v : ||u||2 = 1, ||u||2 = 1 } = 1\n2 max x\u2208Rk1+k2\n{ x>AS,Sx : ||x1:k1 ||2 = 1, ||xk1+1:k1+k2 ||2 = 1 } , (52)\nwhich proves the second equality of Part (ii).\nAs for the last equality of Part (ii), we let w\u0302\u2217SVD denote the optimal value of the righthand side SDP problem. Then we must have w\u0302\u2217SVD \u2265 \u03c3max(AS1,S2) as the SDP problem is exactly a SDP relaxation of the maximization problem over x in (52) by relaxing the rank-one\nconstraint. On the other hand, summing up two constraints in the SDP problem, we obtain an upper bound of w\u0302\u2217SVD, i.e.,\nw\u0302\u2217SVD \u2264 1\n2 max\nX\u2208Sk1+k2+\n{ tr(AS,SX) : tr(X) = 2 } = \u03bbmax(AS,S) = \u03c3max(AS1,S2),\nwhere the first equality is due to Part (ii) in Lemma 1.\nA.8 Proof of Theorem 11\nTheorem 11 The continuous relaxation value wSVD1 of formulation (34) satisfies\nw\u2217SVD \u2264wSVD1 \u2264 \u221a mnk\u221211 k \u22121 2 w \u2217 SVD.\nProof. For the matrix A # defined in (32), using Part (i) in Lemma 3, we can derive that its largest eigenvalue is equal to 2\u03c3max(A). Let (z\u0302,X\u0302,W\u03021, \u00b7 \u00b7 \u00b7 ,W\u0302m+n) denote an optimal solution to the continuous SDP relaxation of problem (34). We now have\n2\u03c3max(A) = \u03bbmax\n( A # )\n= max X 0,tr(X)=1 { \u2211 i\u2208[m+n] c>i Xci } \u2265 \u2211 i\u2208[m+n] c>i X\u0302ci \u2265 \u2211 i\u2208[m+n] c>i W\u0302ici,\nwhere the last inequality is because X\u0302 W\u0302i for each i \u2208 [m+ n]. Note that the right-hand side above is equal to wSVD1 +\u03c3max(A) and the inequalities above lead to\nwSVD1 = \u2211\ni\u2208[m+n]\nc>i W\u0302ici\u2212\u03c3max(A)\u2264 2\u03c3max(A)\u2212\u03c3max(A) = \u03c3max(A).\nNow it remains to show that Claim 1 \u03c3max(A)\u2264 \u221a mnk\u221211 k \u22121 2 w \u2217 SVD.\nProof. Let u1, v1 denote the top right and left eigenvectors of A, i.e., u > 1Av1 = \u03c3max(A),Av1 = \u03c3max(A)v1,u > 1A= \u03c3max(A)u > 1 . We tailor u1, v1 to meet the feasibility of R1-SSVD (27) as below\nu\u0302j1 = { uj1, if uj1 is one of the k1 largest entries of u1 0, otherwise ,\u2200j \u2208 [n],\nv\u0302j1 = { (A>u\u0302)j, if |(A>u\u0302)j| is one of the k2 largest entries of |A>u\u0302| 0, otherwise ,\u2200j \u2208 [m].\nLet us normalize u\u03021 = u\u03021 ||u\u03021||2 and v\u03021 = v\u03021 ||v\u03021||2 . Clearly, (u\u03021, v\u03021) is feasible R1-SSVD (27). Then we\nhave \u221a k1 n \u03c3max(A)\u2264 \u03c3max(A)u\u0302>1 u1 = u\u0302>1Av1 \u2264 \u2016u\u0302>1A\u20162 \u2264 \u221a m k2 u\u0302>1Av\u03021 \u2264 \u221a m k2 w\u2217SVD,\nwhere the first inequality is due to the definition of u\u03021, the equality is because of the definition of v1, the second inequality is due to the Cauchy-Schwartz inequality, the third one is based on the choice of v\u03021, and the last one is due to the feasibility of (u\u03021, v\u03021). This completes the proof.\nA.9 Proof of Lemma 4\nLemma 4 For R1-SSVD (35), the following second-order conic inequalities are valid: (i) \u2211\nj\u2208[m]X 2 ij \u2264 ziXii, \u2211 j\u2208[m+1,m+n]X 2 ij \u2264 ziXii for all i\u2208 [m+n]; and\n(ii) ( \u2211 j\u2208[m] |Xij|)2 \u2264 k1Xiizi, ( \u2211 j\u2208[m+1,m+n] |Xij|)2 \u2264 k2Xiizi for all i\u2208 [m+n].\nProof. According to Proposition 7, there must exist an optimal solution (z\u2217,X\u2217) to MISDP (35) such that X\u2217 is rank-one. Thus, without loss of generality, for any feasible solution (z,X) of SPCA\n(14), we can assume that X = [ u v ][ u v ]> , where vectors (u,v) thus satisfy\n||u||2 = ||v||2 = 1, ||u||1 \u2264 \u221a k1, ||v||1 \u2264 \u221a k2.\nThen the rest of the proof is almost identical to that of Lemma 2 and is thus omitted for brevity.\nA.10 Proof of Theorem 15\nTheorem 15 For R1-SSVD (27), the truncation algorithm yields an approximation ratio\nmax {\u221a k\u221211 , \u221a k\u221212 , \u221a k1k2m\u22121n\u22121 } ."
    },
    {
      "heading": "In particular, the approximation ratio is O(n\u22121/3) when k1 \u2248 k2 and m\u2248 n.",
      "text": "Proof. We derive the three approximation ratios of the truncation algorithm below.\n(i) According to the truncation in the standard basis, the obtained vector u\u0302i is feasible to the\nR1-SSVD problem for each i\u2208 [n] and is also optimal to the following problem\nu\u0302i \u2208 arg max ||ui||2=1,||ui||0=k1\n{ u>i Aei } ,\u2200i\u2208 [n].\nSuppose the optimal solution of the R1-SSVD (27) to be u\u2217 and v\u2217, let S\u22171 , S \u2217 2 denote their supports, respectively. We then rewrite v\u2217 = \u2211\ni\u2208S\u22172 v\u2217i ei and we have\nw\u2217SVD = (u \u2217)>Av\u2217 = \u2211 i\u2208S\u22172 v\u2217i (u \u2217)>Aei \u2264 \u221a\u2211 i\u2208S\u22172 (v\u2217i ) 2 \u221a\u2211 i\u2208S\u22172 [(u\u2217)>Aei]2 \u2264 \u221a k2 max i\u2208[n] u\u0302>i Aei,\nwhere the first inequality is due to Cauchy-Schwartz and the second one is because of maximality of maxi\u2208[n] u\u0302 > i Aei.\nSince (1\u2212 (k2 \u2212 1) )ei + \u2211\nj\u2208[k2]\u222a{i}\\{i} ej with sufficiently small > 0 is feasible to R1-\nSSVD (27), thus the right-hand side above is an lower bound of R1-SSVD according to the continuity by letting \u2192 0. This prove the approximation ratio \u221a k\u221212 .\nSimilarly, we can derive\nw\u2217SVD = (u \u2217)>Av\u2217 \u2264 \u221a k1 max\nj\u2208[m] e>j Av\u0302j,\nwhich prove the approximation ratio \u221a k\u221211 .\n(ii) Following the proof of Claim 1, for the truncation in the eigen-space basis, we have\u221a k1 n w\u2217SVD \u2264 \u221a k1 n \u03c3max(A)\u2264 \u03c3max(A)u\u0302>1 u1 = u\u0302>1Av1 \u2264 \u2016u\u0302>1A\u20162 \u2264 \u221a m k2 u\u0302>1Av\u03021,\nwhich proves the approximation ratio of \u221a k1k2m\u22121n\u22121.\nA.11 Proof of Theorem 16\nTheorem 16 For the greedy Algorithm 3 and the local search Algorithm 4, we have (i) both algorithms achieve a ( \u221a k1k2) \u22121-approximation ratio of R1-SSVD (37), and (ii) the ratio is tight.\nProof. The proof is split into two parts.\n(i) In R1-SSVD (37), according to the part (i) of the proof of Theorem 15, we have\nw\u2217SVD \u2264 \u221a k2 max\nj\u2208[n] u\u0302>j Aej \u2264\n\u221a k1k2 max\ni\u2208[m],j\u2208[n] Aij,\nwhere vectors {u\u0302i}i\u2208[n] \u2286 Rm are obtained by the normalized k1-truncation in the standard basis of A. Then, following the similar analyses of Theorem 8 and Theorem 9, the largest singular value from greedy Algorithm 3 and local search Algorithm 4 must be lower bounded by maxi\u2208[m],j\u2208[n]Aij.\n(ii) We next show an example in which the ratio \u221a k\u221211 k \u22121 2 can be achieved. Suppose that, without\nloss of generality, k1 \u2264 k2. Then, consider m= 2k2, n= 2k2, and matrix A\u2208Rm\u00d7n as\nA :=\n[ Ik2 0k2\u00d7k2\n0k2\u00d7k2 1k2\u00d7k2\n] .\nAbove, the submatrix A[k1],[k2] satisfies greedy and local optimality conditions with the objective value equal to 1, while the best size k1\u00d7 k2 submatrix is A[k2+1,k2+k1],[k2+1,2k2] with the optimal value \u221a k1k2."
    }
  ],
  "title": "Exact and Approximation Algorithms for Sparse PCA",
  "year": 2020
}
