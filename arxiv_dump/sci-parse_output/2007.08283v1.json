{
  "abstractText": "Interpretable Machine Learning (IML) methods are used to gain insight into the relevance of a feature of interest for the performance of a model. Commonly used IML methods differ in whether they consider features of interest in isolation, e.g., Permutation Feature Importance (PFI), or in relation to all remaining feature variables, e.g., Conditional Feature Importance (CFI). As such, the perturbation mechanisms inherent to PFI and CFI represent extreme reference points. We introduce Relative Feature Importance (RFI), a generalization of PFI and CFI that allows for a more nuanced feature importance computation beyond the PFI versus CFI dichotomy. With RFI, the importance of a feature relative to any other subset of features can be assessed, including variables that were not available at training time. We derive general interpretation rules for RFI based on a detailed theoretical analysis of the implications of relative feature relevance, and demonstrate the method\u2019s usefulness on simulated examples.",
  "authors": [
    {
      "affiliations": [],
      "name": "Gunnar K\u00f6nig"
    },
    {
      "affiliations": [],
      "name": "Christoph Molnar"
    },
    {
      "affiliations": [],
      "name": "Bernd Bischl"
    },
    {
      "affiliations": [],
      "name": "Moritz Grosse-Wentrup"
    }
  ],
  "id": "SP:7f4e58e4e0066e287e630e80a35d538d9ce326b0",
  "references": [
    {
      "authors": [
        "Philip Adler",
        "Casey Falk",
        "Sorelle A. Friedler",
        "Tionney Nix",
        "Gabriel Rybeck",
        "Carlos Scheidegger",
        "Brandon Smith",
        "Suresh Venkatasubramanian"
      ],
      "title": "Auditing black-box models for indirect influence",
      "venue": "Knowledge and Information Systems,",
      "year": 2018
    },
    {
      "authors": [
        "Rina Foygel Barber",
        "Emmanuel J Cands",
        "others"
      ],
      "title": "Controlling the false discovery rate via knockoffs",
      "venue": "The Annals of Statistics,",
      "year": 2015
    },
    {
      "authors": [
        "Solon Barocas",
        "Moritz Hardt",
        "Arvind Narayanan"
      ],
      "title": "Fairness and Machine Learning",
      "venue": "fairmlbook.org,",
      "year": 2019
    },
    {
      "authors": [
        "Vence L Bonham",
        "Shawneequa L Callier",
        "Charmaine D Royal"
      ],
      "title": "Will precision medicine move us beyond race",
      "venue": "The New England journal of medicine,",
      "year": 2016
    },
    {
      "authors": [
        "David V Budescu"
      ],
      "title": "Dominance analysis: a new approach to the problem of relative importance of predictors in multiple regression",
      "venue": "Psychological bulletin,",
      "year": 1993
    },
    {
      "authors": [
        "Emmanuel Cands",
        "Yingying Fan",
        "Lucas Janson",
        "Jinchi Lv"
      ],
      "title": "Panning for gold: model-X knockoffs for high dimensional controlled variable selection",
      "venue": "Journal of the Royal Statistical Society. Series B: Statistical Methodology,",
      "year": 2018
    },
    {
      "authors": [
        "Ian Covert",
        "Scott Lundberg",
        "Su-In Lee"
      ],
      "title": "Understanding Global Feature Contributions Through Additive Importance Measures",
      "venue": "arXiv preprint arXiv:2004.00668,",
      "year": 2020
    },
    {
      "authors": [
        "Jeffrey (Reuters"
      ],
      "title": "Dastin. Amazon scraps secret AI recruiting tool that showed bias against women",
      "year": 2018
    },
    {
      "authors": [
        "Lydia de la Torre"
      ],
      "title": "A Guide to the California Consumer Privacy Act of 2018",
      "venue": "SSRN Electronic Journal,",
      "year": 2018
    },
    {
      "authors": [
        "Julia Dressel",
        "Hany Farid"
      ],
      "title": "The accuracy, fairness, and limits of predicting recidivism",
      "venue": "Science Advances,",
      "year": 2018
    },
    {
      "authors": [
        "Aaron Fisher",
        "Cynthia Rudin",
        "Francesca Dominici"
      ],
      "title": "All models are wrong, but many are useful: Learning a variable\u2019s importance by studying an entire class of prediction models simultaneously",
      "venue": "Journal of Machine Learning Research,",
      "year": 2019
    },
    {
      "authors": [
        "Ulrike Grmping"
      ],
      "title": "Variable importance assessment in regression: linear regression versus random forest",
      "venue": "The American Statistician,",
      "year": 2009
    },
    {
      "authors": [
        "Giles Hooker",
        "Lucas Mentch"
      ],
      "title": "Please Stop Permuting Features: An Explanation and Alternatives",
      "venue": "arXiv preprint arXiv:1905.03151v,",
      "year": 1905
    },
    {
      "authors": [
        "Jing Lei",
        "Max GSell",
        "Alessandro Rinaldo",
        "Ryan J Tibshirani",
        "Larry Wasserman"
      ],
      "title": "Distribution-free predictive inference for regression",
      "venue": "Journal of the American Statistical Association,",
      "year": 2018
    },
    {
      "authors": [
        "Stan Lipovetsky",
        "Michael Conklin"
      ],
      "title": "Analysis of regression in game theory approach",
      "venue": "Applied Stochastic Models in Business and Industry,",
      "year": 2001
    },
    {
      "authors": [
        "Scott M. Lundberg",
        "Su In Lee"
      ],
      "title": "A unified approach to interpreting model predictions",
      "venue": "Advances in Neural Information Processing Systems,",
      "year": 2017
    },
    {
      "authors": [
        "Christoph Molnar",
        "Gunnar K\u00f6nig",
        "Bernd Bischl",
        "Giuseppe Casalicchio"
      ],
      "title": "Model-agnostic feature importance and effects with dependent features\u2013a conditional subgroup approach",
      "venue": "arXiv preprint arXiv:2006.04628,",
      "year": 2020
    },
    {
      "authors": [
        "Christoph Molnar",
        "Gunnar K\u00f6nig",
        "Bernd Bischl",
        "Giuseppe Casalicchio"
      ],
      "title": "Model-agnostic feature importance and effects with dependent features-a conditional subgroup approach",
      "venue": "arXiv preprint arXiv:2006.04628,",
      "year": 2020
    },
    {
      "authors": [
        "Christoph Molnar",
        "Gunnar K\u00f6nig",
        "Julia Herbinger",
        "Timo Freiesleben",
        "Susanne Dandl",
        "Christian A. Scholbeck",
        "Giuseppe Casalicchio",
        "Moritz Grosse-Wentrup",
        "Bernd Bischl"
      ],
      "title": "Pitfalls to avoid when interpreting machine learning models",
      "year": 2007
    },
    {
      "authors": [
        "Judea Pearl",
        "Azaria Paz"
      ],
      "title": "Graphoids: A graph-based logic for reasoning about relevance relations. University of California (Los Angeles)",
      "venue": "Computer Science Department,",
      "year": 1985
    },
    {
      "authors": [
        "Yaniv Romano",
        "Matteo Sesia",
        "Emmanuel"
      ],
      "title": "Cands. Deep knockoffs",
      "venue": "Journal of the American Statistical Association,",
      "year": 2019
    },
    {
      "authors": [
        "Rajen D Shah",
        "Jonas Peters"
      ],
      "title": "The hardness of conditional independence testing and the generalised covariance measure",
      "venue": "arXiv preprint arXiv:1804.07203,",
      "year": 2018
    },
    {
      "authors": [
        "Carolin Strobl",
        "Anne Laure Boulesteix",
        "Thomas Kneib",
        "Thomas Augustin",
        "Achim Zeileis"
      ],
      "title": "Conditional variable importance for random forests",
      "venue": "BMC Bioinformatics,",
      "year": 2008
    },
    {
      "authors": [
        "Laura Tolo\u00c5i",
        "Thomas Lengauer"
      ],
      "title": "Classification with correlated features: unreliability of feature ranking and solutions",
      "year": 1986
    },
    {
      "authors": [
        "Eric J Topol"
      ],
      "title": "High performance medicine: the convergence of human and artificial intelligence",
      "venue": "Nature Medicine,",
      "year": 2019
    },
    {
      "authors": [
        "Eugene Tuv",
        "Alexander Borisov",
        "George Runger",
        "Kari Torkkola"
      ],
      "title": "Feature selection with ensembles, artificial variables, and redundancy elimination",
      "venue": "Journal of Machine Learning Research,",
      "year": 2009
    },
    {
      "authors": [
        "Paul Voigt",
        "Axel dem Bussche"
      ],
      "title": "The eu general data protection regulation (gdpr)",
      "venue": "A Practical Guide,",
      "year": 2017
    },
    {
      "authors": [
        "David S. Watson",
        "Marvin N. Wright"
      ],
      "title": "Testing Conditional Independence in Supervised Learning Algorithms",
      "venue": "arXiv preprint arXiv:1901.09917,",
      "year": 2019
    },
    {
      "authors": [
        "Pengfei Wei",
        "Zhenzhou Lu",
        "Jingwen Song"
      ],
      "title": "Variable importance analysis: a comprehensive review",
      "venue": "Reliability Engineering & System Safety,",
      "year": 2015
    },
    {
      "authors": [
        "Yufei Xia",
        "Chuanzhe Liu",
        "Yu Ying Li",
        "Nana Liu"
      ],
      "title": "A boosted decision tree approach using Bayesian hyper-parameter optimization for credit scoring",
      "venue": "Expert Systems with Applications,",
      "year": 2017
    },
    {
      "authors": [
        "Erik \u00c5trumbelj",
        "Igor Kononenko"
      ],
      "title": "Explaining prediction models and individual predictions with feature contributions",
      "venue": "Knowledge and information systems,",
      "year": 2014
    }
  ],
  "sections": [
    {
      "text": "Index Terms\u2014feature importance, interpretable machine learning, explainable artificial intelligence, causality\nI. Introduction\nPredictive modelling is increasingly deployed in highstakes environments, e.g., in the criminal justice system [11], loan approval [32], recruiting [9] and medicine [27]. Due to legal regulations [10], [29] and ethical considerations, ML methods need not only perform robustly in such environments but also be able to justify their recommendations in a human-intelligible fashion. This development has given rise to the field of interpretable machine learning (IML) that involves studying methods that provide insight into the relevance of features for model performance, referred to as feature importance. Prominent feature importance techniques include permutation feature importance (PFI) [5], [12] and conditional feature importance (CFI) [12], [19], [25]. PFI is based on replacing the feature of interest X j with a perturbed version sampled from the marginal distribution P(X j) while CFI perturbs X j such that the conditional distribution with respect to the set R of remaining features P(X j|XR) is preserved. The sampling strategy defines the method\u2019s reference point and therefore affects the method\u2019s implicit notion of relevance. While PFI quantifies the overall reliance of the model on the feature of interest, CFI quantifies its unique contribution given\nThis work is funded by the German Federal Ministry of Education and Research (BMBF) under Grant No. 01IS18036A and supported by the Bavarian State Ministry of Science and the Arts in the framework of the Centre Digitisation.Bavaria (ZD.B). The authors of this work take full responsibility for its content.\nall remaining features. While both PFI and CFI are useful, they fail to answer more nuanced questions of feature importance. For instance, a stakeholder may be interested in the importance of a feature relative to a subset of features. Also, the user may want to know how important a feature is relative to variables that had not been available at training time. We suggest relative feature importance (RFI) as a generalization of PFI and CFI that moves beyond the dichotomy between PFI, which breaks all dependencies with features, and CFI, which preserves all dependencies with features. In contrast to PFI and CFI, RFI is based on a perturbation that is restricted to preserve the relationships with a set of variables G that can be chosen arbitrarily. We show that RFI is (1) semantically meaningful and (2) practically useful. We demonstrate the semantical meaning of RFI in Section IV. In particular, we derive general interpretation rules that link nonzero RFI to (1) the conditional dependence of the feature of interest with the target and nonconditioned features XR given the conditioned variables XG in the data and (2) the conditional dependence of the input to the feature of interest X j with the model\u2019s prediction Y\u0302 given fixed inputs to the remaining features XR (Theorem 1). Furthermore, we show that a nonzero difference between RFIGj and RFI G\u222aN j , with N being an arbitrary set disjunct with G, implies the conditional dependence X j 6y XN |XG (Theorem 2). In Section V, we provide an implementation of RFI estimation that is based on recent results from the related knockoff research field [7], [23]. Furthermore, we translate the testing framework developed for conditional feature importance [30] to RFI. We support our theoretical analysis and findings by various simulation studies in Section VI. In particular, we show that RFI can expose the indirect contribution of variables that are not directly used by the model but provide information via dependent variables (Section VI-A). Similarly, we show how RFI can be used to assess feature importance with respect to variables not included at training time (Section VI-B)."
    },
    {
      "heading": "A. Contributions and Related Work",
      "text": "While conditioning on subsets of variables has been suggested before [12], [25], the implications of this generalized variant of CFI have not yet been rigorously analyzed. Some IML methods perturb or hide subsets\nar X\niv :2\n00 7.\n08 28\n3v 1\n[ st\nat .M\nL ]\n1 6\nJu l 2\n02 0\nof features, e.g., in the context of multiple regression relative importance analysis is a model-specific technique that averages over all importances of models trained on feature subsets [6], [16]. Model-agnostic, local approximations to the respective feature effect that avoid retraining and instead perturb subsets of features have also been proposed [17], [33]. A very recent global, modelagnostic feature importance proposal called SAGE quantifies feature importance by perturbing multiple features [8]. While the aforementioned approaches are all based on removing several features to provide more nuanced insight into the model, our proposal only modifies the feature of interest. Our approach is model-agnostic and global, while most aforementioned approaches are model-specific or local. The exception is the global, model-agnostic SAGE [8], however the approaches are not only computationally but also semantically different. E.g. our method assigns an importance of zero for features that are not used by the model1, which is not the case for SAGE. While our approach aims to provide nuanced insights into variable importance relative to a specific set, SAGE aims to quantify the overall importance of variables for the model. Feature importance relative to variables that have not been included in the training set has not been studied before. The indirect influence of variables that the model does not computationally rely but statistically depend on has been studied e.g. in [1].\nII. Background and Notation A. Notation\nWe denote the target variable, i.e., the variable the model predicts, as Y and feature variables by X(.). We refer to the variables as features to emphasize when they were used in model training. Their observations are denoted by y and x(.). We use D := {1, . . . , p} for the index set of all features included in model training and j for the index of our feature of interest, X j. The index set of the remaining variables is denoted as R := D\\{ j} (rest, remainder). The index set of features, relative to which the importance of X j is considered, is denoted as G. As G can refer to any index set of variables, we denote its intersection with R as G = R \u2229 G and its complement as R = R\\G. We denote the index set of\n1A proof of this property is given in Lemma 2.\nconditioning variables that were not made available to the model during training as G\u2217 = G\\R. In case we add new elements to the conditioning set G, we will denote this set as N. The set may include variables within and outside D. The respective components are denoted as N\u2217 = N\\R and as N = R \u2229 N. The remainder of R without G and N is denoted as R = R\\N. We denote perturbed variables of interest relative to G as X\u0303Gj . We refer to the original and perturbed probability distribution of X j as the observational and interventional distribution P(X j, . . . ) and P(X\u0303Gj , . . . ). The inspected model is denoted as f , its prediction as Y\u0302. Independence of Y and X conditional on Z is denoted using X y Y|Z, the respective conditional dependence as X 6y Y|Z."
    },
    {
      "heading": "B. Feature Importance",
      "text": "Performance-based feature importance methods assess the relevance of a feature of interest X j by assessing the impact of a perturbation of X j on the model\u2019s performance. Local feature importance methods focus on the importance of features for specific data points, whereas global feature importance methods assess the impact over the whole domain. In the following, we focus on global methods. Global feature importance is computed according to the following general schemata:\nFI j = R\u0303 j \u2212 R or FI j = R\u0303 j R\nwhere we denote the original risk of the model and the risk after perturbing X j as R and R\u0303 j, respectively. For estimation, the true risk R is replaced with the empirical risk Remp. Feature importance methods furthermore differ in how they perturb and whether they rely on retraining the model. While some methods retrain the model after the perturbation (e.g. LOCO, [15]), others evaluate the impact of the perturbation on the same original model (e.g. [5], [25]). In this work, we focus on methods that avoid retraining. For methods that avoid retraining, we observe a dichotomy between two general perturbation approaches: resampling that preserves the marginal and resampling that preserves the conditional distribution. Marginal resampling was originally proposed to compute perturbed versions of X j by permuting the observations x(i)j within the sample [5]. The respective sample breaks the dependence between X j and (Y,XR) while preserving the marginal distribution P(X j). More recently, Model Reliance was proposed [12], which takes the expectation over all possible permutations. Resampling from the marginal distribution has been criticized to introduce bias, in particular because it overestimates the importance of correlated variables\n[25], resulting in incorrect feature rankings [26]. It also leads to extrapolation under dependent features [14], [19], i.e. conclusions about the model are being drawn using unrealistic data points on which the model was not trained. CFI, on the other hand, samples from the conditional distribution P(X j|XR) [2], [7], [12], [14], [19], [25], [28]. A large variety of model-specific methods exist [13], [31]. Conditional variants quantify the importance of a feature given the information that all remaining features R contain about X j [20], thereby avoiding evaluation of the model on unrealistic datapoints [19].\nIII. Relative Feature Importance\nRelative Feature Importance is a general framework that assesses feature importance relative to arbitrary variable sets G. The frameworks subsumes PFI and CFI as two extreme special cases. In PFI, X j is replaced with a perturbed version that preserves the marginal distribution P(X j) while breaking the dependencies with Y and all features. In CFI, a perturbed version of X j is used that preserves the conditional distribution P(X j|XR), thereby only breaking conditional dependence between X j and Y given all features. As our analysis in Section IV establishes, the replacement strategies of PFI and CFI define extreme reference points. CFI quantifies the contribution relative to all remaining features R, whereas PFI regards a feature in isolation. We go beyond the PFI versus CFI dichotomy. We argue that it is (1) meaningful (Section IV) and (2) practically useful (Section VI) to replace X j with perturbed versions that preserve the conditional distribution P(X j|XG) with respect to arbitrary sets G while requiring X\u0303Gj y (XR,Y)|XG. G can be a subset of R, but can also include variables not available at training time such that G\\R , \u2205. We term the resulting method Relative Feature Importance (RFI):\nDefinition 1 (Relative Feature Importance \u2013 RFI): We define Relative Feature Importance with respect to a feature set G with Y < G and a fixed model f as\nRFIGj := R\u0303 j|G \u2212 R,\nwhere R\u0303 j|G := R(Y, f (XR, X\u0303Gj )) is the risk w.r.t. to a replacement variable X\u0303Gj and R = R(Y, f (X j,XR)) refers to the original risk. The replacement variable has to satisfy\n\u2022 X\u0303Gj \u223c P(X j|XG) and \u2022 X\u0303Gj y (XR,Y)|XG.\nIn the following section, we discuss the semantic meaning of RFI. The estimation of RFI is discussed in Section V.\nIV. Interpreting Relative Feature Importance IML techniques aim to provide insight into the model and, possibly, into the underlying data generating mechanism. However, IML techniques themselves are subject to interpretation. The characterization of an IML method by its mathematical definition is computationally precise, but has limited aid in guiding users to make conclusions about the underlying model and data. In this section we provide a (noncomprehensive) list of interpretation rules for RFI, that characterize the method by how it behaves in its context. This context includes both the model and the underlying data generating mechanism. More specifically, we link RFI to (conditional) independence in the underlying data set as well as to whether the model\u2019s prediction Y\u0302 is constant in the argument x j for a fixed xR. While RFI can be used for quantification of feature importance, we focus our analysis on relevance as a binary property and characterize relative feature relevance (RFI , 0). We show that the implicit notion of relevance of RFI is defined by the choice of G. By modifying the conditioning set G beyond the PFI versus CFI dichotomy, we are able to gain insight into more nuanced aspects of the model and the data generating mechanism. The main results are given in Theorem 1 and Theorem 2. Furthermore, we highlight limitations stemming from the choice of the loss function L and the model fit for the interpretation, which are, in our humble opinion, underrepresented in the current discussion. We structure our analysis by taking the user\u2019s perspective and asking \u201dWhat can we infer from relative feature relevance?\u201d.\nA. Implications of Relative Feature Relevance In the following, we analyze the implications of RFI without further assumptions about model and data. We thereby distinguish between two levels of explanation. Relative feature relevance provides insight, both into model and data.\nTheorem 1: If RFIGj , 0 then \u2022 X j 6y (Y,XR)|XG in the underlying distribution (data\nlevel) \u2022 X\u0303 j 6y Y\u0302|XR w.r.t. the interventional distribution\nP(X j|XG)P(XG,XR) > 0 (model level)\nWe prove Theorem 1 in two steps. First, we assess the implications of the respective independence for the underlying data set (Lemma 1). Then, we assess the implications of the respective independence for the model (Lemma 2). The contrapositions yield Theorem 1.\nLemma 1: If X j y (Y,XR)|XG for any G with Y < G then RFIGj = 0.\nWe base the proof of Lemma 1 on the insight that (because the model f is fixed) an equivalence in distribution implies an equivalence in risk (Proposition 1). Therefore conditions under which the interventional distribution P(X\u0303Gj ,XR,Y) coincides with the original distribution P(X j,XR,Y) are sufficient for RFI = 0.\nProposition 1: If observational and interventional distribution coincide, then risks with and without perturbation are equal:\nP(Y,X j,XR) = P(Y, X\u0303Gj ,XR)\u21d2 R( f ) = R\u0303 j|G( f )\nProof of Proposition 1: Given that P(Y,X j,XR) = P(Y, X\u0303 j,XR) we can write\nR( f ) = EY,X j,XR [L(Y, f (X j,XR))] = EY,X\u0303 j,XR [L(Y, f (X\u0303 j,XR))] = R\u0303( f ).\nWe show next that the conditional independence X j y (XR,Y)|XG is a sufficient condition for identity of both distributions.\nProof of Lemma 1: It holds that\nP(Y,X j,XR,XG) = P(X j|Y,XR,XG)P(Y,XR,XG) Xj y (XR ,Y)|XG\n= P(X j|XG)P(Y,XR,XG) (def) = P(X\u0303Gj |XG)P(Y,XR,XG) = P(X\u0303Gj ,Y,XR,XG).\nUsing Proposition 1 we can infer that RFIGj = 0.\nSo far, we have assessed implications for the underlying data generating mechanism. Next, we assess implications for the inspected model f .\nLemma 2: If X\u0303Gj y Y\u0302|XR w.r.t. the interventional distribution P(X\u0303Gj ,XG,XR) then RFI G j = 0 for any G.\nProof of Lemma 2: If the prediction for an observation (x1, . . . , xp) is independent of the value x\u2032j w.r.t. the interventional distribution, the prediction is unaffected when replacing x j with any value x\u2032j with P(x \u2032 j|XG = xG)P(XG = xG,XR = xR) > 0. Consequently, any sample from X\u0303Gj yields the same prediction. Furthermore values x\u2032j with nonzero probability over the interventional distribution also have nonzero probability over the observational distribution. The interventional distribution can be rewritten as\nP(X\u0303Gj ,XG,XR) = P(X\u0303 G j |XG,XR)P(XG,XR)\n= P(X\u0303Gj |XG)P(XG,XR) = P(X j|XG)P(XG,XR).\nSimilarly, the observational distribution can be factorized into P(X j|XG,XR)P(XG,XR). As P(X j|XG,XR) > 0 \u21d2 P(X j|XG) > 0 (which can be derived from, e.g., the law of total probability) it follows that P(X\u0303Gj ,XG,XR) > 0 \u21d2 P(X j,XG,XR) > 0. Consequently the prediction y\u0302 for any value x j with positive probability P(X j = x j|XR = xR) is identical given unchanged xR. As the conditional distributions of X j and X\u0303Gj overlap and the distribution of XR is unaffected, the prediction Y\u0302 is identical with and without perturbation. Therefore R = R\u0303 j|G and RFIGj = 0.\nTo summarize, we have shown that independence on the dataset and on the model level respectively imply RFIGj = 0 and can thereby prove Theorem 1.\nProof of Theorem 1: The result follows from contraposition of Lemma 1 and contraposition of Lemma 2.\nTheorem 1 shows that nonzero RFIGj implies dependencies between sets of variables on the model level as well as on the data level. Which dependencies are relevant for RFIGj can be controlled with the conditioning set G. Consequently, the conditioning set G determines the method\u2019s implicit definition of relevance. I.e., on the data level, if X j y (XR,Y)|XG holds, RFIGj is zero irrespective of any other dependencies that may hold, e.g. with XG (Lemma 1). Nonzero RFI, a difference in performance on interventional and observational distribution, can only be caused by dependencies that have been destroyed in the interventional distribution, the dependencies with and via XG are preserved by the replacement X\u0303Gj and can therefore not be responsible for RFIGj , 0. Similarly, on the model level, X\u0303 G j y Y\u0302|XR over the interventional distribution P(X j|XG)P(XG,XR) yields zero RFI (Lemma 2). The behavior of the model outside the domain in which it is evaluated is irrelevant for RFIGj . What domain the model is evaluated over depends on the choice of G. Because we can control RFI\u2019s implicit definition of relevance with G, RFI allows more nuanced insights into model and data than PFI or CFI alone. In Theorem 1, we aim to make the implicit definition of relevance explicit. On the data level, nonzero RFI implies the dependence of X j with the tuple (Y,XR) given XG (X j 6y (Y,XR)|XG). In order to understand the aforementioned dependence, using the graphoid axioms contraction and weak union [22], the equivalent formulation below can be adduced:\n(X j 6y Y|XG) \u2228 (X j 6y XR|XG,Y).\nAt least one of the two conditional dependencies has to hold for nonzero RFIGj . The first dependence can be rephrased as: X j is informative of Y, even if we already know XG. It is more difficult to make sense of the second\ndependence. Under dependent features (X j 6y XR|XG,Y), the distribution of X j with XR is not preserved under perturbation X\u0303Gj . In the interventional distribution P(X\u0303Gj ,XR) observations that are improbable or impossible w.r.t. the observational distribution P(X j,XR) can be possible and probable (and vice versa). Consequently, in the interventional distribution the feature distribution differs from the observation feature distribution. Even if X j y Y|XG holds, the model may perform suboptimally due to this distribution shift and cause RFIGj nonzero\n2. If the conditioning set is a superset of R (G \u2287 R), such that set of remaining variables XR is empty, it holds that (X j y XR|XG,Y). Therefore nonzero RFI must be attributed to (X j 6y Y|XG) for G \u2287 R. On the model level, nonzero RFI implies that the model\u2019s predictions are conditionally dependent on X\u0303Gj given the remaining features R are fixed. E.g. for a linear model that has coefficient zero for all terms involving X j, this dependence would not be fulfilled, and RFIGj would be zero (Lemma 2). The model is evaluated over the interventional distribution P(X j|XG)P(XG,XR) > 0, which varies depending on G. If G contains a nearly perfect correlate of X j, X j can be reconstructed well. In contrast, if G = \u2205, for every possible xR the model is evaluated over the whole marginal distribution of X j. Although choosing a smaller set G \u2282 R leads to extrapolation under dependent features, it allows more insight into the model\u2019s mechanism. For interpretation purposes like safety, this is highly desirable. In the preceding paragraphs we have highlighted the importance of the conditioning set G for the method\u2019s implicit notion of relevance and illustrated the results from Theorem 1. We have argued that the conditioning set controls which potential dependencies can be responsible for nonzero RFIGj . The insights lead to a further, interesting application of RFI. By assessing the difference \u2206RFIG\u2192G\u222aNj = RFI G j \u2212 RFIG\u222aNj when modifying the conditioning set G by adding new elements N, we are able to assess the role of the dependencies with variables in N relative to a baseline G. While for RFIGj only dependencies of X j with and via G are preserved, for RFIG\u222aNj also dependencies with and via N are maintained. If \u2206RFIG\u2192G\u222aNj is nonzero, this change has to be due to dependencies involving N, but not G. We substantiate this claim with Theorem 2. In order for \u2206RFIG\u2192G\u222aNj to be positive, the dependence X j 6y XN |XG has to hold.\nTheorem 2: If the difference \u2206RFIG\u2192G\u222aNj = RFI G j -\nRFIG\u222aNj , 0, then X j 6y XN |XG.\n2Let e.g. X1,X2 be perfectly correlated and independent of Y. Then adding X1 \u2212 X2 does not alter its prediction performance, unless the dependence between the variables is broken. Also see [14] for a discussion in PFI.\nProof of Theorem 2: Under independence X j y Xn|XG it holds that\nP(X\u0303Gj ,Y,XR,XG,XN) = P(X\u0303 G j |Y,XR,XG,XN)P(Y,XR,XG,XN)\n(def X\u0303Gj )\n= P(X j|XG)P(Y,XR,XG,XN) Xj y Xn |XG\n= P(X j|XG,XN)P(Y,XR,XG,XN) (def X\u0303G\u222aNj )\n= P(X\u0303G\u222aNj |XG,XN)P(Y,XR,XG,XN) (def X\u0303G\u222aNj )\n= P(X\u0303G\u222aNj |Y,XG,XN,XR)P(Y,XR,XG,XN) = P(X\u0303G\u222aNj ,Y,XR,XG,XN)\nThe equality P(X\u0303Gj ,Y,XR,XG,XN) = P(X\u0303 G\u222aN j ,Y,XR,XG,XN) implies P(X\u0303Gj ,Y,XR) = (X\u0303 G\u222aN j ,Y,XR). Invoking Proposition 1 it holds that the corresponding risks R j|G and R j|G\u222aN are equal. As RFIGj \u2212 RFIG\u222aNj = R j|G \u2212 R j|G\u222aN it holds that X j 6y Xn|XG \u21d2 \u2206RFIG\u2192G\u222aNj = 0. Contraposition proves Theorem 2.\nWhile nonzero RFIGj as well as nonzero \u2206RFI G\u2192G\u222aN j have clear implications, interpreting zero RFIGj or zero \u2206RFIG\u2192G\u222aNj is difficult. For example, we may be tempted to interpret RFIGj = 0 as conditional independence in the data. However, the general principle that absence of evidence is no evidence for absence also applies in the context of RFI. A dependence in the data may not be captured by the model when it has a poor fit and does not rely on the respective variable. Similarly, although f may be optimal, a dependence in higher moments may simply not be modeled by f or captured by the loss L. As all aforementioned causes of nonzero RFI are potentially sufficient, but not necessary, it is unclear which of the causes nonzero RFI can be attributed to. Furthermore, the related problem of conditional independence testing is provably hard [24]. The theoretical insights that we derive in this Section (Theorem 1 and 2) are applied and illustrated in a simulation study in Section VI.\nV. Estimation and Testing\nEstimating and sampling from the conditional distribution is in general difficult, especially in highdimensional continuous settings. Various approaches for replacing X j with samples from its conditional distribution exist, e.g., knockoff approaches [2], [7], [23], imputation and weighting [12] or permutation within decision tree leaves [18]. We used Model-X knockoffs [7] in this work, but note that the RFI approach is agnostic to its algorithmic implementation.\nUsing (standard) empirical risk estimates, our RFI estimate is\n\u02c6RFI G j = 1 n n\u2211 i=1 L ( y(i), f (x\u0303(i)j , x (i) R ) ) \u2212 1 n n\u2211 i=1 L ( y(i), f (x(i)j , x (i) R ) ) where x\u0303(i)j is a sample from X\u0303 G j . We can then test for nonzero RFIGj using procedures for conditional independence tests, e.g., [30], thereby quantifying the uncertainty coming from empirical risk minimization. Because of the central limit theorem, the empirical risk converges (in probability) to a Gaussian distribution with increasing number of observations. Therefore, one-sided, paired t-tests can be used to infer tests and confidence intervals [30]. The test procedures proposed in [30] are agnostic to the conditioning set for the perturbation X\u0303Gj . For smaller samples, the Exact Test by Fisher may be used. The t-test and Fisher Exact Test ignore uncertainty and bias of the estimation procedures, i.e. the ML model and the knockoff-sampler are treated as \u201cfixed\u201d. E.g. misspecified, suboptimal models may not capture dependencies. Or dependencies are in higher moments that are not captured by the loss. Consequently, without further assumptions, the framework does not provide a test for conditional independence in the dataset. The popular testing procedures for knockoffs proposed by [7] provide FDR over all features, but does not test the significance of the importance of individual features.\nVI. Simulation Studies In the following, we demonstrate the usefulness of RFI on two simulation studies. In the first example, we use RFI to expose indirect influence of variables that are not computationally used by the model. In the second example, we assess feature importance relative to a confounder that was unavailable at training time. In both examples, we represent the underlying data generating mechanism, that gives rise to the dependencies in the data, with a causal directed acyclic graph (DAG). The code for the examples is available online3.\nA. Indirect Influence A prominent application of interpretable machine learning is auditing models regarding its reliance on protected attributes A like age or sex. A reliance on the respective attributes may result in unfair discrimination and requires further inspection. With approaches like fairness through unawareness [3], the model does not rely on protected attributes directly. However, by implicitly reconstructing the sensitive attributes using seemingly harmless correlates, the model can indirectly make use of the protected attribute resulting in potentially harmful, unfair discrimination [3].\n3Link to Code: https://github.com/gcskoenig/icpr2020-rfi\nPFI and CFI cannot expose such indirect influence. As Lemma 2 proves, RFIGA is zero for a model that does not (directly) use the feature of interest A for the prediction for any conditioning set G. Furthermore, from PFI and CFI alone, we cannot infer whether the importance of a variable can be attributed to its dependence with an indirect influence. Using RFIGj with G = A we preserve the influence of A on the prediction and can thereby restrict the attribution of importance to contributions stemming from dependencies not involving A (Theorem 1, Lemma 1). The difference to \u2206RFIG\u2192G\u222aNj with G = \u2205 and N = A exposes the indirect influence. Not every indirect influence from a sensitive attribute is considered undesirable. Certain correlates of A may indeed be valid criteria for a decision (e.g. [4]). Importance stemming from dependencies with A via such resolving variables Z would be considered acceptable. We can assess the indirect influence beyond contributions stemming from dependence via Z by comparing to a baseline G = Z. In this baseline, contributions via Z are preserved and therefore irrelevant for RFI. Consequently, when setting N = A, the difference \u2206RFIG\u2192G\u222aNj only quantifies indirect influence that is not resolved by Z. We demonstrate the usefulness of RFI to expose indirect influence in a simulation study. The dataset is a sample drawn from the distribution induced by a structural causal model (SCM) depicted in Figure 2. All relationships are additive linear with coefficients 1 and Gaussian noise terms (\u03c31 = \u03c32 = \u03c34 = 1, \u03c33 = 0.3 and \u03c3y = 0.5). An ordinary least squares linear regression model was fit to predict Y from X1, . . . ,X4 (MSE = 0.25, f (x1, x2, x3, x4) = 0.00x1 \u2212 0.01x2 + 1.01x3 + 1.00x4). We trained model-X knockoffs [7] on the training data and evaluated RFI on test data. Sample size is 105 with 10% test data. In order to quantify the direct influence of the features we compute PFI. As we can see in Figure 3, X1 and X2 are considered irrelevant. In order to expose their indirect influence, we additionally compute RFI with respect to G = {X1} and G = {X2} respectively. For both variables we observe a drop in importance of X3 and X4. Consequently both X1 and X2 have an indirect influence on the target (Theorem 2). Furthermore we are interested in whether the indirect influence of X1 can be resolved by X2. We therefore compute RFIG\u222aNj with G = {X2} and N = {X1}. We see that for X3 no change in importance can be observed. This is due to the independence X1 y X3|X24 (Theorem 2). The indirect influence is resolved. However, for X4 the importance decreases further and is therefore not resolved by X2. This is in alignment with the dependence X1 6y X4|X2 implied by the graph (Figure 2).\n4As faithfulness and causal markov condition hold, d-separation in the graph and (conditional) independence coincide [21]. We can therefore read the independence structures off Figures 2 and 4.\n4 (Figure 3). All\nrelationships are additive linear Gaussian with all coefficients being equal to 1 and \u03c31 = \u03c32 = \u03c34 = 1, \u03c33 = 0.3 and \u03c3y = 0.5.\nB. Variables Outside Training Set When designing a model f , a practitioner may have decided to exclude a variable from the feature set, e.g., because it was then considered irrelevant, it belongs to a different modality or would have required further preprocessing. Furthermore, when auditing a machine learning model f , variables that have not been available for the training of the model may be accessible. In this example, we demonstrate that variables outside the training set can be included in the conditioning set for RFI. Consequently, importance of the features relative to variables outside the training set and the indirect influence of such variables can be assessed. More specifically, we simulate a hypothetical situation where the influence of a previously unknown confounder C shall be evaluated. This variable C is available for the model audit. In particular, we wonder whether the features X1, X2 and X3 are only or partly important due to a dependence via C. The dataset was sampled from a structural causal model (SCM) depicted in Figure 4. Assuming faithfulness and the causal Markov condition, this DAG implies the following (conditional) (in-)dependencies: X1 is independent of C, X3 is independent of Y conditional on C, and\nX2 is dependent on Y. Note that the dependence between X2 and Y is due to the common cause C as well as due to a direct effect of X2 on Y. All relationships are additive linear with coefficients 1 and additive Gaussian noise (\u03c31 = \u03c32 = \u03c3C = 1.0 and \u03c33 = \u03c3Y = 0.5). We fit an ordinary least squares linear regression model on X1, X2 and X3 to predict Y (MSE = 0.40, f (x1, x2, x3) = 1.0x1 + 1.17x2 + 0.67x3). C was not available for model training. We trained Model-X knockoffs [7] on training data and sampled from X\u0303Gj on test data. Sample size is 105 with 10% test data. When computing RFICj (G = {C}) for each variable, the different relationships with C become apparent. The respective results are depicted in Figure 5. For X1 the feature importance relative to C remains unchanged as the variables are pairwise independent (Theorem 2). For X3, that is only dependent with Y via C, it completely vanishes (Lemma 1). For X2 the feature importance decreases but remains nonzero, as X2 is dependent with Y directly and via C. Consequently, using RFI, we can (1) identify variables that are important due to a variable unavailable at training time and (2) distinguish between variables that only depend on Y via C from those that do not. With PFI (G = \u2205) or CFI (G = R) such a distinction is in general not possible.\nVII. Discussion\nWe proposed relative feature importance (RFI), a general conditional feature importance framework which allows to condition on arbitrary sets of other features, including features outside the training set. We underpin the method with theoretical results allowing insight into both model and underlying dataset. In a simulation study, the usefulness of the method for the exposure of indirect influence is demonstrated. Relative feature importance requires sampling from (unknown) conditional distributions. For continuous variables and in high-dimensional settings this task is challenging and an open area of research [7], [23]. Uncertainty stemming from inaccurate sampling may affect the interpretation. The quality of insight into the underlying dataset strongly depends on the training and evaluation of the model. Dependencies in higher moments are usually not modeled and not captured by standard loss functions and can therefore not be detected. Especially the interpretation of zero RFI requires careful assessment of the model specification. Further research is needed to assess necessary assumptions for the interpretation of RFI. These challenges are not unique to RFI, but apply more generally in the field of interpretable machine learning [20].\nReferences\n[1] Philip Adler, Casey Falk, Sorelle A. Friedler, Tionney Nix, Gabriel Rybeck, Carlos Scheidegger, Brandon Smith, and Suresh Venkatasubramanian. Auditing black-box models for indirect influence. Knowledge and Information Systems, 54(1):95\u2013122, 2018. arXiv: 1602.07043. [2] Rina Foygel Barber, Emmanuel J Cands, and others. Controlling the false discovery rate via knockoffs. The Annals of Statistics, 43(5):2055\u20132085, 2015. Publisher: Institute of Mathematical Statistics. [3] Solon Barocas, Moritz Hardt, and Arvind Narayanan. Fairness and Machine Learning. fairmlbook.org, 2019. http://www. fairmlbook.org. [4] Vence L Bonham, Shawneequa L Callier, and Charmaine D Royal. Will precision medicine move us beyond race? The New England journal of medicine, 374(21):2003, 2016. [5] Leo Breiman. Random forests. Machine Learning, pages 1\u2013122, 2001. [6] David V Budescu. Dominance analysis: a new approach to the problem of relative importance of predictors in multiple regression. Psychological bulletin, 114(3):542, 1993. [7] Emmanuel Cands, Yingying Fan, Lucas Janson, and Jinchi Lv. Panning for gold: model-X knockoffs for high dimensional controlled variable selection. Journal of the Royal Statistical Society. Series B: Statistical Methodology, 80(3):551\u2013577, 2018. arXiv: 1610.02351. [8] Ian Covert, Scott Lundberg, and Su-In Lee. Understanding Global Feature Contributions Through Additive Importance Measures. arXiv preprint arXiv:2004.00668, 2020. [9] Jeffrey (Reuters) Dastin. Amazon scraps secret AI recruiting tool that showed bias against women. Reuters, 2018. [10] Lydia de la Torre. A Guide to the California Consumer Privacy Act of 2018. SSRN Electronic Journal, pages 1\u201317, 2018. [11] Julia Dressel and Hany Farid. The accuracy, fairness, and limits of predicting recidivism. Science Advances, 4(1):1\u20136, 2018. [12] Aaron Fisher, Cynthia Rudin, and Francesca Dominici. All models are wrong, but many are useful: Learning a variable\u2019s importance by studying an entire class of prediction models simultaneously. Journal of Machine Learning Research, 20(177):1\u201381, 2019.\n[13] Ulrike Grmping. Variable importance assessment in regression: linear regression versus random forest. The American Statistician, 63(4):308\u2013319, 2009. Publisher: Taylor & Francis. [14] Giles Hooker and Lucas Mentch. Please Stop Permuting Features: An Explanation and Alternatives. arXiv preprint arXiv:1905.03151v, pages 1\u201315, 2019. arXiv: 1905.03151v1. [15] Jing Lei, Max GSell, Alessandro Rinaldo, Ryan J Tibshirani, and Larry Wasserman. Distribution-free predictive inference for regression. Journal of the American Statistical Association, 113(523):1094\u20131111, 2018. [16] Stan Lipovetsky and Michael Conklin. Analysis of regression in game theory approach. Applied Stochastic Models in Business and Industry, 17(4):319\u2013330, 2001. Lipovetsky2001. [17] Scott M. Lundberg and Su In Lee. A unified approach to interpreting model predictions. Advances in Neural Information Processing Systems, 2017-Decem(Section 2):4766\u20134775, 2017. arXiv: 1705.07874. [18] Christoph Molnar, Gunnar Ko\u0308nig, Bernd Bischl, and Giuseppe Casalicchio. Model-agnostic feature importance and effects with dependent features\u2013a conditional subgroup approach. arXiv preprint arXiv:2006.04628, 2020. [19] Christoph Molnar, Gunnar Ko\u0308nig, Bernd Bischl, and Giuseppe Casalicchio. Model-agnostic feature importance and effects with dependent features-a conditional subgroup approach. arXiv preprint arXiv:2006.04628, 2020. [20] Christoph Molnar, Gunnar Ko\u0308nig, Julia Herbinger, Timo Freiesleben, Susanne Dandl, Christian A. Scholbeck, Giuseppe Casalicchio, Moritz Grosse-Wentrup, and Bernd Bischl. Pitfalls to avoid when interpreting machine learning models. arXiv preprint arXiv:2007.04131, 2020. [21] Judea Pearl. Causality. Cambridge university press, 2009. [22] Judea Pearl and Azaria Paz. Graphoids: A graph-based logic for\nreasoning about relevance relations. University of California (Los Angeles). Computer Science Department, 1985. [23] Yaniv Romano, Matteo Sesia, and Emmanuel Cands. Deep knockoffs. Journal of the American Statistical Association, pages 1\u201312, 2019. Publisher: Taylor & Francis. [24] Rajen D Shah and Jonas Peters. The hardness of conditional independence testing and the generalised covariance measure. arXiv preprint arXiv:1804.07203, 2018. [25] Carolin Strobl, Anne Laure Boulesteix, Thomas Kneib, Thomas Augustin, and Achim Zeileis. Conditional variable importance for random forests. BMC Bioinformatics, 9:1\u201311, 2008. [26] Laura Tolo\u00c5i and Thomas Lengauer. Classification with correlated features: unreliability of feature ranking and solutions. Bioinformatics, 27(14):1986\u20131994, 2011. Publisher: Oxford University Press. [27] Eric J Topol. High performance medicine: the convergence of human and artificial intelligence. Nature Medicine, 25(January), 2019. Publisher: Springer US. [28] Eugene Tuv, Alexander Borisov, George Runger, and Kari Torkkola. Feature selection with ensembles, artificial variables, and redundancy elimination. Journal of Machine Learning Research, 10(Jul):1341\u20131366, 2009. [29] Paul Voigt and Axel dem Bussche. The eu general data protection regulation (gdpr). A Practical Guide, 1st Ed., Cham: Springer International Publishing, 2017. Publisher: Springer. [30] David S. Watson and Marvin N. Wright. Testing Conditional Independence in Supervised Learning Algorithms. arXiv preprint arXiv:1901.09917, 2019. arXiv: 1901.09917. [31] Pengfei Wei, Zhenzhou Lu, and Jingwen Song. Variable importance analysis: a comprehensive review. Reliability Engineering & System Safety, 142:399\u2013432, 2015. Publisher: Elsevier. [32] Yufei Xia, Chuanzhe Liu, Yu Ying Li, and Nana Liu. A boosted decision tree approach using Bayesian hyper-parameter optimization for credit scoring. Expert Systems with Applications, 78:225\u2013 241, 2017. Publisher: Elsevier Ltd. [33] Erik \u00c5trumbelj and Igor Kononenko. Explaining prediction models and individual predictions with feature contributions. Knowledge and information systems, 41(3):647\u2013665, 2014. Publisher: Springer."
    }
  ],
  "title": "Relative Feature Importance",
  "year": 2020
}
