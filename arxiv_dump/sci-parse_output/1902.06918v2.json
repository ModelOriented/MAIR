{"abstractText": "Interpretable machine learning has gained much attention recently. Briefness and comprehensiveness are necessary in order to provide a large amount of information concisely when explaining a black-box decision system. However, existing interpretable machine learning methods fail to consider briefness and comprehensiveness simultaneously, leading to redundant explanations. We propose the variational information bottleneck for interpretation, VIBI, a system-agnostic interpretable method that provides a brief but comprehensive explanation. VIBI adopts an information theoretic principle, information bottleneck principle, as a criterion for finding such explanations. For each instance, VIBI selects key features that are maximally compressed about an input (briefness), and informative about a decision made by a black-box system on that input (comprehensive). We evaluate VIBI on three datasets and compare with state-of-the-art interpretable machine learning methods in terms of both interpretability and fidelity evaluated by human and quantitative metrics.", "authors": [{"affiliations": [], "name": "Seojin Bang"}, {"affiliations": [], "name": "Pengtao Xie"}, {"affiliations": [], "name": "Heewook Lee"}, {"affiliations": [], "name": "Wei Wu"}, {"affiliations": [], "name": "Eric Xing"}], "id": "SP:c1b1edd7f2c6cc6da9858cf4badf04c2a457db47", "references": [{"authors": ["Representations", "S. 2017. Bach", "A. Binder", "G. Montavon", "F. Klauschen", "M\u00fcller", "K.-R", "W. Samek"], "title": "On pixel-wise explanations for non-linear classifier", "year": 2017}, {"authors": ["D. Baehrens", "T. Schroeter", "S. Harmeling", "M. Kawanabe", "K. Hansen", "M\u00fcller", "K.-R"], "title": "How to explain individual classification decisions", "venue": "Journal of Machine Learning Research,", "year": 2010}, {"authors": ["A. Binder", "G. Montavon", "S. Lapuschkin", "M\u00fcller", "K.-R", "W. Samek"], "title": "Layer-wise relevance propagation for neural networks with local renormalization layers", "venue": "In International Conference on Artificial Neural Networks,", "year": 2016}, {"authors": ["J. Chen", "L. Song", "M.J. Wainwright", "M.I. Jordan"], "title": "Learning to explain: An information-theoretic perspective on model interpretation", "venue": "International Conference on Machine Learning (ICML),", "year": 2018}, {"authors": ["T.M. Cover", "J.A. Thomas"], "title": "Elements of information theory", "year": 2012}, {"authors": ["P. Dabkowski", "Y. Gal"], "title": "Real time image saliency for black box classifiers", "venue": "In Advances in Neural Information Processing Systems,", "year": 2017}, {"authors": ["F. Doshi-Velez", "B. Kim"], "title": "Towards a rigorous science of interpretable machine learning", "venue": "arXiv preprint arXiv:1702.08608,", "year": 2017}, {"authors": ["R.C. Fong", "A. Vedaldi"], "title": "Interpretable explanations of black boxes by meaningful perturbation", "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "year": 2017}, {"authors": ["S. Henikoff", "J.G. Henikoff"], "title": "Amino acid substitution matrices from protein blocks", "venue": "Proceedings of the National Academy of Sciences,", "year": 1992}, {"authors": ["E. Jang", "S. Gu", "B. Poole"], "title": "Categorical reparameterization with gumbel-softmax", "venue": "International Conference on Learning Representations,", "year": 2017}, {"authors": ["E. Jokinen", "M. Heinonen", "J. Huuhtanen", "S. Mustjoki", "H. L\u00e4hdesm\u00e4ki"], "title": "Tcrgp: Determining epitope specificity of t cell receptors", "venue": "bioRxiv, pp", "year": 2019}, {"authors": ["V.I. Jurtz", "L.E. Jessen", "A.K. Bentzen", "M.C. Jespersen", "S. Mahajan", "R. Vita", "K.K. Jensen", "P. Marcatili", "S.R. Hadrup", "B Peters"], "title": "Nettcr: sequence-based prediction of tcr binding to peptide-mhc complexes using convolutional neural networks. bioRxiv", "year": 2018}, {"authors": ["D.P. Kingma", "J. Ba"], "title": "Adam: A method for stochastic optimization", "venue": "arXiv preprint arXiv:1412.6980,", "year": 2014}, {"authors": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "title": "Gradient-based learning applied to document recognition", "venue": "Proceedings of the IEEE,", "year": 1998}, {"authors": ["Z.C. Lipton"], "title": "The mythos of model interpretability", "venue": "arXiv preprint arXiv:1606.03490,", "year": 2016}, {"authors": ["S.M. Lundberg", "Lee", "S.-I"], "title": "A unified approach to interpreting model predictions", "venue": "In Advances in Neural Information Processing Systems,", "year": 2017}, {"authors": ["G. Lythe", "R.E. Callard", "R.L. Hoare", "C. Molina-Par\u00eds"], "title": "How many TCR clonotypes does a body maintain", "venue": "Journal of Theoretical Biology,", "year": 2016}, {"authors": ["A.L. Maas", "R.E. Daly", "P.T. Pham", "D. Huang", "A.Y. Ng", "C. Potts"], "title": "Learning word vectors for sentiment analysis", "venue": "In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,", "year": 2011}, {"authors": ["A. Paszke", "S. Gross", "S. Chintala", "G. Chanan", "E. Yang", "Z. DeVito", "Z. Lin", "A. Desmaison", "L. Antiga", "A. Lerer"], "title": "Automatic differentiation in pytorch", "venue": "NIPS-W,", "year": 2017}, {"authors": ["V. Petsiuk", "A. Das", "K. Saenko"], "title": "Rise: Randomized input sampling for explanation of black-box models", "venue": "The British Machine Vision Conference,", "year": 2018}, {"authors": ["M.T. Ribeiro", "S. Singh", "C. Guestrin"], "title": "Why should i trust you?: Explaining the predictions of any classifier", "venue": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "year": 2016}, {"authors": ["A.S. Ross", "M.C. Hughes", "F. Doshi-Velez"], "title": "Right for the right reasons: Training differentiable models by constraining their explanations", "venue": "In Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence,", "year": 2017}, {"authors": ["A. Shrikumar", "P. Greenside", "A. Kundaje"], "title": "Learning important features through propagating activation differences", "venue": "In Proceedings of the 34th International Conference on Machine Learning-Volume", "year": 2017}, {"authors": ["M. Shugay", "D.V. Bagaev", "I.V. Zvyagin", "R.M. Vroomans", "J.C. Crawford", "G. Dolton", "E.A. Komech", "A.L. Sycheva", "A.E. Koneva", "Egorov", "E. S"], "title": "Vdjdb: a curated database of t-cell receptor sequences with known antigen specificity", "venue": "Nucleic Acids Research,", "year": 2017}, {"authors": ["R. Shwartz-Ziv", "N. Tishby"], "title": "Opening the black box of deep neural networks via information", "venue": "arXiv preprint arXiv:1703.00810,", "year": 2017}, {"authors": ["K. Simonyan", "A. Vedaldi", "A. Zisserman"], "title": "Deep inside convolutional networks: Visualising image classification models and saliency maps", "venue": "arXiv preprint arXiv:1312.6034,", "year": 2013}, {"authors": ["M. 2017. Sundararajan", "A. Taly", "Q. Yan"], "title": "Axiomatic attribution for deep networks", "venue": "In Proceedings of the 34th International Conference", "year": 2017}, {"authors": ["N. Tishby", "N. Zaslavsky"], "title": "Deep learning and the information bottleneck principle", "venue": "Machine Learning-Volume", "year": 2015}, {"authors": ["N. Tishby", "F.C. Pereira", "W. Bialek"], "title": "The information bottleneck method", "venue": "IEEE, pp", "year": 2015}, {"authors": ["A Sette"], "title": "The immune epitope database (iedb) 3.0", "venue": "Nucleic Acids Research,", "year": 2014}, {"authors": ["818\u2013833. Springer", "L.M. 2014. Zintgraf", "T.S. Cohen", "T. Adel", "M. Welling"], "title": "Visualizing deep neural network decisions: Prediction difference analysis", "year": 2014}, {"authors": ["Alemi"], "title": "Our variational approximation is similar to the one", "year": 2017}, {"authors": ["Ribeiro"], "title": "LIME, SHAP, Saliency, and L2X all have their own approximators. LIME and SHAP have a sparse linear approximator for each black-box instance (See Section", "venue": "Simonyan et al", "year": 2017}], "sections": [{"heading": "1 Introduction", "text": "Interpretability is crucial in building and deploying black-box decision systems such as deep learning models. Interpretation of a black-box system helps decide whether or not to follow its decisions, or understand the logic behind the system. In recent years, the extensive use of deep learning black-box systems has given rise to interpretable machine learning approaches (Lipton, 2016; Doshi-Velez & Kim, 2017), which aim to explain how black-box systems work or why they reach certain decisions. In order to provide sufficient information while avoiding redundancy when explaining a black-box decision, we need to consider both briefness and comprehensiveness. However, existing approaches lack in-depth consideration for and fail to find both brief but comprehensive explanation.\nIn order to obtain brief but comprehensive explanation, we adopt the information bottleneck principle (Tishby et al., 2000). This principle provides an appealing information theoretic perspective for learning supervised models by defining what we mean by a \u2018good\u2019 representation. The principle says that the optimal model transmits as much information as possible from its input to its output through a compressed representation called the information bottleneck. Then, the information bottleneck will maximally compress the mutual information (MI) with an input while preserving\n\u2217seojinb@cs.cmu.edu, part of this work was done while SB was an intern at Petuum Inc.\ni\nar X\niv :1\n90 2.\n06 91\n8v 2\n[ cs\n.L G\n] 3\nas much as possible MI with the output. Recently, it has been shown that the principle also applies to deep neural networks and each layer of a deep neural network can work as an information bottleneck (Tishby & Zaslavsky, 2015; Shwartz-Ziv & Tishby, 2017). Using this idea of information bottleneck principle, we define a brief but comprehensive explanation as maximally informative about the black-box decision while compressive about a given input.\nIn this paper, we introduce the variational information bottleneck for interpretation (VIBI), a system-agnostic information bottleneck model that provides a brief but comprehensive explanation for every single decision made by a black-box model. VIBI is composed of two parts: explainer and approximator, each of which is modeled by a deep neural network. The explainer returns a probability whether a chunk of features such as a word, phrase, sentence or a group of pixels will be selected as an explanation or not for each instance, and an approximator mimics behaviour of a black-box model. Using the information bottleneck principle, we learn an explainer that favors brief explanations while enforcing that the explanations alone suffice for accurate approximations to a black-box model."}, {"heading": "1.1 Contribution", "text": "Our main contribution is to provide a new framework that systematically defines and generates a \u2018good\u2019 (i.e. brief but comprehensive) explanation using the information bottleneck principle. Based on this principle, we develop VIBI that favors a brief but comprehensive explanation. In order to make the objective function of VIBI tractable, we derive a variational approximation to the objective.\nThe benefits of our method are as follows. 1) System-agnostic: VIBI can be applied to explain any black-box system. 2) Post-hoc learning: VIBI is learned in a post-hoc manner, hence there is no trade-off between task accuracy of a black-box system and interpretability of an explainer. 3) Cognitive chunk: Cognitive chunk is defined as a group of raw features whose identity is understandable to human. VIBI groups non-cognitive raw features such as a pixel and letter into a cognitive chunk (e.g. a group of pixels, a word, a phrase, a sentence) and selects each unit as an explanation. 4) Separate explainer and approximator: The explainer and approximator are designed for separated tasks so that we do not need to limit the approximator to have a simple structure, which may reduce the fidelity (the ability to imitate the behaviour of a black-box) of approximator."}, {"heading": "2 Related Work", "text": "Most prior interpretable machine learning methods have been focusing on local interpretation, which implies knowing the reasons why a black-box system makes a certain decision at a very local point of interest, and our work is also situated in this line. Existing methods can be categorized into system-specific and system-agnostic method. System-specific methods only explain certain black-box decision systems (e.g. using backpropagation algorithm, or having CNN structure), while system-agnostic methods explain any black-box decision systems.\nSystem-specific methods. To measure a change of output with respect to changes of input is an intuitive way of obtaining feature attribution for the output. Using this idea, Zeiler & Fergus (2014), and Zintgraf et al. (2017) observe how outputs change when they make perturbations to each instance. Baehrens et al. (2010); Simonyan et al. (2013), and Smilkov et al. (2017) use computationally more efficient approaches; they measure change of output by propagating contributions through layers of a deep neural network towards an input than the perturbation. However, these approaches fail to detect the changes of output when the prediction function is flattened at the instance (Shrikumar et al., 2017), which leads to interpretations focusing on irrelevant features. In order to solve this problem,\nii\nthe layer-wise relevance propagation (Bach et al., 2015; Binder et al., 2016), DeepLIFT (Shrikumar et al., 2017), and Integrated Gradients (Sundararajan et al., 2017) compare the changes of output to its reference output. Ross et al. (2017) learn a more generalizable model as well as desirable explanations by constraining its explanations (i.e. input gradient) to match domain knowledge.\nSystem-agnostic methods. The great advantage of system-agnostic interpretable machine learning methods over system-specific methods is that their usage is not restricted to a specific blackbox system. One of the most well-known system-agnostic methods is LIME (Ribeiro et al., 2016). It explains the decision of an instance by locally approximating the black-box decision boundary around the instance with an inherently interpretable model such as sparse linear or decision trees. The approximator is learned by samples generated by perturbing a given instance. Lundberg & Lee (2017) proposed SHAP, a unified measure defined over the additive feature attribution scores in order to achieve local accuracy, missingness, and consistency. L2X (Chen et al., 2018) learns a stochastic map that selects instance-wise features that are most informative for black-box decisions. Unlike LIME and SHAP, which approximate local behaviors of a black-box system with a simple (linear) model, L2X does not put a limit on the structure of the approximator; hence it avoids losing fidelity of the approximator. As SHAP does, Dabkowski & Gal (2017); Fong & Vedaldi (2017), and Petsiuk et al. (2018) use sample perturbation but they rather learn or estimate desired perturbation masks than using perturbed samples to learn an approximator. Dabkowski & Gal (2017), and Fong & Vedaldi (2017) learn a smallest perturbation mask that alters black-box outputs as much as possible. Petsiuk et al. (2018) empirically estimate feature attribution as a sum of random masks weighted by class scores corresponding to masked inputs. Our method VIBI is similar with L2X in that both learn a stochastic explainer that returns a distribution over the subset of features given the input and performs instance-wise feature selection based on that. However, given the same number of explanations, our explainer favors both briefness and comprehensiveness while the L2X explainer favors comprehensiveness of the explanation and does not account for briefness.\n3 Method\niii"}, {"heading": "3.1 Perspective from information bottleneck principle", "text": "The information bottleneck principle (Tishby et al., 2000) provides an appealing information theoretic view for learning a supervised model by defining what we mean by a \u2018good\u2019 representation. The principle says that the optimal model transmits as much information as possible from the input x to the output y through a compressed representation t (called the information bottleneck). The representation t is stochastically defined and the optimal stochastic mapping p(t|x) is obtained by optimizing the following problem with a Markov chain assumption y\u2192 x\u2192 t:\np(t|x) = argmax p(t|x),p(y|t),p(t) I(t,y)\u2212 \u03b2 I(x, t) (1)\nwhere I(\u00b7, \u00b7) is the MI and \u03b2 is a Lagrange multiplier representing the trade-off between the compressiveness \u2212I(x, t) and informativeness I(t,y) of the representation t.\nWe adopt the information bottleneck principle as a criterion for finding brief but comprehensive explanations. Our aim is to learn an explainer generating explanations that are maximally informative about the black-box decision while compressive about a given input. The primary difference between our information bottleneck objective (2) and the one in Tishby et al. (2000) is as follows: the latter aims to identify a stochastic map of the representation t that itself works as an information bottleneck, whereas our objective aims to identify a stochastic map of z performing instance-wise selection of cognitive chunks and define information bottleneck as a function of z and the input x."}, {"heading": "3.2 Proposed approach", "text": "We introduce VIBI, a system-agnostic interpretation approach that provides brief but comprehensive explanations for decisions made by black-box decision system. In order to achieve this, we optimize the following information bottleneck objective.\np(z|x) = argmax p(z|x),p(y|t) I(t,y)\u2212 \u03b2 I(x, t) (2)\nwhere I(t,y) represents the sufficiency of information retained for explaining the black-box output y, \u2212I(x, t) represents the briefness of the explanation t, and \u03b2 is a Lagrange multiplier representing a trade-off between the two.\nTo achieve compressiveness, in addition to encouraging small MI between explanations and inputs, we also encourage the number of selected cognitive chunks to be small, i.e., encouraging z to be sparse. Note that MI and sparsity are two complementary approaches for achieving compression. MI aims at reducing semantic redundancy in explanations. Sparsity cannot achieve such a goal. For example, consider two explanations in judging the sentiment of movie reviews: \"great, great\" and \"great, thought provoking\". They have the same level of sparsity. However, the first explanation has a large MI with the input document where \"great\" occurs a lot. As a result, this explanation has a lot of redundancy in semantics. The second explanation has smaller MI and hence is more brief and preferable.\nAs illustrated in Figure 1A, VIBI is composed of two parts: the explainer and the approximator, each of which is modeled by a deep neural network. The explainer selects a group of k key cognitive chunks given an instance while the approximator mimics the behaviour of the black-box system using the selected keys as the input. k controls the level of sparsity in z. In detail, the explainer p(z|x;\u03b8e) is a map from an input x to its attribution scores pj(x) = p(zj |x) (where j is for the j-th cognitive chunk). The attribution score indicates the probability that each cognitive chunk to be selected. In order to select top k cognitive chunks as an explanation, a k-hot vector z is sampled\niv\nfrom a categorical distribution with class probabilities pj(x) = p(zj |x) and the j-th cognitive chunk is selected if zj = 1. More specifically, the explanation t is defined as follows:\nti = (x z)i = xi \u00d7 zj ,\nwhere j indicates a cognitive chunk, each of which corresponds to multiple row features i. The approximator is modeled by another deep neural network p(y|t;\u03b8a), which mimics the black-box decision system. It takes t as an input and returns an output approximating the black-box output for the instance x. \u03b8a and \u03b8e represent the weight parameters of neural networks. The explainer and approximator are trained jointly by minimizing a cost function that favors concise explanations while enforcing that the explanations alone suffice for accurate prediction."}, {"heading": "3.2.1 The variational bound", "text": "The current form of information bottleneck objective is intractable due to the MIs I(t,y) and I(x, t). We address this problem by using a variational approximation of our information bottleneck objective. In this section, we summarize the results and refer to Supplementary Material S1 for details.\nVariational bound for I(x, t): We first show that I(x, t) \u2264 I(x, z) + C where C is constant and use the lower bound for \u2212I(x, z)\u2212 C as a lower bound for \u2212I(x, t). As a result, we obtain:\nI(x, t) \u2264 I(x, z) + C \u2264 E(x,z)\u223cp(x,z) [ log\np(z|x) r(z)\n] + C = Ex\u223cp(x)DKL(p(z|x), r(z)) + C (3)\nNote that with proper choices of r(z) and p(z|x), we can assume that the Kullback-Leibler divergence DKL(p(z|x), r(z)) has an analytical form.\nVariational bound for I(t,y): We obtain the lower bound for I(t,y) by using q(y|t) to approximate p(y|t), which works as an approximator to the black-box system. As a result, we obtain:\nI(t,y) \u2265 E(t,y)\u223cp(t,y) [log q(y|t)] = Ex\u223cp(x)Ey|x\u223cp(y|x)Et|x\u223cp(t|x) [log q(y|t)] (4)\nwhere p(t|x,y) = p(t|x) by the Markov chain assumption y\u2194 x\u2194 t. Combining Equations (3) and (4), we obtain the following variational bound:\nI(t,y)\u2212 \u03b2 I(x, t) \u2265 Ex\u223cp(x)Ey|x\u223cp(y|x)Et|x\u223cp(t|x) [log q(y|t)]\u2212 \u03b2 Ex\u223cp(x)DKL(p(z|x), r(z)) + C\u2217. (5)\nwhere C\u2217 = \u2212C\u03b2 can be ignored since it is independent of the optimization procedure. We use the empirical data distribution to approximate p(x,y) = p(x)p(y|x) and p(x)."}, {"heading": "3.2.2 Continuous relaxation and reparameterization", "text": "Current form of the bound (5) is still intractable because we need to sum over the ( d k ) combinations of feature subsets. This is because we sample top k out of d cognitive chunks where each chunk is assumed drawn from a categorical distribution with class probabilities pj(x) = p(zj |x). In order to avoid this, we use the generalized Gumbel-softmax trick (Jang et al., 2017; Chen et al., 2018). This is a well-known technique that are used to approximate a non-differentiable categorical subset sampling with differentiable Gumbel-softmax samples. The steps are as follows.\nFirst, we independently sample a cognitive chunk for k times. For each time, a random perturbation ej is added to the log probability of each cognitive chunk log pj(x). From this, Concrete\nv\nrandom vector c = (c1, \u00b7 \u00b7 \u00b7 , cd) working as a continuous, differentiable approximation to argmax is defined:\ngj = \u2212 log (\u2212 log ej) where ej \u223c U(0, 1)\ncj = exp\n(( gj + log pj(x) ) /\u03c4 )\u2211d\nj=1 exp (( gj + log pj(x) ) /\u03c4 ) ,\nwhere \u03c4 is a tuning parameter for the temperature of Gumbel-Softmax distribution. Next, we define a continuous-relaxed random vector z\u2217 = [z\u22171, \u00b7 \u00b7 \u00b7 , z\u2217d]> as the element-wise maximum of the independently sampled Concrete vectors c(l) where l = 1, \u00b7 \u00b7 \u00b7 , k:\nz\u2217j = max l c(l)j for l = 1, \u00b7 \u00b7 \u00b7 , k\nWith this sampling scheme, we approximate the k-hot random vector and have the continuous approximation to the variational bound (5). This trick allows using standard backpropagation to compute the gradients of the parameters via reparameterization.\nBy putting everything together, we obtain:\n1\nnL n\u2211 i L\u2211 l [ log q(yi|xi f(e(l),xi))\u2212 \u03b2 DKL(p(z\u2217|xi), r(z\u2217)) ] where q(yi|xi f(e(l),xi)) is the approximator to the black-box system and \u2212DKL(p(z\u2217|xi), r(z\u2217)) represents the compactness of the explanation. Once we learn the model, the attribution score pj(x) for each cognitive chunk is used to select top k key cognitive chunks that are maximally compressive about the input x and informative about the black-box decision y on that input."}, {"heading": "4 Experiments", "text": "We evaluated VIBI on three datasets and compared with state-of-the-art interpretable machine learning methods. The evaluation is performed from two perspectives: interpretability and fidelity. The interpretability indicates the ability to explain a black-box model with human understandable terms. The fidelity implies how accurately our approximator approximates the black-box model. Based on these criteria, we compared VIBI with three state-of-the-art system-agnostic methods (LIME (Ribeiro et al., 2016), SHAP (Lundberg & Lee, 2017) and L2X (Chen et al., 2018)), and a commonly used model-specific method called Saliency Map (Simonyan et al., 2013). For Saliency Map, we used the smooth gradient technique (Smilkov et al., 2017) to get visually sharp gradient-based sensitivity maps over the basic gradient saliency map. See Supplementary Material S2 for further experimental details.\nWe examined how VIBI performs across different experimental settings varying the number of selected chunks k (amount or number of explanation), size of chunk (unit of explanation), and tradeoff parameter \u03b2 (trade-off between the compressiveness of explanation and information preserved about the output). The settings of hyperparameter tuning include (bold indicate the choice for our final model): the temperature for Gumbel-softmax approximation \u03c4 \u2013 {0.1, 0.2, 0.5,0.7, 1}, learning rate \u2013 5\u00d7 10\u22123, 10\u22123, 5\u00d7 10\u22124,10\u22124, 5\u00d7 10\u22125} and \u03b2 \u2013 {0, 0.001,0.01, 0.1, 1, 10, 100}. We use Adam algorithm (Kingma & Ba, 2014) with batch size 100 for MNIST and 50 for IMDB, the coefficients used for computing running averages of gradient and its square (\u03b21, \u03b22) = (0.5, 0.999), and = 10\u22128. We tuned the hyperparameters via grid search and picked up the hyperparameters that yield the best fidelity score on the validation set. The code is publicly available on GitHub https://github.com/SeojinBang/VIBI. All implementation is performed via PyTorch an open source deep learning platform (Paszke et al., 2017).\nvi"}, {"heading": "4.1 LSTM movie sentiment prediction model using IMDB", "text": "The IMDB (Maas et al., 2011) is a large text dataset containing movie reviews labeled by sentiment (positive/negative). We grouped the reviews into training, validation, and test sets, which have 25,000, 12,500, and 12,500 reviews respectively. Then, we trained a hierarchical LSTM for sentiment prediction, which achieved 87% of test accuracy. In order to explain this LSTM black-box model, we applied VIBI. We parameterized the explainer using a bidirectional LSTM and approximator using a 2D CNN. For the details of the black-box model and VIBI architectures, see Supplementary Material S2.1.\nVIBI explains why the LSTM predicts each movie review to be positive/negative and provides instance-wise key words that are the most important attributes to the sentiment prediction. As seen in the top-right and top-left of Figure 2, VIBI shows that the positive (or negative) words pass through the bottleneck and make a correct prediction. The bottom of Figure 2 shows that the LSTM sentiment prediction model makes a wrong prediction for a negative review because the review includes several positive words such as \u2018enjoyable\u2019 and \u2018exciting\u2019."}, {"heading": "4.2 CNN digit recognition model using MNIST", "text": "Same digit with different angles. Different digits, upside down Difference between 7 and 1 Same digit similar features\nThe MNIST (LeCun et al., 1998) is a large dataset contains 28\u00d7 28 sized images of handwritten digits (0 to 9). We grouped the images into training, validation, and test sets, which have 50,000,\nvii\n10,000, and 10,000 images respectively, and trained a simple 2D CNN for the digit recognition, which achieved 97% of test accuracy. In order to explain this CNN black-box model, we applied VIBI. We parameterized each the explainer and approximator using a 2D CNN. For the details of the black-box model and VIBI architectures, see Supplementary Material S2.2.\nVIBI explains how the CNN characterizes a digit and recognizes differences between digits. The first two examples in Figure 3 show that the CNN recognizes digits using both shapes and angles. In the first example, the CNN characterizes \u20181\u2019s by straightly aligned patches along with the activated regions although \u20181\u2019s in the left and right panels are written at different angles. Contrary to the first example, the second example shows that the CNN recognizes the difference between \u20189\u2019 and \u20186\u2019 by their differences in angles. The last two examples in Figure 3 show that the CNN catches a difference of \u20187\u2019s from \u20181\u2019s by patches located on the activated horizontal line on \u20187\u2019 (see the cyan circle) and recognizes \u20188\u2019s by two patches on the top of the digits and another two patches at the bottom circle. More qualitative examples for VIBI and the baselines are shown in Supplementary Figure S2."}, {"heading": "4.3 TCR to epitope binding prediction model using VDJdb and IEDB", "text": "We next illustrate how VIBI can be used to get insights from a model and ensure the safety of a model in a real world application. Identifying which T-cell receptor (TCR) will bind to a specific epitope (i.e. cancer induced peptide molecules presented by the major histocompatibility complex to T-cells) is important for screening T-cells or genetically engineering T-cells that are effective in recognizing and destroying tumor cells. Therefore, there has been efforts in developing computational methods to predict binding affinity of given TCR-epitope pairs (Jurtz et al., 2018; Jokinen et al., 2019). These approaches rely on known interacting TCR-epitope pairs available from VDJdb (Shugay et al., 2017) and IEDB (Vita et al., 2014), which are the largest databases of several thousand entries. However, the number of unique TCRs harbored in a single individual is estimated to be 1010 (Lythe et al., 2016) and a theoretical number of epitopes of length l is 20l, which are much larger than the number of known interacting TCR-epitope pairs.\nOne of the main concerns is whether a black-box model trained on such limited dataset can accurately predict TCR-epitope bindings of out-of-samples. This concern becomes pressing in a TCR-epitope binding prediction model trained on VDJdb (For details of the data, black-box model architecture, and parameter tuning, see Supplementary Material S2.3). The model accurately predicted the (in-sample) bindings from VDJdb (recall 0.79, Figure 4A). However, it achieved poor prediction performance when it is used to predict the (out-of-sample) bindings from another\nviii\ndataset, IEDB (recall 0.40, Figure 4B). In an attempt to address this problem, we applied VIBI and determined whether or not to accept a decision made by the black-box model based on VIBI\u2019s explanation. As illustrated in Figure 4E, VIBI provided matched explanations\u2014the identical amino acids in same positions (S and Y in this example) are highlighted in different TCR sequences when they are bound to the same epitope (GILGFVFTL in this example). Moreover, we found that if two TCR sequences binding to the same epitope, each from IEDB and VDJdb, are assigned with matched explanations by VIBI, then it significantly better predicts the binding than the others with no matching TCRs (Figure 4C-D, p-values are shown). Therefore, if a TCR sequence from IEDB has a matched explanation to a TCR from VDJdb, then we safely follow the positive decision made by the black-box model."}, {"heading": "4.4 Interpretability evaluated by humans", "text": "We evaluated interpretability of the methods on the LSTM movie sentiment prediction model and the CNN digit recognition model. For the movie sentiment prediction model, we provided instances that the black-box model had correctly predicted and asked humans to infer the output of the primary sentiment of the movie review (Positive/Negative/Neutral) given five key words selected by each method. Each method was evaluated by the human intelligences on MTurk1 who are awarded the Masters Qualification, high-performance workers who have demonstrated excellence across a wide range of tasks). We randomly selected and evaluated 200 instances for VIBI and 100 instances for the others. Five workers were assigned per instance. For the digit recognition model, we asked humans to directly score the explanation on a 0\u20135 scale. Each method was evaluated by 16 graduate students at Carnegie Mellon University who have taken at least one graduate-level machine learning class. For each method, 100 instances were randomly selected and evaluated. Four cognitive chunks with the size 4\u00d7 4 were provided as an explanation for each instance (\u03b2 = 0.1 for VIBI). On average, 4.26 students were assigned per instance. Further details regarding the experiments can be found in Supplementary Material S3.\nFor IMDB, the percentage indicates how well the MTurk worker\u2019s answers match the black-box output. For MNIST, the score indicates how well the highlighted chunks catch key characteristics of handwritten digits. The average scores over all samples is shown on a 0 to 5 scale. See the survey example and detailed result in Supplementary Material Tables S1 and S2 for the detailed result.\nAs shown by the Table 1, VIBI better explains the black-box models. When explaining the movie sentiment prediction model, humans better inferred the (correctly predicted) black-box output given the five keywords when they were provided by VIBI. Therefore, it better captures the most contributing key words to the LSTM decision and better explains why the LSTM predicted each movie review by providing five key words. For explaining the digit recognition model, VIBI also highlighted the most concise chunks for explaining key characteristics of handwritten digit. Thus, it better explains how the CNN model recognized each the handwritten digit.\n1Amazon Mechanical Turk, https://www.mturk.com/\nix"}, {"heading": "4.5 Fidelity", "text": "IMDB\nMNIST\nNote that the black-box models achieved 87% accuracy for IMDB and 97% accuracy for MNIST. \u03b2 = 0.1 for VIBI. Accuracy and 0.95 confidence interval is shown. We performed three runs for each method and reported the best results. See more evaluations using F1-score and further results from different parameter settings in Supplementary Material Table S4 and S6 for approximator fidelity and Table S3 and S5 for rationale fidelity.\nWe assessed fidelity of the methods in approximating the black-box output. First, we compared the ability of the approximators to imitate behaviour of the black-box, denoted as Approximator fidelity. (See Supplementary Material S2.4 for details about how each approximator fidelity is evaluated.) As shown in Table 2, VIBI has a better approximator fidelity than Saliency, LIME and SHAP in most cases. VIBI and L2X showed similar levels of approximator fidelity, so we further compared them based on Rationale fidelity. The difference between approximator and rationale fidelity is as follows. Approximator fidelity is quantified by prediction performance of the approximators that takes t\u2217, the continuous relaxation of t, as an input and the black-box output as a targeted label; rationale fidelity is quantified by using t instead of t\u2217. Note that t only takes the top k chunks and sets the others to be zero, while t\u2217 sets the others to be small, non-zero values. Therefore, rationale fidelity allows to evaluate how much information purely flows through the explanations, not through a narrow crack made during the continuous relaxation procedure. As shown in Table 2, VIBI has a better rationale fidelity than L2X in most cases. Note that L2X can be viewed as a special case of VIBI without the compressiveness term, i.e., \u03b2 = 0. The rationale fidelity empirically demonstrates that the compressiveness term can help the information to flow purely through the explanations."}, {"heading": "5 Conclusion", "text": "We employ the information bottleneck principle as a criterion for learning \u2018good\u2019 explanations. Instance-wisely selected cognitive chunks work as an information bottleneck, hence, provide concise but comprehensive explanations for each decision made by a black-box system."}, {"heading": "S2 Experimental Details", "text": "The settings of hyperparameter tuning include the followings (bold indicate the choice for our final model): the temperature for Gumbel-softmax approximation \u03c4 \u2013 {0.1, 0.2, 0.5,0.7, 1}, learning rate \u2013 5 \u00d7 10\u22123, 10\u22123, 5 \u00d7 10\u22124,10\u22124, 5 \u00d7 10\u22125} and \u03b2 \u2013 {0, 0.001,0.01, 0.1, 1, 10, 100}. We use Adam algorithm (Kingma & Ba, 2014) with batch size 100 for MNIST and 50 for IMDB, the coefficients used for computing running averages of gradient and its square (\u03b21, \u03b22) = (0.5, 0.999), and = 10\u22128. We tuned the hyperparameters via grid search and picked up the hyperparameters that yield the best fidelity score on the validation set. All implementation is performed via PyTorch an open source deep learning platform (Paszke et al., 2017).\nApproximator fidelity is prediction performance of the approximator that takes relaxation t\u2217 as an input and black-box output as a target. In detail, we get a t\u2217 with the Gumble-softmax sampling and make a prediction based on t\u2217. This procedure is repeated for 12 times and the final prediction is made by averaging the 12 prediction scores. Rationale fidelity is prediction performance of the approximator that takes t as an input and black-box output as a target. (Note that t is a masked input that only takes the top k chunks and set others to zero.) The final prediction is made by its prediction score.\nS2.1 LSTM movie sentiment prediction model using IMDB\nBlack-Box Model Structure. Each review is padded or cut to contain 15 sentences and 50 words for each sentence. The architecture consists of a word-embedding layer with size 50 for each word followed by two bidirectional LSTMs, a fully connected layer with two units, and a soft-max layer. The first LSTM layer encodes the word embedding vector and generates a word-representation vector with size 100 for each word. Within each sentence, the word representation vectors are elementwisely averaged to form a size 100 sentence representation vector. The second LSTM layer encodes the sentence representation vector and generates a size 60 review embedding vector.\nVIBI Structure. We parameterize the explainer with a bidirectional LSTM and approximator with a 2D CNN. For the explainer, we use a bidirectional LSTM layer from which the output vector is averaged and followed by log-softmax calculation. The final layer is formed to return a log-probability indicating which cognitive chunks should be taken as an input to the approximator. For the approximator, we use a convolutional layer followed by a ReLU activation function and max-pooling layer and a fully connected layer returning a size-2 vector followed by a logsoftmax calculation. The final layer returns a vector of log-probabilities for the two sentiments (positive/negative).\nS2.2 CNN digit recognition model using MNIST\nBlack-Box Model Structure. The architecture consists of two convolutional layers with the kernel size 5 followed by a max-pooling layer with the pool size 2, two fully connected layers and a soft-max layer. The two convolutional layers contain 10 and 20 filters respectively and the two fully connected layers are composed of 50 and 10 units respectively.\nVIBI Structure. We parameterize each the explainer and approximator using 2D CNNs. Structure of the explainer differs depending on the chunk size. For example, when 4\u00d74 cognitive chunk is used, we use two convolutional layers with the kernel size 5 followed by a ReLU activation function and max-pooling layer with the pool size 2, and one convolutional layer with kernel size 1 returning a 7\u00d77 2D matrix followed by a log-softmax calculation. The final layer returns a vector of log-probabilities for the 49 chunks. The three convolutional layers contains 8, 16, and 1 filters respectively. The output from the explainer indicates which cognitive chunks should be taken as an input for the approximator. We parameterize the approximator using two convolutional layers with kernel size 5 followed by a ReLU activation function and max-pooling layer with pool size 2 and with 32 and 64 filters respectively, and one fully connected layer returning a size-10 vector followed by a log-softmax calculation so that the final layer returns a vector of log-probabilities for the ten digits.\nS2.3 TCR to epitope binding prediction model using VDJdb and IEDB\nData. We use two public datasets: VDJdb and IEDB. VDJdb (Shugay et al., 2017) contains the T-cell receptor (TCR) sequences with known antigen specificity (i.e., epitope sequences). We use a VDJdb dataset preprocessed by Jokinen et al. (2019) (See their paper for details in data preprocessing). The preprocessed dataset contains 5,784 samples (2,892 positively and 2,892 negatively binding pairs of TCR and epitope). This dataset consists of 4,363 unique TCR sequences and 21 unique epitope sequences. We group the pairs of TCR and epitope sequences into training, validation, and test sets, which have 4,627, 578, and 579 pairs, respectively. IEDB (Vita et al., 2014) contains TCR sequences and corresponding epitopes. We used an IEDB dataset preprocessed by Jurtz et al. (2018). The preprocessed dataset contains 9,328 samples (positively binding pairs only) and consists of 9,221 unique TCR sequences and 98 unique epitopes sequences. We used the IEDB dataset as out-ouf-samples. There are 6 epitopes contained in both VDJdb and IEDB dataset: GILGFVFTL, GLCTLVAML, NLVPMVATV, LLWNGPMAV, YVLDHLIVV, CINGVCWTV.\nBlack-Box Model Structure. Each TCR and epitope sequence is padded or cut to contain 20 and 13 amino acids, respectively. The embedding matrix has a size 24 and is initialized with BLOSUM50 matrix (Henikoff & Henikoff, 1992). The architecture consists of two sequence encoders that process TCR and epitope sequences each, and three dense layers that process the encoded sequences together. The TCR encoder consists of a dropout layers with the drop-out probability 0.3, two convolutional layers with the kernel size 3 followed by a batch normalization layer, a ReLU activation function and max-pooling layer with the pool size 3. The two convolutional layers contain 32 and 16 filters respectively. Structure of the epitope encoder is the same with the one from the TCR encoder. We optimized the models with the following search space (bold indicate the choice for our final model): the batch size \u2013 {25,50, 100}, learning rate \u2013 0.05, 0.01, 0.005,0.001, 0.0005} and filter size \u2013 {(16, 8), (32,16)}.\nVIBI Structure. We parameterize the explainer and approximator using 2D CNNs and several dense layers. Each TCR and epitope sequence is preprocessed and embedded in the same way as for the black-box model. We have two types of explainers: one for TCR sequence and another for epitope sequence. Each explainer encodes both TCR and epitope sequences and concatenates them through three dense layers followed by a ReLU activation function. For the TCR explainer, the three dense layers contain 32, 16, 20 hidden-units, respectively. For the epitope explainer, the three dense layers contain 32, 16, 13 hidden-units, respectively. The TCR and epitope encoders have the same architectures with those of the black-box model. Each explainer then returns a vector of log-probabilities that indicate which peptides in TCR or epitope should be selected as an\nexplanation. The approximator has the same architecture as the black-box model. We optimize the models with the following search space (bold indicate the choice for our final model): the batch size \u2013 {25,50, 100}, learning rate \u2013 0.0001,0.001, 0.01, 0.1}.\nS2.4 Baseline methods\nHyperparameter tuning. For L2X, we used the same hyperparameter search space of VIBI. For LIME, we tuned the segmentation filter size over {1,2, 4} for MNIST and {10, 25} for IMDB. For all methods, we tuned the hyperparameters via grid search and picked up the hyperparameters that yield the best fidelity score on the validation set.\nCognitive chunk. LIME, SHAP and Saliency yield an attribution score for each feature. A chunk-attribution score is an average of (absolute) attribution scores of features that belong to the chunk. Top k chunks that have the highest chunk-attribution scores are selected. L2X selects chunks in the same way as VIBI.\nFidelity evaluation. LIME, SHAP, Saliency, and L2X all have their own approximators. LIME and SHAP have a sparse linear approximator for each black-box instance (See Section 3.4 in Ribeiro et al. (2016), and Equation (3) in Lundberg & Lee (2017)). Saliency has a 1st-order Taylor approximation to each black-box instance (See Equation (3) in Simonyan et al. (2013)). The approximator fidelity of LIME, SHAP, and Saliency is calculated using their proposed approximators that are composed of the selected features. L2X has a deep neural network that approximates a black-box model (See Section 4 in Chen et al. (2018)). The approximator and rationale fidelity of L2X is calculated in the same way as VIBI.\nS3 Interpretability evaluated by humans\nS3.1 LSTM movie sentiment prediction model using IMDB dataset\nEvaluation\nTable S1: Evaluation of Interpretability on an LSTM movie sentiment prediction model using IMDB.\nBlack-Box Output Recognizedby Mturk worker Saliency LIME L2X VIBI (Ours)\nPositive Positive 19.8 17.4 17.6 24.3 Positive Negative 12.6 6.8 7.2 16.9 Positive Neutral 25.6 18.8 24.2 11.1 Negative Positive 11.0 10.6 11.6 16.2 Negative Negative 14.4 16.4 18.0 20.4 Negative Neutral 16.6 30.0 21.4 11.2\nThe percentage of samples belongs to each combination of the black-box output and the sentiment recognized by workers at Amazon Mechanical Turk (https://www.mturk.com/) are showed\nWe evaluated interpretability of the methods on the LSTM movie sentiment prediction model. The interpretable machine learning methods were evaluated by workers at Amazon Mechanical Turk (https://www.mturk.com/) who are awarded the Masters Qualification (i.e. high performance workers who have demonstrated excellence across a wide range of task). Randomly selected instances (200 for VIBI and 100 for the others) were evaluated for each method. 5 workers are assigned per instance.\nWe provided instances that the black-box model had correctly predicted and asked humans to infer the output of the primary sentiment of the movie review (Positive/Negative/Neutral) given five key words selected by each method. See the survey example below for further details. Note that this is a proxy for measuring how well humans infer the black-box output given explanations; we use such proxy because the workers are general public who are not familiar with the term \u2018black-box\u2019 or \u2018output of the model.\u2019\nIn Table S1, the percentage of samples belongs to each combination of the black-box output and the sentiment recognized by the workers are showed. VIBI has the highest percentage of samples belonging to the Positive/Positive or Negative/Negataive and the lowest percentage of samples belonging to the Positive/Neutral or Negative/Neutral. LIME has the lowest percentage of samples belonging to the Positive/Negative or Negative/Positive, but it is because LIME tends to select words such as \u2018that\u2019, \u2018the\u2019, \u2018is\u2019 so that most of samples are recognized as Neutral.\nSurvey example for IMDB\nTitle: Label sentiment given a few words. Description: Recognize the primary sentiment of the movie review given a few words only.\nFigure S1: A survey example of MTurk evaluation on the LSTM movie sentiment prediction model\nS3.2 CNN digit recognition model using MNIST\nEvaluation\nTable S2: Evaluation of Interpretability on a 2d CNN digit recognition model using MNIST.\nMNIST Digit Saliency LIME L2X VIBI (Ours)\n0 3.200 2.000 2.333 3.000 1 4.393 0.795 1.263 3.913 2 3.125 1.200 1.400 3.200 3 3.286 1.833 2.429 3.625 4 3.333 1.000 1.857 3.857 5 3.167 1.381 2.000 2.875 6 3.333 1.000 1.889 3.625 7 3.667 2.000 1.667 4.000 8 3.750 1.333 2.667 3.500 9 3.222 1.143 1.857 3.667\nAve. over digits 3.448 1.369 1.936 3.526\nThe average scores (0\u20135 scale) evaluated by graduate students at at Carnegie Mellon University are showed.\nWe evaluated interpretability of the methods on the CNN digit recognition model. The interpretable machine learning methods were evaluated by 16 graduate students at XXX University (Blinded due to the double blinding reviewing) who have taken at least one graduate-level machine learning class. Randomly selected 100 instances were evaluated for each method. On average, 4.26 students are assigned per instance. See the survey example below for further details. For the digit recognition model, we asked humans to directly score the explanation on a 0\u20135 scale.\nIn Table S2, the average score per digit is showed. VIBI outperforms L2X and LIME and slightly outperforms Saliency in terms of the average score over digits. VIBI outperforms at digit 2, 3, 4, 6, 7, and 9, and performs comparable to Saliency at 0, 1, 5, 8.\nL2 X\nV IB I\nS H A P\nLI M E\nS al ie nc y\nL2 X\nV IB I\nS H A P\nLI M E\nS al ie nc y\nFigure S2: The hand-written digits and explanations provided by VIBI and the baselines. The examples are randomly selected from the validation set. The selected patches are colored red if the pixel is activated (i.e. white) and yellow otherwise (i.e. black). A patch composed of 4\u00d7 4 pixels is used as a cognitive chunk and k = 4 patches are identified for each image.\nSurvey example for MNIST\nInstruction \u25cf 2D CNN model is used for digit recognition for MNIST dataset\nExplain by highlighting key pixels that play an\nimportant role in the 2D CNN digit recognition.\n2D CNN model Recognized\nas 9\n\u25cf Now, an interpretable learning method explains a decision made by the 2D CNN model:\nRecognized as 9\nMNIST is a large dataset contains 28 x 28 sized images of handwritten digits (0 to 9). Here, a 2D convolutional neural network (CNN) is used for the digit recognition for MNIST. Several interpretable machine learning methods are learned to explain the model by highlighting key pixels that play an important role in the CNN digit recognition. The highlighted pixels provides an explanation for a handwritten image why the CNN model recognized the handwriting as it does. Your task is to evaluate the explanation for each instance on a scale 0 to 5.Instruction\n0 53\n:the pixels are randomly highlighted.\n: the highlighted pixel concisely catches key characteristic of each digit.\nPlease score each instance based on following criteria:\n1 2 4\nConcise explanationNo explanation Insufficient or redundant explanation\n: some highlighted pixels are redundant or\nFigure S3: A survey example of evaluation on the MNIST digit recognition model."}, {"heading": "S4 Fidelity", "text": "Table S3: Evaluation of rationale fidelity on LSTM movie sentiment prediction model using IMDB.\nL2X VIBI (Ours)\nchunk size k 0 0.001 0.01 0.1 1 10 100\nAccuracy\nsentence 1 0.727 0.693 0.711 0.731 0.729 0.734 0.734 word 5 0.638 0.657 0.666 0.657 0.648 0.640 0.654 5 words 1 0.601 0.630 0.624 0.632 0.628 0.623 0.628 5 words 3 0.694 0.660 0.662 0.660 0.662 0.660 0.660\nF1-score\nsentence 1 0.581 0.547 0.562 0.567 0.585 0.586 0.586 word 5 0.486 0.521 0.551 0.512 0.516 0.508 0.526 5 words 1 0.478 0.506 0.500 0.540 0.501 0.498 0.504 5 words 3 0.551 0.528 0.529 0.522 0.529 0.528 0.525\nRationale fidelity quantifies ability of the selected chunks to infer the black-box output. A large rationale fidelity implies that the selected chunks account for a large portion of the approximator fidelity. Prediction accuracy and F1-score of the approximator for the CNN model are shown. (\u03b2 = 0, 0.001, 0.01, 0.1, 1, 10, 100)\nTable S4: Evaluation of approximator fidelity on LSTM movie sentiment prediction model using IMDB.\nSaliency LIME SHAP L2X VIBI (Ours)\nchunk size k 0 0.001 0.01 0.1 1 10 100\nAccuracy\nsentence 1 0.387 0.727 0.495 0.876 0.877 0.869 0.877 0.879 0.879 0.884 word 5 0.419 0.756 0.501 0.738 0.766 0.772 0.744 0.773 0.763 0.767 5 words 1 0.424 0.290 0.496 0.759 0.784 0.780 0.764 0.774 0.778 0.774 5 words 3 0.414 0.679 0.491 0.833 0.836 0.831 0.835 0.834 0.830 0.833\nF1-score\nsentence 1 0.331 0.564 0.400 0.721 0.693 0.707 0.730 0.730 0.727 0.734 word 5 0.350 0.585 0.413 0.565 0.607 0.616 0.594 0.620 0.609 0.612 5 words 1 0.360 0.302 0.418 0.621 0.641 0.622 0.624 0.615 0.622 0.616 5 words 3 0.352 0.523 0.409 0.680 0.683 0.674 0.681 0.677 0.669 0.682\nApproximator fidelity quantifies ability of the approximator to imitate the behaviour of a black-box. Prediction accuracy and F1-score of the approximator for the LSTM model are shown. (\u03b2 = 0, 0.001, 0.01, 0.1, 1, 10, 100)\nTable S5: Evaluation of the rationale fidelity on CNN digit recognition model using MNIST.\nL2X VIBI (Ours)\nchunk size k 0 0.001 0.01 0.1 1 10 100\nAccuracy\n1\u00d7 1 64 0.694 0.690 0.726 0.689 0.742 0.729 0.766 1\u00d7 1 96 0.814 0.831 0.780 0.806 0.859 0.765 0.826 1\u00d7 1 160 0.903 0.907 0.905 0.917 0.917 0.928 0.902 2\u00d7 2 16 0.735 0.795 0.750 0.771 0.732 0.753 0.769 2\u00d7 2 24 0.776 0.855 0.834 0.856 0.868 0.854 0.847 2\u00d7 2 40 0.811 0.914 0.914 0.915 0.903 0.918 0.935 2\u00d7 2 80 0.905 0.949 0.940 0.939 0.962 0.941 0.923 4\u00d7 4 4 0.650 0.655 0.650 0.775 0.717 0.682 0.681 4\u00d7 4 6 0.511 0.858 0.706 0.701 0.708 0.690 0.730 4\u00d7 4 10 0.835 0.835 0.824 0.933 0.875 0.854 0.782 4\u00d7 4 20 0.954 0.962 0.815 0.934 0.929 0.946 0.943\nF1-score\n1\u00d7 1 64 0.684 0.679 0.716 0.670 0.734 0.710 0.755 1\u00d7 1 96 0.808 0.825 0.750 0.803 0.854 0.750 0.820 1\u00d7 1 160 0.898 0.902 0.899 0.912 0.913 0.924 0.897 2\u00d7 2 16 0.720 0.786 0.738 0.761 0.723 0.744 0.769 2\u00d7 2 24 0.766 0.848 0.836 0.851 0.858 0.859 0.840 2\u00d7 2 40 0.798 0.914 0.910 0.910 0.898 0.914 0.931 2\u00d7 2 80 0.901 0.946 0.936 0.930 0.959 0.938 0.918 4\u00d7 4 4 0.634 0.658 0.637 0.763 0.704 0.671 0.669 4\u00d7 4 6 0.493 0.852 0.693 0.687 0.692 0.675 0.720 4\u00d7 4 10 0.828 0.827 0.816 0.928 0.869 0.849 0.773 4\u00d7 4 20 0.950 0.959 0.806 0.931 0.926 0.942 0.940\nRationale fidelity quantifies ability of the selected chunks to infer the black-box output. A large rationale fidelity implies that the selected chunks account for a large portion of the approximator fidelity. Prediction accuracy and F1-score of the approximator for the CNN model are shown. (\u03b2 = 0, 0.001, 0.01, 0.1, 1, 10, 100)\nTable S6: Evaluation of the approximator fidelity on CNN digit recognition model using MNIST.\nSaliency LIME SHAP L2X VIBI (Ours)\nchunk size k 0 0.001 0.01 0.1 1 10 100\nAccuracy\n1\u00d7 1 64 0.944 0.982 0.955 0.933 0.959 0.962 0.959 0.960 0.952 0.953 1\u00d7 1 96 0.956 0.986 0.950 0.963 0.963 0.951 0.967 0.968 0.953 0.962 1\u00d7 1 160 0.964 0.989 0.958 0.970 0.967 0.973 0.974 0.974 0.974 0.967 2\u00d7 2 16 0.912 0.770 0.942 0.934 0.945 0.941 0.948 0.938 0.939 0.940 2\u00d7 2 24 0.938 0.807 0.954 0.951 0.956 0.955 0.953 0.953 0.953 0.960 2\u00d7 2 40 0.957 0.859 0.954 0.967 0.965 0.966 0.962 0.967 0.965 0.967 2\u00d7 2 80 0.966 0.897 0.966 0.976 0.977 0.974 0.972 0.977 0.971 0.973 4\u00d7 4 4 0.863 0.609 0.948 0.953 0.922 0.928 0.948 0.942 0.942 0.953 4\u00d7 4 6 0.906 0.637 0.936 0.957 0.963 0.954 0.956 0.953 0.963 0.962 4\u00d7 4 10 0.949 0.705 0.951 0.965 0.971 0.959 0.967 0.961 0.969 0.964 4\u00d7 4 20 0.963 0.771 0.955 0.974 0.977 0.975 0.975 0.973 0.975 0.974\nF1-score\n1\u00d7 1 64 0.938 0.981 0.952 0.930 0.956 0.960 0.956 0.957 0.950 0.950 1\u00d7 1 96 0.950 0.985 0.948 0.961 0.961 0.954 0.965 0.966 0.951 0.960 1\u00d7 1 160 0.959 0.989 0.954 0.969 0.965 0.971 0.973 0.972 0.972 0.966 2\u00d7 2 16 0.902 0.755 0.938 0.930 0.942 0.936 0.944 0.934 0.936 0.936 2\u00d7 2 24 0.932 0.795 0.951 0.949 0.954 0.952 0.950 0.951 0.950 0.958 2\u00d7 2 40 0.952 0.853 0.946 0.965 0.963 0.964 0.961 0.965 0.963 0.965 2\u00d7 2 80 0.962 0.892 0.963 0.974 0.976 0.973 0.972 0.975 0.969 0.971 4\u00d7 4 4 0.849 0.588 0.943 0.951 0.917 0.923 0.944 0.938 0.939 0.950 4\u00d7 4 6 0.895 0.621 0.920 0.954 0.961 0.951 0.953 0.950 0.960 0.959 4\u00d7 4 10 0.943 0.689 0.946 0.963 0.969 0.956 0.965 0.958 0.968 0.961 4\u00d7 4 20 0.959 0.763 0.952 0.972 0.976 0.972 0.974 0.972 0.973 0.972\nApproximator fidelity quantifies ability of the approximator to imitate the behaviour of a black-box. Prediction accuracy and F1-score of the approximator for the CNN model are shown. (\u03b2 = 0, 0.001, 0.01, 0.1, 1, 10, 100)"}], "title": "Explaining A Black-box By Using A Deep Variational Information Bottleneck Approach", "year": 2019}