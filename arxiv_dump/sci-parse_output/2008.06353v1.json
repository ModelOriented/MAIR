{"abstractText": "Artificial intelligence (AI) has shown great promise for diagnostic imaging assessments. However, the application of AI to support medical diagnostics in clinical routine comes with many challenges. The algorithms should have high prediction accuracy but also be transparent, understandable and reliable. Thus, explainable artificial intelligence (XAI) is highly relevant for this domain. We present a survey on XAI within digital pathology, a medical imaging sub-discipline with particular characteristics and needs. The review includes several contributions. Firstly, we give a thorough overview of current XAI techniques of potential relevance for deep learning methods in pathology imaging, and categorise them from three different aspects. In doing so, we incorporate uncertainty estimation methods as an integral part of the XAI landscape. We also connect the technical methods to the specific prerequisites in digital pathology and present findings to guide future research efforts. The survey is intended for both technical researchers and medical professionals, one of the objectives being to establish a common ground for cross-disciplinary discussions.", "authors": [{"affiliations": [], "name": "Milda Pocevi\u010di\u016bt\u0117"}, {"affiliations": [], "name": "Gabriel Eilertsen"}, {"affiliations": [], "name": "Claes Lundstr\u00f6m"}], "id": "SP:15454a6b6bf9cac01ce79c12c8699df9e0ee1ea1", "references": [{"authors": ["A. Adadi", "M. Berrada"], "title": "Peeking inside the black-box: A survey on explainable artificial intelligence (XAI)", "venue": "IEEE Access", "year": 2018}, {"authors": ["J. Adebayo", "J. Gilmer", "M. Muelly", "I. Goodfellow", "M. Hardt", "B. Kim"], "title": "Sanity checks for saliency maps", "venue": "Proceedings of the 32nd International Conference on Neural Information Processing Systems. p. 9525\u20139536", "year": 2018}, {"authors": ["B. Alsallakh", "A. Jourabloo", "M. Ye", "X. Liu", "L. Ren"], "title": "Do convolutional neural networks learn class hierarchy", "venue": "IEEE Transactions on Visualization and Computer Graphics", "year": 2018}, {"authors": ["L. Alzubaidi", "R. Resan", "H. Abdul Hussain"], "title": "A Robust Deep Learning Approachto Detect Nuclei in Histopathological Images", "venue": "International Journal of Innovative Research in Computer and Communication Engineering 5(March),", "year": 2017}, {"authors": ["E. Arvaniti", "K.S. Fricker", "M. Moret", "N. Rupp", "T. Hermanns", "C. Fankhauser", "N. Wey", "P.J. Wild", "J.H. R\u00fcschoff", "M. Claassen"], "title": "Automated Gleason grading of prostate cancer tissue microarrays via deep learning", "venue": "Scientific Reports", "year": 2018}, {"authors": ["M.S. Ayhan", "P. Berens"], "title": "Test-time data augmentation for estimation of heteroscedastic aleatoric uncertainty in deep neural networks", "venue": "Medical Imaging with Deep Learning (Midl 2018),", "year": 2018}, {"authors": ["S. Bach", "A. Binder", "G. Montavon", "F. Klauschen", "K.R. M\u00fcller", "W. Samek"], "title": "On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation", "venue": "PLoS ONE", "year": 2015}, {"authors": ["M. Balkenhol", "D. Tellez", "W. Vreuls", "P. Clahsen", "H. Pinckaers", "F. Ciompi", "P. Bult", "J. van der Laak"], "title": "Deep learning assisted mitotic counting for breast cancer", "venue": "Laboratory Investigation", "year": 2019}, {"authors": ["D. Bau", "B. Zhou", "A. Khosla", "A. Oliva", "A. Torralba"], "title": "Network dissection: Quantifying interpretability of deep visual representations", "venue": "IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017. vol. 2017-Janua,", "year": 2017}, {"authors": ["K. Bera", "K.A. Schalper", "D.L. Rimm", "V. Velcheti", "A. Madabhushi"], "title": "Artificial intelligence in digital pathology \u2014 new tools for diagnosis and precision oncology", "venue": "Nature Reviews Clinical Oncology", "year": 2019}, {"authors": ["C. Blundell", "J. Cornebise", "K. Kavukcuoglu", "D. Wierstra"], "title": "Weight uncertainty in neural networks", "venue": "International Conference on Machine Learning,", "year": 2015}, {"authors": ["D. Bouchacourt", "M. Pawan Kumar", "S. Nowozin"], "title": "DISCO nets: DISsimilarity COefficient Networks", "venue": "Advances in Neural Information Processing Systems", "year": 2016}, {"authors": ["W. Bulten", "H. Pinckaers", "H. van Boven", "R. Vink", "T. de Bel", "B. van Ginneken", "J. van der Laak", "C. Hulsbergen-van de Kaa", "G. Litjens"], "title": "Automated deep-learning system for Gleason grading of prostate cancer using biopsies: a diagnostic study", "venue": "The Lancet. Oncology", "year": 2020}, {"authors": ["C.J. Cai", "E. Reif", "N. Hegde", "J. Hipp", "B. Kim", "D. Smilkov", "M. Wattenberg", "F. Viegas", "G.S. Corrado", "Stumpe", "M.C"], "title": "Human-centered tools for coping with imperfect algorithms during medical decision-making", "venue": "Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems. CHI \u201919,", "year": 2019}, {"authors": ["A.J. Cannon"], "title": "Non-crossing nonlinear regression quantiles by monotone composite quantile regression neural network, with application to rainfall extremes", "venue": "Stochastic Environmental Research and Risk Assessment 32(11),", "year": 2018}, {"authors": ["S. Carter", "Z. Armstrong", "L. Schubert", "I. Johnson", "C. Olah"], "title": "Activation atlas. Distill 4(3) (mar 2019)", "year": 2019}, {"authors": ["H. Chen", "Q. Dou", "X. Wang", "J. Qin", "P.A. Heng"], "title": "Mitosis detection in breast cancer histology images via deep cascaded networks", "venue": "Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence. p. 1160\u20131166", "year": 2016}, {"authors": ["J. Chen", "C. Srinivas"], "title": "Automatic lymphocyte detection in h&e images with deep neural networks", "year": 2016}, {"authors": ["P. Dabkowski", "Y. Gal"], "title": "Real time image saliency for black box classifiers", "venue": "Advances in Neural Information Processing Systems. vol. 2017-Decem,", "year": 2017}, {"authors": ["S. Depeweg", "J.M. Hernandez-Lobato", "F. Doshi-Velez", "S. Udluft"], "title": "Decomposition of uncertainty in bayesian deep learning for efficient and risksensitive learning", "venue": "International Conference on Machine Learning, ICML 2018", "year": 2018}, {"authors": ["A. Der Kiureghian", "O. Ditlevsen"], "title": "Aleatory or epistemic? Does it matter", "venue": "Structural Safety 31,", "year": 2008}, {"authors": ["A. Dosovitskiy", "T. Brox"], "title": "Inverting visual representations with convolutional networks", "venue": "Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition. vol. 2016-Decem,", "year": 2016}, {"authors": ["F.K. Do\u0161ilovi\u0107", "M. Br\u010di\u0107", "N. Hlupi\u0107"], "title": "Explainable artificial intelligence: A survey", "year": 2018}, {"authors": ["B. Efron", "R. Tibshirani"], "title": "An introduction to the bootstrap", "venue": "Monographs on statistics and applied probability:", "year": 1994}, {"authors": ["B. Ehteshami Bejnordi", "M. Veta", "P. Johannes van Diest", "B. van Ginneken", "N. Karssemeijer", "G. Litjens", "van der Laak"], "title": "J.A.W.M., , the CAMELYON16 Consortium: Diagnostic Assessment of Deep Learning Algorithms for Detection of Lymph Node Metastases in Women With Breast Cancer", "year": 2017}, {"authors": ["G. Eilertsen", "D. J\u00f6nsson", "T. Ropinski", "J. Unger", "A. Ynnerman"], "title": "Classifying the classifier: dissecting the weight space of neural networks (2020)", "venue": "arXiv preprint,", "year": 2002}, {"authors": ["D. Erhan", "Y. Bengio", "A. Courville", "P. Vincent"], "title": "Visualizing higher-layer features of a deep network", "venue": "Technical Report, Univeriste\u0301 de Montre\u0301al", "year": 2009}, {"authors": ["K. Fang", "C. Shen", "D. Kifer"], "title": "Evaluating aleatoric and epistemic uncertainties of time series deep learning models for soil moisture predictions (2019)", "venue": "arXiv preprint,", "year": 1906}, {"authors": ["R.C. Fong", "A. Vedaldi"], "title": "Interpretable Explanations of Black Boxes by Meaningful Perturbation", "venue": "Proceedings of the IEEE International Conference on Computer Vision. vol. 2017-Octob,", "year": 2017}, {"authors": ["M.M. Fraz", "M. Shaban", "S. Graham", "S.A. Khurram", "N.M. Rajpoot"], "title": "Uncertainty driven pooling network for microvessel segmentation in routine histology", "venue": "Computational Pathology and Ophthalmic Medical Image Analysis", "year": 2018}, {"authors": ["Y. Gal", "Z. Ghahramani"], "title": "Bayesian convolutional neural networks with Bernoulli approximate variational inference (2015)", "venue": "arXiv preprint,", "year": 2015}, {"authors": ["Y. Gal", "Z. Ghahramani"], "title": "Dropout as a Bayesian approximation: Representing model uncertainty in deep learning", "venue": "International Conference on Machine Learning, ICML 2016", "year": 2016}, {"authors": ["E. Garcia", "R. Hermoza", "C.B. Castanon", "L. Cano", "M. Castillo", "C. Castan\u00f1eda"], "title": "Automatic lymphocyte detection on gastric cancer IHC images using deep learning", "venue": "IEEE 30th International Symposium on Computer-Based Medical Systems (CBMS)", "year": 2017}, {"authors": ["H. Garud", "S.P. Karri", "D. Sheet", "J. Chatterjee", "M. Mahadevappa", "A.K. Ray", "A. Ghosh", "A.K. Maity"], "title": "High-Magnification Multi-views Based Classification of Breast Fine Needle Aspiration Cytology", "venue": "Cell 26 M. Pocevic\u030ciu\u0304te\u0307 et al. Samples Using Fusion of Decisions from Deep Convolutional Networks. In: IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops", "year": 2017}, {"authors": ["R. Goebel", "A. Chander", "K. Holzinger", "F. Lecue", "Z. Akata", "S. Stumpf", "P. Kieseberg", "A. Holzinger"], "title": "Explainable AI: the new 42", "venue": "Machine Learning and Knowledge Extraction, vol. LNCS-", "year": 2018}, {"authors": ["D. Hafner", "D. Tran", "T. Lillicrap", "A. Irpan", "J. Davidson"], "title": "Reliable uncertainty estimates in deep neural networks using noise contrastive priors", "venue": "Iclr pp", "year": 2019}, {"authors": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "title": "Deep residual learning for image recognition", "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR)", "year": 2016}, {"authors": ["J.M. Hern\u00e1ndez-Lobato", "R.P. Adams"], "title": "Probabilistic backpropagation for scalable learning of Bayesian neural networks", "venue": "International Conference on Machine Learning,", "year": 2015}, {"authors": ["H. H\u00f6fener", "A. Homeyer", "N. Weiss", "J. Molin", "C.F. Lundstr\u00f6m", "H.K. Hahn"], "title": "Deep learning nuclei detection: A simple approach can deliver stateof-the-art results", "venue": "Computerized Medical Imaging and Graphics", "year": 2018}, {"authors": ["F. Hohman", "M. Kahng", "R. Pienta", "D.H. Chau"], "title": "Visual Analytics in Deep Learning: An Interrogative Survey for the Next Frontiers", "venue": "IEEE Transactions on Visualization and Computer Graphics", "year": 2019}, {"authors": ["A. Holzinger", "A. Carrington", "H. M\u00fcller"], "title": "Measuring the quality of explanations: The System Causability Scale (SCS). comparing human and machine explanations", "venue": "Special Issue on Interactive Machine Learning,", "year": 2020}, {"authors": ["A. Holzinger", "G. Langs", "H. Denk", "K. Zatloukal", "H. M\u00fcller"], "title": "Causability and explainabilty of artificial intelligence in medicine", "venue": "Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery p", "year": 2019}, {"authors": ["L. Hou", "D. Samaras", "T.M. Kurc", "Y. Gao", "J.E. Davis", "J.H. Saltz"], "title": "Patch-based convolutional neural network for whole slide tissue image classification", "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR). pp", "year": 2016}, {"authors": ["Y. Huang", "A.C.S. Chung"], "title": "Evidence localization for pathology images using weakly supervised learning", "venue": "MICCAI", "year": 2019}, {"authors": ["H. Jung", "B. Lodhi", "J. Kang"], "title": "An automatic nuclei segmentation method based on deep convolutional neural networks for histopathology", "venue": "images. BMC Biomedical Engineering 1(1),", "year": 2019}, {"authors": ["H. Kaur", "H. Nori", "S. Jenkins", "R. Caruana", "H. Wallach", "J. Wortman Vaughan"], "title": "Interpreting interpretability: Understanding data scientists\u2019 use of interpretability tools for machine learning", "venue": "CHI Conference on Human Factors in Computing Systems", "year": 2020}, {"authors": ["A. Kendall", "Y. Gal"], "title": "What uncertainties do we need in Bayesian deep learning for computer vision", "venue": "Advances in Neural Information Processing Systems. vol", "year": 2017}, {"authors": ["F. Kevin", "X. Quin", "H. Dominick", "G. Kartikay", "V. Zoya", "D. Ugljesa", "D. Phedias"], "title": "Visualizing histopathologic deep learning classification and anomaly detection using nonlinear feature space dimensionality reduction", "venue": "BMC Bioinformatics", "year": 2018}, {"authors": ["M.E. Khan", "D. Nielsen", "V. Tangkaratt", "W. Lin", "Y. Gal", "A. Srivastava"], "title": "Fast and scalable Bayesian deep learning by weight-perturbation in Adam", "venue": "International Conference on Machine Learning, ICML 2018", "year": 2018}, {"authors": ["M.E.E. Khan", "A. Immer", "E. Abedi", "M. Korzepa"], "title": "Approximate inference turns deep networks into gaussian processes", "venue": "Advances in Neural Information Processing Systems", "year": 2019}, {"authors": ["B. Kim", "M. Wattenberg", "J. Gilmer", "C. Cai", "J. Wexler", "F. Viegas", "R. Sayres"], "title": "Interpretability beyond feature attribution: Quantitative Testing with Concept Activation Vectors (TCAV)", "venue": "In: 35th International Conference on Machine Learning, ICML 2018", "year": 2018}, {"authors": ["P.J. Kindermans", "K.T. Sch\u00fctt", "M. Alber", "K.R. M\u00fcller", "D. Erhan", "B. Kim", "S. D\u00e4hne"], "title": "Learning how to explain neural networks: Patternnet and Patternattribution", "venue": "In: 6th International Conference on Learning Representations,", "year": 2018}, {"authors": ["V. Koelzer", "A. Gisler", "J.C. Hanhart", "J. Griss", "S. Wagner", "N. Willi", "G. Cathomas", "M. Sachs", "W. Kempf", "D. Thommen", "K. Mertz"], "title": "Digital image analysis improves precision of programmed death ligand 1 (PD-L1) scoring in cutaneous melanoma", "venue": "Histopathology", "year": 2018}, {"authors": ["P.W. Koh", "P. Liang"], "title": "Understanding black-box predictions via influence functions", "venue": "Proceedings of the 34th International Conference on Machine Learning - Volume", "year": 2017}, {"authors": ["D. Komura", "S. Ishikawa"], "title": "Machine learning methods for histopathological image analysis", "venue": "Computational and Structural Biotechnology Journal", "year": 2018}, {"authors": ["Y. Kwon", "J.H. Won", "B.J. Kim", "M.C. Paik"], "title": "Uncertainty quantification using Bayesian neural networks in classification: Application to biomedical image segmentation", "venue": "Computational Statistics & Data Analysis", "year": 2020}, {"authors": ["B. Lakshminarayanan", "A. Pritzel", "C. Blundell"], "title": "Simple and scalable predictive uncertainty estimation using deep ensembles", "venue": "Advances in Neural Information Processing Systems. vol. 2017-Decem,", "year": 2017}, {"authors": ["S. Lapuschkin", "S. W\u00e4ldchen", "A. Binder", "G. Montavon", "W. Samek", "K.R. M\u00fcller"], "title": "Unmasking Clever Hans predictors and assessing what machines really learn", "venue": "Nature Communications", "year": 2019}, {"authors": ["K. Leino", "S. Sen", "A. Datta", "M. Fredrikson", "L. Li"], "title": "Influencedirected explanations for deep convolutional networks", "venue": "Proceedings - International Test Conference. vol. 2018-Octob", "year": 2018}, {"authors": ["G.J.S. Litjens", "T. Kooi", "B.E. Bejnordi", "A.A.A. Setio", "F. Ciompi", "M. Ghafoorian", "van der Laak", "J.A.W.M", "B. van Ginneken", "C.I. S\u00e1nchez"], "title": "A survey on deep learning in medical image analysis", "venue": "Medical Image Analysis", "year": 2017}, {"authors": ["Y. Liu", "K. Gadepalli", "M. Norouzi", "G.E. Dahl", "T. Kohlberger", "A. Boyko", "S. Venugopalan", "A. Timofeev", "P.Q. Nelson", "G.S. Corrado", "J.D. Hipp", "L. Peng", "M.C. Stumpe"], "title": "Detecting cancer metastases on gigapixel pathology", "year": 2017}, {"authors": ["S.M. Lundberg", "S.I. Lee"], "title": "A unified approach to interpreting model predictions", "venue": "Advances in Neural Information Processing Systems", "year": 2017}, {"authors": ["Maaten", "L.v.d", "G. Hinton"], "title": "Visualizing data using t-sne", "venue": "Journal of machine learning research 9(Nov),", "year": 2008}, {"authors": ["A. Mahendran", "A. Vedaldi"], "title": "Understanding deep image representations by inverting them", "venue": "Tech. rep", "year": 2015}, {"authors": ["A. Mahendran", "A. Vedaldi"], "title": "Visualizing deep convolutional neural networks using natural pre-images", "venue": "International Journal of Computer Vision 120(3),", "year": 2016}, {"authors": ["N. Meinshausen"], "title": "Quantile regression forests", "venue": "Journal of Machine Learning Research 7,", "year": 2006}, {"authors": ["B. Mittelstadt", "C. Russell", "S. Wachter"], "title": "Explaining explanations in AI", "venue": "Proceedings of the Conference on Fairness, Accountability, and Transparency", "year": 2019}, {"authors": ["J. Molin", "A. Bod\u00e9n", "D. Treanor", "M. Fjeld", "C. Lundstr\u00f6m"], "title": "Scale stain: Multi-resolution feature enhancement in pathology visualization (2016)", "venue": "arXiv preprint,", "year": 2016}, {"authors": ["J. Molin", "P.W. Woundefinedniak", "C. Lundstr\u00f6m", "D. Treanor", "M. Fjeld"], "title": "Understanding design for automated image analysis in digital pathology", "venue": "Proceedings of the 9th Nordic Conference on Human-Computer Interaction. NordiCHI \u201916,", "year": 2016}, {"authors": ["G. Montavon", "S. Lapuschkin", "A. Binder", "W. Samek", "K.R. M\u00fcller"], "title": "Explaining nonlinear classification decisions with deep Taylor decomposition", "venue": "Pattern Recognition", "year": 2017}, {"authors": ["S.T. Mueller", "R.R. Hoffman", "W. Clancey", "A. Emrey", "G. Klein"], "title": "Explanation in Human-AI Systems: A Literature Meta-Review, Synopsis of Key Ideas and Publications, and Bibliography for Explainable AI (2019)", "venue": "arXiv preprint,", "year": 1902}, {"authors": ["K. Nagpal", "D. Foote", "Y. Liu", "P.H.C. Chen", "E. Wulczyn", "F. Tan", "N. Olson", "J.L. Smith", "A. Mohtashamian", "J.H. Wren", "G.S. Corrado", "R. MacDonald", "L.H. Peng", "M.B. Amin", "A.J. Evans", "A.R. Sangoi", "C.H. Mermel", "J.D. Hipp", "M.C. Stumpe"], "title": "Development and validation of a deep learning algorithm for improving Gleason scoring of prostate cancer. npj Digital Medicine", "year": 2019}, {"authors": ["P.L. Narayanan", "S.E.A. Raza", "A. Dodson", "B. Gusterson", "M. Dowsett", "Y. Yuan"], "title": "Deepsdcs: Dissecting cancer proliferation heterogeneity in Ki67 digital whole slide images (2018)", "venue": "arXiv preprint,", "year": 2018}, {"authors": ["R.M. Neal"], "title": "Bayesian Learning for Neural Networks", "venue": "Ph.D. thesis,", "year": 1995}, {"authors": ["A. Nguyen", "A. Dosovitskiy", "J. Yosinski", "T. Brox", "J. Clune"], "title": "Synthesizing the preferred inputs for neurons in neural networks via deep generator networks", "venue": "Proceedings of the 30th International Conference on Neural Information Processing Systems. p. 3395\u20133403", "year": 2016}, {"authors": ["A. Nguyen", "J. Yosinski", "J. Clune"], "title": "Multifaceted Feature Visualization: Uncovering the Different Types of Features Learned By Each Neuron in Deep Neural Networks (2016)", "venue": "arXiv preprint,", "year": 2016}, {"authors": ["W. Nie", "Y. Zhang", "A. Patel"], "title": "A theoretical explanation for perplexing behaviors of backpropagation-based visualizations", "year": 2018}, {"authors": ["C. Olah", "A. Satyanarayan", "I. Johnson", "S. Carter", "L. Schubert", "K. Ye", "A. Mordvintsev"], "title": "The building blocks of interpretability", "venue": "Distill", "year": 2018}, {"authors": ["I. Osband", "C. Blundell", "A. Pritzel", "B. Van Roy"], "title": "Deep exploration via bootstrapped DQN", "venue": "Advances in Neural Information Processing Systems", "year": 2016}, {"authors": ["I. Palatnik de Sousa", "E. Costa da Silva"], "title": "Local interpretable model-agnostic explanations for classification of lymph node metastases", "venue": "Sensors 19(13),", "year": 1913}, {"authors": ["H. Papadopoulos", "V. Vovk", "A. Gammerman"], "title": "Conformal prediction with neural networks", "venue": "Proceedings - International Conference on Tools with Artificial Intelligence, ICTAI", "year": 2007}, {"authors": ["N. Papernot", "P. McDaniel"], "title": "Deep k-Nearest Neighbors: Towards confident, interpretable and robust deep learning (2018)", "venue": "arXiv preprint,", "year": 2018}, {"authors": ["T. Pearce", "F. Leibfried", "A. Brintrup", "M. Zaki", "A. Neely"], "title": "Uncertainty in Neural Networks: Approximately Bayesian Ensembling (2018)", "venue": "arXiv preprint,", "year": 2018}, {"authors": ["T. Pearce", "M. Zaki", "A. Brintrup", "A. Neely"], "title": "High-quality prediction intervals for deep learning: A distribution-free, ensembled approach", "venue": "In: 35th International Conference on Machine Learning, ICML 2018", "year": 2018}, {"authors": ["B. Pohn", "M. Kargl", "R. Reihs", "A. Holzinger", "K. Zatloukal", "H. Muller"], "title": "Towards a deeper understanding of how a pathologist makes a diagnosis: Visualization of the diagnostic process in histopathology", "venue": "IEEE Symposium on Computers and Communications (ISCC), Computers and Communications (ISCC),", "year": 2019}, {"authors": ["J. Postels", "F. Ferroni", "H. Coskun", "N. Navab", "F. Tombari"], "title": "Sampling-free epistemic uncertainty estimation using approximated variance propagation (2019)", "venue": "arXiv preprint,", "year": 1908}, {"authors": ["M.T. Ribeiro", "S. Singh", "C. Guestrin"], "title": "Why should I trust you?\u201d Explaining the predictions of any classifier", "venue": "Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. vol. 13-17-Augu,", "year": 2016}, {"authors": ["H. Ritter", "A. Botev", "D. Barber"], "title": "A scalable laplace approximation for neural networks", "venue": "Iclr pp", "year": 2018}, {"authors": ["M. Saha", "C. Chakraborty"], "title": "Her2net: A deep framework for semantic segmentation and classification of cell membranes and nuclei in breast cancer evaluation", "venue": "IEEE Transactions on Image Processing", "year": 2018}, {"authors": ["W. Samek", "A. Binder", "G. Montavon", "S. Lapuschkin", "K. M\u00fcller"], "title": "Evaluating the visualization of what a deep neural network has learned", "venue": "IEEE Transactions on Neural Networks and Learning Systems", "year": 2017}, {"authors": ["J. Seah", "J. Tang", "A. Kitchen"], "title": "Generative visual rationales (2018), arXiv preprint, arXiv: 1804.04539 Survey of XAI in digital pathology", "year": 2018}, {"authors": ["R.R. Selvaraju", "M. Cogswell", "A. Das", "R. Vedantam", "D. Parikh", "D. Batra"], "title": "Grad-CAM: Visual explanations from deep networks via gradient-based localization", "venue": "Proceedings of the IEEE International Conference on Computer Vision. vol", "year": 2017}, {"authors": ["A. Serag", "A. Ion-Margineanu", "H. Qureshi", "R. McMillan", "M.J. Saint Martin", "J. Diamond", "P. O\u2019Reilly", "P. Hamilton"], "title": "Translational AI and deep learning in diagnostic pathology", "venue": "Frontiers in Medicine", "year": 2019}, {"authors": ["H. Sharma", "N. Zerbe", "I. Klempert", "O. Hellwich", "P. Hufnagl"], "title": "Deep convolutional neural networks for automatic classification of gastric carcinoma using whole slide images in digital histopathology", "venue": "Computerized Medical Imaging and Graphics", "year": 2017}, {"authors": ["W. Shimoda", "K. Yanai"], "title": "Distinct class saliency maps for multiple object images. Workshop track - ICLR 2016", "year": 2016}, {"authors": ["K. Shridhar", "F. Laumann", "M. Liwicki"], "title": "A comprehensive guide to bayesian convolutional neural network with variational inference (2019)", "venue": "arXiv preprint,", "year": 1901}, {"authors": ["K. Simonyan", "A. Vedaldi", "A. Zisserman"], "title": "Deep inside convolutional networks: Visualising image classification models and saliency", "venue": "maps. In: 2nd International Conference on Learning Representations, ICLR 2014 - Workshop Track Proceedings", "year": 2014}, {"authors": ["D. Smilkov", "N. Thorat", "B. Kim", "F. Vi\u00e9gas", "M. Wattenberg"], "title": "Smoothgrad: removing noise by adding noise (2017)", "venue": "arXiv preprint,", "year": 2017}, {"authors": ["J. Snoek", "Y. Ovadia", "E. Fertig", "B. Lakshminarayanan", "S. Nowozin", "D. Sculley", "J. Dillon", "J. Ren", "Z. Nado"], "title": "Can you trust your model\u2019s uncertainty? Evaluating predictive uncertainty under dataset shift", "venue": "Advances in Neural Information Processing Systems", "year": 2019}, {"authors": ["J.T. Springenberg", "A. Dosovitskiy", "T. Brox", "M. Riedmiller"], "title": "Striving for simplicity: The all convolutional net", "venue": "In: 3rd International Conference on Learning Representations, ICLR 2015 - Workshop Track Proceedings", "year": 2015}, {"authors": ["S. Srinivas", "F. Fleuret"], "title": "Full-gradient representation for neural network visualization", "venue": "Advances in Neural Information Processing Systems", "year": 2019}, {"authors": ["K. Stacke", "G. Eilertsen", "J. Unger", "C. Lundstr\u00f6m"], "title": "A closer look at domain shift for deep learning in histopathology", "venue": "arXiv preprint,", "year": 1909}, {"authors": ["P. Str\u00f6m", "K. Kartasalo", "H. Olsson", "L. Solorzano", "B. Delahunt", "D.M. Berney", "D.G. Bostwick", "A.J. Evans", "D.J. Grignon", "P.A. Humphrey", "K.A. Iczkowski", "J.G. Kench", "G. Kristiansen", "T.H. van der Kwast", "Leite", "32 M. Pocevi\u010di\u016bt\u0117 et al. K.R.M", "J.K. McKenney", "J. Oxley", "C.C. Pan", "H. Samaratunga", "J.R. Srigley", "H. Takahashi", "T. Tsuzuki", "M. Varma", "M. Zhou", "J. Lindberg", "C. Lindskog", "P. Ruusuvuori", "C. W\u00e4hlby", "H. Gr\u00f6nberg", "M. Rantalainen", "L. Egevad", "M. Eklund"], "title": "Artificial intelligence for diagnosis and grading of prostate cancer in biopsies: a population-based, diagnostic study", "venue": "The Lancet. Oncology", "year": 2020}, {"authors": ["M. Sundararajan", "A. Taly", "Q. Yan"], "title": "Axiomatic attribution for deep networks", "venue": "International Conference on Machine Learning, ICML 2017", "year": 2017}, {"authors": ["Z. Swiderska-Chadaj", "H. Pinckaers", "M. van Rijthoven", "M. Balkenhol", "M. Melnikova", "O. Geessink", "Q. Manson", "M. Sherman", "A. Polonia", "J. Parry", "M. Abubakar", "G. Litjens", "J. van der Laak", "F. Ciompi"], "title": "Learning to detect lymphocytes in immunohistochemistry with deep learning", "venue": "Medical Image Analysis", "year": 2019}, {"authors": ["N. Tagasovska", "D. Lopez-Paz"], "title": "Single-model uncertainties for deep learning", "venue": "Advances in Neural Information Processing Systems", "year": 2019}, {"authors": ["Z. Tang", "K. Chuang", "C. DeCarli", "L.W. Jin", "L. Beckett", "M. Keiser", "B. Dugger"], "title": "Interpretable classification of alzheimer\u2019s disease pathologies with a convolutional neural network pipeline", "venue": "Nature Communications", "year": 2019}, {"authors": ["D. Tellez", "G. Litjens", "P. B\u00e1ndi", "W. Bulten", "J.M. Bokhorst", "F. Ciompi", "J. van der Laak"], "title": "Quantifying the effects of data augmentation and stain color normalization in convolutional neural networks for computational pathology", "venue": "Medical Image Analysis", "year": 2019}, {"authors": ["M. Teye", "H. Azizpour", "K. Smith"], "title": "Bayesian uncertainty estimation for batch normalized deep networks", "venue": "International Conference on Machine Learning, ICML 2018", "year": 2018}, {"authors": ["S. Thorstenson", "J. Molin", "C. Lundstr\u00f6m"], "title": "Implementation of large-scale routine diagnostics using whole slide imaging in sweden: Digital pathology experiences 2006-2013", "venue": "Journal of Pathology Informatics", "year": 2014}, {"authors": ["E. Tjoa", "C. Guan"], "title": "A survey on explainable artificial intelligence (XAI): Towards medical XAI (2019)", "venue": "arXiv preprint,", "year": 1907}, {"authors": ["M. Veta", "Y.J. Heng", "N. Stathonikos", "B.E. Bejnordi", "F. Beca", "T. Wollmann", "K. Rohr", "M.A. Shah", "D. Wang", "M. Rousson", "M. Hedlund", "D. Tellez", "F. Ciompi", "E. Zerhouni", "D. Lanyi", "M.P. Viana", "V. Kovalev", "V. Liauchuk", "H.A. Phoulady", "T. Qaiser", "S. Graham", "N.M. Rajpoot", "E. Sj\u00f6blom", "J. Molin", "K. Paeng", "S. Hwang", "S. Park", "Z. Jia", "E.I. Chang", "Y. Xu", "A.H. Beck", "P.J. van Diest", "J.P.W. Pluim"], "title": "Predicting breast tumor proliferation from whole-slide images: the TUPAC16 challenge", "year": 2018}, {"authors": ["T. Wu", "X. Song"], "title": "Towards interpretable object detection by unfolding latent structures", "venue": "The IEEE International Conference on Computer Vision (ICCV)", "year": 2019}, {"authors": ["Y. Xue", "N. Ray", "J. Hugh", "G. Bigras"], "title": "Cell counting by regression using convolutional neural network", "venue": "Computer Vision \u2013 ECCV 2016 Workshops", "year": 2016}, {"authors": ["J. Yosinski", "J. Clune", "A. Nguyen", "T. Fuchs", "H. Lipson"], "title": "Understanding neural networks through deep visualization", "venue": "arXiv preprint,", "year": 2015}, {"authors": ["G.A. Young", "R.L. Smith"], "title": "Essentials of statistical inference. Cambridge series in statistical and probabilistic mathematics", "year": 2005}, {"authors": ["M.D. Zeiler", "R. Fergus"], "title": "Visualizing and understanding convolutional networks", "venue": "Computer Vision \u2013 ECCV", "year": 2014}, {"authors": ["J. Zhang", "S.A. Bargal", "Z. Lin", "J. Brandt", "X. Shen", "S. Sclaroff"], "title": "Top-Down Neural Attention by Excitation Backprop", "venue": "International Journal of Computer Vision 126(10),", "year": 2018}, {"authors": ["Zhang", "Q.s", "Zhu"], "title": "S.c.: Visual interpretability for deep learning: a survey", "venue": "Frontiers of Information Technology & Electronic Engineering 19(1),", "year": 2018}, {"authors": ["Q. Zhang", "Y.N. Wu", "S.C. Zhu"], "title": "Interpretable convolutional neural networks", "venue": "Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition", "year": 2018}, {"authors": ["B. Zhou", "A. Khosla", "A. Lapedriza", "A. Oliva", "A. Torralba"], "title": "Object detectors emerge in deep scene cnns", "year": 2015}, {"authors": ["B. Zhou", "A. Khosla", "A. Lapedriza", "A. Oliva", "A. Torralba"], "title": "Learning deep features for discriminative localization", "venue": "Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition", "year": 2016}, {"authors": ["J.Y. Zhu", "T. Park", "P. Isola", "A.A. Efros"], "title": "Unpaired image-to-image translation using cycle-consistent adversarial networks", "venue": "The IEEE International Conference on Computer Vision (ICCV)", "year": 2017}], "sections": [{"text": "Keywords: XAI \u00b7 digital pathology \u00b7 AI \u00b7 medical imaging."}, {"heading": "1 Introduction and Motivation", "text": ""}, {"heading": "1.1 Background", "text": "Artificial intelligence (AI) applications are showing great promise for assisting diagnostic tasks in medical imaging. Nevertheless, it is difficult to translate the technology from academic experiments to clinical use. A central challenge for AI in medicine is that mistakes can have serious consequences. This means that human experts must be able to gauge the trustworthiness of machine predictions, and put it into the context of other diagnostic information. This is the purpose of explainable artificial intelligence (XAI) techniques. XAI research embraces the insight that AI solutions should not only have high accuracy performance, but also be transparent, understandable and reliable from the end user\u2019s point of view.\n? This work was supported by the Swedish e-Science Research Center.\nar X\niv :2\n00 8.\n06 35\n3v 1\n[ cs\n.C V\n] 1\n4 A\nug 2\nThis survey investigates XAI in the domain of digital pathology. The adoption of digital microscopy, whole-slide imaging (WSI), at clinical pathology departments is progressing at a fast pace in many parts of the world. A key motivation for this investment is the potential to use AI assistance for image analysis tasks. XAI has been described as an essential component to make AI successful in imaging diagnostics [63], and we argue this is particularly pertinent for digital pathology. For example, assume that a pathologist is faced with an AI result marking a WSI region as \u201cbenign tumour\u201d, whereas the pathologist deemed it as probably malignant. It is easy to see how the pathologist would need further information on the rationale of the machine prediction in order to accept or reject the result that was in conflict with his/her own initial assessment.\nThere are several motivations to specifically target XAI for digital pathology, as we do in this survey. XAI has so far been dominated by the explainability tailored for AI developers, whereas the needs of pathologists and other medical professionals are distinctly different, as will be described below. Pathology is also quite different from other medical imaging. Gigapixel images are the norm, which are visually scrutinised on many scales. The characteristics of the large histology \u201clandscapes\u201d are very different both from photos such as in ImageNet and from radiological images. We believe that describing the XAI prerequisites in pathology will be valuable for informing much needed future research efforts in this domain.\nThis survey is a niched drill-down complementing previous more general reviews. There are broad XAI overviews [1, 24, 36, 42, 74], and more specialised reviews such as for Convolutional Neural Network (CNN) methods [122]. A few efforts discuss the potential of XAI for medicine in general [44, 114].\nThere are several specific contributions in our survey. We have elicited a classification of XAI techniques from three separate aspects: explanation target, result representation, and technical approach. We have identified previous XAI efforts with relevance for digital pathology, most of them not applied to this domain yet, and categorise them into the defined classes. Estimation and visualisation of uncertainty are sometimes treated as a topic separate from explainability. We echo previous researchers arguing against such a separation [4, 44, 109] and incorporate uncertainty methods as an inherent part of the XAI overview in this review. Finally, based on an analysis of the survey outcome, we provide some key findings that could guide future research efforts to solve XAI challenges in digital pathology. We believe that this paper is suitable for both technical researchers and medical domain professionals. For example, the categorisation is made with both target groups in mind, where result representation and explanation target are of interest to the medical experts, whereas the technical approach is separated into an isolated group. Thus, we believe that the survey can assist in understanding across the disciplines by providing a joint structure as a base for discussions.\nOur survey places the focus on image recognition tasks as most AI algorithms in digital pathology work with image data. Therefore, all methods described in this survey are applicable for CNN models as, currently, this is the state-of-\nthe-art in digital pathology. We use the terms AI tools, AI solutions and AI algorithms interchangeably."}, {"heading": "1.2 AI in pathology", "text": "The workload for pathologists is predicted to increase continuously due to the ageing population, shortage of physicians, increased cancer screening programmes and increased complexity of diagnostic tests [96]. One way of addressing this problem is to introduce digital pathology; that is, new workflows and tools based on the digitisation of microscopy images [113]. The possibility to add assistive AI tools is a major component of the foreseen improvements. As a foundation for our discussion on XAI, we will in this section first provide a brief overview of some important types of AI use cases for the clinical routine setting of digital pathology. For more exhaustive overviews of applied AI research in this area, we refer to previous review efforts [11, 42, 96], and for an introduction to the diagnostic work of a pathologist, we refer to [88].\nA common diagnostic task in pathology is to detect the existence of cancer. Thus, AI development efforts are often directed towards assisting the tumour detection process. One improvement aspect is to make the search more efficient. Since the lesions may be just a handful of cells in a sea of normal tissue, it can be very time-consuming to locate them. For some scenarios the search can stop when a first lesion is found, meaning that normal/benign cases are the most time-consuming as the search then covers the entire sample. Metastasis detection in breast cancer lymph nodes is a common AI research application [26, 64]. The other task aspect is to determine whether a finding actually is malignant or not, and often this includes performing a subtype classification of the cancer in question. Illustrative Cresearch efforts in this subarea include a tool for detection and subtype classification of gliomas and non-small-cell lung carcinomas [45], classification of gastric carcinoma [97], and malignancy prediction in breast cancer cytology [35].\nDetection tools could also help to reorganise the worklist of a pathologist so that the cases with a high risk of malignant tumours would be prioritised. Apart from tumours, potential detection tasks for AI include needle-in-a-haystack searches for tuberculosis or helicobacter pylori bacteria [71].\nIn the diagnostic work-up of oncology cases, the pathologist typically provides further granularity in the analysis in the form of grading and staging assessments. These assessments often suffer from poor inter-observer reproducibility as well as high time consumption, making AI assistance attractive. In breast cancer, quantification of cell proliferation is part of the grading. Detecting and counting mitotic cells is a common target for AI methods [9, 18, 115]. AI solutions are commonly employed also for other cell quantification in breast cancer diagnostics, regarding positive nuclei in sections stained through immunohistochemistry (IHC) [5, 41, 48, 76, 117]. Quantified IHC analysis is relevant to predict response for many targeted treatments, with active research efforts in AI method development. Important examples include detection of positive cell membranes in the PD-L1 [56] and HER2 [92] stains.\nThe Gleason score is used to stage prostate cancer by assessing the extent of different architectural patterns of a tumour. This analysis has been in focus for applied AI research [6, 75] and recent larger studies show results on par with human experts [14, 106].\nAnother cell identification task is to count lymphocytes, which for example is important for predicting treatment response for immunotherapy on cancer patients. Deep learning methods have shown the potential to provide support for this diagnostic task as well [19, 34, 108].\nA pathologist\u2019s assessment can be underpinned by other more generic quantification where AI could contribute to higher efficiency. This is relevant for area measures such as tumour/stroma ratio or tumour necrosis rate, and potentially for automation of distance measurements such as tumour thickness in skin cancer and the margin from the tumour to the resection border.\nApart from the tasks described above, neural networks have the potential to be used for content-based image retrieval (CBIR) to find previous cases with similar histology patterns as the patient at hand. This can not only assist the daily work of a pathologist but also improve the education of new physicians [58]. Deep learning has been employed for this purpose, both in research [15] and in a commercial application [47].\nOverall, deep learning is a flexible approach that can be used to assist pathologists in many different aspects of their work. However, the path from promising AI models to actual clinical usage is very challenging. We argue that a key part of meeting that challenge is to develop effective and tailored XAI methods. Next, we will drill down into the specific needs of XAI in this domain."}, {"heading": "1.3 Needs of XAI in digital pathology", "text": "XAI serves different purposes depending on the role of the person receiving the explanation, and depending on the reason for interacting with the AI technology. In digital pathology for clinical use we see three main scenarios, having quite different characteristics (Figure 1). The arguably most common target for XAI is to assist the AI developer working to create or improve the model, which of course is relevant also in this domain. The second scenario is when the clinical end user, typically the pathologist, employs an AI solution in the diagnostic routine. The third XAI target area, perhaps less considered than the others, is healthcare professionals doing quality assurance (QA) of AI solutions. This role may be taken by pathologists or other medical staff, but we may also see data scientists entering the diagnostic departments to carry out such assignments. QA can correspond to initially assessing how well an algorithm performs at the lab in question, for calibrating or configuring the solution to fit local characteristics, to evaluate if there is a drift in performance over time, and more.\nThe AI developer perspective on XAI needs is fairly generic, at the conceptual level it is the same in digital pathology as in other application areas. We will give an outline here, while further details can be found in the survey by Hohman et al. [42]. Explainability is a key component to support the developer in improving the performance of the trained model. For example, studying er-\nroneous predictions is effective for evaluating the next development steps to be taken. Prediction accuracy aside, the developer also benefits from XAI analysing the generalisability of the results; is the training data, including augmentations made, representative for wider use, and is there any bias to consider? As collecting and preparing data could be very laborious, often including human experts spending many hours annotating images, the collection may not have sufficient coverage for the intended application. For instance, if there are many classes to separate, some of them may have too few examples in the data collected.\nFor the routine diagnostic use of AI solutions, there are many situations where explainability would be beneficial. In fact, we argue that effective XAI is essential for broad successful deployment of computational pathology in the clinical environment.\nFor a physician using AI assistance, a main task is to spot and correct any errors made by the algorithm. XAI would then provide assistance to critically assess the result. The typical ML model always predicts one of its predetermined outcomes, even if the evidence is lacking for any of the alternatives. An important aspect is therefore to convey the uncertainty of the prediction made. This is particularly useful when there is not a black-or-white assessment to be made, but a conclusion from a complex set of contributing factors.\nThe physician would likely also benefit from a deeper understanding of the source of the AI tool\u2019s limitations in the context it is used. For models trained by supervised learning, the representativeness of the training data in relation to the local data characteristics is a key factor, i.e. its ability to generalise. Image differences due to e.g. staining variations is a well-known challenge in computational pathology [105, 111], creating a domain gap between the training data and the data used for inference. There may also be discrepancies in the definition of the diagnostic task trained for and the local diagnostic protocol. Such problems makes it important to highlight when the diagnosis provided by an AI application cannot be trusted due to a lack in its ability to generalise to the current situation.\nAchieving the above transparency is useful for making correct conclusions for individual cases, but also to allow the medical professionals to gain trust in the solution in general. Such trust is necessary in order to arrive at an effective\ndivision of labour between man and machine. Powerful XAI can, however, induce too high levels of trust, counteracting the objective of critical assessment [49]. Therefore, XAI methods should be carefully designed to provoke sound reflection, rather than just creating blind trust.\nAn additional benefit of explainable AI predictions is for teaching. Whereas it today is very difficult to verbalise the complex assessments a senior pathologist does [44], XAI visualisations may be able to better convey such knowledge. Furthermore, there is a direct connection to medical research, as XAI may help uncover previously unknown disease characteristics.\nIn digital pathology, AI researchers face the challenge of dealing with very large data sets, typically gigapixel images. Diagnostic pathology assessments almost always include considering small-scale and large-scale features in tandem, and computational pathology needs to do the same. This is a particular challenge for XAI as well. An AI prediction will likely need to be explained both at local cellular level and at a higher tissue structure level.\nThe QA scenario shares many of the XAI needs of the diagnostic assessments described above. Identifying errors, assessing uncertainty, and understanding limitations are in focus here as well. The difference is that the focus shifts from individual cases to analyses across larger case sets representing the entire operations at the lab. This poses additional requirements for XAI solutions, to support systematic, in-depth investigations akin to the diligent validation procedures for other lab equipment.\nThe aspects discussed above clearly shows that there is a great diversity of situations where pathology diagnostics require transparency and interpretability. In summary, there is strong rationale for XAI advances tailored for digital pathology."}, {"heading": "2 Glossary", "text": "XAI: explainable artificial intelligence, a field of study on how to increase transparency and intepretabily of artificial intelligence algorithms such that the results could be understood by a human expert. Standard neural network (NN): the most common type of neural network used in research as well as in practical applications. They are based on frequentist probability theory which states that each parameter of a model has a true fixed value. It is hard and often impossible to find the exact true values, hence the backpropogation algorithm provides a means of approximating them [37]. In the context of the uncertainty estimation, the parameters of a NN are not random variables, hence, probabilities cannot be associated with them and other frequentist techniques have to be used [119]. Bayesian neural network (BNN): a type of neural network that is based on Bayesian theory and requires more complex procedures of training and inference. According to the Bayesian approach, a probability is a degree of the belief that a certain event will occur and can be modelled by Bayes\u2019 theorem. Bayes\u2019 theorem states that the conditional probability of an event depends on the data as well as\nprior information/belief. The model parameters are viewed as random variables, and Bayesian inference is used to estimate the probability distribution over them. This distribution reflects the strength of the belief regarding what parameter values that are possible, and it is used during forward pass through a BNN to sample some likely parameters and produce a range of possible outcomes [119]."}, {"heading": "3 State-of-the-art", "text": "In this section, we provide an overview of the current methods that aim to help in developing transparent and interpretable AI solutions. We focus on methods that are specifically developed for, or can be easily applied to, visual detection tasks. There are many ways in which the methods can be grouped. In this work, we provide three alternative taxonomies, namely Explanation target, Result representation and Technical approach. Each taxonomy is described in detail in the corresponding subsection below, followed by examples of representative XAI methods that can be assigned to it. Table 1 in Appendix A contains all reviewed XAI methods, classified according to the three alternative ways of categorisation.\nThe explanation target gives a general understanding of what can be explained in visual detection tasks and for which group of professionals \u2013 AI developers, QA specialists or pathologists \u2013 this explanation is most relevant. Result representation illustrates how the explainability may be presented in an AI solution while the Technical approach provides an insight into what techniques and mathematical theories are used in order to achieve the explainability. Many of the existing XAI techniques focus on explaining classification tasks, or at least illustrate their work with examples of classification algorithms. Some methods encompass other computer vision tasks, such as detection, localisation and segmentation. Furthermore, it is important to note that some methods can be or have already been adapted for different tasks which could make them fall under several different categories. We decided to base our categorisation on how the method is described in the original paper.\nFigure 2 summarises the reviewed papers based on our three dimensions of categorisation, and Figure 3 shows the development over time. In Figure 2, the matrix plot in the top gives an overview of what technical approaches are most commonly used for which explanation targets. In contrast, the plot in the bottom gives an overview of what result presentation types are most commonly used for which explanation targets. We can see that irreducible uncertainty so far has been only presented as an auxiliary measure even though there are quite a few different technical approaches for determining it. In contrast, the explanation of inner workings can be presented in many different ways, but the activation optimisation approach is the most commonly used to achieve the results. Figure 2 not only summarises previous work in XAI but also highlights which combinations of the categories that have not yet been explored.\nIt is important to note that this section is aimed at providing a general understanding of existing methods, hence the text does not focus on the pathology specific aspects. However, the result representation part is mainly illustrated by\nXAI methods applied to histopathological data. Furthermore, a discussion on how the different methods can be used for fulfilling the need for reliable and transparent AI tools of digital pathology is provided in Section 5."}, {"heading": "3.1 Explanation target", "text": "Explainability could have several objectives in the context of AI assisting visual analysis. Figure 4 illustrates the four targets that an XAI method may help to understand better. In this section we describe in more detail each of the explainability types and illustrate with some examples from the reviewed papers.\nExplaining predictions Explaining predictions refers to methods that are developed to understand why a certain image led to a corresponding NN output. The expectation is often that the reasoning of an NN would match the logic that human experts use when solving the problem. For example, if an NN is detecting the presence of a tumour, we would like to see that the NN is basing its prediction on the areas of the image that contain tumour cells. This would reassure pathologist that the prediction is trustworthy. Some well-known examples in this category include saliency maps [100], Excitation Backprop [121] and explanations generated using meaningful perturbations [30]. Such explanations could be useful for the AI developers to debug their created algorithms, could aid QA specialists in guaranteeing an appropriate behaviour of an NN, and could foster trust in the community of the end-users.\nExplaining inner workings Methods in the category of inner workings explanation aim to increase the understanding of how an algorithm works. There are a few ways that researchers have attempted to achieve this. For example, activation maximisation techniques show the patterns to which neurons react most [28, 79, 118]. Other methods analyse image representations encoded by the model in order to understand what information is retained and what is lost. This can be done by recreating the original images from the representations saved at different layers of a CNN [23, 67]. Finally, some techniques examine what interpretable concepts, such as textures and patterns, that are detected for a specific class of interest. This enables explanation of behavioural properties of deep neural networks [54, 62]. This explanation type is most relevant to the AI developers as it can give new insights into what an NN is doing and how its performance could be boosted.\nUnderstanding reducible uncertainty Reducible uncertainty, also known as epistemic uncertainty, refers to when the training of the model has been imperfect, insufficiently covering the phenomenon of the interest. Having high such uncertainty means that the parameters of an NN are not tuned properly to make an informed prediction for some inputs [22], i.e., data points being outliers\nin relation to the training data. This results in the model being incapable of providing informed predictions about outliers as the prior knowledge gathered during training is insufficient. If we would expand the training data and ensure it contains all outliers, this uncertainty could be reduced to zero. Even without more data, an improved training scheme could also reduce the epistemic uncertainty. Explanations targeting this uncertainty type enable us to understand the limitations of our model via training data and how we can improve it to increase prediction accuracy.\nThere are two main ways of estimating the epistemic uncertainty. It can be modelled on the NN\u2019s weights directly [12, 40, 91]. Other methods use the exploration of the data sets used for training and validating an NN to find out which points that are outliers and tune the uncertainty estimates to match this information [38, 84, 85].\nUnderstanding irreducible uncertainty The irreducible uncertainty arises due to the intrinsic randomness of a phenomenon of interest and the fact that our model is an approximation of it. It is is also known as an aleatoric uncertainty [22]. This uncertainty cannot be reduced by increasing the size of the training data set. For an intuitive understanding, consider that even if we train a model to detect a tumour in WSIs on all of the relevant images existing in the world, our model is still an approximation of the complex phenomenon (tumour in\nhuman tissue). Hence, the upcoming new images from new patients may inherit some variability that our model is unable to classify correctly due to the missing variables/parameters that are necessary for capturing it.\nSo what does such uncertainty explain to us? It reminds the users that they should never trust an AI prediction completely as the nature of the world contains intrinsic variations that none of the models can perfectly capture. Furthermore, aleatoric uncertainty also gives an insight into whether developers chose an appropriate architecture of the model for a particular problem. If the developers observe low epistemic uncertainty but high aleatoric uncertainty, this indicates that the model is too simple to approximate the properties of the phenomenon of the interest well [53]. In an ideal situation, the chosen model would only contain low aleatoric uncertainty and the prediction variability would be as low as possible. Some examples of methods for estimating the aleatoric uncertainty include [7, 13, 33]."}, {"heading": "3.2 Result representation", "text": "The categorisation of result representations assesses what type of results the user should expect to receive from applying an explainability technique. This allows users and developers to quickly pick a subgroup of methods that would provide the desired explanation output. We have distinguished four main groups of how the results are presented: synthetic visualisations, visualisations on an input image, showing by example and auxiliary measures. If we are working on helping a pathologist with a tumour detection task or want to boost the pathologist\u2019s confidence in an AI solution, we may be most interested in the techniques that provide visualisations on the original input images or show how an NN works by an example. However, if we are debugging or validating the overall classification strategy of an NN, the methods that generate synthetic visualisations or auxiliary measures may be preferred.\nSynthetic visualisations Synthetic visualisations is a broad group of methods that all generate a synthetic image as an outcome. The group can be further divided based on what sort of image is generated.\nThe first subgroup generates surrealistic images from scratch that can be interpreted as illustrations of what some part of the NN reacts to. These methods are also known as activation maximisation: they attempt at finding, through optimisation, the input images that maximally activate a chosen neuron, channel or layer. Some examples of the methods can be found in [17, 28, 79, 81, 118]. The visualisations give an insight into what patterns the neurons are looking for. For example, this knowledge may aid the analysis of a domain shift problem [105]. Figure 5 shows an example of patterns that maximally activate some filters of a Mini-GoogLeNet that is trained to classify tumours in WSI patches [105]. The different rows use different strategies for formulating the training data but are all based on the same original dataset. The results show how very different representations are learned by the convolutional layers depending on how the\ndata is pre-processed, e.g. how colour augmentation makes the representation less sensitive to absolute colour values in the input (second row).\nAnother subgroup of methods focuses on using the feature representations in an NN to generate images that resemble parts of the original input. The aim is not to retrieve the original image but to conclude which patterns could be most responsible for the NN\u2019s prediction (or a neuron\u2019s activation). A well-known method of this category is deconvolutional networks [120]. Other examples that aim to explain the target NN include PatternNet [55] and Guided Backprop [103]. The main drawback these methods have is that they produce visualisations that are not class-specific. This means that the techniques give an insight into which patterns are important in general, but cannot be used to understand why an NN predicted the specific class [80].\nFinally, some methods have focused on reconstructing the original images from the feature representations in an NN. They are also known as model inversion methods. This type of visualisation illustrates what information an NN is keeping about the image which, in turn, helps to understand better how an NN works. For example, comparing the reconstructed images to the original input, Dosovitskiy and Brox [23] found that the colour seems to be a crucial feature for the prediction they studied. Similar visualisations can be found in [67, 68].\nVisualisations on an input image The methods in this group produce three types of visualisations: heatmaps, important patches and receptive fields. A\nheatmap is a graphical representation of 2D scalar data where the values are encoded by colour. With such representation, this first type of visualisation shows how much each pixel contributes to the prediction. This information is visually conveyed by overlaying the colour gradient on the original input image. Well-known techniques that produce heatmaps are Excitation Backprop [121], Grad-CAM [95] and Layer-wise relevance propagation [8]. The second type of visualisation is produced by keeping the important patches (pixel regions) of the original input and cropping out the remaining. These techniques reveal which objects or regions in the input that are contributing to the prediction. However, they do not provide the knowledge of importance distribution over the pixels [90, 124]. The third type of visualisation marks the receptive field, the areas on the original input image that indicate what regions that activate most a target unit [123, 124]. This gives an insight into how neurons in NNs and filters in CNNs work.\nVisualisations on an input image methods, such as Grad-CAM, can be used to increase the transparency as well as uncover some potential biases that the model has. Figure 6 shows an example of such a case. We trained a ResNet18 neural network to predict if a patch from a WSI of skin contains a tumour. ResNet18 contains four residual blocks, units consisting of several layers. It is connected with the previous block with a skip connection [39]. We used the Grad-\nCAM technique to visualise each of the four blocks. The resulting heatmaps show which pixels that are most important for the prediction. This provides an overview of how the attention of an NN is changing through the layer blocks when the image is classified. The model predicted that there is no tumour, but the patch does contain tumour cells in the lower part of the image. The Grad-CAM method uncovers a potential bias: the model uses fat tissue as an indication of the absence of the tumour cells, which could be caused by an over-representation of fat tissue in patches without a tumour in the training data set.\nAll methods described so far aim to provide a deeper understanding of an NN trained to do classification. However, AI is not only used for classification tasks. Wu and Song [116] has proposed how to improve the interpretability of a CNN trained for object detection. They create an architecture of an NN based on R-CNN that provides not only the classification score but also the bounding box on the region of interest (the target object). Furthermore, XAI visualisation on the input image can increase the interpretability of the segmentation task. Kwon et al. [59] have used a technique for estimating uncertainty first described by Kendall and Gal [50] and created heatmaps of uncertain regions of the segmentation. A large area of uncertain regions can warn a user that the NN is making a poorly informed guess.\nShowing by example The methods in this group are diverse but share the foundation that their explanations are based on presenting and discussing examples, either from the original data or from auxiliary data. This is a small group of methods so far, but it has great potential as some research claims that this type of explainability may be the most intuitive for a human user [70].\nThe first subgroup of methods uses examples from the original data set in order to understand an NN better. Lapuschkin et al. [61] provide a visualisation of clustering input images that an NN determined to be from the same class. The clustered images reveal potential rationale for how an NN assigns samples to a certain class. For example, such exploration may reveal that an image is classified as containing a horse if there is a white fence, if there is a person in a certain position or if there is a clearly visible horse head. Similar results are achieved in work by Kevin et al. [51] where authors grouped WSIs of neuropathological tissue that an NN perceived to be similar. Moreover, at each layer of an NN, we can compare the representations of the input image to the representations of all other images in the data set and see what labels have the k most similar other images [85]. This may help to detect out-of-distribution images. Alsallakh et al. [3] explore a confusion matrix produced by an algorithm in order to determine which classes that are causing most trouble and the possible reason for the confusion.\nOther methods show examples that do not come from the original data set. Seah et al. [94] proposed an idea that if we generate the most similar image to the original one but that would be classified to a different class by an NN, we could compare these images and understand which parts or differences are used by the NN to predict the correct class. Figure 7 illustrates such a method\nused on a tumour classifier. We trained a Cycle-GAN [126] to transform patches that contain tumour cells to healthy ones. The score in the left corner of each image shows the prediction score for each patch by the NN under scrutiny; the high confidence of the NN proves that the transformation was successful. Such counterfactual illustrations could capture and convey complex explanations of diagnostic predictions.\nFurthermore, some researchers explore if human-comprehensible concepts, such as striped patterns, gender, or a tie, are influencing an NN [10, 54]. These methods require a separate data set that contains the images with the concepts of interest. They can show which images of a certain concept that are most relevant to a chosen class that an NN is predicting. Also, they can reveal which particular images in the original data set that are influenced by the concept most. For example, they can show images with the striped patterns (from the concepts data set) that are most related to a Chief Enterprise Officer (CEO) class in an NN and which images labelled CEO (from the original data set) are mostly influenced by the stripe patterns. This knowledge could potentially alleviate biased predictions in an AI solution.\nAuxiliary measures This subsection provides an overview of various measures that have been developed for understanding an NN better. They do not necessarily have anything in common apart from the main aim to make an NN more\ntransparent. However, all of them explore an important way of representing the results: providing an informative score or measure.\nUncertainty measures give insight into how much we can trust the outcome of an AI solution. These measures can be used i) to construct prediction intervals that show how much the prediction could vary [87], ii) to create heatmaps as discussed in Section 3.2 above, iii) to create plots illustrating uncertainty [7], or iv) to be presented as a score to the user [33].\nOther scores provide a measure of importance that helps to understand how an NN makes the prediction. For example, Koh and Liang [57] assigns an importance score for each training image by exploring how the predictions of the target NN would change if the particular image would be removed from the training data."}, {"heading": "3.3 Technical approach", "text": "This subsection gives an insight into how the explainability methods work technically. Thus, the categorisation is probably more relevant for readers with a technical background. Knowing what strategies that have been commonly applied in acquiring the visualisations or the scores may provide a good starting point for identifying what technical approaches that have not yet been explored. It is important to note that we do not aim to describe the technical details of each method thoroughly, instead, we provide a general overview of what kind of techniques researchers have used so far.\nActivation optimisation All methods in this subsection use an optimisation algorithm on the activations of the target NN in order to create their visualisations. There are two distinct ways of how optimisation in the context of neural network understandability may be used. Some techniques aim to explore the inner workings of an NN by finding the patterns that maximise the activation of a chosen neuron or a combination of neurons. The main idea can be illustrated by the following optimisation problem:\nx\u2217 = arg max x hi,j(\u03b8, x) + regulariser, (1)\nwhere hi,j is the activation of the target neuron with indices i, j, that has been computed from the input sample x.\n\u03b8 is the set of parameters used for computing the activation and x\u2217 is the estimated sample that maximises the output the neuron at i, j. This objective is usually achieved using gradient descent, and most proposed activation maximisation methods mainly differ in what regulariser they propose to use [17, 28, 79, 81, 118].\nOptimisation techniques are also used for inverting NNs. Mahendran and Vedaldi [68] proposed a method to reconstruct the original image from the activations of a chosen layer of an NN. Given that a neural network is some composite function \u03a60 = \u03a6(x0) with input image x0, they acknowledge that finding an inverse of \u03a60 is difficult; neural networks are composed of many non-invertable\nfunctions. Hence, this method aims to find an image whose representation best matches the target input image by reducing the loss between \u03a6(x) of some input x and the target \u03a60. There are a few other efforts where model inversion tools also are proposed to achieve a better understanding of an NN [23, 67].\nBack-projection to input space Back-projection methods reflect a prediction score of an NN or activation of a target neuron back to the input space. They generate the patterns that triggered the neuron or create sensitivity maps over the input pixels. We describe this category by highlighting two representative approaches, deconvolution and saliency map.\nDeconvolution is a technique that inverts the direction of the information flow in an NN [120]. In a higher layer a target neuron is chosen and the activations of all the other neurons in that layer are set to 0. Then, the activation of the target neuron is passed back through the layers to the input space. In order to invert the max pooling layers which are normally non-invertable, Zeiler and Fergus [120] propose to compute so-called switches during the forward pass: they record which positions that contained the maximum value within each pooling region. The resulting signal in the input layer is used to construct an image which shows what patterns that triggered the activation of the target neuron. Guided Backprop [103] is another method also built on this approach.\nSaliency map is a technique that uses Taylor expansion in order to backproject a prediction score to the input space. The technique is based on the idea that the prediction of a neural network can be approximated by a linear function:\nSc(x) = w Tx+ b, (2)\nwhere Sc is the class score, w and b are the weight vector and bias of the model, respectively, and x is the input image. Then the first-order Taylor expansion is used to determine the approximation for a weight vector wT . The approximated wT reflects how much each pixel in the input contributed to the final score [100]. There are a few other XAI methods that achieve back-projection to input space by using Taylor expansions [8, 73] or other linear approximations that distribute the target score among the input pixels [95, 125].\nInput perturbations Explainability can be achieved by inducing variations of the input. Methods in this category use perturbations on the input images in order to determine the important pixels for a prediction, or to estimate the uncertainty. It is important to note that the methods under this category may or may not analyse each perturbation separately. An example of such an algorithm is Local Interpretable Model-agnostic Explanations (LIME) [90]. The target input image is split into k superpixels. A Ridge regression is then trained on k perturbed images (each time only one superpixel is non-black) to predict the black-box classifier\u2019s prediction score of that particular input. The parameters of the Ridge regression are used to determine which superpixel is the most important for the correct prediction.\nAnother method is using Shapley values for the explanation of a classifier [65]. Normally, these are computed by rerunning the classifier as many times as we have features (pixels in our case) and excluding one feature at a time. This provides an insight to which features that are important for the classification. However, such a procedure becomes very expensive with deep learning, hence the authors have offered several approximations for computing Shapley values for image classification. Similar use of perturbations or approximations of perturbations can be found in other XAI methods [7, 20, 30, 127].\nInterpretable network design Another approach to explainability is to modify the architecture of an NN in order to arrive at models that produce more interpretable feature maps. This means that visualising them may give an insight into what objects a unit of an NN is detecting when making a classification decision.\nInterpretable CNNs [123] is a method modifying the CNN filters; a special loss function is introduced that enforces the filters to capture human-interpretable objects better. The idea is that visualising the receptive field of such filters could reveal behaviour of NN units, such as using cat head depictions to label the image as a cat. Another example is Interpretable R-CNN [116]. Here, an architecture of an NN based on Faster R-CNN is created that provides not only the classification score but also the bounding box on the region of interest (the target object). This is achieved by introducing a new layer in the model called terminal-node sensitive feature maps that is based on graph theory.\nFrequentist techniques The methods in the group of frequentist techniques are built for standard neural networks. Therefore, applying such techniques usually does not require heavy modifications to the algorithm or the training pipeline. The methods are based on frequentist statistical theory and there are three main techniques commonly used: ensembles, bootstrapping and quantile regression.\nEnsemble methods use multiple learning algorithms to obtain a better predictive performance compared to the learning algorithms alone. Recently it has been proposed to generate the prediction intervals via ensembles of NNs [60, 87], i.e. utilising the variation between different models as a measure of uncertainty.\nBootstrapping is a technique that uses random sampling with replacement in order to approximate the population distribution with the sample distribution. This provides means to estimate statistical measures such as standard error, bias, variance, and confidence intervals [25]. Osband et al. [82] developed an efficient and scalable method that enables generation of bootstrap samples from a deep neural network to estimate the uncertainty.\nFinally, quantiles in statistics are dividing the range of the probability distribution or the sample distribution into continuous intervals that have equal probabilities [119]. Quantile regression, methods for estimating quantiles, can be used to build confidence intervals around the target variable (for example, the prediction of the NN) which enables estimation of the uncertainty. Tagasovska\nand Lopez-Paz [109] have proposed to implement an additional output layer called Simultaneous Quantile Regression for any deep learning algorithm. Simultaneous Quantile Regression would be trained on a special loss function that learns the conditional quantiles of a target variable.\nIt is also worth mentioning that while some of the methods described in this section are developed for regression problems, it is also possible to adapt them for classification tasks.\nBayesian neural networks Bayesian Neural Networks (BNNs) are designed to incorporate uncertainty about the networks\u2019 parameters, weights and biases. Unfortunately, BNNs are computationally more complex than conventional NNs, require a different training pipeline, and may not scale well to big data sets nor deep architectures that are required for state-of-the-art performance [86]. While it is out of scope for this survey to rigorously describe the research on BNNs, we next provide a brief overview of some key approaches.\nMany researchers have worked on producing approximations of the intractable posterior distributions of BNNs \u2013 approximations that can be used in practise. This includes, among others, applying Markov chain Monte Carlo methods [77] and variational Bayesian methods [12]. Another challenge in training BNNs is performing the backpropagation, as there is a large number of possible values of the parameters. A few of the efforts have focused on improving the backpropagation algorithm for BNNs by introducing alternative algorithms, namely probabilistic backpropagation [40], Bayes by Backprop [12], and natural-gradient algorithms [52]. Essential work has been done on determining the best way of finding a good prior as it strongly influences the quality of the uncertainty measurement [38]. Finally, some work focused on specifically developing Bayesian convolutional neural networks [32, 99], while others developed ways to distribute the estimated uncertainty between epistemic and aleatoric parts in BNNs [21, 50, 59].\nBayesian approximations Bayesian theory can be applied also to standard NNs to estimate uncertainty. In this way, uncertainty awareness can be introduced without having to tackle the shortcomings of BNNs. They use derivations and modifications to a standard NN training in order to incorporate the Bayesian inference.\nPearce et al. [86] showed how to add a Bayesian prior via regularization of the parameters and train an ensemble of standard NNs in order to approximate the posterior distribution. Postels et al. [89] proposed to reduce the computational complexity of ensembles and other resampling-based methods by adding noise to the parameters of an NN during training. Some researchers focused on how to derive Bayesian inference approximations using some common techniques in modern NN training, such as dropout [33] and batch normalisation [112]. Finally, Ritter et al. [91] applied a Laplace approximation to obtain uncertainty estimates after an NN is trained.\nOther techniques A number of methods base the generation of explanations on techniques that do not fall into any of the previous categories. Some methods use Generative Adversarial Networks (GANs) as part of their XAI solution [78, 94] (similar to Figure 7). Moreover, uncertainty estimates can be based on conformal prediction; a technique that determines the confidence of a new prediction based on the past experience [84, 85]. The training data set can be explored using influence functions, a classic technique from statistics, which help to detect the points most contributing to a given prediction [57]. Linear binary classifiers and binary segmentation algorithms have also been used to determine to what concepts (defined by a separate data set) a target NN is responding [10, 54]. Finally, techniques for dimensionality reduction, such as t-distributed Stochastic Neighbour Embedding (t-SNE) [66], makes it possible to reduce the dimensionality of feature representations of an NN and highlight which input images the model is perceiving to be similar [51]. It is also possible to extract information on how different NNs relate to each other by considering the weights of a large number of trained models [27]."}, {"heading": "3.4 XAI methods in medical imaging", "text": "In this final subsection of the XAI method overview, we highlight some previous efforts of XAI techniques specifically addressing the medical imaging domain.\nPalatnik de Sousa et al. [83] explored an AI algorithm that classifies lymph node metastases by creating heatmaps of the area in the input patch that contributed most to the prediction. The authors found that deep learning algorithms trained for this task have underlying \u2018reasoning\u2019 behind their predictions similar to human logic. Similar findings are reported in a study on classifying Alzheimer\u2019s disease [110]. Huang and Chung [46] addressed the need for an informed decision by training a CNN model to predict the presence of cancer for a given WSI. At the test time of this algorithm, XAI methods that explain the predictions by creating visualisations on the input are used to detect the areas of the tissue that are unhealthy. It provides \u2018evidence\u2019 why this WSI should be classified as unhealthy as well as localises the cancerous cells. The authors showed that the detected areas closely correlated with the pathologists labelling. Therefore, it provides a meaningful intuition of why AI categorised the whole slide as containing a tumour.\nIncorporating uncertainty measures in AI solutions for digital pathology could help to increase transparency as well. Kwon et al. [59] has shown that the highly uncertain regions in ischemic stroke lesion segmentation often correlate with the pixels that are incorrectly labelled by the algorithm. This means that uncertainty measures provides means for a doctor to spot possibly wrong predictions and understand when he or she should be cautious of the AI decision. Fraz et al. [31] fulfilled this idea by incorporating an uncertainty measure in their model, and showed that the quality of microvessel segmentation did indeed improve.\nThese results are inspiring and demonstrate the potential of XAI techniques in medical imaging. Nevertheless, the current body of work is still quite limited.\nThere is a need for more in-depth research on applied XAI methods in the domain, and particularly so in digital pathology, as will be further discussed in the next section."}, {"heading": "4 Open Problems", "text": "There are several open problems arising with XAI application for digital pathology. The stakeholders involved in XAI development and usage should be aware of the potentially misleading explanations as well as the lack of ways to evaluate different XAI methods.\nThe first problem is caused due to the design of the explanations and their user interactions. To increase human understanding, it is important to formulate causal explanations, that is, why the AI algorithm made that prediction. Holzinger et al. [43] argue that analysing how an AI solution arrived to the prediction may not always result in a satisfactory explanation as a more thorough understanding of how humans interpret and use explanations is needed. This point is also highlighted by Mittelstadt et al. [70], further detailing that explanations may become damaging if they are phrased confusingly or do not match the users\u2019 expected format.\nAnother pitfall is if the explanation designer has a certain agenda. As there may be a strong incentive to promote trust in the AI predictions, explanation design runs the risk of being more persuasive than informative, as demonstrated in the work by Kaur et al. [49]. This could have especially severe consequences in XAI solutions developed for digital pathology, as overconfidence in them could result in patient hazards, as well as in a setback for the needed development of AI assistance to improve diagnostics.\nEven though XAI tools could increase the understandability of an AI solution, so far we lack a solid scientific evaluation framework that would allow us to understand when they work well and what limitations they have. This challenge arises due to the fact that usually the \u2018ground truth\u2019 is unknown for most of the outputs by any XAI technique. There are a few works attempting to answer these questions with some studies showing alarming results that the assessed methods do not always live up to the expectations [2, 29, 80, 93, 102]. The question marks about the performance and evaluation of XAI methods remain for both the general case and specifically for digital pathology.\nIt is often stated that AI algorithms are black boxes that need to become transparent. While this is an illustrative metaphor, it is also necessary to carefully consider what type of transparency that is meaningful and effective in different scenarios and for different stakeholders. In summary, there is strong rationale for XAI advances tailored for digital pathology, however the challenge of constructing meaningful explanation and evaluating the performance of a chosen XAI method still remains."}, {"heading": "5 Future outlook", "text": "Our analysis of the overview presented in the previous sections has led to several key findings that can be informative for future research in XAI for digital pathology. A first insight is that the comprehensive list of identified explainability scenarios points to the fact that there are many techniques to consider when developing XAI solutions in the domain. XAI is also an important concept that needs to be incorporated for decision support \u2013 due to the potentially high costs of errors in healthcare, pathologists are concerned about using black-box algorithms in their daily practices and call for an increased transparency [72]. Moreover, there is a substantial heterogeneity both in terms of the desired benefits that AI tools should bring, and the types of prediction tasks to be performed. Thus, it appears clear that even within this niche a multi-faceted XAI toolbox will be needed.\nIn order for the XAI researcher to navigate the digital pathology landscape, it is valuable to consider the three usage scenarios: development, QA, and diagnostic work. For example, for the model developer, all of the explanation types would be relevant, whereas the inner workings of the neural network ought to be of little relevance for QA or diagnostics. Understanding training data quality is probably unnecessary for diagnostic work but may be quite important for the QA specialist to assess limitations of the model at the specific lab.\nSimilar mappings can be done with respect to result representation. A likely difference between usage scenarios here is that synthetic visualisations would only be valuable to the model developer. There may, however, be exceptions. Mittelstadt et al. [70] argue that counterfactual and contrastive explanations are suitable for intuitive human understanding. The synthesised counterfactual image proposed by Seah et al. [94] is an interesting direction, combining the synthesis and showing by example explanations. It is our expectation that combinations of XAI techniques will be needed in order to be sufficiently effective in most digital pathology applications.\nFinally, this survey sheds some light on the role of uncertainty in relation to XAI. Whereas uncertainty estimation sometimes is seen as a separate topic, the survey indicates that uncertainty is an integral part of the XAI scope when seen from an end-user perspective. Our review lists some useful existing methods for performing and evaluating uncertainty estimation. There is, however, ample room for further research efforts, not the least directed towards imaging diagnostics applications. Moreover, we argue that AI solutions for clinical routine need to have some level of uncertainty awareness. While this is not part of bread-andbutter AI development today, we hope that incorporating uncertainty soon will be the standard, which in turn will have a direct positive impact for broad XAI deployment."}, {"heading": "A Reviewed Methods", "text": "Method Explains Result representation Technical approach Quantile Regression Irreducible Auxiliary measures Frequentist techniques Forests [69] uncertainty Conformal Prediction [84] Reducible uncertainty Auxiliary measures Other techniques Activation Inner workings Synthetic visualisations Activation optimisation maximisation [28] Saliency maps [100] Predictions Visualisations on the input Back-projection to input space DeconvNet [120] Predictions Synthetic visualisations Back-projection to input space Guided Backprop [103] Predictions Synthetic visualisations Back-projection to of NN input space Object detectors in Prediction; Visualisations on the input Input perturbations Deep Scene [124] Inner workings Layer-wise relevance Predictions Visualisations on the input Back-projection to propagation [8] input space Probabilistic Reducible uncertainty Auxiliary measures Bayesian neural backpropagation [40] networks Inverting Deep Image Inner workings Synthetic visualisations Activation optimisation Representations [67] BCNN with Bernoulli Reducible uncertainty Auxiliary measures Bayesian neural approximations [32] networks Deep visualization [118] Inner workings Synthetic visualisations Activation optimisation Bayes by Backprop [12] Reducible uncertainty Auxiliary measures Bayesian neural networks Bootstrapped DQN [82] Reducible uncertainty Auxiliary measures Frequentist techniques Multifaceted Feature Inner workings Synthetic visualisations Activation optimisation Visualization [79] Inverting feature Inner workings Synthetic visualisations Activation optimisation representations [23] Class activation Prediction Visualisations on the input Back-projection to mapping (CAM) [125] input space Dropout as Bayesian Reducible uncertainty Auxiliary measures Bayesian approximation [33] approximations Disco nets [13] Irreducible uncertainty Auxiliary measures Bayesian approximations Deep generator Inner workings Synthetic visualisations GANs methods networks [78] Distinct class Prediction Visualisations on the input Back-projection to saliency maps [98] input space LIME [90] Prediction Visualisations on the input Input perturbations Uncertainty with Reducible uncertainty; Auxiliary measures Frequentist techniques deep ensembles [60] Irreducible uncertainty\nof NN\nDeep Taylor Predictions Visualisations on the input Back-projection to decomposition [73] input space SHAP [65] Predictions Visualisations on the input Input perturbations\nPatternNet and Predictions Visualisations on the input Back-projection to PatternAttribution [55] input space Integrated Gradients [107] Predictions Visualisations on the input Back-projection to input space Network Dissection [10] Inner workings Showing by example Other techniques\nGrad-CAM [95] Predictions Visualisations on the input Back-projection to input space Uncertainties in Bayesian Reducible uncertainty; Auxiliary measures Bayesian neural deep learning [50] Irreducible uncertainty networks Meaningful Predictions Visualisations on the input Input perturbations Perturbation [30] SmoothGrad [101] Predictions Visualisations on the input Back-projection to input space Real Time Image Predictions Visualisations on the input Input perturbations Saliency [20] Prediction Difference Analysis [127] Predictions Visualisations on the input Input perturbations Influence Functions [57] Predictions Auxiliary measures Other techniques Interpretable CNNs [123] Predictions Visualisations on the input Interpretable design Generative Visual Rationales [94] Predictions Synthetic visualisations GANs methods Weight-perturbation Reducible uncertainty Auxiliary measures Bayesian neural in ADAM [52] networks Uncertainty in Bayesian Reducible uncertainty; Auxiliary measures Bayesian neural Deep Learning [21] Irreducible uncertainty networks Monotone composite Irreducible uncertainty Auxiliary measures Frequentist techniques quantile regression NN [16] Deep k-Nearest Predictions; Showing by example; Other techniques Neighbors [85] Reducible uncertainty Auxiliary measures Laplace approximation for Reducible uncertainty Auxiliary measures Bayesian approximations estimating uncertainty [91] TCAV [54] Inner workings Showing by example Other techniques Bayesian uncertainty in Reducible uncertainty Auxiliary measures Bayesian approximations batch normalized NN [112] Excitation Backprop [121] Predictions Visualisations on the input Back-projection to input space Prediction Intervals for Irreducible uncertainty Auxiliary measures Frequentist techniques Deep Learning [87] Bayesian Ensembling [86] Reducible uncertainty Auxiliary measures Bayesian approximations Blocks [81] Inner workings Synthetic visualisations Activation optimisation Class Hierarchy in CNNs [3] Inner workings Showing by example Other techniques Data augmentation for Irreducible uncertainty Auxiliary measures Input perturbations uncertainty estimation [7] Visualization and Anomaly Inner workings; detection using t-SNE [51] Reducible uncertainty Showing by example Other techniques"}], "title": "Survey of XAI in digital pathology", "year": 2020}