{"abstractText": "The Rashomon effect occurs when many different explanations exist for the same phenomenon. In machine learning, Leo Breiman used this term to characterize problems where many accurate-but-different models exist to describe the same data. In this work, we study how the Rashomon effect can be useful for understanding the relationship between training and test performance, and the possibility that simple-yet-accurate models exist for many problems. We consider the Rashomon set\u2014the set of almost-equally-accurate models for a given problem\u2014and study its properties and the types of models it could contain. We present the Rashomon ratio as a new measure related to simplicity of model classes, which is the ratio of the volume of the set of accurate models to the volume of the hypothesis space; the Rashomon ratio is different from standard complexity measures from statistical learning theory. For a hierarchy of hypothesis spaces, the Rashomon ratio can help modelers to navigate the trade-off between simplicity and accuracy. In particular, we find empirically that a plot of empirical risk vs. Rashomon ratio forms a characteristic \u0393-shaped Rashomon curve, whose elbow seems to be a reliable model selection criterion. When the Rashomon set is large, models that are accurate\u2014but that also have various other useful properties\u2014can often be obtained. These models might obey various constraints such as interpretability, fairness, or monotonicity.", "authors": [{"affiliations": [], "name": "Lesia Semenova"}, {"affiliations": [], "name": "Cynthia Rudin"}], "id": "SP:65d3bd025324106338073b9fa89645790fa13ebf", "references": [{"authors": ["Peter L Bartlett", "Olivier Bousquet", "Shahar Mendelson"], "title": "Local Rademacher complexities", "venue": "The Annals of Statistics,", "year": 2005}, {"authors": ["Olivier Bousquet", "Andr\u00e9 Elisseeff"], "title": "Stability and generalization", "venue": "Journal of machine learning research,", "year": 2002}, {"authors": ["Leo Breiman"], "title": "Statistical modeling: The two cultures (with comments and a rejoinder by the author)", "venue": "Statistical science,", "year": 2001}, {"authors": ["Christopher JC Burges"], "title": "A tutorial on support vector machines for pattern recognition", "venue": "Data mining and knowledge discovery,", "year": 1998}, {"authors": ["Xiao Cai", "Feiping Nie", "Heng Huang", "Chris Ding"], "title": "Multi-class L2, 1-norm support vector machine", "venue": "In Data Mining (ICDM),", "year": 2011}, {"authors": ["Feilong Cao", "Tingfan Xie", "Zongben Xu"], "title": "The estimate for approximation error of neural networks: A constructive approach", "year": 2008}, {"authors": ["Rich Caruana", "Yin Lou", "Johannes Gehrke", "Paul Koch", "Mark Sterm", "No\u00e9mie Elhadad"], "title": "Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission", "venue": "In Proceedings of Knowledge Discovery in Databases (KDD),", "year": 2015}, {"authors": ["Oleg Davydov"], "title": "Algorithms and error bounds for multivariate piecewise constant approximation. In Approximation Algorithms for Complex Systems, pages 27\u201345", "year": 2011}, {"authors": ["Ronald A DeVore"], "title": "Nonlinear approximation", "venue": "Acta numerica,", "year": 1998}, {"authors": ["Laurent Dinh", "Razvan Pascanu", "Samy Bengio", "Yoshua Bengio"], "title": "Sharp minima can generalize for deep nets", "venue": "arXiv preprint arXiv:1703.04933,", "year": 2017}, {"authors": ["Aaron Fisher", "Cynthia Rudin", "Francesca Dominici"], "title": "All models are wrong, but many are useful: Learning a variable\u2019s importance by studying an entire class of prediction models simultaneously", "venue": "Journal of Machine Learning Research,", "year": 2019}, {"authors": ["David Galvin"], "title": "Three tutorial lectures on entropy and counting", "venue": "arXiv preprint arXiv:1406.7872,", "year": 2014}, {"authors": ["2009. Ravi Kannan", "L\u00e1szl\u00f3 Lov\u00e1sz", "Mikl\u00f3s Simonovits"], "title": "Random walks and an O\u2217(n5) volume", "venue": "Information Processing Systems,", "year": 2009}, {"authors": ["Vladimir Koltchinskii", "Dmitry Panchenko"], "title": "Empirical margin distributions and bound", "venue": "minima. arXiv preprint arXiv:1609.04836,", "year": 2016}, {"authors": ["Guillaume Lecu\u00e9"], "title": "Interplay between concentration, complexity and geometry in learning theory with applications to high dimensional data analysis", "venue": "PhD thesis,", "year": 2011}, {"authors": ["Benjamin Letham", "Cynthia Rudin", "Tyler H. McCormick", "David Madigan"], "title": "Interpretable classifiers using rules and bayesian analysis: Building a better stroke prediction model", "venue": "Annals of Applied Statistics,", "year": 2015}, {"authors": ["Benjamin Letham", "Portia A. Letham", "Cynthia Rudin", "Edward Browne"], "title": "Prediction uncertainty and optimal experimental design for learning dynamical systems", "venue": "Chaos,", "year": 2016}, {"authors": ["G\u00e1bor Lugosi", "Andrew B Nobel"], "title": "Adaptive model selection using empirical complexities", "venue": "The Annals of Statistics,", "year": 1999}, {"authors": ["G\u00e1bor Lugosi", "Marten Wegkamp"], "title": "Complexity regularization via localized random penalties", "venue": "The Annals of Statistics,", "year": 2004}, {"authors": ["Florence Jessie MacWilliams", "Neil James Alexander Sloane"], "title": "The theory of errorcorrecting codes, volume 16", "year": 1977}, {"authors": ["Nicolai Meinshausen", "Peter B\u00fchlmann"], "title": "Stability selection", "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "year": 2010}, {"authors": ["Shahar Mendelson"], "title": "A few notes on statistical learning theory", "venue": "In Advanced lectures on machine learning,", "year": 2003}, {"authors": ["Daniel Nevo", "Ya\u2019acov Ritov"], "title": "Identifying a minimal class of models for high-dimensional data", "venue": "The Journal of Machine Learning Research,", "year": 2017}, {"authors": ["DJ Newman", "TJ Rivlin"], "title": "Approximation of monomials by lower degree polynomials", "venue": "Aequationes Mathematicae,", "year": 1976}, {"authors": ["Ramamohan Paturi"], "title": "On the degree of polynomials that approximate symmetric boolean functions (preliminary version)", "venue": "In Proceedings of the Twenty-Fourth Annual ACM Symposium on Theory of Computing,", "year": 1992}, {"authors": ["Narges Razavian", "Saul Blecker", "Ann Marie Schmidt", "Aaron Smith-McLallen", "Somesh Nigam", "David Sontag"], "title": "Population-level prediction of type 2 diabetes from claims data and analysis of risk factors", "venue": "Big Data,", "year": 2015}, {"authors": ["William H Rogers", "Terry J Wagner"], "title": "A finite sample distribution-free performance bound for local discrimination rules", "venue": "The Annals of Statistics,", "year": 1978}, {"authors": ["Cynthia Rudin", "Caroline Wang", "Beau Coker"], "title": "The age of secrecy and unfairness in recidivism prediction", "venue": "Harvard Data Science Review,", "year": 2019}, {"authors": ["Robert E Schapire", "Yoav Freund", "Peter Bartlett", "Wee Sun Lee"], "title": "Boosting the margin: A new explanation for the effectiveness of voting methods", "venue": "The Annals of Statistics,", "year": 1998}, {"authors": ["John Shawe-Taylor", "Peter L Bartlett", "Robert C Williamson", "Martin Anthony"], "title": "Structural risk minimization over data-dependent hierarchies", "venue": "IEEE Transactions on Information Theory,", "year": 1998}, {"authors": ["Nathan Srebro", "Karthik Sridharan", "Ambuj Tewari"], "title": "Smoothness, low noise and fast rates", "venue": "In Advances in Neural Information Processing Systems,", "year": 2010}, {"authors": ["Nikolaj Tollenaar", "P.G.M. van der Heijden"], "title": "Which method predicts recidivism best?: a comparison of statistical, machine learning and data mining predictive models", "venue": "Journal of the Royal Statistical Society: Series A (Statistics in Society),", "year": 2013}, {"authors": ["Theja Tulabandhula", "Cynthia Rudin"], "title": "Machine learning with operational costs", "venue": "The Journal of Machine Learning Research,", "year": 2013}, {"authors": ["Theja Tulabandhula", "Cynthia Rudin"], "title": "On combining machine learning with decision making", "venue": "Machine Learning (ECML-PKDD journal track),", "year": 2014}, {"authors": ["Theja Tulabandhula", "Cynthia Rudin"], "title": "Robust optimization using machine learning for uncertainty sets", "venue": "arXiv preprint arXiv:1407.1097,", "year": 2014}, {"authors": ["Berk Ustun", "Cynthia Rudin"], "title": "Supersparse Linear Integer Models for Optimized Medical Scoring Systems", "venue": "Machine Learning,", "year": 2016}, {"authors": ["Vladimir N Vapnik"], "title": "The Nature of Statistical Learning", "year": 1995}, {"authors": ["VN Vapnik", "A Ya Chervonenkis"], "title": "On the uniform convergence of relative frequencies of events to their probabilities", "venue": "Theory of Probability and its Applications,", "year": 1971}, {"authors": ["Christopher S Wallace", "David M Boulton"], "title": "An information measure for classification", "venue": "The Computer Journal,", "year": 1968}, {"authors": ["Jiaming Zeng", "Berk Ustun", "Cynthia Rudin"], "title": "Interpretable classification models for recidivism prediction", "venue": "Journal of the Royal Statistical Society: Series A (Statistics in Society),", "year": 2017}, {"authors": ["Ding-Xuan Zhou"], "title": "The covering number in learning theory", "venue": "Journal of Complexity,", "year": 2002}, {"authors": ["Ji Zhu", "Saharon Rosset", "Robert Tibshirani", "Trevor J Hastie"], "title": "1-norm support vector machines", "venue": "In Advances in Neural Information Processing Systems,", "year": 2004}], "sections": [{"text": "Keywords: Rashomon Set, Model Multiplicity, Simplicity, Generalization, Interpretable Machine Learning, Model Selection"}, {"heading": "1. Introduction", "text": "The 1950 Kurosawa film \u201cRashomon\u201d(Kurosawa, 1950) revolves around four characters describing entirely different perspectives on the same crimes. Based on this idea that there could be many seemingly accurate descriptions of the same data, Leo Breiman (Breiman et al., 2001) coined the term \u201cRashomon effect\u201d to describe cases when there exist many\nar X\niv :1\n90 8.\n01 75\n5v 2\n[ cs\n.L G\ndifferent approximately-equally accurate models. He noticed that this effect happens very often; there is no \u201cbest\u201d model from most finite data sets, only many good descriptions. Of course, in machine learning, our goal is not necessarily to find out the truth; it is to predict well out-of-sample.\nDecades of study about generalization in machine learning have provided many different mathematical theories. Many of them measure the complexity of classes of functions without considering the data (e.g., VC theory, Vapnik, 1995), or measure properties of specific algorithms (e.g., algorithmic stability, see Bousquet and Elisseeff, 2002). However, none of these theories seems to directly capture a phenomenon that occurs throughout practical machine learning. In particular, there are a vast number of data sets for which many standard machine learning algorithms perform similarly. In these cases, the machine learning models tend to generalize well. Furthermore, in these same cases, there is often a simpler model that performs similarly and also generalizes well. Perhaps the Rashomon effect can help us to explain that phenomenon.\nIn this work, we aim to quantify the Rashomon effect, show that it has implications for generalization, and show it has implications for the existence of simpler models. If the Rashomon effect is large, it means that there exists a large number of models (the empirical Rashomon set) that perform approximately-equally-well on the training data. When the empirical Rashomon set is large, it is reasonable to assume that models with various desirable properties can exist inside it. For example, sparser, more transparent models, models that obey domain-specific constraints such as monotonicity along a given set of features, models that obey fairness constraints, and models that rely more on features that we can trust\u2014these models may all live within the same large Rashomon set. Proving whether an interpretable (or fair, or monotonic) model exists in the Rashomon set is a challenging practical problem in general, but solving for such a model directly can be even harder. For instance, finding optimal sparse, accurate models of various forms (linear models with integer coefficients, decision sets, rule lists, decision trees) can be NP hard, sometimes with no polynomial time approximation. Let us thus return to the possibility of aiming to prove that desirable (\u201csimpler\u201d) models exist within the Rashomon set prior to finding them. As we will show in Section 4.2, there are assumptions that allow us to prove existence of simpler models within the Rashomon set. If the assumptions are satisfied, a model from a simpler class is approximately as accurate as the most accurate model within the hypothesis space, which consequently leads to better generalization guarantees. The assumptions are based in approximation theory, which models how one class of functions can approximate another.\nProving the existence of interpretable (or fair, or otherwise constrained) models before aiming to find them differs from the current approach to machine learning in practice. Increasingly, machine learning practitioners have access to a plethora of machine learning techniques, such as deep networks, that, with some tweaking, can produce reasonable test and training performance. Such exceedingly complex models are often among the first models practitioners try to fit, rather than the last. The practical question that can arise in such cases is whether such complex models are really necessary, or if similarly accurate models that satisfy other criteria, such as interpretability or fairness, might be proven to exist with little computational effort. As we discuss later, the knowledge of a large\n.\nRashomon set provides evidence in advance that such a search may be fruitful, and that there are computationally inexpensive ways to gauge whether the Rashomon set is large.\nWe quantify the magnitude of the Rashomon effect through the Rashomon volume, which is the size of the set of models that performs almost equally well on the training data. An illustration of the Rashomon set is shown in Figure 1. The Rashomon ratio is a ratio of the Rashomon volume to the volume of a hypothesis space. In this manuscript, we explore the connections between the Rashomon volume, Rashomon ratio, hierarchies of hypothesis spaces, training performance and generalization.\nThe Rashomon ratio can serve as a gauge of simplicity for a learning problem. As a property of both a data set and a hypothesis space, it differs from the VC dimension (Vapnik and Chervonenkis, 1971) (as Rashomon ratio is specific to a data set), it differs from algorithmic stability (see Rogers and Wagner, 1978; Kearns and Ron, 1999) (as the Rashomon ratio does not rely on robustness of an algorithm with respect to changes in the data), it differs from local Rademacher complexity (Bartlett et al., 2005) (as the Rashomon ratio does not measure the ability of the hypothesis space to handle random changes in targets and actually benefits from multiple similar models), and it differs from geometric margins (Vapnik, 1995) (as one can have a small margin with a large Rashomon ratio, and margins are measured with respect to one model, whereas the Rashomon ratio considers the existence of many). We provide theorems that show simple cases when size of the Rashomon set does not correlate with these complexity measures in Section 6. The Rashomon set is not simply a flat minimum; it could consist of many non-flat local minima as illustrated in Figure 2b, and it works for discrete hypothesis spaces where gradients, and thus \u201csharpness\u201d (Dinh et al., 2017) do not exist. We provide basic generalization bounds showing how the Rashomon ratio gauges the existence of simpler-yet-accurate solutions that generalize well.\nWe empirically observe a trend across different data sets between the Rashomon ratio and empirical risk when considering a hierarchy of hypothesis spaces (in particular, decision trees with increasing depth, or linear models with increasing model size). This trend, which we discuss in Section 8 is in the form of a \u0393-shaped curve, which is formed as we move\nup the hierarchy to increase the size of the hypothesis space. Specifically, as the size of the hypothesis space increases, first the empirical risk of the best model in the class decreases, and the Rashomon ratio grows or remains approximately the same; then the empirical risk of the best model in the class is relatively constant, but the Rashomon ratio decreases. This trend, which we call the Rashomon curve, occurs for all 52 out of the 52 data sets (38 classification and 14 regression) that we downloaded from the UCI Machine Learning Repository (Dua and Graff, 2019) for decision tree classifiers of various depths, and polynomial regressors of varying degrees.\nIn our experiments there is a connection between model selection and the Rashomon ratio. In particular, we find that the turning point in the \u0393-shaped Rashomon curve, which can be formulated as a simple minimax optimization problem, is often a good choice for model selection. This Rashomon elbow balances between a low complexity (or a smaller size) hypothesis space (corresponding to a small Rashomon volume) and a low empirical risk (corresponding to a high accuracy). We show how the Rashomon elbow can be useful to choose the complexity of a hypothesis space to balance training accuracy with generalization.\nOur results have implications beyond those where the size of the Rashomon volume can be estimated in practice. In particular, our results indicate that when many machine learning methods perform similarly on the same data set (without overfitting), it could be because the Rashomon set of the functions these algorithms consider is large. In that case, it may be worthwhile to find models within the Rashomon set that have desirable properties, such as interpretability. Since it is harder to optimize for constrained models to achieve interpretibility rather than simply to run several different standard machine learning methods, it is beneficial to run the standard machine learning methods first. This would allow the practitioner to gauge whether the constrained optimization is likely to yield a model that is approximately as accurate as the standard methods.\nWe also discuss different methods of measuring the Rashomon volume and the Rashomon ratio. For linear regression, we derive a closed form solution for the Rashomon volume in parameter space. When the Rashomon set is bounded and convex in parameter space, we design a separating oracle and use known randomized algorithm guarantees as discussed in Appendix D.2. In the more general case, we propose to use rejection sampling and importance sampling methods in the hypothesis space for estimation of the Rashomon ratio, and use these for our experiments.\nWe summarize the contributions of this work as follows: (i) We define the Rashomon volume and the corresponding Rashomon ratio as important characteristics of the Rashomon set. (ii) We provide generalization bounds for models from the Rashomon set, and show that the size of the Rashomon set serves as a barometer for the existence of accurate-yet-simpler models that generalize well. These are different from standard learning theory bounds that consider the distance between the true and empirical risks for the same function. (iii) We illustrate the \u0393-shaped Rashomon curve on many data sets, and show that the Rashomon elbow can be a useful choice for model selection. (iv) We show empirically that in cases when a large Rashomon set occurs, most machine learning methods tend to perform similarly, and also in these cases, interpretable or sparse (yet accurate) models exist. (v) We provide several approaches for estimating the size of the Rashomon set. (vi) We demonstrate that the Rashomon ratio, as a gauge of simplicity of a machine learning problem, is different from other known complexity measures such as VC-dimension, algorithmic stability, geometric margin, and Rademacher complexity."}, {"heading": "2. Related Work", "text": "There are several bodies of relevant literature as discussed below.\nRashomon sets: Rashomon sets have been used for various purposes (Breiman et al., 2001; Srebro et al., 2010; Fisher et al., 2019; Coker et al., 2018; Tulabandhula and Rudin, 2014b; Meinshausen and Bu\u0308hlmann, 2010; Letham et al., 2016; Nevo and Ritov, 2017). For instance, Srebro et al. (2010) consider a loss restricted class of close-to-optimal models, and with an assumption of H-smoothness of a loss function, they obtain a tighter excess risk bound through local Rademacher complexity (Bartlett et al., 2005). Our bounds do not work the same way and aim to prove a different type of result. Other works aim to search through the Rashomon set to find the most extreme models within it, rather than looking at the size of the Rashomon set, as we do in this work. Fisher et al. (2019) leverages the Rashomon set in order to understand the spectrum of variable importance and other statistics across the set of good models. Our work considers the existence of models from simpler classes rather than exploring the Rashomon set to find a range of variable importance or other statistics. The work of Tulabandhula and Rudin (2013, 2014a,b) uses the Rashomon set to assist with decision making, by finding the range of downstream operational costs associated with the Rashomon set. Rashomon sets are related to phacking and robustness of estimation, because the Rashomon set is a set over which one might conduct a sensitivity analysis to choices made by an analyst (Coker et al., 2018).\nFlat minima or wide valleys: The concept of flat minima (wide valleys) has been explored in the deep learning literature as a possible way to understand convergence properties of the complicated, non-convex loss functions that deep networks traverse during training\n(Hochreiter and Schmidhuber, 1997; Dinh et al., 2017; Keskar et al., 2016; Chaudhari et al., 2016; Keskar et al., 2016). Based on a minimum-message-length argument (Wallace and Boulton, 1968), several works claim that flat loss functions lead to better generalization due to a robustness to noise around the minimum (Hochreiter and Schmidhuber, 1997; Keskar et al., 2016; Chaudhari et al., 2016). Following Hochreiter and Schmidhuber (1997), Dinh et al. (2017) define -flatness, which constitutes a special case of our Rashomon sets, as shown in Figure 2. In particular, our Rashomon set is defined over the hypothesis (functional) space, while -flatness is defined in a parameter space (though sometimes we use parameter space for ease of computation), and the Rashomon set is not necessarily a single connected component (although it might be in the case of a convex loss over a continuous domain), while -flatness pertains only to a connected set. This means that the Rashomon set can contain models from different local minima, or can be defined on discrete spaces, while -flatness is relevant only for continuous loss functions. Another way of quantifying flatness is \u03c3-sharpness (Keskar et al., 2016; Dinh et al., 2017), which measures the change of the loss function inside a \u03c3-ball in a parameter space. In the case of a connected Rashomon set, this loss difference corresponds to the Rashomon parameter \u03b8.\nStatistical learning theory: Numerous works provide generalization bounds based on different complexity measures, and under different assumptions. Some of them include Rademacher (Srebro et al., 2010; Kakade et al., 2009) and Gaussian complexities (Kakade et al., 2009), PAC-Bayes theorems (Langford and Shawe-Taylor, 2003), covering numbers bounds (Zhou, 2002), and margin bounds (Vapnik and Chervonenkis, 1971; Schapire et al., 1998; Koltchinskii et al., 2002), etc. In contrast, under assumptions elaborated in Section 4, the Rashomon ratio provides a certificate of the existence of a simpler model that generalizes, rather than acting itself as a simplicity measure. The use of approximating sets, as used extensively in this paper, is used throughout the literature on learning theory (Lecue\u0301, 2011; Lugosi and Wegkamp, 2004; Schapire et al., 1998; Mendelson, 2003). An excellent example of this is the classical generalization bound for boosting and margins (Schapire et al., 1998), which uses combinations of several random draws of base classifiers to represent combinations of base classifiers. This is an instance of the so-called \u201cMaurey\u2019s lemma,\u201d which often provides this approximating set for linear model classes.\nModel selection: The closest model selection literature to our work (Lugosi and Nobel, 1999; Shawe-Taylor et al., 1998) focuses on separately estimating the complexity of each hypothesis space within a hierarchy. The complexity of each member of the hierarchy is measured through VC-dimension (Shawe-Taylor et al., 1998) or empirical covers (Lugosi and Nobel, 1999); once a member of the hierarchy is chosen (a specific hypothesis space), they would choose a model that minimizes risk within the hypothesis space. Our work also chooses models that minimize the risk in each hypothesis space, and provides a specific guideline on how to choose the hypothesis space using the Rashomon elbow.\nWe know of no other works that illustrate the Rashomon curve, which we have observed universally among data sets we have considered."}, {"heading": "3. Rashomon Set Definitions and Notation", "text": "Consider a training set of n data points S = {z1, z2, ..., zn}, zi = (xi, yi) drawn i.i.d. from an unknown distribution D on a bounded set Z = X \u00d7Y, where X \u2282 Rp and Y \u2282 R are an\ninput and an output space respectively. Our hypothesis space is F = {f : X \u2192 Y}. We limit the hypothesis space F to contain only models that vary within the bounded domain Z where the data reside. We will assume that the hypothesis space is bounded and that there is a prior distribution \u03c1 over functions in F . Often we will use a term functional space to refer to a non-parametric hypothesis space. To measure the quality of a prediction made by a hypothesis, we use a loss function \u03c6 : Y \u00d7Y \u2192 R+. Specifically, for each given point z = (x, y) and a hypothesis f , the loss function is \u03c6(f(x), y). For a given f we will also overload notation by writing l : F \u00d7Z \u2192 R+ that takes f explicitly as an argument: l(f, z) = \u03c6(f(x), y). We are interested in learning a model f that minimizes the true risk L(f) = Ez\u223cD[\u03c6(f(x), y)], which depends on an unknown distribution D and therefore is estimated with an empirical risk : L\u0302(f) = 1n \u2211n i=1 \u03c6(f(xi), yi). For the rest of this paper, data are drawn from the unknown distribution D on X \u00d7Y, unless otherwise specified."}, {"heading": "3.1 Basic Rashomon Set and Ratio Definition", "text": "We define the empirical Rashomon set (or simply Rashomon set) as a subset of models of the hypothesis space F that have training performance close to the best model in the class, according to a loss function (Breiman et al., 2001; Srebro et al., 2010; Fisher et al., 2019; Coker et al., 2018; Tulabandhula and Rudin, 2014b). More precisely:\nDefinition 1 (Rashomon set) Given \u03b8 \u2265 0, a data set S, a hypothesis space F , and a loss function \u03c6, the Rashomon set R\u0302set(F , \u03b8) is the subspace of the hypothesis space defined as follows:\nR\u0302set(F , \u03b8) := {f \u2208 F : L\u0302(f) \u2264 L\u0302(f\u0302) + \u03b8},\nwhere f\u0302 is an empirical risk minimizer for the training data S with respect to a loss function \u03c6: f\u0302 \u2208 argminf\u2208F L\u0302(f).\nIf we want to specify the data set S that is used to compute the Rashomon set, we indicate the data set in the subscript, as R\u0302setS (F , \u03b8). Fisher et al. (2019)\u2019s definition of Rashomon set is distinct from ours in that we typically use an empirical risk minimizer to define the Rashomon set instead of a prespecified reference model which is independent of the sample.\nThe hypothesis space F in the definition of the Rashomon set can be a specific welldefined hypothesis space, such as the space of decision trees of depth D or neural nets with D hidden layers, or it can be a more general space (a meta-hypothesis space) that contains models from different hypothesis spaces (e.g., linear functions, polynomials up to degree D, and piece-wise constant functions) with the training error as a loss function.\nWe call \u03b8 the Rashomon parameter. To compute the size of the Rashomon set we will use the Rashomon volume V(R\u0302set(F , \u03b8)), where V(\u00b7) : F \u2192 R\u22650 measures the volume of a set in the hypothesis space, potentially weighted by the prior \u03c1 over functions. The practitioner can define the Rashomon volume in a way that would be specific to a learning problem and hypothesis space. In general, the Rashomon volume is V(R\u0302set(F , \u03b8)) = \u222b f\u2208F 1f\u2208R\u0302set(F ,\u03b8)\u03c1(f)d\u03c1, where \u03c1 is a prior on the hypothesis space. We assume that the Rashomon set is bounded and therefore V(R\u0302set(F , \u03b8)) < \u221e. If the hypothesis space is discrete with uniform prior then the Rashomon volume can be calculated\nby counting how many models are inside R\u0302set(F , \u03b8) or, in other words, by computing the cardinality of the Rashomon set directly: V(R\u0302set(F , \u03b8)) = \u2211 f\u2208F 1f\u2208R\u0302set(F ,\u03b8).\nWhen we compare the size of the Rashomon set for different hypothesis spaces, the Rashomon volume might not be an informative measure, especially if the hypothesis spaces we consider have very different complexity. To normalize the Rashomon volume, we introduce the Rashomon ratio, which uses the hypothesis space to form the denominator of the ratio. Assumptions that F is bounded and contains models within the bounded domain Z allow us to keep the ratio denominator from containing functions that are irrelevant (and thus increase the denominator without a chance of increasing the numerator). Consequently, V(F) <\u221e. We define the Rashomon ratio as follows:\nDefinition 2 (Rashomon ratio) Let F be a hypothesis space given a data set S. The Rashomon ratio is a ratio of the volume of models inside the Rashomon set R\u0302set(F , \u03b8) to the volume of models in the hypothesis space F :\nR\u0302ratio(F , \u03b8) = V(R\u0302set(F , \u03b8)) V(F) . (1)\nBy our definitions, this ratio is always between 0 and 1. It represents the fraction of models that are good (the fraction of models that fit the data about equally well). If \u03b8 is small, a larger Rashomon ratio implies that more models perform about equally well. When \u03b8 is large enough, the Rashomon set contains all models in F and the ratio is 1.\nAs before, we indicate the data set S that is used to compute the Rashomon ratio in the subscript, as R\u0302ratioS (F , \u03b8).\nIn the definitions above, the Rashomon set considered multiplicity of models. In the definition in the next subsection, we consider multiplicity of predictions instead. Whereas in Definition 1, two models that are different but make the same predictions would be considered different, these two models would join the same equivalence class in the definition of the pattern Rashomon ratio."}, {"heading": "3.2 Pattern Rashomon Ratio for Binary Classification", "text": "For a binary classification on a given data set S we introduce the pattern Rashomon ratio as follows.\nDefinition 3 (Pattern Rashomon ratio) Given a hypothesis space F , data set S, and a binary-valued function \u03b6 : F \u00d7Zn \u2192 {0, 1} (which is usually either sign(f(xi)) or the loss 1[sign(f(xi)6=yi]), the pattern Rashomon ratio is the ratio of possible realizations of \u03b6 vectors from functions within the Rashomon set. Denote \u03b6(f, S) = [\u03b6(f, z1), \u03b6(f, z2), ..., \u03b6(f, zn)]. Also denote binary(i) as the vectorized binary representation of i of size n (e.g., binary(1) = [0, 0, 0, 0, 1] when n=5). The pattern Rashomon ratio is:\nR\u0302patratio(F , \u03b8) = \u22112n\u22121\nj=0 1[\u2203f \u2208 R\u0302setS : \u03b6(f, S) = binary(j)]\u22112n\u22121 j=0 1[\u2203f \u2208 F : \u03b6(f, S) = binary(j)] .\nThe ratio based on patterns is different from the Rashomon ratio defined in (1) as a multiplicity of models, as shown in Figure 3. The pattern Rashomon ratio measures the diversity of predictions made by functions in the Rashomon set compared to the diversity of prediction of functions within the hypothesis space. If the pattern Rashomon ratio is high, it means that the Rashomon set contains not only multiple models, but also multiple models with different properties, depending on the definition of \u03b6.\nThe pattern Rashomon ratio has useful approximation guarantees. In particular as the size of the model space D grows to be infinitely large (e.g., the depth of the decision tree grows infinitely, or number of parameters grows to infinity), the pattern Rashomon ratio approaches a fixed value that depends on the Rashomon parameter and number of points in the data set only. This intuition is summarized in the next proposition.\nProposition 4 (Approximation guarantees for the pattern Rashomon ratio) Let D represent the size of the hypothesis space F . For binary classification and sign performance function \u03b6(f, z) = sign(f(x)), as D \u2192 \u221e, the pattern Rashomon ratio R\u0302patratio(F , \u03b8) \u2192\nR\u0304pat = \u2211 i\u2264b\u03b8nc ( n i)\n2n , and for \u03b8 \u2264 1/2: 2n(H(\u03b8)\u22121)\u221a 8n\u03b8(1\u2212\u03b8) \u2264 R\u0304pat \u2264 2n(H(\u03b8)\u22121), where n is the size of\nthe training data set, and H(\u03b8) = \u2212\u03b8 log2 \u03b8 \u2212 (1\u2212 \u03b8) log2(1\u2212 \u03b8) is the binary entropy.\nIn contrast, there is no obvious limit value for the Rashomon ratio. There exist data distributions such that for a fixed value \u03b8, as the size of the hypothesis space grows, the Rashomon ratio will converge to 0. There also exist data distributions such that the Rashomon ratio may not converge to either zero or one. For example, separable data with a large margin may lead to a limiting Rashomon ratio that is greater than zero.\nThe Rashomon ratio and the pattern Rashomon ratio, as properties of a data set and a hypothesis space, serve as gauge of simplicity of the learning problem. A large Rashomon\nratio means that the Rashomon set contains multiple models within the hypothesis space that have approximately constant empirical risk. These accurate models could either come from multiple local minima or from one wide local minimum. In this case, potentially every reasonable optimization procedure could lead to a hypothesis from the Rashomon set. Therefore, for large Rashomon sets, accurate models tend to be easier to find (as optimization procedures can find them). In other words, if the Rashomon ratio is large, the Rashomon set could contain many accurate and simple models, and the learning problem becomes simpler. On the other hand, smaller Rashomon ratios might imply a harder learning problem, especially in the case of few deep and narrow local minima.\nWe introduce one more variation of the Rashomon set, where the threshold that restricts the risk does not depend on the empirical risk minimizer. Given a parameter \u03b3 \u2265 0, we call the Rashomon set with restricted empirical risk an anchored Rashomon set :\nR\u0302ancset (F , \u03b3) := {f \u2208 F : L\u0302(f) \u2264 \u03b3}.\nWe define also the true anchored Rashomon set based on the true risk as follows:\nRancset (F , \u03b3) := {f \u2208 F : L(f) \u2264 \u03b3}.\nWe will denote R\u0302ancratio(F , \u03b3) and Rancratio(F , \u03b3) as the Rashomon ratios computed on the anchored Rashomon set and the true anchored Rashomon set. Potentially, we could choose \u03b3 so that the true anchored Rashomon set definition mirrors Definition 1 and the true risk is restricted by quantity \u03b3 = L(f\u2217) + \u03b8, where f\u2217 \u2208 F is a model that minimizes the true risk.\nThe true anchored Rashomon set, as it turns out, can be a (practically unmeasurable) certificate of the existence of a simpler model. Since we can never actually explore the anchored Rashomon set, we would never know whether it will be (or has been) useful for a particular problem. We explain this in the next section, and then spend most of our effort considering empirical Rashomon sets, which are easier to work with in practice. In the next section, we discuss the simplicity and generalization properties of models that are in the Rashomon set."}, {"heading": "4. Rashomon Set Models: Simplicity and Generalization", "text": "Building on the discussions from the previous section, let us consider two hypothesis (functional) spaces with different levels of complexity, where the lower-complexity space serves as a good approximating set for the higher-complexity space. The functional spaces are called F1, for the simpler space, and F2, for the more complex space, where F1 \u2282 F2. Here, to determine the complexity of a functional space, we use traditional notions of complexity (conversely, simplicity) such as covering numbers or VC dimension. For a useful example of a simple and a more complex space, consider F2 to be the space of linear models with real-valued coefficients in a space of d dimensions, and consider F1 to be the space of scoring systems (Ustun and Rudin, 2016), which are sparse linear models, with at most d\u2032 nonzero integer coefficients, d\u2032 d.\nGeneralization bounds would be tighter if we could use the lower complexity space F1, but as we are considering functions from F2, learning theory often has us include the complexity of F2 in the bound. Given this, we have several questions to answer:\n1. What if the higher-complexity hypothesis space we chose was more complex than necessary for modeling the data? In that case, can we still have guarantees on test performance of the best classifier in the complex space F2 that leverage the complexity of the lower-complexity space F1 instead of that of space F2? In particular, perhaps special properties of the more complex space can help our analysis; if these properties hold, then we can get all the guarantees we need about the best possible attainable test performance on the more complex space F2 by looking only at optimal training performance on the less complex space F1. (As a preview to Section 4.1 where we answer this question, the property on the complex space that will help us is that the true anchored Rashomon set of F2 is large. However, we cannot know in practice when this property holds, which is a disadvantage of this analysis. When the property holds, even if we do not know it holds, then this property still becomes helpful in practice as it guarantees when it is beneficial to choose a simper hypothesis space.)\n2. Before looking at the data, let us assume we chose to examine the more complex space F2, not knowing that the lower-complexity space F1 would suffice. We may have done this for computational reasons, since perhaps it is much easier to optimize over the higher-complexity space than the lower-complexity space. For instance, as before, perhaps the lower-complexity space consists of scoring systems, which are combinatorially hard to optimize over, whereas the larger space considers standard linear models with real-valued coefficients, which are much easier to optimize. Our question is, after having done some analysis on the higher complexity space, can we still have generalization guarantees that use only the complexity of the lowercomplexity space? Specifically, can we guarantee the existence of a simple-yet-accurate model (a model from the lower-complexity space with low training error) that will generalize well? Can we guarantee that many such simple-yet-accurate models exist? (As a preview to Section 4.2, we will use smoothness and the size of the Rashomon set as key tools to prove the existence of simple-yet-accurate models.)\n3. Again let us assume we chose to examine the more complex hypothesis space, F2. If F1 serves as a good approximating set for F2, then we can easily get a generalization bound for all f2 \u2208 R\u0302set(F2, \u03b8) that uses the complexity of F1. This bound would indicate that the learning problem is not actually as complicated as it might seem if we had only looked at the more complex space of functions F2. In Section 4.3 we will again use smoothness to create this bound.\nIn creating these bounds, we hoped to distill the problems to their essence, so that the bounds are as close as possible to basic learning theory bounds. These bounds, including those for discrete hypothesis spaces can be generalized to more complex statistical learning theory analyses if desired. Note that the bounds in Section 4.1 do not serve the same purpose as standard statistical learning theoretic bounds, as they do not aim to bound generalization error for a single function (that is, the difference between training and test loss for a function). Rather, we are interested in differences between training loss of one function and test loss of another. Standard learning theory analysis handles the single function case nicely; we are concerned with other questions here."}, {"heading": "4.1 The True Anchored Rashomon Set Can Be Very Helpful... But You Might Not Know When", "text": "As in classic Occham\u2019s razor bounds, we start with finite hypothesis spaces. Let us consider finite hypothesis spaces F1 and F2, where F1 \u2282 F2. Consider the first question discussed above: Given F1 and F2, can we have guarantees on the best possible test performance of models in F2, which depend only on F1\u2019s complexity and empirical performance? In the following theorem, we will make a key assumption that allows us to do this: we assume a sufficiently large anchored true Rashomon set for F2. Specifically, we assume that there are a large number of functions from F2 that have true risk below \u03b3. This large number of functions is assumed to be large enough to contain at least one function from F1 (later, in Section 4.2 we show conditions under which simple models from F1 exist in the Rashomon set of F2). Since this special function from F1 is likely to generalize between training and test error (due to learning theory results on F1), we will be able to analyze the best possible true risk from F2 in terms of what we can observe from F1 on the data.\nHere, |F| denotes the cardinality of the finite space F . These bounds can be generalized to infinite hypothesis spaces, but they are designed for intuition, which works nicely with finite hypothesis spaces.\nTheorem 5 (The advantage of a large true anchored Rashomon set I) Consider finite hypothesis spaces F1 and F2, such that F1 \u2282 F2. Let the loss l be bounded by b, l(f2, z) \u2208 [0, b] \u2200f2 \u2208 F2, \u2200z \u2208 Z. Define an optimal function f\u22172 \u2208 argminf2\u2208F2L(f2). Let us assume that the true anchored Rashomon set is large enough to include a function from F1, so there exists a model f\u03031 \u2208 F1 such that f\u03031 \u2208 Rancset (F2, \u03b3). In that case, for any > 0 with probability at least 1\u2212 with respect to the random draw of data:\n|L(f\u22172 )\u2212 L\u0302(f\u03021)| \u2264 \u03b3 + 2b \u221a\nlog | F1 |+ log 2/ 2n ,\nwhere f\u03021 \u2208 argminf1\u2208F1 L\u0302(f1).\nThat is, when the assumption on the geometry of the true risk landscape is correct, we can approximate an optimal model from F2 with the best empirical model within F1, and the bound depends only on the complexity of F1 and not F2 as shown in Figure 4(a). In this case, if we work only with F1 empirically, we would achieve test performance on par with the best test set performance achievable in F2. We can further improve the bound in Theorem 5 by eliminating the complexity term, at the expense of doubling the \u03b3 term.\nTheorem 6 (The advantage of a good approximating set) Consider hypothesis spaces F1 and F2, such that F1 \u2282 F2. Let the loss l be bounded by b, l(f2, z) \u2208 [0, b] \u2200f2 \u2208 F2, \u2200z \u2208 Z. Define an optimal function f\u22172 \u2208 argminf2\u2208F2L(f2). Let us assume that the true anchored Rashomon set is large enough to include a function from F1, so there exists a model f\u03031 \u2208 F1 such that f\u03031 \u2208 Rancset (F2, \u03b3). In that case, for any > 0 with probability at least 1\u2212 with respect to the random draw of data:\n|L(f\u22172 )\u2212 L\u0302(f\u03021)| \u2264 2\u03b3 + b \u221a log 1/\n2n ,\nwhere as before, f\u03021 \u2208 argminf1\u2208F1 L\u0302(f1).\nTheorem 6 can be tighter than Theorem 5 if the approximating set F1 contains a function that is very close to the minimizer of the true loss, relative to the log of the size of F1. Here, smaller values of \u03b3 will result in a tighter bound, though on the other hand, larger values may be needed to ensure the existence of a model f\u03031 in the Rashomon set. In this way, the bound indicates that finding a good approximating set can be important: a better approximating set could allow Theorem 6 to yield a tighter bound than Theorem 5.\nThe main assumption (of a sufficiently large anchored true Rashomon set) in Theorems 5 and 6 is an assumption about the population, and does not rely on the sample. It relies only on the existence of one special function in the true anchored Rashomon set. There are no smoothness assumptions on the loss function. If the main assumption of these theorems holds, then we gain the benefit of guarantees on F2 from looking only at F1 empirically. We cannot check whether the assumption holds since it involves the true risk, but practitioners can reap the benefits of it anyway: when minimizing over F1, they may unknowingly be achieving test error close to the best of F2.\nTo make the connection of this result to Rashomon sets more explicit, we will choose a specific relationship between F1 and F2, specifically, F1 will be a random sample of F2 that is chosen prior to, and separately from, learning. This is an artificial example in that F1 would never actually be chosen as a random sample from F2 in reality. However, the random sampling assumption permits F1 to be distributed fairly evenly around F2, which, arguably, could approximate the way some simpler spaces are embedded in more complex functional spaces. For instance, approximation theory results (discussed later) ensure that functions from some spaces can approximate all functions from other spaces.\n4.1.1 An Example of F1 and F2 Where We Can Estimate the Probability with which a Simpler1 Model is Guaranteed to Be in the Rashomon Set.\nIf F1 is a random sample of functions from F2 (as illustrated in Figure 4(b)), and if F2 has a large true anchored Rashomon set, then F1 is likely to include at least one model from the true anchored Rashomon set. In that case, Theorem 5 applies. Conversely, if F2 has a small true anchored Rashomon set, F1 is unlikely to contain a model from the true anchored Rashomon set, in which case, Theorem 5 does not apply, and there is no guarantee.\nTheorem 7 (The advantage of a large true anchored Rashomon set II) Consider finite hypothesis spaces F1 and F2, such that F1 \u2282 F2 and F1 is uniformly drawn from F2 without replacement. Define an optimal function f\u22172 \u2208 argminf2\u2208F2L(f2). For a loss l bounded by b and any > 0, with probability at least (1\u2212 ) p with respect to the random draw of functions from F2 to form F1 and with respect to the random draw of data:\n|L(f\u22172 )\u2212 L\u0302(f\u03021)| \u2264 \u03b3 + 2b \u221a\nlog | F1 |+ log 2/ 2n , (2)\nwhere p = 1 \u2212 ((1\u2212R anc ratio(F2,\u03b3))| F2 | | F1 | )\n(| F2 || F1 |) = 1 \u2212\n\u220f|Rancset (F2,\u03b3)| i=1 ( 1\u2212 | F1 || F2 |\u2212|Rancset (F2,\u03b3)|+i ) , and f\u03021 \u2208\nargminf1\u2208F1 L\u0302(f1).\nPlease refer to Table 1 for lower bounds of | F1 | from Theorem 7, given values of | F2 | and R\u0302ratio(F2, \u03b3).\nTheorem 7 holds with a probability that depends on the likelihood of drawing F1 from F2. If | F1 | is small compared with | F2 |, the probability p in the theorem statement is small as well. In the following lemma, we provide a lower bound on the Rashomon ratio so that the bound in Theorem 9 holds with probability (1\u2212 )2, instead of (1\u2212 )p. As a key step of the proof of the lemma, we will lower bound the probability of sampling without\n1. \u201cSimpler\u201d in this section means that the model comes from a discrete hypothesis space with smaller cardinality.\nreplacement with the probability of sampling with replacement. Please see the details of the proof in Appendix C.4.\nLemma 8 For a finite hypothesis space F2 of size | F2 |, we will draw | F1 | functions uniformly without replacement from F2 to form F1. If the true anchored Rashomon ratio of the hypothesis space F2 is at least\nRancratio(F2, \u03b3) \u2265 1\u2212 1 | F1 |\nthen with probability at least 1\u2212 with respect to the random draw of functions from F2 to form F1, the Rashomon set contains at least one model f\u03031 from F1.\nCombining Lemma 8 and Theorem 5 we get the following theorem:\nTheorem 9 (The advantage of a large true anchored Rashomon set III) Consider finite hypothesis spaces F1 and F2, such that F1 \u2282 F2 and F1 is uniformly drawn from F2 without replacement. For a loss l bounded by b if the Rashomon ratio is at least\nRancratio(F2, \u03b3) \u2265 1\u2212 1 | F1 |\nthen for any > 0, with probability at least (1\u2212 )2 with respect to the random draw of functions from F2 to form F1 and with respect to the random draw of data:\n|L(f\u22172 )\u2212 L\u0302(f\u03021)| \u2264 \u03b3 + 2b \u221a\nlog | F1 |+ log 2/ 2n ,\nwhere f\u22172 \u2208 argminf2\u2208F2 L(f2), and f\u03021 \u2208 argminf1\u2208F1 L\u0302(f1).\nPlease refer to Table 2 for possible values of the lower bound on the Rashomon ratio, given | F1 | and .\nNote that if each model from the Rashomon set has different performance according to the performance function \u03b6, then Theorem 7 and 9 apply to the pattern Rashomon ratio as well.\nTheorems 7 and 9 guarantee that if the true anchored Rashomon set is sufficiently large and if F1 is selected at random from F2, then with high probability, the best empirical risk over the simpler space F1 is close to the best possible true risk over the larger space F2. The generalization guarantee comes from the size of the simpler space F1.\nThe bound shows directly how the size of the Rashomon set could potentially impact generalization guarantees. The intuition for Theorems 7 and 9 holds beyond the case when F1 is randomly sampled from F2, it holds, for example, when F1 covers F2 sufficiently well. In particular, as the true anchored Rashomon ratio increases, it is more likely that the empirical risk minimum of F1 will be close to the minimum of the true risk of F2."}, {"heading": "4.1.2 Membership in Anchored Rashomon Sets", "text": "Theorems 5, 7, and 9 show the advantage of a large Rashomon set through the possibility of choosing a model from a simpler hypothesis space. However, they have caveats as discussed earlier. As discussed, the theorems\u2019 assumption that the true anchored Rashomon set contains a model from a simpler class is unverifiable. Even if it holds, we cannot check it, as we cannot actually compute the true anchored Rashomon set in practice. However, there might be cases when the empirical and the true Rashomon sets are close (e.g., largely overlapping, or one is a cover for the other), and therefore it is beneficial to know the properties of one to understand of the properties of the other. In particular, with high probability, if a fixed model is contained within the anchored Rashomon set, it also belongs to a slightly larger true anchored Rashomon set. The reverse statement holds as well.\nProposition 10 (Empirical anchored Rashomon set is close to true) For a loss l bounded by b and for any > 0 with probability at least 1 \u2212 e\u22122n( /b)2 with respect to the random draw of training data, if f \u2208 R\u0302ancset (F , \u03b3) then f \u2208 Rancset (F , \u03b3 + ).\nAn analogous statement holds for a model from a true anchored Rashomon set:\nProposition 11 (True anchored Rashomon set is close to empirical) For a loss l bounded by b and for any > 0, if f \u2208 Rancset (F , \u03b3) then with probability at least 1\u2212e\u22122n( /b) 2 with respect to the random draw of training data,\nf \u2208 R\u0302ancset (F , \u03b3 + ).\nProposition 11 is based on the same intuition as Lemma 23 in the work of Fisher et al. (2019), which is used to bound the probability with which a given model is not in the empirical Rashomon set; this is used in a proof of a bound for model class reliance. We use the proposition to indicate the probability with which the empirical anchored Rashomon set is as close as possible to the true anchored Rashomon set for a given model.\nPropositions 10 and 11 show that, for a fixed model, with high probability, its membership in the true anchored and the anchored Rashomon sets are closely related. Therefore, when we consider a given data set S to compute a model in the (empirical) Rashomon set, we can infer that this model is likely to belong to a related true Rashomon set.\nTheorems 5, 7, 9 in this section do not take advantage of the fact that we can investigate F2 empirically, and more easily than we can investigate F1; these theorems instead only discuss the exploration of F1. In what follows, we will study empirical Rashomon sets. Because we are studying empirical Rashomon sets, and because we will work with F2 instead of F1, we will need some mechanism to approximate F2 in terms of F1 and to ensure that functions from F2 generalize; for these purposes, we will use smoothness of the loss over functional space."}, {"heading": "4.2 Existence of Simple-yet-Accurate Models with Good Generalization", "text": "As discussed above, we will now consider empirical Rashomon sets, rather than true Rashomon sets as the assumptions in the earlier theorems are unverifiable, and rely on optimization of the less complex functional space F1 rather than the more complex functional space F2. If optimization over F2 is easier (as it is less constrained), we may want to optimize over F2 first, and be guaranteed the existence of at least one function in F1 based on what we observe with F2. Thus, what we aim to prove in this section is the existence of functions in F1 that are in the Rashomon set of F2. In order to do this using an approximating set argument, we use more assumptions than in the previous section, specifically smoothness. The following theorem shows that, under certain conditions, if there is a function close to F2\u2019s minimizer in hypothesis space that is also in F1, then it is a function we would be looking for: it is also in the Rashomon set of F2 and it probably generalizes.\nFor a hypothesis space F and some f \u2032 \u2208 F let us define the \u03b4-ball of functions centered at f \u2032 as B\u03b4(f\n\u2032) = {f \u2208 F : \u2016f \u2032 \u2212 f\u2016p \u2264 \u03b4}. A loss l : F \u00d7X \u2192 Y is said to be K-Lipschitz, K \u2265 0, if for all f1, f2 \u2208 F and for all z \u2208 Z: |l(f1, z) \u2212 l(f2, z)| \u2264 K\u2016f1 \u2212 f2\u2016p. The p-norm can be defined for example as \u2016f\u2016p = (\u222b X |f | pd\u00b5 )1/p , where \u00b5 is a measure on X .\nTheorem 12 (Existence of a simpler-but-accurate model I) For K-Lipschitz loss l bounded by b consider hypothesis spaces F1 and F2 such that F1 \u2282 F2. With probability greater than 1 \u2212 w.r.t. the random draw of training data, if there exists f\u03041 \u2208 F1 such that \u2016f\u03022 \u2212 f\u03041\u2016p \u2264 \u03b8K , where f\u03022 is the empirical risk minimizer within F2, then for a fixed parameter \u2208 (0, 1):\n1. f\u03041 is in the Rashomon set R\u0302set(F2, \u03b8). 2. \u2223\u2223\u2223L(f\u03041)\u2212 L\u0302(f\u03041)\u2223\u2223\u2223 \u2264 2KRn(F1) + b\u221a log(2/ )2n , where Rn(F) is the standard Rademacher complexity of a functional space F . (This bound arises from standard learning theory.)\nIn the case of one local minimum, if that minimum is wide, then it contains many models that perform similarly and thus yields a large Rashomon set. A wide minimum can occur when, for example, the loss is somewhat flat with a bounded derivative. These are often cases where the loss is locally Lipschitz continuous on the Rashomon set with a small Lipschitz constant K, which would create a tighter bound.\nTheorem 12 also illustrates how we can use a known Lipschitz constant to choose a Rashomon parameter \u03b8. In particular, if we would like to consider a \u03b4-ball in the hypothesis space \u2016f\u0302 \u2212 f\u2016p < \u03b4, then we would choose \u03b8 = K\u03b4, which would keep the bound as small as possible but still permit the result to hold.\nTheorem 13 below is a variation on Theorem 12. It still uses the approximating set argument, but also requires the Rashomon set to be large enough to include a ball of functions. As long as the set of simpler functions is distributed well among the full functional space, the ball contains at least one function from the simpler class.\nTheorem 13 (Existence of a simpler-but-accurate model II) For a K-Lipschitz loss l bounded by b, and hypothesis spaces F1 and F2 such that F1 \u2282 F2. With probability greater\nthan 1\u2212 w.r.t. the random draw of training data, if for every model f2 \u2208 R\u0302set(F2, \u03b8) there exists a model f1 \u2208 F1 such that \u2016f2 \u2212 f1\u2016p \u2264 \u03b4 and if the Rashomon set is large, e.g. it contains a ball of size at least \u03b4, that is, R\u0302set(F2, \u03b8) \u2283 B\u03b4( ), then there exists a model f\u03041 \u2208 R\u0302set(F2, \u03b8), such that for a fixed parameter \u2208 (0, 1):\n1. f\u03041 is from the simpler space F1. 2. \u2223\u2223\u2223L(f\u03041)\u2212 L\u0302(f\u03041)\u2223\u2223\u2223 \u2264 2KRn(F1) + b\u221a log(2/ )2n , where Rn(F) is the standard Rademacher complexity of a functional space F . (This bound arises from standard learning theory.)\nAs we have made approximating set arguments several times, with the most recent theorem (Theorem 13) making an assumption that all models from F2\u2019s Rashomon set are close to a model in F1, let us discuss this assumption. The field of Approximation Theory provides general conditions under which classes of functions can approximate each other. Given a target function from one space, we want to know whether a sequence of functions from another space can converge to the target. Table 3 shows classes of functions F2 that can be approximated with functions from classes F1 within \u03b4 using a specified norm. For instance, piecewise constant functions, such as decision trees, can be approximated by smooth functions.\nThe previous theorems showed the existence of a single function from F1 with desirable properties. Ideally, we would want multiple functions in F1 with these properties, since in practice, when we search F1, we may not find the single function guaranteed by the previous theorem. Theorem 14 below guarantees the existence of more such functions.\nFor a hypothesis space F , define a -packing as a finite set \u039e = {\u03be1, ..., \u03bek|\u03bei \u2208 F} such that \u2016\u03bei \u2212 \u03bej\u2016p > \u03b4 and B\u03b4/2(\u03bei) \u2229 B\u03b4/2(\u03bej) = \u2205 for all i 6= j. The packing number B(F , \u03b4) is the largest \u03b4-packing. Then we have the following:\nTheorem 14 (Existence of multiple simpler models) For K-Lipschitz loss l bounded by b, consider hypothesis spaces F1 and F2 such that F1 \u2282 F2. With probability greater than 1\u2212 w.r.t. the random draw of training data, if for every model f2 \u2208 R\u0302set(F2, \u03b8) there exists a model f1 \u2208 F1 such that \u2016f2 \u2212 f1\u2016p \u2264 \u03b4 then there exists at least B = B(R\u0302set(F2, \u03b8), 2\u03b4) functions f\u030411 , f\u0304 2 1 ..., f\u0304 B 1 \u2208 R\u0302set(F , \u03b8) such that:\n1. They are from a simpler space: f\u030411 , f\u0304 2 1 ..., f\u0304 B 1 \u2208 F1.\n2. \u2223\u2223\u2223L(f\u0304 i1)\u2212 L\u0302(f\u0304 i1)\u2223\u2223\u2223 \u2264 2KRn(F1) + b\u221a log(2/ )2n , for all i \u2208 [1, .., B], where Rn(F) is the Rademacher complexity of a functional space F . (This is from standard learning theory.)\nFrom Theorem 14, we see that since larger Rashomon sets have larger packing numbers, they therefore contain more simpler models with good generalization guarantees.\nNote that in Theorems 12, 13, and 14, other complexity measures from learning theory could be used, such as VC dimension or fat-shattering dimension, to bound the generalization of the hypothesis space F1. We chose the Rademacher complexity as it provides the tightest bound among standard complexity measures.\nAs mentioned briefly earlier, Theorem 14 could have implications in practice, because if our data and algorithm admit a large Rashomon set on a complex space, Theorem 14 suggests that it could be beneficial to locate models from simpler classes within the Rashomon set. These simpler models could be, for instance, models that are constrained to be interpretable. Finding interpretable models can often be computationally demanding, since this generally involves minimizing training loss subject to interpretability constraints, which are often discrete or challenging in other ways. The existence of a large Rashomon set on a more complex space of functions implies that there exist possible many solutions to the constrained optimization problem over the simpler space, and thus it is worthwhile to actually solve this optimization problem over the simpler space. In other words, if the Rashomon set is large, and the other conditions are obeyed, Theorem 14 shows that many interpretable-yet-accurate models would exist, prior to actually finding them."}, {"heading": "4.3 Generalization Bound for All Models from the Rashomon Set", "text": "By using F1 as an approximating set for F2 under smoothness assumptions, we can show that not only do F1\u2019s functions generalize, but also the entire set of functions within F2\u2019s Rashomon set generalize. The generalization guarantee uses the complexity measure of the simpler space F1, rather than that of the more complex space F2.\nThe importance of this result, stated formally in Theorem 15, is the implication that good approximating sets mean that the learning problem is inherently lower complexity than a na\u0308\u0131ve learning theory analysis (which uses F2\u2019s complexity measure) might suggest. This bound is basic and is a distilled version of standard analyses that use approximating sets.\nTheorem 15 (Generalization and reduced complexity of the Rashomon set) For a K-Lipschitz loss l bounded by b consider two hypothesis spaces F1 \u2282 F2 such that for any model f2 \u2208 R\u0302set(F2, \u03b8) there exists a model f1 \u2208 F1 such that \u2016f2 \u2212 f1\u2016p \u2264 \u03b4, then for all f2 \u2208 R\u0302set(F2, \u03b8) and for any > 0 with probability at least 1\u2212 :\u2223\u2223\u2223L(f2)\u2212 L\u0302(f2)\u2223\u2223\u2223 \u2264 2K (\u03b4 +Rn(F1)) + b\u221a log(2/ )\n2n ,\nwhere Rn(F) is the standard Rademacher complexity of a functional space F .\nTheorem 15 shows that if F1 is a good approximation set for the Rashomon set of F2, then we can obtain a generalization guarantee for any function from the Rashomon set of the complex space F2 using only the complexity of the simpler space F1. As a reminder, Table 3 shows examples of function classes where good approximating sets occur. The approximating set is better when \u03b4 is small, and this is when the generalization bound is tighter.\nA large Rashomon set is thus a certificate of better generalization, because it allows us to find models from a simpler space, and/or use complexity measures of a simpler space. Despite the connection of the Rashomon set to generalization, the size of the Rashomon set is not the same as the typical complexity measures of functional spaces. Section 6 relates the size of the Rashomon set to several established complexity measures. Before that, in Section 5, we discuss computation."}, {"heading": "5. Computation of the Rashomon Ratio and Volume", "text": "Often in practice, we do not need to compute the Rashomon ratio, as there is a practical way to check if the Rashomon ratio could be large as we discuss in Section 7. However there are cases when we actually can derive a closed-form solution for the Rashomon volume or estimate the Rashomon ratio using sampling techniques. We discuss these methods of the Rashomon ratio computation in this section.\nThe Rashomon ratio can be viewed as a probability that a model is contained within the Rashomon set, taking a prior over the hypothesis space. In Equation 1 we compute this probability by comparing the volume of the Rashomon set to that of the full hypothesis space. We can compute this probability directly by sampling models from the hypothesis\nspace, and taking the fraction of times the sample lies inside the Rashomon set. By Hoeffding\u2019s inequality, this rejection sampling procedure has probabilistic guarantees and can be used in cases when the hypothesis space is discrete, e.g., tree structures. We discuss this in Section 5.1.\nWhen the hypothesis space has a parameterized representation, we can compute the Rashomon volume in parameter space. We assume that we can parameterize each model f \u2208 F in a hypothesis space with a unique, finite number of parameters and denote f(z) = f\u03c9(z), where \u03c9 is a parameter vector. For a parameter space \u2126, the parametric hypothesis is denoted: F\u2126 = {f\u03c9(z) : \u03c9 \u2208 \u2126 \u2286 Rp}. To be consistent with how the Rashomon set is defined on hypothesis spaces (using similarity directly between functions, rather than similarity between parameters), we will assume that there exists a parametrization such that distances according to the provided metric are similar in both the hypothesis and the parameter space. Otherwise, due to overparametrization or reparametrization, the Rashomon volume can be artificially changed (Dinh et al., 2017). This problem can be avoided by a choice of parameterization that encourages distances in parameter space to be similar to distances in hypothesis space. In Section 5.2, we show how to compute the Rashomon volume in parameter space directly in closed form for ridge and least squares regression."}, {"heading": "5.1 Sampling Methods", "text": "As before, assume that there exists a prior distribution \u03c1 over the hypothesis space F . From the definition, the Rashomon ratio can be computed as a probability of a model being in the Rashomon set:\nR\u0302ratio(F , \u03b8) = P [f \u2208 R\u0302set(F , \u03b8)] = Ef\u223c\u03c1 1[f\u2208R\u0302set(F ,\u03b8)] .\nTo approximate the Rashomon ratio, we perform rejection sampling with replacement. In particular, after k draws from distribution \u03c1,\nP\u0302 [f \u2208 R\u0302set(F , \u03b8)] = 1\nk k\u2211 i=1 1[f\u2208R\u0302set(F ,\u03b8)] .\nBy Hoeffding\u2019s inequality: P ( |P\u0302 [f \u2208 R\u0302set(F , \u03b8)]\u2212 P [f \u2208 R\u0302set(F , \u03b8)]| \u2265 t ) \u2264 2e\u22122kt2 , or\nalternatively 1\u2212\u03b1 = P ( |P\u0302 [f \u2208 R\u0302set(F , \u03b8)]\u2212 P [f \u2208 R\u0302set(F , \u03b8)]| < t ) \u2264 1\u22122e\u22122kt2 . Then, in\norder to estimate the Rashomon ratio P [f \u2208 R\u0302set(F , \u03b8)] to within t, with a (1\u2212\u03b1) confidence interval, we need to sample at least k \u2265 log 2/\u03b1\n2t2 hypotheses from F . The guarantees from\nthis rejection sampling approach are tight enough to be used in practice, and can be used in most hypothesis spaces where hypotheses can be randomly generated.\nThere are cases when the Rashomon set is very small and therefore rejection sampling contributes very little to the Rashomon ratio approximation. It makes sense to draw models from the region around the Rashomon set instead of from the set of all reasonable models. Importance sampling allows us to sample from an alternative distribution, namely the proposal distribution, that is concentrated on the importance region. However, after sampling is done, we need to adjust the weight of the sample to match the probability of sampling it\nfrom the original distribution. Let \u03c1 be a target distribution over the hypothesis space F and q be a proposal distribution that is focused around the Rashomon set. We can estimate the Rashomon ratio through importance sampling as follows:\nR\u0302ratio(F , \u03b8) = Ef\u223cq \u03c1(f)\nq(f) \u00d7 1[f\u2208R\u0302set(F ,\u03b8)].\nIn Section 7 and Appendix H we discuss how to design a target distribution for the hypothesis space of decision trees."}, {"heading": "5.2 Analytical Calculation of Rashomon Volume for Ridge Regression", "text": "A special case of when the Rashomon volume can be computed in closed form in a parameter space is ridge regression. For a space of linear models F\u2126 = {\u03c9Tx, \u03c9 \u2208 Rp}, ridge regression chooses a parameter vector by minimizing the penalized sum of squared errors for a training data set S = [X,Y ]:\nmin \u03c9 L\u0302(\u03c9) = min \u03c9 (X\u03c9 \u2212 Y )T (X\u03c9 \u2212 Y ) + C\u03c9T\u03c9, (3)\nwhere the optimal solution of the ridge regression estimator is \u03c9\u0302 = (XTX + CIp) \u22121XTY.\nGeometrically, the optimal solution to ridge regression will be a parameter vector that corresponds to the intersection of ellipsoidal isosurfaces of the sum of squares term and a hypersphere centered at the origin, with the regularization parameter C determining the trade off between the loss and the radius of the sphere. More generally, isosurfaces of the ridge regression loss function are ellipsoids, and the volume of such an ellipsoid corresponds to the Rashomon volume. Using this geometric intuition, we compute the Rashomon volume in parameter space by the following theorem:\nTheorem 16 (Rashomon volume for ridge regression) For a parametric hypothesis space of linear models F\u2126 = {f\u03c9(x) = \u03c9Tx, \u03c9 \u2208 Rp} and a data set S = X \u00d7 Y , the Rashomon set R\u0302set(F\u2126, \u03b8) of ridge regression is an ellipsoid, containing vectors \u03c9 such that:\n(\u03c9 \u2212 \u03c9\u0302)T X TX + CIp\n\u03b8 (\u03c9 \u2212 \u03c9\u0302) \u2264 1,\nand the Rashomon volume can be computed as:\nV(R\u0302set(F\u2126, \u03b8)) = J(\u03b8, p) p\u220f i=1 1\u221a \u03c32i + C , (4)\nwhere \u03c3i are singular values of matrix X, J(\u03b8, p) = \u03c0p/2\u03b8p/2 \u0393(p/2+1) and \u0393(\u00b7) is the gamma function.\nNote that for least squares regression, we can use results of Theorem 16 with penalization constant C = 0. When features in matrix X are linearly dependent, some singular values will be 0. In this case, the volume of the Rashomon set based on Equation 4 goes to infinity. We avoid this problem by making sure that the Gram matrix of the feature matrix X is always positive definite, meaning that all singular values are non-zero. One way to ensure\nthis is to perform principal component analysis and use the most significant components as replacement features in order to reduce the hypothesis space prior to learning. We follow this technique in our experiments in Section 8.3.\nInterestingly, from Theorem 16, it follows that for ridge regression, the Rashomon volume depends on the feature space only and does not depend on the regression targets Y . Indeed, assume that every parameter vector \u03c9 such that f\u03c9 \u2208 R\u0302set(F\u2126, \u03b8) can be represented as \u03c9 = \u03c9\u0302 + \u03b4. By a simple transformation we have that L\u0302(f\u03c9) \u2212 L\u0302(f\u03c9\u0302) = \u03b4TXTX\u03b4, meaning that if we take a step in parameter space, the empirical risk difference will depend only on the feature space and the step itself, and not on the targets of the problem. This observation can help us choose the parameter \u03b8 as \u03b8 = \u03b4TXTX\u03b4 if we want to ensure some dependence between the optimal model \u03c9\u0302 and a model of interest \u03c9. Then, by choosing the direction as \u03b4 = \u03c9 \u2212 \u03c9\u0302 we can compute the Rashomon parameter \u03b8.\nFor other algorithms, the Rashomon volume generally depends on the targets; in that sense, ridge regression is unusual.\nIn Appendix D.1.3 we discuss lower bounds on the Rashomon volume that follow from Theorem 16. These bounds do not depend on the singular values of the feature matrix X and might be easier to compute in practice in order to estimate the Rashomon volume faster.\nThe cases we considered in this section restrict the structure or properties of the learning problem, but these restrictions allow us to compute the Rashomon volume directly, or estimate the Rashomon ratio with high probability. In Appendix D we further discuss how to approximate the Rashomon volume to any pre-specified precision when the Rashomon set is convex. We also provide an optimization problem to under-approximate the Rashomon volume in the parameter space for support vector machine classification.\nWe will use both sampling techniques and the ridge regression closed-form solution for the experiments in later sections.\nOver the decades, the statistical learning theory community developed beautiful measures that show the expressive power and richness of hypothesis spaces, and how they relate to data and algorithms. The most popular are VC dimension, algorithmic stability, geometric margins, and Rademacher complexity. The Rashomon ratio is different from all of these well-known complexity measures: we can find cases where there is no correspondence between them. In Section 6, we illustrate that there exist data sets and distributions that illuminate differences between the Rashomon ratio and the standard complexity measures."}, {"heading": "6. Rashomon Ratio as Compared to Simplicity Measures from Learning", "text": "Theory\nThe Rashomon ratio can give insight into the simplicity of a learning problem, but it is different from well-known complexity measures from learning theory. The Rashomon ratio depends on a loss function, the hypothesis space, and a data set, while the majority of other measures are either data set agnostic or focus on properties of a specific model in the space. We will compare the Rashomon ratio to different quantities that are used for generalization in statistical learning theory, including VC dimension (Vapnik and Chervonenkis, 1971), the stability of a learning algorithm (Bousquet and Elisseeff, 2002), geometric margins (Schapire et al., 1998; Burges, 1998), Rademacher complexity and local Rademacher\ncomplexity (Bartlett et al., 2005), because the Rashomon ratio is both similar and different to each of these measures. We will use demonstrations to show the differences.\nVC dimension. Vapnik-Chervonenkis (VC) dimension (Vapnik and Chervonenkis, 1971) and the Rashomon set are completely different concepts in terms of data-dependence. The VC dimension is the cardinality of the largest set of points that the learning algorithm can shatter. The hypothesis space shatters a set of points if it can achieve any possible target labeling on this set. In other words, the VC dimension shows the expressive power of a hypothesis space for any data set including an extreme arrangement of data points and labels. On the contrary, the Rashomon set depends on an empirical risk minimizer that we compute directly for a specific data set, which may not be extreme.\nAlgorithmic stability. The main motivation for algorithmic stability theory is to ensure robustness of a learning algorithm. Following Bousquet and Elisseeff (2002), we define the hypothesis stability of a learning algorithm as follows.\nDefinition 17 (Hypothesis stability) A learning algorithm A has \u03b2 hypothesis stability with respect to the loss l if for all i \u2208 {1, ..., n},\nES,z[|l(fS , z)\u2212 l(fS\\i , z)|] \u2264 \u03b2,\nwhere \u03b2 \u2208 R+, hypothesis fS is learned by an algorithm A on a data set S, loss l(fS , z) = \u03c6(fS(x), y) for z = (x, y), data set S = {z1, ...zn}, and S\\i is modified from the training data by removing the ith element of the data set: S\\i = {z1, ..., zi\u22121, zi+1, ...zn}.\nIn Section 5.2 we showed that in the case of linear least squares regression, the Rashomon volume depends on features of X only, and does not depend on regression targets Y . In contrast, hypothesis stability depends heavily on Y . In fact, if we can control how we change the set of targets, hypothesis stability can be made to change by an arbitrarily large amount\u2014the Rashomon volume is fundamentally different from hypothesis stability. This is formalized in Theorem 18.\nTheorem 18 (Rashomon ratio and algorithmic stability) Consider a distribution PX over a discrete domain X = {x1, ...xN} and a learning algorithm A that minimizes ridge regression\u2019s empirical risk L\u0302 for a linear hypothesis space F\u2126, as in Equation (3). For any \u03bb > 0 there exist joint distributions PX,Y1 and PX,Y2 where for X drawn i.i.d. from PX , Y1 is drawn from PY1|X over Y |X and Y2 is drawn from PY2|X over Y |X , such that the expected Rashomon ratios are the same:\nEPX,Y1 [RratioS1 (F\u2126, \u03b8)] = EPX,Y2 [RratioS2 (F\u2126, \u03b8)],\nyet hypothesis stability constants are different by an arbitrarily chosen value of \u03bb:\n\u03b2\u03032 \u2212 \u03b2\u03031 \u2265 \u03bb,\nwhere S1 and S2 denote data sets S1 = [X,Y1] and S2 = [X,Y2], \u03b2\u03031 is the hypothesis stability coefficient of algorithm A for distribution PX,Y1 and \u03b2\u03032 is the hypothesis stability coefficient for distribution PX,Y2.\nGeometric margin. Intuitively both the Rashomon ratio and the width of the geometric margin are data-dependent and show how expressive the hypothesis space is with respect to a given data set. However, the margin depends on the closest data points to the decision boundary (e.g., support vectors), while the Rashomon set does not necessarily rely on the support vectors and may depend on the full data set. Theorem 19 summarizes this idea.\nBefore stating the theorem, we provide a definition of the margin. For the parametric hypothesis space of linear models F\u2126 = f(x) = \u03c9Tx, \u03c9 \u2208 Rp and binary classification, denote d+ and d\u2212 as the shortest distances from a decision boundary to the closest points with targets y = 1 and y = \u22121 respectively. Then the margin d is a sum of these distances d = d+ + d\u2212 (Burges, 1998). Moreover, for the model f\u03c9\u0302 that maximizes the margin, the margin width is 2\u2016\u03c9\u0302\u20162 .\nTheorem 19 (Rashomon ratio and geometric margin) For any fixed 0 < \u03bb < 1, there exists a fixed hypothesis space F\u2126, a Rashomon parameter \u03b8, and there exist two data sets S1 and S2 with the same empirical risk minimizer f\u0302 \u2208 F\u2126 such that the width of the geometric margin d is the same for both data sets, yet the Rashomon ratios are different:\n|RratioS1 (F\u2126, \u03b8)\u2212RratioS2 (F\u2126, \u03b8)| > \u03bb.\nEmpirical local Rademacher complexity. The empirical Rademacher complexity is another complexity measure of the hypothesis space. Following Bartlett et al. (2005), for binary classification we define it as follows.\nDefinition 20 (Empirical Rademacher complexity) Given a data set S, and a hypothesis space F of real-valued functions, the empirical Rademacher complexity of F is defined as:\nR\u0302Sn(F) = 1\nn E\u03c3 [ sup f\u2208F n\u2211 i=1 \u03c3if(zi) ] ,\nwhere \u03c31, \u03c32, . . . , \u03c3n are independent random variables drawn from the Rademacher distribution i.e. P (\u03c3i = +1) = P (\u03c3i = \u22121) = 1/2 for i = 1, 2, . . . , n.\nSince we are interested only in models that are inside the Rashomon set, in this section we will consider local empirical Rademacher complexity (Bartlett et al., 2005), which is defined using the Rashomon set R\u0302set(F , \u03b8). Empirical Rademacher complexity measures how well the hypothesis space can fit to random assignments of the labels. In contrast, the Rashomon volume is different, as it measures the number of models that are close to optimal. In other words, the Rashomon set benefits from having multiple similar models, while Rademacher complexity treats them as equivalent. In the following theorem, we provide a simple example to show this discrepancy between the two measures.\nTheorem 21 (Rashomon ratio and local Rademacher complexity) For 0 < \u03bb < 1, there exist two data sets S1 and S2, a hypothesis space F\u2126, and a Rashomon parameter \u03b8 such that the local Rademacher complexities defined on the Rashomon sets for S1 and S2 are the same:\nR\u0302S1n ( R\u0302set(F\u2126, \u03b8) ) = R\u0302S2n ( R\u0302set(F\u2126, \u03b8) ) ,\nyet the Rashomon ratios are different:\u2223\u2223\u2223RratioS1 (F\u2126, \u03b8)\u2212RratioS2 (F\u2126, \u03b8)\u2223\u2223\u2223 > \u03bb. Pattern Rashomon ratio. The pattern Rashomon ratio defined in (3) is different from both the Rademacher complexity and geometric margins. Intuitively the pattern Rashomon ratio is closer to the Rademacher complexity, as it tries to find the number of models that fit the best under different label permutations; in contrast, the standard multiplicity-based Rashomon ratio is closer to geometric margins (the multiplicity-based Rashomon ratio tends to be larger when the classification margins are larger).\nAdditionally, there is a straightforward connection between the growth function and the pattern Rashomon ratio. Recall that the growth function, or shattering coefficient, is the maximum number of ways any n data points can be classified using functions from the hypothesis space. The connection is that the volume of the hypothesis space measured using pattern distance is exactly the growth function defined on the current data set. More specifically, the pattern Rashomon ratio and the growth function are equivalent under very specific conditions: (i) the Rashomon set is the full hypothesis space (this is unlikely in practice), (ii) we consider classification with 0-1 loss as the performance measure \u03b6, and (iii) we consider only one data set and do not take supremum over all data sets (as is usual for the growth function).\nNow that we have established what we expect to see theoretically from the Rashomon ratio, we move to experiments."}, {"heading": "7. Larger Rashomon Ratios Correlate with Similar Performance of", "text": "Machine Learning Algorithms, and Good Generalization\nIn this section, we present several observed properties of larger Rashomon ratios.\nWe would like to measure the Rashomon ratio of a hypothesis space that includes a broad variety of functions, including some that are capable of fitting the data approximately as well as boosted decision trees, support vector machines with gaussian kernels, or other complex machine learning algorithms. Since it is not clear how to perform direct sampling of this class in functional space, we chose to approximate this space by a function class that (1) we could sample from, and (2) would potentially be a good approximating set for the desired hypothesis space. The class we chose was decision trees of depth 7, which is flexible and large, yet easy to sample from. Since decision trees can refine an input space arbitrarily finely as their depth increases, we can view sufficiently deep decision trees as a rich hypothesis space that approximates many other types of hypothesis spaces, including those used by other machine learning methods. Thus, it is conceivable that large Rashomon sets for decision trees translate into large Rashomon sets for the hypothesis spaces we would like to consider in functional space.\nTo estimate the Rashomon ratio of depth 7 decision trees, we used importance sampling, as discussed in Section 5.1. The proposal distribution assigns the correct labels to the leaves of the tree based on the training data. Since the data are populated on a bounded domain, to grow a tree up to a depth D fully, we make 2D\u22121 splits. For each data set and each\ndepth we average our results over ten folds for datasets with less than 200 points and over five folds for datasets with more than 200 points, and we sample 250,000 decision trees per fold. We choose the Rashomon parameter \u03b8 to be 5%, and, therefore, all the models in the Rashomon set have empirical risk not more than L\u0302(f\u0302) + 0.05, where L\u0302(f\u0302) is the lowest achievable empirical risk across all algorithms we considered. (We vary the \u03b8 value later; the results did not seem to be sensitive to that choice.)\nSeparately, we assess whether many different machine algorithms perform similarly on the data set. If many different algorithms with different functional forms and different levels of smoothness perform similarly on a data set, the Rashomon set contains all of these diverse functions. In that case, we conjecture that the Rashomon set could be large. In this first experiment, we test this conjecture, by investigating whether large Rashomon sets (as measured with decision trees of depth 7) correlate with many machine learning methods performing similarly on the same data set. Here, large Rashomon ratios are on the order of 10\u221237% or 10\u221238%, whereas small Rashomon ratios are 10\u221240% or less. We further discuss experimental setup in Appendix H.\nOur experiments considered five popular machine learning algorithms: logistic regression, CART, random forests, gradient boosted trees, and support vector machines with RBF kernels. CART, random forests and gradient boosted trees were regularized by varying the tree depth, the minimum number of samples required to split a node, the minimum number of samples required to create a leaf node, and the number of estimators. Support vector machines were tuned by varying the regularization parameter and the kernel coefficient. We used 38 machine learning classification data sets from the UCI Machine Learning Repository (Dua and Graff, 2019), among which 16 have categorical features and 22 have real-valued features. The majority of the data sets are binary classification data sets and we adapted the rest of the data sets to binary classification as well. The number of features varies from 3 to 784, with the majority of the data sets being in the 15\u201425 feature range. Appendix F contains a description of the data sets we considered.\nTo recap, we used decision trees of depth 7 to estimate the Rashomon ratio directly, and separately use a variety of machine learning methods on the same data to assess whether large Rashomon ratios correlate with similar training performance across algorithms, as well as good generalization between training and test sets."}, {"heading": "7.1 Similar Performance Across Algorithms", "text": "Figure 5(a) illustrates the performance of the five machine learning algorithms with the largest Rashomon ratios in the space of decision trees of depth 7. Across all of these cases, larger Rashomon ratios led to approximately similar training results (within \u223c 5% difference between algorithms). Moreover, all of the models chosen by the algorithms generalized well and produce very similar test accuracy (within \u223c 5% difference between training and generalization errors).\nInterestingly, the converse statement, that similar performance across different algorithms should lead to large Rashomon sets, does not always hold; sometimes, generalization occurs with small Rashomon ratios. This observation could be explained in several different ways. First, the Rashomon ratio is not the only driver of good generalization performance. Second, even when the Rashomon ratio is a good driver of generalization performance,\nit may appear artificially small because of a poor representation of data or poor choice of the ratio\u2019s denominator. For instance, if the features are highly correlated, this artificially deflates the size of the Rashomon ratio, as discussed in Appendix G. Moreover, if the denominator of the Rashomon ratio (which is the size of the hypothesis space) is poorly designed to include an overly large number of models, then the Rashomon ratio may appear artificially small.\nThe issues with measuring the Rashomon ratio may be a possible explanation for some of the results in Figure 5(b), which includes some data sets with high-performing algorithms, yet (by the way we measured it) a small Rashomon ratio."}, {"heading": "7.2 Good Generalization", "text": "In all cases we observed, if training performance was consistent across algorithms, test performance was also similar. One thing we notably did not observe were cases where algorithms did not generalize, performance differed across algorithms, and the Rashomon set was large. Actually, we did not observe cases where the Rashomon set was large and performance differed among algorithms. All of these observations are consistent with (but do not definitively prove) the theory that consistent performance across algorithms occur because of large Rashomon sets, which in turn leads to generalization.\nFigure 5(c) shows small Rashomon sets and wildly different performance across algorithms, and in that case, sometimes the models generalize and sometimes they do not. We\nshow one example of each of these cases in Figure 5(c). Our theory does not apply to the case of small Rashomon sets, but again the appearance of small Rashomon sets could be due to poor ways of measuring the size of the Rashomon sets in our experiments."}, {"heading": "7.3 Regularization Changes the Hypothesis Space and the Rashomon Set", "text": "Regularization limits the hypothesis space and thus changes the nature of the Rashomon set\u2019s measurements. Each value of the regularization parameter corresponds to a soft constraint on the hypothesis space, which in turn can be realized as a hard constraint on this space. The Rashomon ratio in the regularized case will typically be larger or equal to the Rashomon ratio in the unregularized case. There are two reasons for this, explained below.\nFirst, regularization reduces the hypothesis space. Hypotheses that were available when learning without regularization may be excluded when learning with regularization. As a result, the volume of the hypothesis space reduces, which decreases the denominator of the Rashomon ratio.\nSecond, the empirical risk minimizer changes between the regularized and unregularized hypothesis sets, which means the criterion for falling into the Rashomon set changes as well. Recall that the Rashomon set is defined based on the best performing model on the training set. The regularized hypothesis space is less likely to contain overfitted models than the unregularized space. This means the regularized hypothesis space\u2019s empirical risk minimizer typically has higher empirical risk than that of the unregularized hypothesis space. Then, if the Rashomon parameter \u03b8 is fixed when comparing the two hypothesis spaces, there may be more models in the Rashomon set for the regularized case. Thus, in the regularized case, the Rashomon volume would be larger, and, therefore, the Rashomon ratio would be larger too.\nIn Appendix H, we show a performance comparison of different machine learning algorithms with and without generalization for all datasets. In both cases, the experiments within Sections 7.1 and 7.2 lead to the same conclusions."}, {"heading": "7.4 Diversity Across Algorithms", "text": "In Section 7.1, our experimental results suggested that similar performance across machine learning algorithms correlates with larger Rashomon sets. This result implicitly relies on the ability of these algorithms to produce diverse models: it could happen that all of the algorithms produce exactly the same model, in which case our Rashomon set could be small, even if all algorithms perform the same. So how do we know whether our algorithms actually produce diverse results?2 That is, what assumptions and measurements would we make in order to determine that the Rashomon set is indeed large?\nWe consider two different sets of assumptions on the structure of the hypothesis space that allow us to show the connection between diversity of models within the Rashomon set and the size of the Rashomon set. In the first case, we estimate the lower bound on the number of diverse models in the Rashomon set for a hypothesis space that itself contains a hierarchy of hypothesis spaces. To do so, we introduce a growth assumption that allows us to navigate through the hierarchy. In the second case, we assume that the hypothesis\n2. Thank you to Vipin Kumar for asking this question.\nspace is diverse enough to contain subspaces of models of different complexity. In that case, the machine learning algorithms that search these subspaces should produce hypotheses of different complexity as well. If these hypotheses realize different patterns in the Rashomon set, then the Rashomon set is diverse. We discuss each of these cases in more detail below.\nHierarchy-based diversity. One general idea for guaranteeing a large Rashomon set requires an assumption and a particular observation. Specifically, we would observe that at least two of the algorithms produce models of very different levels of complexity (one simple, one complicated), and that these two models are both in the empirical Rashomon set. We would assume that for each simpler class there exist at least C times as many distinct models in a slightly more complicated model class. This creates a Rashomon set that grows at least exponentially in the number of complexity levels between the two observed models. Thus, since the two models we observe are both in the Rashomon set, then there must exist a large number of models in the Rashomon set with complexity in between them. We will formalize the simple conditions described above that allow us to conclude from these models that a large Rashomon set could exist.\nConsider a hierarchy of discrete hypothesis spaces F1 \u2282 F2 \u2282 \u00b7 \u00b7 \u00b7 \u2282 FT . Let us make an assumption on models in the Rashomon set of a simple hypothesis space F t: we assume there exists at least C times more distinct models from a more complicated hypothesis space F t+1, where these models are also in the Rashomon set. We can propagate the growth condition across the hierarchy and compute a lower bound on the Rashomon volume. This intuition is summarized in Proposition 23.\nAssumption 22 (Growth assumption) Consider a hierarchy of discrete hypothesis spaces F1 \u2282 F2 \u2282 \u00b7 \u00b7 \u00b7 \u2282 FT and a given B1 = {f1} \u2282 F1. Assume that for t \u2208 [2, . . . T ], there exist subsets Bt \u2282 F t, Bt\u22121 \u2229Bt = \u2205 such that |Bt||Bt\u22121| > C > 1 and L\u0302(ft\u22121) \u2265 L\u0302(ft), where | \u00b7 | denotes set cardinality.\nThis assumption leads directly to the following proposition.\nProposition 23 For a hierarchy of discrete hypothesis spaces F1 \u2282 F2 \u2282 \u00b7 \u00b7 \u00b7 \u2282 FT such that there exists f1 \u2208 R\u0302set(FT , \u03b8)\u2229F1 6= \u2205, if Assumption 22 holds for B1 = {f1} then the Rashomon set R\u0302set(FT , \u03b8) contains at least C T\u22121 C\u22121 models.\nFor Proposition 23 to ensure the existence of a large Rashomon set, we would be required to observe the existence of only two models from the hierarchy. One model is an empirical risk minimizer from the most complicated class f\u2217 \u2208 FT that defines the Rashomon set. Another model is from the simplest class f1 \u2208 F1 and serves as a base model for the growth assumption.\nWhen considering how realistic Assumption 22 is, we might consider the possibility that it approximately holds: perhaps for each class there are actually sometimes more or sometimes less than C functions from the next complexity class. The complexity classes can be defined any reasonable way, including decision trees, where t is the number of leaves, or one could use linear models with t nonzero terms. Similarly, one could use boosted stumps with t being the total number of stumps. Adding a stump could reasonably provide a new, distinct, model that lowers the loss, as in the assumption. In that way, one could reasonably\nexpect that if, within the Rashomon set, there exist both simple and complex models, that the Rashomon set is large.\nAssumption 22 is complicated to observe, because we would need to lower-bound the count of models in each complexity class. The pattern Rashomon ratio could help with this. While it is not easy to determine whether two functions are the same, it is easy to determine whether they predict the same way on all of the data points. That leads us back to the pattern Rashomon ratio. If we observe a new pattern when we arrive at a complex hypothesis space, the function creating that pattern does not belong to the simpler hypothesis space (we would have seen it when examining that simpler space). Thus, if for every Bt \u2282 F t there exist at least C unique patterns in F t+1 such that L\u0302(ft) \u2265 L\u0302(ft+1), then Assumption 22 holds and the Rashomon set will contain at least C\nT\u22121 C\u22121 models with\nunique patterns.\nLet us consider a more concrete specification of Assumption 22, where we are given the hierarchy of hypothesis spaces as fully-grown decision trees, with respect to depth. In this case, to propagate through the hierarchy, we assume that after each split there will be some correctable impurity remaining in each leaf. In other words, we assume each tree in the Rashomon set of the simpler class will be grown into at least C more distinct trees that are within the next more complicated class. The total number of trees in the Rashomon set for each depth t is at least Ct, which leads to exponential growth. Such a growth assumption for decision trees is a more specific form of Assumption 22, where for each model in a simpler class there exists at least C distinct models in a slightly more complicated model class. An illustration of this is in Figure 6.\nComplexity-based diversity. For more general cases (no hypothesis space hierarchy and exponential growth assumption) we can use simple empirical observations. Consider an embedded space Femb = {F1,F2, . . .FH} that consists of all the hypothesis spaces on which we run different algorithms. Let Halg \u2282 Femb be a set of models that different machine learning algorithms output. If we choose hypothesis spaces F1,F2, . . .FH so that they have different complexity (e.g, VC dimension or Rademacher complexity) then similar performance of models from Halg might indicate a larger Rashomon set that includes\nfunctions from all of these complexity classes, as illustrated in Figure 7(a). Indeed, the random forest models are different from linear models and different from models produced by support vector machines with RBF kernels.\nAgain, one problem with the above argument is that multiple algorithms might compute the same model. In this case, the Rashomon set might be small. We can use a simple empirical measure based on the average Hamming distance to check how diverse the models are within Halg. The Hamming distance between two vectors of targets is the number of predictions that are different for the two models. Therefore, computing the Hamming distance between pairs of Halg models\u2019 predictions and averaging them is an indicator of the diversity of models in Halg.\nLet us consider a different special-case assumption than Assumption 22. In particular, if there is some notion of smoothness for functions in the hypothesis space, and if we have found several diverse functions within the Rashomon set, then there are probably more functions in the Rashomon set near them that we did not find. Beyond smoothness, let us additionally assume that the diverse models lie in a single connected component of the Rashomon set. In that case, the existence of diverse models in one connected component of the Rashomon set suggests that the Rashomon set is likely to contain even more models.\nFor example, let us assume R\u0302set(Femb, \u03b8) is convex. Then, it contains at least the convex hull of models from Halg. (That is, the convex hull of all models found by the different algorithms, all which are within the Rashomon set.) An illustration of this is shown in Figure 7(b). The larger the average Hamming distance is, the more scattered the models from Halg are within the connected component of R\u0302set(Femb, \u03b8), and therefore the larger the Rashomon set might be.\nTo define smoothness between functions of different complexity, let us assume that hypothesis space FH \u2208 Femb can well-approximate models from other hypothesis spaces. For example, support vector machines with small RBF kernels can approximate other models we consider such as decision forests or gradient boosting trees. Now that all models have a representative in the hypothesis space FH , we can work only in FH , defining smoothness in the parameter space of FH .\nTo summarize, in this subsection we discussed two cases when models computed by different algorithms are diverse enough to infer the large Rashomon set. The first case is for a hierarchy of hypothesis spaces, that, given models from the simplest and most complicated spaces, requires the growth assumption, leading to exponential growth of the Rashomon set. The second case suggests considering machine learning algorithms that each search hypothesis spaces of different complexity. Empirically, we can compute the average Hamming distance of the computed models to check if they are diverse. If so, we may have a large Rashomon set.\nThere are several conclusions we can make from our experiments. The most important conclusion is that when the Rashomon ratio is observed to be large, all algorithms perform similarly, and their models tend to generalize. Given these conclusions, one may ask whether it is desirable to aim for the largest possible Rashomon ratio in all cases. In the next section, we introduce Rashomon curves and address this question empirically."}, {"heading": "8. Rashomon Curves", "text": "In this section we introduce a trend, namely the Rashomon curve, that we observe experimentally across all classification data sets we downloaded from the UCI Machine Learning Repository (Dua and Graff, 2019).\nConsider a hierarchy of hypothesis spaces H0 \u2282 H1 \u2282 \u00b7 \u00b7 \u00b7 \u2282 HT , where each Ht = {h : X \u2192 Y}, and X = [0, 1]p is a unit hypercube. We consider the empirical risk over the loss function as follows L\u0302(H) = minh\u2208H 1 n \u2211n i=1 \u03c6(h(xi), yi).\nThe Rashomon curve is a function from the empirical risk to the log of the Rashomon ratio for a hierarchy of hypothesis spaces. More formally, for a hierarchy of hypothesis spaces H0, . . . HT , the Rashomon curve is obtained by connecting the following points:( L\u0302(H0), R\u0302ratio(H0, \u03b80) ) , . . . , ( L\u0302(Ht), R\u0302ratio(Ht, \u03b8t) ) , . . . , ( L\u0302(HT ), R\u0302ratio(HT , \u03b8T ) ) , where \u03b8t = \u03b8 is a fixed Rashomon parameter, t \u2208 [0, T ]."}, {"heading": "8.1 Rashomon Curves Tend to Exist Often", "text": "For a hierarchy of hypothesis spaces, the Rashomon curve shows that as the size of the hypothesis space grows, the empirical risk of classifiers within the space first decreases and then the Rashomon ratio decreases. As a result, the Rashomon curve has a \u0393-shaped trend as illustrated in Figure 8(a).\nThe horizontal part of the \u0393-shape corresponds to a decrease in the empirical risk as we move through the hierarchy of hypothesis spaces. If the hypothesis space with the largest size we consider does not achieve a low value of the empirical risk (e.g., in a case of a complex learning problem) we will observe only the horizontal part of the Rashomon curve. This pattern indicates that none of the hypothesis spaces considered are complex enough to learn the training data well.\nThe vertical part of the Rashomon curve corresponds to changes in the Rashomon ratio. When a learning problem is easy (e.g., separable data sets, data sets with large margin, data sets with only one or two relevant features), a high accuracy on the training data is easily achievable with a smaller hypothesis space. In that case, we will observe only the\nvertical part of the Rashomon curve as illustrated in Figure 8(c). The vertical part of the curve corresponds to more complex hypothesis spaces, which is where overfitting can occur. However, the steep drop in Rashomon ratio tends to overwhelm the increases in training accuracy that correspond to overfitting, which is why the curve appears vertical, rather than diagonally slanted.\nAs before, for our experiments, we used 38 UCI data sets from the UCI Machine Learning Repository. For the hierarchy of hypothesis spaces, we chose fully-grown decision trees up to depth D, where D \u2208 [1, 7]. we denote each space by DT-D. We computed the Rashomon ratios by importance sampling of the decision trees for each depth D. We choose the Rashomon parameter \u03b8 to be 5%, and all the models in the Rashomon sets have empirical risk not more than L\u0302(f\u0302) + 0.05, where L\u0302(f\u0302) is the lowest achievable empirical risk of the hypothesis space at the given level in our hierarchy.\nFigure 8 (b),(d) show the Rashomon curves for some categorical and real-valued data sets. All of the Rashomon curves, for all 38 data sets as described in Appendix J in Figures 24\u201328, follow the same trends illustrated in Figure 8 (a),(c). Most of the curves we have observed have a full \u0393-shape pattern, while some (e.g., MNIST-0-1, Credit Card) follow only the vertical trend of the Rashomon curve, as in Figure 8 (c). As discussed earlier, this trend indicates a form of simplicity of the data set, and, indeed, Nursery-1 is separable, and others can even be separated with a single decision stump.\nAs we change the value of the Rashomon parameter \u03b8, the general shape of the Rashomon curves is preserved across all data sets. Figure 9 shows the Rashomon curves for some of the data sets with various values of the Rashomon parameter \u03b8. As we decrease the value of \u03b8, the Rashomon ratio decreases.\nAlong a Rashomon curve, there is a point that balances between simplicity and empirical error, as illustrated in Figure 10(a). We call this point the Rashomon elbow and define it as follows:\nDefinition 24 (Rashomon elbow) For a hierarchy of hypothesis spaces H0 \u2282 H1 \u2282 \u00b7 \u00b7 \u00b7 \u2282 HT the Rashomon elbow is a hypothesis space He that both minimizes the empirical risk and maximizes the Rashomon ratio:\nHe \u2208 argmax {Ht:L\u0302(h|[h\u2208Ht])\u2248L\u0302(h|[h\u2208HT ])} R\u0302ratio(Ht, \u03b8). (5)\nModels on the elbow should theoretically be both accurate and simple, and therefore generalize well. As we will discuss later, the elbow model is a good choice for model selection. The Rashomon elbow model can differ from a model selected using typical bias-variance trade-offs. To find the Rashomon elbow model, we use only training data, whereas model selection using a bias-variance trade-off requires test (or validation) data.\nNote that when comparing Rashomon sets across a hierarchy, the Rashomon ratios decrease, but the sizes of the Rashomon sets increase, because these Rashomon sets are contained within each other. The Rashomon set for decision trees of depth five is contained within the Rashomon set of depth six, even though the Rashomon ratio at size five is smaller than that of size six; the denominator of the ratio increases faster than the numerator across the hierarchy.\nTo summarize, the Rashomon curve seems to be a fairly universal trend across data sets, which is that as the size of the hypothesis space increases, empirical risk is decreased until the elbow is reached, after which point, the risk stays approximately constant and the Rashomon ratio rapidly decreases. We will study the curve, its elbow, and their implication to generalization in the next section."}, {"heading": "8.2 Rashomon Elbow and Empirical Generalization Properties", "text": "The Rashomon elbow, as illustrated in Figure 10(a), is a balancing point between low-error empirical performance and large Rashomon ratio. Let us now consider the generalization error for each of the hypothesis spaces that are represented as arrows on Figure 10(a). Notice\nthat the generalization error at the elbow is the lowest among all larger-sized hypothesis spaces achieving high accuracy.\nFigure 10(a) is an idealized curve that abstractly represents a trend that we observed largely, but not universally, in the data: see the Rashomon elbow generalization error for the UCI-data sets we considered in Figure 10(b-d). From the figures we can divide the data sets into three categories. For the first category (Figure 10(b)) the Rashomon elbow generalizes similarly to Figure 10(a). We can see, for example, on the Column 2C data set, that all models starting from DT-4 overfit. The second category (Figure 10(c)) shows approximately the same generalization error across all complexities of the hypothesis spaces, meaning that the elbow model is still a good choice because it yields simpler models and generalizes as well as the most complex models. The third category (Figure 10(d)) shows large generalization errors across all of the hypothesis spaces; again the elbow model is not worse than all other models considered. Thus, we seem to find that the elbow model selection criterion either helps, or has no effect, but never achieves worse performance than other possible choices.\nThe Rashomon elbow determines a useful trade-off between generalization and estimation error. Figure 10 illustrates why the Rashomon elbow model might be a good choice for model selection\u2014it often achieves the lowest test error, as discussed in Section 4. The elbow model\u2019s class is the smallest (simplest) among hypothesis spaces that achieves low training empirical risk. As we showed empirically in Figure 9, the location of the Rashomon elbow is not particularly sensitive to the choice of \u03b8. For a hierarchy of fully-grown decision trees DT-D, D \u2208 [1, 7], Figure 8 shows the Rashomon curves. The Rashomon elbow model tends to have both high test accuracy and high Rashomon ratio, e.g., DT-4 for Monks-1, DT-3 for Skin Segmentation, DT-1 for Nursery-2 data.\nPossible ways to formulate the optimization problem to find the elbow are in Appendix K.\nIn cases when the Rashomon ratio is complicated to compute, we can approximately find the elbow model based on changes in the empirical risk as we vary the complexity of the hypothesis space. In particular, consider a hierarchy of hypothesis spaces H0 \u2282 H1 \u2282 \u00b7 \u00b7 \u00b7 \u2282 HT and corresponding empirical risks L\u0302(H0), L\u0302(H1), . . . , L\u0302(HT ) for the best models in these classes. Starting with the most complicated hypothesis space HT and decreasing the size of the hypothesis space Ht, we stop when there is a jump in the empirical risk H\u0302t. The hypothesis space before this significant increase in the empirical risk, which we denote as He\u0304, has the smallest size among HT , . . . He\u0304 and thus is near the top of the vertical trend of the Rashomon curve. Moreover, He\u0304 has low empirical risk and, therefore, is approximately at the Rashomon elbow."}, {"heading": "8.3 Rashomon Curves for Ridge Regression", "text": "The Rashomon curve trend holds beyond classification problems, and here we show that it holds for ridge regression as well. In particular, Figure 11 shows the Rashomon curves for a hierarchy of polynomial hypothesis spaces, where the Rashomon volume was plotted against the empirical risk. For the Rashomon volume computation, we used results of Theorem 16, which allows an analytical computation of the volume. The formula depends on the feature matrix, the Rashomon parameter and the number of features in the data\nset. For our experiments, we choose fourteen real-valued regression data sets from the UCI repository and we choose the three first principal components to form new features that are not redundant with other features. Then, to create a hierarchy of hypothesis spaces, we applied a polynomial transformation to these three features for polynomials of degree 1 through degree 8. As in the case of classification, we see that the Rashomon curve pattern holds, the elbow model exists, and it produces competitive generalization error compared with higher-dimensional spaces.\nAlthough it was possible to compute Rashomon ratios analytically for ridge regression and to estimate them through sampling for decision trees, exhaustive computation of an entire Rashomon curve may not be a practical model selection technique in most cases. We discuss more practical implications in the next section."}, {"heading": "9. Practical Implications of the Rashomon Sets and Ratios", "text": "We begin by recalling the main conclusions from this paper that would be most impactful for practitioners:\n\u2022 Large Rashomon sets can embed models from simpler hypothesis spaces (Section 4).\n\u2022 Similar performance across different machine learning algorithms may correlate with large Rashomon sets (Section 7).\n\u2022 Large Rashomon sets correlate with existence of models that have good generalization performance (Section 7, 8).\n\u2022 The size of the Rashomon set is a measure of model class complexity that trades off with training loss to form Rashomon curves (Section 8).\nHow can a machine learning practitioner benefit from these insights? Let us consider a researcher conducting a standard set of machine learning experiments in which the performance of several different algorithms are compared, and generalization is assessed.\nAlthough it may may not be desirable to compute an entire Rashomon curve explicitly, some commonly occurring scenarios can give an insight into where we are on the Rashomon curve. Consider the possible scenario where all algorithms perform similarly, and when their models tend to generalize well. Since we can assess generalization ability on validation data, we can determine directly whether models generalize. As we discussed in Section 7.4, we can also determine whether all the models form a diverse set by considering the model forms that each algorithm produces. For instance, a random forest model has a different form than a single decision tree: they are both piecewise-constant, but forests have many more pieces. Forests and trees have a different form than a support vector machine model with Gaussian kernels, which is smooth. In the case we are discussing, where the models from the various algorithms are different, yet perform equally well, what we have found is that there are a large number of different well-performing models. These functions can thus constitute different members of a large Rashomon set in an embedding space F2 \u2283 Fsvm,Fboosting,Fforest, etc. of reasonable models. Here, F2 has limited complexity, which permits generalization of the various members of the Rashomon set, as well as other models within F2.\nIf, indeed, F2 exists and has the properties we claim (limited complexity class, large Rashomon set, models achieving highest test performance achievable on that data set), then several doors open. At that point, the researcher could:\nDelve in: find specialized models with specific properties, such as interpretability. If the researcher is interested in interpretable models, they can search the large Rashomon set of F2 to locate simpler models within that set. Based on the result in Section 4, such simpler models are likely to exist in a large Rashomon set of not-too-complex models.\nLook up: improve generalization without losing training accuracy. Since all algorithms perform similarly, the algorithms could be producing models along the vertical part of a Rashomon curve of hypothesis spaces. In that case, it is worth looking higher up the Rashomon curve for simpler models that maintain the same training accuracy. This moves the researcher upwards along the curve towards the Rashomon elbow.\nIn the converse case to the one considered above, the researcher\u2019s algorithms perform differently from each other. Based on our experiments in Section 7, our theory bestows none of the advantages listed above in this setting. There are two possible reasons for this. First, the complexity of the hypothesis spaces considered by the researcher may not be adequate for the task. The researcher thus could:\nBroaden the horizon: use a more complex model class. If all the algorithms perform differently, the researcher could be choosing models along the horizontal trend of the Rashomon curve. In that case, the simpler models are losing accuracy over the more complex models. This suggests that there still is room to move towards the elbow, by selecting a more com-\nplex model class that achieves better performance yet does not overfit.\nA second reason for non-uniformity in performance could be that the task might benefit from specialized hypothesis spaces provided by some machine learning algorithms. For example, convolutional neural networks are particularly well-suited to certain types of vision tasks, where they outperform many general-purpose machine learning algorithms. We would not expect uniformity in performance across different algorithms for such tasks.\nIn the cases considered above, we have shown how an understanding of the Rashomon curve can influence decisions in most cases where a researcher is exploring a data set and iteratively selecting algorithms or hypothesis spaces. As with other fundamental concepts in machine learning, such as the bias-variance tradeoff, an understanding of Rashomon ratios and curves can inform practice even if such quantities are not computed explicitly but are inferred indirectly, such as through experimentation with a variety of algorithms, as we have suggested."}, {"heading": "A perspective on modern machine learning applications and Rashomon curves:", "text": "Recall that the Rashomon set is defined by the interaction between the hypothesis space and the training data. Since algorithms and their performance relative to benchmark data sets change over time as researchers compete on these benchmarks, the placement of problems on the Rashomon curve should not be viewed as something that is static, but that gives insight into the state of the art at a particular point in time. Perhaps the differing perspectives that researchers have on simplicity and generalization are based on what portion of the Rashomon curve their algorithms are exploring.\nAt one time, the MNIST data set was considered a challenging benchmark problem, though accuracies from many modern, general purpose machine learning algorithms are all close to 100%. This suggests that the field is on the vertical part of the Rashomon curve, with no advantage left to be gained from more complex or specialized algorithms. In this case, searching within the Rashomon set for models with other desirable properties, such as simplicity or computational efficiency may be desirable. On the other hand, the newer ImageNet challenge data set has seen increasing performance with increasingly complex model classes in recent years. Perhaps this suggests movement along the horizontal portion of the Rashomon curve towards the elbow, where further gains may yet be obtained.\nThe above vision examples with relatively high accuracy are in contrast with criminal recidivism prediction, where many different machine learning algorithms have essentially identical performance on many different recidivism prediction problems (Zeng et al., 2017; Tollenaar and van der Heijden, 2013; Angelino et al., 2018), where performance is measured across the full ROC curve. Recidivism prediction exemplifies the case where our theories become relevant: many different algorithms perform similarly, and very sparse interpretable models are as accurate as more complicated models. Interestingly, complicated black box models are still used in the justice system (Angwin et al., 2016), despite the fact that there is little evidence to support the need for this level of complexity (see, e.g., Rudin et al., 2019).\nThere is evidence that some credit risk assessment problems, and some medical problems, such as stroke risk in atrial fibrillation patients (see, e.g., Letham et al., 2015), diabetes prediction (Razavian et al., 2015), and pneumonia risk and readmission prediction (Caruana\net al., 2015) are in a similar state. For these problems, fully interpretable models have been derived that are approximately as accurate as the most accurate (potentially most complicated) machine learning models; perhaps these problems are on the vertical part of the Rashomon curve. Credit risk scoring leads to particularly interesting questions of accuracy-complexity trade-offs: there are financial incentives to keep credit risk models proprietary, which incentivizes the use of potentially overly complex models. Yet, a recent credit risk data set released by the Fair Isaac Corporation (FICO) for the purpose of a data mining competition yielded fully-interpretable models whose accuracy was on par with neural networks and other more complex model classes (Chen et al., 2018).\nFigure 12 illustrates the perspective illustrated in this section: if we view current performance as a possible point on a Rashomon curve, it can be useful in determining how to proceed forward with analysis, including whether to delve in, look up, or broaden the horizon."}, {"heading": "Conclusion and Future Work", "text": "This work studies the Rashomon set, which is the set of almost-equally-accurate models. It also studies the Rashomon ratio, which is the fraction of models that are in the Rashomon set. A large Rashomon set serves as certificate of existence for simpler-yet-accurate models, for a given data set and hypothesis space. Although similar to complexity or simplicity\nconcepts in machine learning, these Rashomon sets and ratios have key differences that lead to new insights. In cases where many different algorithms have similar and good performance, we hypothesize that a large Rashomon ratio may be the cause.\nWe also introduce the Rashomon curve, which is a function of the empirical risk versus the Rashomon ratio. The Rashomon curve follows a characteristic \u0393-shape, and has occurred across all 52 data sets that we considered. The Rashomon curve reveals a behavior of a hierarchy of hypothesis spaces: The accuracy first increases, then stabilizes, after which the Rashomon ratio decreases. The Rashomon curve\u2019s elbow model serves as a useful trade-off between performance and simplicity, and empirically tends to generalize well.\nIn some cases, it may be practical to compute Rashomon ratios analytically or through sampling methods, but it is not essential to compute these quantities to benefit from the insight they provide. Clues, such as the similar performance of many different algorithms, can give insight into where a practitioner is on the Rashomon curve, and these insights can inform subsequent actions such as enriching the hypothesis space, or searching for more convenient models within the space(s) already considered.\nThere is also room for further work on techniques for estimating the size of the Rashomon set or ratio, either by sampling or in closed form. We provided a closed form solution for ridge regression only, but closed form solutions may exist for other hypothesis spaces. Better methods of computing or estimating these quantities may facilitate through empirical studies for additional machine learning models, which could bolster the empirical observations made in this paper. There are some challenges in calculating the Rashomon ratio discussed in Section 7, specifically that the Rashomon ratio may appear artificially smaller than it actually is, because of poor parameterization or overparameterization. As we attempt to calculate Rashomon ratios for larger functional spaces, we should remember that the Rashomon ratio should be measured in functional space or pattern space\u2014Rashomon ratios computed in parameter space may not always serve as a good substitute. If we choose not to estimate the Rashomon ratio or size of the Rashomon set directly, but instead choose to look at differing performance among algorithms, we have discussed in Section 7.4 that algorithms should search different complexity hypothesis spaces, and large Hamming distance between patterns can indicate the presence of large Rashomon ratios. Determining measures that indicate a large Rashomon set would be a possible direction for future research.\nGiven that large Rashomon sets have these interesting properties, it would be worth exploring methods that explicitly try to (re)shape the problem to induce large Rashomon sets. Although we are not aware of any work that has directly done this, there are some existing approaches that may be re-interpreted in this way. For example, one practical technique for producing more robust classifiers is to add noise or smoothing to the training data, e.g., applying a slight blur filter to image data before training. This can be seen as flattening the optimization landscape and potentially increasing the size of the Rashomon set. It is also possible that techniques which inject noise directly into parameter space (Hochreiter and Schmidhuber, 1997) could be interpreted as as having a similar effect. The fact that injecting noise into the data set and/or optimization potentially leads to larger Rashomon sets is a possible connection to differential privacy and other types of privacy-preserving computation. One challenge is to determine whether techniques that inject noise actually do increase the Rashomon ratio. Another future direction is to revisit older techniques like that of Hochreiter and Schmidhuber (1997), which inject noise during\noptimization; theoretically, if they widen the Rashomon set, they may improve performance in practice.\nThe theoretical results presented herein are fairly basic and often rely upon quantities that are sometimes difficult to measure in practice. There is room for further theoretical development to establish tighter and more practical bounds that follow from large Rashomon sets or ratios. One possible direction that could strengthen the connection between the theory and the observed trends in the Rashomon curve could develop along the lines explored by Shawe-Taylor et al. (1998), which considers data-dependent hierarchies of hypothesis spaces.\nThe connection between Rashomon sets and interpretability of models occurs in two places. First, we provided theoretical conditions under which simpler, high performing models may exist when the Rashomon set is large. Second, we hypothesize that in cases where many different algorithms perform well, a large Rashomon set containing simpler or more interpretable models may be in play. Further experimental studies and theoretical development could strengthen and give further insight into these connections.\nKurosawa\u2019s window into human nature showed how the same event can be seen through different eyes. Decades of research on learning theory have given a variety of perspectives, such as VC theory or Rademacher complexity, on the relationship between hypothesis spaces and data sets. We have proposed Rashomon sets and ratios as another perspective on this relationship, and we have provided initial theoretical and experimental results showing that this is a unique perspective that may help explain some phenomena observed in practice."}, {"heading": "Acknowledgments", "text": "We thank Theja Tulabandhula, Aaron Fisher, Zhi Chen, and Fulton Wang for comments on the manuscript."}, {"heading": "Appendix A. List of Notation", "text": "Please refer to Table 4 for the list of notation and symbols used in the paper."}, {"heading": "Appendix B. Proof of Proposition 4", "text": "We recall Proposition 4:\nProposition 4 (Approximation guarantees for the pattern Rashomon ratio) Let D represent the size of the hypothesis space F . For binary classification and sign performance function \u03b6(f, z) = sign(f(x)), as D \u2192 \u221e, the pattern Rashomon ratio R\u0302patratio(F , \u03b8) \u2192\nR\u0304pat = \u2211 i\u2264b\u03b8nc ( n i)\n2n , and for \u03b8 \u2264 1/2: 2n(H(\u03b8)\u22121)\u221a 8n\u03b8(1\u2212\u03b8) \u2264 R\u0304pat \u2264 2n(H(\u03b8)\u22121), where n is the size of\nthe training data set, and H(\u03b8) = \u2212\u03b8 log2 \u03b8 \u2212 (1\u2212 \u03b8) log2(1\u2212 \u03b8) is the binary entropy.\nProof Assume the model class becomes arbitrarily flexible, then at some value of D, each possible labeling of points (each pattern) will constitute a separate equivalence class. Then, the total number of all possible patterns, given that we have two classes, will be 2n. Also, since each possible pattern is realized, there will be one pattern that achieves the best possible accuracy, 100%. Given the Rashomon parameter \u03b8, a classification pattern should produce an accuracy of at least 1 \u2212 \u03b8 in order for its equivalence class of functions to be in the Rashomon set. Therefore, the Rashomon set can tolerate at most b\u03b8nc points to be\nmisclassified, which leads to the pattern Rashomon ratio limit R\u0302patratio(F , \u03b8)\u2192 \u2211b\u03b8nc i=0 ( n i) 2n .\nWe obtain the upper bound for R\u0304pat based on \u2211 i\u2264\u03b8n ( n i ) \u2264 2H(\u03b8)n for any fixed \u03b8 \u2264 1/2\n(Galvin, 2014). The lower bound for R\u0304pat follows from simple observations \u2211b\u03b8nc\ni=0 ( n i ) \u2265 ( n \u03b8n ) and ( n \u03b8n ) \u2265 2nH(\u03b8)\u221a\n8n\u03b8(1\u2212\u03b8) (MacWilliams and Sloane, 1977)."}, {"heading": "Appendix C. Proofs for Generalization Results", "text": ""}, {"heading": "C.1 Proof of Theorem 5", "text": "From the definition of the true anchored Rashomon set, it follows that any model in it is \u03b3close to the ERM. Simple observation shows that any model in the true anchored Rashomon set is also \u03b3-close to any other model in it and is summarized is the lemma below.\nLemma 25 For any models f, f \u2032 \u2208 F that are in the true anchored Rashomon set Rancset (F , \u03b3) we have |L(f)\u2212 L(f \u2032)| \u2264 \u03b3.\nProof Consider two models f and f \u2032 from the true anchored Rashomon set Rancset (F , \u03b3). Let L(f) = \u03b3\u2032 and L(f \u2032) = \u03b3\u2032\u2032. Then if \u03b3\u2032 > \u03b3\u2032\u2032: L(f)\u2212L(f \u2032) = \u03b3\u2032\u2212 \u03b3\u2032\u2032 \u2264 \u03b3\u2032 \u2264 \u03b3, otherwise L(f \u2032) \u2212 L(f) = \u03b3\u2032\u2032 \u2212 \u03b3\u2032 \u2264 \u03b3\u2032\u2032 \u2264 \u03b3. Combining these inequalities, we get the statement of the lemma.\nNow we recall and provide the proof of Theorem 5:\nTheorem 5 (The advantage of a large true anchored Rashomon set I) Consider finite hypothesis spaces F1 and F2, such that F1 \u2282 F2. Let the loss l be bounded by b, l(f2, z) \u2208 [0, b] \u2200f2 \u2208 F2, \u2200z \u2208 Z. Define an optimal function f\u22172 \u2208 argminf2\u2208F2L(f2). Let us assume that the true anchored Rashomon set is large enough to include a function from F1, so there exists a model f\u03031 \u2208 F1 such that f\u03031 \u2208 Rancset (F2, \u03b3). In that case, for any > 0 with probability at least 1\u2212 with respect to the random draw of data:\n|L(f\u22172 )\u2212 L\u0302(f\u03021)| \u2264 \u03b3 + 2b \u221a\nlog | F1 |+ log 2/ 2n ,\nwhere f\u03021 \u2208 argminf1\u2208F1 L\u0302(f1).\nProof We apply the union bound and Hoeffding\u2019s inequality. The result is that with probability at least 1\u2212 for every f1 \u2208 F1 we have, for a finite hypothesis space F1:\n|L(f1)\u2212 L\u0302(f1)| \u2264 2b \u221a\nlog | F1 |+ log 2/ 2n . (6)\nCombining this Occham\u2019s razor bound with the definition of f\u22172 \u2208 arg minf\u2208F2 L(f) we get that, under the same conditions:\nL(f\u22172 ) \u2264 L(f\u03021) \u2264 L\u0302(f\u03021) + 2b \u221a\nlog | F1 |+ log 2/ 2n .\nBy assumption of the theorem, there exists a function f\u03031 \u2208 F1 such that f\u03031 \u2208 Rancset (F2, \u03b3). Since f\u22172 is an optimal model, then f \u2217 2 \u2208 Rancset (F2, \u03b3) as well. From Lemma 25, |L(f\u22172 ) \u2212 L(f\u03031)| \u2264 \u03b3, which implies L(f\u03031) \u2264 L(f\u22172 ) + \u03b3. Given that f\u03021 \u2208 arg minf\u2208F1 L\u0302(f), and using (6), we get that with probability at least 1\u2212 , we have:\nL\u0302(f\u03021) \u2264 L\u0302(f\u03031) \u2264 L(f\u03031) + 2b \u221a\nlog | F1 |+ log 2/ 2n\n\u2264 L(f\u22172 ) + \u03b3 + 2b \u221a\nlog | F1 |+ log 2/ 2n .\nCombining the previous two equations together we have:\u2223\u2223\u2223L(f\u22172 )\u2212 L\u0302(f\u03021)\u2223\u2223\u2223 \u2264 \u03b3 + 2b \u221a\nlog | F1 |+ log 2/ 2n ."}, {"heading": "C.2 Proof of Theorem 6 is in Appendix C.7", "text": "We provide the proof of Theorem 6 in Appendix C.7, as we use Proposition 11 in the proof."}, {"heading": "C.3 Proof of Theorem 7", "text": "Theorem 7 (The advantage of a large true anchored Rashomon set II) Consider finite hypothesis spaces F1 and F2, such that F1 \u2282 F2 and F1 is uniformly drawn from F2 without replacement. Define an optimal function f\u22172 \u2208 argminf2\u2208F2L(f2). For a loss l\nbounded by b and any > 0, with probability at least (1\u2212 ) p with respect to the random draw of functions from F2 to form F1 and with respect to the random draw of data:\n|L(f\u22172 )\u2212 L\u0302(f\u03021)| \u2264 \u03b3 + 2b \u221a\nlog | F1 |+ log 2/ 2n ,\nwhere p = 1 \u2212 ((1\u2212R anc ratio(F2,\u03b3))| F2 | | F1 | )\n(| F2 || F1 |) = 1 \u2212\n\u220f|Rancset (F2,\u03b3)| i=1 ( 1\u2212 | F1 || F2 |\u2212|Rancset (F2,\u03b3)|+i ) , and f\u03021 \u2208\nargminf1\u2208F1 L\u0302(f1).\nProof The true anchored Rashomon set Rancset (F2, \u03b3) has Rancratio(F2, \u03b3)| F2 | models. The probability that at least one of these models is from the hypothesis space F1 is: p = 1 \u2212 ((1\u2212R anc ratio(F2,\u03b3))| F2 | | F1 | )\n(| F2 || F1 |) . In the fraction, the numerator is the number of ways we could\nrandomly select | F1 |models that are outside of the Rashomon set, whereas the denominator is the total number of ways we can select | F1 | models from | F2 | at random.\nNow with probability p = 1\u2212 ((1\u2212R anc ratio(F2,\u03b3))| F2 | | F1 | )\n(| F2 || F1 |) we can guarantee that the hypothesis\nspace F1 will contain at least one model from the true anchored Rashomon set, therefore by using Theorem 5 we get the statement of Theorem 7.\nSimplifying the the binomial coefficients we get that:\n1\u2212 p =\n((1\u2212Rancratio(F2,\u03b3))| F2 | | F1 | ) (| F2 | | F1 |\n) = ((1\u2212Rancratio(F2, \u03b3))| F2 |)!| F1 |!(| F2 | \u2212 |F1 |)!| F1 |!((1\u2212Rancratio(F2, \u03b3))| F2 | \u2212 |F1 |)!| F2 |! =\n((1\u2212Rancratio(F2, \u03b3))| F2 |)! | F2 |! (| F2 | \u2212 |F1 |)! ((1\u2212Rancratio(F2, \u03b3))| F2 | \u2212 |F1 |)!\n= Rancratio(F2,\u03b3)| F2 |\u220f i=1 (1\u2212Rancratio(F2, \u03b3))| F2 | \u2212 |F1 |+ i (1\u2212Rancratio(F2, \u03b3))| F2 |+ i\n= Rancratio(F2,\u03b3)| F2 |\u220f i=1 ( 1\u2212 |F1 | (1\u2212Rancratio(F2, \u03b3))| F2 |+ i )\n= |Rancset (F2,\u03b3)|\u220f i=1 ( 1\u2212 |F1 | | F2 | \u2212 |Rancset (F2, \u03b3)|+ i ) .\nTherefore, alternatively p = 1\u2212 \u220f|Rancset (F2,\u03b3)| i=1 ( 1\u2212 | F1 || F2 |\u2212|Rancset (F2,\u03b3)|+i ) , which is an easier\nexpression to compute in practice, especially for large values of | F2 |."}, {"heading": "C.4 Proof of Theorem 9 via Lemma 8", "text": "Theorem 9 follows directly from Lemma 8 and Theorem 5, which guarantees that with high probability the sampled space F1 will contain at least one model from the true anchored Rashomon set. Now we recall and provide a proof of Lemma 8:\nLemma 8 For a finite hypothesis space F2 of size | F2 |, we will draw | F1 | functions uniformly without replacement from F2 to form F1. If the true anchored Rashomon ratio of the hypothesis space F2 is at least\nRancratio(F2, \u03b3) \u2265 1\u2212 1 | F1 |\nthen with probability at least 1\u2212 with respect to the random draw of functions from F2 to form F1, the Rashomon set contains at least one model f\u03031 from F1."}, {"heading": "Proof", "text": "The probability of an individual sample from F2 missing the true anchored Rashomon set is 1 \u2212 Rancratio(F2, \u03b3). The probability if this happening | F1 | times independently is (1\u2212Rancratio(F2, \u03b3))| F1 |. Thus, for any > 0, if the Rashomon ratio is at least Rancratio(F2, \u03b3) \u2265 1 \u2212 1 | F1 | , the probability pw of sampling, with replacement, at least one hypothesis from Rancratio(F2, \u03b3) is: pw = 1\u2212 (1\u2212Rancratio(F2, \u03b3))\n| F1 | \u2265 1\u2212 . Let pi be the probability, under sampling without replacement, that samples 1 . . . i have missed Rancratio(F2, \u03b3). p1 = 1\u2212Rancratio(F2, \u03b3), and pi \u2264 (1\u2212Rancratio(F2, \u03b3))i. The probability, under sampling without replacement, that at least one hypothesis from Rancratio(F2, \u03b3) in F1 is therefore 1\u2212p| F1 | \u2265 pw. Thus the the statement of the lemma holds with the probability at least 1\u2212 ."}, {"heading": "C.5 Proof of Proposition 10", "text": "Proposition 10 (Empirical anchored Rashomon set is close to true) For a loss l bounded by b and for any > 0 with probability at least 1 \u2212 e\u22122n( /b)2 with respect to the random draw of training data, if f \u2208 R\u0302ancset (F , \u03b3) then f \u2208 Rancset (F , \u03b3 + ).\nProof For a fixed f \u2208 R\u0302ancset (F , \u03b3), by Hoeffding\u2019s inequality:\nP [ L\u0302(f)\u2212 L(f) < \u2212 ] = P\n[ 1\nn n\u2211 i=1 l(f, zi)\u2212 E [l(f, z)] < \u2212\n] \u2264 e\u22122n( /b)2 .\nTherefore, with probability at least 1\u2212 e\u22122n( /b)2 with respect to the random draw of data, L(f)\u2212 L\u0302(f) \u2264 .\nSince f \u2208 R\u0302ancset (F , \u03b3), then by definition of the Rashomon set, L\u0302(f) \u2264 \u03b3. Combining this with Hoeffding\u2019s inequality, with probability at least 1\u2212 e\u22122n( /b)2 :\nL(f) \u2264 L\u0302(f) + \u2264 \u03b3 + ,\ntherefore f \u2208 Rancset (F , \u03b3 + )."}, {"heading": "C.6 Proof of Proposition 11", "text": "Proposition 11 (True anchored Rashomon set is close to empirical) For a loss l bounded by b and for any > 0, if f \u2208 Rancset (F , \u03b3) then with probability at least 1\u2212e\u22122n( /b) 2 with respect to the random draw of training data,\nf \u2208 R\u0302ancset (F , \u03b3 + ).\nProof For a fixed f \u2208 Rancset (F , \u03b3) by Hoeffding\u2019s inequality:\nP [ L\u0302(f)\u2212 L(f) > ] = P\n[ 1\nn n\u2211 i=1 l(f, zi)\u2212 E [l(f, z)] >\n] \u2264 e\u22122n( /b)2 .\nTherefore, with probability at least 1\u2212 e\u22122n( /b)2 with respect to the random draw of data, L\u0302(f)\u2212 L(f) \u2264 .\nSince f \u2208 Rancset (F , \u03b3), then by definition of the Rashomon set, L(f) \u2264 \u03b3. Combining this with Hoeffding\u2019s inequality, we get that with probability at least 1\u2212 e\u22122n( /b)2 :\nL\u0302(f) \u2264 L(f) + \u2264 \u03b3 + ,\ntherefore f \u2208 R\u0302ancset (F , \u03b3 + )."}, {"heading": "C.7 Proof of Theorem 6", "text": "Theorem 6 (The advantage of a good approximating set) Consider hypothesis spaces F1 and F2, such that F1 \u2282 F2. Let the loss l be bounded by b, l(f2, z) \u2208 [0, b] \u2200f2 \u2208 F2, \u2200z \u2208 Z. Define an optimal function f\u22172 \u2208 argminf2\u2208F2L(f2). Let us assume that the true anchored Rashomon set is large enough to include a function from F1, so there exists a model f\u03031 \u2208 F1 such that f\u03031 \u2208 Rancset (F2, \u03b3). In that case, for any > 0 with probability at least 1\u2212 with respect to the random draw of data:\n|L(f\u22172 )\u2212 L\u0302(f\u03021)| \u2264 2\u03b3 + b \u221a log 1/\n2n ,\nwhere as before, f\u03021 \u2208 argminf1\u2208F1 L\u0302(f1).\nProof By the assumption of the theorem we have that L(f\u03031) \u2264 \u03b3. Also, by the definition of an optimal model f\u22171 , L(f \u2217 1 ) \u2264 L(f\u03031). Combining these, we get that L(f\u22171 ) \u2264 L(f\u03031) \u2264 \u03b3. Thus f\u22171 is in the true anchored Rashomon set of F2, f\u22171 \u2208 Rancset (F2, \u03b3). Following Proposition 11, we have that for any 1 > 0 with probability at least 1 \u2212 e\u22122n( 1/b) 2 with respect to the random draw of data, f\u22171 is in the slightly larger anchored Rashomon set R\u0302ancset (F2, \u03b3 + 1), and therefore, with high probability, L\u0302(f\u22171 ) \u2264 \u03b3 + 1. Or alternatively, by setting = e\u22122n( 1/b) 2 we get that for any > 0 with probability at least 1 \u2212 , we have\nL\u0302(f\u22171 ) \u2264 \u03b3 + b \u221a log 1/ 2n . Further, by definition of the empirical risk minimizer we get:\nL\u0302(f\u03021) \u2264 L\u0302(f\u22171 ) \u2264 \u03b3 + b \u221a log 1/\n2n . (7)\nOn the other hand, from the definition of the Rashomon set, L(f\u22172 ) \u2264 \u03b3. Combining this with (7) we have that:\n|L(f\u22172 )\u2212 L\u0302(f\u03021)| \u2264 L(f\u22172 ) + L\u0302(f\u03021) \u2264 2\u03b3 + b \u221a log 1/\n2n ."}, {"heading": "C.8 Proof of Theorem 12", "text": "Theorem 12 (Existence of a simpler-but-accurate model I) For K-Lipschitz loss l bounded by b consider hypothesis spaces F1 and F2 such that F1 \u2282 F2. With probability greater than 1 \u2212 w.r.t. the random draw of training data, if there exists f\u03041 \u2208 F1 such that \u2016f\u03022 \u2212 f\u03041\u2016p \u2264 \u03b8K , where f\u03022 is the empirical risk minimizer within F2, then for a fixed parameter \u2208 (0, 1):\n1. f\u03041 is in the Rashomon set R\u0302set(F2, \u03b8).\n2. \u2223\u2223\u2223L(f\u03041)\u2212 L\u0302(f\u03041)\u2223\u2223\u2223 \u2264 2KRn(F1) + b\u221a log(2/ )2n , where Rn(F) is the standard Rademacher complexity of a functional space F . (This bound arises from standard learning theory.)\nProof The first result follows directly from the definition of the Rashomon set and Lipschitz continuity:\nL\u0302(f\u03041)\u2212 L\u0302(f\u03022) = |L\u0302(f\u03041)\u2212 L\u0302(f\u03022)| \u2264 K\u2016f\u03041 \u2212 f\u03022\u2016p = K \u03b8\nK = \u03b8.\nUsing Bartlett and Mendelson\u2019s generalization bound for Lipschitz loss functions (Bartlett and Mendelson, 2002) we have that for every model f1 \u2208 F1, with probability greater that 1 \u2212 , |L(f1) \u2212 L\u0302(f1)| \u2264 2KRn(F1) + b \u221a log(2/ )\n2n . Since f\u03041 \u2208 F1, the bound holds for it as well."}, {"heading": "C.9 Proof of Theorem 13", "text": "Theorem 13 (Existence of a simpler-but-accurate model II) For a K-Lipschitz loss l bounded by b, and hypothesis spaces F1 and F2 such that F1 \u2282 F2. With probability greater than 1\u2212 w.r.t. the random draw of training data, if for every model f2 \u2208 R\u0302set(F2, \u03b8) there exists a model f1 \u2208 F1 such that \u2016f2 \u2212 f1\u2016p \u2264 \u03b4 and if the Rashomon set is large, e.g. it contains a ball of size at least \u03b4, that is, R\u0302set(F2, \u03b8) \u2283 B\u03b4( ), then there exists a model f\u03041 \u2208 R\u0302set(F2, \u03b8), such that for a fixed parameter \u2208 (0, 1):\n1. f\u03041 is from the simpler space F1. 2. \u2223\u2223\u2223L(f\u03041)\u2212 L\u0302(f\u03041)\u2223\u2223\u2223 \u2264 2KRn(F1) + b\u221a log(2/ )2n , where Rn(F) is the standard Rademacher complexity of a functional space F . (This bound arises from standard learning theory.)\nProof Consider a ball B\u03b4(f \u2032 2) of radius \u03b4 centered at f \u2032 2 that is contained within the Rashomon set. By the theorem\u2019s assumption, since f \u20322 \u2208 R\u0302set(F2, \u03b8), there exists f\u03041 \u2208 F1 such that \u2016f \u20322 \u2212 f\u03041\u2016p \u2264 \u03b4. Therefore f\u03041 is inside the \u03b4-ball f\u03041 \u2208 B\u03b4(f \u20322) and thus belongs to the Rashomon set R\u0302set(F2, \u03b8).\nThe generalization bound follows Bartlett and Mendelson (2002) as before."}, {"heading": "C.10 Proof of Theorem 14", "text": "Theorem 14 (Existence of multiple simpler models) For K-Lipschitz loss l bounded by b, consider hypothesis spaces F1 and F2 such that F1 \u2282 F2. With probability greater than 1\u2212 w.r.t. the random draw of training data, if for every model f2 \u2208 R\u0302set(F2, \u03b8) there exists a model f1 \u2208 F1 such that \u2016f2 \u2212 f1\u2016p \u2264 \u03b4 then there exists at least B = B(R\u0302set(F2, \u03b8), 2\u03b4) functions f\u030411 , f\u0304 2 1 ..., f\u0304 B 1 \u2208 R\u0302set(F , \u03b8) such that:\n1. They are from a simpler space: f\u030411 , f\u0304 2 1 ..., f\u0304 B 1 \u2208 F1. 2. \u2223\u2223\u2223L(f\u0304 i1)\u2212 L\u0302(f\u0304 i1)\u2223\u2223\u2223 \u2264 2KRn(F1) + b\u221a log(2/ )2n , for all i \u2208 [1, .., B], where Rn(F) is the Rademacher complexity of a functional space F . (This is from standard learning theory.)\nProof Starting from the packing number of the Rashomon set B(R\u0302set(F2, \u03b8), 2\u03b4), there exists a 2\u03b4-packing \u039e = {\u03be1, ..., \u03bek|\u03bei \u2208 R\u0302set(F2, \u03b8)} such that \u2016\u03bei \u2212 \u03bej\u2016p > 2\u03b4. On the other hand, for each \u03bei \u2208 R\u0302set(F2, \u03b8) there exists f\u0304 i1 \u2208 F1 such that \u2016\u03bei \u2212 f\u0304 i1\u2016p \u2264 \u03b4. Therefore for each ball center \u03bei in the packing there is a distinct model f\u0304 i 1 from the simpler hypothesis space F1. Thus, the Rashomon set contains at least B = B(R\u0302set(F2, \u03b8), 2\u03b4) models from F1.\nThe generalization bound follows Bartlett and Mendelson (2002) as before."}, {"heading": "C.11 Proof of Theorem 15", "text": "Theorem 15 (Generalization and reduced complexity of the Rashomon set) For a K-Lipschitz loss l bounded by b consider two hypothesis spaces F1 \u2282 F2 such that for any model f2 \u2208 R\u0302set(F2, \u03b8) there exists a model f1 \u2208 F1 such that \u2016f2 \u2212 f1\u2016p \u2264 \u03b4, then for all f2 \u2208 R\u0302set(F2, \u03b8) and for any > 0 with probability at least 1\u2212 :\u2223\u2223\u2223L(f2)\u2212 L\u0302(f2)\u2223\u2223\u2223 \u2264 2K (\u03b4 +Rn(F1)) + b\u221a log(2/ )\n2n ,\nwhere Rn(F) is the standard Rademacher complexity of a functional space F .\nProof The theorem follows from the triangle inequality, the theorem\u2019s statement, and generalization bound for K-Lipschitz loss functions from Bartlett and Mendelson (2002):\n|L(f2)\u2212 L\u0302(f2)| \u2264 |L(f2)\u2212 L(f1)|+ |L\u0302(f2)\u2212 L\u0302(f1)|+ |L(f1)\u2212 L\u0302(f1)| \u2264 K\u03b4 +K\u03b4 + 2KRn(F1) + b \u221a log(2/ )\n2n .\nL"}, {"heading": "Appendix D. Approximation of the Rashomon Ratio and Volume", "text": ""}, {"heading": "D.1 Rashomon Volume for Ridge Regression", "text": "D.1.1 Visualization of the Rashomon Set for Ridge Regression\nConsider least squares regression, which is a corner case of ridge regression with the regularization constant C = 0. Figure 13 shows the plot of the empirical risk in two dimensional parameter space. Visually, the Rashomon set consists of the those parameters \u03c9 = [\u03c91, \u03c92] that produces a loss below the dark shaded ellipse on the paraboloid, and then Rashomon volume can be computed exactly as the volume of the shaded ellipsoid."}, {"heading": "D.1.2 Proof of Theorem 16", "text": "Theorem 16 (Rashomon volume for ridge regression) For a parametric hypothesis space of linear models F\u2126 = {f\u03c9(x) = \u03c9Tx, \u03c9 \u2208 Rp} and a data set S = X \u00d7 Y , the Rashomon set R\u0302set(F\u2126, \u03b8) of ridge regression is an ellipsoid, containing vectors \u03c9 such that:\n(\u03c9 \u2212 \u03c9\u0302)T X TX + CIp\n\u03b8 (\u03c9 \u2212 \u03c9\u0302) \u2264 1,\nand the Rashomon volume can be computed as:\nV(R\u0302set(F\u2126, \u03b8)) = J(\u03b8, p) p\u220f i=1 1\u221a \u03c32i + C ,\nwhere \u03c3i are singular values of matrix X, J(\u03b8, p) = \u03c0p/2\u03b8p/2 \u0393(p/2+1) and \u0393(\u00b7) is the gamma function.\nProof Consider all models f\u03c9 \u2208 F\u2126 from the Rashomon set R\u0302set(F\u2126, \u03b8). Then by Definition 1 we get:\nL\u0302(X,Y, \u03c9) \u2264 L\u0302(X,Y, \u03c9\u0302) + \u03b8. (8)\nUsing XTY = (XTX+CIp)\u03c9\u0302 from the optimal solution of the ridge regression estimator \u03c9\u0302 = (XTX + CIp) \u22121XTY , and expanding the difference between empirical risks we have:\n\u03b8 \u2265L\u0302(X,Y, \u03c9)\u2212 L\u0302(X,Y, \u03c9\u0302) =(X\u03c9 \u2212 Y )T (X\u03c9 \u2212 Y ) + C\u03c9T\u03c9 \u2212 (X\u03c9\u0302 \u2212 Y )T (X\u03c9\u0302 \u2212 Y )\u2212 C\u03c9\u2217T \u03c9\u0302 =\u03c9TXTX\u03c9 \u2212 2\u03c9TXTY + C\u03c9T\u03c9 \u2212 \u03c9\u2217TXTX\u03c9\u0302 + 2\u03c9\u2217TXTY \u2212 C\u03c9\u2217T \u03c9\u0302 =\u03c9TXTX\u03c9 \u2212 2\u03c9T (XTX + CIp)\u03c9\u0302 + C\u03c9T\u03c9 \u2212 \u03c9\u2217TXTX\u03c9\u0302 + 2\u03c9\u2217T (XTX + CIp)\u03c9\u0302 \u2212 C\u03c9\u2217T \u03c9\u0302 =\u03c9TXTX\u03c9 + C\u03c9T\u03c9 \u2212 2\u03c9T (XTX + CIp)\u03c9\u0302 + \u03c9\u2217TXTX\u03c9\u0302 + C\u03c9\u2217T \u03c9\u0302 =\u03c9T (XTX + CIp)\u03c9 \u2212 2\u03c9T (XTX + CIp)\u03c9\u0302 + \u03c9\u2217T (XTX + CIp)\u03c9\u0302 =(\u03c9 \u2212 \u03c9\u0302)T (XTX + CIp)(\u03c9 \u2212 \u03c9\u0302).\nTherefore the Rashomon set is an ellipsoid centered at \u03c9\u0302:\n(\u03c9 \u2212 \u03c9\u0302)T X TX + CIp\n\u03b8 (\u03c9 \u2212 \u03c9\u0302) \u2264 1.\nBy the formula of the volume of a p-dimensional ellipsoid the Rashomon volume can be computed as:\nV(R\u0302set(F\u2126, \u03b8)) = \u03c0p/2\u03b8p/2\n\u0393(p/2 + 1) p\u220f i=1 1\u221a \u03c32i + C ,\nwhere \u03c3i are singular values of X."}, {"heading": "D.1.3 Rashomon Volume Lower Bounds for Ridge Regression", "text": "The results described in this section follow directly from the Theorem 16 and are basic observations of the Rashomon volume closed-form formula for ridge regression.\nCorollary 26 For a data set S = X \u00d7 Y such that a Frobenius norm of the feature matrix X is bounded \u2016X\u2016F = \u221a\u2211p,n i,j x 2 ij \u2208 [1, F ] and for a parametric hypothesis space of linear models F\u2126 = {f\u03c9(x) = \u03c9Tx, \u03c9 \u2208 Rp}, the Rashomon volume of ridge regression is at least\nV(R\u0302set(F , \u03b8)) \u2265 2K(\u03b8, p)\nF + pC ."}, {"heading": "Proof", "text": "For real ai \u2265 0, i = 1, .., p we have that ( \u220fp i=1 ai) 1/q \u2264 ( \u2211p i=1 ai) /q, then by by setting q = 2 and ai = \u03c3 2 i + C we get: (\u220fp i=1 ( \u03c32i + C )) 1 2 \u2264 12 (\u2211p i=1 ( \u03c32i + C )) =\n1 2 \u2211p i=1 ( \u03c32i + pC ) = 12 (\u2016X\u2016F + pC) \u2264 1 2 (F + pC) , therefore from the Theorem 16 we have that V(R\u0302set(F , \u03b8)) \u2265 2K(\u03b8,p)F+pC .\nCorollary 27 For a data set S = X\u00d7Y and a parametric hypothesis space of linear models F\u2126 = {f\u03c9(x) = \u03c9Tx, \u03c9 \u2208 Rp}, if \u2202\n2L\u0302 \u2202\u03c92j \u2264 \u03b4, such that \u03b4 \u2265 2C, then the Rashomon volume of\nridge regression is at least\nV(R\u0302set(F , \u03b8)) \u2265 2K(\u03b8, p)\u221a\np( \u03b42 \u2212 C) + pC ."}, {"heading": "Proof", "text": "As in previous Corollary we can bound the singular values product with the Frobenius\nnorm of the feature matrix ( \u220fp i=1(\u03c3 2 i + c)) 1 2 \u2264 12(\u2016X\u2016F + pc). Given the bounded second derivative we have \u2202 2L\u0302 \u2202\u03c92j = 2 \u2211 i \u2211 j x 2 ij + 2C \u2264 \u03b4. By the assumption \u03b4 \u2265 2C we get that\u2211\ni \u2211 j x 2 ij \u2264 \u03b42 \u2212 C and therefore we can upper bound the Frobenius norm as follows:\n\u2016X\u2016F = \u221a\u2211 j( \u2211 i x 2 ij) \u2264 \u221a\u2211 j( \u03b4 2 \u2212 C) = \u221a p( \u03b42 \u2212 C). Taking into account the Theorem 16 V(R\u0302set(F , \u03b8)) \u2265 2K(\u03b8,p)\u221a p( \u03b4\n2 \u2212C)+pC\n.\nCorollary 28 For a data set S = X\u00d7Y , such that xi are on a unit sphere \u2200i : \u2016xi\u2016 = 1 and a parametric hypothesis space of linear models F\u2126 = {f\u03c9(x) = \u03c9Tx, \u03c9 \u2208 Rp}, the Rashomon volume of ridge regression is at least\nV(R\u0302set(F , \u03b8)) \u2265 2K(\u03b8, p)\u221a n+ pC"}, {"heading": "Proof", "text": "As in previous Corollaries we can bound the singular values product with the Frobenius\nnorm of the feature matrix ( \u220fp i=1(\u03c3 2 i +c)) 1 2 \u2264 12(\u2016X\u2016F+pc). Since \u2016X\u2016F = \u221a\u2211 i( \u2211\nj x 2 ij) =\u221a\u2211\ni 1 = \u221a n, then \u220fn i=1 \u221a \u03c32i + c \u2264 \u221a n+pc 2 , and combined with the Theorem 16 we get\nthat V(R\u0302set(F , \u03b8)) \u2265 2K(\u03b8,p)\u221an+pC ."}, {"heading": "D.2 Convex Loss", "text": "For a parametric hypothesis space where the loss l(\u03c9, z) is convex with respect to the parameter vector \u03c9, the Rashomon set as well as the hypothesis space, are convex as well, and we can use random walks to estimate their volumes. In particular, according to Theorem\n2.1 by Kannan et al. (1997), there exists a randomized algorithm that can approximate, with high probability, the volume of a convex body V \u2208 Rp within an error using approximately O(p5) calls to a separating oracle. In particular we can approximate the volume V\u0302(V ) such that:\n(1\u2212 )V\u0302(V ) < V(V ) < (1 + )V\u0302(V ). (9)\nTo adapt the randomized algorithm theorem for Rashomon set estimation in the parameter space \u2126, we need to construct a separating oracle (Gro\u0308tschel et al., 2012): a routine that, for a given point \u03bb and convex set \u039b, tells us whether \u03bb \u2208 \u039b, and if not, provides a separating hyperplane between \u03bb and \u039b. From the Rashomon set definition, given a parameter vector \u03c9, we check if f\u03c9 belongs to the Rashomon set according to L\u0302(f\u03c9) \u2264 L\u0302(f\u0302\u03c9) + \u03b8. If f\u03c9 is not in the Rashomon set, we construct a separating hyperplane in parameter space \u2126 using the perpendicular to the tangent hyperplane. In particular, since the loss is convex, a tangent hyperplane at point \u03c9 looks like:\n\u2207l(\u03c9, \u00b7)(\u03b3 \u2212 \u03c9) = 0.\nLet \u03c9pr = PRR\u0302set(F\u2126,\u03b8)(\u03c9) be a projection of point \u03c9 onto the Rashomon set. Then, we derive a separating hyperplane to be in the middle of \u03c9 and its projection:\n\u2207l(\u03c9, \u00b7)(\u03b3 \u2212 \u03c9) + (\u03c9 \u2212 \u03c9pr)/2 = 0.\nApplying the constructed separating oracle to the randomized algorithm theorem, with high probability, we can achieve approximation guarantees for the Rashomon volume given in (9)."}, {"heading": "D.3 Rashomon Volume Under-Approximation for SVM-1", "text": "In this section we propose an optimization procedure that allows to under-approximate the Rashomon volume in the parameter space for the Support Vector Machine (SVM)(Burges, 1998) with L-1 regularization.\nConsider binary classification for the class of linear models. Directly computing the Rashomon ratio requires sampling over an infinite space of linear functions. Instead we propose a procedure that allows us to compute an L-1 ball that is contained within the Rashomon set for the SVM-1 (Cai et al., 2011; Zhu et al., 2004) learning algorithm.\nTheorem 29 For the SVM-1 with hinge loss \u03c6(f(x), y) = b1\u2212yf(x)c+, L-1 regularization, and for parameterized hypothesis space of linear models F\u2126 = {\u03c9Tx, \u03c9 \u2208 Rp} the Rashomon volume is at least\nV(R\u0302set(F , \u03b8)) \u2265 2p\u03b4p\np! ,\nwhere \u03b4 = \u2016\u03c9v\u2212\u03c9c\u20161, \u03c9c is an optimal solution to the 1-norm SVM problem and \u03c9v satisfies min\u03c9v\u2208R\u0302set(F\u2126,\u03b8) \u2016\u03c9 v \u2212 \u03c9c\u20161"}, {"heading": "Proof", "text": "Given that \u03c9c is an optimal solution to the 1-norm SVM problem, it satisfies:\nmin \u03c9c \u2016\u03c9c\u20161 + n\u2211 i=1 b1\u2212 yif\u03c9c(xi)c+.\nLet \u03c9v be a solution to the following optimization problem:\nmin \u03c9 \u2016\u03c9 \u2212 \u03c9c\u20161, s.t.\nn\u2211 i=1 b1\u2212 yif\u03c9(xi)c+ \u2265 \u03b8 + n\u2211 i=1 b1\u2212 yif\u03c9c(xi)c+,\nthen \u03c9v is in the Rashomon set, \u03c9v \u2208 R\u0302set(F\u2126, \u03b8), and is the closest to \u03c9c in the parameter space. The convexity of the optimization problem ensures that the inequality constraint will be tight since any \u03c9v for which the constraint is not tight can be replaced with one that improves the objective function by moving towards \u03c9c.\nSince \u03c9v is the closest in L-1 norm to \u03c9c and has the largest tolerable loss, then all models in the cross-polytope centered in \u03c9c with a half-diagonal \u03b4 = \u2016\u03c9v \u2212 \u03c9c\u20161 will be in the Rashomon set R\u0302set(F\u2126, \u03b8) because they will have loss no more than that of \u03c9v. Therefore, the Rashomon volume is at least the volume of the cross-polytope, which is given by 2 p\u03b4p\np! ."}, {"heading": "Appendix E. Proofs for Connection to Simplicity Measures", "text": ""}, {"heading": "E.1 Proof of Theorem 18", "text": "Theorem 18 (Rashomon ratio and algorithmic stability) Consider a distribution PX over a discrete domain X = {x1, ...xN} and a learning algorithm A that minimizes ridge regression\u2019s empirical risk L\u0302 for a linear hypothesis space F\u2126, as in Equation (3). For any \u03bb > 0 there exist joint distributions PX,Y1 and PX,Y2 where for X drawn i.i.d. from PX , Y1 is drawn from PY1|X over Y |X and Y2 is drawn from PY2|X over Y |X , such that the expected Rashomon ratios are the same:\nEPX,Y1 [RratioS1 (F\u2126, \u03b8)] = EPX,Y2 [RratioS2 (F\u2126, \u03b8)],\nyet hypothesis stability constants are different by an arbitrarily chosen value of \u03bb:\n\u03b2\u03032 \u2212 \u03b2\u03031 \u2265 \u03bb,\nwhere S1 and S2 denote data sets S1 = [X,Y1] and S2 = [X,Y2], \u03b2\u03031 is the hypothesis stability coefficient of algorithm A for distribution PX,Y1 and \u03b2\u03032 is the hypothesis stability coefficient for distribution PX,Y2.\nProof Consider the least squares regression min\u03c9 \u2211n i=1 l(\u03c9, zi) 2, where \u03c9 \u2208 Rp, and loss l(\u03c9, z) = \u03c6(\u03c9Tx,y) for z = (x,y). For the marginal distribution PX and X = [x1, ...,xn] drawn i.i.d. from PX we design distributions PY1|X and PY2|X as:\nPY1|X(y = 0|x) = 1 \u2200x \u2208 X,\nPY2|X(y = 0|x 6= x0) = 1, PY2|X(y = 0|x = x0) = 0.5, PY2|X(y = H|x = x0) = 0.5,\nwhere x0 \u2208 {x1, ..., xN} is some fixed point with a positive probability PX(x0) and we define H \u2208 R later.\nAccording to the definition of algorithmic stability, for PX,Y1 we have:\nES1,z[|l(fS1 , z)\u2212 l(fS\\i1 , z)|] = 0 = \u03b2\u03031,\nand for distribution PX,Y2 : ES2,z [\u2223\u2223\u2223l(fS2 , z)\u2212 l(fS\\i2 , z)\u2223\u2223\u2223] = \u2211\nS2,z\u223cPX,Y2\nPX,Y2(S2)PX,Y2(z) \u2223\u2223\u2223l(fS2 , z)\u2212 l(fS\\i2 , z)\u2223\u2223\u2223\n\u2265 PX,Y2(Ss2)PX,Y2(zs) \u2223\u2223\u2223l(fsS2 , zs)\u2212 l(fSs,\\i2 , zs)\u2223\u2223\u2223 ,\nwhere Ss2, z s is a special draw such that zs = (x0,H) and S s 2 contains both (x0,H) and (x0,0). Since the domain X is discrete, the probabilities of a special draw are:\nPX,Y2(z s) =\n1 2 Bin(1, n, PX(x0)), PX,Y2(S s 2) = 1 4 Bin(1, n, PX(x0)) 2Bin(n\u22122, n, 1\u2212PX(x0)),\nwhere Bin(k, n, pk) = ( n k ) pkk(1 \u2212 pk)(n\u2212k) is a binomial coefficient, namely a probability of getting exactly k successes from n trials, where each trial has a probability of success pk. Denote P(Ss2,zs) as the probability of getting a special draw, then P(Ss2,zs) = PX,Y2(S s 2)PX,Y2(z\ns). If Ss2 contains only two points (x0,H) and (x0,0), the loss difference |l(fsS2 , z\ns) \u2212 l(f\nS s,\\i 2 , zs)| evaluated at zs for all i will be at least H24 . As we add more points (xi,0) to the data set Ss2 the loss difference in the special draw case will only increase. Therefore for all i:\n|l(f sS2 , z s)\u2212 l(f\nS s,\\i 2\n, zs)| \u2265 H 2\n4 . If we choose H such that H > 2 \u221a \u03bb ( P(Ss2,zs) )\u22121/2 , then from the definition of algorithmic stability we have:\n\u03b2\u03032 \u2265 ES2,z [\u2223\u2223\u2223l(fS2 , z)\u2212 l(fS\\i2 , z)\u2223\u2223\u2223] \u2265 P(Ss2,zs) H24 > \u03bb.\nTherefore for any given \u03bb we get that \u2223\u2223\u2223\u03b2\u03031 \u2212 \u03b2\u03032\u2223\u2223\u2223 > \u03bb.\nOn the other hand, the Rashomon volume for the hypothesis space F\u2126 of linear models does not depend on targets and can be calculated as in (4) for both S1 and S2. Therefore the expected Rashomon volumes are the same:\nES1 [ V(RsetS1 (F\u2126, \u03b8)) ] = ES2 [ V(RsetS2 (F\u2126, \u03b8)) ] .\nGiven the equality of the expected Rashomon volumes the the expected Rashomon ratios are the same as well as we do not change the hypothesis space when drawing different joint distribution."}, {"heading": "E.2 Proof of Theorem 19", "text": "Theorem 19 (Rashomon ratio and geometric margin) For any fixed 0 < \u03bb < 1, there exists a fixed hypothesis space F\u2126, a Rashomon parameter \u03b8, and there exist two data sets S1 and S2 with the same empirical risk minimizer f\u0302 \u2208 F\u2126 such that the width of the geometric margin d is the same for both data sets, yet the Rashomon ratios are different:\n|RratioS1 (F\u2126, \u03b8)\u2212RratioS2 (F\u2126, \u03b8)| > \u03bb."}, {"heading": "Proof", "text": "Consider two-dimensional separable data, X \u2208 [0, 1]2, and a parametrized hypothesis space of origin-centered linear models: F = {\u03c9Tx, \u03c9 = (k,\u22121), x \u2208 R2, k \u2208 R}. Consider also 0-1 loss \u03c6\u03c9(x, y) = 1[y=sign(\u03c9T x)] and an empirical risk minimizer f\u0302 = f\u03c9\u0302 that maximizes the geometric margin. Since the data are populated in a [0, 1]2 hypercube, as a hypothesis space we will consider all models that intersect the unit-hypercube.\nFor some positive constant a \u2208 (0, 1) that we choose later, consider the following regions of the feature space:\nA = {x1 \u2208 [0, 1\u2212 a), x2 > x1 + (1\u2212 2a)}, B = {x1 \u2208 (a, 1], x2 < x1 \u2212 (1\u2212 2a)},\nC = {x1 \u2208 [0, a), x2 \u2208 (1\u2212 a, 1]}, D = {x1 \u2208 (1\u2212 a, 1], x2 \u2208 [0, a)}.\nConstruct data set S1, such that S1 = (xA, 1) \u222a (xB,\u22121) \u222a (xs1S1 , 1) \u222a (x s2 S1 ,\u22121), where xA \u2208 A is any sample from the region A, xB \u2208 B is any sample from the region B, xs1S1 and ss 2\nS1 are special points for the data set S1 such that x s1 S1 = [1 \u2212 2a, 1] and xs2S1 = [1, 1 \u2212 2a]. Please see Figure 14a for details.\nConstruct data set S2, such that S2 = (xC , 1) \u222a (xD,\u22121) \u222a (xs1S2 , 1) \u222a (x s2 S2 ,\u22121), where xC \u2208 C is any sample from the region C, xD \u2208 D is any sample from the region D, xs1S2 and xs2S2 are special points for the data set S2 such that x s1 S2\n= [a, 1\u2212 a] and xs2S2 = [1\u2212 a, a]. Please see Figure 14b for details.\nNote that the data sets we considered have the same width for the geometrical margin d = \u221a\n2(2a\u2212 1) (see Figures 14a, 14b). Now, we are left to show that the Rashomon ratios are different.\nFor the functional space of origin-centered lines we have a unique parameterization and a one-to-one correspondence between an actual model and its parameterization. Therefore, if the Rashomon set is a single connected component, an angle \u03b1 between the two most distant models in the Rashomon set gives us some information about the Rashomon volume. In particular, we can compute the Rashomon ratio as a ratio of the angle \u03b1 that represents the Rashomon set and the angle \u03b2 that corresponds to the hypothesis space as shown on Figure 14c. Since the hypothesis space is defined on the unit-hypercube, \u03b2 = \u03c0/2 and for the Rashomon parameter \u03b8 = 0 the Rashomon ratio is:\nR\u0302ratio(F , 0)) = \u03b1\n\u03b2 = 2 maxf\u2208R\u0302set(F\u2126,0) |arctan(f\u03c9\u0302)\u2212 arctan(f\u03c9)| \u03c0/2 .\nFor data sets S1 and S2 Figures 14d and 14e show the Rashomon set and angles \u03b11 and \u03b12 that represent the Rashomon volume. Given the special points in the data sets we can compute \u03b11 and \u03b12 exactly: \u03b11 = 2 (arctan(1)\u2212 arctan(1\u2212 2a)) = \u03c02 \u2212 2 arctan(1 \u2212 2a) and \u03b12 = 2 ( arctan(1)\u2212 arctan ( a\n1\u2212a )) = \u03c02 \u2212 2 arctan ( a 1\u2212a ) . Then the Rashomon ratios\ndifference is:\n|RratioS1 (F , 0)\u2212RratioS2 (F , 0)| = \u2223\u2223\u2223\u2223\u03b11 \u2212 \u03b12\u03c0/2 \u2223\u2223\u2223\u2223 = \u2223\u2223\u2223\u2223 4\u03c0 ( arctan(1\u2212 2a)\u2212 arctan ( a 1\u2212 a ))\u2223\u2223\u2223\u2223 = \u2223\u2223\u2223\u2223 4\u03c0 arctan ( 1\u2212 4a\u2212 2 2a2 \u2212 1\n)\u2223\u2223\u2223\u2223 . Now if we choose a \u2208 (0, 1) and such that\n\u2223\u2223\u2223 4\u03c0 arctan(1\u2212 4a\u221222a2\u22121)\u2223\u2223\u2223 > \u03bb, then the Rashomon ratio difference |RratioS1 (F , 0)\u2212RratioS2 (F , 0)| is at least \u03bb."}, {"heading": "E.3 Proof of Theorem 21", "text": "Theorem 21 (Rashomon ratio and local Rademacher complexity) For 0 < \u03bb < 1, there exist two data sets S1 and S2, a hypothesis space F\u2126, and a Rashomon parameter \u03b8 such that the local Rademacher complexities defined on the Rashomon sets for S1 and S2 are the same:\nR\u0302S1n ( R\u0302set(F\u2126, \u03b8) ) = R\u0302S2n ( R\u0302set(F\u2126, \u03b8) ) ,\nyet the Rashomon ratios are different:\u2223\u2223\u2223RratioS1 (F\u2126, \u03b8)\u2212RratioS2 (F\u2126, \u03b8)\u2223\u2223\u2223 > \u03bb."}, {"heading": "Proof", "text": "Consider two-dimensional separable symmetric data, X \u2208 [0, 1]2, Y = {0, 1}, 0-1 loss \u03c6f (x, y) = 1[y=signf(x)] with empirical risk minimizer f\u0302 , and a hypothesis space F\u2126 of decision stumps based on the first feature, where for f \u2208 F\u2126: f = 1 if x1 > \u03c9, \u03c9 \u2208 R, f = 0\notherwise. We have a one-to-one correspondence between a function and its threshold parameter \u03c9. Therefore, if the Rashomon set is a single connected component, we can compute the Rashomon volume in a parameter space by computing the difference between the largest and smallest threshold values of models within the Rashomon set, as illustrated in Figure 15a. For \u03b8 = 0, the difference between the largest and the smallest threshold values will be equivalent to the minimal distance between points of opposite classes projected onto the first feature d = minxi,xj :yi 6=yj |PR1(xi) \u2212 PR1(xj)|, where PR1 is the projection of point x onto first feature.\nFor the hypothesis space, we consider all decision stumps in the first dimension that are in the segment [0, 1], where data are populated. The difference in thresholds for the hypothesis space is \u03b2 = 1 and therefore V(F\u2126) = 1. For \u03b8 = 0, the Rashomon volume will be equivalent to d\u2014the projected minimal distance between points of opposite classes, and have that V(R\u0302set(F\u2126, 0)) = d and R\u0302ratio(R\u0302set(F\u2126, 0)) = d1 = d. Now consider any two separable symmetric data sets S1, S2 with different projected minimal distances d1 and d2, such that |d1 \u2212 d2| > \u03bb. (Please see Figure 15c and 15d for details of the data sets S1 and S2.) Consequently we get that:\u2223\u2223\u2223RratioS1 (F\u2126, 0)\u2212RratioS2 (F\u2126, 0)\u2223\u2223\u2223 = |d1 \u2212 d2| > \u03bb.\nFor a separable symmetric data S and 0-1 loss function, the Rashomon set R\u0302set(F\u2126, 0) contains all models that separate data in the same way. Therefore the Rademacher complexity of the Rashomon set is R\u0302Sn ( R\u0302set(F\u2126 ) is:\nR\u0302Sn ( R\u0302set(F\u2126, 0) ) = 1\nn E\u03c3\n[ sup\nf\u2208R\u0302set(F\u2126,0) n\u2211 i=1 \u03c3if(xi)\n] = 1\nn E\u03c3 [ n\u2211 i=1 \u03c3if\u0302(xi) ] = 0,\nwhere in the penultimate equality we have used the fact that, in the case of separable data and \u03b8 = 0, all models in the Rashomon set will perform identically on any permutation of the labels.\nEquality of the empirical Rademacher complexity of the optimal model to zero follows from the symmetric data considered and symmetrical patterns of all possible target assignments. For example, for a toy data set in Figure 15b: R\u0302Sn ( R\u0302set(F\u2126, 0) ) =\n1 2 1 4 (( f\u0302(x1) + f\u0302(x2) ) + ( f\u0302(x1)\u2212 f\u0302(x2) ) + ( \u2212f\u0302(x1) + f\u0302(x2) ) + ( \u2212f\u0302(x1)\u2212 f\u0302(x2) )) = 0. Since both S1 and S2 are separable and symmetric we get that:\nR\u0302S1n ( R\u0302set(F\u2126, 0) ) = 0 = R\u0302S2n ( R\u0302set(F\u2126, 0) ) ."}, {"heading": "Appendix F. Data Set Descriptions", "text": "We provide a description of the data sets used in our experiments in Table 5. All of them we downloaded from the UCI machine learning repository. We show the number of features\nin each data set, sizes of the data set and any preprocessing steps that we used mainly to convert data to binary classification. For each data set, we perform cross-validation over ten folds for data sets with more than 200 points and over five folds for data sets with less than 200 points. We reserve one fold for testing, one for validation (e.g., hyper-parameter optimization) and the rest for training. All of the real-valued data sets were normalized to fit the unit-cube, and we did not standardize the data. During data processing, we omitted data records with missing values. We also omitted non-numerical features (e.g., date or text) when there was not a natural way to convert them to categorical features.\nAdditionally, we performed experiments on twelve synthetic binary classification data sets. These data sets have two real features and represent different geometrical concepts for two-dimensional classification (e.g., large and small margins, concentric circles, half moons, etc.) as in Figure 16. Results and implications for synthetic data sets are consistent with those on the UCI data sets.\nTable 6 describes regression data sets, including characteristics and pre-processing notes, that were used in regression experiments."}, {"heading": "Appendix G. Quality of the Features", "text": "In our experiments, we observed a connection between the quality of the features and Rashomon ratios. The Rashomon ratio, as defined in (1) in its simplest form, is a volume fraction of models that are inside the Rashomon set compared to the models in the hypothesis space. When a data set is augmented with additional features, the size of the hypothesis space grows. If the added features are completely irrelevant (consisting, for instance, of noise) then adding these features increases the size of the hypothesis space but does not increase the size of the Rashomon set. Thus, we might predict that the Rashomon ratio could decrease as irrelevant features are added to a data set.\nAdditionally, if we augment a data set with features that are highly correlated or identical to features that improve performance, then not only is the size of the hypothesis space increased, but also the size of the Rashomon set is likely to increase, as there exist more relevant models (even if the set becomes redundant with models that predict equivalently). Thus, we might predict that the Rashomon ratio increases as we add copies of relevant features.\nIn general, these two examples of irrelevant and redundant features are corner cases, however, they do occur to a lesser degree in real world data sets, and we are interested in whether these cases have potentially influenced our experimental results in Section 7 in our observed Rashomon ratios. To investigate this, we augmented a data set with noise features, and separately, augment the same data set with copies of useful features to see whether\nirrelevant or correlated features may have influenced our findings on the measurement of the Rashomon ratio. We used the Breast Cancer Wisconsin (Diagnostic) data set (shortly, BCW), which has approximately six important features. The results are shown in Figure 17. As before, our hypothesis space is decision trees of depth seven.\nIrrelevant features. If the data set contains a lot of irrelevant or noisy features we expect the Rashomon set to be relatively small compared to the hypothesis space. Figure 17(a) shows how the Rashomon ratio changes as we iteratively decrease the number of features in the Wine data set, eliminating the least relevant features first, leaving the most significant ones (where relevance is determined according to a \u03c72 test with the label). The Rashomon ratio grows as we first remove non-significant features, and after reaching a peak at around six features, it starts to decrease as we remove relevant features, and as models lose accuracy. Similarly, Figure 17(b) shows the influence of noisy features on the Rashomon ratio. Particularly, as we add more noisy irrelevant features, the Rashomon ratio starts to decrease. This is due to the same fact, that we artificially enlarge the hypothesis space while keeping the Rashomon set approximately the same. The noise features do not help improve the empirical risk, they only increase the size of the reasonable set.\nRedundant features. As a contrast to how we increased the hypothesis space in the previous experiment, we can increase the Rashomon set by adding more redundant, good features. Figure 17(c) shows how the Rashomon ratio changes for the BCW data set as we add more copies of the four the most significant features. We observe that the Rashomon ratio increases. By adding copies of relevant features, we increased the number of tree at a given depth that could be good enough to be in the Rashomon set.\nOur findings show a possible connection between the Rashomon ratio and feature analysis. In particular, in the case where different algorithms perform similarly, but the Rashomon ratio is observed to be small, it could be due to the reason that the data set contains noisy or irrelevant features. In that case, it may be possible to iteratively remove features to find those that produce the largest Rashomon ratio without changes to the empirical risk. The other extreme is less likely to be observed in practice, which is when the Rashomon ratio is extremely large due to redundant features. In that case, one could remove redundant (highly correlated) features before measuring the Rashomon ratio. The data sets with smaller numbers of features induce easier learning/optimization problems in general. As we discussed earlier, the Rashomon ratio would generally not be measured in practice, and would be inferred in other ways. Thus, these results mainly pertain to an understanding of the experiments we did in Section 7 to provide a possible explanation for cases of small observed Rashomon ratios but where all methods perform the same and all functions generalize."}, {"heading": "Appendix H. Performance of Different Machine Learning Algorithms", "text": "and Rashomon Ratio\nFigure 18 and Figure 19 show a performance comparison of different machine learning algorithms with regularization for the categorical and real-valued data sets. Data sets shown in Figures 18 and 19 are shown in decreasing order of the Rashomon ratio, from the highest in Figure 18 to the Rashomon ratios that were so small that we were not able to\nmeasure them, in Figure 19. Figure 21 and Figure 22 show a comparison of the performance of different machine learning algorithms without regularization for the categorical and realvalued data sets. Recall that algorithms with regularization and without regularization have different Rashomon sets, and can thus have different Rashomon ratios. Finally, Figure 20 and Figure 23 shows a performance comparison of different machine learning algorithms for the synthetic data sets with and without regularization.\nAs we mentioned before, we estimate the Rashomon ratio with importance sampling. For the proposal distribution, we generate a tree of depth D by randomly splitting on features. We assign labels to all 2D leaves using the training data. If a leaf contains no training points, it acquires its label from the nearest ancestor that any training data pass through.\nThe probability of sampling any tree from the proposal distribution is pp = pf \u00d7 \u220f2D i=1 1, where pf is the probability of randomly sampling all of the features that comprise the splits of the tree. Our target distribution is a randomly sampled decision tree (both features and leaves) of depth D. Therefore, the probability of sampling a given tree from the target\ndistribution is pt = pf \u00d7 \u220f2D i=1 1 2 , since we have two classification classes, where, as before, pf is the probability of randomly sampling all features used within splits of the tree. Thus,\nfor one tree of depth seven, its importance weight will be ptpp = ( 1 2 )27 \u2248 3 \u00d7 10\u221239. The importance weight clearly dictates the order of magnitude of the Rashomon ratio in our experiments. The smallest possible non-zero Rashomon ratio (\u2248 1.175 \u00d7 10\u221242%) arises when we sample one model that is in the Rashomon set among 250,000 total models that were sampled. Therefore, we consider the Rashomon ratios of order 10\u221237% and 10\u221238% to be large, and Rashomon ratios of order 10\u221240%, 10\u221241%, etc., to be small.\nIf we choose another importance sampling method (for example with data assignment for only half of the leaves or with the guidance of both features and leaves) the Rashomon ratio may have different importance weights and therefore might have a different estimated size as well. This issue would be resolved if we sample a huge number of trees, which is hard to do in practice. Therefore, since our goal is to compare the Rashomon ratios across themselves, we use the same consistent method of leaf-based importance sampling across all data sets and sample a manageable number of trees (250,000 in our case).\nAppendix I. Proof of Proposition 23\nProposition 23 For a hierarchy of discrete hypothesis spaces F1 \u2282 F2 \u2282 \u00b7 \u00b7 \u00b7 \u2282 FT such that there exists f1 \u2208 R\u0302set(FT , \u03b8)\u2229F1 6= \u2205, if Assumption 22 holds for B1 = {f1} then the Rashomon set R\u0302set(FT , \u03b8) contains at least C T\u22121 C\u22121 models.\nProof Consider B1 = {f1}, where f1 \u2208 R\u0302set(FT , \u03b8) \u2229 F1 6= \u2205. Given the assumptions, there exists at least C distinct functions f i2 that form B2 and are in the Rashomon set (since L\u0302(f1) \u2265 L\u0302(f i2)). Furthermore, given B2 there exists at least |B1| \u00d7 C \u2265 C2 more distinct functions in F3 that also belong to the Rashomon set, which now contains at least 1 +C+C2 models. Continuing to propagate the growth assumption through the hierarchy, at level T , the Rashomon set will have at least 1+C+C2+C3+\u00b7 \u00b7 \u00b7+CT\u22121 = CT\u22121C\u22121 models."}, {"heading": "Appendix J. Rashomon Curve Plots for All Data Sets", "text": "Figures 24, 25 and Figures 26, 27, 28 show the Rashomon curves with generalization error for all categorical and real-valued data sets respectively. In columns (b) and (d) in the figures, we additionally show the Rashomon curves, where the reference model is the CART model if no better-performing sampled tree was found. In most of the cases, the CART model achieves better performance when the Rashomon ratio is small and we were not able to approximate it by sampling. We plot short Rashomon curves (the part we were ale to\ncompute) for such datasets and indicate the training and test accuracies for the rest of the hierarchy of hypothesis spaces in a lower subplot.\nOften in our experiments, the Rashomon curves that leverage CART (columns (b) and (d)) yield curves similar to the sampling-based Rashomon curves (columns(a) and (c)), especially when the Rashomon set is larger (e.g., HTRU 2 in Figure 28, Skin Segmentation in Figure 27). In other cases, CART-based Rashomon curves show a more tilted vertical trend (e.g., Car Evaluation in Figure 25) or visualize a small Rashomon ratio (e.g., Monks-2 in Figure 24).\nFigures 29 and 30 show the elbow on the Rashomon curve for two-dimensional synthetic data sets. Finally, Figure 31 shows the Rashomon trend for a hierarchy of polynomial hypothesis spaces for ridge regression."}, {"heading": "Appendix K. Possible Ways to Compute the Rashomon Elbow", "text": "Let us create some simple ways to formalize how we might find the elbow. For a fixed \u03b8, the elbow is the hypothesis space He with the highest Rashomon ratio among all model classes in the hierarchy that can approximately minimize the empirical risk as in Equation 5. The location of the Rashomon elbow can be found be solving an approximate maximization problem, where G(\u00b7, \u00b7) : R2 \u2192 R is a practitioner-defined balance between accuracy and the Rashomon ratio. In particular, the elbow can be defined as a hypothesis space He such that:\nHe \u2208 argmax Ht\u2208H1...HT\nG ( 1\u2212 L\u0302(Ht), R\u0302ratio(Ht, \u03b8t) ) . (10)\nHere, G would be chosen by the practitioner to represent the ideal balance between accuracy and the Rashomon ratio, and the same G would be used for potentially many different problems for consistency.\nAs an alternative definition for the Rashomon elbow, we can use a geometric argument, based on intuition provided by Figure 10. Across all hypothesis spaces, the hypothesis space that corresponds to the Rashomon elbow has the largest distance from a line that connects\n.\npoints (L\u0302H0 , R\u0302ratio(H0, \u00b7)) and (L\u0302HT , R\u0302ratio(HT , \u00b7)). This geometrically-defined Rashomon elbow will generally result in the same maximin point as in Equation (10)."}], "title": "A Study in Rashomon Curves and Volumes: A New Perspective on Generalization and Model Simplicity in Machine Learning", "year": 2020}