{
  "abstractText": "The development of fair machine learning models that effectively avert bias and discrimination is an important problem that has garnered attention in recent years. The necessity of encoding complex relational dependencies among the features and variables for competent predictions require the development of fair, yet expressive relational models. In this work, we introduce Fair-A3SL, a fairness-aware structure learning algorithm for learning relational structures, which incorporates fairness measures while learning relational graphical model structures. Our approach is versatile in being able to encode a wide range of fairness metrics such as statistical parity difference, overestimation, equalized odds, and equal opportunity, including recently proposed relational fairness measures. While existing approaches employ the fairness measures on pre-determined model structures post prediction, Fair-A3SL directly learns the structure while optimizing for the fairness measures and hence is able to remove any structural bias in the model. We demonstrate the effectiveness of our learned model structures when compared with the state-of-the-art fairness models quantitatively and qualitatively on datasets representing three different modeling scenarios: i) a relational dataset, ii) a recidivism prediction dataset widely used in studying discrimination, and iii) a recommender systems dataset. Our results show that Fair-A3SL can learn fair, yet interpretable and expressive structures capable of making accurate predictions.",
  "authors": [
    {
      "affiliations": [],
      "name": "Yue Zhang"
    },
    {
      "affiliations": [],
      "name": "Arti Ramesh"
    }
  ],
  "id": "SP:247d9f5c317e3150762d3214025a4cc10d582bf6",
  "references": [
    {
      "authors": [
        "Stephen H Bach",
        "Matthias Broecheler",
        "Bert Huang",
        "Lise Getoor"
      ],
      "title": "Hinge-loss markov random fields and probabilistic soft logic",
      "venue": "Journal of Machine Learning Research (JMLR),",
      "year": 2017
    },
    {
      "authors": [
        "Solon Barocas",
        "Andrew D"
      ],
      "title": "Selbst, \u2018Big data\u2019s disparate impact",
      "venue": "California Law Review,",
      "year": 2016
    },
    {
      "authors": [
        "Stephen Boyd",
        "Neal Parikh",
        "Eric Chu",
        "Borja Peleato",
        "Jonathan Eckstein"
      ],
      "title": "Distributed optimization and statistical learning via the alternating direction method of multipliers",
      "venue": "Foundations and Trends R \u00a9 in Machine learning,",
      "year": 2011
    },
    {
      "authors": [
        "Flavio Calmon",
        "Dennis Wei",
        "Bhanukiran Vinzamuri",
        "Karthikeyan Natesan Ramamurthy",
        "Kush R Varshney"
      ],
      "title": "Optimized pre-processing for discrimination prevention",
      "venue": "Proceedings of the Conference on Advances in Neural Information Processing Systems (NIPS),",
      "year": 2017
    },
    {
      "authors": [
        "L Elisa Celis",
        "Lingxiao Huang",
        "Vijay Keswani",
        "Nisheeth K Vishnoi"
      ],
      "title": "Classification with fairness constraints: A meta-algorithm with provable guarantees",
      "venue": "Proceedings of the Conference on Fairness, Accountability, and Transparency (FAT*),",
      "year": 2019
    },
    {
      "authors": [
        "Michele Donini",
        "Luca Oneto",
        "Shai Ben-David",
        "John S Shawe-Taylor",
        "Massimiliano Pontil"
      ],
      "title": "Empirical risk minimization under fairness constraints",
      "venue": "Proceedings of the Conference on Advances in Neural Information Processing Systems (NIPS),",
      "year": 2018
    },
    {
      "authors": [
        "Julia Dressel",
        "Hany Farid"
      ],
      "title": "The accuracy, fairness, and limits of predicting recidivism",
      "venue": "Science advances,",
      "year": 2018
    },
    {
      "authors": [
        "Golnoosh Farnadi",
        "Behrouz Babaki",
        "Lise Getoor"
      ],
      "title": "Fairness in relational domains",
      "venue": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society (AIES),",
      "year": 2018
    },
    {
      "authors": [
        "Golnoosh Farnadi",
        "Pigi Kouki",
        "Spencer K Thompson",
        "Sriram Srinivasan",
        "Lise Getoor"
      ],
      "title": "A fairness-aware hybrid recommender system",
      "venue": "RecSys Workshop on FATREC,",
      "year": 2018
    },
    {
      "authors": [
        "Michael Feldman",
        "Sorelle A Friedler",
        "John Moeller",
        "Carlos Scheidegger",
        "Suresh Venkatasubramanian"
      ],
      "title": "Certifying and removing disparate impact",
      "venue": "Proceedings of the International Conference on Knowledge Discovery and Data Mining (KDD),",
      "year": 2015
    },
    {
      "authors": [
        "Moritz Hardt",
        "Eric Price",
        "Nati Srebro"
      ],
      "title": "Equality of opportunity in supervised learning",
      "venue": "Proceedings of the Conference on Advances in neural information processing systems (NIPS),",
      "year": 2016
    },
    {
      "authors": [
        "Faisal Kamiran",
        "Toon Calders"
      ],
      "title": "Data preprocessing techniques for classification without discrimination",
      "venue": "Knowledge and Information Systems (KAIS),",
      "year": 2012
    },
    {
      "authors": [
        "Faisal Kamiran",
        "Asim Karim",
        "Xiangliang Zhang"
      ],
      "title": "Decision theory for discrimination-aware classification",
      "venue": "Proceedings of the International Conference on Data Mining (ICDM),",
      "year": 2012
    },
    {
      "authors": [
        "Toshihiro Kamishima",
        "Shotaro Akaho",
        "Hideki Asoh",
        "Jun Sakuma"
      ],
      "title": "Fairness-aware classifier with prejudice remover regularizer",
      "venue": "Proceedings of the Joint European Conference on Machine Learning and Knowledge Discovery in Databases,",
      "year": 2012
    },
    {
      "authors": [
        "Yehuda Koren",
        "Robert Bell",
        "Chris Volinsky"
      ],
      "title": "Matrix factorization techniques for recommender systems",
      "year": 2009
    },
    {
      "authors": [
        "Pigi Kouki",
        "Shobeir Fakhraei",
        "James Foulds",
        "Magdalini Eirinaki",
        "Lise Getoor"
      ],
      "title": "Hyper: A flexible and extensible probabilistic framework for hybrid recommender systems",
      "venue": "Proceedings of the ACM Conference on Recommender Systems (RecSys),",
      "year": 2015
    },
    {
      "authors": [
        "Volodymyr Mnih",
        "Adria Puigdomenech Badia",
        "Mehdi Mirza",
        "Alex Graves",
        "Timothy Lillicrap",
        "Tim Harley",
        "David Silver",
        "Koray Kavukcuoglu"
      ],
      "title": "Asynchronous methods for deep reinforcement learning",
      "venue": "Proceedings of the International Conference on Machine Learning (ICML),",
      "year": 2016
    },
    {
      "authors": [
        "Dino Pedreschi",
        "Salvatore Ruggieri",
        "Franco Turini"
      ],
      "title": "A study of top-k measures for discrimination discovery",
      "venue": "Proceedings of the Annual ACM Symposium on Applied Computing,",
      "year": 2012
    },
    {
      "authors": [
        "Geoff Pleiss",
        "Manish Raghavan",
        "Felix Wu",
        "Jon Kleinberg",
        "Kilian Q Weinberger"
      ],
      "title": "On fairness and calibration",
      "venue": "Proceedings of the Conference on Advances in Neural Information Processing Systems (NIPS),",
      "year": 2017
    },
    {
      "authors": [
        "Sirui Yao",
        "Bert Huang"
      ],
      "title": "Beyond parity: Fairness objectives for collaborative filtering",
      "venue": "Proceedings of the Conference on Advances in Neural Information Processing Systems (NIPS),",
      "year": 2017
    },
    {
      "authors": [
        "Rich Zemel",
        "Yu Wu",
        "Kevin Swersky",
        "Toni Pitassi",
        "Cynthia Dwork"
      ],
      "title": "Learning fair representations",
      "venue": "Proceedings of the International Conference on Machine Learning (ICML),",
      "year": 2013
    },
    {
      "authors": [
        "Brian Hu Zhang",
        "Blake Lemoine",
        "Margaret Mitchell"
      ],
      "title": "Mitigating unwanted biases with adversarial learning",
      "venue": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society (AIES),",
      "year": 2018
    },
    {
      "authors": [
        "Yue Zhang",
        "Arti Ramesh"
      ],
      "title": "Learning interpretable relational structures of hinge-loss markov random fields",
      "venue": "Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI),",
      "year": 2019
    }
  ],
  "sections": [
    {
      "heading": "1 INTRODUCTION",
      "text": "The widespread growth and prevalence of machine learning models for crucial decision-making tasks has raised questions on the fairness of the underlying models. Machine learning models have been mostly employed as a black box with little or no transparency or they are too complex to comprehend for non-experts, which further exacerbates this problem. This has led to an increased interest in creating fair machine learning models. The goal of fairness-aware machine learning is to ensure that the decisions made by models do not discriminate against a certain group(s) of individuals [12, 13, 4].\nFairness has been well studied in the social science and policymaking domains [3] and is emerging as an important area of research in computer science and specifically, the machine learning community. Most existing work on fairness focus on developing metrics to remove biases after prediction and identifying and removing sensitive attributes [13, 15, 22] . There is limited existing work on fairness in relational domains. Farnadi et al.\u2019s [10] work on developing fairness metrics for relational domains and fairness-aware MAP inference for hinge-loss Markov random fields (HL-MRFs) [2] is the first work in this direction. Farnadi et al. [10] note that in many social contexts, discrimination is the result of complex interactions and cannot be described solely in terms of attributes of an individual. While this process is helpful in removing the biases in the inference procedure, it\n1 SUNY Binghamton, USA, email: {yzhan202, artir}@binghamton.edu\nignores the structural biases in the model structure. This is especially relevant for relational models, where the model structure is instrumental in obtaining the predictions and the biases ingrained in the structure are harder to detect and eliminate. Contributions In this work, we develop Fair-A3SL, a fairnessaware structure learning algorithm for hinge-loss Markov random fields (HL-MRFs). Fair-A3SL extends a recently developed deep reinforcement learning-based structure learning algorithm for HL-MRFs, A3SL [26], to automatically learn fair relational graphical model structures. Fair-A3SL has the ability to encode almost all different state-of-the-art widely-used fairness metrics: equalized odds [13], equal opportunity [13], statistical parity difference [16], recently developed relational fairness measures of risk difference, risk reward, and relative chance [10], and fairness measures for collaborative filtering, non-parity and overestimation [23]. Fair-A3SL possesses the ability to encode multiple model-based and post-processing fairness measures in a single algorithm and can jointly optimize for them to learn a fair model structure. It also offers flexibility in encoding and enforcing these measures through user-defined coefficients that capture the impact of these measures, therefore providing the much needed customizability to enable applicability across multiple domains. The added strength of Fair-A3SL arises from its ability to learn interpretable fair structures that do not compromise on performance, further alleviating the problem of opaqueness and lack of interpretability in machine learning models. To the best of our knowledge, ours is the first approach that directly focuses on learning fair relational model structures from data.\nIn our experiments, we demonstrate Fair-A3SL\u2019s versatility in being able to encode many different fairness measures and learn fair models for multiple domains. We evaluate the effectiveness of our learned structures in three datasets: i) paper review dataset, a relational dataset used in Farnadi et al. [10] that showcases the ability of our models to learn fair network and collective model structures, ii) Correctional Offender Management Profiling for Alternative Sanctions (COMPAS) dataset, a popular dataset used in many existing fairness work allowing us to compare Fair-A3SL with many state-of-the-art fairness models, and iii) MovieLens dataset, a popular dataset used in recommender systems, that enables us to integrate fairness measures used in collaborative filtering in Fair-A3SL. Fair-A3SL is able to learn structures that eliminate bias at the structure level, requires minimal pre-processing (no other pre-processing other than what is needed for computing the fairness metrics), and can potentially be used easily in sensitive applications to learn interpretable, expressive, and fair model structures that possess good prediction performance for making accurate predictions."
    },
    {
      "heading": "2 RELATED WORK",
      "text": "The state-of-the-art bias mitigation algorithms can be grouped into three categories that include pre-processing, model-based, and post-\nar X\niv :2\n00 2.\n09 47\n1v 1\n[ cs\n.L G\n] 2\n1 Fe\nb 20\n20\nprocessing methods. Pre-processing methods work by directly mitigating the bias in the training data itself. Examples of this approach include optimized preprocessing [6], which modifies training data features and labels, reweighting [14], which modifies the weights of different training examples, disparate impact remover [12], which edits feature values to improve group fairness, and learning fair representations [24], which learns fair representations by obfuscating information about protected attributes.\nModel-based methods are used to mitigate bias in classifiers; for example, adversarial debiasing [25] uses adversarial techniques to maximize accuracy and reduce evidence of protected attributes in predictions. Prejudice remover [16] adds a discrimination-aware regularization term to the learning objective. Meta Fair Classifier [7] takes the fairness measure as part of the input and returns a classifier optimized for that metric. Our approach falls in this category. Existing approaches only learn the parameter values or apply regularization to lessen the effect of sensitive attributes. The fairness measures are not used to directly induce the structure, hence leaving behind some possibility of bias. Our approach differs from existing approaches in that it directly learns the graphical model structure by optimizing for the fairness measures. Thus, our approach is capable of mitigating structural bias in the model, which helps in creating an overall fairer model.\nThe third class of algorithms focus on post-processing methods to mitigate bias in predictions. For example, reject option classification [15] changes predictions from a classifier to make them fairer. Equalized odds post-processing [13] modifies the predicted labels using an optimization scheme to make predictions fairer. Calibrated equalized odds post-processing [22] optimizes over calibrated classifier score outputs that lead to fair output labels."
    },
    {
      "heading": "3 BACKGROUND FOR FAIR-A3SL",
      "text": "Before delving into the details of Fair-A3SL, we provide necessary background on hinge-loss Markov random fields (HL-MRFs) [2], the probabilistic programming templating language for encoding them, Probabilistic Soft Logic (PSL) [2], and a recently developed structure learning algorithm for learning interpretable relational structures in HL-MRFs, asynchronous advantage actor-critic for structure learning (A3SL) [26]."
    },
    {
      "heading": "3.1 Hinge-loss Markov Random Fields",
      "text": "HL-MRFs are a recently developed scalable class of continuous, conditional graphical models [2]. HL-MRFs can be specified using Probabilistic Soft Logic (PSL) [2], a first-order logic templating language. In PSL, random variables are represented as logical atoms and weighted rules define dependencies between them of the form: \u03bb : P (a) \u2227 Q(a, b) \u2192 R(b), where P, Q, and R are predicates, a and b are variables, and \u03bb is the weight associated with the rule. The weight of the rule r indicates its importance in the HL-MRF model, which is defined as\nP (Y |X ) \u221d exp ( \u2212 M\u2211 r=1 \u03bbr\u03c6r(Y ,X ) )\n\u03c6r(Y ,X ) = (max{lr(Y ,X ), 0})\u03c1r (1)\nwhere P (Y |X ) is the probability density function of a subset of logical atoms Y given observed logical atoms X, \u03c6r(Y ,X ) is a hinge-loss potential corresponding to an instantiation of a rule r, and is specified by a linear function lr and optional exponent \u03c1r \u2208 {1, 2}. HL-MRFs admit tractable MAP inference regardless of the graph structure of\nthe graphical model, making it feasible to reason over complex userspecified dependencies. This is possible because HL-MRFs operate on continuous random variables and encode dependencies using potential functions that are convex, so MAP inference in these models is always a convex optimization problem. Farnadi et al. [10] extend the MAP inference algorithm to be able to maximize the a-posteriori values of unknown variables subject to fairness guarantees.\nOur approach to learning fair structures focuses on learning logical constructs that particularly bring out the modeling capabilities in HL-MRFs. Below, we provide examples from two datasets we use in our experiments, a relational paper review dataset and a correctional center recidivism prediction dataset: 1. Relational Dependencies and Collective Rules: highQuality(P) \u2227 positiveReviews(R1,P)\u2192 positiveReviews(R2,P), which captures if paper P is of high quality and reviewer R1 gives the paper a positive review, then reviewer R2 also gives the paper a positive review. Note that positiveReviews is a target predicate and this rule collectively predicts it for both the reviewers. 2. Feature Dependencies: priorFelony(U, I)\u2227 africanAmerican(U)\u2192 recidivism(U), which captures (unfairly) that if user U has committed a prior felony I and the race of the user is African American, the user has a higher chance of recidivism. These two features come together to predict recidivism."
    },
    {
      "heading": "3.2 Asynchronous advantage actor-critic structure learning (A3SL) for HL-MRFs",
      "text": "Asynchronous advantage actor-critic structure learning algorithm (A3SL) [26], a recently developed structure learning algorithm for HL-MRFs, adapts a neural policy gradient algorithm asynchronous advantage actor-critic (A3C) [20] for the structure learning problem. A3SL learns interpretable and expressive structures for HL-MRFs by finding the clause set C and corresponding weight vector \u039b that maximizes the objective: JA3SL = L(Y,X) + Interpretability Priors, where L(Y,X) is the HL-MRF probability density, logP (Y |X), given by Equation 1. Interpretability Priors consist of a combination of priors on the total number of clauses, the maximum possible length of a clause, and domain-specific semantic constraints. The inclusion of semantic constraints and a performance-based utility function allows the algorithm to learn structures that are interpretable and data-driven, thus optimizing for both while being able to rectify any domain-specific intuitions that are not true in the data. The objective function JA3SL is defined as,\nJA3SL = ( L(Y,X)\u2212 \u03b1len \u2217 1 |C| \u2211 c\u2208C length(c)\n\u2212 \u03b1num \u2217 |C| \u2212 \u03b1sem \u2217 \u2211 c\u2208C (Dist(c) \u2217 \u03bbc) )\n(2)\nwhere \u03b1len, \u03b1num, and \u03b1sem parameters denote the strength of the different constraints, \u03bbc denotes the weight for PSL clause c, and Dist(c) denotes the deviation of clause c from semantic constraints (discussed more in Section 4.4). We refer the reader to [26] for additional details."
    },
    {
      "heading": "4 FAIR-A3SL: FAIRNESS-AWARE STRUCTURE LEARNING FOR HL-MRFS",
      "text": "In this section, we develop Fair-A3SL by incorporating the different fairness measures in the A3SL problem formulation and objective. We first introduce the Fair-A3SL algorithm and then describe all the fairness-related components in the algorithm in detail in the following sections."
    },
    {
      "heading": "4.1 Fair-A3SL algorithm",
      "text": "Algorithm 1 gives the Fair-A3SL algorithm. The algorithm follows an actor-critic reinforcement learning setup to learn the clause list C at each step. Our environment consists of predicates for features (denoted by X), target variables (Y), and data corresponding to X and ground truth data for Y. And each intermediate state st at time t comprises of either a partially constructed or a complete set of first order logic clauses, denoted by C. Our action space is defined by all the predicates X, Y, and their negative counterparts, and a special token END. At time t, action at adds a new predicate to the current clause or chooses to return the clause by adding an END.\nAlgorithm 1 Fair-A3SL algorithm Input: A collection of predicates, X = {xj ; j = 1, ...,m}, Y = {yj ; j = 1, ..., n}, , Ground truth labels Yg for Y Let C = {c0, c1, .., cM} denote set of first-order logic clauses, and corresponding weights \u039b Let Clist denote list of C obtained with reward > 0. Output: Optimal C denoted by C\u2217\n1: function C\u2217 = Fair-A3SL(Y ,X) 2: for each thread asynchronously do 3: Construct clause list C under A3SL agent policy 4: Initialize weights \u039b for C 5: Perform weight learning and update \u039b. 6: Perform fairness-aware inference and get Y\u0302 /* MAP inference with fairness constraints */ 7: Obtain reward Utility(Y , Y\u0302 ) = logP (Y,X) - \u03b1\u2217 fairness priors 8: Add C to Clist 9: Accumulate gradients and update policy and value function\nparameters according to new state C 10: C\u2217 = optimal C from Clist 11: return C\u2217\nIn the Fair-A3SL algorithm, we present two main ways of encoding the fairness measures: i) as MAP inference constraints, and ii) as priors in the objective function. The fairness measures encoded as constraints are integrated as linear inequality constraints in the MAP inference for HL-MRFs; we present more details in Section 4.2. Step 6 in Algorithm 1 captures this step, where fairness-aware inference subject to the fairness MAP inference constraints is performed.\nTo include fairness measures as priors, we turn to the reward/utility function in Step 7 of Algorithm 1. The immediate reward rt is equal to the value of objective function at step t if the clause set construction is complete; rt equals 0 otherwise. The cumulative reward Rt =\u2211\u221e k=0 \u03b3rt+k is equal to the value of the objective function, where \u03b3 is the discount factor, and we set it to 1 in all our experiments. The fairness measures encoded as priors are integrated in the reward utility function, the new utility after incorporating the priors becomes Utility(Y, Y\u0302 ) = logP (Y,X)\u2212 \u03b1 \u2217 fairness priors, where P(Y |X ) is the HL-MRF objective given by Equation 1 and \u03b1 denotes the strength of the fairness prior(s). The algorithm returns the clause list with the best accumulated reward calculated using the utility function as the optimal clause list C\u2217."
    },
    {
      "heading": "4.2 Fairness aeasures as MAP inference constraints",
      "text": "Here, we discuss how to integrate different fairness measures as MAP inference constraints. First, we start with the assumption that we are given a dataset consisting of n samples {(Ai, Xi, Yi)}ni=1. Here, A denotes one or more sensitive attributes such as gender and race,\nX denotes other non-sensitive features, and Y denotes the groundtruth labels. We group instances or users based on their sensitive attributes into two groups, protected and unprotected. We then define, a = \u2211 x\u2208protected group \u00acY\u0302 (x), c = \u2211 x\u2208unprotected group \u00acY\u0302 (x), g1 = |protected group|, g2 = |unprotected group|. Y\u0302 refers to a positive prediction (e.g., acceptance) and \u00acY\u0302 refers to a negative prediction (e.g., denial) from the trained model. The proportions of denial for protected and unprotected groups are p1 = ag1 and p2 = c g2\n, respectively, where g1 and g2 are constants [10, 21].\nFollowing Farnadi et al.\u2019s the definition of \u03b4-fairness, the fairness measures can be defined in terms of p1 and p2 as follows, where 0 \u2264 \u03b4 \u2264 1,\nRisk difference: RD = p1 \u2212 p2; \u2212\u03b4 \u2264 RD \u2264 \u03b4 Risk Ratio: RR = p1 p2 ; 1\u2212 \u03b4 \u2264 RR \u2264 1 + \u03b4 Relative Chance: RC = 1\u2212 p1 1\u2212 p2 ; 1\u2212 \u03b4 \u2264 RC \u2264 1 + \u03b4\nThe \u03b4-fairness constraints above translate to six linear inequality constraints in the HL-MRF framework. For example, the linear inequality constraints l1(Y,X) and l2(Y,X) defined for satisfying the inequality \u2212\u03b4 \u2264 RD \u2264 \u03b4 have the forms shown below, where x1,..., xg1 are instances in the protected group, and xg1+1, ..., xg1+g2 are instances in the unprotected group, and the total number of instances n = g1 + g2.\nl1 \u21d2 RD \u2264 \u03b4 \u21d2 ( g2...g2,\u2212g1, ...,\u2212g1 ) \u2217  Y\u0302 (x1) Y\u0302 (x2) ...\nY\u0302 (xn)\n \u2265 \u2212g1g2\u03b4\nl2 \u21d2 RD \u2265 \u2212\u03b4 \u21d2 ( g2, ..., g2,\u2212g1, ...,\u2212g1 ) \u2217  Y\u0302 (x1) Y\u0302 (x2) ...\nY\u0302 (xn)  \u2264 g1g2\u03b4 Next, we consider a fairness metric for collaborative filtering [23]:\nnon-parity unfairness. Non-parity unfairness is defined as the absolute difference between the overall predicted average ratings of protected users and those of unprotected users:\nUpar = |Eprotected[Y\u0302 ]\u2212 Eunprotected[Y\u0302 ]|\nEprotected[Y\u0302 ] = 1\ng1 \u2211 {(i,j)|i\u2208protected group} Y\u0302i,j\nEunprotected[Y\u0302 ] = 1\ng2 \u2211 {(i,j)|i\u2208unprotected group} Y\u0302i,j\nwhere Y\u0302 is the prediction, g1 is the total rating by protected users and g2 the total rating by unprotected users. Below, we demonstrate how to capture non-parity unfairness in Fair-A3SL as a MAP inference constraint. We get the corresponding \u03b4-fairness linear inequality constraints l3 and l4 below, where n represents number of users u, m represents number of items v.\nl3 \u21d2 Upar \u2265 \u2212\u03b4\n\u21d2 ( g2...g2,\u2212g1, ...,\u2212g1 ) \u2217  Y\u0302 (u1, v1) Y\u0302 (u1, v2) ...\nY\u0302 (un, vm)\n \u2265 \u2212g1g2\u03b4\nl4 \u21d2 Upar \u2264 \u03b4\n\u21d2 ( g2...g2,\u2212g1, ...,\u2212g1 ) \u2217  Y\u0302 (u1, v1) Y\u0302 (u1, v2) ...\nY\u0302 (un, vm)  \u2264 g1g2\u03b4 The linear form of the constraints is consistent with MAP inference in HL-MRF model; they can be seamlessly solved using a consensusoptimization algorithm based on the alternating direction method of multipliers (ADMM) [5]. To accomplish this, we extend the consensus optimization algorithm by Bach et al. [2] for MAP inference in HLMRFs to include above defined fairness linear inequality constraints.\nSimilarly, other fairness measures can also be incorporated in the Fair-A3SL framework as constraints. Statistical Parity Difference measures the difference of the rate of favorable outcomes received by the unprivileged group to the privileged group [16] and Disparate Impact measures the ratio of rate of favorable outcome for the unprivileged group to that of the privileged group [12]. Both these measures are similar to the relative chance (RC) relational measure and can be encoded similarly."
    },
    {
      "heading": "4.3 Fairness measures as objective priors",
      "text": "While certain fairness measures can be modeled as MAP inference constraints in the framework, the post-processing fairness measures can only be modeled as priors in our objective due to the absence of ground truth for target Y at test time as discussed below.\nEqualized Odds Difference [13] measures the difference of false positive rate and true positive rate between unprivileged and privileged groups, which can be defined as \u2211 y\u2208{0,1} |Pr(Y\u0302 = 1|A = 0, Y = y)\u2212Pr(Y\u0302 = 1|A = 1, Y = y)|, where Y\u0302 is the predicted value and Y is the ground truth. We cannot directly incorporate this measure as a MAP inference constraint since at test time the true value of Y is not available. This measure and other similar post-processing measures that rely on true ground-truth labels can be encoded as priors in the Fair-A3SL algorithm. We integrate the priors in the objective function, which then is used in computing the agent\u2019s rewards in the Fair-A3SL algorithm as discussed in Section 4.1.\nOverestimation unfairness measures inconsistency in how much the predictions overestimate the true ratings [23]. This fairness measure is used in the collaborative filtering setting. Following equations give the formula for Uover and the expectation for the protected group Eprotected. The average for Eunprotected is computed analogously.\nUover = 1\nm m\u2211 j=1 |max(0, Eprotected[Y\u0302 ]j \u2212 Eprotected[Y ]j)\n\u2212max(0, Eunprotected[Y\u0302 ]j \u2212 Eunprotected[Y ]j)|\nEprotected[Y\u0302 ]j = 1 |{(i, j)|i \u2208 protected}| \u2211\ni\u2208protected\nY\u0302i,j\nEqual Opportunity Difference measures the difference of true positive rates between the unprivileged and the privileged groups [13]. Average Odds Difference [1] measures the average difference of false positive rate and true positive rate between unprivileged and privileged groups. These measures are comparable to the Equalized Odds Difference measure and can be similarly encoded as priors."
    },
    {
      "heading": "4.4 Domain-specific semantic constraints",
      "text": "An interpretable model lays the foundation for fairness and transparency. In addition to inducing fairness-aware relational structures,\nwe also include semantically meaningful domain constraints that do not contain any structural bias and encourage the algorithm to learn interpretable structures. This is helpful in making the resulting model more appealing to end users. Here, we show how to group predicates and their negative counterparts into two categories, positive signals and negative signals using the semantic interpretation of the predicate. If the user is unsure about the semantics of any predicate, they can be incorporated in both the categories to avoid any unintentional bias.\nWe illustrate this using the COMPAS dataset, one of the datasets widely used in fairness studies and also in our experiments. We capture positive signals P={priorFelonHistory, priorMisdemeanorHistory, priorOtherHistory, juvFelonHistory, juvMisdemeanorHistory, juvOtherHistory, priors, felony, recidivism, \u00acoldAge, longJailDay, \u00aclongJailDay} that capture tendency toward recidivism and negative signals N={\u00acfelony, \u00acrecidivism, oldAge, longJailDay, \u00aclongJailDay} that capture tendency against recidivism. Since at first we are not sure about the effect of longJailDay and its negative counterpart on recidivism prediction from domain knowledge, we place it in both categories. The domain-specific semantic constraints have the general structure in Table 1, where positive signals \u2286 P , negative signals \u2286 N , and any positive signal \u2208 P , negative signal \u2208 N . We use a distance function, Dist(c) to capture if the learned clause structure complies with or deviates from the right reasons identified by the expert: Dist(c) = 0, if the clause complies with the right reasons and Dist(c) = 1, otherwise. This distance function is then integrated in the objective functions discussed in Section 4.5. If the domain-specific guidance is not readily available for the specific domain, the model is able to work without them as well as they are added only to enhance interpretability when appropriate."
    },
    {
      "heading": "4.5 Fair-A3SL objective functions",
      "text": "We present two different objective functions that we use across our three predictive modeling scenarios that demonstrates how a combination of fairness constraints, fairness priors, and semantic constraints can be represented in an objective function. This objective can be easily modified to include/exclude specific fairness/semantic constraints or fairness priors."
    },
    {
      "heading": "4.5.1 Fair-A3SL objective for relational models",
      "text": "In the first objective, we use a combination of fairness measures both encoded as constraints and as priors. Here, we encode the relational fairness measures RR, RC, and RD as MAP inference constraints and the equalized odds difference measure as a prior in the objective along with interpretability priors for the specific domain in question. Equation 3 gives the Fair-A3SL objective function corresponding to this combination. We use this objective function in our experiments in Section 5.1 on the relational dataset and in Section 5.2 on the recidivism prediction dataset.\nJFair-A3SL = logP (Y,X) + Interpretability Priors\n+ \u03b1odds \u2217 Uodds s.t.\u2212 \u03b4 \u2264 RD \u2264 \u03b4\n1\u2212 \u03b4 \u2264 RR \u2264 1 + \u03b4 1\u2212 \u03b4 \u2264 RC \u2264 1 + \u03b4 (3)\nwhere Uodds refers to the equalized odds difference fairness measure and \u03b1odds captures its degree of enforcement."
    },
    {
      "heading": "4.5.2 Fair-A3SL objective for recommender systems",
      "text": "For the recommender systems problem, we turn to the corresponding fairness measures of overestimation and non-parity. Equation 4 gives the Fair-A3SL objective for recommender systems. As is evident from the equation, here again we include a combination of constraints and priors in the objective; we incorporate the non-parity fairness measure as a MAP inference constraint (Upar) and overestimation as an objective prior (Uover). We use this objective for the experimental results in Section 5.3.\nJFair-A3SL = logP (Y,X) + \u03b1over \u2217 Uover s.t.\u2212 \u03b4 \u2264 Upar \u2264 \u03b4 (4)"
    },
    {
      "heading": "4.6 Highlights of Fair-A3SL",
      "text": "Our approach to fairness is versatile in its ability to encode many different fairness measures toward directly learning the graphical model structure. Fair-A3SL provides the capability of encoding fairness measures as constraints and/or as priors and has minimal pre-processing requirements (only those imposed by the underlying fairness measures). While many existing work indicate the importance of combining fairness measures for practitioners, they also note that there is often a trade-off between various fairness measures and it is challenging to construct a single fairness objective that performs well across different measures [23, 11]. While this remains true for conflicting measures, Fair-A3SL is a step in the right direction, where we present a platform that can incorporate a combination of fairness metrics while simultaneously optimizing for them. In Equations 3 and 4, we show some possible combinations and our results indicate Fair-A3SL can indeed optimize for multiple fairness metrics at the same time. These desirable qualities in Fair-A3SL can potentially help downstream users such as policy makers and decision making organizations (e.g., bank loans, student admissions) to successfully adopt the framework."
    },
    {
      "heading": "5 EXPERIMENTAL EVALUATION",
      "text": "We conduct experiments to evaluate the learned structures quantitatively and qualitatively on three fairness datasets. In our experiments, we illustrate the capability of Fair-A3SL to be able to: i) learn fair network and collective structures that bring out the modeling power of statistical relational models, ii) incorporate a wide range of fairness measures and learn model structures using them, and iii) learn model structures that outperform state-of-the-art fairness models both across performance and fairness metrics and are qualitatively meaningful. The Fair-A3SL code and the code for experiments will be made publicly available when the paper is accepted for publication. The best scores and those that are statistically indistinguishable from the best are typed in bold in all the results. All experiments use 5-fold cross-validation."
    },
    {
      "heading": "5.1 Results on relational paper review dataset",
      "text": "We first present results on a paper reviewing problem that can potentially be biased by the author\u2019s affiliation instead of the quality of the paper. We follow Farnadi et al. [10] to generate a similar dataset to theirs in order to facilitate a direct comparison. Table 2 gives the conditional probability distribution table (left) and the\nBayesian network (right) that we use for generating the data. Two specific scenarios parametrized by P(H) that determine the degree of discrimination are: i) probability of the paper receiving a favorable rating given the paper is of high quality and the author is not from a top ranked institution (\u03b81 = P (R1|Q = T,H = F, S = T )), and ii) probability of the paper receiving a favorable reviewer rating given the paper is of high quality and the author is from a top ranked institution (\u03b82 = P (R1|Q = T,H = T, S = T )). We introduce bias in the data when the author is a student (S = T) by setting \u03b81 = 0.5 and \u03b82 = 0.9. We set P (R1|Q = T,H = F, S = F ) and P (R1|Q = T,H = T, S = F ) to 0.85. The train and test dataset both contain data generated using the Bayesian network comprising of 100 papers, 100 authors, 30 reviewers, and each paper is reviewed by 2 random reviewers.\nTable 3 gives the learnt rules the Fair-A3SL model on the paper review dataset. To enable a comparison with Farnadi et al. [10], we also enhance A3SL by adding the ability to encode collective rules. Collective rules jointly predict two or more target variables. Note that the learned model structure is expressive, learning different kinds of rules: network, collective, and combination of features.\nWe compare Fair-A3SL with the following state-of-the-art baselines: i) Fair-PSL [10], manually-defined PSL rules with fairness constraints in inference, ii) Sensitive-PSL, manually-defined PSL rules with no fairness constraints, and iii) Sensitive-A3SL [26], a model structure learned using A3SL with no fairness constraints or priors. Additionally, we experiment with three versions of Fair-A3SL that use different combinations of fairness measures. Fair-A3SL1 in-\ncludes fairness constraints without equalized odds priors. Fair-A3SL2 includes fairness constraints along with equalized odds priors with \u03b1odds = 0.1. Fair-A3SL3 includes fairness constraints along with equalized odds with \u03b1odds = 0.5. We set \u03b4-fairness=0.1 for all fairness inference inequality constraints. The AUC-ROC values from the Sensitive-A3SL model can be considered an upper bound, as it is a purely data-driven model.\nOur specific focus is on the prediction performance for protected/unprotected groups, especially for predicting a positive outcome in both these groups (Table 4). We report area under the AUCPR curve for the positive class (positiveSummary). From the table, we can see that all A3SL versions outperform the human expert counterparts (Sensitive-A3SL vs. Sensitive-PSL, Fair-A3SL versions vs. Fair-PSL). We can see that the Fair-PSL model even when the fairness measures are included in the inference only achieves a prediction performance of \u223c 0.4, while the Fair-A3SL models achieve > 0.6 for the protected group. The Fair-A3SL models also improve the prediction performance of the unprotected groups when compared to the Fair-PSL model. The combined AUC-ROC value for the Fair-A3SL models is also closer to the models that include sensitive attributes (Sensitive-PSL and Sensitive-A3SL). Similarly, all the Fair-A3SL models achieve better or comparable performance across all fairness metrics (RD, RR, RC, Equalized Odds Positive and Negative) when compared with Fair-PSL with manually defined rules (Table 5). Particularly, for the equalized odds measures, Fair-A3SL models clearly outperform Fair-PSL. We also observe that we get better results for the equalized odds fairness measure when we increase the value of \u03b1odds. Thus, Fair-A3SL is able to achieve fairness without compromising on performance."
    },
    {
      "heading": "5.2 Results on COMPAS dataset",
      "text": "The Correctional Offender Management Profiling for Alternative Sanctions (COMPAS) tool produces a risk score that predicts a person\u2019s likelihood of committing a crime in the next two years [19]. The output is a score between 1 to 10 that maps to low, medium, or high. We collapse this to a binary prediction: a score of 0 corresponds to a prediction of low risk according to COMPAS, while a score of 1 indicates high or medium risk. The dataset also contains information on recidivism for each person over the next two years, which we use as ground truth. Existing work shows that the COMPAS risk scores discriminate against black defendants, who were predicted to be far more likely than white defendants to be incorrectly judged to be at a higher risk of recidivism, while white defendants were more likely than black defendants to be incorrectly flagged as low risk [19, 9].\nTable 6 gives the Sensitive-A3SL model. We can see that the model combines other recidivism signals of having committed prior felonies (priors and priorFelony) with the race attribute (africanAmerican), indicating how the race attribute and combinations with it are predictive of recidivism and are a natural albeit unfair and discriminatory choice for models that are solely performance driven. The rules learned by the Fair-A3SL model are given in Table 7. Parameter U represents user, Ii represents a felony instance. For example, priorFelonHistory(U,I1) can be grounded with multiple historical felony instances I1 for each user U . Fair-A3SL\u2019s transparency, interpretability, expressibility,\nalong with fairness, makes it an ideal candidate for automatically learning prediction models for sensitive domains.\nWe compare Fair-A3SL with recently developed state-of-the-art fairness models: i) Calibrated Equalized Odds [22], ii) Prejudice Remover [16], iii) Optimized Pre-processing [6], iv) Adversarial Debiasing [25], and v) Line-FERM [8], where Calibrated Equalized Odds, Prejudice Remover, and Optimized Preprocessing use logistic regression as the backend model; Adversarial Debiasing uses a deep learning neural network model; and FERM uses SVM as the underlying model. Table 8 gives the 5-fold cross-validation results and shows\nthat Fair-A3SL is able to achieve a better prediction performance for both the protected and unprotected groups, individually (AUC-PR for protected and unprotected groups) and combined (AUC-ROC). We use the IBM AI Fairness 360 tool [1] for running the existing state-of-the-art models. We also demonstrate that our learned model outperforms the state-of-the-art fairness models in the fairness metrics as well, achieving the best scores across all metrics (Table 9)."
    },
    {
      "heading": "5.3 Results on Movielens dataset",
      "text": "In the third experiment, we consider another important domain for fairness, recommender systems. To evaluate the effectiveness of FairA3SL in recommender systems, we use the MovieLens 100k dataset. It consists of ratings from 1 to 5 by 943 users for 1682 movies. The users are annotated with demographic variables such as gender, and the movies are each annotated with a set of genres. For convenience, we convert the ratings to range between values 0 and 1. From Table 10, we can see that women rate musical and romance films higher and more frequently than men. Men rate Sci-Fi and crime films higher and more frequently than women. Women and men both give action films an almost equal rating, but men rate these films more frequently.\nFollowing Kouki et al. [18], we extract features that combines multiple different sources of information, including similarity between pairs of users (userPearsonSim(U,U2)), similarity between items (itemPearsonSim(I, I2)), average rating with respect to users and\nitems to serve as priors (avgUserRating(U ) and avgItemRating(I)), and leveraging predictions from existing recommendation algorithms as a feature (ratingMF (U, I)) to enable an appropriate comparison. Table 11 gives the rules learned by Fair-A3SL.\nWe compare our approach to the state-of-the-art recommender systems baseline models: i) HyPER [18], which is a PSL model and includes hybrid recommender systems feature,; ii) matrix factorization based collaborative filtering model [17], iii) Fair-HyPER [11], which defines additional latent variable rules to abstract the rating of unprotected and protected groups in order to ensure there is no overestimation unfairness, iv) baseline model Fair-MF [23], which considers overestimation and non-parity unfairness as regularization terms. Table 12 shows Fair-A3SL achieves the best overall performance for both the protected and unprotected groups. Table 13 shows that our Fair-A3SL model gets a comparable value in the overestimation unfairness measure, and the best value in the non-parity fairness measure. The model learned by Fair-A3SL achieves comparable performance to Fair-HyPER even without the inclusion of carefully designed latent variables that provide additional complexity."
    },
    {
      "heading": "6 CONCLUSION",
      "text": "In this work, we developed Fair-A3SL, a general purpose fair structure learning algorithm for HL-MRFs and demonstrated that it learns fair, semantically interpretable, and expressive relational structures while achieving good prediction performance. Fair-A3SL is capable of encoding various different measures of fairness both as constraints\nand priors and we demonstrate its effectiveness across three different domains and modeling scenarios. Further, Fair-A3SL has minimal pre-processing requirements (only those posed by the underlying fairness measures) and can seamlessly be utilized to learn models for any sensitive prediction problem including those that require complex relational structures. Fair-A3SL\u2019s joint qualities of fairness, interpretability, and performance make it lucrative for many downstream applications (e.g., bank loans, student admissions) to adopt it."
    }
  ],
  "title": "Learning Fairness-aware Relational Structures",
  "year": 2020
}
