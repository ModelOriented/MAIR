{
  "abstractText": "Algorithmic fairness is receiving significant attention in the academic and broader literature due to the increasing use of predictive algorithms, including those based on artificial intelligence. One benefit of this trend is that algorithm designers and users have a growing set of fairness measures to choose from. However, this choice comes with the challenge of identifying how the different fairness measures relate to one another, as well as the extent to which they are compatible or mutually exclusive. We describe some of the most widely used fairness metrics using a common mathematical framework and present new results on the relationships among them. The results presented herein can help place both specialists and non-specialists in a better position to identify the metric best suited for their application and goals.",
  "authors": [
    {
      "affiliations": [],
      "name": "Pratyush Garg"
    },
    {
      "affiliations": [],
      "name": "John Villasenor"
    }
  ],
  "id": "SP:06638b740c53b992ccfc51ad15834c8e4ecba631",
  "references": [
    {
      "authors": [
        "B. Hutchinson",
        "M. Mitchell"
      ],
      "title": "50 years of test (un)fairness: Lessons for machine learning",
      "venue": "Proceedings of the Conference on Fairness, Accountability, and Transparency, ser. FAT* \u201919. New York, NY, USA: ACM, 2019, pp. 49\u201358. [Online]. Available: http://doi.acm.org/10.1145/3287560.3287600",
      "year": 2019
    },
    {
      "authors": [
        "J. Kleinberg",
        "S. Mullainathan",
        "M. Raghavan"
      ],
      "title": "Inherent trade-offs in the fair determination of risk scores",
      "venue": "2016. [Online]. Available: https://arxiv.org/abs/1609.05807",
      "year": 2016
    },
    {
      "authors": [
        "A. Chouldechova"
      ],
      "title": "Fair prediction with disparate impact: A study of bias in recidivism prediction instruments",
      "venue": "9 28TH JANUARY 2020 2017. [Online]. Available: https://arxiv.org/abs/1703.00056",
      "year": 2020
    },
    {
      "authors": [
        "R. Berk",
        "H. Heidari",
        "S. Jabbari",
        "M. Kearns",
        "A. Roth"
      ],
      "title": "Fairness in criminal justice risk assessments: The state of the art",
      "venue": "Sociological Methods & Research, vol. 0, no. 0, p. 0049124118782533. [Online]. Available: https://doi.org/10.1177/0049124118782533",
      "year": 1878
    },
    {
      "authors": [
        "S. Verma",
        "J. Rubin"
      ],
      "title": "Fairness definitions explained",
      "venue": "Proceedings of the International Workshop on Software Fairness, ser. FairWare \u201918. New York, NY, USA: ACM, 2018, pp. 1\u20137. [Online]. Available: http://doi.acm.org/10.1145/3194770.3194776",
      "year": 2018
    },
    {
      "authors": [
        "C. Dwork",
        "M. Hardt",
        "T. Pitassi",
        "O. Reingold",
        "R. Zemel"
      ],
      "title": "Fairness through awareness",
      "venue": "Proceedings of the 3rd Innovations in Theoretical Computer Science Conference, ser. ITCS \u201912. New York, NY, USA: ACM, 2012, pp. 214\u2013226. [Online]. Available: http://doi.acm.org/10.1145/2090236.2090255",
      "year": 2012
    },
    {
      "authors": [
        "A.D. Selbst",
        "D. Boyd",
        "S.A. Friedler",
        "S. Venkatasubramanian",
        "J. Vertesi"
      ],
      "title": "Fairness and abstraction in sociotechnical systems",
      "venue": "Proceedings of the Conference on Fairness, Accountability, and Transparency, ser. FAT* \u201919. New York, NY, USA: ACM, 2019, pp. 59\u201368. [Online]. Available: http://doi.acm.org/10.1145/3287560.3287598",
      "year": 2019
    },
    {
      "authors": [
        "G. S"
      ],
      "title": "Mayson, \u201cBias in, bias out, studies research paper no. 2018-35.",
      "year": 2019
    },
    {
      "authors": [
        "M. MacCarthy"
      ],
      "title": "Standards of fairness for disparate impact assessment of big data algorithms (april 2, 2018).",
      "year": 2018
    },
    {
      "authors": [
        "D. Hellman"
      ],
      "title": "Measuring algorithmic fairness",
      "venue": "2019. [Online]. Available: https://ssrn.com/abstract=3418528",
      "year": 2019
    },
    {
      "authors": [
        "M. Hardt",
        "E. Price",
        "N. Srebro"
      ],
      "title": "Equality of opportunity in supervised learning",
      "venue": "Advances in Neural Information Processing Systems 29, D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett, Eds. Curran Associates, Inc., 2016, pp. 3315\u20133323. [Online]. Available: http: //papers.nips.cc/paper/6374-equality-of-opportunity-in-supervised-learning.pdf",
      "year": 2016
    },
    {
      "authors": [
        "N. Mehrabi",
        "F. Morstatter",
        "N. Saxena",
        "K. Lerman",
        "A. Galstyan"
      ],
      "title": "A survey on bias and fairness in machine learning",
      "venue": "2019. [Online]. Available: https://arxiv.org/abs/1908.09635",
      "year": 2019
    },
    {
      "authors": [
        "M.J. Kusner",
        "J. Loftus",
        "C. Russell",
        "R. Silva"
      ],
      "title": "Counterfactual fairness",
      "venue": "Advances in Neural Information Processing Systems 30, I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, Eds. Curran Associates, Inc., 2017, pp. 4066\u20134076. [Online]. Available: http://papers.nips.cc/paper/6995-counterfactual-fairness.pdf",
      "year": 2017
    },
    {
      "authors": [
        "S. Corbett-Davies",
        "E. Pierson",
        "A. Feller",
        "S. Goel",
        "A. Huq"
      ],
      "title": "Algorithmic decision making and the cost of fairness",
      "venue": "Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, ser. KDD \u201917. New York, NY, USA: ACM, 2017, pp. 797\u2013806. [Online]. Available: http://doi.acm.org/10.1145/3097983.3098095",
      "year": 2017
    },
    {
      "authors": [
        "G. Pleiss",
        "M. Raghavan",
        "F. Wu",
        "J. Kleinberg",
        "K.Q. Weinberger"
      ],
      "title": "On fairness and calibration",
      "venue": "Proceedings of the 31st International Conference on Neural Information Processing Systems, ser. NIPS\u201917. USA: Curran Associates Inc., 2017, pp. 5684\u20135693. [Online]. Available: http://dl.acm.org/citation.cfm?id=3295222.3295319 10",
      "year": 2017
    }
  ],
  "sections": [
    {
      "heading": "1 Introduction",
      "text": "The risks that algorithms\u2014including those embedded in AI systems\u2014can raise concerns relating to bias are well recognized. Addressing algorithmic bias requires an understanding of normative conceptions of fairness, and how they can be quantitatively measured. Moreover, algorithm designers and users will need to make informed decisions regarding which one or more of multiple possible fairness measures should be used for system assessments.\nWhile fairness measures in relation to issues such as testing have been a topic of academic and broader interest for decades [1], recent years have seen rapidly growing interest among researchers within and beyond the technical community in the issue of algorithmic fairness. The proliferation of papers describing fairness measures has spurred examination of the mathematical relationships among them. For example, Kleinberg et al. [2] considered three fairness measures (calibration within groups, balance for the positive class, and balance for the negative class), and showed that \u201cexcept in highly constrained special cases, there is no method that can satisfy these three conditions simultaneously.\u201d Chouldechova [3] has also written on the incompatibilities between fairness criteria and has observed that test-fairness, a measure \u201coriginating in the field of educational and psychological testing\u201d can, when applied in the context of recidivism prediction, \u201clead to considerable disparate impact when recidivism prevalence differs across groups.\u201d In another paper examining criminal risk assessments [4], Berk et al. have observed that it is generally \u201cimpossible to maximize accuracy and fairness at the same time, and impossible simultaneously to satisfy all kinds of fairness.\u201d\nVerma et al. [5] have \u201ccollect[ed] the most prominent definitions of fairness for the algorithmic classification problem, explain[ed] the rationale behind these definitions, and demonstrate[d] each of them on a single unifying case-study.\u201d In a paper titled \u201cFairness Through Awareness\u201d [6], Dwork et al., considered \u201cfairness in classification, where individuals\n\u2217This research was supported under a grant from the Micron Foundation. This is a draft (as of the date noted above) of a paper that we are submitting for formal publication.\nar X\niv :2\n00 1.\n07 86\n4v 2\n[ cs\n.C Y\n] 2\nare classified . . . and the goal is to prevent discrimination against individuals based on their membership in some group, while maintaining utility for the classifier.\u201d Selbst et al. [7], have considered the broader societal context of seeking fair algorithms, and have argued that attempts to \u201cproduce fairness-aware learning algorithms, and to intervene at different stages of a decision-making pipeline to produce \u2018fair\u2019 outcomes\u201d can be \u201cdangerously misguided when they enter the societal context that surrounds decision-making systems.\u201d\nWhile the papers cited above are drawn from technical publications, algorithmic fairness is also receiving rapidly growing attention in the legal scholarship. Examples include Mayson [8] who has argued that \u201c[a]lgorithmic risk assessment has revealed the inequality inherent in all prediction, forcing us to confront a problem much larger than the challenges of a new technology,\u201d MacCarthy [9], who \u201cdescribe[d] and assesse[d] various group and individual statistical standards of fairness, including the mathematical conflict between the two that requires organizations to choose which measure to satisfy,\u201d and Hellman [10], who has explored the role of \u201cparity in the ratio of false positives to false negatives.\u201d\nAgainst this backdrop, the present paper offers several new contributions. First, as many previous publications each only consider a relatively small subset of fairness measures, differences in terminology and definitions can make comparisons among a broader group of measures difficult to perform. To address this, we describe some of the most widely cited fairness measures using a common mathematical and notational framework. Second and more substantively, we derive and discuss a set of mathematical relationships that facilitate comparisons among these metrics. The remainder of this paper is organized as follows. Section 2 presents metrics using a common mathematical framework and also aims to address some of the terminology variations across authors. Section 3 details the relationships between various metric pairs, including consideration of conditions under which they become mutually incompatible as well as trade-offs involved in selecting one metric over the other. Section 4 offers a discussion and conclusions."
    },
    {
      "heading": "2 Fairness metrics",
      "text": ""
    },
    {
      "heading": "2.1 Definitions",
      "text": "In the initial portion of the discussion herein we focus on binary predictions and their relationship to binary outcomes. We subsequently consider metrics that generate a continuous-valued score. In the binary context we assume that prediction involves a classifier that generates a binary prediction y\u0303 based on a feature vector x containing the data corresponding to a particular individual. If the distribution of y\u0303 over x is the same as that of the binary outcome y, we have perfect prediction. Hence, given a sufficiently large sample, the fairness and other attributes of the predictor can be evaluated by comparing predictions y\u0303 with outcomes y.\nWe denote individuals for whom y = 1 as being members of the \u201cpositive\u201d class and individuals for whom y = 0 as being members of the \u201cnegative\u201d class. For instance, if an algorithm predicts whether students will pass a test, y will be 1 for the students who pass (and who are therefore members of the positive class) and 0 for students who fail (and are thus members of the negative class). While in this scenario the \u201cpositive\u201d outcome y = 1 corresponds to the desirable outcome, this will not always be the case. To take another example, in evaluating whether a parolee commits a new crime within a given time frame, y = 1 can be used to denote the \u201cpositive\u201d (but obviously undesirable) outcome that the parolee has committed a new crime.\nWe also assume that the dataset can be divided into different groups based on attributes such as race, gender, etc., that may be of interest when evaluating whether an algorithm is biased. While there can be any number of such groups in different contexts, we will restrict our discussion to scenarios in which it is possible to identify two groups of individuals, with group membership indicated using a binary variable G. Finally, for some of the metrics we discuss, it will be necessary to consider a real-valued score s \u2208 [0, 1] that is computed for each individual which can optionally then be subject to thresholding to generate a binary prediction y\u0303. In sum, we use the following notation:\n\u2022 x: feature vector\n\u2022 G: binary group index\n\u2022 y\u0303: binary prediction\n\u2022 y: binary outcome\n\u2022 s: classifier score\nWe address some of the metrics that have received significant recent attention in the literature and/or that we believe present opportunities for more detailed comparative analysis. We do not claim to consider all possible fairness metrics (for example, we do not address fairness through awareness as proposed by Dwork et al. [6]).\nTo help clarify the discussion, in what follows we will sometimes refer to the following example involving loans. Consider two groups of people, which we will denote as the orange group (denoted group 0) and the blue group (group 1). The orange group has 60 members while the blue group has 40 members. If all the members of both groups were given loans, 40 members of the orange group and 20 members of the blue group would repay on time\u2014these are the positive outcomes. However, among these 60 positive outcomes, we assume that the model predicted that only 28 of the orange group and 8 of the blue group will repay. These are the true positives for the two groups. Thus, the model correctly predicted the positive outcome of 36 individuals.\nSimilarly, if all members of both groups were given loans, 20 members of the orange group and 20 members of the blue group would default on the loan\u2014these are the negative outcomes. From among these 40 negative outcomes, we assume that the model correctly predicted 12 of the defaulters in the orange group and 16 of the defaulters in the blue group. These are the true negatives for the two groups. In the aggregate, the model correctly predicted the negative outcome for 28 individuals. The table below summarizes these predictions and outcomes. We also add that, to make the illustration of the concepts more tractable, this example and the other numerical examples in this paper use small sample sizes (and in doing so implicitly assume statistical significance despite those small sample sizes). Of course, in a real scenario, to make reliable inferences on statistical outcomes and therefore on fairness assessments based on those outcomes, the sample sizes would need to be much larger."
    },
    {
      "heading": "2.2 Equalized odds and equality of opportunity",
      "text": "A predictor satisfies equalized odds if both the true positive rate (TPR) and (separately) the false positive rate (FPR) are the same across groups. More formally, equalized odds requires that the group-specific TPR satisfy p(y\u0303 = 1|y = 1, G = 0) = p(y\u0303 = 1|y = 1, G = 1) and that the group-specific FPR satisfy p(y\u0303 = 1|y = 0, G = 0) = p(y\u0303 = 1|y = 0, G = 1). [11] Since, the metric demands that the error rates be the same across groups, Chouldechova [3] describes this metric using the term \u201derror rate balance\u201d. In the example, for the orange and blue groups, the TPRs are 0.7 and 0.4 respectively, and the FPRs are 0.2 and 0.4 respectively. Thus, the example fails to satisfy equalized odds.\nIt is also worth noting that a related and less stringent metric, equality of opportunity [11], can be defined by requiring only that the TPR be equal across groups, with no requirement imposed on the FPR. Thus, equalized odds implies equality of opportunity, though not vice versa."
    },
    {
      "heading": "2.3 Statistical parity",
      "text": "Statistical parity [3], [5] (sometimes referred to as group fairness [6] or demographic parity [12], [13]) is achieved when members of both groups are predicted to belong to the positive class at the same rate. Mathematically, this means satisfying p(y\u0303 = 1|G = 0) = p(y\u0303 = 1|G = 1). Notably, this metric gives no consideration to the outcomes y. Therefore, when the base rates p(y|G) differ across the two groups, statistical parity rules out the perfect predictor.\nIn the orange/blue group example, 36 of the 60 orange group members and 12 of the 40 blue group members were predicted to belong to the positive class. Since p(y\u0303 = 1|G = 0) = 36/60 = 0.6 is not the same as p(y\u0303 = 1|G = 1) = 12/40 = 0.3, the example does not satisfy statistical parity."
    },
    {
      "heading": "2.4 Predictive parity",
      "text": "Consistent with Chouldechova [3], Verma et al. [5], and MacCarthy [9] we consider that predictive parity is satisfied when the positive predictive value(PPV) is the same for both groups. PPV is defined as the probability that individuals predicted to belong to the positive class actually belong to the positive class. Mathematically, predictive parity therefore requires p(y = 1|y\u0303 = 1, G = 0) = p(y = 1|y\u0303 = 1, G = 1).\nWe note that some authors define predictive parity in a more constrained manner, requiring not only parity for PPV, but also for its counterpart, negative predictive value (NPV), which requires additionally satisfying p(y = 0|y\u0303 = 0, G = 0) = p(y = 0|y\u0303 = 0, G = 1). Mayson [8] uses the term \u201coverall predictive parity\u201d to describe a predictor with equality across groups in both PPV and NPV, while Berk et al. [4] call this \u201cconditional use accuracy equality\u201d.\nIn the loan example, the model predicted a total of 48 members across both groups to belong to the positive class. However, only 36 out of these were correct predictions; the rest were false positives. Hence, the overall PPV is 36/48 = 0.75. Taken separately, the model predicted a total of 36 members of the orange group and a total of 12 members of the blue group to belong to the positive class. From these predictions, however, only 28 of the orange group and 8 of the blue group were correct. Therefore, the PPV for the orange group is p(y = 1|y\u0303 = 1, G = 0) = 28/36 = 7/9 = 0.77 and for the blue group is p(y = 1|y\u0303 = 1, G = 1) = 8/12 = 0.66. Since these values differ, the model from our example does not satisfy predictive parity.\nThe fairness metrics discussed above can be evaluated using knowledge only of binary predictions and outcomes. By contrast, we now discuss a set of metrics involving explicit generation of a continuous-valued score s. Optionally, the score can serve as the input to a thresholding function that outputs a binary prediction, though scores can also be used directly, without any thresholding."
    },
    {
      "heading": "2.5 Calibration",
      "text": "An algorithm is calibrated if for all scores s, the individuals who have the same score have the same probability of belonging to the positive class, regardless of group membership. [3] [14] Mathematically, this is expressed through p(y = 1|S = s,G = 0) = p(y = 1|S = s,G = 1). This metric has been termed test-fairness by Chouldechova [3], Verma et al. [5], and Mehrabi et al. [12] and as matching conditional frequencies by Hardt et al. [11]\nThere is another related metric termed well-calibration [5] or calibration within groups [2][4] that imposes an additional, more stringent condition. In order for a model to be well-calibrated (or to have calibration within groups), individuals assigned score s must have probability s of belonging to the positive class. If this condition is satisfied, then testfairness will also automatically be satisfied, though the reverse does not hold. The difference between calibration and well-calibration is simply one of mapping; the scores of a calibrated predictor can, using a suitable transformation, be converted to scores satisfying well-calibration."
    },
    {
      "heading": "2.6 Balance for positive/negative class",
      "text": "Kleinberg et al. [2] have noted that when the average score s for all individuals constituting the group-specific positive class is the same for both groups of interest, it can be said that there exists balance for the positive class. Similarly, balance for the negative class is satisfied when the average score s for members of the negative class are equal, regardless of group membership. Mathematically this is expressed in terms of expected values. For the negative class, balance requires E[s|y = 0, G = 0] = E[s|y = 0, G = 1], and for the positive class balance requires E[s|y = 1, G = 0] = E[s|y = 1, G = 1].\nAs Pleiss et al. [15] have explained, this can be viewed as a generalization of the equalized odds metric to non-binary cases. To see this, note that when the score s can take on only the two values 0 and 1, the score itself is the prediction and the term E[s|y = 0, G] then represents the false positive rate and the term E[s|y = 1, G] represents the true positive rate. And, as noted above, under equalized odds the TPR and FPR are equal across groups."
    },
    {
      "heading": "3 Comparisons and trade-offs between metrics",
      "text": "Different fairness metrics formalize varying intuitive notions of fairness. This raises the question of the conditions under which more than one metric can be simultaneously satisfied, and relatedly, the ways in which different metrics might be in tension.\nWe will analyze metrics under the assumption that the \u201cbase rate\u201d differs across groups. The base rate of a group is the ratio of people in the group who belong to the positive class (y = 1) to the total number of people in that group. Thus, having non-equal base rates across groups means that p(y = 1|G = 0) 6= p(y = 1|G = 1).\nIn the subsequent discussion, to simplify the equations we will use the following terms as defined here:\n\u2022 TPRg = p(y\u0303 = 1|y = 1, G = g), g \u2208 {0, 1}\u2014 the group-specific true positive rates. \u2022 FPRg = p(y\u0303 = 1|y = 0, G = g), g \u2208 {0, 1}\u2014 the group-specific false positive rates. \u2022 PPVg = p(y = 1|y\u0303 = 1, G = g), g \u2208 {0, 1}\u2014 the group-specific positive predictive value."
    },
    {
      "heading": "3.1 Statistical parity, equalized odds and predictive parity",
      "text": "Trade-offs among statistical parity, equalized odds and predictive parity have received significant attention in algorithmic fairness literature under a variety of formulations. Chouldechova [3] articulates the tradeoffs between predictive parity and equalized odds empirically. Kleinberg et al. [2], while not directly referring to these terms, give a closely related result, writing that \u201cthe calibration condition and the balance conditions for the positive and negative classes\" are \u201cin general incompatible with each other; they can only be simultaneously satisfied in certain highly constrained cases.\u201d Berk et al. [4] discuss incompatibility among metrics by taking several cases and examining the trade-offs in those scenarios.\nWe offer a common mathematical framework to examine trade-offs among statistical parity, equalized odds and predictive parity. We provide proofs regarding the combination of all three of these metrics and also explore conditions under which it may be possible to simultaneously satisfy two metrics. To provide an initial framing, it is interesting to note that using the basic probability relation p(A,B) = p(A|B)p(B) = p(B|A)p(A), the respective probability distributions associated with each of these three metrics can be expressed as follows:\np(y, y\u0303|G) = p(y|y\u0303, G)\ufe38 \ufe37\ufe37 \ufe38 Predictive Parity \u00d7 p(y\u0303|G)\ufe38 \ufe37\ufe37 \ufe38 Statistical Parity = p(y\u0303|y,G)\ufe38 \ufe37\ufe37 \ufe38 Equalized Odds \u00d7 p(y|G)\ufe38 \ufe37\ufe37 \ufe38 Base Rate\n(1)"
    },
    {
      "heading": "3.1.1 All three?",
      "text": "This section considers the feasibility of satisfying all three metrics under the assumption of unequal base rates. We start by assuming a predictor that satisfies both statistical parity and equalized odds, and then examining if it can also satisfy predictive parity. From equation 1, we have:\np(y = 1|y\u0303 = 1, G) = p(y\u0303 = 1|y = 1, G)\u00d7 p(y = 1|G) p(y\u0303 = 1|G) (2)\nSince the predictor satisfies equalized odds, the TPR must be the same across groups and therefore, we denote TPR0 = TPR1 = TPR. And since the predictor by definition also satisfies statistical parity, p(y\u0303 = 1|G = 0) = p(y\u0303 = 1|G = 1) = p(y\u0303 = 1) Imposing these conditions and taking the difference of the PPV values of the two groups gives:\np(y = 1|y\u0303 = 1, G = 0)\u2212 p(y = 1|y\u0303 = 1, G = 1) = TPR[p(y = 1|G = 0)\u2212 p(y = 1|G = 1)] p(y\u0303 = 1)\n(3)\nPredictive parity requires that the PPV be equal across both groups, and therefore that the difference on the left side of the above equation be zero, which in turn can only occur when the base rates across the two groups are equal as indicated by the right side of the equation. Note that equalized odds requires both the TPR and the FPR to be the same. However, by demonstrating the incompatibility of just TPR in this section, we provide a sufficient proof for the incompatibility of equalized odds in the given scenario. Thus, when the two groups have unequal base rates, satisfying all three of statistical parity, predictive parity and equalized odds is impossible. This remains true even if the predictor is perfect, as a perfect predictor can not (when the base rates are unequal) satisfy statistical parity."
    },
    {
      "heading": "3.1.2 Statistical parity and predictive parity",
      "text": "We now consider conditions under which a predictor can satisfy both statistical and predictive parity. Recall that when statistical parity holds, we have p(y\u0303 = 1|G = 0) = p(y\u0303 = 1|G = 1) = p(y\u0303 = 1). Taking the difference in PPV across the two groups gives:\np(y = 1|y\u0303 = 1, G = 0)\u2212 p(y = 1|y\u0303 = 1, G = 1) = TPR0p(y = 1|G = 0)\u2212 TPR1p(y = 1|G = 1) p(y\u0303 = 1)\n(4)\nUnder predictive parity the left side of the equation must be zero, which in turn requires that the ratio of the true positive rates of the two groups be the reciprocal of the ratio of the base rates, i.e.:\nTPR0 TPR1 = p(y\u0303 = 1|y = 1, G = 0) p(y\u0303 = 1|y = 1, G = 1) = p(y = 1|G = 1) p(y = 1|G = 0) = Base Rate of Group 1 Base Rate of Group 0\n(5)\nThus, while statistical and predictive parity can be simultaneously satisfied even with different base rates, the utility of such a predictor is limited when the ratio of the base rates differs significantly from 1, as this forces the true positive rate for one of the groups to be very low.\nAs mentioned above, the definition of predictive parity used here, consistent with [3], only requires different groups to have the same PPV. However, if we were to consider the \u201coverall predictive parity\u201d [8], and require a predictor to also have the same NPV across groups, the system would be overconstrained and it would not generally be possible to simultaneously satisfy statistical parity and predictive parity."
    },
    {
      "heading": "3.1.3 Equalized odds and predictive parity",
      "text": "Chouldechova (2017) [3] observes that \"predictive parity is incompatible with error rate balance when prevalence differs across groups,\" (p. 5). (As noted above, Chouldechova uses \u201cerror rate balance\u201d to describe what we refer to here as equalized odds.) We explore this incompatibility in more detail. As before when equalized odds and predictive parity are satisfied, we have TPR0 = TPR1, FPR0 = FPR1, and PPV0 = PPV1. Noting that\np(y\u0303 = 1|G) = \u2211 y p(y\u0303 = 1|y,G)p(y|G) = p(y\u0303 = 1|y = 1, G)p(y = 1|G) + p(y\u0303 = 1|y = 0, G)p(y = 0|G)\n=\u21d2 p(y\u0303 = 1|G) = TPR0p(y = 1|G) + FPR0p(y = 0|G) (6)\nand considering equation 1, we can write:\np(y\u0303 = 1|y = 1, G = 0)p(y = 1|G = 0) = p(y = 1|y\u0303 = 1, G = 0)[TPR0p(y = 1|G = 0) + FPR0p(y = 0|G = 0)]\n=\u21d2 TPR0p(y = 1|G = 0) = PPV0[TPR0p(y = 1|G = 0) + FPR0p(y = 0|G = 0)]\nTPR0p(y = 1|G = 0) = PPV0[TPR0p(y = 1|G = 0) + FPR0(1\u2212 p(y = 1|G = 0))]\n=\u21d2 p(y = 1|G = 0) = PPV0FPR0 PPV0FPR0 + (1\u2212 PPV0)TPR0\n(7)\nLikewise, p(y = 1|G = 1) = PPV1FPR1 PPV1FPR1 + (1\u2212 PPV1)TPR1\n(8)\nBut since TPR0 = TPR1, FPR0 = FPR1 and PPV0 = PPV1, equations 7 and 8 will be identical, so base rates for groups 1 and 2 will be the same: p(y = 1|G = 0) = p(y = 1|G = 1). Hence, in the absence of perfect prediction, the base rates have to be equal for both equalized odds and predictive parity to simultaneously hold. When perfect prediction is achieved, equations 7 and 8 take on the indefinite form 0/0 so therefore do not convey anything definitive about base rates in that scenario.\nWe also note that the metric equal opportunity (a less strict counterpart to equalized odds that requires only equal TPR across groups) is compatible with predictive parity. This is evident from equations 7 and 8 when the condition FPR0 = FPR1 is removed, thereby allowing equalized opportunity and predictive parity to be simultaneously satisfied even with unequal base rates. However, achieving this condition with unequal base rates will require that the FPR differs across the groups. When the difference between the base rates is large, the variation between group-specific FPRs may have to be significant which may reduce suitability for some applications. Hence, while equal opportunity and predictive parity are compatible in the presence of unequal base rates, practitioners should consider the cost (in terms of FPR difference) before attempting to simultaneously achieve both. A similar analysis is possible when we considering parity in negative predictive value instead of positive predictive value, i.e. equal opportunity and parity in NPV are compatible, but only at the cost of variation between group-specific true negative rates (TNRs)."
    },
    {
      "heading": "3.1.4 Equalized odds and statistical parity",
      "text": "We now consider if a predictor can simultaneously satisfy equalized odds and statistical parity. As before, for TPR = TPR0 = TPR1 (equal TPR) and FPR = FPR0 = FPR1 (equal FPR):\np(y\u0303 = 1|G) = TPR[p(y = 1|G)] + FPR[p(y = 0|G)]\n=\u21d2 p(y\u0303 = 1|G = 0)\u2212 p(y\u0303 = 1|G = 1) =TPR[p(y = 1|G = 0)\u2212 p(y = 1|G = 1)] +FPR[p(y = 0|G = 0)\u2212 p(y = 0|G = 1)] (9)\n=\u21d2 p(y\u0303 = 1|G = 0)\u2212 p(y\u0303 = 1|G = 1) = (TPR\u2212 FPR)[p(y = 1|G = 0)\u2212 p(y = 1|G = 1)] (10)\nStatistical parity requires the left side of equation 10 to be zero. For the equation to hold, this means the right side must also be zero, which can only occur when either TPR = FPR or p(y = 1|G = 0) = p(y = 1|G = 1). The latter case, however, violates the assumption that the base rates are different. Therefore to have both statistical parity and equalized odds, the only possibility is to have TPR = FPR, i.e. the false positive rate and the true positive rate have to be equal. Thus, while simultaneously achieving statistical parity and equalized odds is mathematically possible, it is not particularly useful since the goal is typically to develop a predictor in which the TPR is significantly higher than the FPR."
    },
    {
      "heading": "3.2 Predictive parity and calibration",
      "text": "There has been some confusion in the literature regarding the relationship between these two metrics. Chouldechova [3] has correctly mentioned that \u201cWhile predictive parity and calibration look like very similar criteria, well-calibrated scores can fail to satisfy predictive parity at a given threshold,\u201d However, in other papers, the discussion on these two metrics sometimes gives less attention to the specifics of how they relate than we think is merited. Given this backdrop, in the present section we explain the difference between predictive parity and calibration with a mathematical derivation and an example.\nIt is possible to view calibration as a generalization of predictive parity to the non-binary setting. The score s discussed in relation to calibration is generally continuous-valued. However, in the special case in which it is limited to the two values 0 and 1 and therefore becomes the prediction itself, achieving calibration is the same as achieving equality across groups in both PPV and NPV. Thus, this satisfies both predictive parity (due to equality across groups in PPV) as well as \u201doverall predictive parity\u201d (due to equality across groups in both PPV and NPV).\nOf course, in general the score s is not binary. A continuous-valued score can be binarized through a thresholding operation to generate a binary prediction y\u0303. However, it is not the case that thresholding a calibrated score in this manner necessarily leads to predictive parity.\nTo prove this, consider a threshold sth \u2208 [0, 1], such that \u2200s > sth, y\u0303 = 1 and y\u0303 = 0 otherwise. Hence, the distribution relevant to predictive parity p(y = 1|y\u0303 = 1, G) can be expressed p(y = 1|s > sth, G). Using this we can write:\np(y, s > sth|G) = \u222b 1 sth\np(y|s,G)\ufe38 \ufe37\ufe37 \ufe38 calibration term p(s|G)ds (11)\n=\u21d2 p(y|s > sth, G)\ufe38 \ufe37\ufe37 \ufe38 predictive parity term =\n\u222b 1 sth\np(y|s,G)p(s|G)ds\u222b 1 sth p(s|G)ds (12)\nThe above equation relates predictive parity to calibration, showing that even when the calibration term p(y|s,G) is the same for both groups, the probability distribution of the score, expressed in equation 12 through p(s|G), can vary across groups in a way that causes predictive parity not to be satisfied. To make this more intuitive, we will consider a special case where there are only two score values s1 and s2 above the threshold sth such that p(s|G) 6= 0. In other words, all individuals who receive risk scores above the threshold have the possibility of receiving one of only two scores, s1 or s2. Hence, p(s > sth|G) = p(s = s1|G) + p(s = s2|G).\nUnder this special case equation 12 reduces to:\np(y = 1|y\u0303 = 1, G) = p(y = 1|s = s1, G)p(s = s1|G) + p(y = 1|s = s2, G)p(s = s2|G) p(s = s1|G) + p(s = s2|G)\n(13)\nUsing this scenario, consider an example in which we have 100 people in each of two groups: orange and blue (this is a new example, unrelated to the example using orange and blue groups introduced earlier in the paper). Consider further an algorithm that only gives one of three possible scores (0.25, 0.5 or 0.75) to every individual\u2019s loan application. Suppose that scores are being binarized using a threshold of 0.49, such that any individual with a score above 0.49 is deemed to belong to the positive class. In this example, this would mean there are two possible scores (0.5 and 0.75) that can lead to a positive prediction. This is illustrated in Table 2 given below.\nThe first column represents the score that the model assigned. In the second and third columns, the numbers outside the parentheses convey the number of people in the group assigned that score. The numbers in parentheses represent the number of people from those assigned that score who actually belong to the positive class. In this example the predictor is calibrated, since given a score, the fraction of people who actually belong to the positive class is independent of the group. For example, for score 0.5, 10/20 = 0.5 = 50% of the people in the orange group with that score and 20/40 = 0.5 = 50% of the people in the blue group with that score belong to the positive class.\nDoes this model satisfy predictive parity? Choosing 0.49 as the threshold gives a total of 60 positive predictions for both the orange group and the blue group. However, of the people with scores greater than 0.49, only 40 members in the orange group and 35 members of the blue group are actually in the positive class, resulting in a PPV of 40/60 = 0.66 for the orange group and 35/60 = 0.583 for the blue group. Thus, while the predictor is calibrated, choosing a threshold of 0.49 does not lead to a set of binary predictions that satisfy predictive parity.\nIt is also interesting to note that if all persons who had a score of 0.25 are instead given a score of 0.4, the model will not only be calibrated (because, as before, people with the same score have the same probability of belonging to the positive class) but also well-calibrated (because, due to this change in scoring, for all scores the score itself would give the probability of belonging to the positive class). However, this change in score would have no impact on the thresholding example above, illustrating that even a well-calibrated model does not, after applying a threshold to produce binary predictions, necessarily satisfy predictive parity.\nThis discussion can be further generalized. Consider the comparison between statistical parity and calibration given by Kleinberg et al. [2]. Kleinberg et al. give an alternate definition of statistical parity which is independent of the choice of threshold, defining it as the condition in which both groups have the same average score. They then prove that when there are unequal base rates and imperfect prediction, statistical parity is incompatible with well-calibration.\nIn one sense, the Kleinberg et al. definition of statistical parity can be understood as a generalization of the binary case. This is because in the special case where the score itself is binary and represents the prediction, the Kleinberg et al. definition reduces to the common (i.e., binary) definition of statistical parity that we have provided in section 2.3. However, when there is both a score s and a binary prediction y\u0303 obtained by thresholding the score, the fact that there was statistical parity prior to thresholding in accordance with the definition from Kleinberg et al. does not necessarily imply that there will be statistical parity of the binary predictions in accordance with 2.3. Stated another way, if, prior to thresholding, the average score s is the same across two groups, it does not necessarily follow that thresholding will produce predictions y\u0303 in which p(y\u0303|G = 0) = p(y\u0303|G = 1).\nUnder the standard definition of statistical parity (in 2.3) it is possible to simultaneously satisfy both statistical parity and calibration. Consider, again, the example in table 2, where despite different base rates, when a threshold of 0.49 is applied to binarize the calibrated scores, it also satisfies statistical parity (for both groups of 100, 60 individuals are predicted to belong to the positive class). The base rates are also different since a total of 56 people from the orange\ngroup and 51 people from the blue group belong to the positive class. Again, as before, if all persons who had a score of 0.25 were instead given a score of 0.4, the model will be well-calibrated and still satisfy statistical parity for the threshold of 0.49. Also, the prediction is clearly imperfect. Thus, statistical parity (as we have defined it in section 2.3 above) is not necessarily incompatible with having calibration (and well-calibration) even when the base rates are different and there is imperfect prediction.\nThis underscores the importance of being attentive to the difference between fairness metrics designed for use in relation to scores s and those intended for use with binary predictions. While it is straightforward to convert scores to binary values through thresholding, the ease of the conversion masks important complexities regarding the extent to which fairness metrics might be met after the thresholding process. In the example above, the scores were calibrated, and upon application of a threshold of 0.49, the binary predictions exhibited statistical parity. However, the same underlying distribution of scores, had they been subject to a threshold of 0.55, would have yielded predictions without statistical parity. More generally, we believe that there is room for\u2014and a need for\u2014fairness metrics in relation to both continuous scores s as well as binary predictions y\u0303. But the benefits offered by having a greater number of tools for examining fairness must be balanced with an awareness of the issues that can arise when working across these two domains."
    },
    {
      "heading": "4 Discussion and conclusions",
      "text": "Several conclusions arise from the discussion above. With respect to metrics such as statistical parity, equalized odds and predictive parity that evaluate fairness by comparing binary predictions with binary outcomes, pairwise combinations of metrics can be simultaneously satisfied only under very limited conditions, if at all. For example, when the base rates are different, satisfying both statistical parity and predictive parity requires that the ratio of group-specific true positive rates be the inverse of the ratio of the base rates. When the ratio of the base rates does not deviate too far from 1, this constraint can be met while also preserving high true positive rates for both groups. The question of how far a deviation from 1 in the base rate ratio (and therefore in the inverse of the TPR ratio) would still be acceptable would of course be context dependent. For large (or small) base rate ratios, the resulting predictor would necessarily have a low true positive rate for one of the groups. We also showed that equalized odds and predictive parity are incompatible when the base rates across two groups differs. In addition, we showed that, given unequal base rates, equalized odds and statistical parity can only be simultaneously met in the rather impractical case where the true positive and false positive rates are equal. The above results illustrate that, for unequal base rates, a given pair of metrics can be 1) mathematically incompatible, 2) mathematically compatible but under constraints that are problematic from a policy standpoint, or 3) mathematically compatible under constraints that are consistent with positive policy outcomes.\nWith respect to more generalized predictors that generate a (non-binary) score, we explored the relationship between calibration (computed with respect to score that can in general be continuous) and predictive parity when that score is subject to thresholding to generate a binary prediction. We observed the value of utilizing fairness metrics designed for use in relation to scores, while also emphasizing some of the complexities involved in working across continuous and binary domains. In particular, we noted that calibrated scores, after thresholding, may still generate predictions that satisfy statistical parity, though threshold choice can play an important role in determining whether or not such conditions are satisfied.\nIn closing, we note that in addition to the above considerations, fairness metric selection can also be guided by the observability of each statistic in practice. For example, in the context of loan approvals, loans will typically only be given to the subset of applicants who are predicted to repay. When this occurs, it will be impossible to observe statistics such as false negatives, true negatives, negative predictive value, true positive rate, and true negative rate. By contrast, the positive predictive value will be readily observable, suggesting that a metric such as predictive parity would be easier to evaluate in practice."
    }
  ],
  "title": "FAIRNESS METRICS: A COMPARATIVE ANALYSIS",
  "year": 2020
}
