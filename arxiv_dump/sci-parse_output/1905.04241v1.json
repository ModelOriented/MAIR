{"abstractText": "Interpretable machine learning has become a strong competitor for traditional black-box models. However, the possible loss of the predictive performance for gaining interpretability is often inevitable, putting practitioners in a dilemma of choosing between high accuracy (black-box models) and interpretability (interpretable models). In this work, we propose a novel framework for building a Hybrid Predictive Model (HPM) that integrates an interpretable model with any black-box model to combine their strengths. The interpretable model substitutes the black-box model on a subset of data where the black-box is overkill or nearly overkill, gaining transparency at no or low cost of the predictive accuracy. We design a principled objective function that considers predictive accuracy, model interpretability, and model transparency (defined as the percentage of data processed by the interpretable substitute.) Under this framework, we propose two hybrid models, one substituting with association rules and the other with linear models, and we design customized training algorithms for both models. We test the hybrid models on structured data and text data where interpretable models collaborate with various state-of-the-art black-box models. Results show that hybrid models obtain an efficient trade-off between transparency and predictive performance, characterized by our proposed efficient frontiers.", "authors": [{"affiliations": [], "name": "Tong Wang"}, {"affiliations": [], "name": "Qihang Lin"}], "id": "SP:74a30a44cf18f62f4e9d9904d8758b72e156d6bd", "references": [{"authors": ["Philip Adler", "Casey Falk", "Sorelle A Friedler", "Gabriel Rybeck", "Carlos Scheidegger", "Brandon Smith", "Suresh Venkatasubramanian"], "title": "Auditing black-box models for indirect influence", "venue": "In ICDM,", "year": 2016}, {"authors": ["Elaine Angelino", "Nicholas Larus-Stone", "Daniel Alabi", "Margo Seltzer", "Cynthia Rudin"], "title": "Learning certifiably optimal rule lists", "venue": "In SIGKDD,", "year": 2017}, {"authors": ["Amir Beck", "Marc Teboulle"], "title": "A fast iterative shrinkage-thresholding algorithm for linear inverse problems", "venue": "SIAM journal on imaging sciences,", "year": 2009}, {"authors": ["Leo Breiman"], "title": "Classification and regression trees. Routledge, 2017", "year": 2017}, {"authors": ["Antonin Chambolle", "Thomas Pock"], "title": "A first-order primal-dual algorithm for convex problems with applications to imaging", "venue": "Journal of mathematical imaging and vision,", "year": 2011}, {"authors": ["Chaofan Chen", "Oscar Li", "Alina Barnett", "Jonathan Su", "Cynthia Rudin"], "title": "This looks like that: deep learning for interpretable image recognition", "venue": "arXiv preprint arXiv:1806.10574,", "year": 2018}, {"authors": ["Tianqi Chen", "Carlos Guestrin"], "title": "Xgboost: A scalable tree boosting system", "venue": "In SIGKDD,", "year": 2016}, {"authors": ["Finale Doshi-Velez", "Been Kim"], "title": "A roadmap for a rigorous science of interpretability", "venue": "arXiv preprint arXiv:1702.08608,", "year": 2017}, {"authors": ["John Duchi", "Elad Hazan", "Yoram Singer"], "title": "Adaptive subgradient methods for online learning and stochastic optimization", "venue": "Journal of Machine Learning Research,", "year": 2011}, {"authors": ["Yoav Freund", "Robert E Schapire"], "title": "A desicion-theoretic generalization of on-line learning and an application to boosting", "venue": "In European conference on computational learning theory,", "year": 1995}, {"authors": ["Nicholas Frosst", "Geoffrey Hinton"], "title": "Distilling a neural network into a soft decision tree", "venue": "arXiv preprint arXiv:1711.09784,", "year": 2017}, {"authors": ["A dAvila Garcez", "Tarek R Besold", "Luc De Raedt", "Peter F\u00f6ldiak", "Pascal Hitzler", "Thomas Icard", "Kai-Uwe K\u00fchnberger", "Luis C Lamb", "Risto Miikkulainen", "Daniel L Silver"], "title": "Neural-symbolic learning and reasoning: contributions and challenges", "venue": "In Proceedings of the AAAI Spring Symposium on Knowledge Representation and Reasoning: Integrating Symbolic and Neural Approaches,", "year": 2015}, {"authors": ["Artur S d\u2019Avila Garcez", "Krysia B Broda", "Dov M Gabbay"], "title": "Neural-symbolic learning systems: foundations and applications", "venue": "Springer Science & Business Media,", "year": 2012}, {"authors": ["Ian J Goodfellow", "Jonathon Shlens", "Christian Szegedy"], "title": "Explaining and harnessing adversarial examples", "venue": "arXiv preprint arXiv:1412.6572,", "year": 2014}, {"authors": ["K Hornik", "A Zeileis", "T Hothorn", "C Buchta"], "title": "Rweka: an r interface to weka", "venue": "R package version,", "year": 2007}, {"authors": ["Zhiting Hu", "Xuezhe Ma", "Zhengzhong Liu", "Eduard Hovy", "Eric Xing"], "title": "Harnessing deep neural networks with logic rules", "venue": "arXiv preprint arXiv:1603.06318,", "year": 2016}, {"authors": ["Zhongsheng Hua", "Bin Zhang"], "title": "A hybrid support vector machines and logistic regression approach for forecasting intermittent demand of spare parts", "venue": "Applied Mathematics and Computation,", "year": 2006}, {"authors": ["Rie Johnson", "Tong Zhang"], "title": "Accelerating stochastic gradient descent using predictive variance reduction", "venue": "In Advances in neural information processing systems,", "year": 2013}, {"authors": ["Mahesh Joshi", "Dipanjan Das", "Kevin Gimpel", "Noah A Smith"], "title": "Movie reviews and revenues: An experiment in text regression", "venue": "In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics,", "year": 2010}, {"authors": ["Hian Chye Koh", "Wei Chin Tan", "Chwee Peng Goh"], "title": "A two-step method to construct credit scoring models with data mining techniques", "venue": "International Journal of Business and Information,", "year": 2015}, {"authors": ["Ron Kohavi"], "title": "Scaling up the accuracy of naive-bayes classifiers: A decision-tree hybrid", "venue": "In KDD,", "year": 1996}, {"authors": ["Himabindu Lakkaraju", "Stephen H Bach", "Jure Leskovec"], "title": "Interpretable decision sets: A joint framework for description and prediction", "venue": "In SIGKDD", "year": 2016}, {"authors": ["Himabindu Lakkaraju", "Ece Kamar", "Rich Caruana", "Jure Leskovec"], "title": "Interpretable & explorable approximations of black box models", "venue": "arXiv preprint arXiv:1707.01154,", "year": 2017}, {"authors": ["Himabindu Lakkaraju", "Ece Kamar", "Rich Caruana", "Jure Leskovec"], "title": "Faithful and customizable explanations of black box models", "year": 2019}, {"authors": ["Andy Liaw", "Matthew Wiener"], "title": "Classification and regression by randomforest", "venue": "R news,", "year": 2002}, {"authors": ["Qihang Lin", "Lin Xiao"], "title": "An adaptive accelerated proximal gradient method and its homotopy continuation for sparse optimization", "venue": "Computational Optimization and Applications,", "year": 2015}, {"authors": ["M Lissack"], "title": "Dealing with ambiguity\u2013the black boxas a design choice", "venue": "SheJi (forthcoming),", "year": 2016}, {"authors": ["Tyler McCormick", "Cynthia Rudin", "David Madigan"], "title": "A hierarchical model for association rule mining of sequential events: An approach to automated medical symptom", "year": 2011}, {"authors": ["Kenneth McGarry", "Stefan Wermter", "John MacIntyre"], "title": "Hybrid neural systems: from simple coupling to fully integrated neural networks", "venue": "Neural Computing Surveys,", "year": 1999}, {"authors": ["Arkadi Nemirovski"], "title": "Prox-method with rate of convergence o(1/t) for variational inequalities with lipschitz continuous monotone operators and smooth convex-concave saddle point problems", "venue": "SIAM Journal on Optimization,", "year": 2004}, {"authors": ["Arkadi Nemirovski", "Anatoli Juditsky", "Guanghui Lan", "Alexander Shapiro"], "title": "Robust stochastic approximation approach to stochastic programming", "venue": "SIAM Journal on optimization,", "year": 2009}, {"authors": ["Yurii Nesterov"], "title": "Smoothing technique and its applications in semidefinite optimization", "venue": "Mathematical Programming,", "year": 2007}, {"authors": ["Yurii Nesterov"], "title": "Introductory lectures on convex optimization: A basic course, volume 87", "venue": "Springer Science & Business Media,", "year": 2013}, {"authors": ["Xia Ning", "George Karypis. Slim"], "title": "Sparse linear methods for top-n recommender systems", "venue": "In 2011 IEEE 11th International Conference on Data Mining,", "year": 2011}, {"authors": ["Joy D Osofsky"], "title": "The effect of exposure to violence on young children", "venue": "American Psychologist,", "year": 1995}, {"authors": ["Rutvija Pandya", "Jayati Pandya"], "title": "C5. 0 algorithm to improved decision tree with feature selection and reduced error pruning", "venue": "International Journal of Computer Applications,", "year": 2015}, {"authors": ["Fabian Pedregosa", "Ga\u00ebl Varoquaux", "Alexandre Gramfort", "Vincent Michel", "Bertrand Thirion", "Olivier Grisel", "Mathieu Blondel", "Peter Prettenhofer", "Ron Weiss", "Vincent Dubourg"], "title": "Scikit-learn: Machine learning in python", "venue": "Journal of machine learning research,", "year": 2011}, {"authors": ["Mark Phillips"], "title": "International data-sharing norms: from the oecd to the general data protection regulation (gdpr)", "venue": "Human genetics,", "year": 2018}, {"authors": ["J Ross Quinlan. C"], "title": "5: programs for machine learning", "year": 2014}, {"authors": ["Ross Quinlan"], "title": "Data mining tools", "venue": "see5 and c5", "year": 2004}, {"authors": ["Marco Tulio Ribeiro", "Sameer Singh", "Carlos Guestrin"], "title": "Why should i trust you?: Explaining the predictions of any classifier", "venue": "In SIGKDD,", "year": 2016}, {"authors": ["Marco Tulio Ribeiro", "Sameer Singh", "Carlos Guestrin"], "title": "Anchors: High-precision modelagnostic explanations", "venue": "In Thirty-Second AAAI Conference on Artificial Intelligence,", "year": 2018}, {"authors": ["Peter R Rijnbeek", "Jan A Kors"], "title": "Finding a short and accurate decision rule in disjunctive normal form by exhaustive search", "venue": "Machine learning,", "year": 2010}, {"authors": ["Andrew Slavin Ross", "Michael C Hughes", "Finale Doshi-Velez"], "title": "Right for the right reasons: Training differentiable models by constraining their explanations", "venue": "arXiv preprint arXiv:1703.03717,", "year": 2017}, {"authors": ["Cynthia Rudin"], "title": "Please stop explaining black box models for high stakes decisions", "venue": "arXiv preprint arXiv:1811.10154,", "year": 2018}, {"authors": ["Shai Shalev-Shwartz", "Tong Zhang"], "title": "Accelerated proximal stochastic dual coordinate ascent for regularized loss minimization", "venue": "In International Conference on Machine Learning,", "year": 2014}, {"authors": ["Chung-Kwan Shin", "Ui Tak Yun", "Huy Kang Kim", "Sang Chan Park"], "title": "A hybrid approach of neural network and memory-based learning to data mining", "venue": "IEEE Transactions on Neural Networks,", "year": 2000}, {"authors": ["Ismail A Taha", "Joydeep Ghosh"], "title": "Symbolic interpretation of artificial neural networks", "venue": "IEEE Transactions on knowledge and data engineering,", "year": 1999}, {"authors": ["Robert Tibshirani"], "title": "Regression shrinkage and selection via the lasso", "venue": "Journal of the Royal Statistical Society: Series B (Methodological),", "year": 1996}, {"authors": ["Geoffrey G Towell", "Jude W Shavlik"], "title": "Knowledge-based artificial neural networks", "venue": "Artificial intelligence,", "year": 1994}, {"authors": ["Michael Tsang", "Dehua Cheng", "Yan Liu"], "title": "Detecting statistical interactions from neural network weights", "venue": "arXiv preprint arXiv:1705.04977,", "year": 2017}, {"authors": ["Berk Ustun", "Cynthia Rudin"], "title": "Supersparse linear integer models for optimized medical scoring systems", "venue": "Machine Learning,", "year": 2016}, {"authors": ["Jialei Wang", "Ryohei Fujimaki", "Yosuke Motohashi"], "title": "Trading interpretability for accuracy: Oblique treed sparse additive models", "venue": "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "year": 2015}, {"authors": ["Tong Wang"], "title": "Multi-value rule sets for interpretable classification with feature-efficient representations", "venue": "In Advances in Neural Information Processing Systems,", "year": 2018}, {"authors": ["Tong Wang", "Cynthia Rudin", "F Doshi", "Yimin Liu", "Erica Klampfl", "Perry MacNeille"], "title": "A bayesian framework for learning rule set for interpretable classification", "venue": "Journal of Machine Learning Research,", "year": 2017}, {"authors": ["Hongyu Yang", "Cynthia Rudin", "Margo Seltzer"], "title": "Scalable bayesian rule lists", "venue": "In Proceedings of the 34th International Conference on Machine Learning-Volume", "year": 2017}, {"authors": ["Jiaming Zeng", "Berk Ustun", "Cynthia Rudin"], "title": "Interpretable classification models for recidivism prediction", "venue": "Journal of the Royal Statistical Society: Series A (Statistics in Society),", "year": 2017}, {"authors": ["Quanshi Zhang", "Ying Nian Wu", "Song-Chun Zhu"], "title": "Interpretable convolutional neural networks", "venue": "In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "year": 2018}, {"authors": ["Yuchen Zhang", "Lin Xiao"], "title": "Stochastic primal-dual coordinate method for regularized empirical risk minimization", "venue": "The Journal of Machine Learning Research,", "year": 2017}, {"authors": ["Paul C Zikopoulos", "Chris Eaton", "Dirk DeRoos", "Thomas Deutsch", "George Lapis"], "title": "Understanding big data: Analytics for enterprise class hadoop and streaming data", "venue": "Mcgraw-hill New York,", "year": 2012}], "sections": [{"text": "Keywords: interpretable machine learning, hybrid model, association rules, linear model, efficient frontier"}, {"heading": "1. Introduction", "text": "As data from various fields have seen rapid growths in volume, variety and velocity (Zikopoulos et al., 2012), machine learning models tend to grow bigger and more complicated in structure, especially with the recent break out of deep learning. The predictive performance, however, is not obtained for free. Systems such as deep neural networks and ensemble models are black-box in nature, which means the models have a complex and opaque decision-making process that is difficult for humans to understand. In many heavily regulated industries such as judiciaries and healthcare, black-box models, despite their good predictive performance, often find it challenging to find their way into real-world deploy-\nc\u00a92019 Tong Wang and Qihang Lin.\nLicense: CC-BY 4.0, see https://creativecommons.org/licenses/by/4.0/. Attribution requirements are provided at http://jmlr.org/papers/v1/meila00a.html.\nar X\niv :1\n90 5.\n04 24\n1v 1\n[ cs\n.L G\n] 1\nment since domain experts are often reluctant to trust and adopt a model if they do not understand it. EU\u2019s General Data Protection Regulation (GDPR) has recently called for the \u201cright to explanation\u201d (a right to information about individual decisions made by algorithms) (Parliament and of the European Union, 2016; Phillips, 2018) that requires human understandable predictive processes of models.\nTo facilitate human understandability, interpretable machine learning is proposed. Interpretable machine learning models use a small number of cognitive chunks presented in a human-understandable way (e.g., linear, logical, etc.) to construct a model so users can comprehend how predictions are produced. Various forms of interpretable models have been developed, including rule-based models (Wang et al., 2017; Lakkaraju et al., 2017), sparse linear models (Zeng et al., 2017), case-based reasoning (Richter and Weber, 2016), etc. They are easy to understand, use, and diagnose compared to complex black-box models.\nYet, the benefits of interpretable machine learning models often come at a price of compromised predictive performance. While the state-of-the-art research in interpretable machine learning has further pushed the limit of the models, and they can, sometimes, perform as well as black-box models, it is more often observed that interpretable models lose to black-box models on complicated tasks since they need to have a simple structure to maintain interpretability while black-box models are trained to optimize a single objective, the predictive performance. In this paper, we focus on this more common situation where the best model is a black-box and choosing any of its interpretable competitors will lead to a loss in predictive performance.\nA popular solution to still gain interpretability in the above situation is to use blackbox explainers, which generate post hoc explanations for a black-box model, either locally (Ribeiro et al., 2016) or globally (Adler et al., 2016; Lakkaraju et al., 2017). However, two concerning issues exist. First, explainers only approximate but do not characterize exactly the decision-making process of a black-box model, yielding an imperfect explanation fidelity. Second, ambiguity and inconsistency (Ross et al., 2017; Lissack, 2016) exist in explanations since different explanations can be generated for the same prediction, by different explainers or the same explainer with different parameters. Both issues result from the fact that explainers are not the decision-making process themselves and only approximate it afterward.\nIn this paper, we propose an alternative solution that introduces interpretability in a novel way. Our solution does not aim to explain a black-box but to substitute it, on a subset of data where the black-box model is overkill or nearly overkill. We argue that even if a complex black-box model has the best predictive performance overall, it is not necessarily the best everywhere in the data space. We hypothesize that there exists a subspace in the feature space where a simpler (interpretable) model can be as accurate as or almost as accurate as the black-box model. Therefore, the black-box model could be replaced by the simple model with no or low cost of the predictive performance while gaining interpretability.\nIf such an interpretable substitute can be found, we can design a hybrid predictive model where input is first sent to the interpretable model to see if a prediction can be directly generated, if not, the black-box model will be activated. We call the percentage of data processed by an interpretable model transparency of the model, which represents how often a prediction is interpretable. The goal of a hybrid model is to \u201csqueeze\u201d transparency from the decision-making process at a minimum cost of predictive performance.\nTo build a hybrid model, we need to address the following research questions. How to find such a sub-area in the feature space? What kind of interpretable models can be used as a local substitute for a complex model? And what is the form of the decision-making process? Finally, what does a system benefit from utilizing an interpretable local substitute?\nTo answer these question, we design a general framework for hybrid predictive models and formulate a principled objective that considers predictive performance, complexity of the interpretable substitute and the transparency of the model. We instantiate this framework with the two most popular forms of interpretable models in this paper, association rules and linear models, to build two hybrid models with customized training algorithms. We apply both models to structured datasets and text data, where the interpretable models partially substitute state-of-the-art black-box models including ensembles and neural networks. To evaluate the models, we propose efficient frontiers that characterize the trade-off between transparency and accuracy. Efficient frontiers unify interpretable models, black-box models, and hybrid models, with black-box models located at transparency equal to zero, interpretable models at transparency equal to one, and hybrid models connecting the two extremes and spanning the entire spectrum of transparency.\nThe proposed framework forms a collaboration between an interpretable model and a black-box model, utilizing the strengths of them where they are most competent. An important advantage of the proposed framework is that it is agnostic to the black-box model, which means any implementation or algorithmic detail of the black-box model or even what kind of model it is remains unknown during training. We only need the input and output of the black-box and the true labels of the input to train a hybrid model. This training mechanism will largely conceal information of the black-box system from its collaborator. This property is critical in building collaborations among different systems which do not wish to share technical details.\nFinally, we would like to emphasize that the proposed model is not a black-box explainer, but can serve as a pre-step for a black-box explainer: we first find a region that can be taken over by an interpretable model and then sends the rest of the data to a black-box to predict and an explainer to explain.\nThe rest of the paper is organized as follows. Section 2 reviews related work on interpretable machine learning. We present the framework of hybrid decision making in Section 3 and instantiate the model with association rules and linear models in Section 4 and Section 5, respectively, where we cast the two interpretable models into the hybrid framework and design customized training algorithms. In Section 6, we test the performance of the hybrid models on structured and text datasets from different domains where interpretability is most pursued and discuss the results in detail."}, {"heading": "2. Literature Review", "text": "Our work is broadly related to new methods for interpretable machine learning, most of which fall into two categories. The first is developing models that are interpretable standalone and do not need external explainers. These interpretable models often have a few cognitive chunks and simple logic to be easy to understand. Previous works in this category include rule-based models such as rule sets (Wang et al., 2017; Rijnbeek and Kors, 2010; McCormick et al., 2011), rule lists (Yang et al., 2017; Angelino et al., 2017). Rule-based\nmodels check an input against a set of ordered or unordered rules. If an input is captured by a rule, the corresponding consequence of the rule is produced as the prediction. Linear models, especially sparse linear systems (Zeng et al., 2017; Ustun and Rudin, 2016; Koh et al., 2015) are also a canonical and popular form of interpretable models. They learn a possibly small set of non-zero weights for features and then compare the weighted sum of features with a threshold to determine the class labels. When learning an interpretable model, the model complexity is regularized to improve interpretability and avoid overfitting. For rule-based models, model regularization often refers to reducing the number of rules, the number of features, the total number of conditions, etc (Wang, 2018; Lakkaraju et al., 2016). Sparse linear models regularize the number of non-zero coefficients or force the coefficients to be integers (Ustun and Rudin, 2016). We use rules and linear models to build two types of hybrid models in this paper.\nThe second line of research in interpretable machine learning is to provide explanations for a black-box model, locally (Ribeiro et al., 2016) or globally (Adler et al., 2016; Lakkaraju et al., 2017), providing some insights into the black-box model by identifying key features, interactions of features (Tsang et al., 2017), etc. One representative work is LIME (Ribeiro et al., 2016) that explains the predictions of any classifier in an interpretable and faithful manner, by learning an interpretable linear model locally around the prediction. Others like Ribeiro et al. (2018); Lakkaraju et al. (2019) use association rules to explain a black-box system. More recently, developments in deep learning have been connected strongly with interpretable machine learning (Goodfellow et al., 2014; Frosst and Hinton, 2017) and have contributed novel insights into representational issues. But there has been recent debate (Rudin, 2018) on potential issues of black-box explainers since explainers only approximate but do not characterize exactly the decision-making process of a black-box model, often yielding an imperfect fidelity. In addition, there exists ambiguity and inconsistency (Ross et al., 2017; Lissack, 2016) in the explanation since there could be different explanations for the same prediction generated by different explainers, or by the same explainer with different parameters. Both issues result from the fact that the explainers only approximate in a post hoc way. They are not the decision-making process themselves.\nOur work is fundamentally different from the research above. A hybrid predictive model is not a pure interpretable model. It utilizes the partial predictive power of black-box models to preserve the predictive accuracy. It is also not an explainer model which only observes but does not participate in the decision process. Here, a hybrid predictive model uses an interpretable model and a black-box model jointly to make a prediction.\nThere exist a few singleton works that combine multiple models. One of the earliest works (Kohavi, 1996) proposed NBTree which combines decision-tree classifiers and Naive-Bayes classifiers, Shin et al. (2000) proposed a system combining neural network and memory-based learning. Hua and Zhang (2006) combined SVM and logistic regression to forecast intermittent demand of spare parts, etc. A recent work (Wang et al., 2015) divides feature spaces into regions with sparse oblique tree splitting and assign local sparse additive experts to individual regions. Aside from the singletons, there has been a large body of continuous work on neural-symbolic or neural-expert systems (Garcez et al., 2015) pursued by a relatively small research community over the last two decades and has yielded several significant results (McGarry et al., 1999; Garcez et al., 2012; Taha and Ghosh, 1999; Tow-\nell and Shavlik, 1994). This line of research has been carried on to combine deep neural networks with expert systems to improve predictive performance (Hu et al., 2016).\nCompared to the collaborative models discussed above, our model is distinct in that the proposed framework can work with any black-box model. This black-box model could be a carefully calibrated, advanced model using confidential features or techniques. Our model only needs predictions from the black-box model and do not need to alter the blackbox during training or know any other information from it. This minimal requirement of information from the black-box collaborator renders much more flexibility in creating collaboration between different models, largely preserving confidential information from the more advanced partner."}, {"heading": "3. Hybrid Predictive Model", "text": "We present a general framework for building a hybrid predictive model and formulate an objective function combining important properties of a predictive model. Let D = {(x(n), y(n))}Nn=1 represent a set of training examples where x(n) \u2208 X is a set of attributes for instance n and y(n) \u2208 Y is the target variable. In this paper we focus on binary classification so y(n) \u2208 {\u22121, 1}. Let there be a black-box model fb which can be any pre-trained model with unknown type. We are only provided with its predictions on the training set, denoted as {y\u0302b(n)}Nn=1.\nGiven {(x(n), y(n), y\u0302b(n))}Nn=1, our goal is to build an interpretable model f\u03b9 that replaces fb on a subset of data, forming a hybrid predictive model, denoted as f = \u3008f\u03b9, fb\u3009. Let D\u03b9 represent the subset of data processed by f\u03b9 and Db is the remaining data processed by fb. We design the predictive process as below: an input x is first sent to f\u03b9. If a prediction can be generated, f\u03b9 will directly output a prediction y\u0302\u03b9, else, x is sent to fb to generate a prediction y\u0302b. See Figure 1 for an illustration.\nWe consider three properties critical when building a hybrid predictive model. First, we desire a model with high predictive performance. Since fb is pre-given, the accuracy of a hybrid model is determined by two factors, the predictive accuracy of f\u03b9 and the collaboration of f\u03b9 and fb, i.e., the partition of D into D\u03b9 and Db. f\u03b9 and fb being completely different models allows the hybrid model to exploit the strengths of the two if the training examples are partitioned strategically, sending examples to the model which can obtain the maximum benefit. Second, we desire high model interpretability of f\u03b9. Bringing interpretability into the decision-making process is one of the motivations of building a hybrid model. The definition of interpretability is model specific and usually refers to having\na small size and low model complexity, often measured by the number of cognitive chunks (Doshi-Velez and Kim, 2017). For example, in rule-based models, the smallest cognitive chunks are conditions (feature-value pairs) in rules. The total number of conditions in a model represents the complexity of the model (Wang, 2018). For linear models, the smallest cognitive chunks are features, and therefore sparse linear models regularize the number of non-zero coefficients (Ning and Karypis, 2011). Finally, we want to maximize the amount of data sent to f\u03b9, which represents how often a prediction is interpretable, i.e., captured by f\u03b9. To do that we define a novel metric, transparency, which is the percentage of D\u03b9 in D.\nDefinition 1 The transparency of a hybrid model f = \u3008f\u03b9, fb\u3009 on D is the percentage of data processed by f\u03b9, i.e., D\u03b9 D , denoted as E(f,D).\nFollowing this definition, a stand-alone interpretable model has transparency equal to one, and a black-box model has transparency equal to zero.\nIn light of the considerations above, we formulate a learning objective \u039b(f\u03b9,D, {y\u0302(n)b } N n=1) as a linear combination of the three metrics. Given training data D, predictions of a pretrained black-box on D, {y\u0302(n)b } N n=1, and a pre-defined form of interpretable models F\u03b9, our goal is to find the optimal f\u2217\u03b9 that\nf\u2217\u03b9 \u2208 arg min f\u03b9\u2208F\u03b9 \u039b(f\u03b9,D, {y\u0302(n)b } N n=1). (1)\nNext, we instantiate the proposed framework with selected interpretable substitutes. An interpretable substitute needs to fulfill two objectives, to produce a human-understandable prediction and to partition the data. We choose two interpretable models as examples, association rules and linear models, corresponding to the model space of Frule and Flinear, respectively. Both are popular forms of models in the interpretable machine learning literature. In the next two sections, we will cast the two interpretable models into the proposed framework and develop customized training algorithms optimizing the objective (1).\n4. Hybrid Rule Set\nIn this section, we choose association rules for f\u03b9 and call the model Hybrid Rule Set. We use Frule to denote the model space for f\u03b9. Rules are easy to understand for their simple logic and symbolic presentation. They also naturally handle the partition of data by separating examples according to if they satisfy the rules."}, {"heading": "4.1 Model Formulation", "text": "We construct two rule sets for f\u03b9, a positive rule set, R+, and a negative rule set, R\u2212. If x(n) satisfies any rules in R+, it is classified as positive. Otherwise, if it satisfies any rules in R\u2212, it is classified as negative. A decision produced from rules is denoted as y\u0302\u03b9(n). If x(n) does not satisfy any rules in R+ or R\u2212, it means f\u03b9 fails to decide on x(n). Then x(n) is sent to the black-box model fb to generate a decision y\u0302b (n). The predictive process of f\nis shown below.\nif x(n) obeys R+, Y = 1 else if x(n) obeys R\u2212, Y = \u22121 else Y = fb(x) (2)\nHere R+ and R\u2212 are embedded in an \u201celse if \u201d logic. If an instance satisfies both R+ and R\u2212, it is classified as positive since R+ is ranked higher. Define R = (R+,R\u2212). We will use R and f\u03b9 interchageably for hybrid rule sets.\nThe association rules not only produce predictions but also naturally partition the feature space into D\u03b9 and Db, covering a subset of \u201ccertainly positive\u201d instances with R+ and a subset of \u201ccertainly negative\u201d instances with R\u2212.\nWe define the coverage of rules below.\nDefinition 2 A rule r covers x(n) if x(n) obeys the rule, denoted as covers(r,x(n)) = 1.\nDefinition 3 A set of rules R covers x(n) if x(n) obeys at least one rule in R, i.e.\ncovers(R,x(n)) = 1 (\u2211 r\u2208R covers(r,x(n)) \u2265 1 ) . (3)\nDefinition 4 Given a data set D, the support of a rule set R in D is the number of observations covered by R, i.e.,\nsupport(R,D) = \u2211 n=1 covers(R,x(n)). (4)\nLearning Objective\nNow we cast learning objective \u039b(f\u03b9,D, {y\u0302(n)b } N n=1) to Hybrid Rule Sets. First, we measure the predictive performance with misclassification error, represented `(\u3008R, fb\u3009,D). `(\u3008R, fb\u3009,D) = N\u2211 n=1 ( (1\u2212 y(n)) 2 covers(R+,x(n)) . errors from R+\n+ (1 + y(n))\n2\n( 1\u2212 covers(R+,x(n)) ) covers(R\u2212,x(n)) . errors from R\u2212\n+ ( (1\u2212 covers(R+,x(n))(1\u2212 covers(R\u2212,x(n)) ) . instances not covered by R+ or R\u2212\n\u00d7 ((1 + y(n))\n2\n(1\u2212 y\u0302b(n)) 2 + (1\u2212 y(n)) 2 (1 + y\u0302b (n)) 2\n)) /N. . errors from fb (5)\nNext, we measure the interpretability of R, denoted as \u2126(R). There exists a list of choices from the literature (Wang et al., 2017; Lakkaraju et al., 2016; Wang, 2018). In this paper, we use the total number of conditions in rules, denoted as \u2126(R). Regularizing the number of conditions not only increases interpretability but also naturally avoids overfitting. Without this regularization, the model will find long rules (having many conditions in a rule)\nwith very small support but highly accurate, causing overfitting. Therefore, regularizing the number of conditions will benefit the model in both model interpretability and predictive performance. The design of interpretability is customizable. Other metrics such as the overlap of rules, lengths of rules, etc., can also be used here.\nFinally, we compute the transparency of a model. Following Definition 1, the transparency of a hybrid rule set \u3008R, fb\u3009 is\nE(\u3008R, fb\u3009,D) = support(R) |D| . (6)\nWe will write `(\u3008R, fb\u3009,D) and E(\u3008R, fb\u3009,D) as `(R) and E(R), respectively, ignoring the dependence on D and fb for simpler notations since fb and D are fixed.\nThus, we derive an objective function combining predictive accuracy, model interpretability and transparency\n\u039b(R) = `(R) + \u03b11\u2126(R)\u2212 \u03b12E(R), (7)\nand our goal is to find an optimal model R\u2217 such that\nR\u2217 = (R\u2217+,R\u2217\u2212) \u2208 arg minR \u039b(R). (8)\nHere, \u03b11, \u03b12 are non-negative coefficients. Tuning the parameters will produce models with different accuracy, interpretability, and transparency. For example, in an extreme case when \u03b12 >> \u03b11, the output will be a model that sends all data to f\u03b9, producing a pure interpretable model. Then increasing \u03b11 will produce a simpler and sparser model, using fewer rules and fewer conditions. When \u03b11 >> \u03b12, the model will force f\u03b9 to have no conditions and no rules, i.e., producing a pure black-box model."}, {"heading": "4.2 Training with Adaptive Stochastic Local Search", "text": "We describe a training algorithm to find an optimal solution R\u2217 to objective (8). Learning rule-based models is challenging because the solution space is a power set of the rule space. Fortunately, our objective function suggests theoretical bounds that can be exploited for reducing computation. We first present the algorithm structure, then detail the theoretically grounded strategies we embed into the algorithm, and finally describe the proposing step.\nAlgorithm Structure Given training examples, D, black-box predictions {y\u0302(n)b } N n=1, parameters \u03b11, \u03b12, base temperature C0 and the total number of iterations T , we design an adaptive stochastic local search algorithm. Each state corresponds to a tuple of a positive rule set and a negative rule set, indexed by the time stamp t, denoted as R[t] = (R[t]+ ,R [t] \u2212 ). The starting state R[0] is initialized with two empty sets for R[0]+ and R [0] \u2212 , respectively. The temperature is a function of time t, C 1\u2212 t\nT 0 and it decreases with time. The neighboring\nstates are defined as rule sets that are obtained via a small change to the current model. At each iteration, the algorithm improves one of the three terms (accuracy, interpretability, and transparency) with approximately equal probabilities, by removing a rule from the current model or adding a rule to the current model. The proposed neighbor is then accepted with\nprobability exp(\u039b(R [t])\u2212\u039b(R[t+1])\nC 1\u2212 t\nT 0\n) which gradually decreases as the temperature cools down\ntill eventually converging to the final output. Next, we introduce the strategies and theoretical bounds to prune the search space before and during the search.\nRule Space Pruning We first derive a bound to prune the search space before the algorithm begins. Assume the rules in R\u2217 are drawn from candidate rules. We use FPgrowth1 to generate two sets of candidate rules, \u03a5+ for positive rules and \u03a5\u2212 for negative rules. The algorithm will search within this rule space so the complexity of the algorithm is directly determined by the size of the rule space. To reduce the search space, we derive a lower bound on the support of rules in R\u2217.\nTheorem 5 (Bound on Support) \u2200r \u2208 R\u2217+, support(r) \u2265 N\u03b11; \u2200r \u2208 R\u2217\u2212, support(r) \u2265 N\u03b11 1\u2212\u03b12 . This means R\u2217 does not contain rules with too small a support. Rules that fail to satisfy the bounds can be safely deleted from the rule space without hurting the optimality. This theorem is used before the search begins to prune the rule space, which greatly reduces computation. On the other hand, removing rules with low support naturally helps prevent overfitting. The bound increases as \u03b11 increases, since \u03b11 represents the penalty for adding a rule. When \u03b11 is large, the model is encouraged to use fewer rules, thus each rule need to cover enough instances in order to qualify for a spot in the model, therefore they are encouraged to have large support. All proofs are in the supplementary material. Search Chain Bounding We also derive two bounds that dynamically prune the search space during the search. The bounds are applied in each iteration to confine the Markov Chain within promising solution space. We first derive a bound on \u2126(R\u2217), the total number of conditions in R\u2217. Let \u03bb[t] represent the best objective value found till time t, i.e.\n\u03bb[t] = min \u03c4\u2264t \u039b(R[\u03c4 ]).\nWe claim\nTheorem 6 (Bound on Interpretability) \u2126(R\u2217) \u2264 \u03bb[t]+\u03b12\u03b11 . This theorem says that the number of conditions in R\u2217 is upper bounded. The theorem implies that the Markov Chain only needs to focus on the solution space of small models. Therefore, in the proposing step, if the current state violates the bound, neighbors with more conditions should not be proposed, equivalent to pruning the neighbors.\nNext we derive an upper bound on the transparency with the similar purpose of confining the Markov Chain.\nTheorem 7 (Bound on Transparency) support(R\u2217) \u2265 N(\u03b11\u2212\u03bb [t])\n\u03b12 .\nThe theorem says the transparency of R\u2217 is lower bounded. Therefore when proposing the next state, if this theorem is violated, models with smaller transparency should not be proposed.\n1. FP-Growth is an off-the-shelf rule miner. Other rule miners such as Apriori or Eclat can also be used.\nBoth bounds in Theorem 6 and Theorem 7 become smaller as \u03bb[t] continuously gets smaller along the search steps. Exploiting the theorems in the search algorithm, the algorithm checks the intermediate solution at each iteration and pull the search chain back to the promising area (models of sizes smaller than \u03bb [t]+\u03b12 \u03b11\nand transparency larger than N(\u03b11\u2212\u03bb[t])\n\u03b12 ) whenever the bounds are violated, equivalent to pruning the search space at each iteration. The algorithm is presented in Algorithm 1. Here we detail the proposing step. To propose a neighbor, at each iteration, we choose to improve one of the three terms (accuracy, interpretability, and transparency) with approximately equal probabilities. With probability 13 , or when the bound in Theorem 6 is violated, we aim to decrease the size of R[t] (improve interpretability) by removing a rule from R[t] (line 8 - 9). With probability 1 3 or when the lower bound on transparency (Theorem 7) is violated, we aim to increase the coverage of R[t] (improve transparency) by adding a rule to R[t] (line 10-11). Finally, with another probability 13 , we aim to decrease the classification error (improve accuracy) (line 13-26). To decrease the misclassification error, at each iteration, we draw an example from examples misclassified by the current model (line 13). If the example is covered by R[t] (line 14), it means it was sent to the interpretable model but was covered by the wrong rule set. If the instance is negative (line 14), we remove a rule from the positive rule set that covers it. If the instance is positive, it means it is covered by the negative rule set. Then we either add a rule to the positive set to cover it or remove a rule in the negative rule set that covers it, re-routing it to fb. If the example is not covered by R[t] (line 20), it means it was previously sent to the black-box model but misclassified, since we cannot alter the black-box model, we add a rule to the positive or negative rule set (consistent with the label of the instance) to cover the example. When choosing a rule to add or remove, we first evaluate the rules using precision, which is the percentage of correctly classified examples in the rule. Then we balance between exploitation, choosing the best rule, and exploration, selecting a random rule, to avoid getting stuck in a local minimum.\n5. Hybrid Linear Models\nNext, we instantiate the hybrid framework with linear models and call the model Hybrid Linear Model. Our goal is to learn f\u03b9 = flinear = (w, \u03b8+, \u03b8\u2212), where w is the coefficient of a linear model and \u03b8+ and \u03b8\u2212 ( \u03b8+ \u2265 \u03b8\u2212) are two thresholds, in order to form f = \u3008flinear, fb\u3009 as:\nf(x) =  1 if w>x \u2265 \u03b8+ \u22121 if w>x \u2264 \u03b8\u2212 fb(x) otherwise\n(9)\nSimilar to a hybrid rule set, a hybrid linear model partitions the data space into three regions, \u201ccertainly positive\u201d, \u201ccertainly negative\u201d and an undetermined region left to the black-box model."}, {"heading": "5.1 Model Formulation", "text": "We cast the optimization for a hybrid linear model into the hybrid decision-making framework and re-formulate the learning objective.\nLearning Objective\nThe (in-sample) predictive performance characterizes the fitness of the model to the training data. Since fb is pre-given, the predictive performance is determined by two factors, the accuracy of flinear on instances with w\n>x \u2265 \u03b8+ or w>x \u2264 \u03b8\u2212 and the accuracy of fb on the remaining examples. We wish to obtain a good partition of data D by assigning fb and flinear to different region of the data such that the strength of fb and flinear are properly exploited. Second, we include the gap \u03b8+\u2212\u03b8\u2212 as a penalty term in the objective to account for transparency of a hybrid linear model. The smaller gap between \u03b8+ and \u03b8\u2212 there is, more data are classified by the linear model. In the most extreme case where \u03b8+\u2212\u03b8\u2212 = 0, all data are sent to the linear model, and the hybrid linear model is reduced to a pure linear classifier, i.e., transparency equals one. Finally, we also need to consider model regularization in the objective. As the weight for the sparsity enforcing regularization term increases, the model encourages using a smaller number of features which increases the interpretability of the model as well as preventing overfitting.\nCombining the three factors discussed above, we formulate the learning objective for hybrid linear model as:\nF \u2217 := min w,\u03b8+\u2265\u03b8\u2212 {F (w, \u03b8+, \u03b8\u2212) := L(w, \u03b8+, \u03b8\u2212; fb) + \u03b11r(w) + \u03b12(\u03b8+ \u2212 \u03b8\u2212)} , (10)\nwhere L(w, \u03b8+, \u03b8\u2212; fb) is the loss function defined on the training set, \u03b8+ \u2212 \u03b8\u2212 is a penalty term to increase the transparency of f , r is a convex and closed regularization term (e.g. \u2016w\u20161, 12\u2016w\u2016 2 2 or an indicator function of a constraint set), and \u03b11 and \u03b12 are non-negative coefficients which balance the importance of the three components in (10).\nNext, we define L(w, \u03b8+, \u03b8\u2212; fb) in details. According to (9), the hybrid model f will misclassify an instance x(n) in two scenarios. When y\n(n) b = 1, w >x = \u03b8\u2212 becomes the active decision boundary in the sense that x(n) with w>x(n) \u2265 \u03b8\u2212 is predicted as positive (if w>x(n) \u2265 \u03b8+, it is predicted as positive by the linear model; if \u03b8\u2212 \u2264 w>x(n) \u2264 \u03b8+, it is predicted as positive by the black-box since y\n(n) b = 1) and x (n) with w>x < \u03b8\u2212 is\nclassified as negative according to (9). When y (n) b = \u22121, w >x = \u03b8+ becomes the active decision boundary in the sense that x(n) with w>x(n) \u2265 \u03b8+ is predicted as positive and x(n) with w>x(n) < \u03b8+ is classified as negative according to (9) (if w\n>x(n) \u2264 \u03b8\u2212, it is predicted as negative by the linear model; if \u03b8\u2212 \u2264 w>x(n) \u2264 \u03b8+, it is predicted as negative by the black-box since y\n(n) b = \u22121). See Figure 2 for an illustration of the decision boundaries.\nTo define the losses associated to f in these two scenarios, we partition the data into two subsets based on the labels predicted by the black-box {y(n)b } N n=1. In particular, we define\nI+b = {n|y (n) b = 1} and I \u2212 b = {n|y (n) b = \u22121}. (11)\nThe loss function in (10) over the dataset D is then defined as\nL(w, \u03b8+, \u03b8\u2212; fb) = 1\nN \u2211 n\u2208I+b \u03c6(y(n)(w>x(n) \u2212 \u03b8\u2212)) + 1 N \u2211 n\u2208I\u2212b \u03c6(y(n)(w>x(n) \u2212 \u03b8+)), (12)\nwhere \u03c6(z) : R \u2192 R is a non-increasing convex closed loss function which can be one of those commonly used in linear classification such as hinge loss \u03c6(z) = (1\u2212 z)+ smoothed hinge loss \u03c6(z) = 12(1\u2212 z) 2 +\nlogistic loss \u03c6(z) = log(1 + exp(\u2212z)). (13)\nNote that I+b and I \u2212 b form a partition of {1, 2, . . . , N} and each data point corresponds to one loss term in (12). The intuition of this loss function is as follows. Take a data point x(n) with y(n) = 1 and y (n) b = 1 as an example. Our hybrid model (9) will classify x\n(n) correctly as long as x(n) does not fall in the \u201cnegative half space\u201d, namely, w>x(n) > \u03b8\u2212. Hence, with the non-increasing property of \u03c6, the loss term \u03c6(y(n)(w>x(n) \u2212 \u03b8\u2212)) will encourage a positive value of w>x(n) \u2212 \u03b8\u2212. On the other hand, for a data point x(n) with y(n) = 1 and y\n(n) b = \u22121, our hybrid model will classify x (n) correctly only when x(n) falls in the \u201cpositive half space\u201d, namely, w>x(n) \u2265 \u03b8+. We use the loss term \u03c6(y(n)(w>x(n)\u2212\u03b8+)) to encourage a positive value of w>x(n) \u2212 \u03b8+. The true label y(n) = 1 in both examples above but the similar interpretation applies to the case of y(n) = \u22121."}, {"heading": "5.2 Model Training", "text": "With the loss function defined in (12), the hybrid model can be trained by solving the convex minimization problem (10) for which many efficient optimization techniques are available in literature including subgradient methods (Nemirovski et al., 2009; Duchi et al., 2011), accelerated gradient methods (Nesterov, 2013; Beck and Teboulle, 2009), primal-dual methods (Nemirovski, 2004; Chambolle and Pock, 2011) and many stochastic first-order methods based on randomly sampling over coordinates or data (Johnson and Zhang, 2013; Shalev-Shwartz and Zhang, 2014; Duchi et al., 2011; Zhang and Xiao, 2017). The choice of algorithms for (10) depends on various characteristics of the problem such as smoothness, strong convexity and data size.\nSince numerical optimization is not the focus of this paper, we will simply utilize the accelerated proximal gradient method (APG) by Nesterov (2013) to solve (10) when \u03c6 is smooth (e.g. the second and third cases in (13)). When \u03c6 is non-smooth (e.g. the first\nAlgorithm 1 Smoothing Accelerated Proximal Gradient Method for (10) (Nesterov, 2007, 2013)\n1: Input: The initial solution w[0] \u2208 dom(r), (\u03b8[0]+ , \u03b8 [0] \u2212 ) satisfying \u03b8 [0] + \u2265 \u03b8 [0] \u2212 , step length parameters\n\u03b7 > 0 and \u03b10 \u2208 (0, 1), the smoothing parameter \u00b5 \u2265 0 and the number of iterations T . 2: Set (w\u0302[0], \u03b8\u0302\n[0] + , \u03b8\u0302 [0] \u2212 ) = (w [0], \u03b8 [0] + , \u03b8 [0] \u2212 )\n3: for t = 1, 2, . . . , T do 4:\nw[t] = arg min w\u2208Rd\n\u2329 \u2207wL\u00b5(w\u0302[t\u22121], \u03b8\u0302[t\u22121]+ , \u03b8\u0302 [t\u22121] \u2212 ),w \u232a + 1\n2\u03b7 \u2225\u2225\u2225w \u2212 w\u0302[t\u22121]\u2225\u2225\u22252 2 + \u03b11r(w)\n(\u03b8 [t] + , \u03b8 [t] \u2212 ) = arg min\n\u03b8+\u2265\u03b8\u2212  [\u2207\u03b8+L(w\u0302[t\u22121], \u03b8\u0302 [t\u22121] + , \u03b8\u0302 [t\u22121] \u2212 ) + \u03b12]\u03b8+ +[\u2207\u03b8\u2212L(w\u0302[t\u22121], \u03b8\u0302 [t\u22121] + , \u03b8\u0302 [t\u22121] \u2212 )\u2212 \u03b12]\u03b8\u2212 + 12\u03b7 \u2225\u2225\u2225\u2225\u2225 ( \u03b8+ \u03b8\u2212 ) \u2212 ( \u03b8\u0302 [t\u22121] +\n\u03b8\u0302 [t\u22121] \u2212\n)\u2225\u2225\u2225\u2225\u2225 2\n2  5: Compute \u03b1t \u2208 (0, 1) from the equation \u03b12t = (1\u2212 \u03b1t)\u03b12t\u22121 and set \u03b2t =\n\u03b1t\u22121(1\u2212\u03b1t\u22121) \u03b12t\u22121+\u03b1t .\n6:  w\u0302[t]\u03b8\u0302[t]+ \u03b8\u0302 [t] \u2212  =  w[t]\u03b8[t]+ \u03b8 [t] \u2212 + \u03b2t   w[t]\u03b8[t]+ \u03b8 [t] \u2212 \u2212  w[t\u22121]\u03b8[t\u22121]+ \u03b8 [t\u22121] \u2212  \n7: end for 8: Output: w[t], \u03b8\n[t] + , \u03b8 [t] \u2212\ncases in (13)), we apply the well-known smoothing technique (Nesterov, 2007) to create a smooth approximation for \u03c6 and then solve the smooth problem using APG. To describe the smoothing technique, we denote by \u03c6\u2217(\u03b1) : R \u2192 R \u222a {+\u221e} the Fenchel conjugate of \u03c6, namely, \u03c6\u2217(\u03b1) = supz{\u03b1z \u2212 \u03c6(z)}. It is well-known in convex analysis that, given our assumptions on \u03c6, we have \u03c6(z) = sup\u03b1{z\u03b1 \u2212 \u03c6\u2217(\u03b1)}. Let h(\u03b1) : R \u2192 R be a 1-strongly convex function with respect to the Euclidean norm. Given a smoothing parameter \u00b5 \u2265 0, the smooth approximation for \u03c6 can be constructed as \u03c6\u00b5 := sup\u03b1{z\u03b1 \u2212 \u03c6\u2217(\u03b1) \u2212 \u00b5h(\u03b1)}. According to Nesterov (2007), the function \u03c6\u00b5 is smooth and its gradient is \u2207\u03c6\u00b5(z) = arg min\u03b1{z\u03b1\u2212\u03c6\u2217(\u03b1)\u2212\u00b5h(\u03b1)} which has a closed-form for most commonly used \u03c6. According to Nesterov (2007), we have the relationship \u03c6\u00b5(z) \u2264 \u03c6(z) \u2264 \u03c6\u00b5(z) + \u00b5max\u03b1\u2208dom(\u03c6\u2217) h(\u03b1) where dom(\u03c6\u2217) is typically compact for non-smooth \u03c6.2 These facts suggest that, to solve (10) with non-smooth \u03c6, we can approximate \u03c6 with \u03c6\u00b5 and then apply the APG method (Nesterov, 2013) to the smooth problem\nmin w,\u03b8+\u2265\u03b8\u2212\nL\u00b5(w, \u03b8+, \u03b8\u2212; fb) + \u03b11r(w) + \u03b12(\u03b8+ \u2212 \u03b8\u2212), (14)\nwhere\nL\u00b5(w, \u03b8+, \u03b8\u2212; fb) = 1\nN \u2211 n\u2208I+b \u03c6\u00b5(y (n)(w>x(n) \u2212 \u03b8\u2212)) + 1 N \u2211 n\u2208I\u2212b \u03c6\u00b5(y (n)(w>x(n) \u2212 \u03b8+)).\n2. In this paper, we use dom(f) to represent the domain of a function f .\nThe returned solution for (14) will also be a good solution to (10) when \u00b5 is small. Note that when \u00b5 = 0, we have \u03c6\u00b5(z) = \u03c6(z) and (14) and (15) are reduced to (10) and (12). This smoothing APG method is presented as Algorithm 1. When \u03c6 is smooth, one just needs to choose \u00b5 = 0. For simplicity of notation, we have suppressed the dependence of L\u00b5(w, \u03b8+, \u03b8\u2212; fb) on fb.\nThe two subproblems in Step 4 and Step 5 in Algorithm 1 can be solved in closed forms for most commonly used r (e.g., r(w) = \u2016w\u20161). In Algorithm 1, users need to provide a step length parameter \u03b7. In order to guarantee the convergence in theory, this parameter can be chosen as \u03b7 = 1L\u00b5 . Here, L\u00b5 is the Lipchitz continuity constant\n3 of \u2207L\u00b5 which is in the order of O( 1\u00b5) if \u03c6 is non-smooth and \u00b5 > 0 and is identical to the Lipchitz continuity constant \u2207L if \u03c6 is smooth and \u00b5 = 0. When \u03b7 is chosen to be this theoretical value, Algorithm 1 can ensure F (w[t], \u03b8\n[t] + , \u03b8 [t] \u2212 ) \u2212 F \u2217 \u2264 in T = O(1 ) iterations when \u03c6 is non-\nsmooth and in T = O( 1\u221a ) iterations when \u03c6 is smooth. See Nesterov (2007, 2013) for the\ndetails of the convergence analysis. Although a theoretical value for \u03b7 has a closed form, it is generally too conservative and leads to slow convergence in practice. To address this issue, a line search scheme, for example the one proposed in Lin and Xiao (2015), can be applied to adaptively choose \u03b7 during the iterations, which can improve the convergence speed in practice significantly while still preserve the theoretical convergence property. We implement APG with this adaptive line search scheme in our numerical experiments."}, {"heading": "6. Experiments", "text": "We perform a detailed experimental evaluation of hybrid models on public data from domains where interpretability is critical, including healthcare, judiciaries, and business problems. On these datasets, the two forms of interpretable models will collaborate with popular black-box models including ensemble models and neural networks and solve problems on structured data and text data. We first discuss the evaluation metrics, then describe experiments setup and finally analyze the experimental results."}, {"heading": "6.1 Evaluation Metrics", "text": "The goal for building a hybrid model is to provide interpretable and accurate predictions on as many data points as possible. Thus we propose two metrics below to evaluate the predictive performance, interpretability, and transparency. Model Complexity The two types of models we choose (rules and linear models) have naturally interpretable forms. To provide a consistent evaluation of model complexity for hybrid rule sets and hybrid linear models, we measure the number of smallest cognitive units in a model. For rule sets, we count the total number of conditions (feature-value pairs) in association rules. Note that we choose the total number of conditions instead of the number of features because a feature could be used multiple times by different rules in a model. For linear models, we count the number of non-zero coefficients. Both of them represent the amount of units a user needs to comprehend in order to understand a model. Efficient Frontier To evaluate the trade-off between transparency and accuracy, we propose efficient frontiers. An Efficient Frontier represents how much accuracy a model needs\n3. The explicit expression of L\u00b5 can be found in Nesterov (2007).\nto trade to obtain a certain level of transparency. See Figure 3 for an illustration. The curves provide a practical and straightforward way for model selection. Instead of choosing between two discrete choices of either black-box models or interpretable models, now users can find a middle ground where partial transparency and good predictive performance can co-exist, providing one with a wider range of models. Users can choose the best operating point on this curve, based on one\u2019s desired transparency as well as the tolerable loss in predictive performance."}, {"heading": "6.2 Structured Datasets", "text": "We use four structured datasets that are publicly available. 1) census (Kohavi, 1996) (48,842 \u00d7 14) predicts if an indivisual\u2019s annual income is greater than 50K based on the individual\u2019s demographic information, education background, and work information. 2) juvenile(Osofsky, 1995) (4,023 observations and 55 reduced features) studies the consequences of juvenile exposure to violence. The dataset was collected via a survey. 3) coupon (2017 \u00d7 90 binarized features) predicts if a customer will accept a coupon recommended by an in-vehicle recommender system. 4) stop-and-frisk (80,755 \u00d7 27) predicts if someone will be frisked by a police officer, using data from NYPD Stop, Question, and Frisk database from 2014 to 2016.\nBaselines We process each dataset by binarizing all categorical features. For each structured dataset, we build three canonical black-box models as baselines and also later as input for hybrid models, Random Forests (Liaw et al., 2002), AdaBoost (Freund and Schapire, 1995) and extreme gradient boosting trees (XGBoost) (Chen and Guestrin, 2016). We partition each dataset into 80% training and 20% testing. We do 5-fold cross-validation for parameter tuning on the training set to choose the best parameters and then evaluate the best model on the test set. To provide a baseline for interpretability and accuracy comparison, we also build interpretable models. The methods we choose here include traditional decision trees CART (Breiman, 2017), C4.5 (Quinlan, 2014) and C5.0 (Pandya and Pandya, 2015) and more recent rule-based classifiers, Bayesian Rule Sets (BRS) (Wang et al., 2017) and Scalable Bayesian Rule List (Yang et al., 2017). BRS and SBRL are two recent representative methods which have proved to achieve simpler models with competitive predictive\naccuracy compared to the older rule-based classifiers. We also include Lasso (Tibshirani, 1996) to provide a baseline for hybrid linear models.\nImplementations We use R packages (Hornik et al., 2007; Quinlan, 2004) to build interpretable methods except for BRS which has the code publicly available4. For C4.5 and C5.0, we tune the minimum number of samples at splits. For BRS, we set the maximum length of rules to 3 as recommended by the paper (Wang et al., 2017). For BRS, there are parameters \u03b1+, \u03b2+, \u03b1\u2212, \u03b2\u2212 that govern the likelihood of the data. We set \u03b2+, \u03b2\u2212 to 1 and vary \u03b1+, \u03b1\u2212 from {100, 1000, 10000}. For SBRL, there are hyperparameters \u03bb for the expected length of the rule list and \u03b7 for the expected cardinality of the rules in the optimal rule list. We vary \u03bb from {5, 10, 15, 20} and \u03b7 from {1, 2, 3, 4, 5}. To preserve interpretability, all interpretable baselines have the number of rules under 30 (for decision trees we count the number of leaves). Then we use python packages (Pedregosa et al., 2011) for the black-box models. For random forest, we tune the number of trees and the number of features for each tree. For Adaboost, we set the number of estimators to 800 and tune the maximum number of features and the maximum depth of trees. For XGBoost, we set the number of base learners to be 800 and set the learning rate to 0.01. Then we tune the maximum depth of trees, maximum features used by a tree and the minimum samples that need to exist in a leaf. For Lasso, we tuned the regularizing parameter \u03bb. Models with the best cross-validated validation performance are selected.\nThe predictive performance of the black-box and interpretable models are reported in Table 1. On all four datasets, black-box models outperform interpretable models with one exception, Lasso on juvenile dataset. But Lasso uses 303 non-zero coefficients in this model, making it hardly interpretable. To compare interpretability against the hybrid models, we report the number of conditions for rule-based models and the number of non-zero coefficients for Lasso in Figure 4.\nBuilding Hybrid Models Given the black-box models built above, we create hybrid rule sets and hybrid linear models. For hybrid rule sets, we set the maximum length of rules to be 4. We choose \u03b11 from [0.001, 0.06] and choose \u03b12 from [0.001, 0.5] to obtain a list of models. For hybrid linear models, we choose \u03b11 from [0.02, 0.06] in order to select a small number of features. We choose \u03b12 from [0.001, 0.5] to obtain a list of models. When building the hybrid linear model for all datasets, we choose \u03c6 to be the hinge loss in (13), choose r(x) = \u2016x\u20161, and run Algorithm 1 for 5, 000 iterations with \u00b5 = 0.0001. The changes of the objective value in Algorithm 1 is less than 0.01% in the last iterations, indicating the convergence of the algorithm.\n4. https://github.com/wangtongada/BOA\nWe report the run time in Table 2. Hybrid linear models are much more efficient since the search algorithm is based on gradient descent and computationally convenient. Hybrid rule sets, on the other hand, involve evaluating intermediate solutions and proposing next states, and therefore they are computationally heavier.\nEfficient Frontiers Analysis We plot the transparency efficient frontiers for \u3008frules, fb\u3009 and \u3008flinear, fb\u3009 in Figure 4, evaluated on test sets. For easier comparison, we set the range of the Y-axis to be the same for each dataset (row). The black-box baseline models are located at transparency equal to zero in each plot, represented by a black square. The hybrid models span the entire spectrum of transparency with accuracy slowly decreasing as transparency increases till reaching an interpretable model at transparency equal to one, represented by an empty circle. Besides, we show baseline interpretable models at the same row for each dataset. All of them are located at transparency equal to one.\nThe efficient frontiers are concave with different decaying rates for the four datasets. For the census dataset, the curves remain almost flat as transparency is less than 50%. This suggests that transparency is gained at nearly no cost of predictive performance if the transparency is not too large. In addition, this free transparency is obtained using very few conditions for hybrid rule sets, only 4 or 5 conditions (1 or 2 rules in total), compared to the interpretable baselines using a lot more conditions but achieving lower accuracy overall. For the juvenile dataset, it is interesting to observe the accuracy even increases, although slightly, when 80% of the data are sent to an interpretable model, then drops rapidly if continuing sending data to interpretable models. This means the majority of the data can be easily explained by a couple of short rules or simple linear models, and the rest 20% need to be handled by a more advanced black-box model. If forcing everything to interpretable models, not only the performance will decrease, but the model complexity will also increase significantly. We notice that for this dataset, the highest accuracy is achieved by Lasso, 0.91. But Lasso uses 303 non-zero coefficients, turning it into a practically black-box model for having so many cognitive units. The results on census and juvenile datasets proved our hypothesis that even a black-box model is globally better, there might exist a subspace where a very simple model can do just as well or even slightly better, since simpler models are less likely to overfit. Finally, for the last two datasets, coupon recommendation and stop-and-frisk prediction, the efficient frontier shows a clear trade-off between transparency and accuracy, with the accuracy steadily decreases as transparency increases.\nWe observe that hybrid linear models generally use more cognitive units (non-zero coefficients) in a model than hybrid rule sets (conditions). This is because linear models need more features to accurately define a decision boundary in the feature space. A slight change in the coefficients may affect the partitioning of the entire data. Therefore, regularizing the number of coefficients too much will hurt the performance. Thus the model has to keep many non-zero coefficients. On the other hand, hybrid rule sets use association rules, which\nare hyper-cubes in the data space and each are defined by a couple of features only. It has been shown in the literature that allowing rules to have length up to 4 can already accurately place it on correct subsets and achieve high predictive performance (Wang et al., 2017).\nBelow we show examples of hybrid models from two datasets."}, {"heading": "6.2.1 Example 1: Hybrid Models for the Census Dataset", "text": "The census dataset predicts if an individual\u2019s annual income is greater than $50K, based on his/her demographic information (gender, age, marital status, race, etc), education background (degree and the number of years of education) and work information (occupation, hours for work per week, etc). We choose a hybrid rule set model that substitute Ad-\naboost with reasonably large transparency and accuracy. We show the model in Table 3. The positive rule set is empty in this model. The negative rule set consists of two rules that capture \u201ccertainly negative\u201d instances. They cover 71.6% of the data and the overall accuracy is 85.0%.\nWe also show a hybrid linear model that also substitutes Adaboost model in Figure 5. The model identifies the most positive features include the number of years of education, if the marital status is married to a civilian spouse, and if the occupation is executive managerial or professional specialty; the most negative features are if education is highschool graduate and the marital status is never married."}, {"heading": "6.2.2 Example 2: Hybrid Models for the Juvenile Deliquency Dataset", "text": "We show hybrid models built from the juvenile delinquency data. The dataset was collected from a survey where children were asked questions regarding their exposure to violence from family members, friends, or other sources in their community and use that the predict if s/he will commit delinquency. Therefore, each feature is a question from the survey and the corresponding values are respondents\u2019 answer to the questions. For example, question 18 asks \u201chas anyone (including family members or friends) ever attacked you with a gun, knife or some other weapon?\u201d and the answer is \u201cYes\u201d, \u201cNo\u201d, \u201cRefused to answer\u201d, or \u201cNot sure\u201d.\nWe first show rule sets substitutes in an XGBoost model in Table 4. The positive rule set contains one rule with three conditions, and the negative rule set contains one rule with four conditions. They collectively cover and explain 80% of the data without losing any predictive performance from XGBoost alone.\nFor comparison, we show in Figure 6 the hybrid linear model that substitutes Adaboost, with the same accuracy as the hybrid rule set but larger transparency, 85.5%.\nThe two models are very consistent in choosing the features. Both models include question 18A, 18B, 18C, 48AG and 48AH and there is slight difference in using a couple of other questions."}, {"heading": "6.3 Text Classification", "text": "We apply the hybrid models to two text mining tasks. We convert text datasets into bagof-words, and each feature is a word.\nReview Usefulness Classification In this task, the goal is to predict whether a consumer review in Yelp is useful. The dataset we use is a subset of the publically available\nYelp Challenge data 5 in 2017. The subset we select contains all the reviews in the original dataset that are about the businesses in the category of \u201cDoctors\u201d. On Yelp, consumers can vote \u201cuseful\u201d for a review (but cannot vote \u201cnot useful\u201d). We define a useful review as one that had received at least one useful vote before it was collected into this dataset. With this definition, we obtain 25,901 reviews where 12,970 are not useful, and 12,931 are useful. We randomly partition the data into training (20,721 reviews), and testing (5,180 reviews) sets. We then create the document-term matrix (DTM) in terms of TF-IDF after removing punctuation, numbers, and stopwords and applying stemming. In the DTM, we only use the terms with frequency at least 50. The obtained DTM contains 2,315 columns (terms). We used two different black-box models fb for this task. The first model is the extreme gradient boosting trees (XGBoost) (Chen and Guestrin, 2016) with the maximum depth of a tree being 5. The second model is a three-layer fully connected neural network, where the activation function is the sigmoid function, and there are 128 and 16 neurons in the first and second hidden layers. The hyperparameters of the black-box models are selected from a finite grid using hold-out validation.\nMovie Success Prediction The goal of this task is to predict the financial success of a movie based on its professional critics\u2019 reviews. We frame this problem into a binary classification task. In particular, we define as \u201cSuccess\u201d a movie that produced a gross profit of over 30% above its production budget and define as \u201cFailure\u201d otherwise. The dataset is publicly available (Joshi et al., 2010) 6 and contains reviews on 1,718 movies. We have 616 movies (230 successes and 386 failures) remained after eliminating the movies that did not have the required financial information (production budget and gross profit). We randomly partition the data into training (492 movies), and testing (124 movies) sets. After merging all reviews on the same movies into one document, we then create the documentterm matrix (DTM) with the frequencies of terms after removing punctuation, numbers, and stopwords and applying stemming. In the DTM, we only use the terms with frequency\n5. https://www.yelp.com/dataset/challenge 6. www.cs.cmu.edu/~ark/movie$-data/\nat least 100 and appearing in at least five documents. The obtained DTM contains 2,069 columns (terms). We also use two black-box models for this task. The first model is the XGBoost model using the same hyper-parameters as in the previous task. The second model is the same neural network as in the previous task except that there are 64 neurons in the first hidden layers. The hyperparameters of the black-box models are selected from a finite grid using hold-out validation.\nWe train the two types of hybrid models for the tasks above and tune parameters in both methods to obtain a list of models. When training hybrid rule sets, we binarize the data and consider both the existence and the non-existence of words (considering the negation of features), thus doubling the features. When training hybrid linear models, we continue to choose \u03c6 to be the hinge loss and r(x) = \u2016x\u20161 and choose the parameters in Algorithm 1 as in Section 6.2. We vary \u03b12 from 0 to 1 to show the trade-off between transparency and accuracy. For each value of \u03b12, we search \u03b11 over a grid using cross-validation to research a good balance between the classification accuracy on the validation set and the number of non-zero coefficeints in w. In particular, we choose \u03b11 such that the number of non-zero coefficeints is no more than 80 in order to obtain an interprable model. The efficient frontiers evaluated on the test set are shown in Figure 7. For the Yelp review dataset, the curves for the two models are very similar. They decrease slowly when transparency is small and then more quickly when transparency is larger. For the movie review dataset, it is interesting to observe that the hybrid rule sets models are able to outperform the black-box by a little when transparency is small. This is because rules are less likely to overfit especially the black-box model uses thoursands of features when the rules only uses a very small subset.\nCase Study: Yelp Review Usefulness Prediction\nWe examine hybrid models for the yelp review data set. We choose models that collaborate with XGBoost and with reasonably large transparency. Table 5 shows the hybrid rule set model. The positive rule set contains two rules and the negative rule set contains one rule. The rules describe which words need to be included or cannot be included in order to be classified as \u201ccertainly positive\u201d or \u201ccertainly negative\u201d. Figure 8 shows the words selected by the linear model and their corresponding coefficients. Note that the coefficients are all positive in this model, indicating each word listed will contributing to predicting positive (a useful review). However, this does not mean that the linear model only captures the positive class. The threshold for predicting negative is 0.16, which means if a review does not contain any of the words in Figure 8, or only a few words with low weight, then w>x will marginally above 0, but less than 0.16 and the instance is predicted as negative. Only when the scores are large enough, for example, containing many keywords from the list, the model will predict positive. If w>x is between 0.16 and 0.82, the XGBoost model will be activated to make a prediction."}, {"heading": "7. Conclusion and Future Directions", "text": "We proposed a novel framework for learning a Hybrid Predictive Model that integrates an interpretable model with any black-box model. The interpretable model substitutes the black-box on a subset of data to gain transparency at an efficient cost (sometimes no cost) of predictive performance. The model is trained to jointly optimize predictive performance, interpretability, and transparency, with carefully designed training algorithms to achieve the best balance among the three.\nThe main contribution of this work is that we proposed a novel idea of combining interpretable models with black-box models and proved the collaboration of the two benefits transparency and sometimes even accuracy. Hybrid models provide more choices for users. Instead of choosing between black-box models (transparency equal to zero) and interpretable models (transparency equal to one), hybrid models span the entire spectrum of transparency, and users can choose the best operating point based on the desired transparency and tolerable loss in accuracy. Another notable contribution is that our framework is generalizable. In addition to the two types of interpretable substitutes discussed in this paper, our framework can support a variety of interpretable models, such as rule lists (ordered rules), case-based models, decision trees, etc.\nOur model can be combined seamlessly with a black-box explainer, working as a prestep for a black-box explainer: we first construct an interpretable model to capture and explain a sub-region with 100% fidelity and leave the rest of the data to a black-box model to predict and an explainer to provide post hoc explanations.\nFuture Directions\nFor future research, there are several directions worth pursuing. First, we can explore using other forms of interpretable models, such as case-based models: if an instance is close enough to a prototype, the prediction is determined by the prototype; otherwise, it is sent to a black-box model. In addition, other regularization terms of interpretable models can be included in the objective function, such as using fewer features or adding a fairness component that penalizes disparity in different subgroups of data.\nIn this paper, the black-box is pre-trained, and only the outputs are provided during training. It will be interesting to co-train a black-box and an interpretable model. We hypothesize that the co-training is likely to achieve a better balance among accuracy, interpretability, and transparency, but the computation complexity will increase significantly. It remains a question whether the benefits of co-training are worth the effort.\nFinally, the current models only work on structured data and text data. Association rules and linear models are not designed for raw image processing or other problems where the features themselves are not interpretable. The notions of interpretability used in image classification tend to be completely different from those for structured data where each feature is separately meaningful. There are some recent attempts on building interpretable models for image classification (Chen et al., 2018; Zhang et al., 2018) and it will be interesting to combine these models with black-box neural nets such as CNN or ResNet for image classfication."}, {"heading": "Appendix A: Proofs for Theorems", "text": "Proof (of Theorem 5) To prove that an optimal model does not contain any rules with support smaller than a threshold, we prove by contradiction that if there\u2019s a model that contains such a rule z, removing it will always decrease the objective value, thus violating the optimality assumption.\nWe start with positive rules. For a rule r \u2208 R\u2217+, we define\nR\u2217\\r = {z \u2208 R \u2217 +, z 6= r}. (15)\nWe want to find conditions on the support such that the following inequality always holds.\n\u039b(R\u2217\\r) \u2264 \u039b(R \u2217), (16)\nwhere\n\u039b(R\u2217) =`(R\u2217) + \u03b11 \u00b7 \u2126(R\u2217)\u2212 \u03b12 \u00b7 support(R\u2217)\nN (17)\n\u039b(R\u2217\\r) \u2264`(R \u2217 \\r) + \u03b11 \u00b7 (\u2126(R \u2217)\u2212 1)\u2212 \u03b12 \u00b7 support(R\u2217\\r)\nN . (18)\nSince we want inequality (16) to hold for any r \u2208 R\u2217, we upper bound \u039b(R\u2217\\r) by upper bounding `(R\u2217\\r) and support(R \u2217 \\r).\n`(R\u2217\\r) \u2264 `(R \u2217) + support(R\u2217\\r) N , (19)\nwith the minimum achieved when instances originally covered by r are all incorrectly classified after removing r.\nsupport(R\u2217\\r) \u2264 support(R \u2217) (20)\nwith the minimum achieved when all instances originally covered by r are now covered by R\u2217\u2212, therefore does not change the coverage of R\u2217 overall.\nPlugging formula (19) and (26) into equation (18) and combine it with (17) and (16) yields\nsupport(r) \u2264 N\u03b11. (21)\nThus, if support(r) \u2264 N\u03b11, removing it from a R\u2217 will produce a better model. Therefore, such rules do not exist in an optimal model R\u2217.\nThen we follow the similar steps to prove for negative rules. We define R\u2217\\r as a set of rules where r is removed from R\u2217\u2212. The proofs here use the same steps from inequality (15) to (19). The effective coverage, however, equals to support(R\u2217)\u2212 support(R\u2217\\r). Thus\nsupport(r) \u2264 N\u03b11 1\u2212 \u03b12 . (22)\nTo summarize, R\u2217+ does not contain any rules with support(r) \u2264 N\u03b11 and R\u2217\u2212 does not contain any rules with support(r) \u2264 N\u03b111\u2212\u03b12 .\nProof (of Theorem 6) We choose the optimal model found till time t to be the benchmark to compare with R\u2217. Since R\u2217 \u2208 min \u039b(R),\n\u03bb[t] \u2265 \u039b(R\u2217), (23)\nNow we lower bound \u039b(R\u2217), following equation (16)\n\u039b(R\u2217) \u2265 0 + \u03b11\u2126(R\u2217)\u2212 \u03b12 (24)\nCombining inequality (26) and (24) yields\n\u2126(R\u2217) \u2264 \u03bb [t] + \u03b12 \u03b11 . (25)\nProof (of Theorem 7) We again compare R\u2217 with the best model we found till time t and\n\u03bb[t] \u2265 \u039b(R\u2217), (26)\nThen we lower bound \u039b(R\u2217),\n\u039b(R\u2217) \u2265 0 + \u03b11\u2126(R\u2217)\u2212 \u03b12 support(R\u2217)\nN (27)\nIf R\u2217 6= \u2205, then \u2126(R\u2217) \u2265 1, then\n\u03bb[t] \u2265 \u03b11 \u2212 \u03b12 support(R\u2217)\nN , (28)\nThus\nsupport(R\u2217) \u2265 N(\u03b11 \u2212 \u03bb [t])\n\u03b12 (29)"}, {"heading": "Appendix B: Questions listed in Figure 6", "text": "\u2022 Q18A: has anyone (including family members or friends) ever attacked you with a gun, knife or some other weapon, regardless of when it happened or whether you ever reported to police\n\u2022 Q18B: has anyone (including family members or friends) ever physically attacked you without a weapon, but you thought they were trying to kill or seriously injure you?\n\u2022 Q18C: has anyone (including family members or friends) ever threatened you with a gun or knife, but didn\u2019t actually shoot or cut you?\n\u2022 Q18E: has anyone (including family members or friends) ever beat you up with their fists so hard that you were hurt pretty bad?\n\u2022 Q48AF: has your friends ever broken into a vehicle or building to steal something?\n\u2022 Q48AG: has your friends sold hard drugs such as heroin, cocaine, and LSD?\n\u2022 Q48AH: has your friends stolen something worth more than $50 ?"}], "title": "Hybrid Predictive Model: When an Interpretable Model Collaborates with a Black-box Model", "year": 2019}