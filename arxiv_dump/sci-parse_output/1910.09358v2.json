{
  "abstractText": "A salient approach to interpretable machine learning is to restrict modeling to simple models. In the Bayesian framework, this can be pursued by restricting the model structure and prior to favor interpretable models. Fundamentally, however, interpretability is about users\u2019 preferences, not the data generation mechanism; it is more natural to formulate interpretability as a utility function. In this work, we propose an interpretability utility, which explicates the trade-off between explanation fidelity and interpretability in the Bayesian framework. The method consists of two steps. First, a reference model, possibly a black-box Bayesian predictive model which does not compromise accuracy, is fitted to the training data. Second, a proxy model from an interpretable model family that best mimics the predictive behaviour of the reference model is found by optimizing the interpretability utility function. The approach is model agnostic \u2013 neither the interpretable model nor the reference model are restricted to a certain class of models \u2013 and the optimization problem can be solved using standard tools. Through experiments on real-word data sets, using decision trees as interpretable models and Bayesian additive regression models as reference models, we show that for the same level of interpretability, our approach generates more accurate models than the alternative of restricting the prior. We also propose a systematic way to measure stability of interpretabile models constructed by different interpretability approaches and show that our proposed approach generates more stable models.",
  "authors": [
    {
      "affiliations": [],
      "name": "Homayun Afrabandpey"
    },
    {
      "affiliations": [],
      "name": "Tomi Peltola"
    },
    {
      "affiliations": [],
      "name": "Juho Piironen"
    },
    {
      "affiliations": [],
      "name": "Aki Vehtari"
    },
    {
      "affiliations": [],
      "name": "Samuel Kaski"
    }
  ],
  "id": "SP:41c141bc0545fd00c14275184c37938d7e88f7b3",
  "references": [
    {
      "authors": [
        "Hamsa Bastani",
        "Osbert Bastani",
        "Carolyn Kim"
      ],
      "title": "Interpreting predictive models for human-in-the-loop analytics",
      "venue": "arXiv preprint arXiv:1705.08504,",
      "year": 2018
    },
    {
      "authors": [
        "L Breiman",
        "J Friedman",
        "R Olshen",
        "C Stone"
      ],
      "title": "Classification and regression trees",
      "year": 1999
    },
    {
      "authors": [
        "Leo Breiman",
        "Nong Shang"
      ],
      "title": "Born again trees",
      "venue": "Technical Report,",
      "year": 1996
    },
    {
      "authors": [
        "B\u00e9n\u00e9dicte Briand",
        "Gilles R Ducharme",
        "Vanessa Parache",
        "Catherine Mercat-Rommens"
      ],
      "title": "A similarity measure to assess the stability of classification trees",
      "venue": "Computational Statistics & Data Analysis,",
      "year": 2009
    },
    {
      "authors": [
        "Hugh A Chipman",
        "Edward I George",
        "Robert E McCulloch"
      ],
      "title": "Bayesian CART model search",
      "venue": "Journal of the American Statistical Association,",
      "year": 1998
    },
    {
      "authors": [
        "Hugh A. Chipman",
        "Edward I. George",
        "Robert E McCulloch"
      ],
      "title": "BART: Bayesian additive regression trees",
      "venue": "The Annals of Applied Statistics,",
      "year": 2010
    },
    {
      "authors": [
        "Paulo Cortez",
        "Ant\u00f3nio Cerdeira",
        "Fernando Almeida",
        "Telmo Matos",
        "Jos\u00e9 Reis"
      ],
      "title": "Modeling wine preferences by data mining from physicochemical properties",
      "venue": "Decision Support Systems,",
      "year": 2009
    },
    {
      "authors": [
        "Mark Craven",
        "Jude W Shavlik"
      ],
      "title": "Extracting tree-structured representations of trained networks",
      "venue": "In Advances in neural information processing systems,",
      "year": 1996
    },
    {
      "authors": [
        "Houtao Deng"
      ],
      "title": "Interpreting tree ensembles with intrees",
      "venue": "International Journal of Data Science and Analytics,",
      "year": 2019
    },
    {
      "authors": [
        "Finale Doshi-Velez",
        "Been Kim"
      ],
      "title": "Towards a rigorous science of interpretable machine learning",
      "venue": "arXiv preprint arXiv:1702.08608,",
      "year": 2017
    },
    {
      "authors": [
        "Mengnan Du",
        "Ninghao Liu",
        "Xia Hu"
      ],
      "title": "Techniques for interpretable machine learning",
      "venue": "arXiv preprint arXiv:1808.00033,",
      "year": 2018
    },
    {
      "authors": [
        "Hadi Fanaee-T",
        "Joao Gama"
      ],
      "title": "Event labeling combining ensemble detectors and background knowledge",
      "venue": "Progress in Artificial Intelligence,",
      "year": 2014
    },
    {
      "authors": [
        "Yarin Gal",
        "Zoubin Ghahramani"
      ],
      "title": "Bayesian convolutional neural networks with Bernoulli approximate variational inference",
      "venue": "In 4th International Conference on Learning Representations (ICLR) workshop track,",
      "year": 2016
    },
    {
      "authors": [
        "Yarin Gal",
        "Zoubin Ghahramani"
      ],
      "title": "Dropout as a Bayesian approximation: Representing model uncertainty in deep learning",
      "venue": "In Proceedings of the 33rd International Conference on Machine Learning,",
      "year": 2016
    },
    {
      "authors": [
        "Jingyi Guo",
        "Andrea Riebler",
        "H\u030aavard Rue"
      ],
      "title": "Bayesian bivariate meta-analysis of diagnostic test studies with interpretable priors",
      "venue": "Statistics in medicine,",
      "year": 2017
    },
    {
      "authors": [
        "Satoshi Hara",
        "Kohei Hayashi"
      ],
      "title": "Making tree ensembles interpretable: A Bayesian model selection approach",
      "venue": "In International Conference on Artificial Intelligence and Statistics,",
      "year": 2018
    },
    {
      "authors": [
        "David Harrison Jr.",
        "Daniel L Rubinfeld"
      ],
      "title": "Hedonic housing prices and the demand for clean air",
      "venue": "Journal of environmental economics and management,",
      "year": 1978
    },
    {
      "authors": [
        "Belinda Hern\u00e1ndez",
        "Adrian E Raftery",
        "Stephen R Pennington",
        "Andrew C Parnell"
      ],
      "title": "Bayesian additive regression trees using bayesian model averaging",
      "venue": "Statistics and computing,",
      "year": 2018
    },
    {
      "authors": [
        "David C Hoaglin",
        "Paul F Velleman"
      ],
      "title": "A critical look at some analyses of major league baseball salaries",
      "venue": "The American Statistician,",
      "year": 1995
    },
    {
      "authors": [
        "Roger W Johnson"
      ],
      "title": "Fitting percentage of body fat to simple body measurements",
      "venue": "Journal of Statistics Education,",
      "year": 1996
    },
    {
      "authors": [
        "Jongbin Jung",
        "Connor Concannon",
        "Ravi Shroff",
        "Sharad Goel",
        "Daniel G Goldstein"
      ],
      "title": "Simple rules for complex decisions",
      "venue": "arXiv preprint arXiv:1702.04690,",
      "year": 2017
    },
    {
      "authors": [
        "Dennis Kibler",
        "David W Aha",
        "Marc K Albert"
      ],
      "title": "Instance-based prediction of real-valued attributes",
      "venue": "Computational Intelligence,",
      "year": 1989
    },
    {
      "authors": [
        "Been Kim",
        "Elena Glassman",
        "Brittney Johnson",
        "Julie Shah"
      ],
      "title": "ibcm: Interactive Bayesian case model empowering humans via intuitive interaction",
      "venue": "Technical Report: MIT-CSAIL- TR,",
      "year": 2015
    },
    {
      "authors": [
        "John K Kruschke"
      ],
      "title": "Bayesian estimation supersedes the t test",
      "venue": "Journal of Experimental Psychology: General,",
      "year": 2013
    },
    {
      "authors": [
        "Deepthi Praveenlal Kuttichira",
        "Sunil Gupta",
        "Cheng Li",
        "Santu Rana",
        "Svetha Venkatesh"
      ],
      "title": "Explaining black-box models using interpretable surrogates",
      "venue": "In Pacific Rim International Conference on Artificial Intelligence,",
      "year": 2019
    },
    {
      "authors": [
        "Isaac Lage",
        "Andrew Slavin Ross",
        "Been Kim",
        "Samuel J Gershman",
        "Finale Doshi-Velez"
      ],
      "title": "Human-in-the-loop interpretability prior",
      "venue": "arXiv preprint arXiv:1805.11571,",
      "year": 2018
    },
    {
      "authors": [
        "Himabindu Lakkaraju",
        "Stephen H Bach",
        "Jure Leskovec"
      ],
      "title": "Interpretable decision sets: A joint framework for description and prediction",
      "venue": "In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining,",
      "year": 2016
    },
    {
      "authors": [
        "Himabindu Lakkaraju",
        "Ece Kamar",
        "Rich Caruana",
        "Jure Leskovec"
      ],
      "title": "Faithful and customizable explanations of black box models",
      "venue": "In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society,",
      "year": 2019
    },
    {
      "authors": [
        "Yann LeCun",
        "L\u00e9on Bottou",
        "Yoshua Bengio",
        "Patrick Haffner"
      ],
      "title": "Gradient-based learning applied to document recognition",
      "venue": "Proceedings of the IEEE,",
      "year": 1998
    },
    {
      "authors": [
        "Benjamin Letham",
        "Cynthia Rudin",
        "Tyler H McCormick",
        "David Madigan"
      ],
      "title": "Interpretable classifiers using rules and Bayesian analysis: Building a better stroke prediction model",
      "venue": "The Annals of Applied Statistics,",
      "year": 2015
    },
    {
      "authors": [
        "Zachary C Lipton"
      ],
      "title": "The mythos of model interpretability",
      "venue": "Communications of the ACM,",
      "year": 2018
    },
    {
      "authors": [
        "Yin Lou",
        "Rich Caruana",
        "Johannes Gehrke"
      ],
      "title": "Intelligible models for classification and regression",
      "venue": "In Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining,",
      "year": 2012
    },
    {
      "authors": [
        "Tomi Peltola"
      ],
      "title": "Local interpretable model-agnostic explanations of Bayesian predictive models via kullback-leibler projections",
      "venue": "arXiv preprint arXiv:1810.02678,",
      "year": 2018
    },
    {
      "authors": [
        "Juho Piironen",
        "Markus Paasiniemi",
        "Aki Vehtari"
      ],
      "title": "Projective inference in highdimensional problems: Prediction and feature selection",
      "venue": "arXiv preprint arXiv:1810.02406,",
      "year": 2018
    },
    {
      "authors": [
        "Anna-Lena Popkes",
        "Hiske Overweg",
        "Ari Ercole",
        "Yingzhen Li",
        "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato",
        "Yordan Zaykov",
        "Cheng Zhang"
      ],
      "title": "Interpretable outcome prediction with sparse Bayesian neural networks in intensive care",
      "year": 1905
    },
    {
      "authors": [
        "J Ross Quinlan"
      ],
      "title": "Combining instance-based and model-based learning",
      "venue": "In Proceedings of the tenth international conference on machine learning,",
      "year": 1993
    },
    {
      "authors": [
        "Marco Tulio Ribeiro",
        "Sameer Singh",
        "Carlos Guestrin"
      ],
      "title": "Why should i trust you?: Explaining the predictions of any classifier",
      "venue": "In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining,",
      "year": 2016
    },
    {
      "authors": [
        "Xiaogang Su",
        "Morgan Wang",
        "Juanjuan Fan"
      ],
      "title": "Maximum likelihood regression trees",
      "venue": "Journal of Computational and Graphical Statistics,",
      "year": 2004
    },
    {
      "authors": [
        "Iiris Sundin",
        "Tomi Peltola",
        "Luana Micallef",
        "Homayun Afrabandpey",
        "Marta Soare",
        "Muntasir Mamun Majumder",
        "Pedram Daee",
        "Chen He",
        "Baris Serim",
        "Aki Havulinna"
      ],
      "title": "Improving genomics-based predictions for precision medicine through active elicitation of expert",
      "venue": "knowledge. Bioinformatics,",
      "year": 2018
    },
    {
      "authors": [
        "Berk Ustun",
        "Cynthia Rudin"
      ],
      "title": "Supersparse linear integer models for optimized medical scoring systems",
      "venue": "Machine Learning,",
      "year": 2016
    },
    {
      "authors": [
        "Aki Vehtari",
        "Janne Ojanen"
      ],
      "title": "A survey of Bayesian predictive methods for model assessment, selection and comparison",
      "venue": "Statistics Surveys,",
      "year": 2012
    },
    {
      "authors": [
        "Tong Wang"
      ],
      "title": "Multi-value rule sets for interpretable classification with feature-efficient representations",
      "venue": "In Advances in Neural Information Processing Systems,",
      "year": 2018
    },
    {
      "authors": [
        "Tong Wang",
        "Cynthia Rudin",
        "Finale Doshi-Velez",
        "Yimin Liu",
        "Erica Klampfl",
        "Perry Mac"
      ],
      "title": "Neille. A Bayesian framework for learning rule sets for interpretable classification",
      "venue": "The Journal of Machine Learning Research,",
      "year": 2017
    },
    {
      "authors": [
        "Mike Wu",
        "Michael C Hughes",
        "Sonali Parbhoo",
        "Maurizio Zazzi",
        "Volker Roth",
        "Finale Doshi-Velez"
      ],
      "title": "Beyond sparsity: Tree regularization of deep models for interpretability",
      "venue": "In Thirty-Second AAAI Conference on Artificial Intelligence,",
      "year": 2018
    },
    {
      "authors": [
        "Hongyu Yang",
        "Cynthia Rudin",
        "Margo Seltzer"
      ],
      "title": "Scalable Bayesian rule lists",
      "venue": "In Proceedings of the 34th International Conference on Machine Learning-Volume",
      "year": 2017
    },
    {
      "authors": [
        "Yichen Zhou",
        "Giles Hooker"
      ],
      "title": "Interpreting models via single tree approximation",
      "venue": "arXiv preprint arXiv:1610.09036,",
      "year": 2016
    }
  ],
  "sections": [
    {
      "heading": "1 Introduction and Background",
      "text": "Accurate machine learning (ML) models are usually complex and opaque, even to the modelers who built them [32]. This lack of interpretability remains a key barrier to the adoption of ML models in some applications including health care and economy. To bridge this gap, there is growing interest among the ML community to interpretability methods. Such methods can be divided into (i) interpretable model construction, and (ii) post-hoc interpretation. The former aims at constructing models that are understandable. Post-hoc interpretation approaches can be categorized further into (i) model-level interpretation (a.k.a. global interpretation), and (ii)\nar X\niv :1\n91 0.\n09 35\n8v 2\n[ cs\n.L G\n] 3\nprediction-level interpretation (a.k.a. local interpretation) [12]. Model-level interpretation aims at making existing black-box models interpretable. Prediction-level interpretation aims at explaining each individual prediction made by the model [11]. In this paper, we focus mostly on post-hoc interpretation.\nPrior research on the construction of interpretable models has mainly focused on restricting modeling to simple and easy-to-understand models. Examples of such models include sparse linear models [42], generalized additive models [33], decision sets [28], and rule lists [22]. In the Bayesian framework, this approach maps to defining model structure and prior distributions that favor interpretable models [31, 37, 44, 45]. We call this approach interpretability prior. Letham et al. [31] established an interpretability prior approach for classification by use of decision lists. Interpretability measures used to define the priors were (i) the number of rules in the list and (ii) the size of the rules (number of statements in the left-hand side of rules). A prior distribution was defined over rule lists to favor decision lists with a small number of short rules. Wang et al. [45] developed two probabilistic models for interpretable classification by constructing rule sets in the form of Disjunctive Normal Forms (DNFs). In this work, interpretability is achieved similar to [31], using prior distributions which favor rule sets with a smaller number of short rules. In [44], the authors extended [45] by presenting a multi-value rule set for interpretable classification, which allows multiple values per condition and thereby induces more concise rules compared to single-value rules. As in [45], interpretability is characterized by a prior distribution that favors a smaller number of short rules. Popkes et al. [37] built up an interpretable Bayesian neural network for clinical decision-making tasks, where interpretability is attained by employing a sparsity-inducing prior over feature weights. For more examples, see [16, 17, 24, 47].\nA common practice in model-level interpretability is to use simple models as interpretable surrogates to highly predictive black-box models [1, 8, 26, 29, 48]. Craven and Shavlik [8] were among the first to adopt this approach for explaining neural networks. They used decision trees as surrogates and trained them to approximate predictions of a neural network. In [48], the authors presented an approach to approximate the predictive behavior of a random forest by use of a single decision tree. With the same objective as [48], Bastani et al. [1] developed an approach to interpret random forests using simple decision trees as surrogates. They employed active learning to construct more accurate decision trees with help from a human. Lakkaraju et al. [29] established an approach to interpret black-box classifiers by highlighting the behavior of the black-box model in subspaces characterized by features of user interest. In [26], the authors used decision trees to extract rules to describe the decision-making behavior of black-box models. For more examples of this approach, see [3, 9, 34, 46]. The common characteristic of these approaches is that they seek an optimal trade-off between interpretability of the surrogate model and its faithfulness to the black-box model. To the best of our knowledge, there is no Bayesian counterpart for this approach in the interpretability literature.\nWe argue that an interpretability prior is not the best way to optimize interpretability in the Bayesian framework for the following reasons:\n1. Interpretability is about users\u2019 preferences, not about our assumptions about the data. The prior is meant for the latter. One should distinguish the data generation mechanism from the decision-making process, which in this case includes optimization of interpretability.\n2. Optimizing interpretability may sacrifice some of the accuracy of the model. If interpretability is pursued by revising the prior, there is no reason why the trade-off between accuracy and interpretability would be optimal. This has been shown for a different but related scenario in [36] where the authors showed that fitting a model using sparsity-inducing\nTo address these concerns, we develop a general principle for interpretability in the Bayesian framework, formalizing the idea of approximating black-box models with interpretable surrogates. The approach can be used to both constructing, from scratch, interpretable Bayesian predictive models, or to interpreting existing black-box Bayesian predictive models. The approach consists of two steps: first, a highly accurate Bayesian predictive model, called a reference model, is fitted to the training data without compromising the accuracy. In the second step, an interpretable surrogate model is constructed which best describes locally or globally the behavior of the reference model. The proxy model is obtained by optimizing a utility function, referred to as interpretability utility, which consists of two terms: (i) a term to minimize the discrepancy of the proxy model from the reference model, and (ii) a term to penalize the complexity of the model to make the proxy model as interpretable as possible. Term (i) corresponds to selection of reference predictive model in the Bayesian framework [43, Section 3.3].\nThe proposed approach can be used both for constructing interpretable Bayesian predictive models and to generate post-hoc interpretation for black-box Bayesian predictive models. When using the approach for post-hoc interpretability, it can be used to generate both global or local interpretation. The approach is model-agnostic, meaning that neither the reference model nor the interpretable proxy are constrained to a particular model family. However, when using the approach to construct interpretable Bayesian predictive models, the surrogate model should be from the family of Bayesian predictive models. We also emphasize that the proposed approach is feasible for non-Bayesian models as well, which can be interpreted to produce point estimates of the parameters of the model instead of posterior distributions. Table 1 compares the characteristics of the proposed approach with some of the related works from literature.\nWe demonstrate with experiments on real-world data sets that the proposed approach generates more accurate and more stable interpretable models than the alternative of fitting an a priori interpretable model to the data, i.e., using the interpretability prior approach. For the experiments in this paper, decision trees and logistic regression were used as interpretable proxies, and Bayesian additive regression tree (BART) models [6], Bayesian neural networks, and Gaussian Processes (GP) were used as reference models."
    },
    {
      "heading": "1.1 Our Contributions:",
      "text": "Main contributions of this paper are:\n\u2022 We propose a principle for interpretable Bayesian predictive modeling. It combines a reference model with interpretability utility to produce more interpretable models in a decision-theoretically justified way. The proposed approach is model agnostic and can be used with different notions of interpretability.\n\u2022 For the special case of classification and regression tree (CART) [2] as interpretable models and BART as the black-box Bayesian predictive model, we show that the proposed approach outperforms the earlier interpretability prior approach in accuracy, explicating the tradeoff between explanation fidelity and interpretability. Further, through experiments with different reference models, i.e., GP and BART, we demonstrate that the predictive power of the reference model positively affects the accuracy of the interpretable model. We also demonstrate that our proposed approach can find a better trade-off between accuracy and interpretability when compared to its non-Bayesian counterparts, i.e., BATrees [3] and node harvest [34].\n\u2022 We propose a systematic approach to compare stability of interpretable models and show that the proposed method produces more stable models."
    },
    {
      "heading": "2 Motivation",
      "text": "In this section, we discuss the motivation for formulating interpretability optimization in the Bayesian framework as a utility function. We also discuss how this formulation allows to account for model uncertainty in the explanation. Both discussions are accompanied with illustrative examples."
    },
    {
      "heading": "2.1 Interpretability as a Decision-Making Problem",
      "text": "Bayesian modeling allows encoding prior information into the prior probability distribution (similarly, one might use regularization in maximum likelihood based inference). This might be tempting to change the prior distribution to favor models that are easier for humans to understand, as has been done in earlier works, using some measure of interpretability. A simple example is to use shrinkage priors in linear regression to find a smaller set of practically important covariates. However, we argue that based on the observation, interpretability is not an inherent characteristic of data generation processes. The approach can be misleading and results in leaking user preferences about interpretability into the model of the data generation process.\nWe suggest to separate the construction of a model for the data generating process from construction of an interpretable proxy model. In a prediction task, the former corresponds to building a model that predicts as accurately as possible, without restricting it to be interpretable. Interpretability is introduced in the second stage by building an interpretable proxy to explain the behavior of the predictive model. We consider the second step as a decision-making problem, where the task is to choose a proxy model that trades off between human interpretability and fidelity (w.r.t. the original model)."
    },
    {
      "heading": "2.2 The Issue with Interpretability in the Prior",
      "text": "Let M denote the assumptions about the data generating process and I the preferences toward interpretability. Consider an observation model for data y, p(y | \u03b8,M), and alternative prior distributions p(\u03b8 |M) and p(\u03b8 |M, I). Here, \u03b8 can, for example, be continuous model parameters (e.g., weights in a regression or classification model) or it can index a set of alternative models (e.g., each configuration of \u03b8 could correspond to using some subset of input variables in a predictive model). Clearly, the posterior distributions p(\u03b8 |D,M) and p(\u03b8 |D,M, I) (and their corresponding posterior predictive distributions) are in general different and the latter includes a bias towards interpretable models. In particular, when I does not correspond to prior information about the data generation process, there is no guarantee that p(\u03b8 |D,M, I) provides a reasonable quantification of our knowledge of \u03b8 given the observations D, or that, p(y\u0303 | D,M, I) provides good predictions. We will give an example of this below. In the special case, where I does describe the data generation process, it can directly be included in M.\nLage et al [27] propose to find interpretable models in two steps: (1) fit a set of models to data and take ones that give high enough predictive accuracy, (2) build a prior over these models, based on an indirect measure of user interpretability (human interpretability score). In practice, the process requires the set of models for step 1 to contain interpretable models, which means that there is still the possibility of leaking user preferences for interpretability into the knowledge about the data generation process. This may lead to an unreasonable trade-off between accuracy and interpretability."
    },
    {
      "heading": "2.2.1 Illustrative Example",
      "text": "We give an example to illustrate the effect of adding interpretability constraints to the prior distribution when these constraints do not match data generating process. For simplicity, we define a single interpretability constraint which is over the structure of the model: regression tree with a fixed depth of 4. The interpretability prior approach corresponds to fitting an interpretable model with the above constraint directly to the training data. In the alternative approach, first a reference model is fitted to the data, and then the reference model is approximated with a proxy model that satisfies the interpretability constraint, using the interpretability utility introduced in Section 3. For simplicity of visualization, we use a one-dimensional smooth function as the data-generating process, with Gaussian noise added to observations (Figure 1:left, black curve and red dots). Regression tree is a piece-wise constant function which does not correspond to the true prior knowledge about the ground-truth function, i.e. being a 1D smooth function. A Gaussian process with the MLP kernel function is used as a reference model for the two-stage approach (Figure 1:left, magenta).\nThe regression tree of depth 4 fitted directly to the data (blue line) overfits and does not give an accurate representation of the underlying data generation process (black line). The two-stage approach, on the other hand, gives a clearly better representation of the smooth, increasing function. This is because the reference model (green line) captures the smoothness of the underlying data generation process and this is transferred to the regression tree (magenta line). The choice of the complexity of the interpretable model is also easier because the tree can only \u201coverfit\u201d to the reference model, meaning that it becomes a more accurate (but possibly less easy to interpret) representation of the reference model as shown in Figure 1:right."
    },
    {
      "heading": "2.3 Interpreting Uncertainty",
      "text": "In many applications, such as medical treatment effectiveness prediction [41], knowing the uncertainty in the prediction is important. Any explanation of the predictive model should also provide insight about the uncertainties and their sources. The posterior predictive distribution of the reference model contains both the aleatoric (predictive uncertainty given the model parameter, i.e., noise in the output) and the epistemic uncertainty (uncertainty about model parameters). We can capture both of these into our interpretable model, since it is fitted to match the reference posterior predictive distribution. The former is captured by conditioning the interpretable model on a posterior draw from the reference model, while the latter is captured by fitting the interpretable model on multiple posterior draws. Details will be given later in Section 3. Here, we demonstrate with an example that the proposed method can provide useful information about model uncertainty."
    },
    {
      "heading": "2.3.1 Practical Example",
      "text": "We demonstrate uncertainty interpretation in locally explaining a prediction of a Bayesian deep convolutional neural network in the MNIST dataset of images of digits [30]. The reference model is classifying between digits 3 and 8. We use the Bernoulli dropout method [14, 15], with a\ndropout probability of 0.2 and 20 Monte Carlo samples at test time, to approximate Bayesian neural network inference (the posterior predictive distribution). Logistic regression is used as the interpretable model family1.\nSince we are classifying images, we can conveniently visualize the explanation model. Figure 2 shows visually the logistic regression weights for a digit, comparing the reference model in an early training phase (upper row) and fully trained (lower row). The mean explanations show that the fully trained model has spatially smooth contributions to the class probability, while the model in early training is noisy. Moreover, being able to look at the explanations of individual posterior predictive samples illustrates the epistemic uncertainty. For example, the reference model in early training has not yet been able to confidently assign the upper loop to either indicate a 3 or an 8 (samples 1 and 2 have reddish loop, while sample 3 has bluish). Indeed, the variance plot shows that the model variance spreads evenly over the digit. On the other hand, the fully trained model has little uncertainty about which parts of the digit indicate a 3 or an 8, with most model uncertainty being about the magnitude of the contributions."
    },
    {
      "heading": "3 Method: Interpretability Utility for Bayesian Predictive",
      "text": "Models\nHere we first explain the procedure to obtain interpretability utility for regression tasks. The case of classification models is similar and is explained in Section 3.3.\n1The optimization of the interpretable model follows the general framework explained in Section 3, with logistic regression used as the interpretable model family instead of CART. No penalty for complexity was used here, since the logistic regression model weights are easy to visualize as pseudo-colored pixels"
    },
    {
      "heading": "3.1 Regression models",
      "text": "Let D = {(xn, yn)}Nn=1 denote a training set of size N , where xi = [xi1, \u00b7 \u00b7 \u00b7 , xid] T\nis a ddimensional feature vector and yi \u2208 R is the target variable. Assume that a highly predictive (reference) modelM is fitted to the training data without concerning interpretability constraints. Denote the likelihood of the reference model by p (y |x,\u03b8,M) and the posterior distribution p (\u03b8 |D,M). Posterior predictive distribution of the reference model obtains as p(y\u0303 | D) =\u222b \u03b8 p(y\u0303 | \u03b8)p(\u03b8 | D)d\u03b8. Our goal is to find an interpretable model that best explains the behavior of the reference model locally or globally. We introduce an interpretable model family T with likelihood p(y | x,\u03b7, T ) and posterior p(\u03b7 | D, T ), belongs to a probabilistic model family with parameters \u03b7. The best interpretable model is the one closest to the reference model prediction\u2013 wise, and at the same time easily interpretable. To measure the closeness of the predictive behavior of the interpretable model to the reference model, we compute the Kullback-Leibler (KL) divergence between their posterior predictive distribution. Assuming we want to locally interpret the reference model, and following simplifications of [36] for computing the KL divergence of posterior predictive distributions, the best interpretable model can be found by optimizing the following utility function:\n\u03b7\u0302 = arg min \u03b7\n\u222b \u03c0x(z)KL [p(y\u0303 |z,\u03b8,M) \u2016 p(y\u0303 |z,\u03b7, T )] dz + \u2126(\u03b7) (1)\nwhere KL denotes the KL divergence, \u2126 is the penalty function for the complexity of the interpretable model, and \u03c0x(z) is a probability distribution defining the local neighborhood around x, data point the prediction of which is to be explained. Minimization of the KL divergence verifies that the interpretable model has similar predictive performance to the reference model while the complexity penalty cares for the interpretability of the model.\nWe compute the expectation in Eq. 1 with Monte Carlo approximation by drawing {zs}Ss=1 samples from \u03c0x(z):\n\u03b7\u0302(l) = arg min \u03b7\n1\nS S\u2211 s=1 KL [ p(y\u0303s |zs,\u03b8(l),M) \u2016 p(y\u0303s |zs,\u03b7, T ) ] + \u2126(\u03b7), (2)\nfor l = 1, . . . , L posterior draws from p(\u03b8 | D,M). Eq. 2 can be solved by first drawing a sample \u03b8(l) from the posterior of the reference model and then finding a sample \u03b7(l) from the posterior of the interpretable model that minimizes the objective function. It has been shown in [36] that minimization of the KL-divergence in Eq. 2 is equivalent to maximizing the expected log-likelihood of the interpretable model over the likelihood obtained by a posterior draw from the reference model:\narg max \u03b7\n1\nS S\u2211 s=1 Ey\u0303s|zs,\u03b8(l) [log p (y\u0303s |zs,\u03b7)] . (3)\nUsing this equivalent form and by adding the complexity penalty term, the interpretability utility obtains as\narg max \u03b7\n1\nS S\u2211 s=1 Ey\u0303s|zs,\u03b8(l) [log p (y\u0303s |zs,\u03b7)]\u2212 \u2126 (\u03b7) . (4)\nThe complexity penalty term should be chosen to match the resulting model; possible options are the number of leaf nodes for decision trees, number of rules and/or size of the rules for rule list models, number of non-zero weights for linear regression models, etc. Although the proposed approach is general and can be used for any family of interpretable models, in the\nfollowing, we use CART models with tree size (the number of leaf nodes) as the measure of interpretability. With this assumption, similar to the illustrative example in Section 2.2.1, the interpretability constraint is defined over the model space; it could also be defined over the parameter space of a particular model, such as tree shape parameters of Bayesian CART models [5]. The interpretability prior approach corresponds to fitting a CART model to the training data, i.e. samples drawn from the neighborhood distribution of x.\nA CART model describes p(y |z,\u03b7) with two main components \u03b7 = (T,\u03c6): a binary tree T with b terminal nodes and a parameter vector \u03c6 = (\u03c61, \u03c62, \u00b7 \u00b7 \u00b7 , \u03c6b) that associates the parameter value \u03c6i with the ith terminal node. If z lies in the region corresponding to the ith terminal node, then y |z,\u03b7 has distribution f(y |\u03c6i), where f denotes a parametric probability distribution with parameter \u03c6i. For CART models, it is typically assumed that, conditionally on \u03b7, values y within a terminal node are independently and identically distributed, and y values across terminal nodes are independent. In this case, the corresponding likelihood of the interpretable model for the lth draw from the posterior of \u03b8 has the form\np ( y |Z,\u03b7(l) ) = b\u220f i=1 f ( yi |\u03c6(l)i ) = b\u220f i=1 ni\u220f j=1 f ( yij |\u03c6(l)i ) , (5)\nwhere yi \u2261 (yi1, \u00b7 \u00b7 \u00b7 , yini) denotes the set of the ni observations assigned to the partition generated by the ith terminal node with parameter \u03c6\n(l) i , and Z is the matrix of all the zs. For\nregression problems, assuming a mean-shift normal model for each terminal node i,2 the likelihood of the interpretable model is defined as\nf ( y | \u03c6(l) ) = b\u220f i=1 ni\u220f j=1 N ( yij |\u00b5(l)i , \u03c3 2(l) ) , (6)\nwhere \u03c6(l) = (\u00b5(l) = {\u00b5(l)i }bi=1, \u03c32 (l)\n). With this formulation, the task of finding an interpretable proxy to the reference model M is reformed to find a tree structure T with parameters \u03c6(l) such that its predictive performance is as close as possible to M , while being as interpretable as possible. Interpretability is measured by the complexity term \u2126.\nThe log-likelihood of the tree with the S samples drawn from the neighborhood of x is\nL = \u2212S 2 log(2\u03c0\u03c32)\u2212 1 2\u03c32 b\u2211 i=1 ni\u2211 j=1 (yij \u2212 \u00b5i)2 . (7)\nProjecting this into Eq. 4, the interpretability utility has the following form:\narg max \u03b7 \u22121 2 log(2\u03c0\u03c32)\u2212 1 2S\u03c32 b\u2211 i=1 ni\u2211 j=1 Eyij |\u03b8(l) [ (yij \u2212 \u00b5i)2 ] \u2212 \u2126(T )\n\u221d arg max \u03b7 \u22121 2 log(2\u03c0\u03c32)\u2212 1 2S\u03c32 b\u2211 i=1 ni\u2211 j=1 [ \u03c32ij + (y\u0304ij \u2212 \u00b5i) 2 ] \u2212 \u2126(T ),\n(8)\nwhere y\u0304ij and \u03c3 2 ij are respectively the mean and variance of the reference model for the jth sample in the ith terminal node. \u2126(T ) is a function of the interpretability of the CART model. Here we set it to \u03b1b using \u03b1 as a regularization parameter. The pseudocode of the proposed approach is shown in Algorithm 1.\n2In the mean-variance shift model, each terminal node has its own \u03c32i variable and the number of parameters is 2 \u00d7 b.\nAlgorithm 1: Decision\u2013theoretic approach for local interpretability in the Bayesian framework\nInput: training data D = {(xn, yn)}Nn=1, a test sample xtest to be explained Output: a decision tree explaining the prediction for the test sample xtest\n/* REFERENCE MODEL CONSTRUCTION */ fit the Bayesian predictive model to D without concerning interpretability constraints; draw {zs}Ss=1 from the neighborhood of xtest defined by \u03c0x; for each draw zs do\nget the mean y\u0304s and variance \u03c3 2 s of the Bayesian predictive distribution;\nend /* INTERPRETABILITY OPTIMIZATION */ fit a CART model to {(zs, y\u0304s)}Ss=1 by optimizing Eq. 8\nWhen fitting a global interpretable model, instead of drawing samples from \u03c0x, we use training inputs {xn}Nn=1 with their corresponding output computed by the reference model {yrefn }Nn=1 as the target value.\nThe next subsection explains how to solve Eq. 8 for CART models."
    },
    {
      "heading": "3.2 Optimization Approach",
      "text": "We optimize Eq. 8 by using the backward fitting idea which involves first growing a large tree and then pruning it back to obtain a smaller tree with better generalization. For this goal, we use the formulation of maximum likelihood regression tree (MLRT) [40]."
    },
    {
      "heading": "3.2.1 Growing a large tree",
      "text": "Given the training data3, MLRT automatically decides on the splitting variable xj and split point (a.k.a. pivot) c using a greedy search algorithm that aims to maximize the log-likelihood of the tree by splitting the data in the current node into two parts: the left child node satisfying xj \u2264 c and the right child node satisfying xj > c. The procedure of growing the tree is as follows:\n1. For each node i, determine the maximum likelihood estimate of its mean parameter \u00b5i given observations associated with the node, and then compute the variance parameter of the tree given {\u00b5i}bi=1:\n\u00b5\u0302i = 1\nni ni\u2211 j=1 y\u0304ij\n\u03c3\u03022 =\n\u2211b i=1 \u2211ni j=1 [ \u03c32ij + (y\u0304ij \u2212 \u00b5\u0302i) 2 ]\nS .\nThe log-likelihood score of the node is then computed, up to a constant, by Li \u221d \u2212ni log(\u03c3\u03022). 3Here, for local interpretation, training data refers to the S samples (with their corresponding predictions\nmade by the reference model) taken from the neighborhood distribution to fit the explainable model.\n2. For each variable xj , determine the amount of increase in the log-likelihood of the node i caused by a split r as\n\u2206(r,xj ,i) = LiR + LiL \u2212 Li,\nwhere LiR and LiL are the log-likelihood scores of the right and left child nodes of the parent node i generated by the split r on the variable xj , respectively.\n3. For each variable xj , select the best split r \u2217 j with largest increase to the log-likelihood.\n4. Among the best splits, the one that causes the global maximum increase in the loglikelihood score will be selected as the global best split, r\u2217, for the current node, i.e. r\u2217 = maxr\u2217j , j=1,\u00b7\u00b7\u00b7 ,d \u2206(r\u2217j ,xj ,i) .\n5. Iterate steps 1 to 4 until reaching the stopping criteria.\nIn our implementation, we used the minimum size of a terminal node (the number of samples lie in the region generated by the terminal node) as the stopping condition."
    },
    {
      "heading": "3.2.2 Pruning",
      "text": "We adopt the cost-complexity pruning using the following cost function:\nC\u03b1 (T ) = log(\u03c3\u0302 2) + \u03b1b. (9)\nPruning is done iteratively; in each iteration i, the internal node h that minimizes \u03b1 = (C(h)\u2212C(Ti)) (|leaves(Th)|\u22121) is selected for pruning, where C(h) refers to the cost of the decision tree with h as terminal node, C(Ti) denote the cost of the full decision tree in iteration i, and Th denotes the subtree with h as its root. The output is a sequence of decision trees and a sequence of \u03b1 values. The best \u03b1 and its corresponding subtree are selected using 5-fold cross-validation."
    },
    {
      "heading": "3.3 Classification models",
      "text": "For classification problems, assuming the CART models as the interpretable model family, the form of the interpretability utility is the same as Equation 4 except that the likelihood of the interpretable model follows a multinomial distribution with the following log-likelihood:\nL = b\u2211 i=1 ni\u2211 j=1 K\u2211 k=1 I (yij \u2208 Ck) log pik s.t. pik \u2265 0, K\u2211 k=1 pik = 1 (10)\nwhere I(yjk \u2208 Ck) is the indicator function determining wheter or not the jth sample of the ith node belongs to the kth category assuming that there are in total K categories. The pik denote the probability of the occurrence of the kth category in the ith terminal node and the set of parameters are \u03c6 = {pi = (pi1, . . . , pik)}bi=1. Therefore, the final form of the interpretability utility for Bayesian classification models is\narg max \u03b7\n1\nS b\u2211 i=1 K\u2211 k=1 nk log pik + \u2126(T ) s.t. pik \u2265 0, K\u2211 k=1 pik = 1 (11)\nwhere \u03b7 = (T,\u03c6) and nk = \u2211ni j=1 I (yjk \u2208 Ck). The optimization approach is again similar to the process explained in Section 3.2 with the difference that the maximum likelihood estimate of\nthe parameters of each node i obtains as p\u0302ik = nk ni . Finally, the log-likelihood score of each node i is determined by Li = \u2211K k=1 nk log p\u0302ik."
    },
    {
      "heading": "3.4 Connection With Local Interpretable Model-agnostic Explanation (LIME)",
      "text": "LIME [39] is a prediction-level interpretation approach that fits a sparse linear model to the black-box model\u2019s prediction via drawing samples from the neighborhood of the data point to be explained. Our proposed approach extends LIME to KL divergence based interpretation of Bayesian predictive models (although it can also be used for non-Bayesian probabilistic models as well). This is achieved by combining the idea of LIME with the idea of projection predictive variable selection [36]. The approach is able to handle different types of predictions (continuous valued, class labels, counts, censored and truncated data, etc.) and interpretations (model-level or prediction-level) as long as we can compute KL divergence between the predictive distributions of the original model and the explanation model. For a more detailed explanation of the connection, check the preliminary work of [35]."
    },
    {
      "heading": "4 Experiments",
      "text": "We demonstrate the efficacy of the proposed approach through experiments on several real-world data sets. Subsection 4.1 discusses the experiments related to global intepretation. We first investigate the effect of reference models with different predictive powers on the performance of the final interpretable model. Secondly, we compare our approach with the interpretability prior alternative, of fitting directly an interpretable model to the data, in terms of their capability to trade off between accuracy and interpretability. We also compare the performance of our approach with non-Bayesian counterparts introduced in Section 1. Further, we investigate the stability of our approach and the interpretability prior approach. Section 4.2 examines local interpretation, where we compare our approach with LIME. Our codes and data are available online at github.com/homayunafra/Decision Theoretic Approach for Interpretability in Bayesian Framework."
    },
    {
      "heading": "4.1 Global Interpretation",
      "text": ""
    },
    {
      "heading": "4.1.1 Data",
      "text": "In our experiments, we use the following data sets: body fat [21], baseball players [20], auto risk [23], bike rental [13], auto mpg [38], red wine quality [7], and Boston housing [18]. Each data set is divided into training and test set containing 75% and 25% of samples, respectively."
    },
    {
      "heading": "4.1.2 Effect of Reference Model",
      "text": "The purpose of this test is to evaluate how the predictive power of the reference model affects the performance of the interpretable model when it is used to globally explain the reference model. Three data sets are adopted for this test: body fat, baseball players, and auto risk. Furthermore,\nthree reference models with different predictive powers are adopted: two BART models, and a Gaussian process (GP).\nFor the BART models, we used the BART package in R with two different values for the \u201cntree\u201d (number of trees) parameter. For one model, \u201cntree\u201d is set to the value that gives the highest predictive performance on the validation set (blue dotted line in Figure 3), while for another one, this parameter is set to 3, a low value, which gives poor predictive performance (red-dotted line in Figure 3). The rest of the parameters are set to their default values except \u201cnskip\u201d and \u201cndpost\u201d, which are set to 2000 and 4000, respectively. For the BART models, mean of the predictions of the posterior draws is used as their output. For the GP (green-dotted line in Figure 3), \u201cMatern52\u201d is used as the kernel with variance and length scales obtained by cross-validation over a small grid of values4. CART models are used as the interpretable model family. The size of the tree, i.e., the total number of leaf nodes, is used as the measure of interpretability [1, 17].\nFigure 3 demonstrates the results, which are averaged over 50 runs. The difference in the predictive performance of the interpretable models fitted to different reference models suggests that using more accurate reference models (BART in Baseball and Auto risk data sets, and GP in Body fat data set) can generate more accurate interpretable models as well. This is expected since by the performance of the interpretable model converges to the performance of the reference model; therefore the interpetable model will be more accurate when fitted to a more accurate reference model. The gap between the predictive performance of the interpretable models and their corresponding reference models is due to the limited predictive capability of the interpretable model. For some tasks, this gap can be made narrower by increasing the complexity of the interpretable model, while for others, a different family of interpretable models may be needed.\nFinally, in Figure 3.c, the performance of the interpretable model fitted to the GP reference model is better than the reference model itself, for some complexities. This may be because of different extrapolation behavior of CART and GP. In the high-dimensional space, the test data\n4This may not be the best setting for the GP. We did not attempt to optimize that since our objective is not to compare the performance of GP with BART, but instead to compare the performance of the interpretable models fitted to them.\nmay be outside the support of the training data; thus, extrapolation behavior matters. Simpler models can make more conservative extrapolations which may be helpful in this case."
    },
    {
      "heading": "4.1.3 Interpretability Prior vs Interpretability Utility",
      "text": "In this subsection, we compare our approach with the interpretability prior approach, in terms of the capability of the methods to trade off between accuracy and interpretabilityr. BART is used as the reference model, and CART is used as the interpretable model family. The interpretability prior approach fits a CART model directly to the training data where the prior assumption is that CART models are simple to interpret. On the other hand, our approach fits the CART model to the reference model, by optimization of the interpretability utility.\nFigure 4 demonstrates the results using all the data sets introduced in Subsection 4.1.1. The results are averaged over 50 runs. It can be seen that the most accurate models with any level of complexity (interpretability) are obtained with our proposed approach5.\nTo test the significance of the differences in the results, we performed the Bayes t-test [25]. The approach works by building up a complete distributional information for the mean and standard\n5The single exception happened in auto risk data set with tree size of 3.\ndeviation of each group6 and constructing a probability distribution over their differences using MCMC estimation. From this distribution, the mean credible value as the best guess of the actual difference and the 95% Highest Density Interval (HDI) as the range were the actual difference is with 95% credibility are shown in Figure 5. When the 95% HDI does not include zero, there is a credible difference between the two groups. As shown in the figure, for all data sets and for highly interpretable models (highly inaccurate), the difference between the two approaches is not significant (HDI contains zero). This is expected since by increasing the interpretability, the ability of the interpretable model to explain variability of the data or of the reference model decreases, and both approaches provide almost equally poor performance. However, by increasing the complexity (equivalently decreasing interpretability) to a reasonable level, we see that the differences of the two approaches become significant for all data sets.\nFinally, we further compared the performance of our proposed approach with two non-Bayesian counterparts, i.e., BATrees [3] and node harvest [34]. BATrees employs a single decision tree that best mimics the predictive behavior of a tree ensemble. Random forest is used as the reference model for BATrees. Node harvest simplifies a tree ensemble, i.e., random forest, by use of the shallow parts of the trees. We chose these approaches with random forest as their black-box model for the comparison for two reasons:\n6For each tree size, there are two groups of 50 RMSE values: one for the interpretability prior approach, and one interpretability utility approach.\nFor node harvest, we used the R implementation with default setting. For BATrees, the Python implementation in [17] is used with the depth of BATrees chosen from {3, 4, 5, 6} using 5-fold cross validation. The measure of complexity for node harvest is the total number of nodes with non-zero coefficients.\nTable 2 demonstrates the results. The results are averaged over 50 runs with the same seed value used for the experiments in Figure 4. The table shows that our proposed approach attained much better trade-off between accuracy and interpretability compared to BATrees and node harvest. For 4 data sets, our approach provides higher accuracies even with smaller sizes. For the rest, still our approach provides comparable predictive performance with a complexity of about half of the complexities of node harvest and BATrees. The differences between the bolded RMSE values with the rest of the RMSEs in Baseball, Auto risk and Wine quality data sets are significant using the Bayes t-test, while for other data sets the differences are not significant. According to the table, node harvest tends to generate more complex surrogate models. This is expected since in node harvest, the surrogate model is still an ensemble of shallow trees."
    },
    {
      "heading": "4.1.4 Stability Analysis",
      "text": "The goal of interpretable ML is to provide a comprehensive explanation of the predictive behavior of the black-box model to the decision maker. However, perturbation in the data or adding new samples may affect the learned interpretable model and lead to a very different explanation. This instability can cause problems for decision makers. Thereby, it is important to evaluate the stability of different interpretable ML approaches. For this objective, we propose the following procedure for stability analysis of interpretable ML approach.\nUsing a bootstrapping procedure with 10 iterations, we compute pairwise dissimilarities of the interpretable models obtained using each approach and report the mean and standard deviation of the dissimilarity values as their instability measure (smaller is better). We used the dissimilarity measure proposed in [4]. Assuming we are given two regression trees T1 and T2, for each internal node t, the similarity of the trees at node t is computed by\nSt(1,2) = I t k=k\u2032\n( 1\u2212 |\u03b4 t 1 \u2212 \u03b4t2 |\nrange(Xk)\n) (12)\nwhere Itk=k\u2032 is the indicator that determines whether the feature used to grow node t in T1 is identical to the one used in T2 (I t k=k\u2032 = 1) or not, \u03b4 t 1 and \u03b4 t 2 are pivots used to grow the node t in T1 and T2, respectively, and range(Xk) is the range of values of feature k. Finally, the dissimilarity of the two decision trees is computed as d (T1, T2) = 1\u2212 \u2211 t\u2208internal nodes q\ntSt(1,2) where qt are user specified weight value which we set to 1/b where b is the number of terminal nodes. The reported values are averaged over 45 values (10 bootstraping iterations result in (10\u00d7 9) /2 = 45 pairs of explainable models).\nTable 3 compares the two approaches over the data sets introduced in Section 4.1.1. The interpretability utility approach generated on average more stable models for most data sets; however, drawing a general conclusion is not possible because except body fat, for the rest of the data sets, the differences are not significant according to the Bayes t-test."
    },
    {
      "heading": "4.2 Local Interpretation",
      "text": "We next demonstrate the ability of the proposed approach in locally interpreting the predictions of a Bayesian predictive model. BART 7 is used as the black box model and CART is used as the interpretable model family. For the CART model, we set the maximum depth of the decision trees to 3 to obtain more interpretable local explanations. We compare with LIME8 which is a commonly used baseline for local interpretation approaches. Decision trees obtained by our approach to locally explain predictions of the BART model, used on average 2.03 and\n7In this experiment, we set the number of trees to 50 with nskip and ndpost set to 1000 and 2000 respectively, for faster run.\n8We use the \u2018lime\u2019 package in R (https://cran.r-project.org/web/packages/lime/lime.pdf) for the implementation.\n2.4 features for the Boston housing and the auto risk data sets, respectively. Therefore, to maximize comparability, we set the feature selection approach of LIME to ridge regression and select the 2 features with the highest absolute weights to be used in the explanation9. We use the standard quantitative metric for local fidelity: Ex [loss (interpx(x),pred(x))] where given a test data x, interpx(x) refers to the prediction of the local interpretable model (fitted locally to the neighborhood of x) for x, and pred(x) refers to the prediction of the black-box model for x. We used locally weighted square loss as the loss function with \u03c0x = N ( x, \u03c32I ) where \u03c3 = 1.\nEach data set is divided into 90%/10% training/test split. For each test data, we draw 200 samples from the neighborhood distribution. Table 4 shows that our approach produces more accurate local explanation for both data sets. Figure 6 shows, as an example, a decision tree constructed by our proposed approach to locally explain the prediction of the BART model for the particular test data shown in the figure from Boston housing data set. It can be seen that using only two features, our proposed approach obtains good local fidelity while maintaining interpretability with a decision tree with only 3 leaf nodes."
    },
    {
      "heading": "5 Conclusion",
      "text": "We presented a novel approach to construct interpretable explanations in the Bayesian framework by formulating the task as optimizing a utility function instead of changing the priors. This is obtained by first fitting a Bayesian predictive model which does not compromise accuracy, termed as a reference model, to the training data, and then project the information in the predictive distribution of the reference model to an interpretable model. The approach is model agnostic, implying that neither the reference model nor the interpretable model is restricted to a certain model. In the current implementation, the interpretable model, i.e., CART, is not a Bayesian predictive model; however, it is straightforward to extend the formulation to the case where a Bayesian predictive model, e.g., Bayesian CART [10], is used as the interpretable model. This remains for future. The approach also allows accounting for model uncertainty in the explanations. Through experiments, we demonstrated that the proposed approach outperforms the alternative approach of restricting the prior, in terms of accuracy, interpretability and stability. Furthermore, we showed that the proposed approach performs comparable to non-Bayesian counterparts such as BATrees and node harvest even when they have higher complexities (equivalently less interpretability)."
    },
    {
      "heading": "6 Acknowledgments",
      "text": "This work was supported by the Academy of Finland (Flagship programme: Finnish Center for Artificial Intelligence FCAI, grants 294238, 319264 and 313195), by the Vilho, Yrjo\u0308 and Kalle\n9MSEs of LIME with 3 features are, respectively, 2.48 and 0.006 for Boston housing and Auto risk data sets.\nVa\u0308isa\u0308la\u0308 Foundation of the Finnish Academy of Science and Letters, by the Foundation for Aalto University Science and Technology, and by the Finnish Foundation for Technology Promotion (Tekniikan Edista\u0308missa\u0308a\u0308tio\u0308). We acknowledge the computational resources provided by the Aalto Science-IT Project."
    }
  ],
  "title": "A Decision-Theoretic Approach for Model Interpretability in Bayesian Framework",
  "year": 2020
}
