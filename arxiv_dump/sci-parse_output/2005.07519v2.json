{"abstractText": "Machine learning (ML) techniques have been increasingly used in anomaly-based network intrusion detection systems (NIDS) to detect unknown attacks. However, ML has shown to be extremely vulnerable to adversarial attacks, aggravating the potential risk of evasion attacks against learning-based NIDSs. In this situation, prior studies on evading traditional anomaly-based or signature-based NIDSs are no longer valid. Existing attacks on learning-based NIDSs mostly focused on feature-space and/or white-box attacks, leaving the study on practical gray/black-box attacks largely unexplored. To bridge this gap, we conduct the first systematic study of the practical traffic-space evasion attack on learning-based NIDSs. We outperform the previous work in the following aspects: (i) practical\u2014instead of directly modifying features, we provide a novel framework to automatically mutate malicious traffic with extremely limited knowledge while preserving its functionality; (ii) generic\u2014the proposed attack is effective for any ML classifiers (i.e., model-agnostic) and most non-payload-based features; (iii) explainable\u2014we propose a feature-based interpretation method to measure the robustness of targeted systems against such attacks. We extensively evaluate our attack and defense scheme on Kitsune, a state-of-the-art learning-based NIDS, as well as measuring the robustness of various NIDSs using diverse features and ML classifiers. Experimental results show promising results and intriguing findings.", "authors": [{"affiliations": [], "name": "Dongqi Han"}, {"affiliations": [], "name": "Zhiliang Wang"}, {"affiliations": [], "name": "Ying Zhong"}, {"affiliations": [], "name": "Wenqi Chen"}, {"affiliations": [], "name": "Jiahai Yang"}, {"affiliations": [], "name": "Shuqiang Lu"}, {"affiliations": [], "name": "Xingang Shi"}, {"affiliations": [], "name": "Xia Yin"}], "id": "SP:d030524d7376bbecb3fead8c11e8989a3594dbb6", "references": [{"authors": ["R. Sommer", "V. Paxson"], "title": "Outside the closed world: On using machine learning for network intrusion detection", "venue": "IEEE Symposium on Security and Privacy (S&P), pp. 305\u2013316, IEEE, 2010.", "year": 2010}, {"authors": ["T.H. Ptacek", "T.N. Newsham"], "title": "Insertion, Evasion, and Denial of Service: Eluding Network Intrusion Detection", "venue": "tech. rep., SECURE NETWORKS INC CALGARY ALBERTA, Jan. 1998.", "year": 1998}, {"authors": ["I. Corona", "G. Giacinto", "F. Roli"], "title": "Adversarial attacks against intrusion detection systems: Taxonomy, solutions and open issues", "venue": "Information Sciences, vol. 239, pp. 201\u2013225, Aug. 2013.", "year": 2013}, {"authors": ["D.J. Chaboya", "R.A. Raines", "R.O. Baldwin", "B.E. Mullins"], "title": "Network intrusion detection: Automated and manual methods prone to attack and evasion", "venue": "IEEE Symposium on Security and Privacy (S&P), vol. 4, no. 6, pp. 36\u201343, 2006.", "year": 2006}, {"authors": ["M. Handley", "V. Paxson", "C. Kreibich"], "title": "Network Intrusion Detection: Evasion, Traffic Normalization, and End-to-End Protocol Semantics", "venue": "USENIX Security Symposium, 2001.", "year": 2001}, {"authors": ["T.-H. Cheng", "Y.-D. Lin", "Y.-C. Lai", "P.-C. Lin"], "title": "Evasion Techniques: Sneaking through Your Intrusion Detection/Prevention Systems", "venue": "IEEE Communications Surveys & Tutorials, vol. 14, pp. 1011\u20131020, 2012.", "year": 2012}, {"authors": ["G. Vigna", "W. Robertson", "D. Balzarotti"], "title": "Testing network-based intrusion detection signatures using mutant exploits", "venue": "ACM Conference on Computer and Communications Security (CCS), pp. 21\u201330, ACM, 2004. 13", "year": 2004}, {"authors": ["D. Mutz", "G. Vigna", "R. Kemmerer"], "title": "An experience developing an IDS stimulator for the black-box testing of network intrusion detection systems", "venue": "19th Annual Computer Security Applications Conference, 2003. Proceedings., pp. 374\u2013383, IEEE, 2003.", "year": 2003}, {"authors": ["P. Fogla", "M.I. Sharif", "R. Perdisci", "O.M. Kolesnikov", "W. Lee"], "title": "Polymorphic Blending Attacks", "venue": "USENIX Security Symposium, 2006.", "year": 2006}, {"authors": ["P. Fogla", "W. Lee"], "title": "Evading network anomaly detection systems: formal reasoning and practical techniques", "venue": "ACM Conference on Computer and Communications Security (CCS), 2006.", "year": 2006}, {"authors": ["H.G. Kayacik", "A.N. Zincir-Heywood", "M.I. Heywood", "S. Burschka"], "title": "Generating mimicry attacks using genetic programming: a benchmarking study", "venue": "2009 IEEE Symposium on Computational Intelligence in Cyber Security, pp. 136\u2013143, IEEE, 2009.", "year": 2009}, {"authors": ["K. Wang", "S.J. Stolfo"], "title": "Anomalous Payload-Based Network Intrusion Detection", "venue": "International Symposium on Recent Advances in Intrusion Detection (RAID), 2004.", "year": 2004}, {"authors": ["Y. Mirsky", "T. Doitshman", "Y. Elovici", "A. Shabtai"], "title": "Kitsune: an ensemble of autoencoders for online network intrusion detection", "venue": "Network and Distributed System Security Symposium (NDSS), 2018.", "year": 2018}, {"authors": ["M. Barreno", "B. Nelson", "A.D. Joseph", "J.D. Tygar"], "title": "The security of machine learning", "venue": "Machine Learning, vol. 81, pp. 121\u2013148, 2010.", "year": 2010}, {"authors": ["J.D. Tygar"], "title": "Adversarial Machine Learning", "venue": "IEEE Internet Computing, vol. 15, pp. 4\u20136, 2011.", "year": 2011}, {"authors": ["C. Szegedy", "W. Zaremba", "I. Sutskever", "J. Bruna", "D. Erhan", "I.J. Goodfellow", "R. Fergus"], "title": "Intriguing properties of neural networks", "venue": "International Conference on Learning Representations (ICLR), 2014.", "year": 2014}, {"authors": ["I.J. Goodfellow", "J. Shlens", "C. Szegedy"], "title": "Explaining and Harnessing Adversarial Examples", "venue": "International Conference on Learning Representations (ICLR), 2015.", "year": 2015}, {"authors": ["S.-M. Moosavi-Dezfooli", "A. Fawzi", "P. Frossard"], "title": "DeepFool: A Simple and Accurate Method to Fool Deep Neural Networks", "venue": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 2574\u20132582, 2016.", "year": 2016}, {"authors": ["N. Carlini", "D.A. Wagner"], "title": "Towards Evaluating the Robustness of Neural Networks", "venue": "IEEE Symposium on Security and Privacy (S&P), pp. 39\u201357, 2017.", "year": 2017}, {"authors": ["S. Li", "A. Neupane", "S. Paul", "C. Song", "S.V. Krishnamurthy", "A.K.R. Chowdhury", "A. Swami"], "title": "Stealthy Adversarial Perturbations Against Real-Time Video Classification Systems", "venue": "Network and Distributed System Security Symposium (NDSS), 2019.", "year": 2019}, {"authors": ["J. Li", "S. Ji", "T. Du", "B. Li", "T. Wang"], "title": "TextBugger: Generating Adversarial Text Against Real-world Applications", "venue": "Network and Distributed System Security Symposium (NDSS), 2019.", "year": 2019}, {"authors": ["P. Laskov", "others"], "title": "Practical evasion of a learning-based classifier: A case study", "venue": "IEEE Symposium on Security and Privacy (S&P), pp. 197\u2013211, IEEE, 2014.", "year": 2014}, {"authors": ["W. Xu", "Y. Qi", "D. Evans"], "title": "Automatically evading classifiers", "venue": "Network and Distributed System Security Symposium (NDSS), 2016.", "year": 2016}, {"authors": ["W. Hu", "Y. Tan"], "title": "Generating adversarial malware examples for blackbox attacks based on GAN", "venue": "arXiv preprint arXiv:1702.05983, 2017.", "year": 2017}, {"authors": ["Z. Lin", "Y. Shi", "Z. Xue"], "title": "Idsgan: Generative adversarial networks for attack generation against intrusion detection", "venue": "arXiv preprint arXiv:1809.02077, 2018.", "year": 1809}, {"authors": ["K. Yang", "J. Liu", "V.C. Zhang", "Y. Fang"], "title": "Adversarial Examples Against the Deep Learning Based Network Intrusion Detection Systems", "venue": "MILCOM 2018 - 2018 IEEE Military Communications Conference (MILCOM), pp. 559\u2013564, 2018.", "year": 2018}, {"authors": ["O. Ibitoye", "O. Shafiq", "A. Matrawy"], "title": "Analyzing adversarial attacks against deep learning for intrusion detection in iot networks", "venue": "arXiv preprint arXiv:1905.05137, 2019.", "year": 1905}, {"authors": ["Z. Wang"], "title": "Deep Learning-Based Intrusion Detection With Adversaries", "venue": "IEEE Access, vol. 6, pp. 38367\u201338384, 2018.", "year": 2018}, {"authors": ["D.L. Marino", "C.S. Wickramasinghe", "M. Manic"], "title": "An Adversarial Approach for Explainable AI in Intrusion Detection Systems", "venue": "IECON 2018 - 44th Annual Conference of the IEEE Industrial Electronics Society, pp. 3237\u20133243, 2018.", "year": 2018}, {"authors": ["J.H. Clements", "Y. Yang", "A. Sharma", "H. Hu", "Y. Lao"], "title": "Rallying Adversarial Techniques against Deep Learning for Network Security", "venue": "CoRR, vol. abs/1903.11688, 2019.", "year": 1903}, {"authors": ["A. Piplai", "S.S.L. Chukkapalli", "A. Joshi"], "title": "Nattack! adversarial attacks to bypass a gan based classifier trained to detect network intrusion", "venue": "arXiv preprint arXiv:2002.08527, 2020.", "year": 2002}, {"authors": ["E. Stinson", "J.C. Mitchell"], "title": "Towards Systematic Evaluation of the Evadability of Bot/Botnet Detection Methods", "venue": "WOOT, vol. 8, 2008.", "year": 2008}, {"authors": ["I. Homoliak", "M. Teknos", "M. Ochoa", "D. Breitenbacher", "S. Hosseini", "P. Hanacek"], "title": "Improving Network Intrusion Detection Classifiers by Non-payload-Based Exploit-Independent Obfuscations: An Adversarial Approach", "venue": "arXiv preprint arXiv:1805.02684, 2018.", "year": 1805}, {"authors": ["M.J. Hashemi", "G. Cusack", "E. Keller"], "title": "Towards evaluation of nidss in adversarial setting", "venue": "Proceedings of the 3rd ACM CoNEXT Workshop on Big DAta, Machine Learning and Artificial Intelligence for Data Communication Networks, pp. 14\u201321, 2019.", "year": 2019}, {"authors": ["J.J. Davis", "A.J. Clark"], "title": "Data preprocessing for anomaly based network intrusion detection: A review", "venue": "computers & security, vol. 30, no. 6-7, pp. 353\u2013375, 2011.", "year": 2011}, {"authors": ["A.H. Lashkari", "G. Draper-Gil", "M.S.I. Mamun", "A.A. Ghorbani"], "title": "Characterization of Tor Traffic using Time based Features", "venue": "Proceedings of the 2nd international conference on information systems security and privacy (ICISSP), pp. 253\u2013262, 2017.", "year": 2017}, {"authors": ["G. Draper-Gil", "A.H. Lashkari", "M.S.I. Mamun", "A.A. Ghorbani"], "title": "Characterization of encrypted and vpn traffic using time-related", "venue": "Proceedings of the 2nd international conference on information systems security and privacy (ICISSP), pp. 407\u2013414, 2016.", "year": 2016}, {"authors": ["B. Anderson", "D. McGrew"], "title": "Machine learning for encrypted malware traffic classification: accounting for noisy labels and non-stationarity", "venue": "Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 1723\u20131732, ACM, 2017.", "year": 2017}, {"authors": ["N. Rajasinghe", "J. Samarabandu", "X. Wang"], "title": "INSecS-DCS: a highly customizable network intrusion dataset creation framework", "venue": "2018 IEEE Canadian Conference on Electrical & Computer Engineering (CCECE), pp. 1\u20134, IEEE, 2018.", "year": 2018}, {"authors": ["M. Tavallaee", "E. Bagheri", "W. Lu", "A.A. Ghorbani"], "title": "A detailed analysis of the KDD CUP 99 data set", "venue": "2009 IEEE Symposium on Computational Intelligence for Security and Defense Applications, pp. 1\u2013 6, July 2009.", "year": 2009}, {"authors": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "title": "Generative adversarial nets", "venue": "Advances in Neural Information Processing Systems (NIPS), pp. 2672\u2013 2680, 2014.", "year": 2014}, {"authors": ["J. Kennedy"], "title": "Particle swarm optimization", "venue": "Encyclopedia of machine learning, pp. 760\u2013766, 2010.", "year": 2010}, {"authors": ["I. Guyon", "A. Elisseeff"], "title": "An introduction to variable and feature selection", "venue": "Journal of machine learning research, vol. 3, no. Mar, pp. 1157\u20131182, 2003.", "year": 2003}, {"authors": ["I. Sharafaldin", "A.H. Lashkari", "A.A. Ghorbani"], "title": "Toward Generating a New Intrusion Detection Dataset and Intrusion Traffic Characterization", "venue": "Proceedings of the 2nd international conference on information systems security and privacy (ICISSP), pp. 108\u2013116, 2018.", "year": 2018}, {"authors": ["A.L. Buczak", "E. Guven"], "title": "A survey of data mining and machine learning methods for cyber security intrusion detection", "venue": "IEEE Communications Surveys & Tutorials, vol. 18, no. 2, pp. 1153\u20131176, 2015.", "year": 2015}, {"authors": ["E. Hodo", "X. Bellekens", "A. Hamilton", "C. Tachtatzis", "R. Atkinson"], "title": "Shallow and deep networks intrusion detection system: A taxonomy and survey", "venue": "arXiv preprint arXiv:1701.02145, 2017.", "year": 2017}, {"authors": ["S. Gulghane", "V. Shingate", "S. Bondgulwar", "G. Awari", "P. Sagar"], "title": "A survey on intrusion detection system using machine learning algorithms", "venue": "International Conference on Innovative Data Communication Technologies and Application, pp. 670\u2013675, Springer, 2019.", "year": 2019}, {"authors": ["R.C. Eberhart", "Y. Shi"], "title": "Comparing inertia weights and constriction factors in particle swarm optimization", "venue": "Proceedings of the 2000 congress on evolutionary computation, vol. 1, pp. 84\u201388, IEEE, 2000.", "year": 2000}, {"authors": ["K.M. Tan", "K.S. Killourhy", "R.A. Maxion"], "title": "Undermining an anomalybased intrusion detection system using common exploits", "venue": "International Symposium on Recent Advances in Intrusion Detection (RAID), pp. 54\u201373, Springer, 2002.", "year": 2002}, {"authors": ["H.G. Kayacik", "A.N. Zincir-Heywood"], "title": "Mimicry attacks demystified: What can attackers do to evade detection", "venue": "2008 Sixth Annual Conference on Privacy, Security and Trust, pp. 213\u2013223, IEEE, 2008.", "year": 2008}, {"authors": ["M. Barreno", "B. Nelson", "R. Sears", "A.D. Joseph", "J.D. Tygar"], "title": "Can machine learning be secure", "venue": "ACM ASIA Conference on Computer and Communications Security (AsiaCCS), ACM, 2006.", "year": 2006}, {"authors": ["X. Yuan", "P. He", "Q. Zhu", "X. Li"], "title": "Adversarial examples: Attacks and defenses for deep learning", "venue": "IEEE transactions on neural networks and learning systems, 2019.", "year": 2019}, {"authors": ["X. Peng", "W. Huang", "Z. Shi"], "title": "Adversarial attack against dos intrusion detection: An improved boundary-based method", "venue": "2019 IEEE 31st International Conference on Tools with Artificial Intelligence (ICTAI), pp. 1288\u20131295, IEEE, 2019.", "year": 2019}, {"authors": ["G. Apruzzese", "M. Colajanni", "M. Marchetti"], "title": "Evaluating the effectiveness of adversarial attacks against botnet detectors", "venue": "2019 IEEE 18th International Symposium on Network Computing and Applications (NCA), pp. 1\u20138, IEEE, 2019.", "year": 2019}], "sections": [{"text": "To bridge this gap, we conduct the first systematic study of the practical traffic-space evasion attack on learning-based NIDSs. We outperform the previous work in the following aspects: (i) practical\u2014instead of directly modifying features, we provide a novel framework to automatically mutate malicious traffic with extremely limited knowledge while preserving its functionality; (ii) generic\u2014the proposed attack is effective for any ML classifiers (i.e., model-agnostic) and most non-payload-based features; (iii) explainable\u2014we propose a feature-based interpretation method to measure the robustness of targeted systems against such attacks. We extensively evaluate our attack and defense scheme on Kitsune, a state-of-the-art learning-based NIDS, as well as measuring the robustness of various NIDSs using diverse features and ML classifiers. Experimental results show promising results and intriguing findings.\nIndex Terms\u2014Network intrusion detection systems (NIDS), adversarial example, adversarial machine learning, evasion attack.\nI. INTRODUCTION\nNETWORK intrusion detection systems (NIDS) play acritical role on detecting malicious activities in networks. Based on the detection mechanism, NIDSs can be generally classified into two types: signature-based ones match abnormal patterns in the predefined signatures\u2019 database while anomalybased ones find deviations from normal profiles [1]. However, due to the intrinsic adversarial nature of NIDSs, skilled attackers continually strive to conduct evasion attacks to prevent their malicious activities from being detected [2]\u2013 [4]. Evasive techniques against signature-based NIDSs have been extensively studied in prior studies [2], [5]\u2013[8], of which the main principle is finding a stealthy way to conceal their abnormal signatures. As for anomaly-based NIDSs, some early work [9]\u2013[11] conducted mimic attack that encoded malicious traffic to mimic the normal profile, in order to evade systems using simple statistics such as byte frequency [12].\nThis work was supported in part by the National Key Research and Development Program of China under Grant 2018YFB1800205.\nZ. Wang (Corresponding author), D. Han, Y. Zhong, W. Chen, J. Yang, and X. Shi are with the Institute for Network Sciences and Cyberspace, Tsinghua University, Beijing 100084, China (e-mail: wzl@cernet.edu.cn; {handq19, zhongy18, chenwq19}@mails.tsinghua.edu.cn; {yang, shixg}@cernet.edu.cn;). S. Lu and X. Yin are with the Department of Computer Science, Tsinghua University, Beijing 100084, China (e-mail: lusq18@mails.tsinghua.edu.cn; yxia@csnet1.cs.tsinghua.edu.cn).\nPGA/PBA (Ours): Practical (traffic-space) Gray/Black box Attack \u2217Under this assumption, means the attacker has full knowledge about the ML model (including parameters, outputs, etc.), can only acquire the output probabilities, and neither has any knowledge nor can access to the ML model.\nOver the last decade, the paradigm on network attacks and intrusion detection has dramatically shifted. Anomaly-based NIDSs tend to play an incremental role to find unknown attacks (such as zero-day attack) that cannot be detected by signaturebased ones. Moreover, as traffic volume grows exponentially as well as more and more traffic is encrypted, payloadbased detection becomes difficult and inefficient. Recently, machine learning (ML) techniques are increasingly employed in anomaly-based NIDSs for non-payload-based detection. For example, Mirsky et al. presented Kitsune [13], an online NIDS using an ensemble of neural networks, achieving over 99% AUC (Area Under the Curve) in most cases.\nIn this context, existing evasion methods on signature-based [2], [5], or anomaly-based NIDSs using payload [9], [10] are no longer effective. However, more opportunities can be provided due to internal vulnerabilities of ML discovered in recent research [14]\u2013[16]. Although there have been many studies on evading other learning-based systems classifying images [17]\u2013[19], videos [20], texts [21], and malware [22]\u2013[24], it is still non-trivial for NIDSs since: 1) feature extraction/mapping on network traffic is much more sophisticated; 2) we need to ensure that there is no communication violation or compromise of maliciousness when modifying malicious traffic.\nUnfortunately, there are three defects in existing studies on evading learning-based NIDSs: 1) Firstly, most studies conduct the feature-space attacks\u2014by modifying the input of classifiers (i.e., features) rather than the input of systems (i.e., network traffic) [25]\u2013[31]. However, feature extraction/mapping in NIDSs is neither invertible nor differentiable. Thus, the assumption that evasive traffic can be inferred from the evasive features is unrealistic. Although there are a few studies conducting traffic-space attacks, they are just randomly mutating the traffic [32], [33], or based on a strong whitebox assumption of having full knowledge about the NIDS [34]. Table I divides existing attacks on learning-based NIDSs into four categories (i.e., FWA/FGA/FBA and TWA) and lists their impractical assumptions. 2) Secondly, most feature-space attacks [28]\u2013[31] employ gradient-based adversarial example attacks against image classification [17]\u2013[19]. However, such\nar X\niv :2\n00 5.\n07 51\n9v 2\n[ cs\n.C R\n] 2\n5 Ju\nn 20\n20\n2 attacks are only useful for Deep Neural Networks but cannot be applied to other ML models without obtainable gradients such as Decision Tree. 3) Thirdly, How to interpret and defend evasion attacks on learning-based NIDSs is still unexplored.\nIn summary, we face three challenges to develop a practical and generic study on evading learning-based NIDSs: \u2022 Practicability. How to perform a functionality-preserving\ntraffic-space evasion attack with extremely limited knowledge and affordable overhead? \u2022 Generality. How to propose a generic framework effective for NIDSs using various features and ML models? \u2022 Explainability. How to interpret the fragility and improve the robustness of learning-based NIDSs against such attacks? In light of the challenges, we formulate the evasion attack as a bi-level optimization problem and solve it by presenting a heuristic adversarial packet crafting framework, which can automatically mutate malicious traffic and select the best traffic mutant whose extracted features are most like benign. Our threat model provides a more practical assumption of the attacker\u2019s ability, that is, the attacker has very limited (graybox) or no knowledge (black-box) of the target NIDS. To solve the first and second challenges, we summarize commonly used features in most state-of-the-art learning-based NIDSs and then present several traffic mutation operators which can influence all summarized features without breaking malicious functionality. Attackers without detailed information about the feature extractor in the targeted system can also benefit from this summarized intelligence. Moreover, we extend the prior ideas of using Generative Adversarial Network (GAN) to treat the targeted ML classifiers as a black box, thus can evade any ML models. To solve the third challenge, we propose a feature-space interpretation method to effectively measure the robustness of NIDSs. By quantifying the extent to which each feature is manipulated by attackers, we propose a defense strategy by removing features with poor security scores. Contributions. Our major contributions involve presenting a novel evasion attack and defense on learning-based NIDSs in practical settings, followed by evaluation and interpretation. Specifically, they are elaborated as follows: \u2022 We present the first practical traffic-space evasion attack on\nlearning-based NIDSs under gray and black box assumptions. \u2022 We propose a feature interpretation method for evaluating\nthe adversarial robustness of targeted NIDSs. \u2022 We use six attack traffic sets to extensively evaluate our attack\nand defense method on the state-of-the-art NIDS Kitsune, as well as various learning-based NIDSs including six typical ML classifiers and two feature extractors (packet-based and flow-based). Significant insights behind the attack are also explored through in-depth analysis. The rest of the paper is organized as follows: We start by providing backgrounds in Section II. Section III introduces the threat model, as well as the formulation, motivation, and overview of our attack. Section IV and Section V elaborate two steps in our attack. The defense schemes are provided in Section VI. Experimental results and findings are shown in Section VII. We make discussions on limitations and improvements in Section VIII. Finally, we summarize the related work in Section IX and conclude in Section X.\nFig. 1: The general architecture of learning-based NIDSs."}, {"heading": "II. BACKGROUND", "text": "In this section, we introduce the general architecture of learning-based NIDSs and introduce non-payload-based detection. Then we formulate existing evasion attacks and summarize their unreasonable assumptions. Learning-based NIDS. In general, a learning-based NIDS consists of traffic capture, feature engineering and classification as shown in Fig. 1. First, network traffic is captured for generating traffic dataset. Then the set of features is extracted, selected, and eventually fed into the ML classifier for training or prediction. To give an illustration of learning-based NIDSs, we briefly introduce Kitsune [13] as a state-of-the-art case. Kitsune employs external libraries (e.g. tshark) to acquire raw packets. Then its feature extractor called AfterImage retrieves one feature vector from the meta information of each packet which contains over 100 statistics. Finally, features are mapped into groups and then fed into two-layer ensemble Autoencoders. Non-payload-based NIDS. In this study, we focus on evading learning-based NIDSs in which packets\u2019 payload is not inspected (called non-payload-based). We think this is reasonable due to two considerations: Firstly, we find that most learningbased NIDSs are likely to use non-payload-based features as inspecting payload is heavy and even impossible for encrypted traffic nowadays. Secondly, evading payload-based anomalybased NIDSs has been well studied [9], [10]. Note that nonpayload-based NIDSs prefer to detect attacks that rely on volume and/or iteration such as DoS/DDoS (Distributed Denial of Service), scanning, brute force, and Bot/Botnet. Other attacks related to specific content such as remote code execution and SQL injection are out of the scope of such NIDSs. Existing evasion attacks. In a nutshell, evasion attacks aim at finding variants with the same malicious behavior as original samples but can be misclassified as benign by the targeted system. For illustration purposes, we use the function E(\u00b7) to represent the extraction from a series of related traffic to feature vectors and C(\u00b7) to represent ML classifiers that take feature vectors as input and output the malicious probabilities. At any certain time, we denote by t and t\u0302 two series of related original and mutated malicious traffic used to extract two feature vectors f and f\u0302 , respectively (i.e., E(t) = f and E(t\u0302) = f\u0302 ). For examples, we formulate prior attacks under two impractical assumptions listed in Table I: 1) Directly modifying features\u2019 value (e.g., [25], [28], [30]).\nMany previous studies merely find evasive features as solving argminf\u0302 C(f\u0302 ), which directly modify features\u2019 value without considering how to mutate traffic. 2) Requiring targeted classifiers\u2019 output (e.g., [23], [25], [34]). Some work assumes that classifiers\u2019 specific output is attainable, then evasion attacks can be regarded as solving an optimization problem: argmint\u0302 C ( E(t\u0302) ) .\n3 (a) Intuitive attack framework in feature space\n(b) Motivation examples of adversarial features generation (c) The complete framework of our evasion attack.\nFig. 2: Attack methodology. In (a) and (b), each plot depicts a high-dimensional feature space, in which the distribution of benign features in the targeted classifier is enclosed by a solid line with green; benign, malicious, and adversarial features are represented by small solid circles, crosses, and triangles respectively. In (b), the limited ability/overhead of an attacker is represented by a red neighborhood.\nHowever, directly modifying features cannot be operated in practice as extraction from traffic to features is not invertible. Detailed settings or output probability of ML classifiers are often unavailable since they are just an intermediate step in real-world NIDSs. Since the above two assumptions are rather impractical in real-world settings, we relax them in our attack."}, {"heading": "III. ATTACK METHODOLOGY", "text": "In this section, we firstly define the threat model of two practical attacks, and then present our attack method by formulating it into an optimization problem. Finally, the motivation and overview of our solution are introduced."}, {"heading": "A. Threat Model", "text": "We consider an attacker starts with a series of traffic with malicious intent and wants to evade a learning-based NIDS using non-payload-based features. Unlike previous white/graybox attacks, the attacker neither requires any knowledge about the target classifier nor its output label or probability. Unlike previous feature-space attacks, the attacker can only mutate original traffic generated from the devices he/she controls (i.e. traffic-space attack) at an affordable overhead. Additionally, based on the different knowledge of features used in the targeted NIDS, the attacker can perform the following two attacks: \u2022 Practical Gray-box Attack (PGA). In this case, the full\nfeatures used in the targeted NIDS are known, which means that the attacker can build the same feature extractor as the NIDS and use it to extract features exactly. This may seem extreme, but the features are often published [13], [35]\u2013[40]. \u2022 Practical Black-box Attack (PBA). We assume a more practical case, in which the attacker has very limited knowledge about the features used in the targeted NIDS. In this case, the attacker can only use his/her knowledge about widely-used features to build a surrogate feature extractor."}, {"heading": "B. Practical Traffic-space Evasion Attack Problem", "text": "According to the threat model, firstly, we relax two assumptions in Section II by training a substitute classifier C\u2032(\u00b7) with probabilistic output to approximate C(\u00b7). This also solves the problem that some ML models without continuous output\nvalues (such as Isolation Forest) are difficult to optimize. Secondly, we build the surrogate feature extractor E \u2032(\u00b7). As for PGA, E \u2032(\u00b7) is exactly the same as E(\u00b7), while is simulated as for PBA. Additionally, we denote the mutation operation as M(\u00b7) which can transform original traffic t to a set consisting of all possible mutated traffic t\u0302. We say a mutation operation is safe (denoted by Ms(\u00b7)) if the mutation can preserve the malicious functionality of t. Therefore, the evasion attack can be formulated as solving:\nargmint\u0302 C\u2032 ( E \u2032(t\u0302) ) s.t. t\u0302 \u2208 Ms(t) (1)\nObviously, problem (1) is intuitive but hard to solve. We now transform it through the idea of mimicking normal features: Since the normal profile in a learning-based NIDS is constructed by the classifier fed with features, there are reasonable grounds to believe that features can implicitly give expression to the normal profile. Therefore, given a rational distance metric L(\u00b7, \u00b7) and a benign feature f?, the closer a feature vector f\u0302 is to f?, the more likely f\u0302 is classified as benign by the targeted classifier. Consequently, we transform problem (1) into the following bi-level optimization problem:\nargmint\u0302 L ( E \u2032 ( t\u0302 ) , f? ) (2)\ns.t. f? = argminf? L ( f?, E \u2032 ( t ) )\n(3) C\u2032 ( f? ) < h (4)\nt\u0302 \u2208 Ms(t) (5) where h is the anomaly threshold in the ML model. C\u2032 ( f? ) < h means that f? is classified as benign. Overall, we solve the above problem by separately solving the lower and upper-level objective function. In other words, we firstly solve f? in Eq. (3) under the constraint Eq. (4), and then use the solved f? to search t\u0302 in Eq. (2) under the constraint Eq. (5). To give intuition of this solving process, Fig. 2a depicts these two steps from the perspective of feature-space. The classifier in a targeted NIDS is trained beforehand to make a distinction between the distribution of benign and malicious features. Firstly, for each malicious feature, an f? is produced which can be not only classified as benign but also as close as possible to the malicious feature in terms of features\u2019 value;\n4 we refer to such feature as adversarial feature. In general, f? lies on the low-confidence region of the classifier. Generating adversarial features is what problem (3) aims to solve. Secondly, original malicious traffic is mutated to transfer its features to the closest adversarial/benign ones, which is what the whole problem (2) aims to solve."}, {"heading": "C. Motivation and Overview of Solution", "text": "We now introduce specific methods employed in the aforementioned two steps to solve the bi-level optimization. We use an enhanced generative adversarial network (GAN) model to generate the adversarial features (Eq. (3)), and particle swarm optimization (PSO) to search evasive traffic mutants (Eq. (2)). We now introduce the motivation of adopting GAN and PSO, and how to combine them to complete the evasion attack. Why generating f?? In a nutshell, adversarial features f? can save the overhead of modifying traffic/features when the attacker\u2019s overhead budget or ability is limited. For one thing, an attacker is likely to have a budget of overhead (such as the extra time and crafted traffic volume to evade detection). For another, we note that the attacker\u2019s ability to modify traffic/features is limited in practice. For examples, excessively increasing the interval time will cause the connection timeout, and injecting excessive traffic may be perceived by the victim. Fig. 2b provides examples to demonstrate the necessity of f?. Without guidance of adversarial features, a malicious feature may eventually fail to reach the nearest benign space (Scenario 1) or miss the transient benign space (Scenario 2). Why GAN? Adversarial features generation needs to be: 1) model-agnostic\u2014we assume the attacker has no knowledge about the ML classifier; and 2) efficient\u2014there may be tons of malicious/traffic in practice. GAN [41] consists of two neural networks, generator and discriminator, contesting with each other to complete a min-max game. Inspired by previous ideas [24], [25], GAN is highly competent to generate adversarial features since 1) the discriminator can be trained as a substitute for the targeted classifier, thus can conduct model-agnostic attack; 2) once the generator is trained, it can generate f? efficiently for any malicious feature. Why PSO? Network traffic is difficult to directly participate in the numerical calculation, so we vectorize traffic as high dimensional vectors involving meta information of packets\u2019 header. Note that, the vectorization from traffic to vectors is invertible. Therefore, problem (2) turns into finding best metainfo vectors. However, unlike continuous feature space, each dimension of the meta-info vectors has various discrete values, thus problem (2) is indeed a hard combinatorial optimization task (NP-complete). Hence, we resort to swarm intelligence algorithms to find approximate solutions. We employ PSO [42] since it is a simple but powerful method with great adaptability on dealing with high dimensional tasks. How they work together? In a nutshell, GAN generates the optimization objective of PSO. In other words, when mutating malicious traffic based on PSO, we find traffic mutants with the best evasive effectiveness by measuring their similarity to the adversarial features generated by GAN. Specifically, the proposed evasion attack method is illustrated in Fig. 2c, including the following two steps:\n\u2022 Adversarial Features Generation: We assume an attacker wants to launch some activities, which will induce a series of malicious traffic. First, the attacker needs to collect some benign traffic in the network he/she controls. Then, two kinds of traffic are extracted into features by the surrogate extractor, and fed into our GAN model. After the training phase, the generator is capable to generate adversarial features.\n\u2022 Malicious Traffic Mutation: After generating adversarial features, we employ PSO with predefined operators to mutate malicious traffic automatically. Each particle in the swarm represents a vector consists of meta-info of mutated malicious traffic. The swarm is iteratively searching the traffic-space under the guidance of the temporary best particle whose features are most similar to the adversarial feature. Finally, the best particle is selected after several iterations. The details of the above two steps are elaborated in next two sections IV and V, respectively."}, {"heading": "IV. GENERATING ADVERSARIAL FEATURES", "text": "We now introduce the procedure of generating adversarial features. Our enhanced GAN model is shown in Fig. 2c on the top, which consists of a generator and a discriminator. Generator. The generator is a feed-forward neural network whose aim is to transform a malicious feature into its adversarial version. It takes the concatenation of a malicious feature vector f and a noise vector z from a distribution pz(z) as input and outputs a generated feature vector represented by G(f, z). To train the generator, its loss function is defined in (6) as:\nlG = Ef \u2208Fmal,z\u223cpz(z)[log D(G(f, z)) + L(f,G(f, z))] (6)\nwhere Fmal is the set of original malicious features. lG should be minimized with respect to the weights in the generator\u2019s network. In this study, we extend prior GANs by additionally computing a construct error L(\u00b7, \u00b7) between the input and output. In this study, we use the root mean\nsquare error RMSE (x,x\u2032) = \u221a\u2211nd i=1 ( xi \u2212 x\u2032i )2 /n where nd is the dimensionality of the input and output features. Thus, the generated features can mimic the distribution of benign features while approaching malicious ones. Discriminator. The discriminator is used to distinguish generated features from benign ones. It is also a feed-forward neural network whose input consists of the above two classes of feature vectors, and its output is a probability of determining an input vector is generated. The discriminator is trained to maximize the output of generated input vector while minimize the output of benign input vectors. Thus, its loss function is:\nlD = \u2212Ef \u2208Fben log(1 \u2212 D(f )) \u2212 Ef \u2208Fgen log D(f ) (7)\nwhere Fben is the set of features extracted from benign traffic collected in the network the attacker controls beforehand. Fgen is the set of features generated by the generator.\nThe training process is an iterative and mutual optimization between the generator and discriminator until a convergence. Then, the generated features from the generator can work as adversarial features Fadver .\n5"}, {"heading": "V. MUTATING MALICIOUS TRAFFIC", "text": "In this section, we introduce how to automatically mutate traffic through our heuristic method. First, we design the mutation operators on malicious traffic. Then, we introduce the vectorization from traffic to meta-info vectors. Finally, we propose the PSO-based traffic mutation algorithm."}, {"heading": "A. Basic Traffic Mutation Operators", "text": "We now introduce how to design basic mutation operators on malicious traffic (i.e. Ms(\u00b7) in Eq. (5)). Generally, the mutation operators should be able to affect as many types of features as possible (in order to be generic), even when there is only limited knowledge of the features used in the targeted system (for PBA). Besides, the mutation operators should be functionality-preserving and stealthy to prevent being perceived by victims. However, the feature extraction methods of learningbased NIDSs in recent studies seem to be different, which raises the difficulty to conduct a generic attack. Nonetheless, we find a consensus on extraction methods among studies on nonpayload-based NIDSs and other network anomaly detection systems [13], [35]\u2013[40]. Based on this, we summarize the feature types widely used in these works from a high-level (Appendix A). This summarized intelligence basically covers the features used in related works, which also helps attackers without detailed knowledge of features used in the targeted system (i.e., PBA) to build the surrogate feature extractor.\nThen, we design mutation operators which can affect all summarized features (Fig. 7 in Appendix A provides an intuitive illustration) while preserving the functionality. Specifically, they consist of modifying original malicious traffic and injecting/adjusting crafted stealthy traffic: Original malicious traffic modification. We ensure that the original traffic will not be deleted and the order of packets will not be changed. Hence the only mutation operator is: (a) Altering the interarrival time of packets in original traffic\nCrafted stealthy traffic injection. It is non-trivial to determine packer headers\u2019 content of crafted traffic. Firstly, we can only craft traffic sent from the attacker, and some fields (MAC/IP/port) in crafted packets need to be consistent with that of original packets nearby; otherwise the crafted packets cannot affect features extracted from original packets. Secondly, the assignment of other fields in header must meet the following requirements: 1) it will not compromise the maliciousness of the original traffic; 2) it will not cause the protocol semantics or communication violation (such as connection breakdown of TCP traffic); 3) it will not induce responses by the victim (for stealth and consistency of replay). In light of these requirements, we list optional methods for generating crafted traffic in Table II. We note that a previous method used in [34] by modifying TTL requires the knowledge of the victim\u2019s network topology, which is extremely strict. Hence, we propose other methods for different types of traffic without additional knowledge. Crafted traffic adjustment. There are several adjustments for crafted packets after being injected: (b1) Altering the interarrival time of packets in crafted traffic (b2) Altering the # protocol layer of packets in crafted traffic (b3) Altering the payload size of packets in crafted traffic"}, {"heading": "B. Meta-information Vectorization", "text": "To facilitate numerical operations on structured traffic data, we vectorize traffic into meta-info vectors containing meta-information of original traffic. Note that unlike feature extraction, this vectorization is invertible, which means that it is effortless to rebuild traffic from meta-info vectors. Meanwhile, the aforementioned mutation operators on traffic need to be reflected in the vectors. Details of the meta-info vectors and an illustrative example about vectorization and rebuilding between vectors and traffic are shown in Fig. 3, where x denotes the meta-info vectors.\nTo illustrate the meaning of each dimension in x and how they reflect the mutation operators, an x[i] is further divided into xmal and xcra f t [] used to represent a packet in original malicious traffic and several crafted packets right in front of it in time, respectively. The xmal contains two parts: Timestamp corresponds to Mutation (a), and Number of crafted packets determines the size of the list xcra f t . Each craft packet denoted with xcra f t [i] contains three parts: Interarrival time related to Mutation (b1) is the time interval from the previous packet; # protocol layers corresponding to Mutation (b2) refers to its number of layers in the TCP/IP protocol; Payload size directly reflects Mutation (b3)."}, {"heading": "C. PSO-based Automatic Traffic Mutation", "text": "We now present our algorithm for automatically searching the best traffic mutants based on PSO. The general framework of PSO is shown in Fig. 2c. Each particle represents a mutant in traffic-space. PSO optimizes a problem by iteratively moving\n6 Algorithm 1: PSO-based Traffic Mutation Algorithm Input: Hyperparameters \u03c9, c1, c2, Niter, Nswarm in PSO;\nattacker\u2019s overhead budget lc, lt ; the original malicious traffic T;\nOutput: The mutated (/evasive) malicious traffic T\u0302. 1 for each t in T do 2 tv \u2190Vectorize(t). . meta-info vectorization 3 S\u2190Initialize(tv, Nswarm, lc, lt). . initialize population 4 for i = 1 to Niter do 5 for j = 1 to Nswarm do O distance evaluation 6 S.dj \u2190 L ( E \u2032(Rebuild(S.xj)), {Fadver,Fben} ) ; 7 Update individual best S.yj referring to S.dj . 8 Update global best S.y\u0302 referring to S.yj . 9 end 10 for j = 1 to Nswarm do . update each v and x 11 vcog \u2190 S.yj \u2212 S.xj ; . cognitive force 12 vsoc \u2190 S.y\u0302 \u2212 S.xj ; . social force 13 Randomly sample r1, r2 \u223c puniform(0,1). 14 S.vj \u2190 \u03c9S.vj + r1c1vcog + r2c2vsoc ; 15 S.xj \u2190 UpdateX(S.xj,S.vj, lc, lt). 16 end 17 end 18 Append t\u0302\u2190 Rebuild(S.y\u0302) into T\u0302. 19 end 20 return T\u0302\neach particle according to its position and velocity vector. Each particle\u2019s movement in an iteration is computed by its velocity, and the velocity is decided by three items: the individual position in the last iteration (also called inertia), its individual best known position in all previous iterations (also called cognitive force) denoted with y, and best known position in the current iteration of other particles in this swarm (also called social force) denoted with y\u0302. In this study, particles\u2019 position vectors denoted with x are exactly meta-info vectors, and velocity vectors denoted with v share the same structure with x. Each dimension of v represents the difference between the corresponding position vectors.\nThe proposed algorithm is shown in Algorithm 1, where Niter denotes the number of iterations and Nswarm number of particles in the swarm. In each iteration, we firstly evaluate each particle\u2019s evasive effectiveness (on line 6) and update individual best and global best positions (on lines 7-8), which are respectively used to compute cognitive force (on line 11) and social force (on line 12). Then, each particle\u2019s v is updated by multiplying constant weights \u03c9, c1, and c2 with inertia, cognitive, and social items, respectively (on line 14). Each particle\u2019s x is then updated according to v (on line 15). Some highlights in the algorithm are elaborated as follows: Overhead budget. In this study, we limit the attacker\u2019s overhead budget from two aspects. The first overhead denoted with lc is the rate of the number of crafted packets to that of original packets. The second overhead denoted with lt is the rate of time elapsed of mutated traffic to that of original traffic. In other words, the crafted packets number and time elapsed of mutated traffic must no more than lc and lt , respectively. Population initialization (on line 3). To sufficiently disperse initial particles in the search-space, fields in xcra f t [i] and # crafted pkts in xmal are randomly initialized within the valid range. As for Timestamp of xmal , we divide the maximum\ninterarrival time (related to lt ) between every two original packets into n equal parts, and Timestamp is randomly selected from these n-section points. And v is filled with 0 initially. Effectiveness evaluation and traffic rebuilding (on line 6). Evasive effectiveness is evaluated in three steps: First, mutated traffic is rebuilt from the position (i.e., meta-info) vector. Original traffic is directly retrieved after a replacement with Timestamp in xmal . As for crafted packets, after randomly determining their protocol type with reference to # protocol layers in xcra f t [i], they can be rebuilt through the methods listed in V-A. Fig. 3 also shows an example of traffic rebuilding. Second, mutated traffic is extracted into features through the surrogate extractor. Third, the distance between extracted and adversarial features is computed as the effectiveness. Position update (on line 15). The x is simply updated by adding with v computed on line 14. However, some dimensions of x are discrete (e.g., # crafted pkts). We discretize them by approximating them to the nearest discrete values."}, {"heading": "VI. DEFENSE SCHEME", "text": "We introduce three probable mitigations against the proposed attack, including two prior works and our novel scheme: Adversarial training [17]. This is a promising method widely used to defend against adversarial examples in the image domain by retraining the classifiers with correctly-labeled adversarial examples. However in our traffic-space attack, it can only reduce the attack effectiveness by limiting the generation of adversarial features. Feature selection [43]. This is an important step in feature engineering to remove redundant/irrelevant dimensions of features used in ML models, which can effectively improve detection performance and robustness. Adversarial feature reduction. We propose a novel scheme dedicated to evaluate the robustness and defend against such attacks. In a nutshell, we proactively simulate the proposed attack and then calculate the degree to which the value of each feature dimension in the mutated traffic is close to the adversarial features compared to original value (see Appendix B for details). The proximity rates of each feature dimension can be viewed as the adversarial robustness scores. Our main claim is the high dimensionality of features gives attackers an opportunity to exploit some vulnerable dimensions to evade detection. Hence, we propose a defense scheme by deleting a fraction of feature dimensions with low robustness score."}, {"heading": "VII. EXPERIMENTAL EVALUATION", "text": "In this section, we extensively evaluate the performance of attacks and defenses. We introduce the experimental settings in VII-A. In VII-B and VII-C, several attacks are evaluated using different kinds of malicious traffic and different NIDSs. We extend our attack to PBA and evaluate PBA attacks in VII-D. Execution cost and impact of parameters are measured in VII-E. We verify our attack is functionality-preserve in VII-F. Finally, defense methods are evaluated in VII-G (For reasons of space, details settings of ML/DL models and experiments, as well as implementation, optimization and availability are in the supplemental material).\n7"}, {"heading": "A. Experimental Settings", "text": "Datasets. Table III summarizes the information of traffic sets used in this study, including six well-known attacks from two up-to-date traffic datasets. Kitsune Dataset [13] was used to evaluate Kitsune by proactively performing a number of attacks in their video surveillance network. CIC-IDS2017 [44] collected traffic for common attacks in a large-scale testbed, which covers all common devices and middleboxes. Targeted NIDSs. Firstly, the entire Kitsune [13] is evaluated as the state-of-the-art off-the-shelf NIDS. Secondly, NIDSs using different ML classifiers and features are also evaluated: \u2022 Feature extractors: We evaluate two representative feature\nextractors: AfterImage [13] is a packet-based extractor in Kitsune. It computes incremental statistics of packet\u2019s size, count and jitter in various damped time windows. CICFlowMeter [37] is a flow-based extractor. It extracts several statistics (e.g., size, count, and duration) of connections. \u2022 ML classifiers: We apply six classifiers that are widely used in related work to comprehensively cover ML models [45]\u2013 [47]. KitNET is a deep, unsupervised, and ensemble learning classifier used in Kitsune. We also use classical supervised ML models including Logistics Regression (LR), Decision Tree (DT) and Support Vector Machine (SVM), as well as Multi-Layer Perceptron (MLP) representing deep learning models. An anomaly detection model Isolation Forest (IF) is also used. Baseline attacks. For one thing, we find that previous attacks against signature-based NIDSs (like [2], [5], [6]) and traditional anomaly-based NIDSs (like [9]) perform nearly no evasive effect. This is because these methods focused more on manipulating the payload. For another, feature-space attacks (e.g., [28], [30]) including FWA/FGA/FBA cannot participate either since they did not mutate the traffic. Hence, we employ traffic-space attacks in very few related studies as baselines: \u2022 Random Mutation. Note that randomly mutating traffic is\nnot a weak attack we imagined, but appears in published works [32], [33]. We use two random mutation methods: Random-ST is randomly spreading interval-time between packets; Random-Dup is randomly duplicating partial original traffic. As for other methods, packet injection is not used since we find it has no effect on all traffic sets; deleting/reordering packets compromises functionality of the original traffic. \u2022 Traffic-space White-box Attack (TWA). The only work of TWA we find is [34], which uses similar mutation operators as ours. Since attackers have full knowledge of the targeted NIDSs in their assumption, the output probability of the classifier can be directly used as the optimization objective. Metrics. We firstly present four new metrics with their formulations and intuitive descriptions listed in Table V. Notations used in the metrics are listed in Table IV (Higher is better for all metrics). According to usage, evaluation metrics used in this work can be divided into three categories: \u2022 Evasive effectiveness (MER, DER, and PDR). Roughly\nspeaking, original Malicious traffic Evasion Rate (MER) and Detection Evasion Rate (DER) respectively reflect how much original malicious traffic and all mutated traffic (including crafted traffic) become evasive after attack. That is to say,\nDER additionally considers whether the crafted traffic is classified as malicious, which can reflect whether our attack is stealthy. Since MER and DER are highly dependent on the selection of the anomaly threshold, we propose a more accurate metric by measuring the decline rate of the malicious probabilities outputted by the targeted classifier, namely malicious Probability Decline Rate (PDR). \u2022 Interpretable indicator (MMR). In order to explain and understand the reason and principle of evasion attacks on learning-based NIDSs, we propose an interpretable indicator Malicious features Mimicry Rate (MMR) which can explicitly show the change of features in the latent space during attacks. Specifically, MMR reflects the degree to which malicious features are close to adversarial features during the mutation. \u2022 Detection performance. We additionally use three typical metrics\u2014Precision, Recall, and F1-score \u2014to measure the detection performance of NIDSs."}, {"heading": "B. Evasive Effectiveness of Different attacks", "text": "In this section, we compare the evasive effectiveness of our PGA attacks with three baselines by evading Kitsune under different traffic sets. We also evaluate the effectiveness of adversarial features in our attacks by comparing our PSO-based algorithm with (GAN+PSO) and without (PSO) adversarial features. We also compare the impact of overhead budget in our attacks using a lower (lc = 0.2, lt = 2) and a higher budget (lc = 0.5, lt = 5). Note that, baseline attacks are all with the higher overhead budget. The results are shown in Fig. 4. Evasive effectiveness comparison. As evident in the results of MER/PDR, our attack GAN+PSO performs very well relative to random mutations at the same budget (lc = 0.5, lt = 5). The effectiveness of random mutations is extremely unstable; each mutation only works under specific traffic sets. Moreover, thanks to our two-step attack framework, our gray-box attack surprisingly outperforms the state-of-the-art white-box attack (TWA) in all traffic sets. As for DER, results show that the drop from MER to DER is <3% in most cases, which shows that the crafted traffic in our evasion attack is stealthy and unobservable even with lc = 0.5 (more crafted packets).\n8\nRandom-Dup(lc = 0.5) Random-ST(lt = 5) TWA(lc = 0.5, lt = 5) PSO(lc = 0.2, lt = 2) GAN+PSO(lc = 0.2, lt = 2) GAN+PSO(lc = 0.5, lt = 5)\n0.00 0.25 0.50 0.75 1.00\n0. 75 04 0. 99 42\nBotnet\n0. 98 14 0. 98 69\nFuzzing\n0.00 0.25 0.50 0.75 1.00\n0. 54 97 0. 78 53\nSSDP DoS\n0. 70 21 0. 97 66\nPort Scan\n0.00\n0.25\n0.50\n0.75\n0. 50 66 0. 71 82\nBrute Force\n0. 40 26 0. 55 94\nDDoS\n0.00 0.25 0.50 0.75 1.00\n0. 72 18 0. 96 86\nBotnet\n0. 95 46 0. 95 65\nFuzzing\n0.00 0.25 0.50 0.75 1.00\n0. 53 23 0. 77 28\nSSDP DoS\n0. 70 06 0. 96 61\nPort Scan\n0.00\n0.25\n0.50\n0.75\n0. 50 15 0. 70 15 Brute Force 0. 39 87 0. 55 49 DDoS\n0.00 0.25 0.50 0.75 1.00\n0. 59 79 0. 80 84\nBotnet\n0. 87 12 0. 88 67\nFuzzing\n0.00 0.25 0.50 0.75 1.00\n0. 56 95 0. 65 42\nSSDP DoS\n0. 29 83 0. 54 57\nPort Scan\n0.00 0.25 0.50 0.75 1.00\n0. 29 73 0. 39 86\nBrute Force\n0. 99 99 0. 99 99\nDDoS\n0.00 0.25 0.50 0.75 1.00\n0. 41 62 0. 64 16\nBotnet\n0. 82 90 0. 83 66\nFuzzing\n0.00 0.25 0.50 0.75 1.00\n0. 54 37 0. 71 33\nSSDP DoS\n0. 73 47 0. 85 23\nPort Scan\n0.00 0.25 0.50 0.75 1.00\n0. 74 40 0. 84 33\nBrute Force\n0. 29 26 0. 68 03\nDDoS\nFig. 4: The evasive effectiveness of our attacks compared with baselines (higher is better).\nImpact of adversarial features. We observe that using adversarial features indeed increases the evasive effectiveness (by 10-20% usually). Especially in Fuzzing, GAN+PSO increases MER/DER by more than 90% compared with PSO. Impact of overhead budget. It is easy to understand that a higher overhead budget (namely, looser limitation) performs better results. Specifically, GAN+PSO with larger lc and lt have 20-30% higher MER/DER in most cases. Performance of different traffic sets. As shown in the results, our attack achieves >97% MER/DER on half of the traffic sets, as well as >70% MER/DER on five of six traffic sets. As for reasons of the relatively poor MER/DER in DDoS, we believe this is because malicious features are originally farther from the benign space and beyond the attacker\u2019s ability/budget (See Scenario 3 in Figure 2b, if a malicious feature is beyond the attacker\u2019s overhead budget/ability, it cannot be transformed into a benign one by any means). In fact, we find the anomaly score (i.e, RMSE in Kitsune) of original features in DDoS is many orders of magnitude larger than other scenarios. This is exactly why its PDR is higher than others (over 99.99%) but MER/DER is lower. This finding also shows that it is necessary to consider attacker\u2019s ability/budget on mutating traffic as well as the original intensity of anomaly instead of purely comparing the evasion rate."}, {"heading": "C. Robustness of other Classifiers and Features", "text": "We conduct evasion attacks (our PGA and baselines with the higher budget) on different NIDSs described in Section VII-A under Botnet and DDoS traffic (Other four traffic sets are not used since the main purpose of this part is to prove the versatility of our method, i.e., the robustness of different ML models can be evaluated). Since DER has been found to be very similar to MER, we use MER to measure the evasive performance, which is also the most concerning indicator for attackers. PDR is not used since it is measured differently among ML models. Table VI lists the results. Evasive effectiveness comparison. Compared with baseline attacks, our attack has broader generality for evading various ML classifiers using different kinds of features. Specifically, Random-RT always performs very poorly while Random-Dup only has evasive effectiveness for a few cases. Once again, our attack outperforms TWA in all cases, especially for the Isolation Forest model. We attribute the generality to the feature-level mimicking in our model-agnostic attack. Robustness of different feature sets. NIDSs with flow-based features are slightly more robust against our attack as well as other attacks than packet-based ones. Note that, our mutation operators are packet-based (but not flow-based) due to the generic black/gray-box assumption\u2014per-packet mutation can attack flow-based NIDSs (since flows consist of packets), but not vice versa. Robustness of different classifiers. Based on the results in Botnet , we find that traditional ML methods are more robust than deep neural networks. Specifically, KitNET (in Kitsune) has the (almost) best detection performance but also suffers the highest evasion rate. The probable reason is that KitNET clusters the features into groups, which gives attackers a better chance to influence more feature groups. Through experiments, we find the top 10% dimensions in original features exploited by our attack can eventually cover more than 50% of the features groups. The robustness of the different methods in DDoS is poor, while only IF still maintains good robustness.\n9\nMeanwhile, it can be observed that the evasive performance of different classifiers is diverse significantly. We think this is a normal phenomenon, which is the same as different models with different detection performance. In order to achieve modelagnostic, we use Euclidean distance to measure the similarity of features for any models. However, this may not be accurate methods for some ML models to understand and measure features. Our goal in this work is to present a generic method to measure the robustness of different ML models to provide NIDS designers with some important insights."}, {"heading": "D. Attacks with Limited Knowledge of Features", "text": "So far, we have evaluated the effectiveness of our attack under the PGA assumption. We now extend our attack with limited knowledge of features used in targeted NIDSs (i.e., PBA). Specifically, we evaluate three types of attackers, who know the 75%, 50%, and 0% features that are accurately used by NIDSs, respectively. Recall Section III-A, the only difference between PBA and PGA is the surrogate feature extractor used by the attacker. For PBA, besides limited known features, the attacker also extracts all commonly used features in Appendix A. Especially for PBA(0%), the attacker without any knowledge about the target system can only simulate the extractor by using other features. We use Kitsune as the targeted NIDS and evaluate the MER and PDR of three PBAs and the PGA in all traffic sets. The results are in Table VII.\nAs before, we think that PDR can better reflect the evasive effectiveness compared with MER. As shown in the results, PBA(50%) and PBA(75%) perform high PDR that is similar to PGA. Even for attackers without any knowledge, PBA(0%) still has a strong evasive ability (Compared with PGA, the drop of PDR is within 20%). The key insight is that even if we cannot accurately know the features used by NIDSs, the mutation method computed by our attack through simulated\nfeatures is also effective on real features (that is, it can also effectively transform real malicious features into benign). This finding seems to be very frustrating and frightening for NIDSs like Kitsune, meaning that a weak attacker can easily make a considerable portion of malicious traffic become evasive."}, {"heading": "E. Execution Cost and Impact of Parameters", "text": "It is necessary to measure the execution cost for attacks, especially for attackers with limited computing resources. Here, we use TWA as a comparison algorithm to represent the lower bound of execution time. This is because TWA requires the output value of the classifier, so the quality of candidate solutions (i.e., traffic mutants) can be quickly measured, but we do not have this knowledge in our attack. We also compare the impact of three key parameters on the execution time and evasive performance. We denote our attacks using different parameters with (Niter ,Nswarm,Nadver ): Niter and Nswarm denotes the number of iterations and particles in PSO, and Nadver denotes the number of adversarial features. The results are shown in Fig. 5, where we use Kitsune under two traffic sets since other sets have similar results.\nIt is shown that our attack with (3,6,100) can approximate the execution time of TWA while performing better evasive\n10\nperformance than TWA. For other parameters, our attacks are acceptable in execution time. Larger parameters have better evasive performance but will consume more time. To balance this trade-off, we think (5,10,1000) is the best combination of parameters, which is also chosen for other parts of experiments.\nF. Verification of Malicious Functionality\nTo be rigorous, although we guarantee the mutation operators in Section V-A will not compromise the malicious functionality of original traffic, we still verify the malicious functionality of the mutated traffic in all six traffic sets.\nTo measure the malicious functionality, we use three types of indicators: attack effect, malicious behavior and attack efficiency, and compare them in original and mutated traffic. Take Botnet as an example to illustrate the three indicators: In our selected traffic, an attacker used a malware called Mirai to scan IoT devices in the LAN and successfully scanned 8 open devices. In this scenario, attack effect is the final result of the attack, which is that 8 devices were successfully scanned. Malicious behavior contains all offensive behaviors regardless of whether they eventually affect, which is the number of scans. Attack efficiency is related to the elapsed time of the attack. Obviously, attack effect has a greater impact on functionality than malicious behavior, and the change rate of attack efficiency must be within the attacker\u2019s overhead budget lt .\nWe use VMs and Dockers to simulate the experimental testbed for each traffic set by referring to their papers [13], [44]. Then we use Tcpreplay and Tcplivereplay to replay original and mutated attack traffic in the testbed and observe the three indicators. Since different attacks have diverse functionalities, the specific meanings of three indicators are case-by-case, making the validation experiment straightforward but tedious, so we put the details in Appendix C and leave the result here: Mutated traffic generated through our evasion attack can preserve the malicious functionality. Specifically, attack effect keeps unchanged in all cases and malicious behavior is reduced only in DoS/DDoS attacks (attack bandwidth is decreased due to increased time interval, but this reduction does not exceed the attacker\u2019s budget (lt ). As for attack efficiency, although our method may slow down some kinds of attack, the change rate of elapsed time is always less than lt ."}, {"heading": "G. Performance of Defense Schemes", "text": "In this section, we evaluate three mitigations mentioned in Section VI. For adversarial training (AT), we retrain the ML classifiers with 80% relabeled adversarial features and use the remaining 20% for testing. For feature selection (FS), we use embedded Lasso regression model to retain 80% dimensions. As for our adversarial feature reduction (AFR), we also retain 80% feature dimensions. We use Kitsune in all traffic sets, and use the decline of metrics (\u2206MER/PDR/MMR) to evaluate the defense performance. The results are shown in Fig. 6.\nCompared with AT and FS, our AFR is very effective in reducing MER/PDR/MMR. We observe that AT has a very limited and unstable defensive effect against the proposed attack. This is because it can limit the generation of adversarial features, but cannot prevent vulnerable feature dimensions from being exploited during traffic mutation. FS can perform\nbetter defense effectiveness in some cases. This shows that using fewer feature dimensions can increase the difficulty for attackers to transform the entire malicious features to some extent. We also measure the change of F1-score to evaluate whether the defense methods compromise the original detection performances. We find the change is quite small (within \u00b15%), so they are not depicted in the figure for reasons of spaces."}, {"heading": "VIII. DISCUSSIONS", "text": "We discuss limitations and potential improvements of our attacks as follows. Limitations. As mentioned in Section II, our method is designed for evading NIDSs without payload inspection, so it is invalid for systems additionally using payload-based detection. However, this problem can be easily solved by combining the polymorphic blending attack [9] with ours. And this can be easily implemented: leveraging polymorphic blending attack to encrypt the payload of original malicious traffic and using our method to inject crafted packets. Another limitation is that our attack is offline at present, but this can be solved by replaying mutated traffic since we have proofed that replayed traffic can conduct the same malicious intent as the original attack. Background traffic. In the proposed attack, we inject some crafted traffic which can be aggregated with original packets in order to impact features. However, some unpredictable background traffic (i.e., some traffic that is not controlled by the attacker but can also reach the victim or NIDS) may disrupt our mutated traffic on some features. Nonetheless, we find that only features aggregated by destination information (e.g., dstIP) are affected. Thus, the impact is extremely little (e.g., Kitsune has no features extracted only by destination). Improving the evasion attack. In this paper, we pay more attention to explore a more practical attack rather than try our best to improve the evasion rate. For one thing, we only use the default settings in the implementation of PSO and GAN in this study. For example, we simply use the set of parameters recommended in [48] (\u03c9=0.7298, c1=c2=1.49618) in the PSO algorithm. For another, we use Euclidean distance to measure the similarity of features in this work. We suggest that future work should focus on whether other distance function or careful parameter tuning can perform better results.\n11"}, {"heading": "IX. RELATED WORK", "text": "Evasion attacks on IDS. Adversarial Attacks on (N)IDS itself have been extensively studied [2]\u2013[4]. Evasion attack, as an important and common form, can be divided into two types: evading signature-based systems [5]\u2013[8] and anomalybased systems [9]\u2013[11], [49], [50]. Recently, machine learning, especially deep learning, has been increasingly used in NIDS for anomaly-based detection [1], [13], [38], [45], [46]. In this situation, existing evasion methods on traditional NIDSs are no longer valid. However, more opportunities can be provided due to internal vulnerabilities of ML [14], [15], [51]. Evasion attacks on learning-based systems. There have been several works on evasion attacks against learning-based systems in other domains. Adversarial example in the domain of computer vision has been widely studied [17]\u2013[19], [52]. Gradient Descent method [22] and Genetic Programming (GP) [23] were used for evading PDF malware classifier. MalGAN [24] was proposed to generate adversarial malware examples to evade learning-based malware detection systems using binary vectors. GAN-based method was also used to fool realtime video classification systems [20]. Text sentiment analysis system was evaded by stochastic optimization method [21]. However, due to the specificity of network traffic and learningbased NIDS, these methods cannot be directly applied. Feature-space attacks on learning-based NIDSs. Most prior studies on evading learning-based NIDSs assume that attackers can directly modify the feature vectors. According to the attacker\u2019s knowledge of the targeted NIDS, feature-space attacks can be divided into three categories (recall Table I): \u2022 Feature-space White-box Attack (FWA). FWA requires\nfull knowledge of the targeted NIDS. In [28], four gradientbased adversarial example attacks were directly used to evade an MLP classifier. Likewise, adversarial examples were leveraged in [30] to evade Kitsune. Similar gradientbased methods were also used in [27] to attack NIDSs for IoT networks, and in [31] to attack GAN-based NIDSs. \u2022 Feature-space Gray-box Attack (FGA). FGA requires the feedbacks of targeted classifier (without other knowledge of the classifier compared with FWA). In [25], a GAN-based architecture IDS-GAN was proposed to generate evasive features. In [53], a boundary-based method was proposed to evade DoS intrusion detection systems by perturbing continuous and discrete features. \u2022 Feature-space Black-box Attack (FBA). FBA neither requires feedbacks nor any knowledges on the classifier. In [54], four feature dimensions were randomly modified to attack learing-based botnet detectors. However, feature-space attacks are impractical since feature extraction in learning-based NIDSs is always irreversible. Traffic-space attacks on learning-based NIDSs. There were also a few studies directly change network traffic, which can be divided into two categories: \u2022 Traffic-space White-box Attack (TWA). In [34], a white-\nbox attack using similar mutation operators as ours was proposed. However, their assumption that the attacker has full knowledge of the NIDS is hard to achieve in practice. \u2022 Random mutations. Several mutations were proposed in [32] to evade botnet detectors. Random obfuscations on\ntraffic were proposed in [33]. However, these methods are purely stochastic and lack of theoretical guidance."}, {"heading": "X. CONCLUSION", "text": "This paper describes the first step toward developing a systematic study on practical traffic-space evasion attacks on learning-based NIDSs. Experimental results show our attack is effective (>97% evasion rate in half cases) and the proposed defense method can effectively mitigate such attacks. Surprisingly, our attack outperforms the state-of-the-art whitebox attack while using approximate execution cost, and is effective even without any knowledge of the targeted systems. We extensively measure the robustness of various learningbased NIDSs and provide important findings. Our finding demonstrates that the paradigm of feature engineering should be shifted; we deem the detection performance together with anti-evasion robustness both need to be taken into consideration while designing features used in systems. We firmly believe that our work provides important insights for improving the robustness of learning-based NIDSs and inspires more attention to the robust feature engineering in learning-based systems."}, {"heading": "APPENDIX A SUMMARIZATION OF FEATURE EXTRACTION METHODS", "text": "As mentioned in Section V-A, we summarize the feature types widely used in related work [13], [35]\u2013[40] from a highlevel. This summarized intelligence basically helps to conduct a generic attack and build the surrogate feature extractor for PBA. Specifically, feature extraction methods can be described in the following three aspects: \u2022 The data form of network traffic. This is also referred\nto as basic units for further process, which generally involves packet-based and session-based (including flowbased and connection-based) methods in a majority of cases. Specifically, a packet-based method inspects headers of all packets going through a network link, while session-based methods look at aggregated information of related packets of network traffic in the form of flow or connection. \u2022 Basic measurements. There are three network measurements extensively used in current research, which are sizerelated, count-related, and time-related. Take the packetbased extraction for illustration, size-related and countrelated measurements are packets\u2019 length (in bytes) and number of packets, respectively. And time-related measurements are inter-arrival time between packets. \u2022 Methods for processing measurements. Given the data form and measurements, we need to determine that measurements are collected from which packets or sessions as well as how to compute feature values from them. Specifically, these two phases can be summarized as follows: (i) Aggregate collection: Some work collects measurements with same traffic direction (inbound or outbound) or fields in the packet header such as IP address and port number. Other maintaining a window with a fixed time interval or packet length/bytes. (ii) Computing statistics: Besides using measurements directly, statistics are computed such as the mean, variance, and standard deviation of a single measurement or the\n12\ncovariance and distance between different measurements. Some other methods in mathematical statistics such as frequency distribution are also employed. Based on the summarization, Fig. 7 describes two extractors used in this work. Besides, we also illustrate how our mutation operators can influence different features in the figure."}, {"heading": "APPENDIX B ADVERSARIAL FEATURE EVALUATION ALGORITHM", "text": "Algorithm 2 shows the specific robustness evaluation method mentioned in Section VI. we proactively simulate the evasion attack and measure the MMR (on line 7). Then by considering whether this feature vector can evade the classifier, a penalty or reward is added to adversarial robustness score (on lines 8-9). Finally, the adversarial robustness of a feature set is quantized into the score between \u22121 to 1 (on line 12) of each feature.\nAlgorithm 2: Adversarial Feature Evaluation Algorithm Input: Fmal , F\u0302mal , Fadver , Fben, anomaly threshold h; Output: Adversarial feature score s of each dimension.\n1 nd \u2190 the dimensionality of a feature vector; 2 n f \u2190 Number of features in Fmal or F\u0302mal ; 3 Initialize s with nd zeros. 4 for each f ,f\u0302 in Fmal ,F\u0302mal ; i = 0 to n f \u2212 1 do 5 Initialize an r with nd zeros. 6 for j = 1 to nd do . each dimension 7 rj \u2190MMR(fj, f\u0302j,Fadver,Fben); O add a penalty if successfully evading 8 if E(f ) > h and E(f\u0302 ) < h then sj \u2190 sj \u2212 rj ; 9 else sj \u2190 sj + (1 \u2212 rj ) ; . add a reward\n10 end 11 end 12 Normalize s through dividing each dimension by n f . 13 return s"}, {"heading": "APPENDIX C VERIFYING THE MALICIOUS FUNCTIONALITY", "text": "Detailed results of verifying malicious functionality of all six attack traffic sets are listed in Table VIII. Note that, although the attack effect cannot be measured in some cases, in fact the results of attack effect are generally the same as malicious behavior. For example, in Brute Force, we do not know the\ntrue password of the victim server, but if we guarantee that all the original password attempts exist in the mutated traffic, then obviously the final result is the same."}], "title": "Practical Traffic-space Adversarial Attacks on Learning-based NIDSs", "year": 2020}