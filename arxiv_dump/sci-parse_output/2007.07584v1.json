{
  "abstractText": "Despite the growing body of work in interpretable machine learning, it remains unclear how to evaluate different explainability methods without resorting to qualitative assessment and user-studies. While interpretability is an inherently subjective matter, previous works in cognitive science and epistemology have shown that good explanations do possess aspects that can be objectively judged apart from fidelity), such as simplicity and broadness. In this paper we propose a set of metrics to programmatically evaluate interpretability methods along these dimensions. In particular, we argue that the performance of methods along these dimensions can be orthogonally imputed to two conceptual parts, namely the feature extractor and the actual explainability method. We experimentally validate our metrics on different benchmark tasks and show how they can be used to guide a practitioner in the selection of the most appropriate method for the task at hand.",
  "authors": [
    {
      "affiliations": [],
      "name": "An-phi Nguyen"
    },
    {
      "affiliations": [],
      "name": "Mar\u00eda Rodr\u00edguez Mart\u00ednez"
    }
  ],
  "id": "SP:c5acac4700acc609fbff92fd40d430351bbea8f6",
  "references": [
    {
      "authors": [
        "Paul Thagard"
      ],
      "title": "Philosophical and computational models of explanation",
      "venue": "Philosophical Studies, 64(1):87\u2013104,",
      "year": 1991
    },
    {
      "authors": [
        "Daniel Vishwanath",
        "Tara",
        "Kaufmann"
      ],
      "title": "Toward transparency : new approaches and their application to financial markets",
      "venue": "The World Bank research observer,",
      "year": 2001
    },
    {
      "authors": [
        "Riccardo Guidotti",
        "Anna Monreale",
        "Salvatore Ruggieri",
        "Franco Turini",
        "Fosca Giannotti",
        "Dino Pedreschi"
      ],
      "title": "A survey of methods for explaining black box models",
      "venue": "ACM Computing Surveys, 51(5),",
      "year": 2018
    },
    {
      "authors": [
        "Leilani H. Gilpin",
        "David Bau",
        "Ben Z. Yuan",
        "Ayesha Bajwa",
        "Michael Specter",
        "Lalana Kagal"
      ],
      "title": "Explaining explanations: An overview of interpretability of machine learning",
      "venue": "IEEE 5th International Conference on Data Science and Advanced Analytics,",
      "year": 2018
    },
    {
      "authors": [
        "Diogo V. Carvalho",
        "Eduardo M. Pereira",
        "Jaime S. Cardoso"
      ],
      "title": "Machine Learning Interpretability: A Survey on Methods and Metrics",
      "venue": "Electronics, 8(8):832,",
      "year": 2019
    },
    {
      "authors": [
        "Alejandro Barredo Arrieta",
        "Natalia D\u00edaz-Rodr\u00edguez",
        "Javier Del Ser",
        "Adrien Bennetot",
        "Siham Tabik",
        "Alberto Barbado",
        "Salvador Garcia",
        "Sergio Gil-Lopez",
        "Daniel Molina",
        "Richard Benjamins",
        "Raja Chatila",
        "Francisco Herrera"
      ],
      "title": "Explainable Explainable Artificial Intelligence (XAI): Concepts, taxonomies, opportunities and challenges toward responsible AI",
      "venue": "Information Fusion, 58:82\u2013115,",
      "year": 2020
    },
    {
      "authors": [
        "Erico Tjoa",
        "Cuntai Guan"
      ],
      "title": "A Survey on Explainable Artificial Intelligence (XAI): Towards Medical XAI",
      "year": 2019
    },
    {
      "authors": [
        "Tim Miller"
      ],
      "title": "Explanation in artificial intelligence: Insights from the social sciences, 2 2019",
      "year": 2019
    },
    {
      "authors": [
        "Finale Doshi-Velez",
        "Been Kim"
      ],
      "title": "Towards a rigorous science of interpretable machine learning",
      "venue": "arXiv preprint arXiv:1702.08608,",
      "year": 2017
    },
    {
      "authors": [
        "Tania Lombrozo"
      ],
      "title": "Explanatory Preferences Shape Learning and Inference, 10 2016",
      "venue": "ISSN 1879307X",
      "year": 2016
    },
    {
      "authors": [
        "Petri Ylikoski",
        "Jaakko Kuorikoski"
      ],
      "title": "Dissecting explanatory power",
      "venue": "Philosophical Studies, 148(2):201\u2013219,",
      "year": 2010
    },
    {
      "authors": [
        "Christoph Molnar"
      ],
      "title": "Interpretable Machine Learning",
      "year": 2019
    },
    {
      "authors": [
        "Chih-Kuan Yeh",
        "Cheng-Yu Hsieh",
        "Arun Suggala",
        "David I Inouye",
        "Pradeep K Ravikumar"
      ],
      "title": "On the (In)fidelity and Sensitivity of Explanations",
      "venue": "Advances in Neural Information Processing Systems",
      "year": 2019
    },
    {
      "authors": [
        "Zachary C. Lipton"
      ],
      "title": "The Mythos of Model Interpretability",
      "venue": "Communications of the ACM,",
      "year": 2016
    },
    {
      "authors": [
        "David Alvarez Melis",
        "Tommi Jaakkola"
      ],
      "title": "Towards Robust Interpretability with Self- Explaining Neural Networks",
      "venue": "Advances in Neural Information Processing Systems",
      "year": 2018
    },
    {
      "authors": [
        "Marco Tulio Ribeiro",
        "Sameer Singh",
        "Carlos Guestrin"
      ],
      "title": " Why should i trust you?\" Explaining the predictions of any classifier",
      "venue": "In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining,",
      "year": 2016
    },
    {
      "authors": [
        "Marco Tulio Ribeiro",
        "Sameer Singh",
        "Carlos Guestrin"
      ],
      "title": "Anchors: High-Precision Model- Agnostic Explanations",
      "venue": "In AAAI Conference on Artificial Intelligence (AAAI),",
      "year": 2018
    },
    {
      "authors": [
        "Marco Ancona",
        "Enea Ceolini",
        "Cengiz Oztireli",
        "Markus Gross"
      ],
      "title": "Towards better understanding of gradient-based attribution methods for Deep Neural Networks",
      "venue": "In 6th International Conference on Learning Representations (ICLR",
      "year": 2018
    },
    {
      "authors": [
        "Been Kim",
        "Rajiv Khanna",
        "Oluwasanmi O Koyejo"
      ],
      "title": "Examples are not enough, learn to criticize! Criticism for Interpretability",
      "venue": "Advances in Neural Information Processing Systems",
      "year": 2016
    },
    {
      "authors": [
        "Kacper Sokol",
        "Alexander Hepburn",
        "Raul Santos-Rodriguez",
        "Peter Flach"
      ],
      "title": "bLIMEy: Surrogate Prediction Explanations Beyond LIME",
      "venue": "URL",
      "year": 2019
    },
    {
      "authors": [
        "Been Kim",
        "Martin Wattenberg",
        "Justin Gilmer",
        "Carrie Cai",
        "James Wexler",
        "Fernanda Viegas",
        "Rory sayres"
      ],
      "title": "Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)",
      "venue": "Proceedings of the 35th International Conference on Machine Learning,",
      "year": 2018
    },
    {
      "authors": [
        "Ren\u00e9 Marois",
        "Jason Ivanoff"
      ],
      "title": "Capacity limits of information processing",
      "venue": "in the brain,",
      "year": 2005
    },
    {
      "authors": [
        "Thomas M. Cover",
        "Joy A. Thomas"
      ],
      "title": "Elements of Information Theory. Wiley Series in Telecommunications",
      "venue": "ISBN 0471062596",
      "year": 1991
    },
    {
      "authors": [
        "Chaofan Chen",
        "Oscar Li",
        "Daniel Tao",
        "Alina Barnett",
        "Cynthia Rudin",
        "Jonathan K Su"
      ],
      "title": "URL http://papers.nips.cc/paper/ 9095-this-looks-like-that-deep-learning-for-interpretable-image-recognition",
      "venue": "This Looks Like That: Deep Learning for Interpretable Image Recognition",
      "year": 2019
    },
    {
      "authors": [
        "Riccardo Guidotti",
        "Anna Monreale",
        "Stan Matwin",
        "Dino Pedreschi"
      ],
      "title": "Black Box Explanation by Learning Image Exemplars in the Latent Feature Space. 1 2020",
      "venue": "URL http://arxiv.org/ abs/2002.03746",
      "year": 2002
    },
    {
      "authors": [
        "Amit Dhurandhar",
        "Pin-Yu Chen",
        "Ronny Luss",
        "Chun-Chen Tu",
        "Paishun Ting",
        "Karthikeyan Shanmugam",
        "Payel Das"
      ],
      "title": "Explanations based on the Missing: Towards Contrastive Explanations with Pertinent Negatives",
      "venue": "Advances in Neural Information Processing Systems",
      "year": 2018
    },
    {
      "authors": [
        "Scott M Lundberg",
        "Su-In Lee"
      ],
      "title": "A Unified Approach to Interpreting Model Predictions",
      "venue": "Advances in Neural Information Processing Systems",
      "year": 2017
    },
    {
      "authors": [
        "Wayne W Daniel"
      ],
      "title": "Applied nonparametric statistics",
      "year": 1978
    },
    {
      "authors": [
        "Mukund Sundararajan",
        "Ankur Taly",
        "Qiqi Yan"
      ],
      "title": "Axiomatic Attribution for Deep Networks",
      "venue": "In Proceedings of the 34th International Conference on Machine Learning - Volume 70,",
      "year": 2017
    },
    {
      "authors": [
        "C. De Stefano",
        "M. Maniaci",
        "F. Fontanella",
        "A. Scotto di Freca"
      ],
      "title": "Reliable writer identification in medieval manuscripts through page layout features: The \u201cAvila",
      "venue": "Bible case. Engineering Applications of Artificial Intelligence, 72:99\u2013110,",
      "year": 2197
    },
    {
      "authors": [
        "Corrado Gini"
      ],
      "title": "Variabilit\u00e0 e mutabilit\u00e0",
      "venue": "Reprinted in Memorie di metodologica statistica (Ed. Pizetti E,",
      "year": 1912
    },
    {
      "authors": [
        "Alexander Kraskov",
        "Harald St\u00f6gbauer",
        "Peter Grassberger"
      ],
      "title": "Estimating mutual information. Physical Review E - Statistical Physics, Plasmas, Fluids, and Related Interdisciplinary Topics",
      "venue": "ISSN 1063651X. doi: 10.1103/PhysRevE.69.066138",
      "year": 2004
    },
    {
      "authors": [
        "Yann LeCun",
        "L\u00e9on Bottou",
        "Yoshua Bengio",
        "Patrick Haffner"
      ],
      "title": "Gradient-based learning applied to document recognition",
      "venue": "Proceedings of the IEEE,",
      "year": 1998
    },
    {
      "authors": [
        "Hugo Larochelle",
        "Dumitru Erhan",
        "Aaron Courville",
        "James Bergstra",
        "Yoshua Bengio"
      ],
      "title": "An empirical evaluation of deep architectures on problems with many factors of variation",
      "venue": "In ACM International Conference Proceeding Series,",
      "year": 2007
    },
    {
      "authors": [
        "Leonard Kaufman",
        "Peter J. Rousseeuw"
      ],
      "title": "Clustering by means of medoids",
      "venue": "North Holland / Elsevier, page 405\u2013416,",
      "year": 1987
    },
    {
      "authors": [
        "Karthik S. Gurumoorthy",
        "Amit Dhurandhar",
        "Guillermo Cecchi",
        "Charu Aggarwal"
      ],
      "title": "Efficient Data Representation by Selecting Prototypes with Importance Weights",
      "venue": "Proceedings - IEEE International Conference on Data Mining, ICDM, 2019-November:260\u2013269,",
      "year": 2017
    },
    {
      "authors": [
        "Jeong Soo Park"
      ],
      "title": "Tuning Complex Computer Codes to Data and Optimal Designs",
      "venue": "PhD thesis,",
      "year": 1992
    },
    {
      "authors": [
        "Karen Simonyan",
        "Andrea Vedaldi",
        "Andrew Zisserman"
      ],
      "title": "Deep inside convolutional networks: Visualising image classification models and saliency maps",
      "venue": "arXiv preprint arXiv:1312.6034,",
      "year": 2013
    },
    {
      "authors": [
        "Pieter-Jan Kindermans",
        "Kristof Sch\u00fctt",
        "Klaus-Robert M\u00fcller",
        "Sven D\u00e4hne"
      ],
      "title": "Investigating the influence of noise and distractors on the interpretation of neural networks",
      "year": 2016
    },
    {
      "authors": [
        "Xiang Zhang",
        "Junbo Zhao",
        "Yann LeCun"
      ],
      "title": "Character-Level Convolutional Networks for Text Classification",
      "venue": "In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1,",
      "year": 2015
    },
    {
      "authors": [
        "Sara Hooker",
        "Dumitru Erhan",
        "Pieter-Jan Kindermans",
        "Been Kim"
      ],
      "title": "A Benchmark for Interpretability Methods in Deep Neural Networks",
      "venue": "Advances in Neural Information Processing Systems",
      "year": 2019
    },
    {
      "authors": [
        "Ernest Davis"
      ],
      "title": "Ethical guidelines for a superintelligence",
      "venue": "Artificial Intelligence,",
      "year": 2015
    },
    {
      "authors": [
        "Dario Amodei",
        "Chris Olah",
        "Jacob Steinhardt",
        "Paul Christiano",
        "John Schulman",
        "Dan Man\u00e9"
      ],
      "title": "Concrete problems in AI safety",
      "venue": "arXiv preprint arXiv:1606.06565,",
      "year": 2016
    }
  ],
  "sections": [
    {
      "heading": "1 Introduction",
      "text": "The issue of interpretability and explainability in machine learning, albeit being a relatively old one [1], has experienced a recent surge in interest from society at large, from researchers to policymakers [2]. An evidence of this increased interest is the amount of review papers that has been written on the topic, e.g. [3\u20137]. Despite the rapid growth of the interpretable machine learning field, it is still unclear how different methods can be programmatically compared without the need of qualitative assessments or user-studies.\nA general computational benchmark across all possible methods is unlikely to be possible. After all, interpretability is an inherently subjective matter. As Miller [8] stated, explanations are contextual. That is, the perceived quality of an explanation is dependent on the (background of the) two interacting agents, i.e. the provider and the receiver of an explanation, and the type of information that is of interest to the receiver.\nInspired by previous work from cognitive science and epistemology, in this paper we identify three possible quantitative aspects of interpretability and provide a novel set of metrics to measure them. These metrics can guide the practitioner in the selection of the most appropriate method for the task at hand. The metrics are not meant to replace user-studies, but to guide the selection of a small subset of explanations to present to participants of an user-study, reducing the overall financial and time burden of such experiments. Using nomenclature introduced by Doshi-Velez and Kim [9], our metrics can be classified as functionally-grounded metrics, i.e. they rely on some definition of interpretability, rather than on human experiments.\nA crucial step to define these metrics is the conceptual separation of an interpretability method into a feature extractor and the actual explainability method. This is consequence of the contextual nature of interpretability: using a natural language analogy, the feature extractor provides the alphabet, while the explainability method provides the grammar.\nOur work primarily focuses on metrics for model interpretability, and not data interpretability/analysis. While the two fields may share a great number of tools, the former is interested in understanding how\nPreprint. Under review.\nar X\niv :2\n00 7.\n07 58\n4v 1\n[ cs\n.L G\n] 1\n5 Ju\nl 2 02\n0\na (generally human-implemented) model will behave on given data. The latter, instead, is interested in understanding the true unknown stochastic process generating the data."
    },
    {
      "heading": "1.1 Quantitative aspects: simplicity, broadness, and fidelity",
      "text": "Studies by Lombrozo [10] show that explanations play an important role in humans\u2019 cognitive processes, especially learning and inference. Perhaps not too surprisingly, the explanations generally considered good are simple and broad, i.e. more generally applicable. Ylikoski and Kuorikoski [11] provide a more elaborate account of these aspects. In particular, they propose dimensions along which the simplicity and broadness of explanations could be objectively assessed:\n\u2022 non-sensitivity: how robust is an explanation to unimportant details; \u2022 factual accuracy: how detailed is an explanation. An example of a non-factually accurate\nexplanation is an idealised one. In a machine learning context, it may be a feature with discretized values;\n\u2022 degree of integration: how generally applicable is the explanation; \u2022 cognitive salience: how easily can an explanation be grasped.\nThese works do not consider fidelity, or faithfulness, i.e. how correct is the explanation. This is simply because they assume that the explanations to be evaluated are true, since it would be meaningless, from an epistemologic perspective, to discuss a false explanation. On the other hand, fidelity plays a central role in interpretable machine learning [12\u201315], since often the computed explanations are only an approximation (e.g. surrogate modeling [12, 16, 17])."
    },
    {
      "heading": "2 Simplicity, broadness, and fidelity in practice",
      "text": "While the previously introduced aspects play a crucial role, we believe that it is not possible to define an implementation of metrics which can be applied to all the state-of-the-art interpretability methods. This is consequence of the contextual nature of interpretability, which manifests itself in two ways: in the features used for the explanation, and in the actual explainability modality. For example, in the case of an image classification task, we argue it would be difficult to define metrics applicable to LIME [16] (which assigns attributions to super-pixels), traditional feature attribution methods [18] (which assign attribution to individual pixels) or MMD-Critic [19] (which presents explanations in form of examples instead of attributions).\nTherefore different metrics need to be defined for the feature extractor (Section 2.1) and the actual explainability method. Furthermore, for an explainability method the implementation of the metrics will change according to the explanation modality (Section 2.2).\nWe emphasize that the feature extractor and the explainability model play orthogonal roles in the performance of a method in terms of simplicity, broadness, and fidelity. That is, the quality of an explanation can be easily changed by independently modifying either the features used or the underlying explainability model."
    },
    {
      "heading": "2.1 Feature Extractor metrics",
      "text": "Feature extraction is leveraged to create an interpretable data representation [16, 20]. This is especially true for high-dimensional data which is generally difficult to understand for a human [21, 22]. Since data processing may change the information content of the original samples, we propose to use mutual information [23] to monitor simplicity, broadness and fidelity.\nMetric 2.1 (Mutual Information) Let us denote with X a random variable (r.v.) taking values in the original space X , with Z = g(X) \u2208 Z the corresponding r.v. after feature extraction, and with Y \u2208 Y the target values (e.g. class labels). Simplicity and broadness can be monitored with the feature mutual information I(X, Z), while fidelity with the target mutual information I(Z, Y).\nLow mutual information can be achieved by using a non-injective feature extractor g(\u00b7). A noninjective extractor means that some information is discarded (simpler, but less factually accurate explanation) and that multiple original samples are mapped to the same value in the feature space\nZ (broader explanations, with high degree of integration). However, a too low feature mutual information would lead to a loss of information about the target Y (because of the data processing inequality [23]). Indeed, we should maintain the target mutual information as high as possible to provide a faithful explanation. A trade-off between the feature mutual information and the target mutual information corresponds to a trade-off between simplicity, broadness and fidelity."
    },
    {
      "heading": "2.2 Metrics for different modalities",
      "text": ""
    },
    {
      "heading": "2.2.1 Example-based methods",
      "text": "Example-based methods are interpretability methods providing a summarized overview of a model using representative examples, or high-level concepts [15, 21, 24, 25]. Given a prediction of interest, an example-based method explains the prediction either directly, providing examples with the same prediction [24], or counterfactually, by providing examples with a different prediction [26]. For example-based methods, we propose the following metrics.\nMetric 2.2 (Example-based method metrics) Let us denote with f : X \u2192 Y the model that we want to interpret. To evaluate the explanation for a prediction of interest y\u2217 \u2208 Y with a set of NE examples E = {xE1 , . . . ,xEi , . . . ,xENE}, the following quantities should be monitored:\n\u2022 Non-Representativeness: \u2211 x\u2208E l(y \u2217,f(x))\nNE \u2022 Diversity: \u2211\nxi,xj\u2208E xi 6=xj\nd(xi,xj) 2NE\nwhere d(\u00b7, \u00b7) is a distance function defined in the input space X .\nThe non-representativeness is mainly a measure of the fidelity of the explanation. High nonrepresentativeness, however, can also be indicative of factual inaccuracy. A highly diverse set of examples demonstrates the degree of integration of the explanation. The cognitive saliency, and ultimately the simplicity, of the explanation is simply encoded in the number NE of examples used: the least number of examples, the easier it is for a human to process the explanation [22]."
    },
    {
      "heading": "2.2.2 Feature Attributions",
      "text": "Feature attribution methods seek to assign an importance value to each feature depending on its contribution to the prediction. Feature attribution methods are arguably the most studied (and benchmarked) methods in interpretable machine learning, e.g. [18]. Here we take a slightly different perspective from previous work. We posit that the importance of a feature should be proportional to how imprecise would the prediction be if we did not know its value. More precisely, let us assume that we are provided with attributions ai to explain the importances of features i with i = 1, . . . , N for a function assuming value y\u2217 = f(x\u2217) at a point x\u2217 = (x\u22171, . . . , x \u2217 N ). Then, we claim that the following desideratum should hold:\n|ai| \u221d E(l(y\u2217, fi)|x\u2217\u2212i) = \u222b Xi l(y\u2217, fi(xi))p(xi)dxi (1)\nwhere fi is the restriction of the function f to the feature i obtained by fixing the other features at the values x\u2217\u2212i = (x1, . . . , xi\u22121, xi+1, xN ), and l is a performance measure of interest (e.g. cross-entropy). With a slight abuse of notation, the probability density in Eq. (1) could either be independent or dependent (i.e. conditional) on the values of the other features, depending if the wanted interpretation is true to the model or true to the data [27].\nThe desideratum in Eq. (1) naturally leads to two possible metrics.\nMetric 2.3 (Monotonicity) The monotonicity for feature attributions ai is defined as the Spearman\u2019s correlation coefficient \u03c1S(a, e) [28]. a = (. . . , |ai|, . . . ) is a vector containing the absolute values of the attributions. e = (. . . ,E(l(y\u2217, fi);Xi|x\u2217\u2212i), . . . ) contains the corresponding (estimated) expectations, as computed in Eq.(1).\nMetric 2.4 (Non-sensitivity) Let A0 \u2282 {1, . . . , N} be the subset of indeces i denoting features with assigned zero attribution, i.e. ai = 0. Further, let X0 = {i \u2208 {1, . . . , N}|E(l(y\u2217, fi) = 0} denote the subset of indeces i of features to which the model f is not functionally dependent on. The non-sensitivity is computed as |A04X0| where | \u00b7 | is the cardinality of a set, and4 denotes a symmetric difference.\nMonotonicity and non-sensitivity are indicative of how faithful a feature attribution explanation is. If attributions are not monotonic then we argue that they are not providing the correct importance of the features (Section 3.3). Non-sensitivity ensures that a method assigns zero-importance only to features to which the model f is not functionally dependent on. This is aligned with the Sensitivity(b) axiom proposed by Sundararajan et al. [29]. The number of non-zero attributions |{1, . . . , N} \\A0| serves as a measure of complexity.\nMonotonicity, non-sensitivity, and complexity account only for individual features. We therefore propose the effective complexity to account for feature interactions.\nMetric 2.5 (Effective Complexity) Let a(i) be the attributions ordered increasingly w.r.t. their absolute value, and x(i) the corresponding features. Let Mk = {x(N\u2212k), . . . , x(N)} be the set of top k features. Given a chosen tolerance > 0, the effective complexity is defined as\nk\u2217 = argmink\u2208{1,...,N}|Mk| s.t. E(l(y\u2217, f\u2212Mk)|x\u2217Mk) < (2)\nwhere f\u2212Mk , similarly to the definition above, is the restriction of the model f to the non-important features, given fixed values for the (important) features in Mk.\nA low effective complexity means that we can ignore some of the features even though they do have an effect (reduced cognitive salience) because the effect is actually small (non-sensitivity \u00e0 la Ylikoski and Kuorikoski [11]). This comes at the cost of reduced factual accuracy, but provides a higher degree of integration since we can freely change the value of the unimportant features without significant effects on the prediction. Therefore explanations with low effective complexity are both simple and broad."
    },
    {
      "heading": "3 Experimental validation",
      "text": ""
    },
    {
      "heading": "3.1 Feature extractor effect on LIME",
      "text": "In this section we analyze the effect of the feature extractor on the local explanation provided by LIME [16]. The target model is a decision tree classifier trained on the Avila dataset [30]. The dataset consists of 10 normalised features describing the writing patterns of 12 copyists of the Avila bible from the XII century. The task is to identify the copyist. The tree is trained using the Gini Index [31] for split selection, and a maximum depth of five to maintain the decision rules easy for comparison. This setting leads to a relatively poor model with test accuracy of 60.38%.\nWe analyze the local explanation provided for a sample from the test set, which was (mis-)classified as copyist \u201c7\u201d. The true classification rules of the target tree classifier, to predict class \u201c7\u201d are:\n\u2022 Intercolumnar distance > \u22120.10 AND Upper margin > \u22120.03 AND Lower margin > 0.17 AND Exploitation \u2264 0.05 AND Row number \u2264 0.84; OR\n\u2022 Intercolumnar distance > \u22120.10 AND 0.05 \u2264 Exploitation \u2264 0.73 AND Row number \u2264 0.84 AND Peak number > 1.11.\nFor feature extraction we consider Identity (i.e. maintaining the original features), Random (i.e. assigning the out-of-distribution value \u221210.00 to three randomly selected features), and a discretizer provided in the original implementation of LIME:1 Entropy. This discretizer first trains relatively shallow trees for each individual feature using the information entropy criterion. It then discretizes the features using the split learned by such trees. The right-most part of Figure 1c shows an example of this discretization.\nIn Table 1 we report the mutual information estimates (using [32]) computed for each feature extractor. The target random variable Y used for the computation of MI(Z, Y ) is the prediction generated\n1https://github.com/marcotcr/lime/tree/master/lime\nby the decision tree that we are (locally) interpreting. We also report the R2 score of LIME in approximating the tree prediction probabilities (\u201c7\u201d vs. \u201cnot-7\u201d) for the sample at hand.\nThe Identity extractor expectedly is the best in terms of mutual information between the original features and \u201cextracted features\u201d: for this case the mutual information just corresponds to the upperbound, i.e. the entropy of the original features. Figure 1a shows the feature rankings generated by LIME using these features. LIME correctly assigns top ranks to all the features used by the target decision tree to classify a sample as copyist \u201c7\u201d. Further, note how LIME correctly assigns the positivity/negativity of the evidence (color-coded in the figure) in accordance to the true rules, e.g. Intercolumnar distance= 0.20 constitutes positive evidence, in agreement with the rule Intercolumnar distance> \u22120.10. The Random extractor understandably performs worse in terms of mutual information MI(X,Z) between features. However, the mutual information with the target variable MI(Z, Y ) is comparable to the computed for the Identity extractor. This suggests that, on average, we lost information contained in the original features, but managed to maintain enough information to potentially replicate the predictions of the target decision tree. The sample we are studying here is, however, an exception. Figure 1b shows that one of the randomly modified features is the Intercolumnar distance, which is important for the classification of a sample as copyist \u201c7\u201d, as discussed above. This seems to cause LIME to provide incorrect results, especially in terms of positivity/negativity of the evidence. Interestingly, the approximation R2 score reported by LIME misleadingly suggests that by leveraging a Random extractor, a good (w.r.t. to the Identity case) explanation can be generated. However, as we just discussed this is not true, which shows how local approximation scores should not be completely relied on in evaluating a local explanation.\nNote however that, while a lower value of the mutual information metrics does correspond to an inaccurate explanation, the value itself may not monotonically correlate with the degree of inaccuracy of the explanation. This is shown in the case of the Entropy extractor. The discretization introduced by this extractor (shown in Figure 1c) lead to even lower mutual information scores. Nonetheless, LIME managed to provide rules more in agreement with the true rules compared to the Random extractor case. Furthermore, simplifying the original feature through discretization provides a further advantage: many of the original samples actually have the same representation in the discretized space. This means that the explanation is not only simpler, but more broad."
    },
    {
      "heading": "3.2 Example-based explanations on Rotated MNIST",
      "text": "We showcase the metrics for example-based explanations (Section 2.2.1) on an image classification task. We train a CNN model [33] on the Rotated MNIST dataset [34]. The trained model achieves a test accuracy of 92%. For interpretability, we aim at understanding how the target CNN model performs the classification by producing exemplars/prototypes for each of the ten classes. We consider three different methods: K-medoids [35], MMD [19], and ProtoDash [36]. In Figure 2, we show the prototypes selected by each method when selecting six prototypes per class. Table 2 reports the relative metrics averaged over all classes.\nThe metrics reveal that K-medoids provides the most representative prototypes, suggesting that the classification rules that can potentially be inferred by looking at these exemplars are the most faithful ones. Unfortunately this comes at the cost of lower diversity, as shown by both the diversity metric and Figure 2a. This means that the inferred classification rules will not generalize. An example of these claims can be seen in the prototypes for class 4. Looking at the exemplars generated by K-medoids, one may infer that an open upper part and a relatively thin stroke are distinctive features of the digit 4. While these may be high-precision rules, they are not broad since the prototypes produced by the other methods show that digits classified as 4 can have very different strokes and styles, including having a closed upper part (fifth prototype generated by MMD). Another advantage of more general rules is that they may help reveal the reasons for a relatively low test accuracy. For example, the ProtoDash prototypes suggest that the target CNN may confuse 7 as 2, or 4 as 5.\nThe above discussion has shown how our metrics can be used to assess the fidelity and broadness of example-based explanations. The simplicity of an example-based explanation is simply the number of the exemplars. It is interesting to show how our metrics evolve in relation to the number of examples used (Figure 3). ProtoDash in general seems to be consistently less representative and more diverse. While (expectedly) the diversity for different methods converges the more prototypes are selected, the mean representativeness seem to stay relatively constant. This suggests that, when aiming to have simple explanations, ProtoDash should be preferred for more general explanations, while K-Medoids for more faithful ones."
    },
    {
      "heading": "3.3 Evaluating feature attributions",
      "text": "We validate our metrics for feature attribution methods on two tasks. First we study how various methods perform in interpreting a non-linear function with known analytical formula which will act as a ground truth. We then extend our conclusions to a text classification task with higher dimensionality."
    },
    {
      "heading": "3.3.1 Test Functions",
      "text": "We showcase our proposed metrics to evaluate feature attribution methods applied to interpret the test function f : x \u2208 [0, 1)6 7\u2192 23e\n(x0+x1) \u2212 x3 sin(x2) + x2 proposed by Park [37] at point x\u2217 = (0.24, 0.48, 0.56, 0.99, 0.68, 0.86). Note that the function includes two \u201cnon-active\u201d variables to further test the non-sensitivity of the methods. The methods we consider are: pure gradient, also known as Saliency [38], gradients multiplied by the input (InpXGrad) [39],integrated gradients (IntGrad) [29]. For sanity check we analyze also randomly assigned attributions (Random). The metrics are reported in Table 3a. The computed attributions are provided in the supplementary for reference (Figure 4).\nAs expected, Random performs worse than the other methods on all four metrics: it fails to assign the correct importances leading to a non-monotonic behaviour and to a higher complexity. In particular, it fails to detect the dummy variables. This is not the case for the other considered methods which are non-sensitive to the dummy variables, correctly assigning importance only to the other 4 variables.\nIntGrad performs is less monotonic than other methods. This is because IntGrad, for the sample at hand, assigns less importance to x0 than to x2 and x3, which we argue is incorrect (i.e. not faithful). Indeed, for the test function we are analyzing, the contribution of the terms \u2212x3 sin(x2) + x2 is smaller than the exponential term, in the given domain. Therefore, not knowing the value of x0 (or x1) would lead to a higher uncertainty when predicting the value of the test function f . For a similar reason, InpXGrad performs worse than Saliency: it assigns less importance to x0 than to x3.\nFinally, let us consider the effective complexity. InpXGrad and Saliency report 3 effectively important variables. We argue that, once again, they provide a more faithful representation of the function f . Indeed, for x3 = 0.99 and x2 \u2208 [0, 1), we can approximate the non-exponential terms \u2212x3 sin(x2) + x2 \u2248 \u22120.99x2 + x2 = 0.01x2, meaning that the contribution of x2 to the function f is effectively negligible in this case. The effective complexity therefore shows that InpXGrad and Saliency provide more broad explanations in the sense discussed in Section 2.2.2."
    },
    {
      "heading": "3.3.2 Text Classification",
      "text": "We further validate our metrics on a more complex task: text classification. The task consists of classifying news excerpts from the AG News Corpus [40] according to the topic. We train a CNN model reaching 89% test accuracy. We now want to understand which words from the excerpt are the most important. The sample we picked for this demonstration is shown in the supplementary material (Figure 5). The performance of Saliency, InpXGrad, and IntGrad according to our metrics is reported in Table 3b.\nOur results show that IntGrad provide the most general explanation (low effective complexity), while not being monotonic. This suggests that, for this sample, IntGrad is able to find the most important group of words for the classification, but it is not able to accurately estimate the importance of each individual word.\nTo demonstrate that the explanation provided by IntGrad is the most general, we perform a perturbation test. For each method, we fix the most important words, while substituting the other words with another word randomly picked from the dataset corpus. We then classify the perturbed samples with the trained CNN. The score (PT) for this test, reported in Table 3b, indicates the ratio of perturbed samples that maintained the same predicted class as the original sample. Note that the number of words kept fixed for each method depends on the identified effective complexity. Our results show that, despite having a higher number of perturbed words, IntGrad performed similarly to the other methods, confirming the broadness of its explanation."
    },
    {
      "heading": "4 Conclusion and Related Work",
      "text": "In this work we provide a novel set of metrics for the evaluation of interpretability methods on the basis of objective and quantifiable aspects of interpretability. Key steps in our work are: 1. the identification of the quantifiable aspects; 2. the identification of two conceptual parts of an interpretability method, which is necessary for a proper evaluation; 3. the definition of interpretability metrics. We experimentally validated the metrics on different benchmark datasets and showcased how they can be leveraged to identify the most appropriate for the task at hand. The most appropriate method will depend on the task and on the target user of the explanation: these two aspects will define the trade-off between simplicity, broadness, and fidelity.\nTo the best of our knowledge, we are the first to propose a systematic accounting of functionallygrounded [9] metrics for interpretability evaluation. In particular, we are not aware of any work towards programmatically evaluating the interpretability of the feature extractor and example-based methods. We however acknowledge that previous work did recognize the pivotal role of the feature extractor in the design of an interpretability method [9, 16, 17, 20]. On the other hand, this conceptual separation was not leveraged for evaluation purposes.\nFeature attributions are perhaps the most studied methods from a benchmarking perspective. Ancona et al. [18] propose sensitivity-n. This metric allowed the authors to reach conclusions about the algorithmic behaviour of the considered methods. In particular, if all the models satisfy sensitivity-n for all n, they can conclude that the model they are interpreting behaves linearly. It remains however unclear how to use this metric to compare the methods in terms of interpretability. Hooker et al. [41] propose a re-training strategy, ROAR, as a metric for interpretability evaluation. We argue that a re-trained model would give little insight about the decision process of the original model that was to be interpreted, i.e. ROAR is not measuring the fidelity of the attributions. Indeed, if a model re-trained on a dataset with removed feature performs similarly to the original model, it may just mean that the feature the model was using has high correlation with another feature in the dataset: ROAR measures data interpretability, rather than model interpretability. Yeh et al. [13] recently proposed an infidelity metric. Interestingly it can be shown that various attribution methods optimize this measure, depending on the distribution used to compute the metric. However, their proposed metric requires the definition of baselines, which we argue are not necessary in general. The metric that perhaps is closest to our proposals is the monotonicity metric proposed by Arya et al. [42]. However, their monotonicity measure is w.r.t. a probability drop, while ours is w.r.t. the uncertainty in probability estimation. Furthermore, depending on the chosen loss function l, our definition can be extended to regression tasks.\nBroader Impact\nThe need of interpretability towards societal issues is undeniable, starting from ethical [43] and safety [44] concerns. As discussed in the introduction, despite the rapid growth of the interpretable machine learning field, it is not clear how to compare methods without resorting to user-studies. We argue that this lack of metrics will lead to an inefficient development of the field, ultimately working against its rapid growth. Our work is therefore a step forward towards clarifying which objectives should be pursued to \u201coptimize interpretability\u201d, with consequent advantages to the downstream societal issues. Currently, we do not see any direct negative impact of our work presented here."
    },
    {
      "heading": "A Additional Figures",
      "text": "(a )E\nxa m\npl e\nof pe\nrt ur\nba tio\nns w\nith at\ntr ib\nut io\nns pr\nov id\ned by\nSa li\nen cy\n.\n(b )E\nxa m\npl e\nof pe\nrt ur\nba tio\nns w\nith at\ntr ib\nut io\nns pr\nov id\ned by\nIn pX\nGr ad\n.\n(c )E\nxa m\npl e\nof pe\nrt ur\nba tio\nns w\nith at\ntr ib\nut io\nns pr\nov id\ned by\nIn tG\nra d.\nFi gu\nre 5:\nPe rt\nur ba\ntio n\nsa m\npl es\nfo re\nac h\nan al\nys ed\nm et\nho d.\nT he\nor ig\nin al\nsa m\npl e\nis at\nth e\nto p\nof ea\nch su\nbfi gu\nre .N\not e\nho w\nth e\nsa m\npl es\nfo rI\nnt Gr\nad co\nnt ai\nn m\nor e\npe rt\nur be\nd w\nor ds\n."
    }
  ],
  "title": "On quantitative aspects of model interpretability",
  "year": 2020
}
