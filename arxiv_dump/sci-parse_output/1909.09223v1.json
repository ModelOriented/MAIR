{"abstractText": "InterpretML is an open-source Python package which exposes machine learning interpretability algorithms to practitioners and researchers. InterpretML exposes two types of interpretability \u2013 glassbox, which are machine learning models designed for interpretability (ex: linear models, rule lists, generalized additive models), and blackbox explainability techniques for explaining existing systems (ex: Partial Dependence, LIME). The package enables practitioners to easily compare interpretability algorithms by exposing multiple methods under a unified API, and by having a built-in, extensible visualization platform. InterpretML also includes the first implementation of the Explainable Boosting Machine, a powerful, interpretable, glassbox model that can be as accurate as many blackbox models. The MIT licensed source code can be downloaded from github.com/microsoft/interpret.", "authors": [{"affiliations": [], "name": "Harsha Nori"}, {"affiliations": [], "name": "Samuel Jenkins"}], "id": "SP:3544aed6400a5196ffe1eb1d8405fe2ee21c663a", "references": [{"authors": ["Muhammad Aurangzeb Ahmad", "Carly Eckert", "Ankur Teredesai"], "title": "Interpretable machine learning in healthcare", "venue": "In Proceedings of the 2018 ACM International Conference on Bioinformatics, Computational Biology, and Health Informatics,", "year": 2018}, {"authors": ["Rich Caruana", "Paul Koch", "Yin Lou", "Marc Sturm", "Johannes Gehrke", "Noemie Elhadad"], "title": "Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission", "venue": "In KDD\u201915,", "year": 2015}, {"authors": ["Chaofan Chen", "Kangcheng Lin", "Cynthia Rudin", "Yaron Shaposhnik", "Sijia Wang", "Tong Wang"], "title": "An interpretable model with globally consistent explanations for credit risk", "venue": "arXiv preprint arXiv:1811.12615,", "year": 2018}, {"authors": ["Tianqi Chen", "Carlos Guestrin"], "title": "XGBoost: A scalable tree boosting system", "venue": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD", "year": 2016}, {"authors": ["Jerome H. Friedman"], "title": "Greedy function approximation: A gradient boosting machine", "venue": "Ann. Statist., 29(5):1189\u20131232,", "year": 2001}, {"authors": ["Petr Hajek"], "title": "Interpretable fuzzy rule-based systems for detecting financial statement fraud", "venue": "In IFIP International Conference on Artificial Intelligence Applications and Innovations,", "year": 2019}, {"authors": ["Trevor Hastie", "Robert Tibshirani"], "title": "Generalized additive models: some applications", "venue": "Journal of the American Statistical Association,", "year": 1987}, {"authors": ["Jon Herman", "Will Usher"], "title": "SALib: An open-source python library for sensitivity analysis", "venue": "The Journal of Open Source Software,", "year": 2017}, {"authors": ["Fred Hohman", "Andrew Head", "Rich Caruana", "Robert DeLine", "Steven M. Drucker"], "title": "Gamut: A design probe to understand how data scientists understand machine learning models", "venue": "In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems,", "year": 2019}, {"authors": ["Yin Lou", "Rich Caruana", "Johannes Gehrke"], "title": "Intelligible models for classification and regression", "venue": "In The 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201912,", "year": 2012}, {"authors": ["Yin Lou", "Rich Caruana", "Johannes Gehrke", "Giles Hooker"], "title": "Accurate intelligible models with pairwise interactions", "venue": "In The 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "year": 2013}, {"authors": ["Scott M Lundberg", "Su-In Lee"], "title": "A unified approach to interpreting model predictions", "venue": "Advances in Neural Information Processing Systems", "year": 2017}, {"authors": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay"], "title": "Scikit-learn: Machine learning in Python", "venue": "Journal of Machine Learning Research,", "year": 2011}, {"authors": ["Marco Tulio Ribeiro", "Sameer Singh", "Carlos Guestrin"], "title": "why should I trust you?\u201d: Explaining the predictions of any classifier", "venue": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data", "year": 2016}, {"authors": ["Sarah Tan", "Rich Caruana", "Giles Hooker", "Yin Lou"], "title": "Distill-and-compare: Auditing blackbox models using transparent model distillation", "venue": "In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society,", "year": 2018}, {"authors": ["Nori", "Jenkins", "Koch", "Caruana Xuezhou Zhang", "Sarah Tan", "Paul Koch", "Yin Lou", "Urszula Chajewska", "Rich Caruana"], "title": "Interpretability is harder in the multiclass setting: Axiomatic interpretability for multiclass additive models", "venue": "CoRR, abs/1810.09092,", "year": 2018}], "sections": [{"text": "Keywords: Interpretability, Explainable Boosting Machine, Glassbox, Blackbox"}, {"heading": "1. Introduction", "text": "As machine learning has matured into wide-spread adoption, building models that users can understand is becoming increasingly important. This can easily be observed in high-risk applications such as healthcare (Ahmad et al., 2018; Caruana et al., 2015), finance (Hajek, 2019; Chen et al., 2018) and judicial environments (Tan et al., 2018; Soundarajan and Clausen). Interpretability is also important in general applied machine learning problems such as model debugging, regulatory compliance, and human computer interaction.\nWe address these needs with InterpretML by exposing many state of the art interpretability algorithms under a unified API. This API covers two major interpretability forms: \u201dglassbox\u201d models, which are inherently intelligible and explainable to the user, and \u201dblackbox\u201d interpretability, methods that generate explanations for any machine learning pipeline, no matter how opaque it is. This is further supported with interactive visualizations and a built-in dashboard designed for interpretability algorithm comparison. InterpretML is MIT licensed, and emphasizes extensibility and compatibility with popular open-source projects such as scikit-learn (Pedregosa et al., 2011) and Jupyter Notebook environments (Kluyver et al., 2016).\nar X\niv :1\n90 9.\n09 22\n3v 1\n[ cs\n.L G\n] 1"}, {"heading": "2. Package Design", "text": "InterpretML follows four key design principles that influence its architecture and API.\nEase of comparison. Make it as easy as possible to compare multiple algorithms. ML interpretability is in its infancy, and many algorithmic approaches have emerged from research, each of which has pros and cons. Comparison is critical to find the algorithm that best suits the users\u2019 needs. InterpretML enables this by enforcing a scikit-learn style uniform API, and providing a visualization platform centered around algorithmic comparison.\nStay true to the source. Use reference algorithms and visualizations as much as possible. Our goal is to expose interpretability algorithms to the world, in their most accurate form.\nPlay nice with others. Leverage the open-source ecosystem, and don\u2019t reinvent the wheel. InterpretML is highly compatible with popular projects like Jupyter Notebook and scikit-learn, and builds off of many libraries like plotly, lime, shap, and SALib.\nTake what you want. Use and extend any component of InterpretML without pulling in the whole framework. For example, it\u2019s possible to produce a computationally intensive explanation on a server, without InterpretML\u2019s visualization and its related dependencies.\nThe code architecture and unified API is best expressed in Figure 1, providing an overview and relevant example code."}, {"heading": "3. Explainable Boosting Machine", "text": "As part of the framework, InterpretML also includes a new interpretability algorithm \u2013 the Explainable Boosting Machine (EBM). EBM is a glassbox model, designed to have accuracy comparable to state-of-the-art machine learning methods like Random Forest and Boosted Trees, while being highly intelligibile and explainable. EBM is a generalized additive model (GAM) of the form:\ng(E[y]) = \u03b20 + \u2211 fj(xj)\nwhere g is the link function that adapts the GAM to different settings such as regression or classification. EBM has a few major improvements over traditional GAMs (Hastie and Tibshirani, 1987). First, EBM learns each feature function fj using modern machine learning techniques such as bagging and gradient boosting. The boosting procedure is carefully restricted to train on one feature at a time in round-robin fashion using a very low learning rate so that feature order does not matter. It round-robin cycles through features to mitigate the effects of co-linearity and to learn the best feature function fj for each feature to show how each feature contributes to the model\u2019s prediction for the problem. Second, EBM can automatically detect and include pairwise interaction terms of the form:\ng(E[y]) = \u03b20 + \u2211 fj(xj) + \u2211 fij (xi, xj)\nwhich further increases accuracy while maintaining intelligibility. EBM is a fast implementation of the GA2M algorithm (Lou et al., 2013), written in C++ and Python. The implementation is parallelizable, and takes advantage of joblib to provide multi-core and multi-machine parallelization. The algorithmic details for the training procedure, selection of pairwise interaction terms, and case studies can be found in (Lou et al., 2012, 2013; Caruana et al., 2015).\nEBMs are highly intelligible, because the contribution of each feature to a final prediction can be visualized and understood by plotting fj. Because EBM is an additive model, each feature contributes to predictions in a modular way that makes it easy to reason about the contribution of each feature to the prediction.\nTo make individual predictions, each function fj acts as a lookup table per feature, and returns a term contribution. These term contributions are simply added up, and passed through the link function g to compute the final prediction. Because of the modularity (additivity), term contributions can be sorted and visualized to show which features had the most impact on any individual prediction.\nIn terms of predictive power, EBM often performs surprisingly well, and is comparable with state of the art methods like Random Forest and XGBoost.1 To keep the individual terms additive, EBM pays an additional training cost, making it somewhat slower than similar methods. However, because making predictions involves simple additions and lookups inside of the feature functions fj, EBMs are one of the fastest models to execute at prediction time. EBM\u2019s light memory usage and fast predict times makes it particularly attractive for model deployment in production."}, {"heading": "Acknowledgments", "text": "We would like to acknowledge everyone on our acknowledgements.md file for their support on this project. We also depend on many amazing software packages and research: scikit-learn\n1. All models were trained with their default parameters. EBM\u2019s current default parameters are chosen for computational speed, to enable ease of experimentation. For the best accuracy and interpretability, we recommend using reference parameters: 100 inner bags, 100 outer bags, 5000 epochs, and a learning rate of 0.01.\n(Pedregosa et al., 2011), plotly (Plotly, 2015), lime (Ribeiro et al., 2016), shap (Lundberg and Lee, 2017), SALib (Herman and Usher, 2017), partial dependence (Friedman, 2001), Jupyter (Kluyver et al., 2016), pandas (McKinney), and more."}], "title": "InterpretML: A Unified Framework for Machine Learning Interpretability", "year": 2019}