{"abstractText": "What does it mean for a machine learning model to be \u2018fair\u2019, in terms which can be operationalised? Should fairness consist of ensuring everyone has an equal probability of obtaining some benefit, or should we aim instead to minimise the harms to the least advantaged? Can the relevant ideal be determined by reference to some alternative state of affairs in which a particular social pattern of discrimination does not exist? Various definitions proposed in recent literature make different assumptions about what terms like discrimination and fairness mean and how they can be defined in mathematical terms. Questions of discrimination, egalitarianism and justice are of significant interest to moral and political philosophers, who have expended significant efforts in formalising and defending these central concepts. It is therefore unsurprising that attempts to formalise \u2018fairness\u2019 in machine learning contain echoes of these old philosophical debates. This paper draws on existing work in moral and political philosophy in order to elucidate emerging debates about fair machine learning.", "authors": [{"affiliations": [], "name": "Reuben Binns"}, {"affiliations": [], "name": "Christo Wilson"}], "id": "SP:f9cf2ebde6b5106c41dcf36a5f363fd0078d5363", "references": [{"authors": ["Elizabeth S Anderson"], "title": "What is the point of equality? Ethics", "year": 1999}, {"authors": ["Richard J Arneson"], "title": "Equality and equal opportunity for welfare", "venue": "Philosophical studies,", "year": 1989}, {"authors": ["Solon Barocas", "Andrew D Selbst"], "title": "Big data\u2019s disparate impact", "venue": "Cal. L. Rev.,", "year": 2016}, {"authors": ["Reuben Binns", "Michael Veale", "Max Van Kleek", "Nigel Shadbolt"], "title": "Like trainer, like bot? inheritance of bias in algorithmic content moderation", "venue": "In International Conference on Social Informatics,", "year": 2017}, {"authors": ["Aylin Caliskan-Islam", "Joanna J Bryson", "Arvind Narayanan"], "title": "Semantics derived automatically from language corpora necessarily contain human biases", "venue": "arXiv preprint arXiv:1608.07187,", "year": 2016}, {"authors": ["Gerald A Cohen"], "title": "On the currency of egalitarian justice", "venue": "Ethics, 99(4):906\u2013944,", "year": 1989}, {"authors": ["William Dieterich", "Christina Mendoza", "Tim Brennan"], "title": "Compas risk scales: Demonstrating accuracy equity and predictive parity", "venue": "Northpoint Inc,", "year": 2016}, {"authors": ["Cynthia Dwork", "Moritz Hardt", "Toniann Pitassi", "Omer Reingold", "Richard Zemel"], "title": "Fairness through awareness", "venue": "In Proceedings of the 3rd Innovations in Theoretical Computer Science Conference,", "year": 2012}, {"authors": ["Ronald Dworkin"], "title": "What is equality? part 1: Equality of welfare", "venue": "Philosophy & public affairs,", "year": 1981}, {"authors": ["Benjamin Eidelson"], "title": "Discrimination and Disrespect", "year": 2015}, {"authors": ["Hanming Fang", "Andrea Moro"], "title": "Theories of statistical discrimination and affirmative action: A survey", "venue": "Technical report, National Bureau of Economic Research,", "year": 2010}, {"authors": ["Nancy Fraser", "Axel Honneth"], "title": "Redistribution or recognition?: a political-philosophical exchange", "venue": "Verso,", "year": 2003}, {"authors": ["Margaret Gilbert"], "title": "Collective epistemology", "venue": "Episteme, 1(2):95\u2013107,", "year": 2004}, {"authors": ["Sara Hajian", "Josep Domingo-Ferrer"], "title": "A methodology for direct and indirect discrimination prevention in data mining", "venue": "IEEE transactions on knowledge and data engineering,", "year": 2013}, {"authors": ["Moritz Hardt", "Eric Price", "Nati Srebro"], "title": "Equality of opportunity in supervised learning", "venue": "In Advances in Neural Information Processing Systems,", "year": 2016}, {"authors": ["Deborah Hellman"], "title": "When is discrimination wrong", "year": 2008}, {"authors": ["Elisa Holmes"], "title": "Anti-discrimination rights without equality", "venue": "The Modern Law Review,", "year": 2005}, {"authors": ["Bell Hooks"], "title": "Yearning: Race, gender, and cultural politics", "year": 1992}, {"authors": ["Robert Huseby"], "title": "Can luck egalitarianism justify the fact that some are worse off than others", "venue": "Journal of Applied Philosophy,", "year": 2016}, {"authors": ["Matthew Joseph", "Michael Kearns", "Jamie Morgenstern", "Seth Neel", "Aaron Roth"], "title": "Rawlsian fairness for machine learning", "venue": "arXiv preprint arXiv:1610.09559,", "year": 2016}, {"authors": ["Faisal Kamiran", "Toon Calders", "Mykola Pechenizkiy"], "title": "Techniques for discrimination-free predictive models", "venue": "In Discrimination and Privacy in the Information Society,", "year": 2013}, {"authors": ["Jon Kleinberg", "Sendhil Mullainathan", "Manish Raghavan"], "title": "Inherent trade-offs in the fair determination of risk scores", "venue": "arXiv preprint arXiv:1609.05807,", "year": 2016}, {"authors": ["Annabelle Lever"], "title": "Racial profiling and the political philosophy of race", "venue": "The Oxford Handbook of Philosophy and Race,", "year": 2016}, {"authors": ["Kasper Lippert-Rasmussen"], "title": "Born free and equal?: a philosophical inquiry into the nature of discrimination", "year": 2014}, {"authors": ["Edmund S Phelps"], "title": "The statistical theory of racism and sexism", "venue": "The american economic review,", "year": 1972}, {"authors": ["John Rawls"], "title": "A theory of justice", "venue": "Harvard university press,", "year": 2009}, {"authors": ["Frederick F Schauer"], "title": "Profiles, probabilities, and stereotypes", "year": 2009}, {"authors": ["Shlomi Segall"], "title": "What\u2019s so bad about discrimination? Utilitas", "year": 2012}, {"authors": ["Amartya Sen"], "title": "Inequality reexamined", "year": 1992}, {"authors": ["Jens Damgaard Thaysen", "Andreas Albertsen"], "title": "When bad things happen to good people: luck egalitarianism and costly rescues", "venue": "Politics, Philosophy & Economics,", "year": 2017}, {"authors": ["Michael Veale", "Reuben Binns"], "title": "Fairer machine learning in the real world: Mitigating discrimination without collecting sensitive data", "venue": "Big Data & Society,", "year": 2017}, {"authors": ["Michael Walzer"], "title": "Spheres of justice: A defense of pluralism and equality", "venue": "Basic books,", "year": 2008}, {"authors": ["Meike Zehlike", "Francesco Bonchi", "Carlos Castillo", "Sara Hajian", "Mohamed Megahed", "Ricardo Baeza-Yates"], "title": "Fa* ir: A fair top-k ranking algorithm", "venue": "arXiv preprint arXiv:1706.06368,", "year": 2017}], "sections": [{"text": "ar X\niv :1\n71 2.\n03 58\n6v 2\n[ cs\n.C Y\n] 2\nJ an\n2 01\nmodel to be \u2018fair\u2019, in terms which can be operationalised? Should fairness consist of ensuring everyone has an equal probability of obtaining some benefit, or should we aim instead to minimise the harms to the least advantaged? Can the relevant ideal be determined by reference to some alternative state of affairs in which a particular social pattern of discrimination does not exist? Various definitions proposed in recent literature make different assumptions about what terms like discrimination and fairness mean and how they can be defined in mathematical terms. Questions of discrimination, egalitarianism and justice are of significant interest to moral and political philosophers, who have expended significant efforts in formalising and defending these central concepts. It is therefore unsurprising that attempts to formalise \u2018fairness\u2019 in machine learning contain echoes of these old philosophical debates. This paper draws on existing work in moral and political philosophy in order to elucidate emerging debates about fair machine learning.\nKeywords: fairness, discrimination, machine learning, algorithmic decisionmaking, egalitarianism"}, {"heading": "1. Introduction", "text": "Machine learning allows us to predict and classify phenomena, by training models using labeled data from the real world. When consequential decisions are made about individuals on the basis of the outputs of such models, concerns about discrimination and fairness inevitably arise. What if the model\u2019s outputs result in decisions that are systematically biased against people with certain protected\ncharacteristics like race, gender or religion? If there are underlying patterns of discrimination in the real world, such biases will likely be picked up in the learning process. This could result in certain groups being unfairly denied loans, insurance, or employment opportunities. Cognisant of this problem, a burgeoning research paradigm of \u2018discrimination-aware data mining\u2019 and \u2018fair machine learning\u2019 has emerged, which attempts to detect and mitigate unfairness Hajian and Domingo-Ferrer (2013); Kamiran et al. (2013); Barocas and Selbst (2016).\nOne question which immediately arises in such an endeavour is the need for formalisation. What does it mean for a machine learning model to be \u2018fair\u2019 or \u2018non-discriminatory\u2019, in terms which can be operationalised? Various measures have been proposed. A common theme is comparing differences in treatment between protected and non-protected groups, but there are multiple ways to measure such differences. The most basic would be \u2018disparate impact\u2019 or \u2018statistical / demographic parity\u2019, which consider the overall percentage of positive/ negative classification rates between groups. However, this is blunt, since it fails to account for discrimination which is explainable in terms of legitimate grounds Dwork et al. (2012). For instance, attempting to enforce equal impact between men and women in recidivism prediction systems, if men have higher re-offending rates, could result in women remaining in prison longer despite being less likely to re-offend.\nA range of more nuanced measures have been proposed, including; \u2018accuracy equity\u2019, which considers the overall accuracy of a predictive model for each group Angwin et al. (2016); \u2018conditional accuracy equity\u2019, which considers the accuracy of a predictive model for each group, con-\nc\u00a9 2018 R. Binns.\nditional on their predicted class Dieterich et al. (2016); \u2018equality of opportunity\u2019, which considers whether each group is equally likely to be predicted a desirable outcome given the actual base rates for that group Hardt et al. (2016); and \u2018disparate mistreatment\u2019, a corollary which considers differences in false positive rates between groups Zafar et al. (2017). Another definition involves imagining counterfactual scenarios wherein members of protected groups are instead members of the non-protected group Kusner et al. (2017). To the extent that outcomes would differ, the system is unfair; i.e. a woman classified by a fair system should get the same classification she would have done in a counterfactual scenario in which she had been born a man.\nIdeally, a system might be expected to meet all of these intuitively plausible measures of fairness. But, somewhat problematically, certain measures turn out to be mathematically impossible to satisfy simultaneously except in rare and contrived circumstances Kleinberg et al. (2016), and therefore hard choices between fairness metrics must be made before the technical work of detecting and mitigating unfairness can proceed. A further underlying tension is that fairness may also imply that similar people should be treated similarly, but this is often in tension with the ideal of parity between groups, when base rates for the target variable are different between those groups Dwork et al. (2012). Fair machine learning therefore faces an upfront set of conceptual ethical challenges; which measures of fairness are most appropriate in a given context? Which variables are legitimate grounds for differential treatment, and why? Are all instances of disparity between groups objectionable? Should fairness consist of ensuring everyone has an equal probability of obtaining some benefit, or should we aim instead to minimise the harms to the least advantaged? In making such tradeoffs, should the decision-maker consider only the harms and benefits imposed within the decision-making context, or also those faced by decision-subjects in other contexts? What relevance might past, future or inter-generational injustices have?\nSuch questions of discrimination and fairness have long been of significant interest to moral and political philosophers, who have expended significant efforts in formalising, differentiating and debating many of the central concepts in-\nvolved. It is therefore unsurprising that attempts to formalise fairness in machine learning contain echoes of these old philosophical debates. Indeed, some seminal work in the FAT-ML community explicitly refers to political philosophy as inspiration, albeit in a limited and somewhat ad-hoc way.1 A more comprehensive overview could provide a wealth of argumentation which may usefully apply to the questions arising in the pursuit of more ethical algorithmic decision-making. This article aims to provide an overview of some of the relevant philosophical literature on discrimination, fairness and egalitarianism in order to clarify and situate the emerging debate within the discrimination-aware and fair machine learning literature. Throughout, I aim to address the conceptual distinctions drawn between terms frequently used in the fair ML literature\u2013including \u2018discrimination\u2019 and \u2018fairness\u2019\u2013and the use of related terms in the philosophical literature. The purpose of this is not merely to consider similarities and differences between the two discourses, but to map the terrain of the philosophical debate and locate those points which provide helpful clarification for future research on algorithmic fairness, or otherwise raise relevant problems which have yet to be considered in that research.\nI begin by discussing philosophical accounts of what discrimination is and what makes it wrong, if and when it is wrong. I show how on certain accounts of what makes discrimination wrong, the proposed conditions are unlikely to obtain in algorithmic decision-making contexts. If correct, these accounts do not necessarily imply that algorithmic decision-making is always morally benign\u2013only that its potential wrongness is not to be found in the notion of discrimination as it is traditionally understood. This leads us to consider other grounds on which algorithmic decision-making might be problematic, which are primarily captured by a variety of considerations connected to the ideals of egalitarianism\u2013the notion that human beings are in some fundamental sense equal and that efforts should be made to avoid and correct certain forms of inequality. This discussion suggests that \u2018fairness\u2019 as used in the fair machine learning community is best\n1. Notable examples include references to the work of authors such as H. Peyton Young, John Rawls, and John Roemer in Dwork et al. (2012); Joseph et al. (2016).\nunderstood as a placeholder term for a variety of normative egalitarian considerations. Notably, while egalitarianism is a widely held principle, exactly what it requires is the subject of much debate. I provide an overview of some of this debate and finish with implications for the incorporation of \u2018fairness\u2019 into algorithmic decision-making systems."}, {"heading": "2. What is discrimination, and what makes it wrong?", "text": "Early work which explored how to embed fairness constraints in machine learning used the term \u2018discrimination-aware\u2019 rather than \u2018fair\u2019. While this terminological difference may seem relatively insignificant amongst computer scientists, it points to a potentially important distinction which has been the preoccupation of much philosophical writing on the topic. For philosophers, the division of moral phenomena into conceptually coherent categories is both intrinsically satisfying, and, hopefully, brings helpful clarification to otherwise perplexing issues. Getting a closer conceptual grip over what discrimination consists in, what (if anything) makes it distinctively wrong, and how it relates to other moral concepts such as fairness, should therefore help clarify whether and when algorithmic systems can be wrongfully discriminatory and what ought to be done about them. It may turn out that socalled \u2018algorithmic discrimination\u2019 differs in important ways to classical forms of discrimination, such that different counter-measures are appropriate."}, {"heading": "2.1. Mental state accounts", "text": "Paradigm cases of discrimination include differential treatment on the basis of membership of a salient social group\u2013e.g. gender or \u2018race\u2019\u2013by those with decision-making power to distribute harms or benefits. Examples include employers who prefer to hire a male job applicant over an equally qualified female applicant, or parole officers who impose stricter conditions on parolees of a particular minority in comparison to a privileged majority. Focusing on these paradigm cases, a range of accounts of what makes discrimination wrong place emphasis on the intentions, beliefs and values of the decision-maker. For\nsuch mental state accounts, defended by among others Richard Arneson, Thomas Scanlon, and Annabelle Lever, the existence of systematic animosity or preferences for or against certain salient social groups on the part of the decision-maker is what makes discrimination wrong Arneson (1989); Lever (2016); Scanlon (2009). Such concerns might be couched in terms of the defective moral character of the decision maker\u2013that they show bad intent or animosity, for example-or in terms of the harmful effect of such intentions on the discriminated-against individual, such as humiliation as a result of lack of respect.\nOn such accounts, the decision-maker\u2019s intent is key to discrimination. A decision-maker with no such intent, who nonetheless accidentally and unwittingly created such disparities, would arguably not be guilty of wrongful discrimination (even if the situation is morally problematic on other grounds). Such cases\u2013often called indirect or institutional discrimination in the UK, or in the U.S., disparate impact\u2013might still count as discriminatory on a mental state account of discrimination if the failure of decision-makers to anticipate such disparities, or to redress them when they become apparent, is sufficiently similarly morally objectionable to the failure to treat people equally in the first instance. However, if such conditions do not obtain, then indirect discrimination, while it may be wrong, may not usefully be described as an instance of discrimination at all Eidelson (2015).\nThis line of thinking suggests a potential challenge to the notion of algorithmic discrimination. If the possession of certain mental states by decision-makers is a necessary condition of a decision being discriminatory, one might argue that algorithmic decision-making systems can never be discriminatory as such, because such systems are incapable of possessing the relevant mental states. This is not the place for discussion of the possibility of machine consciousness, but assuming that it is not (yet) a reality, it seems that AI and autonomous decision-making systems cannot be the bearers of states such as contempt, animosity, or disrespect that would be required for a decision to qualify as discriminatory on such accounts.\nThat said, proponents of mental state accounts might similarly argue that algorithmic decision-making systems might involve discrim-\nination, without imputing the algorithm with mental states. First, they might argue that human decision-makers, and data scientists responsible for building decision models, might sometimes be condemned if they possess bad intentions which result in them intentionally embedding biases in their system, or if they negligently ignore or overlook unintended disparities resulting from it. Second, as social epistemologists have argued, we can sometimes still morally evaluate decisions which do not stem from one individual but are the result of multiple individual judgements aggregated in variously complex ways Gilbert (2004); Pettit (2007). Drawing from work on judgement aggregation problems in economics and social choice theory, they argue that when suitably arranged in institutional decision-making scenarios, groups of people can be held morally responsible for their collective judgements. Machine learning models trained on data consisting of previous human judgements might therefore be critiqued on similar grounds if those individual judgements were themselves the result of similarly discriminatory intent on the part of those individuals.\nHowever, aside from such special cases, it seems that mental state accounts of discrimination do not naturally transfer to the context of algorithmic decision-making. If these accounts are correct, we might therefore conclude that algorithmic decision-making, while potentially morally problematic, should not be characterised as an example of wrongful discrimination. Alternatively, other accounts of why discrimination is (sometimes) wrongful which are not based on mental states of the decision-maker, might be better able to accommodate discrimination of an algorithmic variety."}, {"heading": "2.2. Failing to treat people as individuals", "text": "One such account, which has received significant attention in recent writing on discrimination, locates the wrongness of discrimination\u2013in both its direct and indirect varieties\u2013in the extent to which it relies on inferences about individuals based on generalisations about groups of which they are a member Lippert-Rasmussen (2014). This objection is frequently raised in response to what is often called statistical discrimination Phelps (1972). Statistical discrimina-\ntion is the use of statistical generalisations about groups to infer attributes or future behaviours of members of those groups. For instance, an employer might read evidence purporting to show that smokers are generally less hard-working than non-smokers, and reject a job application from a smoker and give the job a less qualified nonsmoker who is anticipated to be more productive.\nAs economists have argued on the basis of models, such generalisations about groups can be an efficient means for firms to reduce risk when more direct evidence about an individual is lacking Phelps (1972). Despite the potential efficiency benefits of generalisation, it is widely regarded as wrong in at least some cases. While intuitions about the wrongfulness of statistical discrimination are widely shared, it has proven surprisingly difficult to articulate coherent objections to the practice, particularly when we go beyond simple cases in which the inference is simply unsupported by rigorous statistical analysis, to those where membership of a group really does correlate with an outcome of interest to the decision-maker. Since algorithmic decisionmaking could be regarded as a form of generalisation on steroids, any account of discrimination which is grounded in the wrongness of generalisation would be particularly pertinent to our present concerns.\nOn one popular account, statistical discrimination is wrong, even if the generalisations involved have some truth, because it fails to treat the decision-subject as an individual. Subjecting travellers from Muslim-majority countries to harsher border checks on the basis of generalisations (whether true or false) about the preponderance of terrorism amongst such groups fails to consider each individual traveler on their own merit. Similarly, rejecting a job applicant who smokes because of evidence that smokers are on average (let us assume, for the sake of argument) less productive, unfairly punishes them as a result of the behaviour of other members of that group. Such examples have led some to ground objections to statistical discrimination in its failure to treat people as individuals. If this is correct, it presents a strong challenge to the very existence of algorithmic decision-making systems; since they fail to treat people as individuals by design. Given any two individuals with the same attributes, a deterministic model will give the\nsame output, and would therefore, on this account, be quintessentially discriminatory.\nHowever, others have argued that the failure to treat people as individuals cannot plausibly be the essence of wrongful discrimination Schauer (2009); Dworkin (1981); Lippert-Rasmussen (2014). One concern is that this criterion is too broad because it encompasses generalisation against any kind of group, not just those categories enshrined in human rights law such as gender, \u2018race\u2019, religion, etc. While such categories readily trigger concerns about wrongful discrimination, other categories like \u2018smoker\u2019 do not obviously invoke discrimination concerns. This suggests that it is only generalisations about certain groups that matter vis--vis discrimination. Others object that the very notion of \u2018treating someone as an individual\u2019 is misconceived; they argue that, on closer inspection, even decisions which appear to consider the individual are in fact a disguised form of generalisation Schauer (2009). Suppose that rather than using \u2018smoker / non-smoker\u2019 as a predictor of productivity, the employer requires applicants to undergo some test which more accurately predicts productivity; even in this case, as Schauer argues, the employer must rely upon generalizations from test scores to on-the-job behavior. Test scores might be a more accurate predictor than \u2018smoker / non-smoker\u2019, but they are still fundamentally a form of generalization ( Schauer (2009), p68). Unless the test is perfect, some people who perform badly on it would nevertheless turn out to be relatively productive on-the-job.\nIf this line of critique is correct, then it cannot be the case that treating people differently on the basis of generalisations about a category they fit into is necessarily discriminatory in any wrongful sense. What appear to be criticisms of generalization in general(!), may in fact boil down to criticisms of insufficiently precise means of generalization. If the border security system (or the recruitment process) could identify more accurate predictor variables, which resulted in fewer burdens on innocent tourists (or hardworking smokers), then the charge of discrimination might lose some force. Of course, more accurate predictions are likely to be more costly, and as such tradeoffs must be made between the harms and benefits of generalization; but either way, this view suggests that anti-discrimination\ndoes not necessarily require treating people as individuals. Rather, statistical discrimination may be more or less morally permissible depending on who and how many people are wrongly judged on the basis of membership of whatever statistical groups they may be part of, compared to the costs involved in improving the accuracy of the generalisation. If correct, this should be a welcome conclusion for proponents of algorithmic systems, since they are essentially based on generalisations in some form or other.\nSo far I have presented arguments which suggest that there are difficulties with accounting for algorithmic discrimination in terms of the wrongness of mental states or of generalisations. Some of these difficulties are internal to the philosophical accounts of discrimination, and others stem from the dis-analogy between human and algorithmic decision-makers. If the wrongness of (algorithmic) discrimination does not consist in the morally suspect intentions of decision-makers, or in failing to treat people as individuals, then what might it consist in? A more general set of egalitarian norms might provide a better footing for a theory of algorithmic fairness. 2"}, {"heading": "3. Egalitarianism", "text": "Broadly speaking, egalitarianism is the idea that people should be treated equally, and (sometimes) that certain valuable things should be equally distributed. It might seem entirely obvious that what makes discrimination wrongful is something to do with egalitarianism. However, this connection has, perhaps surprisingly, been resisted by many of the previously mentioned theorists of discrimination, with one even claiming that any \u2018connection between antidiscrimination laws and equality it is at best negligible, and in any event is insufficient to count as a justification\u2019 Holmes (2005). Meanwhile, others argue the opposite: that only a direct appeal to egalitarian norms can satisfactorily account for everything that is wrong about discrimination Segall (2012). For our current purposes,\n2. However, even if philosophical accounts of discrimination do not easily apply to algorithmic decisions, calling an algorithmic system discriminatory, (or specifically sexist, racist, etc.) might still be justified by its rhetorical power, or as useful shorthand in everyday discourse.\nthis debate can be safely sidestepped. This is not because it is uninteresting or unimportant as a philosophical project. Rather, our purpose here is to examine how egalitarian norms might provide an account of why and when algorithmic systems can be considered unfair; whether or not such unfairness should rightfully be considered a form of discrimination, per se, is not our concern.\nThis section therefore provides a brief overview of some major debates within egalitarianism and draws out their significance for fair machine learning."}, {"heading": "3.1. The currency of egalitarianism and spheres of justice", "text": "Invariably in machine learning contexts where fairness issues arise, a system is mapping individuals to different outcome classes which are assumed to have negative and positive effects; such as being approved / denied a financial loan, high or low insurance prices, or a greater or fewer number of years spent under incarceration. We assume that these outcome classes are means or barriers to some fundamentally valuable object or set of objects which ought to be to some extent equally distributed. But what exactly is the \u2018currency\u2019 of egalitarianism that lies behind the valuation of these outcome classes? Egalitarians have articulated various competing views, including welfare, understood in terms of pleasure or preference-satisfaction Cohen (1989); resources such as income and assets Rawls (2009); Dworkin (1981); or capabilities, understood as both the ability and resources necessary to do certain things Sen (1992). Others propose that inequalities in welfare, resources, or capabilities may be acceptable, so long as citizens have equal political and democratic status Anderson (1999).\nThe question of what egalitarianism is concerned with (the \u2018equality of what?\u2019 debate as it is sometimes referred to), is relevant to our assumptions about the impact of different algorithmic decision outcomes. In many cases \u2013 like the automated allocation of loans, or the setting of insurance premiums \u2013 the decision outcomes primarily affect distribution of resources. In others \u2013 like algorithmic blocking or banning of users from an online discussion \u2013 the decisions may be more directly related to distribution of welfare or capabilities. The importance of each cur-\nrency of egalitarianism may plausibly differ between contexts, thus affecting how we account for the potentially differential impacts of algorithmic decision-making systems.\nThis debate also trades heavily on the intuition that different people may value the same outcome or set of harms and benefits differently; yet most existing work on fair machine learning assumes a uniform valuation of decision outcomes across different populations. In some cases it may be safe to assume that different sub-groups are equally likely to regard a particular outcome class \u2013 e.g. being denied a loan, or being hired for a job - as bad or good. But in other cases, especially in personalisation and recommender systems where there are multiple outcome classes with no obvious universally agreed-on rank order, this assumption may be flawed.\nA connected debate concerns whether we should apply a single egalitarian calculus across different social contexts, or whether there are internal \u2018spheres of justice\u2019 in which different incommensurable logics of fairness might apply, and between which redistributions might not be appropriate Walzer (2008). For instance, with regard to civil and democratic rights like voting in elections, the aim of egalitarianism might be absolute equal distribution of the good, rather than merely equality of opportunity to compete for it. This idea would explain the intuition that voter registration tests are wrong, while tests for jobs are not. Requiring some form of test prior to voting may ensure equality of opportunity in the sense that everyone has an equal opportunity to take the test; but since talent and efforts are not equally distributed, some people may fail the test, and there would not be equality of outcome. But an essential element of democracy, one might argue, is that voting rights shouldn\u2019t depend on talent or effort. However, when it comes to competition for social positions and economic goods, we may be concerned with ensuring equality of opportunity but less concerned about equality of outcome. We may consider it fair, other things being equal, that the most qualified applicant obtains the role, and that the most industrious and / or talented individuals deserve more economic benefits than others (even if we believe that current systems do not actually ensure a level playing field, and some level of income redistribution is also morally required).\nDifferent contexts being subject to different spheres of justice would have a direct bearing on the appropriateness of certain fairness-correcting methods in those contexts. Equality of opportunity may be an appropriate metric to apply to models that will be deployed in the sphere of \u2018economic justice\u2019; e.g. selection of candidates for job interviews or calculation of insurance. But in contexts which fall under the sphere of civil justice, we may want to impose more blunt metrics like equality of outcome (or \u2018parity of impact\u2019). This might be the case for airport security checks, where it is important to a sense of social solidarity that no group is over-examined as a result of a predictive system, even if there really are differences in base rates Hellman (2008). We therefore can\u2019t assume that fairness metrics which are appropriate in one context will be appropriate in another."}, {"heading": "3.2. Luck and desert", "text": "A second major strand of debate in egalitarian thinking considers the role of notions like choice Huseby (2016) and desert Temkin (1986) in determining which inequalities are acceptable? In which circumstances, and to what extent, should people be held responsible for the unequal status they find themselves in? So-called \u2018luck egalitarians\u2019 aim to answer this question by proposing that the ideal solution should allow those inequalities resulting from people\u2019s free choices and informed risk-taking, but disregard those which are the result of brute luck Arneson (1989). As free-willed individuals, we are capable of making choices and bearing their consequences, which may sometimes make us better or worse off than others. The choices we make may deserve certain rewards and punishments. However, where inequalities are the result of circumstances outside an individual\u2019s control (e.g. being born with a debilitating health condition, or being born into a culture in which one\u2019s skin colour results in systemically worse treatment), egalitarians argue for their correction. However, defining a principle which can demarcate between those inequalities which are and are not chosen or deserved is a tricky prospect, one which has vexed egalitarians for centuries.\nThe luck egalitarian aim of pursuing redistribution only where inequalities are due to pure\nluck, and leaving in place inequalities which are the result of personal choices and informed gambles, raises interesting questions for which variables should be included in fair ML models. High-profile controversies around the creation of criminal recidivism risk scoring in the US, notably the COMPAS system, have focused primarily on the differential impacts on African American and Caucasian subjects Angwin et al. (2016). But one of the potentially objectionable features of the COMPAS scoring system was not the use of \u2018race\u2019 as a variable (which it did not), but rather its use of variables which are not the result of individuals\u2019 choices, such as being part of a family, social circle, or neighbourhood with higher rates of crime. These may be objectionable in part because they correlate with \u2018race\u2019 in the U.S., but they are also objectionable more generally to the extent that they are not the result of personal choices. As such, any inequalities that arise from them should not, on the luck egalitarian view, be tolerated.\nFurthermore, as critics of luck egalitarianism have argued, sometimes even inequalities which are the result of choice still ought to be compensated. For instance, as Elizabeth Anderson has argued, standard luck egalitarianism leads to the vulnerability of dependent caretakers, because it would not compensate those who are responsible for choosing to care for dependents rather than working a wage-earning job full time Anderson (1999). This is what Thayson and Albertson call a \u2018costly rescue\u2019 Thaysen and Albertsen (2017); on their view, luck egalitarianism should only be sensitive to responsibility for creating advantages and disadvantages \u2013 not to responsibility for distributing them. Thus, individuals who voluntarily place themselves in unequal positions might sometimes deserve compensation if their choice served some important social purpose. To return to the COMPAS case, even if certain variables like one\u2019s social circle or neighbourhood were the result of individual choice (which is perhaps more likely to be the case for the economically advantaged), such choices may nevertheless deserve protection from negative consequences. This might apply in cases like those outlined by Anderson, Thayson and Albertson above; for instance, one might choose to remain a resident in a high crime neighbourhood in order to make a positive difference to the community."}, {"heading": "3.3. Deontic justice", "text": "In applying egalitarian political philosophy to the analysis of particular instances of inequality, these abstract principles need to be supplemented with empirical claims about how and why certain circumstances obtain. This reflects the sense in which egalitarianism can be, in Derek Parfit\u2019s terms, deontic; that is, not concerned with an unequal state of affairs per se, but rather with the way in which that state of affairs was produced Parfit (1997). Where analytic philosophy ends, sociology, history, economics, and other nascent disciplines are needed, to understand the specific ways in which some groups come to be unfairly disadvantaged Fang and Moro (2010); Hooks (1992). Only then can we meaningfully evaluate whether and to what extent a given disparity is unfair. But consideration of historical and sociological contexts can also inform philosophical accounts and raise new questions. One example is in the attribution of responsibility; who should be held responsible for the initial creation of inequalities, and who should be held responsible for correcting them? Can historical injustices perpetrated by an institution in the past ground present-day claims for redistribution? Is a particular instance of unequal treatment of a particular group worse if it takes place in a wider context of inequality for that group, compared to a general pattern of benign or advantageous treatment? Abstracting away from particular cases and considering broader historical and social trends may better enable us to account for what makes certain forms of unequal treatment particularly concerning. In discussing what makes racial profiling worse than other forms of profiling, even if it is based on statistical facts, Kasper LippertRasmussen argues ( Lippert-Rasmussen (2014), p. 300):\n\u2018Statistical facts are often facts about how we choose to act. Since we can morally evaluate how we choose to act, we cannot simply take statistical facts for granted when justifying policies: we need to ask the prior question of whether it can be justified that we make these statistical facts obtain\u2019\nOn this view, the particular wrongness of racial profiling can only be understood by appealing\nto the social processes which cause the statistical regularities to obtain in the first place. In Lippert-Rasmussen\u2019s example, the reason racial profiling for crime in the U.S. \u2018works\u2019 (if it does at all), may be due to things that the white majority do or fail to do which might reduce or eliminate the reliability of such statistical inferences.\nSimilarly, such \u2018deontic\u2019, historical and sociological considerations can provide critical background information which is likely to be crucially relevant in determinations of fairness in particular algorithmic decision-making contexts. Historical causes of inequalities and broader existing social structures cannot be ignored when deploying models in such contexts. At a basic level, this means that feature selection\u2013both for protected characteristics and other variables\u2013 should be informed by such information, but it also might determine which disparities take priority in fairness analysis and mitigation if more than one exists. More broadly, deontic considerations may help situate and illuminate the moral tensions that arise between different and incompatible fairness metrics. Returning to the debate about the COMPAS criminal recidivism scoring system, where unequal base rates mean that accuracy equity is mathematically impossible to achieve alongside equalised false positive rates, a deontic egalitarian perspective suggests focusing attention on the historic reasons for such unequal base rates. While this may not in itself directly resolve the question of which fairness metric ought to apply, it does suggest that part of the answer should involve consideration of the historic and contemporary factors responsible for the broader social situation from which the incompatibility dilemma arises."}, {"heading": "3.4. Distributive versus representative harms", "text": "Finally, it is important to note that some aspects of egalitarian fairness may not be distributive in a direct way, in the sense that they concern the distribution of benefits and harms to specific people from specific decisions. Rather, they may be about the fair representation of different identities, cultures, ethnicities, languages, or other social categories. For instance, states with multiple official languages may have a duty to ensure equal representation of each language, a duty\nwhich need not derive from any specific claims about the unequal benefits and harms to individual members of each linguistic group Taylor (1994). Similar arguments might be made about cultural representation in official documents, or even within the editorial policies of institutions in receipt of public money. Even private institutions might well voluntarily impose such duties upon themselves. There is a debate about the extent to which equal cultural recognition and distributive egalitarianism are truly distinct notions; some argue that \u2018recognition and distribution are two irreducible elements of a satisfactory theory of justice\u2019, while others that \u2018any dispute regarding redistribution of wealth or resources is reducible to a claim over the social valorisation of specific group or individual traits\u2019 Fraser and Honneth (2003). Such notions of representational fairness capture many of the most high-profile controversial examples of algorithmic bias. For instance, much-reported work on gender and other biases in the language corpora used to produce word embeddings is an example of representational fairness Bolukbasi et al. (2016); Caliskan-Islam et al. (2016). In such cases, the problem is not necessarily one of specific harms to specific members of a social group, but rather one of the way in which certain groups are represented in digital cultural artefacts, such as natural language classifiers or search engine results. This may require different ways of approaching fairness and bias since the notions of differential group impacts and treating like people alike do not apply; instead, the goal might be to ensure equal representation of groups in a ranking Zehlike et al. (2017), or to give due weight to different normative / ideological outlooks in a classifier which automates the enforcement of norms Binns et al. (2017)."}, {"heading": "4. Conclusion", "text": "Current approaches to fair machine learning are typically focused on interventions at the data preparation, model-learning or post-processing stages. This is understandable given the typical remit of data scientists who are intended to carry out these processes. However, there is a danger that this results in an approach which focuses on a narrow, static set of prescribed pro-\ntected classes, derived from law and devoid of context, without considering why those classes are protected and how they relate to the particular justice aspects of the application in question. Philosophical accounts of discrimination and fairness prompt reflection on these more fundamental questions, and suggest avenues for further consideration of what might be relevant and why.\nThis raises a series of practical challenges which may limit how effective and systematic fair ML approaches can be in practice. Attempting to translate and elucidate the differences between such egalitarian theories in the context of particular machine learning tasks will likely be tricky. In simple cases, it may be that feature vectors used to train models include personal characteristics which can intuitively be classed as either chosen or unchosen (and therefore legitimate or illegitimate grounds for differential treatment according to e.g. luck egalitarianism). But more often, a contextually appropriate approach to fairness which truly captures the essence of the relevant philosophical points may hinge on factors which are not typically present in the data available in situ. Such missing data may include the protected characteristics of affected individuals Veale and Binns (2017), but also information relevant to an assessment of an individual\u2019s responsibility, culpability or desert\u2013such as their socio-economic circumstances, life experience, personal development, and the relationships between them. Attempts to draw such conclusions from training data and lists of legally protected categories alone, are unlikely to do justice to the way that questions of justice arise in idiosyncratic lives and differing social contexts."}, {"heading": "Acknowledgments", "text": "The author was supported by funding from the UK Engineering and Physical Sciences Research Council (EPSRC) under SOCIAM: The Theory and Practice of Social Machines, grant number EP/J017728/2, and PETRAS: Cyber Security of the Internet of Things, under grant number EP/N02334X/1."}], "title": "Fairness in Machine Learning: Lessons from Political Philosophy", "year": 2018}