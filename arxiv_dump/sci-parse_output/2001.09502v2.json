{"abstractText": "In the context of some machine learning applications, obtaining data instances is a relatively easy process but labeling them could become quite expensive or tedious. Such scenarios lead to datasets with few labeled instances and a larger number of unlabeled ones. Semi-supervised classification techniques combine labeled and unlabeled data during the learning phase in order to increase classifier\u2019s generalization capability. Regrettably, most successful semi-supervised classifiers do not allow explaining their outcome, thus behaving like black boxes. However, there is an increasing number of problem domains in which experts demand a clear understanding of the decision process. In this paper, we report on an extended experimental study presenting an interpretable self-labeling grey-box classifier that uses a black box to estimate the missing class labels and a white box to explain the final predictions. Two different approaches for amending the self-labeling process are explored: a first one based on the confidence of the black box and the latter one based on measures from Rough Set Theory. The results of the extended experimental study support the interpretability by means of transparency and simplicity of our classifier, while attaining superior prediction rates when compared with state-of-the-art self-labeling classifiers reported in the literature.", "authors": [{"affiliations": [], "name": "A PREPRINT"}, {"affiliations": [], "name": "Isel Grau"}, {"affiliations": [], "name": "Dipankar Sengupta"}], "id": "SP:9e787b4c1aa2c2d67962232ead8d3bd7610295a2", "references": [{"authors": ["Chen Gong", "Dacheng Tao", "Stephen J Maybank", "Wei Liu", "Guoliang Kang", "Jie Yang"], "title": "Multi-modal curriculum learning for semi-supervised image classification", "venue": "IEEE Transactions on Image Processing,", "year": 2016}, {"authors": ["Lili Yin", "Huangang Wang", "Wenhui Fan", "Li Kou", "Tingyu Lin", "Yingying Xiao"], "title": "Incorporate active learning to semi-supervised industrial fault classification", "venue": "Journal of Process Control,", "year": 2019}, {"authors": ["Nikos Fazakis", "Stamatis Karlos", "Sotiris Kotsiantis", "Kyriakos Sgarbas"], "title": "Speaker identification using semisupervised learning", "venue": "In Proceeding of the 2015 International Conference on Speech and Computer,", "year": 2015}, {"authors": ["Yutong Xie", "Jianpeng Zhang", "Yong Xia"], "title": "Semi-supervised adversarial model for benign\u2013malignant lung nodule classification on chest CT", "venue": "Medical Image Analysis,", "year": 2019}, {"authors": ["Kristin P. Bennett", "Ayhan Demiriz"], "title": "Semi-supervised support vector machines", "venue": "In Advances in Neural Information Processing Systems", "year": 1999}, {"authors": ["Avrim Blum", "Shuchi Chawla"], "title": "Learning from labeled and unlabeled data using graph mincuts", "venue": "In Proceedings of the Eighteenth International Conference on Machine Learning,", "year": 2001}, {"authors": ["Akinori Fujino", "Naonori Ueda", "Kazumi Saito"], "title": "Semisupervised learning for a hybrid generative/discriminative classifier based on the maximum entropy principle", "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "year": 2008}, {"authors": ["Isaac Triguero", "Salvador Garc\u00eda", "Francisco Herrera"], "title": "Self-labeled techniques for semi-supervised learning: taxonomy, software and empirical study", "venue": "Knowledge and Information Systems,", "year": 2015}, {"authors": ["Tim Salimans", "Ian Goodfellow", "Wojciech Zaremba", "Vicki Cheung", "Alec Radford", "Xi Chen"], "title": "Improved techniques for training gans", "venue": "In Advances in Neural Information Processing Systems", "year": 2016}, {"authors": ["Mohamed Farouk Abdel Hady", "Friedhelm Schwenker"], "title": "Co-training by committee: a new semi-supervised learning framework", "venue": "In Proceedings of the 2008 IEEE International Conference on Data Mining Workshops,", "year": 2008}, {"authors": ["David Yarowsky"], "title": "Unsupervised word sense disambiguation rivaling supervised methods", "venue": "In Proceedings of the 33rd Annual Meeting on Association for Computational Linguistics,", "year": 1995}, {"authors": ["Bryce Goodman", "Seth Flaxman"], "title": "European union regulations on algorithmic decision-making and a \u201cright to explanation", "venue": "AI Magazine,", "year": 2017}, {"authors": ["Finale Doshi-Velez", "Been Kim"], "title": "Considerations for Evaluation and Generalization in Interpretable Machine Learning, pages 3\u201317", "year": 2018}, {"authors": ["Leilani H Gilpin", "David Bau", "Ben Z Yuan", "Ayesha Bajwa", "Michael Specter", "Lalana Kagal"], "title": "Explaining explanations: An overview of interpretability of machine learning", "venue": "In Proceedings of the IEEE 5th International Conference on Data Science and Advanced Analytics,", "year": 2018}, {"authors": ["Zachary C. Lipton"], "title": "The mythos of model interpretability", "venue": "In Proceedings of the 33rd International Conference on Machine Learning. Workshop on Human Interpretability in Machine Learning,", "year": 2016}, {"authors": ["Alejandro Barredo Arrieta", "Natalia D\u00edaz-Rodr\u00edguez", "Javier Del Ser", "Adrien Bennetot", "Siham Tabik", "Alberto Barbado", "Salvador Garcia", "Sergio Gil-Lopez", "Daniel Molina", "Richard Benjamins", "Raja Chatila", "Francisco Herrera"], "title": "Explainable artificial intelligence (xai): Concepts, taxonomies, opportunities and challenges toward responsible ai", "venue": "Information Fusion,", "year": 2020}, {"authors": ["Oliver Nelles"], "title": "Nonlinear system identification: from classical approaches to neural networks and fuzzy models", "year": 2013}, {"authors": ["Marco Tulio Ribeiro", "Sameer Singh", "Carlos Guestrin"], "title": "Why should i trust you?: Explaining the predictions of any classifier", "venue": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "year": 2016}, {"authors": ["Isel Grau", "Dipankar Sengupta", "Maria M. Garcia Lorenzo", "Ann Nowe"], "title": "Grey-box model: An ensemble approach for addressing semi-supervised classification problems", "venue": "In Belgian-Dutch Conference on Machine Learning BENELEARN 2016,", "year": 2016}, {"authors": ["Isel Grau", "Dipankar Sengupta", "Maria M. Garcia Lorenzo", "Ann Nowe"], "title": "Interpretable self-labeling semisupervised classifier", "venue": "Proceedings of the 2nd Workshop on Explainable Artificial Intelligence, International Joint Conference on Artificial Intelligence IJCAI/ECAI", "year": 2018}, {"authors": ["Xiaojin Zhu"], "title": "Semi-supervised learning literature survey", "venue": "Technical Report 1530,", "year": 2005}, {"authors": ["Hakan Cevikalp", "Vojtech Franc"], "title": "Large-scale robust transductive support vector machines", "year": 2017}, {"authors": ["Yanchao Li", "Yongli Wang", "Cheng Bi", "Xiaohui Jiang"], "title": "Revisiting transductive support vector machines with margin distribution embedding", "venue": "Knowledge-Based Systems,", "year": 2018}, {"authors": ["Olivier Chapelle", "Bernhard Schlkopf", "Alexander Zien"], "title": "Semi-Supervised Learning", "year": 2010}, {"authors": ["C. Gong", "D. Tao", "W. Liu", "L. Liu", "J. Yang"], "title": "Label propagation via teaching-to-learn and learning-to-teach", "venue": "IEEE Transactions on Neural Networks and Learning Systems,", "year": 2017}, {"authors": ["M. Fan", "X. Zhang", "L. Du", "L. Chen", "D. Tao"], "title": "Semi-supervised learning through label propagation on geodesics", "venue": "IEEE Transactions on Cybernetics,", "year": 2018}, {"authors": ["Raif M Rustamov", "James T Klosowski"], "title": "Interpretable graph-based semi-supervised learning via flows", "venue": "In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence,", "year": 2018}, {"authors": ["J. Chen", "Y. Chang", "B. Hobbs", "P. Castaldi", "M. Cho", "E. Silverman", "J. Dy"], "title": "Interpretable clustering via discriminative rectangle mixture model", "venue": "In Proceedings of the IEEE 16th International Conference on Data Mining (ICDM),", "year": 2016}, {"authors": ["Jason Weston", "Fr\u00e9d\u00e9ric Ratle", "Hossein Mobahi", "Ronan Collobert"], "title": "Deep learning via semi-supervised embedding", "venue": "In Neural Networks: Tricks of the Trade,", "year": 2012}, {"authors": ["Diederik P Kingma", "Shakir Mohamed", "Danilo Jimenez Rezende", "Max Welling"], "title": "Semi-supervised learning with deep generative models", "venue": "In Advances in Neural Information Processing Systems", "year": 2014}, {"authors": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "title": "Generative adversarial nets", "venue": "In Advances in Neural Information Processing Systems", "year": 2014}, {"authors": ["Augustus Odena"], "title": "Semi-supervised learning with generative adversarial networks", "venue": "arXiv preprint arXiv:1606.01583,", "year": 2016}, {"authors": ["Zihang Dai", "Zhilin Yang", "Fan Yang", "William W Cohen", "Ruslan R Salakhutdinov"], "title": "Good semi-supervised learning that requires a bad GAN", "venue": "Advances in Neural Information Processing Systems", "year": 2017}, {"authors": ["Supriyo Chakraborty", "Richard Tomsett", "Ramya Raghavendra", "Daniel Harborne", "Moustafa Alzantot", "Federico Cerutti", "Mani Srivastava", "Alun Preece", "Simon Julier", "Raghuveer M Rao"], "title": "Interpretability of deep learning models: a survey of results", "venue": "In Proceedings of the IEEE Smart World Congress", "year": 2017}, {"authors": ["Jesper E Van Engelen", "Holger H Hoos"], "title": "A survey on semi-supervised learning", "venue": "Machine Learning,", "year": 2020}, {"authors": ["Siddharth N", "Brooks Paige", "Jan-Willem van de Meent", "Alban Desmaison", "Noah Goodman", "Pushmeet Kohli", "Frank Wood", "Philip Torr"], "title": "Learning disentangled representations with semi-supervised deep generative models", "venue": "Advances in Neural Information Processing Systems", "year": 2017}, {"authors": ["Anindya Halder", "Susmita Ghosh", "Ashish Ghosh"], "title": "Ant based semi-supervised classification", "venue": "Proceedings of the 7th International Conference on Swarm Intelligence,", "year": 2010}, {"authors": ["Ming Li", "Zhi-Hua Zhou"], "title": "Setred: Self-training with editing", "venue": "In Proceedings of the 9th Pacific-Asia Conference on Knowledge Discovery and Data Mining,", "year": 2005}, {"authors": ["Ian H. Witten", "Eibe Frank", "Mark A. Hall", "Christopher J. Pal"], "title": "Chapter 11 - Beyond supervised and unsupervised learning, pages 467\u2013478", "year": 2017}, {"authors": ["Avrim Blum", "Tom Mitchell"], "title": "Combining labeled and unlabeled data with co-training", "venue": "In Proceedings of the Eleventh Annual Conference on Computational Learning Theory,", "year": 1998}, {"authors": ["Yan Zhou", "Sally Goldman"], "title": "Democratic co-learning", "venue": "In Proceedings of the 16th IEEE International Conference on Tools with Artificial Intelligence,", "year": 2004}, {"authors": ["Zhi-Hua Zhou", "Ming Li"], "title": "Tri-training: Exploiting unlabeled data using three classifiers", "venue": "IEEE Transactions on Knowledge and Data Engineering,", "year": 2005}, {"authors": ["Ming Li", "Zhi-Hua Zhou"], "title": "Improve computer-aided diagnosis with machine learning techniques using undiagnosed samples", "venue": "IEEE Transactions on Systems, Man, and Cybernetics Part A: Systems and Humans,", "year": 2007}, {"authors": ["Nikos Fazakis", "Stamatis Karlos", "Sotiris Kotsiantis", "Kyriakos Sgarbas"], "title": "Self-trained LMT for semisupervised learning", "venue": "Computational Intelligence and Neuroscience,", "year": 2016}, {"authors": ["Julio Albinati", "Samuel E.L. Oliveira", "Fernando E.B. Otero", "Gisele L. Pappa"], "title": "An ant colony-based semisupervised approach for learning classification rules", "venue": "Swarm Intelligence,", "year": 2015}, {"authors": ["Stamatis Karlos", "Nikos Fazakis", "Angeliki-Panagiota Panagopoulou", "Sotiris Kotsiantis", "Kyriakos Sgarbas"], "title": "Locally application of naive bayes for self-training", "venue": "Evolving Systems,", "year": 2017}, {"authors": ["Sarah Vluymans", "Neil Mac Parthal\u00e1in", "Chris Cornelis", "Yvan Saeys"], "title": "Fuzzy rough sets for self-labelling: An exploratory analysis", "venue": "In Proceedings of the 2016 IEEE International Conference on Fuzzy Systems,", "year": 2016}, {"authors": ["D. Wu", "X. Luo", "G. Wang", "M. Shang", "Y. Yuan", "H. Yan"], "title": "A highly accurate framework for self-labeled semisupervised classification in industrial applications", "venue": "IEEE Transactions on Industrial Informatics,", "year": 2018}, {"authors": ["Emmanuel Pintelas", "Ioannis E Livieris", "Panagiotis Pintelas"], "title": "A grey-box ensemble model exploiting black-box accuracy and white-box intrinsic interpretability", "year": 2020}, {"authors": ["Cosmin Lazar", "Stijn Meganck", "Jonatan Taminau", "David Steenhoff", "Alain Coletta", "Colin Molter", "David Y Weiss- Sol\u00eds", "Robin Duque", "Hugues Bersini", "Ann Now\u00e9"], "title": "Batch effect removal methods for microarray gene expression data integration: a survey", "venue": "Briefings in Bioinformatics,", "year": 2012}, {"authors": ["Alexandru Niculescu-Mizil", "Rich Caruana"], "title": "Predicting good probabilities with supervised learning", "venue": "In Proceedings of the 22nd International Conference on Machine Learning,", "year": 2005}, {"authors": ["John Platt"], "title": "Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods", "venue": "Advances in Large Margin Classifiers,", "year": 1999}, {"authors": ["Bianca Zadrozny", "Charles Elkan"], "title": "Obtaining calibrated probability estimates from decision trees and naive bayesian classifiers", "venue": "In Proceedings of the Eighteenth International Conference on Machine Learning,", "year": 2001}, {"authors": ["D Randall Wilson", "Tony R Martinez"], "title": "Improved heterogeneous distance functions", "venue": "Journal of Artificial Intelligence Research,", "year": 1997}, {"authors": ["Rafael Bello", "Jos\u00e9 Luis Verdegay"], "title": "Rough sets in the Soft Computing environment", "venue": "Information Sciences,", "year": 2012}, {"authors": ["Chongsheng Zhang", "Changchang Liu", "Xiangliang Zhang", "George Almpanidis"], "title": "An up-to-date comparison of state-of-the-art classification algorithms", "venue": "Expert Systems with Applications,", "year": 2017}, {"authors": ["Manuel Fern\u00e1ndez-Delgado", "Eva Cernadas", "Sen\u00e9n Barro", "Dinani Amorim"], "title": "Do we need hundreds of classifiers to solve real world classification problems", "venue": "Journal of Machine Learning Research,", "year": 2014}, {"authors": ["Michael Wainberg", "Babak Alipanahi", "Brendan J Frey"], "title": "Are random forests truly the best classifiers", "venue": "The Journal of Machine Learning Research,", "year": 2016}, {"authors": ["Robert Hecht-Nielsen"], "title": "Theory of the backpropagation neural network", "venue": "In Proceedings of the International Joint Conference on Neural Networks,", "year": 1989}, {"authors": ["J. Platt"], "title": "Fast training of support vector machines using sequential minimal optimization", "venue": "Advances in Kernel Methods - Support Vector Learning", "year": 1998}, {"authors": ["S. Sathiya Keerthi", "Shirish Krishnaj Shevade", "Chiranjib Bhattacharyya", "Karuturi Radha Krishna Murthy"], "title": "Improvements to Platt\u2019s SMO algorithm for SVM classifier design", "venue": "Neural Computation,", "year": 2001}, {"authors": ["J Ross Quinlan"], "title": "Programs for machine learning", "year": 1993}, {"authors": ["Eibe Frank", "Ian H. Witten"], "title": "Generating accurate rule sets without global optimization", "venue": "In Proceedings of the Fifteenth International Conference on Machine Learning,", "year": 1998}, {"authors": ["William W. Cohen"], "title": "Fast effective rule induction", "venue": "Proceedings of the Twelfth International Conference on Machine Learning,", "year": 1995}, {"authors": ["Jacob Cohen"], "title": "A coefficient of agreement for nominal scales", "venue": "Educational and Psychological Measurement,", "year": 1960}, {"authors": ["Nathalie Japkowicz", "Mohak Shah"], "title": "Evaluating learning algorithms: a classification perspective", "year": 2011}, {"authors": ["Arie Ben-David"], "title": "Comparison of classification accuracy using cohen\u2019s weighted kappa", "venue": "Expert Systems with Applications,", "year": 2008}, {"authors": ["Milton Friedman"], "title": "The use of ranks to avoid the assumption of normality implicit in the analysis of variance", "venue": "Journal of the American Statistical Association,", "year": 1937}, {"authors": ["F. Wilcoxon"], "title": "Individual comparisons by ranking", "venue": "methods. Biometrics,", "year": 1945}, {"authors": ["Sture Holm"], "title": "A simple sequentially rejective multiple test procedure", "venue": "Scandinavian Journal of Statistics,", "year": 1979}, {"authors": ["Alessio Benavoli", "Giorgio Corani", "Francesca Mangili"], "title": "Should we really use post-hoc tests based on meanranks", "venue": "Journal of Machine Learning Research,", "year": 2016}, {"authors": ["Wouter G Touw", "Jumamurat R Bayjanov", "Lex Overmars", "Lennart Backus", "Jos Boekhorst", "Michiel Wels", "Sacha AFT van Hijum"], "title": "Data mining in the life sciences with random forest: a walk in the park or lost in the jungle", "venue": "Briefings in Bioinformatics,", "year": 2012}, {"authors": ["Colin PD Birch"], "title": "A new generalized logistic sigmoid growth equation compared with the richards growth equation", "venue": "Annals of Botany,", "year": 1999}, {"authors": ["Roxana R\u0103dulescu", "Patrick Mannion", "Yijie Zhang", "Diederik M. Roijers", "Ann Now\u00e9"], "title": "A utility-based analysis of equilibria in multi-objective normal-form games", "venue": "The Knowledge Engineering Review,", "year": 2020}, {"authors": ["Luisa M Zintgraf", "Diederik M Roijers", "Catholijn M Jonker", "Ann Now\u00e9"], "title": "Ordered preference elicitation strategies for supporting multi-objective decision making", "venue": "In Proceedings of the 17th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2018),", "year": 2018}, {"authors": ["Diederik M Roijers", "Peter Vamplew", "Shimon Whiteson", "A Whitesonuva Nl", "Richard Dazeley"], "title": "A survey of multi-objective sequential decision-making", "venue": "Journal of Artificial Intelligence Research,", "year": 2013}], "sections": [{"text": "Keywords Semi-supervised Classification \u00b7 Self-labeling \u00b7 Interpretability \u00b7 Explainable Artificial Intelligence \u00b7 Grey-Box Model \u00b7 Rough Sets Theory"}, {"heading": "1 Introduction", "text": "Gathering data examples for training a machine learning classifier in a real-world scenario is often simple, but the process of assigning labels to the examples can be costly in terms of money, time or effort. In such scenarios we might obtain datasets with more unlabeled than labeled data. This is often the case in applications such as image classification [1], industrial fault classification [2], sentiment analysis [3], speaker identification [4] and bioinformatics or medical applications [5]. Semi-supervised classification (SSC) techniques arise from the need to address this problem using both labeled and unlabeled data for training a classifier. The aim is to increase the generalization ability of the classifier when compared to a supervised classifier that only uses the available labeled data.\n\u2217This work was supported by the IMAGica project, financed by the Interdisciplinary Research Programs and Platforms (IRP) funds of the Vrije Universiteit Brussel; and the BRIGHTanalysis project, funded by the European Regional Development Fund (ERDF) and the Brussels-Capital Region as part of the 2014-2020 operational program through the F11-08 project ICITY-RDI.BRU (icity.brussels).\nar X\niv :2\n00 1.\n09 50\n2v 2\n[ cs\n.L G\nThe SSC literature reports several techniques including transductive Support Vector Machines [6], Graph-based methods [7], Generative Mixture Models [8], Self-labeling techniques [9] and more recently semi-supervised Generative Adversarial Networks [10]. In general, state-of-the-art SSC methods involve three main shortcomings that may vary from a specific family of algorithms to the whole field. The first potential issue affecting all SSC models refers to the assumption that the unlabeled data helps elucidating the distribution of the labeled instances. When this assumption is not met in any of its forms, semi-supervised learning may not be useful. Secondly, some techniques such as Graph-based methods mainly focus on transductive learning, i.e. predicting the label for a given set of unlabeled data rather than finding a model capable of predicting the classification of unseen instances with a proper generalization. Thirdly, while self-labeling approaches such as Co-training [11], Self-training [12] and their variants perform quite well in terms of accuracy, they often result in complex structures combining several classifiers and failing to give the user insight in how the classification process comes about.\nAn increasing requirement observed in machine learning is to obtain not only precise models but also interpretable ones. End users often demand an insight into how an algorithm arrives at a particular outcome and need an explanation of the decisions to some extent. In general, explainable artificial intelligence is starting to be a central concern in both governing and research communities. For example, the EU General Data Protection Regulation includes a right to obtain an explanation on the decisions made by an algorithm affecting human beings [13]. This regulation might limit the potential of using artificial intelligence in a variety of domains, unless we start developing more transparent models.\nRecent studies [14, 15, 16, 17] formalize terms such as interpretability or explainability in sometimes overlapping concepts. However, a common conclusion is that a certain grade of global interpretability can be reached through the use of more transparent techniques as proxies for solving a task. In this paper, we refer to intrinsically interpretable models (e.g., linear regression, decision trees or rule induction algorithms) as white boxes, as opposed to the less interpretable black-box ones (e.g. artificial neural networks or support vector machines). Black boxes are normally more accurate techniques that learn exclusively from data but they are not easily understandable at a global level. Whereas white boxes refer to models which are constructed based on laws or principles of the problem domain, or those who are built from data but their structure allows for explanations or interpretation, since pure white boxes rarely exists [18]. Intrinsically interpretable models can be recommended when a transparent model that can be inspected as a whole is needed and the prediction problem does not require a very powerful technique. On the other hand, agnostic post-hoc methods [19, 20] are a suitable alternative when a black-box is already built and we need to compute explanations for input and output pairs, preserving accuracy. However, post-hoc methods generate explanations that are often local or limited to feature attribution rather than a holistic view of the model. Grey-box models, i.e. using white boxes as surrogates for distilling previously trained black boxes are an approach in between intrinsically interpretability and model agnostic post-hoc. While the white boxes attempt to explain the problem domain directly, the grey-boxes are devoted to explain the domain by approximating the predictions produced by a black-box classifier.\nIn this paper, we study the SSC problem from the interpretability angle. We conduct a detailed revision of methods reported in the literature and discuss their shortcomings when interpretability comes to play. We explore the performance of our semi-supervised classifier termed self-labeling grey-box (SlGb) [21, 22], which exploits the strength of black-box models being good classifiers with the interpretability of white boxes. In terms of interpretability, we refer to a grey-box model as the combination of a black-box model with a white-box one. Our classifier uses a black box to estimate the decision class for unlabeled instances in order to increase the amount of training data. Afterwards, our approach builds a surrogate white-box classifier from the enlarged dataset that allows explaining the predictions. In addition we explore the effects of using two weighting strategies to reduce the effect of misclassifications when building the enlarged dataset. The former is based on the black box\u2019s confidence for the inferred class label, while the latter is based on granular computing principles. The use of an enlarged dataset combined with a weighting strategy results in a white box with improved prediction rates. Numerical experiments using 55 datasets in different settings show that our proposal attains a good balance between prediction rates and explainability, while outperforming most state-of-the-art methods.\nThe rest of this paper is structured as follows. Section 2 provides an overview of state-of-the-art SSC algorithms reported in the literature and their interpretability, while making emphasis on self-labeling techniques. Section 3 describes the SlGb approach and Section 4 depicts two alternatives for the amending of the self-labeling performed by the black-box classifier. Section 5 introduces the numerical simulations and an extensive discussion covering the performance and interpretability of the SlGb. Section 6 formalizes the concluding remarks and research directions to be explored in the future."}, {"heading": "2 Semi-supervised Classification Methods and their Interpretability", "text": "In supervised classification the goal is to identify the right category (among those in a predefined set) to which an observation belongs. These observations (henceforth called instances) are often described by a set of numerical and/or\nnominal attributes. Solving this problem implies to define a mapping f : X \u2192 Y that assigns to each instance x \u2208 X , described by a set of attributes A = {a1 . . . , ap}, a decision class y \u2208 Y . The mapping is learned from data in a supervised fashion, i.e., by relying on a set of previously labeled examples, used to train the classifier.\nSemi-supervised techniques attempt to use both labeled and unlabeled instances during the learning process for increasing the prediction capacity when only labeled data is used. More formally, in a SSC scenario we have a set of m instances L = {l1, . . . , lm} which are associated with their respective class labels in Y , and a set of n unlabeled instances U = {u1, . . . , un}, where usually n > m. In the context of SSC, the classifier performance can be evaluated in two settings: (1) transductive learning, which only attempts to predict the labels for the given unlabeled instances in U ; or (2) inductive learning, which tries to infer a mapping g : L\u222aU \u2192 Y for predicting the class label of any instance associated with the classification problem.\nIn this section, we review the main state-of-the-art methods for semi-supervised classification, including an analysis of their interpretability. Here, we evaluate the interpretability as the inherent model transparency, as described in [16]."}, {"heading": "2.1 Semi-supervised Classification Methods", "text": "As mentioned, SSC methods often involve assumptions about the distribution or characteristics of the unlabeled data [23]. For example, transductive Support Vector Machines (tSVMs) [6] assume that the decision boundary lies in a low-density region. This method uses unlabeled data for maximizing the margin between the different classes by placing the decision boundaries in sparse regions. However, given the fact that the complexity of the optimization problem increases in the semi-supervised setting, its computational burden is quite high and it does not scale well for large-scale data. Recent studies [24, 25] try to overcome this limitation by using the concave-convex procedure and variations of stochastic gradient descent to solve the optimization problem. Although SVMs are a powerful technique with a strong mathematical framework for building classifiers, it has the drawback of working as a black box from the interpretability point of view. The lack of transparency of SVMs does not allow them to produce explanations or interpretations of the obtained model. In this case, the use of post-hoc methods for generating explanations is necessary when requiring explanations over the obtained predictions.\nGraph-based methods [7] assume that high-dimensional data lie on a low-dimensional manifold [26]. These methods represent the data space as a graph (i.e., if two instances are strongly connected, then they likely belong to the same class) and estimate a continuous function which is close enough to the label values, with the ultimate goal of propagating labels between similar instances. Recent works on Label Propagation methods [27, 28] are mainly focused on the construction of an effective graph over data with complex distribution and reducing the risk of error propagation through outliers. This approach could be interpretable to some extent by visually inspecting the obtained graph from the structural point of view, allowing some transparency at the parameters level. A first work toward this direction can be found in [29], where the authors propose a flow sub-graph framework which visualizes the path along the information flow from a source labeled instance to a target unlabeled instance. These sub-graphs can be seen as rather local explanations in the form of visualizations of the model. Their usability is limited to data that can be represented in the graph replacing the abstract representation of the node (e.g. images). A more general option is to obtain kNN-like explanations with examples by leveraging the graph structure, e.g. \u201cthe predicted label of instance xi was propagated from instances x1, x2 and x3\u201d.\nA third approach assumes that the data follow an identifiable mixture distribution (Generative Mixture Models [8]), henceforth they learn a joint probability for identifying the mixture components using the unlabeled data. This approach may be convenient when the available data produce well-separated clusters [23], but most of the time the joint distribution is not easily identifiable. Here the estimated mixture distribution could be interpretable at a very high abstraction level if the representation space of the problem at hand is not too complex. However, the unlabeled data could have a negative effect on algorithm\u2019s performance if the generative model is wrong. From the interpretability point of view, the classification of a new instance can leverage the Bayes rule for building a (rather abstract) explanation: \u201cyj is the most probable value of y for xi since the probability p(xi) is high when y = yi\u201d. Moreover, the estimated mixture distribution could only be visualized in a low-dimensional feature space for gaining insights into the clusters found by the model. In our opinion, GMMs require the use of post-hoc methods or global surrogates for gaining in interpretability of their results. An interesting work in this direction includes generating rectangular regions from the clusters and transforming them into rules [30].\nMore recently, deep architectures have been explored by extending graph-based methods [31] and generative models [32]. Particularly successful has been the extension of Generative Adversarial Networks (GAN) [33] to the SSC context [34, 10]. For example, Feature Matching GANs [10] use a discriminator for c+1 labels instead of the binary \u201creal/fake\" distinction, where the first c are the class labels of the problem and c+ 1 corresponds to the generated instances. The authors in [35] theoretically analyze whether a good generator and a good discriminator for semi-supervised learning\ncan be obtained at the same time. The study concludes that the generator should be \u201cbad\u201d in the sense of assigning high probabilities to low-density regions of the input space according to the true distribution, in order to complement the true data distribution and improve the semi-supervised performance. Regarding interpretability, deep neural networks are black-box models that need post-hoc procedures for generating explanations of their predictions. The majority of contributions are focused on local surrogate models or feature importance methods specially designed for deep multilayer, convolutional or recurrent neural networks [36, 37]. Interesting works connected to the semi-supervised setting include learning disentangled latent representations in a variational autoencoder, i.e. latent variables with an interpretable meaning coming from labeled data are added to the latent representation [38]. These latent interpretable variables can be used later on for inspecting their influence in the prediction.\nFinally, self-labeling refers to a wide family of very powerful and versatile wrapper methods that employ one or more base classifiers for enlarging the available labeled dataset assuming the predictions they produce on the unlabeled data are correct. Since our contribution falls within this category, we decided to revise those SSC methods in a separate subsection to gain in clarity."}, {"heading": "2.2 Self-labeling Techniques", "text": "According to [9], self-labeling techniques can be categorized into single-view or multi-view methods based on whether they need one or multiple datasets for learning. Self-training approaches [12] are single-view wrapper classifiers, which rely on the prediction of only one base classifier to repeatedly increase the size of the labeled dataset by predicting the unlabeled instances. The instances are added incrementally, in batch [39] or in an amending procedure [40]. The use of amending procedures allows selecting or weighting the self-labeled instances for enlarging the labeled dataset, hence avoiding error propagation.\nThe multi-view methods assume that the data space can be described from two or more different viewpoints. These different views normally correspond to distinct sets of attributes describing the same instances [41]. A classic example of multi-view methods is the Co-training [42] approach, where different classifiers are trained separately, each using a different attribute subset. Thereafter, the prediction of each classifier over the unlabeled dataset is used for enlarging the training set of the other. Other alternatives using multiple classifiers but not needing multi-view datasets are Democratic Co-learning [43], Tri-training [44], Co-training by committee [11] and Co-Forest [45] which use several base classifiers of the same type. Tri-training uses three base classifiers that collaborate in the learning process by labeling an unlabeled example if the other two classifiers agree. An alternative to Co-training is Co-training by committee, which does not require multi-view nor different learning algorithms, and explores different ensemble strategies with Bagging as the best performing one. Similarly, Co-Forest adopts a Random Forest classifier as an alternative for Co-training.\nSelf-labeling techniques are easy to implement and apply to almost all existing classifiers [46, 47, 48, 49]. A wide experiment conducted in [9] shows that CoTraining using Support Vector Machines as a base classifier [11], TriTraining using C4.5 decision tree [44], CoBagging using C4.5 [11] and Democratic Co-learning (as an ensemble of Na\u00efve Bayes, C4.5 and K-Nearest Neighbors) [43], are the best performing self-labeling classifiers evaluated against a comprehensive collection of benchmark datasets. Other semi-supervised classifiers that have demonstrated competitive performance in a variety of datasets are self-training using logistic model trees [46], differential evolution [50] or naive Bayes [48].\nIn terms of interpretability, a self-training scheme producing a simulatable model (e.g., relatively simple tree structure) as the final classifier can be considered a transparent model. More complex schema such as Tri-training, Co-Bagging, Co-Forest or Co-training are less likely to be interpretable due to the collaborative nature of the algorithms and the complexity of the resulting structure. However, the ensemble character of self-labeling is a perfect match with the use of local or global surrogate models for explaining predictions. Combining base classifiers using self-labeling in a way that the resulting ensemble works as a surrogate white box is the challenge we want to address. In the next section, we describe a simple yet effective self-labeling method which uses two base classifiers, a black box and a white box, for reaching a suitable trade-off between performance and interpretability."}, {"heading": "3 Self-labeling Grey-box Approach", "text": "In this section we describe the self-labeling grey-box proposed in our previous works [21]2. Here, we use a black-box classifier to predict the decision class of the unlabeled instances, while a surrogate white box is used to build an\n2The authors in [51] report on an ensemble grey-box strongly inspired in our previous work [21]. They claim that their main contribution is the use of the confidence of the black box predictions as amending of the self-labeling, however this idea was earlier proposed in our previous work [22]. The only difference between their method and our model is that they use an iterative process to select the instances to be included in the enlarged dataset. However, this seems to be a strange approach considering that the confidence amending reduces the need of such an iterative process.\ninterpretable predictive model (e.g., a rule-based approach), based on the whole instance set. The aim is to outperform the base white-box component using only the originally labeled data, while maintaining a good balance between performance and interpretability. It is worth mentioning that the main motivation behind SlGb is not to outperform the most complicated state-of-the-art algorithms but to provide a simple approach allowing for interpretability. In other words, we should be able to produce competitive solutions without significantly increasing the complexity inherent to the base classifiers.\nThe learning process is performed in a sequential order. In a first step, we provide the available labeled dataset (L, Y ) to a black-box classifier for training. Once the supervised learning is completed, the black-box component has learned a function f : L\u2192 Y , where f \u2208 F , being F the hypothesis space that associates each instance with a class label. The f function can be computed from the scoring function h : L\u00d7 Y \u2192 [0, 1] such that f(x) = argmaxy\u2208Y {h(l, y)}, l \u2208 L. Thereafter, the trained black-box component is used for generating new tuples (u, y) by mapping all unlabeled instances u \u2208 U to a class label y \u2208 Y as y = f(u), adding a self-labeling character to the approach. From this step we obtain an enlarged training set (L \u222a U, Y ) comprising the original labeled instances and the extra labeled ones. In the second step, the enlarged training set (L \u222a U, Y ) is used to train a surrogate white-box classifier. Once the learning process in the white-box component is completed, we obtain a function g : (L \u222a U) \u2192 Y resulting in a classifier which is more likely to have better generalization capabilities than the original white-box component, when trained on only the labeled data. Figure 1 summarizes this process.\nWhen applying self-labeling, we should be aware of the risk of having imbalanced data with respect to the class labels. It might be easier to obtain unlabeled data of a certain class, for example, in the context of credit fraud detection or rare diseases classification. In order to deal with this problem, our approach additionally incorporates a simple strategy for balancing instances as a preprocessing step. This weight is computed as:\nw(lj ,yi) = |L[ymin]|/|L[yi]| (1)\nwhere L[yi], L[ymin] \u2282 L denote the sets of labeled instances that are mapped to the class label yi and the minority class ymin, respectively. In this way we assign higher importance to instances belonging to the minority class.\nIn general, the SlGb approach is only based on the general assumption of SSC methods: the distribution of unlabeled instances helps elucidate the distribution of all examples. In addition, our approach allows retaining the inherent interpretability of the chosen white-box surrogate. According to the taxonomy proposed in [9], our approach can be categorized as follows:\nsingle-view: the SlGb classifier does not need different attribute sets for describing the instances, adding simplicity to the model;\nmulti-classifier: two different base classifiers are used, connected in a sequential process, the first classifier should be a good performing black-box supervised classifier, whereas the second should be a white-box technique guaranteeing interpretability to the final model;\nmulti-learning: the learning process comprises two steps, where two different learning algorithms are used depending on the base classifiers.\nIt can be noticed that the performance of the whole SlGb approach largely depends on the prediction capability of the black-box classifier when classifying unseen instances. Obviously, like any other machine learning algorithm, when solving application problems the performance will also depend on the quality of the data and the application of domain-dependent preprocessing steps [52]. However, in the context of self-labeling, the classification mistakes can reinforce themselves if no amending procedure is used during self-training. Therefore, in the next section we describe two amending strategies for the self-labeled instances, in order to prevent the error from propagating through the model."}, {"heading": "4 Amending Strategies", "text": "In this section, we describe two strategies for weighting the instances that result from the self-training stage. The goal is to improve the quality of the final model either in terms of performance or interpretability. The first strategy uses the confidence of the predictions made by the base black box and the second one focuses on the possible inconsistency of the enlarged dataset. Therefore, both procedures assign more importance to more reliable instances in the second learning step, avoiding the propagation of errors or inconsistent information."}, {"heading": "4.1 Using the Class Membership Probabilities of the Black-box Classifier", "text": "For the first strategy, the amending process for each unlabeled instance u is based on the class membership probability, which is computed by the black-box classifier in the self-labeling. The weights are assigned to the instances after they are labeled by the black-box classifier, thus expressing the confidence degree associated to the self-labeling process. Equation (2) shows how to compute the weight w(uk,yi) using the scoring function of the black-box base classifier h(uk, yi) that expresses the class membership probability of uk being correctly assigned to the yi class,\nw(uk,yi) = h(uk, yi). (2)\nThe proposed amending strategy constitutes an alternative to the use of incremental or batch procedures. Our amending does not need several iterations, thus reducing the computational burden of the self-labeling process. The pseudo-code in Algorithm 1 formalizes the method and incorporates the amending step in the general scheme.\nData: Labeled instances (L, Y ), Unlabeled instances U Result: g : (L \u222a U)\u2192 Y begin\n/* Preprocessing: Weight labeled instances according to Eq. (1) */ forall (lj , yi) \u2208 (L, Y ) do\nw(lj ,yi) \u2190\u2212 |Lmin|/|Li| end /* Train black-box component with weighted labeled data */ f, h\u2190\u2212 blackboxClassifier.fit(L, Y,w) /* Self-labeling process: Assign a label to unlabeled instances using black-box\ninference */ forall uk \u2208 U do\nyi \u2190\u2212 f(uk) /* Compute weight of instance uk according to Eq.(2) */ w(uk,yi) \u2190\u2212 h(uk, yi) /* Add the instance to enlarge dataset */ (L \u222a U, Y ) \u222a {(uk, yi)} end /* Train white-box component with the weighted (L \u222a U, Y ) dataset */ g \u2190\u2212 whiteboxClassifier.fit(L \u222a U, Y,w) return g\nend Algorithm 1: SlGb learning algorithm with confidence amending.\nIt is important to mention that the black-box classifier should be able to measure calibrated probabilities in order to correctly interpret them as the confidence of its predictions. Not all machine learning models are able to provide probabilities that match with the expected distribution of probabilities for each class. According to a study on different supervised classifiers regarding probabilities estimation [53], logistic regression, multilayer perceptrons and bagged\ntrees naturally provide well calibrated probabilities, whereas others such as boosted trees and SVM produce distorted ones. When the calibration of probabilities is needed, two main options are available: Platt\u2019s scaling [54] and isotonic regression [55]. Platt\u2019s scaling is more recommended when the distortion in the predicted probabilities has a sigmoid shape, whereas isotonic regression is able to correct any monotonic distortion but it requires large amounts of data for avoiding overfitting.\nThe amending based on class membership probabilities assumes the ground truth labels are correct and induces the white box to focus its learning on instances that are certain according to that. However, when dealing with limited labeled data we should not discard the existence of noise in the class labels. This can generate class inconsistency, especially when unlabeled data is added from different sources."}, {"heading": "4.2 Using the Inclusion Degree Measures from Rough Set Theory", "text": "In this subsection we describe a second strategy for amending the enlarged dataset, which is based on the knowledge structures attached to Rough Set Theory [56]. This formalism allows handling uncertainty in the form of inconsistency through the computation of the lower and upper approximations for any set of instances in the decision space. The rough regions associated to these approximations can be used to weight the instances after performing the self-labeling process. Particularly, we assign higher weights to more confident instances as they have more chance to be correctly classified by the base black box."}, {"heading": "4.2.1 Rough Set Theory", "text": "Rough Set Theory (RST) [56] is a mathematical formalism for handling uncertainty in the form of inconsistency. Given a decision system DS = (U , A \u222a {d}) where the universe of instances U is described by a non-empty finite set of attributes A and its respective decision class d, any concept (subset of instances) X \u2208 U can be approximated by two crisp sets. These sets are called lower and upper approximations of X (BX and BX , respectively) and can be computed taking into account an equivalence relation, as follows:\nBX = {x \u2208 U | [x]B \u2286 X} (3)\nBX = {x \u2208 U | [x]B \u2229X 6= \u2205} (4)\nThe equivalence class [x]B gathers the instances in the universe U which are inseparable according to a subset of attributes B \u2286 A. From the formulations of upper and lower approximation, we can derive the positive, negative and boundary regions of any subset X \u2208 U . The positive region P(X) = BX includes those instances that are surely contained in X; the negative region N (X) = U \u2212 BX denotes those instances that are surely not contained in X , while the boundary region B(X) = BX \u2212BX captures the instances whose membership to the set X is uncertain, i.e., they might be members of X .\nThe classic RST is regularly defined over a subset of discrete attributes, thus generating a partition of U . A more relaxed formulation of RST establishes the inseparability between instances based on a weak binary relation. Equation (5) formalizes the similarity relation used in this paper, which define whether any pair of instances xi and xj can be considered similar,\nR : xiRxj \u2192 \u03b4(xi, xj) \u2265 \u03b5 (5)\nwhere \u03b4(xi, xj) computes the extent to which xi and xj are deemed inseparable as indicated by the similarity threshold \u03b5. Under this assumption, the universe is arranged in similarity classes that are not longer disjoint but overlapped. In this paper, \u03b5 = 0.98 and the inseparability relation is defined as the complement of a distance function, such as the Heterogeneous Euclidean-Overlap Metric [57]. This distance function computes the normalized Euclidean distance between numerical attributes and an overlap metric for nominal attributes. Equations (6) and (7) define this dissimilarity function,\n\u03b4(xi, xj) = \u221a\u221a\u221a\u221a\u2211|B|t=1 \u03c9t\u03c1t(xi, xj)\u2211|B| t=1 \u03c9t\n(6)\nwith,\n\u03c1t(xi, xj) =  0 if bt is nominal \u2227 xi(t) = xj(t) 1 if bt is nominal \u2227 xi(t) 6= xj(t) (xi(t)\u2212 xj(t))2 if bt is numerical\n(7)\nwhere xi(t) and xj(t) denote the normalized values of the t-th attribute for heterogeneous instances xi and xj , respectively, and \u03c9t is the information gain of the bt attribute.\nOnce the covering of the decision space is generated according to the similarity function, several RST based measures can be computed for measuring the uncertainty contained in a dataset [58]. In the following subsection, we adopt one of these measures to weight the instances belonging to the enlarged training set obtained after performing the self-labeling process."}, {"heading": "4.2.2 Inclusion Degree", "text": "The use of this amending strategy is based on the fact that the black box could produce wrong labels for unlabeled instances. In addition, there is no guarantee that the knowledge concerning the original labeled instances is confident. To address both situations together, we propose a mechanism to weight the instances after the self-labeling process. Unlike the confidence-based strategy, this amending procedure is adopted for the entire enlarged dataset, instead of only the self-labeled instances. Therefore, it treats the uncertainty in the form of inconsistency of the labeled and unlabeled instances together.\nMore explicitly, the second weighting strategy is based on the inclusion degree of both labeled and self-labeled instances into the RST granules, thus let X = L \u222a U and d = y. Let \u00b5RP(yi)(x), \u00b5 R B(yi)\n(x) and \u00b5RN(yi)(x) denote the membership degrees of any instance x to the positive, boundary and negative region of each class label yi, respectively. These membership degrees are computed from the inclusion degree of the similarity class of x into each information granule,\n\u00b5RP(yi) (x) = |R\u0304(x) \u2229 P(X[yi])| |P(X[yi])|\n(8)\n\u00b5RB(yi)(x) = |R\u0304(x) \u2229 B(X[yi])| |B(X[yi])|\n(9)\n\u00b5RN (yi)(x) = |R\u0304(x) \u2229N (X[yi])| |N (X[yi])|\n(10)\nwhere R\u0304(x) is the similarity class associated with the instance x, whereas X[yi] denotes the set of instances with label yi. The similarity class of an instance x groups all instances that are similar to x according to the subset of attributes taken into account. By computing how much x and its similar instances are included in the positive region of a class yi, we are estimating how sure we are of this classification. The same reasoning holds for the negative and boundary regions.\nEquation (11) computes the weight for the instance x belonging to the enlarged dataset, given its label yi and a similarity relationR. The sigmoid function \u03d5(.) is used to maintain the weight in the (0, 1) range.\nw(x,yi) = \u03d5 ( \u00b5RP(yi)(x) + 0.5 \u2217 \u00b5 R B(yi)(x)\u2212 \u00b5 R N (yi)(x) ) (11)\nThe intuition of this weight is that if an instance and its similar ones are included mostly in the positive region of a class, therefore their label must be correct and they should have a high weight in the second learning phase. In the same way, if the instance x and its similarity class are mostly contained in the negative region of a class then the class assigned to x by the black box must be a mistake. Observe that the boundary information is also interesting since a high inclusion degree of an instance in the boundary region of a class is to some extent positive evidence (see Equation 4). This boundary region role can be reinforced or diluted according to the evidence coming from the inclusion degrees in the other two regions. When using the RST-based amending, Equation (11) replaces Equation (2) in the pseudo-code of Algorithm 1.\nFigure 2 illustrates the inclusion of the amending procedures into the learning algorithm of the SlGb approach. It is important to note that the amending process is only carried out in the learning phase of the SlGb. Therefore, the amending strategies do not affect the transparency of the white-box surrogate during the inference on new cases.\nThe use of amending by weighting could have some implications for the interpretability. Assigning high weights to a small subset of instances transforms the global surrogate model towards a more local one. In other words, the weighting of instances makes the white box biased towards learning from the most confident ones, thus providing explanations for that subspace of the domain mostly. However, it makes sense to provide interpretability or explanations over the predictions that are most certain in the problem domain. In addition, it could have a positive influence on reducing the number of explanations produced by the white box."}, {"heading": "5 Experiments and Discussion", "text": "In this section, we evaluate the SlGb approach through a three-step methodology using standard benchmark datasets. Unlike other experiments reported in the literature, the one developed in this section evaluates both algorithms\u2019 performance and interpretability, when having different percentages of labeled instances. As a complement, we propose three new evaluation measures that go beyond the prediction rates.\nBeing more specific, the first step of our experimentation methodology is devoted to determining which black-box classifier produces the best results in terms of prediction performance. This step is quite important since the overall performance will depend on the discriminatory ability of the black box. The second step is dedicated to determining which combination of white box and amending reaches the best commitment between prediction rates and interpretability. As a third step, we further explore the impact of having different percentages of labeled and unlabeled instances on the algorithm\u2019s performance.\nAs a complement of the evaluation methodology for interpretable SSC methods, in the last part of this section we compare SlGb against the best-performing state-of-the-art methods. In this case, the evaluation is confined to the prediction rates as these methods cannot be interpreted, thus the goal here is to show that SlGb is not just simple and elegant, but also able to outperform other self-labeling methods reported in the literature."}, {"heading": "5.1 Benchmark Datasets, Base Classifiers and Parameter Settings", "text": "Our experimental design includes 55 challenging and diverse datasets for classification tasks where features are structured (i.e. the dataset has tabular form) and therefore are potentially interpretable. Four ratios of labeled instances in the training set (from 10% to 40%) allow studying the influence of the number of labeled examples on the overall performance. Testing with a 10% ratio means that the training set contains only a 10% of labeled instances and the rest of are unlabeled, the instances in the test set are all labeled but set apart. These datasets comprise different characteristics: the number of instances ranges from 100 to 19000, the number of attributes from 2 to 90, and the number of decision classes from 2 to 28. Moreover, we have 25 datasets with different degrees of class imbalance and roughly half of the datasets are multiclass problems (see Table 5).\nThese datasets are partitioned into training and test sets as done in a 10-fold cross-validation process, but each training set consists of labeled and unlabeled instances. The subset of unlabeled instances is obtained by performing a random selection without replacement and neglecting the class label of such instances. The ratio (10% to 40%) determines the number of labeled instances that are kept in this process for each training set. These datasets (including the cross-validation fold partitions) were provided as supplementary material in [9] and constitute an standard in the evaluation of shallow SSC techniques. We use these datasets, including the partitions as a form of guaranteeing a fair comparison against state-of-the-art SSC methods.\nThere are several algorithms that can be adopted as base classifiers. On one hand, the selected classifier for the base black box should exhibit a strong predictive capability as it is used to determine the decision class of unlabeled instances. Next, we describe three mainstream supervised classifiers that will be used in the experiments for instantiating the black-box component. Our choice is motivated by experimental evidence of their superior performance in a wide range of classification problems [59, 60, 61] and their ability to produce calibrated probabilities (except for support vector machines where a calibration post-hoc is needed).\nBlack-box classifiers\n\u2022 Random Forests (RF) [62]: Ensemble of decision trees that uses bagging technique for aggregating the results in order to reduce the high variance of individual decision trees. Individual decision trees are built with a random subset of attributes and a random sample with replacement of instances. In our implementation 100 trees are aggregated and the number of random attributes to consider for each tree equals log2(|A|).\n\u2022 Multilayer Perceptron (MLP) [63]: Feed-forward neural network using backpropagation algorithm for adjusting its weights. Our implementation uses learning rate equals to 0.3, momentum equals to 0.2, 500 epochs for learning and one hidden layer with (|A|+ |Y |)/2 as the number of neurons.\n\u2022 Support Vector Machine (SVM) [64, 65]: Support vector machine classifier using sequential minimal optimization algorithm for training. Our implementation uses a polynomial kernel with Platt\u2019s scaling (logistic) calibration of probabilities.\nOn the other hand, for the white-box component any intrinsically interpretable classifier can be used as a surrogate model. Therefore, the choice of a white box must be driven by the type of explanations that are desired, e.g. rules, feature coefficients, probabilities, examples, etc. We decide to explore decision trees and decision lists alternatives as they provide both intuitive individual explanations in the form of if-then rules and a view of the model as a whole. For decision trees, the hierarchical structure provides this view and it can be considered transparent as long as the size of the tree remains manageable. For the case of the decision lists, rules sets are generally more concise than the ones extracted from decision trees. Additionally, these algorithms are able to handle weighted instances in the learning process. Next, we describe three classifiers explored in the scope of this experiment.\nWhite-box classifiers\n\u2022 Decision Tree (C45) [66]: Our implementation uses C4.5 algorithm for inducing a decision tree. We allow two instances as the minimum number of instances per leaf. The confidence factor for pruning is 0.25, where a lower value incurs in more pruning. When pruning the sub-tree raising operation is used.\n\u2022 PART Decision List (PART) [67]: PART uses the separate-and-conquer strategy for building a rule set by generating a partial C4.5 decision tree and making the most confident leaf into a rule. In the next iteration, all covered instances are removed from the dataset and the process is repeated. Thus, decision lists must be interpreted in order. Our implementation uses the same hyper-parameters of the decision tree described above for generating the partial C4.5 decision trees.\n\u2022 RIPPER Decision List (RIP) [68]: This method is a propositional rule learner with a separate-and-conquer strategy, as described for PART. Additionally, the training data is split into a growing set and a pruning set for performing reduced error pruning. The rule set formed from the growing set is simplified with pruning operations optimizing the error on the pruning set. For our implementation, the minimum allowed support of a rule is two and the data is split in three folds where one is used for pruning. Besides, two optimization iterations are performed.\nFor completeness, we enumerate the amending procedures that will be tested in combination with the previous base classifiers."}, {"heading": "5.1.1 Amending procedures", "text": "\u2022 No amending (NONE): The first option is not using amending. All self-labeled instances are provided as extra data to the surrogate white box. This is used as a baseline for evaluating the contribution of the two amending procedures proposed.\n\u2022 Amending based on class membership probabilities (CONF): Amending procedure based on calibrated class membership probabilities obtained from the black-box base classifier [22].\n\u2022 Amending based on RST inclusion degree measure (RST): Amending procedure based on RST aiming to correct the inconsistency in the classifications, as described in the previous section.\nHereinafter, when referring to a particular configuration of SlGb we denote it as \u201cbb-wb-am\" where bb represents the base black box, wb represents the surrogate white box and am represents the amending procedure3."}, {"heading": "5.2 Impact of the Black-box Base Classifiers on the Performance", "text": "We first focus on evaluating the influence of the base black box in the performance of the algorithm. Here no amending procedure is taken into account yet since it does not directly affects the ability of the black box to produce correct classifications. In order to measure the configurations in terms of prediction rates we report the Cohen\u2019s kappa coefficient [69]. This measure estimate the inter-rater agreement for categorical items and ranges in [\u22121, 1], where \u22121 indicates no agreement between the prediction and the actual values, 0 means no learning (i.e., random prediction), and 1 total agreement or perfect performance. While accuracy is considered mainstream when measuring classification rates, the kappa is a more robust measure since this coefficient takes into account the agreement occurring by chance, which is especially relevant for datasets with class imbalance [70, 71].\nTable 1 gives the mean and the standard deviation of the kappa coefficient achieved on each setting. We group the results for different percentages of labeled instances. The numerical simulations indicate that using RF as the blackbox component leads to higher prediction rates. In particular, the RF-PART-NONE configuration stands as the best performing one for varying amounts of labeled instances, very closely followed by RF-C45-NONE and RF-RIP-NONE.\nWith the aim of providing a rigorous statistical analysis of the differences, we compute the Friedman two-way analysis of variances by ranks [72], per ratio. The test suggests rejecting the null hypotheses for all labeled ratios based on a\n3Code, datasets and results using different measures (kappa, accuracy, number of rules, etc.) are available as supplementary material for reproducibility purposes in gitlab.ai.vub.ac.be/igraugar/slgb_scripts/tree/paper. All SlGb configurations were implemented using weka library [41] and its default parameters listed above.\nconfidence interval of 95% (see Table 6 in appendix4). This means that there exist significant differences between at least two configurations on each ratio.\nThe next step is focused on determining whether RF black box is truly superior compared to other configurations. To do so, we adopt the Wilcoxon signed rank test [73] and Holm\u2019s post-hoc procedure [74] to correct the p-values, as suggested by Benavoli et al. [75]. Table 7 reports the unadjusted p-value computed by the Wilcoxon test and the corrected p-value associated with each pairwise comparison. In order to discover the influence of the black box we compare the pairs of configurations using the same surrogate white box. Each section of the table represents the ratio of labeled instances. The null hypothesis states that there is no significant difference between the performance of each pair of configurations. All null hypotheses are rejected, except for RF-RIP vs. MLP-RIP in the 40% ratio (however RF still has higher prediction rates).\nThis suggests that RF is clearly the best-performing base black box for the self-learning grey-box. This result is not surprising since RF has proven to be very competent classifier in different experimental studies [59, 60, 61, 76]. Furthermore, RF produces consistent probabilities that does not need to be calibrated [53], which is a requirement for the later use of confidence-based amending."}, {"heading": "5.3 Impact of using Different White Boxes and Amending Configurations", "text": "In this section, we study how different choices of the amending processes and white-box surrogates impact the overall results. We propose some measures for evaluating performance taking both accuracy and interpretability into account.\nWe first explore the influence on the prediction rates. Based on the selection of RF as black-box base, Table 2 shows very similar results across each ratio. Going deeper with the statistical analysis, we apply Friedman and Wilcoxon tests with post-hoc correction. Although Friedman test finds significant differences in the four groups (Table 8), examining Wilcoxon corrected tests we ascertain that the null hypothesis cannot be rejected for the vast majority of pairs compared (see Tables 9 and 10 for details). This means that there are no statistically significant differences in the prediction rates when comparing different amending procedures with a fixed white box and vice versa. This behavior suggests that the overall prediction rates of the approach mostly relies on the correct choice of the black-box algorithm.\nHowever, when examining the number of rules obtained, the difference is significantly visible. Figure 3 plots of the number of rules produced by each combination, per ratio of labeled data. Two results are consistent across ratios: both amending strategies (specially RST) reduce the number of rules while RIP as surrogate white box produces the lowest number of rules for all possible combinations.\n4All tables related to statistical tests are included in the appendix.\nToward exploring this result further, we also propose two new measures to evaluate models\u2019 interpretability via a quantifiable proxy. The first measure can be used in the context of self-labeling and the second one is applicable to any model containing explanation units. According to [14], there are three main forms of evaluating interpretability: application-grounded, human-grounded and functionally-grounded metrics. The functionally-grounded approach is the only one not requiring human experiments and collaboration. As an alternative it uses desiderata for interpretability (e.g. transparency, trust, etc.) as a proxy for assessing the quality of the model. Since we are working with benchmark datasets, we use the functionally-grounded approach for creating measures based on the simplicity as a mean for gaining transparency and simulatability (i.e. a human is able to simulate and reason about the model\u2019s entire decision-making process). The first measure can be used in the context of self-labeling for base methods that produce tree structures, rules or decision lists. It involves the number of rules in the decision lists (or equivalently the number of leaves in a decision tree) and expresses the relative growth in structure as:\n\u0393 = |Eg|/|Ew| (12)\nwhereEg is the set of rules produced by the self-labeling method (here the grey-box) andEw is the set of rules produced by the baseline white box when using only labeled data. For this measure, a number much greater than one indicates that a major growth in the structure of the self-labeling method is needed when using the extra unlabeled data. In that case, the balance between interpretability and performance must be taken into account for further evaluation.\nThe second measure is more general and applicable to any model whose structure is formed by quantifiable explanation units (e.g. rules, prototypes, features, derived features, etc.). For our case, this measure estimates the simplicity of the model according to the size of the structure in terms of the number of rules. Although the first notion would be that the smaller the rule set the better, this is not necessarily a linear relation. The desired simplicity in terms of the number of rules has a smooth behavior which can drop quickly. Therefore, we propose to measure simplicity through a generalized sigmoid function which has been historically used for fitting growth curves [77], since it allows representing this relation with enough flexibility. The simplicity can be formalized as the following equation:\n\u03a5(|Eg|) = \u03b81 + \u03b82 \u2212 \u03b81\n(1 + e\u2212\u03bb(|Eg|\u2212\u03b7))1/\u03bd (13)\nwhere \u03b81 = 1 and \u03b82 = 0 represent the upper and lower asymptotes of the function respectively, \u03bb is the slope of the curve, \u03b7 regulates the shift over the x-axis and \u03bd affects near which asymptote maximum growth occurs. In this way, a result value of one indicates high simplicity and it decreases smoothly towards zero. A bigger \u03bb would make the function less smooth, generating a drastic drop in simplicity after a threshold in the number of rules is surpassed. The value of \u03b7 defines where the middle value of the function is obtained. While a value of \u03bd = 1 makes no change in the curve, \u03bd < 1 moves the growth toward the upper asymptote and \u03bd > 1 toward the lower one. Observe that both \u03b7 and \u03bd influence where 0.5 simplicity is obtained. Given the diversity of our benchmark data, we take \u03bb = 0.1, \u03b7 = 30, \u03bd = 0.5 for illustrating a general setting (see Figure 4).\nWith these values, the function produces medium evaluations (around 0.5) when the number of rules is around 40. Similarly, it obtains rather high simplicity (higher than 0.8) when the number of rules goes below 30. However, parameter values should be estimated based on expert knowledge for specific applications. This highly flexible function allows customizing the value of simplicity according with the specifics of a given case study.\nTable 3 shows the average relative growth and simplicity over the 55 datasets tested for the four ratios. Regarding the relative growth, the increase in the structure of the grey-box is on average larger when using small amounts of labeled data, while for bigger ratios this difference decreases. This growth in the structure is an expected consequence of providing more unlabeled data to the white-box surrogate in the grey-box scheme. However, the use of amending procedures alleviates this effect by giving more importance to relevant unlabeled instances. In general, a smaller growth is observed when using RST amending, especially in combination with PART as the white box, thus resulting in the winner combination for all ratios.\nIn addition, the simplicity measure (the closer the value to one the better) also indicates that the use of amending is convenient for obtaining more concise sets of rules. It is also evident that using RIP as surrogate generates the least number of rules, followed by PART. For this measure the absolute winner is RF-RIP-RST combination, exhibiting the highest values of simplicity for all ratio values used for experimentation. A similar statistical validation support this statement (see tables 11 and 12), finding significant statistical differences when comparing RF-RIP-RST with other configurations using simplicity as interpretability measure.\nIt is important to remark that the simplicity measure solely quantifies what it would be considered a simulatable model. Of course, a very simple model with only one rule and poor prediction rates is not desirable, whereas for a very simple dataset three or four rules might be enough to reach accurate results. That is why taking into account the prediction performance is fundamental for a proper assessment. To measure algorithms\u2019 quality based on the balance between the prediction rates and the simplicity of the learned model, we propose a third measure, called utility, combining the kappa (re-scaled to (0,1)) and the simplicity values with a weighing parameter \u03b1,\n\u03a8(Eg) = \u03b1 \u2217 \u03ba(Eg)\u2032 + (1\u2212 \u03b1) \u2217\u03a5(|Eg|) (14)\nwhere \u03b1 is set to 0.6 in our experimental setting, representing a scenario where the accuracy and the interpretability have almost the same preference. Utility functions are commonly used in multi-objective optimization for mapping a vector of pay-offs to a single scalar value [78]. In this case, the utility function is a linear combination of two terms parameterized by the weight \u03b1. This weighing parameter allows adjusting the preference of the user for prioritizing the accuracy or the interpretability objectives. Here, the two objectives are measured based on kappa and simplicity, respectively. It would be interesting to extend the proposed utility to involve more objectives where the parameters should be obtained from the preferences of a panel of domain experts [79, 80].\nAs a partial summary, Figure 5 visualizes the utility values in a heat-map plot. From this figure, it is easy to perceive that the RIP algorithm, as a white-box surrogate, positively contributes to the overall performance of the approach when taking both kappa and simplicity into account. Additionally, RST amending also increases the value of utility when compared with CONF amending or not using amending at all. This measure reflects that, in general, the best trade-off\nis reached when using the RF-RIP-RST combination and the highest values are achieved when more labeled data is available."}, {"heading": "5.4 Influence of the Number of Labeled and Unlabeled Instances", "text": "In this subsection, we use RF-RIP-RST to explore the impact of having different amounts of labeled and unlabeled instances on algorithm\u2019s results. In the evaluation of semi-supervised techniques, it is a common strategy to vary the size of L by systematically neglecting the label of different amounts of instances and adding them to U . But this procedure does not explore the scenario where also the unlabeled instances could be hard to obtain [81]. Observe that since this is a controlled experiment we can safely assume that the unlabeled instances follow the same class distribution as the labeled ones. In reality, one might need to re-balance the dataset after self-labeling if the unlabeled instances per-class distribution significantly differs.\nDue to the fact that we do not have truly unlabeled instances, we use the same datasets from the previous experiment. First, a test set with 20% of instances is kept aside for evaluation. Then, we divide the train set into two equally sized and disjoint subsets (each with 40% of the total instances). Each subset is a source for labeled and unlabeled instances, respectively, from where we vary the amount of instances we use for training. Figure 6 shows the surfaces resulting from the average of different measures over the 55 datasets.\nFrom the first two surfaces (Figures 6a and 6b), it can be observed that the prediction rates (accuracy and kappa) have a pronounced increment when adding more labeled and unlabeled instances. The most dramatic change is observed when adding labeled data to a few unlabeled instances (5%), which is an expected result as it tends to be a more supervised setting. However, when labeled instances are very limited (5% of the dataset), adding unlabeled instances clearly increase the overall performance. In addition, even when more labeled data is available (40%), an increase in performance is observed by adding more unlabeled data. This result confirms that our approach fulfills the main aim of SSC approaches.\nThe number of rules (Figure 6c) increases almost linearly with the number of training instances, either labeled or unlabeled. However, the relative growth (Figure 6d) is more sensitive to adding unlabeled data when labeled data is very scarce, i.e. a bigger amount of unlabeled instances rapidly increases the structure and loses in interpretability, compared with the baseline white box. However, the base white boxes generally perform very poor when the labeled data is scarce. When the labeled data is not so scarce, then the growth is more robust to adding more unlabeled data. This means that even when a base white box can achieve good performance with some labeled data, adding unlabeled data does not generates too much growth in structure and can benefit the performance of the grey-box (Figures 6a and 6b).\nThe simplicity (Figure 6e) shows the expected behavior: the best values of this measure are observed with the least number of instances and it decreases uniformly in both directions. This means that adding more unlabeled instances does not generate a greater number of extra rules compared to adding more labeled instances. This is a consequence of using amending procedures for adjusting the confidence of the unlabeled instances, thus avoiding that the white box learns from inconsistent instances. Finally, the utility surface (Fig. 6f) summarizes all results reflecting the increase in the overall performance when adding both labeled and unlabeled instances."}, {"heading": "5.5 Comparing against State-of-the-Art Self-labeling Classifiers", "text": "In this section we compare the predictive capability of SlGb against the four best self-labeling techniques reported in the review paper in [9]: Co-training using support vector machine [11] (CT(SMO)), Tri-training using C45 decision tree [44] (TT(C45)), Co-Bagging using C45 decision tree [11] (CB(C45)) and Democratic Co-learning [43] (DCT). Since these algorithms are not inherently interpretable we focus our comparison on the prediction rates only. For this section, SlGb refers to the RF-PART-RST combination which exhibits the best results in kappa, as showed in Subsection 5.3.\nTable 4 reports the mean and standard deviation of kappa coefficient for each classifier, taking into account the four studied ratios. The results reveal that SlGb has the highest mean for all ratios. In order to support this assertion, we compute the Friedman p-value per ratio. The test suggests rejecting the null hypotheses for all labeled ratios based on a confidence interval of 95% (see Table 13). This means that there is an indication that there exist significant differences between at least two algorithms in each comparison.\nThe next step is focused on determining whether the superiority of the SlGb classifier is responsible for the significant difference reported by the Friedman test. Similar to previous sections we use the Wilcoxon signed rank test and the Holm post-hoc procedure for computing the corrected p-values associated with each pairwise comparison. Each section of the Table 14 represents a ratio of labeled instances. The null hypothesis states that there is no significant difference between the performance of each pair of algorithms, taking SlGb as the control one.\nFrom the statistical tests we can draw the following conclusions. First, there is no doubt about the superiority of the SlGb classifier when tested with datasets with ratios of 10% and 20% of labeled instances, as all the null hypotheses were rejected. This result, in combination with the first place in the Friedman ranking, demonstrates that our algorithm significantly outperforms the other four algorithms in these settings. In the case of datasets comprising 30% and 40% of labeled instances, the results show that SlGb is the best-performing classifier, but with no significant differences observed between the pairs SlGb vs. DCT (for 30%), and SlGb vs. CT(SMO) (for both ratios), as these null hypotheses could not be rejected. However, DCT and CT(SMO) cannot be considered transparent due to their complex structure involving support vector machines and collaboration between base classifiers. Although our main goal was not to outperform the SSC methods in terms of classification rates, the analysis reported above supports our claim that we obtain a favorable balance between performance and interpretability by using the SlGb approach for solving SSC problems."}, {"heading": "6 Conclusions", "text": "In this paper, we report on an extended experimental study to determine the suitability of SlGb classifier for semisupervised classification problems where interpretability is a requirement. We explore two different amending procedures for weighting the instances coming from the self-labeling process. Such procedures aim at preventing the effect of misclassifications to propagate across the whole model. The experiments have shown that using Random Forests as the base black box for the self-labeling process is the best choice in terms of prediction rates. The choice of a white box and amending does not significantly affect the prediction rates but it is relevant for the size of the structure.\nThree measures based on the number of rules were proposed for estimating the relative growth, simplicity, and utility of the SlGb. SlGb produces simpler models when using decision lists instead of a C4.5 decision tree as surrogate white boxes, even when no amending is performed. However, the amending procedures help further increase the simplicity (and therefore transparency) without affecting the prediction rates by giving more importance to confident instances in the self-labeling. Especially RST based amending looks more promising since it does not need the black-box base classifier to provide calibrated probabilities. Furthermore, RST based amending could be a good choice for a given case study where the uncertainty coming from inconsistency is high, even on the available labeled data. Therefore, we strongly advise the use of random forests as a base black box and RST for amending the self-labeling, while the choice of white box is more flexible to the desired interpretability, either a decision tree with rules or a decision list. Although, the best trade-off between accuracy and interpretability (utility) is reached when using the RF-RIP-RST combination.\nThe study varying the number of unlabeled instances and labeled instances together shows that even when the number of labeled instances is not that scarce, the SlGb is able to leverage unlabeled instances for increasing the performance. Another conclusion is that adding unlabeled instances does not make the interpretability worse compared to adding more labeled instances. This evidences that the amending procedure (in this case RST-based amending) avoids that the SlGb generates more rules from inconsistent instances. Finally, the experimental comparison shows that our SlGb method outperforms the state-of-the-art self-labeling approaches, yet being far more simple in structure than these techniques."}, {"heading": "7 Appendix: Benchmark Datasets and Detailed Results of Statistical Tests", "text": "Ratio Friedman p-value H0 10% 2.10E-08 Rejected 20% 8.91E-13 Rejected 30% 9.87E-06 Rejected 40% 5.35E-04 Rejected"}], "title": "AN INTERPRETABLE SEMI-SUPERVISED CLASSIFIER USING TWO DIFFERENT STRATEGIES FOR AMENDED SELF-LABELING", "year": 2020}