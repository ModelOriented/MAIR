{
  "abstractText": "We present a brief history of the field of interpretable machine learning (IML), give an overview of state-of-the-art interpretation methods and discuss challenges. Research in IML has boomed in recent years. As young as the field is, it has over 200 years old roots in regression modeling and rule-based machine learning, starting in the 1960s. Recently, many new IML methods have been proposed, many of them model-agnostic, but also interpretation techniques specific to deep learning and tree-based ensembles. IML methods either directly analyze model components, study sensitivity to input perturbations, or analyze local or global surrogate approximations of the ML model. The field approaches a state of readiness and stability, with many methods not only proposed in research, but also implemented in open-source software. But many important challenges remain for IML, such as dealing with dependent features, causal interpretation, and uncertainty estimation, which need to be resolved for its successful application to scientific problems. A further challenge is a missing rigorous definition of interpretability, which is accepted by the community. To address the challenges and advance the field, we urge to recall our roots of interpretable, data-driven modeling in statistics and (rule-based) ML, but also to consider other areas such as sensitivity analysis, causal inference, and the social sciences.",
  "authors": [
    {
      "affiliations": [],
      "name": "Bernd Bischl"
    }
  ],
  "id": "SP:3d981dab8ad1a25c9afbff029d73f92ac21d3fe2",
  "references": [
    {
      "authors": [
        "A. Adadi",
        "M. Berrada"
      ],
      "title": "Peeking inside the black-box: A survey on explainable artificial intelligence (xai)",
      "venue": "IEEE Access 6, 52138\u201352160",
      "year": 2018
    },
    {
      "authors": [
        "J. Adebayo",
        "J. Gilmer",
        "M. Muelly",
        "I. Goodfellow",
        "M. Hardt",
        "B. Kim"
      ],
      "title": "Sanity checks for saliency maps",
      "venue": "Advances in Neural Information Processing Systems. pp. 9505\u20139515",
      "year": 2018
    },
    {
      "authors": [
        "H. Akaike"
      ],
      "title": "Information theory and an extension of the maximum likelihood principle",
      "venue": "Selected papers of Hirotugu Akaike, pp. 199\u2013213. Springer",
      "year": 1998
    },
    {
      "authors": [
        "A. Altmann",
        "L. Tolo\u015fi",
        "O. Sander",
        "T. Lengauer"
      ],
      "title": "Permutation importance: a corrected feature importance measure",
      "venue": "Bioinformatics 26(10), 1340\u20131347",
      "year": 2010
    },
    {
      "authors": [
        "R. Andrews",
        "J. Diederich",
        "A.B. Tickle"
      ],
      "title": "Survey and critique of techniques for extracting rules from trained artificial neural networks",
      "venue": "Knowledge-based systems 8(6), 373\u2013389",
      "year": 1995
    },
    {
      "authors": [
        "S. Anjomshoae",
        "A. Najjar",
        "D. Calvaresi",
        "K. Fr\u00e4mling"
      ],
      "title": "Explainable agents and robots: Results from a systematic literature review",
      "venue": "18th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2019), Montreal, Canada, May 13\u201317, 2019. pp. 1078\u20131088. International Foundation for Autonomous Agents and Multiagent Systems",
      "year": 2019
    },
    {
      "authors": [
        "D.W. Apley",
        "J. Zhu"
      ],
      "title": "Visualizing the effects of predictor variables in black box supervised learning models",
      "venue": "arXiv preprint arXiv:1612.08468",
      "year": 2016
    },
    {
      "authors": [
        "V. Arya",
        "R.K. Bellamy",
        "P.Y. Chen",
        "A. Dhurandhar",
        "M. Hind",
        "S.C. Hoffman",
        "S. Houde",
        "Q.V. Liao",
        "R. Luss",
        "A Mojsilovic"
      ],
      "title": "AI explainability 360: An extensible toolkit for understanding data and machine learning models",
      "venue": "Journal of Machine Learning Research 21(130), 1\u20136",
      "year": 2020
    },
    {
      "authors": [
        "M.G. Augasta",
        "T. Kathirvalavakumar"
      ],
      "title": "Rule extraction from neural networks\u2014a comparative study",
      "venue": "International Conference on Pattern Recognition, Informatics and Medical Engineering (PRIME-2012). pp. 404\u2013408. IEEE",
      "year": 2012
    },
    {
      "authors": [
        "O. Bastani",
        "C. Kim",
        "H. Bastani"
      ],
      "title": "Interpreting blackbox models via model extraction",
      "venue": "arXiv preprint arXiv:1705.08504",
      "year": 2017
    },
    {
      "authors": [
        "P. Biecek"
      ],
      "title": "DALEX: explainers for complex predictive models in r",
      "venue": "The Journal of Machine Learning Research 19(1), 3245\u20133249",
      "year": 2018
    },
    {
      "authors": [
        "T. Botari",
        "F. Hvilsh\u00f8j",
        "R. Izbicki",
        "A.C. de Carvalho"
      ],
      "title": "MeLIME: Meaningful local explanation for machine learning models",
      "venue": "arXiv preprint arXiv:2009.05818",
      "year": 2020
    },
    {
      "authors": [
        "L. Breiman"
      ],
      "title": "Random forests",
      "venue": "Machine learning 45(1), 5\u201332",
      "year": 2001
    },
    {
      "authors": [
        "R. Caruana",
        "Y. Lou",
        "J. Gehrke",
        "P. Koch",
        "M. Sturm",
        "N. Elhadad"
      ],
      "title": "Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission",
      "venue": "Proceedings of the 21th ACM SIGKDD international conference on knowledge discovery and data mining. pp. 1721\u20131730",
      "year": 2015
    },
    {
      "authors": [
        "D.V. Carvalho",
        "E.M. Pereira",
        "J.S. Cardoso"
      ],
      "title": "Machine learning interpretability: A survey on methods and metrics",
      "venue": "Electronics 8(8), 832",
      "year": 2019
    },
    {
      "authors": [
        "G. Casalicchio",
        "C. Molnar",
        "B. Bischl"
      ],
      "title": "Visualizing the feature importance for black box models",
      "venue": "Joint European Conference on Machine Learning and Knowledge Discovery in Databases. pp. 655\u2013670. Springer",
      "year": 2018
    },
    {
      "authors": [
        "M. Chromik",
        "M. Schuessler"
      ],
      "title": "A taxonomy for human subject evaluation of blackbox explanations in XAI",
      "venue": "ExSS-ATEC@ IUI",
      "year": 2020
    },
    {
      "authors": [
        "M. Craven",
        "J.W. Shavlik"
      ],
      "title": "Extracting tree-structured representations of trained networks",
      "venue": "Advances in neural information processing systems. pp. 24\u201330",
      "year": 1996
    },
    {
      "authors": [
        "D.R. Cutler",
        "T.C. Edwards Jr",
        "K.H. Beard",
        "A. Cutler",
        "K.T. Hess",
        "J. Gibson",
        "J.J. Lawler"
      ],
      "title": "Random forests for classification in ecology",
      "venue": "Ecology 88(11), 2783\u2013 2792",
      "year": 2007
    },
    {
      "authors": [
        "S. Dandl",
        "C. Molnar",
        "M. Binder",
        "B. Bischl"
      ],
      "title": "Multi-objective counterfactual explanations",
      "venue": "arXiv preprint arXiv:2004.11165",
      "year": 2020
    },
    {
      "authors": [
        "A. Dhurandhar",
        "V. Iyengar",
        "R. Luss",
        "K. Shanmugam"
      ],
      "title": "TIP: typifying the interpretability of procedures",
      "venue": "arXiv preprint arXiv:1706.02952",
      "year": 2017
    },
    {
      "authors": [
        "F. Doshi-Velez",
        "B. Kim"
      ],
      "title": "Towards a rigorous science of interpretable machine learning",
      "venue": "arXiv preprint arXiv:1702.08608",
      "year": 2017
    },
    {
      "authors": [
        "M. Du",
        "N. Liu",
        "X. Hu"
      ],
      "title": "Techniques for interpretable machine learning",
      "venue": "Communications of the ACM 63(1), 68\u201377",
      "year": 2019
    },
    {
      "authors": [
        "K. Fabi",
        "J. Schneider"
      ],
      "title": "On feature relevance uncertainty: A Monte Carlo dropout sampling approach",
      "venue": "arXiv preprint arXiv:2008.01468",
      "year": 2020
    },
    {
      "authors": [
        "L. Fahrmeir",
        "G. Tutz"
      ],
      "title": "Multivariate statistical modelling based on generalized linear models",
      "venue": "Springer Science & Business Media",
      "year": 2013
    },
    {
      "authors": [
        "M. Fasiolo",
        "R. Nedellec",
        "Y. Goude",
        "S.N. Wood"
      ],
      "title": "Scalable visualization methods for modern generalized additive models",
      "venue": "Journal of computational and Graphical Statistics 29(1), 78\u201386",
      "year": 2020
    },
    {
      "authors": [
        "M. Fasiolo",
        "S.N. Wood",
        "M. Zaffran",
        "R. Nedellec",
        "Y. Goude"
      ],
      "title": "Fast calibrated additive quantile regression",
      "venue": "Journal of the American Statistical Association pp. 1\u201311",
      "year": 2020
    },
    {
      "authors": [
        "A. Fisher",
        "C. Rudin",
        "F. Dominici"
      ],
      "title": "All models are wrong, but many are useful: Learning a variable\u2019s importance by studying an entire class of prediction models simultaneously",
      "venue": "Journal of Machine Learning Research 20(177), 1\u201381",
      "year": 2019
    },
    {
      "authors": [
        "T. Freiesleben"
      ],
      "title": "Counterfactual explanations & adversarial examples\u2013 common grounds, essential differences, and potential transfers",
      "venue": "arXiv preprint arXiv:2009.05487",
      "year": 2020
    },
    {
      "authors": [
        "A.A. Freitas"
      ],
      "title": "Comprehensible classification models: a position paper",
      "venue": "ACM SIGKDD explorations newsletter 15(1), 1\u201310",
      "year": 2014
    },
    {
      "authors": [
        "S.A. Friedler",
        "C.D. Roy",
        "C. Scheidegger",
        "D. Slack"
      ],
      "title": "Assessing the local interpretability of machine learning models",
      "venue": "arXiv preprint arXiv:1902.03501",
      "year": 2019
    },
    {
      "authors": [
        "J.H. Friedman"
      ],
      "title": "Greedy function approximation: a gradient boosting machine",
      "venue": "Annals of statistics pp. 1189\u20131232",
      "year": 2001
    },
    {
      "authors": [
        "J.H. Friedman",
        "Popescu",
        "B.E"
      ],
      "title": "Predictive learning via rule ensembles",
      "venue": "The Annals of Applied Statistics 2(3), 916\u2013954",
      "year": 2008
    },
    {
      "authors": [
        "N. Frosst",
        "G. Hinton"
      ],
      "title": "Distilling a neural network into a soft decision tree",
      "venue": "arXiv preprint arXiv:1711.09784",
      "year": 2017
    },
    {
      "authors": [
        "J. F\u00fcrnkranz",
        "D. Gamberger",
        "N. Lavra\u010d"
      ],
      "title": "Foundations of rule learning",
      "venue": "Springer Science & Business Media",
      "year": 2012
    },
    {
      "authors": [
        "K. Gade",
        "S.C. Geyik",
        "K. Kenthapadi",
        "V. Mithal",
        "A. Taly"
      ],
      "title": "Explainable AI in industry",
      "venue": "Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. pp. 3203\u20133204",
      "year": 2019
    },
    {
      "authors": [
        "C.F. Gauss"
      ],
      "title": "Theoria motus corporum coelestium in sectionibus conicis solem ambientium, vol",
      "venue": "7. Perthes et Besser",
      "year": 1809
    },
    {
      "authors": [
        "A. Gelman",
        "J. Hill"
      ],
      "title": "Data analysis using regression and multilevel/hierarchical models",
      "venue": "Cambridge university press",
      "year": 2006
    },
    {
      "authors": [
        "A. Goldstein",
        "A. Kapelner",
        "J. Bleich",
        "E. Pitkin"
      ],
      "title": "Peeking inside the black box: Visualizing statistical learning with plots of individual conditional expectation",
      "venue": "Journal of Computational and Graphical Statistics 24(1), 44\u201365",
      "year": 2015
    },
    {
      "authors": [
        "B.M. Greenwell",
        "B.C. Boehmke",
        "A.J. McCarthy"
      ],
      "title": "A simple and effective modelbased variable importance measure",
      "venue": "arXiv preprint arXiv:1805.04755",
      "year": 2018
    },
    {
      "authors": [
        "R. Guidotti",
        "A. Monreale",
        "S. Ruggieri",
        "F. Turini",
        "F. Giannotti",
        "D. Pedreschi"
      ],
      "title": "A survey of methods for explaining black box models",
      "venue": "ACM computing surveys (CSUR) 51(5), 1\u201342",
      "year": 2018
    },
    {
      "authors": [
        "M. Hall",
        "D. Harborne",
        "R. Tomsett",
        "V. Galetic",
        "S. Quintana-Amate",
        "A. Nottle",
        "A. Preece"
      ],
      "title": "A systematic method to understand requirements for explainable AI(XAI) systems",
      "venue": "Proceedings of the IJCAI Workshop on eXplainable Artificial Intelligence (XAI 2019), Macau, China",
      "year": 2019
    },
    {
      "authors": [
        "P. Hall",
        "N. Gill",
        "M. Kurka",
        "W. Phan"
      ],
      "title": "Machine learning interpretability with h2o driverless AI",
      "venue": "H2O. ai. URL: http://docs. h2o. ai/driverless-ai/lateststable/docs/booklets/MLIBooklet. pdf",
      "year": 2017
    },
    {
      "authors": [
        "A. Hapfelmeier",
        "T. Hothorn",
        "K. Ulm",
        "C. Strobl"
      ],
      "title": "A new variable importance measure for random forests with missing data",
      "venue": "Statistics and Computing 24(1), 21\u201334",
      "year": 2014
    },
    {
      "authors": [
        "T.J. Hastie",
        "R.J. Tibshirani"
      ],
      "title": "Generalized additive models, vol",
      "venue": "43. CRC press",
      "year": 1990
    },
    {
      "authors": [
        "S. Hauenstein",
        "S.N. Wood",
        "C.F. Dormann"
      ],
      "title": "Computing AIC for black-box models using generalized degrees of freedom: A comparison with cross-validation",
      "venue": "Communications in Statistics-Simulation and Computation 47(5), 1382\u20131396",
      "year": 2018
    },
    {
      "authors": [
        "V. Haunschmid",
        "E. Manilow",
        "G. Widmer"
      ],
      "title": "audioLIME: Listenable explanations using source separation",
      "venue": "arXiv preprint arXiv:2008.00582",
      "year": 2020
    },
    {
      "authors": [
        "M.L. Head",
        "L. Holman",
        "R. Lanfear",
        "A.T. Kahn",
        "M.D. Jennions"
      ],
      "title": "The extent and consequences of p-hacking in science",
      "venue": "PLoS Biol 13(3), e1002106",
      "year": 2015
    },
    {
      "authors": [
        "R.R. Hoffman",
        "S.T. Mueller",
        "G. Klein",
        "J. Litman"
      ],
      "title": "Metrics for explainable AI: Challenges and prospects",
      "venue": "arXiv preprint arXiv:1812.04608",
      "year": 2018
    },
    {
      "authors": [
        "G. Hooker"
      ],
      "title": "Generalized functional anova diagnostics for high-dimensional functions of dependent variables",
      "venue": "Journal of Computational and Graphical Statistics 16(3), 709\u2013732",
      "year": 2007
    },
    {
      "authors": [
        "G. Hooker",
        "L. Mentch"
      ],
      "title": "Please stop permuting features: An explanation and alternatives",
      "venue": "arXiv preprint arXiv:1905.03151",
      "year": 2019
    },
    {
      "authors": [
        "T. Hothorn",
        "K. Hornik",
        "A. Zeileis"
      ],
      "title": "ctree: Conditional inference trees",
      "venue": "The Comprehensive R Archive Network 8",
      "year": 2015
    },
    {
      "authors": [
        "L. Hu",
        "J. Chen",
        "V.N. Nair",
        "A. Sudjianto"
      ],
      "title": "Locally interpretable models and effects based on supervised partitioning (LIME-SUP)",
      "venue": "arXiv preprint arXiv:1806.00663",
      "year": 2018
    },
    {
      "authors": [
        "J. Huysmans",
        "K. Dejaeger",
        "C. Mues",
        "J. Vanthienen",
        "B. Baesens"
      ],
      "title": "An empirical evaluation of the comprehensibility of decision table, tree and rule based predictive models",
      "venue": "Decision Support Systems 51(1), 141\u2013154",
      "year": 2011
    },
    {
      "authors": [
        "H. Ishwaran",
        "U.B. Kogalur",
        "E.Z. Gorodeski",
        "A.J. Minn",
        "M.S. Lauer"
      ],
      "title": "Highdimensional variable selection for survival data",
      "venue": "Journal of the American Statistical Association 105(489), 205\u2013217",
      "year": 2010
    },
    {
      "authors": [
        "H Ishwaran"
      ],
      "title": "Variable importance in binary regression trees and forests",
      "venue": "Electronic Journal of Statistics 1, 519\u2013537",
      "year": 2007
    },
    {
      "authors": [
        "D. Janzing",
        "L. Minorics",
        "P. Bl\u00f6baum"
      ],
      "title": "Feature relevance quantification in explainable AI: A causality problem",
      "venue": "arXiv preprint arXiv:1910.13413",
      "year": 2019
    },
    {
      "authors": [
        "J. Klaise",
        "A. Van Looveren",
        "G. Vacanti",
        "A. Coca"
      ],
      "title": "Alibi: Algorithms for monitoring and explaining machine learning models",
      "venue": "URL https://github. com/SeldonIO/alibi",
      "year": 2020
    },
    {
      "authors": [
        "P.W. Koh",
        "P. Liang"
      ],
      "title": "Understanding black-box predictions via influence functions",
      "venue": "arXiv preprint arXiv:1703.04730",
      "year": 2017
    },
    {
      "authors": [
        "G. K\u00f6nig",
        "C. Molnar",
        "B. Bischl",
        "M. Grosse-Wentrup"
      ],
      "title": "Relative feature importance",
      "venue": "arXiv preprint arXiv:2007.08283",
      "year": 2020
    },
    {
      "authors": [
        "S. Krishnan",
        "E. Wu"
      ],
      "title": "Palm: Machine learning explanations for iterative debugging",
      "venue": "Proceedings of the 2nd Workshop on Human-In-the-Loop Data Analytics. pp. 1\u20136",
      "year": 2017
    },
    {
      "authors": [
        "I.E. Kumar",
        "S. Venkatasubramanian",
        "C. Scheidegger",
        "S. Friedler"
      ],
      "title": "Problems with Shapley-value-based explanations as feature importance measures",
      "venue": "arXiv preprint arXiv:2002.11097",
      "year": 2020
    },
    {
      "authors": [
        "T. Laugel",
        "M.J. Lesot",
        "C. Marsala",
        "X. Renard",
        "M. Detyniecki"
      ],
      "title": "The dangers of post-hoc interpretability: Unjustified counterfactual explanations",
      "venue": "arXiv preprint arXiv:1907.09294",
      "year": 2019
    },
    {
      "authors": [
        "A.M. Legendre"
      ],
      "title": "Nouvelles m\u00e9thodes pour la d\u00e9termination des orbites des com\u00e8tes",
      "venue": "F. Didot",
      "year": 1805
    },
    {
      "authors": [
        "J. Lei",
        "M. G\u2019Sell",
        "A. Rinaldo",
        "R.J. Tibshirani",
        "L. Wasserman"
      ],
      "title": "Distribution-free predictive inference for regression",
      "venue": "Journal of the American Statistical Association 113(523), 1094\u20131111",
      "year": 2018
    },
    {
      "authors": [
        "B. Letham",
        "C. Rudin",
        "T.H. McCormick",
        "D Madigan"
      ],
      "title": "Interpretable classifiers using rules and bayesian analysis: Building a better stroke prediction model",
      "venue": "The Annals of Applied Statistics 9(3), 1350\u20131371",
      "year": 2015
    },
    {
      "authors": [
        "Z.C. Lipton"
      ],
      "title": "The mythos of model interpretability",
      "venue": "Queue 16(3), 31\u201357",
      "year": 2018
    },
    {
      "authors": [
        "S.M. Lundberg",
        "G.G. Erion",
        "S.I. Lee"
      ],
      "title": "Consistent individualized feature attribution for tree ensembles",
      "venue": "arXiv preprint arXiv:1802.03888",
      "year": 2018
    },
    {
      "authors": [
        "S.M. Lundberg",
        "S.I. Lee"
      ],
      "title": "A unified approach to interpreting model predictions",
      "venue": "Advances in neural information processing systems. pp. 4765\u20134774",
      "year": 2017
    },
    {
      "authors": [
        "S. Ma",
        "R. Tourani"
      ],
      "title": "Predictive and causal implications of using Shapley value for model interpretation",
      "venue": "Proceedings of the 2020 KDD Workshop on Causal Discovery. pp. 23\u201338. PMLR",
      "year": 2020
    },
    {
      "authors": [
        "T. Miller"
      ],
      "title": "Explanation in artificial intelligence: Insights from the social sciences",
      "venue": "Artificial Intelligence 267, 1\u201338",
      "year": 2019
    },
    {
      "authors": [
        "Y. Ming",
        "H. Qu",
        "E. Bertini"
      ],
      "title": "Rulematrix: Visualizing and understanding classifiers with rules",
      "venue": "IEEE transactions on visualization and computer graphics 25(1), 342\u2013352",
      "year": 2018
    },
    {
      "authors": [
        "S. Mohseni",
        "E.D. Ragan"
      ],
      "title": "A human-grounded evaluation benchmark for local explanations of machine learning",
      "venue": "arXiv preprint arXiv:1801.05075",
      "year": 2018
    },
    {
      "authors": [
        "S. Mohseni",
        "N. Zarei",
        "E.D. Ragan"
      ],
      "title": "A multidisciplinary survey and framework for design and evaluation of explainable AI systems",
      "venue": "arXiv pp. arXiv\u20131811",
      "year": 2018
    },
    {
      "authors": [
        "C. Molnar"
      ],
      "title": "Interpretable Machine Learning (2019), https://christophm",
      "venue": "github.io/interpretable-ml-book/",
      "year": 2019
    },
    {
      "authors": [
        "C. Molnar",
        "B. Bischl",
        "G. Casalicchio"
      ],
      "title": "iml: An R package for interpretable machine learning",
      "venue": "JOSS 3(26), 786",
      "year": 2018
    },
    {
      "authors": [
        "C. Molnar",
        "G. Casalicchio",
        "B. Bischl"
      ],
      "title": "Quantifying model complexity via functional decomposition for better post-hoc interpretability",
      "venue": "Joint European Conference on Machine Learning and Knowledge Discovery in Databases. pp. 193\u2013204. Springer",
      "year": 2019
    },
    {
      "authors": [
        "C. Molnar",
        "G. K\u00f6nig",
        "B. Bischl",
        "G. Casalicchio"
      ],
      "title": "Model-agnostic feature importance and effects with dependent features\u2013a conditional subgroup approach",
      "venue": "arXiv preprint arXiv:2006.04628",
      "year": 2020
    },
    {
      "authors": [
        "C. Molnar",
        "G. K\u00f6nig",
        "J. Herbinger",
        "T. Freiesleben",
        "S. Dandl",
        "C.A. Scholbeck",
        "G. Casalicchio",
        "M. Grosse-Wentrup",
        "B. Bischl"
      ],
      "title": "Pitfalls to avoid when interpreting machine learning models",
      "venue": "arXiv preprint arXiv:2007.04131",
      "year": 2020
    },
    {
      "authors": [
        "G. Montavon",
        "S. Lapuschkin",
        "A. Binder",
        "W. Samek",
        "K.R. M\u00fcller"
      ],
      "title": "Explaining nonlinear classification decisions with deep taylor decomposition",
      "venue": "Pattern Recognition 65, 211\u2013222",
      "year": 2017
    },
    {
      "authors": [
        "R.K. Mothilal",
        "A. Sharma",
        "C. Tan"
      ],
      "title": "Explaining machine learning classifiers through diverse counterfactual explanations",
      "venue": "Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency. pp. 607\u2013617",
      "year": 2020
    },
    {
      "authors": [
        "W.J. Murdoch",
        "C. Singh",
        "K. Kumbier",
        "R. Abbasi-Asl",
        "B. Yu"
      ],
      "title": "Definitions, methods, and applications in interpretable machine learning",
      "venue": "Proceedings of the National Academy of Sciences 116(44), 22071\u201322080",
      "year": 2019
    },
    {
      "authors": [
        "H. Nori",
        "S. Jenkins",
        "P. Koch",
        "R. Caruana"
      ],
      "title": "Interpretml: A unified framework for machine learning interpretability",
      "venue": "arXiv preprint arXiv:1909.09223",
      "year": 2019
    },
    {
      "authors": [
        "C. Olah",
        "A. Mordvintsev",
        "L. Schubert"
      ],
      "title": "Feature visualization",
      "venue": "Distill",
      "year": 2017
    },
    {
      "authors": [
        "A. Paluszynska",
        "P. Biecek",
        "Y. Jiang"
      ],
      "title": "randomForestExplainer: Explaining and Visualizing Random Forests in Terms of Variable Importance (2020), https:// CRAN.R-project.org/package=randomForestExplainer, r package version",
      "year": 2020
    },
    {
      "authors": [
        "M. Philipp",
        "T. Rusch",
        "K. Hornik",
        "C. Strobl"
      ],
      "title": "Measuring the stability of results from supervised statistical learning",
      "venue": "Journal of Computational and Graphical Statistics 27(4), 685\u2013700",
      "year": 2018
    },
    {
      "authors": [
        "F. Poursabzi-Sangdeh",
        "D.G. Goldstein",
        "J.M. Hofman",
        "J.W. Vaughan",
        "H. Wallach"
      ],
      "title": "Manipulating and measuring model interpretability",
      "venue": "arXiv preprint arXiv:1802.07810",
      "year": 2018
    },
    {
      "authors": [
        "A. Preece",
        "D. Harborne",
        "D. Braines",
        "R. Tomsett",
        "S. Chakraborty"
      ],
      "title": "Stakeholders in explainable AI",
      "venue": "arXiv preprint arXiv:1810.00184",
      "year": 2018
    },
    {
      "authors": [
        "N. Puri",
        "P. Gupta",
        "P. Agarwal",
        "S. Verma",
        "B. Krishnamurthy"
      ],
      "title": "Magix: Model agnostic globally interpretable explanations",
      "venue": "arXiv preprint arXiv:1706.07160",
      "year": 2017
    },
    {
      "authors": [
        "L.A.J. Quetelet"
      ],
      "title": "Recherches sur la population, les naissances, les d\u00e9c\u00e8s, les prisons, les d\u00e9p\u00f4ts de mendicit\u00e9, etc",
      "venue": "dans le royaume des Pays-Bas",
      "year": 1827
    },
    {
      "authors": [
        "R R Core Team"
      ],
      "title": "A Language and Environment for Statistical Computing. R Foundation for Statistical Computing, Vienna, Austria",
      "year": 2020
    },
    {
      "authors": [
        "J. Rabold",
        "H. Deininger",
        "M. Siebers",
        "U. Schmid"
      ],
      "title": "Enriching visual with verbal explanations for relational concepts\u2013combining LIME with Aleph",
      "venue": "Joint European Conference on Machine Learning and Knowledge Discovery in Databases. pp. 180\u2013192. Springer",
      "year": 2019
    },
    {
      "authors": [
        "J. Rabold",
        "M. Siebers",
        "U. Schmid"
      ],
      "title": "Explaining black-box classifiers with ilp\u2013 empowering LIME with aleph to approximate non-linear decisions with relational rules",
      "venue": "International Conference on Inductive Logic Programming. pp. 105\u2013117. Springer",
      "year": 2018
    },
    {
      "authors": [
        "A.H.A. Rahnama",
        "H. Bostr\u00f6m"
      ],
      "title": "A study of data and label shift in the LIME framework",
      "venue": "arXiv preprint arXiv:1910.14421",
      "year": 2019
    },
    {
      "authors": [
        "M.T. Ribeiro",
        "S. Singh",
        "C. Guestrin"
      ],
      "title": " why should i trust you?\u201d explaining the predictions of any classifier",
      "venue": "Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining. pp. 1135\u20131144",
      "year": 2016
    },
    {
      "authors": [
        "A. Rosenfeld",
        "A. Richardson"
      ],
      "title": "Explainability in human\u2013agent systems",
      "venue": "Autonomous Agents and Multi-Agent Systems 33(6), 673\u2013705",
      "year": 2019
    },
    {
      "authors": [
        "W. Samek",
        "K.R. M\u00fcller"
      ],
      "title": "Towards explainable artificial intelligence",
      "venue": "Explainable AI: interpreting, explaining and visualizing deep learning, pp. 5\u201322. Springer",
      "year": 2019
    },
    {
      "authors": [
        "F. Santosa",
        "W.W. Symes"
      ],
      "title": "Linear inversion of band-limited reflection seismograms",
      "venue": "SIAM Journal on Scientific and Statistical Computing 7(4), 1307\u20131330",
      "year": 1986
    },
    {
      "authors": [
        "R.E. Schapire"
      ],
      "title": "The strength of weak learnability",
      "venue": "Machine learning 5(2), 197\u2013227",
      "year": 1990
    },
    {
      "authors": [
        "J. Schmidhuber"
      ],
      "title": "Deep learning in neural networks: An overview",
      "venue": "Neural networks 61, 85\u2013117",
      "year": 2015
    },
    {
      "authors": [
        "B. Sch\u00f6lkopf"
      ],
      "title": "Causality for machine learning",
      "venue": "arXiv preprint arXiv:1911.10500",
      "year": 2019
    },
    {
      "authors": [
        "G Schwarz"
      ],
      "title": "Estimating the dimension of a model",
      "venue": "The annals of statistics 6(2), 461\u2013464",
      "year": 1978
    },
    {
      "authors": [
        "S.M. Shankaranarayana",
        "D. Runje"
      ],
      "title": "ALIME: Autoencoder based approach for local interpretability",
      "venue": "International Conference on Intelligent Data Engineering and Automated Learning. pp. 454\u2013463. Springer",
      "year": 2019
    },
    {
      "authors": [
        "L.S. Shapley"
      ],
      "title": "A value for n-person games",
      "venue": "Contributions to the Theory of Games 2(28), 307\u2013317",
      "year": 1953
    },
    {
      "authors": [
        "A. Shrikumar",
        "P. Greenside",
        "A. Shcherbina",
        "A. Kundaje"
      ],
      "title": "Not just a black box: Learning important features through propagating activation differences",
      "venue": "arXiv preprint arXiv:1605.01713",
      "year": 2016
    },
    {
      "authors": [
        "J. Sill"
      ],
      "title": "Monotonic networks",
      "venue": "Advances in neural information processing systems. pp. 661\u2013667",
      "year": 1998
    },
    {
      "authors": [
        "K. Simonyan",
        "A. Vedaldi",
        "A. Zisserman"
      ],
      "title": "Deep inside convolutional networks: Visualising image classification models and saliency maps",
      "venue": "arXiv preprint arXiv:1312.6034",
      "year": 2013
    },
    {
      "authors": [
        "S.M. Stigler"
      ],
      "title": "The history of statistics: The measurement of uncertainty before 1900",
      "venue": "Harvard University Press",
      "year": 1986
    },
    {
      "authors": [
        "C. Strobl",
        "A.L. Boulesteix",
        "T. Kneib",
        "T. Augustin",
        "A. Zeileis"
      ],
      "title": "Conditional variable importance for random forests",
      "venue": "BMC bioinformatics 9(1), 307",
      "year": 2008
    },
    {
      "authors": [
        "C. Strobl",
        "A.L. Boulesteix",
        "A. Zeileis",
        "T. Hothorn"
      ],
      "title": "Bias in random forest variable importance measures: Illustrations, sources and a solution",
      "venue": "BMC bioinformatics 8(1), 25",
      "year": 2007
    },
    {
      "authors": [
        "E. \u0160trumbelj",
        "I. Kononenko"
      ],
      "title": "Explaining prediction models and individual predictions with feature contributions",
      "venue": "Knowledge and information systems 41(3), 647\u2013665",
      "year": 2014
    },
    {
      "authors": [
        "M. Sundararajan",
        "A. Najmi"
      ],
      "title": "The many Shapley values for model explanation",
      "venue": "arXiv preprint arXiv:1908.08474",
      "year": 2019
    },
    {
      "authors": [
        "M. Sundararajan",
        "A. Taly",
        "Q. Yan"
      ],
      "title": "Axiomatic attribution for deep networks",
      "venue": "arXiv preprint arXiv:1703.01365",
      "year": 2017
    },
    {
      "authors": [
        "R. Tibshirani"
      ],
      "title": "Regression shrinkage and selection via the lasso",
      "venue": "Journal of the Royal Statistical Society: Series B (Methodological) 58(1), 267\u2013288",
      "year": 1996
    },
    {
      "authors": [
        "G. Tolomei",
        "F. Silvestri",
        "A. Haines",
        "M. Lalmas"
      ],
      "title": "Interpretable predictions of tree-based ensembles via actionable feature tweaking",
      "venue": "Proceedings of the 23rd ACM SIGKDD international conference on knowledge discovery and data mining. pp. 465\u2013474",
      "year": 2017
    },
    {
      "authors": [
        "B. Ustun",
        "C. Rudin"
      ],
      "title": "Supersparse linear integer models for optimized medical scoring systems",
      "venue": "Machine Learning 102(3), 349\u2013391",
      "year": 2016
    },
    {
      "authors": [
        "B. Ustun",
        "A. Spangher",
        "Y. Liu"
      ],
      "title": "Actionable recourse in linear classification",
      "venue": "Proceedings of the Conference on Fairness, Accountability, and Transparency. pp. 10\u201319",
      "year": 2019
    },
    {
      "authors": [
        "V. Vapnik",
        "A. Chervonenkis"
      ],
      "title": "Theory of pattern recognition",
      "year": 1974
    },
    {
      "authors": [
        "G. Vilone",
        "L. Longo"
      ],
      "title": "Explainable artificial intelligence: a systematic review",
      "venue": "arXiv preprint arXiv:2006.00093",
      "year": 2020
    },
    {
      "authors": [
        "G. Visani",
        "E. Bagli",
        "F. Chesani"
      ],
      "title": "Optilime: Optimized LIME explanations for diagnostic computer algorithms",
      "venue": "arXiv preprint arXiv:2006.05714",
      "year": 2020
    },
    {
      "authors": [
        "S. Wachter",
        "B. Mittelstadt",
        "C. Russell"
      ],
      "title": "Counterfactual explanations without opening the black box: Automated decisions and the gdpr",
      "venue": "Harv. JL & Tech. 31, 841",
      "year": 2017
    },
    {
      "authors": [
        "F. Wang",
        "C. Rudin"
      ],
      "title": "Falling rule lists",
      "venue": "Artificial Intelligence and Statistics. pp. 1013\u20131022",
      "year": 2015
    },
    {
      "authors": [
        "D.S. Watson",
        "M.N. Wright"
      ],
      "title": "Testing conditional independence in supervised learning algorithms",
      "venue": "arXiv preprint arXiv:1901.09917",
      "year": 2019
    },
    {
      "authors": [
        "P. Wei",
        "Z. Lu",
        "J. Song"
      ],
      "title": "Variable importance analysis: a comprehensive review",
      "venue": "Reliability Engineering & System Safety 142, 399\u2013432",
      "year": 2015
    },
    {
      "authors": [
        "J. Wexler",
        "M. Pushkarna",
        "T. Bolukbasi",
        "M. Wattenberg",
        "F. Vi\u00e9gas",
        "J. Wilson"
      ],
      "title": "The what-if tool: Interactive probing of machine learning models",
      "venue": "IEEE transactions on visualization and computer graphics 26(1), 56\u201365",
      "year": 2019
    },
    {
      "authors": [
        "B.D. Williamson",
        "J. Feng"
      ],
      "title": "Efficient nonparametric statistical inference on population feature importance using Shapley values",
      "venue": "arXiv preprint arXiv:2006.09481",
      "year": 2020
    },
    {
      "authors": [
        "A. Zeileis",
        "T. Hothorn",
        "K. Hornik"
      ],
      "title": "Model-based recursive partitioning",
      "venue": "Journal of Computational and Graphical Statistics 17(2), 492\u2013514",
      "year": 2008
    },
    {
      "authors": [
        "M.D. Zeiler",
        "R. Fergus"
      ],
      "title": "Visualizing and understanding convolutional networks",
      "venue": "European conference on computer vision. pp. 818\u2013833. Springer",
      "year": 2014
    },
    {
      "authors": [
        "Q. Zhang",
        "Y. Nian Wu",
        "S.C. Zhu"
      ],
      "title": "Interpretable convolutional neural networks",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. pp. 8827\u20138836",
      "year": 2018
    },
    {
      "authors": [
        "Q. Zhou",
        "F. Liao",
        "C. Mou",
        "P. Wang"
      ],
      "title": "Measuring interpretability for different types of machine learning models",
      "venue": "Pacific-Asia Conference on Knowledge Discovery and Data Mining. pp. 295\u2013308",
      "year": 2018
    },
    {
      "authors": [
        "H. Zou",
        "T. Hastie"
      ],
      "title": "Regularization and variable selection via the elastic net",
      "venue": "Journal of the royal statistical society: series B (statistical methodology) 67(2), 301\u2013320",
      "year": 2005
    }
  ],
  "sections": [
    {
      "text": "Keywords: Interpretable Machine Learning \u00b7 Explainable Artificial Intelligence"
    },
    {
      "heading": "1 Introduction",
      "text": "Interpretability is often a deciding factor when a machine learning (ML) model is used in a product, a decision process, or in research. Interpretable machine learning (IML)1 methods can be used to discover knowledge, to debug or justify\n? This project is funded by the Bavarian State Ministry of Science and the Arts and coordinated by the Bavarian Research Institute for Digital Transformation (bidt) and supported by the German Federal Ministry of Education and Research (BMBF) under Grant No. 01IS18036A. The authors of this work take full responsibilities for its content. 1 Sometimes the term Explainable AI is used.\nar X\niv :2\n01 0.\n09 33\n7v 1\n[ st\nat .M\nL ]\n1 9\nO ct\nthe model and its predictions, and to control and improve the model [1]. In this paper, we take a look at the historical building blocks of IML and give an overview of methods to interpret models. We argue that IML has reached a state of readiness, but some challenges remain."
    },
    {
      "heading": "2 A Brief History of IML",
      "text": "A lot of IML research happened in the last couple of years. But learning interpretable models from data has a much longer tradition. Linear regression models were used by Gauss, Legendre, and Quetelet [109,64,37,90] as early as the beginning of the 19th century and have since then grown into a vast array of regression analysis tools [115,98], for example, generalized additive models [45] and elastic net [132]. The philosophy behind these statistical models is usually to make certain distributional assumptions or to restrict the model complexity beforehand and thereby imposing intrinsic interpretability of the model.\nIn ML, a slightly different modeling approach is pursued. Instead of restricting the model complexity beforehand, ML algorithms usually follow a non-linear, non-parametric approach, where model complexity is controlled through one or more hyperparameters and selected via cross-validation. This flexibility often results in less interpretable models with good predictive performance. A lot of ML research began in the second half of the 20th century with research on, for example, support vector machines in 1974 [119], early important work on neural networks in the 1960s [100], and boosting in 1990 [99]. Rule-based ML, which covers decision rules and decision trees, has been an active research area since the middle of the 20th century [35].\nWhile ML algorithms usually focus on predictive performance, work on interpretability in ML \u2013 although underexplored \u2013 has existed for many years. The built-in feature importance measure of random forests [13] was one of the important IML milestones.2 In the 2010s came the deep learning hype, after a deep neural network won the ImageNet challenge. A few years after that, the IML field really took off (around 2015), judging by the frequency of the search terms \u201dInterpretable Machine Learning\u201d and \u201dExplainable AI\u201d on Google (Figure 1, right) and papers published with these terms (Figure 1, left). Since then, many model-agnostic explanation methods have been introduced, which work for different types of ML models. But also model-specific explanation methods have been developed, for example, to interpret deep neural networks or tree ensembles. Regression analysis and rule-based ML remain important and active research areas to this day and are blending together (e.g., model-based trees [128], RuleFit [33]). Many extensions of the linear regression model exist [45,25,38] and new extensions are proposed until today [26,14,27,117]. Rule-based ML also remains an active area of research (for example, [123,66,52]). Both regression models and\n2 The random forest paper has been cited over 60,000 times (Google Scholar; September 2020) and there are many papers improving the importance measure ([110,111,44,56]) which are also cited frequently.\nrule-based ML serve as stand-alone ML algorithms, but also as building blocks for many IML approaches."
    },
    {
      "heading": "3 Today",
      "text": "IML has reached a first state of readiness. Research-wise, the field is maturing in terms of methods surveys [75,41,120,96,1,6,23,15], further consolidation of terms and knowledge [42,22,82,97,88,17], and work about defining interpretability or evaluation of IML methods [74,73,95,49]. We have a better understanding of weaknesses of IML methods in general [75,79], but also specifically for methods such as permutation feature importance [51,110,7,111], Shapley values [57,113], counterfactual explanations [63], partial dependence plots [51,50,7] and saliency maps [2]. Open source software with implementations of various IML methods is available, for example, iml [76] and DALEX [11] for R [91] and Alibi [58] and InterpretML [83] for Python. Regulation such as GDPR and the need for ML trustability, transparency and fairness have sparked a discussion around further needs of interpretability [122]. IML has also arrived in industry [36], there are startups that focus on ML interpretability and also big tech companies offer software [126,8,43]."
    },
    {
      "heading": "4 IML Methods",
      "text": "We distinguish IML methods by whether they analyze model components, model sensitivity3, or surrogate models, illustrated in Figure 4.4\n3 Not to be confused with the research field of sensitivity analysis, which studies the uncertainty of outputs in mathematical models and systems. There are methodological overlaps (e.g., Shapley values), but also differences in methods and how input data distributions are handled. 4 Some surveys distinguish between ante-hoc (or transparent design, white-box models, inherently interpretable model) and post-hoc IML method, depending on whether"
    },
    {
      "heading": "4.1 Analyzing Components of Interpretable Models",
      "text": "In order to analyze components of a model, it needs to be decomposable into parts that we can interpret individually. However, it is not necessarily required that the user understands the model in its entirety (simulatability [82]). Component analysis is always model-specific, because it is tied to the structure of the model.\nInherently interpretable models are models with (learned) structures and (learned) parameters which can be assigned a certain interpretation. In this context, linear regression models, decision trees and decision rules are considered to be interpretable [30,54]. Linear regression models can be interpreted by analyzing components: The model structure, a weighted sum of features, allows to interpret the weights as the effects that the features have on the prediction.\nDecision trees and other rule-based ML models have a learned structure (e.g.,\u201cIF feature x1 > 0 and feature x2 \u2208 {A,B}, THEN predict 0.6\u201d). We can interpret the learned structure to trace how the model makes predictions.\nThis only works up to a certain point in high-dimensional scenarios. Linear regression models with hundreds of features and complex interaction terms or deep decision trees are not that interpretable anymore. Some approaches aim to reduce the parts to be interpreted. For example, LASSO [98,115] shrinks the coefficients in a linear model so that many of them become zero, and pruning techniques shorten trees."
    },
    {
      "heading": "4.2 Analyzing Components of More Complex Models",
      "text": "With a bit more effort, we can also analyze components of more complex blackbox models. 5 For example, the abstract features learned by a deep convolutional neural network (CNN) can be visualized by finding or generating images that\ninterpretability is considered at model design and training or after training, leaving the (black-box) model unchanged. Another category separates model-agnostic and model-specific methods. 5 This blurs the line between an \u201cinherently interpretable\u201d and a \u201cblack-box\u201d model.\nactivate a feature map of the CNN [84]. For the random forest, the minimal depth distribution [85,55] and the Gini importance [13] analyze the structure of the trees of the forest and can be used to quantify feature importance. Some approaches aim to make the parts of a model more interpretable with, for example, a monotonicity constraint [106] or a modified loss function for disentangling concepts learned by a convolutional neural network [130].\nIf an ML algorithm is well understood and frequently used in a community, like random forests in ecology research [19], model component analysis can be the correct tool, but it has the obvious disadvantage that it is tied to that specific model. And it does not combine well with the common model selection approach in ML, where one usually searches over a large class of different ML models via cross-validation."
    },
    {
      "heading": "4.3 Explaining Individual Predictions",
      "text": "Methods that study the sensitivity of an ML model are mostly model-agnostic and work by manipulating input data and analyzing the respective model predictions. These IML methods often treat the ML model as a closed system that receives feature values as an input and produces a prediction as output. We distinguish between local and global explanations.\nLocal methods explain individual predictions of ML models. Local explanation methods have received much attention and there has been a lot of innovation in the last years. Popular local IML methods are Shapley values [69,112] and counterfactual explanations [122,20,81,116,118]. Counterfactual explanations explain predictions in the form of what-if scenarios, which builds on a rich tradition in philosophy [108]. According to findings in the social sciences [71], counterfactual explanations are \u201cgood\u201d explanations because they are contrastive and focus on a few reasons. A different approach originates from collaborative game theory: The Shapley values [104] provide an answer on how to fairly share a payout among the players of a collaborative game. The collaborative game idea can be applied to ML where features (i.e., the players) collaborate to make a prediction (i.e., the payout) [112,69,68].\nSome IML methods rely on model-specific knowledge to analyze how changes in the input features change the output. Saliency maps, an interpretation method specific for CNNs, make use of the network gradients to explain individual classifications. The explanations are in the form of heatmaps that show how changing a pixel can change the classification. The saliency map methods differ in how they backpropagate [114,69,80,107,105]. Additionally, model-agnostic versions [95,69,129] exist for analyzing image classifiers."
    },
    {
      "heading": "4.4 Explaining Global Model Behavior",
      "text": "Global model-agnostic explanation methods are used to explain the expected model behavior, i.e., how the model behaves on average for a given dataset. A useful distinction of global explanations are feature importance and feature effect.\nFeature importance ranks features based on how relevant they were for the prediction. Permutation feature importance [28,16] is a popular importance measure, originally suggested for random forests [13]. Some importance measures rely on removing features from the training data and retraining the model [65]. An alternative are variance-based measures [40]. See [125] for an overview of importance measures.\nThe feature effect expresses how a change in a feature changes the predicted outcome. Popular feature effect plots are partial dependence plots [32], individual conditional expectation curves [39], accumulated local effect plots [7], and the functional ANOVA [50]. Analyzing influential data instances, inspired by statistics, provides a different view into the model and describes how influential a data point was for a prediction [59]."
    },
    {
      "heading": "4.5 Surrogate Models",
      "text": "Surrogate models6 are interpretable models designed to \u201ccopy\u201d the behavior of the ML model. The surrogate approach treats the ML model as a black-box and only requires the input and output data of the ML model (similar to sensitivity analysis) to train a surrogate ML model. However, the interpretation is based on analyzing components of the interpretable surrogate model. Many IML methods are surrogate model approaches [89,75,72,95,34,10,18,61] and differ, e.g., in the targeted ML model, the data sampling strategy, or the interpretable model that is used. There are also methods for extracting, e.g., decision rules from specific models based on their internal components such as neural network weights [5,9]. LIME [95] is an example of a local surrogate method that explains individual predictions by learning an interpretable model with data in proximity to the data point to be explained. Numerous extensions of LIME exist, which try to fix issues with the original method, extend it to other tasks and data, or analyze its properties [53,93,92,121,47,94,103,12]."
    },
    {
      "heading": "5 Challenges",
      "text": "This section presents an incomplete overview of challenges for IML, mostly based on [79]."
    },
    {
      "heading": "5.1 Statistical Uncertainty and Inference",
      "text": "Many IML methods such as permutation feature importance or Shapley values provide explanations without quantifying the uncertainty of the explanation. The model itself, but also its explanations, are computed from data and hence are subject to uncertainty. First research is working towards quantifying uncertainty of explanations, for example, for feature importance [124,28,4], layer-wise relevance propagation [24], and Shapley values [127].\n6 Surrogate models are related to knowledge distillation and the teacher-student model.\nIn order to infer meaningful properties of the underlying data generating process, we have to make structural or distributional assumptions. Whether it is a classical statistical model, an ML algorithm or an IML procedure, these assumptions should be clearly stated and we need better diagnostic tools to test them. If we want to prevent statistical testing problems such as p-hacking [48] to reappear in IML, we have to become more rigorous in studying and quantifying the uncertainty of IML methods. For example, most IML methods for feature importance are not adapted for multiple testing, which is a classic mistake in a statistical analysis."
    },
    {
      "heading": "5.2 Causal Interpretation",
      "text": "Ideally, a model should reflect the true causal structure of its underlying phenomena, to enable causal interpretations. Arguably, causal interpretation is usually the goal of modeling if ML is used in science. But most statistical learning procedures reflect mere correlation structures between features and analyze the surface of the data generation process instead of its true inherent structure. Such causal structures would also make models more robust against adversarial attacks [101,29], and more useful when used as a basis for decision making. Unfortunately, predictive performance and causality can be conflicting goals. For example, today\u2019s weather directly causes tomorrow\u2019s weather, but we might only have access to the feature \u201cwet ground\u201d. Using \u201cwet ground\u201d in the prediction model for \u201ctomorrow\u2019s weather\u201d is useful as it has information about \u201ctoday\u2019s weather\u201d, but we are not allowed to interpret it causally, because the confounder \u201ctoday\u2019s weather\u201d is missing from the ML model. Further research is needed to understand when we are allowed to make causal interpretations of an ML model. First steps have been made for permutation feature importance [60] and Shapley values [70]."
    },
    {
      "heading": "5.3 Feature Dependence",
      "text": "Feature dependence introduces problems with attribution and extrapolation. Attribution of importance and effects of features becomes difficult when features are, for example, correlated and therefore share information. Correlated features in random forests are preferred and attributed a higher importance [110,51]. Many sensitivity analysis based methods permute features. When the permuted feature has some dependence with another feature, this association is broken and the resulting data points extrapolate to areas outside the distribution. The ML model was never trained on such combinations and will likely not be confronted with similar data points in an application. Therefore, extrapolation can cause misleading interpretations. There have been attempts to \u201cfix\u201d permutation-based methods, by using a conditional permutation scheme that respects the joint distribution of the data [78,110,28,51]. The change from unconditional to conditional permutation changes the respective interpretation method [78,7], or, in worst case, can break it [57,113,62]."
    },
    {
      "heading": "5.4 Definition of Interpretability",
      "text": "A lack of definition for the term \u201dinterpretability\u201d is a common critique of the field [67,22]. How can we decide if a new method explains ML models better without a satisfying definition of interpretability? To evaluate the predictive performance of an ML model, we simply compute the prediction error on test data given the groundtruth label. To evaluate the interpretability of that same ML model is more difficult. We do not know what the groundtruth explanation looks like and have no straightforward way to quantify how interpretable a model is or how correct an explanation is. Instead of having one groundtruth explanation, various quantifiable aspects of interpretability are emerging [87,86,77,46,131,3,102,87,21,31].\nThe two main ways of evaluating interpretability are objective evaluations, which are mathematically quantifiable metrics, and human-centered evaluations, which involve studies with either domain experts or lay persons. Examples of aspects of interpretability are sparsity, interaction strength, fidelity (how well an explanation approximates the ML model), sensitivity to perturbations, and a user\u2019s ability to run a model on a given input (simulatability). The challenge ahead remains to establish a best practice on how to evaluate interpretation methods and the explanations they produce. Here, we should also look at the field of human-computer interaction."
    },
    {
      "heading": "5.5 More Challenges Ahead",
      "text": "We focused mainly on the methodological, mathematical challenges in a rather static setting, where a trained ML model and the data are assumed as given and fixed. But ML models are usually not used in a static and isolated way, but are embedded in some process or product, and interact with people. A more dynamic and holistic view of the entire process, from data collection to the final consumption of the explained prediction is needed. This includes thinking how to explain predictions to individuals with diverse knowledge and backgrounds and about the need of interpretability on the level of an institution or society in general. This covers a wide range of fields, such as human-computer interaction, psychology and sociology. To solve the challenges ahead, we believe that the field has to reach out horizontally \u2013 to other domains \u2013 and vertically \u2013 drawing from the rich research in statistics and computer science."
    }
  ],
  "year": 2020
}
