{"abstractText": "We present an experimentation platform for coalition situational understanding research that highlights capabilities in explainable artificial intelligence/machine learning (AI/ML) and integration of symbolic and subsymbolic AI/ML approaches for event processing. The Situational Understanding Explorer (SUE) platform is designed to be lightweight, to easily facilitate experiments and demonstrations, and open. We discuss our requirements to support coalition multi-domain operations with emphasis on asset interoperability and ad hoc human-machine teaming in a dense urban terrain setting. We describe the interface functionality and give examples of SUE applied to coalition situational understanding tasks.", "authors": [{"affiliations": [], "name": "Katie Barrett-Powell"}, {"affiliations": [], "name": "Jack Furby"}, {"affiliations": [], "name": "Liam Hiley"}, {"affiliations": [], "name": "Marc Roig Vilamala"}, {"affiliations": [], "name": "Harrison Taylor"}, {"affiliations": [], "name": "Federico Cerutti"}, {"affiliations": [], "name": "Alun Preece"}, {"affiliations": [], "name": "Tianwei Xing"}, {"affiliations": [], "name": "Luis Garcia"}, {"affiliations": [], "name": "Mani Srivastava"}, {"affiliations": [], "name": "Dave Braines"}], "id": "SP:7f3d4b0ebc161d99003794e6a183cf577cad589c", "references": [{"authors": ["S. Bach", "A. Binder", "G. Montavon", "F. Klauschen", "K.-R. M\u00fcller", "W. Samek"], "title": "On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation", "venue": "PloS One 10(7):e0130140.", "year": 2015}, {"authors": ["D. Braines", "F. Cerutti", "M.R. Vilamala", "M. Srivastava", "L. Kaplan", "A. Preece", "G. Pearson"], "title": "Towards human-agent knowledge fusion (hakf) in support of distributed coalition teams", "venue": "under review.", "year": 2020}, {"authors": ["M. Cummings"], "title": "Man versus machine or man + machine", "venue": "IEEE Intelligent Systems", "year": 2014}, {"authors": ["L. De Raedt", "A. Kimmig", "H. Toivonen"], "title": "ProbLog: A probabilistic Prolog and its application in link discovery", "venue": "IJCAI, 2462\u20132467.", "year": 2007}, {"authors": ["B.C. Dostal"], "title": "Enhancing situational understanding through the employment of unmanned aerial vehicles", "venue": "Army Transformation Taking Shape. . . Interim Brigade Combat Team Newsletter (01\u201318).", "year": 2007}, {"authors": ["D. Fierens", "G. Van den Broeck", "J. Renkens", "D. Shterionov", "B. Gutmann", "I. Thon", "G. Janssens", "L. De Raedt"], "title": "Inference and learning in probabilistic logic programs using weighted Boolean formulas", "venue": "Theory and Practice of Logic Programming 15(03):358\u2013401.", "year": 2015}, {"authors": ["M. Law", "A. Russo", "E. Bertino", "K. Broda", "J. Lobo"], "title": "Fastlas: Scalable inductive logic programming incorporating domain-specific optimisation criteria", "venue": "The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, 2877\u20132885. AAAI Press.", "year": 2020}, {"authors": ["A. Preece", "F. Cerutti", "D. Braines", "S. Chakraborty", "M. Srivastava"], "title": "Cognitive computing for coalition situational understanding", "venue": "First International Workshop on Distributed Analytics InfraStructure and Algorithms for Multi-Organization Federations (IEEE Smart World", "year": 2017}, {"authors": ["A. Preece", "D. Braines", "F. Cerutti", "T. Pham"], "title": "Explainable ai for intelligence augmentation in multi-domain operations", "venue": "Proc AAAI FSS-19: Artificial Intelligence in Government and Public Sector.", "year": 2019}, {"authors": ["A. Skarlatidis", "A. Artikis", "J. Filippou", "G. Paliouras"], "title": "A probabilistic logic programming event calculus", "venue": "Theory and Practice of Logic Programming 15(2):213\u2013245.", "year": 2015}, {"authors": ["K. Soomro", "A.R. Zamir", "M. Shah"], "title": "UCF101: A dataset of 101 human actions classes from videos in the wild", "venue": "preprint arXiv:1212.0402.", "year": 2012}, {"authors": ["H. Taylor", "L. Hiley", "J. Furby", "A. Preece", "D. Braines"], "title": "VADR: Discriminative multimodal explanations for situational understanding", "venue": "Proc 23rd International Conference on Information Fusion.", "year": 2020}, {"authors": ["R. Tomsett", "A. Preece", "D. Braines", "F. Cerutti", "S. Chakraborty", "M. Srivastava", "G. Pearson", "L. Kaplan"], "title": "Rapid trust calibration through interpretable and uncertainty-aware AI", "venue": "Patterns 1(3).", "year": 2020}, {"authors": ["D. Tran", "L. Bourdev", "R. Fergus", "L. Torresani", "M. Paluri"], "title": "Learning spatiotemporal features with 3D convolutional networks", "venue": "Proc IEEE International Conference on Computer Vision, 4489\u20134497.", "year": 2015}, {"authors": ["M.R. Vilamala", "L. Hiley", "Y. Hicks", "A. Preece", "F. Cerutti"], "title": "A pilot study on detecting violence in videos fusing proxy models", "venue": "Proc 22nd International Conference on Information Fusion.", "year": 2019}, {"authors": ["G. White", "S. Pierson", "B. Rivera", "M. Touma", "P. Sullivan", "D. Braines"], "title": "DAIS-ITA scenario", "venue": "Proc Artificial Intelligence and Machine Learning for Multi-Domain Operations Applications (SPIE Vol 11006). SPIE.", "year": 2019}], "sections": [{"heading": "Introduction", "text": "A key characteristic of multi-domain operations (MDO) (U.S. Army 2018) is that near-peer adversaries will be contesting all domains \u2014 from dense urban terrain to space and cyberspace \u2014 wherein those adversaries attempt to achieve stand-off by reducing allies\u2019 speed of recognition, decision and action, as well as by fracturing alliances through multiple means: diplomatic, economic, conventional and unconventional warfare, including information warfare. A critical requirement for allies is rapid and continuous integration of capabilities to collect, process, disseminate and exploit actionable information and intelligence. To achieve this, the MDO layered ISR concept envisions exploitation of \u2018an existing intelligence, surveillance, and reconnaissance (ISR) network developed with partners. . . that consists of overlapping systems of remote and autonomous sensors, human intelligence, and friendly special operations forces\u2019 ((U.S. Army 2018), pp.33\u201334).\nThis research was sponsored by the U.S. Army Research Laboratory and the UK Ministry of Defence under Agreement Number W911NF-16-3-0001. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. Army Research Laboratory, the U.S. Government, the UK Ministry of Defence or the UK Government. The U.S. and UK Governments are authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation hereon.\nMaximally utilising ISR assets in the contested MDO environment requires an ability to share resources among coalition partners in an open but controlled environment, with knowable levels of trust and assurance. Our research is focused on three facets of MDO (Preece et al. 2019):\nEnhanced asset interoperability Here we focus specifically on assets based on artificial intelligence (AI) and machine learning (ML) technologies, both subsymbolic (e.g., deep neural networks) and symbolic (e.g., logic-based approaches). To achieve interoperability between these assets \u2014 as well as human assets \u2014 in a coalition MDO setting, we need subsymbolic AI/ML agents to be able to share uncertainty-aware representations of insights and knowledge that can then be communicated to humans and symbolic AI/ML assets; and we need to enable symbolic AI assets \u2014 and humans \u2014 to share inferences and insights to subsymbolic AI/ML agents.\nHuman-machine teaming Specifically we focus on ad hoc coalition teams involving humans and hybrid (subsymbolic and symbolic) AI/ML assets where there is a key need to rapidly build trust and confidence. Therefore, our work seeks to advance capabilities in explainable AI/ML to allow a human operative to \u2018calibrate their trust\u2019 in an AI/ML asset potentially provided by a different coalition partner (Tomsett et al. 2020). The purpose of human-machine teaming is to aim for each party to exploit the strengths of, and compensate for the weaknesses of, the other (Cummings 2014).\nDense urban terrain analytics Accelerating global rates of urbanisation and the emerging strategic importance of cities/megacities means that MDO operations will take place within dense urban terrain, giving rise to specific physical, cognitive, and operational characteristics. ISR will exploit and augment civilian infrastructure, for example, civilian closed circuit television (CCTV) cameras, augmented with processing for the detection and tracking of activities, and to support building pattern-of-life models. ISR tasks cannot necessarily plan in advance what collection and processing will be needed: analytics composition will be dynamic and context-specific, with continual re-provisioning and resource optimisation (White et al. 2019).\nOur concept of coalition situational understanding\nar X\niv :2\n01 0.\n14 38\n8v 2\n[ cs\n.A I]\n9 N\nov 2\n(CSU) (Preece et al. 2017) extends the notion of situational understanding (the \u2018product of applying analysis and judgment to the unit\u2019s situation awareness to determine the relationships of the factors present and form logical conclusions concerning threats to the force or mission accomplishment, opportunities for mission accomplishment, and gaps in information\u2019 (Dostal 2007)) to a coalition MDO context, specifically featuring by a layered architecture consisting of sensing, information representation and fusion, and prediction/reasoning assets (Figure 1). Assets at all layers are distributed across the network and may be provided by any coalition partner. Moreover, CSU emphasises real-time analytics and temporal information processing, exploitation of multimodal data sources (e.g., imagery, acoustic, open source media), and the decentralised placement of assets at or near the network edge.\nThe focus of this paper is on an experimentation platform for our CSU research, that specifically highlights capabilities in (1) explainable AI/ML for asset trust calibration and (2) integration of symbolic and subsymbolic AI/ML assets for event processing. This Situational Understanding Explorer (SUE) platform has been designed to be very lightweight (to more easily facilitate experiments and demonstrations) and open1. The next section discusses the requirements for SUE; subsequent sections describe the interface functionality and give examples of SUE applied to CSU tasks in line with (1) and (2) above."}, {"heading": "SUE Requirements", "text": "This section summarises the main requirements for SUE, which map to the main functional elements of the platform.\nSensors Corresponding to the data source layer in Figure 1, SUE must represent sensors of various kinds placed in the physical environment. Sensors need to be associated with (owned by) multiple coalition partners. As noted above,\n1Open source: https://github.com/KBarrett-Powell/SUE\nour focus is on dense urban settings. Moreover, our initial focus has been on 2D representations given the ease of acquiring open source 2D mapping data; a 3D environment is under exploration.\nEvents As the main product of the AI/ML services in the higher layers in Figure 1, we focus on the detection of, and reasoning about, events. Events involve objects and actors, and are situated in time and space. A simple event is an event detected by a single sensor; a complex event is a spatiotemporal combination of multiple simple events from one or more sensors. SUE must visualise events in time and space and provide details of their components (e.g., objects, actors) and relationships (e.g., between simple and complex events).\nExplanations The AI/ML services need to be capable of providing explanations for their detection and reasoning decisions with respect to events. In previous work we assert that explanations should be layered to provide distinct kinds of information relating to system verification (that the decision was reached in the correct way) and validation (that it\u2019s the correct decision) (Preece et al. 2019). SUE needs to be open to plugging-in these different elements of explanations.\nUncertainty management A key element of our work in trust calibration is the management of uncertainty, including distinctions between aleatoric and epistemic uncertainty (Tomsett et al. 2020). Again, SUE needs to be open to visualising uncertainties generated by the various kinds of AI/ML model in relation to detection and inference of events.\nHuman-machine collaboration In relation to our need to explore human-machine teaming, especially where human operatives may have limited technical training, we need interface affordances that are widely familiar, e.g., via smartphone apps. For the initial version of SUE we focused on direct manipulation of map-based and visual elements, with control also afforded by a conversational interface.\nOnline and offline modes To support experimentation and demonstration, SUE is required to work both offline, displaying a pre-prepared temporal stream of events, or online, receiving \u2018live\u2019 events.\nIn developing the platform to meet these requirements, we considered building upon a number of pre-existing works. The NodeRED dashboard2 offers convenient Internet of Things (IoT) sensor integration and simple events. We opted not to build on this since its emphasis is on lower-level service interoperability rather than higher-level situational understanding functionality; however, we designed SUE to interoperate with NodeRED via its online mode. We also considered using an open knowledge graph-based platform, Cogni-Sketch (Braines et al. 2020), which is well-suited to the higher-level situational understanding functions but not geared towards handling event streams. Again, we integrated SUE with knowledge graph functionality via linking to Cogni-Sketch.\n2https://flows.nodered.org/node/node-red-dashboard"}, {"heading": "Overview of SUE and Examples", "text": "This section presents a brief overview of the platform, its interface and features. SUE is implemented in JavaScript on Node.js3, with LeafletJS4 providing map integration using OpenStreetMap5 map tiles. The conversational user interface (CUI) functionality is provided via Rasa6.\nFigure 2 shows the SUE interface with tabs on the left for the CUI, analytics panel and event marker panel. The map panel shows sensor and event positions. Each sensor can be shown either as its type (camera, microphone, etc) or coalition owner (seen here: USA and UK). Events are shown within coloured nested circles; in each case the outer circle approximates the region in which the event is located based on the sensed data, and the inner circle is always a constant size and serves to highlight the colour associated with the event, where the colours denote the degree of belief that something significant to the user is happening: red = strong, amber = medium, yellow = weak, blue = not significant. Sensor and event map markers are directly manipulable or controllable via the CUI: in this example we see the CUI being used to toggle between sensor type and owner views. Another CUI option allows the colours to be changed to more accessible variants for users with colour-blindness. Where simple events are related as part of a complex event (see the second example below for further details) they are joined by red lines to a complex event marker. Further details of events by type and by timeline are available on the analytics tab as shown in Figure 3.\nThe following subsections give two examples of how SUE emphasises our CSU research.\nMultimodal Explanations Figure 4 shows the detail associated with a simple event where a shooting has been\n3https://nodejs.org/ 4https://leafletjs.com 5https://www.openstreetmap.org 6https://rasa.com\ndetected in the city. This uses the selective relevance technique (Taylor et al. 2020) to highlight features in the input that were most salient to the detection of the event. Here we can see red highlighting on the person to the left, who is the active shooter (the other person is merely walking past). The input here consists of both video and audio modalities and the selective relevance method allows us to determine which of the modalities is most salient to the event detection and, moreover, which are the key moments in the video and audio streams. The explanation shown in Figure 4 has selected the most salient explanation as being visual.7\nThe selective relevance method also separates temporal from spatial information in generating explanations. Figure 5 shows an expanded view of this explanation where the original input \u2014 video and audio in the form of a spectogram \u2014 are on the left and the red-highlighted saliency re-\n7Original video: https://www.youtube.com/watch?v=evgpWTm - rE \u2014 the gunshots are heard out-of-context which is why the audio is less salient to the detection decision.\ngions are on the right. These frames are from a little later in the video where again we see highlights on the shooter, indicating temporal relevance (due to the person\u2019s movements).\nComplex Events The main approach we use for complex event detection in our work is a neuro-symbolic combination of neural network services to detect simple events and a symbolic event calculus-based reasoner to identify interrelated complex events (Vilamala et al. 2019). The previouslyseen example in Figure 2 showed a detected complex event (the \u2018!\u2019 marker) consisting of three simple events on the map. Details of this event are available on the marker tab as shown in Figure 6. Details of the event calculus-based reasoning are also available as a symbolic explanation."}, {"heading": "Current Status", "text": "The SUE platform has been used as a front-end for two integrated demonstrations of CSU research capabilities comprising parts of the Distributed Analytics and Information Science International Technology Alliance (DAIS ITA)8. These demonstrations included the processing of real audiovisual sensor date previously collected as part of live exercises, streamed into SUE via its WebSocket interface. Two CSU services are currently integrated into SUE via WebSocket endpoints and JSON-based messages: (1) multimodal activity recognition incorporating the selective relevance technique (Taylor et al. 2020), which in turn builds on previous work including layerwise relevance propagation (LRP) (Bach et al. 2015) for 3D convolutional neural networks (Tran et al. 2015) trained on video activity recognition tasks (Soomro, Zamir, and Shah 2012); (2) complex event processing (Vilamala et al. 2019) based on probabilistic logic programming via ProbLog (De Raedt, Kimmig, and Toivonen 2007; Fierens et al. 2015) and the event calculus via ProbEC (Skarlatidis et al. 2015). Integration of the latter with a third service based on the FastLAS scalable inductive logic programming system (Law et al. 2020) for learning complex event detection rules has also been demonstrated via SUE."}, {"heading": "Conclusion and Future Work", "text": "This paper has presented an experimentation platform for coalition situational understanding research that highlights capabilities in explainable artificial intelligence/machine learning (AI/ML) and integration of symbolic and subsymbolic AI/ML approaches for event processing. The platform is open and lightweight, to facilitate experiments and demonstrations. Its features are tailored to coalition multidomain operations with emphasis on asset interoperability and ad hoc human-machine teaming in a dense urban terrain setting. The platform is easily extensible to incorporate non-\n8http://sl.dais-ita.org/science-library/projects\nspatial data sources such as social media, and on-the-spot reports from human operatives (e.g., patrols or guards). Current work is geared towards technology integration experiments; planned future work will involve human participant experiments to measure the utility of layered explanations in ad hoc coalition teaming tasks."}], "title": "An Experimentation Platform for Explainable Coalition Situational Understanding", "year": 2020}