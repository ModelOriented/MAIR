{"abstractText": "When we consult with a doctor, lawyer, or financial advisor, we generally assume that they are acting in our best interests. But what should we assume when it is an artificial intelligence (AI) system that is acting on our behalf? Early examples of AI assistants like Alexa, Siri, Google, and Cortana already serve as a key interface between consumers and information on the web, and users routinely rely upon AIdriven systems like these to take automated actions or provide information. Superficially, such systems may appear to be acting according to user interests. However, many AI systems are designed with embedded conflicts of interests, acting in ways that subtly benefit their creators (or funders) at the expense of users. Unlike the relationship between an individual and a doctor, lawyer, or financial advisor, there is no requirement that AI systems act in ways that are consistent with the users\u2019 best interests. To address this problem, in this paper we introduce the concept of AI loyalty. AI systems are loyal to the degree that they are designed to minimize, and make transparent, conflicts of interest, and to act in ways that prioritize the interests of users. Properly designed, such systems could have considerable functional and competitive \u2013 not to mention ethical \u2013 advantages relative to those that do not. Loyal AI products hold an obvious appeal for the end-user and could serve to promote the alignment of the long-term interests of AI developers and customers. To this end, we suggest criteria for assessing whether an AI system is sufficiently transparent about conflicts of interest, and acting in a manner that is loyal to the user, and argue that AI loyalty should be deliberately considered during the technological design process alongside other important values in AI ethics such as fairness, accountability privacy, and equity. We discuss a range of mechanisms, from pure market forces to strong regulatory frameworks, that could support incorporation of AI loyalty into a variety of future AI systems. * Professor of Physics, University of California at Santa Cruz and co-founder, Future of Life Institute, \u2020 7th Future \u2021 Professor of Law, University of Colorado at Boulder \u00a7 Professor of Psychiatry and founder, Neuroethics Collective, University of British Columbia", "authors": [{"affiliations": [], "name": "Anthony Aguirre"}, {"affiliations": [], "name": "Gaia Dempsey"}, {"affiliations": [], "name": "Harry Surden"}, {"affiliations": [], "name": "Peter B. Reiner"}], "id": "SP:241db741e0385324356c215efb24b857a5ef5e42", "references": [{"authors": ["Z Tufekci"], "title": "How Recommendation Algorithms Run the World", "venue": "WIRED. 22 Apr. 2019 https://www.wired.com/story/how-recommendationalgorithms-run-the-world/. Accessed", "year": 2020}, {"authors": ["J Lariner"], "title": "Who Owns the Future", "venue": "Simon and Schuster", "year": 2014}, {"authors": ["J Del Rey"], "title": "The best Cyber Monday deals according to Alexa: any Amazon owned brand", "venue": "Vox.\" 3 Dec. 2019, https://www.vox.com/recode/2019/12/3/20992885/best-amazon-cyber-monday-deals-alexa-private-label-brands. Accessed", "year": 2020}, {"authors": ["B Frischmann", "E Selinger"], "title": "Re-Engineering Humanity", "year": 2018}, {"authors": ["D Susser", "B Roessler", "H Nissenbaum"], "title": "Technology, autonomy, and manipulation", "venue": "Internet Policy Review", "year": 2019}, {"authors": ["MJ Duncan"], "title": "The Case for Executive Assistants", "venue": "Harvard Business Review.\"", "year": 2011}, {"authors": ["Lukosius V", "Hyman MR"], "title": "Personal Internet Shopping Agent (PISA): A Framework.\" 2018 Atlantic Marketing Association", "year": 2018}, {"authors": ["P Pico-Valencia", "JA Holgado-Terriza"], "title": "Agentification of the Internet of Things: A systematic literature Review", "venue": "International Journal of Distributed Sensor Networks", "year": 2018}, {"authors": ["Murphy Z", "Dan"], "title": "Ariely on how Qapital uses behavioral finance principles to help people save more", "venue": "Tearsheet", "year": 2019}, {"authors": ["L Specker Sullivan", "PB Reiner"], "title": "Digital Wellness and Persuasive Technologies", "venue": "Philos. Technol", "year": 2019}, {"authors": ["N Carr"], "title": "The Glass Cage", "venue": "W. W. Norton & Company", "year": 2014}, {"authors": ["JA Obar"], "title": "The biggest lie on the Internet: ignoring the privacy policies and terms of service policies of social networking services. Information, Communication", "year": 2018}, {"authors": ["I Gabriel"], "title": "Artificial Intelligence, Values and Alignment", "venue": "ArXiv https://arxiv.org/abs/2001.09768", "year": 2020}, {"authors": ["P Gillin"], "title": "The Art and Science of How Spam Filters", "venue": "Work. Security Intelligence", "year": 2016}, {"authors": ["N Richards", "W Hartzog"], "title": "Taking trust seriously in privacy law", "venue": "Stanford Technology Law Review", "year": 2016}, {"authors": ["C Dwork", "F McSherry", "K Nissim", "A Smith"], "title": "Differential Privacy: A Primer for the Perplexed.\" Joint UNECE/Eurostat work session on statistical data confidentiality", "year": 2011}, {"authors": ["H Nissenbaum"], "title": "Technology, Policy, and the Integrity of Social Life - Stanford University", "year": 2009}, {"authors": ["MS Lam", "G Campagna", "S Xu", "M Fischer", "M Moradshahi"], "title": "Protecting privacy and open competition with Almond: An open-source virtual assistant. XRDS", "year": 2019}, {"authors": ["Bensinger R", "Stuart Russell"], "title": "AI value alignment problem must be an \"intrinsic part\" of the field's mainstream agenda", "venue": "Less Wrong", "year": 2014}, {"authors": ["S Russell"], "title": "Provably Beneficial Artificial Intelligence - EECS at UC Berkeley.", "venue": "https://people.eecs.berkeley.edu/~russell/papers/russellbbvabook17-pbai.pdf. Accessed", "year": 2020}, {"authors": ["P Eckersley"], "title": "Impossibility and Uncertainty Theorems in AI Value Alignment ArXiv https://arxiv.org/abs/1901.00064", "year": 1901}, {"authors": ["F Rossi", "N Mattei"], "title": "Building Ethically Bounded AI", "venue": "AAAI", "year": 2019}, {"authors": ["R Noothigattu", "D Bouneffouf", "N Mattei", "R Chandra", "P Madan", "KR Varshney", "M Campbell", "M Singh", "F Rossi"], "title": "Teaching AI Agents Ethical Values Using Reinforcement and Policy Orchestration", "year": 2019}, {"authors": ["DC Engelbart"], "title": "Augmenting Human Intellect: A Conceptual Framework", "venue": "Stanford Research Institute Report", "year": 1962}, {"authors": ["NA Draper", "J Turow"], "title": "The corporate cultivation of digital resignation. new media and society", "year": 2019}, {"authors": ["JM Balkin"], "title": "Information Fiduciaries and the First Amendment", "venue": "UC Davis Law Review", "year": 2016}, {"authors": ["LM Khan", "DE Pozen"], "title": "A Skeptical View of Information Fiduciaries - Harvard Law Review", "year": 2019}, {"authors": ["J Collins"], "title": "Turning the Flywheel", "year": 2019}, {"authors": ["M Tegmark"], "title": "Life 3.0: Being Human in the Age of Artificial Intelligence", "year": 2017}], "sections": [{"text": "When we consult with a doctor, lawyer, or financial advisor, we generally assume that they are acting in our best interests. But what should we assume when it is an artificial intelligence (AI) system that is acting on our behalf? Early examples of AI assistants like Alexa, Siri, Google, and Cortana already serve as a key interface between consumers and information on the web, and users routinely rely upon AIdriven systems like these to take automated actions or provide information. Superficially, such systems may appear to be acting according to user interests. However, many AI systems are designed with embedded conflicts of interests, acting in ways that subtly benefit their creators (or funders) at the expense of users. Unlike the relationship between an individual and a doctor, lawyer, or financial advisor, there is no requirement that AI systems act in ways that are consistent with the users\u2019 best interests. To address this problem, in this paper we introduce the concept of AI loyalty. AI systems are loyal to the degree that they are designed to minimize, and make transparent, conflicts of interest, and to act in ways that prioritize the interests of users. Properly designed, such systems could have considerable functional and competitive \u2013 not to mention ethical \u2013 advantages relative to those that do not. Loyal AI products hold an obvious appeal for the end-user and could serve to promote the alignment of the long-term interests of AI developers and customers. To this end, we suggest criteria for assessing whether an AI system is sufficiently transparent about conflicts of interest, and acting in a manner that is loyal to the user, and argue that AI loyalty should be deliberately considered during the technological design process alongside other important values in AI ethics such as fairness, accountability privacy, and equity. We discuss a range of mechanisms, from pure market forces to strong regulatory frameworks, that could support incorporation of AI loyalty into a variety of future AI systems.\n* Professor of Physics, University of California at Santa Cruz and co-founder, Future of Life Institute,\n\u2020 7th Future \u2021 Professor of Law, University of Colorado at Boulder\n\u00a7 Professor of Psychiatry and founder, Neuroethics Collective, University of British Columbia\nConflicts of interest can arise when we try to satisfy our duties to two or more entities. Traditionally, the parties that find themselves in such situations have been individuals or organizations. However, we are witnessing a radical shift in terms of the players that might be involved: in the modern world, artificial intelligence (AI)** systems introduce a new set of stakeholder dynamics where conflicts of interests arise. For instance, a user who searches for a product using an AI system might assume that the results that are returned are the most relevant, highest quality, or best value, but in fact, such systems often prioritize results that provide the most financial benefit to the software designers, and algorithmic news feeds may prioritize user engagement time and advertising dollars over users\u2019 desire for true and useful information1. As the capability and sophistication of AI engines providing recommendations, making decisions, and taking action visibly and invisibly within the complex web of corporate and consumer stakeholders mature, these misalignments in interest can be expected to grow in importance. The issue is especially problematic when we consider the power and knowledge imbalance between transnational technology companies with financial resources greater than that of many sovereign governments2, and individual users. This article aims to address the issue of conflict of interest in AI software through the lens of the concept of AI loyalty. While the framework that we develop may have widespread applications for AI systems more generally, we begin with the implications of AI loyalty in products that offer AI-based services to individual consumers. For this reason, we explore the concept through the particularly salient context of AI personal assistants, in which an AI system acts as an agent that makes recommendations and takes actions on behalf of a user in a way that echoes a human assistant or consultant.\nContemporary AI personal assistants\nVirtual assistants powered by AI are becoming a ubiquitous presence in the modern world. Systems such as Apple\u2019s Siri, Amazon\u2019s Alexa, Google\u2019s Assistant and Microsoft\u2019s Cortana can be found on billions of smartphones, smart speakers, and digital buttons and are capable of handling a growing array of digital tasks. From the users\u2019 perspective, these assistants generally present themselves as performing tasks or providing information for the user\u2019s convenience and benefit, without making their limitations or conflicts of interest explicit. However from a business perspective, they may play various roles: as a means of harvesting data that can be used to form digital profiles to inform future product development, target advertising, or feed machine learning systems, as a value-add to a hardware system, as an interface to increase the purchase of particular products, and so on3.\nOver the last 20 years, business practices enabled by accelerating technological advances in consumer tracking have changed long standing assumptions about conflicts of interest between businesses and customers. Traditionally, users purchasing a product or service were safe in assuming that the service would align with the user\u2019s interests in exchange for the financial compensation they paid. But this economic arrangement and its attendant assumptions have been undermined by the new class of widely available digital products, whose replication and distribution costs are negligible in comparison to that of physical goods. Today, businesses provide technological services to millions or billions of users at no direct charge, and monetize this relationship through less obvious pathways4. These alternative pathways include the gathering and analysis of personal consumer data, aggregation and sale of that data, and use of that data to influence consumer purchasing, capture engagement time in order to sell\n** We use \u201cAI\u201d here as a catchall term to include a variety of machine-learning and related techniques incorporated into contemporary systems.\nads, or other personal actions. Importantly, these monetization strategies are often not apparent to end-users. Worse, companies often take active steps to obfuscate these activities from consumers; and many of these same companies are developing the most widespread AI assistants.\nThis represents a potential divergence of interest that has garnered increasing attention and concern. A virtual assistant may, without disclosure, encourage (or only allow) purchases from a particular vendor or otherwise serve its home company\u2019s interests5. Moreover, the data-gathering business model has been heavily critiqued as being ethically fraught. Companies\u2019 financial incentive is to extract ever increasing amounts of personal information regardless of benefit to the user.\u2020\u2020 As a result, consumer backlash and accusations of manipulation, privacy violation and more have been levelled at technology companies6,7,8. In an attempt to manage the situation, governments have imposed historically heavy fines for breaches of privacy regulations both in the US9 and in the EU where GDPR rules hold sway10.\nWe propose to turn this situation on its head, beginning with an ethical view of how our relationship to AI assistants should look, from the perspective of the consumer. We will argue that a key feature of a new, ethically sound product category for AI assistants is that the assistant should be loyal to the consumer \u2013 that it should consider the user\u2019s interests first and foremost. Importantly, this loyalty principle applies to consumer AI systems more broadly, and not just to AI assistants. AI assistants are a specific, tractable example of a system where the AI loyalty framework can be used to shed light on and ameliorate analogous problems that are endemic in AI systems more generally and will become higher stakes as these systems\u2019 capabilities increase. In our recommendations section, we will discuss the range of factors that would need to be present in order to command a sea change in the stakeholder mindsets and ultimately widespread development and adoption of loyal AI assistants, including market demand, regulatory pressure, consumer advocacy, and media attention.\nWhat might we want AI assistants to do?\nThe current crop of AI assistants, even if quite limited as compared to the promise of coming years, are reasonably capable. Voice recognition software allows users to communicate with them using natural language, and they can answer factual questions, take dictation, send texts and emails, schedule events, make recommendations and control the expanding suite of \u201csmart\u201d devices taking up residence in the home, car, and office (the market share of which tech giants and myriad startups are vying for).\nWith time, the capabilities of AI assistants are likely to increase such that they become more and more like bona fide executive assistants11 with some tasks carried out autonomously without user oversight and others \u2013 in particular higher stakes scenarios that involve greater complexity \u2013 requiring significant user input. Even without invoking the development of full-fledged artificial general intelligence, it seems plausible that one day AI assistants could be involved in making significant purchases12, booking travel reservations, negotiating the purchase of a home, supporting detailed logistics and event planning, and supporting the decision-making process on topics related to questions of health, nutrition, childcare,\n\u2020\u2020 Such systems can act not only to extract data but also to deliberately increase reliance on the ecosystems of products they connect with.\nBecause of this, negative feedback loops can result that decrease the agency of the individual yet lock them into an ecosystem that is all but\nnecessary for their participation in large swathes of the global economy and social fabric; see Maldoff G & Tene O, The Costs of Not Using Data:\nBalancing Privacy and the Perils of Inaction 15 J.L. Econ. & Pol'y 41 (2019) and Siemoneit A, An offer you can't refuse \u2013 Enhancing personal productivity through 'efficiency consumption', ZOE Discussion Papers (2019). .\noperator or highly intelligent advisor for support on13.\nAI assistants could protect users from an array of cyberattacks, warding off not just spam but also phishing, malware, and even legal but exploitative scams. They could also serve as a central repository and negotiator of user preferences on privacy and similar matters, communicating those preferences to a system with which the user is interacting, and/or warning a user when a system does not respect those preferences. Similarly, users\u2019 ethical preferences (for example to favor or avoid particular industry practices or company qualities) could be folded into recommendations to the user.\nOn a more aspirational level, an AI assistant could help users achieve a better state of well-being \u2013 a software layer that users can cooperate with to help them more effectively steer, drive, and coursecorrect along their personal path of self-improvement. Developers are already releasing AI assistants tasked with helping users improve their health and material welfare, gently nudging14 them towards better nutrition, serving as AI therapists, and helping them save for a rainy day15. Even more ambitious would be programming that helps users further develop the skills that provide meaning to the human experience: the satisfaction that derives from accomplishment, the value of social connectedness, the nourishment of one\u2019s inner life. AI assistants could \u201cknow\u201d when and how it is more appropriate to encourage the user to reflect on the values underlying their habits and teach them to evaluate their lives\u2019 competing demands16. Designing AI assistants with these features would go some distance in lessening the worry that over-reliance upon AI assistants for instrumental tasks might enfeeble us, diminishing our capability to effectively navigate the world17. The concept of AI loyalty becomes of paramount importance in systems with such a high degree of access to the user\u2019s sense of identity, wellbeing, and general mental and emotional state.\nAligning interests \u2013 the case for loyal AI Assistants\nFor most people, using an AI assistant such as Siri or Alexa simply involves speaking into a device and hearing a response. While the small print in the Terms of Use generally make it clear that collecting and exploiting user information is a condition of gaining access to the service, most users blithely ignore the Terms of Use entirely18. The result is often relatively unfettered collection of user information, setting the stage for AI-driven algorithms to persuade, cajole and manipulate user behavior7,19 - all without much incentive or any requirement to consider the best interests of the user. We suggest that loyal AI Assistants \u2013 AI agents that explicitly put the interests of the user at the fore \u2013 represent an appropriate resolution of this problem, and a significant marketing advantage, provided they are implemented in a trustworthy manner\u2021\u2021. Loyalty of this sort represents a version of the value alignment problem \u2013 the challenge of ensuring that an AI-driven agent act in accord with some set of values. We will not recapitulate the full set of arguments that have been proffered on this topic but note that its solution is fundamental to the development of the next generation of AI20,21.\nThe idea of \u201cloyalty\u201d is well established in human professional relationships in the form of the fiduciary duty owed to their clients by physicians, therapists, and lawyers22. We expect these fiduciaries to put\n\u2021\u2021 It might be objected that conflicts-of-interest between AI users and developers should simply be transparently declared, and that this information is sufficient for users to fold into their decision-making. But in a system that has such conflicts built-in with some requirement to declare them, the incentives most naturally lead to the declarations either being hidden in lengthy, abstruse, universally unread terms-of-use agreements# or \u2013 where this is disallowed \u2013 so ubiquitous as to be simply wasteful and dismissed without effect, as in endless GDPR notices. Neither case leads to meaningful mitigation of the conflict.\nwe feel comfortable sharing important and personal information and trusting their recommendations. AI loyalty would logically be even stronger than that of a human fiduciary, as unlike a doctor or lawyer the AI presumably has no interest (for example salary, pride, etc.) of its own\u00a7\u00a7. The wellbeing of the enduser could be absolutely paramount in terms of recommended products or purchases. In terms of actions, depending upon the business model employed (see below), a loyal AI assistant could be configured such that it was primarily responsive to the interests of the user so long as actions that it might be asked to provide are not illegal. (Even here exceptions could obtain. Imagine a scenario in which one is rushing to the hospital in a self-driving car, and the user asks the AI assistant to go 5 miles per hour above the speed limit. The AI assistant might respond by reminding the user that speeding is against the law, and the user could accept responsibility for the legal infraction.)\nA well-functioning loyal AI Assistant \u2013 preferably functioning in the context of a clear and robust legal and regulatory framework \u2013 would reinforce the possibility of trust between user and algorithmic system. Assuming that the duty of loyalty is maintained over time, one can imagine the relationship moving towards the thick sort of trust that develops in healthy interpersonal relationships, in which the parties have a shared history that reinforces mutual confidence in their actions15.\nThis is important not just on a social level but also to improve the overall function of interactions between systems. The deeper the trust that users ascribe to their loyal AI assistants, the more freely they will share information with it. As a result, the loyal AI assistant will have more robust information at its (virtual) fingertips and therefore be better able to discharge its duties towards the user. This virtuous circle is exactly the opposite of the vicious circle we see underway in the current regime, where people who distrust social media are encouraged to be careful about what they share23, resulting in less robust images of who they are. Trust allows the AI system not just to receive information but also to carefully, appropriately, and where applicable, anonymously share information, opening the possibility of new functionalities. Modern anti-spam systems, for example, overpower spam generators by sharing information about what is labeled as spam24; loyal AI assistants, similarly, could help their user coordinate with others with common interests. This could include identifying and mitigating low-value drains on users\u2019 time and money, as well as protecting the user against manipulative tactics. A loyal AI could also be entrusted to identify (and if so requested, take advantage of) opportunities for its user that are genuinely in the user\u2019s interest.\nA well-established system of loyal AI assistants would also have significant wider social implications. It is already known that loyalty promotes trust in data stewards25. In order to garner the trust of users, loyal AI Assistants would need to satisfy other features including, at a minimum, the ability to explain their actions in easily understandable terms, to maintain data security, and, importantly, be able to communicate with outside entities and agents without compromising privacy. Models for the latter include the well-developed examples of differential privacy26 and contextual integrity27, or the federated approach of the privacy-preserving open-source AI assistant framework Almond28. The idea of AI loyalty also aligns closely with that of \u201chuman compatible\u201d AI20, in which AI systems aim to accomplish human goals rather than their own (even if human-provided) goals.\n\u00a7\u00a7 We leave aside the issue of whether future advanced AI systems could meaningfully have their own interest, but suggest that even if it were\npossible to create such systems, AI assistants should probably not be among them.\ninteractions. For example, how should the user\u2019s interests be balanced against interests other than those of the AI (which presumably do not exist) or its parent company? An objection to the loyalty requirement has been raised by AI pioneer and practitioner Stuart Russell who rightly points out20 (in the context of quite advanced AI assistants) that an AI might follow the law but still unscrupulously trespass social norms in the process of executing its loyalty obligations to its user. He posits a situation in which the user has accidentally double booked for dinner \u2013 both with his wife for their twentieth anniversary and the secretary-general who is flying in for the occasion. As a solution to the dilemma, the AI causes the secretary-general\u2019s plane to be delayed. Russell\u2019s solution is for the AI to follow a set of moral rules known as consequentialism. We suggest that true loyalty means reflecting the user\u2019s values, their social etiquette, and their general preferences about navigating the world. For simple AI assistants such as those available today, such regard would need to be instantiated directly into the system, implicitly consented to by the user when adopting the system. More advanced systems with a greater action space could include, for example, user-definable settings that characterize how to weigh the user\u2019s interest against others\u2019, with some required minimum weight accorded to the latter. Very sophisticated systems, having a detailed model of their users\u2019 preferences, could learn from their user\u2019s decisions how to accord weight to the interests and preferences of others, with some hard ethical limits to the loyal AI assistant\u2019s behavior in place. That is, an advanced loyal AI assistant would include the consideration of others in its recommendations and actions specifically because its user would. However, this arrangement would do nothing to mitigate the amoral actions of bad actors. Such individuals would likely use the power of AI to further their amoral interests \u2013 this is an inevitable consequence of putting the growing power of AI at the service of people who enjoy significant liberty in their actions. New legal and social norms will develop to address this reality, often driven by notable/newsworthy transgressions. On the other hand, in the case of people trying to do the right thing, AI assistants with such advanced capabilities could even help better align the actions of the user with the user\u2019s own moral compass: anytime the user makes a request that appears to transgress what the loyal AI assistant understands to be the morally appropriate thing to do, it could prompt the user to reflect on the issue and await further instructions. Small nudges such as this are already in place in the AI that monitors offensive posts on Instagram16, and could easily be expanded upon and deployed more widely.\nCriteria for near-future Loyal AIs\nAlthough some loyal AI capabilities will require future technical advances, this framework applies to contemporary AI assistants. Here we sketch some principles and criteria for designating an AI assistant system \u201cloyal.\u201d\nValue alignment. To the extent that the system acts independently of the user to facilitate the completion of some action or task:\n\u25cf The system should be deliberately designed to eliminate clear conflicts of interest \u2013 e.g. AI\nsystems taking automated actions or providing prioritized recommendations that financially benefit the creators or funders of the AI system;\n\u25cf When clear conflicts of interests do exist, the system should transparently and saliently indicate\nto users the presence of such conflicts;\nmade transparent so that users (and/or auditors) can determine whether they are in alignment with the user\u2019s own goals;\n\u25cf Preferably, the criteria and goals should be adjustable in terms of balancing tradeoffs in the\nprioritization of optimizable factors (e.g. price, speed, cost, privacy, etc);\n\u25cf Optimally, the values utilized by loyal AI should be derived from revealed preferences21, learned\ndirectly from the user where possible***, appropriately and efficiently requesting user input as needed.\nMaintaining value alignment between an AI system and its user is a highly nontrivial problem29,30 but also an area of active investigation21,31,32,33. As discussed below, AI assistants may be a useful provingground for alignment techniques.\nDecision transparency. To the extent that decisions are made independently of the user, the decisionmaking process should be transparent and explainable. The system should be designed to empower and include users in decisions, educating the user on the relevant factors forming the basis for those decisions. This accomplishes three important objectives: first it conditions and trains users in the powers and limitations of the system34, second, it protects against a form of learned helplessness that has been termed digital resignation35, and third, it facilitates human intervention in the case of automated decision failures.\nData integrity. The system should be aware of the provenance of data and attribute the appropriate legal and privacy rights to its originator. For example, the system should be capable of tracking the origin of data that is being monetized and directing payments (or other value commensurate with that of the data) to the correct entities. Or, in the case of sensitive information such as medical data, the system should be capable of keeping track of which data may legally be shared with which party and use appropriate encryption or other privacy technologies to ensure the right protections are in place\u2020\u2020\u2020.\nPersonal Privacy. The system should have extreme regard for privacy, including both how and why it retains user data, and how and why it shares user data as appropriate (to accomplish a given task, for example, in an anonymized and encrypted method allowing network effects with other AI assistants.) In addition, the system makes explicit to the user the privacy risks of any particular action, when appropriate.\nLegal framework\nTo what extent, if any, should the law play in encouraging or requiring AI loyalty? One possible approach involves limited (or no) government involvement. One could imagine a form of industry self-policing that might arise in which the corporate creators of AI systems voluntarily develop and implement AI loyalty best practices during the process of technological design. The possibility of industry self-regulation is not completely far-fetched. There have been past examples of such self-regulatory efforts in the area of\n*** This fits the existing machine learning framework of Inverse Reinforcement Learning; see Arora S & Doshi P, A Survey of Inverse Reinforcement Learning: Challenges, Methods and Progress, https://arxiv.org/abs/1806.06877 and, addressing longer term issues, S. Russel, \u201cHuman Compatible\u201d, Viking, 2019. \u2020\u2020\u2020 As a salient example, it is very likely that many consumers would happily share substantial personal data with an agency like the U.S.\u2019s Center for Disease Control and it alone during a pandemic; this could prove extremely helpful both individually and collectively. But there is currently no trustable mechanism for doing this.\nlargely as a reaction to public criticism and as a means to head off pending government regulation. However, once implemented, such voluntarily self-policing efforts are often criticized as half-hearted or self-interested, designed in such a way to placate critics and garner positive publicity, while doing little of substance.\nAnother possible approach with little or no government role involves regulation through \u201cmarket discipline.\u201d Market discipline is the concept that economic competition (as opposed to government regulation) can bring about some desired social goal when implementing that goal actually confers a competitive advantage. In this vein, one could imagine consumer rights groups desiring or demanding that consumer AI systems become transparent, free of conflict of interest, and loyal to user interests. If such consumer demand were to arise, firms could voluntarily implement and advertise AI loyalty as a market distinguisher for their products. That could result in increased market share and economic advantage over competitors who produce AI products with embedded conflicts of interests, producing competitive pressure that could cause AI loyalty to diffuse more broadly. However, such a \u201cmarketdiscipline\u201d scenario might not actually transpire, as it would require both consumers and producers to value AI loyalty (over other features) such that it would shift their usage preferences in a meaningful way. By contrast, history has shown that consumers often prefer other factors such as speed, ease of use, or low or free price, even at the expense of the concepts such as transparency or privacy that are related to AI loyalty \u2013 or consumers may not recognize the importance of these factors until their absence is so baked-in to large-scale systems as to be both problematic and difficult to change. Thus while loyalty may be a significant product differentiator, this market discipline driver is unlikely to suffice on its own to create the conditions necessary for widespread adoption.\nThere are other approaches in which the law might take a larger role in promoting AI loyalty. Most obviously, Congress or state governments could introduce legislation requiring companies to adopt AI loyalty principles in some way. This could include anything from a requirement that creators of AI systems consider issues of AI loyalty during the product development pipeline, to rules that AI systems disclose and make transparent and obvious clear conflicts of interest. Other regulations might insist upon clear and transparent explanations for how automated results were produced. We see analogs to these requirements in the privacy realm, where recent regulation from California (the California Consumer Privacy Act) and the European Union (The General Data Protection Act) contain similar requirements with respect to privacy and personal data. This approach seems reasonably promising, as there has been much discussion (and proposed legislation) at the state and federal level concerning the regulation of Artificial Intelligence more broadly. The additional step here would be to simply include \u201cAI Loyalty\u201d as an additional principle to be considered in such regulation.\nHowever, some obvious problems that will arise concern the difficult nature of coherently defining, providing standards for, and applying concepts such as \u201cconflict of interest,\u201d \u201cbest interest,\u201d and \u201cloyalty\u201d in the legal context of AI automation. This will raise difficult practical questions such as: how do we actually determine or measure a user\u2019s \u201cbest interest\u201d, which user interests are we measuring, what if one user\u2019s interests harm others, and what if an individual user has multiple interests that conflict with one another? Similarly, one could imagine legally distinguishing issues of \u201cAI disloyalty\u201d, where AI systems are clearly acting out of conflict of interest (i.e. a system that makes a sub-par automated decision for a user that results in a financial kickback for the AI creator), from more difficult \u201cAI loyalty\u201d issues where user interests are simply under-specified, or are not maximally satisfied by AI systems.\nArguably, AI systems that seem to produce accurate and relevant actions for users but that are actually acting in the interests of others could be characterized as deceptive or unfair trade practices. In this legal framing, there are already numerous federal agencies (such as the Federal Trade Commission), and state agencies (such as State Consumer Protection Offices) that could promulgate regulations, or take enforcement actions against AI systems that engage in controversial, but non-transparent, conflicts of interest against consumers.\nContract law might also play a role in promoting AI loyalty. In some cases, vendors or users of AI assistants may promulgate explicit terms of service or policies governing their use and behavior. In certain circumstances, one could imagine explicit contractual promises that AI assistants will make reasonable efforts to implement systems that promote loyalty, privacy, explainability, and data security. However, unless specifically required by law, such explicit contractual promises do not appear to be a promising avenue due to the difficulty in defining these standards, the subjective nature of these criteria, and the risk of abuse by opportunistic litigants. Relatedly, one could imagine the courts creating common law contractual duties of implied AI loyalty, much in the way that consumer goods have implied warranties of merchantability even in the absence of explicit warranties. Finally, legal rules governing AI loyalty might develop incrementally through consumer class action litigation routes.\nA distinct aspect in which the law might play a role concerns liability for the actions of AI systems that have embedded conflicts of interest. Should the creators of AI systems be liable for automated decisions that are clearly suboptimal for users, and if so, to what extent? Perhaps more forward-looking, what happens if AI systems innocently, but autonomously, act on behalf of users in ways that hurt the financial, legal, or physical interests of others? Should the creators of AI systems be liable for such actions? Should the users be liable? What if AI systems are knowingly directed to autonomously act in a way that will likely violate the law? Who should bear the civil or criminal liability for such actions? These critical open questions remain around the intersectionality of loyalty in AI, from both an ethical and a legal perspective.\nPaying for loyal AI assistants\nThe concept of loyal AI Assistants builds upon, but is distinct from the proposal that large technology companies ought to be regulated as information fiduciaries36. Indeed, the information fiduciary model has been critiqued for condoning the tech titans in maintaining their existing business model, collecting user data to drive microtargeting of advertising37, thereby perpetuating conflicts of interest. Loyal AI demands modification of this business model. At the same time, loyalty could be a major product differentiator and competitive advantage, particularly as privacy and related concerns continue to grow and potentially come under stronger regulatory sway.\nIt is likely that any model in which the AI system provider has a financial model with revenue resulting from the way the loyal AI Assistant operates is likely to lead to conflicts between revenue maximization and loyalty to the user. This conflict can be removed if the revenue is tied to the fact that the assistant is used, rather than what it actually does.\nOne option, therefore, would be for a loyal AI to be bundled with another product, consistent with a business model in which \u2018turning the flywheel\u2019 represents a path to success38. For example, Apple\u2019s AI assistant Siri is incorporated into all iPhones. Offering a loyal version of Siri would represent an added inducement for people to purchase iPhones, with Apple incorporating the cost of the AI functionality\nand service plans, such as Apple Care and iCloud. Whether introduced by Apple or its competitors, one can imagine a scenario in which a loyal AI assistant would confer significant competitive advantages to its producers. If successful, such a consumer product would create social and market pressure that could modify the unhealthy balance of power that exists today in the industry.\nAn alternative model might involve subscription pricing. Basic versions of a loyal AI assistant could be offered for a modest cost, with capability enhancements available as the equivalent of in-app purchases. These are only the most obvious solutions, but the power of entrepreneurial ideas will undoubtedly lead to alternative approaches, each competing for market share in their own way. Some of these business models may come with their own problems \u2013 for example there is the worry that if AI assistants with a wide range of capabilities are available at a wide range of cost points, loyal AI assistants might be available most easily only to those with the means to pay for them, widening the gap between the haves and the have-nots.\nPower asymmetries and coordination\nOur modern internet-mediated economy embodies an unprecedented asymmetry of information, and in some ways power, between large companies that can collect and process information from many people and enact decisions that affect many people at once, and individuals who must each make decisions and take action based on their own information. Governments and the legal system (including class-action lawsuits) can and do protect citizens from physical harm and (at some level) fraud etc.; but what protection is available to people against pervasive manipulation, privacy invasion, false (and toonumerous) choices, etc.? The traditional model \u2013 successful in many ways \u2013 has been that informed consumers and citizens \u201cvote with their wallet\u201d and target their resources toward companies and products that serve them best. But the modern information economy is arguably so complex and information-asymmetric that individuals cannot possibly make rational informed decisions reflective of their own interests, values, and objectives without a level of time, effort and expertise that is unavailable to the vast majority of people. This trend is unlikely to change unless a new ingredient is added that helps tip the balance back in favor of autonomous individuals, by providing them with trusted tools that can analyze and navigate a high-complexity economic environment, as well as allow coordinated action to counteract the power of corporations and nation-states. Loyal AI assistants are an example (but only one) of such a new ingredient.\nRelation to the issue of AI alignment\nThe alignment of AI assistants with their users\u2019 interest is a special case of a wider problem. As AI systems become more capable it will make sense to cede more decisions to them. But how can these decisions then be assured to be consonant with the goals and values of their operators, so that it makes sense for an individual (or organization) to delegate these decisions or trust these recommendations? Numerous AI experts among others have pointed out the profound and fundamental difficulty of this problem20,39,40 arguing that any explicitly specified set of objectives for an agent operating over a very wide action space are virtually guaranteed to lead to unforeseen negative side effects. Just as with human assistants, AI assistants will make mistakes \u2013 both in failing to do what they are trying to do, and in trying to do the wrong thing. This will make AI assistants challenging as products, because many users employing different and customized AI assistants for various tasks will lead to instances of users pushing AI assistants in potentially problematic directions, and instances of users objecting to (i.e. complaining\nframework of the assistants and their design underpinnings will be repeatedly and adversarially tested and improved, starting when the stakes are low. This seems much more likely led to robustly aligned AI architectures than for AI systems with limited interaction with a limited number of users and may very well significantly contribute to the broader challenge of AI alignment.\nThe type of loyalty discussed here is not the only form of AI alignment. Loyalty as such need not be to an individual person. We may imagine AI systems designed to be loyal to a group, a legal entity such as a corporation or government, a nation, or even to humanity as a whole. As with human affairs, it may be challenging to determine how to aggregate the preferences of multiple entities within the group, and in those cases the label of loyalty would fit less well, even for systems that are explicitly designed to behave according to agreed upon moral tenets.\nRecommendations\nThe aim of this paper has been to bring attention and conversation to the importance of loyalty \u2013 or lack thereof \u2013 in current and future AI systems, focusing on the example of AI assistants. To the extent that loyalty is desirable, what could make it the norm? Commonly, issues of public concern (e.g. climate change) are only considered by powerful economic players when a combination of market incentives, government regulation, consumer demand, consumer advocacy, media pressure, and shareholder activism are present \u2013 no single ingredient alone is sufficient to catalyze meaningful, widespread change in corporate operations and policy. Thus, we offer a diverse set of initial recommendations toward the end of AI loyalty:\n\u25cf AI researchers and developers should continue to devise, develop, and prototype systems and\nprotocols necessary for loyal AI systems, including private information storage and appropriate sharing, data provenance tracking, decision and goal transparency, and conflict of interest monitoring.\n\u25cf AI companies in or adjoining the market of personal assistants should consider loyalty as a\npotential design feature for their existing products and consider developing products with loyalty at their core.\n\u25cf Major tech companies offering personal assistants should consider loyalty alongside properties\nlike privacy and discrimination. Moreover, they should create internal processes that empower technologists and other product development specialists) to explicitly query the issue of conflict of interest throughout the development pipeline. Companies for whom AI loyalty is a natural part of their product offering should leverage this fact for market advantage.\n\u25cf Policymakers should consider loyalty alongside other pro-social AI system attributes such as\ntransparency, non-discrimination, privacy, and safety. At minimum, transparency and disclosure of conflicts of interest should be strongly considered as part of any set of strictures placed on consumer-facing AI products.\n\u25cf Consumers, while demanding appropriate levels of effectiveness, privacy, safety, and fairness\nfrom the products they use, should consider how important AI loyalty is to them, and exhibit this both explicitly and in purchase/use preferences.\nThe past 20 years have seen the explosive growth in the power and capability of online platforms that mediate the connection between users and nearly every important part of the social, economic, intellectual and even natural world. The power and reach of these platforms have created enormous convenience for many users and increasingly are becoming an indispensable part of modern life. However, their advertising-focused ethos and the nearly regulation-free model in which they have been developed \u201cfast\u201d without concern for \u201cbreaking things\u201d has resulted in a system with fundamental drawbacks.\nMany of these drawbacks are embodied in current-day AI assistants. There are many instances in which AI systems appear to be acting in the users\u2019 best interests, but in fact do not do so. Rather, these systems are subject to either deliberate or accidental technical designs that promote the interests of the system creators (or others) to the detriment of users. The absence of what we have called \u201cAI loyalty\u201d has largely flown under the radar \u2013 as compared for example to privacy and equality \u2013 for two reasons. First, although AI systems make automated decisions on behalf of lay users today, these automated decisions tend to be fairly inconsequential data retrieval tasks, such as playing music, creating task reminders, or returning results for search queries about product purchasing or navigation. Second, the technology underlying contemporaneous AI systems tends to be relatively limited, thereby limiting their scope and reach.\nThis will change. When an AI system with an embedded conflict of interest subtly promotes one less attractive product at the expense of another, the impact is relatively small. But if one considers the widespread adoption of AI in the analysis of job interviews, in the automated assessment of banking and credit applications, in medical diagnostics, and as a (widely derided) means of parole assessment, it becomes clear that AI systems are rapidly being implemented in high-stakes arenas. It is only a matter of time before AI systems develop sufficient technological capabilities that they will be assisting consumers with more consequential decisions, such as assisting parents in finding the best daycare centers for their children, or helping doctors prescribe medication, or managing wealth portfolios independent of human input for long periods of time. These are precisely the scenarios for which we must avoid conflicts of interest. It is therefore important to open up the conversation into AI loyalty today, while the stakes and capabilities remain comparatively low, and to explicitly incorporate considerations of AI loyalty into the technological design and deployment process. In the paper we have outlined criteria to be considered in doing so.\nWe have also emphasized that beyond mitigating conflicts of interest that are detrimental to users, AI loyalty presents an opportunity: genuinely trustable AI systems can provide services \u2013 for example using highly private or proprietary data \u2013 that could not (or at least certainly should not) be provided by potentially disloyal ones.\nIn the years to come, the role of AI-based software agents in the affairs of humans will grow very substantially. We suggest that the current trajectory in which AI systems grow increasingly capable and versatile while harvesting (and selling) immensely detailed user profiles, providing opaquely sourced and reasoned recommendations, and acting in the interest of the user only insofar as it aligns with the overall growth and profit goals of the system\u2019s parent company runs the risk of leading to ever more dystopian outcomes. We can also imagine, however, a trajectory in which individual autonomy is\ncan, will, and should trust.\nWe thank Jared Brown, Richard Mallah, Adrian Byram, and Imre Bard for comments on an earlier version of this manuscript.\n1 Tufekci Z, How Recommendation Algorithms Run the World. WIRED. 22 Apr. 2019 https://www.wired.com/story/how-recommendation-\nalgorithms-run-the-world/. Accessed 19 Jan. 2020.\n2 Khana P, These 25 Companies Are More Powerful Than Many Countries: Going stateless to maximize profits, multinational companies are\nvying with governments for global power. Who is winning? Foreign Policy. 15 Mar. 2016, https://foreignpolicy.com/2016/03/15/these-25-\ncompanies-are-more-powerful-than-many-countries-multinational-corporate-wealth-power/. Accessed March 3, 2020.\n3 Horgan C, We Already Know What Our Data Is Worth. OneZero.\" 26 Jun. 2019, https://onezero.medium.com/we-already-know-what-our-\ndata-is-worth-48bca5643844. Accessed 20 Jan. 2020.\n4 Lariner J, Who Owns the Future? Simon and Schuster (2014).\n5 Del Rey J, The best Cyber Monday deals according to Alexa: any Amazon owned brand. Vox.\" 3 Dec. 2019,\nhttps://www.vox.com/recode/2019/12/3/20992885/best-amazon-cyber-monday-deals-alexa-private-label-brands. Accessed 20 Jan.\n2020.\n6 Frischmann B and Selinger E, Re-Engineering Humanity. Cambridge University Press (2018).\n7 Susser D, Roessler B and Nissenbaum H, Technology, autonomy, and manipulation. Internet Policy Review 8:1-22 (2019)\n8 Zuboff S, The Age of Surveillance Capitalism. Public Affairs (2019).\n9 Federal Trade Commission, FTC Imposes $5 Billion Penalty and Sweeping New Privacy Restrictions on Facebook. 24 Jul. 2019,\nhttps://www.ftc.gov/news-events/press-releases/2019/07/ftc-imposes-5-billion-penalty-sweeping-new-privacy-restrictions.\n10 Alpin, Major GDPR Fine Tracker - An Ongoing, Always-Up-To-Date List of Enforcement Actions https://alpin.io/blog/gdpr-fines-list/.\nAccessed 16 Feb. 2020.\n11 Duncan MJ, The Case for Executive Assistants. Harvard Business Review.\" May 2011.\n12 Lukosius V, Hyman MR. Personal Internet Shopping Agent (PISA): A Framework.\" 2018 Atlantic Marketing Association At: New Orleans 13 Pico-Valencia P, Holgado-Terriza JA Agentification of the Internet of Things: A systematic literature Review. International Journal of\nDistributed Sensor Networks 14:1-20 (2018).\n14 Thaler RH and Sunstein CR, Nudge. Penguin (2008).\n15 Murphy Z, Dan Ariely on how Qapital uses behavioral finance principles to help people save more. Tearsheet 24 May. 2019\nhttps://tearsheet.co/new-banks/dan-ariely-on-how-qapital-uses-behavioral-finance-principles-to-help-people-save-more/. Accessed 20\nJan. 2020.\n16 Specker Sullivan L and Reiner PB, Digital Wellness and Persuasive Technologies. Philos. Technol. (2019). https://doi.org/10.1007/s13347-\n019-00376-5.\n17 Carr N, The Glass Cage. W. W. Norton & Company (2014) 18 Obar JA, The biggest lie on the Internet: ignoring the privacy policies and terms of service policies of social networking services.\nInformation, Communication & Society 23:128-147 (2018).\n19 Psychological targeting as an effective approach to digital mass persuasion. PNAS 114: 12714\u201312719 (2017). 20 Russell S, Human Compatible. Viking. (2019).\n21 Gabriel I, Artificial Intelligence, Values and Alignment. ArXiv https://arxiv.org/abs/2001.09768 (2020)\n22 Gold AS and Miller PB, Philosophical Foundations of Fiduciary Law. Oxford University Press (2014).\n23 Brunton F and Nissenbaum H, Obfuscation: A User's Guide for Privacy and Protest MIT Press (2015).. 24 Gillin P, The Art and Science of How Spam Filters Work. Security Intelligence 2 Nov. 2016, https://securityintelligence.com/the-art-and-\nscience-of-how-spam-filters-work/. Accessed 21 Jan. 2020. 25 Richards N and Hartzog W, Taking trust seriously in privacy law. Stanford Technology Law Review 19:431-472 (2016)..\n26 Dwork C, McSherry F, Nissim K and Smith A, Differential Privacy: A Primer for the Perplexed.\" Joint UNECE/Eurostat work session on\nstatistical data confidentiality (2011).\n27 Nissenbaum H, Technology, Policy, and the Integrity of Social Life - Stanford University Press (2009)..\n28 Lam MS, Campagna G, Xu S, Fischer M and Moradshahi M, Protecting privacy and open competition with Almond: An open-source virtual\nassistant. XRDS 26: DOI: 10.1145/3355757 (2019)\n29 Bensinger R, Stuart Russell: AI value alignment problem must be an \"intrinsic part\" of the field's mainstream agenda. Less Wrong 26 Nov.\n2014, https://www.lesswrong.com/posts/S95qCHBXtASmYyGSs/stuart-russell-ai-value-alignment-problem-must-be-an. Accessed 16 Feb.\n2020.\n30 Russell S, Provably Beneficial Artificial Intelligence - EECS at UC Berkeley.\" https://people.eecs.berkeley.edu/~russell/papers/russell-\nbbvabook17-pbai.pdf. Accessed 16 Feb. 2020.\n31 Eckersley P, Impossibility and Uncertainty Theorems in AI Value Alignment ArXiv https://arxiv.org/abs/1901.00064.\n32 Rossi F and Mattei N, Building Ethically Bounded AI \u2013 AAAI 2019 pp. 9785-9789 (2019).\n33 Noothigattu R, Bouneffouf D, Mattei N, Chandra R, Madan P, Varshney KR, Campbell M, Singh M and Rossi F, Teaching AI Agents Ethical\nValues Using Reinforcement and Policy Orchestration. IJCAI-19) pp. 6377-6381 (2019)\n34 Engelbart DC, Augmenting Human Intellect: A Conceptual Framework. Stanford Research Institute Report (1962).\n35 Draper NA and Turow J, The corporate cultivation of digital resignation. new media and society doi/10.1177/1461444819833331 (2019).\n36 Balkin JM, Information Fiduciaries and the First Amendment, UC Davis Law Review 49:1183-1284 (2016).. 37 Khan LM and Pozen DE, A Skeptical View of Information Fiduciaries - Harvard Law Review 133:497-541 (2019). 38 Collins J, Turning the Flywheel. Harper Collins (2019).\n39 Tegmark M, Life 3.0: Being Human in the Age of Artificial Intelligence. Knopf (2017).\n40 Bostrom N, Superintelligence. Oxford University Press (2014)."}], "title": "AI loyalty: A New Paradigm for Aligning Stakeholder Interests", "year": 2020}