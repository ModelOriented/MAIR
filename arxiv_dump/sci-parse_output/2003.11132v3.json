{"abstractText": "The use of machine learning algorithms in finance, medicine, and criminal justice can deeply impact human lives. As a consequence, research into interpretable machine learning has rapidly grown in an attempt to better control and fix possible sources of mistakes and biases. Tree ensembles offer a good prediction quality in various domains, but the concurrent use of multiple trees reduces the interpretability of the ensemble. Against this background, we study born-again tree ensembles, i.e., the process of constructing a single decision tree of minimum size that reproduces the exact same behavior as a given tree ensemble in its entire feature space. To find such a tree, we develop a dynamic-programming based algorithm that exploits sophisticated pruning and bounding rules to reduce the number of recursive calls. This algorithm generates optimal born-again trees for many datasets of practical interest, leading to classifiers which are typically simpler and more interpretable without any other form of compromise.", "authors": [{"affiliations": [], "name": "Thibaut Vidal"}, {"affiliations": [], "name": "Toni Pacheco"}, {"affiliations": [], "name": "Maximilian Schiffer"}], "id": "SP:f07a445deef2c73b164a1504d4ea76fc243587f1", "references": [{"authors": ["J. Bai", "Y. Li", "J. Li", "Y. Jiang", "S. Xia"], "title": "Rectified decision trees: Towards interpretability, compression and empirical soundness", "year": 1903}, {"authors": ["R.E. Banfield", "L.O. Hall", "K.W. Bowyer", "W.P. Kegelmeyer"], "title": "Ensemble diversity measures and their application to thinning", "venue": "Information Fusion,", "year": 2005}, {"authors": ["O. Bastani", "C. Kim", "H. Bastani"], "title": "Interpretability via model extraction", "venue": "arXiv preprint arXiv:1706.09773,", "year": 2017}, {"authors": ["K. Bennett"], "title": "Decision tree construction via linear programming", "venue": "In Proceedings of the 4th Midwest Artificial Intelligence and Cognitive Science Society Conference,", "year": 1992}, {"authors": ["K. Bennett", "J. Blue"], "title": "Optimal decision trees", "venue": "Technical report, Rensselaer Polytechnique Institute,", "year": 1996}, {"authors": ["D. Bertsimas", "J. Dunn"], "title": "Optimal classification trees", "venue": "Machine Learning,", "year": 2017}, {"authors": ["L. Breiman", "N. Shang"], "title": "Born again trees", "venue": "Technical report, University of California Berkeley,", "year": 1996}, {"authors": ["R. Caruana", "A. Niculescu-Mizil", "G. Crew", "A. Ksikes"], "title": "Ensemble selection from libraries of models", "venue": "In Proceedings of the twenty-first International Conference on Machine Learning,", "year": 2004}, {"authors": ["K. Clark", "Luong", "M.-T", "U. Khandelwal", "C.D. Manning", "Le", "Q.V. Bam"], "title": "born-again multi-task networks for natural language understanding", "year": 1907}, {"authors": ["J. Frankle", "M. Carbin"], "title": "The lottery ticket hypothesis: Finding sparse, trainable neural networks", "venue": "arXiv preprint arXiv:1803.03635,", "year": 2018}, {"authors": ["J. Friedman"], "title": "Greedy function approximation: A gradient boosting machine", "venue": "Annals of Statistics,", "year": 2001}, {"authors": ["J.H. Friedman", "B.E. Popescu"], "title": "Predictive learning via rule ensembles", "venue": "The Annals of Applied Statistics,", "year": 2008}, {"authors": ["N. Frosst", "G. Hinton"], "title": "Distilling a neural network into a soft decision tree", "venue": "arXiv preprint arXiv:1711.09784,", "year": 2017}, {"authors": ["R. Guidotti", "A. Monreale", "S. Ruggieri", "F. Turini", "F. Giannotti", "D. Pedreschi"], "title": "A survey of methods for explaining black box models", "venue": "ACM Computing Surveys (CSUR),", "year": 2018}, {"authors": ["O. G\u00fcnl\u00fck", "J. Kalagnanam", "M. Menickelly", "K. Scheinberg"], "title": "Optimal decision trees for categorical data via integer programming", "venue": "arXiv preprint arXiv:1612.03225,", "year": 2018}, {"authors": ["S. Hara", "K. Hayashi"], "title": "Making tree ensembles interpretable: A bayesian model selection approach", "venue": "arXiv preprint arXiv:1606.09066,", "year": 2016}, {"authors": ["D. Hern\u00e1ndez-Lobato", "G. Martinez-Muoz", "A. Su\u00e1rez"], "title": "Statistical instance-based pruning in ensembles of independent classifiers", "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "year": 2009}, {"authors": ["G. Hinton", "O. Vinyals", "J. Dean"], "title": "Distilling the knowledge in a neural network", "venue": "arXiv preprint arXiv:1503.02531,", "year": 2015}, {"authors": ["X. Hu", "C. Rudin", "M. Seltzer"], "title": "Optimal sparse decision trees", "venue": "In Advances in Neural Information Processing Systems,", "year": 2019}, {"authors": ["K. Kisamori", "K. Yamazaki"], "title": "Model bridging: To interpretable simulation model from neural network", "venue": "arXiv preprint arXiv:1906.09391,", "year": 2019}, {"authors": ["D. Margineantu", "T. Dietterich"], "title": "Pruning adaptive boosting", "venue": "In Proceedings of the Fourteenth International Conference Machine Learning,", "year": 1997}, {"authors": ["G. Mart\u0131\u0301nez-Mu\u00f1oz", "D. Hern\u00e1ndez-Lobato", "A. Su\u00e1rez"], "title": "An analysis of ensemble pruning techniques based on ordered aggregation", "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "year": 2008}, {"authors": ["D.A. Melis", "T. Jaakkola"], "title": "Towards robust interpretability with self-explaining neural networks", "venue": "In Advances in Neural Information Processing Systems,", "year": 2018}, {"authors": ["I. Mollas", "G. Tsoumakas", "N. Bassiliades"], "title": "Lionforests: Local interpretation of random forests through path selection", "venue": "arXiv preprint arXiv:1911.08780,", "year": 2019}, {"authors": ["W. Murdoch", "C. Singh", "K. Kumbier", "R. Abassi-Asl", "B. Yu"], "title": "Interpretable machine learning: definitions, methods, and applications", "venue": "arXiv preprint arXiv:1901.04592v1,", "year": 2019}, {"authors": ["S. Nijssen", "E. Fromont"], "title": "Mining optimal decision trees from itemset lattices", "venue": "In Proceedings of the 13th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "year": 2007}, {"authors": ["S. Park", "J. Furnkranz"], "title": "Efficient prediction algorithms for binary decomposition techniques", "venue": "Data Mining and Knowledge Discovery,", "year": 2012}, {"authors": ["I. Partalas", "G. Tsoumakas", "I. Vlahavas"], "title": "An ensemble uncertainty aware measure for directed hill climbing ensemble pruning", "venue": "Machine Learning,", "year": 2010}, {"authors": ["A.L. Prodromidis", "S.J. Stolfo"], "title": "Cost complexity-based pruning of ensemble classifiers", "venue": "Knowledge and Information Systems,", "year": 2001}, {"authors": ["A.L. Prodromidis", "S.J. Stolfo", "P.K. Chan"], "title": "Effective and efficient pruning of metaclassifiers in a distributed data mining system", "venue": "Knowledge Discovery and Data Mining Journal,", "year": 1999}, {"authors": ["L. Rokach"], "title": "Collective-agreement-based pruning of ensembles", "venue": "Computational Statistics & Data Analysis,", "year": 2009}, {"authors": ["L. Rokach"], "title": "Decision forest: Twenty years of research", "venue": "Information Fusion,", "year": 2016}, {"authors": ["L. Rokach", "O. Maimon", "R. Arbel"], "title": "Selective votinggetting more for less in sensor fusion", "venue": "International Journal of Pattern Recognition and Artificial Intelligence,", "year": 2006}, {"authors": ["C. Rudin"], "title": "Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead", "venue": "Nature Machine Intelligence,", "year": 2019}, {"authors": ["N. Sirikulviriya", "S. Sinthupinyo"], "title": "Integration of rules from a random forest", "venue": "In International Conference on Information and Electronics Engineering,", "year": 2011}, {"authors": ["J. Smith", "J. Everhart", "W. Dickson", "W. Knowler", "R. Johannes"], "title": "Using the ADAP learning algorithm to forecast the onset of diabetes mellitus", "venue": "In Proceedings of the Annual Symposium on Computer Applications in Medical Care,", "year": 1988}, {"authors": ["C. Tamon", "J. Xiang"], "title": "On the boosting pruning problem", "venue": "In Proceedings of the 11th European Conference on Machine Learning,", "year": 2000}, {"authors": ["H.F. Tan", "G. Hooker", "M.T. Wells"], "title": "Tree space prototypes: Another look at making tree ensembles interpretable", "venue": "arXiv preprint arXiv:1611.07115,", "year": 2016}, {"authors": ["G. Vandewiele", "K. Lannoye", "O. Janssens", "F. Ongenae", "F. De Turck", "S. Van Hoecke"], "title": "A genetic algorithm for interpretable model extraction from decision tree ensembles", "venue": "In Pacific-Asia Conference on Knowledge Discovery and Data", "year": 2017}, {"authors": ["S. Verwer", "Y. Zhang"], "title": "Learning optimal classification trees using a binary linear program formulation", "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,", "year": 2019}, {"authors": ["T. Windeatt", "G. Ardeshir"], "title": "An empirical comparison of pruning methods for ensemble classifiers", "venue": "In International Symposium on Intelligent Data Analysis,", "year": 2001}, {"authors": ["H. Zhang", "M. Wang"], "title": "Search for the smallest random forest", "venue": "Statistics and its Interface,", "year": 2009}, {"authors": ["Q. Zhang", "Y. Nian Wu", "Zhu", "S.-C"], "title": "Interpretable convolutional neural networks", "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "year": 2018}, {"authors": ["Y. Zhang", "S. Burer", "W.N. Street"], "title": "Ensemble pruning via semi-definite programming", "venue": "Journal of Machine Learning Research,", "year": 2006}, {"authors": ["Y. Zhou", "G. Hooker"], "title": "Interpreting models via single tree approximation", "venue": "arXiv preprint arXiv:1610.09036,", "year": 2016}, {"authors": ["Z. Zhou", "J. Wu", "W. Tang"], "title": "Ensembling neural networks: many could be better than all", "venue": "Artificial Intelligence,", "year": 2002}, {"authors": ["Zhou", "Z.-H", "W. Tang"], "title": "Selective ensemble of decision trees", "venue": "In International Workshop on Rough Sets, Fuzzy Sets, Data Mining, and Granular-Soft Computing,", "year": 2003}], "sections": [{"heading": "1. Introduction", "text": "Tree ensembles constitute a core technique for prediction and classification tasks. Random forests (Breiman, 2001) and boosted trees (Friedman, 2001) have been used in various application fields, e.g., in medicine for recurrence risk prediction and image classification, in criminal justice for custody decisions, or in finance for credit risk evaluation. Although tree ensembles offer a high prediction quality, distorted predictions in high-stakes decisions can be exceedingly harmful. Here, interpretable machine learning models are essential to understand potential distortions and biases. Research in this domain has significantly increased (Murdoch et al., 2019) with numerous works focusing on the construction of optimal sparse trees (Hu et al., 2019) or on\n1Department of Computer Science, Pontifical Catholic University of Rio de Janeiro (PUC-Rio), Rio de Janeiro, Brazil. 2TUM School of Management, Technical University of Munich, Munich, Germany. Correspondence to: Thibaut Vidal <vidalt@inf.pucrio.br>.\nthe interpretability of neural networks (Zhang et al., 2018; Melis & Jaakkola, 2018).\nCurrently, there exists a trade-off between the interpretability and the performance of tree (ensemble) classifiers. Single decision trees (e.g., those produced by CART) are wellknown for their interpretability, whereas tree ensembles and gradient boosting approaches allow for high prediction quality but are generally more opaque and redundant. Against this background, we study born-again tree ensembles in a similar notion as born-again trees (see, Breiman & Shang, 1996), and search for a simpler classifier that faithfully reproduces the behavior of a tree ensemble.\nFormally, let (X,y) = {xi, yi}ni=1 be a training set in which each xi \u2208 Rp is a p-dimensional numerical feature vector, and each yi \u2208 N is its associated class. Each sample of this training set has been independently drawn from an unknown distribution (X ,Y). Based on this training set, a tree ensemble T learns a function FT : X \u2192 Y that predicts yi for each xi drawn from X . With this notation, we state Problem 1, which is the core of our studies.\nProblem 1 (Born-again tree ensemble) Given a tree ensemble T , we search for a decision tree T of minimal size that is faithful to T , i.e., such that FT (x) = FT (x) for all x \u2208 Rp.\nWe note that the condition FT (x) = FT (x) applies to the entire feature space. Indeed, our goal is to faithfully reproduce the decision function of the tree ensemble for all possible inputs in X . In other words, we are looking for a new representation of the same classifier. Problem 1 depends on the definition of a size metric. In this study, we refer to the size of a tree either as its depth (D) or its number of leaves (L). Additionally, we study a hierarchical objective (DL) which optimizes depth in priority and then the number of leaves. For brevity, we detail the methodology for the depth objective (D) in the main paper. The supplementary material contains the algorithmic adaptations needed to cover the other objectives, rigorous proofs for all theorems, as well as additional illustrations and experimental results.\nTheorem 1 states the computational complexity of Problem 1.\nar X\niv :2\n00 3.\n11 13\n2v 3\n[ cs\n.L G\n] 2\n7 A\nug 2\n02 0\nTheorem 1 Problem 1 is NP-hard when optimizing depth, number of leaves, or any hierarchy of these two objectives.\nThis result uses a direct reduction from 3-SAT. Actually, the same proof shows that the sole fact of verifying the faithfulness of a solution is NP-hard. In this work, we show that despite this intractability result, Problem 1 can be solved to proven optimality for various datasets of practical interest, and that the solution of this problem permits significant advances regarding tree ensemble simplification, interpretation, and analysis."}, {"heading": "1.1. State of the Art", "text": "Our work relates to the field of interpretable machine learning, especially thinning tree ensembles and optimal decision tree construction. We review these fields concisely and refer to Guidotti et al. (2018), Murdoch et al. (2019) and Rudin (2019) for surveys and discussions on interpretable machine learning, as well as to Rokach (2016) for an overview on general work on decision forests.\nThinning tree ensembles has been studied from different perspectives and divides in two different streams, i) classical thinning of a tree ensemble by removing some weak learners from the original ensemble and ii) replacing a tree ensemble by a simpler classifier, e.g., a single decision tree.\nEarly works on thinning focused on finding reduced ensembles which yield a prediction quality comparable to the full ensemble (Margineantu & Dietterich, 1997). Finding such reduced ensembles has been proven to be NP-hard (Tamon & Xiang, 2000) and in some cases reduced ensembles may even outperform the full ensemble (Zhou et al., 2002). While early works proposed a static thinning, dynamic thinning algorithms that store the full ensemble but dynamically query only a subset of the trees have been investigated by Herna\u0301ndez-Lobato et al. (2009), Park & Furnkranz (2012), and Mart\u0131\u0301nez-Mun\u0303oz et al. (2008). For a detailed discussion on this stream of research we refer to Rokach (2016), who discusses the development of ranking-based methods (see, e.g., Prodromidis et al., 1999; Caruana et al., 2004; Banfield et al., 2005; Hu et al., 2007; Partalas et al., 2010; Rokach, 2009; Zhang & Wang, 2009) and search-based methods (see, e.g., Prodromidis & Stolfo, 2001; Windeatt & Ardeshir, 2001; Zhou et al., 2002; Zhou & Tang, 2003; Rokach et al., 2006; Zhang et al., 2006).\nIn their seminal work about born-again trees, Breiman & Shang (1996) were the first to introduce a thinning problem that aimed at replacing a tree ensemble by a newly constructed simpler classifier. Here, they used a tree ensemble to create a data set which is then used to build a born-again tree with a prediction accuracy close to the accuracy of the tree ensemble. Ensuing work followed three different concepts. Meinshausen (2010) introduced the concept of\nnode harvesting, i.e., reducing the number of decision nodes to generate an interpretable tree. Recent works along this line used tree space prototypes to sparsen a tree (Tan et al., 2016) or rectified decision trees that use hard and soft labels (Bai et al., 2019). Friedman & Popescu (2008) followed a different concept and proposed a linear model to extract rules from a tree ensemble, which can then be used to rebuilt a single tree. Similarly, Sirikulviriya & Sinthupinyo (2011) focused on deducing rules from a random forest, while Hara & Hayashi (2016) focused on rule extraction from tree ensembles via bayesian model selection, and Mollas et al. (2019) used a local-based, path-oriented similarity metric to select rules from a tree ensemble. Recently, some works focused on directly extracting a single tree from a tree ensemble based on stabilized but yet heuristic splitting criteria (Zhou & Hooker, 2016), genetic algorithms (Vandewiele et al., 2017), or by actively sampling training points (Bastani et al., 2017a;b). All of these works focus on the creation of sparse decision trees that remain interpretable but can be used to replace a tree ensemble while securing a similar prediction performance. However, these approaches do not guarantee faithfulness, such that the new classifier is not guaranteed to retain the same decision function and prediction performance.\nIn the field of neural networks, related studies were done on model compression (Bucilua\u030c et al., 2006). The proposed approaches often use knowledge distillation, i.e., using a high-capacity teacher to train a compact student with similar knowledge (see, e.g., Hinton et al., 2015). Recent works focused on creating soft decision trees from a neural network (Frosst & Hinton, 2017), decomposing the gradient in knowledge distillation (Furlanello et al., 2018), deriving a class of models for self-explanatory neural networks (Melis & Jaakkola, 2018), or specified knowledge representations in high conv-layers for interpretable convolutional neural networks (Zhang et al., 2018). Focusing on feed-forward neural networks, Frankle & Carbin (2018) proposed pruning techniques that identify subnetworks which perform close to the original network. Clark et al. (2019) studied born-again multi task networks for natural language processing, while Kisamori & Yamazaki (2019) focused on synthesizing an interpretable simulation model from a neural network. As neural networks are highly non-linear and even less transparent than tree ensembles, all of these approaches remain predominantly heuristic and faithfulness is typically not achievable.\nOptimal decision trees. Since the 1990\u2019s, some works focused on constructing decision trees based on mathematical programming techniques. Bennett (1992) used linear programming to construct trees with linear combination splits and showed that this technique performs better than conventional univariate split algorithms. Bennett & Blue (1996)\nfocused on building global optimal decision trees to avoid overfitting, while Nijssen & Fromont (2007) presented an exact algorithm to build a decision tree for specific depth, accuracy, and leaf requirements. Recently, Bertsimas & Dunn (2017) presented a mixed integer programming formulation to construct optimal classification trees. On a similar note, Gu\u0308nlu\u0308k et al. (2018) presented an integer programming approach for optimal decision trees with categorical data, and Verwer & Zhang (2019) presented a binary linear program for optimal decision trees. Hu et al. (2019) presented a scalable algorithm for optimal sparse binary decision trees. While all these works show that decision trees are in general amenable to be built with optimization techniques, none of these works focused on constructing born-again trees that match the accuracy of a given tree ensemble.\nSummary. Thinning problems have been studied for both tree ensembles and neural networks in order to derive interpretable classifiers that show a similar performance than the aforementioned algorithms. However, all of these works embed heuristic construction techniques or an approximative objective, such that the resulting classifiers do not guarantee a behavior and prediction performance equal to the original tree ensemble or neural network. These approaches appear to be plausible for born-again neural networks, as neural networks have highly non-linear structures that cannot be easily captured in an optimization approach. In contrast, work in the field of building optimal decision trees showed that the construction of decision trees is generally amenable for optimization based approaches. Nevertheless, these works focused so far on constructing sparse or optimal trees that outperform heuristically created trees, such that the question whether one could construct an optimal decision tree that serves as a born-again tree ensemble remains open. Answering this question and discussing some of its implications is the focus of our study."}, {"heading": "1.2. Contributions", "text": "With this work, we revive the concept of born-again tree ensembles and aim to construct a single \u2014minimum-size\u2014 tree that faithfully reproduces the decision function of the original tree ensemble. More specifically, our contribution is fourfold. First, we formally define the problem of constructing optimal born-again tree ensembles and prove that this problem is NP-hard. Second, we highlight several properties of this problem and of the resulting born-again tree. These findings allow us to develop a dynamic-programing based algorithm that solves this problem efficiently and constructs an optimal born-again tree out of a tree ensemble. Third, we discuss specific pruning strategies for the born-again tree that allow to reduce redundancies that cannot be identified in the original tree ensemble. Fourth, besides providing theoretical guarantees, we present numerical studies which\nallow to analyze the characteristics of the born-again trees in terms of interpretability and accuracy. Further, these studies show that our algorithm is amenable to a wide range of real-world data sets.\nWe believe that our results and the developed algorithms open a new perspective in the field of interpretable machine learning. With this approach, one can construct simple classifiers that bear all characteristics of a tree ensemble. Besides interpretability gains, this approach casts a new light on tree ensembles and highlights new structural properties."}, {"heading": "2. Fundamentals", "text": "In this section, we introduce some fundamental definitions. Afterwards, we discuss a worst-case bound on the depth of an optimal born-again tree.\nTree ensemble. We define a tree ensemble T as a set of trees t \u2208 T with weights wt. For any sample x, the tree ensemble returns the majority vote of its trees: FT (x) = WEIGHTED-MAJORITY{(Ft(x), wt)}t\u2208T (ties are broken in favor of the smaller index).\nCells. Let Hj be the set of all split levels (i.e., hyperplanes) extracted from the trees for each feature j. We can partition the feature space Rp into cells SELEM = {1, . . . , |H1|+ 1}\u00d7 \u00b7 \u00b7 \u00b7\u00d7 {1, . . . , |Hp|+ 1} such that each cell z = (z1, . . . , zp) \u2208 SELEM represents the box contained between the (zj \u2212 1)th and zthj hyperplanes for each feature j \u2208 {1, . . . , p}. Cells such that zj = 1 (or zj = |Hj | + 1) extend from \u2212\u221e (or to\u221e, respectively) along dimension j. We note that the decision function of the tree ensemble FT (z) is constant in the interior of each cell z, allowing us to exclusively use the hyperplanes of {Hj}dj=1 to construct an optimal born-again tree.\nRegions. We define a region of the feature space as a pair (zL, zR) \u2208 S2ELEM such that zL \u2264 zR. Region (zL, zR) encloses all cells z such that zL \u2264 z \u2264 zR. Let SREGIONS be the set of all regions. An optimal born-again tree T for a region (zL, zR) is a tree of minimal size such that FT (x) = FT (x) within this region.\nFigure 1 depicts a cell and a region on a two-dimensional feature space. We also provide a more extensive example of the born-again tree generation process in the supplementary material. The number of cells and regions increases rapidly with the number of hyperplanes and features, formally:\n|SELEM| = p\u220f\nj=1\n(|Hj |+ 1) (1)\n|SREGION| = p\u220f\nj=1\n(|Hj |+ 1)(|Hj |+ 2) 2 . (2)\nMoreover, Theorem 2 gives initial bounds on the size of the born-again decision tree.\nTheorem 2 The depth of an optimal born-again tree T satisfies \u03a6(T ) \u2264 \u2211 t\u2208T \u03a6(t), where \u03a6(t) represents the depth of a tree t. This bound is tight.\nThis bound corresponds to a worst case behavior which is usually attained only on purposely-designed pathological cases. As highlighted in our computational experiments, the average tree depth remains generally lower than this analytical worst case. Beyond interpretability benefits, the tree depth represents the number of sequential operations (hyperplane comparisons) needed to determine the class of a given sample during the test stage. Therefore, an optimized born-again tree is not only more interpretable, but it also requires less test effort, with useful applications for classification in embarked systems, typically occurring within limited time and processing budgets."}, {"heading": "3. Methodology", "text": "In this section, we introduce a dynamic programming (DP) algorithm which optimally solves Problem 1 for many data sets of practical interest. Let \u03a6(zL, zR) be the depth of an optimal born-again decision tree for a region (zL, zR) \u2208 SREGION. We can then limit the search to optimal born-again trees whose left and right sub-trees represent optimal bornagain trees for the respective sub-regions. Hence, we can recursively decompose a larger problem into subproblems using\n\u03a6(zL, zR) = (3) 0 if ID(zL, zR)\nmin 1\u2264j\u2264p\n{ min\nzLj\u2264l<zRj\n{ 1 + max{\u03a6(zL, zRjl),\u03a6(zLjl, zR)} }} ,\nin which ID(zL, zR) takes value TRUE if and only if all cells z such that zL \u2264 z \u2264 zR admit the same weighted majority class. In this equation, zRjl = z\nR + ej(l \u2212 zRj ) represents the \u201ctop right\u201d corner of the left region obtained in the\nsubdivision, and zLjl = z L + ej(l + 1\u2212 zLj ) is the \u201cbottom left\u201d corner of the right region obtained in the subdivision.\nWhile Equation (3) bears the main rationale of our algorithm, it suffers in its basic state from two main weaknesses that prevent its translation into an efficient algorithm: firstly, each verification of the first condition (i.e., the base case) requires evaluating whether ID(zL, zR) is true and possibly requires the evaluation of the majority class on an exponential number of cells if done brute force. Secondly, the recursive call considers all possible hyperplanes within the region to find the minimum over j \u2208 {1, . . . , p} and zLj \u2264 l < zRj . In the following, we propose strategies to mitigate both drawbacks.\nTo avoid the evaluation of ID(zL, zR) by inspection, we integrate this evaluation within the recursion to profit from the memory structures of the DP algorithm. With these changes, the recursion becomes:\n\u03a6(zL, zR) = (4)\nmin j\n{ min\nzLj\u2264l<zRj\n{ 1jl(z L, zR) + max{\u03a6(zL, zRjl),\u03a6(zLjl, zR)} }}\nwhere 1jl(zL, zR) = 0 if \u03a6(zL, zRjl) = \u03a6(z L jl, z R) = 0 and FT (zL) = FT (zR);\n1 otherwise.\nTo limit the number of recursive calls, we can filter out for each dimension j any hyperplane l \u2208 {1, . . . , |Hj |} such that FT (z) = FT (z + ej) for all z such that zj = l, and exploit two additional properties of the problem.\nTheorem 3 Let (zL, zR) and (z\u0304L, z\u0304R) be two regions such that zL \u2264 z\u0304L \u2264 z\u0304R \u2264 zR, then \u03a6(z\u0304L, z\u0304R) \u2264 \u03a6(zL, zR).\nTheorem 3 follows from the fact that any feasible tree satisfying FT (x) = FT (x) on a region (zL, zR) also satisfies this condition for any subregion (z\u0304L, z\u0304R). Therefore, \u03a6(z\u0304L, z\u0304R) constitutes a lower bound of \u03a6(zL, zR). Combining this bound with Equation (4), we get\nmax{\u03a6(zL, zRjl),\u03a6(zLjl, zR)} \u2264 \u03a6(zL, zR) \u2264 1jl(zL, zR) + max{\u03a6(zL, zRjl),\u03a6(zLjl, zR)}\nfor each j \u2208 {1, . . . , p} and zLj \u2264 l < zRj .\nThis result will be fundamental to use bounding techniques and therefore save numerous recursions during the DP algorithm. With Theorem 4, we can further reduce the number of candidates in each recursion.\nTheorem 4 Let j \u2208 {1, . . . , p} and l \u2208 {zLj , . . . , zRj \u2212 1}.\n\u2022 If \u03a6(zL, zRjl) \u2265 \u03a6(zLjl, zR) then \u2200l\u2032 > l\n1jl(z L, zR) + max{\u03a6(zL, zRjl),\u03a6(zLjl, zR)}\n\u2264 1jl\u2032(zL, zR) + max{\u03a6(zL, zRjl\u2032),\u03a6(zLjl\u2032 , zR)}\n\u2022 If \u03a6(zL, zRjl) \u2264 \u03a6(zLjl, zR) then \u2200l\u2032 < l\n1jl(z L, zR) + max{\u03a6(zL, zRjl),\u03a6(zLjl, zR)}\n\u2264 1jl\u2032(zL, zR) + max{\u03a6(zL, zRjl\u2032),\u03a6(zLjl\u2032 , zR)}.\nBased on Theorem 4, we can discard all hyperplane levels l\u2032 > l in Equation (4) if \u03a6(zL, zRjl) \u2265 \u03a6(zLjl, zR). The same argument holds when \u03a6(zL, zRjl) \u2264 \u03a6(zLjl, zR) with l\u2032 < l. We note that the two cases of Theorem 4 are not mutually exclusive. No other recursive call is needed for the considered feature when an equality occurs. Otherwise, at least one case holds, allowing us to search the range l \u2208 {zLj , . . . , zRj \u2212 1} in Equation (4) by binary search with only O(log(zRj \u2212 zLj )) subproblem calls.\nGeneral algorithm structure. The DP algorithm presented in Algorithm 1 capitalizes upon all the aforementioned properties. It is initially launched on the region representing the complete feature space, by calling BORNAGAIN(zL, zR) with zL = (1, . . . , 1)\u1d40 and zR = (|H1| + 1, . . . , |Hp|+ 1)\u1d40.\nFirstly, the algorithm checks whether it attained a base case in which the region (zL, zR) is restricted to a single cell (Line 1). If this is the case, it returns an optimal depth of zero corresponding to a single leaf, otherwise it tests whether the result of the current subproblem defined by region (zL, zR) is not yet in the DP memory (Line 2). If this is the case, it directly returns the known result.\nPast these conditions, the algorithm starts enumerating possible splits and opening recursions to find the minimum of Equation (4). By Theorem 4 and the related discussions, it can use a binary search for each feature to save many possible evaluations (Lines 9 and 10). By Theorem 3, the exploitation of lower and upper bounds on the optimal solution value (Lines 7, 9, 20, and 21) allows to stop the iterative search whenever no improving solution can exist. Finally, the special case of Lines 13 and 14 covers the case in which \u03a6(zL, zRjl) = \u03a6(z L jl, z\nR) = 0 and FT (zL) = FT (zR), corresponding to a homogeneous region in which all cells have the same majority class. As usual in DP approaches, our algorithm memorizes the solutions of sub-problems and reuses them in future calls (Lines 15, 17, and 26).\nWe observe that this algorithm maintains the optimal solution of each subproblem in memory, but not the solution itself in order to reduce memory consumption. Retrieving the solution after completing the DP can be done with a simple inspection of the final states and solutions, as detailed in the supplementary material.\nThe maximum number of possible regions is |SREGION| =\u220f j (|Hj |+1)(|Hj |+2) 2 (Equation 2) and each call to BORN-\nAGAIN takes up to O( \u2211\nj log |Hj |) elementary operations due to Theorem 4, leading to a worst-case complexity of O(|SREGION| \u2211 j log |Hj |) time for the overall recursive algorithm. Such an exponential complexity is expectable for an NP-hard problem. Still, as observed in our experiments, the number of regions explored with the bounding strategies is much smaller in practice than the theoretical worst case.\nAlgorithm 1 BORN-AGAIN(zL, zR) 1: if (zL = zR) return 0 2: if (zL, zR) exists in memory then 3: return MEMORY(zL, zR) 4: end if 5: UB \u2190\u221e 6: LB \u2190 0 7: for j = 1 to p and LB < UB do 8: (LOW, UP)\u2190 (zLj , zRj ) 9: while LOW < UP and LB < UB do 10: l\u2190 b(LOW + UP)/2c 11: \u03a61 \u2190 BORN-AGAIN(zL, zR + ej(l \u2212 zRj )) 12: \u03a62 \u2190 BORN-AGAIN(zL + ej(l + 1\u2212 zLj ), zR) 13: if (\u03a61 = 0) and (\u03a62 = 0) then 14: if f(zL, T ) = f(zR, T ) then 15: MEMORIZE((zL, zR), 0) and return 0 16: else 17: MEMORIZE((zL, zR), 1) and return 1 18: end if 19: end if 20: UB \u2190 min{UB, 1 + max{\u03a61,\u03a62}} 21: LB \u2190 max{LB,max{\u03a61,\u03a62}} 22: if (\u03a61 \u2265 \u03a62) then UP \u2190 l 23: if (\u03a61 \u2264 \u03a62) then LOW \u2190 l + 1 24: end while 25: end for 26: MEMORIZE((zL, zR), UB) and return UB"}, {"heading": "4. Computational Experiments", "text": "The goal of our computational experiments is fourfold:\n1. Evaluating the computational performance of the proposed DP algorithm as a function of the data set characteristics, e.g., the size metric in use, the number of trees in the original ensemble, and the number of samples and features in the datasets. 2. Studying the structure and complexity of the bornagain trees for different size metrics. 3. Measuring the impact of a simple pruning strategy applied on the resulting born-again trees. 4. Proposing and evaluating a fast heuristic algorithm to find faithful born-again trees.\nThe DP algorithm was implemented in C++ and compiled with GCC 9.2.0 using flag -O3, whereas the original random forests were generated in Python (using scikit-learn v0.22.1). All our experiments were run on a single thread of an Intel(R) Xeon(R) CPU E5-2620v4 2.10GHz, with 128GB of available RAM, running CentOS v7.7. In the remainder of this section, we discuss the preparation of the data and then describe each experiment. Detailed computational results, data, and source codes are available in the supplementary material and at the following address: https://github.com/vidalt/BA-Trees."}, {"heading": "4.1. Data Preparation", "text": "We focus on a set of six datasets from the UCI machine learning repository [UCI] and from previous work by Smith et al. (1988) [SmithEtAl] and Hu et al. (2019) [HuEtAl] for which using random forests (with ten trees) showed a significant improvement upon stand-alone CART. The characteristics of these datasets are summarized in Table 1: number of samples n, number of features p, number of classes K and class distribution CD. To obtain discrete numerical features, we used one-hot encoding on categorical data and binned continuous features into ten ordinal scales. Then, we generated training and test samples for all data sets using a ten-fold cross validation. Finally, for each fold and each dataset, we generated a random forest composed of ten trees with a maximum depth of three (i.e., eight leaves at most), considering p/2 random candidate features at each split. This random forest constitutes the input to our DP algorithm."}, {"heading": "4.2. Computational Effort", "text": "In a first analysis, we evaluate the computational time of Algorithm 1 for different data sets and size metrics. Figure 2 reports the results of this experiment as a box-whisker plot, in which each box corresponds to ten runs (one for each fold) and the whiskers extend to 1.5 times the interquartile range. Any sample beyond this limit is reported as outlier and noted with a \u201c\u25e6\u201d. D denotes a depth-minimization objective, whereas L refers to the minimization of the number of leaves, and DL refers to the hierarchical objective which prioritizes the smallest depth, and then the smallest number of leaves. As can be seen, constructing a born-again tree with\nobjective D yields significantly lower computational times compared to using objectives L and DL. Indeed, the binary search technique resulting from Theorem 4 only applies to objective D, leading to a reduced number of recursive calls in this case compared to the other algorithms.\nIn our second analysis, we focus on the FICO case and randomly extract subsets of samples and features to produce smaller data sets. We then measure the computational effort of Algorithm 1 for metric D (depth optimization) as a function of the number of features (p \u2208 {2, 3, 5, 7, 10, 12, 15, 17}), the number of samples (n \u2208 {250, 500, 750, 1000, 2500, 5000, 7500, 10459}), and the number of trees in the original random forest (T \u2208 {3, 5, 7, 10, 12, 15, 17, 20}). Figure 3 reports the results of this experiment. Each boxplot corresponds to ten runs, one for each fold.\nWe observe that the computational time of the DP algorithm is strongly driven by the number of features, with an exponential growth relative to this parameter. This result is in line with the complexity analysis of Section 3. The number of trees influences the computational time significantly less. Surprisingly, the computational effort of the algorithm actually decreases with the number of samples. This is due to the fact that with more sample information, the decisions of the individual trees of the random forest are less varied, leading to fewer distinct hyperplanes and therefore to fewer possible states in the DP."}, {"heading": "4.3. Complexity of the Born-Again Trees", "text": "We now analyze the depth and number of leaves of the bornagain trees for different objective functions and datasets in Table 2.\nAs can be seen, the different objectives can significantly\ninfluence the outcome of the algorithm. For several data sets, the optimal depth of the born-again tree is reached with any objective, as an indirect consequence of the minimization of the number of leaves. In other cases, however, prioritizing the minimization of the number of leaves may generate 50% deeper trees for some data sets (e.g., PD). The hierarchical objective DL succeeds in combining the benefits of both objectives. It generates a tree with minimum depth and with a number of leaves which is usually close to the optimal one from objective L."}, {"heading": "4.4. Post-Pruned Born-Again Trees", "text": "Per definition, the born-again tree reproduces the same exact behavior as the majority class of the original ensemble classifier on all regions of the feature space X . Yet, some regions of X may not contain any training sample, either due to data scarcity or simply due to incompatible feature values (e.g., \u201csex = MALE\u201d and \u201cpregnancy = TRUE\u201d). These regions may also have non-homogeneous majority classes from the tree ensemble viewpoint due to the combinations of decisions from multiple trees. The born-again tree, however, is agnostic to this situation and imitates the original classifi-\ncation within all the regions, leading to some splits which are mere artifacts of the ensemble\u2019s behavior but never used for classification.\nTo circumvent this issue, we suggest to apply a simple postpruning step to eliminate inexpressive tree sub-regions. We therefore verify, from bottom to top, whether both sides of each split contain at least one training sample. Any split which does not fulfill this condition is pruned and replaced by the child node of the branch that contains samples. The complete generation process, from the original random forest to the pruned born-again tree is illustrated in Figure 4. In this simple example, it is noteworthy that the born-again tree uses an optimal split at the root node which is different from all root splits in the ensemble. We also clearly observe the role of the post-pruning step, which contributes to eliminate a significant part of the tree.\nTo observe the impact of the post-pruning on a larger range of datasets, Table 3 reports the total number of leaves of the random forests, as well as the average depth and number of leaves of the born-again trees before and after post-pruning. As previously, the results are averaged over the ten folds. As can be seen, post-pruning significantly reduces the size of the born-again trees, leading to a final number of leaves which is, on average, smaller than the total number of leaves in the original tree ensemble. This indicates a significant gain of simplicity and interpretability.\nHowever, post-pruning could cause a difference of behavior between the original tree ensemble classifier and the final pruned born-again tree. To evaluate whether this filtering had any significant impact on the classification performance of the born-again tree, we finally compare the out-of-sample accuracy (Acc.) and F1 score of the three classifiers in Table 4.\nFirst of all, the results of Table 4 confirm the faithfulness of our algorithm, as they verify that the prediction quality of the random forests and the born-again tree ensembles are identical. This was expected per definition of Problem 1. Furthermore, only marginal differences were observed between the out-of-sample performance of the born-again tree with pruning and the other classifiers. For the considered datasets, pruning contributed to eliminate inexpressive regions of the tree without much impact on classification performance."}, {"heading": "4.5. Heuristic Born-Again Trees", "text": "As Problem 1 is NP-hard, the computational time of our algorithm will eventually increase exponentially with the number of features (see Figure 3). This is due to the increasing number of recursions, and to the challenge of testing homogeneity for regions without exploring all cells. Indeed, even proving that a given region is homogeneous (i.e., that it contains cells of the same class) remains NP-hard, although it is solvable in practice using integer programming techniques. Accordingly, we take a first step towards scalable heuristic algorithms in the following. We therefore explain how our born-again tree construction algorithm can be modified to preserve the faithfulness guarantee while achieving only a heuristic solution in terms of size.\nWe made the following adaptations to Algorithm 1 to derive its heuristic counterpart. For each considered region\n(zL, zR), we proceed as follows.\n1. Instead of evaluating all possible splits and opening recursions, we randomly select Nc = 1000 cells in the region and pick the splitting hyperplane that maximizes the information gain.\n2. If all these cells belong to the same class, we rely on an integer programming solver to prove whether the region is homogeneous or not. If the region is homogeneous, we define a leaf. Otherwise, we have detected a violating cell, and continue splitting until all regions are homogeneous to guarantee faithfulness.\nWith these adaptations, the heuristic algorithm finds bornagain trees that are guaranteed to be faithful but not necessarily minimal in size. Table 5 compares the computational time of the optimal born-again tree algorithm using objective D \u201cTD(s)\u201d, objective L \u201cTL(s)\u201d with that of the heuristic algorithm \u201cTH(s)\u201d. It also reports the percentage gap of the heuristic tree depth \u201cGapD(%)\u201d and number of leaves \u201cGapL(%)\u201d relative to the optimal solution values of each objective.\nAs visible in these experiments, the CPU time of the heuristic algorithm is significantly smaller than that of the optimal method, at the expense of an increase in tree depth and number of leaves, by 22.53% and 20.10% on average, respectively. To test the limits of this heuristic approach, we also verified that it could run in a reasonable amount of time (faster than a minute) on larger datasets such as Ionosphere, Spambase, and Miniboone (the latter with over 130,000 samples and 50 features)."}, {"heading": "5. Conclusions", "text": "In this paper, we introduced an efficient algorithm to transform a random forest into a single, smallest possible, decision tree. Our algorithm is optimal, and provably returns a single tree with the same decision function as the original tree ensemble. In brief, we obtain a different representation of the same classifier, which helps us to analyze random forests from a different angle. Interestingly, when investigating the structure of the results, we observed that born-again\ndecision trees contain many inexpressive regions designed to faithfully reproduce the behavior of the original ensemble, but which do not contribute to effectively classify samples. It remains an interesting research question to properly understand the purpose of these regions and their contribution to the generalization capabilities of random forests. In a first simple experiment, we attempted to apply post-pruning on the resulting tree. Based on our experiments on six structurally different datasets, we observed that this pruning does not diminish the quality of the predictions but significantly simplifies the born-again trees. Overall, the final pruned trees represent simple, interpretable, and high-performance classifiers, which can be useful for a variety of application areas.\nAs a perspective for future work, we recommend to progress further on solution techniques for the born-again tree ensembles problem, proposing new optimal algorithms to effectively handle larger datasets as well as fast and accurate heuristics. Heuristic upper bounds can also be jointly exploited with mathematical programming techniques to eliminate candidate hyperplanes and recursions. Another interesting research line concerns the combination of the dynamic programming algorithm for the construction of the born-again tree with active pruning during construction, leading to a different definition of the recursion and to different base-case evaluations. Finally, we recommend to pursue the investigation of the structural properties of tree ensembles in light of this new born-again tree representation."}, {"heading": "Acknowledgements", "text": "The authors gratefully thank the editors and referees for their insightful recommendations, as well as Simone Barbosa, Quentin Cappart, and Artur Pessoa for rich scientific discussions. This research has been partially supported by CAPES, CNPq [grant number 308528/2018-2] and FAPERJ [grant number E-26/202.790/2019] in Brazil."}], "title": "Born-Again Tree Ensembles ", "year": 2020}