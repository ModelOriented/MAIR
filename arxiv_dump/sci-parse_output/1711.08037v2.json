{"abstractText": "\u201cI work with medical data. We work with doctors and they\u2019re interested in predicting risk of mortality, recognizing cancer in radiologic scans, and spotting diagnoses based on electronic health record data. We can train a model, and it can even give us the right answer. But we can\u2019t just tell the doctor \u201cmy neural network says this patient has cancer!\u201d The doctor just won\u2019t accept that! They want to know why the neural network says what it says. They want an explanation. They need interpretable models.\" \u2014 Prototypical call to arms", "authors": [], "id": "SP:ae9a452e8d6bce863de345111c3d5ab3330cb796", "references": [{"authors": ["Solon Barocas", "Andrew D Selbst"], "title": "Big data\u2019s disparate impact. California Law Review, 2016", "venue": "Bryce Goodman and Seth Flaxman. European Union regulations on algorithmic decision-making", "year": 2016}, {"authors": ["Sandra Wachter", "Brent Mittelstadt", "Luciano Floridi"], "title": "Why a right to explanation of automated decision-making does not exist in the general data protection regulation", "venue": "Algorithms and explanations. http://www.law.nyu.edu/centers/ili/events/algorithms-and-explanations,", "year": 2017}], "sections": [{"text": "ar X\niv :1\n71 1.\n08 03\n7v 2\n[ st\nat .M\nL ]\n2 4\nN ov\n2 01\n7"}, {"heading": "The Doctor Just Won\u2019t Accept That!", "text": "Zachary C. Lipton Carnegie Mellon University,\nAmazon AI zlipton@cmu.edu\n\u201cI work with medical data. We work with doctors and they\u2019re interested in predicting risk of mortality, recognizing cancer in radiologic scans, and spotting diagnoses based on electronic health record data. We can train a model, and it can even give us the right answer. But we can\u2019t just tell the doctor \u201cmy neural network says this patient has cancer!\u201d The doctor just won\u2019t accept that! They want to know why the neural network says what it says. They want an explanation. They need interpretable models.\" \u2014 Prototypical call to arms"}, {"heading": "Introduction", "text": "Each day, machine learning practitioners integrate predictive models more deeply into real-world decisions. As the steady creep touches sensitive domains, like medical decision-making, financial lending, and criminal justice, refrains like the fictional exemplar printed above echo throughout technical, industrial, and political spheres. In order to know if the model is {fair, safe, reliable, robust, sensible}, we need to know how the model makes its decisions. Or perhaps we need to know why the model makes its decisions. We will return to interpretability\u2019s polysemy [Lipton, 2016] later.\nThese calls to arms express a well-founded discomfort with machine learning. How can a software agent that does not even know what a loan is decide who qualifies for one? Indeed, we ought to be cautious about injecting machine learning (or anything else, for that matter) into applications where there may be a significant risk of causing social harm.\nHowever, \u201cthe doctor just won\u2019t accept that\u201d does not provide a solid foundation for a proposed field of study. For the field of interpretable machine learning to advance, we must ask the following questions: What precisely won\u2019t the various stakeholders accept? What do they want? Are these desiderata reasonable? Are they feasible? Returning to automated lending, if stakeholders demanded that the decision-maker understand what a loan is, and what it means to pay one off, and the purpose of an explanation were to demonstrate such understanding, then no conceivable interpretation technique could salvage a supervised learning model. In order to answer these questions, we\u2019ll have to give real-world problems and their respective stakeholders greater consideration.\nThe Interpreters\nEager to heed the demand for interpretable models, researchers regularly propose algorithms claimed to constitute interpretable machine learning. However, these papers wield the term interpretability in many disparate senses. For example, some propose that interpretable models are by definition simple, tying the interpretability of the model to the (inverse) cognitive strain endured by a wouldbe interpreter. Others advocate models where each parameter has some intuitive significance. For example, linear models are often held up as interpretable owing to the correspondence between each weight and an input feature. Practitioners and customers might eyeball the parameters to see whether they accord with intuition. While this mode of interpreting linear models may be specious, we put\nInterpretable ML Symposium, 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\nHowever, amid the worry of machine learning run amok, the ensuing call for interpretability, and the rush to satisfy this demand, technical solutions have arrived ahead of technical questions. Papers claim interpretability, often axiomatically, without defining it. Sometimes the proof that a model is interpretable is simply that it yields an appealing visualization. Just look, see how interpretable it is!\nConfusing matters, the conversation around interpretability has lurched to the fore amid the rise of deep learning. As a result, some specific interpretability problems are misattributed to the opacity of neural networks even when they are shared more widely by all supervised learning algorithms. For example, linear models also fail to differentiate essential vs. spurious correlations. They do not necessarily elucidate causal structure and they are not cognizant of human notions of fairness.\nIn the rush to satisfy demand, important steps are routinely skipped. Namely, we fail to ask what end the proposed interpretability serves. We also fail to ask if these desiderata can be satisfied by new models within current frameworks (say, supervised learning) or if they speak to fundamental limitations of the paradigm. Only when we know what is desired and that it might feasibly be provided within a given paradigm, does it make sense to propose model properties and post-hoc techniques. Absent these considerations, algorithmic proposals lack clear problem definitions. With a surfeit of hammers, and no agreed-upon nails, the field risks failing to mature owing to shaky foundations."}, {"heading": "The Detractors", "text": "Meanwhile, detractors often question the entire enterprise of interpretable machine learning, suggesting that the calls to arms are misguided. Others question whether human decisions are interpretable in the first place. The logic goes: if we presently trust humans with all decisions, despite their lacking interpretability, then why should interpretability be required of the decision-makers that succeed us? However, these \u201chow do we know that humans can X?\u201d questions sit on shaky ground as well. Every speaker at this conference can explain why they submitted papers, and even elucidate a precise chain of events by which they expected the action might culminate in a trip to California. Surely, humans can explain their actions by reference to a robust knowledge of the dynamics by which our society and the physical world operate, and surely such reasoning speaks to what some (but not all) of what people mean by explanations.\nTo drive the point home, of all the people who claim that humans are unable to explain their actions, it would be hard to find one person among them who actually lives by this logic, never asking any other person to explain their actions. While humans can explain actions, they are neither transparent, parsimonious (in parameters), decomposable, nor monotonic. Humans may lie, and our explanations may fail to fully explain our decisions, but the whole sale dismissal of human explainability tramples on common sense.\nBoth proponents and detractors of interpretable machine learning go astray in similar fashion: by failing to fix more specific definitions. Is a linear model interpretable? In the sense that it is simple, yes. In the sense that it provides verbal explanations of its actions, maybe, via post-hoc techniques. In the sense that it explains what dynamics in the world it depends upon, decidedly not. For a human decision-maker, the answers are nearly opposite. In order to advance our knowledge of machine learning and its amenability to interpretation, we must move past the word interpretable and decide which definitions to focus on. Moreover, we must confront the normative question of deciding which ones are important."}, {"heading": "The Stakeholders", "text": "If demands for interpretability speak to real shortcomings of machine learning, especially vis-a-vis socially impactful applications, and if interpretable machine learning simultaneously admits no clear definition that might serve as a foundation upon which a traditional technical (vs. critical) discipline might flourish, then what next steps might we take? What should we do at an interpretability symposium? Should we teach beginners the current tools of interpretable AI? Or should we debate the\nCuriously, while interpretable machine learning is touted to address the demands of stakeholders, those stakeholders are seldom taken seriously. In machine learning papers, we consider their needs carelessly. While variants of \u201cthe doctors just won\u2019t accept that!\u201d echo throughout convention centers packed with machine learners across the globe, few have bothered to ask what precisely they do want.\nIn my subjective experience, the policy communities take a considerably broader view. At NYU Law, a recent workshop on Algorithms and Explanations [NYU Information Law Institute, 2017] engaged policy experts alongside stakeholders in fields spanning medicine, journalism, policing, and lending, and also machine learning scientists.\nBy contrast, the machine learning community, despite claiming these topics as motivation, remains conspicuously isolated. More mature subfields might thrive amid such disciplinary modularity. Image recognition is sufficiently advanced and its applicability to industrial problems so well understood, that you could develop a classifier for ripe vs. unripe fruit without a deep investment in the produce industry. The API is known by both stakeholders and scientists. But absent consensus on problem definitions, this isolation can impede progress. Consider the widely-discussed provision of the European Union General Data Privacy Regulation (GDPR) popularly described as a right to explanation. While both the meaning and enforceability of the law and whether it amounts at all to a right to explanation remain under debate [Wachter et al., 2017, Goodman and Flaxman, 2016], machine learning papers reference the ordinance glancingly via the one paper [Goodman and Flaxman, 2016] which presented the material for a machine learning audience. Similar issues have emerged in algorithmic fairness, where a single genre-crossing paper [Barocas and Selbst, 2016] holds uncontested authority owing not only its high quality, but also to a general ignorance of policy in machine learning circles."}, {"heading": "The Conclusions", "text": "These are hard questions and the academic incentives don\u2019t encourage researchers to pursue them. Careers are judged on technical work, and technical machine learning conferences do not frequently publish position papers. So we find ourselves in a world where machine learning conferences will publish papers that offer a claimed interpretable algorithm, but not those that address the foundations of such claims. For work in this field to be meaningful and for this field to progress, we must take problem formulation more seriously. The conversation must move beyond \u201cthe doctor just won\u2019t accept that!\u201d We should determine what the various stakeholders demanding interpretability want, and which of these desiderata can actually be satisfied within the current learning paradigm. To do any of this effectively, we must invite the stakeholders to participate in the conversation."}], "year": 2017}