{"abstractText": "In this paper, we explore connections between interpretable machine learning and learning theory through the lens of local approximation explanations. First, we tackle the traditional problem of performance generalization and bound the testtime accuracy of a model using a notion of how locally explainable it is. Second, we explore the novel problem of explanation generalization which is an important concern for a growing class of finite sample-based local approximation explanations. Finally, we validate our theoretical results empirically and show that they reflect what can be seen in practice.", "authors": [{"affiliations": [], "name": "Jeffrey Li"}, {"affiliations": [], "name": "Vaishnavh Nagarajan"}, {"affiliations": [], "name": "Gregory Plumb"}, {"affiliations": [], "name": "Ameet Talwalkar"}], "id": "SP:745f5c54d4994016dc3d9cce05cc6ec6565f10e3", "references": [{"authors": ["Sylvain Arlot", "Robin Genuer"], "title": "Analysis of purely random forests bias", "year": 2014}, {"authors": ["Sanjeev Arora", "Rong Ge", "Behnam Neyshabur", "Yi Zhang"], "title": "Stronger generalization bounds for deep nets via a compression approach", "venue": "Proceedings of Machine Learning Research,", "year": 2018}, {"authors": ["Dheeru Dua", "Casey Graff"], "title": "UCI machine learning repository, 2017", "venue": "URL http://archive. ics.uci.edu/ml", "year": 2017}, {"authors": ["Gintare Karolina Dziugaite", "Daniel M. Roy"], "title": "Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data", "venue": "In Proceedings of the Thirty-Third Conference on Uncertainty in Artificial Intelligence,", "year": 2017}, {"authors": ["Jianqing Fan"], "title": "Local linear regression smoothers and their minimax efficiencies", "venue": "The Annals of Statistics, 21,", "year": 1993}, {"authors": ["Jianqing Fan", "Ir\u00e8ne Gijbels"], "title": "Local polynomial modelling and its applications. Number 66 in Monographs on statistics and applied probability series", "year": 1996}, {"authors": ["John Langford", "Rich Caruana"], "title": "Not) bounding the true error", "venue": "Advances in Neural Information Processing Systems", "year": 2002}, {"authors": ["John Langford", "John Shawe-Taylor"], "title": "Pac-bayes & margins", "venue": "Advances in Neural Information Processing Systems", "year": 2003}, {"authors": ["David McAllester"], "title": "Simplified pac-bayesian margin bounds", "venue": "Learning Theory and Kernel Machines,", "year": 2003}, {"authors": ["David A McAllester"], "title": "Some pac-bayesian theorems", "venue": "In 11th annual conference on Computational learning theory,", "year": 1998}, {"authors": ["Vaishnavh Nagarajan", "J. Zico Kolter"], "title": "Uniform convergence may be unable to explain generalization in deep learning", "venue": "In Advances in Neural Information Processing Systems", "year": 2019}, {"authors": ["Behnam Neyshabur", "Ryota Tomioka", "Nathan Srebro"], "title": "In search of the real inductive bias: On the role of implicit regularization in deep learning", "year": 2014}, {"authors": ["Gregory Plumb", "Denali Molitor", "Ameet S Talwalkar"], "title": "Model agnostic supervised local explanations", "venue": "Advances in Neural Information Processing Systems", "year": 2018}, {"authors": ["Gregory Plumb", "Maruan Al-Shedivat", "Angel Alexander Cabrera", "Adam Perer", "Eric Xing", "Ameet Talwalkar"], "title": "Regularizing black-box models for improved interpretability, 2020", "venue": "URL https: //arxiv.org/abs/1902.06787", "year": 1902}, {"authors": ["Marco Tulio Ribeiro", "Sameer Singh", "Carlos Guestrin"], "title": "Why should i trust you?\u201d: Explaining the predictions of any classifier", "venue": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "year": 2016}, {"authors": ["Marco Tulio Ribeiro", "Sameer Singh", "Carlos Guestrin"], "title": "Anchors: High-precision model-agnostic explanations", "venue": "In AAAI Conference on Artificial Intelligence", "year": 2018}, {"authors": ["Lesia Semenova", "Cynthia Rudin", "Ronald Parr"], "title": "A study in rashomon curves and volumes: A new perspective on generalization and model simplicity in machine learning, 2019", "venue": "URL https: //arxiv.org/abs/1908.01755", "year": 1908}, {"authors": ["Jinsung Yoon", "Sercan O. Arik", "Tomas Pfister"], "title": "RL-LIM: Reinforcement learning-based locally interpretable modeling, 2019", "venue": "URL https://arxiv.org/abs/1909.12367", "year": 1909}, {"authors": ["Chiyuan Zhang", "Samy Bengio", "Moritz Hardt", "Benjamin Recht", "Oriol Vinyals"], "title": "Understanding deep learning requires rethinking generalization", "venue": "In 5th International Conference on Learning Representations,", "year": 2017}], "sections": [{"text": "In this paper, we explore connections between interpretable machine learning and learning theory through the lens of local approximation explanations. First, we tackle the traditional problem of performance generalization and bound the testtime accuracy of a model using a notion of how locally explainable it is. Second, we explore the novel problem of explanation generalization which is an important concern for a growing class of finite sample-based local approximation explanations. Finally, we validate our theoretical results empirically and show that they reflect what can be seen in practice."}, {"heading": "1 INTRODUCTION", "text": "There has been a growing interest in interpretable machine learning, which seeks to help people understand their models. While interpretable machine learning encompasses a wide range of problems, it is a fairly uncontroversial hypothesis that there exists a trade-off between a model\u2019s complexity and general notions of interpretability. This hypothesis suggests a seemingly natural connection to the field of learning theory, which has thoroughly explored relationships between a function class\u2019s complexity and generalization. However, formal connections between interpretability and learning theory remain relatively unstudied.\nThough there are several notions of conveying interpretability, one common and flexible approach is to use local approximations. Formally, local approximation explanations (which we will refer to as \u201clocal explanations\u201d) provide insight into a model\u2019s behavior as follows: for any black-box model f \u2208 F and input x, the explanation system produces a simple function, gx(x\u2032) \u2208 Glocal, which approximates f in a specified neighborhood, x\u2032 \u223c Nx. Crucially, the freedom to specify both Glocal andNx grants local explanations great versatility. In this paper, we provide two connections between learning theory and how well f can be approximated locally (i.e. the fidelity of local explanations).\nOur first result studies the standard problem of performance generalization by relating test-time performance to a notion of local interpretability. As it turns out, our focus on local explanations leads us to unique tools and insights from a learning theory point of view. Our second result identifies and addresses an unstudied \u2013 yet important \u2013 question regarding explanation generalization. This question pertains to a growing class of explanation systems, such as MAPLE (Plumb et al., 2018) and RL-LIM (Yoon et al., 2019), which we call finite sample-based local explanations1. These methods learn their local approximations using a common finite sample drawn from D (in contrast to local approximation methods such as LIME (Ribeiro et al., 2016)) and, as a result, run the risk of overfitting to this finite sample. In light of this, we answer the following question: for these explanation-learning systems, how well do they generalize to data not seen during training?\n\u2217Denotes equal contribution 1This terminology is not to be confused with example-based explanations where the explanation itself is in\nthe form of data instances rather than a function.\nar X\niv :2\n01 1.\n01 20\n5v 1\n[ cs\n.L G\n] 2\nN ov\n2 02\nWe address these questions with two bounds, which we outline now. Regarding performance generalization, we derive our first main result, Theorem 1, which bounds the expected test mean squared error (MSE) of any f in terms of its MSE over the m samples in the training set, S = {(xi, yi)}mi=1:\nE(x,y)\u223cD[(f(x)\u2212 y)2]\ufe38 \ufe37\ufe37 \ufe38 Test MSE\n\u2264 O\u0303 ( 1 m m\u2211 i=1\n(f(xi)\u2212 yi)2\ufe38 \ufe37\ufe37 \ufe38 Train MSE +E x\u223cD, x\u2032\u223cNx\n[ (gx\u2032(x)\u2212 f(x))2 ] \ufe38 \ufe37\ufe37 \ufe38\nInterpretability Term (MNF)\n+ \u03c1SR\u0302S(Glocal)\ufe38 \ufe37\ufe37 \ufe38 Complexity Term\n)\nRegarding explanation generalization for finite sample-based explanation-learning systems, we apply a similar proof technique to obtain Theorem 2, which bounds the quality of the system\u2019s explanations on unseen data in terms of their quality on the data on which the system was trained:\nE x\u223cD, x\u2032\u223cNx\n[ (gx\u2032(x)\u2212 f(x))2 ] \ufe38 \ufe37\ufe37 \ufe38\nTest MNF\n\u2264 1 m m\u2211 i=1 Ex\u2032\u223cNxi [ (f(xi)\u2212 gx\u2032(xi))2 ] \ufe38 \ufe37\ufe37 \ufe38\nTrain MNF\n+ O\u0303 ( \u03c1SR\u0302S(Glocal) ) \ufe38 \ufe37\ufe37 \ufe38\nComplexity Term\nBefore summarizing our contributions, we discuss the key new terms and their relationship.\n\u2022 Interpretability terms: The terms involving MNF correspond to Mirrored Neighborhood Fidelity, a metric we use to measure local explanation quality. As we discuss in Section 3, this is a reasonable modification of the commonly used Neighborhood Fidelity (NF) metric (Ribeiro et al., 2016; Plumb et al., 2018). Intuitively, we generally expect MNF to be larger when the neighborhood sizes are larger since the gx\u2032 are required to extrapolate farther.\n\u2022 Complexity term: This term measures the complexity of the local explanation system g in terms of (a) the complexity of the local explanation class Glocal and (b) \u03c1S , a quantity that we define and refer to as neighborhood disjointedness factor. As we discuss in Section 4, \u03c1S is a value in [1, \u221a m] (where m = |S|) that is proportional to the level of disjointedness of the neighborhoods\nfor points in the sample S. Intuitively, we expect \u03c1S to be larger when the neighborhoods sizes are smaller since smaller neighborhoods will overlap less.\nNotably, both our bounds capture the following key trade-off: as neighborhood widths increase, MNF increases but \u03c1S decreases. As such, our bounds are non-trivial only if Nx can be chosen such that MNF remains small but \u03c1S grows slower than O\u0303( \u221a m) (since R\u0302S(Glocal) decays as O\u0303(1/ \u221a m)).\nWe summarize our main contributions as follows:\n(1) We make a novel connection between performance generalization and local explainability, arriving at Theorem 1. Given the relationship between MNF and \u03c1S , this bound roughly captures that an easier-to-interpret f enjoys better generalization guarantees, a potentially valuable result when reasoning about F is difficult (e.g. for neural networks). Further, our proof technique may be of independent theoretical interest as it provides a new way to bound the Rademacher complexity of a randomized function (see Section 4).\n(2) We motivate and explore an important generalization question about expected explanation quality. Specifically, we arrive at Theorem 2, a bound for test MNF in terms of training MNF. This bound suggests that practitioners can better guarantee good local explanation quality (measured by MNF) using methods which encourage the neighborhood widths to be wider (see Section 5).\n(3) We verify empirically on UCI Regression datasets that our results non-trivially reflect the two types of generalization in practice. First, we demonstrate that \u03c1 can indeed exhibit slower than O\u0303( \u221a m) growth without significantly increasing the MNF terms. Also, for Theorem 2, we show\nthat the generalization gap indeed improves with larger neighborhoods (see Section 6). (4) To aid in our theoretical results, we propose MNF as a novel measure of local explainability.\nThis metric naturally complements NF and offers a unique advantage over NF when evaluating local explanations on \u201crealistic\u201d on-distribution data (see Section 3)."}, {"heading": "2 RELATED WORK", "text": "Interpretability meets learning theory: Semenova et al. (2019) study the performance generalization of models learned from complex classes when they can be globally well-approximated by\nsimpler (e.g. interpretable) classes. In such cases, their theory argues that if the complex class has many models that perform about as optimally on training data, generalization from the complex class can be more closely bounded in terms of the simpler class\u2019s complexity. In our corresponding results, we similarly aim to avoid involving the larger class\u2019s complexity. However, we directly study generalization via a function\u2019s local explainability, rather than instantiate abstract \u201dcomplex\u201d and \u201csimple\u201d classes for global approximations. The two are fundamentally different technical problems; standard learning theory results cannot be directly applied as they are for single-function global approximations.\nStatistical localized regression: (Fan, 1993; Fan & Gijbels, 1996) are canonical results which bound the squared error of a nonparametric function defined using locally fit models. These local models are both simple (e.g. linear) and similarly trained by weighting real examples with a kernel (i.e. neighborhood). However, in these works, each local model is only used to make a prediction at its source point and the theory requires shrinking the kernel width towards 0 as the sample size grows. We instead fit local models as explanations for a trained model (i.e. which is considered the \u201ctrue regression function\u201d) and more importantly, care about the performance of each local model over whole (non-zero) neighborhoods. Unlike localized regression, this allows us to use uniform convergence to bound test error with empirical and generalization terms. While the previous results do not have empirical terms, the learning rates are exponential in the number of samples.\nLearning Theory: One line of related work also studies how to explain generalization of overparameterized classes. As standard uniform convergence on these classes often leads to vacuous bounds, a general approach that has followed from (Nagarajan & Kolter, 2019; Zhang et al., 2017; Neyshabur et al., 2014) has been to study implications of different biases placed on the learned models. We study what would happen if an overparameterized model had an unexplored type of bias, one that is inspired by local explainability. Additionally, our work\u2019s technical approach also parallels another line of existing results which likewise try to apply uniform convergence on a separate surrogate class. This includes PAC-Bayesian bounds, a large family of techniques that come from looking at a stochastic version of in parameter space (McAllester, 1998; 2003; Langford & Caruana, 2002; Langford & Shawe-Taylor, 2003). In a different vein, some results in deep learning look at compressed/sparsified/explicitly regularized surrogates of neural networks (Arora et al., 2018; Dziugaite & Roy, 2017). In our case, the surrogate class is a collection of local explanations."}, {"heading": "3 MIRRORED NEIGHBORHOOD FIDELITY", "text": "In order to connect local explanations to generalization, recall that we study a measure of local interpretability which we call \u201cmirrored neighborhood fidelity\u201d (MNF). As we explain below, this quantity comes from a slight modification to an existing measure of interpretability, namely, that of neighborhood fidelity (NF).\nTo define our terms, we use the following notations. Let X be an input space and let D be a distribution over X \u00d7Y where Y \u2286 R. Let F be a class of functions f : X \u2192 Y . For our theoretical results, we specifically assume that Y is bounded as Y = [\u2212B,B] for someB > 0 (though this does not matter until following sections). In order to provide local explanations, we need to fix a local neighborhood around each x \u2208 X . To this end, for any x, let Nx correspond to some distribution denoting a local neighborhood at x e.g., typically this is chosen to be a distribution centered at x. For any distribution N , we use pN (x) to denote its density at x. Now, let G be a class of explainers g : X \u00d7 X \u2192 Y such that for each x \u2208 X , the local explanation g(x, \u00b7) : X \u2192 Y belongs to a class of (simple) functions (e.g. linear), Glocal. In short, we denote g(x, \u00b7) as gx(\u00b7) and we\u2019ll use g(x, \u00b7) to locally approximate f in the neighborhood defined by Nx.\nThe accuracy of this local approximation is usually quantified by a term called \u201cneighbhorhood fidelity\u201d which is defined as follows (Ribeiro et al., 2016; 2018; Plumb et al., 2018; 2020)\nNF(f, g) := Ex\u223cD [ Ex\u2032\u223cNx [ (f(x\u2032)\u2212 gx(x\u2032))2 ]] .\nTo verbally interpet this, let us call x as the \u201csource\u201d point which gives rise to a local explanation gx(\u00b7) and x\u2032 the \u201ctarget\u201d point that we try to fit using g. To compute NF(f, g), we need to do the following: for each source point x, we first compute the average error in the fit of gx(\u00b7) over target points x\u2032 in the local neighborhood of the source point x (i.e., Nx); then, we globally average this error across draws of the source point x \u223c D.\nNow, to define MNF, we take the same expression as NF but swap x and x\u2032 within the innermost expectation (without modifying the expectations). In other words, we now sample a target point x from D, and sample a source point x\u2032 from a distribution over points near x. Since this distribution is over source points rather than target points, just for the sake of distinguishing, we\u2019ll call this a mirrored neighborhood distribution and denote it as Nmirx . Formally we define this measure of local interpretability below, following which we explain how to understand it: Definition 3.1. (Mirrored Neighborhood Fidelity) We define MNF : F \u00d7 G \u2192 R as\nMNF(f, g) := Ex\u223cD [ Ex\u2032\u223cNmirx [ (f(x)\u2212 gx\u2032(x))2 ]] .\nand with an abuse of notation, we let MNF(f, g, x) := Ex\u2032\u223cNmirx [ (f(x)\u2212 gx\u2032(x))2 ] .\nUnderstanding MNF. It is helpful to parse the expression for MNF in two different ways. First, we can think of it as measuring the error in approximating every target point x \u2208 X through a randomized locally-approximating function gx\u2032(\u00b7) where x\u2032 is randomly drawn from the local neighborhood Nmirx . A second way to parse this is in a manner similar to how we parsed NF. To do this, first we note that the expectations in MNF can be swapped around and rewritten equivalently as follows:\nMNF(f, g) = Ex\u2032\u223cD\u2020 [ Ex\u223cN\u2020\nx\u2032\n[ (f(x)\u2212 gx\u2032(x))2 ]] ,\nwhere D\u2020 and N\u2020x\u2032 are suitably defined distributions (defined in Appendix A) that can be thought of as modified counterparts of D and Nmirx\u2032 respectively. With this rewritten expression, one can read MNF like NF: for each source point (here that is x\u2032), we compute the average error in the fit of the corresponding local function (gx\u2032(\u00b7)) over target points (x) in the local neighborhood of the source point (N\u2020x\u2032 ); this error is then globally averaged over different values of the source point (x\n\u2032 \u223c D\u2020). While both NF and MNF are closely related measures of local interpretability for f , studying MNF allows us to make connections between local interpretability and different notions of generalization (Sections 4 and 5). Furthermore, MNF may also be of interest to the interpretability community, as it offers a unique advantage over NF when the intended usage of local explanations is centered around understanding how the model works on the specific learning task it was trained on.\nSpecifically, we argue that selecting the target point distribution to be D rather than D perturbed by Nx (as for NF) better emphasizes the ability for explanations to accurately convey how well g will predict at realistic points. This is relevant for ML (and deep learning particularly) because (a) high-dimensional datasets often exhibit significant feature dependencies and adherence to lower dimensional manifolds; (b) f can often be highly unpredictable and unstable when extrapolating beyond the training data. As such, when one measures NF with standard neighborhood choices that ignore feature dependencies (i.e. most commonly Nx = N (x, \u03c3I)), the resulting target distribution may concentrate significantly on regions that are non-relevant to the actual task at hand. As can be shown, this can lead to overemphasis on fitting noisy off-manifold behavior, deteriorating the fit of explanations relative to task-relevant input regions (we defer a more detailed presentation of this point, as well as other trade-offs between NF and MNF to Appendix A).\n4 GENERALIZATION OF MODEL PERFORMANCE VIA MNF\nThe generalization error of the function f is typically bounded by some notion of the representational capacity/complexity of f . While standard results bound complexity in terms of parameter counts, there is theoretical value in deriving bounds involving other novel terms. By doing so, we can understand how regularizing for those terms can affect the representation capacity, and in turn, the generalization error of f . Especially when f \u2019s complexity may be intractable to bound on its own, introducing these terms provides a potentially useful new way to understand f \u2019s generalization.\nHere specifically, we are interested in establishing a general connection between the representation complexity and the local intrepretability of any f . This naturally requires coming up with a notion that appropriately quantifies the complexity of G, which we discuss in the first part of this section. In the second part, we then relate this quantity to the generalization of f to derive our first main result.\nKey technical challenge: bounding the complexity of G. The overall idea behind how one could tie the notions of generalization and local interpretability is fairly intuitive. For example, consider\na simplified setting where we approximate f by dividing X into K disjoint pieces/neighborhoods, and then approximating each neighborhood via a simple (say, linear) model. Then, one could bound the generalization error of f as the sum of two quantities: first, the error in approximating f via the piecewise linear model, and second, a term involving the complexity of the piecewise linear model. It is straightforward to show that this complexity grows polynomially with the piece-count, K, and also the complexity of the simple local approximator (see Appendix C.0.1). Similarly, one could hope to bound the generalization error of f in terms of MNF(f, g) and the complexity of G. However, the key challenge here is that the class G is a much more complex class than the above class of piecewise linear models. For example, a straightforward piece-count-based complexity bound would be infinitely large since there are effectively infinitely many unique pieces in g.\nOur core technical contribution here is to bound the Rademacher complexity of G in this more complex local-interpretability setting. At a high level, the resulting bound (which will be stated shortly) grows with \u201cthe level of overlap\u201d between the neighborhoods {Nmirx |x \u2208 X}, quantified as: Definition 4.1. Given a dataset S \u2208 (X \u00d7 Y)m, we define the disjointedness factor \u03c1S as\n\u03c1S := \u222b x\u2032\u2208X \u221a\u221a\u221a\u221a 1 m m\u2211 i=1 (pNmirxi (x\u2032))2dx\u2032\nUnderstanding the disjointedness factor. \u03c1S can be interpreted as bounding the \u201ceffective number\u201d of pieces induced by the set of neighborhood distributions {Nmirx |x \u2208 X}. This turns out to be a quantity that lies in [1, \u221a m] (shown formally in Appendix Fact B.1). To intuit about this quantity, it is helpful to consider its behavior in extreme scenarios. First, consider the case where Nmirx is the same distribution (say N ) regardless of x i.e., neighborhoods are completely overlapping. Then, \u03c1S = \u222b x\u2032\u2208X (pN (x\n\u2032))dx\u2032 = 1. In the other extreme, consider if neighborhoods centered on the training data are all disjoint with supports X1, . . . ,X|S|. Here the integral splits into m summands as: \u03c1S = \u2211m i=1 \u222b x\u2032\u2208Xi 1\u221a m pNmirxi (x\u2032)dx\u2032 = \u221a m. Thus, intuitively \u03c1S grows from 1 to \u221a m as the level of overlap between the neighborhoods Nmirx1 , . . . , N mir x|S|\nreduces. For intuition at non-extreme values, we show in Appendix B.2 that in a simple setting, \u03c1 = \u221a m1\u2212k (where 0 \u2264 k \u2264 1) if every neighborhood is just large enough to encompass a 1/m1\u2212k fraction of mass of the distribution D.\nRademacher complexity of G. We now use \u03c1S to bound the Rademacher complexity of G. First, in order to define the complexity of G, it is useful to think of g as a randomized function. Specifically, at any target point x, the output of g is a random variable gx\u2032(x) where the randomness comes from x\u2032 \u223c Nmirx . Then, in Lemma 4.1, we take this randomization into account to define and bound the complexity of G (which we use prove our main results). To keep our statement general, we consider a generic loss function L : R \u00d7 R \u2192 R (e.g., the squared error loss is L(y, y\u2032) = (y \u2212 y\u2032)2). Whenever L satisfies a standard Lipschitz assumption, we can bound the complexity of G composed with the loss function L, in terms of \u03c1S , the complexity of Glocal and the Lipschitzness of L: Lemma 4.1. (see Appendix Lemma D.1 for full, precise statement) Let L(\u00b7, y\u2032) be a c-Lipschitz function w.r.t. y\u2032 in that for all y1, y2 \u2208 [\u2212B,B], |L(y1, y\u2032) \u2212 L(y2, y\u2032)| \u2264 c|y1 \u2212 y2|. Then, the empirical Rademacher complexity of G under the loss function L is defined and bounded as:\nR\u0302S(L \u25e6 G) := E~\u03c3 [ sup g\u2208G 1 m m\u2211 i \u03c3iEx\u2032\u223cNmirxi [L(gx\u2032(xi), yi)] ] \u2264 O ( c\u03c1SR\u0302S(Glocal) \u00b7 lnm ) .\nWe note that the proof technique employed here may be of independent theoretical interest as it provides a novel way to bound the complexity of a randomized function. Although techniques like PAC-Bayes provide ways to do this, they do not apply here since the stochasticity in the function is of a different form.\nMain result. With the above key lemma in hand, we are now ready to prove our main result, which bounds the generalization error of f in terms of the complexity of G, thereby establishing a connection between model generalization and local interpretability.\nTheorem 1. (see Appendix Theorem 3 for full, precise statement) With probability over 1 \u2212 \u03b4 over the draws of S = {(x1, y1), . . . , (xm, ym)} \u223c Dm, for all f \u2208 F and for all g \u2208 G, we have\n(ignoring ln 1/\u03b4 factors):\nE(x,y)\u223cD[(f(x)\u2212 y)2] \u2264 4\nm m\u2211 i=1 (f(xi)\u2212 yi)2 + 2Ex\u223cD[Ex\u2032\u223cNmirx [ (f(x)\u2212 gx\u2032(x))2 ] ]\ufe38 \ufe37\ufe37 \ufe38\nMNF(f,g)\n+ 4\nm m\u2211 i=1 Ex\u2032\u223cNmirx [ (f(xi)\u2212 gx\u2032(xi))2 ]\ufe38 \ufe37\ufe37 \ufe38 MNF(f,g,xi) +O(B\u03c1SR\u0302S(Glocal) lnm),\nand R\u0302S(Glocal) is the empirical Rademacher complexity of Glocal defined as R\u0302S(Glocal) := E~\u03c3 [ suph\u2208Glocal 1 m \u2211m i=1 \u03c3ih(xi) ] where ~\u03c3 is uniformly distributed over {\u22121, 1}m.\nThis result decomposes the test error of f into four quantities. The first quantity corresponds to the training error of f on the training set S. The second and the third correspond to the mirrored neighborhood fidelity of f with respect to g (computed on test and training data respectively). The fourth and final quantity corresponds to a term that bounds the complexity of G in terms of the \u201cdisjointedness factor\u201d and the complexity of the simpler function class Glocal. Takeaway. A key aspect of this bound is the trade-off that it captures with varying neighborhood widths. Consider shrinking the neighborhood widths to smaller and smaller values, in turn creating less and less overlap between the neighborhoods of the training data. Then, on the one hand, we\u2019d observe that the complexity term (the fourth term on the R.H.S) increases. Specifically, since R\u0302S(Glocal) typically scales as O(1/ \u221a m), as we go from the one extreme of full overlap to the other\nextreme of complete disjointedness, the complexity term would increase from O(1/ \u221a m) to O(1) (eventually rendering the bound trivial). On the other hand, as the widths decrease, the fidelity terms (the second and the third term) would likely decrease \u2013 this is because the simple functions in Glocal would find it easier and easier to approximate the shrinking neighborhoods.\nThis tradeoff is intuitive. A function f that is hardly amenable to being fit by local explanations would require extremely tiny neighborhoods for Glocal to locally approximate it (i.e. make the MNF terms small). For example, in an extreme case, when the neighborhoodsNmirx are set be point masses at x, it is trivially easy to find gx(\u00b7) \u2208 Glocal with no approximation error. Thus, the complexity term would be too large in this case, implying that a hard-to-interpret f results in bad generalization. On the other hand, when f is easy to interpret, then we\u2019d expect it to be well-approximated by Glocal even with wider neighborhoods. This allows one to afford smaller values for both the complexity and MNF terms. In other words, an easy-to-interpret f enjoys better generalization guarantees.\nCaveats. Our bound has two limitations worth noting. First, for high-dimensional datasets (like image datasets), practical choices of Nx can lead to almost no overlap between neighborhoods, thus rendering the bound trivial in practice. This potentially poor dimension-dependence is a caveat similarly shared by bounds for non-parametric local regression, whereby increasing d results in an exponential increase in the required sample size (Fan, 1993; Fan & Gijbels, 1996). Nevertheless, for low-dimensional datasets, we show in the experiments that for practical choices of the neighborhood distributions, there is sufficient neighborhood overlap to achieve values of \u03c1S that are o( \u221a m).\nA second caveat is that the second quantity, MNF(f, g), requires unlabeled test data to be computed, which may be limiting if one is interested in numerically computing this bound in practice. It is however possible to get a bound without this dependence, although only on the test error of g rather than f (see Appendix Theorem 4). Nevertheless, we believe that the above bound has theoretical value in how it establishes a connection between the interpretability of f and its generalization."}, {"heading": "5 GENERALIZATION OF LOCAL EXPLAINABILITY", "text": "We now turn our attention to a more subtle kind of generalization that is both unstudied yet important. Typically, the way gx\u2032 is learned at any source point x\u2032 is by fitting a finite set of points sampled near x\u2032, with the hope that this fit generalizes to unseen, neighboring target points. Naturally, we would want to ask: how well do the explanations gx\u2032 themselves generalize in this sense?\nThe subtlety in this question is that it is not always a worthwhile question to ask. In particular, assume that we learn gx\u2032 by sampling a set Sx\u2032 of nearby points from a Gaussian centered at x\u2032, and\nthat we care about the fit of gx\u2032 generalizing to the same Gaussian. Here, we have access to unlimited amounts of data from the known Gaussian distribution (and free labels using f(\u00b7)), so we can be sure that with sufficiently large Sx\u2032 , gx\u2032 will fit to arbitrarily small error on local neighborhoods. Hence, the above generalization question is neither conceptually nor practically interesting here.\nHowever, consider finite sample-based local explanation settings like MAPLE (Plumb et al., 2018) and RL-LIM (Yoon et al., 2019) where the training procedure is vastly different from this: in these procedures, the goal is to learn local explanations gx\u2032 in a way that is sensitive to the local structure of the (unknown) underlying data distributionD. So, instead of fitting the gx\u2032 to samples drawn from an arbitrarily defined Gaussian distribution, here one first draws a finite sample S from the underlying distribution D (and then labels it using f ). Then, across all x\u2032 \u2208 X , one reuses a reweighted version of the same dataset S (typically, points x in S that are near x\u2032 are weighted more) and then learns a gx\u2032 that fits this reweighted dataset. Contrast this with the former setting, where for each x\u2032, one has access to a fresh dataset (namely, Sx\u2032 ) to learn gx\u2032 . This distinction then makes it interesting to wonder when the reuse of a common dataset S could cause the explanations to generalize poorly.\nMotivated by this question, we present Theorem 2. By using Lemma 4.1, we provide a bound on the \u201ctest MNF\u201d (which corresponds to the fit of gx\u2032 on the unseen data averaged across D) in terms of the \u201ctrain MNF\u201d (which corresponds to the fit of gx\u2032 on S, averaged across x\u2032) and the complexity term from Lemma 4.1. We must however caution the reader that this theorem does not answer the exact question posed in the above paragraph; it only addresses it indirectly as we discuss shortly.\nTheorem 2. (see Appendix Theorem 2-full for full, precise statement) For a fixed function f , with high probability 1\u2212 \u03b4 over the draws of S \u223c Dm, for all g \u2208 G, we have (ignoring ln 1/\u03b4 factors):\nE x\u223cD, x\u2032\u223cNx\n[ (f(x)\u2212 gx\u2032(x))2 ] \ufe38 \ufe37\ufe37 \ufe38\ntest MNF i.e., MNF(f,g)\n\u2264 1 m m\u2211 i=1 Ex\u2032\u223cNmirx [ (f(xi)\u2212 gx\u2032(xi))2 ] \ufe38 \ufe37\ufe37 \ufe38\ntrain MNF\n+O(\u03c1SRS(Glocal) lnm).\nUnderstanding the overall bound. We first elaborate on how this bound provides an (indirect) answer to our question about how well explanations generalize. Consider a procedure like MAPLE that learns g using the finite dataset S. For each x\u2032 \u2208 X , we would expect this procedure to have learned a gx\u2032 that fits well on at least those target points x in S that are near x\u2032. In doing so, it\u2019s reasonable to expect the training procedure to have implicitly controlled the \u201ctrain MNF\u201d term The reasoning for this is that the train MNF computes the error in the fit of gx\u2032 on S for different values of x\u2032, and sums these up in a way that errors corresponding to nearby values of (x, x\u2032) are weighted more (where the weight is given by pNmirx (x\n\u2032)). Now, our bound suggests that when this train MNF is minimized, this carries over to test MNF too (provided the complexity term is not large). That is, we can say that the fit of gx\u2032 generalizes well to unseen, nearby target points x that lie outside of S.\nThe indirectness of our result. Existing finite sample-based explainers do not explicitly minimize the train MNF term (e.g., MAPLE minimizes an error based upon NF). However, as argued above, they have implicit control over train MNF. Hence, our bound essentially treats MNF as a surrogate for reasoning about the generalization of the explanations learned by an arbitrary procedure. As such, our bound does not comment on how well the exact kind of fidelity metric used during training generalizes to test data. Nevertheless, we hope that this result offers a concrete first step towards quantifying the generalization of explanations. Furthermore, we also note that one could also imagine a novel explanation-learning procedure that does explicitly minimize the train MNF term to learn g; in such a case our bound would provide a direct answer to how well its explanations generalize. Indeed, we derive such a theoretically-principled algorithm in Appendix A.\nTakeaway. While the above bound captures a similar trade-off with neighborhood width as the Theorem 1, it is worth pausing to appreciate the distinct manner in which this tradeoff arises here. In particular, when the width is too small, we know that the complexity term approachesO( \u221a m) and generalization is poor. Intuitively, this is because in this case, the procedure for learning gx\u2032 would have been trained to fit very few datapoints from S that would have fallen in the small neigbhorhood of x\u2032. On the other hand, when the neighborhoods are large, this issue would not persist which is captured by the fact that \u03c1S approaches O(1). However, with large neighborhoods, it may also be hard to find functions in Glocal that fit so many points in S. Overall, one practical takeaway from\nthis bound is that it is important to not excessively shrink the neighborhood widths if one wants explanations that generalize well for predicting how f behaves at unseen points (see Section 6).\nCaveats. We remark that this particular bound applies only when the dataset S is used to learn only g i.e., f and the neighborhoods must be learned beforehand with separate data. This sort of a framework is typical when deriving theoretical results for models like random forests, where it greatly aids analysis to assume that the trees\u2019 splits and their decisions are learned from independent datasets (i.e. two halves of an original dataset) (Arlot & Genuer, 2014). Now, if one is interested in a bound where S is also used to simultaneously learn f , the only change to the bound is an added factor corresponding to the complexity of F . Another caveat is that our bound only tells us how well the explanations gx\u2032 generalize on average over different values of x\u2032. This does not tell us anything about the quality of the generalization of gx\u2032 for an arbitrary value of x\u2032. That being said, just as average accuracy remains a central metric for performance (despite ignoring discrepancies across inputs), average MNF can still be a useful quantity for evaluating an explainer\u2019s overall performance."}, {"heading": "6 EMPIRICAL RESULTS", "text": "We present two sets of empirical results to illustrate the the usefulness of our bounds. First, we demonstrate that \u03c1S grows much smaller than O( \u221a m) which, as stated before, establishes that our bounds yield meaningful convergence rates. Second, we show that Theorem 2 accurately reflects the relationship between explanation generalization (Theorem 2) and the width of Nmirx used to both generate and evaluate explanations.\nSetup. For both experiments, we use several regression datasets from the UCI collection (Dua & Graff, 2017) and standardize each feature to have mean 0 and variance 1. We train neural networks as our \u201cblack-box\u201d models with the same setup as in (Plumb et al., 2020), using both their nonregularized and ExpO training procedures. The latter explicitly regularizes for NF during training, which we find also decreases MNF on all datasets. For generating explanations, we define Glocal to be linear models and optimize each gx using the empirical MNF minimizer (see Appendix A). Finally, we approximate \u03c1S using a provably accurate sampling-based approach (see Appendix E).\nGrowth-rate of \u03c1S . In Figure 1 (top), we track the sample dependence of \u03c1S for various neighborhoods of width \u03c3 (setting Nmirx = N (x, \u03c3I)). We specifically approximate the growth rate as polynomial, estimating the exponent by taking the overall slope of a log-log plot of \u03c1S over m. To cover a natural range for each dataset, \u03c3 is varied to be between the smallest and half the largest inter-example l2 distances. In these plots, while small \u03c3 result in a large exponent for \u03c1S and large \u03c3 cause g to intuitively saturate towards a global linear model, we observe that there do exist values of \u03c3, where both these terms are in control i.e., we observe that we can achieve a growth rate of approximately O(m0.2) without causing g to saturate and MNF metrics to rise sharply. Generalization and neighborhood size. As per the setting of Theorem 2, we generate all explanations using data not used to learn the black-box model. Specifically, we split the original test data into two halves, using only the first half for explanation training and the second for explanation\ntesting. We plot MNF as measured over these two subsets of examples in Figure 1 (bottom). From the results, it is evident that a generalization gap between train and test MNF exists. Further, recall that Theorem 2 predicts that this gap decreases as wider neighborhoods are used, a phenomena reflected in most of these plots. As a result, while training MNF monotonically increases with larger neighborhoods, test MNF always decreases at certain ranges of \u03c3."}, {"heading": "7 CONCLUSION AND FUTURE WORK", "text": "In this work, we have studied two novel connections between learning theory and local explanations. We believe these results may be of use in guiding the following directions of future work: (1) developing new local explanation algorithms inspired by our theory and the metric of MNF; (2) resolving caveats or otherwise strengthening the theory presented in this paper; and (3) exploring applications of our techniques beyond interpretability, such as the general problem of deep learning generalization or others that require reasoning about the complexity of randomized functions."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work was supported in part by DARPA FA875017C0141, the National Science Foundation grants IIS1705121 and IIS1838017, an Okawa Grant, a Google Faculty Award, an Amazon Web Services Award, a JP Morgan A.I. Research Faculty Award, and a Carnegie Bosch Institute Research Award. Vaishnavh Nagarajan was supported by a grant from the Bosch Center for AI. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of DARPA, the National Science Foundation, or any other funding agency."}, {"heading": "A MORE ON MIRRORED NEIGHBORHOOD FIDELITY", "text": "Here we elaborate on how the expression for MNF can be parsed in the same way as NF after juggling some terms around. Recall that MNF is defined as:\nMNF(f, g) := Ex\u223cD [ Ex\u2032\u223cNmirx [ (f(x)\u2212 gx\u2032(x))2 ]] .\nand with an abuse of notation, we let MNF(f, g, x) = Ex\u2032\u223cNmirx [ (f(x)\u2212 gx\u2032(x))2 ] .\nHere the outer expectation is over the target points x that the explanations try to fit, and the inner expectation is over the source points x\u2032 which give rise to the explanations gx\u2032 .\nIf we can swap these expectations around, we can afford a similar parsing as NF. To get there, first consider the joint distribution over x and x\u2032 that is induced by generating x \u223c D and then picking x\u2032 \u223c Nmirx . Under this joint distribution, we need an expression for the marginal distribution of x\u2032. This distribution, which we denote by D\u2020, is given by:\npD\u2020(x \u2032) = \u222b X pD(x)pNmirx (x \u2032)dx.\nTo get a sense of what D\u2020 looks like, imagine that Nmirx is a Gaussian centered at x. Then D \u2020 corresponds to convolving D with a Gaussian i.e., a smoother version of D.\nNext, under the same joint distribution, we need an expression for the distribution of x conditioned on x\u2032. This distribution, denoted as N\u2020x\u2032 , is given by:\npN\u2020 x\u2032 (x) =\npD(x)pNmirx (x \u2032)\u222b\nX pD(x)pNmirx (x \u2032)dx\n.\nIntuitively, N\u2020x\u2032 is distribution that is centered around x \u2032 and is also weighted by the distribution D i.e., points that are both close to x\u2032 and realistic under D have greater weight under N\u2020x\u2032 . This is because the term pNmirx (x\n\u2032) in the numerator prioritizes points that are near x\u2032 (imagine Nmirx being a Gaussian centered at x), and the term pD(x) prioritizes realistic points.\nWith these definitions in hand, we can swap the expectations around and get: MNF(f, g) = Ex\u2032\u223cD\u2020 [ Ex\u223cN\u2020\nx\u2032\n[ (f(x)\u2212 gx\u2032(x))2 ]] ,\nThis then has the same structure as NF in that the outer expectation is over the source points and the inner distribution over target points, and hence can be interpreted similarly.\nA.1 ALGORITHM FOR MINIMIZING EMPIRICAL MIRRORED NEIGHBORHOOD FIDELITY\nWe now consider how one might actually fit explanations to minimize MNF. Recall from the above discussion that from the point of view of each source point x\u2032, MNF measures how well gx\u2032 fits f on\nthe distribution with density pN\u2020 x\u2032 (x) =\npD(x)pNmirx (x\u2032)\u222b\nX pD(x)pNmirx (x\u2032)dx\n. Note that one does not have access to\nsamples from this distribution due to the dependence onD. However as we argue, one can minimize the empirical version of MNF given access to a finite sample S drawn i.i.d. from D by solving the following weighted regression problem:\ngx\u2032 = argmin gx\u2032\u2208Glocal\n1\n|S| |S|\u2211 i=1 (gx\u2032(xi)\u2212 f(xi))2pNmirxi (x \u2032)\nTo see what the empirical version of MNF is, we can replace the outer expectation (over x \u223c D) with the samples S = {xi}|S|i=1, giving us\nEmpirical MNF = 1\n|S| |S|\u2211 i=1 Ex\u2032\u223cNmirxi [ (gx\u2032(xi)\u2212 f(xi))2 ] = 1\n|S| |S|\u2211 i=1 \u222b X (gx\u2032(xi)\u2212 f(xi))2pNmirxi (x \u2032)dx\u2032\n= 1\n|S| \u222b X |S|\u2211 i=1 (gx\u2032(xi)\u2212 f(xi))2pNmirxi (x \u2032)dx\u2032\nTo minimize the overall empirical MNF, one needs to choose gx\u2032 for each x\u2032 such that it minimizes the above integrand, which is akin to performing one weighted least squares regression. Thus, one notable difference between minimizing empirical MNF and NF is that we need to use real examples to fit gx\u2032 for MNF but not for NF since the target distribution of interest there can be user-defined (i.e. it may be chosen such that it can be easily sampled from).\nA.2 TRADE-OFFS BETWEEN MNF AND NF\nWe now discuss in further detail the comparison between MNF and NF, listing both the relative advantages and disadvantages of each. It should be noted that this discussion is of a somewhat more exploratory nature; we do not aim to make definitive value judgments (i.e. one metric is always more useful than the other), but rather to provide a better qualitative understanding of how these two metrics might be expected to behave. We hope that this discussion prompts a more careful consideration of fidelity metrics in future works involving local explanations.\nA.2.1 ADVANTAGES OF MNF\nIn many practical situations (esp. for i.i.d. cases), it is reasonable to assume that practitioners will care significantly about generating explanations for predictions at realistic on-distribution points and hoping that those (local) models correctly approximate what the model will do at nearby points which are also realistic. Our core argument for the usefulness of MNF compared to NF is that it can be used to come closer to characterizing performance relative to the second part of this goal (i.e. predicting what the model will do at realistic points).\nTo reiterate Section 3, this is an especially important concern for modern ML settings, which often involve significant feature dependencies (i.e. lower dimensional data manifolds) and models that behave unstably when extrapolating beyond the given task and training data. As we illustrate below in a toy example, when one uses NF with standard neighborhood choices (i.e. Nx = N (0, \u03c3I)), one may overemphasize the ability of explanations to fit this noisy behavior on regions that are off-manifold.\nToy example. We compare the abilities of MNF and NF to serve as the basis for generating local explanations. In what follows, we refer to gNF and gMNF as the explanations that minimize NF and MNF respectively. We specifically consider a simple setup where the full input space has dimension d = 2 but the data exists on a manifold of dimension k = 1. Under task-distribution D, let x1 \u223c N (0, 1) while x2 = 0. Further consider the learned model f(x) = x1 \u2212 \u03b2x1x22, where one may assume \u03b2 0. As an important note, on the task distribution D, f(x) \u2261 x1.\nMinimizing NF: To learn gNFx , we may simply sample many x\u2032 \u223c Nx and find a linear gNFx (\u00b7) that fits these points well. Now, we can expect this process to generalize in a way that Ex\u2032\u223cNx [(gx(x\u2032)\u2212 f(x\u2032))2] is minimized. In fact, one could consider the ideal scenario where we sample infinitely many unlabeled examples, and thus find the best possible linear approximation given this neighborhood distribution. However, observe that minimizing the above quantity provides absolutely no guarantee whatsoever as far as the error committed on D i.e., Ex\u2032\u223cD[(gx(x\u2032) \u2212 f(x\u2032))2]. This is because D has zero measure. This means that by creating f that is arbitrarily volatile along the irrelevant x2 direction, we can force gx to be severely incorrect on D. Indeed, this is the case in the setting above. Let gx(x\u2032) = w1x\u20321 + w2x \u2032 2 and Nx = N(0, I). Then, it can be shown that NF(f, g, x) is minimized by w1 = 1 \u2212 \u03b2. Since \u03b2 can be arbitrarily large, this explanation can be unboundedly arbitrarily poor at recovering a function equivalent to f(x) \u2261 x1 on D.\nMinimizing MNF: Note that none of the above is a problem when we learn gMNF, because we fit gMNFx only on target points that are from the real data manifold. This will ensure that g\nMNF is in line with a potentially important desiderata for local explanations i.e., that they can faithfully capture a function that is accurate along the task-relevant data directions (of course, only upto a linear approximation). To illustrate more completely, recall that gMNF is learned as follows: assuming access to S = {x1, . . . , xm} \u223c Dm, we have\ngMNFx\u2032 = argmin gx\u2032\u2208Glocal\n1\nm m\u2211 i=1 (gx\u2032(xi)\u2212 f(xi))2pNmirxi (x \u2032)\nNow since S lies on the manifold of D, we have that x2 = 0 on all those points. Therefore, for each x, we find the solution which minimizes\ngMNFx\u2032 = argmin w1\u2208R\n1\nm m\u2211 i=1 (xi,1 \u2212 w1xi,1)2pNmirxi (x \u2032)\nIt is easy to see that with just two distinct datapoints from S, we would get w1 = 1, which leads to perfect predictions for how the function behaves on D.\nAs a remark, an even more natural version of the above setting would be one where f is non-linear even on the data manifold. But even here we can still argue that gMNF would be close to the best possible linear function within the manifold up to a 1/ \u221a m error (e.g., a generlization bound like our Theorem 2 guarantees this on average over x). On the other hand, regardless of how many unlabeled datapoints we fit gNF with, we would learn gNF that can behave arbitrarily poorly on the manifold.\nA.2.2 LIMITATIONS OF MNF\nBelow, we discuss some limitations of MNF as well as potential future directions for possibly addressing them. At a high-level, we believe while each represents a legitimate concern, they may arguably be (depending on context) \u201creasonable prices to pay\u201d for the advantages of MNF compared to NF described previously.\nMNF explanations may lose local meaning: Using MNF to evaluate/generate explanations at lowprobability source points x\u2032 may have little to do with how f actually behaves around x\u2032. Because the target point distribution is x|x\u2032 \u221d pD(x)pNx(x\u2032), very little probability mass might be placed in the vicinity around x\u2032 when pD(x) is small. This would be the case when x\u2032 is off-manifold or in low-density regions on the support of the real data distribution. The former might be dismissable if one cares about i.i.d. settings but the latter could be very important in applications where rare cases correspond to high-stakes decisions (e.g. disease diagnostics). In these scenarios, the explanation might still be too biased towards how the model is behaving at higher density regions. However, some potential future directions to remedy this are:\n\u2022 It might help to allow Nmirx to have smaller width around lower probability points from D (allowing you to concentrate Nmirx around x despite the form of D). It\u2019s remains a challenge to see how one would actually set these widths but it could be of help if a limit can be assumed on how quickly the value pD(x) can change around x.\n\u2022 There also could be some use in considering a more general definition of MNF that lets you choose an arbitrary outer distribution x \u223c Q other than simply the task distribution D. That is, if one really cares about mutually consistent explanations in some arbitrary region (which could be on or off-manifold), then this would potentially allow one to able to customize a metric for that purpose.\nLess intuitive target point neighborhoods: Very closely related to the previous limitation, in interpreting MNF-based explanations, an end-user would have to understand that gx\u2032 are not exactly approximations for the locality around x\u2032 but rather for the true target distribution that captures in some sense \u201con-manifold points near x\u2032 (modulated by the concentration of Nmirx ).\u201d This makes it harder for a user to know the exact window in which their explanation is directly valid for (compared to a user-specified target neighborhood for NF). In practice, this shortcoming could be mitigated as\nlong is it is carefully communicated to users that this limitation exists, i.e. they should focus on using MNF explanations only at and for predicting what happens at realistic points.\nUnnaturalness of source points: While MNF does emphasize realistic target points, it also focuses on explanations generated at potentially off-manifold source points. Further, one could argue that the advantages of MNF are partly because Nx is chosen naively for NF. For instance if one defined Nx = N \u2020 x in the definition for MNF, then gNF and gMNF would produce the same explanations because the inner target point expectations would be the same (comparing NF and the reversed expectation form of MNF). However, the average metric for NF seem more natural in an additional sense since it also only reflects caring about realistic source points when looking at the outer expectation over x \u223c D.\nNF = Ex\u223cDEx\u2032\u223cN\u2020x [ [(gx(x \u2032)\u2212 f(x\u2032)]2 ]\nMNF = Ex\u2032\u223cD\u2020Ex\u223cN\u2020 x\u2032\n[ [(gx\u2032(x)\u2212 f(x)]2 ] Given this, might MNF be less interesting on its own? Using standard \u201cnaive\u201d settings of Nx, one could argue that NF is also \u201cunnatural\u201d in that it takes into account how explanations at on-manifold source points perform at off-manifold target points. And though the above NF setting may be more ideal as a metric, it also becomes less clear how to evaluate it as the inner distribution cannot be sampled from easily. On the other hand, we can use the original form of writing out MNF (without the expecations flipped) to directly approximate MNF with relevant samples from D.\nDoes not reflect what model causally depends on: In the second toy-example, it was shown that if f(x) = x1 \u2212 \u03b2x1x22 but the data manifold is (x1, x2) = (x1, 0), one could get arbitrarily poor fidelity and feature relevancy (for x1) on this manifold using standard neighborhoods. But MNF runs into a new problem when the feature set actually includes a highly correlated third feature: for example, consider (x1, x2, x3) where the manifold is defined by points (x1, x2, x3) = (x1, 0, x1). Thus according to MNF, g(x) = x1, g(x) = x3, and indeed g(x) = \u2212x1 + 2x3 are all equally good explanations (because MNF only cares about whether g(x) = f(x) on manifold). However, f clearly only \u201cdepends\u201d on x1 for its decisions (in a causal sense). On the other hand, because NF samples target points both on and off manifold, it would correctly see that x3 has no effect. The larger argument here is that in any conversation involving manifolds, one inherently is speaking about some sort of feature dependencies, which may similarly suffer from the same issues of not being causal w.r.t. f and having non-identifiable explanations. On the other hand, we note that in the new toy-example, NF is not an ideal fix either because the cost is potentially an arbitrary coefficient for x1 and extremely poor fidelity on D. More generally, finding \u201cwhat the model uses for its decision\u201d is simply not what MNF explanations are trying to do. What one could describe MNF as actually looking at is \u201ccan I build a simpler local model relevant to the actual task at hand?\u201d"}, {"heading": "B MORE ON THE DISJOINTEDNESS FACTOR", "text": "B.1 BOUNDS Recall that the disjointedness factor is defined as \u03c1S := \u222b x\u2032\u2208X \u221a\u2211m j=1(pNmirxi (x\u2032))2 m dx \u2032. Here, we show that the disjointedness factor is bounded between 1 and \u221a m.\nFact B.1. The disjointedness factor \u03c1S satisfies 1 \u2264 \u03c1S \u2264 m.\nProof. For the lower bound, we note that since the arithmetic mean lower bounds the quadratic mean, we have:\n\u222b x\u2032\u2208X\n\u221a\u2211m j=1(pNmirxi (x\u2032))2\nm dx\u2032 \u2265 \u222b x\u2032\u2208X \u2211m j=1 pNmirxi (x\u2032) m dx\u2032\n\u2265 m\u2211 j=1 1 m \u222b x\u2032\u2208X pNmirxi (x\u2032)dx\u2032\n\u2265 m\u2211 j=1 1 m = 1\nFor the upper bound, we make use of the fact that the `2 norm of a vector is smaller than its `1 norm to get:\n\u222b x\u2032\u2208X\n\u221a\u2211m j=1(pNmirxi (x\u2032))2\nm dx\u2032 \u2264 \u222b x\u2032\u2208X \u2211m j=1 pNmirxi (x\u2032) \u221a m dx\u2032\n\u2264 m\u2211 j=1 1\u221a m \u222b x\u2032\u2208X pNmirxi (x\u2032)dx\u2032\n\u2264 m\u2211 j=1 1\u221a m = \u221a m\nB.2 VALUES OF \u03c1S IN-BETWEEN 1 AND \u221a m We know that the disjointedness factor \u03c1S takes the values 1 and \u221a m in the two extreme cases where the neighborhoods are completely overlapping or disjoint respectively. We also know from Fact B.1 that the only other values it takes lie in between 1 and \u221a m. When does it take these values?\nTo get a sense of how these in-between values can be realized, we present a toy example here. Specifically, we can show that under some simplistic assumptions, \u03c1S = \u221a m1\u2212k (where 0 \u2264 k \u2264 1) if every neighborhood is just large enough to encompass a 1 m1\u2212k\nfraction of mass of the distribution D.\nOur main assumption is that Nmirxi is a uniform distribution over whatever support it covers. Further, to simplify the discussion, assume that X is a discrete set containing M datapoints in total (think of M as very, very large).\nThen, if every neighborhood contains 1 m1\u2212k fraction of mass of the distribution D, it means it contains M\nm1\u2212k points in it. Therefore, since Nmirxi is a uniform distribution, we have that the probability\nmass of Nmirxi on any point x \u2032 in its support is 1 Mmk\u22121 . Plugging this in the definition of \u03c1S , we get:\n\u03c1S = \u222b x\u2032\u2208X \u221a\u221a\u221a\u221a 1 m m\u2211 i=1 (pNmirxi (x\u2032))2dx\u2032 = M\u2211 j=1 \u221a\u221a\u221a\u221a 1 m m\u2211 i=1 ( Pr x\u2032\u223cNmirxi [x\u2032 = xj ] )2\n= M\u2211 j=1 \u221a\u221a\u221a\u221a 1 m m\u2211 i=1 I[xj \u2208 supp ( Nmirxi ) ] ( 1 Mmk\u22121 )2\n= M\u2211 j=1\n1\nMmk\u22120.5 \u221a\u221a\u221a\u221a m\u2211 i=1 I[xj \u2208 supp ( Nmirxi ) ]\nTo further simplify this, we need to compute the innermost summation, which essentially corresponds to the number of mirrored neighborhoods that each point belongs to. For simplicity, let\u2019s assume that every point belongs to n neighborhoods. To estimate n, observe that for each of the m neighborhoods to contain M\nm1\u2212k points, and for each of the M points to be in n neighborhoods, we\nmust have:\nMn = m M\nm1\u2212k .\nThus, n = mk. Plugging this back in, we get \u03c1S = m 1\u2212k 2 ."}, {"heading": "C PIECE-WISE GLOBAL APPROXIMATION", "text": "C.0.1 GENERALIZATION BOUND ASSUMING PIECEWISENESS\nWe now discuss the Rademacher complexity of a simpler class of local-approximation functions: a class of piecewise-simple functions g \u2208 G with K pieces. In particular, one can show that the complexity of these functions grows with K as \u221a K.\nTo see why, first let us call the K regions that g is defined over as R1, . . . , RK . Correspondingly, the original training set S = {xi}mi can be divided into the subsets S1 = {x1,i} m1 i=1, . . . , Sk = {xK,i}mKi=1 and the pieces of g are g1, . . . , gK \u2208 Glocal are simple functions. Then, one can split the Rademacher complexity over the whole dataset in terms of these subsets, to get:\nR\u0302S(G) = E\u03c3 [ sup g\u2208G 1 m m\u2211 i=1 \u03c3ig(xi) ]\n= E\u03c3 [ sup g\u2208G K\u2211 k=1 mk m m\u2211 i=1 1 mk \u03c3igj(xi)I{xi \u2208 Sj} ]\n= E\u03c3 [ sup g\u2208G K\u2211 k=1 mk m mk\u2211 i=1 1 mk \u03c3k,igj(xk,i) ]\n\u2264 K\u2211 k=1 mk m E\u03c3 [ sup gj\u2208G\u0303 1 mk \u03c3k,igj(xk) ]\n\u2264 K\u2211 k=1 mk m R\u0302Sk(Glocal)\nNow, assuming each R\u0302Sk(Glocal) is O ( 1\u221a mk ) , and assuming each subset as the same number of\npoints mk = m/K, the sum in the last expression can be bounded as O (\u221a K m ) ."}, {"heading": "D PROOFS", "text": "Below, we present the full statement and proof of Lemma 4.1 which bounds the Rademacher complexity of G. The main difference between this statement and the version in the main paper is that we replace the Rademacher complexity of Glocal with a slightly more carefully defined version of it defined below:\nR\u0302\u2217S(Glocal) := max i\u2264m max T\u2286S,|T |=i\nR\u0302T (Glocal) \u221a i\nm (1)\nThis quantity is essentially a bound on the empirical Rademacher complexity of Glocal on all possible subsets of S, with an appropriate scaling factor.\nWe note that although this quantity is technically larger than the original quantity namely R\u0302S(Glocal), for all practical purposes, it is reasonable to think of R\u0302\u2217S(Glocal) as being identical to R\u0302S(Glocal) modulo some constant factor. For example, if we have that for all h \u2208 Glocal, h(x) = w \u00b7 x where\n\u2016w\u20162 \u2264 \u03b1, then one would typically bound R\u0302S(Glocal) by O ( \u03b1 \u221a\u2211m i=1 \u2016xi\u201622/m\u221a m ) . The bound on\nR\u0302\u2217S(Glocal) however would resolve to O ( \u03b1 \u221a maxi\u2264m \u2016xi\u201622\u221a m ) . Now, as long as we assume that \u2016xi\u2016\nare all bounded by some constant, both these bounds are asymptotically the same, and have the same 1/ \u221a m dependence on m. Additionally, we also remark that that it is possible to write our results in terms of tighter definitions of R\u0302\u2217S(Glocal), however our statements read much cleaner with the above definition.\nLemma D.1. (full, precise statement of Lemma 4.1) Let L(\u00b7, y\u2032) be a c-Lipschitz function w.r.t. y\u2032 in that for all y1, y2 \u2208 [\u2212B,B], |L(y1, y\u2032) \u2212 L(y2, y\u2032)| \u2264 c|y1 \u2212 y2|. Let S = {(x1, y1), . . . , (xm, ym)} \u2208 Xm. Then, the empirical Rademacher complexity of G under the loss function L is defined and bounded as:\nR\u0302S(L \u25e6 G) := E~\u03c3 [ sup g\u2208G 1 m m\u2211 i \u03c3iEx\u2032\u223cNmirxi [L(gx\u2032(xi), yi)] ] \u2264 c\u03c1S(lnm+ 1) \u00b7 R\u0302\u2217S(Glocal).\nwhere recall that \u03c1S := \u222b x\u2032\u2208X\n\u221a\u2211m j=1(pNmirxi (x\u2032))2\nm dx \u2032 is the disjointedness factor.\nOur high level proof idea is to first construct a distribution D\u0303 over X in a way that each the inner expectations over Nmirxi (for each i) can be rewritten as an expectation over x\n\u2032 \u223c D\u0303. This removes the dependence on i from this expectation, which then allows us to pull this expectation all the way out. This further allows us to take each x\u2032 and compute a Rademacher complexity corresponding to the loss of gx\u2032 , and then finally average that complexity over x\u2032 \u223c D\u0303.\nProof. We begin by noting that the inner expectations in the Rademacher complexity are over m unique distributions Nmirxi . our first step is to rewrite these expectations in a way that they all apply on the same distribution. Let us call this distribution D\u0303 and define what it is later. As long as D\u0303 has a support that contains the support of the above m distributions, we can write:\nR\u0302S(L \u25e6 G) = E~\u03c3 [ sup g\u2208G 1 m m\u2211 i \u03c3iEx\u2032\u223cD\u0303 [ L(gx\u2032(xi), yi) pNmirxi (x\u2032) pD\u0303(x \u2032) ]]\nthis allows us to pull the inner expectation in front of the supremum (which makes this an inequality now):\n\u2264 E~\u03c3 [ Ex\u2032\u223cD\u0303 [ sup g\u2208G 1 m m\u2211 i \u03c3iL(gx\u2032(xi), yi) pNmirxi (x\u2032) pD\u0303(x \u2032) ]]\nwhich further allows us rewrite the supremum to be over Glocal instead of G:\n\u2264 E~\u03c3 [ Ex\u2032\u223cD\u0303 [ sup h\u2208Glocal 1 m m\u2211 i \u03c3iL(h(xi), yi) pNmirxi (x\u2032) pD\u0303(x \u2032) ]] next, let us simply interchange the two outer expectations and rewrite it as:\n\u2264 Ex\u2032\u223cD\u0303 [ E~\u03c3 [ sup h\u2208Glocal 1 m m\u2211 i \u03c3iL(h(xi), yi) pNmirxi (x\u2032) pD\u0303(x \u2032) ]] .\nWhat we now have is an inner expectation which boils down to an empirical Rademacher complexity for a fixed x\u2032, and an outer expectation that averages this over x\u2032 \u223c D\u0303. For the rest of the discussion, we will fix x\u2032 and focus on bounding the inner term. For convenience, let us define wi := p Nmirxi (x\u2032)\npD\u0303(x \u2032) .\nWithout loss of generality, assume that w1 \u2264 w2 \u2264 . . . \u2264 wm. Also define w0 := 0. We then begin by expanding wi into a telescopic summation:\nE~\u03c3 [ sup h\u2208Glocal 1 m m\u2211 i=1 \u03c3iL(h(xi), yi)wi ] = E~\u03c3  sup h\u2208Glocal 1 m m\u2211 i=1 \u03c3iL(h(xi), yi) i\u2211 j=1 (wj \u2212 wj\u22121)  then, we interchange the two summations while adjusting their limits appropriately:\n= E~\u03c3  sup h\u2208Glocal 1 m m\u2211 j=1 m\u2211 i=j \u03c3iL(h(xi), yi)(wj \u2212 wj\u22121)  and we pull out the outer summation in front of the supremum and expectation, making it an upper bound:\n\u2264 m\u2211 j=1 E~\u03c3  sup h\u2208Glocal 1 m m\u2211 i=j \u03c3iL(h(xi), yi)(wj \u2212 wj\u22121)  . Intuitively, the above steps have executed the following idea. The Rademacher complexity on the LHS can be thought of as involving a dataset with weights w1, w2, . . . , wm given to the losses on each of the m datapoints. We then imagine decomposing this \u201cweighted\u201d dataset into multiple weighted datasets while ensuring that the weights summed across these datasets equal w1, w2, . . . , wm on the respective datapoints. Then, we could compute the Rademacher complexity for each of these datasets, and then sum them up to get an upper bound on the complexity corresponding to the original dataset.\nThe way we decomposed the datasets is as follows: first we extract a w1 weight out of all the m data points (which is possible since it\u2019s the smallest weight), giving rise to a dataset of m points all with equal weights w1. What remains is a dataset with weights 0, w2\u2212w1, w3\u2212w1, . . . , wm\u2212w1. From this, we\u2019ll extract a w2 \u2212 w1 weight out of all but the first data point to create a dataset of m\u22121 datapoints all equally weighted as w2\u2212w1. By proceeding similarly, we can generatem such datasets of cardinality m, m \u2212 1, . . ., 1 respectively, such that all datasets have equally weighted points, and the weights follow the sequence w1 \u2212 w0, w2 \u2212 w1, . . . and so on. As stated before, we will eventually sum up Rademacher complexity terms computed with respect to each of these datasets.\nNow, we continue simplifying the above term by pulling out (wj\u2212wj\u22121) since it is only a constant:\nE~\u03c3 [ sup h\u2208Glocal 1 m m\u2211 i=1 \u03c3iL(h(xi), yi)wi ] \u2264 m\u2211 j=1 (wj \u2212 wj\u22121)E~\u03c3  sup h\u2208Glocal 1 m m\u2211 i=j \u03c3iL(h(xi), yi) \nnext, we apply the standard contraction lemma (Lemma D.2) to make use of the fact h(xi) is composed with a c-Lipschitz function to get:\n\u2264 c m\u2211 j=1 (wj \u2212 wj\u22121)E~\u03c3  sup h\u2208Glocal 1 m m\u2211 i=j \u03c3ih(xi)  using Sj:m to denote the datapoints indexed from j to m, we can rewrite this in short as:\n\u2264 c m\u2211 j=1 (wj \u2212 wj\u22121) m+ 1\u2212 j m R\u0302Sj:m(Glocal)\nand finally, we make use of the definition ofR\u2217S(Glocal) in Equation 1 to get:\n\u2264 c m\u2211 j=1 (wj \u2212 wj\u22121) \u221a m+ 1\u2212 j\u221a m R\u0302\u2217S(Glocal).\nWhat remains now is to simplify the summation over w\u2019s. To do this, we rearrange the telescopic summation as follows:\nm\u2211 j=1 (wj \u2212 wj\u22121) \u221a m+ 1\u2212 j = m\u2211 j=1 wj( \u221a m+ 1\u2212 j \u2212 \u221a m\u2212 j)\n= m\u2211 j=1 wj \u00b7 1\u221a m+ 1\u2212 j + \u221a m\u2212 j\n\u2264 m\u2211 j=1 wj 1\u221a m+ 1\u2212 j\n\u2264 \u221a\u221a\u221a\u221a m\u2211 j=1 w2j \u00b7 \u221a\u221a\u221a\u221a m\u2211 j=1 1 j\n\u2264 \u221a\u221a\u221a\u221a m\u2211 j=1 w2j \u00b7 (lnm+ 1)\nNote that in the penultimate step we\u2019ve used the Cauchy-Schwartz inequality and in the last step, we have made use of the standard logarithmic upper bound on the m-th harmonic number. Plugging this back on the Rademacher complexity bound, we get:\nR\u0302S(L \u25e6 G) \u2264 Ex\u2032\u223cD\u0303\nc \u221a\u221a\u221a\u221a m\u2211\nj=1\nw2j \u00b7 (lnm+ 1) \u00b7 R\u0302\u2217S(Glocal)\u221a\nm  plugging in the values of wj , we get:\n\u2264 Ex\u2032\u223cD\u0303\nc \u221a\u2211m j=1(pNmirxi (x\u2032))2\n(pD\u0303(x \u2032))2\n\u00b7 (lnm+ 1) \u00b7 R\u0302 \u2217 S(Glocal)\u221a m  . \u2264 cEx\u2032\u223cD\u0303  \u221a\u221a\u221a\u221a\u2211mj=1 (pNmirxi (x\u2032))2m\n(pD\u0303(x \u2032))2\n (lnm+ 1) \u00b7 R\u0302\u2217S(Glocal).\nNow we finally set D\u0303 such that pD\u0303(x \u2032) =\n\u221a\u2211m j=1 (p Nmirxi (x\u2032))2 m\n\u03c1S where \u03c1S is a normalization constant\nsuch that \u03c1S = \u222b x\u2032\u2208X \u221a\u2211m j=1 (p Nmirxi (x\u2032))2 m dx \u2032. Then, the above term would simplify as:\nR\u0302S(L \u25e6 G) \u2264 cEx\u2032\u223cD\u0303 [\u03c1S ] (lnm+ 1) \u00b7 R\u0302 \u2217 S(Glocal)\n\u2264 c\u03c1S(lnm+ 1) \u00b7 R\u0302\u2217S(Glocal).\nNext, we state and prove the full version of Theorem 1 which provided a generalization guarantee for the test error of f in terms of its local interpretability. Theorem 3. (full, precise version of Theorem 1) With probability over 1 \u2212 \u03b4 over the draws of S = {(x1, y1), . . . , (xm, ym)} \u223c Dm, for all f \u2208 F and for all g \u2208 G, we have (ignoring ln 1/\u03b4 factors):\nE(x,y)\u223cD[(f(x)\u2212 y)2] \u2264 4\nm m\u2211 i=1 (f(xi)\u2212 yi)2 + 2Ex\u223cD[Ex\u2032\u223cNmirx [ (f(x)\u2212 gx\u2032(x))2 ] ]\ufe38 \ufe37\ufe37 \ufe38\nMNF(f,g)\n+ 4\nm m\u2211 i=1 Ex\u2032\u223cNmirx [ (f(xi)\u2212 gx\u2032(xi))2 ]\ufe38 \ufe37\ufe37 \ufe38 MNF(f,g,xi) +16B\u03c1SR\u0302\u2217S(Glocal)(lnm+ 1)\n+ 2\n\u221a ln 1/\u03b4\nm ,\nwhere \u03c1S denotes the disjointedness factor defined as \u03c1S := \u222b x\u2032\u2208X \u221a 1 m \u2211m i=1(pNmirxi (x\u2032))2dx\u2032 and R\u0302\u2217S(Glocal) is defined in Equation 1.\nProof. First, we split the test error into two terms by introducing the g function as follows:\nE(x,y)\u223cD[(f(x)\u2212 y)2] = E(x,y)\u223cD[Ex\u2032\u223cNmirx [(f(x)\u2212 y) 2]] \u2264 2 ( Ex\u223cD[Ex\u2032\u223cNmirx [(f(x)\u2212 gx\u2032(x)) 2]] + Ex\u223cD[Ex\u2032\u223cNmirx [(gx\u2032(x)\u2212 y) 2]] )\n(2)\nIn the first step, we have introduced a dummy expectation over x\u2032, and in the next step, we have used the following inequality: for any a, b, c \u2208 R, (a\u2212 b)2 \u2264 (|a\u2212 c|+ |c\u2212 b|)2 \u2264 2(|a\u2212 c|2 + |c\u2212 b|2) (the first inequality in this line is the triangle inequality and the second inequality is the root mean square inequality).\nThe first term on the RHS above is MNF(f, g). To simplify the second term, we first apply a generalization bound based on Rademacher complexity. Specifically, we have that w.h.p 1\u2212 \u03b4 over the draws of S, for all g \u2208 G,\nEx\u223cD[Ex\u2032\u223cNmirx [(gx\u2032(x)\u2212 y) 2]] \u2264 1\nm m\u2211 i=1 Ex\u2032\u223cNmirxi [(gx\u2032(xi)\u2212 yi) 2] + 2R\u0302S(G) +\n\u221a ln 1/\u03b4\nm (3)\nNow, R\u0302S(G) can be bounded using Lemma 4.1 under Lipschitzness of the squared error loss. Specifically, we have that for h, h\u2032 \u2208 Glocal, and for all y \u2208 [\u2212B,B], |(h(x) \u2212 y)2 \u2212 (h\u2032(x) \u2212 y)2| \u2264 4B|h(x) \u2212 h\u2032(x)|, since all of h(x), h\u2032(x) and y lie in [\u2212B,B]. Therefore, from Lemma 4.1 we have that:\nR\u0302S(G) \u2264 4B(lnm+ 1)\u03c1SR\u0302\u2217S(Glocal). (4)\nThe only term that remains to be bounded is the first term on the RHS. This can bounded again using the inequality that for any a, b, c \u2208 R, (a\u2212 b)2 \u2264 (|a\u2212 c|+ |c\u2212 b|)2 \u2264 2(|a\u2212 c|2 + |c\u2212 b|2):\n1\nm m\u2211 i=1 Ex\u2032\u223cNmirxi [(gx\u2032(xi)\u2212 yi) 2)] \u2264 2 m m\u2211 i=1 Ex\u2032\u223cNmirxi [(gx\u2032(xi)\u2212 f(xi)) 2] + 2 m m\u2211 i=1 (f(xi)\u2212 yi)2\n(5)\nBy combining the above three chains of inequalities, we get the final bound.\nBelow, we present an alternative version of Theorem 1 where the generalization bound does not involve the test MNF and hence does not require any unlabeled data from D; however the bound is not on the test error of f but the test error of g.\nTheorem 4. (an alternative version of Theorem 1) With probability over 1 \u2212 \u03b4 over the draws of S = {(x1, y1), . . . , (xm, ym)} \u223c Dm, for all f \u2208 F and for all g \u2208 G, we have:\nE(x,y)\u223cD[Ex\u2032\u223cNmirx [(gx\u2032(x)\u2212 y) 2]] \u2264 2\nm m\u2211 i=1 (f(xi)\u2212 yi)2 + 2 m m\u2211 i=1 Ex\u2032\u223cNmirx [ (f(xi)\u2212 gx\u2032(xi))2 ]\ufe38 \ufe37\ufe37 \ufe38 MNF(f,g,xi)\n+ 8B\u03c1SR\u0302S(Glocal)(lnm+ 1) + \u221a ln 1/\u03b4\nm .\nProof. The proof follows directly from the proof of Theorem 4 starting from Equation 3.\nWe now state and prove the full version of Theorem 2 which provided a generalization guarantee for the quality of explanations.\nTheorem 5. (full, precise statement of Theorem 2) For a fixed function f , with high probability 1\u2212\u03b4 over the draws of S \u223c Dm, for all g \u2208 G, we have:\nEx\u223cD [ Ex\u2032\u223cNmirx [ (f(x)\u2212 gx\u2032(x))2 ]] \ufe38 \ufe37\ufe37 \ufe38\ntest MNF i.e., MNF(f,g)\n\u2264 1 m m\u2211 i=1 Ex\u2032\u223cNmirx [ (f(xi)\u2212 gx\u2032(xi))2 ] \ufe38 \ufe37\ufe37 \ufe38\ntrain MNF + 8B\u03c1SRS(Glocal) lnm+ \u221a ln 1/\u03b4\nm .\nwhere R\u0302\u2217S(Glocal) is defined in Equation 1.\nProof. For this result, we need to think of f as a fixed labeling function since it is independent of the dataset S that is used to train g. Then, one can apply a standard Rademacher complexity bound and invoke Lemma 4.1 to get the final result (as invoked in Equation 4).\nBelow, we state the standard contraction lemma for Rademacher complexity. The lemma states that composing a function class with a c-Lipschitz function can scale up its Rademacher complexity by a multiplicative factor of atmost c.\nLemma D.2. (Contraction lemma) For each i = 1, 2, . . . ,m, let \u03c6i : R \u2192 R be a c-Lipschitz function in that for all t, t\u2032 \u2208 B \u2286 R, |\u03c6i(t)\u2212 \u03c6i(t\u2032)| \u2264 |t\u2212 t\u2032|. Then, for any class H of functions h : R\u2192 B, we have:\nE~\u03c3 [ m\u2211 i=1 \u03c3i\u03c6i(h(xi)) ] \u2264 cE~\u03c3 [ m\u2211 i=1 \u03c3i(h(xi) ] ."}, {"heading": "E EXPERIMENT DETAILS", "text": "E.1 PROCEDURE FOR CALCULATING \u03c1S\nAs a reminder, we define \u03c1S to be an integral over X , which is not trivial to evaluate in practice, especially in higher dimensions.\n\u03c1S = \u222b x\u2032\u2208X \u221a\u221a\u221a\u221a 1 m m\u2211 i=1 (pNmirxi (x\u2032))2dx\u2032\nCommon numerical integration techniques usually incur significant computational costs due to the dimension of x. Though a variety of methods exist, one can intuit this blow-up by considering the naive approach of simply constructing a Riemann sum across a rectangular meshgrid of points in X . If one wants to create a grid of c points per dimension, then cd points (and thus evaluations of the integrand) must be processed.\nInstead, we can apply Monte-Carlo Integration to evaluate \u03c1S . As we will see, a key feature of this approach is that error will not scale with data dimension and can be bounded probabilistically via a Hoeffding bound. Currently, the integral does not look like an expectation so we must introduce a dummy distribution q(x\u2032) as follows\n\u03c1S = \u222b x\u2032\u2208X\n\u221a 1 m \u2211m i=1(pNmirxi (x\u2032))2\nq(x\u2032) q(x\u2032)dx\u2032 = Ex\u2032\u223cq\n \u221a 1 m \u2211m i=1(pNmirxi (x\u2032))2\nq(x\u2032)  Now, we can estimate \u03c1S with n independent samples from q.\n\u03c1\u0302S,n = 1\nn n\u2211 j=1\n\u221a 1 m \u2211m i=1(pNmirxi (x\u2032j)) 2\nq(x\u2032j)\nThis is an unbiased estimate of \u03c1S , but that in itself is not sufficient. This is only a feasible approach if we can choose q such that (1) we can actually sample from it, (2) we can calculate q(x\u2032) for\narbitrary x\u2032 and (3) we can control the variance of\n\u221a 1 m \u2211m i=1(pNmirxi (x\u2032))2\nq(x\u2032) .\nIt can be shown by choosing q to be a uniform mixture of the m training set neighborhoods, we can satisfy all 3 properties. (1) and (2) are dependent on those same properties being satisfied by Nmirx . If Nmirx can be sampled from, the mixture over m such distributions can obviously be sampled from. The same goes for calculating the density, which in this case is:\nq(x\u2032) = m\u2211 i=1 1 m \u00b7 pNmirxi (x \u2032) = 1 m m\u2211 i=1 pNmirxi (x\u2032)\nWe observe that (3) can also be shown because we can upper and lower bound the quantity in question. To show this, we first re-write it as\n\u221a 1 m \u2211m i=1(pNmirxi (x\u2032))2\nq(x\u2032) =\n\u221a 1 m \u2211m i=1(pNmirxi (x\u2032))2\n1 m \u2211m i=1 \u00b7pNmirxi (x \u2032)\n= \u221a m\n\u221a\u2211m i=1(pNmirxi\n(x\u2032))2\u2211m i=1 \u00b7pNmirxi (x \u2032)\n= \u221a m ||pS(x\u2032)||2 ||pS(x\u2032)||1\nwhere pS(x\u2032) is a m-dimensional vector of densities each evaluated at x\u2032 (i.e. one for each of the m training points). Since ||x||2 \u2264 ||x||1 \u2264 \u221a m||x||2, the upper and lower bounds for this quantity are\u221a\nm and 1 respectively. Thus we can bound the variance of this quantity by 14 ( \u221a m\u2212 1)2 \u2264 m4 and Var(\u03c1\u0302S,n) \u2264 m4n . This does not scale with dimension but only the number of training points! To be even more concrete, for a given m and n, we can now apply a Hoeffding bound to control the error.\nP(|\u03c1\u0302S,n \u2212 \u03c1S | > t) \u2264 2e \u22122nt2 m\nIn our experiments we choose n to be 10m, meaning that the probability that \u03c1S is off by more than 0.5 is capped at about 1% (recall that \u03c1S scales from [1, \u221a m].\nE.2 FULL SET OF RESULTS"}], "title": "A LEARNING THEORETIC PERSPECTIVE ON LOCAL EXPLAINABILITY", "year": 2020}