{"abstractText": "AutoML systems are currently rising in popularity, as they can build powerful models without human oversight. They often combine techniques from many different sub-fields of machine learning in order to find a model or set of models that optimize a user-supplied criterion, such as predictive performance. The ultimate goal of such systems is to reduce the amount of time spent on menial tasks, or tasks that can be solved better by algorithms, while leaving decisions that require human intelligence to the end-user. In recent years, the importance of other criteria, such as fairness and interpretability, and many others has become more and more apparent. Current AutoML frameworks either do not allow to optimize such secondary criteria, or only do so by limiting the system\u2019s choice of models and preprocessing steps. We propose to optimize additional criteria defined by the user directly to guide the search towards an optimal machine learning pipeline. In order to demonstrate the need and usefulness of our approach, we provide a simple multi-criteria AutoML system and showcase an exemplary application.", "authors": [{"affiliations": [], "name": "Florian Pfisterer"}, {"affiliations": [], "name": "Stefan Coors"}, {"affiliations": [], "name": "Bernd Bischl"}], "id": "SP:afcba2ae73442e81801d1d7fc6764e99dc4d3ab0", "references": [{"authors": ["D.W. Apley"], "title": "Visualizing the effects of predictor variables in black box supervised learning models", "venue": "arXiv preprint arXiv:1612.08468.", "year": 2016}, {"authors": ["S. Barocas", "M. Hardt", "A. Narayanan"], "title": "Fairness and Machine Learning", "venue": "fairmlbook.org. http://www.fairmlbook.org.", "year": 2018}, {"authors": ["B. Bischl", "M. Lang", "L. Kotthoff", "J. Schiffner", "J. Richter", "E. Studerus", "G. Casalicchio", "Z.M. Jones"], "title": "mlr: Machine learning in R", "venue": "Journal of Machine Learning Research, 17(170):1\u20135.", "year": 2016}, {"authors": ["B. Bischl", "J. Richter", "J. Bossek", "D. Horn", "J. Thomas", "M. Lang"], "title": "mlrMBO: A Modular Framework for Model-Based Optimization of Expensive Black-Box Functions", "year": 2018}, {"authors": ["A. Blot", "H.H. Hoos", "L. Vermeulen-Jourdan", "Kessaci-Marmion", "M.-\u00c9.", "H. Trautmann"], "title": "Mo-paramils: A multi-objective automatic algorithm configuration framework", "venue": "LION.", "year": 2016}, {"authors": ["O. Bousquet", "A. Elisseeff"], "title": "Stability and generalization", "venue": "J. Mach. Learn. Res., 2:499\u2013526.", "year": 2002}, {"authors": ["T. Chen", "C. Guestrin"], "title": "XGBoost: A scalable tree boosting system", "venue": "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201916, pages 785\u2013794, New York, NY, USA. ACM.", "year": 2016}, {"authors": ["A.V. Dorogush", "V. Ershov", "A. Gulin"], "title": "Catboost: gradient boosting with categorical features support", "year": 2017}, {"authors": ["R.M. Everson", "J.E. Fieldsend"], "title": "Multi-class roc analysis from a multiobjective optimisation perspective", "venue": "Pattern Recognition Letters, 27(8):918\u2013927.", "year": 2006}, {"authors": ["Z. Fan", "Y. Fang", "W. Li", "J. Lu", "X. Cai", "C. Wei"], "title": "A comparative study of constrained multi-objective evolutionary algorithms on constrained multi-objective optimization problems", "venue": "2017 IEEE Congress on Evolutionary Computation (CEC), pages 209\u2013216. IEEE.", "year": 2017}, {"authors": ["M. Feurer", "A. Klein", "K. Eggensperger", "J. Springenberg", "M. Blum", "F. Hutter"], "title": "Efficient and robust automated machine learning", "venue": "Cortes, C., Lawrence, N. D., Lee, D. D., Sugiyama, M., and Garnett, R., editors, Advances in Neural Information Processing Systems 28, pages 2962\u20132970. Curran Associates, Inc.", "year": 2015}, {"authors": ["J.H. Friedman"], "title": "Greedy function approximation: A gradient boosting machine", "venue": "Ann. Statist., 29(5):1189\u20131232.", "year": 2001}, {"authors": ["J. Gonz\u00e1lez", "Z. Dai", "A. Damianou", "N.D. Lawrence"], "title": "Preferential bayesian optimization", "venue": "Proceedings of the 34th International Conference on Machine Learning-Volume 70, pages 1282\u20131291. JMLR. org.", "year": 2017}, {"authors": ["I. Guyon", "L. Sun-Hosoya", "M. Boull\u00e9", "H.J. Escalante", "S. Escalera", "Z. Liu", "D. Jajetic", "B. Ray", "M. Saeed", "M. Sebag", "A. Statnikov", "Tu", "W.-W.", "E. Viegas"], "title": "Analysis of the AutoML Challenge Series 2015\u20132018, pages 177\u2013219", "venue": "Springer International Publishing, Cham.", "year": 2019}, {"authors": ["J. Hakanen", "J.D. Knowles"], "title": "On using decision maker preferences with parego", "venue": "EMO.", "year": 2017}, {"authors": ["M. Hall", "E. Frank", "G. Holmes", "B. Pfahringer", "P. Reutemann", "I.H. Witten"], "title": "The weka data mining software: An update", "venue": "SIGKDD Explor. Newsl., 11(1):10\u201318.", "year": 2009}, {"authors": ["J. Handl", "D.B. Kell", "J. Knowles"], "title": "Multiobjective optimization in bioinformatics and computational biology", "venue": "IEEE/ACM Transactions on Computational Biology and Bioinformatics (TCBB), 4(2):279\u2013292.", "year": 2007}, {"authors": ["M. Hardt", "E. Price", "N Srebro"], "title": "Equality of opportunity in supervised learning", "venue": "In Advances in neural information processing systems,", "year": 2016}, {"authors": ["J.M. Hern\u00e1ndez-Lobato", "M.A. Gelbart", "R.P. Adams", "M.W. Hoffman", "Z. Ghahramani"], "title": "A general framework for constrained bayesian optimization using information-based search", "venue": "The Journal of Machine Learning Research, 17(1):5549\u2013 5601.", "year": 2016}, {"authors": ["D. Horn", "B. Bischl"], "title": "Multi-objective parameter configuration of machine learning algorithms using model-based optimization", "venue": "Computational Intelligence (SSCI), 2016 IEEE Symposium Series on, pages 1\u20138. IEEE.", "year": 2016}, {"authors": ["A.G. Howard", "M. Zhu", "B. Chen", "D. Kalenichenko", "W. Wang", "T. Weyand", "M. Andreetto", "H. Adam"], "title": "Mobilenets: Efficient convolutional neural networks for mobile vision applications", "venue": "CoRR, abs/1704.04861.", "year": 2017}, {"authors": ["J. Huang", "V. Rathod", "C. Sun", "M. Zhu", "A. Korattikara", "A. Fathi", "I. Fischer", "Z. Wojna", "Y. Song", "S. Guadarrama", "K. Murphy"], "title": "Speed/accuracy trade-offs for modern convolutional object detectors", "venue": "CoRR, abs/1611.10012.", "year": 2016}, {"authors": ["F. Hutter", "H.H. Hoos", "K. Leyton-Brown"], "title": "Sequential Model-Based Optimization for General Algorithm Configuration, pages 507\u2013523", "venue": "Springer Berlin Heidelberg, Berlin, Heidelberg.", "year": 2011}, {"authors": ["Y. Jin", "B. Sendhoff"], "title": "Pareto-based multiobjective machine learning: An overview and case studies", "venue": "IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews), 38(3):397\u2013415.", "year": 2008}, {"authors": ["J. Johnson", "M. Douze", "H. J\u00e9gou"], "title": "Billion-scale similarity search with gpus", "venue": "CoRR, abs/1702.08734.", "year": 2017}, {"authors": ["G. Ke", "Q. Meng", "T. Finley", "T. Wang", "W. Chen", "W. Ma", "Q. Ye", "Liu", "T.-Y."], "title": "Lightgbm: A highly efficient gradient boosting decision tree", "venue": "Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R., editors, Advances in Neural Information Processing Systems 30, pages 3149\u20133157. Curran Associates, Inc.", "year": 2017}, {"authors": ["J. Knowles"], "title": "Parego: A hybrid algorithm with on-line landscape approximation for expensive multiobjective optimization problems", "venue": "Technical Report TRCOMPSYSBIO-2004-01, University of Manchester.", "year": 2004}, {"authors": ["T. Lange", "M.L. Braun", "V. Roth", "J.M. Buhmann"], "title": "Stability-based model selection", "venue": "Advances in neural information processing systems, pages 633\u2013642.", "year": 2003}, {"authors": ["K. Miettinen", "M.M. M\u00e4kel\u00e4"], "title": "On scalarizing functions in multiobjective optimization", "venue": "OR spectrum, 24(2):193\u2013213.", "year": 2002}, {"authors": ["C. Molnar"], "title": "Interpretable Machine Learning", "venue": "https://christophm. github.io/interpretable-ml-book/.", "year": 2019}, {"authors": ["C. Molnar", "G. Casalicchio", "B. Bischl"], "title": "Quantifying interpretability of arbitrary machine learning models through functional decomposition", "venue": "arXiv preprint arXiv:1904.03867.", "year": 2019}, {"authors": ["R.S. Olson", "R.J. Urbanowicz", "P.C. Andrews", "N.A. Lavender", "L.C. Kidd", "J.H. Moore"], "title": "Automating biomedical data science through tree-based pipeline optimization", "venue": "Squillero, G. and Burelli, P., editors, Applications of Evolutionary Computation, pages 123\u2013137, Cham. Springer International Publishing.", "year": 2016}, {"authors": ["N. Papernot", "P.D. McDaniel", "X. Wu", "S. Jha", "A. Swami"], "title": "Distillation as a defense to adversarial perturbations against deep neural networks", "venue": "CoRR, abs/1511.04508.", "year": 2015}, {"authors": ["B. Paria", "K. Kandasamy", "B. P\u00f3czos"], "title": "A flexible multi-objective bayesian optimization approach using random scalarizations", "venue": "CoRR, abs/1805.12168.", "year": 2018}, {"authors": ["G. Pleiss", "M. Raghavan", "F. Wu", "J. Kleinberg", "K.Q. Weinberger"], "title": "On fairness and calibration", "venue": "Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPS\u201917, pages 5684\u20135693, USA. Curran Associates Inc.", "year": 2017}, {"authors": ["J. Snoek", "H. Larochelle", "R.P. Adams"], "title": "Practical bayesian optimization of machine learning algorithms", "venue": "Proceedings of the 25th International Conference on Neural Information Processing Systems - Volume 2, NIPS\u201912, pages 2951\u20132959, USA. Curran Associates Inc.", "year": 2012}, {"authors": ["R.E. Steuer", "Choo", "E.-U."], "title": "An interactive weighted tchebycheff procedure for multiple objective programming", "venue": "Math. Program., 26(3):326\u2013344.", "year": 1983}, {"authors": ["C. Szegedy", "W. Zaremba", "I. Sutskever", "J. Bruna", "D. Erhan", "I. Goodfellow", "R. Fergus"], "title": "Intriguing properties of neural networks", "venue": "arXiv preprint arXiv:1312.6199.", "year": 2013}, {"authors": ["J. Thomas", "S. Coors", "B. Bischl"], "title": "Automatic gradient boosting", "venue": "International Workshop on Automatic Machine Learning at ICML.", "year": 2018}, {"authors": ["C. Thornton", "F. Hutter", "H.H. Hoos", "K. Leyton-Brown"], "title": "Auto-WEKA: Combined selection and hyperparameter optimization of classification algorithms", "venue": "Proc. of KDD-2013, pages 847\u2013855.", "year": 2013}, {"authors": ["Q. Wang", "Y. Ming", "Z. Jin", "Q. Shen", "D. Liu", "M.J. Smith", "K. Veeramachaneni", "H. Qu"], "title": "Atmseer: Increasing transparency and controllability in automated machine learning", "venue": "Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, CHI \u201919, pages 681:1\u2013681:12, New York, NY, USA. ACM.", "year": 2019}, {"authors": ["J. Wilson", "A.K. Meher", "B.V. Bindu", "M. Sharma", "V. Pareek", "S. Chaudhury", "B. Lall"], "title": "Autogbt:automatically optimized gradient boosting trees for classifying large volume high cardinality data streams under concept-drift", "venue": "https://github. com/flytxtds/AutoGBT.", "year": 2018}, {"authors": ["K. Zhang", "B. Sch\u00f6lkopf", "K. Muandet", "Z. Wang"], "title": "Domain adaptation under target and conditional shift", "venue": "Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28, ICML\u201913, pages III\u2013819\u2013III\u2013827. JMLR.org.", "year": 2013}, {"authors": ["T. Zhang", "M. Georgiopoulos", "G.C. Anagnostopoulos"], "title": "Sprint multi-objective model racing", "venue": "Proceedings of the 2015 Annual Conference on Genetic and Evolutionary Computation, GECCO \u201915, pages 1383\u20131390, New York, NY, USA. ACM.", "year": 2015}, {"authors": ["I. \u017dliobait\u0117"], "title": "Measuring discrimination in algorithmic decision making", "venue": "Data Mining and Knowledge Discovery, 31(4):1060\u20131089.", "year": 2017}], "sections": [{"heading": "1 Introduction", "text": "While many stages of a data analysis project still need to be done manually by human data scientists, other parts, such as model selection and algorithm configuration can be efficiently handled by algorithms. This does not only reduce the time required by humans, but also allows to leverage parallelization. A typical challenge is the selection of appropriate algorithms and corresponding hyperparameters for a given problem. Multiple methods for solving this Combined Algorithm Selection and Hyperparameter optimization (CASH) problem (Thornton et al., 2013) already exist and are typically referred to as Automatic Machine Learning (AutoML).\nThere is a growing number of approaches for AutoML available to non-specialists. As one of the first frameworks, Auto-WEKA (Thornton et al., 2013) introduced a system for automatically choosing from a broad variety of learning algorithms implemented in the open source software WEKA (Hall et al., 2009). Auto-WEKA simultaneously tunes hyperparameters over several learning algorithms using the Bayesian optimization framework SMAC (Hutter et al., 2011). Similar to Auto-WEKA is auto-sklearn (Feurer et al., 2015), which is based on the scikit-learn toolkit for python and includes all of its learners as well as available preprocessing operations. It stacks multiple models to achieve high predictive performance. Another python-based AutoML tool is called Tree-based Pipeline Optimization Tool (TPOT) by Olson et al. (2016) and uses genetic programming instead of Bayesian optimization to tune over a similar space as autosklearn. ar X\niv :1\n90 8.\n10 79\n6v 1\n[ st\nat .M\nL ]\n2 8\nA ug\n2 01\nIn this work we consider an approach that configures a machine learning pipeline, i.e. an approach that optimizes pre- and post-processing steps along with algorithm hyperparameters of a single gradient boosting model (Friedman, 2001). By focusing on a single learning algorithm, hyperparameters can be optimized much more thoroughly and the resulting model can be analyzed and deployed more easily. Gradient boosting models can vary from very simple to highly complex models through the choice of appropriate hyper-parameters. Single learner systems reduce the complexity of the configuration space, however, a drawback is, that this search-space possibly does not include optimal configurations, benefits from stacking and ensembling are not explored, and that thus optimal predictive performance may not be achieved. Only few single-learner AutoML methods exist. The autoxgboost software proposed by Thomas et al. (2018) is a single-learner strategy using the xgboost (Chen and Guestrin (2016)) algorithm, with model based optimization for hyperparameter tuning. The success of such single-learner strategies was shown in the NIPS 2018 AutoML Challenge (Guyon et al., 2019): The winning entry, AutoGBT (Wilson et al. (2018)) only used LightGBM (Ke et al., 2017) models with a simple preprocessing scheme.\nSeveral new challenges occur when adapting AutoML systems for multi-criteria optimization. Depending on the objective to be optimized, different pre- and post-processing methods might be required in order to obtain optimal performances. Additionally, different user-preferences regarding which trade-offs between objectives a user is willing to make have to be incorporated. We argue that giving the user the opportunity to intervene in the process can be beneficial here. Lastly, measures that quantify objectives such as fairness, interpretability and robustness are often not readily available. In this work, we want to i) emphasize the necessity for considering multiple objectives in AutoML, ii) provide several measures that can be useful in such a context and iii) propose a simple system that allows the user to automatically optimize over a set of measures. In contrast to previous work, we focus on optimizing multiple user-defined criteria simultaneously. Being able to transparently optimize multiple criteria is a crucial missing step in many existing frameworks. In order to underline the need for several different criteria, we demonstrate the functionality of our proposed framework in a practical use case."}, {"heading": "2 The case for additional criteria in AutoML", "text": "Multi-criteria optimization is well-established in machine learning for example in ROC analysis (Everson and Fieldsend (2006)), computational biology (Handl et al. (2007)) and other fields. Jin and Sendhoff (2008) study various use cases, among others, models are optimized jointly with respect to interpretability and predictive performance. Multi-criteria optimization is also actively researched in the field of Algorithm Configuration. Blot et al. (2016) introduce a multi-criteria iterative local search procedure for configuring SAT solvers, while Zhang et al. (2015) introduce a racing-based approach. Different Multi-criteria Bayesian Optimization approaches have also been proposed (c.f. Paria et al. (2018)), but it has not been thoroughly investigated as a part of AutoML frameworks until now. Many different algorithms, such as approaches based on iterated local search or racing as well as genetic algorithm based approaches can be used to\noptimize machine learning pipelines, given that they can deal with hierarchical mixed continuous and discrete spaces. We choose Bayesian Optimization because it has been shown to work well with relatively small budgets (Bischl et al., 2018) and complex hierarchical spaces can be optimized by using random forests as surrogate models.\nOnly being able to optimize a single performance measure entails multiple pitfalls that can possibly be avoided when multiple performance measures are optimized jointly. This has been emphasized recently in the FatML (Fairness, Accountability, and Transparency in Machine Learning) community which made the case for models that emphasize transparency and fairness (Barocas et al., 2018). The need for models that do not discriminate against parts of the population in order to achieve optimal predictive performance has garnered widespread support, yet no real options that allow users to jointly search for fair, transparent and well-performing models are available. The case for other criteria, that might be relevant to a user has been made in many other areas of machine learning. Examples include models that emphasize sparseness, a lower inference time, i.e., when searching for items in databases (Johnson et al., 2017), a low memory footprint, for example when deploying models on mobile phones (Howard et al., 2017) or a combination of those when doing inference on edge devices (Huang et al. (2016)). Similarly, the case for requiring robust models, i.e., models that are robust to perturbations in the data (adversarial perturbations, c.f Papernot et al. (2015)) can be made. Models that satisfy a user-desired trade-off might not be found using single-criteria optimization (Jin and Sendhoff (2008)). It is important to distinguish between jointly optimizing multiple optimization criteria, and constrained optimization (c.f. Hern\u00e1ndez-Lobato et al. (2016) for an overview). Achieving a certain model size might be paramount to be able to deploy a machine learning pipeline to a end user device, but having a model smaller than this size threshold is only of minor interest. The concept of constraints in multi-criteria optimization is a well researched topic (c.f. Fan et al. (2017))."}, {"heading": "2.1 Human in the loop approaches in AutoML", "text": "The original aim of AutoML systems is to transfer the CASH problem from the hands of a human to the machine. This does not only allow experienced machine learning researchers to focus on other tasks, such as validating data and feature engineering leveraging domain knowledge, but also enables a broader public to apply Machine Learning, as steps that require a machine learning expert, like selecting algorithms and tuning their hyperparameters, are fully automated. The AutoML system is thus treated as a black-box, that can only be influenced by some hyperparameters at the beginning of the training, essentially removing the human from the optimization loop.\nIn situations, where multiple criteria have to be optimized simultaneously, a trade-off between the different measures is often required. Specifying this trade-off a priori can be difficult when possible trade-offs are not known. Hakanen and Knowles (2017) propose an interactive Bayesian Optimization extension to parEgo (Knowles, 2004), that allows a user to iteratively select preferred ranges for the different optimization criteria. This does not only allow to search for solutions in the region a user is interested in, but also allows the user to adapt preferences throughout the procedure. This emphasizes the need\nfor users to guide the AutoML process, essentially putting the user back into the loop, albeit in a different fashion. Instead of manually configuring the pipeline, the user is now able to occasionally supervise the search process and make adjustments where needed. A different approach, that allows the user to guide the search process by adapting the search space and tries to visualize and explain decisions made within AutoML systems has been proposed in (Wang et al., 2019)."}, {"heading": "2.2 Measures for Multi-Criteria AutoML", "text": "Fairness\nInterpretability\nRobustness\nPredictive Performance\nMemory Footprint\nSparsity\nAutoML System\nBudget\nUser Preferences\nFig. 1: User Input to AutoML Systems.\nMulti-criteria optimization methods usually explore the whole pareto front defined by trade-offs between the different objectives. In our work, we mainly allow the user to guide the search process in two ways: We enable the user to focus on exploring different parts of the pareto front by selecting upper and lower trade-offs relevant to the user. Second, we allow the user to adapt the search space, by adjusting hyperparameter ranges and activating or deactivating processing steps. This allows the user to shape the result towards personal preferences.\nIn order to start the investigation into Multi-Criteria AutoML, we aim to provide a list of measures that cover a wide variety of use-cases. We want to stress, that the proposed measures are not comprehensive or final, but instead can be thought of as exchangeable building blocks that can serve as a useful proxy in the AutoML process. We hope to emphasize the necessity for measures, that better reflect the underlying model characteristics we aim to optimize.\nPredictive Performance can be quantified using many different measures, such as Accuracy, F-Score or Area under the Curve for classification and Mean Squared Error or Mean Absolute Error for regression. As those measures are already widely known, we refrain from going into more detail in this work.\nInterpretability In order to make a machine learning model\u2019s decisions more transparent, different methods that aim at providing human-understandable explanations have been proposed (c.f. Molnar (2019)). Many of those work in a model-agnostic and post-hoc fashion, which is desirable for AutoML processes, as this allows the user to explain arbitrary models resulting from AutoML processes. Interpretability methods can produce misleading results if a model is too complex. Quantifying interpretability, i.e. determining\nhow complex predictive decisions of a given model are could be a first step, making it a useful criterion to optimize for AutoML systems. A first approach has been proposed in Molnar et al. (2019), describing 3 measures that can be used as a proxy for interpretability. We implement those measures and briefly present each:\n\u2013 Complexity of main effects Molnar et al. (2019) propose to determine the average shape complexity of ALE (Apley, 2016) main effects by the number of parameters needed to approximate the curve with linear segments.\n\u2013 Interaction Strength Quantifying the impact of interaction effects is relevant when explanations are required, as most interpretability techniques use linear relationships to obtain explanations. Interaction Strength is measured as the fraction of variance that can not be explained by main effects. \u2013 Sparsity can be a desired property in case a simple explanation of a model is required, or obtaining features is costly and can potentially be avoided. In this work we measure sparsity as the fraction of features used.\nA different approach towards achieving interpretability, would to instead focus on limiting an AutoML system to models, that are inherently interpretable. As those models rarely achieve optimal performances and trade-offs between interpretability and predictive perfromance cannot be assessed, we resort instead to look for models that are well-suited for post-hoc interpretability.\nFairness has been established as a relevant criterion in Machine Learning when humans are subject to algorithmic decisions. The aim of the field is to encourage models that do not discriminate between certain sub-populations in the data. Hardt et al. (2016) define the concept of equalized odds and equal opportunity. Given a protected attribute A (e.g. gender), an outcome X , a binary predictor Yb, several criteria can be derived.\n\u2013 Independence or equalized odds can be measured as follows:\nPr{Yb = 1|A = 0, Y = 1} = Pr{Yb = 1|A = 1, Y = 1}\ni.e. if the true positive rate is equal in sub-populations indicated by A. \u2013 Sufficiency or equality of opportunity can be measured as follows:\nPr{Yb = 1|A = 0, Y = y} = Pr{Yb = 1|A = 1, Y = y}, y \u2208 {0, 1}\ni.e. if the false positive and the false negative rates are equal in sub-populations. \u2013 Calibration is another desirable criterion for classifier, especially in the context of\nfairness, where we might want to have calibrated probabilities in all groups. Pleiss et al. (2017) show, that models that are well-calibrated but also have equalized odds are only possible in case the predictor is perfect, i.e does not make any errors.\n\u017dliobaite\u0307 (2017) provide a review of various discrimination measures that can be used in this context. A score for fairness can now be derived for example from the absolute differences of the given measure in each subgroup. In the use-case below, we use the differences in F1-Scores as a measure we want to minimize. The F1 score is the harmonic mean between the True Positive Rate and the Positive predictive value, and thus trades off true positives, false negatives and false positives.\nRobustness as a concept, describes the behaviour of machine learning algorithms in situations where the data originally used to train a model is changed. A formal definition of robustness is currently lacking, which might arise from the many different concepts such a definition would need to cover. Bousquet and Elisseeff (2002) define the notion of stability, which essentially measures how much a pre-defined loss-function deteriorates if an observation is held-out during training. This is not exactly what we are interested in as it requires extensive retraining. Instead we require a post-hoc method that operates on a fitted model and training or testing data. A different approach, also coined stability is provided in Lange et al. (2003). Their notion of stability measures the disagreement between a trained model on training data and test data. In this work, we detail three measures of robustness, which we deem helpful in certain situations.\n\u2013 Perturbations A very simple measure of robustness could be a classifiers\u2019 robustness to minimal perturbations in the input data. We create a copy X? of our data X by adding a small magnitude noise N(0, ) scaled by , typically 0.001\u2212 0.01 times the range of the numerical feature. The robustness to perturbations can then be measured via the absolute difference of some loss L, for example accuracy.\n|L(X,Y )\u2212 L(X?, Y )|\n\u2013 Adversarial Examples A widely researched area of robustness is the field of Adversarial Examples Szegedy et al. (2013); Papernot et al. (2015). Various different adversarial attacks and defenses against such attacks have been proposed. A variety of robustness measures can be derived from the different types of attacks proposed.\n\u2013 Distribution shift is a concept that is gathering widespread interest not only as a research field Zhang et al. (2013), but also as a problem in AutoML, which became evident from the AutoML Challenge organized at the NIPS 2018 conference Guyon et al. (2019). To the author\u2019s knowledge, no measure that serves as a proxy for a model\u2019s robustness to distribution shift is available.\nInference Time and Memory requirements have been widely used as a measurement of the performance of machine learning algorithms. The time required for inference can be incorporated as a criterion.\nSparsity is also an important desideratum in other contexts, where interpretability is not necessarily required. In cases, where observing each feature incurs different costs, a user might want to find a model that achieves optimal performances using as few features as possible."}, {"heading": "3 Method", "text": "This section introduces the structure of a first simple approach for multi-criteria AutoML. We heavily base our software on Thomas et al. (2018), and include several design choices, such as the selection of preprocessing steps. The general workflow is detailed\nin Figure 2. The implementation can be obtained from github 1. Automatic gradient boosting simplifies AutoML to a fixed choice of machine learning algorithm by only using gradient boosting with trees (GBT). Gradient Boosted Decision Trees are widely successfull for learning on tabular data and have desirable properties for AutoML systems, as they can deal with missing observations, are insensitive to outliers and can handle large amounts of features and data points. Additionally they are numerically stable and memory efficient. Additionally modern GBT frameworks like xgboost Chen and Guestrin (2016) or lightgbm (Ke et al., 2017) are highly configurable with a large number of hyperparameters for regularization and optimization. As a result, they can approximate or cover many other scenarios, such as decision trees, random forests or linear models. Categorical feature transformation is performed as a preprocessing step. We employ Sequential model-based optimization (SMBO), also known as Bayesian Optimization as a hyperparameter optimization strategy (Snoek et al., 2012). The hyperparameter space we optimize is identical to Thomas et al. (2018). We use multi-criteria Bayesian Optimization ( Bischl et al. (2016, 2018); Horn and Bischl (2016)) (c.f section 3.2) in order to optimize the machine-learning pipeline."}, {"heading": "3.1 Sub-evaluations", "text": "In the context of multi-criteria optimization, early stopping is no longer trivial, as multiple pareto-optimal solutions might exist. The same holds for the selection of an optimal classification threshold as a postprocessing step in case a measure requiring binary outcomes instead of probabilities. At the same time, evaluating different thresholds or a\n1 https://github.com/pfistfl/autoxgboostMC\nsub-model using only a fraction of a model\u2019s gradient boosting iterations is very cheap after fitting a full model. In order to make use of this information we adopt the following procedure:\nAlgorithm 1 Bayesian Optimization using sub-evaluations Require: Sm \u2190 \u22c3m 1 (\u03b8 ? i , yi): m Initial evaluations\nj \u2190 m+ 1 while Budget left do\n\u03b8?j \u2190 proposed using Bayesian Optimization on Sj\u22121 f\u03b8?,j \u2190 fitted on data using \u03b8j , nroundsj and applying thrj . yj \u2190 obtained by evaluating f\u03b8?,j for each measure. Sj \u2190 Sj\u22121 \u222a (\u03b8?j , yj) Ssub,j \u2190 obtain sub-evaluations (Algorithm 2) Ssub,j \u2190 keep only Ssub,j which are on the pareto front of Sj \u222a Ssub,j Sj \u2190 Sj \u222a Ssub,j j \u2190 j + 1\nend while\nA full pipeline configuration \u03b8? \u2208 \u0398? is composed of a threshold thr \u2208 [0; 1], the number of boosting iterations nrounds and several other pipeline hyperparameters, denoted by \u03b8 for simplicity. From a set of m randomly chosen initial configurations and their corresponding performances Sm we start multi-criteria Bayesian Optimization as described in Algorithm 1. The method for obtaining sub-evaluations is described in Algorithm 2. In order to decrease the number of sub-evaluations, we resort to evaluating only 25%, 50%, 75% and 90% of nrounds (rounded to the next integer). Although this section describes the case of classifying a binary target variable, extensions to multi-class classification can be made by instead using a vector of thresholds instead.\nAlgorithm 2 Obtaining Sub-evaluations Require:\nModel f\u03b8?,j ; i\u2190 1 for n in {1, ..., nrounds} do\nfor thr in {0, 0.1, 0.2, ..., 1} do Si \u2190 evaluate f?\u03b8 using n iterations, applying threshold thr i\u2190 i+ 1\nend for end for Ssub \u2190 \u22c3i 1 Si"}, {"heading": "3.2 Multi-Criteria Bayesian Optimization", "text": "Formally multi-criteria optimization problems are defined by a set of target functions f(\u03b8) = (f1(\u03b8), . . . , fk(\u03b8)) which should be optimized simultaneously. As there is no inherent order between the targets, the concept of Pareto dominance is used to rank different candidate configurations. One configuration \u03b8 pareto-dominates another configuration \u03b8\u0303, \u03b8 \u03b8\u0303, if fi(\u03b8) \u2264 fi(\u03b8\u0303) for i = 1, . . . , k and \u2203 j fj(\u03b8) < fj(\u03b8\u0303), i.e., \u03b8 needs to be as good as \u03b8\u0303 in each component and strictly better in at least one. A configuration \u03b8 is said to be non-dominated if it is not dominated by any other configuration. The set of all non-dominated points is the Pareto set, which contains all trade-off solutions. Finally, the Pareto front is evaluation of all configurations in the Pareto set. The goal of multi-criteria optimization is to learn the Pareto set. There exists a plethora of different ways to extend Bayesian optimization to the multi-criteria case. We choose parEgo Knowles (2004), as it is a simple method, and it naturally lends itself to focussing regions of the pareto front. parEgo is a rather simple extension, which scalarizes the set of target functions by using the augmented Tchebycheff norm\nmax i=1,...,k (wifi(\u03b8)) + \u03c1 k\u2211 i=i wifi(\u03b8),\nwith a different uniformly sampled weight vector w such that \u2211k\ni=1 wi = 1 in each iteration. The augmentation term \u03c1 \u2211k i=i wifi(\u03b8); \u03c1 > 0 is used to guarantee paretooptimal solutions (Miettinen and M\u00e4kel\u00e4, 2002). This allows to apply standard singlecriteria Bayesian optimization to the scalarized target function. Furthermore the use of the augmented Tchebycheff norm allows to exclude regions of the pareto front which are not of practical relevance, e.g., models with extremely low predictive accuracy do not have to be considered regardless of their interpretability or fairness (Steuer and Choo (1983), Hakanen and Knowles (2017)). This is done by constraining the values of some wi between certain values. We adapt a similar procedure, where the user can choose ranges for the weights wi, such that the algorithm focuses on a selected region of the pareto front (see e.g. the blue line in Figure 3a). For the case of k = 2 objectives, the weight vector w \u2208 [0; 1]k can range from (1, 0) (only optimize first objective) to (0, 1) (only optimizing the second objective). By limiting w to [l, 1\u2212 l]\u00d7 [u, 1\u2212 u]; 0 < l < u < 1 we can effectively limit the possible trade-offs we might be willing to make."}, {"heading": "4 Application: A Fair Model for Income Prediction", "text": "Income prediction of employers is a versatile use-case for the application of multi-criteria AutoML, as several important criteria for a model can be derived. While trying to minimize the missclassification error, moral and ethical principles must also be adhered to. Thus, a model cannot be biased or unfair towards different sub-populations, e.g. men can not be systematically favoured over women regarding their income. As a third possible criterion, we might require an interpretable model, as a model might need to be accepted by regulatory bodies.\n0.00\n0.02\n0.04\n0.06\n0.2 0.4 0.6 mmce\nfa ir\nne ss\n.f1\n(a) Full pareto front of first AutoxgboostMC run. Not pareto-optimal points are displayed in red.\nm m ce fairness.f1\n0 25 50 75 100 125\n0.2\n0.4\n0.6\n0.00\n0.02\n0.04\n0.06\niter\n(b) Final optimization path.\nWe use a fairness measure described in section 2.2, namely the absolute difference in F1-Scores between two sub-populations male and female. In order to start the AutoML process, we simply need to specify our tuning budget by either setting the number of MBO-iterations or the desired time for tuning. To get a first impression of the pareto front, we start with a tuning budget of only 20 iterations. Figure 3a shows the resulting pareto front. We can now use this, to focus the search towards trade-offs we are interested in. This is done by limiting the range of projections available to parEgo. For a first investigation, we choose values between 0.1 and 0.9. Those lower and upper limits on the projections can be adapted throughout the process.\nAfterwards, we can simply continue training with additional budget. Continuing this training twice, we can also access the final optimization path, which is shown in Figure 3b. For each chosen measure, it shows the achieved performance for each function evaluation, and the (single-criteria) optimum achieved. Finally, we observe the pareto fronts as illustrated in Figure 4 for final tuning after 20, 70 and 120 iterations. We compare to Thomas et al. (2018), optimizing a single objective (mmce). Note that solely optimizing for Fairness is not sensible, as many models achieve a fairness score of 0 (on validation data). The user can then choose an optimal hyperparameter configuration from the pareto front which matches her preferences best. Table 1 displays different points from the pareto front as well as the single-objective method evaluated on test data."}, {"heading": "5 Outlook", "text": "In this work we conduct a first investigation into AutoML systems that can optimize a machine learning pipeline with respect to many different criteria. We provide several measures, that can be used as proxies for concepts such as Fairness, Interpretability, Robustness and others. Additionally, we implement a simplified AutoML system, that can optimize multiple objectives simultaneously, and can therefore serve as a tool for investigating such scenarios. The potential and necessity of our approach is demonstrated in a use-case.\nThe proposed method can be extended and improved in multiple directions. In a first iteration, we aim to include a wider array of gradient boosting methods, such as LightGBM (Ke et al. (2017)) and catboost (Dorogush et al. (2017)) into our framework. By combining this with a larger set of different pre- and post-processing methods, which can be tailored towards improving the different measures listed in section 2.2, we hope to obtain a toolbox that is suitable for many different situations where multiple criteria are required. Several interesting enhancements to the optimization procedure could also be made, either by adopting promising approaches from Bayesian Optimization (c.f. Paria et al. (2018)), or by adopting other search procedures.\nThe real underlying preferences a user has towards selecting a model might not always be easily quantify-able, because they rely on previous experience, implementation or other details. At the same time, a user can be asked to provide (noisy) labels for a set of models or to indicate preferences of one model over another (c.f Gonz\u00e1lez et al.\n(2017)). In future research, this might serve as an interesting avenue towards more human-centered AutoML. A third important part of research we aim to conduct is towards making AutoML methods more readily available to other user-groups, while at the same time providing them with sufficient tools to obtain models tailored towards the specific applications needs. In order to achieve this, we aim to research User Interfaces that make the AutoML more transparent to the user, while at the same time ensuring reproducibility."}], "title": "Multi-Objective Automatic Machine Learning with AutoxgboostMC", "year": 2019}