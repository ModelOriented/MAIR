{"abstractText": "\u008ce ability to interpret machine learning models has become increasingly important now thatmachine learning is used to inform consequential decisions. We propose an approach calledmodel extraction for interpreting complex, blackbox models. Our approach approximates the complex model using a much more interpretable model; as long as the approximation quality is good, then statistical properties of the complex model are reflected in the interpretable model. We show how model extraction can be used to understand and debug random forests and neural nets trained on several datasets from the UCI Machine Learning Repository, as well as control policies learned for several classical reinforcement learning problems.", "authors": [{"affiliations": [], "name": "Osbert Bastani"}, {"affiliations": [], "name": "Carolyn Kim"}, {"affiliations": [], "name": "Hamsa Bastani"}], "id": "SP:81ff25379a5a72b8e958ad1af5794a5b16120fb1", "references": [{"authors": ["Elaine Angelino", "Nicholas Larus-Stone", "Daniel Alabi", "Margo Seltzer", "Cynthia Rudin"], "title": "Learning certifiably optimal rule lists for categorical data", "year": 2017}, {"authors": ["Jimmy Ba", "Rich Caruana"], "title": "Do deep nets really need to be deep? In Advances in neural information processing", "year": 2014}, {"authors": ["Andrew G Barto", "Richard S Su\u008aon", "Charles W Anderson"], "title": "Neuronlike adaptive elements that can solve difficult learning control problems", "venue": "IEEE transactions on systems, man, and cybernetics,", "year": 1983}, {"authors": ["Leo Breiman", "Jerome Friedman", "Charles J Stone", "Richard A Olshen"], "title": "Classification and regression trees", "venue": "CRC press,", "year": 1984}, {"authors": ["Rich Caruana", "Yin Lou", "Johannes Gehrke"], "title": "Intelligible models for classification and regression", "venue": "In Proceedings of the 23rd ACM SIGKDD Conference on Knowledge Discovery and Data Mining. Citeseer,", "year": 2012}, {"authors": ["Rich Caruana", "Yin Lou", "Johannes Gehrke", "Paul Koch", "Marc Sturm", "Noemie Elhadad"], "title": "Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission", "venue": "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "year": 2015}, {"authors": ["Paulo Cortez", "Alice Maria Gon\u00e7alves Silva"], "title": "Using data mining to predict secondary school student", "year": 2008}, {"authors": ["Berkeley J Dietvorst", "Joseph P Simmons", "Cade Massey"], "title": "Algorithm aversion: People erroneously avoid algorithms after seeing them err", "venue": "Journal of Experimental Psychology: General,", "year": 2015}, {"authors": ["Finale Doshi-Velez", "Been Kim"], "title": "A roadmap for a rigorous science of interpretability", "venue": "arXiv preprint arXiv:1702.08608,", "year": 2017}, {"authors": ["Cynthia Dwork", "Moritz Hardt", "Toniann Pitassi", "Omer Reingold", "Richard Zemel"], "title": "Fairness through awareness", "venue": "In Proceedings of the 3rd Innovations in\u008aeoretical Computer Science Conference,", "year": 2012}, {"authors": ["M Forina"], "title": "An extendible package for data exploration, classification and correlation", "venue": "Institute of Pharmaceutical and Food Analisys and Technologies, Via Brigata Salerno,", "year": 1991}, {"authors": ["Moritz Hardt", "Eric Price", "Nati Srebro"], "title": "Equality of opportunity in supervised learning", "venue": "In Advances in Neural Information Processing Systems,", "year": 2016}, {"authors": ["Jongbin Jung", "Connor Concannon", "Ravi Shroff", "Sharad Goel", "Daniel G Goldstein"], "title": "Simple rules for complex decisions", "venue": "arXiv preprint arXiv:1702.04690,", "year": 2017}, {"authors": ["Jon Kleinberg", "Himabindu Lakkaraju", "Jure Leskovec", "Jens Ludwig", "Sendhil Mullainathan"], "title": "Human decisions and machine predictions", "venue": "Technical report, National Bureau of Economic Research,", "year": 2017}, {"authors": ["P.W. Koh", "P. Liang"], "title": "Understanding black-box predictions via influence functions", "venue": "arXiv preprint arXiv:1703.04730,", "year": 2017}, {"authors": ["Igor Kononenko"], "title": "Machine learning formedical diagnosis: history, state of the art and perspective", "venue": "Artificial Intelligence in medicine,", "year": 2001}, {"authors": ["Benjamin Letham", "Cynthia Rudin", "Tyler H McCormick", "David Madigan"], "title": "Interpretable classifiers using rules and bayesian analysis: Building a be\u008aer stroke prediction model", "venue": "\u008ae Annals of Applied Statistics,", "year": 2015}, {"authors": ["Dino Pedreshi", "Salvatore Ruggieri", "Franco Turini"], "title": "Discrimination-aware data mining", "venue": "In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining,", "year": 2008}, {"authors": ["Marco Tulio Ribeiro", "Sameer Singh", "Carlos Guestrin"], "title": "Why should i trust you?: Explaining the predictions of any classifier", "venue": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "year": 2016}, {"authors": ["Cynthia Rudin"], "title": "Algorithms for interpretable machine learning", "venue": "In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining,", "year": 2014}, {"authors": ["Selim Temizer", "Mykel Kochenderfer", "Leslie Kaelbling", "Tomas Lozano-P\u00e9rez", "James Kuchar"], "title": "Collision avoidance for unmanned aircra\u0089 using markov decision processes", "venue": "In AIAA guidance, navigation, and control conference,", "year": 2010}, {"authors": ["Berk Ustun", "Cynthia Rudin"], "title": "Supersparse linear integer models for optimized medical scoring systems", "venue": "Machine Learning,", "year": 2016}, {"authors": ["Gilmer Valdes", "Jos\u00e9 Marcio Luna", "Eric Eaton", "Charles B Simone"], "title": "Mediboost: a patient stratification tool for interpretable decision making in the era of precision medicine", "venue": "Scientific Reports,", "year": 2016}, {"authors": ["Anneleen Van Assche", "Hendrik Blockeel"], "title": "Seeing the forest through the trees", "venue": "In International Conference on Inductive Logic Programming,", "year": 2007}, {"authors": ["Gilles Vandewiele", "Olivier Janssens", "Femke Ongenae", "Filip De Turck", "Sofie Van Hoecke"], "title": "Genesim: genetic extraction of a single, interpretable model", "venue": "arXiv preprint arXiv:1611.05722,", "year": 2016}, {"authors": ["Fulton Wang", "Cynthia Rudin"], "title": "Falling rule lists", "venue": "In AIS- TATS,", "year": 2015}, {"authors": ["William H Wolberg", "Olvi L Mangasarian"], "title": "Multisurface method of pa\u008aern separation for medical diagnosis applied to breast cytology", "venue": "Proceedings of the national academy of sciences,", "year": 1990}], "sections": [{"text": "ar X\niv :1\n70 6.\n09 77\n3v 4\n[ cs\n.L G\n] 1\n3 M\nar 2\n01 8"}, {"heading": "1 INTRODUCTION", "text": "Recent advances in machine learning have revolutionized our ability to use data to inform critical decisions, such as medical diagnosis [8, 19, 27], bail decisions for defendants [16, 17], and aircra collision avoidance systems [25]. At the same time, machine learning algorithms have been shown to exhibit unexpected defects when deployed in the realworld; examples include causality (i.e., inability to distinguish causal effects from correlations) [8, 21], fairness (i.e., internalizing prejudices present in training data) [13, 15], and algorithm aversion (i.e., lack of trust by end users) [11]. Interpretability is a promising approach to address these challenges [12, 24]\u2014we can help human users diagnose issues and verify correctness of machine learning models by providing insight into the model\u2019s reasoning [3, 18, 20, 23, 26]. For example, suppose the user is trying to train a model that does not depend on a prejudiced feature (e.g., ethnicity). Omi ing the feature might not suffice to avoid prejudice, since the\n\u2217Presented as a poster at the 2017Workshop on Fairness, Accountability, and Transparency in Machine Learning (FAT/ML 2017).\nmodel could reconstruct that feature from other features [22]. A be er approach might be to train the model with the prejudiced feature, and then assess the dependence of themodel on that feature. is approach requires the ability to understand the model\u2019s reasoning process, i.e., how model predictions are affected by changing the prejudiced feature [12]. Similarly, the user may want to determine whether dependence on a feature is causal, or understand the high-level structure of the model to gain confidence in its correctness. In this paper, we propose an technique that we call model extraction for interpreting the overall reasoning process performed by amodel. Given amodel f : X \u2192 Y, the interpretation produced by our algorithm is an approximationT (x) \u2248 f (x), whereT is an interpretable model. In this paper, we take T to be a decision tree, which has been established as highly interpretable [3, 20, 23]. Intuitively, if T is a sufficiently good approximation of f , then any issues in f should be reflected in T as well. us, the user can understand and debug f by examining T ; then, the original model f can be deployed so that performance is not sacrificed. Previous model extraction approaches have focused on specific model families [10, 28, 29], enabling them to leverage the internal structure of the model. In contrast, our approach is blackbox, i.e., it only requires the ability to obtain the output f (x) \u2208 Y corresponding to a given input x \u2208 X. us, our approach works with any model family and is independent of the implementation. Complimentary approaches to interpretability focus on learning interpretable models [7, 26, 30] or on explaining the model\u2019s behavior on specific inputs rather than the model as a whole [23]. e key challenge to learning accurate decision trees is that they o en overfit and obtain poor performance, whereas complex models such as random forests and deep neural nets are be er regularized [4]. For example, random forests use ensembles of trees to avoid overfi ing to specific features or training points.\nus, our algorithmuses active learning to construct T from f \u2014it actively samples a large number of training points, and computes the corresponding labels y = f (x). e large quantity of data ensures that T does not overfit to the small set of initial training points. We prove that under mild assumptions, by generating a sufficient quantity of data, the extracted tree T converges to the exact greedy decision tree, i.e., it avoids overfi ing since the sampling error goes to zero. We implement our algorithm and use it to interpret random forests and neural nets, as well as control policies trained using reinforcement learning. We show that our active learning approach substantially improves over using CART [6], a standard decision tree learning algorithm. Furthermore, we demonstrate how the decision trees extracted can be used to debug issues with thesemodels, for example, to assess the dependence on prejudiced features, to determine why certain models perform worse, and to understand the high-level structure of a learned control policy."}, {"heading": "2 MODEL EXTRACTION", "text": "We describe our model extraction algorithm."}, {"heading": "2.1 Problem Formulation", "text": "Given a training set Xtrain \u2286 X and blackbox access to a function f : X \u2192 Y, our goal is to learn an interpretable model T : X \u2192 Y that approximates f . In this paper, we take T to be an axis-aligned decision tree, since these models are both expressive highly interpretable. For simplicity, we focus on the case of classification, i.e., Y = [m] = {1, ...,m}. We measure performance using accuracy relative to f on a held out test set, i.e., 1\n|Xtest |\n\u2211\nx \u2208Xtest I[T (x) = f (x)]."}, {"heading": "2.2 Algorithm", "text": "Our algorithm is greedy, both for scalability and because it is a natural fit for interpretability, since more relevant features occur higher in the tree.\nInput distribution. First, our algorithm constructs a distribution P over the input space X by fi ing a mixture of axis-aligned Gaussian distributions to the training data using expectation maximization.\nExact greedy decision tree. Wedescribe the exact greedy decision tree T \u2217. We cannot constructT \u2217 since we treat f as a blackbox; as we describe below, our algorithm\napproximates T \u2217. Essentially, T \u2217 is constructed greedily as a CART tree [6], except the gain is computed exactly according to P. For example, if the gain is the Gini impurity, then it is computed as follows:\nGain(f ,CN ) = 1 \u2212 \u2211\ny\u2208Y\nPrx\u223cP[f (x) = y | CN ],\nwhere CN are the constraints encoding which points flow to the node N in T \u2217 for which a branch is currently being constructed. Similarly, the most optimal leaf labels are computed exactly according to P.\nEstimated greedy decision tree. Given n \u2208 N, our algorithm estimates Gain(f ,CN ) using n i.i.d. samples x \u223c P | CN , whereCN is a conjunction of axis-aligned constraints. We briefly describe how our algorithm obtains such samples. It is straightforward to show that the constraint CN can be simplified so it contains at most one inequality (xi \u2264 t) and at most one inequality (xi > s) per i \u2208 [d]. For simplicity, we assume CN contains both inequalities for each i \u2208 [d]:\nCN = (s1 \u2264 x1 \u2264 t1) \u2227 ... \u2227 (sd \u2264 xd \u2264 td ).\nen, the probability density function of P | CN is\npP |CN (x) \u221d\nK \u2211\nj=1\n\u03d5 j\nd \u220f\ni=1\npN(\u00b5ji ,\u03c3ji ) |(si \u2264xi \u2264ti )(xi ).\nSince theGaussians are axis-aligned, the unnormalized probability of each component is\n\u03d5\u0303 \u2032j =\n\u222b\n\u03d5 j\nd \u220f\ni=1\npN(\u00b5ji ,\u03c3ji ) |(si \u2264xi \u2264ti )(xi )dx\n= \u03d5 j\nd \u220f\ni=1\n(\n\u03a6\n(\nti \u2212 \u00b5 ji\n\u03c3ji\n)\n\u2212 \u03a6\n(\nsi \u2212 \u00b5 ji\n\u03c3ji\n))\n,\nwhere \u03a6 is the cumulative density function of the standard Gaussian distribution N(0, 1). en, the component probabilities are \u03d5\u0303 = Z\u22121\u03d5\u0303 \u2032, where Z = \u2211K\nj=1 \u03d5\u0303 \u2032 j .\nTo sample x \u223c P | CN , sample j \u223c Categorical(\u03d5\u0303), and\nxi \u223c N(\u00b5 ji ,\u03c3ji ) | (si \u2264 xi \u2264 ti ) (for each i \u2208 [d]).\nWeuse standard algorithms for sampling truncatedGaussian distributions to sample each xi ."}, {"heading": "2.3 eoretical Guarantees", "text": "e extracted treeT converges to T \u2217 as n \u2192 \u221e:\nTheorem 2.1. Assume the exact greedy tree T \u2217 is well defined, and the probability density function p(x) is bounded, continuous, and has bounded support. en, for any \u03f5,\u03b4 > 0, there exists n \u2208 N such that the tree T extracted by our algorithm using n samples satisfies Prx\u223cP[T (x) = T\n\u2217(x)] \u2264 \u03f5 , with probability at least 1\u2212\u03b4 over the training samples."}, {"heading": "3 EVALUATION", "text": "Weuse ourmodel extraction algorithm to interpret several supervised learningmodels trainedon datasets from the UCI Machine Learning Repository [2], as well as a learned control policy from OpenAI Gym [1], i.e., the learned control policy \u03c0 : S \u2192 A."}, {"heading": "3.1 Comparison to CART", "text": "First, we compare our algorithm to a baseline that uses CART to train a decision tree approximating f on the training set {(x, f (x)) | x \u2208 Xtrain}. For both algorithms, we restrict the decision tree to have 31 nodes. We show results in Table 1. We show the test set performance of the extracted tree compared to ground truth (or for MDPs, estimated the reward when it is used as a policy), as well as the relative performance compared to the model f on the same test set. Note that our goal is to obtain high relative performance: a be er approximation of f is a be er interpretation of f , even if f has poor performance. Our algorithm outperforms the baseline on every problem instance."}, {"heading": "3.2 Examples of Use Cases", "text": "We show how the extracted decision trees can be used to interpret and debug models.\nUse of invalid features. Using an invalid feature is a common problem when training models. In particular, some datasets contain multiple response variables; then, one response should not be used to predict the other. For example, the breast cancer dataset contains two response variables indicating cancer recurrence: the length of time before recurrence and whether recurrence occurs within 24 months. is issue can be thought of as a special case of using a non-causal feature, an important problem in healthcare se ings. We train a random forest f to predict whether recurrence occurs within 24 months, where time to recurrence is incorrectly included as a feature. en, we extract a\ndecision tree approximating f of size k = 7 nodes, using 10 random splits of the dataset into training and test sets. e invalid feature occured in every extracted tree, and as the top branch in 6 of the 10 trees.\nUse of prejudiced features. We can use our algorithm to evaluate how a model f depends on prejudiced features. For example, gender is a feature in the student grade dataset, and may be considered sensitive when estimating student performance. However, if we simply omit gender, then f may reconstruct it from the remaining features. For a model f trained with gender available, we show how a decision tree extracted from f can be used to understand how f depends on gender. Our approach does not guarantee fairness, but it can be useful for evaluating the fairness of f . We extract decision treesT from the random forests f trained on 10 random splits of the student grades dataset. e top features are consistently grades in other classes or number of failing grades received in the past. Gender occurs below these features (at the fourth or fi h level) in 7 of 10 of the trees. We can estimate the overall effect of changing the gender label:\n\u2206 = Ex\u223cP[f (x) | male] \u2212 Ex\u223cP[f (x) | female].\nWhen gender occurs, \u2206 is between 0.31 and 0.70 grade points (average 0.49) out of 20 total grade points. For the remaining models, \u2206 is between 0.11 and 0.32 (average 0.25). us, the extracted tree includes gender when f has a relatively large dependence on gender. Furthermore, becauseT approximates f , we can use it to identify a subgroup of students where f has particularly strong dependence on gender. In particular, points that flow to the internal node N ofT branching on gender are a subset of inputs whose labelT (x) \u2208 Y is determined in part by gender. We can use T to measure the dependence on gender within this subset:\n\u2206N = Ex\u223cP[f (x) | CNL ] \u2212 Ex\u223cP[f (x) | CNR ],\nwhere NL and NR are the le and right children of N . Also, we can estimate the fraction of students in this subset using the test set, i.e., P = \u2211\nx \u2208Xtest I[x \u2208 F (CN )]. Finally, P \u00b7 \u2206N /\u2206 measures the fraction of the overall dependence of f on gender that is accounted for by the subtree rooted at N . For models where gender occurs in the extracted tree, the subgroup effect size \u2206N ranged from 0.44 to 0.77 grade points, and the estimated fraction of students in this subroup ranged from\n18.3% to 39.1%. e two trees that had the largest effect size had \u2206N of 0.77 and 0.43, resp., and P of 39.1% and 35.7%, resp. e identified subgroup accounted for 67.3% and 65.6% of the total effect of gender, resp. Having identified a subgroup of students likely to be adversely affected, the user might be able to train a be er model specifically for this subgroup. In 5 of the 7 extracted trees where gender occurs, the affected students were students with low grades, in particular, the 27% of students who scored fewer than 8.5 points in another class. is fine-grained understanding of f relies on the extracted model, and cannot be obtained using feature importance metrics alone.\nComparing models. We can use the extracted decision trees to compare different models trained on the same dataset, and gain insight into why some models perform be er than others. For example, random forests trained on the wine origin dataset performed very well, all achieving an F1 score of at least 0.961. In contrast, the performance of the neural nets was bimodal\u20145 had F1 score of at least 0.955, and the remaining had an F1 score of at most 0.741. We examined the top 3 layers of the extracted decision trees T , and made two observations. First, occurrence of the feature \u201cchlorides\u201d in T was almost perfectly correlated with poor performance of the neural nets. is feature occured in only one of the 10 trees extracted from random forests, and in none of the trees extracted from high performing neural nets. A weaker observation was the branching of T on the feature \u201calcohol\u201d, which is a very important feature\u2014it is the top branch for all but one of the 20 extracted decision trees. For the high performing models, the branch threshold t tended to be higher (749.8 to 999.6) than those for\nthe poorly performing models (574.4 to 837.3). e latter observation relies on having an extracted model\u2014 feature influence metrics alone are insufficient.\nUnderstanding control policies. Wecan use the extracted decision tree to understand a control policy. For example, we extracted a decision tree of size k = 7 from the cartpole control policy. While its estimated reward of 152.3 is lower than for larger trees, it captures a significant fraction of the policy behavior. e tree says to move the cart to the right exactly when\n(pole velocity \u2265 \u22120.286) \u2227 (pole angle \u2265 \u22120.071),\nwhere the pole velocity is in [\u22122.0, 2.0] and the pole angle is in [\u22120.5, 0.5]. In other words, move the cart to the right exactly when the pole is already on the right relative to the cart, and the pole is also moving toward the le (or more precisely, not moving fast enough toward the right). is policy is asymmetric, focusing on states where the cart is moving to the le . Examining an animation of simulation, this bias occurs because the cart initially moves toward the le , so the portion of the state space where the cart is moving toward the right is relatively unexplored."}, {"heading": "4 CONCLUSIONS", "text": "We have proposed model extraction as an approach for interpreting blackboxmodels, and shown how it can be used to interpret a variety of different kinds of models. Important directions for future work include devising algorithms for model extraction using more expressive input distributions, and developing new ways to gain insight from the extracted decision trees."}], "title": "Interpretability via Model Extraction", "year": 2018}