{"abstractText": "The thesis explores the role machine learning methods play in creating intuitive computational models of neural processing. We take the perspective that, combined with interpretability techniques, machine learning could replace human modeler and shift the focus of human effort from creating the models to extracting the knowledge from the already-made models and articulating that knowledge into intuitive representations. Automatic model-building methods can process larger volumes of data and explore more computationally complex relationships than a human modeler could. This perspective makes the case in favor of the larger role that exploratory and data-driven approach to computational neuroscience could play while coexisting alongside the traditional hypothesis-driven approach. We provide an example of how an intuitive model can be extracted from machinelearned knowledge, explore major machine learning algorithms in the context of the knowledge representation they employ, and propose a taxonomy of machine learning algorithms based on the knowledge representation that is driving their decision-making process. We exemplify the illustrated approach in the context of the knowledge representation taxonomy with three research projects that employ interpretability techniques on top of machine learning methods at three different levels of neural organization. In each case we demonstrate the applicability of the approach and present the neuroscientific knowledge it allowed us to extract. The first study (Chapter 3) explores feature importance analysis of a random forest decoder trained on intracerebral recordings from 100 human subjects to identify spectrotemporal signatures that characterize local neural activity during the task of visual categorization. The second study (Chapter 4) employs representation similarity analysis to compare the neural responses of the areas along the ventral stream with the activations of the layers of a deep convolutional neural network. The analysis allowed us to make conclusions and observations about the hierarchical organization of the human visual cortex and the similarities between the biological and an artificial system of vision. The third study (Chapter 5) proposes a method that allows test subjects to visually explore the state representation of their neural signal in real time. This is achieved by using a topology-preserving dimensionality reduction technique that allows to transform the neural data from the multidimensional representation used by the computer into a two-dimensional representation a human can grasp. Taken together, the approach, the taxonomy, and the examples, present a strong case for the applicability of machine learning methods in conjunction with interpretability techniques to automatic knowledge discovery in neuroscience. Seen from this perspective, machine learning models cease to be mere statistical black boxes and, by capturing the underlying dynamics of real life processes, reintroduce themselves as candidate models of reality.", "authors": [{"affiliations": [], "name": "Raul Vicente Zafra"}], "id": "SP:6ddeb89aa7a2c5eafcc6161db4c0aa67796eeb6e", "references": [{"authors": ["Aguirre", "Geoffrey K", "E Zarahn"], "title": "An area within human ventral cortex sensitive to \u201cbuilding\u201d stimuli: evidence and implications", "venue": "D\u2019esposito", "year": 1998}, {"authors": ["Alivisatos", "Bessie", "Michael Petrides"], "title": "Functional activation of the human brain during mental rotation", "venue": "In: Neuropsychologia 35.2, pp. 111\u2013118 (cit. on p. 92).", "year": 1997}, {"authors": ["Allison", "Brendan Z", "Clemens Brunner", "Vera Kaiser", "Gernot R M\u00fcller-Putz", "Christa Neuper", "Gert Pfurtscheller"], "title": "Toward a hybrid brain\u2013 computer interface based on imagined movement and visual attention", "venue": "In: Journal of neural engineering 7.2, p. 026007 (cit. on p. 92).", "year": 2010}, {"authors": ["Alvernhe", "Alice", "Etienne Save", "Bruno Poucet"], "title": "Local remapping of place cell firing in the Tolman detour task", "venue": "In: European Journal of Neuroscience 33.9, pp. 1696\u20131705 (cit. on p. 17).", "year": 2011}, {"authors": ["Anderson", "Charles W", "Zlatko Sijercic"], "title": "Classification of EEG signals from four subjects during five mental tasks", "venue": "In: Solving engineering problems with neural networks: proceedings of the conference on engineering applications in neural networks (EANN\u201996). Turkey, pp. 407\u2013 414 (cit. on p. 92).", "year": 1996}, {"authors": ["Andrews", "Robert", "Joachim Diederich", "Alan B Tickle"], "title": "Survey and critique of techniques for extracting rules from trained artificial neural networks", "venue": "In: Knowledge-based systems 8.6, pp. 373\u2013389 (cit. on p. 39).", "year": 1995}, {"authors": ["Anumanchipalli", "Gopala K", "Josh Chartier", "Edward F Chang"], "title": "Speech synthesis from neural decoding of spoken sentences", "venue": "In: Nature 568.7753, p. 493 (cit. on p. 20).", "year": 2019}, {"authors": ["Axelrod", "Vadim", "Galit Yovel"], "title": "Successful decoding of famous faces in the fusiform face area", "venue": "In: PLoS One 10.2, e0117126 (cit. on p. 46).", "year": 2015}, {"authors": ["Ba", "Jimmy", "Volodymyr Mnih", "Koray Kavukcuoglu"], "title": "Multiple object recognition with visual attention", "venue": "In: arXiv preprint arXiv:1412.7755 (cit. on p. 17).", "year": 2014}, {"authors": ["EH Baeg", "YB Kim", "K Huh", "I Mook-Jung", "HT Kim", "MW Jung"], "title": "Dynamics of population code for working memory in the prefrontal cortex", "venue": "In: Neuron 40.1, pp. 177\u2013188 (cit. on p. 19).", "year": 2003}, {"authors": ["Bahdanau", "Dzmitry", "Kyunghyun Cho", "Yoshua Bengio"], "title": "Neural machine translation by jointly learning to align and translate", "venue": "In: arXiv preprint arXiv:1409.0473 (cit. on p. 17).", "year": 2014}, {"authors": ["Banino", "Andrea", "Caswell Barry", "Benigno Uria", "Charles Blundell", "Timothy Lillicrap", "Piotr Mirowski", "Alexander Pritzel", "Martin J Chadwick", "Thomas Degris", "Joseph Modayil"], "title": "Vector-based navigation using grid-like representations in artificial agents", "venue": "Nature 557.7705,", "year": 2018}, {"authors": ["Barlow", "Horace B"], "title": "Sensory mechanisms, the reduction of redundancy, and intelligence", "venue": "In: Mechanisation of thought processes (cit. on p. 23).", "year": 1959}, {"authors": ["Barlow", "Horace B", "Colin Blakemore", "John D Pettigrew"], "title": "The neural mechanism of binocular depth discrimination", "venue": "In: The Journal of physiology 193.2, pp. 327\u2013342 (cit. on p. 23).", "year": 1967}, {"authors": ["Ba\u015far", "Erol", "Murat \u00d6zg\u00f6ren", "Adile \u00d6niz", "Christina Schmiedt", "Canan Ba\u015far-Ero\u011flu"], "title": "Brain oscillations differentiate the picture of one\u2019s own grandmother", "venue": "In: International Journal of Psychophysiology 64.1, pp. 81\u201390 (cit. on p. 92).", "year": 2007}, {"authors": ["Bassett", "Danielle S", "Olaf Sporns"], "title": "Network neuroscience", "venue": "In: Nature neuroscience 20.3, p. 353 (cit. on p. 23).", "year": 2017}, {"authors": ["Bassett", "Danielle S", "Perry Zurn", "Joshua I Gold"], "title": "On the nature and use of models in network neuroscience", "venue": "In: Nature Reviews Neuroscience, p. 1 (cit. on p. 23).", "year": 2018}, {"authors": ["Bastin", "Julien", "Giorgia Committeri", "Philippe Kahane", "Gaspare Galati", "Lorella Minotti", "Jean-Philippe Lachaux", "Alain Berthoz"], "title": "Timing of posterior parahippocampal gyrus activity reveals multiple scene processing stages", "venue": "In: Human brain mapping 34.6, pp. 1357\u20131370 (cit. on p. 55).", "year": 2013}, {"authors": ["Bastin", "Julien", "Juan R Vidal", "Seth Bouvier", "Marcela Perrone-Bertolotti", "Damien B\u00e8nis", "Philippe Kahane", "Olivier David", "Jean-Philippe Lachaux", "Russell A Epstein"], "title": "Temporal components in the parahippocampal place area revealed by human intracerebral recordings", "venue": "In: Journal of Neuroscience 33.24, pp. 10123\u201310131 (cit. on p. 47).", "year": 2013}, {"authors": ["Bastos", "Andre Moraes", "Julien Vezoli", "Conrado Arturo Bosman", "Jan-Mathijs Schoffelen", "Robert Oostenveld", "Jarrod Robert Dowdall", "Peter De Weerd", "Henry Kennedy", "Pascal Fries"], "title": "Visual areas exert feedforward", "year": 2015}, {"authors": ["Zhouhan Lin"], "title": "Towards biologically plausible deep learning", "venue": "In:", "year": 2015}, {"authors": ["Warland"], "title": "Reading a neural code", "venue": "In: Science 252.5014, pp. 1854\u2013", "year": 1991}, {"authors": ["Cabrera", "Alvaro Fuentes", "Dario Farina", "Kim Dremstrup"], "title": "Comparison of feature selection and classification methods for a brain\u2013computer interface driven by non-motor imagery", "venue": "In: Medical & biological engineering & computing 48.2, pp. 123\u2013132 (cit. on p. 97).", "year": 2010}, {"authors": ["Cadena", "Santiago A", "George H Denfield", "Edgar Y Walker", "Leon A Gatys", "Andreas S Tolias", "Matthias Bethge", "Alexander S Ecker"], "title": "Deep convolutional models improve predictions of macaque V1 responses to natural images", "venue": "In: bioRxiv, p. 201764 (cit. on p. 90).", "year": 2017}, {"authors": ["Cadieu", "Charles F", "Ha Hong", "Daniel LK Yamins", "Nicolas Pinto", "Diego Ardila", "Ethan A Solomon", "Najib J Majaj", "James J DiCarlo"], "title": "Deep neural networks rival the representation of primate IT cortex for core visual object recognition", "venue": "In: PLoS computational biology 10.12, e1003963 (cit. on p. 21).", "year": 2014}, {"authors": ["Carlson", "Thomas", "David A Tovar", "Arjen Alink", "Nikolaus Kriegeskorte"], "title": "Representational dynamics of object vision: the first 1000 ms", "venue": "In: Journal of vision 13.10, pp. 1\u20131 (cit. on p. 65).", "year": 2013}, {"authors": ["Chang", "Le", "Doris Y Tsao"], "title": "The code for facial identity in the primate brain", "venue": "In: Cell 169.6, pp. 1013\u20131028 (cit. on p. 20).", "year": 2017}, {"authors": ["Chaudhuri", "Rishidev", "Ila Fiete"], "title": "Computational principles of memory", "venue": "In: Nature neuroscience 19.3, p. 394 (cit. on p. 23).", "year": 2016}, {"authors": ["F Chochon", "L Cohen", "PF van de Moortele", "Stanislas Dehaene"], "title": "Differential contributions of the left and right inferior parietal lobules to number processing", "venue": "Journal of cognitive neuroscience 11.6,", "year": 1999}, {"authors": ["Cichy", "Radoslaw M", "Aditya Khosla", "Dimitrios Pantazis", "Antonio Torralba", "Aude Oliva"], "title": "Deep neural networks predict hierarchical spatiotemporal cortical dynamics of human visual object recognition", "venue": "In: arXiv preprint arXiv:1601.02970 (cit. on p. 73).", "year": 2016}, {"authors": ["Cichy", "Radoslaw Martin", "Aditya Khosla", "Dimitrios Pantazis", "Antonio Torralba", "Aude Oliva"], "title": "Comparison of deep neural networks to spatio-temporal cortical dynamics of human visual object recognition reveals hierarchical correspondence", "venue": "In: Scientific reports 6 (cit. on pp. 68, 87, 89, 90).", "year": 2016}, {"authors": ["Cichy", "Radoslaw Martin", "Dimitrios Pantazis", "Aude Oliva"], "title": "Resolving human object recognition in space and time", "venue": "In: Nature neuroscience 17.3, p. 455 (cit. on p. 65).", "year": 2014}, {"authors": ["Cohen", "Laurent", "Stanislas Dehaene", "Lionel Naccache", "St\u00e8phane Leh\u00e8ricy", "Ghislaine Dehaene-Lambertz", "Marie-Anne H\u00e8naff", "Fran\u00e7ois Michel"], "title": "The visual word form area: spatial and temporal characterization of an initial stage of reading in normal subjects and posterior split-brain patients", "venue": "In: Brain 123.2, pp. 291\u2013307 (cit. on pp. 42, 46, 55,", "year": 2000}, {"authors": ["Comon", "Pierre"], "title": "Independent component analysis, a new concept?", "year": 1994}, {"authors": ["Cover", "Thomas M", "Peter E Hart"], "title": "Nearest neighbor pattern", "venue": "(cit. on pp", "year": 1967}, {"authors": ["Dienes", "Zoltan", "Andy Field"], "title": "Redefine statistical significance", "venue": "(cit. on pp", "year": 2017}, {"authors": ["Engel", "Andreas K", "Pascal Fries"], "title": "Beta-band oscillations ", "year": 2010}, {"authors": ["Fried"], "title": "Neural \u201cignition\u201d: enhanced activation", "year": 2009}, {"authors": ["L Looger", "Misha B Ahrens"], "title": "Mapping brain activity at scale", "year": 2014}, {"authors": ["Fries", "Pascal"], "title": "A mechanism for cognitive dynamics: neuronal", "year": 2005}, {"authors": ["Tomas Mikolov"], "title": "Devise: A deep visual-semantic embedding", "year": 2013}, {"authors": ["Fuchs", "Eberhard", "Gabriele Fl\u00fcgge"], "title": "Adult neuroplasticity: more", "venue": "(cit. on p", "year": 2014}, {"authors": ["Glaser", "Joshua I", "Ari S Benjamin", "Roozbeh Farhoodi", "Konrad P Kording"], "title": "The roles of supervised machine learning in systems neuroscience", "venue": "In: Progress in neurobiology (cit. on p. 18).", "year": 2019}, {"authors": ["Graves", "Alex", "Greg Wayne", "Ivo Danihelka"], "title": "Neural turing machines", "venue": "In: arXiv preprint arXiv:1410.5401 (cit. on p. 15).", "year": 2014}, {"authors": ["Graves", "Alex", "Greg Wayne", "Malcolm Reynolds", "Tim Harley", "Ivo Danihelka", "Agnieszka Grabska-Barwinska", "Sergio G\u00f2mez Colmenarejo", "Edward Grefenstette", "Tiago Ramalho", "John Agapiou"], "title": "Hybrid computing using a neural network with dynamic external memory", "venue": "Nature 538.7626,", "year": 2016}, {"authors": ["Gray", "Charles M", "Wolf Singer"], "title": "Stimulus-specific neuronal oscillations in orientation columns of cat visual cortex", "venue": "In: Proceedings of the National Academy of Sciences 86.5, pp. 1698\u20131702 (cit. on p. 87).", "year": 1989}, {"authors": ["Griffiths", "Thomas L", "Nick Chater", "Charles Kemp", "Amy Perfors", "Joshua B Tenenbaum"], "title": "Probabilistic models of cognition: Exploring representations and inductive biases", "venue": "In: Trends in cognitive sciences 14.8, pp. 357\u2013364 (cit. on p. 32).", "year": 2010}, {"authors": ["Grill-Spector", "Kalanit", "Rafael Malach"], "title": "The human visual cortex", "venue": "In: Annu. Rev. Neurosci. 27, pp. 649\u2013677 (cit. on pp. 77, 88).", "year": 2004}, {"authors": ["Grill-Spector", "Kalanit", "Kevin S Weiner"], "title": "The functional architecture of the ventral temporal cortex and its role in categorization", "venue": "In: Nature Reviews Neuroscience 15.8, p. 536 (cit. on pp. 42, 55, 63).", "year": 2014}, {"authors": ["Grosenick", "Logan", "Stephanie Greer", "Brian Knutson"], "title": "Interpretable classifiers for FMRI improve prediction of purchases", "venue": "In: IEEE transactions on neural systems and rehabilitation engineering 16.6, pp. 539\u2013548 (cit. on p. 32).", "year": 2008}, {"authors": ["Grover", "Aditya", "Jure Leskovec"], "title": "node2vec: Scalable feature learning for networks", "venue": "In: Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining. ACM, pp. 855\u2013864 (cit. on pp. 33, 37).", "year": 2016}, {"authors": ["G\u00fc\u00e7l\u00fc", "Umut", "Marcel AJ van Gerven"], "title": "Deep neural networks reveal a gradient in the complexity of neural representations across the ventral stream", "venue": "In: The Journal of Neuroscience 35.27, pp. 10005\u201310014 (cit. on pp. 21, 68, 72, 87, 89).", "year": 2015}, {"authors": ["Guidotti", "Riccardo", "Anna Monreale", "Salvatore Ruggieri", "Franco Turini", "Fosca Giannotti", "Dino Pedreschi"], "title": "A survey of methods for explaining black box models", "venue": "In: ACM computing surveys (CSUR) 51.5, p. 93 (cit. on pp. 31, 38, 39).", "year": 2018}, {"authors": ["Hagmann", "Patric", "Leila Cammoun", "Xavier Gigandet", "Reto Meuli", "Christopher J Honey", "Van J Wedeen", "Olaf Sporns"], "title": "Mapping the structural core of human cerebral cortex", "venue": "In: PLoS biology 6.7, e159 (cit. on p. 23). 114", "year": 2008}, {"authors": ["Hall", "Mark Andrew"], "title": "Correlation-based feature selection for machine learning", "venue": "In: (cit. on p. 26).", "year": 1999}, {"authors": ["Hamam\u00e8", "Carlos M", "Marcin Szwed", "Michael Sharman", "Juan R Vidal", "Marcella Perrone-Bertolotti", "Philippe Kahane", "Olivier Bertrand", "JeanPhilippe Lachaux"], "title": "Dejerine\u2019s reading area revisited with intracranial EEG Selective responses to letter strings", "venue": "In: Neurology 80.6, pp. 602\u2013 603 (cit. on p. 47).", "year": 2013}, {"authors": ["Hamam\u00e8", "Carlos M", "Juan R Vidal", "Marcela Perrone-Bertolotti", "Tomas Ossand\u00f2n", "Karim Jerbi", "Philippe Kahane", "Olivier Bertrand", "JeanPhilippe Lachaux"], "title": "Functional selectivity in the human occipitotemporal cortex during natural vision: Evidence from combined intracranial EEG and eye-tracking", "venue": "In: NeuroImage 95, pp. 276\u2013286 (cit.", "year": 2014}, {"authors": ["Hardoon", "David R", "John Shawe-Taylor"], "title": "Decomposing the tensor kernel support vector machine for neuroscience data with structured labels", "venue": "In: Machine learning 79.1-2, pp. 29\u201346 (cit. on p. 32).", "year": 2010}, {"authors": ["Harris", "Richard J", "AndrewW Young", "Timothy J Andrews"], "title": "Morphing between expressions dissociates continuous from categorical representations of facial expression in the human brain", "venue": "In: Proceedings of the National Academy of Sciences 109.51, pp. 21164\u201321169 (cit. on p. 46).", "year": 2012}, {"authors": ["Hartigan", "John A", "Manchek A Wong"], "title": "Algorithm AS 136: A kmeans clustering algorithm", "venue": "In: Journal of the Royal Statistical Society. Series C (Applied Statistics) 28.1, pp. 100\u2013108 (cit. on p. 27).", "year": 1979}, {"authors": ["Hassabis", "Demis", "Dharshan Kumaran", "Christopher Summerfield", "Matthew Botvinick"], "title": "Neuroscience-inspired artificial intelligence", "venue": "In: Neuron 95.2, pp. 245\u2013258 (cit. on pp. 13, 15, 18).", "year": 2017}, {"authors": ["Haufe", "Stefan", "Frank Meinecke", "Kai G\u00f6rgen", "Sven D\u00e4hne", "John-Dylan Haynes", "Benjamin Blankertz", "Felix Bie\u00dfmann"], "title": "On the interpretation of weight vectors of linear models in multivariate neuroimaging", "venue": "In: Neuroimage 87, pp. 96\u2013110 (cit. on p. 32).", "year": 2014}, {"authors": ["Haxby", "James V"], "title": "Multivariate pattern analysis of fMRI: the early beginnings", "venue": "In: Neuroimage 62.2, pp. 852\u2013855 (cit. on p. 32).", "year": 2012}, {"authors": ["Haxby", "James V", "Andrew C Connolly", "J Swaroop Guntupalli"], "title": "Decoding neural representational spaces using multivariate pattern analysis", "venue": "In: Annual review of neuroscience 37, pp. 435\u2013456 (cit. on p. 32).", "year": 2014}, {"authors": ["Haxby", "James V", "M Ida Gobbini", "Maura L Furey", "Alumit Ishai", "Jennifer L Schouten", "Pietro Pietrini"], "title": "Distributed and overlapping representations of faces and objects in ventral temporal cortex", "venue": "In: Science 293.5539, pp. 2425\u20132430 (cit. on pp. 19, 42, 63).", "year": 2001}, {"authors": ["Haynes", "John-Dylan", "Geraint Rees"], "title": "Neuroimaging: decoding mental states from brain activity in humans", "venue": "In: Nature Reviews Neuroscience 7.7, p. 523 (cit. on pp. 32, 42, 99). 115", "year": 2006}, {"authors": ["Hebb", "Donald O"], "title": "The organization of behavior; a neuropsycholocigal", "year": 1949}, {"authors": ["Romo"], "title": "Decoding a perceptual decision process across cortex", "venue": "In:", "year": 2010}, {"authors": ["Nguyen", "Brian Kingsbury"], "title": "Deep neural networks for acous", "year": 2012}, {"authors": ["Julie A Fiez", "Avniel Singh Ghuman"], "title": "Decoding and disrupting", "year": 2016}, {"authors": ["D Penn", "John P Donoghue"], "title": "Neuronal ensemble control", "year": 2006}, {"authors": ["Hopfield", "John J"], "title": "Neural networks and physical systems with emergent collective computational abilities", "venue": "In: Proceedings of the national academy of sciences 79.8, pp. 2554\u20132558 (cit. on pp. 15, 33).", "year": 1982}, {"authors": ["Huang", "Cheng-Ning", "Chun-Han Chen", "Hung-Yuan Chung"], "title": "Application of facial electromyography in computer mouse access for people with disabilities", "venue": "In: Disability and Rehabilitation 28.4, pp. 231\u2013237 (cit. on p. 99).", "year": 2006}, {"authors": ["Huang", "Nicholas", "Malcolm Slaney", "Mounya Elhilali"], "title": "Connecting Deep Neural Networks to Physical, Perceptual, and Electrophysiological Auditory Signals", "venue": "In: Frontiers in neuroscience 12 (cit. on p. 21).", "year": 2018}, {"authors": ["Hubel", "David H", "Torsten N Wiesel"], "title": "Receptive fields of single neurones in the cat\u2019s striate cortex", "venue": "In: The Journal of physiology 148.3, pp. 574\u2013591 (cit. on p. 13).", "year": 1959}, {"authors": ["Hung", "Chou P", "Gabriel Kreiman", "Tomaso Poggio", "James J DiCarlo"], "title": "Fast readout of object identity from macaque inferior temporal cortex", "venue": "In: Science 310.5749, pp. 863\u2013866 (cit. on p. 20).", "year": 2005}, {"authors": ["Huth", "Alexander G", "Shinji Nishimoto", "An T Vu", "Jack L Gallant"], "title": "A continuous semantic space describes the representation of thousands of object and action categories across the human brain", "venue": "In: Neuron 76.6, pp. 1210\u20131224 (cit. on p. 19).", "year": 2012}, {"authors": ["Huysmans", "Johan", "Karel Dejaeger", "Christophe Mues", "Jan Vanthienen", "Bart Baesens"], "title": "An empirical evaluation of the comprehensibility of decision table, tree and rule based predictive models", "venue": "In: Decision Support Systems 51.1, pp. 141\u2013154 (cit. on p. 38).", "year": 2011}, {"authors": ["Hwang", "Han-Jeong", "Kiwoon Kwon", "Chang-Hwang Im"], "title": "Neurofeedback-based motor imagery training for brain\u2013computer interface (BCI)", "venue": "In: Journal of neuroscience methods 179.1, pp. 150\u2013156 (cit. on p. 97).", "year": 2009}, {"authors": ["Ishai", "Alumit", "Conny F Schmidt", "Peter Boesiger"], "title": "Face perception is mediated by a distributed cortical network", "venue": "In: Brain research bulletin 67.1-2, pp. 87\u201393 (cit. on p. 64).", "year": 2005}, {"authors": ["Ishai", "Alumit", "Leslie G Ungerleider", "Alex Martin", "Jennifer L Schouten", "James V Haxby"], "title": "Distributed representation of objects in the human ventral visual pathway", "venue": "In: Proceedings of the National Academy of Sciences 96.16, pp. 9379\u20139384 (cit. on pp. 42, 55, 63).", "year": 1999}, {"authors": ["Jackson", "Andrew", "Jaideep Mavoori", "Eberhard E Fetz"], "title": "Longterm motor cortex plasticity induced by an electronic neural implant", "venue": "In: Nature 444.7115, p. 56 (cit. on p. 21). 117", "year": 2006}, {"authors": ["Jensen", "Ole", "Ali Mazaheri"], "title": "Shaping functional architecture by oscillatory alpha activity: gating by inhibition", "venue": "In: Frontiers in human neuroscience 4, p. 186 (cit. on p. 43).", "year": 2010}, {"authors": ["Jerbi", "Karim", "Juan R Vidal", "Tomas Ossandon", "Sarang S Dalal", "Julien Jung", "Dominique Hoffmann", "Lorella Minotti", "Olivier Bertrand", "Philippe Kahane", "Jean-Philippe Lachaux"], "title": "Exploring the electrophysiological correlates of the default-mode network with intracerebral EEG", "venue": "In: Frontiers in systems neuroscience 4, p. 27 (cit. on p. 66).", "year": 2010}, {"authors": ["Jia", "Yangqing", "Evan Shelhamer", "Jeff Donahue", "Sergey Karayev", "Jonathan Long", "Ross Girshick", "Sergio Guadarrama", "Trevor Darrell"], "title": "Caffe: Convolutional Architecture for Fast Feature Embedding", "venue": "In: arXiv preprint arXiv:1408.5093 (cit. on p. 71).", "year": 2014}, {"authors": ["Jonas", "Jacques", "Corentin Jacques", "Joan Liu-Shuang", "H\u00e8l\u00e8ne Brissart", "Sophie Colnat-Coulbois", "Louis Maillard", "Bruno Rossion"], "title": "A face-selective ventral occipito-temporal map of the human brain with intracerebral potentials", "venue": "In: Proceedings of the National Academy of Sciences 113.28, E4088\u2013E4097 (cit. on p. 55).", "year": 2016}, {"authors": ["Jonas", "Jacques", "Bruno Rossion", "H\u00e8l\u00e8ne Brissart", "Solene Frismand", "Corentin Jacques", "Gabriela Hossu", "Sophie Colnat-Coulbois", "Herv\u00e8 Vespignani", "JeanPierre Vignal", "Louis Maillard"], "title": "Beyond the core face-processing network: Intracerebral stimulation of a face-selective area in the right anterior fusiform gyrus elicits transient prosopagnosia", "venue": "In: Cortex 72,", "year": 2015}, {"authors": ["Jones", "Eric", "Travis Oliphant", "Pearu Peterson"], "title": "SciPy: Open source scientific tools for Python. [Online; accessed 2016-11-08", "venue": "url: http://www.scipy.org/ (cit. on p", "year": 2001}, {"authors": ["Juphard", "Alexandra", "Juan R Vidal", "Marcela Perrone-Bertolotti", "Lorella Minotti", "Philippe Kahane", "Jean-Philippe Lachaux", "Monica Baciu"], "title": "Direct evidence for two different neural mechanisms for reading familiar and unfamiliar words: an intra-cerebral EEG study", "venue": "In: Frontiers in human neuroscience 5, p. 101 (cit. on p. 61).", "year": 2011}, {"authors": ["Kadipasaoglu", "Cihan Mehmet", "Christopher Richard Conner", "Meagan Lee Whaley", "Vatche George Baboyan", "Nitin Tandon"], "title": "Categoryselectivity in human visual cortex follows cortical topology: a grouped icEEG study", "venue": "In: PloS one 11.6, e0157109 (cit. on p. 55).", "year": 2016}, {"authors": ["Kamitani", "Yukiyasu", "Frank Tong"], "title": "Decoding seen and attended motion directions from activity in the human visual cortex", "venue": "In: Current biology 16.11, pp. 1096\u20131102 (cit. on p. 42).", "year": 2006}, {"authors": ["Kanwisher", "Nancy", "Josh McDermott", "Marvin M Chun"], "title": "The fusiform face area: a module in human extrastriate cortex specialized for face perception", "venue": "In: Journal of neuroscience 17.11, pp. 4302\u20134311 (cit. on pp. 42, 46, 55, 63, 64). 118", "year": 1997}, {"authors": ["Kell", "Alexander JE", "Daniel LK Yamins", "Erica N Shook", "Sam V NormanHaignere", "Josh H McDermott"], "title": "A task-optimized neural network replicates human auditory behavior, predicts brain responses, and reveals a cortical processing hierarchy", "venue": "In: Neuron 98.3, pp. 630\u2013644 (cit. on p. 21).", "year": 2018}, {"authors": ["Kepler", "Johannes"], "title": "Epitome Astronomiae Copernicanae (cit", "venue": "on p. 23).", "year": 1621}, {"authors": ["Khaligh-Razavi", "Seyed-Mahdi", "Nikolaus Kriegeskorte"], "title": "Deep supervised, but not unsupervised, models may explain IT cortical representation", "venue": "In: PLoS Comput Biol 10.11, e1003915 (cit. on pp. 21, 68, 89, 90).", "year": 2014}, {"authors": ["Kipf", "Thomas N", "Max Welling"], "title": "Semi-supervised classification with graph convolutional networks", "venue": "In: arXiv preprint arXiv:1609.02907 (cit. on pp. 33, 37).", "year": 2016}, {"authors": ["Koch", "Christof"], "title": "Biophysics of computation: information processing in single neurons", "venue": "Oxford university press (cit. on p. 23).", "year": 2004}, {"authors": ["Kohonen", "Teuvo"], "title": "The self-organizing map", "venue": "In: Proceedings of the IEEE 78.9, pp. 1464\u20131480 (cit. on pp. 33, 39, 94).", "year": 1990}, {"authors": ["Koller", "Daphne", "Nir Friedman"], "title": "Probabilistic graphical models: principles and techniques", "venue": "MIT press (cit. on p. 33).", "year": 2009}, {"authors": ["K\u00f6rding", "Konrad P", "Daniel M Wolpert"], "title": "Bayesian integration in sensorimotor learning", "venue": "In: Nature 427.6971, p. 244 (cit. on p. 24).", "year": 2004}, {"authors": ["Koul", "Anurag", "Sam Greydanus", "Alan Fern"], "title": "Learning Finite State Representations of Recurrent Policy Networks", "venue": "In: arXiv preprint arXiv:1811.12530 (cit. on p. 39).", "year": 2018}, {"authors": ["Kreiman", "Gabriel", "Christof Koch", "Itzhak Fried"], "title": "Category-specific visual responses of single neurons in the human medial temporal lobe", "venue": "In: Nature neuroscience 3.9, p. 946 (cit. on p. 42).", "year": 2000}, {"authors": ["Kriegeskorte", "Nikolaus"], "title": "Deep neural networks: a new framework for modeling biological vision and brain information processing", "venue": "In: Annual Review of Vision Science 1, pp. 417\u2013446 (cit. on pp. 68, 86, 89, 90).", "year": 2015}, {"authors": ["Kriegeskorte", "Nikolaus", "Pamela K Douglas"], "title": "Cognitive computational neuroscience", "venue": "In: Nature neuroscience, p. 1 (cit. on pp. 23, 24).", "year": 2018}, {"authors": ["Kriegeskorte", "Nikolaus", "Rainer Goebel", "Peter Bandettini"], "title": "Information-based functional brain mapping", "venue": "In: Proceedings of the National Academy of Sciences 103.10, pp. 3863\u20133868 (cit. on p. 42).", "year": 2006}, {"authors": ["Kriegeskorte", "Nikolaus", "Rogier A Kievit"], "title": "Representational geometry: integrating cognition, computation, and the brain", "venue": "In: Trends in cognitive sciences 17.8, pp. 401\u2013412 (cit. on p. 32).", "year": 2013}, {"authors": ["Kriegeskorte", "Nikolaus", "Marieke Mur", "Peter A Bandettini"], "title": "Representational similarity analysis-connecting the branches of systems neuroscience", "venue": "In: Frontiers in systems neuroscience 2, p. 4 (cit. on pp. 21, 33, 72). 119", "year": 2008}, {"authors": ["Krizhevsky", "Alex", "Ilya Sutskever", "Geoffrey E Hinton"], "title": "Imagenet classification with deep convolutional neural networks", "venue": "In: Advances in neural information processing systems, pp. 1097\u20131105 (cit. on pp. 13, 26, 71).", "year": 2012}, {"authors": ["Kuzovkin", "Ilya", "Raul Vicente", "Mathilde Petton", "Jean-Philippe Lachaux", "Monica Baciu", "Philippe Kahane", "Sylvain Rheims", "Juan R Vidal", "Jaan Aru"], "title": "Activations of deep convolutional neural networks are aligned with gamma band activity of human visual cortex", "venue": "In: Communications Biology 1.107 (cit. on pp. 21, 64).", "year": 2018}, {"authors": ["Lachaux", "Jean-Philippe", "Nikolai Axmacher", "Florian Mormann", "Eric Halgren", "Nathan E Crone"], "title": "High-frequency neural activity and human cognition: past, present and possible future of intracranial EEG research", "venue": "In: Progress in neurobiology 98.3, pp. 279\u2013301 (cit. on pp. 43, 65).", "year": 2012}, {"authors": ["Lachaux", "Jean-Philippe", "Nathalie George", "Catherine Tallon-Baudry", "Jacques Martinerie", "Laurent Hugueville", "Lorella Minotti", "Philippe Kahane", "Bernard Renault"], "title": "The many faces of the gamma band response to complex visual stimuli", "venue": "In: Neuroimage 25.2, pp. 491\u2013501 (cit. on p. 68).", "year": 2005}, {"authors": ["Lachaux", "Jean-Philippe", "Eugenio Rodriguez", "Jacques Martinerie", "Francisco J Varela"], "title": "Measuring phase synchrony in brain signals", "venue": "Human brain mapping", "year": 1999}, {"authors": ["Lapicque", "Louis"], "title": "Recherches quantitatives sur l\u2019excitation electrique des nerfs traitee comme une polarization", "venue": "In: Journal de Physiologie et de Pathologie Generalej 9, pp. 620\u2013635 (cit. on p. 23).", "year": 1907}, {"authors": ["LeCun", "Yann", "Yoshua Bengio", "Geoffrey Hinton"], "title": "Deep learning", "venue": "In: nature 521.7553, p. 436 (cit. on pp. 13, 26, 33).", "year": 2015}, {"authors": ["LeCun", "Yann", "L\u00e8on Bottou", "Yoshua Bengio", "Patrick Haffner"], "title": "Gradient-based learning applied to document recognition", "venue": "Proceedings of the IEEE", "year": 1998}, {"authors": ["Lerner", "Yulia", "Talma Hendler", "Dafna Ben-Bashat", "Michal Harel", "Rafael Malach"], "title": "A hierarchical axis of object processing stages in the human visual cortex", "venue": "In: Cerebral Cortex 11.4, pp. 287\u2013297 (cit. on p. 77).", "year": 2001}, {"authors": ["Levy", "Jonathan", "Juan R Vidal", "Pascal Fries", "Jean-Fran\u00e7ois D\u00e8monet", "Abraham Goldstein"], "title": "Selective neural synchrony suppression as a forward gatekeeper to piecemeal conscious perception", "venue": "In: Cerebral Cortex 26.7, pp. 3010\u20133022 (cit. on p. 68).", "year": 2015}, {"authors": ["Li", "Fei Fei", "Rufin VanRullen", "Christof Koch", "Pietro Perona"], "title": "Rapid natural scene categorization in the near absence of attention", "venue": "In: Proceedings of the National Academy of Sciences 99.14, pp. 9596\u2013 9601 (cit. on p. 42). 120", "year": 2002}, {"authors": ["MacQueen", "James"], "title": "Some methods for classification", "venue": "(cit. on pp", "year": 1967}, {"authors": ["Jean-Philippe Lachaux"], "title": "Cortical dynamics of word recognition", "year": 2008}, {"authors": ["len", "Henry Kennedy", "Pascal Fries"], "title": "Alpha-beta and gamma", "year": 2016}, {"authors": ["Miller", "Michael B", "Christa-Lynn Donovan", "Craig M Bennett", "Elissa M Aminoff", "Richard E Mayer"], "title": "Individual differences in cognitive style and strategy predict similarities in the patterns of brain activity between individuals", "venue": "In: Neuroimage 59.1, pp. 83\u201393 (cit. on p. 92).", "year": 2012}, {"authors": ["Mnih", "Volodymyr", "Nicolas Heess", "Alex Graves"], "title": "Recurrent models of visual attention", "venue": "Advances in neural information processing systems,", "year": 2014}, {"authors": ["Mnih", "Volodymyr", "Koray Kavukcuoglu", "David Silver", "Alex Graves", "Ioannis Antonoglou", "Daan Wierstra", "Martin Riedmiller"], "title": "Playing atari with deep reinforcement learning", "venue": "In: arXiv preprint arXiv:1312.5602 (cit. on p. 15).", "year": 2013}, {"authors": ["Mnih", "Volodymyr", "Koray Kavukcuoglu", "David Silver", "Andrei A Rusu", "Joel Veness", "Marc G Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K Fidjeland", "Georg Ostrovski"], "title": "Human-level control through deep reinforcement learning", "venue": "Nature 518.7540,", "year": 2015}, {"authors": ["Momennejad", "Ida", "Evan M Russek", "Jin H Cheong", "Matthew M Botvinick", "ND Daw", "Samuel J Gershman"], "title": "The successor representation in human reinforcement learning", "venue": "In: Nature Human Behaviour 1.9, p. 680 (cit. on p. 17).", "year": 2017}, {"authors": ["Mountcastle", "Vernon B", "William H Talbot", "Hideo Sakata", "J Hyv\u00e4rinen"], "title": "Cortical neuronal mechanisms in flutter-vibration studied in unanesthetized monkeys", "venue": "Neuronal periodicity and frequency discrimination.\u201d In: Journal of neurophysiology 32.3, pp. 452\u2013484 (cit. on p. 19).", "year": 1969}, {"authors": ["Murdoch", "W James", "Chandan Singh", "Karl Kumbier", "Reza Abbasi-Asl", "Bin Yu"], "title": "Interpretable machine learning: definitions, methods, and applications", "venue": "In: arXiv preprint arXiv:1901.04592 (cit. on p. 31).", "year": 2019}, {"authors": ["Murphy", "Kevin P"], "title": "Machine learning: a probabilistic perspective", "venue": "MIT press (cit. on pp. 24, 26, 33).", "year": 2012}, {"authors": ["Narayanan", "Menaka", "Emily Chen", "Jeffrey He", "Been Kim", "Sam Gershman", "Finale Doshi-Velez"], "title": "How do humans understand explanations from machine learning systems? An evaluation of the human-interpretability of explanation.", "venue": "arXiv preprint arXiv:1802.00682 (cit. on p", "year": 2018}, {"authors": ["Neuper", "Christa", "Reinhold Scherer", "Miriam Reiner", "Gert Pfurtscheller"], "title": "Imagery of motor actions: Differential effects of kinesthetic and visual\u2013motor mode of imagery in single-trial EEG", "venue": "In: Cognitive brain research 25.3, pp. 668\u2013677 (cit. on p. 97).", "year": 2005}, {"authors": ["Newton", "Isaac"], "title": "Philosophiae naturalis principia mathematica (cit", "venue": "on p. 23). 123", "year": 1687}, {"authors": ["Kahane", "Jean-Philippe Lachaux"], "title": "Transient suppression", "year": 2011}, {"authors": ["p. 55). Parvizi", "Josef", "Sabine Kastner"], "title": "Promises and limitations", "year": 2018}, {"authors": ["Price", "Cathy J", "Joseph T Devlin"], "title": "The myth of the visual word", "year": 2003}, {"authors": ["Steven S Hsiao"], "title": "Neural correlates of high-gamma oscillations", "year": 2008}, {"authors": ["Ray", "Supratim", "John HR Maunsell"], "title": "Different origins of gamma rhythm and high-gamma activity in macaque visual cortex", "venue": "In: PLoS Biol 9.4, e1000610 (cit. on pp. 65, 87).", "year": 2011}, {"authors": ["Rescorla", "Robert A", "Allan R Wagner"], "title": "A theory of Pavlovian conditioning: Variations in the effectiveness of reinforcement and nonreinforcement", "venue": "(cit. on p", "year": 1972}, {"authors": ["Ribeiro", "Marco Tulio", "Sameer Singh", "Carlos Guestrin"], "title": "Modelagnostic interpretability of machine learning", "venue": "In: arXiv preprint arXiv:1606.05386 (cit. on p. 31).", "year": 2016}, {"authors": ["Rich", "Erin L", "Jonathan D Wallis"], "title": "Decoding subjective decisions from orbitofrontal cortex", "venue": "In: Nature neuroscience 19.7, p. 973 (cit. on p. 20).", "year": 2016}, {"authors": ["Richiardi", "Jonas", "Hamdi Eryilmaz", "Sophie Schwartz", "Patrik Vuilleumier", "Dimitri Van De Ville"], "title": "Brain decoding of fMRI connectivity graphs using decision tree ensembles", "venue": "In: 2010 IEEE International Symposium on Biomedical Imaging: From Nano to Macro. IEEE, pp. 1137\u2013 1140 (cit. on p. 32).", "year": 2010}, {"authors": ["Ritchie", "J Brendan", "David Michael Kaplan", "Colin Klein"], "title": "Decoding the brain: Neural representation and the limits of multivariate pattern analysis in cognitive neuroscience", "venue": "In: The British Journal for the Philosophy of Science (cit. on p. 32).", "year": 2017}, {"authors": ["Rodriguez", "Eugenio", "Nathalie George", "Jean-Philippe Lachaux", "Jacques Martinerie", "Bernard Renault", "Francisco J Varela"], "title": "Perception\u2019s shadow: long-distance synchronization of human brain activity", "venue": "In: Nature 397.6718, p. 430 (cit. on p. 66).", "year": 1999}, {"authors": ["Rolls", "Edmund T", "Gustavo Deco"], "title": "The noisy brain: stochastic dynamics as a principle of brain function", "venue": "Vol. 34. Oxford university press Oxford (cit. on p. 18).", "year": 2010}, {"authors": ["C Rorden"], "title": "MRICron [computer software", "venue": "(cit. on pp", "year": 2007}, {"authors": ["Rosenblatt", "Frank"], "title": "The perceptron, a perceiving and recognizing automaton Project Para", "venue": "Cornell Aeronautical Laboratory (cit. on p. 13).", "year": 1957}, {"authors": ["Rousselet", "Guillaume A", "Mich\u00e8le Fabre-Thorpe", "Simon J Thorpe"], "title": "Parallel processing in high-level categorization of natural images", "venue": "In: Nature neuroscience 5.7, p. 629 (cit. on p. 42).", "year": 2002}, {"authors": ["Rumelhart", "David E", "Geoffrey E Hinton", "Ronald J Williams"], "title": "Learning internal representations by error propagation", "venue": "Tech. rep. California Univ San Diego La Jolla Inst for Cognitive Science (cit. on p. 13).", "year": 1985}, {"authors": ["Russakovsky", "Olga", "Jia Deng", "Hao Su", "Jonathan Krause", "Sanjeev Satheesh", "Sean Ma", "Zhiheng Huang", "Andrej Karpathy", "Aditya Khosla", "Michael Bernstein", "Alexander C. Berg", "Li Fei-Fei"], "title": "ImageNet Large Scale Visual Recognition Challenge", "venue": "In: International Journal of Computer Vision (IJCV) 115.3, pp. 211\u2013252. doi: 10.1007/s11263-015-", "year": 2015}, {"authors": ["Russek", "Evan M", "Ida Momennejad", "Matthew M Botvinick", "Samuel J Gershman", "Nathaniel D Daw"], "title": "Predictive representations can link model-based reinforcement learning to model-free mechanisms", "venue": "In: PLoS computational biology 13.9, e1005768 (cit. on p. 17).", "year": 2017}, {"authors": ["Saltelli", "Andrea"], "title": "Sensitivity analysis for importance assessment", "venue": "In: Risk analysis 22.3, pp. 579\u2013590 (cit. on p. 38).", "year": 2002}, {"authors": ["Samek", "Wojciech", "Thomas Wiegand", "Klaus-Robert M\u00fcller"], "title": "Explainable artificial intelligence: Understanding, visualizing and interpreting deep learning models", "venue": "In: arXiv preprint arXiv:1708.08296 (cit. on p. 32).", "year": 2017}, {"authors": ["Sargolini", "Francesca", "Marianne Fyhn", "Torkel Hafting", "Bruce L McNaughton", "Menno P Witter", "May-Britt Moser", "Edvard I Moser"], "title": "Conjunctive representation of position, direction, and velocity in entorhinal cortex", "venue": "In: Science 312.5774, pp. 758\u2013762 (cit. on p. 17).", "year": 2006}, {"authors": ["Scellier", "Benjamin", "Yoshua Bengio"], "title": "Equilibrium propagation: Bridging the gap between energy-based models and backpropagation", "venue": "In: Frontiers in computational neuroscience 11, p. 24 (cit. on p. 22).", "year": 2017}, {"authors": ["Schmidhuber", "J\u00fcrgen"], "title": "Deep learning in neural networks: An overview", "venue": "In: Neural networks 61, pp. 85\u2013117 (cit. on p. 13).", "year": 2015}, {"authors": ["Schmitz", "Gregor PJ", "Chris Aldrich", "Francois S Gouws"], "title": "ANNDT: an algorithm for extraction of decision trees from artificial neural networks", "venue": "In: IEEE Transactions on Neural Networks 10.6, pp. 1392\u2013 1401 (cit. on p. 39).", "year": 1999}, {"authors": ["Schrauwen", "Benjamin", "David Verstraeten", "Jan Van Campenhout"], "title": "An overview of reservoir computing: theory, applications and implementations", "venue": "In: Proceedings of the 15th european symposium on artificial neural networks. p. 471-482 2007, pp. 471\u2013482 (cit. on p. 39).", "year": 2007}, {"authors": ["K Seeliger", "M Fritsche", "U G\u00fc\u00e7l\u00fc", "S Schoenmakers", "J-M Schoffelen", "SE Bosch", "MAJ van Gerven"], "title": "CNN-based Encoding and Decoding of Visual Object Recognition in Space and Time", "venue": "In: bioRxiv,", "year": 2017}, {"authors": ["Fellows", "John P Donoghue"], "title": "Brain-machine interface: Instant", "year": 2002}, {"authors": ["p. 20). Seung", "H Sebastian", "Haim Sompolinsky"], "title": "Simple models for read", "year": 1993}, {"authors": ["Adrian Bolton"], "title": "Mastering the game of go without human", "year": 2017}, {"authors": ["Sutton", "Richard S", "Andrew G Barto"], "title": "pavlovian reinforcement.", "venue": "(cit. on p", "year": 1998}, {"authors": ["Thorpe", "Simon", "Denis Fize", "Catherine Marlot"], "title": "Speed of processing in the human visual system", "venue": "In: nature 381.6582, p. 520 (cit. on p. 42).", "year": 1996}, {"authors": ["Tolman", "Edward C"], "title": "Cognitive maps in rats and men.", "venue": "Psychological review 55.4,", "year": 1948}, {"authors": ["Tsukimoto", "Hiroshi"], "title": "Extracting rules from trained neural networks", "venue": "In: IEEE Transactions on Neural networks 11.2, pp. 377\u2013389 (cit. on p. 39).", "year": 2000}, {"authors": ["Tulving", "Endel"], "title": "How many memory systems are there?", "venue": "American psychologist 40.4,", "year": 1985}, {"authors": ["Urbanczik", "Robert", "Walter Senn"], "title": "Learning by the dendritic prediction of somatic spiking", "venue": "In: Neuron 81.3, pp. 521\u2013528 (cit. on p. 22).", "year": 2014}, {"authors": ["Van Drongelen", "Wim"], "title": "Modeling neural activity", "venue": "In: ISRN Biomathematics 2013 (cit. on p. 18).", "year": 2013}, {"authors": ["Van Hasselt", "Hado", "Arthur Guez", "David Silver"], "title": "Deep reinforcement learning with double q-learning", "venue": "In: Thirtieth AAAI Conference on Artificial Intelligence (cit. on p. 18).", "year": 2016}, {"authors": ["Van Kerkoerle", "Timo", "Matthew W Self", "Bruno Dagnino", "Marie-Alice GarielMathis", "Jasper Poort", "Chris Van Der Togt", "Pieter R Roelfsema"], "title": "Alpha and gamma oscillations characterize feedback and feedforward processing in monkey visual cortex", "venue": "In: Proceedings of the National Academy of Sciences 111.40, pp. 14332\u201314341 (cit. on pp. 68, 87).", "year": 2014}, {"authors": ["Vanderplaats", "Garret N"], "title": "Numerical optimization techniques for engineering design", "venue": "Vanderplaats Research and Development, Incorporated (cit. on p. 27).", "year": 2001}, {"authors": ["VanRullen", "Rufin"], "title": "Perceptual cycles", "venue": "In: Trends in Cognitive Sciences 20.10, pp. 723\u2013735 (cit. on p. 43).", "year": 2016}, {"authors": ["Vapnik", "Vladimir"], "title": "The nature of statistical learning theory", "venue": "Springer science & business media (cit. on p. 24).", "year": 2013}, {"authors": ["Vapnik", "Vladimir N", "A Ya Chervonenkis"], "title": "On the uniform convergence of relative frequencies of events to their probabilities", "venue": "In:Measures of complexity. Springer, pp. 11\u201330 (cit. on p. 35).", "year": 2015}, {"authors": ["Varela", "Francisco", "Jean-Philippe Lachaux", "Eugenio Rodriguez", "Jacques Martinerie"], "title": "The brainweb: phase synchronization and large-scale integration", "venue": "In: Nature reviews neuroscience 2.4, p. 229 (cit. on p. 66).", "year": 2001}, {"authors": ["Vaswani", "Ashish", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "\u0141ukasz Kaiser", "Illia Polosukhin"], "title": "Attention is all you need", "venue": "In: Advances in neural information processing systems, pp. 5998\u20136008 (cit. on p. 17). 131", "year": 2017}, {"authors": ["Vellido", "Alfredo", "Jose David Martin-Guerrero", "Paulo JG Lisboa"], "title": "Making machine learning models interpretable.", "venue": "In: ESANN", "year": 2012}, {"authors": ["Vidal", "Juan R", "Maximilien Chaumon", "J Kevin O\u2019Regan", "Catherine Tallon-Baudry"], "title": "Visual grouping and the focusing of attention induce gamma-band oscillations at different frequencies in human magnetoencephalogram signals", "venue": "Journal of Cognitive Neuroscience", "year": 2006}, {"authors": ["Vidal", "Juan R", "Tom\u00e0s Ossand\u00f2n", "Karim Jerbi", "Sarang S Dalal", "Lorella Minotti", "Philippe Ryvlin", "Philippe Kahane", "Jean-Philippe Lachaux"], "title": "Category-specific visual responses: an intracranial study comparing gamma, beta, alpha, and ERP response selectivity", "venue": "In: Frontiers in human neuroscience 4, p. 195 (cit. on pp. 43, 44, 64, 65).", "year": 2010}, {"authors": ["Vincent", "Pascal", "Hugo Larochelle", "Yoshua Bengio", "Pierre-Antoine Manzagol"], "title": "Extracting and composing robust features with denoising autoencoders", "venue": "In: Proceedings of the 25th international conference on Machine learning. ACM, pp. 1096\u20131103 (cit. on p. 33).", "year": 2008}, {"authors": ["Cai", "David Budden", "Tom Paine", "Caglar Gulcehre", "Ziyu Wang", "Tobias Pfaff", "Toby Pohlen", "Yuhuai Wu", "Dani Yogatama", "Julia Cohen", "Katrina McKinney", "Oliver Smith", "Tom Schaul", "Timothy Lillicrap", "Chris Apps", "Koray Kavukcuoglu", "Demis Hassabis", "David Silver"], "title": "AlphaStar: Mastering the Real-Time Strategy Game StarCraft II", "venue": "https://", "year": 2019}, {"authors": ["Vu", "Mai-Anh T", "T\u00fclay Adal\u0131", "Demba Ba", "Gy\u00f6rgy Buzs\u00e0ki", "David Carlson", "Katherine Heller", "Conor Liston", "Cynthia Rudin", "Vikaas S Sohal", "Alik S Widge"], "title": "A shared vision for machine learning in neuroscience", "venue": "Journal of Neuroscience", "year": 2018}, {"authors": ["Ward Jr", "Joe H"], "title": "Hierarchical grouping to optimize an objective function", "venue": "In: Journal of the American statistical association 58.301, pp. 236\u2013 244 (cit. on p. 33).", "year": 1963}, {"authors": ["Wayne", "Greg", "Chia-Chun Hung", "David Amos", "Mehdi Mirza", "Arun Ahuja", "Agnieszka Grabska-Barwinska", "Jack Rae", "Piotr Mirowski", "Joel Z Leibo", "Adam Santoro"], "title": "Unsupervised predictive memory in a goaldirected agent", "venue": "arXiv preprint arXiv:1803.10760 (cit. on p", "year": 2018}, {"authors": ["Weiss", "Yair", "Eero P Simoncelli", "Edward H Adelson"], "title": "Motion illusions as optimal percepts", "venue": "In: Nature neuroscience 5.6, p. 598 (cit. on p. 23). 132", "year": 2002}, {"authors": ["Welch", "Peter"], "title": "The use of fast Fourier transform for the estimation", "year": 1967}, {"authors": ["eller", "Theresa M Vaughan"], "title": "Brain\u2013computer interfaces for", "year": 2002}, {"authors": ["Eberhard E Fetz"], "title": "The Neurochip-2: an autonomous head-fixed", "year": 2011}, {"authors": ["Springer", "pp"], "title": "The visual association cortex", "venue": "(cit. on pp", "year": 1993}, {"authors": ["I. Kuzovkin", "J.R. Vidal", "M. Perrone-Bertlotti", "P. Kahane", "S. Rheims", "J. Aru", "J.-P. Lachaux", "R. Vicente"], "title": "Identifying task-relevant spectral signatures of perceptual categorization in the human cortex", "venue": "In review,", "year": 2020}, {"authors": ["A. Leontjeva", "I. Kuzovkin"], "title": "Combining static and dynamic features for multivariate sequence classification", "venue": "Proceedings of the 3rd IEEE International Conference on Data Science and Advanced Analytics (DSAA),", "year": 2016}], "sections": [{"text": "DISSERTATIONES INFORMATICAE UNIVERSITATIS TARTUENSIS 19\nar X\niv :2\n01 0.\n08 71\n5v 1\n[ q-\nbi o.\nN C\n] 1\n7 O\nct 2\n02 0\nDISSERTATIONES INFORMATICAE UNIVERSITATIS TARTUENSIS 19\nILYA KUZOVKIN\nUnderstanding Information Processing in Human Brain by Interpreting Machine\nLearning Models"}, {"heading": "A Data-Driven Approach to Computational Neuroscience", "text": "TARTU 2020\nInstitute of Computer Science, Faculty of Science and Technology, University of Tartu, Estonia.\nDissertation has been accepted for the commencement of the degree of Doctor of Philosophy (PhD) in informatics on June 18, 2020 by the Council of the Institute of Computer Science, University of Tartu.\nSupervisor\nProf. Dr. Raul Vicente Zafra Computational Neuroscience Lab University of Tarty, Estonia\nOpponents\nProf. Dr. Fabian Sinz IRG Neuronal Intelligence University of T\u00fcbingen, Germany\nDr. Tim C Kietzmann Donders Institute for Brain, Cognition and Behaviour Radboud University, Netherlands\nThe public defense will take place on September 22, 2020 at 14:15 in the University of Tartu Delta Centre, room 1021, Narva mnt 18, Tartu, Estonia.\nThe publication of this dissertation was financed by the Institute of Computer Science, University of Tartu.\nCopyright \u00a9 2020 by Ilya Kuzovkin\nISSN 1024-4212 ISBN 978-9949-03-398-0 (print) ISBN 978-9949-03-399-7 (PDF)\nUniversity of Tartu Press http://www.tyk.ee/\nTo the academic spirit of the city of Tartu\nAbstract The thesis explores the role machine learning methods play in creating intuitive computational models of neural processing. We take the perspective that, combined with interpretability techniques, machine learning could replace human modeler and shift the focus of human effort from creating the models to extracting the knowledge from the already-made models and articulating that knowledge into intuitive representations. Automatic model-building methods can process larger volumes of data and explore more computationally complex relationships than a human modeler could. This perspective makes the case in favor of the larger role that exploratory and data-driven approach to computational neuroscience could play while coexisting alongside the traditional hypothesis-driven approach. We provide an example of how an intuitive model can be extracted from machinelearned knowledge, explore major machine learning algorithms in the context of the knowledge representation they employ, and propose a taxonomy of machine learning algorithms based on the knowledge representation that is driving their decision-making process.\nWe exemplify the illustrated approach in the context of the knowledge representation taxonomy with three research projects that employ interpretability techniques on top of machine learning methods at three different levels of neural organization. In each case we demonstrate the applicability of the approach and present the neuroscientific knowledge it allowed us to extract. The first study (Chapter 3) explores feature importance analysis of a random forest decoder trained on intracerebral recordings from 100 human subjects to identify spectrotemporal signatures that characterize local neural activity during the task of visual categorization. The second study (Chapter 4) employs representation similarity analysis to compare the neural responses of the areas along the ventral stream with the activations of the layers of a deep convolutional neural network. The analysis allowed us to make conclusions and observations about the hierarchical organization of the human visual cortex and the similarities between the biological and an artificial system of vision. The third study (Chapter 5) proposes a method that allows test subjects to visually explore the state representation of their neural signal in real time. This is achieved by using a topology-preserving dimensionality reduction technique that allows to transform the neural data from the multidimensional representation used by the computer into a two-dimensional representation a human can grasp.\nTaken together, the approach, the taxonomy, and the examples, present a strong case for the applicability of machine learning methods in conjunction with interpretability techniques to automatic knowledge discovery in neuroscience. Seen from this perspective, machine learning models cease to be mere statistical black boxes and, by capturing the underlying dynamics of real life processes, reintroduce themselves as candidate models of reality.\n6\nContents\nIntroduction 9"}, {"heading": "1. Synergy between neuroscience and machine learning 12", "text": "1.1. Neuroscience-inspired machine learning . . . . . . . . . . . . . 12\n1.1.1. Historical influence of neuroscience . . . . . . . . . . . 13 1.1.2. Examples of modern machine learning techniques\ninspired by neuroscientific insights . . . . . . . . . . . . 14 1.2. The role of machine learning in neuroscience . . . . . . . . . . 18"}, {"heading": "2. Machine learning as automatic builder of computational models 23", "text": "2.1. Gaining an intuitive understanding of computation carried out by machine-learned models . . . . . . . . . . . . . . . . . . . 25 2.1.1. General mechanics of the machine learning approach . . 25 2.1.2. An example of intuitive understanding emerging from a\nmachine-built decision tree . . . . . . . . . . . . . . . . 28 2.1.3. Understanding the models built by different machine learning algorithms . . . . . . . . . . . . . . . . . . . . 30 2.1.4. Techniques to analyze machine learning models and ex-\ntract knowledge from representations . . . . . . . . . . 37 2.2. Interpretation of machine-learned models for neuroscientific in-\nquiry at different levels of organization . . . . . . . . . . . . . 40"}, {"heading": "3. Feature importances of random forest models inform on localized cortical activity 41", "text": "3.1. Spectral and temporal signatures of human brain activity . . . . . . . . . . . . . . . . . . . . . 42 3.2. Large-scale intracortical recordings during visual object recognition task . . . . . . . . . . . . . . . . . . 44 3.2.1. Patients and recordings . . . . . . . . . . . . . . . . . . 44 3.2.2. Processing of neural data . . . . . . . . . . . . . . . . . 45 3.3. Feature importances of a decoder are indicative of task-relevant brain activity . . . . . . . . . . . . . . . . . . . . . . . . . . . 47 3.3.1. Random Forest as a decoding model . . . . . . . . . . . 47\n7\n3.3.2. Feature importance for the analysis of task-relevant neural activity . . . . . . . . . . . . . . . . . . . . . . . . . 49\n3.3.3. Hierarchical clustering to reveal types of activity patterns 52 3.4. The role and diversity of time-frequency patterns of individual\nlocations and area networks in perceptual categorization . . . 52 3.4.1. Feature importance allows to separate out the neural\nsignals that are predictive of perceptual categorization from the mixture of stimulus-induced responses . . . . 53\n3.4.2. Polypredictive and monopredictive probes . . . . . . . . 57 3.4.3. Further decomposition of important activity reveals clus-\nters of distinct time-frequency patterns . . . . . . . . . 58 3.5. Significance of bottom-up approach to the\nanalysis of human intracerebral neural activity . . . . . . . . 63"}, {"heading": "4. Representational similarities between biological visual processing and artificial neural networks inform on structural organization of human visual cortex 67", "text": "4.1. The search for the model of human visual system . . . . . . . . . . . . . . . . . . . . . . . 67 4.2. Simultaneous recordings of human intracortical responses and of responses of an artificial neural network to the same visual stimuli . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 69 4.2.1. Processing of neural data . . . . . . . . . . . . . . . . . 69 4.2.2. Processing of DCNN data . . . . . . . . . . . . . . . . . 71 4.3. The mapping between the Brodmann areas and layers of a Deep Convolutional Neural Network . . . . . . . . . . . . . . 72 4.3.1. Mapping neural activity to the layers of DCNN . . . . 72 4.3.2. Quantifying properties of the mapping . . . . . . . . . 75 4.4. Alignment between the layers of the DCNN and layers of human visual cortex . . . . . . . . . . . . . . . . . . . . . . . . . 77 4.4.1. Activity in gamma band is aligned with the DCNN . . 77 4.4.2. Activity in other frequency bands . . . . . . . . . . . . 79 4.4.3. Alignment is dependent on having two types of layers in\nDCNN . . . . . . . . . . . . . . . . . . . . . . . . . . . 82 4.4.4. Visual complexity varies across areas and frequencies . 84 4.4.5. Gamma activity is more specific to convolutional layers 86\n4.5. Extending the methodology beyond the visual system . . . . . 86"}, {"heading": "5. State space visualization informs on representation of mental concepts in human brain 91", "text": "5.1. The search for distinguishable mental patterns . . . . . . . . . 91 5.2. BCI via topology-preserving visualization of the feature space 92\n5.2.1. Self-organizing map . . . . . . . . . . . . . . . . . . . . 94\n8\n5.2.2. Predictive online SOM . . . . . . . . . . . . . . . . . . 95 5.2.3. POSOM-based BCI training system . . . . . . . . . . . 95\n5.3. Experimental validation on brain-computer interface for control 97 5.3.1. Preprocessing of EEG data . . . . . . . . . . . . . . . . 99 5.4. Feedback based on mental state space visualization leads to higher decoding accuracy . . . . . . . . . . . . . . . . . . . . 99 5.5. The general quest of navigating human mental state space . . 101\nConclusion 103\nBibliography 105"}, {"heading": "A. Code and data 135", "text": ""}, {"heading": "B. Supplementary materials 136", "text": "B.1. Detailed visualizations of spectral signatures of visual process-\ning filtered and feature importance maps . . . . . . . . . . . . 136 B.2. Mappings of Brodmann areas to layers of DCNN per area, layer\nand subject . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136\nAcknowledgements 138\nList of abbreviations 139\nSisukokkuv\u00f5te (Summary in Estonian) 141\nCurriculum Vitae 143\nElulookirjeldus (Curriculum Vitae in Estonian) 144\nList of original publications 145\n9\nIntroduction\nIt has been a very long time since humans began to use their reasoning machinery \u2013 the brain, to reason, among other things, about that same reasoning machinery itself. Some claim that such self-referential understanding is impossible to attain in full, but others are still trying and call it Neuroscience. The approach we take is the very same we use to understand almost any other phenomenon \u2013 observe, collect data, infer knowledge from the data and formalize the knowledge into elegant descriptions of reality. In neuroscience we came to refer to this later component as modeling. Many aspects of the phenomenon in question were addressed and explained using this approach by neuroscientists over the years. Some aspects remain unexplained, some others even unaddressed.\nEntering the era of digital computing allowed us to observe and collect data at ever-growing rate. The amount of data gave rise to the need, and the increase in computational power provided the means, to develop automatic ways of inferring knowledge from data, and the field of Machine Learning was born. In its essence it is the very same process of knowledge discovery that we have been using for years: a phenomenon is observed, the data is collected, the knowledge is inferred and a formal model of that knowledge is created. The main difference being that now a large portion of this process is done automatically.\nNeuroscience is traditionally a hypothesis-driven discipline, a hypothesis has to be put forward first, before collecting and analyzing the data that will support or invalidate the hypothesis. Given the amount of work that is required to complete a study, the reason for the process being set up in this way has a solid ground. In a setting where collecting data and extracting the knowledge takes a long time, exploratory analysis would indeed have a low yield in terms of solid and actionable knowledge as exploratory analysis can often result in finding nothing of value. However, with the new ways of automatic knowledge discovery the time that is required to complete the process has decreased and the balance between hypothesis-driven and exploratory, data-driven, approach is starting to change. In this work we put forward the argument that machine learning algorithms can act as automatic builders of insightful computational models of neurological pro-\n10\ncesses. These methods can build models that rely on much larger arrays of data and explore much more complex relationships than a human modeler could. The tools that exist to estimate model\u2019s generalization ability can act as a test of model\u2019s elegance and applicability to the general case. The human effort can thus be shifted from manually inferring the knowledge from data to interpreting the models that were produced automatically and articulating their mechanisms into intuitive explanations of reality.\nIn Chapter 1 we explore the history of the symbiosis between the fields of neuroscience and machine learning, evidencing the fact that those areas of scientific discovery have a lot in common and discoveries in one often lead to progress in another. Chapter 2 explores more formally what would it take to create an intuitive description of a neurological process from a machine-learned model. We present the subfield called interpretable machine learning, that provides the tools for in-depth analysis of machine learning models. When applied to neural data, it makes those models to be a source of insights about the inner workings of the brain. We propose a taxonomy of machine learning algorithms that is based on the internal knowledge representation a model relies on to make its predictions. In the following chapters 3, 4 and 5 we provide examples of scientific studies that gained knowledge about human brain by interpreting machine learning models trained on neurological data. The studies present the applicability of this approach on three different levels of organization: Chapter 3 shows how the analysis of a decoder trained on human intracerebral recordings leads to a better understanding of category-specific patterns of activity in human visual cortex. Chapter 4 compares the structure of human visual system with the structure of an artificial system of vision by quantifying the similarities between knowledge representations these two systems use. The final chapter makes a step into even higher level of abstraction and employs topologypreserving dimensionality reduction technique in conjunction with real-time visualization to explore relative distances between human subject\u2019s mental states.\nWith this work we aim to demonstrate that machine learning provides a set of readily available tools to facilitate automatic knowledge discovery in neuroscience, make a step forward in our ways of creating computational models, and highlight the importance and unique areas of applicability of exploratory data-driven approach to neuroscientific inquiry.\n11\nChapter 1\nSynergy between neuroscience and machine learning\nBoth, neuroscience and artificial intelligence, share, as one of their goals, the purpose of uncovering and understanding the mechanisms of intelligence1. Neuroscience analyzes the existing examples of intelligent systems that animals and humans have, and tries to figure out how these systems work. Artificial intelligence approaches the task by searching through the space of possible solutions, implementing them one by one and using incremental improvements in performance as the guiding light. Sharing a common goal makes it inevitable that the paths of those two fields of scientific inquiry will cross."}, {"heading": "1.1 Neuroscience-inspired machine learning", "text": "Before exploring the ways machine learning can contribute to neuroscientific research, we first review the role neuroscience has played in establishing one of the most important machine learning methods of the present day. Since both fields contribute to the quest of solving intelligence, we find that it is important to explore the symbiosis between the fields, establish the benefit it had and highlight the importance of maintaining that symbiotic relationship going forward. This section provides the context for our work and helps to advocate in favor of interdisciplinary scientific inquiry, by which the results and methods of one field can greatly benefit the progress in another.\n1Here and throughout this work we adhere to using this loosely defined term to denote the collection of properties and behavior patterns that we attribute to systems that have analytic capabilities, can operate using abstract notions and carry out high level planning. The search for mechanisms of intelligence is congruent to the search for the precise definition of what the intelligence is, until that search is over, we need a term we can use, we use intelligence.\n12"}, {"heading": "1.1.1 Historical influence of neuroscience", "text": "The first contribution from the field of neuroscience to the field of logical calculus, and thus to the early stages of AI research, can be traced to McCulloch and Pitts (1943), where the authors describe the nervous system as \u201ca net of neurons, each having a soma and an axon. Their adjunctions, or synapses, are always between the axon of one neuron and the soma of another. At any instant a neuron has some threshold, which excitation must exceed to initiate an impulse\u201d. They then show that \u201cto each reaction of any neuron there is a corresponding assertion of a simple proposition\u201d, propose a mathematical model of an artificial neuron that is capable of the same behaviour as simplified biological neuron in the description, and postulate \u201cTheorem II: Every temporal propositional expression is realizable by a net of order zero.\u201d, allowing to draw parallels between mathematical logic and inner workings of human brain.\nGrowing attention towards the \u201cfeasibility of constructing a device possessing human-like functions as perception, recognition, concept formation, and the ability to generalize from experience\u201d (Rosenblatt, 1957) led to the first mechanism that was able to modify it\u2019s behavior by learning from examples \u2013 perceptron (Rosenblatt, 1958), a physical system built from artificial neurons that were able to adjust their weights (artificial simplistic analog of synaptic connections).\nAccording to Schmidhuber (2015) early works on animal visual cortex such as the ones by Hubel and Wiesel (1959) and Hubel and Wiesel (1962) inspired layered architectures in artificial neural networks that became known as multilayer perceptrons (Rosenblatt, 1961), which, paired with the power of backpropagation algorithm (Werbos, 1974; Rumelhart, G. E. Hinton, and Williams, 1985), are the backbone of modern deep learning (LeCun, Y. Bengio, and G. Hinton, 2015). The concept of the perceptive field from the same work has contributed to the notion and success of convolutional neural networks in computer vision (Fukushima, 1980; LeCun, Bottou, et al., 1998; Krizhevsky, Sutskever, and G. E. Hinton, 2012) by proposing a way of how visual information is being processed in animal brain.\nA second pillar of contemporary AI (Hassabis et al., 2017) is the field of reinforcement learning (RL). Dating back to the work on animals done by Pavlov (1903) that later became known as classical conditioning (Rescorla, Wagner, et al., 1972) the principles of reinforcement learning made their way into computer science and machine learning with the works of Sutton and Barto (1990) and Sutton, Barto, et al. (1998). Paired with deep learning, reinforcement learning was instrumental for achieving such results as computers learning to play computer games with no prior knowledge (Mnih, Kavukcuoglu, Silver, Rusu, et al., 2015; Vinyals et al., 2019; OpenAI, 2018),\n13\nwinning world champion at Go (Silver et al., 2017), and others. The historical lens that we have presented here allows us to appreciate the enormous impact neuroscience had on the development of the fields of machine learning and artificial intelligence."}, {"heading": "1.1.2 Examples of modern machine learning techniques inspired by neuroscientific insights", "text": "There exists a certain difference in opinion when it comes to the question of how brain-like the modern artificial learning systems are. In its most popular form the question is ill-posed and does not look into the matter deep enough to make that debate useful. We would like to attempt to rectify that by highlighting that it is important to keep the discussion separate for different levels of analysis (Marr and Poggio, 1976): the level of implementation, the level of algorithm and representation, and the most abstract \u2013 the computational level.\nOn the level of implementation (following Marr\u2019s tri-level taxonomy), while there is a superficial similarity between biological neural networks and modern machine learning architectures, the specifics of engineering detail differ a lot. At this lowest level of analysis we would side with the claim that apart from the superficial similarity between a biological neuron and an artificial neuron, the systems are fundamentally different. However, as we move to a higher level of abstraction at the level of algorithm and representation, the design principles, representations and strategies of information processing of biological systems sometimes start to resemble the architectural principles that the best artificial systems rely on. We will show several such examples later in this chapter. On the computational level, that reflects the goal and purpose of the computation, biological and artificial system are often identical: object and speech recognition, speech synthesis, decision-making based on observations, spatial orientation \u2013 these are some of the examples of computational goals that biological and artificial systems share.\nIn this section we will demonstrate several examples where from the similarity on the computational level (the goal of the computation) emerges the similarity on the level of algorithm and representation. In other words, when the goal of an artificial system coincides with the goal of the corresponding biological system, then the algorithmic mechanism of achieving that goal in an artificial system follows the mechanism we know to exist in its biological counterpart. These examples extend the discussion of the similarities between artificial and biological systems and demonstrate that there is more to this question than the simplistic comparison between neurons and units in an artificial neural network.\n14\nWorking memory. The mechanism of working memory is an important cognitive system that allows us to hold and use the information that is immediately relevant to the task at hand. It can contain context, recent occurrences, and bits of information that preceded the current moment. It also allows to hold pieces of information back while running another cognitive process and then recall the held-back information. This ability is crucial for reasoning and decision-making where the next logical step might depend on results of another, intermediate, process. The very similar challenge exists in artificial learning systems: an algorithm might need to remember some information to use it later, to match information across time and make decision based on temporally disjoint inputs. Recurrent Neural Networks (RNNs) (Hopfield, 1982) and later Long Short-Term Memory (LSTM) networks (Hochreiter and Schmidhuber, 1997) were proposed to address that challenge. LSTM network consists of extended artificial neurons, that have a memory cell to hold a certain values and a set of gates, that regulate under which conditions the content of the memory cell can be modified or released back into the network. Since we do not know how biological working memory is working, we cannot claim the similarity on algorithmic level, but the similarity on computational level is clearly present.\nAssociative memory. It has been conjectured that there are multiple memory types in a human brain (Tulving, 1985). Other types of biological memory gave rise to various ideas in machine learning and reinforcement learning. Associative memory, characterized by the ability to recall a certain piece of information by triggering a certain stimulus, found its reflection in an artificial memory model called Hopfield network (Hopfield, 1982) \u2013 a neural network that can store different patterns and a given partial pattern return the whole. According to Hassabis et al. (2017), experience replay, a critical component of Deep Q-Network (DQN) (Mnih, Kavukcuoglu, Silver, Graves, et al., 2013), was \u201cdirectly inspired by theories that seek to understand how the multiple memory systems in the mammalian brain might interact\u201d and draw the parallel between the role of hippocampus and experience replay buffer: \u201cthe replay buffer in DQN might thus be thought of as a very primitive hippocampus, permitting complementary learning in silico much as is proposed for biological brains\u201d. Persistent, long-term memory is also a crucial part of a biological intelligent system, and, although the biological mechanisms of it did not yet find direct reflection in artificial intelligence systems, the conceptual necessity for this type of memory is widely acknowledged and was implemented in Neural Turing Machines (Graves, Wayne, and Danihelka, 2014) and later in an architecture called Differentiable Neural Computer (Graves, Wayne, Reynolds, et al., 2016).\n15\nPredictive coding. The theory of predictive coding (Rao and Ballard, 1999) proposes that the brain learns a statistical model of the sensory input and uses that model to predict neural responses to sensory stimuli. Only in the case when the prediction does not match the actual response the brain propagates the mismatch to the next level of the processing hierarchy. By building and memorizing an internal model of the sensory input such mechanism would reduce the redundancy of fully processing each sensory input anew at all levels and thus greatly reduce the processing load on the sensory system. A recently proposed AI agent architecture called MERLIN (Wayne et al., 2018) achieves a marked improvement on the tasks \u201cinvolving long delays between relevant stimuli and later decisions: <...> navigation back to previously visited goals, rapid reward valuation, where an agent must understand the value of different objects after few exposures, and latent learning, where an agent acquires unexpressed knowledge of the environment before being probed with a specific task\u201d by introducing the similar principle into the architecture of the system. The authors point out that using reinforcement learning to learn the entire system at once, including the representations of the input, recurrent computation, rules for accessing the memory, and the action-making policy is indirect and inefficient. They propose to decouple the learning of the sensory data from learning the behavior policy that drives the decision-making by creating a subsystem that learns to compress sensory observations into efficient representation in an unsupervised manner. The decision-making policy is a recipient of already encoded information and thus does not have to learn the encoding through trial and error. The authors acknowledge the theory of predictive coding as one of the inspirations for the architecture.\nSuccessor representations. The trade-off between model-based and model-free methods is a long-standing question in the field of RL. As the name suggests, the agents in the model-based methods have to learn (or have access to) the model of the environment, while model-free agents try to map observations directly onto actions or value estimates. While having a model would allow the agent to use it to plan ahead and be more sample efficient during learning, it also poses significant challenges as learning a model of the environment, especially if the environment is complex, is a very hard task. Many successful results were achieved with model-free methods as those are easier to implement and learning the mapping between the observations and the actions is in most cases sufficient and is easier than properly learning the model of the environment. The idea of successor representations (Dayan, 1993) lies in-between those two approaches. During the learning the agent counts how often the transition between a state sa and state sb has occurred. After interacting with the environment for some time the agent forms what is called an occupancy matrix M , which holds\n16\nempirical evidence of transitioning between the states. This matrix is much easier to obtain than a full model of the environment and at the same time it provides some of the benefits of the model-based approach by allowing to model which transition is likely to occur next. The hypothesis that the brain is using successor representations proposes that brain stores in some form the occupancy probabilities of future states and is supported by behavioral (Tolman, 1948; Russek et al., 2017; Momennejad et al., 2017) and neural evidence (Alvernhe, Save, and Poucet, 2011; Stachenfeld, Botvinick, and Gershman, 2017). Using this statistics the brain can estimate which states are likely to occur next, serving as a computationally efficient approximation of a full-fledged environment model. The revival of the original concept in the context of RL (Momennejad et al., 2017) proposes a way to introduce some of the benefits of model-based methods without sacrificing the efficiency and ease of implementation of model-free methods.\nGrid cells. In 2014 the Nobel Prize in Physiology or Medicine was awarded for the discovery of cells that constitute the positioning system in the brain (O\u2019Keefe, 1976; Sargolini et al., 2006). In the recent work by Banino et al. (2018) it was demonstrated that an artificial agent trained with reinforcement learning to navigate a maze starts to form periodic space representation similar to that provided by grid cells. This representation \u201cprovided an effective basis for an agent to locate goals in challenging, unfamiliar, and changeable environments\u201d.\nAttention. After providing the initial motivation for convolutional neural networks (CNN) via the ideas of hierarchical organization and the concept of a receptive field, neuroscience served a source of ideas for further improvement though the concept of attention (Desimone and Duncan, 1995; Posner and Petersen, 1990; Olshausen, C. H. Anderson, and Van Essen, 1993). Adding the similar functionality to CNNs (Mnih, Heess, Graves, et al., 2014; J. Ba, Mnih, and Kavukcuoglu, 2014) helped to further improve the performance of visual object recognition. The same concept was found to be useful in artificial neural networks designed for natural language processing tasks (Bahdanau, Cho, and Y. Bengio, 2014; Vaswani et al., 2017) and as a component of the memory module of differentiable neural computers (Graves, Wayne, Reynolds, et al., 2016).\nMemory consolidation. The standard model of systems memory consolidation (Squire and Alvarez, 1995) suggests that a novel memory is first retained in hippocampus, and then, with each new recollection of that memory, its engram is strengthened in neocortex, making the memory permanent (Dudai, 2004). On one hand this mechanism ensures that the important memories, that are being recalled often, become permanent, but\n17\nit also keeps neocortex free of clutter and thus makes it more stable. If every new memory would be immediately consolidated we would remember too much \u201cnoise\u201d. A similar principle is used in Double DQN architecture (Van Hasselt, Guez, and Silver, 2016) to make deep reinforcement learning process more stable: two networks instead of one are maintained at the same time, the online network is used to pick actions and its weights are updated immediately, while the second, target network, is used to evaluate the selected actions and is updated periodically. Periodic update of the network, in contrast to updating the network immediately, provides more stable evaluation of actions \u2013 within the period between the updates the actions are evaluated by the same network, allowing those evaluations to have a common reference point and thus serving as a better relative measure of the quality of an action.\nThe examples we discussed in this section demonstrate that on algorithmic level, the biological and artificial systems sometimes share curious similarities. This observation holds a very promising message in the context of our work: since the systems share some of the properties, it can be informative to analyze one in order to gain knowledge about the other. In our case \u2013 to analyze artificial learning systems, explore their mechanisms and hypothesize the similarities between those mechanisms and the cognitive processes of biological systems."}, {"heading": "1.2 The role of machine learning in neuroscience", "text": "Approximately 20 years after Hodgkin and Huxley (1952) published their fundamental single neuron model that inspired multiple works in mathematical modeling of neural dynamics, the field has accumulated enough methodology and data to start looking into the models of neuronal populations (Wilson and Cowan, 1972; Wilson and Cowan, 1973; Nunez, 1974). Due to the volume of that data and the complexity of the systems being modeled, the community turned to statistical methods to provide approximations of aggregate behavior (Lopes da Silva et al., 1974; Buice and Cowan, 2009; Pillow, 2005; Rolls and Deco, 2010). See Van Drongelen (2013) for more examples. The adoption of statistical modeling, which is a precursor of modern machine learning, has established a link between neuroscience and statistical learning. The advancement of computational power and the growing amount of digital data fueled the development of data processing tools, pattern recognition algorithms and data analysis methods. These tools found multiple applications in various fields, including, of course, the field of neuroscience (Vu et al., 2018; Hassabis et al., 2017; Paninski and J. Cunningham, 2017; G. E. Hinton, 2011; Glaser et al., 2019).\n18\nAccording to Semantic Scholar2, the percentage of neuroscience papers that mention machine learning has risen from 1.3% to 7.6% over the last two decades. In this section we give an overview of the major roles machine learning methods play in neuroscientific inquiry. We then suggest that there is a methodological component that is readily available, would benefit the study of neural systems, and would extend the role of machine learning in neuroscience even further, but, in our observation, this methodology is lacking mass adoption.\nNeural decoding represents the most direct application of machine learning methods to neurological data. A dataset of neural responses to predetermined stimuli is collected, and a machine learning method is tasked with building a model that can reverse the mapping \u2013 given a neural signal it has to learn to identify which stimulus caused that neural response. It does so by inferring a set of rules or statistical associations that map neural responses to the corresponding stimuli in the dataset. One of the earliest applications of data analysis to characterize stimulus specific cortical activity can be traced back to Mountcastle et al. (1969) and displays a case of manual data analysis. With the rise of machine learning techniques the process of looking for patterns in vast arrays of data became automated, and now it is safe to say that machine learning is the default approach to neural decoding. While the studies that employ the approach are too numerous to list here, we would like to mention a few. The algorithm proposed by Bialek et al. (1991) is one of the first direct attempts to read the neural code to identify movement detection in blowfly\u2019s visual system. Already in the work by Seung and Sompolinsky (1993) statistical modeling based on maximum likelihood estimation was applied to a decode direction from the activity of sensory neurons. Zhang et al. (1998) had successfully applied the decoding methods to identify animal\u2019s location based on the activity of the place cells. Haxby, Gobbini, et al. (2001) have demonstrated that it is possible to decode fMRI recordings of responses to 8 different visual categories with average accuracy of 96%. Decoding of prefrontal activity of rats learning an alternating goal task allowed to predict rat\u2019s decision, effectively reading rat\u2019s future intention directly from brain activity (Baeg et al., 2003). In the works of Nishimoto et al. (2011) and Shen et al. (2019) it was demonstrated that it is possible to train a decoder that can, albeit with a limited quality, reconstruct visual information such as images or movies directly from fMRI recordings from occipitotemporal visual cortex of human subjects who watched natural movies or images. In their extensive fMRI study, Huth et al. (2012) mapped 1705 object and action categories to the changes they evoke in human test subjects watching natural movies, allowing them\n2https://www.semanticscholar.org\n19\nto map the semantic space of those categories onto cortical maps of human brain. Applying decoding toolbox to the responses of cells to facial stimuli allowed L. Chang and Tsao (2017) to identify the code for facial identity in the primate brain. The uncovered code allowed the authors to both predict the neural responses that a particular facial stimulus will elicit and also to decode facial identity from the neural activity. Using recurrent neural networks to decode articulatory movements from cortical activity allowed Anumanchipalli, Chartier, and E. F. Chang (2019) to decode the intended utterances and synthesize audible speech.\nNeural decoding of the activity of the motor cortex into the intended movement of a subject has branched into its own field, called brain-computer interfaces (Wolpaw et al., 2002). Fetz (1969) first demonstrated that a monkey can learn to operate a robotic hand that was controlled by the activity of single cells in the motor cortex, effectively learning to operate an artificial limb. With more advanced multi-site neural ensemble recoding capabilities Wessberg et al. (2000) were able to make accurate real-time predictions of the trajectories of arm movements of a non-human primate and successfully use those predictions for the control of a robotic arm. Similar work by Serruya et al. (2002) demonstrated even wider applicability of the method by showing that the same approach allows a monkey to move a computer cursor to any location on the computer screen. Finally in the work by Hochberg et al. (2006) the technology was successfully applied to a human subject, allowing to operate a robotic limb with nothing else other than the mental intention to do so.\nPerformance of a decoding model can be used as a way to quantify the lower bound on the amount of information or selectivity of a certain brain region (C. P. Hung et al., 2005; Raposo, Kaufman, and Churchland, 2014; Rich and Wallis, 2016). Providing a learning algorithm with the data from the region of interest and tasking it with decoding forces the algorithm to uncover the information that is pertinent to the process of decoding. The level of performance of the final model informs the investigator on the existence and the quality of relevant information in that region.\nDifference in performance of the decoders trained under different experimental conditions provides a way to quantify that difference and allow for quantitive comparison. For example, Hern\u00e0ndez et al. (2010) recorded the neuronal activity of diverse cortical areas while monkeys performed a certain task. The level of performance of the decoding models trained on the activity from different cortical areas was used as an indicator of the involvement of each particular area in that particular task. Similar approach is used by Meer et al. (2010) to analyze the contribution of hippocampus, ventral striatum, and dorsal striatum into the information processing during a spatial decision task. By comparing the results of decoding the activity of posterior parietal cortex (PPC) under two different tasks, R. Quiroga\n20\net al. (2006) were able to establish that activity in PPC predicts the location of targets significantly worse than it predicts the intended movements, providing insight into the functional role of that area.\nIn section 1.1 we have described multiple models that the field of artificial intelligence has produced in attempts to solve various perceptual and behavioral tasks. Most of the problems we challenge the artificial intelligence systems with are the ones that we, humans, already are capable of solving. This fact naturally leads to the idea that it could be interesting to compare the biological mechanisms of solving these problems with the mechanisms that are employed and learned by artificial systems. The modeling branch of computational neurosciences approaches this question by proposing models of biological systems and comparing the behavior of the proposed models with biological data. We find, and this is one of the main arguments we would like to put forward in this thesis (see Chapter 2), that the rise of the fields of artificial intelligence and machine learning awarded us with an alternative way to investigate that question. For example, to quantify the similarity between the hierarchies of a convolutional neural network (CNN) and human ventral stream system (Daniel L Yamins et al., 2013) employed representational similarity analysis (RSA) (Kriegeskorte, Mur, and P. A. Bandettini, 2008)) to find that the representations of that are formed in a CNN were similar to the representations in the ventral stream. Similar and more detailed findings were reported by Cadieu et al. (2014), Daniel LK Yamins, Hong, et al. (2014), G\u00fc\u00e7l\u00fc and Gerven (2015), Seeliger et al. (2017), and Kuzovkin et al. (2018) confirming the evidence in favor of the similarities in hierarchical organization of both biological and artificial systems of vision. Khaligh-Razavi and Kriegeskorte (2014) compared representational dissimilarity matrices (RDM) of 37 computational models of vision reaching the same conclusion, that deep convolutional neural networks explain activations of inferior temporal cortex during visual object recognition task. Similar to visual perception there are comparisons between the hierarchical structure of human auditory cortex and hierarchy of artificial neural networks trained to process auditory data (Kell et al., 2018; N. Huang, Slaney, and Elhilali, 2018).\nA new potential role of machine learning in neuroscience was alluded to in the works on Neurochip (Jackson, Mavoori, and Fetz, 2006; Zanos, Richardson, et al., 2011; Nishimura et al., 2013; Zanos, Rembado, et al., 2018). Being an example of a bidirectional brain-computer interface, Neurochip both reads the inputs from biological neurons, and, after running onchip computations on those inputs, stimulates the cortex with its output connections. Seeing the similarities between some computational mechanisms of biological and artificial systems we are very curious to see the development of that idea and creation of a computational system that is a hybrid of biological and artificial circuits.\n21\nBiologically plausible deep learning is a direction of research that develops artificial learning architectures under the restrictions the biological systems are prone to. G. Hinton (2007) outlined a list of reasons why the processes employed by modern deep learning methods cannot be running in the brain. The question was further explored and reviewed by Y. Bengio et al. (2015). This sparked multiple works (Urbanczik and Senn, 2014; T. P. Lillicrap et al., 2016; Liao, Leibo, and Poggio, 2016; Scellier and Y. Bengio, 2017) where those limitations were addressed to demonstrate that it is still possible to achieve learning in an artificial system while respecting some of the biological constraints. This line of research creates yet another way for machine learning to play a role in creating plausible computational models of neuronal processing thus advancing our understanding of the brain.\nThis overview of the major paths of how machine learning benefits the advancement of neuroscience highlights the fact that, for various reasons, numerous machine learning models are being trained on neurological data. While all of those models serve their purpose in the above-mentioned studies, many of them are being treated as \u201cblack box\u201d tools, where the input is provided and the output is tested and accepted to be used for further analysis, interpretation and confirmation of the experimental findings. In the next chapter we will argue that some of the models that were created in the above-mentioned and other scientific studies have inadvertently captured some of the key computational mechanisms of the phenomena the models were being trained on. The analysis of how exactly these models achieve their results and reach their predictions could lead to unearthing those captured computational mechanisms. We find, that while many research groups are working in this direction, more rigorous and widespread adoption of the tools that facilitate interpretation of machine learning models would require little effort but could lead to new and unexpected byproducts of the main investigation.\n22\nChapter 2\nMachine learning as automatic builder of computational models\n\u201cAll models are wrong, but some are useful.\u201d \u2013 George E. P. Box\nBuilding a model of a complex phenomenon is an ancient way for humans to gain knowledge and understanding of that phenomenon. Models of planetary motion (Kepler, 1621), gravity (Newton, 1687; Einstein, 1915), standard model of particle physics (Wilczek et al., 1975) are prominent examples of this approach. By comparing the predictions made by a model to observations in the real world, we theorize that the mechanism driving the model could be the same as the one driving the phenomenon. By building more and more accurate models, we approach the true mechanism closer and closer, hoping to get to the point of being able to artificially replicate the phenomenon in full.\nThis line of scientific inquiry is being widely applied to brain studies as well. The method of mathematical modeling spans across the whole field of computational neuroscience and includes single neuron models (Lapicque, 1907; Hodgkin and Huxley, 1952; Koch, 2004; Herz et al., 2006), network models (White et al., 1986; Hagmann et al., 2008; E. Bullmore and Sporns, 2009; Sporns, 2010; Bassett and Sporns, 2017; Bassett, Zurn, and Gold, 2018), models of memory (Durstewitz, Seamans, and Sejnowski, 2000; Frank, Loughry, and O\u2019Reilly, 2001; Chaudhuri and Fiete, 2016), cognition (Smith and Ratcliff, 2004; Oaksford and Chater, 2009; Tenenbaum et al., 2011; Palmeri, Love, and Turner, 2017; Kriegeskorte and Douglas, 2018) and learning (Hebb, 1949; Raisman, 1969; Zilles, 1992; Fuchs and Fl\u00fcgge, 2014), sensory precessing (Barlow, 1959; Barlow, Blakemore, and Pettigrew, 1967; Ernst and Banks, 2002; Weiss, Simoncelli, and Adelson, 2002;\n23\nOlshausen and D. J. Field, 2004; K\u00f6rding and Wolpert, 2004; Kriegeskorte and Douglas, 2018) and other neural phenomena. Both computational and structural modeling lead to numerous discoveries and important contributions to our understanding of the nervous system.\nThe most prized property of a model is our ability to understand its mechanism and thus understand the phenomenon that is being modeled. Coming up with a theory of how a particular phenomenon works and proposing a model that describes it has always required careful and extensive observation, good intuitive understanding of the process and almost artful ability to consolidate the intuition with the observation into a formal description that generalizes well across all instances of the phenomenon. A good sign of a model being successful is its ability to make predictions about future observations and results of interactions, making predictability the first litmus test of any model or theory. Models and theories that do not pass that test are usually discarded from the pool of scientific knowledge.\nA typical machine learning pipeline involves such consecutive steps as data acquisition, data preprocessing, training a model that employs statistical tools to describe the data, and testing of the resulting model on a hold-out subset of the data (Murphy, 2012). This latter step is of particular interest to us in the context of the argument we put forward in this chapter. Statistical learning theory (V. Vapnik, 2013) addresses the problem of selecting such a model that minimizes its error on the data, while keeping bias and variance of the model as low as possible. Further set of techniques, such as training/test split, cross-validation and others are then applied to estimate model\u2019s performance and its generalization ability. All this theoretical machinery serves one purpose \u2013 the resulting model should accurately describe the data at hand and make correct predictions on previously unseen data samples. A model that does not sufficiently satisfy this requirement is discarded the same way as non-predictive models and theories we discussed in the previous paragraph.\nThe consequence of the machine learning approach being set up in this way is that all of the successful models that were ever built on neural data, including the ones we have discussed in Section 1.2, do, by design, satisfy the primary requirement of a good model and pass the litmus test of generalizability. In this section we put forward the argument that in addition to solving the primary issue those models were created to address (being it neural decoding, comparison of experimental conditions, quantification of information, or else), they also are models (or reflect the dynamics) of the computational mechanisms that gave rise to that neural data. Our predecessors had to analyze such data manually and use their insight to come up with a good model they understood in great detail. In the era of big data and high performance computing we are facing the opposite \u2013 the analysis of the data and building of a model that satisfies that data is done auto-\n24\nmatically, but what we sacrifice is the understanding of the resulting model. Thankfully, modern machine learning toolbox does include various methods to achieve model interpretability, which, combined with the abundance of data and computing power leaves us with the best of two worlds \u2013 we can build models on a grand scale and at a fast pace and interpret those models to read out the formalisms they develop, informing us on the underlying neural mechanisms."}, {"heading": "2.1 Gaining an intuitive understanding of computation carried out by machine-learned models", "text": "The definition of a mathematical model is a broad one and includes statistical models, differential equations, computational models, dynamical systems and more. The precise nature of a model produced by a machine learning approach depends on the particular machine learning algorithm that generated the model. In this section we will describe general mechanics of machine learning process, provide an example based on the decision tree algorithm that demonstrates how a computational model is born from local statistical decisions, and describe major families of machine learning methods to understand what kind of model is being created by each of those families when applied to a set of data.\nTo illustrate the necessity and motivation for the following material let us introduce a hypothetical situation. Let us assume that during a study a group of researchers have obtained vast volumes of data, preprocessed it and successfully trained a machine learning model that accurately differentiates between the experimental conditions and generalizes well to previously unseen data. Now we are in a peculiar situation, where the group of researchers, given the same data, will not be able to decode it, but in their hands they have a model, which does \u201cknow\u201d how to do it. The content of this section explores the feasibility of transferring that knowledge from within the model and into the researchers."}, {"heading": "2.1.1 General mechanics of the machine learning approach", "text": "The process starts with a dataset of observations, where each particular observation is called a sample and is described by a set of values called features. A sample can also have a label associated with it, that, depending on type of the learning problem, can represent the category of the sample (supervised learning, classification problem), numerical outcome (supervised learning, regression problem), reward from the environment (reinforcement learning), or not be present at all (unsupervised learning). An example of a neural dataset could be a set of observations in the frequency domain, where\n25\nthe features are particular frequency bands, a sample is described by the powers of those frequencies and has a label indicating whether test subject\u2019s eyes were open or closed when that sample was recorded. A straightforward application of machine learning on such data would be to train a decoder (a model), that can identify whether test subject\u2019s eyes are open or closed based on the neurological data alone.\nOnce the initial set of observations is collected, the next steps are feature engineering and feature selection. During feature engineering one has to come up with the best way to represent the data from the perspective of a machine learning algorithm. In the example above we took powers of the frequency bands as our features, but that was not the only choice available and we did it only because we know that the information about whether the test subject\u2019s eyes are closed or open is readily available in the alpha frequencies. We made the decision to represent our data in this particular form because we know that this representation will make it easy for the learning algorithm to identify the pattern that separates closed eyes recordings from open eyes recordings. Feature engineering is often a creative process, that requires both domain knowledge and understanding of the machine learning method that will be subsequently applied. One of the reasons for the popularity of deep learning methods is the ability of deep artificial neural networks to automate feature engineering and learn good features directly from data. This methodology has revolutionized the fields of computer vision (Krizhevsky, Sutskever, and G. E. Hinton, 2012) and speech recognition (G. Hinton et al., 2012), and proved to be applicable in other areas as well (LeCun, Y. Bengio, and G. Hinton, 2015).\nThe subsequent (or alternative) step of feature selection is a related, but conceptually different process, where we seek to identify the most representative features and remove the rest to make the learning problem easier. This can be done manually by employing human domain knowledge, or with the help of statistical techniques (Blum and Langley, 1997; Hall, 1999).\nThe next, central, step is running a machine learning algorithm on preprocessed data. The choice of the algorithm will depend on the type of the learning problem (supervised, unsupervised or reinforcement, classification or regression) and on the types of the features we use to describe the data (numerical or categorial, continuous or discrete, etc). The exact learning mechanism can be quite different depending on the chosen algorithm, but the underlying framework of mathematical optimization (Snyman, 2005) is common to all of them. Every machine learning algorithm has two essential components: an objective function (also called the loss function) that the algorithm has to optimize and a set of parameters that it can change in order to optimize the objective. Depending on the algorithm, the parameters can be numerical weight coefficients (examples: linear and logistic regression as presented in Murphy (2012); neural networks), categorical variables\n26\nand numerical thresholds (decision trees by Breiman et al. (1984); random forest by Leo Breiman (2001)), points in the feature space (K-means clustering by Hartigan and Wong (1979); support vector machines by Cortes and V. Vapnik (1995); linear discriminant analysis by Fisher (1936)) or have one of the multiple other possible representations. The final configuration of parameters in conjunction with the computational process that the algorithm runs is the final model that the algorithm will output. Changing the parameters affects the value of the objective function, so all the algorithm has to do is to find the parameters that work best. To give an example of how this can be achieved we consider the case when the objective function is differentiable and the parameters are continuous, which is the case for such algorithms as artificial neural networks, linear and logistic regression and many others. In such a case gradient-based optimization methods can be applied to iteratively approach better and better model parameters. Each configuration of parameters is a point in the parameter space where the objective function is defined. Since the function is differentiable we can compute the gradient (derivative) of the function at every possible configuration point. That gradient is a vector in the parameter space, that tells us which way we should move the point in order to increase the value of the objective function. Depending on whether we want to maximize or minimize the objective function we respectively move in the direction of the gradient or in the direction opposite to it. This optimization technique is called gradient descent (or gradient ascent). For a more detailed and formal description of this and other optimization methods see Vanderplaats (2001) and Snyman (2005). Once the optimization process has approached the global or a local optimum within a predefined tolerance threshold, or is unable to improve the result any further, the learning algorithm stops and outputs the configuration of parameters that has achieved the best result so far.\nThe final step of the process is the evaluation of model\u2019s performance and generalization ability. When a human is designing a model, he or she takes particular care to make their model general, so that it would not only describe the data at hand, but also work correctly on future data samples. A machine learning algorithm has no such natural inclination and, if it has sufficient expressive power, tends to memorize the whole dataset, as such representation will be, in most cases, the most accurate one from the optimization perspective. This phenomenon is called overfitting and has to be avoided if we want the resulting model to capture the underlying dynamics or patterns in the data. The ability of a model to do so is called generalization ability and is as important as accuracy of the representation of the training data. A common approach to estimate generalization ability of a model is to reserve a portion of the data, a test set, run the learning procedure on the remaining training set, and use the performance of the\n27\nfinal model on the test set as the estimate of generalization ability. In most cases the very first algorithm we try will not be successful at finding a good model and we will try many different ones before the one that works is found. In the process of doing so we can overfit to the test set as well. To avoid that the training set is further split in two parts: a smaller training set and a validation set. The complete training procedure looks as follows: learning algorithms are trained only on the smaller version of the training set, then their performance is estimated on the validation set and, if desired, the process is repeated until a good model is found. And only then the test set is used once to gauge model\u2019s true performance. There are variants to this procedure such as cross-validation, leave-one-out and a few others, all of which were developed to ensure that the model that was built by an artificial learning system is able to generalize and make accurate predictions on previously unseen data. This process is set in place to emulate human modeler\u2019s natural strive towards general and elegant models.\nThe process we have outlined above is being applied across multiple branches of neuroscientific research. Often, in the context of a particular scientific study that employs machine learning approach, the question of how the resulting model achieves its result is not in the spotlight, because the focus is on the result itself. However, behind each successful model, lies, encoded in the values of those parameters, the computational principle that allowed the model to succeed. Often trivial, sometimes revelational \u2013 we will only know once we have interpreted the parameters and unearthed the principle."}, {"heading": "2.1.2 An example of intuitive understanding emerging from a machine-built decision tree", "text": "The decisions a machine learning algorithm makes during the process of fitting the model to the data are driven by local statistical, informationtheoretic, probabilistic or combinatorial rules. The question of whether a combination of such decisions can amount to a comprehensive mathematical model is a valid one to ask. In this section we argue in favor of the positive answer to that question and illustrate our reasoning using one particular learning algorithm \u2013 a decision tree (Breiman et al., 1984).\nConsider the task of decoding a neural recording to determine whether a test subject\u2019s eyes are open or closed, that we introduced above. Assume that the data for that task was recorded using an EEG device, the raw signal was cleaned and transformed to frequency domain, and the power spectral densities of 30 frequencies (1 Hz to 30 Hz) constitute the feature space. Building of a decision tree using ID3 (Quinlan, 1986) algorithm would proceed as follows:\n28\n(a) Given the dataset S, for each feature f compute, using entropy H, the information gain IG(S, f) = H(S)\u2212H(S|f). That number shows the amount of additional information that will be obtained if the dataset S is split into two disjoint subsets Sright and Sleft using the value vf of the feature f as the splitting criterion. Maximal information gain will be achieved by splitting at optimal (in terms of information gain) value v\u2217. The data samples that have vf \u2265 v\u2217 are assigned to Sright and the rest to Sleft. (b) If all of the samples in Sright belong to the same class (eyes closed, for example) the branching process stops and this subset becomes a leaf that contains samples of the \u201ceyes closed\u201d category. The same is done with Sleft. (c) If a subset contains samples from both classes, the algorithm goes recursively into this subset and repeats the procedure starting from step (a).\nAssume that we have completed the training process, tested the resulting model on a test set and found that the model is very accurate and can reliably identify if test subject\u2019s eyes are open or closed. If the purpose of our study was to prove that such decoding is possible, or it was an engineering project for clinical purposes (for example to automatically detect whether a patient is asleep), then we have successfully achieved the goal of our study. Many real-world studies do stop at this stage.\nWe would like to note, that at this point we do have a model that works, but we do not know why or how it works. An additional step of interpreting the model should be taken in order to answer those questions. In the case of a decision tree the analysis is very simple \u2013 we can visualize the tree that is the final model. Figure 1 illustrates a made-up example of how such a tree might look like. This analysis will reveal to us, that the model has 8 parameters \u2013 the four features that are put in the branching points and the four threshold values of these features for making the branching decisions. Over the whole set of frequencies from 1 Hz to 30 Hz the model deemed important only the 11 Hz, 10 Hz, 9 Hz and 12 Hz. This informs us that these are the frequencies which are indicative of the \u201ceyes closed\u201d experimental condition. Furthermore we learn that the power spectral density values those frequencies need to reach in order to indicate the \u201ceyes closed\u201d condition are, respectively, 8.3, 7.7, 6.5 and 7.2 \u00b5V 2\nHz . We also find out that the 11 Hz feature provides the highest information gain (since it was selected first and was placed at the root of the tree), and is followed by 10 Hz, and then by 9 Hz and 12 Hz. We can also see, that in the case of 9 Hz reaching the threshold of 6.5 \u00b5V 2\nHz there is still a chance that this could happen even under \u201ceyes open\u201d condition and thus the further check of whether 12 Hz is higher than 7.2 \u00b5V 2\nHz is required, thus indicating that\n29\nonly in conjunction those two features can reliably indicate the \u201ceyes closed\u201d condition. All these observations carry information about the neurological correlates of our experimental conditions and all those details would be missed if we would not pursue the analysis and have stopped as soon as the primary goal of the project has been achieved. Pursuing the analysis, however, allowed to postulate an intuitive rule-based computational model of the neural conditions characteristic of the \u201ceyes closed\u201d state.\nAlthough this example is trivial, its simplicity allows us to describe the process in full detail. In Chapter 3 we provide the details and the findings of a study, that employed similar approach to analyze the contributions of spectral components into the process of visual categorization based on a dataset of 11000 local field potential (LFP) recordings from intracerebral electrodes across 100 human subjects."}, {"heading": "2.1.3 Understanding the models built by different machine learning algorithms", "text": "The example in the previous section has demonstrated that the way to understand a particular machine learning model and the way to interpret it will depend a lot on the algorithm and the architecture that generated the model. The architecture of a decision tree enabled us to readily convert the output of the algorithm into a set of intuitive rules that provide neurological information to a domain expert. Applying other machine learning\n30\nmethods would result in very different representations of the computation that is required to solve the task. The core challenge in gaining an intuitive understanding from observing model parameters lies in the requirement to know the details of the inner mechanism in order to see what is it that the model has learned that allows it to make the decisions.\nInterpretability becomes more and more important topic in the machine learning community, across scientific communities that employ machine learning methods, and even in the global community as machine learning models become embedded in our everyday lives (Doshi-Velez and Kim, 2017). \u201cInterpret means to explain or to present in understandable terms. In the context of ML systems, we define interpretability as the ability to explain or to present in understandable terms to a human\u201d (ibid). Multiple general-purpose methodologies on how interpretability could be achieved have been suggested over the years (Vellido, Martin-Guerrero, and Lisboa, 2012; Ribeiro, Singh, and Guestrin, 2016a; Ribeiro, Singh, and Guestrin, 2016b) along with numerous domain specific approaches. Since the notion of an understandable explanation in ambiguous, it is hard to come up with a rigorous method to quantify and measure interpretability of a machine learning model. As a result of this ambiguity, multiple review articles (Lipton, 2016; Bibal and Fr\u00e8nay, 2016; Doshi-Velez and Kim, 2017; Guidotti et al., 2018; Gilpin et al., 2018; Murdoch et al., 2019) proposed different taxonomies to help systematize the way we think about interpretability. Surveys like the one by Narayanan et al. (2018) are being conducted to empirically estimate interpretability via user-studies. Bibal and Fr\u00e8nay (2016) systematically explore various terms that are used in machine learning literature to denote interpretability and make suggestions how to bring the terminology in order. The same motivation drives Lipton (2016) and leads to suggesting desiderata for interpretability: trust, causality, transferability, informativeness and ethics, followed by a taxonomy of the properties of interpretable models. Another study by Doshi-Velez and Kim (2017) argues for the need of a rigorous approach and introduces the notion of incompleteness of problem formalization. Incompleteness encompasses unquantifiable gaps in knowledge that a model might have and has to be addressed in order to reach the desiderata of a comprehensive model. The outstanding survey by Guidotti et al. (2018) proposes a classification of approaches to model interpretability based on the type of the problem, type of the explanator adopted, type of the model and type of data. The most recent review (Gilpin et al., 2018) provides a good summary of the taxonomies proposed in the previous studies and puts forward a distinction between interpretability and explainability \u2013 ability of a model to summarize the reasons for the behavior of the model.\nExploring the question of interpretability in the context of neuroscience\n31\nallows us to narrow down the scope of applicable desiderata, properties and methods and focus on the ways to uncover knowledge from the models that are the products of automatic scientific discovery. In our case the question we want to answer is \u201cwhen translated back from model\u2019s representation into neuroscientific domain, what is it that allows the model to make accurate predictions?\u201d. In such form the question of interpretability is perhaps best covered by multivariate pattern analysis (Ritchie, Kaplan, and Klein, 2017) on fMRI data (Haxby, 2012), where simple linear methods allowed researchers to decode mental states and analyze their representations (Haynes and Rees, 2006; Norman et al., 2006; O\u2019Toole et al., 2007; Kriegeskorte and Kievit, 2013; Haxby, Connolly, and Guntupalli, 2014). Applying other machine learning methods with the direct goal of extracting neuroscientific knowledge was also attempted by interpreting SVM (Grosenick, Greer, and Knutson, 2008; Hardoon and Shawe-Taylor, 2010; Haufe et al., 2014), decision trees and random forests (Richiardi et al., 2010; Oh, Laubach, and Luczak, 2003), artificial neural networks (Sturm et al., 2016; Samek, Wiegand, and M\u00fcller, 2017), probabilistic models (Ma et al., 2006; Doya et al., 2007; Wolpert, 2007; Griffiths et al., 2010), dimensionality reduction techniques (Freeman et al., 2014; J. P. Cunningham and Byron, 2014), graphical models (E. T. Bullmore and Bassett, 2011), and other methods.\nThe choice of the algorithm for building an interpretable model on neural data is guided by the nature of knowledge representation the authors of the above-mentioned studies were aiming to extract. Such reasoning for the choice of the algorithm leads to yet another basis for a taxonomy of interpretable machine learning models. Given the abundance of different methods and the freedom to choose any of them for a particular neurophysiological study, the obvious choice would be in favor of the method that will uncover the representation that is most interpretable in the context of this particular study. If an investigator is interested in which neural features are the most informative for a given task \u2013 they should choose a method that is naturally suited for feature importance analysis (e.g. Random Forest). If the aim of the investigation is to identify the data samples that are crucial for correct performance \u2013 a method that identifies such samples during the learning process (e.g. SVM). Here we propose a preliminary taxonomy (Table 1) of machine learning methods that forgoes classical distinctions such as supervised or unsupervised, predictive or generative and instead organizes the methods into the groups based on the representation of the core knowledge that the model learns in order to make its decisions.\nLinear coefficients. One of the most straightforwardly interpretable, but also the least expressive in terms of encoded knowledge, are the algorithms like logistic and linear regression, that encode the learned inferences in linear coefficients of features. The learning process directly optimizes\n32\n33\nthe coefficients to minimize (in the most common formulation) the crossentropy or, in the case of linear regression, the mean squared error. The final output of, for example, a logistic classifier can be represented as a separating hyperplane in the feature space. Please note that while the final decision rule of many classification algorithms can be represented by a separating hyperplane, the underlying principles and knowledge on which the separating plane is built are very different across different learning algorithms. From the interpretability perspective linear coefficients indicate each feature\u2019s contribution into the final decision, and, especially if features were normalized before training, can allow for comparison between feature importances. Whether a coefficient is positive or negative provides an additional dimension for interpretation.\nPoints in the feature space. Many popular classification and clustering algorithms encode their findings in particular points in the feature space. Support Vector Machines find the samples in the training dataset that are next to the decision boundary, thus indicating which samples either have particular significance, or are the fringe members of class categories. Linear Discriminant Analysis (also known as Fisher\u2019s discriminant) locates centroids of the samples in each category and then devises the separating boundary that is perpendicular to the straight line connecting the centroids. The centroids are thus characteristic of the groups of samples they represent. Similar knowledge representation can be found in one of the most popular clustering algorithms \u2013 K-Means. The algorithm finds the predetermined number of cluster centers and places them at the locations in the feature space that optimize the objective function. Similar, but extended, concept is employed by self-organizing maps, where each unit of a map is assigned to a centroid in a feature space that represents center of a cluster. In the context of interpretability centroids can obtain special meaning when interpreted by a domain expert that has intuitive understanding of the feature space.\nDistance between samples. Multiple methods make conclusions about similarity of data samples based on pairwise distances between them. The family of hierarchical clustering algorithms and distance-based classification algorithms such as K-Nearest Neighbors are good examples of this knowledge representation. In neuroscientific domain the method of Representation Similarity Analysis facilitated scientists to compare representational geometry of samples that have different representations. The interpretability of the findings that those methods make is straightforward as the intuitive meaning of a distance between the samples is directly applicable in almost any domain of knowledge.\n34\nDistributions in the feature space. Extending the idea of important points in the feature space some methods store distributions in the same space and each distribution is assigned to a category, for example to an experimental condition. Mixture models describe data centroids as distributions, providing much more information that a point-based centroid would. Parameters of a distribution will capture statistical properties of each group of samples, modeling the values of the features that describe the samples, their orientation and extent in the feature space. Bayesian learning methods, such as Na\u00efve Bayes, learn, before applying the Bayes\u2019 rule, the distribution of observations conditioned on the category these observations belong to. The interpretative value of this knowledge representation is similar to that provided by the points in the feature space, but carries considerably more information.\nStates and transitions. Graph-like representation of possible states of a system and transitions between them is a very flexible way to capture the rules inferred by a learning model. Probabilistic graphical models, such as Bayesian networks or Hidden Markov Models, condense the dynamics they observe in the data into probabilistic state machines and are interpretable by human investigator as a set of rules that governs the underlying data generation process.\nTree structure. Decision trees and their ensembles such as Random Forest, represent the inferred decision making in a form of a hierarchically organized sequences of threshold-based rules, where at each step of the process a certain rule is applied to the value of a feature and depending on the outcome the sample is assigned to a certain category. Decision trees are considered to be one of the most interpretable algorithms, they store the whole process of decision in an intuitive form, are directly translatable to deterministic decision rules, rank the features by their importance (the more important a feature is, the closer it will be to the root of a tree) and find the threshold values of those features that are meaningful in the context of the decision-making process. All this information can be easily accessed by the investigator and provide domain-specific insight.\nDistributed representations. The enormous capacity (V. N. Vapnik and Chervonenkis, 2015) of modern deep learning models has led, on one hand, to their success in the last decade, and to obscuring the reasons for model\u2019s decisions on the other. Both the intermediate, and the final knowledge is stored in the model as a set of weights, often measured in millions. However, the final decision of a deep learning model can often be decomposed into (a) hierarchical organization and (b) set of local decision within each layer of the hierarchy. This allows to portray the knowledge\n35\nstored by the model as a collection of distributed representations, each of which describes one of the decision rules that the model makes during computation. Direct interpretation of this knowledge, is, however, impossible and an investigator has to use additional tools to extract the knowledge. We will review these tools in Section 2.1.4.\nCompressed feature space. This group of algorithms employs various methods to reach the same end-goal \u2013 find a feature space of a smaller dimensionality than the original, but preserve as much of the information as possible when the data samples are transformed from the original space to the new one. These methods are often referred to as dimensionality reduction techniques. An autoencoder is an artificial neural network that receives a sample as an input, performs a series of transformations to encode that sample using smaller number of artificial neurons (the bottleneck layer), and then decodes it back from the bottleneck layer to the original representation. The difference between the original sample and its reconstruction serves as the objective function that the algorithm has to minimize. Principle component analysis finds an orthogonal transformation of the feature space, such that the axis of the new space correspond to those data dimensions with largest variance. The new axis are called the principle components. The first component explains largest amount of variance, the second represents the dimension with second-largest variance, etc. After the transformation the user can estimate how many components are to be kept in order to preserve the predetermined percentage of variance (usually 90%, 95% or 99%) and discards the rest. Multidimensional scaling takes another approach where instead of the variance, the new feature space, while being reduced in size, preserves the pairwise distances between the samples as well as possible. There are other example algorithms, but the important common property that allows us to group those algorithms together is that in order to preserve the information given limited expressive power, these algorithms are forced to detect an underlying pattern, or a principle, following which the data can be best reconstructed. Capturing that principle allows to reduce the size of the feature space, but also, importantly for the interpretability context, distills the underlying patterns from the less important ones and from the noise.\nLearned embeddings. While being similar to the previous group in form, embeddings are numerical features vectors that are built, differently from dimensionality reduction techniques, by following a rule that captures a specific property, of even semantics, of the original data. Modern word embeddings (Mikolov et al., 2013), for example, are built by teaching a neural network to predict the word that appears in the context of other words in a language corpus. Words that appear in similar\n36\ncontext will have similar internal representation in the embedding (feature) space. Since appearing in a similar context carries semantic meaning in natural language, this representation even allows for arithmetic operations on words, such as the one demonstrated by the famous example E(king)\u2212 E(man) + E(woman) = E(queen), where E is the function that maps the word to its numerical vector in the embedding space. Subtracting man from king leaves us with the embedding that represents notion of \u201ckingness\u201d, adding woman to that leads to an embedding that combines notion of \u201cfemenineness\u201d with \u201ckingness\u201d, leading to the word queen. The ability of embedding to capture semantic similarity has been also demonstrated in visual domain (Deselaers and Ferrari, 2011; Frome et al., 2013). When applied to graphs, an embedding can reflect topological properties of graph nodes (Grover and Leskovec, 2016), or combine topological data with node attributes (Kipf and Welling, 2016). The key property of embeddings for the interpretability efforts is their ability to capture semantic similarity between the data samples.\nFunctions. Gaussian processes is an example of a learning method that models a distribution over the possible functions, that are consistent with the observed data. After applying initial constraints to reduce the set of possible functions, the learning process narrows the distribution by eliminating the functions that are not consistent with the data. The final output is a set of possible functions that can capture the learnt knowledge.\nThis brief overview of the basis of the proposed taxonomy of machine learning algorithms provides a useful guide for selecting the method that is appropriate for a learning problem at hand when a certain interpretation of the learnt knowledge is desired. While some of the representations above are straightforward to interpret, others require additional tools, such as visualization, rule extraction or simplification, in order to extract intuitive understanding of the knowledge represented by the model. In the next section we provide an overview of such tools."}, {"heading": "2.1.4 Techniques to analyze machine learning models and extract knowledge from representations", "text": "At this point we have completed three out of four steps along the path towards insightful automatically built computational models of neural computation: (1) selected the learning algorithm with accordance to the learning problem and desired end-knowledge representation (Section 2.1.3), (2) trained the model to fit the data, and (3) evaluated model\u2019s performance and generalization ability (Section 2.1.1). The last step is gaining intuitive\n37\nunderstanding of the model\u2019s knowledge. Depending on the algorithm we used to train the model this knowledge can be already in a human-tractable form, or it might require some additional steps.\nLinear models, single decision trees and rules are recognized by the machine learning community as most interpretable (Guidotti et al., 2018) and empirical experiments were conducted to estimate comprehensibility of these models by humans (Huysmans et al., 2011). Following the nomenclature presented in Table 1 we extend the list of straightforwardly interpretable knowledge representations to include linear coefficients, points in the feature space, distances between samples, and distributions in the feature space. For a domain expert, who has the understanding of the data the model was trained on, these representations have direct meaning and no further steps are required to spot an intuitive meaning if the model has uncovered one.\nFeature importance analysis is applicable to any model that has a quantifiable way to estimate each feature\u2019s contribution into the final decision. Linear models, provided that the data was normalized across the dataset prior to training the model, provide this information in the values of linear coefficients. Decision trees and random forests make branching decision based on the information gain, which acts as a measure of how big of a role a feature plays in the decision-making process. In Chapter 3 we use this method to identify time-frequency patterns of neural activity that are important for perceptual categorization in human brain. If a model does not provide quantitative information about each feature\u2019s contribution, sensitivity analysis (Saltelli, 2002) gives us the means to obtain the same information by measuring how the output of a model is affected by altering the input. If altering the input values of a certain feature (or a set of features) does not change model\u2019s behavior, one can conclude that this feature (or features) is not important for the model. If features of the dataset represent specific domain knowledge, their importance is directly interpretable by a domain expert.\nVisualization of model parameters and internal data representation is one of the main tools for achieving interpretability. Simplest methods include plotting, if dimensionality permits, the data points, decision boundary and the knowledge representation (support vectors, centroids, etc). In most cases, however, dimension of the feature space is too high to be visualized directly and investigators resort to visualizing aggregated statistics, such as histograms of value distributions. Dimensionality reduction techniques, that are, on one hand, learning algorithms in their own regard as we noted in the previous section, can, on the other hand, be essential for visualization efforts. Reducing the dimensions down to 2 or 3 allows to plot the data, decision boundaries and internal data representation in a human-readable form. Different dimensionality reduction techniques focus on preserving dif-\n38\nferent properties of the object that undergoes the reduction, allowing the investigator to choose the right one depending on what property should be preserved. Topology preserving algorithms such as t-distributed stochastic neighbor embedding (t-SNE, Maaten and G. Hinton (2008)), self-organizing maps (Kohonen, 1990), and multi-dimensional scaling keep the objects that were similar in the original space also close in the new space. Principle component analysis identifies the data dimensions that have the highest variance and keep the transformation that allows to see the contribution of the dimensions of the original space into the dimensions of the new space. Please refer to Guidotti et al. (2018) for a comprehensive review of visualization methods in the context of interpretability.\nAutomatic rule extraction is another way to achieve interpretability by converting complex model into a set of rules. In additional to trees and tree ensembles that can be naturally converted to this representation, the approach has been proposed for SVMs (N\u00fa\u00f1ez, Angulo, and Catal\u00e0, 2002) and neural networks (R. Andrews, Diederich, and Tickle, 1995; Tsukimoto, 2000; J. Zilke, 2015).\nThe enormous number of trainable parameters and high capacity of deep learning models is one of the reasons for the success of these methods in the last decade. It is also the reason why the decisions made by these methods are even less transparent than the ones made by other machine learning methods. This predicament has led to an explosion in the number of studies proposing, in addition to model-agnostic, also neural networkspecific interpretability methods. Activation maximization methods identify input patterns that maximize the activation values of a particular neuron, later visual inspection of those patterns can be very insightful, especially in visual input domain (Zeiler and Fergus, 2014). Attention mechanism (Mnih, Heess, Graves, et al., 2014) allows the investigators to analyze the areas of the input that the model deemed worth of its attention and understand which part of the feature space, or what data content, was most relevant for the model. Proxy models are trained to approximate the behavior of a neural network on a full, or partial, set of data and build a simpler representation of network\u2019s behavior. Due to ease of interpretation, linear (Ribeiro, Singh, and Guestrin, 2016b) or decision tree (Craven and Shavlik, 1996; Schmitz, Aldrich, and Gouws, 1999; J. R. Zilke, Mencia, and Janssen, 2016) models are most commonly used as proxy models. Readout technique employs linear models to take activations of a subset of neurons as input and train to predict the final outcome (for example the category of a sample), the method draws inspiration from reservoir computing (Schrauwen, Verstraeten, and Van Campenhout, 2007). Ability or inability of a readout model to perform well is an indicator of the involvement of the chosen subset of neurons in the decision-making process of the model. Koul, Greydanus, and Fern (2018) demonstrate how to extract finite state representation of a recurrent\n39\nneural network. Please see Gilpin et al. (2018) for an extensive survey of interpretability techniques developed specifically for deep learning methods."}, {"heading": "2.2 Interpretation of machine-learned models for neuroscientific inquiry at different levels of organization", "text": "In the next three chapters of this thesis we present detailed description of three studies that demonstrate how the approach we have described in this chapter can be realized in the context of neuroscientific inquiry to analyze, interpret and understand neural processes at three different levels of organization. Each study shows how the in-depth analysis of the machine learning models can lead to insights or confirmations of conjectures about inner workings of the human brain.\nChapter 3 demonstrates the analysis on a level of local field potentials. We first train a decoder to predict visual category based on spectro-temporal activity of single iEEG probes using a random forest classifier. We then perform feature importance analysis to understand which locations are relevant for the task of visual decoding and which, while being active and responsive compared to the baseline, do not carry relevant information. Further analysis of the important parts of TF spectrum shows difference in roles of different neuronal locations and uncovers category-specific patterns we call spectral signatures of visual perceptual categorization.\nChapter 4 shows how the comparison of activations of an artificial model of vision (convolutional neural network) with the activations in human visual cortex allows to draw the analogy between the hierarchical structures of those two systems. This study serves as an example of how machine learning methods can be used to analyze the brain on the level of functional organization.\nThe third example, demonstrated in Chapter 5, shows how using dimensionality reduction for clustering and visualization of high-dimensional EEG feature space helps to gain a high-level understanding of relative properties of mental concepts encoded in that space. By visualizing mental state space we can browse the signals generated by the human brain under different conditions and visually assess which ones are close to each other and which ones are further apart. The work shows an application of this concept to the field of brain-computer interfaces and serves as an example of applicability of the interpretable machine approach on the level of mental concepts.\n40\nChapter 3\nFeature importances of random forest models inform on localized cortical activity\nHuman brain has developed mechanisms to efficiently decode sensory information according to perceptual categories of high prevalence in the environment, such as faces, symbols, objects. Neural activity produced within localized brain networks has been associated with the process that integrates both sensory bottom-up and cognitive top-down information processing. Yet, how specifically the different types and components of neural responses reflect the local networks\u2019 selectivity for categorical information processing is still unknown. In this work we train Random Forest classification models to decode eight perceptual categories from broad spectrum of human intracranial signals (4\u2212 150 Hz, 100 subjects) obtained during a visual perception task. We then analyze which of the spectral features the algorithm deemed relevant to the perceptual decoding and gain the insights into which parts of the recorded activity are actually characteristic of visual categorization process in human brain. We show that network selectivity for a single or multiple categories in sensory and non-sensory cortices is related to specific patterns of power increases and decreases in both low (4\u221250 Hz) and high (50\u2212150 Hz) frequency bands. By focusing on task-relevant neural activity and separating it into dissociated anatomical and spectrotemporal groups we uncover spectral signatures describing neural mechanisms of visual category perception in human brain that have not yet been reported in the literature.\nPrevious works have shown where and when perceptual category information can be decoded from the human brain, our study adds to that line of research by allowing to identify spectrotemporal patterns that contribute to category decoding without the need to formulate a priori hypothesis on which spectral components and at which times are worth investigating.\n41\nApplication of this method to an extensive dataset of human intracerebral recordings delineates the locations that are predictive of several perceptual categories from the locations that have narrow specialization, identifies spectral signatures characteristic of each of 8 perceptual categories and allows to observe global and category-specific patterns of neural activity pertinent to visual perception and cognition."}, {"heading": "3.1 Spectral and temporal signatures of human brain activity", "text": "Our capacity to categorize sensory information allows us to quickly process and recognize complex elements in our environment. Early studies revealed strong relations between the brain activity within certain localized networks and the neural representations of certain stimulus categories, as for example faces, bodies, houses, cars, objects and words (Kanwisher, McDermott, and Chun, 1997; Epstein, A. Harris, et al., 1999; Peelen, Fei-Fei, and Kastner, 2009; Malach et al., 1995; Haxby, Gobbini, et al., 2001; Ishai, Ungerleider, et al., 1999; Cohen et al., 2000). These early assessments also revealed brain networks\u2019 capability to rapidly extract categorical information from short exposure to natural scenes (Potter and Faulconer, 1975; S. Thorpe, Fize, and Marlot, 1996; Li et al., 2002) based on models of parallel processing across neural networks (Rousselet, Fabre-Thorpe, and S. J. Thorpe, 2002; Peelen, Fei-Fei, and Kastner, 2009). In both animal and human studies, visual cortices and particularly inferior temporal cortex (ITC) appear as a key region to integrate information at the object-level (Grill-Spector and Weiner, 2014; Tanaka, 1996; DiCarlo, Zoccolan, and Rust, 2012). In humans, a great deal of observations of cortical response selectivity have been achieved using fMRI, but measuring direct neuronal activity (R. Q. Quiroga et al., 2005; Kreiman, Koch, and Fried, 2000) also revealed similar patterns. To further understand how stimulus features and perceptual experience is processed in neural networks, brain activity, especially in sensory cortices, has been decoded using a variety of methods and signals (Haynes and Rees, 2006; Kriegeskorte, Goebel, and P. Bandettini, 2006; Kamitani and Tong, 2006). This decoding often relies on machine learning to avoid a priori selection of partial aspects of the data by the human observer, and unless additional analysis is performed on the model itself it does not emphasize the mechanisms of neuronal communication within and between neural networks involved in this processing.\nA pervasive feature of electrophysiological neural activity are its spectral fingerprints. Neural oscillations have been proposed to reflect functional communication processes between neural networks (Fries, 2009; Buzsaki, 2006; Siegel, Donner, and Engel, 2012; Michalareas et al., 2016). Certain\n42\nfrequency bands are selectively associated with the operating of different cognitive processes in the human and animal brain (Vidal, Chaumon, et al., 2006; Wyart and Tallon-Baudry, 2008; Jensen and Mazaheri, 2010; VanRullen, 2016; Engel and Fries, 2010; Dalal et al., 2011), and lately, direct recordings from the human cortex have revealed the remarkable representation selectivity of broadband high-gamma activity (50 \u2212 150 Hz) (Lachaux, Axmacher, et al., 2012; Parvizi and Kastner, 2018; Fox et al., 2018). Human intracranial recordings have previously shown evidence of functional processing of neural networks related to perceptual category representation (McCarthy et al., 1997) and lately the prominence of broadband high-gamma activity in selective category responses in visual areas (Vidal, Tom\u00e0s Ossand\u00f2n, et al., 2010; Davidesco et al., 2013; Hamam\u00e8, Vidal, et al., 2014; Privman et al., 2007; Fisch et al., 2009). Yet, very little is known about the specific relation between the different components of the full power-spectrum, including high-gamma activity, and their level of selectivity in processing perceptual categories. Previous works have shown where and when perceptual category information can be decoded from the human brain, the approach introduced in this work adds to that line of research by allowing to identify spectrotemporal patterns that contribute to category decoding without the need to formulate a priori hypothesis on which spectrotemporal regions of interest are worth investigating.\nIn this work we capitalize on an extensive dataset of deep intracranial electrical recordings on 100 human subjects to decode neural activity produced by 8 different stimulus categories. We analyzed the decoding models built by a random forest classifier to disentangle the most informative components of the time-frequency spectrum related to the simultaneous classification of 8 different perceptual categories. Via feature importance analysis we quantified the contribution of each TF component into the decoding decision, which allowed us to identify the activity patterns that were either characteristic of the processing of a specific visual category or were shared by several categories. In addition to feature importance we analyzed the predictive power of each activity pattern and identified how informative was their spectral signature for the classification of visual categories. We tested the predictive power of broadband high-gamma activity in comparison to lower frequency activity as they reflect different communication mechanisms elicited by networks seemingly involved in distinct temporal windows of functional neuronal processing. Through the analysis of feature importance we show the specific neuronal spectral fingerprints from highly distributed human cortical networks that were elicited during automatic perceptual categorization. The uncovered spectral signatures provide insight into neural mechanisms of visual category perception in human brain.\n43"}, {"heading": "3.2 Large-scale intracortical recordings during visual object recognition task", "text": "One of the important steps leading up to being able to interpret the results and representations of a machine learning model is the correct choice of representation of the input data. In this section we explain the origin of our dataset and preprocessing choices that were made in order to present the data in a form that is both informative for the algorithm and directly interpretable by human domain expert."}, {"heading": "3.2.1 Patients and recordings", "text": "100 patients of either gender with drug-resistant partial epilepsy and candidates for surgery were considered in this study and recruited from Neurological Hospitals in Grenoble and Lyon (France). All patients were stereotactically implanted with multi-lead EEG depth electrodes (DIXI Medical, Besan\u00e7on, France). All participants provided written informed consent, and the experimental procedures were approved by local ethical committee of Grenoble hospital (CPP Sud-Est V 09-CHU-12). Recording sites were selected solely according to clinical indications, with no reference to the current experiment. All patients had normal or corrected to normal vision.\nElectrode implantation\n11 to 15 semi-rigid electrodes were implanted per patient. Each electrode had a diameter of 0.8 mm and was comprised of 10 or 15 contacts of 2 mm length, depending on the target region, 1.5 mm apart. The coordinates of each electrode contact with their stereotactic scheme were used to anatomically localize the contacts using the proportional atlas of Talairach and Tournoux (Talairach and Tournoux, 1993), after a linear scale adjustment to correct size differences between the patient\u2019s brain and the Talairach model. These locations were further confirmed by overlaying a post-implantation MRI scan (showing contact sites) with a pre-implantation structural MRI with VOXIM\u00ae (IVS Solutions, Chemnitz, Germany), allowing direct visualization of contact sites relative to brain anatomy.\nAll patients voluntarily participated in a series of short experiments to identify local functional responses at the recorded sites (Vidal, Tom\u00e0s Ossand\u00f2n, et al., 2010). The results presented here were obtained from a test exploring visual recognition. All data were recorded using approximately 120 implanted depth electrode contacts per patient using SD LTM Express, Micromed system for signal acquisition with a sampling rate of 512 Hz, high-pass filter 0.15 Hz, low-pass filter 500 Hz. Data were obtained from a total of 11321 recording sites.\n44\nStimuli and task\nThe visual recognition task lasted for about 15 minutes. Patients were instructed to press a button each time a picture of a fruit appeared on screen (visual oddball paradigm). Non-target stimuli consisted of pictures of objects of eight possible categories: houses, faces, animals, scenes, tools, pseudo words, consonant strings, and scrambled images. All the included stimuli had the same average luminance. All categories were presented within an oval aperture of 2\u25e6 \u00d7 3\u25e6 in visual angle (illustrated on Figure 2a) at a distance of 70\u2212 90 cm using NeuroBehavioral Systems (NBS) Presentation\u00ae software. Stimuli were presented for a duration of 200 ms every 1000 \u2212 1200 ms in series of 5 pictures interleaved by 3 second pause periods during which patients could freely blink. Patients reported the detection of a target through a right-hand button press and were given feedback of their performance after each report. A 2 second delay was placed after each button press before presenting the follow-up stimulus in order to avoid mixing signals related to motor action with signals from stimulus presentation. Altogether, responses to 400 unique natural images were measured per subject, 50 from each category."}, {"heading": "3.2.2 Processing of neural data", "text": "The analyzed dataset consisted of 4528400 local field potential (LFP) recordings \u2013 responses from 11321 recording sites to 400 stimuli. To remove the artifacts the signals were linearly detrended and the recordings that contained values\u2265 10\u03c3images, where \u03c3images is the standard deviation of voltage values (in the time window from \u2212500 ms to 1000 ms) of that particular probe over all stimuli, were excluded from data. All electrodes were rereferenced to a bipolar reference and the reference electrodes were excluded from the analysis. The signal was segmented in the range from \u2212500 ms to 1000 ms, where 0 marks the moment when the stimulus was shown. The \u2212500 to \u2212100 ms time window served as a baseline.\nTo quantify the power modulation of the neural signals across time and frequency we used standard time-frequency (TF) wavelet decomposition (Daubechies, 1990). The signal s(t) was convoluted with a complex Morlet wavelet w(t, f0), which has Gaussian shape in time (\u03c3t) and frequency (\u03c3f ) around a central frequency f0 and defined by \u03c3f = 1/2\u03c0\u03c3t and a normalization factor. To achieve good time and frequency resolution over all frequencies we slowly increased the number of wavelet cycles with frequency, f0 \u03c3f\nwas set to: 6 for high (61\u2212 150 Hz) and low (31\u2212 60 Hz) gamma, 5 for beta (15 \u2212 30 Hz), 4 for alpha (9 \u2212 14 Hz) and 3 for theta (4 \u2212 8 Hz) frequency ranges. This method allowed to obtain better frequency resolution than applying a constant cycle length (Delorme and Makeig, 2004). The\n45\nsquare norm of the convolution results in a time-varying representation of spectral power, given by: P (t, f0) = |w(t, f0) \u00b7s(t)|2. Baseline normalization was performed by dividing the average power after stimulus onset (0 to 1000 ms) in each frequency by the average power of that frequency in the baseline window (\u2212500 to \u2212100 ms). Each LFP recording was transformed from 768 data points (1.5 seconds of voltage readings at 512 Hz sampling rate) into a matrix of size 146\u00d7 48 where each row represents a 1 Hz frequency band from 4 Hz to 150 Hz and columns represent 31.25 ms time bins. Value in each cell of that matrix is the power of that specific frequency averaged over 16 time points.\nFurther analysis was done only on the electrodes that were responsive to the visual task. In each frequency band we compared each electrode\u2019s average post-stimulus band power to the average baseline power with a Wilcoxon signed-rank test for matched-pairs. Only the probes that showed a post-stimulus response that is statistically significantly (p-value \u2264 0.005, corrected for multiple comparisons with the false discovery rate (FDR) procedure (Genovese, Lazar, and Nichols, 2002)) different from the baseline response in at least two frequencies were preserved for future analysis. Please note that eliciting a significant response in at least 2 out of 146 frequencies is a relaxed requirement. The use of such a relaxed criterion allowed us to include into analysis not only the areas that had a strong response in the visual areas, but also the responses from other brain areas that might reflect downstream processes related to automatic perceptual categorization. This was possible due to the fact that the proposed method, given sufficiently large dataset, will not be hindered by the additional volume of irrelevant data and is able to detect narrow phenomena even in the large corpus of data.\nTo anatomically localize the source of each signal in subject\u2019s brain each electrode\u2019s MNI coordinates were mapped to a corresponding Brodmann brain area (Brodmann, 1909) using Brodmann area atlas from MRICron (Rorden, 2007) software.\nTo confirm that probe\u2019s predictiveness of a certain category implies that the probe belongs to the network selective of that category we ran a set of experiments on three well-known functional areas: Fusiform Face Area (FFA) (Kanwisher, McDermott, and Chun, 1997), Visual Word Form Area (VWFA) (Cohen et al., 2000) and Parahippocampal Place Area (PPA). Following Montreal Neurological Institute (MNI) coordinates of FFA reported in (R. J. Harris, Young, and T. J. Andrews, 2012) and (Axelrod and Yovel, 2015) we defined FFA bounding box as x \u2208 [\u221244,\u221238], y \u2208 [\u221261,\u221250], z \u2208 [\u221224,\u221215] in the left hemisphere and x \u2208 [36, 43], y \u2208 [\u221255,\u221249], z \u2208 [\u221225,\u221213] in the right hemisphere. Based on the Table 1 from (Price and Devlin, 2003) we defined VWFA area as MNI bounding box x \u2208 [\u221250,\u221238], y \u2208 [\u221261,\u221250], z \u2208 [\u221230,\u221216] in the left hemisphere.\n46\nFrom MNI coordinates reported in (Bastin, Vidal, et al., 2013) and (Park and Chun, 2009; Hamam\u00e8, Szwed, et al., 2013) we defined PPA bounding box to be x \u2208 [\u221231,\u221222], y \u2208 [\u221255,\u221249], z \u2208 [\u221212,\u22126] in the left hemisphere and x \u2208 [24, 32], y \u2208 [\u221254,\u221245], z \u2208 [\u221212,\u22126] in the right hemisphere."}, {"heading": "3.3 Feature importances of a decoder are indicative of task-relevant brain activity", "text": "In the taxonomy of knowledge representation (Section 2.1.3), decision trees were brought forward as the representation best suited for feature importance analysis (see Section 2.1.4). In this work we use spectral power readings in time-frequency domain as our input and look to identify which timefrequency features contribute most to the task at hand. Since feature importance is our desired interpretation we have chosen Random Forest learning algorithm to build the decoding model. In this section we explain the inner workings of this algorithm and present our feature analysis approach in details."}, {"heading": "3.3.1 Random Forest as a decoding model", "text": "A Random Forest (Leo Breiman, 2001) is a collection of decision trees, where each tree gets to operate on a subset of features. Each tree is assigned a random set of features and it has to find the decision boundaries on those features that lead to best classification performance. At each branching point the algorithm must decide which feature will be most efficient in terms of reducing the entropy of class assignations to the data points in the current branch of the decision tree. To achieve that, the feature that is most useful is selected first and will be responsible for largest information gain. For example, if the activity of a probe at 52 Hz at 340 ms is high when a subject is presented with a face and low for all other categories, decision tree will use that fact and rely on the \u201c52 Hz at 340 ms\u201d feature, thus assigning it some importance. How high the importance of a feature is depends on how well does this feature distinguish faces from all other categories. As Random Forest is a collection of trees and the same feature will end up being included into several different trees, being important in many trees contributes to the overall importance of a feature (for the exact computation see the section on feature importance below).\nWe treated each electrode\u2019s responses as a separate dataset consisting of 400 data points (one per stimulus image), and 7008 features \u2013 timefrequency transformation of LFP response into 146 frequencies and 48 time bins. For each electrode we trained a Random Forest with 3000 trees and\n47\n48\nused 5-fold cross-validation to measure the predictive power of the neural activity recorded by each of the electrodes. Per-class F1 score, a harmonic mean of precision and recall of a statistical model, provides us with a metric of success of the classification. The parameters were selected by performing informal parameter search. Random Forest was the algorithm of choice for our analysis due to interpretability of the resulting models, that allowed us to track the process that led each particular model to a decoding decision and due to its previous application to spectrotemporal features (Westner et al., 2018). We used scikit-learn (Pedregosa et al., 2011) implementation of the above-mentioned methods with default parameters unless indicated otherwise.\nAs the first step of the decoding analysis we estimated which of 11321 electrodes have predictive power. For that we split each electrode\u2019s 400- sample dataset into 320 samples for training and 80 for prediction estimation. Repeating this procedure 5 times provided us with 400 predictions that we could compare to the true categories. By running a permutation test 100000 times on electrodes with randomly permuted class labels we estimated that 99.999th percentile (equivalent to significance threshold of p \u2264 0.00001) of F1 score is 0.390278. F1 score is an aggregated metric of the performance of a classifier that combines both the precision (the ratio of the data samples that truly belong to a category among the ones that were assigned to that category by the model) and recall (the ratio of data samples that were correctly identified to belong to a category to the total number of samples of that category in the dataset) into one number: F1 = 2 \u00b7 precision \u00b7 recall precision + recall . In total 787 electrodes had a predictive power of F1 > 0.390278 in at least one of the categories. For each of those electrodes a Random Forest model was retrained once more on whole data (400 samples instead of 320) and that model was used for calculating feature importances and, ultimately, for understanding which parts of the recorded activity were relevant for visual object recognition in human brain."}, {"heading": "3.3.2 Feature importance for the analysis of task-relevant neural activity", "text": "During the process of constructing the decision trees, Random Forest relies on some features more than on the others. We chose Gini impurity (Leo Breiman, 2017) as a measure of which features should be used to make the branching decisions in the nodes of a tree. This score, along with the number of times each particular feature was used across trees, informed us on the relative importance of each particular feature with respect to other\n49\nfeatures. Gini impurity G is calculated as\nG =\ni=nc\u2211\ni=1\npi(1\u2212 pi), (3.1)\nwhere nc is the number of categories and pi is the proportion of class i in a node. To pick a feature for a parent node, Gini impurity of both child nodes of that parent are calculated and used to estimate the reduction in impurity that would be achieved by picking that particular feature as the branching factor for the node. The feature that decreases impurity the most is selected to be the branching factor of that parent node. The reduction in impurity is calculated as\nI = Gparent \u2212Gleft child \u2212Gright child (3.2)\nand is called node importance. Feature importance of a feature f is estimated by calculating the sum of Gini impurity reductions over all samples in the dataset that were achieved with the use of a particular feature f and normalizing it by the total number of samples. Figure 2e is a visual representation of relative feature importance, color intensity shows the importance of each of 7008 (146 frequencies \u00d748 time bins) spectrotemporal features from one probe. In total our analysis has produced 787 \u00d7 8 such images \u2013 one for each probe-class pair.\nThe importance map computed as depicted on Figure 2 is an example of a global map for all 8 categories. The regions that are highlighted on the map are important for distinguishing between all 8 categories. There is, however, a way to look at category-specific importances as well. The final set of nodes of a decision tree, called leaves, are the end-points of the classification process and each leaf is associated with a certain category. If we take one TF activity map (TF components are the features) and start traversing a decision tree following the rules set by the nodes of the tree, we will end up in a certain leaf. That leaf will be associated with a certain category, for example, with faces. The fact that we followed the rules and ended up in that leaf indicates that the TF map we used as the input to the tree probably comes from a trial where a face stimulus was shown to the subject. In order to get category-specific feature importance map we took all the leaves associated with a category, traversed the tree backwards and tracked all the features that were used on the path from the leaf to the root of the tree. This way we got a list of features (TF components) that were used to identify a neural response as belonging to a certain category. Random Forest feature importance allowed us to identify which sub-regions of neural activity (TF maps) are relevant for decoding. It showed that only a small portion of activity is actually crucial for identifying the categories (see Figure 3).\n50\n51\nTo compare importance maps between each other we fit a normal distribution on the difference between two maps and considered statistically significant the differences that are bigger than \u00b5+ 4\u03c3. One spectrotemporal importance map consists of 7008 values. To filter out false positives we stipulated that only 1 false positive out of 7008 pixels can be tolerated and tuned the threshold accordingly. That requirement resulted in the p-value of 0.0001427 and confidence level of 99.99%, corresponding to 3.89\u03c3, which we rounded up to \u03c3 = 4.0."}, {"heading": "3.3.3 Hierarchical clustering to reveal types of activity patterns", "text": "To further analyze the spectrotemporal signatures elicited by different visual categories in different parts of human brain we clustered filtered activity patterns and identified the most prominent groups. The result of this analysis is shown in the second column of Figure 8. For each category, the four most populated (in terms of the number of probes) clusters of activity patterns elicited by this category are shown.\nTo do the clustering we first took each probe\u2019s category-specific activity separately by averaging probe\u2019s responses to 50 images of each particular category in time-frequency domain. We then masked the activity with the category importance map (as shown on Figure 3), leaving only those features out of 146\u00d748 that have importance score larger that \u00b5+\u03c3, where \u00b5 is the average importance score for that category and \u03c3 is one standard deviation of the score distribution.\nMasked activity patterns were hierarchically clustered using Eq 3.3 to calculate the distance between a pair of clusters U and V as the maximal cosine distance between all of the clusters\u2019 member observations (complete linkage clustering):\nd(U, V ) = max ( u \u00b7 v \u2016u\u2016\u2016v\u2016 ) \u2200u \u2208 U, \u2200v \u2208 V (3.3)\nSciPy (Jones, Oliphant, Peterson, et al., 2001) implementation of the hierarchical clustering methods was used in this work. Resulting clustering assignments were visually inspected and corrected."}, {"heading": "3.4 The role and diversity of time-frequency patterns of individual locations and area networks in perceptual categorization", "text": "By choosing the machine learning algorithms with subsequent need for interpretability in mind (random forest, hierarchical clustering) and application\n52\nof interpretability techniques (feature importance analysis, visualization) we were able to extract the knowledge that the model had obtained, present it in a way that is understandable to a neuroscientist and articulate the neurological insights that the model has found. The three main observations we made as a result of this analysis were: (a) the difference between responsiveness of a neural location and its ability to predict an experimental condition (visual category), (b) existence of monopredictive and polypredictive neural locations, where the former are specialized and only are relevant for processing specific visual categories, while the latter carry information that is relevant to decoding of several categories, and (c) to extensively map and describe time-frequency patterns that are characteristic of cognitive processing of each particular visual category. In this section we present these findings in full detail."}, {"heading": "3.4.1 Feature importance allows to separate out the neural signals that are predictive of perceptual categorization from the mixture of stimulus-induced responses", "text": "To identify spectrotemporal features that are characteristic of automatic perceptual categorization of a particular category we relied on time-frequency (TF) maps of the neural responses of intracranially implanted electrodes. Out of the total set of 11321 probes 11094 (98%) were responsive (see the section 3.2.2 on processing of neural data for details) to the stimuli from at least one of the categories. On one hand this provides us with abundance of data, on the other raises the question whether all of that activity was relevant to the processes that encode and process visual input.\nTraining a decoding model (see the section 3.3.1 on Random Forest as decoding model) for each of the probes allowed us to dissociate the predictive probes that exhibited activity that was useful for decoding from the rest of the responsive probes that did not carry such activity.\nGreen markers on Figure 4a show the set of probes that are responsive to the house category, while the blue markers are the probes that are predictive of that category (4.8%, 535 probes). Decoding models built on the neural responses of the predictive probes were successful at classifying at least one perceptual category (F1 > 0.39 for one or more classes), focusing on them in our further analysis allowed to work only with the locations that carry information relevant to the task of perceptual categorization.\nPredictive probes had a heterogeneous distribution in the brain, yet remained mostly concentrated in visual cortices and inferior temporal regions (76%), from BA17 to BA20, including early visual areas (BA 18, 19), fusiform gyrus (BA 37) and inferior temporal cortex (BA 20). A majority of the predictive probes were in fusiform cortex (average of 52% over all categories, Figure 4b), followed by BA 19 (27%), across all category\n53\nnetworks. Within the primary visual cortex, BA 17 and 18, the scrambled was the stimulus that elicited most predictive probes (28) amongst all stimulus categories (Figure 4c), followed by pseudowords (13). Probes predictive of faces were mostly concentrated in BA19, BA37 and BA20 (72%, 108 out of 150). The low number of predictive probes in area 17 is explained by the fact that less than 1% of the implantation sites in the original dataset were located in primary visual cortex.\n54\nPrevious studies have shown that perceptual category-selective networks are located in occipito-temporal cortex (Grill-Spector and Weiner, 2014; Ishai, Ungerleider, et al., 1999; Malach et al., 1995). To test whether predictive power of the Random Forest model trained to decode activity of probes is coherent with known functional processing by cortical networks we evaluated the selectivity of the predictive power in three known functional networks: Fusiform Face Area (FFA) (Kanwisher, McDermott, and Chun, 1997), Visual Word Form Area (VWFA) (Cohen et al., 2000) and Parahippocampal Place Area (PPA) (Epstein and Kanwisher, 1998). We checked whether the probes located in each of these areas and the Random Forest model trained on these probe\u2019s activity to discriminate between 8 categories produces the highest predictive power for the category for which this area is known to be selective. Probes in FFA are associated with facial recognition and encoding facial information (Parvizi, Jacques, et al., 2012; Ghuman et al., 2014; Kadipasaoglu et al., 2016; Jonas, Jacques, et al., 2016; Jonas, Rossion, et al., 2015) and thus we expect their activity to be predictive of the face category, probes in VWFA should be predictive of characters and pseudowords categories (Kadipasaoglu et al., 2016; Lochy, Van Reybroeck, and Rossion, 2016; Hirshorn et al., 2016) and probes in PPA should be responsive to scenes and houses (Aguirre, Zarahn, and D\u2019esposito, 1998; M\u00e8gevand et al., 2014; Epstein and Kanwisher, 1998; Bastin, Committeri, et al., 2013).\nThere were 12 probes in the FFA that were significantly (permutation test p < 1e\u2212 4) predictive (classification score F1 > 0.39) of a category: 5 were predictive of faces, 4 of animals (which mostly have faces on the image), 2 of pseudowords and 1 of scrambled images. Most probes that were in FFA and were predictive, carried information of the categories containing facial features.\nThere were 8 probes in the VWFA that were predictive of a category: 5 were predictive of pseudowords, 2 of characters and 1 of faces. This points to the fact that the predictive probes in VWFA are predictive of the stimuli with written characters on them. These results confirm that predictive power of a Random Forest model trained on probes activity in VWFA reflects the functional role known to be carried by this area.\nFor probes in the PPA results were less selective. There were 23 probes inside that area that were predictive of a category: 5 were predictive of houses, 4 of scenes, 5 of characters, 5 of scrambled images, 2 of tools and 2 of pseudowords. The probes from PPA predicted not only houses and scenes, but also other categories. However, houses and scenes were among the categories that the probes from PPA were able to identify successfully in highest proportion as compared to the other categories.\nThese confirmatory findings give credibility to the methodology by which the probes that are identified as predictive of a certain category are involved\n55\nin the processing of the stimuli that belong to that category. Training per-probe decoding models not only allowed us to identify the predictive locations, but also to apply feature importance analysis to decoding models trained on local activity. Computing the feature importance across the time-frequency map (4\u2212150 Hz and \u2212500 to 1000 ms) allowed us to see which parts of neural activity are crucial for the decoding. Overlaying the importance over time-frequency map showed at which frequencies and at what times the activity that was important for the algorithm has occurred. This can be applied both on aggregated level, where the importance map is averaged over probes, and on individual probe level. Figure 3 illustrates the application of probe importance map to filter irrelevant activity and obtain spectrotemporal signature of a particular category on a particular probe. Now we can use the feature importance map as a mask and perform the analysis of the activity itself, focusing only on the relevant parts of it. When applicable, this methodology helps to filter out irrelevant activity and allows to focus on the activity that is important to the scientific question under investigation.\nWe took an average over importance maps of all individual probes within each category to obtain the global picture of where the category-specific activity lies in time and frequency space. Figure 5 summarizes such analysis and singles out the spectrotemporal signatures that are unique to specific categories and those that are less selective. From these importance maps we notice that certain TF components are distinctly present per category, as for example high (significantly higher than 81 out of 112 regions of interest, Mann-Whitney U p < 8.9e\u22127, corrected) importance of the transient theta activity in all categories, or the almost absence of importance of broadband\n56\ngamma (significantly lower than 10 out of 12 other regions of interest, MannWhitney U p < 8.3e\u22125, corrected) in the control scrambled condition.\nIn the following sections we expand our analysis to the comparison of the feature maps and analyzing the activity in the regions that we have identified as important."}, {"heading": "3.4.2 Polypredictive and monopredictive probes", "text": "The analysis revealed two types of neural locations: polypredictive probes are predictive of multiple visual categories, while monopredictive are useful for decoding only one out of 8 different types of stimuli revealing a high degree of specialization (Figure 6b). We considered a probe to be predictive of a category if cross-validation F1 score for that category was higher than 0.39 (see the section 3.3.1 for the details on the threshold selection), which is a stricter condition than above-chance criterion (F1 > 0.125). Figure 6a shows that polypredictive probes reside mainly (94%, 136 out of 145) in posterior occipital and posterior temporal, while the monopredictive probes extend, in addition to occupying similar posterior occipital and temporal locations, to frontal cortex (92%, 45 out of 49 probes in this area are monopredictive) and anterior temporal cortex (88%, 51 out of 58 probes). Both mono- and polypredictive probes are also observed in parietal cortex. Monopredictive probes that extend beyond ventral stream and temporal cortex pertain to the following perceptual categories: faces (orbitofrontal cortex), animals and pseudowords (dorsofrontal cortex, inferior frontolateral cortex, premotor cortex), and, to a smaller extent, scrambled images (prefrontal cortex).\nThe unique association of specific TF feature importance components with either polypredictive and monopredictive probes was category specific, as shown in figures 7a to 7h. While all of the data presented on these figures shows statistically significant differences between monopredictive and polypredictive neural locations, we will focus only on a few that were supported by the strongest signal in the data. For face stimuli, most of the feature importance in the early broadband gamma response was significantly (4\u03c3) higher in polypredictive probes as compared to monopredictive probes, indicating that the most useful information for distinguishing faces from other visual categories is coded in that region of time-frequency space and is carried by polypredictive probes (Figure 7b). Decoding of animals and tools relied on the activity patterns produced by monopredictive neural locations in late broadband gamma range (> 300 ms) and in even later (350\u2212 600 ms) alpha/beta range, with very little involvement of the activity of polypredictive probes. Scenes and houses also show strong feature importance in late alpha and beta band responses of monopredictive probes (4\u03c3 higher). Interestingly, for characters (Figure 7g),\n57\nfeature importance in the early broadband gamma range was dominant for polypredictive probes (4\u03c3 higher than monopredictive), while the opposite was true for the pseudowords (Figure 7f) \u2013 the late broadband gamma revealed to be dominant for monopredictive probes, also note the difference in the anatomical locations that were the most useful for the decoding of the pseudowords compared to the locations that were useful for decoding characters. Pseudowords also elicited a significantly stronger TF feature importance in monopredictive probes in late (350\u2212 750 ms) low-frequency (4\u2212 12 Hz) range, similar to animal and tool stimulus categories. Finally, an interesting observation was that animals and faces share most of their polypredictive probes (51%) indicating a large overlap of categorization networks of these two categories."}, {"heading": "3.4.3 Further decomposition of important activity reveals clusters of distinct time-frequency patterns", "text": "We ran clustering analysis of the probes predictive of a category based on their activity to see which probes in the category-network behave in a similar way. Left column of Figure 8 shows an averaged feature importance map for a given category. We look into the regions of the time-frequency map that are indicated as important by the feature importance map, extract baselinenormalized activity in those regions and cluster the probes according to that activity using hierarchical complete linkage clustering with cosine distance (see the section 3.3.3 on hierarchical clustering for details). The second column of Figure 8 shows the activity of four most populated clusters for\n58\neach category. Each cluster represents the activity pattern exhibited by the probes in that cluster. Only the probes whose activity had predictive power (F1 > 0.39) are included in this analysis. As the final step we identified the anatomical locations of the probes from each cluster to see whether difference in the activity patterns could be attributed to the functional regions of the brain. The visualization of this step in the last two columns of Figure 8.\nThis analysis allowed us make a number of global and category-specific observations. The set of visual categories presented in our data is diverse enough to consider category-specific findings to be general and emerge under any comparable set of visual stimuli.\nThe first global observation was that it is not only broadband gamma\n59\nactivity that is useful for the decoder\u2019s (Random Forest) performance, but low-frequency activity also contributed significantly (41% of predictive probes exhibited only low-frequency activity in the regions of importance), sometimes overshadowing the activity of higher frequency bands altogether\n60\n(for face and scrambled stimuli low frequency activity was significantly more important than broadband gamma activity, Mann-Whitney U test p < 1e\u22127, corrected). Most clusters were composed of a combination of low and high-frequency components (Figure 8, second column) and were mostly (87%) located in occipito-temporal cortices, though some electrodes in parietal and frontal cortex (7%) also appeared to contribute with predictive responses in the decoding process (Figure 8, two right columns), especially for such stimulus categories as animal and pseudoword.\nThe second observation spanning across all categories was that the classifier used not only the increases in power to perform the classification, but also relied on power decreases in different brain networks (7 out of 32 dominant activity clusters consisted solely from the activity patterns characterized by power decrease). The most prominent examples are the clusters faces-2 (Figure 8b), animals-2 (Figure 8a), tools-2, pseudowords-1, pseudowords-2 (Figure 8c), scrambled-1 and scrambled-2 (Figure 8d). For example, to decode face or pseudowords from the activity of the blue cluster network, the RF classifier used broadband gamma power decreases located in posterior inferior temporal cortex and inferior occipital gyrus. None of the probes for which the decrease in activity was identified as important for decoding were located in classically defined Default Mode Network (Buckner, Andrews-Hanna, and Schacter, 2008; Raichle, 2015).\nAcross all categories, the earliest component that often appeared in clusters was the brief power increase (mean non-zero power increase was 2.8 times the baseline in the region of interest) in the low-frequency interval (4-25 Hz), which for one group of probes can be associated to an almost instantaneous broadband gamma power increase (8b, cluster 3, mean broadband gamma increase of 1.9 times the baseline), but remains the only source of important activity for another group of probes (8b, cluster 1).\nStudying the anatomical locations of the probes belonging to different clusters of activity revealed interesting observations. Figure 8c, pseudowords, clusters 1 and 3 show a clear example how clustering by activity patterns leads to assigning the probes into functionally different anatomical areas. The gamma-band increase signature captured by cluster 3 occurs only in the left hemisphere (red markers on Figure 8c), the late theta-alpha power decrease captured by cluster 1 also occurs only in the left hemisphere (green markers) and is spatially clearly distinct from probes in cluster 3. Because it is known that pseudoword stimuli elicit top-down language-related (orthographic, phonological and semantic) analysis, which elicits highly leftlateralized networks identifiable in iEEG recordings (Juphard et al., 2011; Mainy et al., 2008), we know that this observation reflects a functional brain process. This dissociation in both the spectrotemporal and anatomical domains provides us with valuable data on the locations and associated activity patterns emerging during automatic perceptual categorization and\n61\nhighlights the benefit of disentangling the activity into functionally and anatomically disassociated clusters.\nFinally, the relevance of the different components in the TF domain for the Random Forest classification process was assessed. Specifically, we tested whether the activity in the broadband gamma range, commonly present on most clusters across categories, is in general the most valuable neural signature for category networks as compared to the low-frequency parts of the spectrum. To test whether broadband gamma was solely the most informative frequency interval we statistically compared predictive power of three intervals: broadband gamma (50 \u2212 150 Hz), low-frequency (4 \u2212 50 Hz) and full spectrum (4 \u2212 150 Hz). Overall, across 7 perceptual categories out of 8 (except for scenes), using the full spectrum was more informative than using the broadband gamma interval or the low-frequency interval alone (Mann\u2013Whitney U test, p < 0.001563, corrected to the number of clusters compared, see Figure 9), which is in line with the results reported by K. J. Miller et al. (2016). Importantly, for scrambled images and faces the broadband gamma carried less (Mann-Whitney U p < 1e\u22127, corrected) decoding-relevant information than the lower frequencies.\n62"}, {"heading": "3.5 Significance of bottom-up approach to the analysis of human intracerebral neural activity", "text": "In this chapter we explored the bottom-up approach to the analysis of human intracerebral neural activity. Due to a rich dataset and powerful methodology we were able to uncover facts about neural processing of automatic visual categorization that we would not necessarily address in a hypothesis-driven study. Previous works have shown where and when perceptual category information can be decoded from the human brain, our study adds to that line of research by identifying spectrotemporal patterns that contribute to category decoding without the need to formulate a priori hypothesis on which spectral components and at which times are worth investigating.\nThe classifier model first allowed us to globally identify two types of neural responses: those that were predictive of a certain category and those that did not predict any category despite eliciting strong amplitude modulation across multiple frequency bands. Surprisingly, when comparing the level of predictability of probe responses we found that only 4.8% of the responsive probes were predictive of a category. This very low percentage highlights an important fact regarding the level of \u201cselectivity\u201d of a neural responses. In this decoding approach, the level of single-probe neural response selectivity depends on the diversity and overall quantity of the comparison/reference group to which it is compared to. Stimulus-induced neural signal selectivity is thus a graded quality that can be assessed through multiple comparisons with a broad variety of stimulation conditions. This result also implies that although any stimulus can elicit a local neural response throughout the cerebral cortex, in the light of our results, there is a high probability of it being non-predictive of any of the categories or being polypredictive of several categories at once.\nIn line with a vast literature on the localization of category related networks (Kanwisher, McDermott, and Chun, 1997; Epstein, A. Harris, et al., 1999; Malach et al., 1995; Haxby, Gobbini, et al., 2001; Ishai, Ungerleider, et al., 1999; Cohen et al., 2000; Peelen, Fei-Fei, and Kastner, 2009; GrillSpector and Weiner, 2014; Tanaka, 1996; DiCarlo, Zoccolan, and Rust, 2012) predictive probes concentrated mostly in the inferior temporal cortex, namely the fusiform gyrus (BA 37), yet surprisingly for some categories, probes in primary visual cortex were also predictive of these categories. This effect is probably related to the specifics of the physical content of certain images that uniquely characterize certain categories amongst all others, as for example the content in high-contrast edge information in scrambled and written text stimuli.\nPredictive probes were subsequently classified according to their level of selectivity towards a single or multiple visual categories. Polypredictive\n63\nprobes (36%) clustered in visual cortices and inferior temporal cortex and were associated with early spectral components (< 300 ms) such as broadband gamma power increases and a transient theta burst shortly after stimulus presentation. Monopredictive probes (64%) were abundant in these same regions, but extending uniquely in frontal, parietal, superior temporal and anterior limbic cortex. Their activity was strongly associated with the later (> 300 ms) time and with power suppression of spectral importance features, versus baseline, in the theta (4 \u2212 7 Hz), alpha (8 \u2212 15 Hz) and beta bands (16\u221240 Hz). In a subgroup of probes the associated power suppression of the feature importances extended into the broad gamma band (50\u2212 150 Hz).\nImportantly, the capacity to ascribe category selectivity to predictive probes (mono vs polypredictive probes) arises from the fact that the decoding model was trained to discriminate between all 8 categories simultaneously. The separation between mono and polypredictive probes revealed specific effects in terms of network localization and time-frequency components. The high concentration of polypredictive probes (and local networks) in early visual cortices, from primary visual cortex up to inferior temporal cortex is coherent with the idea that networks in the ventral visual stream progressively integrate more complex features into object representations, thus becoming progressively more selective, and converge within median temporal lobe to more stimulus-invariant representations (R. Q. Quiroga et al., 2005). This progressive information integration by spectral features of neuronal responses across the visual hierarchy has been recently connected with the computations carried out by deep convolutional neural networks trained to solve the task of visual recognition (Kuzovkin et al., 2018).\nGlobally, the random forest data classification provided results that are coherent with current knowledge on 1) the implication of networks located in visual cortex and inferior temporal cortex in processing visual categories, 2) the timing of object categorization in the human brain and 3) the role of broadband gamma responses in processing category-selective information within these networks. Previous studies have shown that certain stimulus categories elicit clustered cortical responses of highly localized networks in the occipito-temporal ventral stream such as the fusiform-face-area (FFA) and the visual-word-form area (VWFA) (Kanwisher, McDermott, and Chun, 1997; Cohen et al., 2000). Yet, other studies have broadened this scope by showing that certain categories, as for example faces, rely on the involvement of a larger brain-wide distributed network (Ishai, Schmidt, and Boesiger, 2005; Vidal, Tom\u00e0s Ossand\u00f2n, et al., 2010). Our classification analysis shows that the spatial extent of this network distribution is category specific, certain stimuli eliciting larger network responses, such as for faces, animals and pseudowords, as compared to scenes, houses and scrambled images which concentrate in the fusiform cortex, the parahip-\n64\npocampal cortex and primary visual cortex respectively. Our results largely agree with previous works trying to decode visual object categories over time with magnetoencephalography (MEG) (T. Carlson et al., 2013; Radoslaw Martin Cichy, Pantazis, and Oliva, 2014) or intracranial recordings (Liu et al., 2009). All these studies converge on the result that perceptual categories can be decoded from human brain signals as early as 100 ms. Our current work goes a step beyond these previous investigations by demonstrating which spectral components underlie this fast decoding. Previous intracranial studies have also shown that broadband gamma is modulated by information about object categories (Vidal, Tom\u00e0s Ossand\u00f2n, et al., 2010; Privman et al., 2007; Fisch et al., 2009). Moreover, broadband gamma has been suggested as a proxy to population spiking output activity (Manning et al., 2009; Ray and Maunsell, 2011; Lachaux, Axmacher, et al., 2012; Ray, Crone, et al., 2008). It has since then been considered as a hallmark of local population processing (Parvizi and Kastner, 2018). Our classification results however show that broadband gamma is not the sole selectivity marker of functional neural processing, and that higher decoding accuracy can be achieved by including low-frequency components of the spectrum. For certain stimulus categories, as scrambled images, the broadband gamma range is even outperformed by the predictive power of the low-frequency range.\nTo understand which spectral components play a specific role in stimulus categorization we analyzed the decision process that drives the decoding model and identified the combined spectrotemporal regions that are informative for the output of the random forest classification procedure. This allowed us 1) to identify the category-selective spectral components of high importance for the automatic visual categorization process, and 2) identify the correlates functional involvement of positive as well as negative power modulations (increases and decreases versus baseline) in early and late time windows of neural processing involved in visual categorization.\nWhile the distinctive activity of polypredictive neural locations is mostly reflected by early TF components (i.e. broadband gamma and theta burst in faces), the sustained decrease in power in the alpha/beta band was extended in space and time. This process is probably dependent on the degree of difficulty for the networks in reaching a perceptual decision and which appeals to the involvement of top-down processing required to resolve perceptual ambiguity elicited by the different stimulus categories. For example, animal and tool stimuli are highly diverse in their physical image structure, as compared to face stimuli. This affects the efficiency of bottom-up process in extracting category information, often associated with increase in gamma activity, and probably in parallel triggers top-down processes through selective activity modulation in low-frequency channels (Bastos et al., 2015). In our data, this latter phenomenon could be mirrored by a de-\n65\ncrease of predictive power in the low-frequency range. Studies have shown that power modulations reflect changes in network connectivity (Tewarie et al., 2018) and that top-down processes, eliciting a decrease in power in the alpha-beta band, are accompanied by an increase in distant network connectivity (Gaillard et al., 2009).\nFinally, we also show that certain probes elicit decreased broadband gamma responses (versus baseline) while representing a significant feature importance for the classification model. It has been shown that neural activity in the Default Mode Network can be negatively modulated by attending sensory stimulation (Buckner, Andrews-Hanna, and Schacter, 2008), and intracranial studies have found that this was reflected by decreases (versus baseline) in the broad gamma range (Tomas Ossand\u00f2n et al., 2011; Jerbi et al., 2010; Dastjerdi et al., 2011). Here we found no evidence of such power decreases in probes located in the DMN (Buckner, Andrews-Hanna, and Schacter, 2008). However, the random forest classifier singled-out broad spectral patterns of power decreases at probes located in visual regions and beyond for categories faces, pseudowords and characters. This is the first time, to our knowledge, that power decreases in the broadband gamma range outside the DMN have been associated with highly functional neural signal classification of perceptual categories. Their functional significance should be studied in the future as they could reflect an important phenomenon of communication regulation between networks during perceptual decision making of visual categories.\nExpanding on this work and established methodology by including more subject data in the future might allow us to make a transition from the observations of local activity and the analysis of its role to being able to detect signatures of global decision-making processes. It is possible that these signatures would be reflected in specific spectral fingerprints as many classic theories would suggest (Rodriguez et al., 1999; F. Varela et al., 2001; Engel, Fries, and Singer, 2001; Siegel, Donner, and Engel, 2012). The methodology proposed in this study can facilitate the search of those fingerprints without the need to formulate a priori hypothesis about which spectrotemporal components are worth investigating.\n66\nChapter 4\nRepresentational similarities between biological visual processing and artificial neural networks inform on structural organization of human visual cortex\nIn the previous chapter we have demonstrated a way to interpret an automatically built model to gain knowledge about neurological mechanisms on the level of local field potentials. In this chapter we move to a higher level of abstraction and employ machine learning and interpretability to peek into functional organization of human visual cortex. Using a metric based on representational similarity analysis we compare the activations of biological neurons in the layers of human visual cortex with the activations of artificial neurons in the layers of a deep convolutional neural network. This comparison allows us to find an alignment between those two hierarchical structures and to investigate which spectrotemporal regions of human brain activity are aligned the best, confirming the role of high gamma activity in visual processing. Using deconvolution technique to interpret the behavior of the DCNN we were able to visualize visual inputs that the artificial neurons are tuned to and observe the similarities with the reported tuning of biological neurons when processing visual inputs."}, {"heading": "4.1 The search for the model of human visual system", "text": "Biological visual object recognition is mediated by a hierarchy of increasingly complex feature representations along the ventral visual stream (DiCarlo, Zoccolan, and Rust, 2012). Intriguingly, these transformations are\n67\nmatched by the hierarchy of transformations learned by deep convolutional neural networks (DCNN) trained on natural images (G\u00fc\u00e7l\u00fc and Gerven, 2015). It has been shown that DCNN provides the best model out of a wide range of neuroscientific and computer vision models for the neural representation of visual images in high-level visual cortex of monkeys (Daniel LK Yamins, Hong, et al., 2014) and humans (Khaligh-Razavi and Kriegeskorte, 2014). Other studies with functional magnetic resonance imaging (fMRI) data have demonstrated a direct correspondence between the hierarchy of the human visual areas and layers of the DCNN (G\u00fc\u00e7l\u00fc and Gerven, 2015; Eickenberg et al., 2016; Seibert et al., 2016; Radoslaw Martin Cichy, Khosla, et al., 2016). In sum, the increasing feature complexity of the DCNN corresponds to the increasing feature complexity occurring in visual object recognition in the primate brain (Kriegeskorte, 2015; Daniel LK Yamins and DiCarlo, 2016).\nHowever, fMRI based studies only allow one to localize object recognition in space, but neural processes also unfold in time and have characteristic spectral fingerprints (i.e. frequencies). With time-resolved magnetoencephalographic recordings it has been demonstrated that the correspondence between the DCNN and neural signals peaks in the first 200 ms (Radoslaw Martin Cichy, Khosla, et al., 2016; Seeliger et al., 2017). Here we test the remaining dimension: that biological visual object recognition is also specific to certain frequencies. In particular, there is a long-standing hypothesis that especially gamma band (30\u2212150 Hz) signals are crucial for object recognition (Singer and Gray, 1995; Singer, 1999; Fisch et al., 2009; Tallon-Baudry, Bertrand, et al., 1997; Tallon-Baudry and Bertrand, 1999; Lachaux, Rodriguez, et al., 1999; Wyart and Tallon-Baudry, 2008; Lachaux, George, et al., 2005; Vidal, Chaumon, et al., 2006; Herrmann, Munk, and Engel, 2004; Srinivasan et al., 1999; Levy et al., 2015). More modern views on gamma activity emphasize the role of the gamma rhythm in establishing a communication channel between areas (Fries, 2005; Fries, 2015). Further research has demonstrated that especially feedforward communication from lower to higher visual areas is carried by the gamma frequencies (Van Kerkoerle et al., 2014; Bastos et al., 2015; Michalareas et al., 2016). As the DCNN is a feedforward network one could expect that the DCNN will correspond best with the gamma band activity. In this work we used the DCNN as a computational model to assess whether signals in the gamma frequency are more relevant for object recognition than other frequencies.\nTo empirically evaluate whether gamma frequency has a specific role in visual object recognition we assessed the alignment between the responses of layers of a commonly used DCNN and the neural signals in five distinct frequency bands and three time windows along the areas constituting the ventral visual pathway. Based on the previous findings we expected that: mainly gamma frequencies should be aligned with the layers of the DCNN;\n68\nthe correspondence between the DCNN and gamma should be confined to early time windows; the correspondence between gamma and the DCNN layers should be restricted to visual areas. In order to test these predictions we capitalized on direct intracranial depth recordings from 100 patients with epilepsy and a total of 11293 electrodes implanted throughout the cerebral cortex.\nWe observe that activity in the gamma range along the ventral pathway is statistically significantly aligned with the activity along the layers of DCNN: gamma (31\u2212 150 Hz) activity in the early visual areas correlates with the activity of early layers of DCNN, while the gamma activity of higher visual areas is better captured by the higher layers of the DCNN. We also find that while the neural activity in the theta range (5 \u2212 8 Hz) is not aligned with the DCNN hierarchy, the representational geometry of theta activity is correlated with the representational geometry of higher layers of DCNN."}, {"heading": "4.2 Simultaneous recordings of human intracortical responses and of responses of an artificial neural network to the same visual stimuli", "text": "The dataset that was created for this study consists of two components: recordings of local field potentials in human visual cortex and activations of artificial neurons of a deep convolutional neural network trained on a visual recognition task. The raw neurological data was the same as the one used in Chapter 3, please refer to section 3.2 for the technical details on the subjects and data acquisition parameters. The preprocessing pipeline was mostly similar to the one performed in the previous study, however there were a few differences, please see the section below. Further in this section we present the protocol we used to obtain activations of artificial neurons once the artificial neural network was presented with the same visual stimuli as the the human subjects."}, {"heading": "4.2.1 Processing of neural data", "text": "The final dataset consists of 2823250 local field potential (LFP) recordings \u2013 11293 electrode responses to 250 stimuli. To remove the artifacts the signals were linearly detrended and the recordings that contained values \u2265 10\u03c3images, where \u03c3images is the standard deviation of responses (in the time window from \u2212500 ms to 1000 ms) of that particular probe over all stimuli, were excluded from data. All electrodes were re-referenced to a bipolar reference. For every electrode the reference was the next electrode on the same rod following the inward direction. The electrode on the deepest end of each rod was excluded from the analysis. The signal was segmented\n69\nin the range from \u2212500 ms to 1000 ms, where 0 marks the moment when the stimulus was shown. The \u2212500 to \u2212100 ms time window served as the baseline. There were three time windows in which the responses were measured: 50\u2212 250 ms, 150\u2212 350 ms and 250\u2212 450 ms.\nWe analyzed five distinct frequency bands: \u03b8 (5\u22128 Hz), \u03b1 (9\u221214 Hz), \u03b2 (15\u2212 30 Hz), \u03b3 (31\u2212 70 Hz) and \u0393 (71\u2212 150 Hz). To quantify signal power modulations across time and frequency we used standard time-frequency (TF) wavelet decomposition (Daubechies, 1990). The signal s(t) is convoluted with a complex Morlet wavelet w(t, f0), which has Gaussian shape in time (\u03c3t) and frequency (\u03c3f ) around a central frequency f0 and defined by \u03c3f = 1/2\u03c0\u03c3t and a normalization factor. In order to achieve good time and frequency resolution over all frequencies we slowly increased the number of wavelet cycles with frequency ( f0 \u03c3f was set to 6 for high and low gamma, 5 for beta, 4 for alpha and 3 for theta). This method allows obtaining better frequency resolution than by applying a constant cycle length (Delorme and Makeig, 2004). The square norm of the convolution results in a time-varying representation of spectral power, given by: P (t, f0) = |w(t, f0) \u00b7 s(t)|2.\nFurther analysis was done on the electrodes that were responsive to the visual task. We assessed neural responsiveness of an electrode separately for each region of interest \u2013 for each frequency band and time window we compared the average post-stimulus band power to the average baseline power with aWilcoxon signed-rank test for matched-pairs. All p-values from this test were corrected for multiple comparisons across all electrodes with the false discovery rate procedure (Genovese, Lazar, and Nichols, 2002). In the current study we deliberately kept only positively responsive electrodes, leaving the electrodes where the post-stimulus band power was lower than the average baseline power for future work. Table 2 contains the numbers of electrodes that were used in the final analysis in each of 15 regions of interest across the time and frequency domains.\nEach electrode\u2019s Montreal Neurological Institute coordinate system (MNI) coordinates were mapped to a corresponding Brodmann brain area (Brodmann, 1909) using Brodmann area atlas contained in MRICron (Rorden, 2007) software.\nTo summarize, once the neural signal processing pipeline is complete, each electrode\u2019s response to each of the stimuli is represented by one number\n70\n\u2013 the average band power in a given time window normalized by the baseline. The process is repeated independently for each time-frequency region of interest."}, {"heading": "4.2.2 Processing of DCNN data", "text": "We feed the same images that were shown to the test subjects to a deep convolutional neural network (DCNN) and obtain activations of artificial neurons (nodes) of that network. We use Caffe (Jia et al., 2014) implementation of AlexNet (Krizhevsky, Sutskever, and G. E. Hinton, 2012) architecture (see Figure 14) trained on ImageNet (Russakovsky et al., 2015) dataset to categorize images into 1000 classes. Although the image categories used in our experiment are not exactly the same as the ones in the ImageNet dataset, they are a close match and DCNN is successful in labelling them.\nThe architecture of the AlexNet artificial network can be seen on Figure 14. It consists of 9 layers. The first is the input layer, where one neuron corresponds to one pixel of an image and activation of that neuron on a scale from 0 to 1 reflects the color of that pixel: if a pixel is black, the corresponding node in the network is not activated at all (value is 0), while a white pixel causes the node to be maximally activated (value 1). After the input layer the network has 5 convolutional layers referred to as conv1-5. A convolutional layer is a collection of filters that are applied to an image. Each filter is a 2D arrangement of weights that represent a particular visual pattern. A filter is convolved with the input from the previous layer to produce the activations that form the next layer. For an example of a visual pattern that a filter of each layer is responsive to, please see Figure 14b. Each layer consists of multiple filters and we visualize only one per layer for illustrative purposes. A filter is applied to every possible position on an input image and if the underlying patch of an image coincides with the pattern that the filter represents, the filter becomes activated and translates this activation to the artificial neuron in the next layer. That way, nodes of conv1 tell us where on the input image each particular visual pattern occurred. Figure 14b shows an example output feature map produced by a filter being applied to the input image. Hierarchical structure of convolutional layers gives rise to the phenomenon we are investigating in this work \u2013 increase of complexity of visual representations in each subsequent layer of the visual hierarchy: in both the biological and artificial systems. Convolutional layers are followed by 3 fully-connected layers (fc6-8). Each node in a fully-connected layer is, as the name suggests, connected to every node of the previous layer allowing the network to decide which of those connections are to be preserved and which are to be ignored. For both convolutional and fully-connected layers we can apply deconvolution (Zeiler and Fergus, 2014)\n71\ntechnique to map activations of neurons in those layers back to the input space. This visualization gives better understanding of inner workings of a neural network. Examples of deconvolution reconstruction for each layer are given in Figure 14b.\nFor each of the images we store the activations of all nodes of DCNN. As the network has 9 layers we obtain 9 representations of each image: the image itself (referred to as layer 0) in the pixel space and the activation values of each of the layers of DCNN. See the step 2 of the analysis pipeline on Figure 10 for the cardinalities of those feature spaces."}, {"heading": "4.3 The mapping between the Brodmann areas and layers of a Deep Convolutional Neural Network", "text": "As a result of the preprocessing steps we were left with two sets of responses to the same set of stimuli: one from a biological system, one from an artificial one. Our ultimate goal was to compare those responses, but since the representations were very different a direct comparison was not possible. To overcome this we used representational similarity analysis \u2013 a technique that relies on the distance measure between the data samples (see taxonomy in Table 1 of Section 2.1.3) to provide a way to compare behaviors of two systems under the same set of stimuli while having different data representations."}, {"heading": "4.3.1 Mapping neural activity to the layers of DCNN", "text": "Once we extracted the features from both neural and DCNN responses our next goal was to compare the two and use a similarity score to map the brain area where a probe was located to a layer of DCNN. By doing that for every probe in the dataset we obtained cross-subject alignment between visual areas of human brain and layers of DCNN. There are multiple deep neural network architectures trained to classify natural images. Our choice of AlexNet does not imply that this particular architecture corresponds best to the hierarchy of visual layers of human brain. It does, however, provide a comparison for hierarchical structure of human visual system and was selected among other architectures due to its relatively small size and thus easier interpretability.\nRecent studies comparing the responses of visual cortex with the activity of DCNN have used two types of mapping methods. The first type is based on linear regression models that predict neural responses from DCNN activations (Daniel LK Yamins, Hong, et al., 2014; G\u00fc\u00e7l\u00fc and Gerven, 2015). The second is based on representational similarity analysis (RSA) (Kriegeskorte, Mur, and P. A. Bandettini, 2008). We used RSA to\n72\ncompare distances between stimuli in the neural response space and in the DCNN activation space (Radoslaw M Cichy et al., 2016).\nWe built a representation dissimilarity matrix (RDM) of size number of stimuli \u00d7 number of stimuli (in our case 250\u00d7 250) for each of the probes and each of the layers of DCNN. Note that this is a non-standard approach: usually the RDM is computed over a population (of voxels, for example), while we do it for each probe separately. We use the non-standard approach because often we only had 1 electrode per patient per brain area. Given a matrix RDMfeature space a value RDMfeature spaceij in the ith row and jth column of the matrix shows the Euclidean distance between the vectors vi and vj that represent images i and j respectively in that particular feature space. Note that the preprocessed neural response to an image in a given frequency band and time window is a scalar, and hence correlation distance is not applicable. Also, given that DCNNs are not invariant to the scaling of the activations or weights in any of its layers, we preferred to use closeness in Euclidean distance as a more strict measure of similarity. In our case there are 10 different feature spaces in which an image can be represented: the original pixel space, 8 feature spaces for each of the layers of the DCNN and one space where an image is represented by the preprocessed neural response of probe p. For example, to analyze region of interest of high gamma in 50 \u2212 250 ms time window we computed 504 RDM matrices on the neural responses \u2013 one for each positively responsive electrode in that region of interest (see Table 2), and 9 RDM matrices on the activations of the layers of DCNN. A pair of a frequency band and a time window, such as \u201chigh gamma in 50-250 ms window\u201d is referred to as region of interest in this work.\nThe second step was to compare the RDMprobe p of each probe p to RDMs of layers of DCNN. We used Spearman\u2019s rank correlation as measure of similarity between the matrices:\n\u03c1probe player l = Spearman(RDM probe p,RDMlayer l). (4.1)\nAs a result of comparing RDMprobe p with every RDMlayer l we obtain a vector with 9 scores: (\u03c1pixels, \u03c1conv1, . . . , \u03c1fc8) that serves as a distributed mapping of probe p to the layers of DCNN (see step 5 of the analysis pipeline on Figure 10). The procedure is repeated independently for each probe in each region of interest. To obtain an aggregate score of the correlation between an area and a layer the \u03c1 scores of all individual probes from that area are summed and divided by the number of \u03c1 values that have passed the significance criterion. The data for the Figures 11 and 13 are obtained in such manner.\nFigure 15 presents the results of applying RSA within the DCNN to compare the similarity of representational geometry between the layers.\n73\n250 images\nbaseline signal\nLF P\nre sp\non se\nto\nea ch\nim ag\ne\n29 04\n00 n\nod es\n72 23\n4 pi\nxe ls\nav er\nag e\nba nd\na ct\niv ity\nof\nre sp\non si\nve p\nro be\np\nbaseline signal\n250 images\nRDM of responses 25\n0 im\nag es\nrpixp r conv1 p r conv2 p r conv3 p r conv4 p r conv5 p r fc6 p r fc7 p r fc8 p\nSpearman correlations between probe RDM and DCNN RDMs\nRDMs in the pixel space and each layer\u2019s space 250\n25 0\nprobe p implanted in a test subject\nProbe p is mapped to a\nBrodmann area\n10,000 permutations to assess the significance of correlation between RDMs\nX X\nMapping of brain areas to layers of DCNN\nbipolar reference\n\u2014\nMNI coordinates of\nprobe p\nStep 3\nStep 1 Step 2\nStep 4\nStep 5\n18 66\n24 n\nod es\n64 89\n6 no\nde s\n64 89\n6 no\nde s\n43 26\n4 no\nde s\n40 96\nn od\nes\n40 96\nn od\nes\n10 00\nn od\nes\n\u03b3\nFigure 10. Overview of the analysis pipeline where 250 natural images are presented to human subjects and to an artificial vision system. The activities elicited in these two systems are compared to map regions of human visual cortex to layers of deep convolutional neural network. Step 1: LFP response of each of 11293 electrodes to each of the images is converted into the frequency domain. Activity evoked by each image is compared to the activity evoked by every other image and results of this comparison are presented as a representational dissimilarity matrix (RDM). Step 2: Each of the images is shown to a pre-trained DCNN and activations of each of the layers are extracted. Each layer\u2019s activations form a representation space, in which stimuli (images) can be compared to each other. Results of this comparison are summarized as a RDM for each DCNN layer. Step 3: Subject\u2019s intracranial responses to stimuli are randomly reshuffled and the analysis performed in step 1 is repeated 10000 times to obtain 10000 random RDMs for each electrode. Step 4: Each electrode\u2019s MNI coordinates are used to map the electrode to a Brodmann area. The figure also gives an example of electrode implantation locations in one of the subjects (blue circles are the electrodes). Step 5: Spearman\u2019s rank correlation is computed between the true (non-permuted) RDM of neural responses and RDMs of each layer of DCNN. Also 10000 scores are computed with the random RDM for each electrode-layer pair to assess the significance of the true correlation score. If the score obtained with the true RDM is significant (the value of p < 0.001 is estimated by selecting a threshold such that none of the probes would pass it on the permuted data), then the score is added to the mapping matrix.The procedure is repeated for each electrode and the correlation scores are summed and normalized by the number of electrodes per Brodmann area. The resulting mapping matrix shows the alignment between the consecutive areas of the ventral stream and layers of DCNN.\n74\nTo assess the statistical significance of the correlations between the RDM matrices we ran a permutation test. In particular, we reshuffled the vector of brain responses to images 10000 times, each time obtaining a dataset where the causal relation between the stimulus and the response is destroyed. On each of those datasets we ran the analysis and obtained Spearman\u2019s rank correlation scores. To determine score\u2019s significance we compared the score obtained on the original (unshuffled) data with the distribution of scores obtained with the surrogate data. If the score obtained on the original data was bigger than the score obtained on the surrogate sets with p < 0.001 significance, we considered the score to be significantly different. The threshold of p = 0.001 is estimated by selecting such a threshold that on the surrogate data none of the probes would pass it.\nTo size the effect caused by training artificial neural network on natural images we performed a control where the whole analysis pipeline depicted on Figure 10 is repeated using activations of a network that was not trained \u2013 its weights are randomly sampled from a Gaussian distributionN (0, 0.01).\nFor the relative comparison of alignments between the bands and the noise level estimation we took 1,000 random subsets of half of the size of the dataset. Each region of interest was analyzed separately. The alignment score was calculated for each subset, resulting in 1,000 alignment estimates per region of interest. This allowed us to run a statistical test between each pair of regions of interest to test the hypothesis that the DCNN alignment with the probe responses in one band is higher than the alignment with the responses in another band. We used Mann-Whitney U test (Mann and Whitney, 1947) to test that hypothesis and accepted the difference as significant at p-value threshold of 0.005 Bonferroni corrected (Dunn, 1961) to 2.22e\u22125."}, {"heading": "4.3.2 Quantifying properties of the mapping", "text": "To evaluate the results quantitatively we devised a set of measures specific to our analysis. Volume is the total sum of significant correlations (see Equation 4.1) between the RDMs of the subset of layers L and the RDMs of the probes in the subset of brain areas A:\nV areas Alayers L = \u2211\na\u2208A\n\u2211\nl\u2208L\n\u2211\np\u2208Sal\n\u03c1probe player l , (4.2)\nwhere A is a subset of brain areas, L is a subset of layers, and Sal is the set of all probes in area a that significantly correlate with layer l.\nWe express volume of visual activity as\nV A={17,18,19,37,20} L=all layers , (4.3)\n75\nwhich shows the total sum of correlation scores between all layers of the network and the Brodmann areas that are located in the ventral stream: 17, 18, 19, 37, and 20.\nVisual specificity of activity is the ratio of volume in visual areas and volume in all areas together, for example visual specificity of all of the activity in the ventral stream that significantly correlates with any of layers of DCNN is\nS A={17,18,19,37,20} L=all layers =\nV A={17,18,19,37,20} L=all layers\nV A=all areasL=all layers (4.4)\nThe measures so far did not take into account hierarchy of the ventral stream nor the hierarchy of DCNN. The following two measures are the most important quantifiers we rely on in presenting our results and they do take hierarchical structure into account.\nThe ratio of complex visual features to all visual features is defined as the total volume mapped to layers conv5, fc6, fc7 divided by the total volume mapped to layers conv1, conv2, conv3, conv5, fc6, fc7:\nCA = V AL={conv5,fc6,fc7}\nV AL={conv1,conv2,conv3,conv5,fc6,fc7} . (4.5)\nNote that for this measure layers conv4 and fc8 are omitted: layer conv4 is considered to be the transition between the layers with low and high complexity features, while layer fc8 directly represents class probabilities and does not carry visual representations of the stimuli (if only on very abstract level).\nFinally, the alignment between the activity in the visual areas and activity in DCNN is estimated as Spearman\u2019s rank correlation between two vectors each of length equal to the number of probes with RDMs that significantly correlate with an RDM of any of DCNN layers. The first vector is a list of Brodmann areas BAp to which a probe p belong if its activity representation significantly correlates with activity representation of a layer l:\nAalign = { BAp | \u2200p \u2203 l : \u03c1(RDMp,RDMl)\nis significant according to the permutation test\n} . (4.6)\nA is ordered by the hierarchy of the ventral stream: BA17, BA18, BA19, BA37, BA20. Areas are coded by integer range from 0 to 4. The second vector lists DCNN layers Lp to which the very same probes p were assigned:\nLalign = { Lp | \u2200p \u2203 l : \u03c1(RDMp,RDMl)\nis significant according to the permutation test\n} . (4.7)\nLayers of DCNN are coded by integer range from 0 to 8. We denote Spearman rank correlation of those two vectors as alignment\n\u03c1align = Spearman(Aalign,Lalign). (4.8)\n76\nWe note that although the hierarchy of the ventral stream is usually not defined through the progression of Brodmann areas, such ordering nevertheless provides a reasonable approximation of the real hierarchy (Lerner et al., 2001; Grill-Spector and Malach, 2004). As both the ventral stream and the hierarchy of layers in DCNN have an increasing complexity of visual representations, the relative ranking within the biological system should coincide with the ranking within the artificial system. Based on the recent suggestion that significance levels should be shifted to 0.005 (Dienes, A. Field, et al., 2017) and after Bonferroni-correcting for 15 time-frequency windows we accepted alignment as significant when it passed p < 0.0003(3)."}, {"heading": "4.4 Alignment between the layers of the DCNN and layers of human visual cortex", "text": "This section present the results and observations that were achieved by comparing the two systems of vision. Here is a brief summary of our findings: activity in gamma band is aligned better than other frequencies to the hierarchical structure of a deep convolutional neural network, this alignment is mostly attributed to having two types of layer in DCNN: convolutional, that are representationally more similar to the activity of early visual areas, and fully connected layers, that are more similar to later visual and temporal areas of the ventral stream. The section describes the evidence in favor of those conclusion and presents more granular and deeper analysis focusing on specific areas of visual cortex and layers of the DCNN."}, {"heading": "4.4.1 Activity in gamma band is aligned with the DCNN", "text": "We tested the hypothesis that gamma activity has a specific role in visual object recognition compared to other frequencies. To that end we assessed the alignment of neural activity in different frequency bands and time windows to the activity of layers of a deep convolutional neural network (DCNN) trained for object recognition. In particular, we used RSA to compare the representational geometry of different DCNN layers and the activity patterns of different frequency bands of single electrodes (see Figure 10). We consistently found that signals in low gamma (31\u221270 Hz) frequencies across all time windows and high gamma (71 \u2212 150 Hz) frequencies in 150 \u2212 350 ms window are aligned with the DCNN in a specific way: increase of the complexity of features along the layers of the DCNN was roughly matched by the transformation in the representational geometry of responses to the stimuli along the ventral stream. In other words, the lower and higher layers of the DCNN explained gamma band signals from earlier and later visual areas, respectively. Figure 11a illustrates assignment of neural activity in\n77\n78\nlow gamma band and Figure 11b the high gamma band to Brodmann areas and layers of DCNN. Most of the activity was assigned to visual areas (areas 17, 18, 19, 37, 20). Focusing on visual areas revealed a diagonal trend that illustrates the alignment between ventral stream and layers of DCNN (see\nFigure 13)."}, {"heading": "4.4.2 Activity in other frequency bands", "text": "To test the specificity of gamma frequency in visual object recognition, we assessed the alignment between the DCNN and other frequencies. Our findings across all subjects, time windows and frequency bands are summarized on Figure 12a. We note that the alignment in the gamma bands is also present at the single-subject level (see supplementary Figure 23 and supplementary materials B.2). Apart from the alignment we looked at the total amount of correlation and its specificity to visual areas. Figure 12b shows the volume of significantly correlating activity was highest in the high gamma range. Remarkably, 97% of that activity was located in visual areas, which is confirmed by Figure 11 where we see that in the gamma range only a few electrodes were assigned to Brodmann areas that are not part of the ventral stream. The detailed mapping results for all frequency bands and time windows are presented in layer-to-area fashion on Figure 13.\nThe results in the right column of Table 3 show the alignment values and significance levels for a DCNN that is trained for object recognition\n79\non natural images. On the left part of Table 3 the alignment between the brain areas and a DCNN that has not been trained on object recognition (i.e. has random weights) is given for comparison. One can see that training a network to classify natural images drastically increases the alignment score \u03c1 and its significance. One can see that weaker alignment (that does not survive the Bonferroni correction) is present in early time window in theta and alpha frequency range. No alignment is observed in the beta band.\nIn order to take into account the intrinsic variability when comparing alignments of different bands between each other, we performed a set of tests to see which bands have statistically significantly higher alignment with DCNN than other bands. See the section 4.3.1 for details. The results of those tests are presented in Table 4. Based on these results we draw a set of statistically significant conclusions on how the alignment of neural responses with the activations of DCNN differs between frequency bands and time windows. In the low gamma range (31\u2212 70 Hz) we conclude that the alignment is larger than with any other band and that within the low\n80\n50 - 250 ms 150 - 350 ms 250 - 450 ms\n\u03b8 5\n- 8 H\nz \u03b1\n9 - 1\n4 H\nz \u03b21\n5 - 3\n0 H\nz \u03b3 3\n1 - 7\n0 H\nz \u03937\n1 - 1\n50 H\nz\nA lig\nnm en\nt: 0.\n22 57\n(p -v\nal ue\n: 0 .0\n02 32\n) A\nlig nm\nen t:\n0. 33\n66 (p\n-v al\nue : 0\n.0 01\n04 )\nA lig\nnm en\nt: 0.\n41 66\n(p -v\nal ue\n: 0 .0\n03 98\n) A\nlig nm\nen t:\n0. 59\n79 (p\n-v al\nue : 0\n.0 00\n05 )\nA lig\nnm en\nt: 0.\n22 50\n(p -v\nal ue\n: 0 .0\n02 23\n)\nA lig\nnm en\nt: 0.\n13 96\n(p -v\nal ue\n: 0 .0\n88 49\n) A\nlig nm\nen t:\n0. 27\n20 (p\n-v al\nue : 0\n.1 31\n99 )\nA lig\nnm en\nt: 0.\n38 08\n(p -v\nal ue\n: 0 .1\n61 41\n) A\nlig nm\nen t:\n0. 53\n32 (p\n-v al\nue : 0\n.0 00\n00 )\nA lig\nnm en\nt: 0.\n32 00\n(p -v\nal ue\n: 0 .0\n00 00\n)\nA lig\nnm en\nt: 0.\n06 95\n(p -v\nal ue\n: 0 .7\n84 00\n) A\nlig nm\nen t:\nna n\n(p -v\nal ue\n: n an\n) A\nlig nm\nen t:\nna n\n(p -v\nal ue\n: n an\n) A\nlig nm\nen t:\n0. 52\n17 (p\n-v al\nue : 0\n.0 00\n02 )\nA lig\nnm en\nt: 0.\n26 88\n(p -v\nal ue\n: 0 .0\n00 48\n)\nF ig\nu re\n13 . M ap\npi ng\nof ac ti vi ty\nin vi su al\nar ea s to\nac ti va ti on\ns of\nla ye rs\nof D C N N\nac ro ss\nfiv e fr eq ue nc y ba\nnd s an\nd th re e ti m e w in do\nw s.\nV er ti ca l ax\nis ho\nld s B ro dm\nan n ar ea s in\nth e or de r of\nth e ve nt ra l st re am\n(t op\nto bo\ntt om\n), ho\nri zo nt al\nax is\nre pr es en ts\nth e su cc es si on of la ye rs of D C N N .N um be r in ea ch ce ll of a m at ri x is th e to ta ls um of co rr el at io ns (b et w ee n R D M s of pr ob es in th at pa rt ic ul ar ar ea an d th e R D M of th at la ye r) no rm al iz ed by th e nu m be r of si gn ifi ca nt ly co rr el at in g pr ob es in an ar ea . T he al ig nm en t sc or e is co m pu te d as Sp ea rm an \u2019s ra nk co rr el at io n be tw ee n el ec tr od e as si gn m en t to B ro dm an n ar ea s an d el ec tr od e as si gn m en t to D C N N la ye rs (E qu at io n 4. 8) . T he nu m be rs on th e le ft of ea ch su bp lo t sh ow th e nu m be r of si gn ifi ca nt ly co rr el at in g pr ob es in ea ch ar ea ou t of th e to ta ln um be r of re sp on si ve pr ob es in th at ar ea .\n81\ngamma the activity in early time window 50\u2212 250 ms is aligned more than in later windows. Alignment in the high gamma (71 \u2212 150 Hz) is higher than the alignment of \u03b8, but not higher than alignment of \u03b1. Within the high gamma band the activity in the middle time window 150\u2212350 ms has the highest alignment, followed by late 250 \u2212 450 ms window and then by the early activity in 50 \u2212 250 ms window. Outside the gamma range we conclude that theta band has the weakest alignment across all bands and that alignment of early alpha activity is higher than the alignment of early and late high gamma."}, {"heading": "4.4.3 Alignment is dependent on having two types of layers in DCNN", "text": "On figures 11 and 13 one can observe that sites in lower visual areas (17, 18) are mapped to DCNN layers 1 to 5 without a clear trend but are not mapped to layers 6-8. Similarly areas 37 and 20 are mapped to layers 6- 8 but not to 1-5. Hence we next asked whether the observed alignment is depending on having two different groups of visual areas related to two groups of DCNN layers. We tested this by computing alignment within the subgroups. We looked at alignment only between the lower visual areas (17, 18, 19) and the convolutional layers 1-5, and separately at the alignment between higher visual areas (37, 20) and fully connected layers of DCNN (6-8). We observed no significant alignment within any of the subgroups. So\n82\n50 -2\n50 m\ns\n15 0-\n35 0\nm s\n25 0-\n45 0\nm s\n\u03b8 \u03b1 5 - 8 Hz 9 - 14 Hz\n\u03b2 \u03b3 \u0393 15 - 30 Hz 31 - 70 Hz 71 - 150 Hz\n\u03b8 \u03b1 5 - 8 Hz 9 - 14 Hz\n\u03b2 \u03b3 \u0393 15 - 30 Hz 31 - 70 Hz 71 - 150 Hz\n\u03b8 \u03b1 5 - 8 Hz 9 - 14 Hz\n\u03b2 \u03b3 \u0393 15 - 30 Hz 31 - 70 Hz\n71 - 150 Hz \u03b8 \u03b1 5 - 8 Hz 9 - 14 Hz\n\u03b2 \u03b3 \u0393 15 - 30 Hz 31 - 70 Hz 71 - 150 Hz\n\u03b8 \u03b1 5 - 8 Hz 9 - 14 Hz\n\u03b2 \u03b3 \u0393 15 - 30 Hz 31 - 70 Hz 71 - 150 Hz\n\u03b8 \u03b1 5 - 8 Hz 9 - 14 Hz\n\u03b2 \u03b3 \u0393 15 - 30 Hz 31 - 70 Hz 71 - 150 Hz\n\u03b8 \u03b1 5 - 8 Hz 9 - 14 Hz\n\u03b2 \u03b3 \u0393 15 - 30 Hz 31 - 70 Hz 71 - 150 Hz\n\u03b8 \u03b1 5 - 8 Hz 9 - 14 Hz\n\u03b2 \u03b3 \u0393 15 - 30 Hz 31 - 70 Hz 71 - 150 Hz\n\u03b8 \u03b1 5 - 8 Hz 9 - 14 Hz\n\u03b2 \u03b3 \u0393 15 - 30 Hz 31 - 70 Hz 71 - 150 Hz\n22 7\n227\n3\n3\n11 11\n55\n55\n96\n96\n5 5\n27\n27\n25 6 2\n56\n3 3\n13\n13\n38 4 3\n84\n3 3\n13\n13\n38 4 3\n84\n3 3\n13\n13\n25 6\n1\n1\n40 96\n1\n1\n40 96\n1\n1\n10 00\npi xe\nls co\nnv 1\nco nv\n2 co\nnv 3\nco nv\n4 co\nnv 5\nfc 6\nfc 7\nfc 8\nA B C F ig\nu re\n14 . Sp\nec ifi ci ty\nof ne ur al\nre sp on\nse s to\nla ye rs\nof D C N N\nac ro ss\nfr eq ue nc y ba\nnd s an\nd ti m e w in do\nw s.\na. T he\nar ch it ec tu re\nof th e\nD C N N .C\non vo lu ti on\nal la ye r 1 co ns is ts\nof 96\nfe at ur e de te ct or s of\nsi ze\n1 1 \u00d7\n1 1 ,t he y ta ke\nas in pu\nt pi xe ls of\nth e im\nag e an\nd th ei r ac ti va ti on s cr ea te 96 fe at ur es m ap s of si ze 55 \u00d7 55 , ar ch it ec tu re of al l co ns ec ut iv e co nv ol ut io na l la ye rs is an al og ou s. F iv e co nv ol ut io na l la ye rs ar e fo llo w ed by 3 fu lly -c on ne ct ed la ye rs of si ze s 40 96 ,4 09 6 an d 1 0 0 0 re sp ec ti ve ly . b . T he le ft m os t im ag e is an ex am pl e in pu t im ag e. Fo r ea ch la ye r w e ha ve se le ct ed on e in te re st in g fil te r th at de pi ct s w ha t is ha pp en in g in si de of th e ne ur al ne tw or k an d pl ot te d: a) a re co ns tr uc ti on of th e or ig in al im ag e fr om th e ac ti vi ty of th at ne ur on us in g th e de co nv ol ut io n (Z ei le r an d Fe rg us ,2 01 4) te ch ni qu e (u pp er la rg er im ag e) , (b ) ac ti va ti on s on th e fe at ur em ap ge ne ra te d by th at ne ur on (l ef t su bim ag e) an d (c ) sy nt he ti c im ag e th at sh ow s w ha t in pu t th e ne ur on w ou ld be m os t re sp on si ve to (r ig ht su bim ag e) . V is ua liz at io ns w er e m ad e w it h D ee p V is ua liz at io n T oo lb ox (Y os in sk i et al ., 20 15 ). A ll fil te rs ar e ca no ni ca l to A le xN et tr ai ne d on Im ag eN et an d ca n be ex pl or ed us in g th e ab ov em en ti on ed vi su al iz at io n to ol or vi su al iz ed di re ct ly fr om th e pu bl ic ly av ai la bl e w ei gh ts of th e ne tw or k. c. Sp ec ifi ci ty of ne ur al re sp on se s ac ro ss fr eq ue nc y ba nd s an d ti m e w in do w s fo r ea ch la ye r of D C N N .S iz e of a m ar ke r is th e to ta la ct iv it y m ap pe d to th is la ye r an d th e in te ns it y of th e co lo r is th e sp ec ifi ci ty of th e ac ti vi ty to th e B ro dm an n ar ea s co ns ti tu ti ng th e ve nt ra ls tr ea m : B A 17 -1 8- 19 -3 7- 20 .\n83\nwe conclude that the alignment mainly comes from having different groups of areas related more or less equally to two groups of layers. The underlying reason for having these two groups of layers comes from the structure of the DCNN \u2013 it has two different types of layers, convolutional (layers 1-5) and fully connected (layers 6-8) (See Figures 14a and 14b for a visualization of the different layers and their learned features and a longer explanation of the differences between the layers in the 4.5). As can be evidenced on Figure 15 the layers 1-5 and 6-8 of the DCNN indeed cluster into two groups. Taken together, we observed that early visual areas are mapped to the convolutional layers of the DCNN whereas higher visual areas match the activity profiles of the fully connected layers of the DCNN."}, {"heading": "4.4.4 Visual complexity varies across areas and frequencies", "text": "To investigate the involvement of each frequency band more closely we analyzed each visual area separately. Figure 16 shows the volume of activity in each area (size of the marker on the figure) and whether that activity was more correlated with the complex visual features (red color) or simple features (blue color). In our findings the role of the earliest area (17) was minimal, however that might be explained by a very low number of electrodes in that area in our dataset (less than 1%). One can see on Figure 16 that activity in theta frequency in time windows 50\u2212250 ms and 150\u2212350 ms had large volume and is correlated with the higher layers of DCNN in\n84\n50 -\n25 0\nm s\n15 0\n- 3 50\nm s\n25 0\n- 4 50\nm s\n\u03b8 \u03b1 5 - 8 Hz 9 - 14 Hz\n\u03b2 \u03b3 \u0393 15 - 30 Hz 31 - 70 Hz 71 - 150 Hz\n\u03b8 \u03b1 5 - 8 Hz 9 - 14 Hz\n\u03b2 \u03b3 \u0393 15 - 30 Hz 31 - 70 Hz 71 - 150 Hz\n\u03b8 \u03b1 5 - 8 Hz 9 - 14 Hz\n\u03b2 \u03b3 \u0393 15 - 30 Hz 31 - 70 Hz 71 - 150 Hz\n\u03b8 \u03b1 5 - 8 Hz 9 - 14 Hz\n\u03b2 \u03b3 \u0393 15 - 30 Hz 31 - 70 Hz 71 - 150 Hz\n\u03b8 \u03b1 5 - 8 Hz 9 - 14 Hz\n\u03b2 \u03b3 \u0393 15 - 30 Hz 31 - 70 Hz 71 - 150 Hz\nF ig\nu re\n16 . A re asp ec ifi c an\nal ys is\nof vo lu m e of\nne ur al\nac ti vi ty\nan d co m pl ex it y of\nvi su al\nfe at ur es\nre pr es en te d by\nth at\nac ti vi ty . Si ze of th e m ar ke r sh ow s th e su m of co rr el at io n co effi ci en ts be tw ee n th e ar ea an d D C N N fo r ea ch pa rt ic ul ar ba nd an d ti m e w in do w . C ol or co de s th e ra ti o of co m pl ex vi su al fe at ur es to si m pl e vi su al fe at ur es , i.e . th e co m pa ri so n be tw ee n th e ac ti vi ty th at co rr el at es w it h th e hi gh er la ye rs (c on v5 , fc 6, fc 7) of D C N N to th e lo w er la ye rs (c on v1 , co nv 2, co nv 3) . In te ns e re d m ea ns th at th e ac ti vi ty w as co rr el at in g m or e w it h th e ac ti vi ty of hi gh er la ye rs of D C N N , w hi le th e in te ns e bl ue in di ca te s th e do m in an ce of co rr el at io n w it h th e lo w er ar ea s. If th e co lo r is cl os e to w hi te th en th e ac ti va ti on s of bo th lo w er an d hi gh er la ye rs of D C N N w er e co rr el at in g w it h th e br ai n re sp on se s in ap pr ox im at el y eq ua lp ro po rt io n.\n85\nhigher visual areas (19, 37, 20) of the ventral stream. This hints at the role of activity reflected by the theta band in visual object recognition. In general, in areas 37 and 20 all frequency bands reflected the information about high level features in the early time windows. This implies that already at early stages of processing the information about complex features was present in those areas."}, {"heading": "4.4.5 Gamma activity is more specific to convolutional layers", "text": "We analysed volume and specificity of brain activity that correlates with each layer of DCNN separately to see if any bands or time windows are specific to particular level of hierarchy of visual processing in DCNN. Figure 14 presents a visual summary of this analysis. In section 4.3.2 we have defined total volume of visual activity in layers L as VL. We used average of this measure over frequency band intervals to quantify the activity in low and high gamma bands. We noticed that while the fraction of gamma activity that is mapped to convolutional layers is high ( V\u0304 \u03b3,\u0393L={conv1...conv5}\nV\u0304 all bands{L=conv1...conv5} = 0.71), this fraction diminished in fully connected layers\nfc6 and fc7 ( V\u0304 \u03b3,\u0393L={fc6,fc7}\nV\u0304 all bandsL={fc6,fc7} = 0.39). Note that fc8 was excluded as it repre-\nsents class label probabilities and does not carry information about visual features of the objects. On the other hand the activity in lower frequency bands (theta, alpha, beta) showed the opposite trend \u2013 fraction of volume in convolutional layers was 0.29, while in fully connected it grew to 0.61. This observation highlighted the fact that visual features extracted by convolutional filters of DCNN are more similar to gamma frequency activity, while the fully connected layers that do not directly correspond to intuitive visual features, carry information that has more in common with the activity in the lower frequency bands."}, {"heading": "4.5 Extending the methodology beyond the visual system", "text": "The recent advances in artificial intelligence research have demonstrated a rapid increase in the ability of artificial systems to solve various tasks that are associated with higher cognitive functions of human brain. One of such tasks is visual object recognition. Not only do the deep neural networks match human performance in visual object recognition, they also provide the best model for how biological object recognition happens (Kriegeskorte,\n86\n2015; Daniel L Yamins et al., 2013; Daniel LK Yamins, Hong, et al., 2014; Daniel LK Yamins and DiCarlo, 2016). Previous work has established a correspondence between hierarchy of the DCNN and the fMRI responses measured across the human visual areas (G\u00fc\u00e7l\u00fc and Gerven, 2015; Eickenberg et al., 2016; Seibert et al., 2016; Radoslaw Martin Cichy, Khosla, et al., 2016). Further research has shown that the activity of the DCNN matches the biological neural hierarchy in time as well (Radoslaw Martin Cichy, Khosla, et al., 2016; Seeliger et al., 2017). Studying intracranial recordings allowed us to extend previous findings by assessing the alignment between the DCNN and cortical signals at different frequency bands. We observed that the lower layers of the DCNN explained gamma band signals from earlier visual areas, while higher layers of the DCNN, responsive for more complex features, matched with the gamma band signals from higher visual areas. This finding confirms previous work that has given a central role for gamma band activity in visual object recognition (Singer and Gray, 1995; Singer, 1999; Fisch et al., 2009) and feedforward communication (Van Kerkoerle et al., 2014; Bastos et al., 2015; Michalareas et al., 2016). Our work also demonstrates that the correlation between the DCNN and the biological counterpart is specific not only in space and time, but also in frequency.\nThe research into gamma oscillations started with the idea that gamma band activity signals the emergence of coherent object representations (Gray and Singer, 1989; Singer and Gray, 1995; Singer, 1999). However, this view has evolved into the understanding that activity in the gamma frequencies reflects neural processes more generally. One particular view (Fries, 2005; Fries, 2015) suggests that gamma oscillations provide time windows for communication between different brain regions. Further research has shown that especially feedforward activity from lower to higher visual areas is carried by the gamma frequencies (Van Kerkoerle et al., 2014; Bastos et al., 2015; Michalareas et al., 2016). As the DCNN is a feedforward network our current findings support the idea that gamma rhythms provide a channel for feedforward communication. However, our results by no means imply that gamma rhythms are only used for feedforward visual object recognition. There might be various other roles for gamma rhythms (Buzs\u00e0ki and Wang, 2012; Fries, 2015).\nWe observed significant alignment to the DCNN in both low and high gamma bands. However, when directly contrasted the alignment was stronger for low gamma signals. Furthermore, for high gamma this alignment was more restricted in time, surviving correction only in the middle time window. Previous studies have shown that low and high gamma frequencies are functionally different: while low gamma is more related to classic narrowband gamma oscillations, high frequencies seem to reflect local spiking activity rather than oscillations (Manning et al., 2009; Ray and Maunsell,\n87\n2011), the distinction between low and high gamma activity has also implications from cognitive processing perspective (Vidal, Chaumon, et al., 2006; Wyart and Tallon-Baudry, 2008). In the current work we approached the data analysis from the machine learning point of view and remained agnostic with respect to the oscillatory nature of underlying signals. Importantly, we found that numerically the alignment to the DCNN was stronger and persisted for longer in low gamma frequencies. However, high gamma was more prominent when considering volume and specificity to visual areas. These results match well with the idea that whereas high gamma signals reflect local spiking activity, low gamma signals are better suited for adjusting communication between brain areas (Fries, 2005; Fries, 2015).\nIn our work we observed that the significant alignment depended on the fact that there are two groups of layers in the DCNN: the convolutional and fully connected layers. We found that these two types of layers have similar activity patterns (i.e. representational geometry) within the group but the patterns are less correlated between the groups (Figure 15). As evidenced in the data, in the lower visual areas (17, 18) the gamma band activity patterns resembled those of convolutional layers whereas in the higher areas (37 and 20) gamma band activity patterns matched the activity of fully connected layers. Area 19 showed similarities to both types of DCNN layers.\nConvolutional layers impose a certain structure on the network\u2019s connectivity \u2013 each layer consists of a number of visual feature detectors, each dedicated to finding a certain pattern on the source image. Each neuron of the subsequent layer in the convolutional part of the network indicates whether the feature detector associated with that neuron was able to find its specific visual pattern (neuron is highly activated) on the image or not (neuron is not activated). Fully connected layers on the other hand, as the name suggests, connect every neuron of a layer to every neuron in the subsequent layer, allowing for more flexibility in terms of connectedness between the neurons. The training process determines which connections remain and which ones die off. In simplified terms, convolutional layers can be thought of as feature detectors, whereas fully connected layers are more flexible: they do whatever needs to be done to satisfy the learning objective. It is tempting to draw parallels to the roles of lower and higher visual areas in the brain: whereas neurons in lower visual areas (17 and 18) have smaller receptive fields and code for simpler features, neurons in higher visual areas (like 37 and parts of area 20) have larger receptive fields and their activity explicitly represents objects (Grill-Spector and Malach, 2004; DiCarlo, Zoccolan, and Rust, 2012). On the other hand, while in neuroscience one makes the broad differences between lower and higher visual cortex (Grill-Spector and Malach, 2004) and sensory and association cortices (Zeki, 1993), this distinction is not so sharply defined as the one between convolutional and fully connected layers. Our hope is that the present work contributes to\n88\nunderstanding the functional differences between lower and higher visual areas.\nVisual object recognition in the brain involves both feedforward and feedback computations (DiCarlo, Zoccolan, and Rust, 2012; Kriegeskorte, 2015). What do our results reveal about the nature of feedforward and feedback compoments in visual object recognition? We observed that the DCNN corresponds to the biological processing hierarchy even in the latest analysed time-window (Figure 12). In a directly relevant previous work Cichy and colleagues compared DCNN representations to millisecond resolved magnetoencephalographic data from humans (Radoslaw Martin Cichy, Khosla, et al., 2016). There was a positive correlation between the layer number of the DCNN and the peak latency of the correlation time course between the respective DCNN layer and magnetoencephalography signals. In other words, deeper layers of the DCNN predicted later brain signals. As evidenced on Figure 3 in (Radoslaw Martin Cichy, Khosla, et al., 2016), the correlation between DCNN and magnetoencephalographic activity peaked between ca 100 and 160 ms for all layers, but significant correlation persisted well beyond that time-window. In our work too the alignment in low gamma was strong and significant even in the latest time-window 250-450 ms, but it was significantly smaller than in the earliest time-window 50-250 ms. In particular, the alignment was the strongest for low gamma signals in the earliest time-window compared to all other frequency-and-time combinations.\nThe present work relies on data pooled over the recordings from 100 subjects. Hence, the correspondence we found between responses at different frequency bands and layers of DCNN is distributed over many subjects. While it is expected that single subjects show similar mappings (see also Supplementary Figure 23), the variability in number and location of recording electrodes in individual subjects makes it difficult a full single-subject analysis with this type of data. We also note that the mapping between electrode locations and Brodmann areas is approximate and the exact mapping would require individual anatomical reconstructions and more refined atlases. Also, it is known that some spectral components are affected by the visual evoked potentials (VEPs). In the present experiment we could not disentangle the effect of VEPs from the other spectral responses as we only had one repetition per image. However, we consider the effect of VEPs to be of little concern for the present results as it is known that VEPs have a bigger effect on low frequency components, whereas our main results were in the low gamma band.\nIt must be also noted that the DCNN still explains only a part of the variability of the neural responses. Part of this unexplained variance could be noise (G\u00fc\u00e7l\u00fc and Gerven, 2015; Khaligh-Razavi and Kriegeskorte, 2014). Previous works that have used RSA across brain regions have in general\n89\nfound the DCNNs to explain a similar proportion of variance as in our results (Radoslaw Martin Cichy, Khosla, et al., 2016; Seibert et al., 2016). It must be noted that the main contribution of DCNN has been that it can explain the gradually emerging complexity of visual responses along the ventral pathway, including the highest visual areas where the typical models (e.g. HMAX) were not so successful (Daniel LK Yamins, Hong, et al., 2014; Khaligh-Razavi and Kriegeskorte, 2014). Recently it also has been demonstrated that the DCNN provides the best model for explaining responses to natural images also in the primate V1 (Cadena et al., 2017). Nevertheless, the DCNNs cannot be seen as the ultimate model explaining all biological visual processing (Kriegeskorte, 2015; Rajalingham et al., 2018). Most likely over the next years deep recurrent neural networks will surpass DCNNs in the ability to predict cortical responses (Kriegeskorte, 2015; Shi et al., 2017).\nIntracranial recordings are both precisely localized in space and time, thus allowing us to explore phenomena not observable with fMRI. In this work we investigated the correlation of DCNN activity with five broad frequency bands and three time windows. Our next steps will include the analysis of the activity on a more granular temporal and spectral scale. Replacing representation similarity analysis with a predictive model (such as regularized linear regression) will allow us to explore which visual features elicited the highest responses in the visual cortex. In this study we have investigated the alignment of visual areas with one of the most widely used DCNN architectures \u2013 AlexNet. The important step forward would be to compare the alignment with other networks trained on visual recognition task and investigate which architectures preserve the alignment and which do not. That would provide an insight into which functional properties of DCNN architecture are compatible with functional properties of human visual system.\nTo sum up, in the present work we studied which frequency components match the increasing complexity of representations of an artificial neural network. As expected by previous work in neuroscience, we observed that gamma frequencies, especially low gamma signals, are aligned with the layers of the DCNN. Previous research has shown that in terms of anatomical location the activity of DCNN maps best to the activity of visual cortex and this mapping follows the propagation of activity along the ventral stream in time. With this work we have confirmed these findings and have additionally established at which frequency ranges the activity of human visual cortex correlates the most with the activity of DCNN, providing the full picture of alignment between these two systems in spatial, temporal and spectral domains.\n90\nChapter 5\nState space visualization informs on representation of mental concepts in human brain\nNumerous studies in the area of BCI are focused on the search for a better experimental paradigm \u2013 a set of mental actions that a user can evoke consistently and a machine can discriminate reliably. Examples of such mental activities are motor imagery, mental computations, etc. We propose a technique that instead allows the user to try different mental actions in the search for the ones that will work best. The system is based on a modification of the self-organizing map (SOM) algorithm and enables interactive communication between the user and the learning system through a visualization of user\u2019s mental state space. During the interaction with the system the user converges on the paradigm that is most efficient and intuitive for that particular user. Results of two experiments, one allowing muscular activity, another permitting mental activity only, demonstrate soundness of the proposed method and empirically validate the performance improvement over the traditional closed-loop feedback approach."}, {"heading": "5.1 The search for distinguishable mental patterns", "text": "In many BCI experiments, participants are asked to perform certain mental actions. Consider an experiment, where a person is asked to control a point on a screen, and have it move to the left. In essense, the subject is requested to focus on a thought of \u201cmoving the point leftwards\u201d. This request is quite ambiguous \u2013 should the user concentrate on the abstract notion of \u201cleft\u201d, engage in motor imagery or think about an unrelated concept?\nThe problem of choosing the best kind of mental activity for BCI has been studied by E. A. Curran and Stokes (2003) and Friedrich, Scherer, and Neuper (2012). Most experiments first propose a particular paradigm and\n91\nthen evaluate its average effectiveness on a sample of users. Many paradigms have been evaluated this way (C. W. Anderson and Sijercic, 1996; Babiloni et al., 2000; Alivisatos and Petrides, 1997; Allison et al., 2010; Ba\u015far et al., 2007; Cabrera and Dremstrup, 2008; Chochon et al., 1999; E. Curran et al., 2004). As brain activity for a particular mental action differs across subjects (M. B. Miller et al., 2012; Ganis, Thompson, and Kosslyn, 2005; Tavor et al., 2016), any general paradigm will be suboptimal compared a user-specific one. In this work we propose a method that facilitates self-guided interactive search for a user-specific paradigm through communication between the user and the learning system. We demonstrate the feasibility of the approach on EEG recordings from two separate experiments on muscular and mental activity. The approach is general and does not depend on the neuroimaging method.\nTo achieve our goal we replace the traditional feedback (Pfurtscheller and Neuper, 2001) with a visualization of the feature space within which the underlying machine learning algorithm is operating. This visualization facilitates a \u2018dialogue\u2019 between the learning algorithm and the user by visually explaining to the user why his current set of mental actions is suboptimal, which ones are being recognized well by the system and which ones should be changed. By exploring how their mental actions affect the visualization, a user can find a suitable mental action for each of the stimuli. The exploration of the mental state space can go for as long as needed to find mental actions that the user can produce consistently over time and that are distinguishable by the algorithm."}, {"heading": "5.2 BCI via topology-preserving visualization of the feature space", "text": "At the core of almost any BCI system lies a machine learning algorithm that classifies user brain signal into desired actions (Lotte et al., 2007). The algorithm sees the incoming data in a high-dimensional space and operates in that space. If an algorithm is unable to correctly discern the desired actions from the signal one can rely on visualization of the data and the space state to figure out why that is the case. Visualization allows to see particular data points in the context of other data, and allows to detect such issues as homogeneous data representation, failure to represent critical features of the data, biases in the data, insufficient flexibility of the feature space to present different data points differently, too high variance of the data points that should belong to the same group, and others. In the case of classification of mental actions we find that the two most important aspects a visualization could help evaluate are the cases where the data points from different classes look too much alike (one mental pattern is too\n92\nsimilar to another) for the algorithm to differentiate between them, and the variance of the data within a class \u2013 mental patterns that a user produces for the same action are not consistent and the algorithm is not able to group them together. With enough simplification we were able to present such a visualization to the user directly, allowing for a real-time evaluation of the above-mentioned issues during the training process. This allows the user to modify his mental activity in accordance with the feedback and try to produce more consistent and more distinguishable mental patterns. Figure 17 depicts the interaction process between the user and the proposed feedback system.\nDirect visualization of the space of mental signals provides more information to the user and allows to make more informed decisions than would be possible with the traditional approach (Pfurtscheller and Neuper, 2001). If in the case of usual system the subject has no information of why the system cannot distinguish the user\u2019s mental states, in the adaptive paradigm, proposed in this work, the subject can see which mental actions are not being recognized or are colliding with others, previously attempted, mental states. The proposed framework naturally addresses a few limitations of the traditional approach, such as limited number of actions that can be\n93\ntrained simultaneously and makes a more efficient use of the training time by shifting training data class distribution towards more complicated cases.\nThe concept described above poses several technological constraints on the choice of the underlying learning algorithm. To facilitate the real-time feedback loop the algorithm should work in an online setting and be fast enough to support real-time operation. In order to present the projection of the feature space to the user the algorithm must be compatible with topology-preserving dimensionality reduction techniques (Gisbrecht and Hammer, 2015). In this section we describe a method that satisfies those requirements."}, {"heading": "5.2.1 Self-organizing map", "text": "Self-organizing map (SOM) (Kohonen, 1990) is one of the topology-preserving dimensionality reduction techniques. These techniques try to preserve the relative distances through the transformation, such that the data points that were close in the original space remain close in the target space, and those that were apart, remain apart. SOM projects the data form the original space onto a map, which is a collection of m units organized into a multidimensional rectangular grid. Most commonly (and also in this work) a two-dimensional grid is used. Each SOM unit u corresponds to a vector wu \u2208 Rd in the space of input data points (signal samples from the EEG device, in our case). This way each unit effectively covers a region in the signal space. In this work the map has 625 units (25\u00d7 25 square grid) with 630-dimensional vectors w initialized from uniform distribution U(0, 0.01).\nThe learning phase consists of updating vectorsw with each new training sample x. Once a new sample is obtained from a neuroimaging device the best matching unit (BMU) for that sample is found according to Equation 5.1 with Euclidean distance used as the distance measure.\nBMU(x) = argmin u\u2208{1...m} distance(wu,x) (5.1)\nOnce BMU is found the weights w of unit u and its neighbors are updated as shown in Equation 5.2, where s is the number of the current iteration.\nws+1u = w s u + \u0398(BMU, u, s)\u03b1(s)(x\u2212wsu) (5.2)\nDefault SOM is an offline learning algorithm that performs several passes over the training data. The update is thus repeated for each iteration s \u2208 {1, . . . , S}, for each input data vector (x1, . . . ,xn) in the training set and for each unit in the map (u1, . . . , um). In total this procedure is being repeated up to S\u00d7n\u00d7m times, where S is the iteration limit, n is the number of samples in the training data and m is the size of the map. Not all units are updated with each new input vector, furthermore, not all units among\n94\nthe updated ones are updated equally. There are two functions in Equation (5.2), which are responsible for deciding which units will be updated and by how much. \u0398(b, u, s) is called the neighborhood function, it determines to what extent unit u is neighbor of a unit b: for b itself \u0398(b, b, s) = 1 and for some unit u, which is too far away to be considered to be a neighbor of b \u0398(b, u, s) = 0. The parameter s is used to decrease the number of neighbors on later iterations. The function \u03b1(s) outputs learning rate that decreases with more iterations allowing the learning process to converge.\nAt the end of the learning process the units of the map represent centers of signal clusters in the training data. Each new data sample can be assigned to one of the clusters and this cluster will hold samples that are similar. The samples that are close in the original space will be assigned to map units that are close to each other on the map."}, {"heading": "5.2.2 Predictive online SOM", "text": "We extend SOM to work in an online setting (D. Deng and Kasabov, 2000; Somervuo, 2004), where the map is updated only once with each new data sample. We also assign a vector of probabilities pu \u2208 RC to each unit u and use that vector to classify each new incoming data sample into one of C classes. The class probability vector p of unit u of Predictive Online SOM (POSOM) is initialized to a random vector of length C with values sampled from uniform distribution U(0.0, 0.2). This vector holds action probability distribution for the unit u. It shows what is the probability that a signal x, which was classified into unit u, has been produced in response to action a.\nClass probability vectors are updated after each sample according to Equation 5.3, ps+1(u) = ps(u)(1\u2212 \u03b1) + c\u03b1 (5.3) where s is iteration number, \u03b1 \u2208 (0, 1) is a parameter, which specifies how fast the contribution of the older data samples deteriorates, and c is a bit vector, where for each class we have value 0 or 1. There can be only one non-zero value in the vector c and its position indicates the true class of a sample.\nThe probability vector pu is used for classification as follows: for each new sample x we first identify POSOM\u2019s BMU u for this sample, and predict the class of this sample by choosing the most probable class in the vector pu."}, {"heading": "5.2.3 POSOM-based BCI training system", "text": "The learning method defined in the previous section satisfies all of the requirements of a system with an interactive feedback based on visualization\n95\non user\u2019s mental state space we have outlined in the beginning of the chapter.\nThe training process begins by presenting an empty SOM map to the user (Figure 19a). A stimulus cue is displayed for a brief period of time and the system starts receiving samples from the neuroimaging device. It finds the best matching unit u for each of the samples and updates the wu and pu vectors of the unit u and its neighbours. Some of the randomly initialized SOM units now represent certain mental patterns and are mapped to corresponding actions, the action each unit is associated with is shown with a pictogram on the map (Figure 19b). a b c\n96\nThe process continues until the user is satisfied with his ability to produce the required set of patterns consistently and system\u2019s ability to assign these patterns to correct units on the map (Figure 19c). The user can see on the map which of the mental patterns are always assigned correctly and which ones are \u2018jumping\u2019 across the map. This informs the user about the variance of a mental pattern, if the variance is too high it might be best to switch to another mental pattern instead. If a user can see that the patterns of two or more different actions tend to occupy the same region of the map he can conclude that the mental patterns he is producing for these actions are not different enough to be distinguishable and he should consider replacing one or all of them."}, {"heading": "5.3 Experimental validation on brain-computer interface for control", "text": "Each of 5 test subjects completed a set of four experimental runs to compare maximal achievable classification accuracy of adaptive (proposed) versus control approach under two different conditions. In the first pair of experiments subjects were allowed to engage facial muscles to achieve the control of the system more easily. In the second pair only mental activity was allowed and the subjects were instructed to rely on mental imagery to control the system.\nSubjects were seated in front of a computer screen in a quiet room with no distractions. All subjects had normal or corrected to normal vision. In all of experiments subjects were presented with 3 different stimuli (left, right and none) and were asked to engage different mental (or, in the case of the experiment where facial expressions were allowed, muscular) activity for each stimulus. A stimulus was shown for 7 seconds. Subjects were briefed on the usual mental paradigms including motor imagery (Pfurtscheller and Neuper, 2001; Hwang, Kwon, and Im, 2009), vivid visual imagery (Marks, 1973; Neuper et al., 2005), mental computations (Chochon et al., 1999) and auditory recollections (Cabrera, Farina, and Dremstrup, 2010).\nThe sequence of the experimental runs each test subject has completed was as follows:\n1. Training of the classification model in the traditional way. Stimuli were presented in a random order for 7 seconds each, for total time of 7 minutes, keeping the number of times each stimulus is shown balanced. The test subject received real-time feedback by observing the height of the performance indicator that was changing with each new data sample. Currently highlighted bar is the current action, height of the bar indicates the performance (Figure 20b).\n97\n2. Testing the traditional model. To avoid measuring the involuntary reaction to the cue image the user interface of the testing stage was different from the training stage and is shown on Figure 20c. Currently highlighted stimulus is the one the user should engage in. Stimuli were shown for 7 seconds in random order for a total length of 3 minutes.\n3. Training of the classification model in adaptive way. The user was presented with a visualization of the projection of the feature space onto 2D grid (Figure 20a). Each stimulus is shown for 7 seconds, the duration of the experiment was not fixed to allow the subject to test different mental activities for the same action until the one that works is found. The stimuli were presented in the order of their performance rate, the actions that have the lowest score are shown more frequently.\n4. Testing of the adaptively trained model. The procedure repeats the steps outlined in (2) exactly, making the testing runs comparable.\nUpon finishing the trials the test subjects were asked of their subjective evaluation of the adaptive system in comparison with the traditional one and whether they were able to feel the interaction with the system and its efforts to adapt to test subject\u2019s mental efforts.\n98"}, {"heading": "5.3.1 Preprocessing of EEG data", "text": "The data was recorded using the Emotiv EPOC (Stytsenko, Jablonskis, and Prahm, 2011) consumer-grade EEG device. Signal from all 14 EEG channels was split into 1000 ms windows with a step size of 250 ms. Each 1000 ms recording was linearly detrended and converted to frequency domain using fast Fourier transform (Welch, 1967). Frequencies outside the 1 to 45 Hz range were excluded from further analysis.\nA 1000 ms signal from one channel was represented by 45 frequency power measurements. By concatenating representations of all 14 channels we obtained feature representation of a signal with 630 dimensions. In machine learning terms a sample x that represents 1000 ms of EEG recording has 630 features and a categorical class label."}, {"heading": "5.4 Feedback based on mental state space visualization leads to higher decoding accuracy", "text": "We have conducted two types of experiments to empirically validate the benefits of the adaptive search for mental BCI paradigm via visual exploration of a projection of subject\u2019s mental state space. During the first experiment the test subjects were allowed to engage in facial muscle activity in response to the stimuli (C.-N. Huang, Chen, and Chung, 2006; Heger, Putze, and Schultz, 2011). The second experiment was aimed at controlling the system via mental efforts only. In both experiments the proposed approach demonstrated statistically significant improvement in performance over the traditional method. Average performance of the model trained on facial expressions in traditional way was 23% higher (Mann-Whitney U test p = 0.006) than that of the traditional approach. For the mental actions the adaptive approach resulted in a model that was significantly and consistently higher than the chance level (F1 score = 0.422), while traditional approach failed to deliver a non-trivial model (F1 score = 0.354). Comparatively, the adaptive approach yielded 19% higher performance (MannWhitney U test p = 0.018).\nFigure 22 (left) presents the detailed analysis of the results of the experiments involving facial expressions. Face muscle activity highly affects the EEG readings (D O\u2019Donnell, Berkhout, and Adey, 1974) and can be observed with the naked eye even on the raw signal. The primary goal of this series of experiments was to demonstrate the benefit of the adaptive approach in a high signal to noise ratio (SNR) scenario. Compared to the facial expressions experiment the task of distinguishing mental states was much harder (Haynes and Rees, 2006). Since the effect of changing the activity was not immediately evident, it required more time for the test subjects to begin to understand how their efforts affect the learning system.\n99\nFigure 22 (right) shows the detailed results of the experiment.\nFigure 22. Details of performance on 3-class control problem. Left: 3-class training results using facial expressions. Circular markers denote the results achieved using the traditional approach and the triangular ones denote the adaptive approach. Each test subject is marked with a different color. On the x-axis we can see the number of samples the algorithm needed to reach the F1 score displayed on the y-axis. Traditional experiments were ran for 240 samples, or, if a subject felt that he would benefit from longer interaction with the system, the experiment was extended to 420 samples. Right: 3-class training results using power of thought via traditional (circle) and interactive (triangle) approach. Horizontal axis shows the number of sample is took to train the model, while the vertical one indicates the performance of the final model on the test run. The experiment continued for as long as test subject felt necessary.\n100"}, {"heading": "5.5 The general quest of navigating human mental state space", "text": "In this work we have proposed, implemented and empirically tested an alternative approach to closed loop feedback BCI training that relies on the visualization of test subject\u2019s mental state space in real time facilitating the interaction with the learning system. In the traditional approach the feedback serves only one purpose \u2013 to inform the test subject on his current performance. We expand the information available to the subject by allowing him to see not only that his current actions work poorly (or well), but also provide him with an insight into why a particular set of mental actions might not be a suitable one. We then provide him with an interactive way to experiment with other mental actions until he finds the ones that he can engage consistently and that are distinguishable by the learning system. By the sheer fact of sharing more information with the user we expect our system to achieve better performance. By facilitating the interactive training process we enable the test subject, given enough effort, to reach the desired level of performance.\nIn addition to the primary benefit described above we find that a few other properties of our approach are beneficial for training BCI systems, namely: the resulting paradigm is personalized to each particular test subject and thus can be tuned better than a one-fits-all paradigm such as motor imagery; system automatically takes care of failed trials and mistakes on the test subject\u2019s side \u2013 a subject can rectify a mistake via further interaction with the system, the failed record does not taint the dataset forever, but is gradually phased out by further training; flexibility in training time allows to deviate from strict stimulation schedule and allows the test subject to focus on the most problematic actions, giving them more attention if more attention is needed.\nWe would like to highlight the choice of the testing paradigm we employed in our work. We find that testing of a general-purpose BCI system must be decoupled from the training in terms of visual cues and protocol. This is necessary to avoid training the subject to simply react to the visual cues and not engage in the corresponding mental activity. By changing the cues we ensure that during the test time the test subject has to invoke the mental activity corresponding to each particular action. Such approach makes the resulting model more robust in the context of real-world applications.\nWe acknowledge the shortcoming of this study, such as low number of test subjects and a low-end EEG device. This work serves the purpose of initial validation of the concept that allows to plan a larger study, ideally involving intracranial neuroimaging techniques that would have sufficient SNR to make the approach applicable for real-world applications. Rectify-\n101\ning the above-mentioned issues and further exploring the possible topologypreserving dimensionality reduction techniques such as t-SNE (Maaten and G. Hinton, 2008) and neural networks-based solutions are the primary directions for our future work.\nThe proposed methodology of visualizing test subject\u2019s mental state space in real time has wider applicability than BCIs. It gives a user the opportunity to roam the visualization space that is topologically consistent with the space of representation of user\u2019s mental signals. This means that if two mental state are close in terms of the brain activity they generate, they will also be close visually, which allows the user to see which thought, emotions, motor actions, and other activities that involve brain activity are close together and which are further apart. The basis for this approach lies in the ability of machine learning interpretability tools to explain what an artificial model is seeing to a human observer. Supplying the proposed system with a high quality neuroimaging device (such as intracortical electrode system) would allow a researcher to gain better understanding of the space of neural signals, or a general user to explore their own mind. Such use of this methodology could lead to new realizations about how human brain works and to new applications of neural technology.\n102\nConclusion\nTraditionally, neuroscience was and to a large extent is a hypothesis-driven science. The growing amount of data that is being generated by modern neuroimaging techniques merits, however, an increase in the role that exploratory, data-driven approach plays in modern neuroscience. In this thesis we make the case for the importance of adopting methods of interpretability of machine learning models in neuroscientific research. We discuss the benefit that machine learning brings by augmenting the ways of neuroscientific inquiry with an additional path of automatic hypothesis generation and validation.\nThe ultimate purpose of proposing new hypotheses and models of brain function is to discover the ones that describe the phenomenon well. In the hypothesis-driven approach most of the process relies on a human investigator, who first observes a certain phenomenon, then comes up with a model or a hypothesis to describe it, collects the data, validates the model based on the data, and, finally, rules the model to be true or insightful or discards it. In this scenario the role of automated data processing is confined to the process of obtaining and processing the data to provide measures on certain narrowly-defined experimental metrics that the investigator needs to reach a conclusion. This approach allows the human investigator to be in full control of the meaning of the model that is being created, but scales up only by increasing the number of investigators, naturally limiting the space of possible hypothesis that is humanly possible to test against the data. In the data-driven approach the process starts with the dataset that contains the observations of a phenomenon. Machine learning methods then automatically generate models (hypotheses) that attempt to explain the dynamics that was captured by the data. Model validation step automatically discards most of the models that merely capture shallow statistical dependencies in the particular data instance that was recorded and do not generalize to capture the underlying process. Some of the models, however, do, and, when validated, show to generalize well to correctly describe previously unseen data from the same phenomenon. When this happens we know that the process of automatic modeling has captured a description of the process that governs the phenomenon. All of the steps leading to this\n103\nstage can be done with minimal human involvement and scale up with the amount of computational resources. This allows the space of models that are proposed and tested to be considerably larger than if we would only use humans to sift through the possible explanations. Now the human effort can be concentrated on the models and hypothesis that were identified by the automatic process as descriptive and general. Interpreting those models will show which ones are true and insightful and which ones are trivial.\nThe hypothesis-driven and data-driven approaches to hypothesis generation cover different parts of the conceptual space of unknown hypotheses and should both be exploited to advance our knowledge of the brain. The data-driven approach is designed to excel in the exploratory analysis, and, given the above-mentioned volumes of data, such exploration of this data has ever-growing chance of making a discovery. To properly facilitate this process, the interpretability techniques that have established their role in general machine learning community have to find their way into neuroscience research on a wider scale. We hope this thesis contributes to this process.\nThe exploration of the symbiosis between the fields of neuroscience and machine learning in Chapter 1 establishes the already existing and also the emerging track record of mutual benefit those two fields have provided each other. We find that one of the ways this benefit can prosper further is by adopting the view presented in Chapter 2 of the machine learning approach playing the role of a builder of computational neurophysiological models. The need to interpret the knowledge the model has acquired and to articulate it in an intuitive manner leads us to the interpretability techniques. The need for a better understanding of the knowledge representation that artificial learning algorithms create require a structured approach to navigating the space of those representations. In Section 2.1.3 we propose a taxonomy of machine learning methods based on knowledge representation and hope that this view angle proves useful when designing next neuroscientific study that involves machine learning methods.\nThe work that became the basis of this thesis serves as an example of adopting the proposed perspective and methodology and demonstrates its applicability on three different levels of organization. In Chapter 3 interpretable machine learning model is used to analyze neural dynamics at the level of localized activity across the human brain. This analysis allowed to characterize neural locations and their activity in during the task of visual perceptual categorization. The uncovered signatures of visual processing in human brain provided a multifaceted view on spectral, temporal and anatomical characteristics of this process. The comparison between biological and artificial systems of vision in Chapter 4 gives an example of the role machine learning models can play at a more abstract level, where the aim is to understand the functional organization of the human brain. In the last\n104\nstudy described in Chapter 5 the dimensionality reduction and visualization techniques provide an actionable insight into relative organization of mental concepts within a subject\u2019s mental state space. Visualizing the mental state space allows to analyze the behavior of our brain at the highest level of abstraction.\nTaken together, the ideas and the results of this thesis highlight one of the roles machine learning could play in advancing our understanding of the human brain. The ability to uncover patterns and extract knowledge from data makes the method of machine learning a suitable tool for augmenting our capacity to create explanations of natural phenomena around us. Neuroscience is a particularly fitting area for application of this methodology due to its symbiosis with the area of artificial intelligence and machine learning. The shared goal of uncovering the mechanism of intelligence made the field of artificial intelligence follow and reapply the discoveries made in neuroscience. In some cases this has led to the realization that both systems, the biological and the artificial ones, if presented with the same functional goal, sometimes develop similar mechanisms of achieving it. The similarities between the mechanisms employed by biological and artificial systems that were discovered to this date, such as hierarchy of the visual system, the mechanism of periodic memory consolidation, grid-like representation of space for navigation, and others endorse the fact that an artificial learning system can emerge a mechanism that is similar to the one that is used by our brain. In this thesis we stress the importance of continued analysis of the ways how machine learning algorithms achieve their results as understanding of these mechanisms can shed light on the mechanisms employed by our brain.\nWe hope you have found the perspective curious and the examples convincing enough to let the proposed approach occupy a part of your mental state space.\n105"}, {"heading": "Appendix A", "text": "Code and data\nAll the pre-processed spectrotemporal data that supports the findings reported in Chapter 3 are available for download under Academic Free License 3.0 from https://web.gin.g-node.org/ilyakuzovkin/SpectralSignatures-of-Perceptual-Categorization-in-Human-Cortex. The code that was used to produce this data from the raw recordings and to perform all of the subsequent analysis steps is available at https://github.com/ kuz/Spectral-signatures-of-perceptual -categorization-in-humancortex.\nThe activations of biological and artificial systems of vision that are the bases for comparison and mapping reported in Section 4 are available for download under Academic Free License 3.0 from https://web.gin.gnode.org/ilyakuzovkin/Human-Intracranial-Recordings-and-DCNN-toCompare-Biological-and-Artificial-Mechanisms-of-Vision. The full code of the analysis pipeline is publicly available at https://github.com/ kuz/Human-Intracranial-Recordings-and-DCNN-to-Compare-Biologicaland-Artificial-Mechanisms-of-Vision under the MIT license.\nRaw human brain recordings that support the findings in chapters 3 and 4 are available from Lyon Neuroscience Research Center but restrictions apply to the availability of these data, which were used under license for the current study, and so are not publicly available. Raw data are however available from the author upon reasonable request and with permission of Lyon Neuroscience Research Center.\nThe data supporting the findings in Chapter 5 along with the code of the analysis and experiments are publicly available at https://bitbucket. org/ilyakuzovkin/adaptive-interactive-bci.\n135"}, {"heading": "Appendix B", "text": "Supplementary materials\nB.1 Detailed visualizations of spectral signatures of visual processing filtered and feature importance maps\nFull figures of averaged time-frequency importance maps, per-subject importance maps and per-area maps are available for download from https:// figshare.com/articles/Time-Frequency_Importance_Maps_average_persubject_per-area/8223356.\nNormalized per-probe time-frequency power activity plots with importance contour overlay are available at https://figshare.com/articles/Normalized_ TF_activity_with_importance_contour_overlay/8223389.\nFull figures of comparison between polypredictive and monopredictive probes are available at https://figshare.com/articles/Difference_between_ polypredictive_and_monopredictive_neural_locations/8223398.\nFor each cluster we have visualized the cluster mean and also the TF activity of all individual probes that constitute that cluster. Full-resolution figures can be downloaded from https://figshare.com/account/projects/ 64523/articles/8223383.\nB.2 Mappings of Brodmann areas to layers of DCNN per area, layer and subject\nVisualizations of the alignment of all Brodmann areas to the layers of Deep Convolutional Neural Network based on representational similarity analysis are available at https://figshare.com/articles/RSA_mapping_of_all_ Brodmann_areas_to_DCNN_layers/8222579. Mappings to ventral stream\n136\nonly are available at https://figshare.com/articles/RSA_mapping_of_ visual_Brodmann_areas_to_DCNN_layers/8222546.\nVisualizations of all single subject mappings of the Brodmann areas in the ventral stream to DCNN layers are available at https://figshare.com/ articles/Per-subject_RSA_mapping_of_visual_Brodmann_areas_to_DCNN_ layers/8222939.\n137"}, {"heading": "Acknowledgements", "text": "I would like to thank the city of Tartu and its inhabitants for supporting the academic spirit of the city, University of Tartu, that is fueling that spirit and the Institute of Computer Science for getting me in touch with that spirit and for all of the knowledge and support it provided during my years here. I am very grateful to Raul Vicente for building the Computational Neuroscience Lab where I was able to apply my knowledge to fascinating challenges in neuroscience, and, of course, for his supervision during my PhD studies. I would like to thank Konstantin Tretyakov for introducing me to the field of machine learning and supervising my bachelor and master theses, and Sven Laur for his role in developing my understanding of machine learning further. This thesis would not be possible without the work that Juan R. Vidal and other co-authors from Lyon Neuroscience Research Center did to create the amazing dataset that facilitated our findings. I am grateful to them for their work and for sharing that data with me. Anna Leontjeva, thank you for all the academic and philosophical discussions we had that helped to shape this thesis. It is hard to overestimate the role and contribution of the constant flow of ideas born at the seminars, lunch breaks, discussions I had with Tambet Matiisen, Ardi Tampuu, Kristjan Korjus and all the lab members and lab alumni, students, and co-authors. Thank you all for having that scientific passion and sharing it with others. \u0421\u043f\u0430\u0441\u0438\u0431\u043e to my parents and family for instilling the value of knowledge and providing me with the opportunity to pursue it.\nThis work has received financial support from Estonian Research Council through the research grants PUT438, PUT1476 and IUT20-40, Estonian Centre of Excellence in IT (EXCITE) funded by the European Regional Development Fund, and the Institute of Computer Science of University of Tartu.\n138\nList of abbreviations\nAI Artificial Intelligence ANN Artificial Neural Network BA Brodmann Area BCI Brain-Computer Interface BMU Best Matching Unit CNN Convolutional Neural Network CND Computational Neuroscience DCNN Deep Convolutional Neural Network DL Deep Learning DMN Default Mode Network DNN Deep Neural Network DQN Deep Q-Network DRL Deep Reinforcement Learning EEG Electroencephalography FDR False Discovery Rate FFA Fusiform Face Area fMRI Functional Magnetic Resonance Imaging GMM Gaussian Mixture Models HMM Hidden Markov Models ICA Independent Component Analysis ITC Inferior Temporal Cortex iEEG Intracerebral electroencephalography kNN k-Nearest Neighbors LDA Linear Discriminant Analysis LFP Local Field Potential LSTM Long Short-Term Memory MDS Multidimensional Scaling MEG Magnetoencephalography ML Machine Learning MNI Montreal Neurological Institute MRI Magnetic Resonance Imaging\n139\nNS Neuroscience POSOM Predictive Online Self-Organizing Map PCA Principle Component Analysis PPA Parahippocampal Place Area PPC Posterior Parietal Cortex RBM Restricted Boltzmann Machines RDM Representational Dissimilarity Matrix RF Random Forest RL Reinforcement Learning RNN Recurrent Neural Network ROI Region of Interest RSA Representational Similarity Analysis SNR Signal to Noise Ration SOM Self-Organizing Map SVM Support Vector Machines TF Time-Frequency t-SNE t-Distributed Stochastic Neighbor Embedding VEP Visual Evoked Potential VWFA Visual Word Form Area\n140\nSisukokkuv\u00f5te\nInimaju arvutuslikke protsesside m\u00f5istmine masin\u00f5pe mudelite t\u00f5lgendamise kaudu Andmep\u00f5hine l\u00e4henemine arvutuslikku neuroteadusesse\nK\u00e4esolev doktorit\u00f6\u00f6 uurib, millist rolli m\u00e4ngivad masin\u00f5ppe meetodid selliste neuroteaduse mudelite loomisel, mis pakuvad modelleeritava n\u00e4htuse intuitiivset kirjeldust. Meie tahame n\u00e4idata, et modelleerimisprotsessis v\u00f5ib asendada inimese poolt mudeli konstrueerimise masin\u00f5ppe mudeli treenimisega. See lubab nihutada neuroteadlase t\u00f6\u00f6 neuroteaduslike mudelite loomiselt masin\u00f5ppega treenitud mudelite t\u00f5lgendamisele. Valideeritud masin\u00f5ppe mudeli puhul saame oletada, et see mudel peegeldab mehhanismi, mis toimus treeningandmed genereerinud ajus. N\u00fc\u00fcd seisneb uurija roll selles, et t\u00f5lgendada masin\u00f5ppe mudeli t\u00f6\u00f6printsiipi ja artikuleerida seda reaalsuse elegantse kirjeldusena.\nMasin\u00f5ppe meetodid suudavad t\u00f6\u00f6delda palju suuremaid andmekoguseid ja uurida palju keerulisemaid seoseid kui inimene. Neuroandmestike hulk ja suurus kasvab v\u00e4ga kiiresti ja koos sellega kasvab ka andmep\u00f5hise anal\u00fc\u00fcsi roll neuroteaduses. Selles t\u00f6\u00f6s me n\u00e4itame, kuidas suurte andmemahtude peal treenitud masin\u00f5ppe mudelit v\u00f5ib t\u00f5lgendada niimoodi, et see t\u00f5lgendus kirjeldab mitte ainult masin\u00f5ppe mudeli mehhanismi ennast, vaid pakub ka seletust modelleeritava ajuprotsessi kohta. Teises peat\u00fckis t\u00f6\u00f6tame v\u00e4lja taksonoomia, mis grupeerib masin\u00f5ppe meetodid selle j\u00e4rgi, kuidas teadmised, mida masin\u00f5ppe algoritm j\u00e4reldas andmetest, on v\u00e4ljendatud algoritmi sisemise esitusena. Erinevad esitusviisid lubavad erinevaid t\u00f5lgendusi, seega (neuro)teadlane, kelle eesm\u00e4rk on mitte pelgalt treenida mudel, vaid ka aru saada, kuidas mudel saavutab oma tulemusi, peab arvestama algoritmi valides selle algoritmi sisemise esitusviisiga.\nMe n\u00e4itlikustame kirjeldatud l\u00e4henemist kolme uuringuga, mis kasutavad masin\u00f5ppe t\u00f5lgendamismeetodeid kolmel erineval neuroloogilisel tasemel. Igas uuringus me n\u00e4itame, kuidas masin\u00f5ppe t\u00f5lgendamismeetodid olid rakendatud ja millist neuroteaduse teadmist see lubas meil avastada. Esimeses uuringus (peat\u00fckk 3) kasutame me tunnuste t\u00e4htsuse anal\u00fc\u00fcsi juhumetsa\n141\n(Random Forest) mudelitel, mis olid treenitud 100 inimese koljusiseste ajusignaalide peal. Anal\u00fc\u00fcs lubas meil tuvastada, millised aegsagedus komponendid iseloomustavad inimese ajusignaali visuaalsete objektide tuvastamise \u00fclesande puhul. Teine uuring (peat\u00fckk 4) kasutab esituste erinevuste anal\u00fc\u00fcsi (Representation Dissimilarity Analysis, RDA), et v\u00f5rrelda samade visuaalsete stiimulite puhul signaale inimese aju ventraalses piirkonnas ja konvolutsiooniliste tehisn\u00e4rviv\u00f5rkude aktivatsioone erinevates kihtides. See metoodika v\u00f5imaldas meil teha j\u00e4reldusi inimese visuaalse ajukoore protsessidest ja kinnitada h\u00fcpoteesi, et m\u00f5lemad s\u00fcsteemid, nii bioloogiline kui ka tehislik, kasutavad hierarhilist struktuuri visuaalsete objektide tuvastamise protsessis. Kolmas uuring (peat\u00fckk 5) pakub v\u00e4lja meetodi, mis lubab kasutaja arvutiekraanil reaalajas visualiseerida tema ajuolekute ruumi projektsiooni. See funktsioonalsus on saavutatud kasutades topoloogiat s\u00e4ilitavat (topology-preserving) m\u00f5\u00f5tmelisuse v\u00e4hendamise meetodit, mis teisendab mitmem\u00f5\u00f5tmelise ajusignaali kahem\u00f5\u00f5tmeliseks visualisatsiooniks. Visualiseeritud kujutis on inimese jaoks arusaadav ja lubab n\u00e4ha, millised ajusignaalid ja m\u00f5tteseisundid on \u00fcksteisele sarnased ja millised erinevad. K\u00f5ik kolm uuringut loovad ajuandmete peal teatud masin\u00f5ppe mudeli ja siis kasutavad t\u00f5lgendamismeetodeid, et saada k\u00e4tte teadmisi, mis on kasulikud neuroteaduse kontekstis. Samas, abstraktsiooni tase igas uuringus on erinev: esimeses me t\u00f6\u00f6tleme lokaalseid elektrof\u00fcsioloogilisi signaale, teises huvitab meid ajuprotsessi \u00fcldine struktuur ja signaali esitus, kolmandas me rakendame andmep\u00f5hist l\u00e4henemist m\u00f5tteseisundite visualiseerimiseks.\nKokkuv\u00f5ttes, selles t\u00f6\u00f6s kirjeldatud ideed ja tulemused t\u00f5stavad esile rolli, mida masin\u00f5pe saaks m\u00e4ngida edendamaks meie arusaamist inimese ajust. Masin\u00f5ppe meetodite oskus leida mustreid ja luua teadmisi on oluline t\u00e4iendus meie p\u00e4devustele loodusprotsesside ja fenomenide seletamisel. Neuroteadus sobib eriti h\u00e4sti nende meetodite rakendamiseks, sest sellel teadusharul on teatav s\u00fcmbioos masin\u00f5ppe ja tehisintellekti uuringutega. Nende teadusharude \u00fchine eesm\u00e4rk on avastada mehhanism, mis seletaks, kuidas t\u00f6\u00f6tab meie aju ja intellekt. Nagu on n\u00e4idatud esimeses peat\u00fckis, kui bioloogilisel ja tehislikul s\u00fcsteemil on sama eesm\u00e4rk, siis tihti t\u00f6\u00f6tavad m\u00f5lemad s\u00fcsteemid v\u00e4lja \u00fcllatavalt sarnased mehhanismid selle eesm\u00e4rgi saavutamiseks. M\u00f5ned n\u00e4ited sellistest sarnasustest on: visuaalse objektituvastuse hierarhia visuaalses ajukoores ja konvolutsioonilistes tehisn\u00e4rviv\u00f5rkudes; m\u00e4lu konsolideerimine hipokampuses ja stiimul\u00f5ppe algoritmides; heksagonaalne v\u00f5re mida aju v\u00f5rerakud ja stiimul\u00f5ppe mudelid \u00f5pivad ruumilise navigatsiooni jaoks. Need n\u00e4ited t\u00f5endavad asjaolu, et m\u00f5ned mehhanismid meie ajus on sarnased mehhanismidega, milleni j\u00f5uavad \u00f5ppimise k\u00e4igus masin\u00f5ppe algoritmid. M\u00f5ned neist mehhanismidest on avastatud ja on ainult loogiline oletada, et on olemas veel m\u00f5ned, mida me veel ei tea. Oma t\u00f6\u00f6ga me r\u00f5hutame masin\u00f5ppe mudelite t\u00f5lgendamise t\u00e4htsust selliste mehhanismide avastamiseks.\n142\nCurriculum Vitae\nPersonal data Name: Ilya Kuzovkin Date of birth: 9 July 1988 Contacts: ilya.kuzovkin@gmail.com www.ikuz.eu Citizenship: Estonia\nEducation"}, {"heading": "2013 \u2013 2019 PhD candidate at University of Tartu, Faculty of Science and", "text": "Technology"}, {"heading": "2011 \u2013 2013 MSc in Computer Science from University of Tartu, Faculty of", "text": "Mathematics and Computer Science"}, {"heading": "2007 \u2013 2011 BSc in Computer Science from University of Tartu, Faculty of", "text": "Mathematics and Computer Science\nEmployment"}, {"heading": "2016 \u2013 ... Machine Learning Architect at OffWorld Inc., USA", "text": ""}, {"heading": "2014 \u2013 2019 Junior Researcher at the Institute of Computer Science of University of Tartu", "text": ""}, {"heading": "2012 \u2013 2016 Software Engineer at Ideelabor O\u00dc", "text": ""}, {"heading": "2012 \u2013 2015 Teaching Assistant at the Institute of Computer Science of University of Tartu", "text": ""}, {"heading": "2011 \u2013 2012 Research Engineer at Department of Psychology of University of", "text": "Tartu and Realeyes Inc."}, {"heading": "2007 \u2013 2011 Software Engineer at Surflink O\u00dc", "text": "143\nElulookirjeldus\nIsikuandmed Nimi: Ilya Kuzovkin S\u00fcnniaeg: 9 Juuli 1988 Kontaktandmed: ilya.kuzovkin@gmail.com www.ikuz.eu Kodakondsus: Eesti\nHaridus"}, {"heading": "2013 \u2013 2019 Doktorant, Tartu \u00dclikool, Loodus- ja t\u00e4ppisteaduste valdkond", "text": ""}, {"heading": "2011 \u2013 2013 Magister informaatikas, Tartu \u00dclikool, Matemaatikainformaatikateaduskond", "text": ""}, {"heading": "2007 \u2013 2011 Bakalaureus informaatikas, Tartu \u00dclikool, Matemaatikainformaatikateaduskond", "text": "Teenistusk\u00e4ik"}, {"heading": "2016 \u2013 ... Masin\u00f5pe arhitekt, OffWorld Inc., USA", "text": ""}, {"heading": "2014 \u2013 2019 Nooremteadur, Arvutiteaduse instituut, Tartu \u00dclikool", "text": ""}, {"heading": "2012 \u2013 2016 Tarkvarainsener, Ideelabor O\u00dc", "text": ""}, {"heading": "2012 \u2013 2015 \u00d5ppeassistent, Arvutiteaduse instituut, Tartu \u00dclikool", "text": ""}, {"heading": "2011 \u2013 2012 Teadusinsener, Ps\u00fchholoogia instituut, Tartu \u00dclikool ja Realeyes Inc.", "text": ""}, {"heading": "2007 \u2013 2011 Tarkvarainsener, Surflink O\u00dc", "text": "144\nList of original publications"}, {"heading": "I. Kuzovkin, J. R. Vidal, M. Perrone-Bertlotti, P. Kahane, S. Rheims,", "text": "J. Aru, J.-P. Lachaux, R. Vicente. Identifying task-relevant spectral signatures of perceptual categorization in the human cortex. In review, 2020\nLead author. Contributed to the idea, methodology, design of the experiments, development of the software, analysis, and writing of the manuscript.\nI. Kuzovkin, R. Vicente, M. Petton, J.-P. Lachaux, M. Baciu, P. Kahane, S. Rheims, J. R. Vidal, J. Aru. Activations of deep convolutional neural networks are aligned with gamma band activity of human visual cortex. Communications Biology, Vol. 1, No. 1, 2018\nLead author. Contributed to the idea, methodology, design of the experiments, development of the software, analysis, and writing of the manuscript.\nI. Kuzovkin, K. Tretyakov, A. Uusberg, R. Vicente. Mental state space visualization for interactive modeling of personalized BCI control strategies. Journal of Neural Engineering, Vol. 17, No. 1, 2020\nLead author. Contributed to the idea, methodology, design of the experiments, data collection, development of the software, analysis, and writing of the manuscript.\nRelevant publications not included in the thesis:\nA. Leontjeva, I. Kuzovkin. Combining static and dynamic features for multivariate sequence classification. Proceedings of the 3rd IEEE International Conference on Data Science and Advanced Analytics (DSAA), pp. 21-30, 2016\nA. Ingel, I. Kuzovkin, R. Vicente. Direct information transfer rate optimisation for SSVEP-based BCI. Journal of Neural Engineering, Vol. 16, No. 1, 016016, 2018\n145"}], "title": "ILYA KUZOVKIN Understanding Information Processing in Human Brain by Interpreting Machine Learning Models", "year": 2020}