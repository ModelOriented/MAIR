{"abstractText": "Recent advancements in signal processing and machine learning domains have resulted in an extensive surge of interest in deep learning models due to their unprecedented performance and high accuracy for different and challenging problems of significant engineering importance. However, when such deep learning architectures are utilized for making critical decisions such as the ones that involve human lives (e.g., in medical applications), it is of paramount importance to understand, trust, and in one word \u201cexplain\u201d the rational behind deep models\u2019 decisions. Currently, deep learning models are typically considered as black-box systems, which do not provide any clue on their internal processing actions. Although some recent efforts have been initiated to explain behavior and decisions of deep networks, explainable artificial intelligence (XAI) domain is still in its infancy. In this regard, we consider capsule networks (referred to as CapsNets), which are novel deep structures; recently proposed as an alternative counterpart to convolutional neural networks (CNNs), and posed to change the future of machine intelligence. In this paper, we investigate and analyze structures and behaviors of the CapsNets and illustrate potential explainability properties of such networks. Furthermore, we show possibility of transforming deep learning architectures in to transparent networks via incorporation of capsules in different layers instead of convolution layers of the CNNs.", "authors": [{"affiliations": [], "name": "Atefeh Shahroudnejad"}, {"affiliations": [], "name": "Arash Mohammadi"}, {"affiliations": [], "name": "Konstantinos N. Plataniotis"}], "id": "SP:a44a861a02e7aa79c439b61f0d1e105fedd8413b", "references": [{"authors": ["C.M. Bishop"], "title": "Pattern Recognition and Machine Learning", "venue": "Springer-Verlag New York, 2006.", "year": 2006}, {"authors": ["M. Havaei", "A. Davy", "D. Warde-Farley", "A. Biard", "A. Courville", "Y. Bengio", "C. Pal", "P.M. Jodoin", "H. Larochelle"], "title": "Brain Tumor Segmentation with Deep Neural Networks", "venue": "arXiv preprint arXiv:1505.03540, 2015.", "year": 2015}, {"authors": ["M. Bojarski", "D.D. Testa", "D. Dworakowski", "B. Firner", "B. Flepp", "P. Goyal", "L.D. Jackel", "M. Monfort", "U. Muller", "J. Zhang", "X. Zhang", "J. Zhao", "K. Zieba"], "title": "End to End Learning for Self- Driving Cars", "venue": "arXiv preprint arXiv:1604.07316, 2016.", "year": 2016}, {"authors": ["I. Goodfellow", "Y. Bengio", "A. Courville"], "title": "Deep Learning", "venue": "MIT press, 2016.", "year": 2016}, {"authors": ["A. Krizhevsky", "I. Sutskever", "G.E .Hinton"], "title": "Imagenet Classification with Deep Convolutional Neural Networks", "venue": "Advances in neural information processing systems, pp. 1097- 1105, 2012.", "year": 2012}, {"authors": ["M.T. Ribeiro", "S. Singh", "C. Guestrin"], "title": "Why Should I Trust You?: Explaining the Predictions of Any Classifier", "venue": "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 1135-1144, 2016.", "year": 2016}, {"authors": ["W. Samek", "T. Wiegand", "K.R. M\u00fcller"], "title": "Explainable Artificial Intelligence: Understanding, Visualizing and Interpreting Deep Learning Models", "venue": "ITU Journal: ICT Discoveries, Special Issue The Impact of AI on Communication Networks and Services, vol. 1, pp. 1-10, 2017.", "year": 2017}, {"authors": ["Z.C. Lipton"], "title": "The Mythos of Model Interpretability", "venue": "arXiv preprint arXiv:1606.03490, 2016.", "year": 2016}, {"authors": ["M. Kahng", "P.Y. Andrews", "A. Kalro", "D.H. Chau"], "title": "ActiVis: Visual Exploration of Industry-Scale Deep Neural Network Models", "venue": "IEEE Transactions on Visualization and Computer Graphics, vol. 24, no. 1, pp. 88-97, Jan. 2018.", "year": 2018}, {"authors": ["D. Baehrens", "T. Schroeter", "S. Harmeling", "M. Kawanabe", "K.R.K. Hansen"], "title": "M\u00fcller \u201cHow to Explain Individual Classification Decisions,", "venue": "Journal of Machine Learning Research,", "year": 2010}, {"authors": ["S. Bach", "A. Binder", "G. Montavon", "F. Klauschen", "K.R. M\u00fcller", "W. Samek"], "title": "On Pixel-Wise Explanations for Non-linear Classifier Decisions by Layer-Wise Relevance Propagation", "venue": "PloS one, vol. 10, no. 7, pp. 130-140, 2015.", "year": 2015}, {"authors": ["P.W. Koh", "P. Liang"], "title": "Understanding Black-Box Predictions via Influence Functions", "venue": "arXiv preprint arXiv:1703.04730, 2017.", "year": 2017}, {"authors": ["C.C. Jay Kuo", "Yueru Chen"], "title": "On data-driven Saak Transform", "venue": "Journal of Visual Communication and Image Representation, vol. 50, pp. 237-246, 2018", "year": 2018}, {"authors": ["W. Guo", "K. Zhang", "L. Lin", "S. Huang", "X. Xing"], "title": "Towards Interrogating Discriminative Machine Learning Models", "venue": "arXiv preprint arXiv:1705.08564 , 2017.", "year": 2017}, {"authors": ["W. Samek", "T. Wiegand", "K.R. M\u00fcller"], "title": "Methods for Interpreting and Understanding Deep Neural Networks", "venue": "Digital Signal Processing, vol. 73, pp. 1-15, 2018.", "year": 2018}, {"authors": ["W. Samek", "A. Binder", "G. Montavon", "S. Bach", "K.R. M\u00fcller"], "title": "Evaluating the Visualization of What a Deep Neural Network has Learned", "venue": "IEEE Transactions on Neural Networks and Learning Systems, vol. 28, no. 11, pp. 2660-2673, 2017.", "year": 2017}, {"authors": ["S. Goferman", "L. Zelnik-Manor", "A. Tal"], "title": "Context-aware Saliency Detection", "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 34, no. 10, pp. 1915-1926, 2012.", "year": 1915}, {"authors": ["R.R. Selvaraju", "A. Das", "R. Vedantam", "M. Cogswell"], "title": "Grad-CAM: Why did you say that?", "venue": "arXiv preprint arXiv:1611.07450,", "year": 2016}, {"authors": ["X. Huang", "M. Kwiatkowska", "S. Wang", "M. Wu"], "title": "Safety Verification of Deep Neural Networks", "venue": "International Conference on Computer Aided Verification pp. 3-29. Springer, Cham, 2017.", "year": 2017}, {"authors": ["S. Sabour", "N. Frosst", "G.E. Hinton"], "title": "Dynamic Routing between Capsules", "venue": "Advances in Neural Information Processing Systems, pp. 3859-3869. 2017.", "year": 2017}, {"authors": ["G.E. Hinton", "A. Krizhevsky", "S.D. Wang"], "title": "Transforming Auto-encoders", "venue": "International Conference on Artificial Neural Networks, pp. 44-51. Springer, Berlin, Heidelberg, 2011.", "year": 2011}, {"authors": ["Y. LeCun"], "title": "The MNIST database of Handwritten Digits", "venue": "1998.", "year": 1998}], "sections": [{"text": "Index Terms: Explainable Machine Learning, Capsule Networks, Deep Neural Networks, Convolutional Neural Networks.\n1. INTRODUCTION\nNowadays, advanced machine learning techniques [1] have encompassed all aspects of human life including complicated tasks. As such, several critical decisions are now made based on predictions provided by machine learning models without any human supervision or participation. It is, therefore, of paramount importance to be able to trust a model, validate its predictions, and make sure that it performs completely well on unseen or unfamiliar real world data. For example, in critical domains such as medical applications [2] or self-driving cars [3], even a single incorrect decision is not acceptable and could possibly lead to a catastrophic result. For guaranteeing reliability of a machine learning model, it is significantly important to understand and analyze rational reasons behind the decisions made by such sophisticated and advanced models, in other words, we need to be able to open the black-box. On the other hand, deep models [4] are considered as one of the most successful methods\nThis work was partially supported by the Natural Sciences and Engineering Research Council (NSERC) of Canada through the NSERC Discovery Grant RGPIN-2016-049988.\nin several areas especially in image processing and computer vision domains. However, as deep architectures become more complex and introduce more nonlinearity, their structures become less transparent and it is harder to understand what operations or input information lead to a specific decision. Moreover, in scenarios where sufficient data for training is not available, which is a common case when it comes to training deep neural networks such as Convolutional Neural Nets (CNNs) [5], probability of error will increase drastically, which further necessitates an urgent need for interpretation of machine learning architectures.\nExplaining a model means providing extra qualitative information about why the model reach to a specific decision regarding the input components\u2019 relationship (e.g., different patches of an image [6]). Explanation can be considered as opening incomprehensible blackbox and seeing inside. In other words, the main goal is to find answers to questions of like: What is happening inside a neural network? What does each layer of a deep architecture do? What features a deep network is looking for? Fig. 1 illustrates a graphical representation of this black-box concept, where the input is an image and the network prediction would be a single word (e.g., face or cat). As can be seen, such a single output provides no evidence for confirming the truth of predictions or rejecting incorrect predictions without having access to the ground-truth. Main advantages of using an explainable model are as follows: Verification of the model; Improving it by understanding failure points; Extracting new insights and hidden laws of the model, and; Finally, identifying modules responsible for incorrect decisions [7].\nExplainability can be fulfilled visually, text based, example based, and/or by relating inputs and learned parameters [8]. In this regard, recently there has been a great surge of interest [9\u201319] for development of methodologies to make neural network models explainable. For example, an early work in this area is named sensitivity analysis (SA) [10], which measures how much changing each pixel effects the final decision and then a heatmap is obtained referred to as the explainability feature. But this heatmap does not show which patches\nar X\niv :1\n80 2.\n10 20\n4v 1\n[ cs\n.C V\n] 2\n7 Fe\nb 20\n18\nor pixels play more important role in the decision making process. Moreover, it more looks like a saliency map [17], which uses unique frequencies or focus points or other features to find regions of interest, however, pixels identified as salient regions are not necessarily the pixels being involved in making the predictions. In layer-wise relevance propagation method (LRP) [11], each prediction is decomposed by redistributing backward through the network\u2019s layers using redistribution rules and finding a relevance path. Relevance of each neuron or pixel indicates how much it contributes to the decision. The LRP approach is unsatisfying when we have a more complex or nested architecture. Reference [6], proposed Local Interpretable Model-agnostic Explanations (LIME) approach, which explains the prediction by approximating the original model with an interpretable model around several local neighbourhoods. Reference [12] proposed to identify the most responsible training points by tracing back the prediction using influence functions, which estimate effects of changing each training point on that prediction. Gradient-weighted Class Activation Mapping (Grad-CAM) [18] approach is another explanation method for CNNs which uses gradient to obtain localization map as a visual explanation and finds important layers for each class. Finally, Reference [19] proposed a verification framework for increasing trustworthy and explainability of deep networks. It assumes that there is a region in each input which specifically determines its class category and if the prediction shows this category, its input should include the saliency points of that region.\nAlthough recently different research works are developed for explaining complex behaviour of deep neural networks, especially for visual tasks as briefly outlined above, explainable artificial intelligence (XAI) is still in its infancy and needs significant research to further open the black-box. In this regard, we focus on a very recently proposed deep network architecture referred to as Capsule Networks (CapsNets) [20], which is a turning point in the deep learning research. In particular, we investigate CapsNets\u2019 architecture and behavior from explanability perspective. Through analyzing different underlying components of the CapsNet architecture and its learning mechanism (routing-by-agreement), we illustrate that this revolutionized deep net has more intrinsic explainability properties. In particular, we show that CapsNets automatically form the verification framework introduced in [19], learn regions of interest which determine class category, and as such improve trustworthy and explainability of deep networks. Besides, CapsNets inherently create the relevancy path (introduced in [6]) and we refer to it as relevance by agreement as a bi-product of the routing-by-agreement algorithm, which adds another level of explanability to this architecture.\nThe rest of this paper is organized as follows: Section 2 reviews CapsNets. Section 3 investigates CapsNets explainability properties. Section 4 shows some explanations of CapsNets on MNIST dataset. Finally, Section 5 concludes the paper.\n2. CAPSNETS PROBLEM FORMULATION\nGenerally speaking, CNNs learn to classify objects using convolutional operations by first extracting low level features in initial layers and then stacking up the preliminary features learned through initial layers to extract more complex features in higher layers. However, CNN architecture ignores the hierarchy between layers (as is present in the human brain), which limits their modeling capabilities. CNNs try to overcome (mask) such limitations by utilization of a large amount of training data. The first problem (limitation) is that CNNs use sub-sampling in pooling steps to transfer more important features to the next layer. Therefore, some viewpoint changes and precise spatial relationships between higher level components\nwill be lost. The other problem is that CNNs are not robust against new viewpoints, because they can not extrapolate their geometric information. For these reasons, CapsNet has been proposed to replace invariance concept with equivalence, which means that if the input is spatially transformed, the network adapts accordingly and responses properly. A capsule is a group of neurons nested inside a layer and is considered as the main building block of the CapsNet architecture. Actually, the idea behind introducing a capsule is to encapsulate (possibly large) number of pose information (e.g., position, orientation, scaling, and skewness) together with other instantiation parameters (such as color and texture) for different parts or fragments of an object. This multilayer structure is deep in width instead of being deep in height. It can be considered as a parse tree because each active capsule chooses a capsule in the next layer as its parent in the tree. By incorporation of capsules instead of neurons and deepening the width, therefore, CapsNets can better handle different visual stimulus and provide a better translational invariance compared to pooling methods (max/average pooling) used in CNNs. Fig. 2 shows a nose CapsNet for face detection problem. In this illustrative example, there are 5 possible facial component capsules (we note that these are being automatically formed by the CapsNet) and 4 instantiation parameters (i.e., shifting, scaling, rotation, and deformation), which are also extracted automatically by the network. It is worth mentioning that in designing a CapsNet architecture, we only specify the number of capsules and the number of instantiation parameters per capsule, the network then learns specifics automatically.\nFig. 3 shows detection architecture for a CapsNet with the following three layers: (i) Convolutional layers; (ii) A PrimaryCapsule (PC) layer (the first capsule layer of a CapsNet architecture) consisting of N1 capsules, and; (iii) A Class-Capsule (CC) layer (the last capsule layer of a CapsNet architecture) with M capsules. For simplicity of the presentation, first we further describe the three layers CapsNet shown in Fig. 3. In this architecture, first the input is fed into the convolutional layers for extracting local features from the pixels. The next layer is the PC layer, where each capsule i, for (1 \u2264 i \u2264 N1), has an activity vector (ui \u2208 RP1 ) to encode P1 spatial information (instantiation parameters). Capsules in the PC layer can be grouped into several blocks referred to as \u201cComponent Capsules\u201d. In the next step, the output vector ui of the ith PC, for (1 \u2264 i \u2264 N1), is fed into the j th CC, for (1 \u2264 j \u2264M ), using weight matrix Wij \u2208 R(R\u00d7P1) and \u201cCoupling Coefficient\u201d cij as follows\nu\u0302j|i = Wijui and sj = N1\u2211 i=1 ciju\u0302j|i, (1)\nwhere u\u0302j|i is the prediction vector indicating how much the PC\ni contributes to the CC j, and scalar cij is a coupling coefficient which links predictions of the PC i to the CC j. Note that, vector sj is a weighted sum of all the PC predictions for the CC j. Also we note that, P1 denotes the dimension of each capsule in the PC layer while R denotes the dimension of each capsule in the CC layer. We also note that, one can introduce L \u2265 1 number of intermediate capsule layers located between the PC and CC layers each with Nl number of localized capsules. If needed, we use index l, for (2 \u2264 l \u2264 L+ 1), to refer to the intermediate capsule layers.\nCoupling coefficients are trained during the routing process such that for each capsule j in the CC layer, we have \u2211N1 i=1 cij = 1. Finally, the vector output for the CC j is obtained through non-linear squashing operator given by\nvj = \u2016sj\u20162 1 + \u2016sj\u20162 sj \u2016sj\u2016 . (2)\nIn each iteration, coupling coefficient cij is updated by agreement (using dot product) between vector vj , which is the output of the CC j, and vector u\u0302j|i, which is the prediction vector of the ith PC. Length of vector vj associated with the CC j, for (1 \u2264 j \u2264 M ), indicates the presence of an object represented by class j in the input.\nLet us further elaborate on the CapsNet structure introduced above, in terms of the example shown in Fig. 3. Here, we have P1 = 8 dimensional PCs, i.e., each capsule entity in the PC layer encodes eight different types of information. Besides, the PC layer consists of 32 component capsules (cubes) each of which consisting of 36 capsules, shown by \u201carrows\u201d in the last component capsule. Therefore, the PC layer in this illustrative example has N1 = 36\u00d7 32 = 1152 individual capsules. Dimension of capsules in the CC layer is R = 2 and there are total of M = 10 individual capsules in the CC layer. Finally, in terms of our running example, v6 has the largest magnitude among the CCs considered here, therefore, the CapsNet\u2019s output decision will be digit 6. This was a brief overview of the CapsNet architecture, next we will investigate its explanability properties.\n3. CAPSNETS\u2019 EXPLAINABILITY\nIn this section, we look at CapsNet architecture described in the previous section from different angles to see whether or not this revolutionized deep learning model has improved explanability characteristics. As we mentioned previously, CNNs can not preserve spatial relationship between components of an object because, some features will be discarded during the pooling process. CNNs compensate this deficiency by increasing the number of training data.\nIn CapsNets, we have capsules (a group of neurons) instead of neurons (single units). Each capsule performs some internal complicated computations and its output is a vector instead of a scalar value. In particular, we investigate the potential properties of using such vectorized outputs, which could lead to more explainability."}, {"heading": "3.1. Relevance Path by Agreement", "text": "The vector representation provided by CapsNets is highly informative and can model possible instantiation parameters for components or fragments of an object [21]. We argue that this vector output of the capsules can essentially lead to improved explanability of the overall network. In other words, while the length (magnitude) of the output vector vj corresponding to capsule j in the CC layer is used to make decisions regarding the input image, the length \u2016ui\u2016 of the output vector ui from the of ith capsule in the PC layer or an intermediate capsule layer can be interpreted as probability of existence of the feature that this capsule has been trained to detect. More specifically, we can assign to each capsule a set consisting of two segments for explanation purposes:\n(i) Likelihood values which can be used to explain existence probability of the feature that a capsule detects, and;\n(ii) Instantiation parameter vector values which can be used to explain consistency among the layers. In other words, when all capsules of an object are in an appropriate relationship with consistence parameters, the higher level capsule of that object will have a higher likelihood. Therefore, explanations can be provided to describe why the network did detect an object.\nFor example, Fig. 4 shows the sets computed based on 5 intermediate component capsules referring to class face (j) in the CC layer. Regarding Item (i), the likelihood part of each of these capsules is relatively high explaining that the input contains all the facial components represented by these 5 component capsules with high probability. However, the network decision is that there is not a face in the input as the likelihood of the face capsule in the CC layer is relatively low. This can be explained based on the non-consistency among the instantiation parameters (Item (ii)). The output vector of the ith capsule in the PC layer is multiplied by its weight matrix Wij corresponding to the j th capsule in the CC layer, which has been learned through backpropagation algorithm to account for all possible transformations of the instantiation parameters. This multiplication is considered as the vote of capsule i for class j (which is represented by vector u\u0302j|i). When all related capsules have similar votes for the j th CC, it means that they agree to each other on presence of object j.\nCapsNet applies non-linear squashing function on output vectors (vj) in each iteration. It actually bounds likelihood of these vectors between 0 and 1, which means that it suppresses small vectors and preserves long vectors in the unit length{\nvj \u2248 \u2016sj\u2016sj \u2248 0, if sj is small vj \u2248 sj\u2016sj\u2016 \u2248 1, if sj is large\n(3)\nTherefore, during agreement iterations, unrelated capsules will become smaller and smaller and the related ones will be remained unchanged. Consequently, introduction of the squashing function results in the coupling coefficients c\u2217j associated with irrelevant capsules to approach zero while coupling coefficient corresponding to the ones responsible for the j th CC to increase. Hence, CapsNets intrinsically construct a relevance path (we refers to it as the relevance path by agreement concept) which eliminates the need for a backward process to construct the relevance path. The reason behind the exitance of the relevance path by agreement is that CapsNet uses dynamic routing instead of common pooling methods. In the other words, when a group of capsules agree for a parent (higher level component), they construct a part whole relationship which can be considered as a relevance path. For example in face prediction case, facial components (eyes, nose, mouth) in a particular relationship will detect a face as a higher level component (Fig. 2).\n4. EXPERIMENTAL SETUP In this section, we investigate explanation capabilities of the CapsNets on 28\u00d7 28 MNIST dataset [22]. The used architecture is similar to the one that has been presented in [20] as outlined in Table 1. After training CapsNet with MNIST training data, in a first experiment we varied each of the two parameters of the CC output vector corresponding to the detected digit and reconstructed it again to find out what is nature of the two plausible features that have been learned (i.e., to explain the learned features at the CC layer). Fig. 5\nillustrates the results for three digits. It is observed that by changing the parameters, thickness and shape of digits have been changed simultaneously. Therefore, we can consider them as explanation of the learned features. Fig. 6 displays the two parameters of the output vector associated with the detected digit capsule for all the testing dataset. As can be seen, the vector output of different digit capsules significantly overlap with each other, e.g., capsules 1/7; 6/9, and; 3/5 generate close values. Therefore, in misclassified samples, there is a high probability for detecting overlapped digits. Recognizing these failure points is another level of explanation that CapsNet can provide on MNIST dataset. Now, we found misclassified samples in 1000 testing data and display their output prediction vectors. In all cases, the true class had the second highest likelihood (magnitude) except one instance in which the third highest likelihood was corresponding to the true label. Moreover, we can usually see a high decreasing magnitude between third and forth positions. Therefore, one can use the likelihood values and present the second and third highest capsules as alternative solutions and explanations. Fig. 7 shows all misclassified samples in 1000 testing data. We reconstructed input digits by prediction and true label outputs. As we see, each input digit in the first row is similar to both reconstructed digits bellow it, which explains why the model has been failed in these cases.\n5. CONCLUSION In this paper, we represented the necessity of explainability in deep neural networks especially in critical decisions where a single incorrect decision is even unacceptable. Previous explainability methods try to find and visualize the most relevant pixels or neurons by adding an extra explanation phase. In this work, we illustrated potential intrinsic explainability properties of Capsule network, by analyzing its behavior and structure.\n6. REFERENCES\n[1] C.M. Bishop, \u201cPattern Recognition and Machine Learning,\u201d Springer-Verlag New York, 2006.\n[2] M. Havaei, A. Davy, D. Warde-Farley, A. Biard, A. Courville, Y. Bengio, C. Pal, P.M. Jodoin, and H. Larochelle, \u201cBrain Tumor Segmentation with Deep Neural Networks,\u201d arXiv preprint arXiv:1505.03540, 2015.\n[3] M. Bojarski, D.D. Testa, D. Dworakowski, B. Firner, B. Flepp, P. Goyal, L.D. Jackel, M. Monfort, U. Muller, J. Zhang, X. Zhang, J. Zhao, and K. Zieba, \u201cEnd to End Learning for SelfDriving Cars,\u201d arXiv preprint arXiv:1604.07316, 2016.\n[4] I. Goodfellow, Y. Bengio, and A. Courville, \u201cDeep Learning,\u201d MIT press, 2016.\n[5] A. Krizhevsky, I. Sutskever and G.E .Hinton, \u201cImagenet Classification with Deep Convolutional Neural Networks,\u201d Advances in neural information processing systems, pp. 1097- 1105, 2012.\n[6] M.T. Ribeiro, S. Singh and C. Guestrin, \u201cWhy Should I Trust You?: Explaining the Predictions of Any Classifier,\u201d Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 1135-1144, 2016.\n[7] W. Samek, T. Wiegand, and K.R. Mu\u0308ller, \u201cExplainable Artificial Intelligence: Understanding, Visualizing and Interpreting Deep Learning Models,\u201d ITU Journal: ICT Discoveries, Special Issue The Impact of AI on Communication Networks and Services, vol. 1, pp. 1-10, 2017.\n[8] Z.C. Lipton, \u201cThe Mythos of Model Interpretability,\u201d arXiv preprint arXiv:1606.03490, 2016.\n[9] M. Kahng, P.Y. Andrews, A. Kalro, and D.H. Chau, \u201cActiVis: Visual Exploration of Industry-Scale Deep Neural Network Models,\u201d IEEE Transactions on Visualization and Computer Graphics, vol. 24, no. 1, pp. 88-97, Jan. 2018.\n[10] D. Baehrens, T. Schroeter, S. Harmeling, M. Kawanabe, K. Hansen, K.R. Mu\u0308ller \u201cHow to Explain Individual Classification Decisions,\u201d Journal of Machine Learning Research, vol. 11, pp. 1803-1831, 2010.\n[11] S. Bach, A. Binder, G. Montavon, F. Klauschen, K.R. Mu\u0308ller, W. Samek, \u201cOn Pixel-Wise Explanations for Non-linear Clas-\nsifier Decisions by Layer-Wise Relevance Propagation,\u201d PloS one, vol. 10, no. 7, pp. 130-140, 2015.\n[12] P.W. Koh, and P. Liang, \u201cUnderstanding Black-Box Predictions via Influence Functions,\u201d arXiv preprint arXiv:1703.04730, 2017.\n[13] C.C. Jay Kuo, Yueru Chen, \u201cOn data-driven Saak Transform,\u201d Journal of Visual Communication and Image Representation, vol. 50, pp. 237-246, 2018\n[14] W. Guo, K. Zhang, L. Lin, S. Huang, X. Xing, \u201cTowards Interrogating Discriminative Machine Learning Models,\u201d arXiv preprint arXiv:1705.08564 , 2017.\n[15] W. Samek, T. Wiegand, and K.R. Mu\u0308ller, \u201cMethods for Interpreting and Understanding Deep Neural Networks,\u201d Digital Signal Processing, vol. 73, pp. 1-15, 2018.\n[16] W. Samek, A. Binder, G. Montavon, S. Bach,and K.R. Mu\u0308ller, \u201cEvaluating the Visualization of What a Deep Neural Network has Learned,\u201d IEEE Transactions on Neural Networks and Learning Systems, vol. 28, no. 11, pp. 2660-2673, 2017.\n[17] S. Goferman, L. Zelnik-Manor, and A. Tal, \u201cContext-aware Saliency Detection,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 34, no. 10, pp. 1915-1926, 2012.\n[18] R.R. Selvaraju, A. Das, R. Vedantam and M. Cogswell, \u201cGrad-CAM: Why did you say that?\u201d arXiv preprint arXiv:1611.07450, 2016.\n[19] X. Huang, M. Kwiatkowska, S. Wang and M. Wu, \u201cSafety Verification of Deep Neural Networks,\u201d In International Conference on Computer Aided Verification pp. 3-29. Springer, Cham, 2017.\n[20] S. Sabour, N. Frosst, and G.E. Hinton, \u201cDynamic Routing between Capsules,\u201d Advances in Neural Information Processing Systems, pp. 3859-3869. 2017.\n[21] G.E. Hinton, A. Krizhevsky, and S.D. Wang, \u201cTransforming Auto-encoders,\u201d International Conference on Artificial Neural Networks, pp. 44-51. Springer, Berlin, Heidelberg, 2011.\n[22] Y. LeCun, \u201cThe MNIST database of Handwritten Digits,\u201d 1998."}], "title": "IMPROVED EXPLAINABILITY OF CAPSULE NETWORKS: RELEVANCE PATH BY AGREEMENT", "year": 2018}