{"abstractText": "We examine the problem of explainable AI (xAI) and explore what delivering xAI means in practice, particularly in contexts that involve formal or informal and ad-hoc collaboration where agency and accountability in decision-making are achieved and sustained interactionally. We use an example from an earlier study of collaborative decision-making in screening mammography and the difficulties users faced when trying to interpret the behavior of an AI tool to illustrate the challenges of delivering usable and effective xAI. We conclude by setting out a study programme for future research to help advance our understanding of xAI requirements for safe and ethical AI. Permission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). CSCW Extended Abstracts \u00a9 2020 Copyright is held by the author/owner(s). ACM ISBN 978-1-4503-5971-9/19/05. DOI: https://doi.org/10.1145/3290607.XXXXXXX Remove headers & page numbers in the footers from your final version Remove headers & page numbers in the footers from your final version", "authors": [{"affiliations": [], "name": "Rob Procter"}, {"affiliations": [], "name": "Peter Tolmie"}, {"affiliations": [], "name": "Mark Rouncefield"}], "id": "SP:03f323f1476a8e7ba6723de17f07694c404e203a", "references": [{"authors": ["David Leslie"], "title": "Understanding artificial intelligence ethics and safety", "venue": "arXiv preprint arXiv:1906.05684", "year": 2019}, {"authors": ["Diogo Carvalho", "Eduardo M. Pereira", "Jaime S. Cardoso"], "title": "Machine learning interpretability: A survey on methods and metrics", "venue": "Electronics 8,", "year": 2019}, {"authors": ["Mengnan Du", "Ninghao Liu", "Xia Hu"], "title": "Techniques for interpretable machine learning", "venue": "Communications of the ACM", "year": 2019}, {"authors": ["Quentin Meteier", "Marine Capallera", "Leonardo Angelini", "Elena Mugellini"], "title": "Workshop on explainable AI in automated driving: a user-centered interaction approach", "venue": "In Proceedings of the 11th International Conference on Automotive User Interfaces and Interactive Vehicular Applications: Adjunct Proceedings (pp. 32-37)", "year": 2019}, {"authors": ["Brent Mittelstadt", "Chris Russell", "Sandra Wachter"], "title": "Explaining explanations in AI", "venue": "In Proceedings of the conference on fairness,", "year": 2019}, {"authors": ["Leilani Gilpin", "David Bau", "Ben Z. Yuan", "Ayesha Bajwa", "Michael Specter", "Lalana Kagal"], "title": "Explaining explanations: An approach to evaluating interpretability of machine learning", "venue": "arXiv preprint arXiv:1806.00069", "year": 2018}, {"authors": ["Mark Hartswood", "Rob Procter", "Mark Rouncefield", "Roger Slack", "James Soutter", "Alex Voss"], "title": "Repairing\u2019 the Machine: A Case Study of the Evaluation of Computer-Aided Detection Tools in Breast Screening", "venue": "ECSCW", "year": 2003}, {"authors": ["Mark Hartswood", "Rob Procter", "Mark Rouncefield", "Roger Slack"], "title": "Cultures of reading in mammography", "venue": "Orders of Ordinary Action: Respecifying Sociological Knowledge. Ashgate Publishing", "year": 2003}, {"authors": ["Richard Mortier", "Hamed Haddadi", "Tristan Henderson", "Derek McAuley", "Jon Crowcroft"], "title": "Human-data interaction: The human face of the data-driven society.\" Available at SSRN 2508051", "year": 2014}, {"authors": ["Stuart Anderson", "Mark Hartswood", "Rob Procter", "Mark Rouncefield", "Roger Slack", "James Soutter", "Alexander Voss"], "title": "Making autonomic computing systems accountable: the problem of human computer interaction", "venue": "In 14th International Workshop on Database and Expert Systems Applications,", "year": 2003}, {"authors": ["Mary Cummings"], "title": "Automation and accountability in decision support system interface design", "year": 2006}, {"authors": ["Philip Brey"], "title": "Values in technology and disclosive computer ethics", "venue": "The Cambridge handbook of information and computer ethics,", "year": 2010}, {"authors": ["Philip Brey"], "title": "Anticipating ethical issues in emerging IT", "venue": "Ethics and Information Technology,", "year": 2012}, {"authors": ["Graham Button", "Paul Dourish"], "title": "Technomethodology: paradoxes and possibilities", "venue": "In CHI (Vol", "year": 1996}, {"authors": ["Christian Heath", "Paul Luff"], "title": "Collaborative activity and technological design: Task coordination in London Underground control rooms", "venue": "In Proceedings of the Second European Conference on Computer-Supported Cooperative Work", "year": 1991}, {"authors": ["Rob Procter", "Mark Rouncefield", "Ellen Balka", "Mark Berg"], "title": "CSCW and dependable healthcare systems", "venue": "Computer Supported Cooperative Work (CSCW),", "year": 2006}, {"authors": ["Lucy Suchman", "Randall Trigg", "Jeanette Blomberg"], "title": "Working artefacts: ethnomethods of the prototype", "venue": "The British Journal of Sociology,", "year": 2002}, {"authors": ["Juliet Corbin", "Anselm Strauss"], "title": "The articulation of work through interaction", "venue": "The sociological quarterly,", "year": 1993}, {"authors": ["Kjeld Schmidt"], "title": "Cooperative work and its articulation: requirements for computer support", "venue": "Le travail humain,", "year": 1994}, {"authors": ["Lucy Suchman"], "title": "Supporting articulation work. Computerization and controversy: Value conflicts and social choices, 2, 407-423. Remove headers & page numbers in the footers from your final version Remove headers & page numbers in the footers from your final version", "year": 1996}, {"authors": ["B. Breda Gray", "Luigina Ciolfi"], "title": "Made To Work: Mobilising Contemporary Worklives", "year": 2020}, {"authors": ["Graham Button", "Wes Sharrock"], "title": "The production of order and the order of production: possibilities for distributed organisations, work and technology in the print industry", "venue": "In Proceedings of the Fifth European Conference on Computer Supported Cooperative Work (pp", "year": 1997}, {"authors": ["Karen Clarke", "Gillian Hardstone", "Mark Rouncefield", "Ian Sommerville", "I. (Eds"], "title": "Trust in technology: A sociotechnical perspective (Vol. 36)", "year": 2006}, {"authors": ["Alex Voss", "Rob Procter", "Roger Slack", "Mark Hartswood", "Mark Rouncefield"], "title": "Understanding and supporting dependability as ordinary action. In Trust in Technology: A Socio-Technical Perspective (pp. 195-216)", "year": 2006}, {"authors": ["Scott McKinney", "Marcin Sieniek", "Varun Godbole", "Jonathan Godwin", "Natasha Antropova", "Hutan Ashrafian", "Trevor Back"], "title": "International evaluation of an AI system for breast cancer screening", "venue": "Nature 577,", "year": 2020}, {"authors": ["Mark Hartswood", "Rob Procter", "Mark Rouncefield", "Roger Slack"], "title": "Cultures of Reading in Mammography", "venue": "Orders of Ordinary Action: Respecifying Sociological Knowledge. Ashgate Publishing", "year": 2007}, {"authors": ["Marina Jirotka", "Rob Procter", "Mark Hartswood", "Roger Slack", "Catelijne Coopmans", "Chris Hinds", "Alex Voss"], "title": "Collaboration and Trust in Healthcare Innovation: the eDiaMoND Case Study", "year": 2005}, {"authors": ["Eugenio Alberdi", "Andrey Povyakal", "Lorenzo Strigini", "Mark Hartswood", "Rob Procter", "R. Roger Slack"], "title": "The use of Computer Aided Detection tools in screening mammography: A multidisciplinary investigation", "venue": "British Journal of Radiology, special issue on Computer-aided diagnosis,", "year": 2005}, {"authors": ["Elaine Victorelli", "Julio Cesar Dos Reis", "Heiko Hornung", "Alysson Prado"], "title": "Understanding human-data interaction: Literature review and recommendations for design", "venue": "International Journal of Human-Computer Studies", "year": 2019}, {"authors": ["David Martin", "Ian Sommerville"], "title": "Patterns of cooperative interaction: Linking ethnomethodology and design", "venue": "ACM Transactions on Computer-Human Interaction (TOCHI),", "year": 2004}, {"authors": ["David Martin", "Mark Rouncefield", "Ian Sommerville"], "title": "Patterns for dependable design. In Trust in Technology: A Socio-technical Perspective (pp. 147-168)", "year": 2006}, {"authors": ["Mark Hartswood", "Rob Procter", "Phillipe Rouchy", "Mark Rouncefield", "Roger Slack", "Alex Voss"], "title": "Co-realisation: Towards a Principled Synthesis of Ethnomethodology and Participatory Design", "venue": "special issue on Challenging Practice: Reflections on the Appropriateness of Fieldwork as Research Method in Information Systems Research, Scandinavian Journal of Information Systems,", "year": 2002}, {"authors": ["Alex Voss", "Rob Procter", "Roger Slack", "Mark Hartswood", "Mark Rouncefield"], "title": "Design as and for collaboration: Making sense of and supporting practical action", "year": 2009}, {"authors": ["Michael Muller", "Melanie Feinberg", "Timothy George", "Steven Jackson", "Bonnie John", "Mary Beth Kery", "Samir Passi"], "title": "Human-Centered Study of Data Science Work Practices", "venue": "In Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems. ACM", "year": 2019}, {"authors": ["Robin Williams", "James Stewart", "Roger Slack"], "title": "Social learning in technological innovation: Experimenting with information and communication technologies", "venue": "Edward Elgar Publishing", "year": 2005}, {"authors": ["Robert Anderson"], "title": "Representations and requirements: the value of ethnography in system design", "venue": "Human-computer interaction,", "year": 1994}, {"authors": ["John Hughes", "Val King", "Tom Rodden", "Hans Andersen"], "title": "Moving out from the control room: ethnography in system design", "venue": "In Proceedings of the 1994 ACM conference on Computer supported cooperative work (pp. 429-439)", "year": 1994}, {"authors": ["Graham Button", "Richard Harper"], "title": "The relevance of \u2018work-practice\u2019 for design", "venue": "Computer Supported Cooperative Work (CSCW),", "year": 1995}, {"authors": ["Dave Randall", "Richard Harper", "Mark Rouncefield"], "title": "Fieldwork for design: theory and practice", "year": 2007}, {"authors": ["Richard Bentley", "John Hughes", "Dave Randall", "Tom Rodden", "Pete Sawyer", "Dan Shapiro", "Ian Sommerville"], "title": "Ethnographically-informed systems design for air traffic control", "venue": "In CSCW,", "year": 1992}], "sections": [{"text": "We examine the problem of explainable AI (xAI) and explore what delivering xAI means in practice, particularly in contexts that involve formal or informal and ad-hoc collaboration where agency and accountability in decision-making are achieved and sustained interactionally. We use an example from an earlier study of collaborative decision-making in screening mammography and the difficulties users faced when trying to interpret the behavior of an AI tool to illustrate the challenges of delivering usable and effective xAI. We conclude by setting out a study programme for future research to help advance our understanding of xAI requirements for safe and ethical AI.\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). CSCW Extended Abstracts \u00a9 2020 Copyright is held by the author/owner(s). ACM ISBN 978-1-4503-5971-9/19/05. DOI: https://doi.org/10.1145/3290607.XXXXXXX\nKEYWORDS\nExplainable AI; xAI; accountability; accounts; agency; collaboration; ethnography"}, {"heading": "INTRODUCTION", "text": "Explainable AI (xAI), i.e., AI\u2019s capacity for transparency and interpretability, is now recognised as a key requirement for the effective and safe application of AI tools [1]. As a consequence, much effort is now being devoted to developing ways to address the limited degree to which current machine learning techniques are able to satisfy this requirement [2-5]. This has led to the identification of a number of promising techniques for xAI [6], such as contrastive explanations, which are counterfactuals that may be applied globally (i.e., techniques that attempt to explain the model as a whole) or locally (i.e. techniques that attempt to explain the model\u2019s behaviour for a specific input). Recently, we have seen evidence of a \u2018turn to the social\u2019 in xAI:\n\u201cExplanations are social, insofar as they involve an interaction between one or more explainers and explainees\u2026\u201d [5]\nWe therefore need to ensure that research in xAI is able to lead to methods for producing explanations that reflect this understanding. Below, we unpack what meeting this requirement of xAI entails through the lenses of agency and accountability."}, {"heading": "UNDERSTANDING REQUIREMENTS FOR xAI", "text": "Designing explainability into AI tools is essential if they are to be trusted [7] and if their users are to be able to exercise agency when making decisions, whether they be professional [7-8] or lay users [9]. In other words, AI tools must be accountable to users for the ways in which they behave [10-11]. There are clearly moral, ethical and legal aspects to this. Of particular relevance is Brey\u2019s notion of the \u2018disclosive\u2019 or embedded ethics of computer systems [12-13]. Fulfilling these requirements is challenging for a number of reasons.\nFirst is the problem of designing accounts \u2013 \u201ccomputational representations which systems continuously offer of their own behaviour and activity\u201d [14] \u2013 so that they will be understood by the user in a range of different circumstances and thus satisfy a requirement of being \u201ca resource for improvised and contextualised action\u201d (Ibid), while preserving the user\u2019s agency as a decision-maker.\nSecondly, in the workplace, agency is a property that is distributed and performed collaboratively. Thus, it depends on participants being able to make sense of the actions of others and to make their own actions understood by others [8, 15-17]. Hence, accounts furnished by AI tools may also serve as resources for collaboration and their design should not only satisfy the needs of individual users but also reflect how (formally and informally) collaboration is done. In other words, they must afford the articulation work [18] on which participants may rely for maintaining their mutual accountability [15, 19-20]. This is a feature of what Gray et al. [21] term \u2018corollary work\u2019: \u201cemergent socio-material practices of place, time, productivity and self-expression\u201d.\nThirdly, whether by design or unintentionally, new technologies in the workplace are likely to result in changes in work practices [22] and the implications of these changes for AI tool accounts for agency and mutual accountability must be understood if tasks are to be performed in ways that are timely and dependable [23-24].\nFinally, and not least, these accounts may assume a formal role in that, by being recordable, they may be seen as furnishing ways to further organisational goals of holding members to account for their past actions [25]. So, the implications of this also need to be investigated and understood."}, {"heading": "AN EXAMPLE: READING MAMMOGRAMS IN THE UK BREAST SCREENING PROGRAMME", "text": "Healthcare is a particularly active area for the adoption of AI. Recently, for example, trials of new AI tools for screening mammography have been conducted in the UK [26]. A previous study we conducted of radiologists reading mammograms in the UK breast screening programme may help to understand what the requirements for xAI might be in this context [27-28].\nIn the UK breast screening programme, mammograms are read by two radiologists and the recall/no recall decision is made on the basis of these two (semi-)independent assessments (ref). We see in Figure 1 a radiologist examining mammograms (2 views: \u2018oblique\u2019 and \u2018CC\u2019). As accounts, mammograms may seem of limited value, but the radiologist is able to work up an account by, e.g.: (a) comparing features across the 2 views; (b) using a magnifying glass; and (c) measuring features using their hands [29].\nFigure 2 shows the screening form that is used to record the comments and decisions the participants make during the process. This begins when the mammograms are taken by the radiographer, who records information about, e.g., exposure but also information gleaned from the woman (cyst; moles; Pain L breast and arm, GP thinks is muscular) that the radiographer adds in order to be accountable. Comments are indexically tied to the mammograms through marking the simple schematics. Similarly, the 1st radiologist doesn\u2019t just record recall/no recall but adds a comment (new), which is then available to the 2nd radiologist, who adds a final comment (BT I think, HRT related). The combination of films and form provides accountability for the people involved and thus for the whole process.\nFigure 3 shows an early prototype of a computer-aided mammography tool. At the top are the original mammograms and at the bottom are displays showing prompts for areas the system (CADe) thinks are suspicious. We studied radiologists using CADe in order to understand how they made sense of the prompts and how the prompts influenced their decisions [30]. Our studies showed that reading mammograms has collaborative aspects, achieved through making available features that are professionally relevant. This is what Goodwin [31] terms \u2018professional vision\u2019, a \u2018way of seeing\u2019, a technique for making relevant features available and accountable, and with which any technologies need to interact and accommodate. However, evidently, the machine knows nothing of what it is to be a competent, professional reader and is (currently) unable to \u2018explain\u2019 its prompts and detections; instead the reader must \u2018repair\u2019 what the machine shows, thereby making it \u2018accountable\u2019.\nFigure 2: Example of a UK Breast Screening report form.\nThis then is the problem of xAI. As CADe was not capable of providing an account for its recall/no recall prompts, the radiologists needed to come up with one based on their accumulated observations of its behaviour. Though useful at times, such ad-hoc explanations may also be unreliable and misleading, underlining the importance of the problem that xAI aims to address."}, {"heading": "A STUDY PROGRAMME FOR xAI", "text": "We plan to investigate, through a series of case studies in different work settings, the requirements for xAI in context. The findings will be used to inform recommendations for design [32], including identification of design patterns [33-34] for accountability that have potential to be reusable in future xAI design. The case studies will also be used to explore co-production methodologies that enable users to have agency in the design and development process [35-37] and to ensure that organisational learning is properly supported over time [38].\nWe will use a combination of interviews and ethnographic studies to explore work practices in a range of current or potential settings for the application of AI tools. Following the \u2018turn to the social\u2019 [38-42], ethnography has become a well-established tool for IT systems requirements gathering. Its value lies in its capacity to recover the \u2018real world\u2019 aspects of a setting, identifying the exceptions, contradictions and contingencies of activities that do not necessarily figure in their more formal representations.\nCase studies will be drawn from actual or potential application domains for AI tools, including: healthcare: digital pathology; assistive technologies; and air traffic control. Regarding the latter, Bentley et al.\u2019s [43] study of air traffic control is a seminal example of the value of ethnography for IT systems requirements gathering, revealing important but sometimes neglected ways in which social affordances of technologies contribute to dependable conduct and decision-making."}, {"heading": "ACKNOWLEDGMENTS", "text": "We would like to acknowledge the support of the UK Alan Turing Institute for Data Science and AI."}], "title": "Accounts, Accountability and Agency for Safe and Ethical AI", "year": 2020}