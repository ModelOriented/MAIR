{"abstractText": "While the interpretability of machine learning models is often equated with their mere syntactic comprehensibility, we think that interpretability goes beyond that, and that human interpretability should also be investigated from the point of view of cognitive science. In particular, the goal of this paper is to discuss to what extent cognitive biases may affect human understanding of interpretable machine learning models, in particular of logical rules discovered from data. Twenty cognitive biases are covered, as are possible debiasing techniques that can be adopted by designers of machine learning algorithms and software. Our review transfers results obtained in cognitive psychology to the domain of machine learning, aiming to bridge the current gap between these two areas. It needs to be followed by empirical studies specifically focused on the machine learning domain.", "authors": [{"affiliations": [], "name": "Tom\u00e1\u0161 Kliegra"}, {"affiliations": [], "name": "\u0160t\u011bp\u00e1n Bah\u0144\u0131k"}, {"affiliations": [], "name": "Johannes F\u00fcrnkranz"}], "id": "SP:010c1ae4cff7441f7c8301adc482c247f21e91f2", "references": [{"authors": ["R. Agrawal", "H. Mannila", "R. Srikant", "H. Toivonen", "A.I. Verkamo"], "title": "Advances in Knowledge Discovery and Data Mining", "venue": "Fast discovery of association rules,", "year": 1995}, {"authors": ["D. Albarra\u0107\u0131n", "A.L. Mitchell"], "title": "The role of defensive confidence in preference for proattitudinal information: How believing that one is strong can sometimes be a defensive weakness", "venue": "Personality and Social Psychology Bulletin", "year": 2004}, {"authors": ["J. Alcala-Fdez", "R. Alcala", "F. Herrera"], "title": "A fuzzy association rulebased classification model for high-dimensional problems with genetic rule selection and lateral tuning", "venue": "IEEE Transactions on Fuzzy Systems", "year": 2011}, {"authors": ["J. Anderson", "D. Fleming"], "title": "Analytical procedures decision aids for generating explanations: Current state of theoretical development and implications of their use", "venue": "Journal of Accounting and Taxation", "year": 2016}, {"authors": ["H.R. Arkes"], "title": "Costs and benefits of judgment errors: Implications for debiasing", "venue": "Psychological Bulletin", "year": 1991}, {"authors": ["H.R. Arkes", "C. Christensen", "C. Lai", "C. Blumer"], "title": "Two methods of reducing overconfidence", "venue": "Organizational Behavior and Human Decision Processes", "year": 1987}, {"authors": ["P.J. Azevedo", "A.M. Jorge"], "title": "Comparing rule measures for predictive association", "venue": "Proceedings of the 18th European Conference on Machine Learning (ECML-07),", "year": 2007}, {"authors": ["M. Bar-Hillel"], "title": "The role of sample size in sample evaluation", "venue": "Organizational Behavior and Human Performance", "year": 1979}, {"authors": ["M. Bar-Hillel"], "title": "Commentary on Wolford, Taylor, and Beck: The conjunction fallacy", "venue": "Memory & Cognition", "year": 1991}, {"authors": ["M. Bar-Hillel", "E. Neter"], "title": "How alike is it versus how likely is it: A disjunction fallacy in probability judgments", "venue": "Journal of Personality and Social Psychology", "year": 1993}, {"authors": ["I. Barberia", "F. Blanco", "C.P. Cubillas", "H. Matute"], "title": "Implementation and assessment of an intervention to debias adolescents against causal illusions", "venue": "PLoS One", "year": 2013}, {"authors": ["A.K. Barbey", "S.A. Sloman"], "title": "Base-rate respect: From ecological rationality to dual processes", "venue": "Behavioral and Brain Sciences", "year": 2007}, {"authors": ["J. Baron", "J. Beattie", "J.C. Hershey"], "title": "Heuristics and biases in diagnostic reasoning: II congruence, information, and certainty", "venue": "Organizational Behavior and Human Decision Processes", "year": 1988}, {"authors": ["C.P. Beaman", "R. McCloy", "P.T. Smith"], "title": "When does ignorance make us smart? Additional factors guiding heuristic inference, in: Proceedings of the Cognitive Science Society", "year": 2006}, {"authors": ["E.S. Becker", "M. Rinck"], "title": "Reversing the mere exposure effect in spider fearfuls: Preliminary evidence of sensitization", "venue": "Biological Psychology", "year": 2016}, {"authors": ["P. Berka"], "title": "Comprehensive concept description based on association rules: A meta-learning approach", "venue": "Intelligent Data Analysis", "year": 2018}, {"authors": ["A. Bibal", "B. Fr\u00e9nay"], "title": "Interpretability of machine learning models and representations: an introduction", "venue": "in: Proceedings of the 24th European Symposium on Artificial Neural Networks (ESANN),", "year": 2016}, {"authors": ["L.E. Boehm"], "title": "The validity effect: A search for mediating variables", "venue": "Personality and Social Psychology Bulletin 20,", "year": 1994}, {"authors": ["S.D. Bond", "K.A. Carlson", "M.G. Meloy", "J.E. Russo", "R.J. Tanner"], "title": "Information distortion in the evaluation of a single option", "venue": "Organizational Behavior and Human Decision Processes", "year": 2007}, {"authors": ["R.F. Bornstein"], "title": "Exposure and affect: overview and meta-analysis of research, 1968\u20131987", "venue": "Psychological Bulletin", "year": 1989}, {"authors": ["P.D. Bruza", "Z. Wang", "J.R. Busemeyer"], "title": "Quantum cognition: a new theoretical approach to psychology", "venue": "Trends in Cognitive Sciences", "year": 2015}, {"authors": ["C. Camerer", "M. Weber"], "title": "Recent developments in modeling preferences: Uncertainty and ambiguity", "venue": "Journal of Risk and Uncertainty", "year": 1992}, {"authors": ["A. Carlson", "J. Betteridge", "B. Kisiel", "B. Settles", "E.R. Hruschka Jr.", "T.M. Mitchell"], "title": "Toward an architecture for never-ending language learning", "venue": "in: Proceedings of the 24th AAAI Conference on Artificial Intelligence,", "year": 2010}, {"authors": ["G. Charness", "E. Karni", "D. Levin"], "title": "On the conjunction fallacy in probability judgment: New experimental evidence regarding Linda", "venue": "Games and Economic Behavior 68,", "year": 2010}, {"authors": ["R.T. Clemen", "K.C. Lichtendahl"], "title": "Debiasing expert overconfidence: A Bayesian calibration model, in: Sixth International Conference on Probablistic Safety Assessment and Management (PSAM6)", "year": 2002}, {"authors": ["W.G. Cochran"], "title": "Sampling techniques", "year": 2007}, {"authors": ["P. Croskerry", "G. Singhal", "S. Mamede"], "title": "Cognitive debiasing 2: impediments to and strategies for change", "venue": "BMJ Quality & Safety", "year": 2013}, {"authors": ["P.B. De Laat"], "title": "Algorithmic decision-making based on machine learning from big data: Can transparency restore accountability", "year": 2017}, {"authors": ["A. Dech\u00eane", "C. Stahl", "J. Hansen", "M. W\u00e4nke"], "title": "The truth about the truth: A meta-analytic review of the truth effect", "venue": "Personality and Social Psychology Review", "year": 2010}, {"authors": ["R. Deutsch", "R. Kordts-Freudinger", "B. Gawronski", "F. Strack"], "title": "Fast and fragile: A new look at the automaticity of negation processing", "venue": "Experimental Psychology", "year": 2009}, {"authors": ["C. D\u0131\u0301az", "C. Batanero", "J.M. Contreras"], "title": "Teaching independence and conditional probability", "venue": "Bolet\u0301\u0131n de Estad\u0301\u0131stica e Investigacio\u0301n Operativa 26,", "year": 2010}, {"authors": ["S. Donovan", "S. Epstein"], "title": "The difficulty of the Linda conjunction problem can be attributed to its simultaneous concrete and unnatural representation, and not to conversational implicature", "venue": "Journal of Experimental Social Psychology", "year": 1997}, {"authors": ["U.K. Ecker", "J.L. Hogan", "S. Lewandowsky"], "title": "Reminders and repetition of misinformation: Helping or hindering its retraction", "venue": "Journal of Applied Research in Memory and Cognition", "year": 2017}, {"authors": ["S.E. Edgell", "J. Harbison", "W.P. Neace", "I.D. Nahinsky", "A.S. Lajoie"], "title": "What is learned from experience in a probabilistic environment", "venue": "Journal of Behavioral Decision Making", "year": 2004}, {"authors": ["D. Ellsberg"], "title": "Risk, ambiguity, and the Savage axioms", "venue": "The Quarterly Journal of Economics", "year": 1961}, {"authors": ["J.S.B. Evans"], "title": "Bias in Human Reasoning: Causes and Consequences", "year": 1989}, {"authors": ["J.S.B. Evans"], "title": "Hypothetical thinking: Dual processes in reasoning and judgement", "year": 2007}, {"authors": ["J.S.B. Evans", "K.E. Stanovich"], "title": "Dual-process theories of higher cognition: Advancing the debate", "venue": "Perspectives on Psychological Science", "year": 2013}, {"authors": ["E. Fantino", "J. Kulik", "S. Stolarz-Fantino", "W. Wright"], "title": "The conjunction fallacy: A test of averaging hypotheses", "venue": "Psychonomic Bulletin & Review", "year": 1997}, {"authors": ["P.M. Fernbach", "A. Darlow", "S.A. Sloman"], "title": "When good evidence goes bad: The weak evidence effect in judgment and decision-making", "year": 2011}, {"authors": ["B. Fischoff"], "title": "Debiasing. Technical Report", "venue": "Decision Research. Eugene,", "year": 1981}, {"authors": ["J.E. Fisk"], "title": "Judgments under uncertainty: Representativeness or potential surprise", "venue": "British Journal of Psychology", "year": 2002}, {"authors": ["S.T. Fiske"], "title": "Attention and weight in person perception: The impact of negative and extreme behavior", "venue": "Journal of Personality and Social Psychology", "year": 1980}, {"authors": ["G.J. Fitzsimons", "B. Shiv"], "title": "Nonconscious and contaminative effects of hypothetical questions on subsequent decision making", "venue": "Journal of Consumer Research 28,", "year": 2001}, {"authors": ["M. Fleischmann", "M. Amirpur", "A. Benlian", "T. Hess"], "title": "Cognitive biases in information systems research: A scientometric analysis", "venue": "in: Proceedings of the 22st European Conference on Information Systems (ECIS", "year": 2014}, {"authors": ["D. Fleisig"], "title": "Adding information may increase overconfidence in accuracy of knowledge retrieval", "venue": "Psychological Reports", "year": 2011}, {"authors": ["G.T. Fong", "D.H. Krantz", "R.E. Nisbett"], "title": "The effects of statistical training on thinking about everyday problems", "venue": "Cognitive Psychology", "year": 1986}, {"authors": ["A.A. Freitas"], "title": "Comprehensible classification models: a position paper", "venue": "ACM SIGKDD Explorations", "year": 2014}, {"authors": ["J. F\u00fcrnkranz"], "title": "Pruning algorithms for rule learning", "venue": "Machine Learning", "year": 1997}, {"authors": ["J. F\u00fcrnkranz"], "title": "Separate-and-conquer rule learning", "venue": "Artificial Intelligence Review", "year": 1999}, {"authors": ["J. F\u00fcrnkranz", "P.A. Flach"], "title": "Roc nrule learningtowards a better understanding of covering algorithms", "venue": "Machine Learning", "year": 2005}, {"authors": ["J. F\u00fcrnkranz", "T. Kliegr", "H. Paulheim"], "title": "On cognitive preferences and the plausibility of rule-based models", "venue": "Machine Learning", "year": 2020}, {"authors": ["A. Gabriel", "H. Paulheim", "F. Janssen"], "title": "Learning semantically coherent rules, in: Proceedings of the 1st International Workshop on Interactions between Data Mining and Natural Language Processing colocated with The European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (DMNLP@ PKDD/ECML)", "venue": "CEUR Workshop Proceedings, Nancy,", "year": 2014}, {"authors": ["D. Gamberger", "N. Lavra\u010d"], "title": "Active subgroup mining: A case study in coronary heart disease risk group detection", "venue": "Artificial Intelligence in Medicine", "year": 2003}, {"authors": ["B. Ganter", "R. Wille"], "title": "Formal Concept Analysis \u2013 Mathematical Foundations", "year": 1999}, {"authors": ["A.B. Geier", "P. Rozin", "G. Doros"], "title": "Unit bias a new heuristic that helps explain the effect of portion size on food intake", "venue": "Psychological Science 17,", "year": 2006}, {"authors": ["C.F. Gettys", "S.D. Fisher", "T. Mehle"], "title": "Hypothesis Generation and Plausibility Assessment", "venue": "Technical Report. Decision Processes Laboratory, University of Oklahoma, Norman. Annual report TR 15-10-78", "year": 1978}, {"authors": ["C.F. Gettys", "T. Mehle", "S. Fisher"], "title": "Plausibility assessments in hypothesis generation", "venue": "Organizational Behavior and Human Decision Processes", "year": 1986}, {"authors": ["G. Gigerenzer"], "title": "Content-blind norms, no norms, or good norms? A reply to Vranas. Cognition", "year": 2001}, {"authors": ["G. Gigerenzer", "D.G. Goldstein"], "title": "Reasoning the fast and frugal way: models of bounded rationality", "venue": "Psychological Review", "year": 1996}, {"authors": ["G. Gigerenzer", "D.G. Goldstein"], "title": "Fast and frugal heuristics, in: Simple heuristics that make us smart", "year": 1999}, {"authors": ["G. Gigerenzer", "U. Hoffrage"], "title": "How to improve Bayesian reasoning without instruction: frequency formats", "venue": "Psychological Review", "year": 1995}, {"authors": ["G. Gigerenzer", "U. Hoffrage"], "title": "Overcoming difficulties in Bayesian reasoning: A reply to Lewis and Keren", "venue": "Mellers and McGraw", "year": 1999}, {"authors": ["D.G. Goldstein", "G. Gigerenzer"], "title": "The recognition heuristic: How ignorance makes us smart, in: Simple heuristics that make us smart", "year": 1999}, {"authors": ["H.P. Grice"], "title": "Logic and conversation, in: Speech Acts", "year": 1975}, {"authors": ["D. Griffin", "A. Tversky"], "title": "The weighing of evidence and the determinants of confidence", "venue": "Cognitive Psychology", "year": 1992}, {"authors": ["R. Guidotti", "A. Monreale", "S. Ruggieri", "F. Turini", "F. Giannotti", "D. Pedreschi"], "title": "A survey of methods for explaining black box models", "venue": "ACM computing surveys (CSUR)", "year": 2018}, {"authors": ["C.C. Hall", "L. Ariss", "A. Todorov"], "title": "The illusion of knowledge: When more information reduces accuracy and increases confidence", "venue": "Organizational Behavior and Human Decision Processes", "year": 2007}, {"authors": ["M.G. Haselton", "D. Nettle"], "title": "The paranoid optimist: An integrative evolutionary model of cognitive biases", "venue": "Personality and Social Psychology Review", "year": 2006}, {"authors": ["L. Hasher", "D. Goldstein", "T. Toppino"], "title": "Frequency and the conference of referential validity", "venue": "Journal of Verbal Learning and Verbal Behavior 16,", "year": 1977}, {"authors": ["F. Herrera", "L. Martinez"], "title": "An approach for combining linguistic and numerical information based on the 2-tuple fuzzy linguistic representation model in decision-making", "venue": "International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems", "year": 2000}, {"authors": ["R. Hertwig", "B. Benz", "S. Krauss"], "title": "The conjunction fallacy and the many meanings of and", "venue": "Cognition", "year": 2008}, {"authors": ["R. Hertwig", "G. Gigerenzer", "U. Hoffrage"], "title": "The reiteration effect in hindsight bias", "venue": "Psychological Review", "year": 1997}, {"authors": ["R. Hertwig", "S.M. Herzog", "L.J. Schooler", "T. Reimer"], "title": "Fluency heuristic: A model of how the mind exploits a by-product of information retrieval", "venue": "Journal of Experimental Psychology: Learning, Memory, and Cognition", "year": 2008}, {"authors": ["N.H. Hess", "E.H. Hagen"], "title": "Psychological adaptations for assessing gossip veracity", "venue": "Human Nature", "year": 2006}, {"authors": ["M. Huber"], "title": "From Mindless to Mindful Decision Making: Reflecting on Prescriptive Processes", "venue": "Ph.D. thesis. University of Colorado at Boulder", "year": 2010}, {"authors": ["Jiang", "Z.q", "Li", "W.h", "Y. Liu", "Luo", "Y.j", "P. Luu", "D.M. Tucker"], "title": "When affective word valence meets linguistic polarity: Behavioral and erp evidence", "venue": "Journal of Neurolinguistics", "year": 2014}, {"authors": ["P. Juslin", "H. Nilsson", "A. Winman"], "title": "Probability theory, not the very guide of life", "venue": "Psychological Review", "year": 2009}, {"authors": ["P. Juslin", "H. Nilsson", "A. Winman", "M. Lindskog"], "title": "Reducing cognitive biases in probabilistic reasoning by the use of logarithm formats", "year": 2011}, {"authors": ["S.J. Kachelmeier", "W.F. Messier Jr."], "title": "An investigation of the influence of a nonstatistical decision aid on auditor sample size decisions", "year": 1990}, {"authors": ["D. Kahneman", "A. Tversky"], "title": "Subjective probability: A judgment of representativeness", "venue": "Cognitive Psychology", "year": 1972}, {"authors": ["D. Kahneman", "A. Tversky"], "title": "On the psychology of prediction", "venue": "Psychological Review", "year": 1973}, {"authors": ["L.S. Kane", "B.A. Lown"], "title": "Stevens\u2019 power law and time perception: Effect of filled intervals, duration of the standard, and number of presentations of the standard", "venue": "Perceptual and motor skills", "year": 1986}, {"authors": ["J.M. Keynes"], "title": "A Treatise on Probability", "year": 1922}, {"authors": ["J. Klayman", "Y.W. Ha"], "title": "Confirmation, disconfirmation, and information in hypothesis testing", "venue": "Psychological Review 94,", "year": 1987}, {"authors": ["T. Kliegr", "V. Sv\u00e1tek", "M. Ralbovsk\u00fd", "M. \u0160im\u016fnek"], "title": "SEWEBAR- CMS: Semantic analytical report authoring for data mining results", "venue": "Journal of Intelligent Information Systems", "year": 2011}, {"authors": ["I. Kononenko"], "title": "Inductive and Bayesian learning in medical diagnosis", "venue": "Applied Artificial Intelligence", "year": 1993}, {"authors": ["Z. Kunda"], "title": "Social Cognition: Making Sense of People", "year": 1999}, {"authors": ["I. Lage", "E. Chen", "J. He", "M. Narayanan", "B. Kim", "S. Gershman", "F. Doshi- Velez"], "title": "An evaluation of the human-interpretability of explanation, in: Advances in Neural Information Processing", "year": 2018}, {"authors": ["H. Lakkaraju", "S.H. Bach", "J. Leskovec"], "title": "Interpretable decision sets: A joint framework for description and prediction", "venue": "in: Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, ACM,", "year": 2016}, {"authors": ["R.P. Larrick"], "title": "Debiasing. Blackwell Handbook of Judgment and Decision Making", "year": 2004}, {"authors": ["A.Y. Lau", "E.W. Coiera"], "title": "Can cognitive biases during consumer health information searches be reduced to improve decision making", "venue": "Journal of the American Medical Informatics Association", "year": 2009}, {"authors": ["B. Letham", "C. Rudin", "T.H. McCormick", "D Madigan"], "title": "Interpretable classifiers using rules and Bayesian analysis: Building a better stroke prediction model", "venue": "The Annals of Applied Statistics", "year": 2015}, {"authors": ["S. Lewandowsky", "U.K. Ecker", "C.M. Seifert", "N. Schwarz", "J. Cook"], "title": "Misinformation and its correction: Continued influence and successful debiasing", "venue": "Psychological Science in the Public Interest", "year": 2012}, {"authors": ["S.O. Lilienfeld", "R. Ammirati", "K. Landfield"], "title": "Giving debiasing away: Can psychological research on correcting cognitive errors promote human welfare", "venue": "Perspectives on Psychological Science", "year": 2009}, {"authors": ["B. Liu", "W. Hsu", "Y. Ma"], "title": "Integrating classification and association rule mining", "venue": "in: Proceedings of the Fourth International Conference on Knowledge Discovery and Data Mining,", "year": 1998}, {"authors": ["D. Martens", "J. Vanthienen", "W. Verbeke", "B. Baesens"], "title": "Performance of classification models from a user perspective", "venue": "Decision Support Systems", "year": 2011}, {"authors": ["K. Martire", "R. Kemp", "M. Sayle", "B. Newell"], "title": "On the interpretation of likelihood ratios in forensic science evidence: Presentation formats and the weak evidence effect", "venue": "Forensic Science International 240,", "year": 2014}, {"authors": ["K.A. Martire", "R.I. Kemp", "I. Watkins", "M.A. Sayle", "B.R. Newell"], "title": "The expression and interpretation of uncertain forensic science evidence: verbal equivalence, evidence strength, and the weak evidence effect", "venue": "Law and Human Behavior", "year": 2013}, {"authors": ["B. Mellers", "R. Hertwig", "D. Kahneman"], "title": "Do frequency representations eliminate conjunction effects? an exercise in adversarial collaboration", "venue": "Psychological Science", "year": 2001}, {"authors": ["R. Meo", "D. Ienco"], "title": "Replacing support in association rule mining, in: Rare Association Rule Mining and Knowledge Discovery: Technologies for Infrequent and Critical Event Detection", "venue": "IGI Global,", "year": 2010}, {"authors": ["M. Michalkiewicz", "K. Arden", "E. Erdfelder"], "title": "Do smarter people employ better decision strategies? the influence of intelligence on adaptive use of the recognition heuristic", "venue": "Journal of Behavioral Decision Making", "year": 2018}, {"authors": ["R.S. Michalski"], "title": "On the quasi-minimal solution of the general covering problem", "venue": "in: Proceedings of the V International Symposium on Information Processing (FCIP 69)(Switching Circuits),", "year": 1969}, {"authors": ["R.S. Michalski"], "title": "A theory and methodology of inductive learning", "venue": "Artificial Intelligence", "year": 1983}, {"authors": ["T. Miller"], "title": "Explanation in artificial intelligence: Insights from the social sciences", "venue": "Artificial Intelligence 267,", "year": 2019}, {"authors": ["T.M. Mitchell"], "title": "The Need for Biases in Learning Generalizations", "year": 1980}, {"authors": ["J.L. Monahan", "S.T. Murphy", "R.B. Zajonc"], "title": "Subliminal mere exposure: Specific, general, and diffuse effects", "venue": "Psychological Science", "year": 2000}, {"authors": ["R.M. Montoya", "R.S. Horton", "J.L. Vevea", "M. Citkowicz", "E.A. Lauber"], "title": "A re-examination of the mere exposure effect: The influence of repeated exposure on recognition, familiarity, and liking", "venue": "Psychological Bulletin", "year": 2017}, {"authors": ["S.H. Muggleton", "U. Schmid", "C. Zeller", "A. Tamaddoni-Nezhad", "T. Besold"], "title": "Ultra-strong machine learning: Comprehensibility of programs learned with ILP", "venue": "Machine Learning", "year": 2018}, {"authors": ["G.H. Mumma", "S.B. Wilson"], "title": "Procedural debiasing of primacy/anchoring effects in clinical-like judgments", "venue": "Journal of Clinical Psychology 51,", "year": 1995}, {"authors": ["M. Narayanan", "E. Chen", "J. He", "B. Kim", "S. Gershman", "F. Doshi-Velez"], "title": "How do humans understand explanations from machine learning systems? an evaluation of the human-interpretability of explanation", "year": 2018}, {"authors": ["J.D. Nelson"], "title": "Finding useful questions: On Bayesian diagnosticity, probability, impact, and information gain", "venue": "Psychological Review", "year": 2005}, {"authors": ["J.D. Nelson"], "title": "Towards a rational theory of human information acquisition, in: The Probabilistic Mind: Prospects for Bayesian Cognitive Science", "year": 2008}, {"authors": ["J.D. Nelson", "C.R. McKenzie", "G.W. Cottrell", "T.J. Sejnowski"], "title": "Experience matters: Information acquisition optimizes probability gain", "venue": "Psychological Science", "year": 2010}, {"authors": ["R.S. Nickerson"], "title": "Confirmation bias: A ubiquitous phenomenon in many guises", "venue": "Review of General Psychology", "year": 1998}, {"authors": ["H. Nilsson", "A. Winman", "P. Juslin", "G. Hansson"], "title": "Linda is not a bearded lady: Configural weighting and adding as the cause of extension errors", "venue": "Journal of Experimental Psychology: General", "year": 2009}, {"authors": ["R.E. Nisbett"], "title": "Rules for Reasoning", "year": 1993}, {"authors": ["R.E. Nisbett", "D.H. Krantz", "C. Jepson", "Z. Kunda"], "title": "The use of statistical heuristics in everyday inductive reasoning", "venue": "Psychological Review", "year": 1983}, {"authors": ["H. Ohira", "W.M. Winton", "M. Oyama"], "title": "Effects of stimulus valence on recognition memory and endogenous eyeblinks: Further evidence for positive-negative asymmetry", "venue": "Personality and Social Psychology Bulletin", "year": 1998}, {"authors": ["E.R. Omiecinski"], "title": "Alternative interest measures for mining associations in databases", "venue": "IEEE Transactions on Knowledge and Data Engineering", "year": 2003}, {"authors": ["C. Ordonez", "N. Ezquerra", "C.A. Santana"], "title": "Constraining and summarizing association rules in medical data", "venue": "Knowledge and Information Systems", "year": 2006}, {"authors": ["M.E. Oswald", "S. Grosjean"], "title": "Confirmation bias. Cognitive illusions: A handbook on fallacies and biases in thinking, judgement and memory", "year": 2004}, {"authors": ["T. Pachur", "R. Hertwig"], "title": "On the psychology of the recognition heuristic: Retrieval primacy as a key determinant of its use", "venue": "Journal of Experimental Psychology: Learning, Memory, and Cognition", "year": 2006}, {"authors": ["T. Pachur", "P.M. Todd", "G. Gigerenzer", "L. Schooler", "D.G. Goldstein"], "title": "The recognition heuristic: A review of theory and tests", "venue": "Frontiers in Psychology", "year": 2011}, {"authors": ["A. P\u00e1ez"], "title": "The pragmatic turn in explainable artificial intelligence (XAI). Minds and Machines", "year": 2019}, {"authors": ["S. Pagliaro"], "title": "Cognitive Biases and the Interpretability of Inductively Learnt Rules", "year": 2020}, {"authors": ["M.C. Parmley"], "title": "The Effects of the Confirmation Bias on Diagnostic Decision Making", "venue": "Ph.D. thesis. Drexel University", "year": 2006}, {"authors": ["R. Piltaver", "M. Lustrek", "M. Gams", "S. Martincic-Ipsic"], "title": "What makes classification trees comprehensible", "venue": "Expert Systems with Applications", "year": 2016}, {"authors": ["S. Pinker"], "title": "Words and Rules: The Ingredients of Language", "venue": "Basic Books", "year": 2015}, {"authors": ["R. Pohl"], "title": "Cognitive Illusions: A Handbook on Fallacies and Biases in Thinking, Judgement and Memory", "year": 2017}, {"authors": ["R.F. Pohl", "M. Michalkiewicz", "E. Erdfelder", "B.E. Hilbig"], "title": "Use of the recognition heuristic depends on the domains recognition validity, not on the recognition validity of selected sets of objects", "venue": "Memory & Cognition", "year": 2017}, {"authors": ["G. Politzer", "I.A. Noveck"], "title": "Are conjunction rule violations the result of conversational rule violations", "venue": "Journal of Psycholinguistic Research", "year": 1991}, {"authors": ["F. Poursabzi-Sangdeh", "D.G. Goldstein", "J.M. Hofman", "J.W. Vaughan", "H. Wallach"], "title": "Manipulating and measuring model interpretability", "year": 2018}, {"authors": ["F. Pratto", "O.P. John"], "title": "Automatic vigilance: The attention-grabbing power of negative social information", "venue": "Journal of Personality and Social Psychology", "year": 1991}, {"authors": ["J. Rauch"], "title": "Expert deduction rules in data mining with association rules: a case study", "venue": "Knowledge and Information Systems", "year": 2019}, {"authors": ["R.T. Reagan"], "title": "Variations on a seminal demonstration of people\u2019s insensitivity to sample size", "venue": "Organizational Behavior and Human Decision Processes", "year": 1989}, {"authors": ["P. Ristoski", "G.K.D. de Vries", "H. Paulheim"], "title": "A collection of benchmark datasets for systematic evaluations of machine learning on the semantic web", "venue": "in: Proceedings of the 15th International Semantic Web Conference (ISWC-16),", "year": 2016}, {"authors": ["G.L. Robinson-Riegler", "W.M. Winton"], "title": "The role of conscious recollection in recognition of affective material: Evidence for positive-negative asymmetry", "venue": "The Journal of General Psychology", "year": 1996}, {"authors": ["P. Rozin", "E.B. Royzman"], "title": "Negativity bias, negativity dominance, and contagion", "venue": "Personality and Social Psychology Review", "year": 2001}, {"authors": ["C. Rudin"], "title": "Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead", "venue": "Nature Machine Intelligence", "year": 2019}, {"authors": ["S. Schulte", "S. Chen", "K. Nahrstedt"], "title": "Stevens\u2019 power law in 3d teleimmersion: towards subjective modeling of multimodal cyber interaction", "venue": "in: Proceedings of the 22nd ACM International Conference on Multimedia,", "year": 2014}, {"authors": ["N. Schwarz"], "title": "Metacognitive experiences in consumer judgment and decision making", "venue": "Journal of Consumer Psychology", "year": 2004}, {"authors": ["N. Schwarz", "H. Bless", "F. Strack", "G. Klumpp", "H. Rittenauer-Schatka", "A. Simons"], "title": "Ease of retrieval as information: Another look at the availability heuristic", "venue": "Journal of Personality and Social Psychology", "year": 1991}, {"authors": ["N. Schwarz", "L.J. Sanna", "I. Skurnik", "C. Yoon"], "title": "Metacognitive experiences and the intricacies of setting people straight: Implications for debiasing and public information campaigns", "venue": "Advances in Experimental Social Psychology", "year": 2007}, {"authors": ["S. Serfas"], "title": "Cognitive Biases in the Capital Investment Context \u2013 Theoretical Considerations and Empirical Experiments on Violations of Normative Rationality", "year": 2011}, {"authors": ["E. Shafir"], "title": "The Behavioral Foundations of Public Policy", "year": 2013}, {"authors": ["A. Sides", "D. Osherson", "N. Bonini", "R. Viale"], "title": "On the reality of the conjunction fallacy", "venue": "Memory & Cognition", "year": 2002}, {"authors": ["I. Simonson", "A. Tversky"], "title": "Choice in context: Tradeoff contrast and extremeness aversion", "venue": "Journal of Marketing Research", "year": 1992}, {"authors": ["J.J. Skowronski", "D.E. Carlston"], "title": "Negativity and extremity biases in impression formation: A review of explanations", "venue": "Psychological Bulletin", "year": 1989}, {"authors": ["R. Slowinski", "I. Brzezinska", "S. Greco"], "title": "Application of Bayesian confirmation measures for mining rules from support-confidence Paretooptimal set", "venue": "Proceedings of the 7th International Conference on Artificial Intelligence and Soft Computing (ICAISC", "year": 2006}, {"authors": ["E.E. Smith", "C. Langston", "R.E. Nisbett"], "title": "The case for rules in reasoning", "venue": "Cognitive Science", "year": 1992}, {"authors": ["P.M. Spengler", "D.C. Strohmer", "D.N. Dixon", "V.A. Shivy"], "title": "A scientist-practitioner model of psychological assessment: Implications for training, practice and research", "venue": "The Counseling Psychologist", "year": 1995}, {"authors": ["K.E. Stanovich", "R.F. West", "M.E. Toplak"], "title": "Myside bias, rational thinking, and intelligence", "venue": "Current Directions in Psychological Science", "year": 2013}, {"authors": ["J. Stecher", "F. Janssen", "J. F\u00fcrnkranz"], "title": "Shorter rules are better, aren\u2019t they", "venue": "in: Proceedings of the 19th International Conference on Discovery Science (DS-16),", "year": 2016}, {"authors": ["S. Stolarz-Fantino", "E. Fantino", "J. Kulik"], "title": "The conjunction fallacy: Differential incidence as a function of descriptive frames and educational context", "venue": "Contemporary Educational Psychology", "year": 1996}, {"authors": ["P. Strossa", "Z. \u010cern\u1ef3", "J. Rauch"], "title": "Reporting data mining results in a natural language, in: Foundations of Data Mining and Knowledge Discovery", "year": 2005}, {"authors": ["H. Taniguchi", "H. Sato", "T. Shirakawa"], "title": "A machine learning model with human cognitive biases capable of learning from small and biased datasets", "venue": "Scientific reports", "year": 2018}, {"authors": ["K. Tentori", "V. Crupi"], "title": "On the conjunction fallacy and the meaning of and, yet again: A reply", "year": 2012}, {"authors": ["K. Tentori", "V. Crupi", "S. Russo"], "title": "On the determinants of the conjunction fallacy: Probability versus inductive confirmation", "venue": "Journal of Experimental Psychology: General", "year": 2013}, {"authors": ["J. Thomas"], "title": "Conversational maxims. Concise Encyclopedia of Philosophy of Language", "venue": "Peter V. Lamarque,", "year": 1992}, {"authors": ["Y. Trope", "B. Gervey", "N. Liberman"], "title": "Wishful thinking from a pragmatic hypothesis-testing perspective, in: The Mythomanias: The Nature of Deception and Self-Deception", "venue": "Lawrence Erlbaum Mahway,", "year": 1997}, {"authors": ["A. Tversky", "D. Kahneman"], "title": "Belief in the law of small numbers", "venue": "Psychological Bulletin", "year": 1971}, {"authors": ["A. Tversky", "D. Kahneman"], "title": "Availability: A heuristic for judging frequency and probability", "venue": "Cognitive Psychology", "year": 1973}, {"authors": ["A. Tversky", "D. Kahneman"], "title": "Judgment under uncertainty: Heuristics and biases", "venue": "Science", "year": 1974}, {"authors": ["A. Tversky", "D. Kahneman"], "title": "Extensional versus intuitive reasoning: the conjunction fallacy in probability judgment", "venue": "Psychological Review", "year": 1983}, {"authors": ["A. Tversky", "I. Simonson"], "title": "Context-dependent preference", "venue": "Management Science", "year": 1993}, {"authors": ["C. Unkelbach", "S.C. Rom"], "title": "A referential theory of the repetitioninduced truth effect", "venue": "Cognition", "year": 2017}, {"authors": ["F.M. Vieider"], "title": "The effect of accountability on loss aversion", "venue": "Acta Psychologica", "year": 2009}, {"authors": ["G. Villejoubert", "D.R. Mandel"], "title": "The inverse fallacy: An account of deviations from Bayess theorem and the additivity principle", "venue": "Memory & Cognition", "year": 2002}, {"authors": ["S. Vojir", "T. Kliegr"], "title": "Editable machine learning models? a rulebased framework for user studies of explainability. URL: https://nb. vse.cz/~klit01/papers/RuleEditor_preprint.pdf. under Minor revision in Advances in Data Analysis and Classification", "year": 2020}, {"authors": ["S. Voj\u0301\u0131\u030cr", "V. Zeman", "J. Kucha\u0159", "T. Kliegr"], "title": "Easyminer.eu: Web framework for interpretable machine learning based on rules and frequent itemsets", "venue": "Knowledge-Based Systems", "year": 2018}, {"authors": ["R. \u0160krabal", "M. \u0160im\u016fnek", "S. Voj\u0301\u0131\u030cr", "A. Hazucha", "T. Marek", "D. Chud\u00e1n", "T. Kliegr"], "title": "Association rule mining following the web search paradigm", "venue": "Proceedings of the European Conference on Machine Learning and Knowledge Discovery in Databases (ECML-PKDD", "year": 2012}, {"authors": ["D. Wang", "Q. Yang", "A. Abdul", "B.Y. Lim"], "title": "Designing theory-driven user-centric explainable AI", "venue": "in: Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems,", "year": 2019}, {"authors": ["T. Wang", "C. Rudin", "F. Doshi-Velez", "Y. Liu", "E. Klampfl", "P. MacNeille"], "title": "A Bayesian framework for learning rule sets for interpretable classification", "venue": "Journal of Machine Learning Research", "year": 2017}, {"authors": ["P.C. Wason"], "title": "On the failure to eliminate hypotheses in a conceptual task", "venue": "Quarterly Journal of Experimental Psychology", "year": 1960}, {"authors": ["G.I. Webb"], "title": "Recent progress in learning decision lists by prepending inferred rules", "venue": "in: Proceedings of the 2nd Singapore International Conference on Intelligent Systems,", "year": 1994}, {"authors": ["C.H. Wei\u00df"], "title": "Statistical mining of interesting association", "venue": "rules. Statistics and Computing", "year": 2008}, {"authors": ["C. Werner", "A.M. Hanea", "O. Morales-N\u00e1poles"], "title": "Eliciting multivariate uncertainty from experts: Considerations and approaches along the expert judgement process, in: Elicitation", "year": 2018}, {"authors": ["A. Wilke", "R. Mata"], "title": "Cognitive bias, in: Ramachandran, V. (Ed.), Encyclopedia of Human Behavior. 2nd ed", "year": 2012}, {"authors": ["R. Wille"], "title": "Restructuring lattice theory: An approach based on hierarchies of concepts, in: Rival, I", "venue": "(Ed.), Ordered Sets. Reidel,", "year": 1982}, {"authors": ["S. Willis"], "title": "Standards for the formulation of evaluative forensic science expert opinion association of forensic science providers", "venue": "Science & Justice", "year": 2010}, {"authors": ["P. Winkielman", "N. Schwarz", "T.A. Fazendeiro", "R. Reber"], "title": "The hedonic marking of processing fluency: Implications for evaluative judgment", "venue": "The Psychology of Evaluation: Affective Processes in Cognition and Emotion,", "year": 2003}, {"authors": ["A. Winman", "P. Juslin", "M. Lindskog", "H. Nilsson", "N. Kerimi"], "title": "The role of ANS acuity and numeracy for the calibration and the coherence of subjective probability judgments", "venue": "Frontiers in Psychology", "year": 2014}, {"authors": ["C.R. Wolfe", "M.A. Britt"], "title": "The locus of the myside bias in written argumentation", "venue": "Thinking & Reasoning", "year": 2008}, {"authors": ["R.B. Zajonc"], "title": "Attitudinal effects of mere exposure", "venue": "Journal of Personality and Social Psychology", "year": 1968}, {"authors": ["C. Zhang", "S. Zhang"], "title": "Association Rule Mining: Models and Algorithms", "year": 2002}, {"authors": ["D.J. Zizzo", "S. Stolarz-Fantino", "J. Wen", "E. Fantino"], "title": "A violation of the monotonicity axiom: Experimental evidence on the conjunction fallacy", "venue": "Journal of Economic Behavior & Organization", "year": 2000}], "sections": [{"text": "While the interpretability of machine learning models is often equated with their mere syntactic comprehensibility, we think that interpretability goes beyond that, and that human interpretability should also be investigated from the point of view of cognitive science. In particular, the goal of this paper is to discuss to what extent cognitive biases may affect human understanding of interpretable machine learning models, in particular of logical rules discovered from data. Twenty cognitive biases are covered, as are possible debiasing techniques that can be adopted by designers of machine learning algorithms and software. Our review transfers results obtained in cognitive psychology to the domain of machine learning, aiming to bridge the current gap between these two areas. It needs to be followed by empirical studies specifically focused on the machine learning domain.\nKeywords: cognitive bias, cognitive illusion, machine learning, interpretability, rule induction"}, {"heading": "1. Introduction", "text": "This paper aims to investigate the possible effects of cognitive biases on human understanding of machine learning models, in particular inductively learned rules. We use the term \u201ccognitive bias\u201d as a representative for various cognitive phenomena that materialize themselves in the form of occasionally irrational reasoning patterns, which are thought to allow humans to make fast judgments and decisions. Their cumulative effect on human reasoning should not be underestimated as \u201ccognitive biases seem reliable, systematic, and difficult to eliminate\u201d [82]. The effect of some cognitive biases is more pronounced when people do\n\u2217Corresponding author\nPreprint submitted to Elsevier June 26, 2020\nar X\niv :1\n80 4.\n02 96\n9v 5\n[ st\nat .M\nL ]\nnot have well-articulated preferences [168], which is often the case in explorative data analysis.\nPrevious works have analysed the impact of cognitive biases on multiple types of human behaviour and decision making. A specific example is the seminal book \u201cSocial cognition\u201d by Kunda [89], which is concerned with the impact of cognitive biases on social interaction. Another, more recent work by Serfas [146] focused on the context of capital investment. Closer to the domain of machine learning, in their article \u201cPsychology of Prediction\u201d, Kahneman and Tversky [83] warned that cognitive biases can lead to violations of the Bayes theorem when people make fact-based predictions under uncertainty. These results directly relate to inductively learned rules, since these are associated with measures such as confidence and support expressing the (un)certainty of the prediction they make. Despite some early works [104, 105] showing the importance of study of cognitive phenomena for rule induction and machine learning in general, there has been a paucity of follow-up research. In previous work [53], we have evaluated a selection of cognitive biases in the very specific context of whether minimizing the complexity or length of a rule will also lead to increased interpretability, which is often taken for granted in machine learning research.\nIn this paper, we attempt to systematically relate cognitive biases to the interpretation of machine learning results. We anchor our discussion on inductively learned rules, but note in passing that a deeper understanding of human cognitive biases is important for all areas of combined human-machine decision making. We focus primarily on symbolic rules because they are generally considered to belong to the class of interpretable models, so that there is little general awareness that different ways of presenting or formulating them may have an important impact on the perceived trustworthiness, safety, or fairness of an AI system. In principle, our discussion also applies to rules that have been inferred by deduction, where, however, such concerns are maybe somewhat alleviated by the proved correctness of the resulting rules. To further our goal, we review twenty cognitive biases and judgmental heuristics whose misapplication can lead to biases that can distort the interpretation of inductively learned rules. The review is intended to help to answer questions such as: How do cognitive biases affect the human understanding of symbolic machine learning models? What could help as a \u201cdebiasing antidote\u201d?\nThis paper is organized as follows. Section 2 provides a brief review of related work published at the intersection of rule learning and psychology. Section 3 motivates our study on the example of the insensitivity to sample size effect. Section 4 describes the criteria that we applied to select a subset of cognitive biases into our review, which eventually resulted in twenty biases. These biases and their respective effects and causes are covered in detail in Section 5. Section 6 provides a concise set of recommendations aimed at developers of rule learning algorithms and user interfaces. In Section 7 we state the limitations of our review and outline directions for future work. The conclusions summarize the contributions of the paper."}, {"heading": "2. Background and Related Work", "text": "We selected individual rules as learnt by many machine learning algorithms as the object of our study. Focusing on simple artefacts\u2014individual rules\u2014 as opposed to entire models such as rule sets or rule lists allows a deeper, more focused analysis since a rule is a small self-contained item of knowledge. Making a small change in one rule, such as adding a new condition, allows to test the effect of an individual factor. In this section, we first motivate our work by putting it into the context of prior research on related topics. Then, we proceed by a brief introduction to inductive rule learning (Section 2.2) and a brief recapitulation of previous work in cognitive science on the subject of decision rules (Section 2.3). Finally, we introduce cognitive biases (Section 2.4) and rule plausibility (Section 2.5), which is a measure of rule comprehension."}, {"heading": "2.1. Motivation", "text": "In the following three paragraphs, we discuss our motivation for this review, and summarize why we think this work is relevant to the larger artificial intelligence community.\nRules as Interpretable Models. Given that neural networks and ensembles of decision trees are increasingly becoming the prevalent type of representation used in machine learning, it might be at first surprising that our review focuses almost exclusively on decision rules. The reason is that rules are widely used as a means for communicating explanations of a variety of machine learning approaches. In fact, quite some work has been devoted to explaining black-box models, such as neural networks, support vector machines and tree ensembles with interpretable surrogate models, such as rules and decision trees (for a survey on this line of work we refer, e.g., to [68]). As such a conversion typically also goes hand-in-hand with a corresponding reduction in the accuracy of the model, this approach has also been criticized [141], and the interest in directly learning rule-based models has recently renewed (see, e.g., [52, 176, 110, 173]).\nEmbedding cognitive biases to learning algorithms. The applications of cognitive biases go beyond explaining existing machine learning models. For example, Taniguchi et al. [158] demonstrate how a cognitive bias can be embedded in a machine learning algorithm, achieving superior performance on small datasets compared to commonly used machine learning algorithms with \u201cgeneric\u201d inductive bias.\nPaucity of research on cognitive biases in artificial intelligence. Several recent position and review papers on explainability in Artificial Intelligence (xAI) recognize that cognitive biases play an important role in explainability research [106, 126]. To our knowledge, the only systematic treatment of psychological phenomena applicable to machine learning is provided by the review of Miller [106], which focuses on reasons and thought processes that people apply during explanation selection, such as causality, abnormality and the use of counterfactuals. This authoritative review observes that there are currently no studies that\nlook at cognitive biases in the context of selecting explanations. Because of the paucity of applicable research focusing on machine learning, the review of Miller [106] \u2014 same as the present paper \u2014 takes the first step of applying influential psychological studies to explanation in the xAI context without accompanying experimental validation specific to machine learning. While Miller [106] summarizes main reasoning processes that drive generation and understanding of explanations, our review focuses specifically on cognitive biases as psychological phenomena that can distort the interpretation of machine learning models, if not properly accounted for. The role of bias mitigation in machine learning has been recently recognized in [175], who describe four biases applicable to machine learning and for each propose a specific debiasing strategy. Our review is more comprehensive as we include twenty biases and we also provide a more detailed analysis of each of the biases included."}, {"heading": "2.2. Decision Rules in Machine Learning", "text": "An example of an inductively learned decision rule, which is a subject of the presented review, is shown in Figure 1.\nFollowing the terminology of Fu\u0308rnkranz et al. [52], A,B,C represent literals, i.e., Boolean expressions which are composed of attribute name (e.g., veil) and its value (e.g., white). The conjunction of literals on the left side of the rule is called antecedent or rule body, the single literal predicted by the rule is called consequent or rule head. Literals in the body are sometimes referred to as conditions throughout the text, and the consequent as the target. While this rule definition is restricted to conjunctive rules, other definitions, e.g., the formal definition given by Slowinski et al. [151], also allow for negation and disjunction as connectives.\nRules in the output of rule learning algorithms are most commonly characterized by two parameters, confidence and support. The confidence of a rule\u2014 sometimes also referred to as precision\u2014is defined as a/(a + b), where a is the number objects that match both the conditions of the rule as well as the consequent, and b is the number of objects that match the antecedent but not the consequent. The support of a rule is either defined as a/N , where N is the number of all objects (relative support), or simply as a (absolute support). A related measure is coverage, which is the total number of objects that satisfy the body of the rule (a + b).\nThe values of support and confidence are often used as indications of how subjectively interesting the given rules is. Research has shown the utility of involving thresholds on a range of additional measures of significance [121]. Out of the dozens of proposed formulas, the one most frequently adopted seems to be the lift measure, which is a ratio of the confidence of the rule and the probability of occurrence of the head of the rule (not considering the body). If lift is greater than 1, this indicates that the rule body and the rule head appear more often together than would correspond to chance.\nIn the special case of learning rules for the purpose of building a classifier, the consequent of a rule consists only of a single literal, the so-called class. In this case, a is also known as the number of true positives, and b as the number of false positives.\nSome rule learning frameworks, in particular association rule learning [1, 188], require the user to set thresholds for minimum confidence and support. Only rules with confidence and support values meeting or exceeding these thresholds are included on the output of rule learning and presented to the user.\nEven though the terminology, \u201csupport\u201d and \u201cconfidence\u201d, is peculiar to symbolic rule learning (in particular to association rule mining), the underlying concepts are universally adopted. For example, they are essentially equivalent to the terms \u201crecall\u201d and \u201cprecision\u201d commonly used in information retrieval and correspond to the concepts of \u201caccuracy\u201d and \u201ccoverage\u201d of general machine learning models."}, {"heading": "2.3. Decision Rules in Cognitive Science", "text": "Rules are used in commonly embraced models of human reasoning in cognitive science [152, 118, 130]. They also closely relate to Bayesian inference, which also frequently occurs in models of human reasoning. Consider the first rule of Figure 1. This rule can be interpreted as a hypothesis corresponding to the logical implication A\u2227B \u2192 C. We can express the plausibility of such a hypothesis in terms of Bayesian inference as the conditional probability Pr(C | A,B). This corresponds to the confidence of the rule, as used in machine learning and as defined above, and to the strength of evidence, a term used by cognitive scientists [165].\nGiven that Pr(C | A,B) is a probability estimate computed on a sample, another relevant piece of information for determining the plausibility of the hypothesis is the robustness of this estimate. This corresponds to the number of instances for which the rule has been observed to be true. The size of the sample (typically expressed as a ratio) is known as rule support in machine learning and as the weight of the evidence in cognitive science [165].1\nPsychological research on hypothesis testing in rule discovery tasks has been performed in cognitive science at least since the 1960s. The seminal article\n1Interestingly, balancing the likelihood of the judgment and the weight of the evidence in the assessed likelihood was already studied by Keynes [85] (according to Camerer and Weber [22]).\nby Wason [177] introduced what is widely referred to as Wason\u2019s 2-4-6 task. Participants are given the sequence of numbers 2, 4 and 6 and asked to find out the rule that generated this sequence. In the search for the hypothesized rule, they provide the experimenter other sequences of numbers and the experimenter answers whether the provided sequence conforms to the rule, or not. While the target rule is simple \u201cascending sequence\u201d, people find it difficult to discover this specific rule, presumably because they use the positive test strategy, a strategy of testing a hypothesis by examining evidence confirming the hypothesis at hand rather than searching for disconfirming evidence [86]. For example, if they have the hypothesis that the rule is a sequence of numbers increasing by two, they can provide a sequence 3-5-7, trying to confirm the hypothesis, rather than a sequence, such as 1-2-3, looking for an alternative hypothesis."}, {"heading": "2.4. Cognitive Bias", "text": "According to the Encyclopedia of Human Behavior [181], the term cognitive bias was introduced in the 1970s by Amos Tversky and Daniel Kahneman [165], and is defined as a\n\u201csystematic error in judgment and decision-making common to all human beings which can be due to cognitive limitations, motivational factors, and/or adaptations to natural environments.\u201d\nThe narrow initial definition of cognitive bias as a shortcoming of human judgment was criticized by German psychologist Gerd Gigerenzer, who started in the late 1990s the \u201cFast and frugal heuristic\u201d program to emphasize ecological rationality (validity) of judgmental heuristics [62]. According to this research program, cognitive biases often result from an application of a heuristic in an environment for which it is not suited rather than from problems with heuristics themselves, which work well in usual contexts.\nIn the present view, we define cognitive biases and associated phenomena broadly. We include cognitive biases related to thinking, judgment, and memory. We also include descriptions of thinking strategies and judgmental heuristics that may result in cognitive biases, even if they are not necessarily biases themselves.\nDebiasing. An important aspect related to the study of cognitive biases is the validation of strategies for mitigating their effects in cases when they lead to incorrect judgment. A number of such debiasing techniques have been developed, with researchers focusing intensely on the clinical and judicial domains (cf. e.g. [93, 27, 99]), apparently due to costs associated with erroneous judgment in these domains. Nevertheless, general debiasing techniques can often be derived from such studies.\nThe choice of an appropriate debiasing technique typically depends on the type of error induced by the bias, since this implies an appropriate debiasing strategy [5]. Larrick [92] recognizes the following three categories: psychophysicallybased error, association-based error, and strategy-based error. The first two are\nattributable to the unconscious, automatic processes, sometimes referred to as \u201cSystem 1\u201d. The last one is attributed to reasoning processes (System 2) [38]. For biases attributable to System 1, the most generic debiasing strategy is to shift processing to the conscious System 2 [96], [147, p. 491].\nAnother perspective on debiasing is provided by Croskerry et al. [27], who organize debiasing techniques by their way of functioning, rather than the bias they address, into the following three categories: educational strategies, workplace strategies and forcing functions. While Croskerry et al. [27] focused on clinicians, our review of debiasing aims to be used as a starting point for analogous guidelines for an audience of machine learning practitioners. For example, the general workplace strategies applicable in the machine learning context include group decision making, personal accountability, and planning time-out sessions to help slowing down. All of these strategies could lead to a higher probability of activating System 2 and thus reducing the biases which originate in the failure of System 1.\nFunction and validity of cognitive biases. The function of cognitive biases is a subject of scientific debate. According to the review of functional views by Pohl [131], there are three fundamental positions among researchers. The first group considers them as dysfunctional errors of the system, the second group as faulty by-products of otherwise functional processes, and the third group as adaptive and thus functional responses. According to Pohl [131], most researchers are in the second group, where cognitive biases are considered to be \u201cbuilt-in errors of the human information-processing systems\u201d.\nIn this work, we consider judgmental heuristics and cognitive biases as strategies that evolved to improve the fitness and chances of survival of the individual in particular situations or as consequences of such strategies. This defense of biases is succinctly expressed by Haselton and Nettle [70]: \u201cBoth the content and direction of biases can be predicted theoretically and explained by optimality when viewed through the long lens of evolutionary theory. Thus, the human mind shows good design, although it is designed for fitness maximization, not truth preservation.\u201d\nAccording to the same paper, empirical evidence shows that cognitive biases are triggered or strengthened by environmental cues and context [70]. Given that the interpretation of machine learning results is a task unlike the simple automatic cognitive processes to which a human mind is adapted, cognitive biases are likely to have an influence upon it."}, {"heading": "2.5. Measures of Interpretability, Perceived and Objective Plausibility", "text": "We claim that cognitive biases can affect the interpretation of rule-based models. However, how does one measure interpretability? According to our literature review, there is no generally accepted measure of interpretability of machine learning models. Model size, which was used in several studies, has recently been criticized [48, 155, 53] primarily on the grounds that the model\u2019s syntactic size does not capture any aspect of the model\u2019s semantics. A particular\nproblem related to semantics is the compliance to pre-existing expert knowledge, such as domain-specific monotonicity constraints.\nIn prior work [53], we embrace the concept of plausibility to measure interpretability. In the following, we will briefly introduce this concept because in the remainder of this article, we will use some material collected in user studies reported on in [53] to illustrate some of the discussed biases and cognitive phenomena. The word \u2019plausible\u2019 is defined according to the Oxford Dictionary of US English as \u201cseeming reasonable or probable\u201d and according to the Cambridge dictionary of UK English as \u201cseeming likely to be true, or able to be believed\u201d. We can link the inductively learned rule to the concept of \u201chypothesis\u201d used in cognitive science. There is a body of work in cognitive science on analyzing the perceived plausibility of hypotheses [58, 59, 4].\nIn a recent review of interpretability definitions by Bibal and Fre\u0301nay [17], the term plausibility is not explicitly covered, but a closely related concept of justifiability is stated to depend on interpretability. Martens et al. [98] define justifiability as \u201cintuitively correct and in accordance with domain knowledge\u201d. By adopting plausibility, we address the concern expressed in Freitas [48] regarding the need to reflect domain semantics when interpretability is measured."}, {"heading": "3. Motivational Example", "text": "It is well known in machine learning that chance rules with deceptively high confidence can appear in the output of rule learning algorithms [7]. For this reason, the rule learning process typically outputs both confidence and support for the analyst to make an informed choice about the merits of each rule. In the following, we illustrate this on two rules inspired by a real-world Movies recommendation dataset that we used for several user studies of plausibility of rules in [53].\nExample. \u2022 IF a film is released in 2006 AND the language of the film is English THEN Rating is good,\nconfidence = 80%, support = 10%.\n\u2022 IF a film is released in 2006 AND the director was John Smith THEN Rating is good,\nconfidence = 90%, support = 1%.\nIn the example above, both rules are associated with values of confidence and support to inform about the strength and weight of evidence for both rules. While the first rule is less strong (80% vs 90% correct), its weight of the evidence is ten times higher than of the second rule.\nAccording to the insensitivity to sample size effect [165] there is a systematic bias in human thinking that makes humans overweigh the strength of evidence (confidence) and underweight the weight of evidence (support). The presence of this bias has been also shown for psychologists knowledgable in statistics [163]\nand thus is likely to be applicable to the widening number of professions that use the rule learning to obtain insights from data.\nThe analysis of relevant literature from cognitive science not only reveals applicable biases but also sometimes provides methods for removing or limiting their effect (debiasing). In principle, we found three categories of debiasing techniques: 1. Training users, 2. Adapting learning algorithms, 3. Adapting the representation of the model and/or the user interface.\nTraining users. Research [47] (cf. also [119, 137]) shows that training can significantly improve statistical reasoning and help people better understand the importance of sample size (\u2019law of large numbers\u2019), which is instrumental for correctly interpreting rule support and rule confidence.\nAdapting learning algorithms. One possibility in terms of adaptation of learning algorithms is to compute confidence intervals for rule confidence (the conditional probability Pr(Consequent | Conditions)) as proposed, e.g., in [179]. As a result, the support of a rule can be\u2014in a way\u2014directly embedded into the presentation of rule confidence [102].\nAdapting the representation. The standard way used in rule learning software for displaying rule confidence and support metrics is to use percentages, as in our example. Extensive research in psychology has shown that if frequencies are used instead, then the number of errors in judgment drops [61, 63]. Reflecting these suggestions, the first rule in our example could be presented as follows:\nExample. \u2022 IF a film is released in 2006 AND the language of the film is English THEN Rating is good.\nIn our data, there are 100 movies which match the conditions of this rule. Out of these, 80 are predicted correctly as having good rating.\nRules can be presented in different ways (as shown), and depending on the way the information is presented, humans may perceive their plausibility differently. In this particular example, confidence is no longer conveyed as a percentage \u201c80%\u201d but using the expression \u201c80 out of 100\u201d. Support is presented as an absolute number (100) rather than as a percentage (10%).\nA correct understanding of machine learning models can be difficult even for experts. In this section, we tried to motivate why addressing cognitive biases can play an important role in making the results of inductive rule learning more understandable. In the remainder of this paper, the bias applied to our example will be revisited in greater depth, along with 19 other biases.\nNote. In this paper, as well as in the motivational example above, we show possible applications of research on cognitive biases and their debiasing as performed in the general psychological literature. However, focusing specifically on rule learning, only very few biases were empirically investigated, and there is virtually no research that would show that a specific debiasing strategy is effective. For this reason, the example above should be interpreted as a motivational illustration, rather than a guideline adapted without further validation (cf. also Section 7.1)."}, {"heading": "4. Selection Criteria", "text": "A number of cognitive biases have been discovered, experimentally studied, and extensively described in the literature. As Pohl [131] states in a recent authoritative book on cognitive illusions: \u201cThere is a plethora of phenomena showing that we deviate in our thinking, judgment and memory from some objective and arguably correct standard.\u201d This book covers 24 cognitive biases, and even 51 biases are covered by Evans [37].\nWe first selected a subset of biases which would be reviewed. To select applicable biases, we looked for those that can interact with the following properties of rules, and their activation could result in an impact on perceived plausibility of rules:\n1. rule length (the number of literals in an antecedent), 2. rule interest measures (especially support and confidence), 3. position (ordering) of conditions in a rule and order of rules in the rule list, 4. specificity and predictive power of conditions (correlation with a target variable), 5. use of additional logical connectives (conjunction, disjunction, negation), 6. treatment of missing information (inclusion of conditions referring to missing values), and 7. conflict between rules in the rule list.\nThrough a selection of appropriate learning heuristics, the rule learning algorithm can influence these properties. For example, most heuristics implement some form of a trade-off between the coverage or support of a rule, and its implication strength or confidence [51, 52].\nBased on these selection criteria, the first author pre-selected initial list of biases, and then this selection was approved by the other two authors.\nWhile doing the initial selection of cognitive biases to study, we tried to identify those most relevant for machine learning research matching our criteria. In the end, our review focused on a selection of 20 judgmental heuristics and cognitive biases. Future work might focus on expanding the review with additional relevant biases, such as labelling and overshadowing effects [131]."}, {"heading": "5. Review of Cognitive Biases", "text": "In this section, we cover a selection of twenty cognitive biases. For all of them, we include a short description including an example of a study demonstrating the bias and its proposed explanation. We pay particular attention to\ntheir potential effect on the interpretability of rule learning results, which has not been covered in previous works.\nIn a recent scientometric survey of research on cognitive biases in information systems [45], no articles are mentioned that aim at machine learning. For general information systems research, the authors claim that \u201cmost articles\u2019 research goal [is] to provide an explanation of the cognitive bias phenomenon rather than to develop ways and strategies for its avoidance or targeted use\u201d. In contrast, our review aims at the advancement of the field beyond the explanation of applicable phenomena, by discussing specific debiasing techniques.\nFor all cognitive biases, we thus suggest a debiasing technique that could be effective in aligning the perceived plausibility of the rule with its objective plausibility. While we include a description only of the most prominent debiasing strategies for each bias, it is possible that some of the debiasing strategies may be more general and could be effective for multiple biases.\nThe utility of this article for the reader would increase if concrete proposals for debiasing machine learning (rule learning) results were included. However, there is a paucity of applicable work on debiasing techniques applied specifically to machine learning (or directly on rule learning), and the invention of a rulelearning-specific debiasing technique for each of the twenty surveyed biases is out of the scope of this article. We, therefore, decided to introduce only one debiasing technique developed specifically for rule learning, choosing an approach that is to our knowledge the most well studied.\nWe chose the \u201cfrequency format\u201d debiasing method, which has been documented to reduce the number of bias-induced judgment errors across a variety of different tasks as supported by a body of psychological studies. We adapted this method for rule learning and used in a user study reported in [53]. Since then, it has been subject of at least one other user study focused specifically on debiasing methods for rule learning. This method is introduced already in Section 5.1 in the context of the motivating Linda problem and the representativeness heuristic. However, since it provides a way for expressing rule confidence and rule support, it is primarily intended as a debiasing method for the base rate neglect (Section 5.5) and the insensitivity to sample size (Section 5.6). The proposed generalized adaptation to rule learning also adopts recommendations from psychological research for addressing the misunderstanding of \u2019and\u2019 (Section 5.2).\nAn overview of the main features of the reviewed cognitive biases is presented in Table 1. Note that the debiasing techniques that we describe have only limited grounding in applied psychological research and require further validation, since as Lilienfeld et al. [96] observe, there is a general paucity of research on debiasing in psychological literature, and the existing techniques suffer from a lack of theoretical coherence and mixed research evidence concerning their efficacy."}, {"heading": "5.1. Conjunction Fallacy and Representativeness Heuristic", "text": "The conjunction fallacy refers to a judgment that is inconsistent with the conjunction rule \u2013 the probability of conjunction, Pr(A,B), cannot exceed the\np h\nen o m\nen o n\nim p\nli ca\nti o n\ns fo\nr ru\nle -l\nea rn\nin g\nd eb\nia si\nn g\nte ch\nn iq\nu e\nC o n\nju n\nct io\nn F\na ll a cy\na n\nd R\nep -\nre se\nn ta\nti v en\nes s\nH eu\nri st\nic O\nv er\nes ti\nm a te\nth e\np ro\nb a b\nil it\ny o f\nco n\nd it\nio n\nre p\nre se\nn ta\nti v e\no f\nru le\nco n\nse q u\nen t\nU se\nn a tu\nra l\nfr eq\nu en\nci es\nin st\nea d\no f\nra ti\no s\no r\np ro\nb a b\nil it\nie s,\nen -\ng a g e\nS y st\nem 2 ,\np ro\nv id\ne tr\na in\nin g\nin lo\ng ic\nM is\nu n\nd er\nst a n\nd in\ng o f\n\u2019a n\nd \u2019\np eo\np le\nin te\nrp re\nt \u2019a\nn d \u2019\nd iff\ner en\ntl y\nth a n\nlo g ic\na l\nco n\nju n\nct io\nn p\nre se\nn t\nco n\nd it\nio n\ns a s\np ro\np o si\nti o n\ns ra\nth er\nth a n\nca te\ng o ri es A v er a g in g H eu ri st ic P ro b a b il it y o f a n te ce d en t a s th e a v er a g e o f p ro b a b il it ie s o f co n d it io n s R em in d er o f p ro b a b il it y th eo ry D is ju n ct io n F a ll a cy P re fe r m o re sp ec ifi c co n d it io n s o v er le ss sp ec ifi c In fo rm o n ta x o n o m ic a l re la ti o n b et w ee n co n d it io n s; ex p\nla in\nb en\nefi ts\no f\nh ig\nh er\nsu p\np o rt\nB a se\n-r a te\nN eg\nle ct\nN eg\nle ct\np ri\no r\np ro\nb a b\nil it\ny o f\nth e\nh ea\nd o f\nth e\nru le\nS h\no w\na n\nd ex\np la\nin th\ne v a lu\ne o f\nli ft\nm ea\nsu re\nIn se\nn si\nti v it\ny to\nS a m\np le\nS iz\ne A\nn a ly\nst d\no es\nn o t\nre a li ze\nth e\nin cr\nea se\nd re\nli a b\nil it\ny o f\nco n\nfi d\nen ce\nes ti\nm a te\nw it\nh in\ncr ea\nsi n\ng v a lu\ne o f\nsu p\np o rt\nP re\nse n t\nsu p\np o rt\na s\na b\nso lu\nte n u\nm b\ner ra\nth er\nth a n\np er\nce n ta\ng e;\nu se\nsu p\np o rt\nto co\nm p\nu te\nco n\nfi d\nen ce\n(r el\nia b\nil it\ny )\nin te\nrv a ls\nfo r\nth e\nv a lu\ne o f\nco n\nfi d\nen ce\nC o n\nfi rm\na ti\no n\nB ia\ns R\nu le\ns co\nn fi\nrm in\ng a n\na ly\nst \u2019s\np ri\no r\nh y p\no th\nes is\na re\n\u201c ch\ner ry\np ic\nk ed\n\u201d E\nx p\nli ci\nt g u\nid a n\nce to\nco n\nsi d\ner ev\nid en\nce fo\nr a n\nd a g a in\nst h y p\no th - es is ; ed u ca ti o n a b o u t th e b ia s; in te rf a ce s m a k in g u se rs sl o w d o w\nn A v a il a b il it y H eu ri st ic E a se o f re co ll ec ti o n o f in st a n ce s m a tc h in g th e ru le E x p\nla in\nto a n\na ly\nst w\nh y\nin st\na n\nce s\nm a tc\nh in\ng th\ne p\na rt\nic u\nla r\nru le\na re\n(n o t)\nea si\nly re\nca ll ed R ei te ra ti o n E ff ec t P re se n ta ti o n o f re d u n d a n t ru le s o r co n d it io n s in cr ea se s p la u - si b il it y ru le p ru n in g ; cl u st er\nin g ;\nex p\nla in\nin g\no v er\nla p\nM er\ne E\nx p\no su\nre E\nff ec\nt R\nep ea\nte d\nex p\no su\nre (e\nv en\nsu b\nco n\nsc io\nu s)\nre su\nlt s\nin in\ncr ea\nse d\np re\nfe re\nn ce\nC h\na n\ng es\nto u\nse r\nin te\nrf a ce\ns th\na t\nli m\nit su\nb li m\nin a l\np re\nse n ta\nti o n\no f\nru le s O v er co n fi d en ce a n d u n d er co n - fi d en ce R u le s w it h sm a ll su p p o rt a n d h ig h co n fi d en ce a re \u201c o v er ra te d \u201d P re se n t\nle ss\nin fo\nrm a ti\no n\nw h\nen n\no t\nre le\nv a n t\nv ia\np ru\nn in\ng ,\nfe a -\ntu re\nse le\nct io\nn ,\nli m\nit in\ng ru\nle le\nn g th\n; a ct\niv el\ny p\nre se\nn t\nco n\nfl ic\nti n g ru le s/ k n o w le d g e.\nR ec\no g n\nit io\nn H\neu ri\nst ic\nR ec\no g n\nit io\nn o f\na tt\nri b\nu te\no r\nit s\nv a lu\ne in\ncr ea\nse s\np re\nfe re\nn ce\nM o re\nti m\ne; k n\no w\nle d\ng e\no f\na tt\nri b\nu te\n/ v a lu e In fo rm a ti o n B ia s b el ie f th a t m o re in fo rm a ti o n (r u le s, co n d it io n s) w il l im p ro v e d ec is io n m a k in g ev en if it is ir re le v a n t C o m m u n ic a te a tt ri b u te im p o rt a n ce A m b ig u it y A v er si o n P re fe r ru le s w it h o u t u n k n o w n co n d it io n s In cr ea se u se r m o ti v a ti o n ; in st ru ct u se rs\nto p\nro v id\ne te\nx tu\na l ju sti fi ca ti o n\ns C o n fu si o n o f th e In v er se C o n fu si n g th e d iff er en ce b et w ee n th e co n fi d en ce o f th e ru le P r( co n se q u en t |a n te ce d en t) w it h P r( a n te ce d en t |c o n se q u en t) T ra in in\ng in\np ro\nb a b\nil it\ny th\neo ry\n; u\nn a m\nb ig\nu o u\ns w\no rd\nin g\nC o n te\nx t\na n\nd T\nra d\neo ff\nC o n tr\na st\nP re\nfe re\nn ce\nfo r\na ru\nle is\nin fl\nu en\nce d\nb y\no th\ner ru\nle s\nR em\no v a l\no f\nru le\ns, es\np ec\nia ll y\no f\nth o se\nth a t\na re\nst ro\nn g ,\ny et\nir re lev a n t\nN eg\na ti\nv it\ny B\nia s\nW o rd\ns w\nit h\nn eg\na ti\nv e\nv a le\nn ce\nin th\ne ru\nle m\na k e\nit a p\np ea\nr m\no re\nim p\no rt\na n t\nR ev\nie w\nw o rd\ns w\nit h\nn eg\na ti\nv e\nv a le\nn ce\nin d\na ta\n, a n\nd p\no ss\nib ly\nre -\np la\nce w\nit h\nn eu\ntr a l\na lt\ner n\na ti\nv es\nP ri\nm a cy\nE ff\nec t\nIn fo\nrm a ti\no n\np re\nse n te\nd fi\nrs t\nh a s\nth e\nh ig\nh es\nt im\np a ct\nE d\nu ca\nti o n\no n\nth e\nb ia\ns; re\nso rt\nin g ;\nru le\na n\nn o ta\nti o n\nW ea\nk E\nv id\nen ce\nE ff\nec t\nC o n\nd it\nio n\no n\nly w\nea k ly\np er\nce iv\ned a s\np re\nd ic\nti v e\no f\nta rg\net d ecr ea se s p la u si b il it y N\nu m\ner ic\na l ex\np re\nss io\nn o f st\nre n\ng th\no f ev\nid en\nce ; o m\nis si\no n\no f w\nea k\np re\nd ic\nto rs\n(c o n\nd it\nio n s) U n it B ia s C o n d it io n s a re p er ce iv ed to h a v e sa m e im p o rt a n ce In fo rm o n d is cr im in a to\nry p\no w\ner o f\nco n\nd it\nio n\ns\nT a b\nle 1 :\nS u\nm m\na ry\no f\na n\na ly\nsi s\no f\nco g n\nit iv\ne b\nia se\ns.\nprobability of its constituents, Pr(A) and Pr(B). It is often illustrated with the \u201cLinda\u201d problem in the literature [167]. In the Linda problem, depicted in Figure 2, subjects are asked to compare conditional probabilities Pr(F,B | L) and Pr(B | L), where B refers to \u201cbank teller\u201d, F to \u201cactive in feminist movement\u201d and L to the description of Linda [9].\nMultiple studies have shown that people tend to consistently select the second hypothesis as more probable, which is in conflict with the conjunction rule. In other words, it always holds for the Linda problem that\nPr(F,B | L) \u2264 Pr(B | L).\nPreference for the alternative F \u2227 B (option (b) in Figure 2) is thus always a logical fallacy. For example, Tversky and Kahneman [167] report that 85% of their subjects indicated (b) as the more probable option for the Linda problem. The conjunction fallacy has been shown across multiple settings (hypothetical scenarios, real-life domains), as well as for various kinds of subjects (university students, children, experts, as well as statistically sophisticated individuals) [159].\nThe conjunction fallacy is often explained by the use of the representativeness heuristic [82]. The representativeness heuristic refers to the tendency to make judgments based on similarity, based on the rule \u201clike goes with like\u201d, which is typically used to determine whether an object belongs to a specific category. When people use the representativeness heuristic, \u201cprobabilities are evaluated by the degree to which A is representative of B, that is by the degree to which A resembles B\u201d [165]. This heuristic provides people with means for assessing the probability of an uncertain event. It is used to answer questions such as \u201cWhat is the probability that object A belongs to class B? What is the probability that event A originates from process B?\u201d [165].\nThe representativeness heuristic is not the only explanation for the results of the conjunction fallacy experiments. Hertwig et al. [73] hypothesized that the\nfallacy is caused by \u201ca misunderstanding about conjunction\u201d, in other words by a different interpretation of \u201cprobability\u201d and \u201cand\u201d by the subjects than assumed by the experimenters. The validity of this alternative hypothesis has been subject to criticism [159], nevertheless, some empirical evidence suggests that the problem of the correct understanding of \u201cand\u201d is of particular importance to rule learning [53].\nRecent research has provided several explanations for conjunctive and disjunctive (cf. Section 5.4) fallacies, such as configural weighting and adding theory [117], applying principles of quantum cognition [21] and inductive confirmation theory [160]. In the following, we will focus on the CWA theory. CWA essentially assumes that the causes of conjunctive and disjunctive fallacies relate to the fact that decision makers perform a weighted average instead of multiplication of the component probabilities. For conjunctions, weights are set so that more weight is assigned to the lower component probability. For disjunctive probabilities, more weight is assigned to the likely component. This assumption was verified in at least one study [42]. For more discussion of the related averaging heuristic, cf. Section 5.3.\nThere is also a body of research that accounts the results of the Linda experiment to the violation of conversational maxims. Politzer and Noveck [133] argue that since the two options given to the participants are nested, it compels the participants to interpret the first choice (A) as A and not-B. According to this interpretation, subjects\u2019 typical responses are in line with logical principles. Donovan and Epstein [32] have conducted follow-up experiments presenting completely disclosing information and accounting for whether a particular choice was made for a good (logical reasoning) or wrong reason (representativeness heuristic). The results have shown that \u201cconjunction errors\u201d cannot be to large extent attributed to the violation of the implicit conversational norms.\nImplications for rule learning. Rules are not composed only of conditions, but also of an outcome (the value of a target variable in the consequent). A higher number of conditions generally allows the rule to filter a purer set of objects with respect to the value of the target variable than a smaller number of conditions. Application of representativeness heuristic can affect the human perception of rule plausibility in that rules that are more \u201crepresentative\u201d of the user\u2019s mental image of the concept may be preferred even in cases when their objective discriminatory power may be lower.\nIn the example depicted in Figure 3, we show a hypothesized effect of the representativeness heuristic. The example shows several justifications of user choices collected in our prior experiments [53].2 As follows from the second response, for a rule to be found plausible, it needs not only to contain isolated representative literals but the collection of literals as a whole needs to provide a coherent impression of a representative description.\n2While as part of the experiments in [53], we collected user explanations for choices made, these qualitative data on user preference were not formally evaluated and are not included in\nDebiasing techniques. A number of factors that decrease the proportion of subjects exhibiting the conjunction fallacy have been identified: Charness et al. [24] found that the number of participants committing the fallacy is reduced under a monetary incentive. Such an addition was reported to drop the fallacy rate in their study from 58% to 33%. The observed rate under a monetary incentive suggests smaller importance of this problem for important real-life decisions. Zizzo et al. [189] found that unless the decision problem is simplified, neither monetary incentives nor feedback can ameliorate the fallacy rate. A reduced task complexity is a precondition for monetary incentives and feedback to be effective.\nStolarz-Fantino et al. [156] observed that the rate of fallacies is reduced but still strongly present when the subjects receive training in logic. Gigerenzer and Goldstein [61], as well as Gigerenzer and Hoffrage [63], showed that the rate of fallacies can be reduced or even eliminated by presenting the problems in terms of frequencies rather than probabilities. To illustrate this debiasing measure, Figure 4 presents a reformulation of the Linda problem in the frequency format, and Figure 5 presents a reformulation of a discovered rule in a frequency format. The wording is based on (but does not exactly correspond to) a formulation used in experiments reported in [53].\nNote that the original proposal of frequency formats [63] suggests replacing\n[53].\nratios with frequencies, however, in Figure 5, we provide frequencies alongside the precomputed ratios of confidence and support. The reason is that it has been empirically shown that using only the frequency format (based on actual values from the rule\u2019s confusion matrix as in [127]) is not helpful to users. In experiments on the UCI soybean dataset reported in [127, p. 51], the users were shown the following explanations for values of confidence and support: \u201cIn our data, which contains 631 observations, 71 of these match the conditions of this rule. Out of these, 55 are predicted as having the phytophthora root disease\u201d. The results of the user study showed that this led to a higher number of errors than if percentage values of confidence and support were shown, which is interpreted in [127] as a result of the mental computation of the ratios by the participants. It is also not an option to use the value 100 or 1000 as a reference value as recommended in [63] and demonstrated in Figure 4. The reason is that while this formulation is useful for expressing confidence, it distorts the user\u2019s understanding of support. As a result of this analysis, we arrived at the suggested wording of the debiasing presented in Figure 5. We emphasize that this is only a proposal, which needs further experimental validation with a between-subject methodology.\nNilsson et al. [117] present a computer simulation showing that when the component probabilities are not precisely known, averaging often provides an equally good alternative to the normative computation of probabilities (cf. also Juslin et al. [79]). The reason is that the normatively correct multiplicative integration can amplify the errors more than linear additive integration. This computational model could be possibly adapted to detect a high risk of fallacy, corresponding to the case when the deviation between the perceived probability and the normative probability is high.\nIn the context of explaining medical machine learning results, Wang et al. [175] propose to show prototypes of patient instances for each diagnosis. In rule learning, this would imply extending the user interface with a functionality that would show instances in data supporting the rule as well as the false positives \u2013 instances matching the antecedent, but not the rule consequent."}, {"heading": "5.2. Misunderstanding of \u201cand\u201d", "text": "The misunderstanding of \u201cand\u201d refers to a phenomenon affecting the syntactic comprehensibility of the logical connective \u201cand\u201d. As discussed by Hertwig et al. [73], \u201cand\u201d in natural language can express several relationships, including\ntemporal order, causal relationship, and most importantly, can also indicate a collection of sets3 as well as their intersection. People can therefore interpret \u201cand\u201d in a different meaning than intended.\nFor example, according to the two experiments reported by Hertwig et al. [73], the conjunction \u201cbank teller and active in the feminist movement\u201d used in the Linda problem (cf. Section 5.1) was found by about half of subjects as ambiguous\u2014they explicitly asked the experimenter how \u201cand\u201d was to be understood. Furthermore, when participants indicated how they understood \u201cand\u201d by shading Venn diagrams, it turned out that about a quarter of them interpreted \u201cand\u201d as union rather than an intersection, which is usually assumed by experimenters using the Linda problem.\nImplications for rule learning. The formation of conjunctions via \u201cand\u201d is a basic building block of rules. Its correct understanding is thus important for effective communication of results of rule learning. Existing studies suggest that the most common type of error is understanding \u201cand\u201d as a union rather than intersection. In such a case, a rule containing multiple \u201cands\u201d will be perceived as having higher support than it actually has. Each additional condition will be incorrectly perceived as increasing the coverage of the rule. This implies higher perceived plausibility of the rule.\nSimilarly as in the previous section (Figure 3), we can illustrate the occurrence of this bias on user feedback collected as part of experiments reported in\n3As in \u201cHe invited friends and colleagues to the party\u201d\n[53]. There, example responses that support misunderstanding of \u2019and\u2019 include: \u201cRule one contains twice as many properties as rule 2 does for determining the edibility of a mushroom so that makes it statistically twice as plausible, hence much higher probability of being believable\u201d, \u201cAn extra group increases the likelihood.\u201d Misunderstanding of \u201cand\u201d will thus generally increase the preference of rules with more conditions.\nDebiasing techniques. According to Sides et al. [148] \u201cand\u201d ceases to be ambiguous when it is used to connect propositions rather than categories. The authors give the following example of a sentence which is not prone to misunderstanding: \u201cIBM stock will rise tomorrow and Disney stock will fall tomorrow.\u201d A similar wording of rule learning results, as depicted in Figure 5, may be preferred, despite its verbosity. In the figure, conditions are represented as propositions (\u201cgill spacing is close and also . . .\u201d) as opposed to categories. These are otherwise often used in machine learning due to the use of various feature recoding methods, which would result in less understandable \u201cgill spacing close and . . .\u201d.\nMellers et al. [101] showed that using \u201cbank tellers who are feminists\u201d or \u201cfeminist bank tellers\u201d rather than \u201cbank tellers and feminists\u201d as a category in the Linda problem (Figure 2) might reduce the likelihood of committing the conjunction fallacy. It follows that using different wording such as \u201cand also\u201d might also help reduce the danger of a misunderstanding of \u201cand\u201d.\nRepresentations that visually express the semantics of \u201cand\u201d such as decision trees may be preferred over rules, which do not provide such visual guidance.4"}, {"heading": "5.3. Averaging Heuristic", "text": "While the conjunction fallacy is most commonly explained by the operation of the representativeness heuristic, the averaging heuristic provides an alternative explanation: it suggests that people evaluate the probability of a conjuncted event as the average of probabilities of the component events [39]. As reported by Fantino et al. [39], in their experiment \u201capproximately 49% of variance in subjects\u2019 conjunctions could be accounted for by a model that simply averaged the separate component likelihoods that constituted a particular conjunction.\u201d\nImplications for rule learning. When applying the averaging heuristic, an analyst may not fully realize the consequences of the presence of a low-probability condition for the overall likelihood of the set of conditions in the antecedent of the rule.\nConsider the following example: Let us assume that the learning algorithm only adds independent conditions that have a probability of 0.8, and we compare a 3-condition rule to a 2-condition rule. Averaging would evaluate both rules equally, because both have an average probability of 0.8. Correct computation\n4We find limited grounding for this proposition in the following: Conditions connected with an arch in a tree are to be interpreted as simultaneously valid (i.e., arch means conjunction). A recent empirical study on the comprehensibility of decision trees [129] does not consider the ambiguity of this notation to be a systematic problem among the surveyed users.\nof the joint probability, however, shows that the longer rule is considerably less likely (0.83 vs. 0.82 because all conditions are assumed to be independent). We expect that the averaging heuristic could be triggered both if the probability of the conditions is explicitly shown and if it is only based on internal knowledge of the decision maker.\nAveraging can also affect same-length rules. Continuing our example, if we compare the above 2-condition rule with another rule with two features with more diverse probability values, e.g., one condition has 1.0 and the other has 0.6, then averaging would again evaluate both rules the same, but in fact the correct interpretation would be that the rule with equal probabilities is more likely than the other (0.82 > 1.0\u00d7 0.6). In this case, the low 0.6 probability in the new rule would \u201cknock down\u201d the normative conjoint probability below the one of the rule with two 0.8 conditions.\nDebiasing techniques. Experiments conducted by Zizzo et al. [189] showed that prior knowledge of probability theory, and a direct reminder of how probabilities are combined, are effective tools for decreasing the incidence of the conjunction fallacy, which is the hypothesized consequence of the averaging heuristic. A specific countermeasure for the biases caused by linear additive integration (weighted averaging) is the use of logarithm formats. Experiments conducted by Juslin et al. [80] show that recasting probability computation in terms of logarithm formats, thus requiring additive rather than multiplicative integration, improves probabilistic reasoning. However, it should be noted that the use of the logarithm format may require additional training and also may lead to increased mental effort for the user. As a result, this is an example of a debiasing technique where the associated costs may outweigh the benefits."}, {"heading": "5.4. Disjunction Fallacy", "text": "The disjunction fallacy refers to a judgment that is inconsistent with the disjunction rule, which states that the probability Pr(X) cannot be higher than the probability Pr(Z), where Z = X \u222a Y is a union of event X with another event Y .\nIn experiments reported by Bar-Hillel and Neter [10], X and Z were nested pairs of categories, such as Switzerland and Europe. Subjects read descriptions of people such as \u201cWrites letter home describing a country with snowy wild mountains, clean streets, and flower decked porches. Where was the letter written?\u201d It follows that since Europe contains Switzerland, Europe must be more likely than Switzerland. However, Switzerland was rated as the more likely place by about 79% of the participants [10].\nThe disjunction fallacy is considered as another consequence of the representativeness heuristic [10]: \u201cWhich of two events\u2014even nested events\u2014will seem more probable is better predicted by their representativeness than by their scope, or by the level in the category hierarchy in which they are located.\u201d The description in the example is more representative of Switzerland than of Europe, so when people use representativeness as the basis for their judgment,\nthey judge Switzerland to be a more likely answer than Europe, even though this judgment breaks the disjunction rule.\nImplications for rule learning. In the context of data mining, it can be the case that the feature space is hierarchically ordered. The analyst can thus be confronted with rules containing attributes (literals) on multiple levels of granularity. Due to the disjunction fallacy, the analyst will generally prefer rules containing more specific attributes, which can result in a preference for rules with fewer backing instances and thus in weaker statistical validity.\nSimilarly as in some of the previous sections, we can illustrate the occurrence of this bias on user feedback collected as part of experiments conducted in [53]. There, multiple responses may be possibly linked to the disjunction fallacy (preference for specific attributes). In the absence of other deciding factors, one group of subjects preferred longer rules on the grounds that these are considered as more \u201cdescriptive\u201d, \u201cspecific\u201d, \u201cless broad\u201d or having \u201cmore data\u201d.\n\u201cThere\u2019s nothing that stands out as an \u201cobvious\u201d indicator of toxicity, so I\u2019ve gone for a weak preference for Rule 2 as it\u2019s describing a smaller number of species than Rule 1 and thus likely to be the more accurate of the two\u201d, \u201cAn extra group increases the likelihood.\u201d\nDebiasing techniques. When asked to assign categories to concepts (such as land of origin of a letter) under conditions of certainty, people are known to prefer a specific category to a more general category that subsumes it, but only if the specific category is considered representative [10]: \u201cwhenever an ordering of events by representativeness differs from their ordering by set inclusion, there is a potential for an extension fallacy to occur.\u201d From this observation a possible debiasing strategy emerges: making the analysts aware of the taxonomical relation of the individual attributes and their values. For example, the user interface can work with the information that Europe contains Switzerland, possibly actively notifying the analyst on the risk of falling for the disjunctive fallacy. This intervention can be complemented by \u201ctraining in rules\u201d [92]. In this case, the analysts should be explained the benefits of a larger supporting sample associated with more general attributes."}, {"heading": "5.5. Base-rate Neglect", "text": "People tend to underweight the evidence provided by base rates, which results in the so-called base-rate neglect. For example, Kahneman and Tversky [83] gave participants a description of a person who was selected randomly from a group and asked them whether the person is an engineer or a lawyer. Participants based their judgment mostly on the description of the person and paid little consideration to the occupational composition of the group, even though the composition was provided as part of the task and should play a significant role in the judgment.\nKahneman and Tversky [83] view the base-rate neglect as a possible consequence of the representativeness heuristic [82]. When people base their judgment of an occupation of a person mostly on the similarity of the person to a\nprototypical member of the occupation, they ignore other relevant information such as base rates, which results in the base-rate neglect.\nImplications for rule learning. Considering an inductively learnt rule of the form IF A AND B THEN C, the concept of base rate corresponds to the prior probability of the head of the rule Pr(C). While this information is important for assessing the plausibility of the rule, the two most commonly used statistical measures of rule quality, support and confidence, are not sufficient to communicate the base rate to the user.\nLet\u2019s consider the following two rules generated from a hypothetical dataset containing personal characteristics of 1000 people along with their occupation, which is the target variable. Out of these, there are 72 lawyers, 18 engineers, and the rest are different professions, which determines the respective base rates of 7.2% for lawyers, and 1.8% for engineers.\nLet us now consider the following two rules, both having 1.8% support. Since the total number of instances is 1.000, each of these two rules correctly classifies 0.018\u00d7 1000 = 18 instances in the training data. The confidence of both rules is 90%, which means that each rule misclassifies 10% of instances covered by its antecedent.\nExample. R1 IF writing is average AND intelligence is high AND\neloquence is average THEN profession is engineer confidence = 90%, support = 1.8%.\nR2 IF writing is excellent AND intelligence is high AND eloquence is high THEN profession is lawyer,\nconfidence = 90%, support = 1.8%.\nOnly based on the values of confidence and support, the two rules may seem equally plausible. However, the rule predicting engineers is better than the rule predicting lawyers in that it correctly covers a higher percentage of instances belonging to the predicted class value. In fact, Rule 1 covers and correctly classifies all 18 engineers in the dataset, while Rule 2 covers only 18 of the 72 lawyers. As follows from the base rate fallacy, even if the users know the base rates (or are presented with this information, e.g., as part of the exploratory data analysis), they will tend to not take full advantage of it when evaluating the plausibility of the presented rules.\nDebiasing techniques. As a possible debiasing technique for the problem introduced above, users can also be presented the value of lift, which can be interpreted as an improvement over the base rate provided by the rule. More precisely, lift is computed as a ratio of the confidence of the rule and a probability of the head of the rule, which corresponds to the base rate (cf. also Section 2.2). The lift of Rule 1 is 0.9/0.018 = 50, while lift of Rule 2 is 0.9/0.018 = 5. As can be seen, the value of lift is much higher for the first rule than for the second\nrule. The reason is that the base rate for the engineer is 1.8%, while for the lawyer profession it is 7.2%, while the confidences of both rules are the same. For additional discussion of the use of lift as a debiasing technique, please refer to the debiasing part of Section 5.19 describing the weak evidence effect.\nGigerenzer and Hoffrage [63] show that representations in terms of natural frequencies, rather than conditional probabilities, facilitate the computation of cause\u2019s probability. Confidence is typically presented as a percentage in current software systems. The support rule quality metric is sometimes presented as a percentage and sometimes as a natural number. It would foster correct understanding if analysts are consistently presented natural frequencies in addition to percentages. By adapting the diagrammatic representation of Bayes theorem (\u2019Euler circles\u2019) [12], visualization of rules using diagrams presenting their coverage may also help reduce the base rate neglect. A specific proposal for debiasing techniques based on natural frequencies is present in Section 5.1, cf. also the discussion present in the following subsection."}, {"heading": "5.6. Insensitivity to Sample Size", "text": "People tend to underestimate the increased benefit of higher robustness of estimates that are made on a larger sample, which is called insensitivity to sample size. The insensitivity to sample size effect can be illustrated by the so-called hospital problem. In this problem, subjects are asked which hospital is more likely to record more days in which more than 60 percent of the newborns are boys. The options are a larger hospital, a smaller hospital, or both hospitals with about a similar probability. The correct expected answer\u2014the smaller hospital\u2014was chosen only by 22% of participants in an experiment reported by Tversky and Kahneman [165]. Insensitivity to sample size may be another bias resulting from use of the representativeness heuristic [82]. When people use the representativeness heuristic, they compare the proportion of newborns who are boys to the proportion expected in the population, ignoring other relevant information. Since the proportion is similarly representative of the whole population for both hospitals, most of the participants believed that both hospitals are equally likely to record days in which more than 60 percent of the newborns are boys [165]. However, these responses do not take into account that small samples have higher variance and therefore there is a higher probability for recording an outlier not conforming to the underlying probability distribution.\nDifferentiation from the Base Rate Neglect. When determining the validity of assertions based on statistical evidence, insensitivity to sample size refers to the inability to appreciate the effect of sample size on the reliability of the provided estimate. In contrast, base rate neglect relates to the tendency to not use the information about base rates, even though it is available.\nImplications for rule learning. This effect implies that analysts may be unable to appreciate the increased reliability of the confidence estimate with increasing the value of support, i.e., they may fail to appreciate that the strength of the connection between antecedent and consequent of a rule rises with an increasing\nthe number of observations. If confronted with two rules, where one of them has a slightly higher confidence and the second rule a higher support, this cognitive bias suggests that the analyst will prefer the rule with higher confidence (all other factors equal).\nIn the context of this bias, it is important to realize that population size is statistically irrelevant for the determination of sample size for large populations [26]. However, previous research [8] has shown that the perceived sample accuracy can incorrectly depend on the sample-to-population ratio rather than on the absolute sample size. For a small population, a 10% sample can be considered as more reliable than 1% sample drawn from a much larger population. This observation has substantial consequences for the presentation of rule learning results as support of a rule is typically presented as a percentage of the dataset size. Assuming that support relates to sample size and the number of instances in the dataset to population size, it follows that the presentation of support as a percentage (relative support) induces the insensitivity to sample size effect.\nTo illustrate this effect on interpretation of rule learning results by actual users, we will use data collected in experiment 3 reported in [53]. In this experiment, participants were asked to evaluate pairs of rules in terms of plausibility. They were also presented values of support and confidence in a frequency format for each of the rules. One pair of the evaluated rules had the following properties:\n\u2022 Rule 1: confidence 52.7%, support 1.95%, antecedent length 1\n\u2022 Rule 2: confidence 52.1%, support 25.9%, antecedent length 1\nExample justifications given by participants finding rule 1 plausible: \u201cconfidence of test results slightly higher\u201d, \u201cBoth of these rules seem pretty terrible to me (53 and 52 percent are not impressive), but Rule 1 has a very marginally higher success rate.\u201d, \u201cconfidence is better\u201d. Example justification for no preference was \u201cboth rules have similar probability of being more plausible\u201d. Interestingly, no participant evaluated rule 2 as more plausible in spite of rule 2 having substantially higher support and very slightly higher confidence. The justifications provided by the participants refer only to the values of the interest measures, which shows that the actual semantics of the condition did not seem to have influenced their judgments. The overall results from this experiment indicate that if both confidence and support are explicitly revealed, confidence but not support will positively increase rule plausibility. A similar user study on a different dataset was performed in [127] confirming our initial finding.\nIt follows that by increasing preference for higher confidence, this effect will generally contribute to a positive correlation between rule length and plausibility, since longer rules can better adapt to a particular group in data and thus have a higher confidence than more general, shorter rules. This is in contrast to the general bias for simple rules that are implemented by state-of-the-art rule learning algorithms, because simple rules tend to be more general, have a higher support, and are thus statistically more reliable.\nDebiasing techniques. There have been successful experiments with providing decision aids to overcome the insensitivity to sample size bias. In particular, Kachelmeier and Messier Jr [81] experimented with providing auditors a formula for computing appropriate sample size for substantive tests of details based on the description of a case and tolerable error. Provision of the aid resulted in larger sample sizes being selected by the auditors in comparison to intuitive judgment without the aid. Similarly, as the auditor can choose the sample size, a user of an association rule learning algorithm can specify the minimum support threshold. To leverage the debiasing strategy validated by Kachelmeier and Messier Jr [81], the rule learning interface should also inform the user of the effects of chosen support threshold on the accuracy of the confidence estimate of the resulting rules. For algorithms and workflows where the user cannot influence the support of a discovered rule, relevant information should be available as a part of rule learning results. In particular, the value of rule support can be used to compute a confidence interval for the value of confidence. Such supplementary information is already provided by Bayesian decision lists [94], a recently proposed algorithmic framework positively evaluated with respect to interpretability (cf., e.g., [28]). Another hypothesized debiasing method is to adopt the present support as an absolute number (absolute support), as demonstrated in Figure 5 on page 17. It should be noted that in the experiment reported in [53], a variation of the frequency format-based debiasing was used, but the insensitivity of sample size effect was still present. We therefore hypothesize that the frequency format supplementing the standard ratio presentation of support and confidence may reduce the incidence of this bias, but not completely remove it (cf. also discussion in Section 5.1)."}, {"heading": "5.7. Confirmation Bias and Positive Test Strategy", "text": "Confirmation bias, sometimes also called myside bias, refers to the notion that people tend to look for evidence supporting the current hypothesis, disregarding conflicting evidence. According to Evans [36, p. 552] confirmation bias is \u201cthe best known and most widely accepted notion of inferential error of human reasoning.\u201d5\nResearch suggests that even neutral or unfavourable evidence can be interpreted to support existing beliefs, or, as Trope et al. [162, p. 115\u2013116] put it, \u201cthe same evidence can be constructed and reconstructed in different and even opposite ways, depending on the perceiver\u2019s hypothesis.\u201d\nA closely related phenomenon is the positive test strategy (PTS) described by Klayman and Ha [86]. This reasoning strategy suggests that when trying to test a specific hypothesis, people examine cases which they expect to confirm the hypothesis rather than the cases which have the best chance of falsifying it. The difference between PTS and confirmation bias is that PTS is applied to test a candidate hypothesis while confirmation bias is concerned with hypotheses that are already established [123, p. 93]. The experimental results of Klayman and\n5Cited according to Nickerson [116].\nHa [86] show that under realistic conditions, PTS can be a very good heuristic for determining whether a hypothesis is true or false, but it can also lead to systematic errors if applied to an inappropriate task.\nImplications for rule learning. This bias can have a significant impact depending on the purpose for which the rule learning results are used. If the analyst has some prior hypothesis before she obtains the rule learning results, according to the confirmation bias she will tend to \u201ccherry pick\u201d rules confirming this prior hypothesis and disregard rules that contradict it. Given that some rule learners may output contradicting rules, the analyst may tend to select only the rules conforming to the hypothesis, disregarding applicable rules with the opposite conclusion, which could otherwise turn out to be more relevant.\nDebiasing techniques. Delaying final judgment and slowing down work has been found to decrease confirmation bias in several studies [153, 128]. User interfaces for rule learning should thus give the user not only the opportunity to save or mark interesting rules, but also allow the user to review and edit the model at a later point in time. An example rule learning system with this specific functionality is EasyMiner [173].\nWolfe and Britt [186] successfully experimented with providing subjects with explicit guidelines for considering evidence both for and against a hypothesis. Provision of \u201cbalanced\u201d instructions to search evidence for and against a given hypothesis reduced the incidence of confirmation bias from 50% exhibited by the control group to a significantly lower 27.5%. The assumption that educating users about cognitive illusions can be an effective debiasing technique for positive test strategy has been empirically validated on a cohort of adolescents by Barberia et al. [11].\nSimilarly, providing explicit guidance combined with modifications of the user interface of the system presenting the rule learning results could also be considered. For example, in the context of explaining decisions of machine learning systems in the medical domain, [175] proposes to show prior probabilities of diagnoses. In the context of rule learning, a prior probability for the rule IF A AND B THEN C is the (unconditional) probability Pr(C) of C occurring in the data. This information is already available by some rule learning systems, but not presented in a form that would be immediately understandable to the domain expert. In Figure 6 (right), the user can use the values to compute the prior probability of the class \u2019poisonous\u2019 predicted by the rule (586 + 3330)/8124 = 48.2%. According to the discussed debiasing strategy, this ratio should be precomputed and readily available to the user."}, {"heading": "5.8. Availability Heuristic", "text": "The availability heuristic is a judgmental heuristic in which a person evaluates the frequency of classes or the probability of events by the ease with which relevant instances come to mind. This heuristic is explained by its discoverers, Tversky and Kahneman [164], as follows: \u201cThat associative bonds are strengthened by repetition is perhaps the oldest law of memory known to man. The\navailability heuristic exploits the inverse form of this law, that is, it uses the strength of the association as a basis for the judgment of frequency.\u201d The availability heuristic is not itself a bias, but it may lead to biased judgments when availability is not a valid cue.\nIn one of the original experiments, participants were asked whether the letter \u201cR\u201d appears more frequently on the first or third position in English texts [164]. About 70% of participants answered incorrectly that it appears more frequently on the first position, presumably because they estimated the frequency by recalling words containing \u201cR\u201d and it is easier to recall words starting with R than words with R on the third position.\nWhile original research did not distinguish between the number of recollected instances and ease of the recollection, later studies showed that to determine availability, it is sufficient to assess the ease with which instances or associations could be brought to mind; it is not necessary to count all the instances one is able to come up with [144].\nImplications for rule learning. An application of the availability heuristic in rule learning would be based on the ease of recollection of instances (examples) matching the complete rule (all conditions and consequent) by the analyst. Rules containing conditions for which instances can be easily recalled would be found more plausible compared to rules not containing such conditions. As an example, consider the rule pair\nR1: IF latitude \u2264 44.189 AND longitude \u2264 6.3333 AND longitude > 1.8397 THEN Unemployment is high R2: IF population \u2264 5 million THEN Unemployment is high.\nIt is arguably easier to recall specific countries matching the second rule, than countries matching the conditions of the first rule.\nIt is conceivable that the availability heuristic could also be applied in case when the easily recalled instances match only some of the conditions in the antecedent of the rule, such as only latitude in the example above. The remaining conditions would be ignored.\nOn the other hand, such a bias can also be implemented as a bias into rule learning algorithms. Often, in particular in cases where many candidate conditions are available, such as datasets with features derived from the semantic web [138], the same information can be encoded in rules that use different sets of conditions. For example, Gabriel et al. [54] proposed an algorithm that gives preference to selecting conditions that are semantically coherent. A similar technique could be used for realizing a preference for attributes that are easier to recall for human analysts.\nDebiasing techniques. Several studies have found that people use the ease of recollection in judgment only when they cannot attribute it to a source that should not influence their judgment [143]. Alerting an analyst to the reason why instances matching the conditions in the rule under consideration are easily recalled should, therefore, reduce the impact of the availability heuristic as long as the reason is deemed irrelevant to the task at hand."}, {"heading": "5.9. Reiteration Effect, Effects of Validity and Illusiory Truth", "text": "The reiteration effect describes the phenomenon that repeated statements tend to become more believable [74, 125]. For example, in one experiment, Hasher et al. [71] presented subjects with general statements and asked them to assess their validity. Some of the statements were false and some were true. The experiment was conducted in several sessions, where some of the statements were repeated in subsequent sessions. The average perceived validity of both true and false repeated statements rose between the sessions, while for nonrepeated statements it dropped slightly.\nThe effect is usually explained by use of the processing fluency in judgment. Statements that are processed fluently (easily) tend to be judged as true and repetition makes processing easier. A recent alternative account argues that repetition makes the referents of statements more coherent and people judge truth based on coherency [169].\nThe reiteration effect is also known under different labels, such as \u201cfrequencyvalidity\u201d or \u201cillusory truth\u201d [74, 195]. However, some research suggests that these are not identical phenomena. For example, the truth effect \u201cdisappears when the actual truth status is known\u201d [131, p. 253], which does not hold for validity effect in general.\nImplications for rule learning. In the rule learning context, a repeating statement which becomes more believable corresponds to the entire rule or possibly a \u201csubrule\u201d consisting of the consequent of the rule and a subset of conditions in its antecedent. A typical rule learning result contains multiple rules that are substantially overlapping. If the analyst is exposed to multiple similar statements, the reiteration effect will increase the analyst\u2019s belief in the repeating subrule.\nEspecially in the area of association rule learning, a very large set of redundant rules\u2014covering the same, or nearly same set of examples\u2014is routinely included in the output.\nSchwarz et al. [145] suggest that a mere 30 minutes of delay can be enough for information originally seen as negative to have a positive influence. Applying this in a data exploration task, consider an analyst who is presented a large number of \u201cweak\u201d rules corresponding to highly speculative patterns of data. Even if the analyst rejects the rule\u2014for example based on the presented metrics, pre-existing domain knowledge or common sense\u2014the reiteration effect will make the analyst more prone to accept a similar rule later.\nDebiasing techniques. The reiteration effect can be suppressed already on the algorithmic level by ensuring that rule learning output does not contain redundant rules. This can be achieved by pruning algorithms [49]. Another possible technique is presenting the result of rule learning in several layers, where only clusters of rules (\u201crule covers\u201d) summarizing multiple subrules are presented at first [122]. The user can expand the cluster to obtain more similar rules. A more recent algorithm that can be used for summarizing multiple rules is the meta-learning method proposed by Berka [16].\nSeveral lessons can be learnt from Hess and Hagen [76], who studied the role of the reiteration effect for spreading of gossip. Interestingly, already simple reiteration was found to increase perceived gossip veracity, but only for those who found the gossip relatively uninteresting. Multiple sources of gossip were found to increase its perceived veracity, especially when these sources were independent. Information that explained the gossip by providing benign interpretation decreased the perceived veracity of gossip. These findings suggest that it is important to explain to the analyst which rules share the same source, i.e. what is the overlap in their coverage in terms of specific instances. Second, explanations can be improved by utilisation of recently proposed techniques that use domain knowledge to filter or explain rules, such as expert deduction rules proposed by Rauch [136].\nThe research related to debiasing the reiteration effect has been largely centered around the problem of debunking various forms of misinformation (cf., e.g., [145, 95, 33]). The current largely accepted recommendation is that to correct misinformation, it is best to address it directly \u2013 repeat the misinformation along with arguments against it [95, 33]. This can be applied, for example, in incremental machine learning settings, when the results of learning are revised when new data arrive, or when mining with formalized domain knowledge. Generally, when the system has knowledge of the analyst being previously presented a rule (a hypothesis), which is falsified following the current state of knowledge, the system can explicitly notify the analyst, listing the rule in question and explaining why it does not hold."}, {"heading": "5.10. Mere Exposure Effect", "text": "According to the mere exposure effect, repeated exposure to an object results in an increased preference (liking, affect) for that object. When a concrete\nstimulus is repeatedly exposed, the preference for that stimulus increases logarithmically as a function of the number of exposures [20]. The size of the mere exposure effect also depends on whether the stimulus the subject is exposed to is exactly the same as in prior exposure or only similar to it [108]\u2014the same stimuli are associated with a larger mere exposure effect.\nThe mere exposure effect is another consequence of increased fluency of processing associated with repeated exposure (cf. Section 5.9) [184].\nDuration of the exposure below 1 second produces the strongest mere exposure effect, with increasing time of exposure the effect drops and repeating exposures decrease the effect. The liking induced by the effect drops more quickly with increasing exposures when the presented stimulus is simple (e.g., an ideogram) as opposed to complex (e.g., a photograph) [20]. A recent metaanalysis suggests that there is an inverted-U shaped relation between exposure and affect [109].\nDifferentiation from the reiteration effect. While the reiteration effect referred to the use of processing fluency in the judgment of truth, the mere exposure effect relates to the positive feeling that is associated with fluent processing. The mere exposure effect, unlike the reiteration effect [29, p. 245], has been also found to depend on the duration of stimulus exposure.\nImplications for rule learning. The extent to which the mere exposure effect can affect the interpretation of rule leaning results is limited by the fact that its magnitude decreases with extended exposure to the stimuli. It can be expected that the analysts inspect the rule learning results for a much longer period of time than the 1 second below which exposure results in the strongest effects [20]. However, it is not unusual for rule-based models to be composed of several thousand rules [3]. When the user scrolls through a list of rules, each rule can be shown only for a fraction of a second. The analyst is not aware of having seen the rule, yet the rule can influence the analyst\u2019s judgment through the mere exposure effect.\nThe mere exposure effect can also play a role when rules from the text mining or sentiment analysis domains are interpreted. The initial research of the mere exposure effect by Zajonc [187] included experimental evidence on the positive correlation between word frequency and affective connotation of the word. From this, it follows that a rule containing frequently occurring words can induce the mere exposure effect.\nDebiasing techniques. While there is a considerable body of research focusing on the mere exposure effect, our literature survey did not result in any directly applicable debiasing techniques. Only recently, Becker and Rinck [15] reported the reversal of the mere exposure effect when people fearful of spiders were presented spider pictures. Contrary to the mere exposure effect, the participants preferred pictures of spiders they had not seen before to those already seen. This result, although interesting, is difficult to transpose to the domain of rules given that it depends on the specific characteristic of the people and evaluated objects.\nNevertheless, there are some conditions known to decrease the mere exposure effect that can be utilized in machine learning interfaces. The effect is strongest for repeated, \u201cflash-like\u201d presentation of information. A possible workaround is to avoid subliminal exposure completely, by changing the mode of operation of the corresponding user interfaces. One attempt at a user interface to rule learning respecting these principles is the EasyMiner system [174, 173], in which the user can precisely formulate the mining task as a query against data. This restricts the number of rules that are discovered and the user is consequently exposed to."}, {"heading": "5.11. Overconfidence and underconfidence", "text": "A decision maker\u2019s judgment is normally associated with the belief that the judgment is true, i.e., with confidence in the judgment. Griffin and Tversky [67] argue that confidence in judgment is based on a combination of the strength of evidence and its weight (credibility). According to their studies, people tend to combine strength with weight in suboptimal ways, resulting in the decision maker being too much or too little confident about the hypothesis at hand than would be normatively appropriate given the available information. This discrepancy between the normative confidence and the decision maker\u2019s confidence is called overconfidence or underconfidence.\nPeople use the provided data to assess a hypothesis, but they insufficiently regard the quality of the data. Griffin and Tversky [67] describe this manifestation of bounded rationality as follows: \u201cIf people focus primarily on the warmth of the recommendation with insufficient regard for the credibility of the writer, or the correlation between the predictor and the criterion, they will be overconfident when they encounter a glowing letter based on a casual contact, and they will be underconfident when they encounter a moderately positive letter from a highly knowledgeable source.\u201d\nImplications for rule learning. Research has revealed systematic patterns of overconfidence and underconfidence [67, p. 426]: If the estimated difference between two competing hypotheses is large, it is easy to say which one is better and there is a pattern of underconfidence. As the degree of difficulty rises (the difference between the normative confidence of two hypotheses is decreasing), there is an increasing pattern of overconfidence.\nThe strongest overconfidence was recorded for problems where the weight of evidence is low and the strength of evidence is high. This directly applies to rules with a high value of confidence and low value of support. The empirical results related to the effect of difficulty, therefore, suggest that the predictive ability of such rules will be substantially overrated by analysts. This is particularly interesting because rule learning algorithms often suffer from a tendency to unduly prefer overly specific rules that have a high confidence on small parts of the data to more general rules that have a somewhat lower confidence, a phenomenon also known as overfitting. The above-mentioned results seem to indicate that humans suffer from a similar problem (albeit presumably for different reasons),\nwhich, e.g., implies that a human-in-the-loop solution may not alleviate this problem.\nDebiasing techniques. Research applicable to debiasing of overconfidence originated in 1950\u2019, but most initial efforts to reduce overconfidence have failed [41, 6]. Some recent research focuses on the hypothesis that the feeling of confidence reflects factors indirectly related to choice processes [46, 69]. For example, in a sports betting experiment performed by Hall et al. [69], participants underweighted statistical cues while betting when they knew the names of players. This research leads to the conclusion that \u201cmore knowledge can decrease accuracy and simultaneously increase prediction confidence\u201d [69]. Applying this to debiasing in the rule learning context, presenting less information can be achieved by reducing the number of rules and removing some conditions in the remaining rules. This can be achieved by a number of methods, such as feature selection or an external setting of maximum antecedent length, which is permitted by some (but not all) implementations of rule learning algorithms. Also, rules and conditions that do not pass a statistical significance test can be removed from the output.\nAs with other biases, research on debiasing overconfidence points at the importance of educating the experts on principles of subjective probability judgment and the associated biases [25]. Shafir [147, p. 487] recommends to debias overconfidence (in policymaking) by making the subject hear both sides of an argument. In the rule learning context, this would correspond to the user interface making rules and knowledge easily accessible, which is in \u201cunexpectedness\u201d or \u201cexception\u201d relation with the rule in question, as, e.g., experimented with in frameworks postprocessing association rule learning results [87]."}, {"heading": "5.12. Recognition Heuristic", "text": "Pachur et al. [125] define the recognition heuristic as follows: \u201cFor twoalternative choice tasks, where one has to decide which of two objects scores higher on a criterion, the heuristic can be stated as follows: If one object is recognized, but not the other, then infer that the recognized object has a higher value on the criterion.\u201d In contrast with the availability heuristic, which is based on ease of recall, the recognition heuristic is based only on the fact that a given object is recognized. The two heuristics could be combined. When only one object in a pair is recognized, then the recognition heuristic would be used for judgment. If both objects are recognized, then the speed of the recognition could influence the choice [75].\nThe use of this heuristic could be seen from an experiment performed by Goldstein and Gigerenzer [65], which focused on estimating which of two cities in a presented pair is more populated. People using the recognition heuristic would say that the city they recognize has a higher population. The median proportion of judgments complying to the recognition heuristic was 93%. It should be noted that the application of this heuristic is in this case ecologically justified since recognition will be related to how many times the city appeared in a newspaper report, which in turn is related to the city size [14].\nImplications for rule learning. The recognition heuristic can manifest itself by a preference for rules containing a recognized attribute name or value in the antecedent of the rule. Analysts processing rule learning results are typically shown many rules, contributing to time pressure. This can further increase the impact of the recognition heuristic.\nEmpirical results reported by Michalkiewicz et al. [103] indicate that people with higher cognitive ability use the recognition heuristic more when it is successful and less when it is not. The work of Pohl et al. [132] shows that people adapt their decision strategy with respect to the more general environment rather than the specific items they are faced with. Considering that the application of the recognition heuristic can in some situations lead to better results than the use of available knowledge, the recognition heuristic may not necessarily have overly negative impacts on the interpretation of rule learning results.\nIn feedback collected as part of experiments conducted in [53] there is some evidence of the recognition heuristic having been applied. For the mushroom dataset, there is a number of responses where the anise smell is explicitly given as a reason for positive preference on the grounds of prior experience (recognition): \u201cAnise smells nice and usually nice smelling things aren\u2019t poisonous\u201d, \u201cpersonal experience\u201d. Similar observations have been made for another dataset, where the participants were asked to say which of two presented rules predicting quality of living in a city is more plausible. The following justification for choosing a particular rule \u2013 \u201cCities in Switzerland tend to be nice\u201d \u2013 bears close resemblance to the general psychological experiment designed by [65] cited above, in which participants were asked which city was larger.\nDebiasing techniques. Under time pressure people assign a higher value to recognized objects than to unrecognized objects. This happens also in situations when recognition is a poor cue [124]. Changes to user interfaces that induce \u201cslowing down\u201d could thus help to address this bias. As to the alleviation of effects of recognition heuristic in situations where it is ecologically unsuitable, Pachur and Hertwig [124] note that suspension of the heuristic requires additional time or direct knowledge of the \u201ccriterion variable\u201d. In typical real-world machine learning tasks, the data can include a high number of attributes that even experts are not acquainted with in detail. When these are recognized (but not understood), even experts may be liable to the recognition heuristic. When information on the meaning of individual attributes and literals is made easily accessible, we conjecture that the application of the recognition heuristic can be suppressed."}, {"heading": "5.13. Information Bias", "text": "Information bias refers to the tendency to seek more information to improve the perceived validity of a statement even if the additional information is not relevant or helpful. The typical manifestation of the information bias is evaluating questions as worth asking even when the answer cannot affect the hypothesis that will be accepted [13].\nFor example, Baron et al. [13] asked subjects to assess to what degree a medical test is suitable for deciding which of three diseases to treat. The test detected a chemical, which was with a certain probability associated with each of the three diseases. These probabilities varied across the cases. Even though in some of the cases an outcome of the test would not change the most likely disease and thus the treatment, people tended to judge the test as worth doing. While information bias is primarily researched in the context of information acquisition [115, 113], some scientists interpret this more generally as judging features with zero probability gain as useful, having potential to change one\u2019s belief [114, p. 158].\nImplications for rule learning. Many rule learning algorithms allow the user to select the size of the generated model \u2013 in terms of the number of rules that will be presented, as well as by setting the maximum length of conditions of the generated rules. Either as part of the feature selection or when defining constraints for the learning the users decide which attributes are relevant. These can then appear among conditions of the discovered rules.\nAccording to the information bias, people will be prone to set up the task so that they receive more information \u2013 resulting in a larger rule list with longer rules containing attributes with little information value.\nIt is unclear if the information effect applies also to the case when the user is readily presented with more information, rather than given the possibility to request more information. Given the proximity of these two scenarios, we conjecture that information bias (or some related bias) will make people prefer more information to less, even if it is obviously not relevant. This conjecture is supported by the feedback collected as part of experiments conducted in [53]. When choosing which of the two presented rules is more plausible, multiple participants have chosen the longer rule providing justification such as: \u201cmore describing factors make identification easier\u201d, \u201cmore indicators means more certainty\u201d, \u201cRule 1 has a much tighter definition of what would constitute a poisonous mushroom with 5 conditions as compared to rule 2 which only contains just 1 condition for the same result, so rule 1 has a much higher plausibility of being believable\u201d.\nDebiasing techniques. While informing people about the diagnosticity of the considered questions does not completely remove the information bias, it reduces it [13]. To this end, communicating attribute importance can help guide the analyst in the task definition phase.\nAlthough existing algorithms and systems already provide ways for determining the importance of individual rules, for example via values of confidence, support, and lift, the cues on the importance of individual conditions in rule antecedent are typically not provided. While feature importance is computed within many learning algorithms, it is often used only internally. Exposing this information to the user can help counter the information bias."}, {"heading": "5.14. Ambiguity Aversion", "text": "Ambiguity aversion refers to the tendency to prefer known risks over unknown risks. This is often illustrated by the Ellsberg paradox [35], which shows that humans tend to systematically prefer a bet with a known probability of winning over a bet with a not precisely known probability of winning, even if it means that their choice is systematically influenced by irrelevant factors.\nAs argued by Camerer and Weber [22], ambiguity aversion is related to the information bias: the demand for information in cases when it has no effect on the decision can be explained by the aversion to ambiguity \u2014 people dislike having missing information. That is, the information bias can be seen as a possible consequence of ambiguity aversion in the case when there is a possibility to seek additional information.\nImplications for rule learning. The ambiguity aversion may have profound implications for rule learning. The typical data mining task will contain a number of attributes the analyst has no or very limited knowledge of. The ambiguity aversion will manifest itself in a preference for rules that do not contain ambiguous conditions.\nIn feedback collected as part of experiments conducted in [53], arguments were given as reasons for selecting the alternative rule not containing unknown value as more plausible. Specifically, in the Mushroom dataset, one rule of the two presented rules was considered more plausible because the other rule contained an unknown value or trait: \u201cCould a mushroom smell of creosote?\u201d, \u201cIt\u2019s hard to tell what creosote smells like.\u201d In this context, the recognition heuristic can be associated with making risk-averse decisions which is one of the hypothesized etiologies of cognitive biases (cf. Section 2.4). In the example above, conditions not recognized (such as creosote), were considered to be predictive of the worse outcome (poisonous mushroom).\nDebiasing techniques. An empirically proven way to reduce ambiguity aversion is accountability \u2013 \u201cthe expectation on the side of the decision maker of having to justify her decisions to somebody else\u201d [170]. This debiasing technique is hypothesized to work through higher cognitive effort that is induced by accountability.\nThis can be applied in the rule learning context by requiring the analysts to provide justifications for why they evaluated a specific discovered rule as interesting. Such an explanation can be textual, but also can have a structured form. To decrease demands on the analyst, the explanation may only be required if a conflict with existing knowledge has been automatically detected, for example, using the approach based on the deduction rules as proposed by Rauch [136].\nSince the application of the ambiguity aversion can partly stem from the lack of knowledge of the conditions included in the rule, it is conceivable that this bias would be alleviated if the description of the meaning of the conditions is made easily accessible to the analyst, as demonstrated in e.g. [87]."}, {"heading": "5.15. Confusion of the Inverse", "text": "This effect corresponds to confusing the probability of cause and effect, or, formally, the confidence of an implication A \u2192 B with its inverse B \u2192 A, i.e., Pr(B | A) is confused with the inverse probability Pr(A | B). For example, Villejoubert and Mandel [171] showed in an experiment that about half of the participants estimating the probability of membership in a class gave most of their estimates that corresponded to the inverse probability.\nImplications for rule learning. The confusion of the direction of the IF-THEN implication sign has significant consequences on the interpretation of a rule. Already Michalski [105] noted that there are two different kinds of rules, discriminative and characteristic. Discriminative rules can quickly discriminate an object of one category from objects of other categories. A simple example is the rule"}, {"heading": "IF trunk THEN elephant", "text": "which states that an animal with a trunk is an elephant. This implication provides a simple but effective rule for recognizing elephants among all animals.\nCharacteristic rules, on the other hand, try to capture all properties that are common to the objects of the target class. A rule for characterizing elephants could be"}, {"heading": "IF elephant THEN heavy, large, grey, bigEars, tusks, trunk.", "text": "Note that here the implication sign is reversed: we list all properties that are implied by the target class, i.e., by an animal being an elephant. From the point of understandability, characteristic rules are often preferable to discriminative rules. For example, in a customer profiling application, we might prefer to not only list a few characteristics that discriminate one customer group from the other, but we are rather interested in all characteristics of each customer group.\nCharacteristic rules are very much related to formal concept analysis [182, 56]. Informally, a concept is defined by its intent (the description of the concept, i.e., the conditions of its defining rule) and its extent (the instances that are covered by these conditions). A formal concept is then a concept where the extension and the intension are Pareto-maximal, i.e., a concept where no conditions can be added without reducing the number of covered examples. In Michalski\u2019s terminology, a formal concept is both discriminative and characteristic, i.e., a rule where the head is equivalent to the body.\nThis confusion may manifest itself strongest in the area of association rule learning, where an attribute can be of interest to the analyst both in the antecedent and consequent of a rule.\nDebiasing techniques. The confusion of the inverse seems to imply that humans will not clearly distinguish between these types of rules, and, in particular, tend to interpret an implication as an equivalence. From this, we can infer that characteristic rules, which add all possible conditions even if they do not have additional discriminative power, may be preferable to short discriminative rules.\nIn terms of generally applicable debiasing techniques, Edgell et al. [34] studied the influence of the effect of training of analysts in probabilistic theory with the conclusion that it is not effective in addressing the confusion of the inverse fallacy.\nWerner et al. [180, p. 195] point at a concern regarding the use of language liable to misinterpretation in statistical textbooks teaching fundamental concepts such as independence. The authors illustrate the misinterpretation on the statement whenever Y has no effect on X as \u201cThis statement is used to explain that two variables, X and Y, are independent and their joint distribution is simply the product of their margins. However, for many experts, the term \u2019effect\u2019 might imply a causal relationship.\u201d From this, it follows that representations of rules should strive for an unambiguous meaning of the wording of the implication construct. The specific recommendations provided by D\u0131\u0301az et al. [31] for teaching probability can also be considered in the next generation of textbooks aimed at the data science audience."}, {"heading": "5.16. Context and Tradeoff Contrast Effects", "text": "People evaluate objects in relation to other available objects, which may lead to various effects of the context of the presentation of a choice. For example, in one of the experiments described by Tversky and Simonson [168], subjects were asked to choose between two microwave ovens: Panasonic and Emerson. The number of subjects who chose Emerson was 57% and 43% chose Panasonic. Another group of subjects was presented the same problem with the following manipulation: A more expensive, but inferior Panasonic was added to the list of possible options. After this manipulation, only 13% chose the more expensive Panasonic, but the number of subjects choosing the cheaper Panasonic rose from 43% to 60%. That is, even though the additional option was dominated by the cheaper Panasonic device and it should have been therefore irrelevant to the relative preference of the other ovens, its addition changed the preference in favour of the better Panasonic device. The experiment thus shows that selection of one of the available alternatives, such as products or job candidates, can be manipulated by addition or deletion of alternatives that are otherwise irrelevant. Tversky and Simonson [168] attribute the tradeoff effect to the fact that \u201cpeople often do not have a global preference order and, as a result, they use the context to identify the most \u2019attractive\u2019 option.\u201d\nIt should be noted that according to Tversky and Simonson [168] if people have well-articulated preferences, the background context has no effect on the decision.\nImplications for rule learning. The effect could be illustrated on the inter-rule comparison level. In the base scenario, a constrained rule learning yields only a rule R1 with a confidence value of 0.7. Due to the relatively low value of confidence, the user does not find the rule very plausible. By lowering the minimum confidence threshold, multiple other rules predicting the same target class are discovered and shown to the user. These other rules, inferior to R1, would increase the plausibility of R1 by the tradeoff contrast effect.\nDebiasing techniques. Marketing professionals sometimes introduce more expensive versions of the main product, which induces the tradeoff contrast. The presence of a more expensive alternative with little added value increases sales of the main product [149]. Somewhat similarly, a rule learning algorithm can have on its output rules with very high confidence, sometimes even 1.0, but very low values of support. Removal of such rules can help to debias the analysts.\nThe influence of context can in some cases improve communication [149, p. 293]. An attempt at making contextual attributes explicit in the rule learning context was made by Gamberger and Lavrac\u030c [55], who introduced supporting factors as a means for complementing the explanation delivered by conventional learned rules. Essentially, supporting factors are additional attributes that are not part of the learned rule, but nevertheless have very different distributions with respect to the classes of the application domain. In line with the results of Kononenko [88], medical experts found that these supporting factors increase the plausibility of the found rules."}, {"heading": "5.17. Negativity Bias", "text": "According to the negativity bias, negative evidence tends to have a greater effect than neutral or positive evidence of equal intensity [140].\nFor example, the experiments by Pratto and John [135] investigated whether the valence of a word (desirable or undesirable trait) has effect on the time required to identify the colour in which the word appears on the screen. The results showed that the subjects took longer to name the colour of an undesirable word than for a desirable word. The authors argued that the response time was higher for undesirable words because undesirable traits get more attention. Information with negative valence is given more attention partly because people seek diagnostic information, and negative information is more diagnostic [150]. Some research suggests that negative information is better memorized and subsequently recognized [139, 120].\nImplications for rule learning. An interesting applicable discovery shows that negativity is an \u201cattention magnet\u201d [43, 120]. This implies that a rule predicting a class phrased with negative valence will get more attention than a rule predicting a class phrased with words with positive valence.\nWe expect this effect to apply also to conditions in the antecedent of the rules. In experiments conducted in [53], we presented participants a rule that contained conditions with clearly negative valence (\u2019foul\u2019 smell) and a rule containing multiple neutral conditions, but also often associated with poisonous mushrooms.\nR1: IF veil color is white and gill spacing is close and mushroom does not have bruises and mushroom has one ring THEN the mushroom is poisonous R2: IF odour is foul THEN the mushroom is poisonous.\nAll participants who provided the rationale for their plausibility judgment referred to the foul smell, even though some of them preferred R1. This could illustrate how the negativity bias could influence analysts, even though in this example the observation is not necessarily a proof of the bias.\nDebiasing techniques. Putting a higher weight to negative information may in some situations be a valid heuristic. What needs to be addressed are cases, when the relevant piece of information (a condition in the rule or the predicted class) is positive and a less relevant piece of information is negative [77, 166]. It is therefore advisable that any such suspected cases are detected in the data preprocessing phase, and the corresponding attributes or values are replaced with more neutral-sounding alternatives."}, {"heading": "5.18. Primacy Effect", "text": "Once people form an initial assessment of plausibility (favourability) of an option, its subsequent evaluations will reflect this initial disposition.\nBond et al. [19] investigated to what extent changing the order of information which is presented to a potential buyer affects the propensity to buy. For example, in one of the experiments, if the positive information (product description) was presented as first, the number of participants indicating they would buy the product was 48%. When the negative information (price) was presented first, this number decreased to 22%. Bond et al. [19] argue that the effect is caused by the distortion of interpretation of new information in the direction of the already held opinion. The information presented first not only influences disproportionately the final opinion, but it also influences the interpretation of novel information.\nImplications for rule learning. Following the primacy effect, the analyst will favour rules that are presented as first in the rule model. Largest negative effects of this bias are likely to occur when such ordering does not correspond to the quality of the rules (as e.g., reflected by confidence, support or lift), for example, when rules are presented in the order in which they were discovered by a breadthfirst algorithm. In this case, mental contamination is another applicable bias related to the primacy effect (or in general order effects). This refers to the case when a presented hypothesis can influence subsequent decision making by its content, even if the subject is fully aware of the fact that the presented information is purely speculative [44]. Note that our application scenario differs from [44] and some other related research, in that cognitive psychology mostly investigated the effect of asking a hypothetical question, while we are concerned with considering the plausibility of a presented hypothesis (inductively learnt rule). Fitzsimons and Shiv [44] found that respondents are generally not able to prevent the contamination effects of the hypothetical questions and that the bias increases primarily when the hypothetical question is relevant. This bias is partly attributed to the application of expectations related to conversational maxims [64].\nDebiasing techniques. Three types of debiasing techniques were examined by Mumma and Wilson [111] in the context of clinical-like judgments. The bias inoculation intervention involves direct training on the applicable bias or biases, consisting of information on the bias, strategies for adjustment, as well as\ncompleting several practical assignments. The second technique was considerthe-opposite debiasing strategy, which sorts the information according to diagnosticity before it is reviewed. The third strategy evaluated was simply taking notes when reviewing each cue before the final judgment was made. Interestingly, bias inoculation, a representative of direct debiasing techniques, was found to be the least effective. Consider-the-opposite and taking notes were found to work equally well.\nTo this end, a possible debiasing strategy can be founded in the presentation of the most relevant rules first. Similarly, the conditions within the rules can be ordered by predictive power. Some rule learning algorithms, such as CBA [97], readily take advantage of the primacy effect, since they naturally create rule models that contain rules sorted by their strength. Other algorithms order rules so that more general rules (i.e., rules that cover more examples) are presented first. This typically also corresponds to the order in which rules are learned with the commonly used separate-and-conquer or covering strategies [50]. Simply reordering the rules output by these algorithms may not work in situations, when rules compose a rule list that is automatically processed for prediction purposes.6In order to take advantage of the note-taking debiasing strategy, the user interface can support the analyst in annotating the individual rules.\nLau and Coiera [93] provide a reason for optimism concerning the debiasing effect stemming from the proposed changes to user interface of machine learning tools. Their paper showed a debiasing effect of similar changes implemented in a user interface to an information retrieval system used by consumers to find health information. Three versions of the system were compared: a baseline \u201cstandard\u201d search interface, anchor debiasing interface, which asked the users to annotate the read documents as providing evidence for/against/neutral the proposition in question. Finally, the order debiasing interface reordered the documents to neutralize the primacy bias by creating a \u201ccounteracting order bias\u201d. This was done by randomly reshuffling a part of the documents. When participants used the baseline and anchor debiasing interface, the order effect was present. On the other hand, the use of the order debiasing interface eliminated the order effect [93]."}, {"heading": "5.19. Weak Evidence Effect", "text": "According to the weak evidence effect, presenting weak evidence in favour of an outcome can actually decrease the probability that a person assigns to the outcome. For example, in an experiment in the area of forensic science reported by Martire et al. [100], it was shown that participants presented with evidence\n6One technique that can positively influence comprehensibility of the rule list is prepending (adding to the beginning) a new rule to the previously learned rules [178]. The intuition behind this argument is that there are often simple rules that would cover many of the positive examples, but also cover a few negative examples that have to be excluded as exceptions to the rule. Placing the simple general rule near the end of the rule list allows us to handle exceptions with rules that are placed before the general rule and keep the general rule simple.\nweakly supporting guilt tended to \u201cinvert\u201d the evidence, thereby counterintuitively reducing their belief in the guilt of the accused. Fernbach et al. [40] argue that the effect occurs because people give undue weight to the weak evidence and fail to take into account alternative evidence that more strongly favours the hypothesis at hand.\nImplications for rule learning. The weak evidence effect can be directly applied to rules: the evidence is represented by the rule antecedent; the consequent corresponds to the outcome. The analyst can intuitively interpret each of the conditions in the antecedent as a piece of evidence in favour of the outcome. Typical of many machine learning problems is the uneven contribution of individual attributes to the prediction. Let us assume that the analyst is aware of the prediction strength of the individual attributes.\nIf the analyst is to choose from a rule containing only one strong condition (predictor) and another rule containing a strong predictor and a weak (weak enough to trigger this effect) predictor, according to the weak evidence effect the analyst would choose the shorter rule with one predictor.\nDifferentiation from information bias. In the context of rule learning, the weak evidence effect needs to be contrasted with the information bias, which can lead to a preference for longer rules. The two phenomena work at a different stage of working with rules. The information bias leads people to request more information, even if the additional information is not helpful. The weak evidence effect is triggered when the analyst evaluates a rule. If the rule contains a condition that the analyst knows to be weak, this effect may cause this rule to be evaluated as less plausible than without the weak predictor.\nNote that in Section 5.13 we suggest that information bias may also apply when the user is readily presented with more information. In that case, we speculate that which bias will be triggered will depend on whether the strength of evidence is known. It is also possible that the effects of both biases will be combined.\nDebiasing techniques. Martire et al. [99] performed an empirical study aimed at evaluating what mode of communication of the strength of evidence is most resilient to the weak evidence effect. The surveyed modes of expression were numerical, verbal, a table, and a visual scale. It should be noted that the study was performed in the specific field of assessing evidence by a juror in a trial and the verbal expressions were following standards proposed by the Association of Forensic Science Providers [183].7 The results clearly suggested that numerical expressions of evidence are most suitable for expressing uncertainty, which\u2014in the scope of rule learning\u2014is typically expressed through rule confidence.\n7These provide guidelines on the translation of numerical likelihood ratios into verbal formats. For example, likelihood \u201c> 1\u221210\u201d is translated as \u201cweak or limited\u201d, and likelihood of \u201c1000\u2212 10, 000\u201d as \u201cstrong\u201d.\nLikelihood ratios studied by Martire et al. [99] are conceptually close to the lift metric, used to characterize association rules. While lift is still typically presented as a number in machine learning user interfaces, there has been research towards communicating rule learning results in the natural language since at least 2005 [157]. With the recent resurgence of interest in interpretable models, the use of natural language has been taken up by commercial machine learning services, such as BigML, which allow to generate predictions via spoken questions and answers using Amazon Alexa voice service.8 Similarly, machine learning interfaces increasingly rely on visualizations. The research on debiasing of the weak evidence effect suggests that when conveying machine learning results using modern means, such as transformation to natural language or through visualizations, care must be taken when numerical information is communicated. For example, it follows from the applicable empirical evidence that it is not advisable to replace the numerical strength of evidence with linguistic labels, which are frequently used in fuzzy systems [72] and also in rule learning. It should be noted that the numerical representation is not without problems as well. In particular, the Steven\u2019s power law shows that relationship between the magnitude of a stimulus and its perceived magnitude is not linear [84]. While this law was initially applied to physical stimuli, it has also been recently shown to hold for virtual concepts [142].\nMartire et al. [99] also observe a high level of miscommunication associated with low-strength verbal expressions. In these instances, it is \u201cappropriate to question whether expert opinions in the form of verbal likelihood ratios should be offered at all\u201d [99]. Transposing this result to the machine learning context, we suggest to consider an intentional omission of weak predictors from rules either directly by the rule learner or as part of feature selection."}, {"heading": "5.20. Unit Bias", "text": "The unit bias refers to the tendency to give each unit similar weight while ignoring or underweighting the size of the unit [57].\nGeier et al. [57] offered people various food items in two different sizes on different days and observed how this would affect the consumption of the food. They found that people ate a larger amount of food when the size of a single unit of the food item was big than when it was small. A possible explanation is that people ate one unit of food at a time without taking into account how big it was. Because the food was not consumed in larger amounts at any single occasion but was rather eaten intermittently, the behaviour led to higher consumption when a unit of food was larger.\nImplications for rule learning. Unit bias has been so far primarily studied for quite different purposes than is the domain of machine learning. Nevertheless, it can be very relevant to the domain of rule learning.\n8https://bigml.com/tools/alexa-voice\nFrom a technical perspective, the number of conditions in rules is not important. What matters is the actual discriminatory power of the individual conditions, which can vary substantially. However, following the application of unit bias, people can view conditions as units of similar importance, disregarding their sometimes vastly different discriminatory and predictive power.\nWe found some support for unit bias in the responses collected in [53]: \u201cRule one contains twice as many properties as rule 2 does for determining the edibility of a mushroom so that makes it statistically twice as plausible, hence much higher probability of being believable\u201d, \u201cAn extra group increases the likelihood.\u201d It follows from the responses above, that when assessing plausibility, these participants mainly relied on the count of conditions in the antecedent of the rule disregarding quality or predictive power of the conditions, which would correspond to the application of the unit bias.\nDebiasing techniques. One of the common ways how regulators address unhealthy food consumption patterns related to varying sizes of packaging is introduction of mandatory labelling of the size and calorie contents. Following an analogy to clearly communicating the size of the food item, informing analysts about the discriminatory power of the individual conditions may alleviate unit bias. Such an indicator can be generated automatically, for example, by listing the number of instances in the entire dataset that meet the condition."}, {"heading": "6. Recommendations for Rule Learning Algorithms and Software", "text": "This section provides a concise list of considerations that is aimed to raise awareness among machine learning practitioners regarding the availability of measures that could potentially suppress the effect of cognitive biases on the comprehension of rule-based models. We expect part of the list to be useful also for other symbolic machine learning models, such as decision trees. In our recommendations, we focus on systems that present the rule model to a human user, which we refer to as the analyst."}, {"heading": "6.1. Adherence to conversational rules", "text": "Especially when results of machine learning are communicated to the general audience, it is essential to ensure that the automatically generated explanations of machine learning models do not violate the conversational rules set out by Grice [66] (wording of maxims adopted from Thomas [161]):\n\u2022 Quantity : Make your contribution as informative as is required (for the current purpose of the exchange). Do not make your contribution more informative than is required.\n\u2022 Quality : Do not say what you believe to be false. Do not say that for which you lack adequate evidence.\n\u2022 Relation: Be relevant.\n\u2022 Manner : Avoid obscurity of expression. Avoid ambiguity. Be brief (avoid unnecessary prolixity). Be orderly.\nSome of these maxims are directly related to rule learning in the following recommendations."}, {"heading": "6.2. Representation of a rule", "text": "The interpretation of natural language expressions used to describe a rule can lead to systematic distortions. Our review revealed the following recommendations applicable to individual rules:\n1. Syntactic elements. There are several cognitive studies indicating that the conjunction AND is often misunderstood [73], [60, p. 95-96]. The results of our experiments [53] support the conclusion that AND needs to be presented unambiguously in the rule learning context. Research has shown that AND ceases to be ambiguous when it is used to connect propositions rather than categories. To help avoid the confusion of the inverse effect, the communication of the implication construct IF THEN connecting antecedent and consequent should be made unambiguous.\nAnother important syntactic construct is negation (NOT). While processing of negation has not been included among the surveyed biases, our review of the literature (cf. Section 7.5) suggests that its use should be discouraged on the grounds that its processing requires more cognitive effort, and because the fact that a specific piece of information was negated may not be remembered in the long term.\n2. Conditions. Attribute-value pairs comprising conditions are typically either formed of words with semantics meaningful to the user, or of codes that are not directly meaningful. A number of biases can be triggered or strengthened by the lack of understanding of attributes and their values appearing in rules. Providing easily accessible information on conditions in the rules, including their predictive power, can thus prove as an effective debiasing technique.\nWhen conditions contain words with negative valence, these need to be reviewed carefully, since negative information is known to receive more attention and is associated with higher weight than positive information.\nPeople have the tendency to put a higher emphasis on information they are exposed to first. By ordering the conditions by strength, machine learning software can conform to human conversational maxims. The output could also visually delimit conditions in the rules based on their significance or predictive strength.\n3. Interestingness measures. The values should be communicated using numerical expressions. Alternate verbal expressions, with wordings such as \u201cstrong relationship\u201d replacing specific numerical values, are discouraged\nbecause there is some evidence that such verbal expressions are prone to miscommunication.\nCurrently, rule interest measures are typically represented as probabilities (confidence) or ratios (lift), whereas results in cognitive science indicate that natural frequencies are better understood.\nThe tendency of humans to ignore base rates and sample sizes (which closely relate to rule support) is a well-established fact in cognitive science. Results of our experiments on inductively learned rules also provide evidence for this conclusion [53]. Our proposition is that this effect can be addressed by presenting confidence (reliability) intervals for the values of measures of interest, where applicable."}, {"heading": "6.3. Rule models", "text": "In many cases, rules are not presented in isolation to the analyst, but instead within a collection of rules comprising a rule model. Here, we relate the results of our review to the following aspects of rule models:\n4. Model size. An experiment by Poursabzi-Sangdeh et al. [134] found that people are better able to simulate results of a smaller regression model composed of two coefficients than of a larger model composed of eight coefficients. The results indicate that removal of any unnecessary variables could improve model interpretability even though the experiment did not find a difference in the trust in the model based on the number of coefficients it consisted of. Similarly to regression models, rule models often incorporate output that is considered as marginally relevant. This can take a form of (nearly) redundant rules or (nearly) redundant conditions in the rule. Our analysis shows that such redundancies can induce a number of biases, which may be accountable for misinterpretation of the model. Size of a rule model can be reduced by utilizing various pruning techniques, or by using learning algorithms that allow the user to set or influence the size of the resulting model. Examples of such approaches include those proposed by Letham et al. [94], Lakkaraju et al. [91], Wang et al. [176]. The Interpretable Decision Sets algorithm [91] can additionally optimize for diversity and non-overlap of discovered rules, directly countering the reiteration effect.\nAnother potentially effective approach to discarding some rules can be using domain knowledge or constraints set by the user to remove the strong (e.g., highly confident), yet \u201cobvious\u201d rules confirming common knowledge.9 Removal of weak rules could help to address the tradeoff contrast as well as the weak evidence effect.\n9For example, it is well-known that diastolic blood pressure rises with body mass index (DBP\u2191\u2191BMI). Rules confirming this relationship might be removed [87].\n5. Rule grouping. The rule learning literature has seen multiple attempts to develop methods for grouping similar rules, often by clustering. Our review suggests that presenting clusters of similar rules can help to reduce cognitive biases caused by reiteration.\n6. Rule ordering. Algorithms that learn rule lists provide mandatory ordering of rules, while the rule order in rule-set learning algorithms is not important. In either case, the rule order as presented to the user will affect perception of the model due to conversational maxims and the primacy effect. It is recommended to sort the presented rules by strength. However, due to paucity of applicable research, it is unclear which particular definition of rule strength would lead to the best results in terms of bias mitigation."}, {"heading": "6.4. User Engagement", "text": "Some results of our review suggest that increasing user interaction can help counter some biases. Some specific suggestions for machine learning user interfaces (UIs) follow:\n7. Domain knowledge. Selectively presenting domain knowledge \u201cconflicting\u201d with the considered rule can help to invoke the \u2019consider-the-opposite\u2019 debiasing strategy. Other research has shown that the plausibility of a model depends on compliance with monotonicity constraints [48]. We thus suggest that UIs make background information on discovered rules easily accessible.\n8. Eliciting rule annotation. Activating the deliberate \u201cSystem 2\u201d is one of the most widely applicable debiasing strategies. One way to achieve this is to require accountability, e.g., through visual interfaces motivating users to annotate selected rules, which would induce the \u2019note-taking\u2019 debiasing strategy. For this technique to be effective, the created annotations would need to be consequently checked or used, either by a human or algorithmically. Giving people additional time to consider the problem has been in some cases shown as an effective debiasing strategy. This can be achieved by making the selection process (at least) two-stage, allowing the user to revise the selected rules.\n9. User search for rules rather than scroll. Repeating rules can affect users via the mere exposure effect even if they are exposed to them even for a short moment, e.g., when scrolling a rule list. To mitigate this effect, the user interfaces should thus deploy alternatives to scrolling in discovered rules, such as search facilities. This can be combined with other measures for limiting the number of initially displayed rules. One option for improving diversity in the preview initially shown to the user is clustering the rules and showing a representative of each cluster."}, {"heading": "6.5. Bias inoculation", "text": "In some studies, basic education about specific biases, such as brief tutorials, decreased the fallacy rate. This debiasing strategy has been called bias inoculation in the literature.\n10. Education on specific biases. Several studies have shown that providing explicit guidance and education on formal logic, hypothesis testing, and critical assessment of information can reduce fallacy rates in some tasks. However, the effect of psychoeducational methods is still a subject of dispute [96], and cannot be thus recommended as a sole or sufficient measure."}, {"heading": "7. Limitations and Future Work", "text": "Our goal was to examine whether cognitive biases can affect the interpretation of machine learning models and to propose possible remedies if they do. Since this field is untapped from the machine learning perspective, we tried to approach the problem holistically. Our work yielded a number of partial contributions, rather than a single profound result. We mapped applicable cognitive biases, identified prior works on their suppression, and proposed how these could be transferred to machine learning.\nIn the following, we outline some promising direction for future work."}, {"heading": "7.1. Validation through human-subject experiments", "text": "All the identified shortcomings of the human judgment pertaining to the interpretation of inductively learned rules are based on empirical cognitive science research. For each cognitive bias, we provided a justification for how it would relate to machine learning. Due to the absence of applicable prior research in the intersection between cognitive science and machine learning, this justification is mostly based on authors\u2019 experience in machine learning.\nA critical next step is empirical validation of the effect of the selected cognitive biases. We have already described several user experiments aimed at validating selected cognitive biases in Fu\u0308rnkranz et al. [53]. Some other machine learning researchers have reported human-subject experiments that do not explicitly refer to cognitive biases, yet the cognitive phenomena they investigate may correspond to a known cognitive bias. One example is a study by Lage et al. [90] (cf. also the extended version in [112]), which investigated the effect of the number of cognitive chunks (conditions) in a rule on response time. While the main outcome confirms the intuition that higher complexity results in higher response times, this study has also revealed several unexpected patterns, such as that defining a new concept and reusing it leads to a higher response time than repeating the description whenever that concept implicitly appears, even though this repetition means that subjects have to read more lines. The findings could possibly be attributed to increased fluency due to repetition.\nFor several biases, we included examples based on data that we collected in our prior work [53]. The main limitation of these examples is that they are a\nproduct of a user experiment performed with traditional psychological methods (questionnaires). Future work could use a complementary method \u2013 observational study of users working in an operational environment (i.e. with real rule learning software). To facilitate these efforts, it is necessary to adjust existing machine learning software with means for supporting cognitive experiments. In Vojir and Kliegr [172], we describe a rule editor adjusted for user studies of explainability, but similar software is needed also for other types of machine learning models.\nDespite the existence of several early studies, much more concentrated and systematic effort is needed to yield insights on the size of effect individual biases can have on the understanding of intrinsically explainable machine learning models, such as rules and decision trees."}, {"heading": "7.2. Role of Domain Knowledge", "text": "It has been long recognized that external knowledge plays an important rule in the rule learning process. Already Mitchell [107] recognized at least two distinct roles external knowledge can play in machine learning: it can constrain the search for appropriate generalizations, and guide learning based on the intended use of the learned generalizations. Interaction with domain knowledge has played an important role in multiple stages of the machine learning process. For example, it can improve semi-supervised learning [23], and in some applications, it is vital to convert discovered rules back into domain knowledge [52, p. 288]. Some results also confirm the common intuition that compliance to constraints valid in the given domain increases the plausibility of the learned models [48].\nOur review shows that domain knowledge can be one of the important instruments in the toolbox aimed at debiasing interpretation of discovered rules. To give a specific example, the presence or strength of the reiteration effect depends on the familiarity of the subject with the topic area from which the information originates [18] (cf. also Section 5.9). Future work should focus on a systematic review of the role of domain knowledge on activation or inhibition of cognitive phenomena applicable to the interpretability of rule learning results."}, {"heading": "7.3. Individual Differences", "text": "The presence of multiple cognitive biases and their strength have been linked to specific personality traits. For example, overconfidence and the rate of conjunctive fallacy have been shown to be inversely related to numeracy [185]. According to Juslin et al. [80], the application of the averaging heuristic rather than the normative multiplication of probabilities seems to depend on the working memory capacity and/or motivation.\nSome research can even be interpreted as indicating that data analysts can be more susceptible to the confirmation bias than the general population. An experiment reported by Wolfe and Britt [186] shows that subjects who defined good arguments as those that can be proved by facts (this stance, we assume,\nwould also apply to many data analysts) were more prone to exhibiting the confirmation bias.10 Stanovich et al. [154] show that the incidence of confirmation bias is surprisingly not related to general intelligence. This suggests that even highly intelligent analysts can be affected. Albarrac\u0301\u0131n and Mitchell [2] propose that the susceptibility to the confirmation bias can depend on one\u2019s personality traits. They also present a diagnostic tool called \u201cdefense confidence scale\u201d that can identify individuals who are prone to confirmational strategies. While it has been shown that education on some biases mitigates their effect, it would not be practical and cost-effective to educate all end-users. Further research into personality traits of users of machine learning outputs, as well as into the development of appropriate personality tests, would help to better target education focused on debiasing."}, {"heading": "7.4. Effectiveness and negative effects of debiasing techniques", "text": "Debiasing strategies discussed in this article aim at mitigating negative effects by eliminating or reducing the biases. While there are very few empirical studies on the incidence of biases in the context of rule learning, there is almost no research on possible negative effects of debiasing strategies. For this reason, as a first step before deploying a debiasing strategy, we suggest that it should be examined that the actually targeted bias occurs and can distort decision making significantly enough to warrant the application of a debiasing strategy. Some early research [127] suggests that application of a debiasing strategy can in some cases distort decision making even more than if it is left untreated. For this reason, we also suggest that it should be verified that the applied debiasing strategy provides a sufficient improvement over the baseline before it is deployed. Future research should thus focus on understanding the trade-off between the costs and feasibility of a particular debiasing technique and its effectiveness."}, {"heading": "7.5. Extending Scope Beyond Biases", "text": "There is a number of cognitive phenomena affecting the interpretability of rules, which are not classified as cognitive biases. Remarkably, since 1960 there is a consistent line of work by psychologists studying cognitive processes related to rule induction, which is centred around the so-called Wason\u2019s 2-4-6 problem [177]. Cognitive science research on rule induction in humans has so far not been noticed in the rule learning subfield of machine learning.11 It was out of the scope of the objectives of this review to conduct an analysis of the significance of these results for rule learning, nevertheless, we believe that such investigation could bring interesting insights for the cognitively-inspired design of rule learning algorithms.\n10This tendency is explained by Wolfe and Britt [186] as follows: \u201cFor people with this belief, facts and support are treated uncritically. . . . More importantly, arguments and information that may support another side are not part of the schema and are also ignored.\u201d\n11Based on our analysis of cited reference search in Google Scholar for [177].\nAnother promising direction for further work is research focused on the interpretation of negations (\u201cnot\u201d). Experiments conducted by Jiang et al. [78] show that the mental processes involved in processing negations slow down reasoning. Negation can be also sometimes ignored or forgotten [30], as it decreases veracity of long-term correct remembrance of information.\nMost rule learning algorithms are capable of generating rules containing negated literals. For example, a healthy company can be represented as status = not(bankrupt). Our precautionary suggestion based on the interpretation of results obtained in general studies performed in experimental psychology [30] and neurolinguistics [78] is that artificial learning systems should refrain, wherever feasible, from the use of negation in the discovered rules that are to be presented to the user. Due to the adverse implications of the use of negation on cognitive load and remembrance, empirical research focused on the interpretability of negation in machine learning is urgently needed."}, {"heading": "7.6. Extending Scope Beyond Inductively Learned Rules", "text": "Most of our discussion focused on logical models in the form of symbolic rules. However, the underlying principle is more general: for the automatic generation of arbitrary but human-interpretable models, we need to understand how humans perceive these models. Findings from the cognitive sciences, such as those regarding cognitive biases, may help to formulate models in a better and more convincing way, not only for logical models, but also for visual, structural, mathematical, or probabilistic models. The principles should not only apply to inductively learned models, but also to models that have been obtained by other forms of reasoning, such as deduction, search-based planning, or CaseBased Reasoning. However, further work will be necessary to understand the specific requirements of each of these cases, such as the fact that the model is guaranteed to be correct (deduction) or that the model consists of one or more suitably chosen prior experiences as in Case-Based Reasoning. Also, different users may have different levels of expertise and might require different levels of abstraction. For example, the acceptance of mathematical models will certainly depend on the user\u2019s proficiency in mathematics, and similar cases can be made for domain experts in various application areas such as medicine. While we have provided some pointers to work that links specific biases to the user\u2019s background (such as formal training in statistics, intelligence), more work in this direction is needed."}, {"heading": "8. Conclusion", "text": "To our knowledge, cognitive biases have not yet been discussed in relation to the interpretability of machine learning results. We thus initiated this review of research published in cognitive science with the intent of providing a psychological basis to further research in inductive rule learning algorithms, and to the way their results are communicated. Our review covered twenty cognitive biases, heuristics, and effects that can give rise to systematic errors when inductively learned rules are interpreted.\nFor most biases and heuristics included in our review, psychologists have proposed \u201cdebiasing\u201d measures. Application of prior empirical results obtained in cognitive science allowed us to propose several methods that could be effective in suppressing these cognitive phenomena when machine learning models are interpreted. In particular, the proposed representation of discovered rules based on \u201cfrequency formats\u201d of [63] is hypothesized to reduce the number of judgmental errors attributable to the empirically proven tendency of users to overvalue rule confidence at the expense of rule support (insensitivity to sample size), as well to the misunderstanding of the and conjunction.\nOverall, in our review, we processed only a fraction of potentially relevant psychological studies of cognitive biases, but we were unable to locate a single study focused on machine learning. Future research should thus focus on empirical evaluation of the effects of cognitive biases in the machine learning domain."}, {"heading": "Acknowledgments", "text": "TK was supported by long term institutional support of research activities. S\u030cB and TK were supported by grant IGA 33/2018 by Faculty of Informatics and Statistics, University of Economics, Prague. An initial version of this review was published as a part of TK\u2019s PhD thesis at Queen Mary University of London.\nWe are grateful for the helpful comments of the reviewers of this paper."}], "title": "A review of possible effects of cognitive biases on interpretation of rule-based machine learning models", "year": 2020}