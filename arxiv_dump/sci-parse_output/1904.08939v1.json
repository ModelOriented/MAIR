{
  "abstractText": "A neuroscience method to understanding the brain is to find and study the preferred stimuli that highly activate an individual cell or groups of cells. Recent advances in machine learning enable a family of methods to synthesize preferred stimuli that cause a neuron in an artificial or biological brain to fire strongly. Those methods are known as Activation Maximization (AM) [10] or Feature Visualization via Optimization. In this chapter, we (1) review existing AM techniques in the literature; (2) discuss a probabilistic interpretation for AM; and (3) review the applications of AM in debugging and explaining networks.",
  "authors": [
    {
      "affiliations": [],
      "name": "Anh Nguyen"
    },
    {
      "affiliations": [],
      "name": "Jason Yosinski"
    },
    {
      "affiliations": [],
      "name": "Jeff Clune"
    }
  ],
  "id": "SP:2eea4ac96a1986452201a63beddea08a26d44c47",
  "references": [
    {
      "authors": [
        "N. Akhtar",
        "A. Mian"
      ],
      "title": "Threat of adversarial attacks on deep learning in computer vision: A survey",
      "venue": "IEEE Access 6, 14410\u201314430",
      "year": 2018
    },
    {
      "authors": [
        "M.A. Alcorn",
        "Q. Li",
        "Z. Gong",
        "C. Wang",
        "L. Mai",
        "W.S. Ku",
        "A. Nguyen"
      ],
      "title": "Strike (with) a pose: Neural networks are easily fooled by strange poses of familiar objects",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. vol. 1, p. 4. IEEE",
      "year": 2019
    },
    {
      "authors": [
        "M. Baer",
        "B.W. Connors",
        "M.A. Paradiso"
      ],
      "title": "Neuroscience: Exploring the brain",
      "year": 2007
    },
    {
      "authors": [
        "D. Bau",
        "B. Zhou",
        "A. Khosla",
        "A. Oliva",
        "A. Torralba"
      ],
      "title": "Network dissection: Quantifying interpretability of deep visual representations",
      "venue": "Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on. pp. 3319\u20133327. IEEE",
      "year": 2017
    },
    {
      "authors": [
        "Y. Bengio",
        "G. Mesnil",
        "Y. Dauphin",
        "S. Rifai"
      ],
      "title": "Better mixing via deep representations",
      "venue": "International Conference on Machine Learning. pp. 552\u2013560",
      "year": 2013
    },
    {
      "authors": [
        "A. Brock",
        "T. Lim",
        "J.M. Ritchie",
        "N. Weston"
      ],
      "title": "Neural photo editing with introspective adversarial networks",
      "venue": "arXiv preprint arXiv:1609.07093",
      "year": 2016
    },
    {
      "authors": [
        "J Deng"
      ],
      "title": "Imagenet: A large-scale hierarchical image database",
      "venue": "CVPR",
      "year": 2009
    },
    {
      "authors": [
        "J. Donahue",
        "L.A. Hendricks",
        "S. Guadarrama",
        "M Rohrbach"
      ],
      "title": "Long-term recurrent convolutional networks for visual recognition and description",
      "venue": "Computer Vision and Pattern Recognition",
      "year": 2015
    },
    {
      "authors": [
        "A. Dosovitskiy",
        "T. Brox"
      ],
      "title": "Generating images with perceptual similarity metrics based on deep networks",
      "venue": "NIPS",
      "year": 2016
    },
    {
      "authors": [
        "D. Erhan",
        "Y. Bengio",
        "A. Courville",
        "P. Vincent"
      ],
      "title": "Visualizing higher-layer features of a deep network",
      "venue": "Dept. IRO, Universit\u00e9 de Montr\u00e9al, Tech. Rep 4323",
      "year": 2009
    },
    {
      "authors": [
        "R. Fong",
        "A. Vedaldi"
      ],
      "title": "Net2vec: Quantifying and explaining how concepts are encoded by filters in deep neural networks",
      "venue": "arXiv preprint arXiv:1801.03454",
      "year": 2018
    },
    {
      "authors": [
        "G. Goh"
      ],
      "title": "Image synthesis from Yahoo Open NSFW",
      "venue": "https://opennsfw.gitlab.io",
      "year": 2016
    },
    {
      "authors": [
        "I. Goodfellow",
        "J. Pouget-Abadie",
        "M. Mirza",
        "B. Xu",
        "D. Warde-Farley",
        "S. Ozair",
        "A. Courville",
        "Y. Bengio"
      ],
      "title": "Generative adversarial nets",
      "venue": "NIPS",
      "year": 2014
    },
    {
      "authors": [
        "D.H. Hubel",
        "T.N. Wiesel"
      ],
      "title": "Receptive fields of single neurones in the cat\u2019s striate cortex",
      "venue": "The Journal of physiology 148(3), 574\u2013591",
      "year": 1959
    },
    {
      "authors": [
        "Y. Jia",
        "E. Shelhamer",
        "J. Donahue",
        "S. Karayev",
        "J. Long",
        "R. Girshick",
        "S. Guadarrama",
        "T. Darrell"
      ],
      "title": "Caffe: Convolutional architecture for fast feature embedding",
      "venue": "arXiv preprint arXiv:1408.5093",
      "year": 2014
    },
    {
      "authors": [
        "V.M. Kabilan",
        "B. Morris",
        "A. Nguyen"
      ],
      "title": "Vectordefense: Vectorization as a defense to adversarial examples",
      "venue": "arXiv preprint arXiv:1804.08529",
      "year": 2018
    },
    {
      "authors": [
        "E.R. Kandel",
        "J.H. Schwartz",
        "T.M. Jessell",
        "S.A. Siegelbaum",
        "Hudspeth",
        "A.J"
      ],
      "title": "Principles of neural science, vol",
      "venue": "4. McGraw-hill New York",
      "year": 2000
    },
    {
      "authors": [
        "A. Krizhevsky",
        "I. Sutskever",
        "G.E. Hinton"
      ],
      "title": "Imagenet classification with deep convolutional neural networks",
      "venue": "Advances in neural information processing systems. pp. 1097\u20131105",
      "year": 2012
    },
    {
      "authors": [
        "Q.V. Le"
      ],
      "title": "Building high-level features using large scale unsupervised learning",
      "venue": "Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on. pp. 8595\u20138598. IEEE",
      "year": 2013
    },
    {
      "authors": [
        "Y. Li",
        "J. Yosinski",
        "J. Clune",
        "H. Lipson",
        "J. Hopcroft"
      ],
      "title": "Convergent learning: Do different neural networks learn the same representations? In: Feature Extraction: Modern Questions and Challenges",
      "venue": "pp. 196\u2013212",
      "year": 2015
    },
    {
      "authors": [
        "A. Mahendran",
        "A. Vedaldi"
      ],
      "title": "Visualizing deep convolutional neural networks using natural pre-images",
      "venue": "Computer Vision and Pattern Recognition (CVPR)",
      "year": 2016
    },
    {
      "authors": [
        "K. Malakhova"
      ],
      "title": "Visualization of information encoded by neurons in the higher-level areas of the visual system",
      "venue": "J. Opt. Technol. 85(8), 494\u2013498",
      "year": 2018
    },
    {
      "authors": [
        "G. Montavon",
        "W. Samek",
        "K.R. M\u00fcller"
      ],
      "title": "Methods for interpreting and understanding deep neural networks",
      "venue": "Digital Signal Processing",
      "year": 2017
    },
    {
      "authors": [
        "A. Mordvintsev",
        "C. Olah",
        "M. Tyka"
      ],
      "title": "Inceptionism: Going deeper into neural networks",
      "venue": "Google Research Blog. Retrieved June 20",
      "year": 2015
    },
    {
      "authors": [
        "A. Nguyen"
      ],
      "title": "Wyoming. Computer Science Department, U.: AI Neuroscience: Visualizing and Understanding Deep Neural Networks. University of Wyoming (2017), https://books.google.com/books?id=QCexswEACAAJ pages",
      "year": 2017
    },
    {
      "authors": [
        "A. Nguyen",
        "J. Clune",
        "Y. Bengio",
        "A. Dosovitskiy",
        "J. Yosinski"
      ],
      "title": "Plug & play generative networks: Conditional iterative generation of images in latent space",
      "venue": "Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on. pp. 3510\u20133520. IEEE",
      "year": 2017
    },
    {
      "authors": [
        "A. Nguyen",
        "A. Dosovitskiy",
        "J. Yosinski",
        "T. Brox",
        "J. Clune"
      ],
      "title": "Synthesizing the preferred inputs for neurons in neural networks via deep generator networks",
      "venue": "Advances in Neural Information Processing Systems. pp. 3387\u20133395",
      "year": 2016
    },
    {
      "authors": [
        "A. Nguyen",
        "J. Yosinski",
        "J. Clune"
      ],
      "title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images",
      "venue": "Computer Vision and Pattern Recognition (CVPR)",
      "year": 2015
    },
    {
      "authors": [
        "A. Nguyen",
        "J. Yosinski",
        "J. Clune"
      ],
      "title": "Multifaceted feature visualization: Uncovering the different types of features learned by each neuron in deep neural networks",
      "venue": "Visualization for Deep Learning Workshop, ICML conference",
      "year": 2016
    },
    {
      "authors": [
        "A. Nguyen",
        "J. Yosinski",
        "J. Clune"
      ],
      "title": "Understanding innovation engines: Automated creativity and improved stochastic optimization via deep learning",
      "venue": "Evolutionary Computation 24(3), 545\u2013572",
      "year": 2016
    },
    {
      "authors": [
        "A.M. Nguyen",
        "J. Yosinski",
        "J. Clune"
      ],
      "title": "Innovation engines: Automated creativity and improved stochastic optimization via deep learning",
      "venue": "Proceedings of the 2015 Annual Conference on Genetic and Evolutionary Computation. pp. 959\u2013966. ACM",
      "year": 2015
    },
    {
      "authors": [
        "C. Olah",
        "A. Satyanarayan",
        "I. Johnson",
        "S. Carter",
        "L. Schubert",
        "K. Ye",
        "A. Mordvintsev"
      ],
      "title": "The building blocks of interpretability",
      "venue": "Distill 3(3), e10",
      "year": 2018
    },
    {
      "authors": [
        "S. Palazzo",
        "C. Spampinato",
        "I. Kavasidis",
        "D. Giordano",
        "M. Shah"
      ],
      "title": "Decoding brain representations by multimodal learning of neural activity and visual features",
      "venue": "arXiv preprint arXiv:1810.10974",
      "year": 2018
    },
    {
      "authors": [
        "K. Pei",
        "Y. Cao",
        "J. Yang",
        "S. Jana"
      ],
      "title": "Deepxplore: Automated whitebox testing of deep learning systems",
      "venue": "Proceedings of the 26th Symposium on Operating Systems Principles. pp. 1\u201318. ACM",
      "year": 2017
    },
    {
      "authors": [
        "C.R. Ponce",
        "W. Xiao",
        "P. Schade",
        "T.S. Hartmann",
        "G. Kreiman",
        "M.S. Livingstone"
      ],
      "title": "Evolving super stimuli for real neurons using deep generative networks",
      "venue": "bioRxiv p. 516484",
      "year": 2019
    },
    {
      "authors": [
        "R.Q. Quiroga",
        "L. Reddy",
        "G. Kreiman",
        "C. Koch",
        "I. Fried"
      ],
      "title": "Invariant visual representation by single neurons in the human brain",
      "venue": "Nature 435(7045), 1102\u20131107",
      "year": 2005
    },
    {
      "authors": [
        "G.O. Roberts",
        "J.S. Rosenthal"
      ],
      "title": "Optimal scaling of discrete approximations to langevin diffusions",
      "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology) 60(1), 255\u2013268",
      "year": 1998
    },
    {
      "authors": [
        "D.E. Rumelhart",
        "G.E. Hinton",
        "R.J. Williams"
      ],
      "title": "Learning representations by backpropagating errors",
      "venue": "nature 323(6088), 533",
      "year": 1986
    },
    {
      "authors": [
        "O Russakovsky"
      ],
      "title": "Imagenet large scale visual recognition challenge",
      "venue": "IJCV 115(3), 211\u2013252",
      "year": 2015
    },
    {
      "authors": [
        "G. Shen",
        "T. Horikawa",
        "K. Majima",
        "Y. Kamitani"
      ],
      "title": "Deep image reconstruction from human brain activity",
      "venue": "PLoS computational biology 15(1), e1006633",
      "year": 2019
    },
    {
      "authors": [
        "K. Simonyan",
        "A. Vedaldi",
        "A. Zisserman"
      ],
      "title": "Deep inside convolutional networks: Visualising image classification models and saliency maps",
      "venue": "ICLR workshop",
      "year": 2014
    },
    {
      "authors": [
        "K. Soomro",
        "A.R. Zamir",
        "M. Shah"
      ],
      "title": "Ucf101: A dataset of 101 human actions classes from videos in the wild",
      "venue": "arXiv preprint arXiv:1212.0402",
      "year": 2012
    },
    {
      "authors": [
        "C. Szegedy",
        "W. Zaremba",
        "I. Sutskever",
        "J. Bruna",
        "D. Erhan",
        "I.J. Goodfellow",
        "R. Fergus"
      ],
      "title": "Intriguing properties of neural networks",
      "venue": "CoRR abs/1312.6199",
      "year": 2013
    },
    {
      "authors": [
        "M. Tyka"
      ],
      "title": "Class visualization with bilateral filters",
      "venue": "https://mtyka.github.io/ deepdream/2016/02/05/bilateral-class-vis.html,",
      "year": 2018
    },
    {
      "authors": [
        "D. Wei",
        "B. Zhou",
        "A. Torrabla",
        "W. Freeman"
      ],
      "title": "Understanding intra-class knowledge inside cnn",
      "venue": "arXiv preprint arXiv:1507.02379",
      "year": 2015
    },
    {
      "authors": [
        "R. Yeh",
        "C. Chen",
        "T.Y. Lim",
        "M. Hasegawa-Johnson",
        "M.N. Do"
      ],
      "title": "Semantic image inpainting with perceptual and contextual losses",
      "venue": "arxiv preprint. arXiv preprint arXiv:1607.07539 2",
      "year": 2016
    },
    {
      "authors": [
        "J. Yosinski",
        "J. Clune",
        "A. Nguyen",
        "T. Fuchs",
        "H. Lipson"
      ],
      "title": "Understanding neural networks through deep visualization",
      "venue": "Deep Learning Workshop, ICML conference",
      "year": 2015
    },
    {
      "authors": [
        "M.D. Zeiler",
        "R. Fergus"
      ],
      "title": "Visualizing and understanding convolutional networks",
      "venue": "Computer Vision\u2013ECCV 2014, pp. 818\u2013833. Springer",
      "year": 2014
    },
    {
      "authors": [
        "B. Zhou",
        "A. Khosla",
        "A. Lapedriza",
        "A. Oliva",
        "A. Torralba"
      ],
      "title": "Object detectors emerge in deep scene cnns",
      "venue": "International Conference on Learning Representations (ICLR)",
      "year": 2015
    },
    {
      "authors": [
        "B. Zhou",
        "A. Lapedriza",
        "J. Xiao",
        "A. Torralba",
        "A. Oliva"
      ],
      "title": "Learning deep features for scene recognition using places database",
      "venue": "Advances in neural information processing systems",
      "year": 2014
    },
    {
      "authors": [
        "B. Zhou",
        "H. Zhao",
        "X. Puig",
        "S. Fidler",
        "A. Barriuso",
        "A. Torralba"
      ],
      "title": "Scene parsing through ade20k dataset",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. vol. 1, p. 4. IEEE",
      "year": 2017
    },
    {
      "authors": [
        "J.Y. Zhu",
        "P. Kr\u00e4henb\u00fchl",
        "E. Shechtman",
        "A.A. Efros"
      ],
      "title": "Generative visual manipulation on the natural image manifold",
      "venue": "European Conference on Computer Vision. pp. 597\u2013613. Springer",
      "year": 2016
    },
    {
      "authors": [
        "A.M. \u00d8ygard"
      ],
      "title": "Visualizing GoogLeNet classes | audun m \u00f8ygard",
      "venue": "https: //www.auduno.com/2015/07/29/visualizing-googlenet-classes/,",
      "year": 2018
    }
  ],
  "sections": [
    {
      "text": "Keywords: Neural networks, feature visualization, activation maximization, generator network, generative models, optimization"
    },
    {
      "heading": "1 Introduction",
      "text": "Understanding the human brain has been a long-standing quest in human history. One path to understanding the brain is to study what each neuron4 codes for [17], or what information its firing represents. In the classic 1950\u2019s experiment, Hubel and Wiesel studied a cat\u2019s brain by showing the subject different images on a screen while recording the neural firings in the cat\u2019s primary visual cortex (Fig. 1). Among a variety of test images, the researchers found oriented edges to cause high responses in one specific cell [14]. That cell is referred to as an edge detector and such images are called its preferred stimuli. The same technique later enabled scientists to discover fundamental findings of how neurons along the visual pathway detect increasingly complex patterns: from circles, edges to faces and high-level concepts such as one\u2019s grandmother [3] or specific celebrities like the actress Halle Berry [37].\nSimilarly, in machine learning (ML), visually inspecting the preferred stimuli of a unit can shed more light into what the neuron is doing [48,49]. An intuitive approach is to find such preferred inputs from an existing, large image collection e.g. the training or test set [49]. However, that method may have undesired\n4In this chapter, \u201cneuron\u201d, \u201ccell\u201d, \u201cunit\u201d, and \u201cfeature\u201d are used interchangeably.\nar X\niv :1\n90 4.\n08 93\n9v 1\n[ cs\n.L G\n] 1\n8 A\npr 2\n01 9\nproperties. First, it requires testing each neuron on a large image set. Second, in such a dataset, many informative images that would activate the unit may not exist because the image space is vast and neural behaviors can be complex [28]. Third, it is often ambiguous which visual features in an image are causing the neuron to fire e.g. if a unit is activated by a picture of a bird on a tree branch, it is unclear if the unit \u201ccares about\u201d the bird or the branch (Fig. 13b). Fourth, it is not trivial how to extract a holistic description of what a neuron is for from the typically large set of stimuli preferred by a neuron.\nA common practice is to study the top 9 highest activating images for a unit [48,49]; however, the top-9 set may reflect only one among many types of features that are preferred by a unit [29].\nInstead of finding real images from an existing dataset, one can synthesize the visual stimuli from scratch [10,25,27,29,32,42,46]. The synthesis approach offers multiple advantages: (1) given a strong image prior, one may synthesize (i.e. reconstruct) stimuli without the need to access the target model\u2019s training set, which may not be available in practice (see Sec. 5); (2) more control over the types and contents of images to synthesize, which helps shed light on more controlled research experiments. Activation Maximization Let \ud835\udf03 be the parameters of a classifier that maps an image x \u2208 R\ud835\udc3b\u00d7\ud835\udc4a \u00d7\ud835\udc36 (that has \ud835\udc36 color channels, each of which is \ud835\udc4a pixels wide and \ud835\udc3b pixels high) onto a probability distribution over the output classes. Finding an image x that maximizes the activation \ud835\udc4e\ud835\udc59\ud835\udc56(\ud835\udf03, x) of a neuron indexed \ud835\udc56 in a given layer \ud835\udc59 of the classifier network can be formulated as an optimization problem:\nx* = arg max x (\ud835\udc4e\ud835\udc59\ud835\udc56(\ud835\udf03, x)) (1)\nThis problem was introduced as activation maximization5 (AM) by Erhan, Bengio and others [10]. Here, \ud835\udc4e\ud835\udc59\ud835\udc56(.) returns the activation value of a single unit as in many previous works [27\u201329]; however, it can be extended to return any neural response \ud835\udc4e(.) that we wish to study e.g. activating a group of neurons [24,26,33]. The remarkable DeepDream visualizations [24] were created by running AM to activate all the units across a given layer simultaneously. In this chapter, we will write \ud835\udc4e(.) instead of \ud835\udc4e\ud835\udc59\ud835\udc56(.) when the exact indices \ud835\udc59, \ud835\udc56 can be omitted for generality.\nAM is a non-convex optimization problem for which one can attempt to find a local minimum via gradient-based [44] or non-gradient methods [30]. In post-hoc interpretability [23], we often assume access to the parameters and architecture of the network being studied. In this case, a simple approach is to perform gradient ascent [10,27,31,48] with an update rule such as:\nx\ud835\udc61+1 = x\ud835\udc61 + \ud835\udf161 \ud835\udf15\ud835\udc4e(\ud835\udf03, x\ud835\udc61)\n\ud835\udf15x\ud835\udc61 (2)\nThat is, starting from a random initialization x0 (here, a random image), we iteratively take steps in the input space following the gradient of \ud835\udc4e(\ud835\udf03, x) to find an input x that highly activates a given unit. \ud835\udf161 is the step size and is chosen empirically.\nNote that this gradient ascent process is similar to the gradient descent process used to train neural networks via backpropagation [39], except that here we are optimizing the network input instead of the network parameters \ud835\udf03, which are frozen.6 We may stop the optimization when the neural activation has reached a desired threshold or a certain number of steps has passed.\nIn practice, synthesizing an image from scratch to maximize the activation alone (i.e. an unconstrained optimization problem) often yields uninterpretable images [28]. In a high-dimensional image space, we often find rubbish examples (also known as fooling examples [28]) e.g. patterns of high-frequency noise that look like nothing but that highly activate a given unit (Fig. 2).\nIn a related way, if starting AM optimization from a real image (instead of a random one), we may easily encounter adversarial examples [44] e.g. an image that is slightly different from the starting image (e.g. of a school bus), but that a network would give an entirely different label e.g. \u201costrich\u201d [44]. Those early AM visualizations [28,44] revealed huge security and reliability concerns with machine learning applications and informed a plethora of follow-up adversarial attack and defense research [1, 16].\nNetworks that we visualize Unless otherwise noted, throughout the chapter, we demonstrate AM on CaffeNet, a specific pre-trained model of the well-known AlexNet convnets [18] to perform single-label image classification on the ILSVRC 2012 ImageNet dataset [7,40].\n5Also sometimes referred to as feature visualization [29,32,48]. In this chapter, the phrase \u201cvisualize a unit\u201d means \u201csynthesize preferred images for a single neuron\u201d.\n6Therefore, hereafter, we will write \ud835\udc4e(\ud835\udc65) instead of \ud835\udc4e(\ud835\udf03, \ud835\udc65), omitting \ud835\udf03, for simplicity."
    },
    {
      "heading": "2 Activation Maximization via Hand-designed Priors",
      "text": "Examples like those in Fig. 2b are not human-recognizable. While the fact that the network responds strongly to such images is intriguing and has strong implications for security, if we cannot interpret the images, it limits our ability to understand what the unit\u2019s purpose is. Therefore, we want to constrain the search to be within a distribution of images that we can interpret e.g. photo-realistic images or images that look like those in the training set. That can be accomplished by incorporating natural image priors into the objective function, which was found to substantially improve the recognizability of AM images [21, 27, 29, 32, 48]. For example, an image prior may encourage smoothness [21] or penalize pixels of extreme intensity [42]. Such constraints are often incorporated into the AM formulation as a regularization term \ud835\udc45(x):\nx* = arg max x (\ud835\udc4e(x) \u2212 \ud835\udc45(x)) (3)\nFor example, to encourage the smoothness in AM images, \ud835\udc45 : R\ud835\udc3b\u00d7\ud835\udc4a \u00d7\ud835\udc36 \u2192 R may compute the total variation (TV) across an image [21]. That is, in each update, we follow the gradients to (1) maximize the neural activation; and (2) minimize the total variation loss:\nx\ud835\udc61+1 = x\ud835\udc61 + \ud835\udf161 \ud835\udf15\ud835\udc4e(x\ud835\udc61)\n\ud835\udf15x\ud835\udc61 \u2212 \ud835\udf162 \ud835\udf15\ud835\udc45(x\ud835\udc61) \ud835\udf15x\ud835\udc61\n(4)\nHowever, in practice, we do not always compute the analytical gradient \ud835\udf15\ud835\udc45(x\ud835\udc61)/\ud835\udf15x\ud835\udc61. Instead, we may define a regularization operator \ud835\udc5f : R\ud835\udc3b\u00d7\ud835\udc4a \u00d7\ud835\udc36 \u2192 R\ud835\udc3b\u00d7\ud835\udc4a \u00d7\ud835\udc36 (e.g. a Gaussian blur kernel), and map x to a more regularized (e.g. slightly blurrier as in [48]) version of itself in every step. In this case, the update step becomes:\nx\ud835\udc61+1 = \ud835\udc5f(x\ud835\udc61) + \ud835\udf161 \ud835\udf15\ud835\udc4e(x\ud835\udc61)\n\ud835\udf15x\ud835\udc61 (5)\nNote that this update form in Eq. 5 is strictly more expressive [48], and allows the use of non-differentiable regularizers \ud835\udc5f(.).\n(b ) \ud835\udc3f\n\" no\nrm\n(c ) G au ss ia n bl ur\n(d ) P\nat ch\nda ta\nse t\n(e ) T\not al\nva\nria tio n (f) C en te r bi as\n(g ) M\nea n\nim ag e in iti al iz at io n\n(h ) G\nen er\nat or\nne tw\nor k\nSi m\non ya\nn et\na l\n(2 01 4) Yo si ns ki e t a l (2 01 5)\n(a ) R\nea l\nim ag\nes\nW ei\ne t a l (2 01 5)\nM ah\nen dr\nan e\nt a l\n(2 01 6) N gu ye n et a l (2 01\n6) N gu ye n et a l (2 01\n6) N gu ye n et a l (2 01 6, 2 01 7)\nFig.3: Activation maximization results of seven methods in the literature (b\u2013h), each employing a different image prior (e.g. \ud835\udc3f2 norm, Gaussian blur, etc.). Images are synthesized to maximize the output neurons (each corresponding to a class) of the CaffeNet image classifier [18] trained on ImageNet. The categories were not cherry-picked, but instead were selected based on the images available in previous papers [21,29,42,46,48]. Overall, while it is a subjective judgement, Activation Maximization via Deep Generator Networks method (h) [27] produces images with more natural colors and realistic global structures. Image modified from [27].\nLocal statistics AM images without priors often appear to have high-frequency patterns and unnatural colors (Fig. 2b). Many regularizers have been designed in the literature to ameliorate these problems including:\n\u2013 Penalize extreme-intensity pixels via \ud835\udefc-norm [42,46,48] (Fig. 3b). \u2013 Penalize high-frequency noise (i.e. smoothing) via total variation [21, 29] (Fig. 3e), Gaussian blurring [48,54] (Fig. 3c) or a bilateral filter [45]. \u2013 Randomly jitter, rotate, or scale the image before each update step to synthesize stimuli that are robust to transformations, which has been shown to make images clearer and more interpretable [24,32]. \u2013 Penalize the high frequencies in the gradient image \ud835\udf15\ud835\udc4e(x\ud835\udc61)\ud835\udf15x\ud835\udc61 (instead of the visualization x\ud835\udc61) via Gaussian blurring [32,54]. \u2013 Encourage patch-level color statistics to be more realistic by (1) matching those of real images from a dataset [46] (Fig. 3d) or (2) learning a Gaussian mixture model of real patches [24].\nWhile substantially improving the interpretability of images (compared to highfrequency rubbish examples), these methods only effectively attempt to match the local statistics of natural images.\nGlobal structures Many AM images still lack global coherence; for example, an image synthesized to highly activate the \u201cbell pepper\u201d output neuron (Fig. 3b\u2013e) may exhibit multiple bell-pepper segments scattered around the same image rather than a single bell pepper. Such stimuli suggest that the network has learned some local discriminative features e.g. the shiny, green skin of bell peppers, which are useful for the classification task. However, it raises an interesting question: Did the network ever learn the global structures (e.g. the whole pepper) or only the local discriminative parts? The high-frequency patterns as in Fig. 3b\u2013e might also be a consequence of optimization in the image space. That is, when making pixel-wise changes, it is non-trivial to ensure global coherence across the entire image. Instead, it is easy to increase neural activations by simply creating more local discriminative features in the stimulus.\nPrevious attempts to improve the global coherence include:\n\u2013 Gradually paint the image by scaling it and alternatively following the gradients from multiple output layers of the network [54]. \u2013 Bias the image changes to be near the image center [29] (Fig. 3g). \u2013 Initialize optimization from an average image (computed from real training\nset images) instead of a random one [29] (Fig. 3h).\nWhile these methods somewhat improved the global coherence of images (Fig. 3g\u2013h), they rely on a variety of heuristics and introduce extra hyperparameters [29,54]. In addition, there is still a large realism gap between the real images and these visualizations (Fig. 3a vs. h).\nDiversity A neuron can be multifaceted in that it responds strongly to multiple distinct types of stimuli, i.e. facets [29]. That is, higher-level features are more invariant to changes in the input [19,49]. For example, a face-detecting unit in CaffeNet [18] was found to respond to both human and lion faces [48]. Therefore, we wish to uncover different facets via AM in order to have a fuller understanding of a unit.\nHowever, AM optimization starting from different random images often converge to similar results [10,29]\u2014a phenomenon also observed when training neural networks with different initializations [20]. Researchers have proposed different techniques to improve image diversity such as:\n\u2013 Drop out certain neural paths in the network when performing backpropagation to produce different facets [46]. \u2013 Cluster the training set images into groups, and initialize from an average image computed from each group\u2019s images [29]. \u2013 Maximize the distance (e.g. cosine similarity in the pixel space) between a reference image and the one being synthesized [32]. \u2013 Activate two neurons at the same time e.g. activating (bird + apron) and (bird + candles) units would produce two distinct images of birds that activate the same bird unit [27] (Fig. 10). \u2013 Add noise to the image in every update to increase image diversity [26].\nWhile obtaining limited success, these methods also introduce extra hyperparameters and require further investigation. For example, if we enforce two stimuli to be different, exactly how far should they be and in which similarity metric should the difference be measured?"
    },
    {
      "heading": "3 Activation Maximization via Deep Generator Networks",
      "text": "Much previous AM research were optimizing the preferred stimuli directly in the high-dimensional image space where pixel-wise changes are often slow and uncorrelated, yielding high-frequency visualizations (Fig. 3b\u2013e). Instead, Nguyen et al. [27] propose to optimize in the low-dimensional latent space of a deep generator network, which they call Deep Generator Network Activation Maximization (DGN-AM). They train an image generator network to take in a highly compressed code and output a synthetic image that looks as close to real images from the ImageNet dataset [40] as possible. To produce an AM image for a given neuron, the authors optimize in the input latent space of the generator so that it outputs an image that activates the unit of interest (Fig. 4). Intuitively, DGN-AM restricts the search to only the set of images that can be drawn by the prior and encourages the image updates to be more coherent and correlated compared to pixel-wise changes (where each pixel is modified independently).\nGenerator networks We denote the sub-network of CaffeNet [18] that maps images onto 4096-D fc6 features as an encoder \ud835\udc38 : R\ud835\udc3b\u00d7\ud835\udc4a \u00d7\ud835\udc36 \u2192 R4096. We train a generator network \ud835\udc3a : R4096 \u2192 R\ud835\udc3b\u00d7\ud835\udc4a \u00d7\ud835\udc36 to invert \ud835\udc38 i.e. \ud835\udc3a(\ud835\udc38(x)) \u2248 x. In addition to the reconstruction losses, the generator was trained using the Generative Adversarial Network (GAN) loss [13] to improve the image realism. More training details are in [9,27]. Intuitively, \ud835\udc3a can be viewed as an artificial general \u201cpainter\u201d that is capable of painting a variety of different types of images, given an arbitrary input description (i.e. a latent code or a condition vector).\nThe idea is that \ud835\udc3a would be able to faithfully portray what a target network has learned, which may be recognizable or unrecognizable patterns to humans. Optimizing in the latent space Intuitively, we search in the input code space of the generator \ud835\udc3a to find a code h \u2208 R4096 such that the image \ud835\udc3a(h) maximizes the neural activation \ud835\udc4e(\ud835\udc3a(h)) (see Fig. 4). The AM problem in Eq. 3 now becomes:\nh* = arg max h (\ud835\udc4e(\ud835\udc3a(h)) \u2212 \ud835\udc45(h)) (6)\nThat is, we take steps in the latent space following the below update rule:\nh\ud835\udc61+1 = h\ud835\udc61 + \ud835\udf161 \ud835\udf15\ud835\udc4e(\ud835\udc3a(h\ud835\udc61))\n\ud835\udf15h\ud835\udc61 \u2212 \ud835\udf162 \ud835\udf15\ud835\udc45(h\ud835\udc61) \ud835\udf15h\ud835\udc61\n(7)\nNote that, here, the regularization term \ud835\udc45(.) is on the latent code h instead of the image x. Nguyen et al. [27] implemented a small amount of \ud835\udc3f2 regularization and also clipped the code. These hand-designed regularizers can be replaced by a strong, learned prior for the code [26].\nOptimizing in the latent space of a deep generator network showed a great improvement in image quality compared to previous methods that optimize in the pixel space (Fig. 5; and Fig. 3b\u2013h vs. Fig. 3i). However, images synthesized by DGN-AM have limited diversity\u2014they are qualitatively similar to the real top-9 validation images that highest activate a given unit (Fig. 6).\nFig.5: Images synthesized from scratch via DGN-AM method [27] to highly activate output neurons in the CaffeNet deep neural network [15], which has learned to classify 1000 categories of ImageNet images. Image from [27].\nTo improve the image diversity, Nguyen et al. [26] harnessed a learned realism prior for h via a denoising autoencoder (DAE), and added a small amount of Gaussian noise in every update step to improve image diversity [26]. In addition to an improvement in image diversity, this AM procedure also has a theoretical probabilistic justification, which is discussed in Section 4."
    },
    {
      "heading": "4 Probabilistic interpretation for Activation Maximization",
      "text": "In this section, we first make a note about the AM objective, and discuss a probabilistically interpretable formulation for AM, which is first proposed in Plug and Play Generative Networks (PPGNs) [26], and then interpret other AM methods under this framework. Intuitively, the AM process can be viewed as sampling from a generative model, which is composed of (1) an image prior and (2) a recognition network that we want to visualize.\n4.1 Synthesizing selective stimuli\nWe start with a discussion on AM objectives. In the original AM formulation (Eq. 1), we only explicitly maximize the activation \ud835\udc4e\ud835\udc59\ud835\udc56 of a unit indexed \ud835\udc56 in layer \ud835\udc59; however, in practice, this objective may surprisingly also increase the activations \ud835\udc4e\ud835\udc59\ud835\udc57 \u0338=\ud835\udc56 of some other units \ud835\udc57 in the same layer and even higher than \ud835\udc4e\ud835\udc59\ud835\udc56 [27]. For example, maximizing the output activation for the \u201chartebeest\u201d class is likely to yield an image that also strongly activates the \u201cimpala\u201d unit because these two animals are visually similar [27]. As the result, there is no guarantee that the target unit will be the highest activated across a layer. In that case, the resultant visualization may not portray what is unique about the target unit (\ud835\udc59, \ud835\udc56).\nFig.6: Side-by-side comparison between real and synthetic stimuli synthesized via DGN-AM [27]. For each unit, we show the top 9 validation set images that highest activate a given neuron (left) and 9 synthetic images (right). Note that these synthetic images are of size 227 \u00d7 227 i.e. the input size of CaffeNet [18]. Image from [27].\nInstead, we are interested in selective stimuli that highly activate only \ud835\udc4e\ud835\udc59\ud835\udc56, but not \ud835\udc4e\ud835\udc59\ud835\udc57 \u0338=\ud835\udc56. That is, we wish to maximize \ud835\udc4e\ud835\udc59\ud835\udc56 such that it is the highest single activation across the same layer \ud835\udc59. To enforce that selectivity, we can either maximize the softmax or log of softmax of the raw activations across a layer [26,42] where the softmax transformation for unit \ud835\udc56 across layer \ud835\udc59 is given as \ud835\udc60\ud835\udc59\ud835\udc56 = exp(\ud835\udc4e\ud835\udc59\ud835\udc56)/ \u2211\ufe00 \ud835\udc57 exp(\ud835\udc4e\ud835\udc59\ud835\udc57). Such selective stimuli (1) are more interpretable and preferred in neuroscience [3] because they contain only visual features exclusively for one unit of interest but not others; (2) naturally fit in our probabilistic interpretation discussed below.\n4.2 Probabilistic framework\nLet us assume a joint probability distribution \ud835\udc5d(x, \ud835\udc66) where x denotes images, and \ud835\udc66 is a categorical variable for a given neuron indexed \ud835\udc56 in layer \ud835\udc59. This model can be decomposed into an image density model and an image classifier model:\n\ud835\udc5d(x, \ud835\udc66) = \ud835\udc5d(x)\ud835\udc5d(\ud835\udc66|x) (8)\nNote that, when \ud835\udc59 is the output layer of an ImageNet 1000-way classifier [18], \ud835\udc66 also represents the image category (e.g. \u201cvolcano\u201d), and \ud835\udc5d(\ud835\udc66|x) is the classification probability distribution (often modeled via softmax).\nWe can construct a Metropolis-adjusted Langevin [38] (MALA) sampler for our \ud835\udc5d(x, \ud835\udc66) model [26]. This variant of MALA [26] does not have the accept/reject step, and uses the following transition operator:7\nx\ud835\udc61+1 = x\ud835\udc61 + \ud835\udf1612\u2207 log \ud835\udc5d(x\ud835\udc61, \ud835\udc66) + \ud835\udc41(0, \ud835\udf1623) (9)\nSince \ud835\udc66 is a categorical variable, and chosen to be a fixed neuron \ud835\udc66\ud835\udc50 outside the sampler, the above update rule can be re-written as:\nx\ud835\udc61+1 = x\ud835\udc61 +\ud835\udf1612\u2207 log \ud835\udc5d(\ud835\udc66 = \ud835\udc66\ud835\udc50|x\ud835\udc61)+\ud835\udf1612\u2207 log \ud835\udc5d(x\ud835\udc61)+\ud835\udc41(0, \ud835\udf1623) (10)\nDecoupling \ud835\udf1612 into explicit \ud835\udf161 and \ud835\udf162 multipliers, and expanding the \u2207 into explicit partial derivatives, we arrive at the following update rule:\nx\ud835\udc61+1 = x\ud835\udc61 + \ud835\udf161 \ud835\udf15 log \ud835\udc5d(\ud835\udc66 = \ud835\udc66\ud835\udc50|x\ud835\udc61)\n\ud835\udf15x\ud835\udc61 + \ud835\udf162 \ud835\udf15 log \ud835\udc5d(x\ud835\udc61) \ud835\udf15x\ud835\udc61 + \ud835\udc41(0, \ud835\udf1623) (11)\nAn intuitive interpretation of the roles of these three terms is illustrated in Fig. 7 and described as follows:\n\u2013 \ud835\udf161 term: take a step toward an image that causes the neuron \ud835\udc66\ud835\udc50 to be the highest activated across a layer (Fig. 7; red arrow) \u2013 \ud835\udf162 term: take a step toward a generic, realistic-looking image (Fig. 7; blue arrow). \u2013 \ud835\udf163 term: add a small amount of noise to jump around the search space to encourage image diversity (Fig. 7; green arrow).\nMaximizing raw activations vs. softmax Note that the \ud835\udf161 term in Eq. 11 is not the same as the gradient of raw activation term in Eq. 2. We summarize in Table 1 three variants of computing this \ud835\udf161 gradient term: (1) derivative of logits; (2) derivative of softmax; and (3) derivative of log of softmax. Several previous works empirically reported that maximizing raw, pre-softmax activations \ud835\udc4e\ud835\udc59\ud835\udc56 produces better visualizations than directly maximizing the softmax values \ud835\udc60\ud835\udc59\ud835\udc56 (Table 1a vs. b); however, this observation had not been fully justified [42]. Nguyen et al. [26] found the log of softmax gradient term (1) working well empirically; and (2) theoretically justifiable under the probabilistic framework in Section 4.2.\nWe refer readers to [26] for a more complete derivation and discussion of the above MALA sampler. Using the update rule in Eq. 11, we will next interpret other AM algorithms in the literature.\n4.3 Interpretation of previous algorithms\nHere, we consider four representative approaches in light of the probabilistic framework:\n7We abuse notation slightly in the interest of space and denote as \ud835\udc41(0, \ud835\udf1623) a sample from that distribution. The first step size is given as \ud835\udf1612 in anticipation of later splitting into separate \ud835\udf161 and \ud835\udf162 terms.\n1. AM with no priors [10,28,44] (discussed in Sec. 1) 2. AM with a Gaussian prior [42,46,48] (discussed in Sec. 2) 3. AM with hand-designed priors [21,29,31,42,46,48] (discussed in Sec. 2) 4. AM in the latent space of generator networks [26,27] (discussed in Sec. 3)\nActivation maximization with no priors. From Eq. 11, if we set (\ud835\udf161, \ud835\udf162, \ud835\udf163) = (1, 0, 0) , we obtain a sampler that follows the neuron gradient directly without\ncontributions from a \ud835\udc5d(x) term or the addition of noise. In a high-dimensional space, this results in adversarial or rubbish images [28,44] (as discussed in Sec. 2). We can also interpret the optimization procedure in [28,44] as a sampler with a non-zero \ud835\udf161 but with a \ud835\udc5d(x) such that \ud835\udf15 log \ud835\udc5d(x)\ud835\udf15x = 0 i.e. a uniform \ud835\udc5d(x) where all images are equally likely. Activation maximization with a Gaussian prior. To avoid producing high-frequency images [28] that are uninterpretable, several works have used \ud835\udc3f2 decay, which can be thought of as a simple zero-mean Gaussian prior over images [42,46,48]. From Eq. 11, if we define a Gaussian \ud835\udc5d(x) centered at the origin (assume the mean image has been subtracted) and set (\ud835\udf161, \ud835\udf162, \ud835\udf163) = (1, \ud835\udf06, 0), pulling Gaussian constants into \ud835\udf06, we obtain the following noiseless update rule:\nx\ud835\udc61+1 = (1 \u2212 \ud835\udf06)x\ud835\udc61 + \ud835\udf15 log \ud835\udc5d(\ud835\udc66 = \ud835\udc66\ud835\udc50|x\ud835\udc61)\n\ud835\udf15x\ud835\udc61 (12)\nThe first term decays the current image slightly toward the origin, as appropriate under a Gaussian image prior, and the second term pulls the image toward higher probability regions for the chosen neuron. Here, the second term is computed as the derivative of the log of a softmax transformation of all activations across a layer (see Table 1). Activation maximization with hand-designed priors. In an effort to outdo the simple Gaussian prior, many works have proposed more creative, handdesigned image priors such as Gaussian blur [48], total variation [21], jitter, rotate, scale [24], and data-driven patch priors [46]. These priors effectively serve as a simple \ud835\udc5d(x) component in Eq. 11. Note that all previous methods considered under this category are noiseless (\ud835\udf163 = 0). Activation maximization in the latent space of generator networks To ameliorate the problem of poor mixing in the high-dimensional pixel space [5], several works instead performed optimization in a semantically meaningful, lowdimensional feature space of a generator network [6,26,27,47,53].\nThat approach can be viewed as re-parameterizing \ud835\udc5d(x) as \u222b\ufe00\nh \ud835\udc5d(x|h)\ud835\udc5d(h), and sampling from the joint probability distribution \ud835\udc5d(h, \ud835\udc66) instead of \ud835\udc5d(x, \ud835\udc66), treating x as a deterministic variable. That is, the update rule in Eq. 11 is now changed into the below:\nh\ud835\udc61+1 = h\ud835\udc61 + \ud835\udf161 \ud835\udf15 log \ud835\udc5d(\ud835\udc66 = \ud835\udc66\ud835\udc50|h\ud835\udc61)\n\ud835\udf15h\ud835\udc61 + \ud835\udf162 \ud835\udf15 log \ud835\udc5d(h\ud835\udc61) \ud835\udf15h\ud835\udc61 + \ud835\udc41(0, \ud835\udf1623) (13)\nIn this category, DGN-AM [27] follows the above rule with (\ud835\udf161,\ud835\udf162,\ud835\udf163) = (1,1,0).8 Specifically, we hand-designed a \ud835\udc5d(h) via clipping and \ud835\udc3f2 regularization (i.e. a Gaussian prior) to keep the code h within a \u201crealistic\u201d range. PPGNs follows exactly the update rule in Eq. 13 with a better \ud835\udc5d(h) prior learned via a denoising autoencoder [26]. PPGNs produce images with better diversity than DGN-AM [26].\n8\ud835\udf163 = 0 because noise was not used in DGN-AM [27]."
    },
    {
      "heading": "5 Applications of Activation Maximization",
      "text": "In this section, we review how one may use activation maximization to understand and explain a pre-trained neural network. The results below are specifically generated by DGN-AM [27] and PPGNs [26] where the authors harnessed a general image generator network to synthesize AM images.\nVisualize output units for new tasks We can harness a general learned ImageNet prior to synthesize images for networks trained on a different dataset e.g. MIT Places dataset [50] or UCF-101 activity videos [27] (Figs. 5 & 8).\nVisualize hidden units Instead of synthesizing preferred inputs for output neurons (Fig. 5), one may apply AM to the hidden units. In a comparison with visualizing real image regions that highly activate a unit [50], we found AM images may provide similar but sometimes also complementary evidence suggesting what a unit is for [27] (Fig. 9). For example, via DGN-AM, we found that a unit that detects \u201cTV screens\u201d also detects people on TV (Fig. 9, unit 106).\nSynthesize preferred images activating multiple neurons First, one may synthesize images activating a group of units at the same time to study the interaction between them [27,32]. For example, it might be useful to study how a network distinguishes two related and visually similar concepts such as \u201cimpala\u201d and \u201chartebeest\u201d animals in ImageNet [7]. One way to do this is to synthesize images that maximize the \u201cimpala\u201d neuron\u2019s activation but also minimize the \u201chartebeest\u201d neuron\u2019s activation. Second, one may reveal different facets of a neuron [29] by activating different pairs of units. That is, activating two units at the same time e.g. (castle + candle); and (piano + candle) would produce two distinct images of candles that activate the same \u201ccandle\u201d unit [27] (Fig. 10). In addition, this method sometimes also produces interesting, creative art [12,27].\nWatch feature evolution during training We can watch how the features evolved during the training of a target classifier network [27]. Example videos of AM visualizations for sample output and hidden neurons during the training of CaffeNet [15] are at: https://www.youtube.com/watch?v=q4yIwiYH6FQ and https://www.youtube.com/watch?v=G8AtatM1Sts. One may find that features at lower layers tend to converge faster vs. those at higher layers.\nSynthesizing videos To gain insights into the inner functions of an activity recognition network [43], one can synthesize a single frame (Fig. 8; right) or an entire preferred video. By synthesizing videos, Nguyen et al. [27] found that a video recognition network (LRCN [8]) classifies videos without paying attention to temporal correlation across video frames. That is, the AM videos9 appear to be a set of uncorrelated frames of activity e.g. a basketball game. Further tests\n9https://www.youtube.com/watch?v=IOYnIK6N5Bg\nconfirmed that the network produces similar top-1 predicted labels regardless of whether the frames of the original UCF-101 videos [43] are randomly shuffled.\nActivation maximization as a debugging tool We discuss here a case study where AM can be used as a debugging tool. Suppose there is a bug in your neural network image classifier implementation that internally and unexpectedly converts all input RGB images (Fig. 11a) into BRG images (Fig. 11b) before feeding them to the neural network. This bug might be hard to notice by only examining accuracy scores or attribution heatmaps [23]. Instead, AM visualizations could reflect the color space of the images that were fed to the neural network and reveal this bug (Fig. 11c).\nSynthesize preferred images conditioned on a sentence Instead of synthesizing images preferred by output units in an image classifier, we can also synthesize images that cause an image captioning network to output a desired sentence (examples in Fig. 12).\nThis reverse-engineering process may uncover interesting insights into the system\u2019s behaviors. For example, we discovered an interesting failure of a state-\nof-the-art image captioner [8] when it declares birds even when there is no bird in an image (Fig. 13).\nSynthesize preferred images conditioned on a semantic segmentation map We can extend AM methods to synthesize images with more fine-grained controls of where objects are placed by matching a semantic map output of a segmentation network (Fig. 14) or a target spatial feature map of a convolutional layer.\nSynthesize preferred stimuli for real, biological brains While this survey aims at visualizing artificial networks, it is also possible to harness our AM techniques to study biological brains. Two teams of Neuroscientists [22,36] have recently been able to reconstruct stimuli for neurons in alive macaques\u2019 brains using either the ImageNet PPGN (as discussed in Sec. 4) [22] or the DGN-AM (as discussed in Sec. 3) [36]. The synthesized images surprisingly resemble monkeys and human nurses that the subject macaque meets frequently [36] or show eyes in neurons previously shown to be tuned for detecting faces [22]. Similar AM frameworks have also been interestingly applied to reconstruct stimuli from EEG or MRI signals of human brains [34,41]."
    },
    {
      "heading": "6 Discussion and Conclusion",
      "text": "While activation maximization has proven a useful tool for understanding neural networks, there are still open challenges and opportunities such as:\n\u2013 One might wish to harness AM to compare and contrast the features learned by different models. That would require a robust, principled AM approach that produces faithful and interpretable visualizations of the learned features for networks trained on different datasets or of different architectures. This is challenging due to two problems: (1) the image prior may not be general enough and may have a bias toward a target network or one dataset over the others; (2) AM optimization on different network architectures, especially of different depths, often requires different hyper-parameter settings to obtain the best performance.\n\u2013 It is important for the community to propose rigorous approaches for evaluating AM methods. A powerful image prior may incur a higher risk of producing misleading visualizations\u2014it is unclear whether a synthesized visual feature comes from the image prior or the target network being studied or both. Note that we have investigated that and surprisingly found the DGN-AM prior to be able to generate a wide diversity of images including the non-realistic ones (e.g. blurry, cut-up, and BRG images [27]).\n\u2013 Concepts in modern deep networks can be highly distributed [4, 11, 44]; therefore, it might be promising to apply AM to study networks at a different, larger scale than individual neurons, e.g. looking at groups of neurons [33].\n\u2013 It might be a fruitful direction to combine AM with other tools such as attribution heatmapping [33] or integrate AM into the testbeds for AI applications [35] as we move towards safe, transparent, and fair AI.\n\u2013 One may also perform AM in the parameter space of a 3D renderer (e.g. modifying the lighting, object geometry or appearances in a 3D scene) that renders a 2D image that strongly activates a unit [2]. AM in a 3D space allows us to synthesize stimuli by varying a controlled factor (e.g. lighting) and thus might offer deeper insights into a model\u2019s inner-workings.\nActivation maximization techniques enable us to shine light into the blackbox neural networks. As this survey shows, improving activation maximization techniques improves our ability to understand deep neural networks. We are excited for what the future holds regarding improved techniques that make neural networks more interpretable and less opaque so we can better understand how deep neural networks do the amazing things that they do.\nAcknowledgments Anh Nguyen is supported by Amazon Research Credits, Auburn University, and donations from Adobe Systems Inc and Nvidia."
    }
  ],
  "title": "Understanding Neural Networks via Feature Visualization: A survey",
  "year": 2019
}
