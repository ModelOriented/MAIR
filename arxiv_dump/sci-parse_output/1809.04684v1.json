{"abstractText": "\u008ce \u0080nancial services industry has unique explainability and fairness challenges arising from compliance and ethical considerations in credit decisioning. \u008cese challenges complicate the use of model machine learning and arti\u0080cial intelligence methods in business decision processes.", "authors": [{"affiliations": [], "name": "Jiahao Chen"}], "id": "SP:1c54796185d4923c53b72a17bfcc5227be904277", "references": [{"authors": ["Michael Aleo", "Pablo Svirsky"], "title": "Foreclosure Fallout: the banking industry\u2019s a\u008aack on disparate impact race discrimination claims under the Fair Housing Act and the Equal Credit Opportunity Act", "venue": "Public Law Interest Journal 18,", "year": 2008}, {"authors": ["Sheila D Ards", "Samuel L Myers"], "title": "\u008ce Color of Money: Bad Credit, Wealth, and Race", "venue": "American Behavioral Scientist", "year": 2001}, {"authors": ["Robert B. Avery", "Kenneth P. Brevoort", "Glenn Canner"], "title": "Does Credit Scoring Produce a Disparate Impact", "venue": "Real Estate Economics 40,", "year": 2012}, {"authors": ["Rmi Bardenet", "Mtys Brendel", "Balzs Kgl", "Michle Sebag"], "title": "Collaborative Hyperparameter Tuning", "venue": "In Proceedings of the 30th International Conference on International Conference on Machine Learning - Volume 28 (ICML\u201913). JMLR.org,", "year": 2013}, {"authors": ["Solon Barocas", "Andrew Selbst"], "title": "Big Data\u2019s Disparate Impact", "venue": "California Law Review 104,", "year": 2016}, {"authors": ["Patrick Bayer", "Fernando Ferreira", "Stephen L. Ross"], "title": "What Drives Racial and Ethnic Di\u0082erences in High-Cost Mortgages? \u008ce Role of High-Risk Lenders", "venue": "\u008ae Review of Financial Studies 31,", "year": 2018}, {"authors": ["Sbastien Bubeck", "Nicol Cesa-Bianchi"], "title": "Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit Problems", "venue": "Foundations and Trends in Machine Learning 5,", "year": 2012}, {"authors": ["Jenna Burrell"], "title": "How the machine \u2018thinks\u2019: Understanding opacity in machine learning algorithms", "venue": "Big Data & Society", "year": 2016}, {"authors": ["Tammy Butler"], "title": "Is your \u201dtarget marketing\u201d breaking fair lending laws? h\u008aps://fairlendingdiversity.com/ target-marketing-breaking-fair-lending-laws", "year": 2016}, {"authors": ["Jonathan Chang", "Jordan Boyd-Graber", "Sean Gerrish", "Chong Wang", "David M. Blei"], "title": "Reading Tea Leaves: How Humans Interpret Topic Models", "venue": "In Proceedings of the 22nd International Conference on Neural Information Processing Systems", "year": 2009}, {"authors": ["Roger Clarke"], "title": "Big data, big risks", "venue": "Information Systems Journal 26,", "year": 2016}, {"authors": ["Peter N. Cubita", "Michelle Hartmann"], "title": "\u008ce ECOA Discrimination Proscription and Disparate Impact\u2014Interpreting the Meaning of the Words \u008cat Actually Are \u008cere", "venue": "\u008ae Business Lawyer 61,", "year": 2006}, {"authors": ["A. Da\u008aa", "S. Sen", "Y. Zick"], "title": "Algorithmic Transparency via \u008bantitative Input In\u0083uence: \u008ceory and Experiments with Learning Systems", "venue": "In 2016 IEEE Symposium on Security and Privacy (SP). 598\u2013617", "year": 2016}, {"authors": ["Marc N. Ellio", "Peter A. Morrison", "Allen Fremont", "Daniel F. McCa\u0082rey", "Philip Pantoja", "Nicole Lurie"], "title": "Using the Census Bureau\u2019s surname list to improve estimates of race/ethnicity and associated disparities", "venue": "Health Services and Outcomes Research Methodology", "year": 2009}, {"authors": ["Manuel Fernndez-Delgado", "Eva Cernadas", "Senn Barro", "Dinani Amorim"], "title": "Do we Need Hundreds of Classi\u0080ers to Solve Real World Classi\u0080cation Problems", "venue": "Journal of Machine Learning Research", "year": 2014}, {"authors": ["Ma\u008ahias Feurer", "Jost Tobias Springenberg", "Frank Hu\u008aer"], "title": "Initializing Bayesian Hyperparameter Optimization via Meta-learning", "venue": "In Proceedings of the Twenty-Ninth AAAI Conference on Arti\u0080cial Intelligence (AAAI\u201915)", "year": 2015}, {"authors": ["Andrea Freeman"], "title": "Payback: A structural analysis of the credit card problem", "venue": "Arizona Law Review", "year": 2013}, {"authors": ["Andrea Freeman"], "title": "Racism in the Credit Card Industry", "venue": "North Carolina Law Review 95,", "year": 2017}, {"authors": ["Mikella Hurley", "Julius Adebayo"], "title": "Credit Scoring in the Era of Big Data", "venue": "Yale Journal of Law & Technology", "year": 2016}, {"authors": ["Beatriz Ibarra", "Eric Rodriguez"], "title": "Latino Credit Card Use: Debt Trap or Ticket to Prosperity", "venue": "National Council of La Raza Issue Brief", "year": 2007}, {"authors": ["Leslie P Kaebling", "Michael L Li\u008aman", "Andrew W Moore"], "title": "Reinforcement Learning: A Survey", "venue": "Journal of Arti\u0080cial Intelligence Research", "year": 1996}, {"authors": ["Fiscella Kevin", "Fremont Allen M"], "title": "Use of Geocoding and Surname Analysis to Estimate Race and Ethnicity", "venue": "Health Services Research 41,", "year": 2006}, {"authors": ["James Rufus Koren"], "title": "Feds use Rand formula to spot discrimination. \u008ce GOP calls it junk science", "venue": "Los Angeles Times", "year": 2016}, {"authors": ["Ron Lieber"], "title": "American Express Kept a (Very) Watchful Eye on Charges", "venue": "\u008ae New York Times (2009-01),", "year": 2009}, {"authors": ["Sabine Mayser"], "title": "Perceived Fairness of Di\u0082erential Customer Treatment", "year": 2011}, {"authors": ["Sabine Mayser", "Florian von Wangenheim"], "title": "Perceived Fairness of Differential Customer Treatment: Consumers\u2019 Understanding of Distributive Justice Really Ma\u008aers", "venue": "Journal of Service Research 16,", "year": 2013}, {"authors": ["Kevin M McDonald"], "title": "Who\u2019s policing the \u0080nancial cop on the beat? A call for judicial review of the Consumer FInancial Protection Bureau\u2019s nonlegislative rules", "venue": "Review of Banking & Financial Law 35,", "year": 2015}, {"authors": ["Volodymyr Mnih", "Koray Kavukcuoglu", "David Silver", "Andrei A. Rusu", "Joel Veness", "Marc G. Bellemare", "Alex Graves", "Martin Riedmiller", "Andreas K. Fidjeland", "Georg Ostrovski", "Stig Petersen", "Charles Bea\u008aie", "Amir Sadik", "Ioannis Antonoglou", "Helen King", "Dharshan Kumaran", "Daan Wierstra", "Shane Legg", "Demis Hassabis"], "title": "Human-level control through deep reinforcement learning", "venue": "Nature", "year": 2015}, {"authors": ["Bang Nguyen", "Philipp Phil Klaus"], "title": "Retail fairness: Exploring consumer perceptions of fairness towards retailers\u2019 marketing tactics", "venue": "Journal of Retailing and Consumer Services 20,", "year": 2013}, {"authors": ["Cathy O\u2019Neill"], "title": "Weapons of Math Destruction: how big data increases inequality and threatens democracy. Crown. h\u008aps://weaponsofmathdestructionbook", "year": 2016}, {"authors": ["Bernhard Pfahringer", "Hilan Bensusan", "Christophe G. Giraud-Carrier"], "title": "Meta-Learning by Landmarking Various Learning Algorithms", "venue": "In Proceedings of the Seventeenth International Conference on Machine Learning (ICML", "year": 2000}, {"authors": ["Marco Tulio Ribeiro", "Sameer Singh", "Carlos Guestrin"], "title": "Why Should I Trust You?\u201d: Explaining the Predictions of Any Classi\u0080er", "venue": "In KDD \u201916 Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM,", "year": 2016}, {"authors": ["John L. Ropiequet", "Christopher S. Naveja", "L. Jean Noonan"], "title": "Fair Lending Developments: Is Disparate Impact Here to Stay? \u008ae", "venue": "Business Lawyer 69,", "year": 2014}, {"authors": ["Richard Rothstein"], "title": "\u008ae Color of Law: A Forgo\u0088en History of How Our Government Segregated America. Liveright", "year": 2018}, {"authors": ["George Rutherglen"], "title": "Disparate Impact under Title VII: An Objective \u008ceory of Discrimination", "venue": "Virginia Law Review 73,", "year": 1987}, {"authors": ["Emily Steel", "Julia Angwin"], "title": "On the Web\u2019s Cu\u008aing Edge, Anonymity in Name Only", "venue": "Wall Street Journal", "year": 2010}, {"authors": ["Yolanda P Strader"], "title": "CFPB Continues Crackdown on Fair Lending: Marketing Materials Targeted", "year": 2015}, {"authors": ["Ryan Turner"], "title": "A model explanation system", "venue": "IEEE 26th International Workshop on Machine Learning for Signal Processing (MLSP)", "year": 2016}, {"authors": ["US Congress"], "title": "2003-12", "venue": "P. L. 108-159: Fair and Accurate Credit Transactions Act of", "year": 2003}, {"authors": ["US Congress"], "title": "2017-03", "venue": "Fair and Equal Housing Act of", "year": 2017}, {"authors": ["Joann\u00e8s Vermorel", "Mehryar Mohri"], "title": "Multi-armed Bandit Algorithms and Empirical Evaluation", "venue": "In Machine Learning: ECML", "year": 2005}, {"authors": ["Yanhao Wei", "Pinar Yildirim", "Christophe Van den Bulte", "Chrysanthos Dellarocas"], "title": "Credit Scoring with Social Network Data", "venue": "Marketing Science 35,", "year": 2016}, {"authors": ["Yan Zhang"], "title": "Assessing Fair Lending Risks Using Race/Ethnicity Proxies", "venue": "Management Science 64,", "year": 2016}, {"authors": ["Michael J Zimmer"], "title": "\u008ce Emerging Uniform Structure of Disparate Treatment Discrimination Litigation", "venue": "Georgia Law Review", "year": 1996}], "sections": [{"heading": "KEYWORDS", "text": "fair lending, credit decisioning, banking, nancial services, disparate impact, racial discrimination, adverse action, credit o ers, credit cards"}, {"heading": "ACM Reference format:", "text": "Jiahao Chen. 2018. Fair lending needs explainable models for responsible recommendation. In Proceedings of Workshop on Responsible Recommendation, Vancouver, Canada, October 6, 2018 (FATREC\u201918), 4 pages. DOI: 10.1145/nnnnnnn.nnnnnnn\nFinancial services companies in the USA make highly regulated business decisions that complicate the use of recommender systems and other forms of arti cial intelligence. Key business processes such as determining who quali es for lines of credit (personal loans, credit cards, mortgages, etc.) must be shown to comply with fair lending laws such as the Equal Credit Opportunity Act (ECOA) [54], Fair Credit Reporting Act (FCRA) [53], Fair and Accurate Credit Transactions Act (FACTA) [55], Fair Housing Act (FHA) [51], Fair and Equal Housing Act (FEHA) [56], and Consumer Credit Protection Act (CCPA) [52]. Some of these laws de ne protected classes (see Table 1), for which discrimination on the basis of a customer\u2019s membership in those classes is prohibited.\nUnder these fair lending laws, lenders must demonstrate that their business decisions do not discriminate. However, there are multiple notions of discrimination in fair lending. e two major theories of discrimination are:\ndisparate treatment [61], informally, intentionally treating people di erently on the basis on a protected class, and disparate impact [47], informally, discriminating against any protected class as a resulting from implementing of a facially neutral policy.\nDi erent theories of discrimination may apply for di erent laws. Disparate impact as a theory of discrimination under FHA has been con rmed by the Supreme Court; however, there is some\nFATREC\u201918, Vancouver, Canada 2018. 978-x-xxxx-xxxx-x/YY/MM. . .$15.00 DOI: 10.1145/nnnnnnn.nnnnnnn"}, {"heading": "Opportunity Act (ECOA) [54].", "text": "uncertainty as to whether disparate impact is a proper theory of discrimination under ECOA [3, 20, 37, 45].1\nis position paper summarizes some of the main compliance, explainability and fairness considerations arising out of regulated business decisioning in the nancial services industry that pose unique challenges for using recommender systems.\nDisparate impact considerations constrain the use of features that correlate strongly with protected classes. e need to demonstrate lack of disparate impact means that a model for credit risk has to avoid features like zip code, which is highly correlated with race [32], a protected class. Using zip code in a model therefore runs the risk of redlining, the denial of services in neighborhoods populated mainly by racial minorities [13, 18, 40]. Other variables that may be predictive of credit risk, such as length of credit history, correlate with age of customer, another prohibited class [3], and may require remediation in automated scoring systems [18]. Even seemingly innocuous policies like a minimum principal amount for a loan [44] may introduce bias against one or more protected classes [1, 41, 48]. Disparate impact considerations also complicate the use of nontraditional data sources such as social network data, which re ect and exacerbate deeply rooted societal inequalities [6, 29, 59].\n1While several courts and regulators believe that disparate impact should be considered under ECOA [18, 40], the Bureau of Consumer Financial Protection (CFPB) has signaled that it may issue guidance in the near future that will limit the use of disparate impact claims under ECOA [19].\nar X\niv :1\n80 9.\n04 68\n4v 1\n[ cs\n.L G\n] 1\n2 Se\np 20\n18\nSimilar considerations apply for models that are used in direct marketing to pre-screened potential customers [10, 49], where credit bureau data are used to generate lists of consumers that qualify for a credit card o er. According to FCRA, each marketing o er to a prescreened customer is a rm o er of credit; all a customer needs to do is accept the o er to obtain credit [53]. erefore, marketing campaigns for credit have compliance considerations similar to credit decisioning models, on top of any issues around the consumers\u2019 perceptions of fairness [35, 36, 39]. Any arti cial intelligence or machine learning models used for these purposes, including recommender systems, must therefore be capable of assessment for fair lending considerations.\nAssessing discrimination poses unique challenges when customers\u2019 memberships in protected classes are unknown. Credit card companies do not generally collect information about an applicant\u2019s race, but may have a compliance need to demonstrate the lack of disparate impact with regard to race. Regulators like the Consumer Financial Protection Bureau (CFPB) have published assessment methodologies that describe the use of proxy models to impute race labels [17, 23]. However, the resulting assessment seems to overestimate the amount of disparate impact [4, 60] and has resulted in some controversy [4, 14\u201316, 19, 33]. To the best of our knowledge, assessing disparate impact in the absence of known labels is an area that has yet to be discussed in the fair, accountable and transparent machine learning community.\nFair lending laws require credit decisions to be explainable. An adverse action notice is required by ECOA and FCRA if a customer is denied credit based on information in a credit report [53, 54]. According to the o cial interpretation of the law, such a notice must provide speci c reasons for denying credit [22, \u00b69(b)(2)]. erefore, AI/ML systems used to extend credit must also be able to provide the explanations necessary for adverse action.\nHaving explainable processes is important not only for determining regulatory compliance, but also to \u201cdebug\u201d and redress errors in business practice, such as data quality errors in credit bureaux and other data sources [57]. Having \u201cdebuggable\u201d credit lending practices is even more desirable when considering that 5% of Americans have errors in their credit reports that adversely a ect their creditworthiness [24]. Accounting for such data quality errors has added moral and ethical dimensions [7, 12], particularly when noting that the presence of such issues is itself correlated with protected classes, such as race, due to complex socioeconomic factors [2, 27, 28, 46]. For example, Latinos/Hispanics are more likely than whites and African-Americans to have no credit history [30]. Remediating such errors and omissions can break vicious cycles for people with bad credit due to erroneous credit reports, and for people who have no credit and hence no credit history [41]. Debuggability can also help businesses quickly identify problematic features that have the potential to generate customer dissatisfaction [34].\nCan complex models be explained and proven to be fair on a caseby-case basis? e desire for explainability appears to clash with the desire to use complex models that may provide improved li or classi cation accuracy [25, 50]. is kind of complexity has sometimes been termed \u201copacity at scale\u201d [9]. Multi-armed bandits\n[8, 58] and other forms of reinforcement learning [31, 38] pose further explainability challenges, due to the statefulness resulting from multiple rounds of learning. However, recent work on automatic model explanations [43, 50] suggest that it is possible to generate relatively short explanations for the predictions produced for any particular set of inputs and outputs. Inspired by these investigations, we propose to study how to generate explanations that are rated favorably by human subjects. Possible avenues of study are stated below:\nHypothesis: di erent audiences require di erent explanations. Explainability cannot exist as a quality purely independent of a target audience. A complete description of a business process that employs machine learning models requires mastery of the language of the business context in addition to understanding the jargon and nuance surrounding statistical modeling. Only an expert data scientist who is intimately familiar with the business has a reasonable chance of understanding the full operational de nition of an abstract concept like creditworthiness. Consequently, explanations that are satisfactory to other audiences such as business executives, data scientists in other industries or academia, lawmakers and regulators, and customers, require di erent levels of distillation to convey appropriate levels of explanation that use vocabularies and phrasing comprehensible to the target audiences. We hypothesize that satisfaction with explanations will be at best moderately correlated when the same explanation is presented to domain experts and to the lay public.\nHypothesis: directly measurable features are more explainable. We argue that features that are directly measurable, such as a cuto based on a customer\u2019s annual salary, are more intuitive and hence convey more explanatory power than a feature that mixes many such features. Examples of the la er include being in the highest risk twentile predicted by another model, or the largest principal component built from all credit bureau information, or even a landmark feature produced by running a fast (if less accurate) machine learning algorithm [42]. We posit that landmark features and principal components, while popular as meta-features for meta-learning [5, 26], are less intuitive and hence more di cult to use in satisfactory explanations. us locality in the space of directly measured features is an important factor that explains explainability.\nHypothesis: explanations from complex models are less satisfactory than explanations from local approximants. Building upon the idea that locality in feature space promotes explainability, we also speculate that explanations built from the output of local approximants, such as Local Interpretable Model-Agnostic Explanations (LIME) [43] or antitative Input In uence (QII) [21], will be rated as more explainable than explanations built directly upon the result of a complex model such as a neural network.\nHypothesis: human ratings of satisfaction with explanations may di er from metrics based on statistical quality. We argue that the relationship between customer satisfaction with explanations and metrics proposed in the literature, such as Turner\u2019s model quality score [50], is well worth investigating. We already know that in other machine learning disciplines such as topic modeling that human evaluations can behave in unexpected ways: in topic modeling, human ratings of topic quality are negatively correlated with\nstatistical likelihood measures [11]. Furthermore, given our assertion above that explainability cannot be independent of the target audience, we expect that general customers will demand di erent level of explanations from domain experts."}, {"heading": "ACKNOWLEDGMENTS", "text": "I would like to thank Andrew Buemi, Carla Greenberg, Brian Larkin, and Brian Mills for their careful reading of the manuscript and their helpful suggestions and comments."}, {"heading": "DISCLAIMER", "text": "e author of this paper is not a lawyer. is paper does not constitute legal advice. e positions herein are presented for the purpose of academic research discussions, and do not necessarily re ect the views of Capital One."}], "title": "Fair lending needs explainable models for responsible recommendation", "year": 2018}