{
  "abstractText": "Establishing unique identities for both humans and end systems has been an active research problem in the security community, giving rise to innovative machine learning-based authentication techniques. Although such techniques offer an automated method to establish identity, they have not been vetted against sophisticated attacks that target their core machine learning technique. This paper demonstrates that mimicking the unique signatures generated by host fingerprinting and biometric authentication systems is possible. We expose the ineffectiveness of underlying machine learning classification models by constructing a blind attack based around the query synthesis framework and utilizing Explainable\u2013AI (XAI) techniques. We launch an attack in under 130 queries on a state-of-the-art face authentication system, and under 100 queries on a host authentication system. We examine how these attacks can be defended against and explore their limitations. XAI provides an effective means for adversaries to infer decision boundaries and provides a new way forward in constructing attacks against systems using machine learning models for authentication.",
  "authors": [
    {
      "affiliations": [],
      "name": "Washington Garcia"
    },
    {
      "affiliations": [],
      "name": "Joseph I. Choi"
    },
    {
      "affiliations": [],
      "name": "Suman K. Adari"
    }
  ],
  "id": "SP:44c0b2129b43141f36956be137f2321a7630e36f",
  "references": [
    {
      "authors": [
        "D. Angluin"
      ],
      "title": "Queries and Concept Learning",
      "venue": "Machine Learning, 2(4):319\u2013342,",
      "year": 1988
    },
    {
      "authors": [
        "M. Barreno",
        "B. Nelson",
        "A.D. Joseph",
        "J.D. Tygar"
      ],
      "title": "The security of machine learning",
      "venue": "Machine Learning, 81(2):121\u2013148, Nov",
      "year": 2010
    },
    {
      "authors": [
        "M. Barreno",
        "B. Nelson",
        "R. Sears",
        "A.D. Joseph",
        "J.D. Tygar"
      ],
      "title": "Can Machine Learning be Secure",
      "venue": "In Proceedings of the ACM Symposium on Information, Computer and Communications Security (ASIACCS),",
      "year": 2006
    },
    {
      "authors": [
        "A.M. Bates",
        "R. Leonard",
        "H. Pruse",
        "D. Lowd",
        "K.R. Butler"
      ],
      "title": "Leveraging USB to Establish Host Identity Using Commodity Devices",
      "venue": "Proceedings of the Network and Distributed System Security (NDSS) Symposium, pages 23\u201326,",
      "year": 2014
    },
    {
      "authors": [
        "B. Biggio",
        "G. Fumera",
        "F. Roli"
      ],
      "title": "Security Evaluation of Pattern Classifiers under Attack",
      "venue": "IEEE Transactions on Knowledge and Data Engineering (TKDE), 26(4):984\u2013996, April",
      "year": 2014
    },
    {
      "authors": [
        "J. Bigun",
        "J. Fierrez-Aguilar",
        "J. Ortega-Garcia",
        "J. Gonzalez-Rodriguez"
      ],
      "title": "Combining Biometric Evidence for Person Authentication",
      "venue": "Advanced Studies in Biometrics, (May 2014):1\u201318,",
      "year": 2005
    },
    {
      "authors": [
        "S. Bratus",
        "C. Cornelius",
        "D. Kotz",
        "D. Peebles"
      ],
      "title": "Active behavioral fingerprinting of wireless devices",
      "venue": "Proceedings of the First ACM Conference on Wireless Network Security (WiSec),",
      "year": 2008
    },
    {
      "authors": [
        "L. Breiman"
      ],
      "title": "Random Forests",
      "venue": "Machine Learning, 45(1):5\u201332,",
      "year": 2001
    },
    {
      "authors": [
        "A.K. Dalai",
        "S.K. Jena"
      ],
      "title": "WDTF: A Technique for Wireless Device Type Fingerprinting",
      "venue": "Wireless Personal Communications, 97,",
      "year": 2017
    },
    {
      "authors": [
        "N. Dalvi",
        "P. Domingos",
        "Mausam",
        "S. Sanghai",
        "D. Verma"
      ],
      "title": "Adversarial Classification",
      "venue": "In Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD),",
      "year": 2004
    },
    {
      "authors": [
        "David Sandberg",
        "others"
      ],
      "title": "Face Recognition using Tensorflow",
      "venue": "https://github.com/ davidsandberg/facenet,",
      "year": 2016
    },
    {
      "authors": [
        "A. De Luca",
        "A. Hang",
        "F. Brudy",
        "C. Lindner",
        "H. Hussmann"
      ],
      "title": "Touch me once and I know it\u2019s you!: Implicit Authentication based on Touch Screen Patterns",
      "venue": "Proceedings of the ACM SIGCHI Conference on Human Factors in Computing Systems (CHI),",
      "year": 2012
    },
    {
      "authors": [
        "M. Fredrikson",
        "S. Jha",
        "T. Ristenpart"
      ],
      "title": "Model Inversion Attacks That Exploit Confidence Information and Basic Countermeasures",
      "venue": "Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security (CCS),",
      "year": 2015
    },
    {
      "authors": [
        "J. Galbally",
        "C. McCool",
        "J. Fierrez",
        "S. Marcel",
        "J. Ortega-Garcia"
      ],
      "title": "On the Vulnerability of Face Verification Systems to Hill-Climbing Attacks",
      "venue": "Pattern Recognition, 43:1027\u20131038, Mar.",
      "year": 2010
    },
    {
      "authors": [
        "M. Gomez-Barrero",
        "J. Galbally",
        "J. Fierrez",
        "J. Ortega-Garcia"
      ],
      "title": "Face Verification Put to Test: A Hill-Climbing Attack Based on the Uphill-Simplex Algorithm",
      "venue": "Proceedings of the 5th IAPR International Conference on Biometrics (ICB),",
      "year": 2012
    },
    {
      "authors": [
        "I. Goodfellow",
        "J. Shlens",
        "C. Szegedy"
      ],
      "title": "Explaining and Harnessing Adversarial Examples",
      "venue": "Proceedings of the International Conference on Learning Representations (ICLR),",
      "year": 2015
    },
    {
      "authors": [
        "G. Goswami",
        "N. Ratha",
        "A. Agarwal",
        "R. Singh",
        "M. Vatsa"
      ],
      "title": "Unravelling Robustness of Deep Learning based Face Recognition Against Adversarial Attacks",
      "venue": "Proceedings of the AAAI Conference on Artificial Intelligence (AAAI),",
      "year": 2018
    },
    {
      "authors": [
        "W. Guo",
        "D. Mu",
        "J. Xu",
        "P. Su",
        "G. Wang",
        "X. Xing"
      ],
      "title": "LEMNA: Explaining Deep Learning based Security Applications",
      "venue": "Proceedings of the 25th ACM Conference on Computer and Communications Security (CCS\u201918),",
      "year": 2018
    },
    {
      "authors": [
        "Y. Guo",
        "L. Zhang",
        "Y. Hu",
        "X. He",
        "J. Gao"
      ],
      "title": "MS-Celeb-1M: Challenge of Recognizing One Million Celebrities in the Real World, 2016",
      "year": 2016
    },
    {
      "authors": [
        "L. Hong",
        "A.K. Jain"
      ],
      "title": "Integrating Faces and Fingerprints for Personal Identification",
      "venue": "Proceedings of the Third Asian Conference on Computer Vision (ACCV),",
      "year": 1998
    },
    {
      "authors": [
        "L. Huang",
        "A.D. Joseph",
        "B. Nelson",
        "B.I. Rubinstein",
        "J.D. Tygar"
      ],
      "title": "Adversarial Machine Learning",
      "venue": "Proceedings of the 4th ACM Workshop on Artificial Intelligence and Security 21 (AISec),",
      "year": 2011
    },
    {
      "authors": [
        "M. Jakobsson",
        "E. Shi",
        "P. Golle",
        "R. Chow"
      ],
      "title": "Implicit Authentication for Mobile Devices",
      "venue": "Proceedings of the 4th USENIX conference on Hot Topics in Security (HotSec),",
      "year": 2009
    },
    {
      "authors": [
        "T. Kohno",
        "A. Broido",
        "K.C. Claffy"
      ],
      "title": "Remote Physical Device Fingerprinting",
      "venue": "IEEE Transactions on Dependable and Secure Computing (TDSC), 2(2):93\u2013108, Apr.",
      "year": 2005
    },
    {
      "authors": [
        "J.J. Lim",
        "R. Salakhutdinov",
        "A. Torralba"
      ],
      "title": "Transfer Learning by Borrowing Examples for Multiclass Object Detection",
      "venue": "Proceedings of the 25th Annual Conference on Neural Information Processing Systems (NIPS),",
      "year": 2011
    },
    {
      "authors": [
        "C. Lu",
        "X. Tang"
      ],
      "title": "Surpassing Human-Level Face Verification Performance on LFW with GaussianFace",
      "venue": "Proceedings of the 29th AAAI Conference on Artificial Intelligence (AAAI),",
      "year": 2015
    },
    {
      "authors": [
        "D. Martens",
        "B. Baesens",
        "T. Van Gestel",
        "J. Vanthienen"
      ],
      "title": "Comprehensible credit scoring models using rule extraction from support vector machines",
      "venue": "European Journal of Operational Research (EJOR), 183(13):1466\u20131476,",
      "year": 2007
    },
    {
      "authors": [
        "S. Mondal",
        "P. Bours"
      ],
      "title": "A study on continuous authentication using a combination of keystroke and mouse biometrics",
      "venue": "Neurocomputing, 230:1\u201322,",
      "year": 2017
    },
    {
      "authors": [
        "M. Mo\u017eina",
        "J. Dem\u0161ar",
        "M. Kattan",
        "B. Zupan"
      ],
      "title": "Nomograms for Visualization of Naive Bayesian Classifier",
      "venue": "Proceedings of the 8th European Conference on Principles and Practice of Knowledge Discovery in Databases (PKDD), pages 337\u2013348,",
      "year": 2004
    },
    {
      "authors": [
        "A.V. Nefian"
      ],
      "title": "Georgia Tech Face Database",
      "venue": "http://www.anefian.com/research/face_reco. htm,",
      "year": 1999
    },
    {
      "authors": [
        "B. Nelson",
        "M. Barreno",
        "F.J. Chi",
        "A.D. Joseph",
        "B.I.P. Rubinstein",
        "U. Saini",
        "C. Sutton",
        "J.D. Tygar",
        "K. Xia"
      ],
      "title": "Exploiting Machine Learning to Subvert Your Spam Filter",
      "venue": "Proceedings of the 1st USENIX Workshop on Large-Scale Exploits and Emergent Threats (LEET),",
      "year": 2008
    },
    {
      "authors": [
        "J. Newsome",
        "B. Karp",
        "D. Song"
      ],
      "title": "Paragraph: Thwarting Signature Learning by Training Maliciously",
      "venue": "Proceedings of the International Workshop on Recent Advances in Intrusion Detection (RAID),",
      "year": 2006
    },
    {
      "authors": [
        "N. Papernot",
        "P. McDaniel",
        "I. Goodfellow",
        "S. Jha",
        "Z.B. Celik",
        "A. Swami"
      ],
      "title": "Practical blackbox attacks against machine learning",
      "venue": "Proceedings of the 2017 ACM Asia Conference on Computer and Communications Security (ASIACCS),",
      "year": 2017
    },
    {
      "authors": [
        "N. Papernot",
        "P. McDaniel",
        "S. Jha",
        "M. Fredrikson",
        "Z.B. Celik",
        "A. Swami"
      ],
      "title": "The limitations of deep learning in adversarial settings",
      "venue": "Proceedings of the IEEE European Symposium on Security and Privacy (Euro S&P), pages 372\u2013387, March",
      "year": 2016
    },
    {
      "authors": [
        "S.V. Radhakrishnan",
        "A.S. Uluagac",
        "R. Beyah"
      ],
      "title": "GTID: A Technique for Physical Device and Device Type Fingerprinting",
      "venue": "IEEE Transactions on Dependable and Secure Computing (TDSC), 12(5):519\u2013532, Sept",
      "year": 2015
    },
    {
      "authors": [
        "M.T. Ribeiro",
        "S. Singh",
        "C. Guestrin"
      ],
      "title": "Why Should I Trust You?: Explaining the Predictions of Any Classifier",
      "venue": "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD),",
      "year": 2016
    },
    {
      "authors": [
        "B.I. Rubinstein",
        "B. Nelson",
        "L. Huang",
        "A.D. Joseph",
        "S.-h. Lau",
        "S. Rao",
        "N. Taft",
        "J.D. Tygar"
      ],
      "title": "ANTIDOTE: Understanding and Defending Against Poisoning of Anomaly Detectors",
      "venue": "In Proceedings of the 9th ACM SIGCOMM Conference on Internet Measurement (IMC),",
      "year": 2009
    },
    {
      "authors": [
        "F. Schroff",
        "D. Kalenichenko",
        "J. Philbin"
      ],
      "title": "FaceNet: A Unified Embedding for Face Recognition and Clustering",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "year": 2015
    },
    {
      "authors": [
        "O. Sener",
        "S. Savarese"
      ],
      "title": "Active Learning for Convolutional Neural Networks: A Core-Set Approach",
      "venue": "Proc. International Conference on Learning Representations (ICLR),",
      "year": 2018
    },
    {
      "authors": [
        "M. Sharif",
        "S. Bhagavatula",
        "L. Bauer",
        "M. Reiter"
      ],
      "title": "Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition",
      "venue": "Proceedings of the ACM SIGSAC Conference on Computer and Communications Security (CCS),",
      "year": 2016
    },
    {
      "authors": [
        "Y. Taigman",
        "M. Yang",
        "M. Ranzato",
        "L. Wolf"
      ],
      "title": "DeepFace: Closing the Gap to Human-Level Performance in Face Verification",
      "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),",
      "year": 2014
    },
    {
      "authors": [
        "G.G. Towell",
        "J.W. Shavlik"
      ],
      "title": "Extracting Refined Rules from Knowledge-Based Neural Networks",
      "venue": "Machine Learning, 13(1):71\u2013101,",
      "year": 1993
    },
    {
      "authors": [
        "F. Tram\u00e8r",
        "F. Zhang",
        "A. Juels",
        "M.K. Reiter",
        "T. Ristenpart"
      ],
      "title": "Stealing Machine Learning Models via Prediction APIs",
      "venue": "Proceedings of the 25th USENIX Security Symposium (USENIX Security),",
      "year": 2016
    },
    {
      "authors": [
        "P. Viola",
        "M.J. Jones"
      ],
      "title": "Robust Real-Time Face Detection",
      "venue": "International Journal of Computer Vision (IJCV), 57:137\u2013154, May",
      "year": 2004
    },
    {
      "authors": [
        "X. Wu",
        "V. Kumar",
        "Q.J. Ross",
        "J. Ghosh",
        "Q. Yang",
        "H. Motoda",
        "G.J. McLachlan",
        "A. Ng",
        "B. Liu",
        "P.S. Yu",
        "Z.H. Zhou",
        "M. Steinbach",
        "D.J. Hand",
        "D. Steinberg"
      ],
      "title": "Top 10 algorithms in data mining",
      "venue": "Knowledge and Information Systems, 14(1):1\u201337,",
      "year": 2008
    },
    {
      "authors": [
        "D. Yi",
        "Z. Lei",
        "S. Liao",
        "S.Z. Li"
      ],
      "title": "Learning Face Recognition from Scratch",
      "venue": "arXiv:1411.7923,",
      "year": 2014
    },
    {
      "authors": [
        "J. Yosinski",
        "J. Clune",
        "Y. Bengio",
        "H. Lipson"
      ],
      "title": "How transferable are features in deep neural networks",
      "venue": "In Proceedings of the 27th International Conference on Neural Information Processing Systems (NIPS),",
      "year": 2014
    },
    {
      "authors": [
        "K. Zhang",
        "Z. Zhang",
        "Z. Li",
        "Y. Qiao"
      ],
      "title": "Joint Face Detection and Alignment Using Multitask Cascaded Convolutional Networks",
      "venue": "IEEE Signal Processing Letters, 23(10):1499\u20131503, Oct",
      "year": 2016
    },
    {
      "authors": [
        "Z. Zhao",
        "D. Dua",
        "S. Singh"
      ],
      "title": "Generating Natural Adversarial Examples",
      "venue": "Proceedings of the International Conference on Learning Representations (ICLR),",
      "year": 2018
    },
    {
      "authors": [
        "Y. Zhou",
        "M. Kantarcioglu",
        "B. Thuraisingham",
        "B. Xi"
      ],
      "title": "Adversarial Support Vector Machine Learning",
      "venue": "Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD),",
      "year": 2012
    }
  ],
  "sections": [
    {
      "heading": "1 Introduction",
      "text": "Authenticating subjects in a system is essential for establishing trust. While authentication is often performed with traditional credential or user-based mechanisms, machine learning (ML) is being increasingly used for authentication. By reformulating authentication as a classification problem, both humans and machines can act as subjects and be authenticated with high accuracy and minimal false positives [4, 6, 21, 24, 26, 36]. The underlying goal of these authentication systems is to use unique information owned by the user to attest a claimed identity. The modality of this information has changed from passphrases and PIN numbers to fuzzy features such as facial features [6] and end-system behavior (e.g., key strokes [29], hardware information [7], and device usage [12, 23, 29]).\nHowever, these authentication techniques tend to only consider naive spoofing attacks that replicate features using domain knowledge, rather than targeting the underlying model\u2019s decision boundary. The field of adversarial machine learning (AML) has uncovered several milestone attacks against state-of-the-art ML models [14, 34, 35]. A limitation of these\nPre\u2013print, work in progress.\nar X\niv :1\n81 0.\n00 02\n4v 1\n[ cs\n.L G\n] 2\nRandom Pixel + LIME Perturb. Strategy Adversary Perturbed Adversary Successful Victim\nVAE Latent Space + LIME Perturb. Strategy Adversary Perturbed Adversary Successful Victim\nproposed attacks is the context in which they are built; their explicit goal is breaking classification tasks, rather than authentication tasks. Some of the key differences that distinguish authentication attacks from recently proposed classification attacks are:\nInformation returned from the authentication system is limited. In authentication tasks, class-level information is limited, and feedback from the model is essentially non-existent, apart from the final authentication decision. Although the underlying model may be the same, the public interface through which authentication may be performed is purposefully limited to a binary classification problem. For example, the interface provided by face authentication services, such as Windows Hello, will only display a fail or success [28]. This is coupled with the fact that the number of queries available to an attacker is limited, so there is an ever\u2013vanishing window during which the attacker can gather information. We can consider an authentication system as an oracle that outputs a binary decision based on a provided username and token pair. An attacker could theoretically extract side-channel information about a target directly from the oracle [14], but the information returned by a real-world authentication system (Yes or No) is not sufficient for proposed inversion attacks to be practical, as such attacks require either confidence information or class predictions [34]. We avoid relying on either of these values as they are likely not available to the user in a real-world authentication system. Our attack relies entirely on the binary authentication result returned by the oracle.\nTarget information is secret and hidden. In previous machine learning attacks, an adversary may perturb the features of a known target, and force it to be mislabelled. These are attacks against classification, where an attacker takes correctly-classified input features and iteratively modifies them with the goal of altering the classification result. In the face authentication domain, this takes the form of compressing a known target into a less visually perceptible version of itself so it can bypass a classifier. For example, images of a known target\u2019s face can be used to construct perturbations that fool the authentication system [41]. This is a consequence of \u201cfuzzy\u201d authentication systems, which rely on mutable properties to establish identity. In these cases, it is attractive to reconstruct the problem into a generative modeling problem, where the target must be reconstructed using some additional information acquired by the adversary. However, in a system which uses physical properties of the target to authenticate, such as physical runtime characteristics of an end host in a system authentication task, the adversary needs direct access to the host to gather any information. Thus, our own attack is motivated to be completely blind. Instead of relying on any known target information, we leverage explainable AI (XAI) techniques [37] to infer an oracle\u2019s decision boundaries. XAI techniques, such as LIME [37], are primarily used for generating human\u2013understandable explanations of a machine learning model\u2019s decision. However, we find that XAI systems also grant the ability to discover an oracle\u2019s decision boundary without needing any secret information about potential victims.\nContributions. This paper circumvents the previous limitations by formulating an attack which combines AML, query synthesis, and XAI techniques. To the best of our knowledge, this attack is the first to use XAI techniques to aid the adversary\u2019s goal of gleaning information about the decision boundary. The attack is first formalized to show that it generalizes to other authentication domains by limiting the number of queries an adversary needs to learn the model\u2019s decision boundary. The attack is then used against state-of-the-art end-host and biometric authentication systems. As illustrated in Figure 1, the attack is effective even in a biometric authentication system where victim signatures are secret. The attack is later verified against a widely-used black-box face authentication API named Face++ [13]. We show an adversary can circumvent defended models and achieve 93% success rate with as few as 100 queries to the Oracle, rivaling recent attacks which relied on top-k class feedback from the oracle [41].\nThe rest of the paper proceeds as follows: Section 2 provides background information on each domain relevant to our attacks; Section 3 presents our threat model and approach; Section 4 gives our evaluation results; Section 5 discusses our use of XAI and the limitations of our approach; Section 6 considers related work; and Section 7 concludes."
    },
    {
      "heading": "2 Background",
      "text": "We focus on attacking authentication systems that use ML models to differentiate between principals. In this section, we briefly discuss each domain relevant to our attack."
    },
    {
      "heading": "2.1 Fingerprinting for Authentication",
      "text": "Authentication is the act of verifying an identity using a set of unique credentials. Traditional credential formats include PINs, passwords, and keys. More recent authentication systems use a combination of traditional credentials and fingerprints (including biometrics). For example, recent systems are able to deduce fingerprints for smartphone users through the way they interact with the touchscreen [23]. For computers with physical keyboards, it is possible to create a unique identity based on keystroke and mouse patterns [29]. Each principal u in a fingerprinting system is capable of generating a unique fingerprint xt at some arbitrary time t after registration (hereafter known as operation register). The fingerprint may be any combination of immutable and mutable features which are secret to all other principals in the system. The principal hereafter owns the identity u and claims this identity in the future using a generated vector z. Given a deterministic lookup function F , the objective of the fingerprinting system is to correctly evaluate the features zt+i claimed by identity u such that F (zt+i, u) = 1, and F (yt+i, u) = 0 for other possible feature vectors y that do not belong to u at any arbitrary time step i.\nAuthentication through fingerprints is intended to be more convenient than traditional methods, as the principal does not have to memorize or store any extra burdensome information. Rather, the principal is only responsible for generating a new sample z upon request by the authentication system. However, the use of mutable features that act as \u201cfuzzy\u201d indicators for identity may be prone to misclassification by the authentication system, which can be considered an estimation over the entire feature space. Due to the nature of these fuzzy features, many samples may be necessary to make a decision on behalf of the principal, in which case a majority voting scheme is generally deployed to make the authentication decision [4]. This abstraction on top of the authentication decision makes the attack more difficult, since the adversary has less control over the model during queries. Still, in the worst case, the mutable features could be exploited to fool the estimation of the lookup function F , which forms the basis for our attack.\nDevice Fingerprinting. Device fingerprinting, in particular, relies on characteristics inherent in any combination of hardware, software, and user behavior of a particular machine. While different in implementation, it shares some similarities with browser-based fingerprinting, a related line of work that tracks visitors of websites based on browser type/version, screen resolution, language, fonts, plugins, etc. Device fingerprinting has been explored extensively in the literature, including by Kohno et al. [24], Bates et al. [4] and Radhakrishnan et al. [36]. For example, the solution proposed by Bates et al. relies on the combined variation of USB firmware and hardware timing to generate a unique signature of the target host device, while Radhakrishnan et al. use network packet inter-arrival times. Early device fingerprinting such as the system proposed by Kohno et al. [24] instead relied on system clock skews. Among these systems, each relied on the intuition that both soft and physical components of a device contribute a unique amount of variation that enables fingerprinting.\nThe key to creating a model that captures all different characteristics to a specific signature was ML classification techniques, such as decision trees in the case of USB transactions, and artificial feed-forward neural networks for network packets. However, the presence of an adversary with malicious data samples was only partially investigated in these systems.\nFacial Verification. Facial verification, at its core, is also a form of fingerprinting. Each face can be considered as a set of physical features which contribute a unique amount of variation that enables fingerprinting. Many approaches have been proposed for facial verification, including FaceNet [11], which uses a deep convolutional network trained to directly learn a mapping from face images to a compact Euclidean space, learning directly from the face pixels. Other approaches include DeepFace [42], which uses Principal Component Analysis (PCA) to perform feature reduction, and GaussianFace [26], which jointly learns several recognition-related tasks to learn better discriminative features. Since a large portion of facial recognition performance relies on feature engineering, much of the literature focuses on an efficient feature representation for human faces. However, like device fingerprinting systems, facial verification systems make extensive use of feature engineering techniques that make the underlying machine learning models vulnerable to attacks."
    },
    {
      "heading": "2.2 Adversarial Machine Learning (AML)",
      "text": "In our authentication task, the machine learning model assumes the role of lookup table as described in the previous section. The model is built as a deterministic function F : D \u2192 U mapping a training set of features D to labels U . In our scenario, U forms the labels for all principals registered with the system, and D is the associated fingerprinting features that F will use to estimate the distinction between principals, such that x \u2208 D for some principal\u2019s feature vector x. From the view of the machine learning model, the classification and authentication tasks are identical. In both cases, labels are assigned based on some criterion, and a decision is made. Many algorithms exist to create the mapping between features and labels, including Decision Trees, Neural Networks, and Support Vector Machines [46]. We investigate these models to determine their susceptibility to the attack, both before and after defenses are applied.\nIn an adversarial environment, the model is susceptible to attacks that target different areas of the decision-making process [3]. Such attacks can be either black-box, grey-box, or whitebox, each denoting a successive increase in the adversary\u2019s knowledge of the underlying model. Authentication systems are considered oracles that output a binary classification based on a principal\u2019s inputs. The internal architecture and parameters of the oracle\u2019s model are not visible to the principals; thus we assume a black-box model throughout our experiments.\nOur attack makes use of transfer learning, in which learned information from one task is transferred to facilitate learning of a new task (e.g., transferring the weights from a trained neural network to a new model that will be further fine\u2013tuned). In this way, existing labeled data of related tasks or domains can be used to train new models. For image classification, general feature information on image composition is accumulated through a convolutional neural network\u2019s (CNN\u2019s) layers, where each layer is activating for a certain set of visual features. A popular technique is to take the final pooling layer of a CNN as a feature vector, as it produces a concise representation of the accumulated knowledge from every prior layer [48]. This technique is powerful as vectors can be measured using some arbitrary distance function. These distances are then used by the model to generate various outputs, as in the case of natural language processing and face recognition models [39], as well as any model that attempts classification over a large quantity of classes [25]."
    },
    {
      "heading": "2.3 Query Synthesis and XAI",
      "text": "We wish to minimize the amount of queries an adversary makes to the authentication system, and create queries that will correctly mis-classify the adversary as some identity T . Thus, we formulate our problem in the context of query synthesis [1]. Query synthesis is a central problem in the domain of active learning, which is itself a sub-field of machine learning. The goal of active learning is to learn some concept by using as few queries, or samples, as possible. Active learning problems can arise in cases where unlabeled data is much more abundant than labeled data. For our purposes, the adversary must make minimal queries in order to learn the unknown concept of a potential victim, where the concept describes the victim\u2019s feature space. Once the adversary learns the concept, they must synthesize queries that will result in successful authentication.\nIn an authentication system, an adversary\u2019s only feedback is a positive or negative result. Since positive results are secret, the adversary must operate on their own data to extract a victim\u2019s concept. We leverage Explainable\u2013AI (XAI) techniques to infer the decision boundary local to the adversary and determine the set of most influential features in the model, thus learning a concept that is useful to the adversary. Such techniques (e.g., LIME [37]) were originally developed to present human-readable explanations of opaque model decisions, but we find they offer an intuitive interface to iteratively discover a decision boundary, and use such information to synthesize successful queries. Specifically, LIME trains a linear model using a local neighborhood of perturbations around some data point. Rather than directly performing iterative perturbations on global features as in previous attacks, we take this linear model to inform the adversary of which features to modify local to the current sample in order to move towards a successful query."
    },
    {
      "heading": "3 Approach",
      "text": "Given the earlier discussion, we formally describe our attack algorithm in this section. One of the challenges is that the internals of the ML model used by the authentication system are not available to us. For example, if the authentication system uses a DNN that outputs a class label corresponding to the user-id and then gives its authentication decision (0 or 1), an attacker does not see the attack label (he only observes whether the authentication request was accepted or denied). We follow the formalization depicted in Figure 2, which offers a high-level overview of the approach."
    },
    {
      "heading": "3.1 Formalized Threat Model & Attack",
      "text": "Authentication system. Earlier in Section 2.2, the model was defined as the mapping F : D \u2192 U . We extend this definition to consider the dimensionality and possible decisions of the feature space, to allow for a more rigorous formalization. Let U = {u1, \u00b7 \u00b7 \u00b7 , un} be the set of current users and F : (U \u00d7 Dk) \u2192 {0, 1} be the classifier that corresponds to the authentication system (D is the domain of features with dimensionality k). If some user wants to authenticate as u \u2208 U , they present a vector x \u2208 Dk, and the authentication succeeds iff F (u, x) = 1. Formally, an authentication system (AS) is 4-tuple (U,D, k, F ). An authentication system AS = (U,D, k, F ) also has an operation register, which adds a new user u\u2032 to AS and thus U is updated to U \u222a{u\u2032} and the classifier F is updated accordingly (i.e., F has to accept inputs of the form (u\u2032, z), where u\u2032 is the new user and z \u2208 Dk is the vector used for authentication). The precise mechanics of the register operation are not important to the discussion.\nAdversary\u2019s capability, constraints and goal. An adversary A is given black-box access to AS (can register new users and obtain answers to F (u, y) for chosen u and y). The goal of the adversary is to impersonate some user t \u2208 U (i.e., craft a z such that F (t, z) = 1). There are additional constraints that we can place on the adversary. For example, we can limit the number of queries and the registration of new users.\nMoreover, A can have some partial knowledge about the vector z (e.g., the make and model of t\u2019s host) that will lead to successful authentication of t. However, we reiterate that although A may have partial knowledge of z, it is not enough information for successful impersonation. Thus, in this paper we assume zero knowledge of z. A is permitted to have knowledge about a subset UA \u2286 U of the users (only in the case that A registered the users themself or might be colluding with them). For our purposes we further assume that t /\u2208 UA.\nConcrete algorithm of the adversary. Now we describe an algorithm for the adversary. Note that at each step, the adversary can interact with the authentication system (registering new users or querying F (u, \u00b7) on vectors, which is implicit throughout the algorithm). Essentially, the adversary is given oracle access to the authentication system.\n\u2022 Initial: The adversary starts with a substitute network G : Dn \u2192 {0, 1} trained on seed dataset DA \u2286 Dn, where DA is similar to a learnable coreset in the context of active learning [40]. We set a counter i = 0 and set z0 to be a vector, which is the initial guess of the attacker. The history H(A) of the adversary is denoted by (G, z0, \u00b7 \u00b7 \u00b7 , zi) and updates as the algorithm proceeds. In practice, this stage is implemented as the algorithm described in Algorithm 1, named QuickStart.\n\u2022 Iterative loop: \u2013 If F (t, zi) = 1, A is done. \u2013 Otherwise A crafts zi+1 based on its history H(A). The adversary updates its\nhistory H(A) by updating G (based on the fact that F (u, zk) = 0 for 0 \u2264 k \u2264 i) and increments the counter i. Following the update, the adversary A returns\nto the beginning of the iterative loop and continues the process until a query is successful.\nNow the question is this: how does A craft zi+1 from the current history H(A)? One option is to use existing algorithms for crafting adversarial examples [35] [34]. However, such algorithms require explicit information from the oracle about the sample\u2019s classification. For this reason, we implement our own adversarial sample crafting algorithm, called LIME\u2013 Sampler, which is discussed in detail in the next section. Start with zi and the substitute network G and craft an example w such that G(w) = 1 and set zi+1 = w. G(w) = 1 implies that w is crafted so it eventually inhabits the feature space nearby some fingerprint x for target identity t (with F (t, x) = 1). The algorithms for constructing adversarial examples can perform better if they rank features that contribute most to the decision. For the purpose of describing the influence of features on the decision of the substitute model G, we use techniques from the XAI literature. Specifically, our algorithm leverages an XAI system, LIME [37] to rank the features that correspond to G\u2019s decision on w."
    },
    {
      "heading": "3.2 Constructing the Seed Dataset",
      "text": "As described in the algorithm above, the adversary must first build a seed dataset DA which is used to train the substitute model G. There are two challenges in constructing DA. First, DA should have enough positive and negative examples such that new information is deduced, rather than memorized, about the cause for a \u201cYes\u201d or a \u201cNo\u201d. Second, as we alluded to before, we can only rely on \u201cYes\u201d and \u201cNo\u201d decisions from the authentication system, rather than class\u2013level information. The idea of discovering the smallest fundamental set from which to learn a feature space is known as coreset construction in active learning, and thus we frame our discussion in a similar context [40].\nThe key factor in an adversary\u2019s ability to infer the decision boundary is the quality of the adversarial sample crafting algorithm. To illustrate this, we start with a simple case following the notation of the previous section. Let D be some sample crafting algorithm which takes as input the adversary\u2019s candidate feature vector, and outputs a highly distorted version of this vector. For the purpose of discussion, the exact heuristic to measure distortion is not important, we simply assume that the distortions produced by D mostly result in outliers within the feature space learned by F . The adversary may construct some feature vector zi = D(zi\u22121) for i > 0, forcing F to associate the adversary with the closest principal t whose feature vector resembles zi = D(zi\u22121), (that is, F (zi, t) = 1). Inuitively, t is an outlier who unfortunately is closest in the feature space to the distortions created by D. However, we assume for any model that the feature space is sufficiently normalized to minimize the number of outliers. If we limit the adversary\u2019s knowledge to only a subset of usernames UA \u2282 U , and t /\u2208 UA, in this case the adversary will not have any chance of success. We can further deduce that if the amount of outliers in the feature space is minimized, and the adversary\u2019s knowledge requirement is also minimized, the probability of success quickly diminishes. In the opposite case, where zi closely resembles the adversary\u2019s own feature vector, they will always be classified as themselves. It follows that if the adversary crafts \u201cNo\u201d samples, and their distortions are too similar to \u201cYes\u201d samples, it will be difficult to model their distinction.\nWe must thus discover some learnable subspaces for \u201cYes\u201d and \u201cNo\u201d in the total feature space. This motivates the creation of an iterative sample crafting algorithm, QuickStart, which can strike a balance between the two extreme cases discussed above. One of the challenges in designing QuickStart is that we must rely only on the oracle\u2019s authentication decision to increase or decrease distortion. QuickStart is described in Algorithm 1.\nUnlike previous algorithms, QuickStart relies only on a binary decision from the oracle it is querying. The primary goal is to build a dataset of YES decisions denoted by DAyes and a dataset of NO decisions denoted by DAno, which combine to form DA. We constrain the algorithm so that each set is balanced, such that |DAyes | = |DAno | (for the purpose of better performance when training G later). The attacker starts with a set of benign samples they own.\nAlgorithm 1 QuickStart, seed dataset builder. Input: AGT , a set of initial samples owned by A. \u03b2, exponential distortion lower bound. \u03b1, exponential distortion upper bound. \u03c3, the desired number of samples for each class (negative and positive), O, the victim Oracle, and subroutine query, a generic interface available to A for querying O.\nDAyes , DAno \u2190 \u2205 R\u2190 YES while | DAyes |< \u03c3l do\nx\u2190R AGT B (random sample from AGT ) R\u2190 query(O,A, x) if R = NO: add(DAno , x) else : add(DAyes , x)\nend while R\u2190 YES, \u03a5\u2190 0 while | DAno |< \u03c3l do\nx\u2190R AGT B (Loop until we meet sufficient distortion) while R = YES do\nincrement(\u03a5) x\u2217 \u2190 Perturb(x,\u03a5) R\u2190 query(O,A, x\u2217)\nend while x\u2217 \u2190 Perturb(x,\u03a5) R\u2190 query(O,A, x\u2217) if R = NO: append(DAno , x\u2217)\nend while return DAno , DAyes\nThe first loop of the algorithm is responsible for dividing the samples from the set AGT (the set of initial samples) into the set DAyes.\nThe second loop performs the process of discovering the set of NO samples (if we had enough negative samples from the first loop, the second loop does not execute). This loop relies on a procedure Perturb(x,\u03a5), which perturbs the vector x according to the parameter \u03a5 (higher \u03a5 means that the perturbation has higher variance). The lower and upper bounds of the exponentially increasing distortion form a sliding window that can be easily controlled by the lower bound \u03b2 and upper bound \u03b1. For example, the adversary may observe their own features and use a fraction of the lower and upper bounds available. The strength of the trained model S is ultimately determined by the number of training samples harvested during QuickStart. Although a larger number of samples is preferable depending on the architecture of G, a primary goal of A is to also minimize queries; thus, the size of sets DAyes and DAno is bounded by a tunable parameter \u03c3."
    },
    {
      "heading": "3.3 Crafting Attack Samples",
      "text": "Once A has the dataset DA, it must train a substitute model G as shown in Figure 3. This step is inspired by black-box attacks which exploit transferability properties of oracle decision boundaries [34]. Rather than use explicit class labels, we rely on the YES and NO decisions made by the oracle with respect to perturbed data to create a model of the decision boundary. The goal of the adversary is to pick a model architecture that does not require a large DA, but can make a reasonable assumption about future adversarial samples zi+1. In practice, the primary restriction on the attack\u2019s effectiveness is the selection of G and size of DA. For example, with a shallow neural network, the accuracy of G ranges from 30%-80% depending on the sample type, size of DA, and quality of samples returned by QuickStart. Although we optimize our network architecture for test-set performance, the limited size of DA can limit the effectiveness of A based on its similarity to other principals.\nHowever, once G is trained, A can easily transform this model into guided perturbations using an XAI system such as LIME. In 1-dimensional feature modalities, LIME can be used to describe specific feature values that will contribute to either oracle decision. Specifically, LIME outputs an interval the feature is expected to occupy for its associated weight on the decision\u2019s class. Using this information, A crafts attack samples zi+1 and attempts to masquerade as some other principal in the system. The algorithm to perform this process is outlined in Algorithm 2.\nThe main goal is to synthesize a new feature vector x\u2217 \u2208 Rk by modifying the r most relevant features in x \u2208 Rk which contributed to the NO class. It follows that r can be tuned to adjust the distortion of x\u2217, such that 0 < r \u2264 k for the feature space Dk. For the sake of the algorithm, we define an explanation returned from LIME\u2013Query as a vector of 3\u2013tuples containing the index of a feature, the weight of the feature on a NO decision, and the interval it is expected to occupy. Thus we have an explanation Ex = {(0, w0, [c0, d0]), ..., (k,wk, [ck, dk])} for feature index i \u2208 {0, ..., k}, feature weight wi \u2208 R, interval lower bound ci \u2208 R, and its respective upper bound di \u2208 R. The top r features and their intervals are found by simply sorting Ex on the values of wi, and taking the top r 3\u2013tuples. To arrive at the new value for feature x\u2217i with i \u2208 {0, ..., r}, we sample uniformly from a feature\u2019s expected interval, such that x\u2217i \u223c U(ci, di). We further define a set of principals V for which A knows the usernames, and further assume that v \u2208 V with V \u2282 U . Thus, Algorithm 2 allows us to affect the most relevant features in x that allow A to masquerade as some other principal, and stop as soon as a successful query is encountered.\nAlgorithm 2 LIME\u2013Sampler, adversarial sample crafting algorithm. Input: DA, the seed dataset created with QuickStart, G, an arbitrary model that can be trained on DA, train, the associated training function for G, LIME\u2013Query, the subroutine provided by LIME to query given a trained model and candidate sample, and subroutine query, a generic interface available to A for querying O.\nG\u2190 train(G, DA) R\u2190 NO while R =NO do\nx\u2190R AGT (random sample from AGT ) B (Get explanation tuples Ex = {(0, w0, [c0, d0]), ..., (k,wk, [ck, dk])} tuples for NO) Ex \u2190 LIME\u2013Query(x, G, YES) B (Sort over feature weights wi and take top r tuples) Ex \u2190 sort(Ex)[: r] for i, wi, [ci, di] \u2208 Ex do\nB (Update feature xi with uniform sample over expected interval.) xi \u2190 U(ci, di)\nend for v \u2190R V (random sample from V ) B (Now attempt to authenticate as principal v) R\u2190 query(O, v, x)\nend while"
    },
    {
      "heading": "4 Evaluation",
      "text": "Our attack is targeted against both host and biometric authentication systems. Our experiments are motivated by the following questions:\n1. Are classification models suitable for use in authentication systems, even with a relatively low number of principals and private data?\n2. Are perturbations during the attack, and their effects, understandable by humans?\n3. Do defenses such as adversarial sample injection afford classification models robustness against masquerade attacks?\nWe answer these questions by attacking models and data that have been vetted by the research community. For clarity, we describe the implementation, mechanisms, and data used for each target system."
    },
    {
      "heading": "4.1 Experimental Highlights",
      "text": "From our experiments, we observe that the attack is effective across the tested domains and machine learning classifiers. In particular:\n1. We find that classification models, even when defended, are vulnerable to our attack, giving the adversary up to 93% success rate depending on the model architecture. The attack is equally effective against a commercial facial recognition system.\n2. Perturbations can be understood by the adversary in both domains. For host authentication, the adversary simply notes the USB enumeration timings which produced success. In biometric authentication, the adversary can use LIME\u2019s graphical plotting capabilities to reveal portions of the image that contributed to successful masquerade.\n3. Adversarial training is effective for certain models, and greatly reduces the adversary\u2019s probability for success. However, future adversarial inputs could be easily modified to evade such training."
    },
    {
      "heading": "4.2 Data and Implementations",
      "text": "Host authentication. We evaluate against a host authentication system based on USB enumeration timings of a computer under test [4]. This system uses a Random Forest as the underlying machine learning model, trained in a target vs. outlier fashion, with a new model created for every principal registered to the system. Outlier classes are balanced with respect to every possible principal in the system. Over-sampling is performed with replacement until the number of target samples matches the number of outlier samples. In this way, there are equal numbers of class data samples registered in the machine learning model. The implementation and data was obtained from the authors of the original system [4]. The trace data for this system consists of identical machines across nine users. This data also had scripts to generate the accompanying datasets for each principal\u2019s model. As described in [4], certain hosts must be filtered from the dataset to remove false positives. We note that two of the nine hosts were removed from the set of principals after applying this constraint. Due to the limited sample size of this dataset, we focus much of our experiments on biometric authentication. Nevertheless, we believe this system\u2019s scope is realistic, and helps us generalize the attack to similar host authentication schemes [24, 36].\nBiometric authentication. We target a facial authentication system based on a stateof-the-art facial recognition model named Facenet [39]. The implementation is taken from a popular open-source repository on GitHub [11]. We use only pre-trained models provided by the repository\u2019s maintainer. The Facenet embedding model was trained on a subset of the Microsoft Celeb-1M image dataset [20], which was released for the MSR Image Recognition Challenge at ACM Multimedia 2016. Since Facenet may act as an embedding model underneath an authentication model, we train a Support Vector Machine classifier as described by the repository\u2019s documentation to perform authentication (denoted Facenet-SVM). For completeness, we also measure the attack against two additional models: Random Forest (Facenet-RF) and Neural Network (Facenet-NN), to understand the attack\u2019s scope in systems which have different architectures. These models were selected as they demonstrated the best classification accuracy when combined with Facenet. After training, none of the authentication systems exhibited false positives on hold-out evaluation data.\nTraining and evaluation data is taken from the Georgia Tech Face Database [31] and contains faces of 50 people, with 15 images per person. The provided faces are composed of frontal shots that vary in facial expressions and lighting conditions, which we deem realistic for a real-world face authentication system. For consistency, this image data is pre-processed\nwith Multi-task CNN [49], the same face alignment model used to pre-process all Facenetrelated training data. We make no other assumptions about the model, and argue that the adversary would be free to use any face-cropping tool available to have consistency in their attack.\nDue to the large feature space of face images, we propose two different paradigms for perturbing attack images, which are demonstrated in Figure 1:\n1. Random Pixel. This attack implements Perturb from Figure 1 as the adversary setting a subset of pixels to random subpixel values according to \u03a5, inspired by recent state-of-the-art attacks [34, 35]. In this attack, the entire image constitutes a single sample fed into G.\n2. VAE Latent Space. This attack makes use of the Facenet Variational Autoencoder (VAE) to create natural and realistic attack images. Similar to hill-climbing attacks which target feature dimensionality reduction [15, 16], the adversary implements Perturb to modify the low-dimensional latent vector created after encoding their face with the VAE, similar to a natural image attack proposed by Zhao et al. [50]. This means the adversary is able to create natural and believable face samples with feature distortion \u03a5. Perturbed latent vectors are used as training data for G, and are decoded to query the authentication system.\nThus, we offer two different techniques for perturbing adversary data before feeding it into LIME. We take the GitHub repository\u2019s version of the Facenet VAE along with the provided weights trained on the CASIA-WebFace dataset [47], a large-scale dataset containing almost 500k faces of over 10k people. We clarify that every pre-trained model used in the experiments was trained on a different and unique dataset. That is, we assume the adversary has no access to the oracle\u2019s training data, other than what the adversary has registered.\nThe use of a Facenet-derived generative model may partially violate the black-box assumption. To verify our attack, we run experiments against a face recognition API named Face++ [13]. To clarify, Face++ does not have any relation to Facenet, apart from their similar naming. We build a small oracle abstraction around the Face++ API to return only a YES or NO to the adversary. A configurable parameter of Face++ is the threshold at which it returns a positive result. If the provided face aligns with an identity in the top k matched identities, it will return a YES. Unlike previous work [41], we configure the Face++ API\u2019s top-k response to k = 1 and only count a query as successful if the top result returned by Face++ matches the identity claimed by the adversary. We believe this scenario is the most realistic, as it targets the most secure setting of k possible within the system, and is simultaneously the worst-case scenario for the adversary."
    },
    {
      "heading": "4.3 Metrics",
      "text": "We define a successful attack as one in which the adversary is able to masquerade as some other principal on the system, that is, reach masquerade, and denote probability of masquerade as P (M). A principal\u2019s login credentials can be interpreted as some pairing of username u and a unique secret token d initially known only by the owner of u. It follows that if u is known, an adversary may attempt to generate d for an observable feature space Dk where d \u2208 Dk, defined prior in Section 3.1. As the adversary\u2019s knowledge of the usernames in the population U increases, so does success probability P (M). Let V denote the set of known usernames (not secret tokens) by the adversary, such that V \u2282 U . With complete knowledge of all usernames (V = U), the adversary has success probability P (M) = 1, assuming the adversary has infinite queries available and may try every combination of features available. It follows that for V = \u2205, P (M) = 0. Due to practical computation restrictions, this assumption is not very useful for the adversary. However, we show that P (M) may be maximized as long as V 6= \u2205. With some arbitrary amount of knowledge of the system\u2019s registered usernames, V , and a realistic bound on feature combinations, the adversary desires to increase probability P (M) and minimize ||V ||. The most straightforward way to accomplish this is to artificially increase the distribution of false positives in the system, such that P (FPO) >> P (FPR) for\nUSB Enumeration Perturb. Attack Coverage\n0 1 2 3 4 5 6\n0 1 2 3 4 5 6 A dv er sa ry\nBaseline\n0 1 2 3 4 5 6\n0 1 2 3 4 5 6\nReplay\n0 1 2 3 4 5 6\n0 1 2 3 4 5 6\nQuickStart\n0 1 2 3 4 5 6\n0 1 2 3 4 5 6\nQuickStart+LIME\nVictim\nFigure 4: Attack coverage map based on authentication success for adversary\u2013victim pairs. An adversary knows all seven usernames in the system during the attack.\nReplay 57.25 \u00b1 65.99 P (M) = 71%\nQuickStart 112.25 \u00b1 41.67 P (M) = 71% QuickStart 133.25 \u00b1 34.64 +LIME P (M) = 57%\nTable 1: The average number of adversarial queries before success in each of three experiments, testing against USB enumeration at 100% knowledge. P (M) is the calculated probability based on attack coverage for each experiment.\nthe optimized approach O, and random approach R. XAI systems such as LIME enhance the adversary\u2019s knowledge of the oracle\u2019s decision boundary, allowing the adversary to increase P (M), thus minimizing the number of queries needed to find a false positive. Thus, for every experiment we measure the number of queries at which the adversary achieves masquerade, and the adversary\u2019s authentication decision for every victim u \u2208 U . P (M) is calculated as the number of principals who could achieve masquerade, divided by the total number of principals in the experiment. Thus, if there is a set of principals in the experiment U , and the set of principals who achieved masquerade UM , we have UM \u2286 U and P (M)AS = ||UM ||||U || for an authentication system AS."
    },
    {
      "heading": "4.4 Experiments",
      "text": "We run experiments to measure the success rate and number of queries for different versions of the approach across different domains. Each experiment measures the authentication system\u2019s decision and number of queries from the scope of both adversary and the principal (i.e., the victim):\n1. Baseline. The baseline experiment establishes metrics when the adversary uses their own unmodified data against the victims.\n2. Naive Replay (host authentication). The adversary exploits assumptions that host authentication systems make about variance in the principal\u2019s authentication stream [4, 9, 36]. The adversary replays one sample of their own data enough times to form the authentication stream, and forms the most effective, yet naive comparison to our attacks.\n3. Naive Random (biometric authentication). The adversary samples x \u223c N (\u00b5, \u03c32) for every feature x \u2208 X in the attack sample. This experiment is the same in both Random Image and VAE Latent Space attacks, such that a potential adversary naively samples random subpixels to create an image.\n4. QuickStart. The QuickStart experiment measures the metrics in Section 4.3, P (M) and number of queries, when only the QuickStart algorithm is used.\n5. QuickStart+LIME. The QuickStart+LIME experiment measures the same metrics as the QuickStart experiment but when QuickStart is combined with the LIME XAI system to enhance perturbations."
    },
    {
      "heading": "4.5 Host Authentication",
      "text": "In Figure 4, the authentication success of each adversary and victim pair is displayed for each experiment. The baseline displays an expected matrix for a proper authentication system, with no false positives and no false negatives. The replay experiment shows the effect of an adversary performing the naive replay attack described in Section 4.2, such that an adversary simply replays one of their samples until they form a valid input stream. We note that in this attack, a potential adversary has P (M) = 100%, but the attack holds little utility because they are still identifiable by the sample passed to the system. The oracle could also easily enforce a minimum level of variance across incoming input streams before performing authentication. Thus, this type of attack is easily mitigated.\nWith the QuickStart attack, adversaries yield successful YES queries and are no longer identified as themselves. Several instances of previously unsuccessful adversary-victim pairs now yield a positive result in favor of the adversary. In total, five of the seven principals in the system were vulnerable to potential adversaries, with one adversary gaining access to two separate identities. The QuickStart+LIME attack yields a lower success rate than QuickStart alone, as shown in Table 1. The number of queries for each are within standard deviation of each other (112.25 \u00b1 41.67 and 133.25 \u00b1 34.64, respectively). We believe the limited feature space of this dataset are limiting factors for LIME\u2019s effectiveness.\nTo further investigate why the QuickStart attack is effective, a victim\u2019s real trace and a successful imposter trace are plotted in Figure 5 to examine their difference in feature distribution. The distribution is binned such that each bin corresponds to the nearest time of an enumeration feature, in milliseconds. Previous work tested robustness of authentication systems by attempting to replay samples with specific feature distributions that mimicked the target [36]. However, Figure 5 reveals that an exact replica of the feature distribution is not needed. The imposter sample has a larger frequency of enumeration timings in the zero millisecond and two millisecond bins, and was still misclassified as the victim. The attacker must simply exploit one blind spot in the decision boundary to launch an attack."
    },
    {
      "heading": "4.6 Biometric Authentication",
      "text": "The performance of the attack varies widely when targeted against Facenet-based classifiers. We first examine each model separately without defenses (row one of each coverage map in Figures 6, 7, and 8, respectively). For all experiments, we take the adversary\u2019s knowledge as 30% of the total registered usernames in the system for ease of presentation (thus ||U || = 50 and ||V || = 15 for an attack against the entire Georgia Tech Face Database [31]). We experimentally observed the same general trends for our attack at lower and higher knowledge levels, and leave the optimization of knowledge level for future work. As discussed in Section 4.3, P (M) is tied to the size of adversary\u2019s knowledge set V . Intuitively, the attacker desires the highest possible coverage of false positives across principals, as this maximizes P (M) when a smaller knowledge set V is taken. For example, if an attack yields only a single false positive for some set U \u2032 of identities, and the adversary takes a smaller subset\nSupport Vector Machine\nU \u2032\u2032 \u2282 U \u2032, such that ||U \u2032\u2032|| << ||U \u2032||, it is unlikely that the subset will yield a successful attack."
    },
    {
      "heading": "4.6.1 Support Vector Machine (Facenet-SVM)",
      "text": "The attack coverage is shown in Figure 6 for the random pixel and VAE attacks. The baseline map for both versions exhibits the expected classifications for an authentication system, confirming the model\u2019s accuracy at test time. The model begins to misclassify when random inputs are used, yielding a high success rate, as observed in Table 2. However, upon closer inspection of Figure 6, every adversary is misclassified as the same victim. In this case, the distribution of false positives is not consistent and gives a low chance for a successful attack if a new subset of victims was chosen. Although calculated P (M) = 87% for QuickStart is lower, false positives are more evenly distributed in the system. This distribution further increases when LIME is used, yielding both high attack coverage and high P (M) of 80%. The VAE latent space attack is very effective and improves upon the random pixel attack. In the VAE attack, QuickStart and QuickStart+LIME perform at the same P (M) of 87%."
    },
    {
      "heading": "4.6.2 Random Forest (Facenet-RF)",
      "text": "The attack coverage against the RF classifier is shown in Figure 7, and exhibits a higher vulnerability in general than SVM. The random attack exhibits the same behavior where adversaries are often classified as the same victim. QuickStart dramatically improves the attack coverage in both the Random Pixel and VAE attacks, with up to 100% success rate as seen in Table 2. Perceptually, QuickStart+LIME has higher coverage for the random pixel attack than for the VAE attack."
    },
    {
      "heading": "4.6.3 Neural Network (Facenet-NN)",
      "text": "As before, the attack is successful against the NN classifier, as shown in Figure 8. Although the random attack yields high success rate, victims are focused on a single identity. The QuickStart attack improves the coverage in both Random Pixel and VAE attacks, yielding P (M) of up 87% in both attack types according to Table 2. The VAE Latent Space does not benefit from LIME as much as the Random Pixel attack, where P (M) increases by 27% with LIME.\nRandom Forest\nNeural Network"
    },
    {
      "heading": "4.7 Defenses",
      "text": "We consider our attack against biometric authentication when defenses derived from the literature [34] are added to the oracle. The selection of defenses is not meant to be exhaustive, but rather examine whether our attack can be mitigated by model-agnostic implementations."
    },
    {
      "heading": "4.7.1 Injecting Random Noise (Random Defense)",
      "text": "Rather than initializing the oracle on the normal training data, we also inject a new class named other to the training set, which consists of images with randomly generated pixel values. The number of images in this class is chosen to match the maximum number of training set images for each principal in the dataset. Thus, when an adversary attempts to launch an attack using such images, they are identified as other and a NO is returned. This defense corresponds to the second row of each coverage map in Figures 6, 7, and 8. The defense completely blocks the naive random attack for every model tested, while also slightly mitigating the effectiveness of the Random Pixel attack. In this scenario, the benefit of LIME is most clear as it offers the best coverage against an oracle with this defense. Since this defense does not target adversarial images from a generative model, it is not effective at blocking the VAE Latent Space attack."
    },
    {
      "heading": "4.7.2 Injecting Random Noise + Fakes (All Defenses)",
      "text": "In addition to training on noisy data, the oracle is also given a class named fake that consists of adversarial samples created by a generative algorithm. This defense must be implemented carefully, as the oracle should still be robust enough to properly classify principals that are partially occluded or blurry. Thus, we devise a defense that will demonstrate whether QuickStart and QuickStart+LIME are producing useful, non-random perturbations to the latent space. The defense is initialized by first using the Facenet-VAE model to encode one of every principal\u2019s images into the latent space. We note that in this scenario, the oracle has the advantage of owning the generative model used by the adversary. If this were not true, the defense would still be valid, so long as the chosen generative model can create fake, yet realistic images of the principals. The oracle takes the maximum and minimum values of the selection\u2019s encodings to create a lower and upper bound of latent variables for each\nprincipal. These bounds are used to randomly sample new latent space vectors, which are decoded to form an adversarial example. Thus, we will observe if perturbations induced by QuickStart and QuickStart+LIME are either meaningful deductions from the adversary\u2019s original data, or simply random samples in the latent space. This defense corresponds to the third row of each coverage map in Figures 6, 7, 8. QuickStart and QuickStart+LIME circumvent defenses for all models, and reduce the adversary\u2019s P (M) by up to 67% in the case of the SVM classifier, which saw a performance drop once LIME was introduced. Overall, LIME tends to improve the attacker\u2019s odds when the feature space is larger. This is expected as LIME is largely data-driven in its implementation, and suffers when a smaller amount of information is available to derive the decision boundary."
    },
    {
      "heading": "4.8 Black-box Attack (Face++ API)",
      "text": "In general, our results suggest that QuickStart is able to circumvent defenses which inject adversarial training data into the oracle. In terms of query count, QuickStart and QuickStart+LIME are within standard deviation of each other. As such, the main benefit of using LIME is a better attack surface, depending on what classification model the oracle uses. We believe the QuickStart+LIME in the VAE Latent Space configuration gives an adversary the highest chance of launching an attack, as in the worst case it will perform slightly worse if the oracle uses an SVM classifier, but better if the model is Random Forest or Neural Network.\nTo verify our results, we target the Face++ face recognition API as described in Section 4.2, providing it the entire Georgia Tech Face Database [31] as training data (as such, ||U || = 50). We use the VAE Latent Space attack with both QuickStart and QuickStart+LIME, to compare their effectiveness on a black-box oracle, assuming the same adversary knowledge of ||V || = 15 from previous experiments. The coverage map for this experiment is shown in Figure 9, and queries from each experiment in Table 3. We note that the naive random attack is not successful, suggesting that Face++ protects against images with randomly generated pixels. The QuickStart and QuickStart+LIME attacks are both successful, with QuickStart+LIME yielding better attack coverage than QuickStart alone, which mimics the earlier experiments against Facenet-NN."
    },
    {
      "heading": "5 Discussion",
      "text": "A unique benefit of using an XAI technique to produce perturbations is the ability to \u201cinterrogate\u201d the substitute model G. By examining the explanations returned by LIME during the adversarial sample crafting process, we can form some intuition of why the attack is successful."
    },
    {
      "heading": "5.1 Explaining Attacks",
      "text": "Figure 10 illustrates an explanation of the top scoring rules in the latent variable encoding L for a successful attack image. We examine if LIME was accurate by perturbing one of the top scoring weights. The top scoring rule, L49 <= \u22128.16, is the rule LIME believes to have the highest weight against the adversary. In order to force a change with only one variable, a large distortion is induced. When the variable is changed from L49 = 2 to L49 = \u221270, the oracle\u2019s decision changes from YES to NO. The new face is perceptually altered, most notably in terms of skin tone and mouth structure. This gives the adversary some insight into how the oracle is making decisions, which is a natural consequence of leveraging XAI techniques."
    },
    {
      "heading": "5.2 Defending with XAI",
      "text": "As noted previously, XAI can be used by the adversary to learn how the oracle makes decisions. However, the oracle is not restricted from using XAI on itself. The oracle may use XAI to examine which features contribute to misclassification, and determine if decisions are being made due to memorization of certain features, or by correctly extrapolating from\nFace++ API\nVAE Latent Space Perturb. Attack Coverage\n0 2 4 6 8 10 12 14\n0 2 4 6 8 10 12 14 Ad ve rs ar y\nBaseline\n0 2 4 6 8 10 12 14\nRandom\n0 2 4 6 8 10 12 14\nQuickStart\n0 2 4 6 8 10 12 14\nQuickStart+LIME\nVictim\nFigure 9: Coverage heatmaps for the Face++ Black-box experiment. Cells denote authentication success for each adversary\u2013victim pair.\nRandom \u2014 QuickStart 163.88 \u00b1 48.35 P (M) = 53% QuickStart 155.11 \u00b1 39.63\n+LIME P (M) = 60%\nTable 3: The average number of adversarial queries before success in each of three experiments, testing against Face++ API at 30% knowledge. P (M) is the calculated probability based on attack coverage for each experiment. Dashes indicate no successful attacks, with P (M) = 0.\nLIME Explanation\nLatent Variable Rules Against Adversary\n\u22120.1 0.0 0.1 0.2 Decision Weight[79\n] < = - 10 .00\n[30 ] > 3. 39\n[87 ] < = 6 .53\n[90 ] < = - 5.2 0\n[11 ] < = - 0.3 5\n[99 ] < = - 14 .87\n[34 ] < = 7 .02\n[55 ] > 9. 64\n[49 ] < = - 8.1 6\nLa te nt V ar ia bl e, R ul e\nYESNONO YES\nthe training data. Note however that the adversary can do the same as soon as it obtains a YES result. For example, the adversary is free to perform the previous experiment, and observe the effect of specific latent variables. Thus, we argue that the oracle can only win if it can generate explanations that are stronger than those deduced by the adversary. This is not unreasonable, as the oracle has white-box access to itself. XAI systems, such as LIME [37], rely on iterative perturbations to discover the decision boundary. It follows that with strong explanations, an oracle can correctly infer the perturbations acting against it, and take action as needed. Future work is necessary in order to better understand the efficacy of this approach when applied as a defensive measure."
    },
    {
      "heading": "5.3 Limitations",
      "text": "The primary limitation in our attack is the restricted amount of data available for G to learn the oracle\u2019s decision boundary. We observe this effect in LIME\u2019s performance as the feature space becomes smaller. Notably, the QuickStart+LIME attack is most effective for the Random Pixel perturbation technique, which uses the entire image as input to G. When training on perturbed VAE latent vectors, which are an order of magnitude smaller, the utility of LIME diminishes. Implementing data augmentation techniques into the system is an immediate improvement for future work, as it allows the adversary to further reduce the number of necessary queries.\nTo the best of our knowledge, we are the first to attempt circumvention of an authentication system while assuming a blind adversary. However, due to the assumption that victim data is secret, it is not possible to target specific principals in the system. Although this is a fair assumption to make in the context of real-world authentication systems, it slightly reduces the attack\u2019s utility. However, we believe the ability to mask an adversary\u2019s malicious acts is still useful and poses a risk to any infrastructures that attempt to mediate interactions between users."
    },
    {
      "heading": "6 Related Work",
      "text": "Radhakrishnan et al. [36] investigated the ability of an attacker to mimic fingerprints, but they did not consider advanced attacks that would infer the classifier\u2019s decision boundary. Such attacks were initially outlined by Barreno et al. [2, 3] and Biggio [5] in terms of an adversary\u2019s knowledge, intended influence, time horizon, and the type of security violation. For example, some attacks might aim to make the classifier mis-classify all future inputs, resulting in denial of service. Another attack could attempt to make gradual changes over a long period of time, successively poisoning each re-training attempt. If the fingerprinting model is rendered unusable, it must be taken offline, nullifying any security benefits it would otherwise offer. In this paper, we focus on only inferring the decision boundary at test-time, rather than trying to change it at train-time.\nMachine learning classifiers designed to defend against adversaries, a technique known as adversarial machine learning, must consider attackers with varying knowledge of the classifier. Huang et al. [22] expanded on the prior work by considering particular domain applications of adversarial machine learning in both e-mail spam detection [32] and anomalous network traffic detection [38]. Attacks in these domains had to take advantage of differences in data distribution, the adversary\u2019s control over specific features, and any assumptions the learning algorithm made about the adversary. This prior work helps lay the framework for an improved fingerprinting model that may encounter adversarial inputs.\nBates et al. [4] and Radhakrishnan et al. [36] explored several possible classifiers in their fingerprinting systems. Adversarial versions for some of these have been proposed, such as work by Dalvi [10] in the form of an adversary-aware Naive Bayes classifier. This technique incorporated the cost of new training samples with respect to how much it affected the decision boundary. In the realm of worm software detection, Newsome et al. [33] proposed defenses against the Red Herring attack, where unnecessary \u201cchaff\u201d features are used to mislead the worm classifier. Zhou et al. [51] proposed AD-SVM, an adversary-aware support vector machine (SVM), which is built against attacks that target either all possible feature modifications, or only certain data points. Although Zhou et al. show that this proposed SVM classifier is more robust, classification performance degrades when the adversary\u2019s attacks are weak. We build our own versions of these defenses to investigate their utility against a blind attacker, and compare their accuracy. Papernot et al. [34] use previously defined adversarial sample crafting algorithms [17, 35] to create an attack model capable of making deep neural network (DNN) oracles mis-classify the majority of image inputs. Unlike previous work by Barreno et al. [3], the target classifier is treated as a black-box model, with the only feedback being classified image labels. When an oracle also gives confidence values, Trame\u0300r et al. [44] show it is possible to replicate an oracle\u2019s decision tree with near-perfect accuracy. These two latter works are of key interest, as they target the final classifier techniques chosen by Bates et al. and Radhakrishnan et al., particularly the\ndecision tree algorithm and neural networks. Although these previous attacks are useful against classification models in general, they rely on class label feedback from the model. For an authentication system, we must assume only a binary classification result, such that positive target results are secret. Regardless, we take inspiration from these previous works to build our own adversarial sample crafting algorithm.\nModern face detection is built atop the foundational object detection framework by Viola and Jones, which uses an \u201cintegral image\u201d representation to quickly evaluate facial features [45]. Face detection supports a number of applications, including face alignment, clustering, recognition, and verification. Specifically, we focus on facial verification systems. Like fingerprinting systems, facial verification is vulnerable to targeted attacks by an adversary. Fredrikson et al. [14] showed that images can be recovered from API access to facial recognition services, using only the model\u2019s output. By contrast, the adversary in our setting perturbs its inputs without recovering the face of target principals. In a hill-climbing attack [15, 16], synthetic templates are iteratively modified based on similarity score until reaching the verification threshold, whereas our approach does not directly iterate on feedback in this way. Goswami et al. [18] break verification models using grid-based occlusion and face-level distortions, while Papernot et al. [34] show that randomly perturbing even a few pixels can cause misclassification. The work by Goswami et al. is the most similar to ours, as they introduce distortions or occlusions into face images to induce a misclassification. However, Goswami et al. do not consider a constrained adversary who is limited in queries. In such a case, the adversary can not only induce image processing distortions, but also construct entirely new faces using generative machine learning models, which we show is more effective (and realistic) than standard image processing distortions.\nEarly XAI techniques sought to determine why opaque neural networks made decisions by investigating their parameters and extracting rules. Towell and Shavlik [43] extracted rules from neural networks with performance that closely resembled that of the original model. Later, Breiman [8] explained random forests by randomly selecting features and finding those with the highest influence, while Martens et al. [27] followed a similar approach to Towell and Shavlik, extracting rules from SVMs. These techniques eventually evolved to display more meaningful interpretations. Although Mozina et al. [30] focused on the set of the most influential features in the model, they displayed them graphically using a nomogram, along with a visualization of the confidence intervals. Modern XAI techniques, such as LIME [37], extend this graphical approach with the use of iterative perturbations in the style of adversarial sample crafting algorithms [17, 35] to create human readable, globally interpretable decision explanations. We parse these explanations to learn a concept of a potential victim\u2019s feature space. Not only are perturbations effective, they also give some insight into potentially vulnerable features.\nTo date, there has been very little investigation of XAI techniques to address security applications. The only work we are aware of in this vein is very recent work by Guo et al. [19], who consider explainable techniques for deep learning-based security applications. These types of applications, which make substantial use of RNNs, present challenges that are considerably different from authentication; they are focused on long sequences, where the sequentially-oriented nature of RNNs are advantageous. By contrast, authentication is a binary decision and lends itself well to XAI approaches such as LIME."
    },
    {
      "heading": "7 Conclusion",
      "text": "Model-based authentication schemes are powerful methods for establishing identity in environments requiring some form of resource mediation. However, these schemes can be broken if the adversary is able to sufficiently query the oracle and learn the distinction between themselves and the other principals in the system. This paper explores the effectiveness of such an approach, and shows that feature distribution is not necessarily tied to an attacker\u2019s success. We also show that XAI is an intuitive and effective technique by which adversaries can infer decision boundaries from a victim model."
    },
    {
      "heading": "Acknowledgements",
      "text": "We thank Nicolas Papernot for his helpful comments. This work was partially supported by the US National Science Foundation under grant CNS-1540217 and by ARO under contract number W911NF-1-0405."
    }
  ],
  "title": "Explainable Black-Box Attacks Against Model-based Authentication",
  "year": 2018
}
