{"abstractText": "Many text classification applications require models with satisfying performance as well as good interpretability. Traditional machine learning methods are easy to interpret but have low accuracies. The development of deep learning models boosts the performance significantly. However, deep learning models are typically hard to interpret. In this work, we propose interpretable capsule networks (iCapsNets) to bridge this gap. iCapsNets use capsules to model semantic meanings and explore novel methods to increase interpretability. The design of iCapsNets is consistent with human intuition and enables it to produce human-understandable interpretation results. Notably, iCapsNets can be interpreted both locally and globally. In terms of local interpretability, iCapsNets offer a simple yet effective method to explain the predictions for each data sample. On the other hand, iCapsNets explore a novel way to explain the model\u2019s general behavior, achieving global interpretability. Experimental studies show that our iCapsNets yield meaningful local and global interpretation results, without suffering from significant performance loss compared to non-interpretable methods.", "authors": [{"affiliations": [], "name": "Zhengyang Wang"}, {"affiliations": [], "name": "Xia Hu"}, {"affiliations": [], "name": "Shuiwang Ji"}], "id": "SP:f4e7aed65358ad1619e94f9be806d19806788d65", "references": [{"authors": ["X. Zhang", "J. Zhao", "Y. LeCun"], "title": "Character-level convolutional networks for text classification", "venue": "Advances in Neural Information Processing Systems, 2015, pp. 649\u2013657.", "year": 2015}, {"authors": ["M. Du", "N. Liu", "X. Hu"], "title": "Techniques for interpretable machine learning", "venue": "Communications of the ACM, vol. 63, no. 1, pp. 68\u201377, 2019.", "year": 2019}, {"authors": ["M. Du", "N. Liu", "F. Yang", "X. Hu"], "title": "Learning credible deep neural networks with rationale regularization", "venue": "arXiv preprint arXiv:1908.05601, 2019.", "year": 1908}, {"authors": ["M. Du", "N. Liu", "Q. Song", "X. Hu"], "title": "Towards explanation of dnnbased prediction with guided feature inversion", "venue": "Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2018, pp. 1358\u20131367.", "year": 2018}, {"authors": ["F. Yang", "S.K. Pentyala", "S. Mohseni", "M. Du", "H. Yuan", "R. Linder", "E.D. Ragan", "S. Ji", "X. Hu"], "title": "Xfake: explainable fake news detector with visualizations", "venue": "The World Wide Web Conference, 2019, pp. 3600\u20133604.", "year": 2019}, {"authors": ["K. Shu", "L. Cui", "S. Wang", "D. Lee", "H. Liu"], "title": "defend: Explainable fake news detection", "venue": "Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2019, pp. 395\u2013405. IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X, MAY 2020 10", "year": 2019}, {"authors": ["T. Joachims"], "title": "Text categorization with support vector machines: Learning with many relevant features", "venue": "European Conference on Machine Learning. Springer, 1998, pp. 137\u2013142.", "year": 1998}, {"authors": ["T. Mikolov", "K. Chen", "G. Corrado", "J. Dean"], "title": "Efficient estimation of word representations in vector space", "venue": "arXiv preprint arXiv:1301.3781, 2013.", "year": 2013}, {"authors": ["T. Mikolov", "I. Sutskever", "K. Chen", "G.S. Corrado", "J. Dean"], "title": "Distributed representations of words and phrases and their compositionality", "venue": "Advances in Neural Information Processing Systems, 2013, pp. 3111\u20133119.", "year": 2013}, {"authors": ["T. Mikolov", "W.-t. Yih", "G. Zweig"], "title": "Linguistic regularities in continuous space word representations", "venue": "Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2013, pp. 746\u2013751.", "year": 2013}, {"authors": ["S. Hochreiter", "J. Schmidhuber"], "title": "Long short-term memory", "venue": "Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.", "year": 1997}, {"authors": ["I. Sutskever", "O. Vinyals", "Q.V. Le"], "title": "Sequence to sequence learning with neural networks", "venue": "Advances in Neural Information Processing Systems, 2014, pp. 3104\u20133112.", "year": 2014}, {"authors": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "title": "Neural machine translation by jointly learning to align and translate", "venue": "International Conference on Learning Representations, 2015.", "year": 2015}, {"authors": ["Y. Bengio", "R. Ducharme", "P. Vincent", "C. Jauvin"], "title": "A neural probabilistic language model", "venue": "Journal of Machine Learning Research, vol. 3, no. Feb, pp. 1137\u20131155, 2003.", "year": 2003}, {"authors": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "title": "Natural language processing (almost) from scratch", "venue": "Journal of Machine Learning Research, vol. 12, no. Aug, pp. 2493\u2013 2537, 2011.", "year": 2011}, {"authors": ["T. Mikolov", "M. Karafi\u00e1t", "L. Burget", "J. \u010cernock\u1ef3", "S. Khudanpur"], "title": "Recurrent neural network based language model", "venue": "Eleventh Annual Conference of the International Speech Communication Association, 2010.", "year": 2010}, {"authors": ["J. Chung", "C. Gulcehre", "K. Cho", "Y. Bengio"], "title": "Empirical evaluation of gated recurrent neural networks on sequence modeling", "venue": "NIPS 2014 Workshop on Deep Learning, December 2014, 2014.", "year": 2014}, {"authors": ["J. Pennington", "R. Socher", "C. Manning"], "title": "Glove: Global vectors for word representation", "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, 2014, pp. 1532\u2013 1543.", "year": 2014}, {"authors": ["C. dos Santos", "M. Gatti"], "title": "Deep convolutional neural networks for sentiment analysis of short texts", "venue": "Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, 2014, pp. 69\u201378.", "year": 2014}, {"authors": ["A. Conneau", "H. Schwenk", "L. Barrault", "Y. Lecun"], "title": "Very deep convolutional networks for text classification", "venue": "European Chapter of the Association for Computational Linguistics, 2017.", "year": 2017}, {"authors": ["Y. Xiao", "K. Cho"], "title": "Efficient character-level document classification by combining convolution and recurrent layers", "venue": "arXiv preprint arXiv:1602.00367, 2016.", "year": 2016}, {"authors": ["Y. Wu", "M. Schuster", "Z. Chen", "Q.V. Le", "M. Norouzi", "W. Macherey", "M. Krikun", "Y. Cao", "Q. Gao", "K. Macherey"], "title": "Google\u2019s neural machine translation system: Bridging the gap between human and machine translation", "venue": "arXiv preprint arXiv:1609.08144, 2016.", "year": 2016}, {"authors": ["R. Johnson", "T. Zhang"], "title": "Effective use of word order for text categorization with convolutional neural networks", "venue": "Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2015, pp. 103\u2013112.", "year": 2015}, {"authors": ["C. Qiao", "B. Huang", "G. Niu", "D. Li", "D. Dong", "W. He", "D. Yu", "H. Wu"], "title": "A new method of region embedding for text classification", "venue": "International Conference on Learning Representations, 2018.", "year": 2018}, {"authors": ["D. Tang", "B. Qin", "T. Liu"], "title": "Document modeling with gated recurrent neural network for sentiment classification", "venue": "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, 2015, pp. 1422\u20131432.", "year": 2015}, {"authors": ["D. Yogatama", "C. Dyer", "W. Ling", "P. Blunsom"], "title": "Generative and discriminative text classification with recurrent neural networks", "venue": "International Conference on Machine Learning. International Machine Learning Society, 2017.", "year": 2017}, {"authors": ["R. Johnson", "T. Zhang"], "title": "Semi-supervised convolutional neural networks for text categorization via region embedding", "venue": "Advances in Neural Information Processing Systems, 2015, pp. 919\u2013927.", "year": 2015}, {"authors": ["\u2014\u2014"], "title": "Deep pyramid convolutional neural networks for text categorization", "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, vol. 1, 2017, pp. 562\u2013570.", "year": 2017}, {"authors": ["N. Kalchbrenner", "E. Grefenstette", "P. Blunsom"], "title": "A convolutional neural network for modelling sentences", "venue": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, 2014.", "year": 2014}, {"authors": ["Y. Kim"], "title": "Convolutional neural networks for sentence classification", "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, 2014, pp. 1746\u20131751.", "year": 2014}, {"authors": ["Z. Yang", "D. Yang", "C. Dyer", "X. He", "A. Smola", "E. Hovy"], "title": "Hierarchical attention networks for document classification", "venue": "Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2016, pp. 1480\u20131489.", "year": 2016}, {"authors": ["A. Vaswani", "N. Shazeer", "N. Parmar", "J. Uszkoreit", "L. Jones", "A.N. Gomez", "\u0141. Kaiser", "I. Polosukhin"], "title": "Attention is all you need", "venue": "Advances in Neural Information Processing Systems, 2017, pp. 5998\u2013 6008.", "year": 2017}, {"authors": ["J. Devlin", "M.-W. Chang", "K. Lee", "K. Toutanova"], "title": "Bert: Pretraining of deep bidirectional transformers for language understanding", "venue": "arXiv preprint arXiv:1810.04805, 2018.", "year": 1810}, {"authors": ["A. Rai"], "title": "Explainable ai: from black box to glass box", "venue": "Journal of the Academy of Marketing Science, vol. 48, no. 1, pp. 137\u2013141, 2020.", "year": 2020}, {"authors": ["A. Joulin", "E. Grave", "P. Bojanowski", "T. Mikolov"], "title": "Bag of tricks for efficient text classification", "venue": "Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics, vol. 2, 2017, pp. 427\u2013431.", "year": 2017}, {"authors": ["L. Kopitar", "L. Cilar", "P. Kocbek", "G. Stiglic"], "title": "Local vs. global interpretability of machine learning models in type 2 diabetes mellitus screening", "venue": "Artificial Intelligence in Medicine: Knowledge Representation and Transparent and Explainable Systems. Springer, 2019, pp. 108\u2013119.", "year": 2019}, {"authors": ["S. Sabour", "N. Frosst", "G.E. Hinton"], "title": "Dynamic routing between capsules", "venue": "Advances in Neural Information Processing Systems, 2017, pp. 3859\u20133869.", "year": 2017}, {"authors": ["G.E. Hinton", "A. Krizhevsky", "S.D. Wang"], "title": "Transforming autoencoders", "venue": "International Conference on Artificial Neural Networks. Springer, 2011, pp. 44\u201351.", "year": 2011}, {"authors": ["Z. Wang", "S. Ji"], "title": "Learning convolutional text representations for visual question answering", "venue": "Proceedings of the 2018 SIAM International Conference on Data Mining. SIAM, 2018, pp. 594\u2013602.", "year": 2018}, {"authors": ["L. Xiao", "H. Zhang", "W. Chen", "Y. Wang", "Y. Jin"], "title": "Mcapsnet: Capsule network for text with multi-task learning", "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, 2018, pp. 4565\u20134574.", "year": 2018}, {"authors": ["M. Yang", "W. Zhao", "J. Ye", "Z. Lei", "Z. Zhao", "S. Zhang"], "title": "Investigating capsule networks with dynamic routing for text classification", "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, 2018, pp. 3110\u20133119.", "year": 2018}, {"authors": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M. Bernstein"], "title": "Imagenet large scale visual recognition challenge", "venue": "International Journal of Computer Vision, vol. 115, no. 3, pp. 211\u2013252, 2015.", "year": 2015}, {"authors": ["L. v. d. Maaten", "G. Hinton"], "title": "Visualizing data using t-sne", "venue": "Journal of Machine Learning Research, vol. 9, no. Nov, pp. 2579\u20132605, 2008.", "year": 2008}, {"authors": ["H. Yuan", "Y. Chen", "X. Hu", "S. Ji"], "title": "Interpreting deep models for text analysis via optimization and regularization methods", "venue": "AAAI-19: Thirty-Third AAAI Conference on Artificial Intelligence. Association for the Advancement of Artificial Intelligence (AAAI), 2019.", "year": 2019}], "sections": [{"text": "Index Terms\u2014Interpretability, capsule networks.\nF"}, {"heading": "1 INTRODUCTION", "text": "Text classification is an important task in natural language processing (NLP) research. With different predefined categorical labels, models for text classification have various applications, including sentiment analysis, topic categorization, and ontology extraction [1]. A considerable body of efforts have been devoted to developing machine learning models for text classification and many successful models have been studied. However, as many practical applications raise the requirement for interpretable models [2], [3], [4], [5], [6], existing models have not achieved a good trade-off between accuracy and interpretability.\nTraditional text classifiers typically rely on statistical methods like bag-of-words and bag-of-n-grams [7]. By simply counting the occurrences of words or n-grams and applying machine learning methods like support vector machines, these methods have achieved some success. However, without semantic understanding of words or n-grams, the success is limited.\nThe development of distributed representations [8], [9], [10] provides an effective way to model semantic meanings of words through word embeddings. It has motivated applications of deep learning models on many NLP tasks [11], [12], [13], [14], [15], [16], [17]. Pre-trained word embeddings, like word2vec [9] and GloVe [18], are made publicly available to accelerate the research of deep learning on NLP. In addition, other levels of text embeddings, such as character embeddings [1], [19], [20], [21], sub-word embeddings [22], and region embeddings [23], [24], have also been explored.\nBased on these embeddings, deep learning models based on recurrent neural networks (RNNs) [25], [26] and convolutional neural networks (CNNs) [23], [27], [28], [29], [30]\n\u2022 Zhengyang Wang, Xia Hu, and Shuiwang Ji are with the Department of Computer Science and Engineering, Texas A&M University, College Station, TX, 77843. E-mail: sji@tamu.edu\nManuscript received May, 2020.\nhave been extensively studied. While variants of RNNs, such as long short-term memory (LSTM) [11] and gated recurrent unit (GRU) [17], are known to be effective for processing sequential data like text, many studies have shown that CNNs are comparable with RNNs on NLP tasks. The attention mechanism [13] is another important model. On one hand, combining it with CNNs and RNNs results in significant performance boost [22], [31]. On the other hand, the attention mechanism can be used as an alternative to CNNs and RNNs to build deep learning models and set the record on various NLP tasks [32], [33]. In terms of the accuracy, deep learning models usually outperform traditional text classifiers. However, complex deep learning models work like black boxes and are hard to interpret [34].\nDistributed representations can also be combined with interpretable traditional machine learning method, resulting in simpler yet effective models. For example, FastText [35] combines the traditional bag-of-words method with word embeddings, and uses the linear regression to perform classification. It achieves comparable classification accuracies with complex deep learning models and is much more efficient in terms of memory usage and computation speed. While the linear regression models can be interpreted, FastText applies an average/sum operation to generate sentence embeddings from word embeddings, preventing it from telling which words are more important.\nIn this work, we focus on developing interpretable deep learning models for text classification. Specifically, there are kinds of interpretabilities in terms of explaining machine learning models [2], [36]. The first one is the local interpretability, represents the ability of explaining why a specific decision is made with respect to a specific data sample. In contrast, the global interpretability, refers to the ability of showing how the model works generally, with respect to the whole dataset. We aim at achieving both kinds of interpretabilities at the same time, without hurting the performance significantly.\nar X\niv :2\n00 6.\n00 07\n5v 1\n[ cs\n.C L\n] 1\n6 M\nay 2\n02 0\nIn this work, we develop a novel deep learning model for text classification, which can be interpreted both locally and globally. Specifically, we extend the CapsNets [37] from computer vision tasks to text classification tasks, and make the following contributions:\n\u2022 We propose interpretable capsule networks (iCapsNets) for text classification. To the best of our knowledge, this is the first work that achieves both local and global interpretability on CapsNets. \u2022 In iCapsNets, interpretation of classification results with respect to each data sample as well as the model\u2019s general behavior can be both obtained through novel, simple yet effective ways. \u2022 Experimental results show that our iCapsNets yield meaningful interpretation results while having competitive accuracies compared to non-interpretable models, achieving a better trade-off between accuracy and interpretability."}, {"heading": "2 ICAPSNETS", "text": "In this section, we first discuss the intuition behind the design of iCapsNets in Section 2.1. Then we introduce the overall architecture of iCapsNets in Section 2.2, followed by details of important layers in Sections 2.3, 2.4 and 2.5. Then we explain how to perform local and global interpretation of iCapsNets in Section 2.6."}, {"heading": "2.1 From CapsNets to iCapsNets", "text": "CapsNets [37] employ capsules as the input and output of a layer and proposes the dynamic routing algorithm to perform the computation between capsules. A capsule is a vector, whose elements are the instantiation parameters of a specific type of entity. CapsNets were designed for computer vision tasks, where entities may refer to objects or object parts. In CapsNets, capsules are always normalized by a non-linear Squash function so that the norm lies in [0, 1). The norm of a capsule, in turn, represents the probability that the corresponding entity is present in the input image. A capsule is said to be activated if the norm is close\nto 1. To understand how a capsule layer works, suppose we are given capsules that represent low-level entities; e.g., eyes, ears, and nose in the task of face detection. A capsule layer employs the dynamic routing algorithm to compute capsules that represent high-level entities, which can be faces in this case. Concretely, if low-level capsules indicate that two eyes and one nose are detected in the image, the dynamic routing algorithm is able to decide whether they belong to the same face and choose to activate the high-level capsule for face accordingly.\nWhile convolutional neural networks (CNNs) have become the dominant approach for classification tasks, CapsNets have two main advantages over CNNs. First, using vector-output capsules avoids the exponential inefficiencies incurred by replicating scalar-output feature detectors on a grid [38]. Second, the dynamic routing algorithm is more effective than max-pooling, and allows neurons in one layer to ignore all but the most active feature detector in a local pool in the layer below [37]. Replacing stacked convolutional layers and max-pooling layers in CNNs with capsule layers results in the more efficient and effective CapsNets.\nIn this work, we extend CapsNets to text classification tasks and develop iCapsNets. As it is necessary for accurate text classification models to semantically understand texts, we propose to use capsules to represent different semantic meanings. Intuitively, there is a clear hierarchy between semantic meanings in text classification tasks. Taking topic categorization as an example, detecting certain basketballrelated phrases will strongly suggest that a document be categorized into the \u201csports\u201d topic. Here, \u201cbasketball\u201d is a low-level semantic entity as opposed to \u201csports\u201d. With such intuitions, iCapsNets are designed to first capture low-level semantic entities (primary capsules) from the entire input texts and then use the dynamic routing algorithm to compute high-level semantic entities (class capsules), which correspond to predefined classes. Note that, given a sentence or document to classify, prior deep learning models usually generate a single sentence or document vector embedding which is fed into classifiers. In contrast, iCapsNets produce several primary capsules from the sentence or document, where each primary capsule focuses on capturing a specific\nsemantic meaning."}, {"heading": "2.2 The Architecture of iCapsNets", "text": "An illustration of the overall architecture of our iCapsNets is provided in Figure 1. As introduced above, the top layer of iCapsNets is a capsule layer, whose inputs are I dpdimensional primary capsules (pc) representing I distinct low-level semantic meanings extracted from the whole input texts. The outputs are J dc-dimensional class capsules (cc) representing high-level semantic meanings, where J corresponds to the number of classes of the task. We apply the original dynamic routing algorithm [37]. In addition, we propose a weight sharing regularization to improve the efficiency and interpretability, which is discussed in detail in Section 2.4.\nIn the following discussions, we assume that the input to iCapsNets is a single sentence of N words. For short documents, it is reasonable to concatenate the sentences into a single one. We discuss how to adapt iCapsNets for long documents in Section 2.5.\nTo generate the primary capsules from input texts, iCapsNets start with a word embedding layer. While using character embeddings may lead to higher accuracies [1], it treats text as a kind of raw signal at character level, which is not helpful in achieving human-understandable interpretation results. The input sentence is transformed into a sequence of word embeddings e1, e2, \u00b7 \u00b7 \u00b7 , eN \u2208 Rde through the word embedding layer. To be specific, given a vocabulary of size V built according to the training dataset, the word embedding layer is essentially a V \u00d7 he look-up table [15]. Here, he is the dimension of word embedding space.\nA 1-D convolutional layer with a kernel size of K then transforms the word embeddings e1, e2, \u00b7 \u00b7 \u00b7 , eN into region embeddings w1,w2, \u00b7 \u00b7 \u00b7 ,wN \u2208 Rdw . Region embeddings, also known as phrase embeddings or N-grams embeddings, have shown to be effective in various deep learning models for NLP tasks [23], [27], [28], [30], [39] due to the use of word order information. We apply appropriate zero paddings to keep the number of embeddings.\nTo obtain primary capsules pc1,pc2, \u00b7 \u00b7 \u00b7 ,pcI from region embeddings w1,w2, \u00b7 \u00b7 \u00b7 ,wN , we propose to use a trainable multi-head attention layer. Many different attention layers have been studied for NLP tasks [13], [22], [31], [32]. Yang et al. [31] used a learnable head vector, instead of a vector from another sources, to attend different positions. Multi-head self-attention was introduced in Vaswani et al. [32], enabling multiple joint views of inputs. Inspired by these studies, we propose the trainable multihead attention layer, which employ multiple trainable head vectors to perform the attention independently. Each head vector will lead to a primary capsule, which is supposed to represent one specific semantic meaning after training. In iCapsNets, we have I different head vectors, corresponding to I primary capsules. In the following sections, we will demonstrate how the trainable multi-head attention layer is suitable to work with the capsule layer and helps interpreting the primary capsules."}, {"heading": "2.3 Trainable Multi-head Attention Layer", "text": "In iCapsNets, the trainable multi-head attention layer actually transforms tensors into capsules [37]. To achieve\nsuch transformation from scalar-output feature detectors to vector-output capsules, prior studies [37], [40], [41] simply group convolutional scalar outputs into vectors to form primary capsules, in order to replicate learned knowledge across space and keep positional information. However, this transformation results in a large number of capsules encoding duplicate information as well as inactive capsules during the computation. It hurts the efficiency of the capsule layer. Moreover, as the total number of primary capsules increases as the spatial sizes of inputs increase, the number of training parameters in the capsule layer becomes excessive when the spatial sizes are large, as explained in Section 2.4. It prohibits the use of CapsNets on largescale datasets, like those built by Zhang et al. [1] and the ImageNet dataset [42]. Applying our trainable multi-head attention layer effectively addresses this problem, since the number of primary capsules is fixed as a hyperparameter of the model.\nAs illustrated in Figure 1, the trainable multihead attention layer takes region embeddings w1,w2, \u00b7 \u00b7 \u00b7 ,wN \u2208 Rdw as inputs. To produce I primary capsules pc1,pc2, \u00b7 \u00b7 \u00b7 ,pcI \u2208 Rdp , there are I trainable head vectors h1,h2, \u00b7 \u00b7 \u00b7 ,hI \u2208 Rdq . We name these head vectors as primary capsule queries. For each hi \u2208 Rdq ,m \u2208 [1, I], the attention mechanism determines region embeddings that are important to a specific sentencelevel semantic meaning and aggregates them accordingly to form a primary capsule pci. Specifically, the attention procedure is\nfor n = 1, 2, \u00b7 \u00b7 \u00b7 , N do vin = W v iwn (1)\nkin = W k i wn (2) \u03b1in = exp(hTi k i n/ \u221a dq)\u2211\nn\u2032 exp(h T i k i n\u2032/ \u221a dq)\n(3)\nwhere Wvi \u2208 Rdp\u00d7dw and Wki \u2208 Rdq\u00d7dw , and the aggregation is achieved by a weighted summation:\npci = \u2211 n \u03b1inv i n. (4)\nNote that for one primary capsule query hi, Wvi and W k i are shared for every region embedding. Here, Wvi and W k i represent linear transformations that map region embeddings to a different embedding space for attention. Trained jointly with the primary capsule query hi, they are supposed to provide appropriate views of region embeddings with the focus on a specific semantic meaning. Consequently, kin \u2208 Rdq , n = 1, 2, \u00b7 \u00b7 \u00b7 , N serve as the attention keys while vin \u2208 Rdp , n = 1, 2, \u00b7 \u00b7 \u00b7 , N serve as the attention values that are used to form the primary capsule pci \u2208 Rdp . The coefficients \u03b1in indicate whether wn is informative in generating pci. In Section 2.6, we use \u03b1 i n to perform local interpretation. Note that Eq. (3) is equivalent to a Softmax operation. And we can easily infer that\u2211 n \u03b1in = 1, (5)\nwhich indicates that in the attention mechanism, inputs compete with each other for their contributions to outputs.\nAlgorithm 1 Dynamic Routing Algorithm [37] 1: procedure ROUTING(p\u0302cj|i, r, l) 2: for all capsule i in layer l and capsule j in layer (l + 1): bij \u2190 0\n3: for r iterations do 4: for all capsule i in layer l: \u03b2ij \u2190 exp(bij)\u2211\nj exp(bij) 5: for all capsule j in layer (l + 1): sj \u2190 \u2211\ni \u03b2ijp\u0302cj|i 6: for all capsule j in layer (l + 1): ccj \u2190 Squash(sj) 7: for all capsule i in layer l and capsule j in layer (l+1): bij \u2190 bij + p\u0302cj|i \u00b7 ccj 8: end for 9: return ccj\nIntuitively, it means that only important parts of inputs go through the attention layer. Information that is irrelevant to the semantic meanings represented by primary capsules is discarded and only useful information is retained. In Section 2.4, we can see that, in the dynamic routing algorithm of the capsule layer, outputs compete with each other for receiving inputs. A class capsule gets activated only when receiving agreements from multiple active primary capsules. Our iCapsNets use the capsule layer after the attention layer, since they have complementary functionalities, i.e., the attention layer filters information and the capsule layer makes full use of the filtered information.\nTo conclude, using the trainable multi-head attention layer to transforms tensors into capsules is not only efficient but also technically sound. Moreover, our trainable multihead attention layer provides a simple way to interpret primary capsules, which leads to global interpretability of iCapsNets. We illustrate the interpretation method in Section 2.6."}, {"heading": "2.4 Capsule Layer", "text": "Algorithm 1 shows the original dynamic routing algorithm [37]. The Squash function is used to normalize the capsules:\nSquash(x) = ||x||2 1 + ||x||2 x ||x|| , (6)\nwhere x is a capsule, i.e., a vector. Note that the inputs to the algorithm is not the original primary capsules. Before the routing, we perform linear transformations on primary capsules to produce \u201cprediction vectors\u201d [37]. To be specific, for each pair of a primary capsule pci \u2208 Rdp and a class capsule ccj \u2208 Rdc , we compute\np\u0302cj|i = W\u0302ijpci + b\u0302ij , (7)\nwhere W\u0302ij \u2208 Rdc\u00d7dp and b\u0302ij \u2208 Rdc . For I primary capsules and J class capsules, it results in I \u00d7 J \u00d7 (dc \u00d7 dp + dc) training parameters, which are excessive when the number of primary capsules is large. Using our trainable multihead attention layer addresses this problem by limiting the number of primary capsules. However, a more direct solution is to have W\u0302ij shared across primary capsules, which means\np\u0302cj|i = W\u0302jpci + b\u0302ij , (8)\nwhere W\u0302j is shared for every pci. iCapsNets employ Eq. (8) to improve the efficiency. More importantly, we find that the weight sharing casts a regularization effect on primary capsule queries in our trainable multi-head attention layer, which is shown in Section 4.2.1.\nThe dynamic routing algorithm computes weights \u03b2ij between every pair of a primary capsule pci and a class capsule ccj in an iterative way. The process is visualized in Figure 2. By comparing the routing weights \u03b2ij , we can see that, after r = 3 iterations, a primary capsule may contribute much more to one of the class capsules than the others. This primary capsule is usually activated with a norm close to 1 and serves as a strong support to a class capsule. In another case, a primary capsule may contribute similarly to each class capsule. It means that either the primary capsule has a norm close to 0 or it captures a semantic meaning that is not helpful to classification. Therefore, it is reasonable to use the the routing weights \u03b2ij to explore the local interpretability of iCapsNets, as explained in Section 2.6.\nTo support the statement in Section 2.3, we point out that line 4 in Algorithm 1 corresponds to a Softmax operation, which indicates \u2211\nj\n\u03b2ij = 1, (9)\nwhich is opposite to Eq. (5) in terms of normalizing weights across inputs or outputs. As mentioned above, Eq. (5) and Eq. (9) show a complementary relationship between the trainable multi-head attention layer and the capsule layer."}, {"heading": "2.5 iCapsNets for Long Documents", "text": "In the discussions above, we assume that the input to iCapsNets is a single sentence of N words, leading to a hierarchical word-region-sentence architecture. While it is reasonable for short documents, removing the assumption and adding a document level to the hierarchy usually results in a performance boost for long documents [25], [31]. Thus, we propose the trainable multi-head hierarchical attention\nlayer to adapt iCapsNets for long documents, as illustrated in Figure 3.\nConsider a document of M sentences, where each sentence has N words. For each sentence, we use the same word embedding layer and 1-D convolutional layer to obtain region embeddings wmn \u2208 Rdw , m = 1, 2, \u00b7 \u00b7 \u00b7 ,M , n = 1, 2, \u00b7 \u00b7 \u00b7 , N . Our trainable multi-head hierarchical attention layer still has I primary capsule queries h1,h2, \u00b7 \u00b7 \u00b7 ,hI \u2208 Rdq , corresponding to I primary capsules pc1,pc2, \u00b7 \u00b7 \u00b7 ,pcI \u2208 Rdp . For each hi, it performs two levels of attention procedures:\nfor m = 1, 2, \u00b7 \u00b7 \u00b7 ,M do for n = 1, 2, \u00b7 \u00b7 \u00b7 , N do\nvimn = W v iwmn, k i mn = W k i wmn (10) \u03b1imn = exp(hTi k i mn/ \u221a dq)\u2211\nn\u2032 exp(h T i k i mn\u2032/ \u221a dq)\n(11)\nsim = \u2211 n \u03b1imnv i mn (12) v\u0303im = W\u0303 v i s i m, k\u0303 i m = W\u0303 k i s i m (13) \u03c1im = exp(hTi k\u0303 i m/ \u221a dq)\u2211\nm\u2032 exp(h T i k\u0303 i m\u2032/ \u221a dq)\n(14)\npci = \u2211 m \u03c1imv\u0303 i m (15)\nwhere sim \u2208 Rds , Wvi \u2208 Rds\u00d7dw , Wki \u2208 Rdq\u00d7dw , W\u0303vi \u2208 Rdp\u00d7ds , and W\u0303ki \u2208 Rdq\u00d7ds . Basically, we first apply the same attention layer on each sentence independently and obtain M sentence embeddings si1, s i 2, \u00b7 \u00b7 \u00b7 , siM \u2208 Rds . These sentence embeddings focus on the semantic meaning that pci aims to capture. Next, we use an attention layer on si1, s i 2, \u00b7 \u00b7 \u00b7 , siM to determine which sentences are more informative and aggregate them to produce pci. The same procedure is applied for each hi. Note that in the two levels of attention procedures, we employ the same set of trainable head vectors h1,h2, \u00b7 \u00b7 \u00b7 ,hI .\nWe denote the iCapsNets in Figure 1 as iCapsNetsShort and the ones with the trainable multi-head hierarchi-\ncal attention layer as iCapsNetsLong . In the experiments, iCapsNetsLong achieve significantly better accuracies for tasks on long documents."}, {"heading": "2.6 Interpreting iCapsNets", "text": "iCapsNets can be interpreted both globally and locally [2]. We first describe how to perform the local interpretation, i.e., explaining why the classification is made given a specific data sample. Note that, for the trainable multi-head attention layer and the capsule layer, each output is a weighted sum of all inputs, leading to a fully-connected pattern between inputs and outputs. However, unlike regular fullyconnected layers where weights are fixed after training, weights in the trainable multi-head attention layer and the capsule layer are input-dependent. This property is crucial to achieving good local interpretation results. Specifically, after feeding a data sample into iCapsNets, we can extract parts of inputs that are important to the classification results, by simply examining large weights in iCapsNets. An illustration of the local interpretation process of iCapsNets is provided in Fig. 4. Suppose iCapsNets categorize a data sample into class j \u2208 [1, J ]. We first obtain the top k1 largest values from routing weights \u03b21j , \u03b22j , \u00b7 \u00b7 \u00b7 , \u03b2Ij in the capsule\nlayer. As explained in Section 2.4, a large \u03b2ij means that the i-th primary capsule strongly support the j-th class capsule. Thus, the top k1 largest weights indicate that the corresponding k1 primary capsules are important to the prediction. Next, for each of these k1 sentence capsules, we check the top k2 largest values among weights in the trainable multi-head attention layer. For example, if pci is one of the k1 primary capsules, we select the top k2 largest values from \u03b1i1, \u03b1 i 2, \u00b7 \u00b7 \u00b7 , \u03b1iN . As pointed out in Section 2.3, large weights indicate that the corresponding inputs are informative in generating pci. The process gives us k1 \u00d7 k2 K-grams, serving as the explanation for the classification result with respect to the input data sample. Here, K is the kernel size of the 1-D convolutional layer. These Kgrams may have overlapping words. The more number of times a word appears, the more important that word is. For iCapsNetsLong with the trainable multi-head hierarchical attention layer, the local interpretation process is similar. For an important pci, we first select the largest weight from \u03c1i1, \u03c1 i 2, \u00b7 \u00b7 \u00b7 , \u03c1iM , say \u03c1im\u2217 . Then we pick the top k2 largest values from \u03b1im\u22171, \u03b1 i m\u22172, \u00b7 \u00b7 \u00b7 , \u03b1im\u2217N . The remaining parts are the same. The global interpretation means interpreting semantically meaningful components in the model. It demonstrates how the model works generally with respect to the whole dataset. In iCapsNets, we attempt to determine the semantic meanings captured by each primary capsule to explore the global interpretability. First, we count the number of times when a primary capsule has the largest routing weight to the class capsule corresponding to the predicted class. Concretely, we maintain a frequency matrix C = [cji] \u2208 NJ\u00d7I where cji is initialized to be 0. For every data sample in the testing dataset, we performs the local interpretation described above with k1 = k2 = 1. If it is classified into the j-th class and \u03b2ij is the largest routing weight among \u03b21j , \u03b22j , \u00b7 \u00b7 \u00b7 , \u03b2Ij , we let cji \u2190 cji + 1. Meanwhile, for each cij , we maintain a list of words in the resulted K-gram. We find the simple statistical method gives good global interpretation results, as shown in Section 4.2. The final C shows a sparse pattern; that is, only a few values in C are large. And the most frequent words can indicate the semantic meaning captured by primary capsules.\nBoth local and global interpretabilities of iCapsNets are achieved using simple methods. In the experiments, iCapsNets show a good trade-off between accuracy and interpretability."}, {"heading": "3 EXPERIMENTAL STUDIES", "text": "We perform thorough experiments to evaluate and analyze iCapsNets. First, we demonstrate the local and global interpretability of iCapsNets. Then, in terms of classification accuracy, we compare iCapsNets with several text classification baselines which are not interpretable. Notably, iCapsNets achieve a good trade-off between interpretability and accuracy."}, {"heading": "3.1 Datasets", "text": "We conduct experiments on 7 publicly available large-scale datasets built by Zhang et al. [1]. These datasets cover\ndifferent text classification tasks, such as sentiment analysis, topic categorization, and ontology extraction. Table 1 is a detailed summary of these datasets. In particular, without loss of generality, we demonstrate the local and global interpretability of iCapsNets using examples from the AG\u2019s News dataset and the Yahoo! Answers dataset."}, {"heading": "4 EXPERIMENTAL SETUPS", "text": "We introduce detailed experimental setups for reproducibility. Code is also provided 1.\nThe word embedding layer of iCapsNets involves the step of generating a vocabulary. The size of the vocabulary V is determined by the training set and a minimum frequency F . Specifically, if a word appears more than F times in the training set, it is included in the vocabulary. In iCapsNets, each word embedding is composed of two parts. The first part is the 300-dimensional pre-trained word2vec [9] and is fixed during training. The second part has dimension (de \u2212 300) and is randomly initialized and trained. The 1-D convolutional layer with a kernel size of K transforms the de-dimensional word embeddings into dw-dimensional region embeddings. For the trainable multihead attention layer in iCapsNetsShort, we let the dimension of each primary capsule query be equal to that of each primary capsule, i.e., dq = dp. For the trainable multi-head hierarchical attention layer in iCapsNetsLong , the dimension of the intermediate sentence embeddings is also set to be equal to dp, i.e., dq = ds = dp. In addition, we set the number of primary capsules I to be dw/dp. The number of class capsules J depends on the number of classes. The dimension of a class capsule is dc. For iCapsNetsShort, the input is a single sentence. We use zero paddings to make all the inputs have the same number of words N for large batch training. For iCapsNetsLong , the input is a document. We also apply zero paddings so that all the inputs have the same number of sentences M and each sentence has the same number of words N . Table 2 and 3 provide our best settings of these hyperparameters of iCapsNetsShort and iCapsNetsLong for each dataset, respectively.\nAs the outputs of iCapsNets are class capsules, the predictions are made based on their norms. That is, the class capsule with the largest norm corresponds to the predicted class. To train iCapsNets, we apply the margin loss proposed by Sabour et al. [37]. To be specific, for each class capsule ccj , j = 1, 2, \u00b7 \u00b7 \u00b7 , J , a separate loss function is given by\nLj =Ij\u2217(j)max(0,m + \u2212 ||ccj ||)2\n+ \u03bb(1\u2212 Ij\u2217(j))max(0, ||ccj || \u2212m\u2212)2, (16)\nwhere m+ = 0.9, m\u2212 = 0.1, \u03bb = 0.5, and Ij\u2217(j) is an indicator function defined as\nIj\u2217(j) =\n{ 1, if j = j\u2217\n0, if j 6= j\u2217 , (17)\nwhere j\u2217 is the index of the true label. The total loss is the sum of the loss function of all the class capsules. With the margin loss, the Adam optimizer is used to train iCapsNets. For iCapsNetsShort, the learning rate is set to 0.0001 for the\n1. https://www.dropbox.com/s/ev06l6x7ddy9pgb/iCapsNet.zip\nTABLE 2 Hyperparameter settings of iCapsNetsShort on the 7 datasets from Zhang et al. [1]. Explanations of these hyperparameters are provided in Section 4.\nDataset V F de K dw dq = dp dc N Yelp Review Polarity 25,102 50 300+64 5 512 16 32 1,296 Yelp Review Full 27,729 50 300+64 5 512 16 32 1,438 Yahoo! Answers 35,194 100 300+64 5 512 16 32 1,000 Amazon Review Polarity 33,207 200 300+64 5 512 16 32 592 Amazon Review Full 17,534 500 300+64 5 512 16 32 592 AG\u2019s News 30,794 5 300+32 3 256 8 16 195 DBPedia 26,141 50 300+64 5 256 8 16 1,588\nTABLE 3 Hyperparameter settings of iCapsNetsLong on the 7 datasets from Zhang et al. [1]. Explanations of these hyperparameters are provided in Section 4.\nDataset V F de K dw dq = ds = dp dc M N Yelp Review Polarity 77,202 5 300+64 5 512 16 32 20 100 Yelp Review Full 82,814 5 300+64 5 512 16 32 20 100 Yahoo! Answers 131,081 10 300+32 5 512 16 32 15 100 Amazon Review Polarity 155,192 10 300+64 5 512 16 32 15 100 Amazon Review Full 142,375 10 300+64 5 512 16 32 15 100 AG\u2019s News 30,794 5 300+32 3 256 8 16 10 86 DBPedia 26,141 50 300+64 5 256 8 16 10 100\nAG\u2019s News and DBPedia datasets and 0.001 for the other 5 datasets. For iCapsNetsLong , the learning rate is set to 0.0005 for all the 7 dataset."}, {"heading": "4.1 Local Interpretation Results", "text": "In order to demonstrate the local interpretability of iCapsNets, we show concrete examples of local interpretation results obtained through the interpretation method as described in Section 2.6. Specifically, we train iCapsNetsShort on the AG\u2019s News dataset and iCapsNetsLong on the Yahoo!\nAnswers dataset, respectively. Then we take examples from the testing set to perform prediction and local prediction.\nFigures 5 and 6 provide examples of local interpretation results of iCapsNets on AG\u2019s News and Yahoo! Answers datasets, respectively. We can observe that, for each data sample, the extracted words well justify why iCapsNets make the predictions, no matter the predictions are correct or not."}, {"heading": "4.2 Global Interpretation Results", "text": "We perform global interpretation for the same iCapsNetsShort on the AG\u2019s News dataset and iCapsNetsLong on the Yahoo! Answers dataset, respectively. To be specific, we first visualize the frequency matrix C as introduced in Section 2.6. The visualizations are provided in Figures 7 and 8. We can see that in both cases, C shows a sparse pattern. Note that the i-th column corresponds to the primary capsule pci, or equivalently the primary capsule query hi. Thus, for pci, we can check the lists of frequent words corresponding to ci1, ci2, \u00b7 \u00b7 \u00b7 , ciJ and use the most frequent words to interpret pci. Interpreting primary capsules leads to explanation on how iCapsNets work generally, i.e. achieving the global interpretability."}, {"heading": "4.2.1 Visualization of Primary Capsule Queries", "text": "We further visualize the sparse pattern of the primary capsule queries in the embedding space. Specifically, we perform t-SNE visualization [43] of the primary capsule queries h1,h2, \u00b7 \u00b7 \u00b7 ,h32. Fig. 9 shows the visualization of iCapsNetsShort trained on the AG\u2019s News dataset. It is observed that the primary capsule queries distribute sparsely on the plane, indicating that they capture different semantic meanings under the same embedding space."}, {"heading": "4.3 Classification Results", "text": "Last, we show that iCapsNets are able to achieve competitive results compared to non-interpretable models."}, {"heading": "4.3.1 Baselines", "text": "We select several popular supervised text classification models as baselines to show that iCapsNets can achieve competitive accuracies. In terms of deep learning models, we compare iCapsNets with the word-level convolutional model (word-CNN) [30] and character-level convolutional model (char-CNN) [1]. Comparisons with two variants of char-CNN, the character-level convolutional recurrent model (char-CNN+RNN) [21] and the very deep characterlevel convolutional model (char-VDCNN) [20], are also conducted. In addition, iCapsNets are compared with the discriminative LSTM model (D-LSTM) [26]. FastText [35] combines distributed representations of words with traditional model BoW and gets improved by using the wordcontext region embeddings (W.C.region.emb) and contextword region embeddings (C.W.region.emb) [24]. We report the accuracies of these baselines from Zhang et al. [1] and Qiao et al. [24]. As iCapsNets are based on CapsNets [37], we also include CapsNets as baselines. CapsNets have been investigated on text classification [41]. However, due to the efficiency problem discussed in Section 2.3, original CapsNets can only work well on small datasets. Therefore, only the accuracy on the AG\u2019s News dataset is available."}, {"heading": "4.3.2 Results", "text": "Table 4 summarizes the classification results of all models. In terms of test accuracies, iCapsNets outperforms all the baselines on 4 of the 7 datasets. On the other 3 datasets, iCapsNets achieve competitive results.\nDeep learning models based on RNNs, like charCNN+RNN and D-LSTM, are usually hard to interpret as RNNs process texts sequentially and do not tell which parts of the sequence are informative. The interpretability of CNN models with word embeddings has been studied [44]. However, the interpretation process is computational expensive. And only local interpretability has been explored. Applying character embeddings usually improve the accuracies. However, as pointed out by Conneau et al. [20], the models may process a sentence as a stream of signals, which we can not understand semantically.\nFastText, W.C.region.emb, and C.W.region.emb combine distributed representations with traditional model BoW. They are efficient and effective on text classification tasks. However, an average/sum operation is employed to gener-\nate sentence embeddings from word or region embeddings, making the model not interpretable.\nAs interpretable models usually suffer from the performance loss [2], the classification performance of iCapsNets is strong considering its interpretability."}, {"heading": "5 CONCLUSIONS", "text": "In this work, we aim to develop a deep learning model that achieves a good trade-off between accuracy and interpretability on text classification tasks. Based on our insights on capsules, we propose the interpretable capsule network (iCapsNets) by employing attention mechanism and adapting CapsNets [37] from computer vision tasks to text classification tasks. We provide novel, simple yet effective way to interpret our iCapsNets. In particular, iCapsNets achieve the local and global interpretability at the same time. Experimental results show that our iCapsNets yield human-understandable interpretation results, without suffering from significant performance loss compared to non-interpretable models."}, {"heading": "ACKNOWLEDGMENTS", "text": "This work was supported in part by National Science Foundation grant IIS-1908198 and Defense Advanced Research Projects Agency grant N66001-17-2-4031."}], "title": "iCapsNets: Towards Interpretable Capsule Networks for Text Classification", "year": 2020}