{
  "abstractText": "Machine learning has recently been widely adopted to address the managerial decision making problems, in which the decision maker needs to be able to interpret the contributions of individual attributes in an explicit form. However, there is a trade-off between performance and interpretability. Full complexity models (such as neural network-based models) are non-traceable black-box, whereas classic interpretable models (such as logistic regression) are usually simplified with lower accuracy. This trade-off limits the application of state-ofthe-art machine learning models in management problems, which requires high prediction performance, as well as the understanding of individual attributes\u2019 contributions to the model outcome. Multiple criteria decision aiding (MCDA) is a family of analytic approaches to depicting the rationale of human decision. It is also limited by strong assumptions (e.g. preference independence). To meet the decision maker\u2019s demand for more interpretable machine learning models, we propose a novel hybrid method, namely Neural Network-based Multiple Criteria Decision Aiding (NN-MCDA), which combines an additive value model and a fully-connected multilayer perceptron (MLP) to achieve good performance while capturing the explicit relationships between individual attributes and the prediction. NN-MCDA has a linear component (in an additive form of a set of polynomial functions) to characterize such relationships through providing explicit marginal value functions, and a nonlinear component (in a standard MLP form) to capture the implicit high-order interactions between attributes and their complex nonlinear transformations. We demonstrate the effectiveness of NN-MCDA with extensive simulation studies and three real-world datasets. \u2217Corresponding author Email addresses: mengzhguo2-c@my.cityu.edu.hk (Mengzhuo Guo), qingpeng.zhang@cityu.edu.hk (Qingpeng Zhang), liaoxiuwu@mail.xjtu.edu.cn (Xiuwu Liao), youhchen@cityu.edu.hk (Frank Youhua Chen), dajun.zeng@ia.ac.cn (Daniel Dajun Zeng) Preprint submitted to Omega October 28, 2019 ar X iv :1 90 6. 01 23 3v 3 [ cs .L G ] 2 5 O ct 2 01 9 To the best of our knowledge, this research is the first to enhance the interpretability of machine learning models with MCDA techniques. The proposed framework also sheds light on how to use machine learning techniques to free MCDA from strong assumptions.",
  "authors": [
    {
      "affiliations": [],
      "name": "Mengzhuo Guo"
    },
    {
      "affiliations": [],
      "name": "Qingpeng Zhangb"
    },
    {
      "affiliations": [],
      "name": "Xiuwu Liao"
    },
    {
      "affiliations": [],
      "name": "Frank Youhua Chen"
    },
    {
      "affiliations": [],
      "name": "Daniel Dajun Zeng"
    }
  ],
  "id": "SP:61d3d0d512b98aa5a72076f9d314afc7ea0da166",
  "references": [
    {
      "authors": [
        "M. Aggarwal",
        "A. Fallah Tehrani"
      ],
      "title": "Modelling human decision behaviour with preference learning",
      "venue": "INFORMS Journal on Computing",
      "year": 2019
    },
    {
      "authors": [
        "G.S. Alexopoulos"
      ],
      "title": "Depression in the elderly",
      "venue": "The Lancet",
      "year": 2005
    },
    {
      "authors": [
        "S. Angilella",
        "S. Corrente",
        "S. Greco",
        "R. S lowi\u0144ski"
      ],
      "title": "MUSA-INT: Multicriteria customer satisfaction analysis with interacting criteria",
      "venue": "Omega",
      "year": 2014
    },
    {
      "authors": [
        "S. Angilella",
        "S. Greco",
        "B. Matarazzo"
      ],
      "title": "Non-additive robust ordinal regression: A multiple criteria decision model based on the Choquet integral",
      "venue": "European Journal of Operational Research",
      "year": 2010
    },
    {
      "authors": [
        "B. Baesens",
        "R. Setiono",
        "C. Mues",
        "J. Vanthienen"
      ],
      "title": "Using neural network rule extraction and decision tables for credit-risk evaluation",
      "venue": "Management Science",
      "year": 2003
    },
    {
      "authors": [
        "M. Barbati",
        "S. Greco",
        "M. Kadzi\u0144ski",
        "R. S lowi\u0144ski"
      ],
      "title": "Optimization of multiple satisfaction levels in portfolio decision analysis",
      "venue": "Omega",
      "year": 2018
    },
    {
      "authors": [
        "W.R. Beardslee",
        "D.A. Brent",
        "V.R. Weersing",
        "G.N. Clarke",
        "G. Porta",
        "S.D. Hollon",
        "T.R. Gladstone",
        "R. Gallop",
        "F.L. Lynch",
        "S Iyengar"
      ],
      "title": "Prevention of depression in at-risk adolescents: Longer-term effects",
      "venue": "JAMA Psychiatry",
      "year": 2013
    },
    {
      "authors": [
        "D. Blazer",
        "B. Burchett",
        "C. Service",
        "L.K. George"
      ],
      "title": "The association of age and depression among the elderly: An epidemiologic exploration",
      "venue": "Journal of Gerontology",
      "year": 1991
    },
    {
      "authors": [
        "D.A. Brent",
        "S.M. Brunwasser",
        "S.D. Hollon",
        "V.R. Weersing",
        "G.N. Clarke",
        "J.F. Dickerson",
        "W.R. Beardslee",
        "T.R. Gladstone",
        "G. Porta",
        "Lynch",
        "F.L"
      ],
      "title": "Effect of a cognitive-behavioral prevention program on depression",
      "year": 2015
    },
    {
      "authors": [
        "D. Bugliari",
        "N. Campbell",
        "C. Chan",
        "O. Hayden",
        "M. Hurd",
        "R. Main",
        "J. Mallett",
        "C. McCullough",
        "E. Meijer",
        "M Moldoff"
      ],
      "title": "RAND HRS data documentation, version P. RAND Center for the Study of Aging",
      "year": 2016
    },
    {
      "authors": [
        "R. Caruana",
        "Y. Lou",
        "J. Gehrke",
        "P. Koch",
        "M. Sturm",
        "N. Elhadad"
      ],
      "title": "Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission",
      "venue": "in: Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
      "year": 2015
    },
    {
      "authors": [
        "H.T. Cheng",
        "L. Koc",
        "J. Harmsen",
        "T. Shaked",
        "T. Chandra",
        "H. Aradhye",
        "G. Anderson",
        "G. Corrado",
        "W. Chai",
        "M Ispir"
      ],
      "title": "Wide and deep learning for recommender systems",
      "venue": "in: Proceedings of the 1st Workshop on Deep Learning for Recommender Systems,",
      "year": 2016
    },
    {
      "authors": [
        "K. Ciomek",
        "V. Ferretti",
        "M. Kadzi\u0144ski"
      ],
      "title": "Predictive analytics and disused railways requalification: Insights from a post factum analysis perspective",
      "venue": "Decision Support Systems",
      "year": 2018
    },
    {
      "authors": [
        "R. lowi\u0144ski"
      ],
      "title": "Robust ordinal regression in preference learning and ranking",
      "venue": "Machine Learning",
      "year": 2013
    },
    {
      "authors": [
        "D. Cui",
        "D. Curry"
      ],
      "title": "Prediction in marketing using the support vector machine",
      "venue": "Marketing Science",
      "year": 2005
    },
    {
      "authors": [
        "C. Dong",
        "L. Sanchez",
        "R. Price"
      ],
      "title": "Relationship of obesity to depression: A family-based study",
      "venue": "International Journal of Obesity",
      "year": 2004
    },
    {
      "authors": [
        "M. Doumpos",
        "C. Zopounidis"
      ],
      "title": "Preference disaggregation and statistical learning for multicriteria decision support: A review",
      "venue": "European Journal of Operational Research",
      "year": 2011
    },
    {
      "authors": [
        "J.S. Dyer",
        "P.C. Fishburn",
        "R.E. Steuer",
        "J. Wallenius",
        "S. Zionts"
      ],
      "title": "Multiple criteria decision making, multiattribute utility theory: The next ten years",
      "venue": "Management Science",
      "year": 1992
    },
    {
      "authors": [
        "J.H. Friedman"
      ],
      "title": "Greedy function approximation: A gradient boosting machine",
      "venue": "Annals of Statistics ,",
      "year": 2001
    },
    {
      "authors": [
        "J. Garber",
        "G.N. Clarke",
        "V.R. Weersing",
        "W.R. Beardslee",
        "D.A. Brent",
        "T.R. Gladstone",
        "L.L. DeBar",
        "F.L. Lynch",
        "E. DAngelo",
        "Hollon",
        "S.D"
      ],
      "title": "Prevention of depression in at-risk adolescents: A randomized controlled trial",
      "year": 2009
    },
    {
      "authors": [
        "D. Gartner",
        "R. Kolisch",
        "D.B. Neill",
        "R. Padman"
      ],
      "title": "Machine learning approaches for early DRG classification and resource allocation",
      "venue": "INFORMS Journal on Computing",
      "year": 2015
    },
    {
      "authors": [
        "M. Ghaderi",
        "F. Ruiz",
        "N. Agell"
      ],
      "title": "A linear programming approach for learning non-monotonic additive value functions in multiple criteria decision aiding",
      "venue": "European Journal of Operational Research",
      "year": 2017
    },
    {
      "authors": [
        "X. Glorot",
        "A. Bordes",
        "Y. Bengio"
      ],
      "title": "Deep sparse rectifier neural networks",
      "venue": "in: Proceedings of the 14th International Conference on Artificial Intelligence and Statistics,",
      "year": 2011
    },
    {
      "authors": [
        "S. Greco",
        "V. Mousseau",
        "R. S lowi\u0144ski"
      ],
      "title": "Ordinal regression revisited: Multiple criteria ranking using a set of additive value functions",
      "venue": "European Journal of Operational Research",
      "year": 2008
    },
    {
      "authors": [
        "D. Gunning"
      ],
      "title": "Explainable artificial intelligence (XAI). Defense Advanced Research Projects Agency (DARPA), nd Web URL: https://www.darpa. mil/attachments/XAIProgramUpdate.pdf",
      "year": 2017
    },
    {
      "authors": [
        "M. Guo",
        "X. Liao",
        "J. Liu",
        "Q. Zhang"
      ],
      "title": "Consumer preference analysis: A data-driven multiple criteria approach integrating online information",
      "venue": "Omega URL: https://doi.org/10.1016/j.omega.2019.05.010",
      "year": 2019
    },
    {
      "authors": [
        "M. Hasan",
        "\u0130.E. B\u00fcy\u00fcktahtak\u0131n",
        "E. Elamin"
      ],
      "title": "A multi-criteria ranking algorithm (mcra) for determining breast cancer therapy",
      "venue": "Omega",
      "year": 2019
    },
    {
      "authors": [
        "T. Hastie",
        "R. Tibshirani"
      ],
      "title": "Generalized additive models",
      "venue": "Statistical Science",
      "year": 1986
    },
    {
      "authors": [
        "E. Jacquet-Lagreze",
        "Y. Siskos"
      ],
      "title": "Preference disaggregation: 20 years of MCDA experience",
      "venue": "European Journal of Operational Research",
      "year": 2001
    },
    {
      "authors": [
        "M. Kadzi\u0144ski",
        "M. Cinelli",
        "K. Ciomek",
        "S.R. Coles",
        "M.N. Nadagouda",
        "R.S. Varma",
        "K. Kirwan"
      ],
      "title": "Co-constructive development of a green chemistrybased model for the assessment of nanoparticles synthesis",
      "venue": "European Journal of Operational Research",
      "year": 2018
    },
    {
      "authors": [
        "R.L. Keeney"
      ],
      "title": "A group preference axiomatization with cardinal utility",
      "venue": "Management Science",
      "year": 1976
    },
    {
      "authors": [
        "R.C. Kessler",
        "M. Essex"
      ],
      "title": "Marital status and depression: The importance of coping resources",
      "venue": "Social Forces",
      "year": 1982
    },
    {
      "authors": [
        "P.J. Korhonen",
        "K. Silvennoinen",
        "J. Wallenius",
        "A. \u00d6\u00f6rni"
      ],
      "title": "Can a linear value function explain choices? An experimental study",
      "venue": "European Journal of Operational Research",
      "year": 2012
    },
    {
      "authors": [
        "M. Kraus",
        "S. Feuerriegel"
      ],
      "title": "Forecasting remaining useful life: Interpretable deep learning approach via variational bayesian inferences",
      "venue": "Decision Support Systems",
      "year": 2019
    },
    {
      "authors": [
        "K. Ladin"
      ],
      "title": "Risk of late-life depression across 10 European Union countries: Deconstructing the education effect",
      "venue": "Journal of Aging and Health",
      "year": 2008
    },
    {
      "authors": [
        "Q.V. Le",
        "J. Ngiam",
        "A. Coates",
        "A. Lahiri",
        "B. Prochnow",
        "A.Y. Ng"
      ],
      "title": "On optimization methods for deep learning",
      "venue": "in: Proceedings of the 28th International Conference on International Conference on Machine Learning,",
      "year": 2011
    },
    {
      "authors": [
        "B. Letham",
        "C. Rudin",
        "T.H. McCormick",
        "D Madigan"
      ],
      "title": "Interpretable classifiers using rules and Bayesian analysis: Building a better stroke prediction model",
      "venue": "The Annals of Applied Statistics",
      "year": 2015
    },
    {
      "authors": [
        "D. Li",
        "Zhang",
        "D.j",
        "Shao",
        "J.j",
        "Qi",
        "X.d",
        "L. Tian"
      ],
      "title": "A meta-analysis of the prevalence of depressive symptoms in Chinese older adults",
      "venue": "Archives of Gerontology and Geriatrics",
      "year": 2014
    },
    {
      "authors": [
        "R. lowi\u0144ski"
      ],
      "title": "Preference disaggregation within the regularization framework for sorting problems with multiple potentially non-monotonic criteria",
      "venue": "European Journal of Operational Research",
      "year": 2019
    },
    {
      "authors": [
        "Y. Lou",
        "R. Caruana",
        "J. Gehrke"
      ],
      "title": "Intelligible models for classification and regression",
      "venue": "in: Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
      "year": 2012
    },
    {
      "authors": [
        "Y. Lou",
        "R. Caruana",
        "J. Gehrke",
        "G. Hooker"
      ],
      "title": "Accurate intelligible models with pairwise interactions",
      "venue": "in: Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
      "year": 2013
    },
    {
      "authors": [
        "F.S. Luppino",
        "L.M. de Wit",
        "P.F. Bouvy",
        "T. Stijnen",
        "P. Cuijpers",
        "B.W. Penninx",
        "F.G. Zitman"
      ],
      "title": "Overweight, obesity, and depression: A systematic review and meta-analysis of longitudinal studies",
      "venue": "Archives of General Psychiatry",
      "year": 2010
    },
    {
      "authors": [
        "T. Miller"
      ],
      "title": "Explanation in artificial intelligence: Insights from the social sciences",
      "venue": "Artificial Intelligence",
      "year": 2018
    },
    {
      "authors": [
        "J. 38 Mirowsky",
        "C.E. Ross"
      ],
      "title": "Age and depression",
      "venue": "Journal of Health and Social Behavior ,",
      "year": 1992
    },
    {
      "authors": [
        "S. Moro",
        "P. Cortez",
        "P. Rita"
      ],
      "title": "A data-driven approach to predict the success of bank telemarketing",
      "venue": "Decision Support Systems",
      "year": 2014
    },
    {
      "authors": [
        "S.A. Murrell",
        "S. Himmelfarb",
        "K. Wright"
      ],
      "title": "Prevalence of depression and its correlates in older adults",
      "venue": "American Journal of Epidemiology",
      "year": 1983
    },
    {
      "authors": [
        "L.I. Pearlin",
        "J.S. Johnson"
      ],
      "title": "Marital status, life-strains and depression",
      "year": 1977
    },
    {
      "authors": [
        "R. Pelissari",
        "M. Oliveira",
        "S.B. Amor",
        "A. Kandakoglu",
        "A. Helleno"
      ],
      "title": "SMAA methods and their applications: A literature review and future research directions",
      "venue": "Annals of Operations",
      "year": 2019
    },
    {
      "authors": [
        "B.W. Penninx",
        "J.M. Guralnik",
        "L. Ferrucci",
        "E.M. Simonsick",
        "D.J. Deeg",
        "R.B. Wallace"
      ],
      "title": "Depressive symptoms and physical decline in communitydwelling older persons",
      "year": 1998
    },
    {
      "authors": [
        "L.S. Radloff"
      ],
      "title": "The use of the Center for Epidemiologic Studies Depression Scale in adolescents and young adults",
      "venue": "Journal of Youth and Adolescence",
      "year": 1991
    },
    {
      "authors": [
        "J. Rezaei"
      ],
      "title": "Best-worst multi-criteria decision-making method: Some properties and a linear model",
      "venue": "Omega",
      "year": 2016
    },
    {
      "authors": [
        "M.T. Ribeiro",
        "S. Singh",
        "C. Guestrin"
      ],
      "title": "Why should I trust you? explaining the predictions of any classifier",
      "venue": "in: Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
      "year": 2016
    },
    {
      "authors": [
        "F. Rosenblatt"
      ],
      "title": "The perceptron: A probabilistic model for information storage and organization in the brain",
      "venue": "Psychological Review",
      "year": 1958
    },
    {
      "authors": [
        "C.E. Ross"
      ],
      "title": "Overweight and depression",
      "venue": "Journal of Health and Social Behavior ,",
      "year": 1994
    },
    {
      "authors": [
        "O. Sobrie",
        "N. Gillis",
        "V. Mousseau",
        "M. Pirlot"
      ],
      "title": "UTA-poly and UTAsplines: Additive value functions with polynomial marginals",
      "venue": "European Journal of Operational Research",
      "year": 2018
    },
    {
      "authors": [
        "T.J. Stewart"
      ],
      "title": "A critical survey on the status of multiple criteria decision making theory and practice",
      "venue": "Omega 20,",
      "year": 1992
    },
    {
      "authors": [
        "T.J. Stewart"
      ],
      "title": "Use of piecewise linear value functions in interactive multicriteria decision support: A Monte Carlo study",
      "venue": "Management Science",
      "year": 1993
    },
    {
      "authors": [
        "J. Wallenius",
        "J.S. Dyer",
        "P.C. Fishburn",
        "R.E. Steuer",
        "S. Zionts",
        "K. Deb"
      ],
      "title": "Multiple criteria decision making, multiattribute utility theory: Recent accomplishments and what lies ahead",
      "venue": "Management Science",
      "year": 2008
    },
    {
      "authors": [
        "C. Zopounidis",
        "E. Galariotis",
        "M. Doumpos",
        "S. Sarri",
        "K. Andriosopoulos"
      ],
      "title": "Multiple criteria decision aiding for finance: An updated bibliographic survey",
      "venue": "European Journal of Operational Research",
      "year": 2015
    }
  ],
  "sections": [
    {
      "text": "Machine learning has recently been widely adopted to address the managerial decision making problems, in which the decision maker needs to be able to interpret the contributions of individual attributes in an explicit form. However, there is a trade-off between performance and interpretability. Full complexity models (such as neural network-based models) are non-traceable black-box, whereas classic interpretable models (such as logistic regression) are usually simplified with lower accuracy. This trade-off limits the application of state-ofthe-art machine learning models in management problems, which requires high prediction performance, as well as the understanding of individual attributes\u2019 contributions to the model outcome. Multiple criteria decision aiding (MCDA) is a family of analytic approaches to depicting the rationale of human decision. It is also limited by strong assumptions (e.g. preference independence). To meet the decision maker\u2019s demand for more interpretable machine learning models, we propose a novel hybrid method, namely Neural Network-based Multiple Criteria Decision Aiding (NN-MCDA), which combines an additive value model and a fully-connected multilayer perceptron (MLP) to achieve good performance while capturing the explicit relationships between individual attributes and the prediction. NN-MCDA has a linear component (in an additive form of a set of polynomial functions) to characterize such relationships through providing explicit marginal value functions, and a nonlinear component (in a standard MLP form) to capture the implicit high-order interactions between attributes and their complex nonlinear transformations. We demonstrate the effectiveness of NN-MCDA with extensive simulation studies and three real-world datasets.\n\u2217Corresponding author Email addresses: mengzhguo2-c@my.cityu.edu.hk (Mengzhuo Guo),\nqingpeng.zhang@cityu.edu.hk (Qingpeng Zhang), liaoxiuwu@mail.xjtu.edu.cn (Xiuwu Liao), youhchen@cityu.edu.hk (Frank Youhua Chen), dajun.zeng@ia.ac.cn (Daniel Dajun Zeng)\nPreprint submitted to Omega October 28, 2019\nar X\niv :1\n90 6.\n01 23\n3v 3\n[ cs\n.L G\n] 2\nTo the best of our knowledge, this research is the first to enhance the interpretability of machine learning models with MCDA techniques. The proposed framework also sheds light on how to use machine learning techniques to free MCDA from strong assumptions.\nKeywords: Decision analysis, Business analytics, Predictive modeling, Big data analytics, Machine learning, Multiple criteria decision analysis"
    },
    {
      "heading": "1. Introduction",
      "text": "Machine learning has recently been widely adopted to address challenging decision making problems in a variety of managerial contexts such as marketing (Cui and Curry, 2005), credit-risk evaluation (Baesens et al., 2003) and healthcare management (Gartner et al., 2015). Many machine learning models, such as support vector machines (SVMs) (Cortes and Vapnik, 1995), boosted trees (Friedman, 2001) and neural network-based deep learning methods (LeCun et al., 2015), are capable of handling high-dimensional data because of the high complexity of the model. However, it comes at the expense of interpretability (Lou et al., 2012).\nThe application of machine learning in management research could benefit from the interpretability of models. In practice, the decision maker (DM) needs to be able to characterize the contributions of attributes in an explicit form. Such interpretability can make the model trustworthy to the DM (Ribeiro et al., 2016), help DM understand the causality (Miller, 2018), and improve the model through incorporating DM\u2019s domain knowledge (Aggarwal and Fallah Tehrani, 2019). Therefore, technology giants like Google, IBM, and Microsoft, have been investigating on the techniques in enhancing the model interpretability recently. As stated in a comprehensive overview conducted by David Gunning, the program manager in the Information Innovation Office (I2O) of the Defense Advanced Research Projects Agency (DARPA), \u201cmachine learning models are opaque, non-intuitive and difficult for people to understand\u201d (Gunning, 2017). DARPA has since funded for developing interpretable machine learning techniques among academics. In the latest budget plan of DARPA, explainable artificial intelligence (XAI) has been listed as the key funding area in the fiscal year 2019-2020, with the total amount of 26.05 million US dollars 1."
    },
    {
      "heading": "1.1. The need for interpretable models in management problems.",
      "text": "In many management problems, the ability in understanding the contributions of individual attributes to the prediction outcome is heavily needed (Guo et al., 2019; Barbati et al., 2018). An explicit form of such contributions, for instance giving a value function that describes the detailed relationship between each attribute and outcome, can help the DM exploit and correct the obtained\n1https://www.darpa.mil/about-us/budget\npatterns and rules with prior knowledge, facilitate the downstream managerial decision making, and incorporate critical domain knowledge (Miller, 2018; Ribeiro et al., 2016; Doumpos and Zopounidis, 2011; Lou et al., 2012). The resulted interpretable model can provide in-depth understanding of the data and patterns (Aggarwal and Fallah Tehrani, 2019). In practice, model interpretability is as important as (if not more important than) accuracy in many managerial decision making tasks such as clinical diagnoses, in which the understanding of how the model makes the prediction is the key to facilitate physicians to trust the model and utilize the prediction results (Caruana et al., 2015). For example, the risk of depression is usually assumed to be positively correlated with the age (Blazer et al., 1991). However, the strength of the correlation could vary over the course of aging (Li et al., 2014). A simple regression analysis assumes that such correlation is consistent, which sometimes simplify the real scenarios. If we can provide the physicians with a value function that characterizes the age\u2019s contribution on the risk with a more explicit and concrete form, the predictions seem to be more convincing. In this study, by interpretability we mean that the model capacity to provide such explicit value functions characterizing the contributions of each individual attributes (Lou et al., 2012)."
    },
    {
      "heading": "1.2. Multiple criteria decision aiding",
      "text": "Multiple criteria decision aiding (MCDA) has been a fast growing area of operational research during the last several decades (Dyer et al., 1992; Stewart, 1992; Wallenius et al., 2008; Ciomek et al., 2018). It involves a finite set of alternatives (e.g. actions, items, policies) that are evaluated from a set of conflicting multiple criteria or attributes2. The DM\u2019s decision is driven by his/her underlying global value (utility) function (Keeney, 1976). This global value measures the DM\u2019s desirability for an alternative and can be disaggregated into a set of per-attribute marginal value functions that represent the DM\u2019s evaluation of the corresponding attribute. These marginal value functions can be learned by the DM\u2019s judgments on learning examples (e.g. pairwise comparison between two alternatives). Once the marginal value functions are deciphered, we can understand the decision making rationale, based on which we can predict the judgment of the DM. This process is referred as the preference disaggregation approaches of MCDA.\nMany machine learning framework can help MCDA accomplish the learning objectives because both of them aim to learn a decision model from data. Thus, MCDA and machine learning naturally have reciprocal interactions (Doumpos and Zopounidis, 2011). MCDA and machine learning are integrated in two directions. First, we can apply machine learning techniques to various tasks in a decision aiding context, such as learning to rank, multi-label classification, etc. The opposite direction is to implement MCDA concepts in a machine learning framework. Utilizing MCDA approaches to adapt the machine learning models\n2In machine learning, criteria refer to attributes or features with preference order scales (Corrente et al., 2013). For consistency, we use \u201cattribute\u201d in this paper.\nto various topics (such as feature selection and extraction, pruning decision rules and multiple objective optimization) has become a trend recently. Our work belongs to the second stream. We aim to construct a hybrid model, which utilizes value function-based preference disaggregation approaches of MCDA to enhance the interpretability of \u201cblack-box\u201d machine learning models.\nThe motivation of introducing the value function-based preference disaggregation approaches of MCDA to machine learning stems from its powerful capacity in depicting the human decision-making process. The deciphered marginal value functions reveal the rationale of DM\u2019s judgment, and thus provide convincing evidence to assist comprehending the decision making (Aggarwal and Fallah Tehrani, 2019; Lou et al., 2012). Our task of learning an interpretable model is essentially to capture the characteristics of the marginal value functions, based on which we obtain a certain degree of interpretability."
    },
    {
      "heading": "1.3. An overview of this paper.",
      "text": "This paper proposes a framework for a Neural Network-based Multiple Criteria Decision Aiding (NN-MCDA) approach. NN-MCDA combines an additive model and a fully-connected multilayer perceptron (MLP) to achieve good performance while capturing the explicit relationships between individual attributes and the prediction. The additive model uses marginal value functions to approximate the explicit relationship between the outcome and individual attributes whereas the MLP is used to capture the implicit high-order correlations between attributes in the model. We estimate the parameters in the model under a neural network framework that automatically balances the trade-off between two components.\nWe validate the proposed model using a set of synthetic datasets and three real datasets. Specifically, the simulation experiments respectively show the impact of pre-defined parameters on model performance when data is either extremely complex or simple. Three real datasets on ranking universities regarding employment reputation, predicting the risk for geriatric depression, and predicting the success of bank telemarketing are used to illustrate the proposed model in real scenarios. We explain the obtained model and compare its performance with that of baseline interpretable models, i.e., GAM and logistic regression models, for the first two datasets. We further verify the efficacy of the proposed model with the third dataset through comparing its performance with the results reported in the literature.\nThe contributions of this paper are fourfold. First, we advocate a new perspective of a hybrid machine learning model that both quantifies the explicit impact of individual attributes on the outcome and captures the implicit highorder correlations among attributes. It helps the DM understand the main effect of single attribute, and at the same time, make better decisions. Second, to the best of our knowledge, this paper is the first pilot work that introduces the value function-based preference disaggregation approaches of MCDA to the machine learning models to enhance the model interpretability. The trained parameters in the proposed framework determine the shape for marginal value functions\nin the additive models. The proposed model is free from preference independence, preference monotonicity, and small learning set assumptions in MCDA approaches, and thus makes MCDA approaches more general and practical for real-world management problems. Third, we examine the model effectiveness given different model parameters and datasets. The empirical conclusions about the relationships between model interpretability and data complexity are managerially intuitive for DM and insightful for future research. Fourth, the proposed framework is flexible and extendible, especially the nonlinear part, which can be modified or replaced by other network structures or modeling schemes according to different tasks.\nThe rest of the paper is organized as follows. We discuss the related work in Section 2. In Section 3, we introduce the framework for the proposed model. The simulation and real case experiments are presented in Section 4 and some discussions about the proposed framework is provided in Section 5. We conclude the paper in Section 6."
    },
    {
      "heading": "2. Related work.",
      "text": ""
    },
    {
      "heading": "2.1. Value function-based preference disaggregation approach of MCDA.",
      "text": "The value function-based preference disaggregation approaches of MCDA provide explicit marginal value functions and numerical scores. A DM can understand the importance of a particular attribute and how the individual attributes contribute to the final decision. This procedure encourages the DM to participate in the decision making process and it provides a comprehensive preference model. These approaches have been successfully applied to many scenarios, such as consumer preference analysis (Guo et al., 2019), financial decisions (Barbati et al., 2018), nano-particles synthesis assessment (Kadzin\u0301ski et al., 2018), territorial transformation management (Ciomek et al., 2018), and medical therapy (Hasan et al., 2019). However, the applications of value function-based preference disaggregation approaches are limited due to some strong assumptions, such as (1) preference independence, (2) monotonic preference, and (3) small set of alternatives.\nRecently, many novel models haven been proposed to generalize the value function-based preference disaggregation approaches of MCDA. Preference independence allows the model to be additive. Considering interacted attributes, Angilella et al. (2010) utilize a fuzzy measure to model the preference system where the alternatives are now evaluated in terms of the Choquet integral. However, it is difficult for the DM to understand the impact of individual attribute evaluated from the Choquet integral. Angilella et al. (2014) account for positive and negative interactions among attributes, and add an interaction term to the additive global value function for each alternative. They require the DM to provide some knowledge about the interacted pairs that are mined by the models. These studies only consider the interaction between pairs of attributes because higher-order interactions require more cognitive efforts and more computational cost.\nThe majority of existing researches assume the marginal value functions are monotonic piece-wise linear. This assumption reduces the model complexity, but it fails to describe preference inflexions. Addressing this problem, Ghaderi et al. (2017) and Liu et al. (2019) relax this assumption and constrain on variations of the slope to obtain non-monotonic marginal value functions without serious over-fitting problem. Both of their approaches obtain non-smooth value functions which are difficult to interpret attitudes towards risks due to the use of non-derivative functions. Since a differentiable marginal value function is essential to analyze consumer behavior, Sobrie et al. (2018) utilize semidefinite programming to infer the key parameters for polynomial marginal value functions. It gives a more flexible and interpretable preference model. However, it still assumes that the DM preference is monotonic.\nThe monotonic piece-wise form of the marginal value functions has a low expressibility for large learning sets (Sobrie et al., 2018). Nowadays, MCDA approaches are expected to deal with large amount of data in many disciplines (Pelissari et al., 2019). Liu et al. (2019) embed the MCDA approach into a regularization framework to approximate marginal value functions in any piecewise linear shapes, and provide efficient algorithms to handle larger learning sets.\nMost existing researches focus on expanding the MCDA approaches from only one perspective. Comparing with these recent advances, the proposed framework tries to solve all aforementioned limitations of MCDA by providing a non-monotonic, smoother, and more powerful MCDA approach for real-world applications considering more complex decision making scenarios."
    },
    {
      "heading": "2.2. Interpretable models in management.",
      "text": "Generalized additive model (GAM) uses a link function to build a connection between the mean of the prediction and a smooth function of the variables (Hastie and Tibshirani, 1986). It is good at both dealing with and presenting the nonlinear and non-monotonic relationship between the variables and the prediction (Lou et al., 2012). Therefore, GAM is usually more accurate than linear additive models. Although GAM does not outperform full complexity models, it possesses more interpretability than these \u201cblack-box\u201d models. Lou et al. (2013) explore the co-effect of pairwise interactions and apply the improved GAM to predicting pneumonia risk and 30-day readmission. This model helps the DM (physician) to find useful patterns in the data and quantifies the contributions of individual attributes. Based on these promising results, they argue that it is necessary to develop more interpretable models in mission-critical applications such as management problems (Caruana et al., 2015).\nA recent work proposes a structured-effect model for forecasting the remaining useful life of machines (Kraus and Feuerriegel, 2019). The model combines non-parametric approaches and a deep recurrent neural network. The benefit of the former part is to provide a linear structure where one can understand the importance of the predictors by comparing the obtained coefficients, while the latter helps to improve the flexibility of the model, thereby increasing the\nmodel performance. The proposed model can achieve state-of-the-art performance compared to some traditional approaches but remains interpretable.\nAnother solution for explaining the predictions is to infer a new model to approximate the true black-box model. The new model may not be as accurate as the original black-box model, but can identify patterns and rules to explain how the predictions are made. In Baesens et al. (2003), explanatory rules are extracted to help the credit-risk managers in explaining their decisions. Similarly, Letham et al. (2015) discretize a high-dimensional attribute space into a series of simpler interpretable if-then statements. They firstly make predictions using complex machine learning techniques and then use Bayesian rule lists to reconstruct the stroke predictions. Given approximately accurate predictions, the obtained model is more interpretable."
    },
    {
      "heading": "3. Framework for the intelligible model.",
      "text": "Let D = {(xi, yi)}N1 be the dataset of size N , xi = (xi,1, \u00b7 \u00b7 \u00b7 , xi,n)T be the i-th attribute vector with n attributes3, and yi be the target/response value. In this study, we consider a binary classification problem where yi \u2208 {0, 1}. The proposed framework can be easily extended to multi-classification and regression problems."
    },
    {
      "heading": "3.1. The additive model.",
      "text": "The value function-based preference disaggregation approaches of MCDA assume that for each attribute vector xi \u2286 Rn, there is a global value function in the following form:\nF (xi) = w1 \u00b7 v1(xi,1) +w2 \u00b7 v2(xi,2) + \u00b7 \u00b7 \u00b7+wn \u00b7 vn(xi,n) = \u2211n\nj=1 wjvj(xi,j) (1)\nwhere 0 \u2264 wj \u2264 1, j = {1, 2, \u00b7 \u00b7 \u00b7 , n} represents the importance of the j-th attribute and vj(\u00b7) is a marginal value function. Note that we reply the shape and positive/negative effect of the marginal value function to capture the contribution of individual attributes. Thus we set the weight wj to be positive to represent the relative importance of the j-th attribute, which can positively or negatively affect the global value. The global value function F (\u00b7) linearly sums contributions of individual attributes.\nAlthough the global value function is in an additive linear form, the marginal value functions themselves can be in any forms, often nonlinear. It has been recognized that the preference in human decision behaviors is rational, and thus the marginal value functions should be stable and smooth. In the literature, the marginal value function can be in a simple linear (weighted sum) form (Rezaei, 2016; Korhonen et al., 2012), monotonic and non-monotonic piecewise linear forms (Stewart, 1993; Jacquet-Lagreze and Siskos, 2001; Greco et al., 2008; Ghaderi et al., 2017), and monotonic polynomial form (Sobrie et al., 2018).\n3In MCDA, xi is called an alternative with n criteria/attributes.\nTo capture the first-order (e.g. monotonicity) and second-order (e.g. marginal rate in substitution) derivative patterns of the attributes\u2019 contributions to the prediction, we extend and generalize state-of-the-art MCDA models (Liu et al., 2019; Sobrie et al., 2018) to allow the marginal value function in any polynomial forms. In this paper, we allow the j-th marginal value function to be in a smooth and non-monotonic form of Dj degrees:\nvj(xi,j) = pj,1x 1 i,j + pj,2x 2 i,j + \u00b7 \u00b7 \u00b7+ pj,Djx Dj i,j (2)\nwhere pj,d \u2208 R, d = {1, 2, \u00b7 \u00b7 \u00b7 , Dj} is the coefficient of the d-th degree and Dj is the highest order of degree on the j-th attribute.\nThe motivations using Eq.(2) as a marginal value function are derived from two facets. First, we enhance the expressiveness of the preference model to capture non-monotonic preferences. For example, piecewise linear or monotonic polynomial functions fail to restore all information in a larger learning set (Sobrie et al., 2018). The nonlinearity and non-monotonicity of Eq.(2) can better fit complex relationships between attributes and the outcome, leading to a better model performance. Second, while analyzing human behavior, it is critical to examine the trade-offs or marginal rates of substitution in economics and management studies. A non-derivative value function, for instance the boosted bagged trees model in Lou et al. (2012), cannot capture the inflexion point where the marginal rate of substitution grows or diminishes more quickly. A model exploiting human behaviors seems convincing and has more managerial meaning for the DM in management scenarios."
    },
    {
      "heading": "3.2. Neural network-based MCDA.",
      "text": "Full complexity models perform well on many machine learning tasks because they can model both the nonlinearity and the interactions between attributes. An additive model like Eq.(1) does not model any interactions between attributes. Therefore, we propose a neural network-based multiple criteria decision aiding (NN-MCDA) model in the following form\nU(xi) = \u03b1 \u2211n\nj=1 wjvj(xi,j) + (1\u2212 \u03b1)f(xi,1, xi,2, \u00b7 \u00b7 \u00b7 , xi,n) (3)\n= \u03b1F (xi) + (1\u2212 \u03b1)f(xi,1, xi,2, \u00b7 \u00b7 \u00b7 , xi,n) (4)\nwhere U(xi) is the global score of xi, f(\u00b7) is a latent function of all attributes, and \u03b1 \u2208 [0, 1] is a trade-off coefficient. Eq.(4) describes (a) a regression model if U(\u00b7) is the identity, and (b) a classification model if U(\u00b7) is the logistic function of the identity. f(\u00b7) is used to capture the high-order interrelations between attributes in the model. We can use any complexity models to fit f(\u00b7) for better performance, for instance we use a MLP in this paper (Rosenblatt, 1958). While using an MLP form of f(\u00b7), it is not transparent, meaning that we do not know the exact structure of f(\u00b7). Since we have the F (\u00b7) to capture the explainable form of the marginal value functions, the non-transparent f(\u00b7) describes the complex patterns that are not readily useful to the DM. Coefficient \u03b1 balances\nthe trade-off between F (\u00b7) and f(\u00b7). If \u03b1 is close to 1, the model tends to be in a simple additive form. If \u03b1 is close to 0, we obtain a full complexity model.\nThe utilized joint training process is shown in Figure 1. The input attribute vectors should be transformed into polynomial forms, i.e., \u03a6(xi) = (x1i,1, \u00b7 \u00b7 \u00b7 , x D1 i,1 , x 1 i,2, \u00b7 \u00b7 \u00b7 , x D2 i,2 , \u00b7 \u00b7 \u00b7 , x1i,n, \u00b7 \u00b7 \u00b7 , x Dn i,n )\nT . In the input layer, a singlelayer network without any activation functions is provided to reconstruct Eq.(1). It has \u2211m j=1Dj units and the weight for each unit corresponds to a particular pj,d. We denote the output of the linear component with z linear i , and\nzlineari = (w1, \u00b7 \u00b7 \u00b7 , wj , \u00b7 \u00b7 \u00b7 , wn)  pT1 \u03c6(xi,1) ... pTj \u03c6(xi,j) ...\npTm\u03c6(xi,n)\n = w TPi (5)\nwhere pj = (pj,1, pj,2, \u00b7 \u00b7 \u00b7 , pj,Dj )T is the vector of coefficients in the j-th polynomial marginal value function, \u03c6(xi,j) = (x 1 i,j , \u00b7 \u00b7 \u00b7 , x Dj i,j )\nT is the transformed input vector, w is the vector of weights of attributes, and Pi contains marginal values of i-th attribute vector. Note that, Eq.(5) is actually a specific case of Eq.(1). In Eq.(1), the marginal value functions v(\u00b7) can be in any shapes (e.g. piece-wise linear). However, in this study, we allow them to be in a polynomial form in Eq.(2). Thus, F (\u00b7) is a generalization of zlineari .\nThe nonlinear component is a standard MLP. It is used to learn high-order correlations between attributes. Similarly, by summing every Dj units we can obtain a marginal value on the j-th attribute. For activation functions, we opt\nfor Rectifier (ReLU), which is is the most commonly used activation function in neural networks (Glorot et al., 2011). We can also use other activation functions such as Sigmoid and TanH functions. An L-layer MLP is defined as:\nz1(xi) = \u03a6(xi), z2(xi) = a1(W T 1 z1 + b1),\n\u00b7 \u00b7 \u00b7 (6) zL(xi) = aL\u22121(W T L\u22121zL\u22121 + bL\u22121),\nznonlineari = h T zL(xi),\nwhere Wl, bl and al denote the weight matrix, bias vector and activation function for the l-th layer, respectively. The input of the MLP model is the same as the input for the linear part, i.e., \u03a6(xi).\nThe output is the probability of yi = 1, we have\nP (y\u0302i = 1|xi) = \u03c3(\u03b1zlineari + (1\u2212 \u03b1)znonlineari ) (7)\nwhere \u03c3(\u00b7) is a sigmoid function. To estimate the parameters, we minimize the mean square error (MSE):\nMSE = 1\nN \u2211N i=1 (P (y\u0302i = 1|xi)\u2212 yi)2 (8)\nWe can adopt a variety of optimization methods to minimize Eq.(8), such as Stochastic Gradient Descent (SGD), Adaptive Gradient Algorithm (Adagrad) and Adaptive Moment Estimation (Adam). Please refer to Le et al. (2011) for details of the optimization procedure. The interpretability of the model refers to the capacity in developing marginal value functions, which capture the relationship between individual attributes and prediction. With the proposed model, the DM can know what attributes are more important for the prediction, what values of an attribute are positively or negatively associated to the prediction, and where the convexity and concavity of the function are changed.\nProposition 1. When al(\u00b7), l = 1, . . . , L are linear activation functions, the proposed NN-MCDA model degenerates to an additive model.\nProof. See Appendix A\nProposition 2. When a1(\u00b7) is a nonlinear activation function, if L = 1, the lower-order interacting attributes can be explicitly expressed. If L \u2265 2, the higher-order interacting attributes cannot be explicitly expressed.\nProof. See Appendix B Given Propositions 1 and 2, a neural network with nonlinear activation functions and more than one layers will bring extremely complex transformations of attribute interactions that are hard to be explicitly presented, thereby detecting the main effects of the individual attributes in the presence of these interactions is an alternative way to interpret the predictions."
    },
    {
      "heading": "3.3. Application to multiple criteria ranking problems.",
      "text": "In this subsection, we will show how to apply NN-MCDA to traditional multiple criteria ranking problems where alternatives are ranked based on the DM\u2019s judgment. In this paper, alternatives are represented as attribute vectors.\nLet xi % xk denote that an attribute vector xi is at least as good as xk, and xi xk denote that xi is better than xk. Note that the symbol \u2018%\u2019(or \u2018 \u2019) does not necessarily require that each element in xi is at least as good as (or better than) that in xk. It actually indicates that one alternative is at least as good as (or better than) another one based on the DM\u2019s judgment. For each pair (xi,xk), we define yi,k as follows:\nyi,k = { 1, if xi % xk, 0, if xk xi,\n(9)\nand the difference between global scores of xi and xk is: U(xi)\u2212 U(xk) = \u03b1 \u2211n\nj=1 wjvj(xi,j) + (1\u2212 \u03b1)f(xi) \u2212[\u03b1 \u2211n\nj=1 wjvj(xk,j) + (1\u2212 \u03b1)f(xk)] = \u03b1 \u2211n\nj=1 wj(vj(xi,j)\u2212 vj(xk,j)) + (1\u2212 \u03b1)[f(xi)\u2212 f(xk)]\n= \u03b1 \u2211n j=1 wj \u2211Dj d=1 pj,d(x d i,j \u2212 xdk,j) + (1\u2212 \u03b1)[f(xi)\u2212 f(xk)]\n= \u03b1 \u2211n\nj=1 wj(p\nT j \u00d7 \u03c6(xi,j , xk,j)) + (1\u2212 \u03b1)\u0398(\u03a6(xi,xk))\n= \u03b1wT (Pi \u2212Pk) + (1\u2212 \u03b1)\u0398(\u03a6(xi,xk))\nwhere \u03c6(xi,j , xk,j) = (x 1 i,j \u2212 x1k,j , x2i,j \u2212 x2k,j , \u00b7 \u00b7 \u00b7 , x Dj i,j \u2212 x Dj k,j) T . Let \u03a6(xi,xk) be the aggregated vector for \u03c6(xi,j , xk,j):\n\u03a6(xi,xk) = x1i,1 \u2212 x1k,1, . . . xD1i,1 \u2212 xD1k,1\ufe38 \ufe37\ufe37 \ufe38 D1 , x1i,2 \u2212 x1k,2, . . . , x D2 i,2 \u2212 x D2 k,2\ufe38 \ufe37\ufe37 \ufe38 D2 , x1i,3 \u2212 x1k,3, . . .\ufe38 \ufe37\ufe37 \ufe38 D3 , . . . , . . . , xDni,n \u2212 x Dn k,n\ufe38 \ufe37\ufe37 \ufe38 Dn  T and \u0398(\u03a6(xi,xk)) be a function of \u03a6(xi,xk). We fit \u0398(\u00b7) function to approximate the value of f(xi)\u2212 f(xk). Note that in some decision problems, the attribute weights wj , j = 1, \u00b7 \u00b7 \u00b7 , n in Eq.(3) are normalized to [0, 1] and \u2211n j=1 wj = 1, which are useful for interpreting the trade-offs between attributes4. To address this issue, we apply the following transformation:\n4Note that the trade-off between attributes is similar to attribute importance, but the trade-off emphasizes that assigning more weight to an attribute would decrease other attributes. That usually leads to a situation where some attributes have almost no effects on the predictions, which is unexpected because the selected attributes are often summarized based on DM\u2019s prior knowledge and their requirements. In this regard, we tend to train our model without normalization but provide normalized weights to evaluate the trade-offs between attributes (Liu et al., 2019). Moreover, there are few minor differences on performances when using normalized weights or not.\n\u2022 For each attribute gj , j = 1, \u00b7 \u00b7 \u00b7 , n, the normalized weight is w\u2032j = wj\u2211n\nj=1 wj .\n\u2022 The new global score is U \u2032(xi) = \u03b1 \u2211n j=1 w \u2032 jvj(xi,j)+ (1\u2212\u03b1)\u2211n j=1 wj\nf(xi,1, xi,2, \u00b7 \u00b7 \u00b7 , xi,n). Moreover, the ordinal relations among all attribute vectors are preserved since U \u2032(xi) \u2212 U \u2032(xk) = U(xi)\u2212U(xk)\u2211n j=1 wj\nand U(xi) \u2265 U(xk) \u21d4 U \u2032(xi) \u2265 U \u2032(xk).\nGiven the input data D = {(\u03a6(xi,xk), yi,k)}, instead of mathematical programming, we can now use the machine learning scheme in section 3.2 to infer the preference model and rank other attribute vectors. The output y\u0302i,k = \u03c3(U(xi) \u2212 U(xk)) is the probability that xi is at least as good as xk. We can pre-define two thresholds \u03b71 and \u03b72, where 0 \u2264 \u03b71 \u2264 \u03b72 \u2264 1. If \u03b72 \u2264 y\u0302i,k, then xi xk, and if \u03b71 \u2264 y\u0302i,k \u2264 \u03b72, then xi \u223c xk and otherwise, xk xi. If we use the normalized weights, since the probability y\u0302\u2032i,k = \u03c3(U\n\u2032(xi) \u2212 U \u2032(xk)) is transformed nonlinearly, the pre-defined thresholds should also be transformed as follows, \u03b71i,k = y\u0302\u2032i,k yi,k \u03b71, \u03b72i,k = y\u0302\u2032i,k yi,k \u03b72, to preserve the ordinal relations. In this way, the traditional multiple criteria ranking approaches can handle larger datasets and obtain smoother and more flexible marginal value functions to assist the DM. We present the simulation results in Section 4.1 and the results using real datasets in Section 4.2."
    },
    {
      "heading": "3.4. Usefulness of the proposed framework in decision making.",
      "text": "As we introduce MCDA into machine learning, the main objective is shifted from achieving the best predictive performance to facilitating the DM in gaining insights into the characteristics of the decision making process and the interpretations of the results (Doumpos and Zopounidis, 2011). Once the marginal value functions are obtained by the proposed NN-MCDA framework, we can further analyze the DM\u2019s preference from the following perspectives.\nFirst, the attribute importance usually has a long-tail distribution, with a few of them being very important and the majority of them being less important (Caruana et al., 2015). The characteristics of the marginal value functions can reveal the importance of the corresponding attribute. If a marginal value function is close to 0 for the whole scale of the attribute values, it indicates\nthat the attribute is either not important to the DM or the characteristic of the marginal value function is wrongly captured, because the change of this attribute has little influence on the predictions. When this is the case, we need to interact with the DM to determine whether we preserve this attribute or calibrate the model. In this regard, the proposed framework can perform model selection and modification (similar to statistical approaches like LASSO). For example, while predicting if a patient has the flu, the marginal value function of \u201croom humidity\u201d is in a shape like the marginal value function 1 in Figure 2, it is possible that \u201croom humidity\u201d has little contribution to the flu. However, whether we abandon it should be determined by a physician.\nSecond, the increasing and decreasing tendencies of the marginal value function curves reveal the change of the DM\u2019s non-monotonic preference. We focus on the monotonicity inflexion points because they can determine that to what attribute values, the DM is more sensitive. Moreover, if we partition the marginal value function curve by these points, we can discretize the continuous attribute into smaller ranges in which the DM\u2019s preference is monotonic. Such smaller intervals are useful for personalization (e.g. customer segmentation) and strategy-making tasks in management. For example, while evaluating the company\u2019s performance, if the marginal value function of \u201ccash to total assets ratio\u201d is like the second function in Figure 2, we can learn that a company with a very small or large \u201ccash to total assets ratio\u201d is in a bad condition. Companies with a large ratio are suggested to use the cash to do more investigations, whereas companies with a small ratio are suggested to save general expenses so that more cash can be used in new investigations.\nThird, since the marginal value function returns a \u201cscore\u201d that is added to the global value, it is crucial to determine whether the attribute positively or negatively contributes to the outcome. If a marginal value function is above/below zero, the corresponding attribute is positively/negatively associated with the prediction. The marginal value function can capture the sign change (if any) of an attribute\u2019s contribution and provide the DM an exact attribute value where the sign changes. This is more informative than the statistical models that only provide a fixed coefficient representing either positive or negative effect of the attribute. For example, when predicting the risk of depression among adults, the marginal value function of \u201cage\u201d may has a shape similar to the third function in Figure 2 (please also refer to Figure 17, which is drawn from the real-data). The shape of this curve indicates that the risk of depression does not increase while aging if the adult is younger than a threshold. The risk will increase once the adult is older than that threshold (the threshold is 71.58 in the real data introduced in section 4.3). Statistical models, on the other hand, can only conclude that age has either a negative or positive effect on the depression risk. We need to segment the adults to pre-defined age groups to capture such sign change effect.\nFourth, the concavity (and convexity) of the marginal value function can directly reflect the changing rate of the DM\u2019s preference. Such information is important to both economics and marketing problems. For example, if the consumer\u2019s preference to \u201cdiscount rate\u201d is in a same shape of marginal value\nfunction 4 in Figure 2, it implies that at the beginning, along with the increase of the discount rate, the consumer\u2019s utility (propensity to consume the product) grows more quickly. However, when the discount rate is over a specific value, it gives a signal that the product is possibly of bad quality. Although the consumer\u2019s utility still grows, its rate of increase starts to slow down. This provides the DM with a conclusion that keeping the discount rate at a medium level could maximize the profit."
    },
    {
      "heading": "4. Experiments.",
      "text": "To validate the proposed NN-MCDA model, we perform experiments with both synthetic and real datasets. We use area under the curve (AUC) of receiver operating characteristic (ROC) curve to measure the model performance. In subsection 4.1, three simulation experiments examine (a) the influence of the degree of polynomial on the prediction performance, (b) the influence of the value of \u03b1, the trade-off coefficient, on prediction performance, and (c) the goodness of the proposed NN-MCDA approach in fitting the given marginal value functions. In Section 4.2, we first apply the NN-MCDA model to a multiple criteria decision problem where we rank universities based on the employer reputation. Then we predict the risk for geriatric depression with useful interpretations of the risk factors with a higher resolution. At last, we perform the NN-MCDA on an open access data to compare the results with a published paper."
    },
    {
      "heading": "4.1. Simulations.",
      "text": "For brevity, we set equal pre-defined degrees of polynomial for all marginal value functions in the subsequent experiments. We generate three typical synthetic datasets (from the simplest to very complex) as follows:\n1. Uniformly draw N attribute vectors with n attributes whose values are within [0,1]. 2. We generate three datasets. (a) For the first dataset Dnl , all n attributes have equal importance and the actual marginal value functions are identity functions. The global score for each attribute vector is a linear summation of n attribute values without any attribute interactions and an additional noise term that is in a standard normal distribution; (b) The second dataset Dnpolynomial\u22123 randomly generates 3-degree polynomials marginal value functions for n attributes, and the global score is the summation of marginal values, all ( n 2 ) attribute interactions and a standard normal\nnoise term. (c) The third dataset Dnpolynomial\u221215 is extremely complex. The global score is the summation of n 15-degree polynomial marginal values, all possible attribute interactions (pairwise, triple-wise and higher interactions) and a standard normal noise term.\n3. We compare global scores between each pair of attribute vectors. If U(xi) \u2212 U(xk) \u2265 0, then yi,k = 1, otherwise, yi,k = 0. Note that the actual input is the transformed attribute vector."
    },
    {
      "heading": "4.1.1. Experiment I: Relationship between degree of polynomial and model performance",
      "text": "The first simulated experiment aims at exploring the relationship between the pre-defined degree of polynomial and AUC. The parameters used in the experiment is shown in Table 1. For each setting, we iteratively repeat the experiment for 10 times and record the averaged AUC. In this experiment, the numbers of iterations are determined using fivefold cross-validation: We partition the training set into five sets and set aside one of them as a validation set. We then train the model using the other four partitions and use the validation set to check the convergence. This procedure is repeated five times and the averaged number of iterations is used to train the final model with the whole dataset (Lou et al., 2012).\nFigure 3: The average AUC for the testing set using training set with different sizes from datasets D3l and D 5 l .\ndeg ree\n1\ndeg ree\n2\ndeg ree\n3\ndeg ree\n5\ndeg ree\n10\nOrder of degree\n0.745\n0.750\n0.755\n0.760\n0.765\n0.770\n0.775\n0.780\nA U C f o r T e st in g s e t\nTraining size 0.5 Training size 0.6 Training size 0.7 Training size 0.8 Training size 0.9\ndeg ree\n1\ndeg ree\n2\ndeg ree\n3\ndeg ree\n5\ndeg ree\n10\nOrder of degree\n0.81\n0.82\n0.83\n0.84\n0.85\n0.86\n0.87\nA U C f o r T e st in g s e t\nTraining size 0.5 Training size 0.6 Training size 0.7 Training size 0.8 Training size 0.9\nFigure 4: The average AUC for the testing set using training set with different sizes from datasets D3polynomial\u22123 and D 5 polynomial\u22123.\nFigures 3, 4 and 5 report the averaged AUC for the testing set with different training sizes using the three synthetic datasets. Though there are no obvious\nrelationships between the training sizes and the model performance, we find two interesting patterns. First, higher pre-defined degrees of polynomials can lead to higher accuracy when convergence. That results from the ability of the underlying model to capture more complicated nonlinearity. However, higher degrees of polynomials usually require more iterations to converge. More specifically, we depict the averaged computational time for each training process in Figure 6. Apparently, while increasing the model complexity, for example, using higher degrees of polynomial marginal value function and considering more attributes, the average computational time to converge also increases almost linearly. Another interesting pattern is that the shape of the AUC curves (Figures 3, 4 and 5) can fit a concave function in general. When the degree increases, the AUC improvement (over the model with the immediate smaller degree) is becoming smaller. For example, the improvement is more obvious if we change the predefined degree from 1 to 3 than that if we change from 3 to 5 and from 5 to 10. The improvement diminishes quickly along with the increase of pre-defined degree of polynomials. The greatest AUC improvement happens if we increase the degree to 3, while the improvement resulted from further increasing the degree to 5 and 10 is slim. The results suggest that it is not necessary to set a very large Dj for the seek of minor improvement because the computational cost increases much faster when Dj increases. Generally, we believe that a polynomial\nof 3 degree is sufficient to capture the characteristics for all the three datasets. Higher degrees of polynomials have a risk for over-fitting and obviously cost more computational time, but contribute little to accuracy.\nWe also compare the proposed NN-MCDA with baseline machine learning models, including the standard MLP, polynomial linear regression (PLR) with 1, 2, 3, 5 and 10 degrees, GAM with 10 splines that are in 3 and 10 degree of polynomials, SVMs with linear, radial basis function (RBF) and polynomial kernels, and single decision tree (DeciTr) models with 6, 10 and 20 maximum depths. Table 2 presents the results for the simplest dataset. All machine learning models, both the interpretable (including NN-MCDA) and full complexity ones, perform well. The performance drops rapidly when we use them to fit the nonlinear and high-order datasets (shown in Tables 3 and 4). Since\nthe proposed NN-MCDA model and MLP can model the nonlinearity and attribute interactions, both of them achieve much higher AUCs as compared to the rest. As expected, although the performance of NN-MCDA is lower than MLP, the difference is relatively small. Both NN-MCDA and MLP outperform other baseline machine learning models significantly. More specifically, we can observe that GAM, SVM and DeciTr have similar accuracy, which is higher than that of PLR, but lower than NN-MCDA. NN-MCDA\u2019s performance is close to the full complexity model, and at the same time, it preserves a certain degree of interpretability (will be shown in the next experiment)."
    },
    {
      "heading": "4.1.2. Experiment II: Impact of \u03b1 on AUC",
      "text": "In this section, we focus on assessing how the performance of the NN-MCDA model is affected by the value of \u03b1, the weight for the linear component. We evenly sample 20 values within [0, 1] as the pre-defined \u03b1. For each fixed \u03b1, we train the NN-MCDA model using synthetic datasets introduced in the previous subsection. In this experiment, we use the SGD algorithm to optimize the parameters and the number of iterations are set as 250.\nIn Figure 7 (results with the linearly generated datasets), though three curves have many monotonicity inflexions, there is a general trend that the greater the\nvalue of \u03b1, the better the performance. NN-MCDA obtains the best results when \u03b1 is between 0.8 and 1.0 on these linearly generated datasets. In contrast, the AUC curves have a general decreasing trend when \u03b1 increases for the nonlinearly generated datasets (Figures 8 and 9). In the extreme cases, where only the nonlinear MLP component (i.e., \u03b1 = 0) or the linear component (i.e.,\u03b1 = 1) works, the model obtains the greatest or smallest average AUC.\nNote that the three datasets have very different patterns. Datasets Dnl use a set of simple linear marginal value functions whereas Dnpolynomial\u22123 and Dnpolynomial\u221215 simulate more complicated patterns. Theoretically, a full complexity model (MLP) can perfectly capture any patterns in the data at the cost of very large numbers of iterations and data samples for convergence. In practice, we often do not have sufficient data or computational time to achieve the optimal MLP solution. In this simulation experiment (31,125 data points and 250 iterations to fit the model), a pure MLP model (NN-MCDA with \u03b1 = 0) does not always lead to the best outcome. This result indicates that, in realworld managerial decision making, a full complexity model is usually not the best one not only because of the lack of interpretability, but also the limited data and computational resources to optimize the model. It is sensible to allow the model to automatically adjust the trade-off coefficient \u03b1 to avoid the scenarios where a very complex model is used to fit simple data, or a simple model is used to fit complex data."
    },
    {
      "heading": "4.1.3. Experiment III: performance in fitting actual marginal value functions",
      "text": "This experiment studies the ability of the NN-MCDA model to reconstruct the actual marginal value functions. From Experiments I and II, we find that the NN-MCDA model with degree equal to 3 has a good balance between prediction performance and computational cost. Therefore, in this experiment, we generate four typical synthetic models with different complexities. Each hypothetical model has three marginal value functions to estimate. Then, we use the synthetic models to generate datasets with the same attribute vectors (model input). We approximate the marginal value functions using an NN-MCDA model with a degree of 3. We also compare the obtained function with a baseline linear regression model. The four synthetic models (from the simplest to extremely complicated) are described as follows:\nFigures 10 to 13 reveal the actual and fitted marginal value functions obtained by the proposed NN-MCDA model and the baseline linear regression model. For a simple model with linear marginal value functions (Synthetic model 1), both the baseline linear regression model and the NN-MCDA can fit the actual functions well. Both models successfully capture the monotonicity of the original marginal value functions. This indicates that the NN-MCDA\nmodel is also applicable to simple prediction tasks that do not have attribute interactions or nonlinear associations between attributes and predictions.\nWhen the attribute interactions are considered (Synthetic model 2), the proposed model outperforms the baseline linear regression model. In the first row of Figure 11, the NN-MCDA model captures correct monotonicity changes of all three actual marginal value functions. Moreover, for the first and third attributes, it captures the concavity of the original functions. While linear regression model gives opposite monotonicity of the actual functions except for the second attribute. In addition, the linear regression model does not capture the concavity of the actual functions. The bad performance of the linear regression model is due to the fact that it can not capture the interactions between attributes, which often brings distorted interpretability.\nFor the model with more complex marginal value functions (Synthetic model 3), the linear regression model performs rather bad. In Figure 12, the first and third attributes have very negligible impact on the prediction in the fitted linear regression model. The NN-MCDA model, on the other hand, correctly captures the main characteristics of the three attributes. Both models failed to capture the inflexion point for the second attribute. These results demonstrate that a low-degree NN-MCDA model (3-degree in this study) can fit both lower and higher degree marginal value functions, because of the nonlinear component helps deal with the complexities that cannot be captured by the predefined linear\ncomponent. In Figure 13, if the marginal value function is extremely complex (Synthetic model 4), though low-degree NN-MCDA cannot fully capture the characteristics of the marginal value functions, it still outperforms the baseline linear regression model. However, the real-world decision making behaviors are usually not that complex (15-degree in Synthetics model 3 and 4). We will further validate the applicability and generalizability of the proposed NNMCDA model with real datasets in the following section."
    },
    {
      "heading": "4.2. A multiple criteria ranking problem.",
      "text": "QS world university ranking organization5 provides five carefully-chosen indicators to measure the universities\u2019 capacity in producing the most employable graduates, including employer reputation, employer-student connection, alumni outcomes, partnerships with employers and graduate employment rate. The metric of employer reputation, regarded as a key performance indicator, is based on over 40,000 responses to the QS Employer Survey. In this experiment, we apply the proposed NN-MCDA model to a multiple criteria ranking problem that predicts the employer reputation (human decision) using the other four quantitative indicators6. The descriptive statistics are shown in Table 5.\n5It is an annual publication of university rankings by Quacquarelli Symonds (QS). https: //www.topuniversities.com\n6Employer-student connection (EC). This indicator involves the number of active presences of employers on a university\u2019s campus over the past 12 months. Such presences are in form of providing students with opportunities to network and acquire information, organizing company presentations or other self-promoting activities, which increase the probability that students have to participate in career-launching internships and research opportunities. Alumni outcomes (AO). The scores based on the outcomes of a university\u2019s graduates produced. A university is successful if its graduates tend to produce more wealth and scientific researches. Partnerships with employers (PE). The number of citable and transformative researches which are produced by a university collaborating successfully with global companies. Graduate employment rate (GER). This indicator is essential for understanding how successful universities are at nurturing employability. It involves measuring the proportion of graduates (excluding those opting to pursue further study or unavailable to work) in full or part time employment within 12 months of graduation.\nThere are ( 250 2 ) pairwise comparisons among 250 universities. To determine the pre-defined degree of polynomials, we respectively set Dj as 1, 2, and 3. We use the same fivefold cross-validation process to fit the model. We record the averaged AUC for both training and testing sets. We also select three baseline models, including a 3-layer MLP, a logistic regression model and a GAM. The results of the average AUC are presented in Table 6. For each predefined degree, the full complexity model always obtain the best results whereas the logistic regression model performs the worst. Since NN-MCDA can model attribute interactions, it slightly outperforms the GAM. We depict the marginal value functions obtained by NN-MCDA, logistic regression model and GAM in Figures 14, 15 and 16, respectively.\nIn Figures 14 and 15, the vertical axis is the individual attributes contributions to employer reputation. For the attributes alumni outcomes, partnerships with employers and graduate employment rate, the value functions obtained\nby NN-MCDA and logistic regression model exhibit a monotonically increasing trend, which makes sense based on our common knowledge. For attribute employer-student connection, GAM obtains a generally increasing curve. Although it also captures a slight dip of the increasing trend for employer-student connectiongreater than 0.8, such effect is not as clear as that in the curve captured by NN-MCDA. For attributes alumni outcomes, partnerships with employers and graduate employment rate, GAM obtains quite different and unstable curves that are difficult to explain."
    },
    {
      "heading": "4.3. Predicting geriatric depression risk.",
      "text": "Depression is a major cause of emotional suffering in later life. Geriatric depression reduces the quality of older adults\u2019 life and increases the risk for acquiring other diseases and committing suicide (Alexopoulos, 2005). The existing literature empirically studied the risk factors of geriatric depression but few of them gave insight into how these risk factors affect the prevalence of geriatric depression in details (i.e.: the shape of the marginal value functions). It is more managerially helpful for clinical decision making to prevent older adults from being depressed if we can understand how each risk factor influences the risk for depression at different scales.\nThe Health and Retirement Study (HRS) is a nationally representative longitudinal study of US adults (Bugliari et al., 2016). It has been widely used in many medical studies because of its massive information about older adults demographics, health status, health care utilization and costs, and other useful variables (Pool et al., 2018). We sample the data in 2014 (N = 17, 696). In this experiment, given five pre-determined attributes (risk factors) that have\nbeen found to be associated with geriatric depression, we want to capture the detailed effect of these risk factors at different scales, which are represented by marginal value functions. The descriptive statistics are described in Table 77.\nThe respondents with CES-D scores higher than or equal to 1 are assumed to be at risk for depression (positive samples). There are totally 9,816 positive samples and 7,880 negative samples. We randomly choose 90% from both positive and negative samples to train the model. We train the NN-MCDA, MLP, GAM, and logistic regression model for 30 times, and present the average results in Table 8. The obtained marginal value functions are visualized in Figures 17, 18, and 19.\nWe first analyze the similar conclusions by three baseline interpretable models. For the last four attributes, both NN-MCDA and logistic regression models capture similar monotonic trends. As for degree of education and marital status,\n7Center for Epidemiologic Studies Depression (CES-D) scale is a self-report measure of the frequency of 20 depressive symptoms during the past week (Radloff, 1991). It is one of the most popular index assessing the risk for being depressed (Beardslee et al., 2013; Brent et al., 2015; Garber et al., 2009). Age in years at the end of the survey is calculated from the respondent birth date and beginning survey date. The samples vary from 18 to 104 but very few of them are within 18-45 and 93-104 years (Blazer et al., 1991; Mirowsky and Ross, 1992). Degree of education is measured by the years of getting education. It is a categorical variable varying from 0 to 18. Note that for respondents whose degree of education is higher than 17 years, we set the degree of education as 18 (Ladin, 2008; Murrell et al., 1983). Marital status is represented by the length of the longest marriage that respondent ever had. It is a continuous variable and varies from 0 to 74.2 years (Pearlin and Johnson, 1977; Kessler and Essex, 1982; Penninx et al., 1998). Out-of-pocket expenditure refers to the expenses that the respondent pays directly to the health care provider without a third-party (insurer, or State). We consider the out-of-pocket medical expenditure in previous 2 years (Gadit, 2004). Body mass index is the weight divided by the square height Bugliari et al. (2016), which determines whether a respondent is overweight (Dong et al., 2004; Luppino et al., 2010; Ross, 1994).\nthe curves are under the baseline rate indicating higher education and the longer length of marriage can reduce the risk for depression. This is consistent with the medical literature (Penninx et al., 1998; Ladin, 2008). More specifically, both models find that the attributes out-of-pocket expenditure and Body mass index would increase the risk for depression. Since the obtained value functions are in a convex shape, the growth of the risk will increase along with the increase of these attribute values.\nBoth the NN-MCDA and logistic regression obtain a convex curve for attribute age with part of the curve being negative and the rest being positive (see Figures 17 and 18). This indicates that the risk of depression does not increase while aging if the adult is younger than a threshold. The risk of depression increases fast after an adult passes an age threshold. The threshold for NN-MCDA is 71.58, which makes sense because most adults younger than 71.58 could be enjoying their retirement and their body functions do not degrade much. However, the threshold for the logistic regression is 95.83, which seems unrealistic and inconsistent with the literature (Blazer et al., 1991).\nSimilar to the previous experiment, GAM obtains less stabler curves, which are relatively more difficult to interpret. For attribute age, GAM obtains similar patterns as NN-MCDA and logistic regression models with an age threshold around 90, which is inconsistent with the literature (Blazer et al., 1991). For attributes degree of education and marital status, GAM even obtains quite counter-intuitive (if not wrong) results, indicating that the increase in educational and marriage time results in the higher risk of depression."
    },
    {
      "heading": "4.4. Predicting the success of bank telemarketing.",
      "text": "In this section, we employ a benchmark telemarketing datasets 8 to further verify the efficacy of NN-MCDA. In this task, we aim to predict the success of selling bank long-term deposits by making telemarketing calls (Moro et al., 2014). After data preprocessing, there are 40,787 records and 48 attributes (11 of them are numeric and the rest are binary). In this experiment, we focus on the contributions of 11 numeric attributes in the linear component, and import all attributes to the nonlinear component. We utilize the 3-degree polynomial functions and randomly choose 80% of data to train the NN-MCDA model for 100 times. In addition to leading to comparable prediction performance (the averaged AUC is around 0.889), NN-MCDA can provide explicit marginal value functions to depict the detailed contributions of individual attributes.\nFigure 20 presents the normalized marginal value functions for all 11 numeric attributes.9 We find that the marginal value function of the age indicates that when a client is younger than 55, he or she is less likely to buy longterm deposit. However, over the course of aging, the client tends to save for future costs. That implies that an experience DM (a telemarketing campaign manager) should target at older clients rather than younger ones. Another interesting pattern is about the Euribor rate. NN-MCDA shows that the Euribor rate is negatively associated with the probability for long-term deposit subscription, and such probability declines more quickly along with the increase of the Euribor rate. Consumer confidence index (CCI) measures the consumers\u2019 attitude towards the economy. The original value of the consumer confidence index ranges from -50.8 to -26.9 indicating that the clients are pessimistic towards investments, and thus they may tend to purchase less long-term deposits and keep the cash. Nevertheless, while the CCI increases, the probability of longterm deposit subscription starts to increase as well because the clients becomes\n8The dataset is available on https://archive.ics.uci.edu/ml/datasets/Bank+ Marketing. The descriptions about the attributes and data, please refer to the website.\n9duration: last contact duration; campaign: number of contacts performed during this campaign and for this client; pdays: number of days that passed by after the client was last contacted from a previous campaign; previous: number of contacts performed before this campaign and for this client.\nmore confident in the economy. Similarly, we can interpret the marginal value functions for all variables. Such interpretability extends the DM\u2019s understanding of the clients\u2019 investment propensity, and could inform better telemarketing strategy.\nThe experiments with real data demonstrate that the proposed NN-MCDA can effectively capture the patterns in human decision making through learning the marginal value functions, which characterize the contribution of individual attributes to the predictions. The NN-MCDA presents good potential in enhancing the empirical studies through providing a detailed marginal value function instead of a single coefficient for each attribute. Moreover, the prediction performance of NN-MCDA is close to a full complexity model (MLP), and much better than that of baseline interpretable models (GAM and logistic regression model). This finding is further verified through comparing its performance with that reported in the literature."
    },
    {
      "heading": "5. Discussion",
      "text": "In this section, we summarize the insights from the experiments, discuss the use and extension of the proposed NN-MCDA model, and compare the proposed NN-MCDA with ensemble learning."
    },
    {
      "heading": "5.1. Attribute importance.",
      "text": "To explain the importance of each attribute, we present the normalized attribute weights obtained from previous experiments in Figure 21. In the university ranking problem, NN-MCDA assigns 0.2732, 0.228, 0.258, and 0.239\nto attributes employer-student connection, alumni outcomes, partnerships with employers and graduate employment rate whereas a regression model assigns 0.3198, 0.3107, 0.1906, and 0.1789 to them, respectively. Both models determine that employer-student connection is the most important attribute, however, they give different orders of other three attributes. Given the limited resources and the obtained importances of attributes, maintaining a good employer-student connection with frequent employer presences on campus is the most effective method to achieve good employer reputation of a university.\nAs for the depression prediction problem, a regression model assigns almost equal importance to the five attributes, which is not intuitive for the DM. On the other hand, the NN-MCDA provides a an order of the attributes according to the importance: BMI \u223c OPE MS \u223c EDU AGE (0.221, 0.218, 0.208, 0.198, 0.155). Such order suggests that obesity and loads of expenditure are the most important risk factors of becoming depressed. While estimating an old adult\u2019s risk for depression, a DM (general physician, specialists and geriatricians) should prioritize the problems related to the older adult\u2019s body weight and economical conditions."
    },
    {
      "heading": "5.2. Interpreting the trade-off coefficient.",
      "text": "As a key coefficient, determining the value of \u03b1 is important to practical applications. It reflects the influence of high-order interactions and complex nonlinearity of variables on the final decisions. In this regard, the NN-MCDA model can be used to explore the complexity of the learning problem. If the convergence \u03b1 is very small, it indicates that the data is highly complex and the assumption of preference independence is not valid. The DM should then use latest models that account for attribute interactions, such as a Choquet integralbased model (Aggarwal and Fallah Tehrani, 2019) or full complexity machine learning models. On the contrary, if \u03b1 is close to 1, the DM is recommended to use a simpler model to avoid massive computational time and non-interpretable results.\nIn the case of prediction for depression (Table 8), the convergence \u03b1 is around 0.4. It indicates that the involved attributes are possibly interacted in this\nproblem. Some related medical studies also empirically demonstrated some interactions between attributes, for example, older adults that are relatively young with higher degree of education may be involved in more social activities and have a more contented life, which lead to lower risk for depression (Li et al., 2014). Moreover, these older adults with longer marriage will obtain more family support and thus they have lower risk for depression (Pearlin and Johnson, 1977). We usually address the pairwise interactions because they are easier to interpret and can be visualized by a heating map (Caruana et al., 2015). Given convergence \u03b1 and marginal value functions obtained in the presence of high-order correlations among attributes, we could develop algorithms to use lower-ordered attribute interactions, e.g.: pairwise and triple-wise interactions, to approximate the higher ones. The framework can then be extended to a twostep procedures, including determining marginal value functions and deducing possible lower-ordered correlations."
    },
    {
      "heading": "5.3. Extending the NN-MCDA framework.",
      "text": "The proposed NN-MCDA presents a general modelling framework, which can be easily extended to enhance the performance and adaptivity for various problems. In this section, we discuss the three extensions, including adding regularizations replacing the model in the nonlinear component, and incorporating attributes in the nonlinear component."
    },
    {
      "heading": "5.3.1. Adding regularizations",
      "text": "For some mission-critical cases where the data is complex, the convergence \u03b1 could be very small. However, the DM still requires certain level of model interpretability to facilitate their decision making. Therefore, we opt for adding a regularization term to prevent the model from being too complicated and noninterpretable. The inclusion of the regularization term also helps prevent the over-fitting problem. For example, we can revise the original MSE as follows MSE1 = MSE+(1\u2212\u03b1)2. The added regularization term (1\u2212\u03b1)2 allocates more weight to the linear component at the cost of lower fitting accuracy. We can also change the regularization term to (2\u03b1 \u2212 1)2, which leads to a balanced model that favors a model with equal weights to the linear and nonlinear components. The exact form of the loss function should be selected according to the problem settings."
    },
    {
      "heading": "5.3.2. Replace the model in the nonlinear component.",
      "text": "Given different types of datasets, the proposed NN-MCDA model can be modified by replacing the neural networks in the nonlinear component by other network structures. In subsection 4.1.1, NN-MCDA has difficulty in handling extremely complex data (Dnpolynomial\u22123 and Dnpolynomial\u221215). To improve the performance on this data, we can introduce more layers in the MLP or increase the number of neurons in each layer. For image classification problem, we can replace the MLP with a convolutional neural network (CNN), and use the features obtained from CNN as the input for the linear component. To fit\ntime-series or free text data, we can replace the MLP with a recurrent neural network.\nIn addition, the proposed model can be progressively modified by iteratively interacting with the DM. We provide a user-interactive process to determine the ultimate model. The framework is shown in Figure 22 and explained as follows:\nStep 1. We first apply the NN-MCDA model to the management problem. While the model converges, we obtain the value of \u03b1. Step 2. If \u03b1 > 0.1, go to step 3. If \u03b1 \u2264 0.1, which indicates that the data are potentially very complex. We opt for a full complexity black-box model to achieve higher accuracy and present the results to the DM. If the DM agrees to use the black-box model, the process is end. Otherwise, we add a regularization term (e.g.: use MSE1) to the original NN-MCDA, and go back to Step 1. Step 3. If \u03b1 < 0.9, go to step 4. If \u03b1 \u2265 0.9, which indicates that a simple additive model is sufficient to fit the data, we explain the results to the DM. If the DM is satisfied with the accuracy, the process ends. Otherwise, we can modify the NN-MCDA model by increasing the complexity of the nonlinear component (e.g.: using deeper MLP or other neural net-based model). Then, we go back to Step 1.\nStep 4. If 0.1 < \u03b1 < 0.9, we present the underlying model and results to the DM. If there are no further requirements, the process is end. If the DM requires further modifications (such as adding regularization terms or modifying the nonlinear component), we modify the model accordingly and then go to Step 1."
    },
    {
      "heading": "5.3.3. Flexible inclusion of attributes in the nonlinear component",
      "text": "In practice, the human decision making usually focuses on a small number of key attributes/criteria (Ribeiro et al., 2016). However, there could exist other\nminor attributes that do not directly contribute to the prediction, but could affect the prediction through non-traceable complex interactions with other attributes (for example, the interaction between the nonlinear transformation of an attribute and the nonlinear transformation of five other attributes). These minor attributes can be incorporated by the nonlinear component.\nIn the geriatric depression experiment, the gender of an older adult may not directly indicate a difference in the risk for depression, however, it might still influence the prediction through complex interactions with other attributes. We further extend the NN-MCDA model in incorporate gender and smoking status into the nonlinear component (as shown in Figure 23). We find that the incorporation of these attributes indeed improves the prediction accuracy (the AUC for testing set increases from 0.669 to 0.675), while still maintains the similar marginal value functions in the linear component (see Figure 24). If we add two more attributes, for instance whether the respondent received any home cares in last two years and whether the health problem limited his/her work, the AUC increases more obviously (from 0.675 to 0.708) and the marginal value functions still provide convincing results (see Figure 25)."
    },
    {
      "heading": "5.4. Joint training process.",
      "text": "In the NN-MCDA model, the linear and nonlinear components are combined by a trade-off coefficient \u03b1. Their sum is then fed to a common logistic function for a joint training process. Note that this joint training process is different from ensemble learning (Cheng et al., 2016), in which multiple classifiers are trained individually and their predictions are simply combined after every model is\noptimized separately. For example, an ensemble learning approach could have a linear logistic regression model and an MLP model to make predictions for the same dataset separately, and then integrate the prediction results of the two models. The joint training process indicates that the linear and nonlinear components are connected. While we tune the parameters in one component, the other component will be affected. If the model is at its global optimal, the predictions can be made."
    },
    {
      "heading": "6. Conclusion and future work.",
      "text": "In this paper, we proposed a framework for a novel hybrid machine learning model, namely the NN-MCDA, which combines traditional MCDA model and neural networks. MCDA uses marginal value functions to describe the explicit contribution of individual attributes to the predictions, while neural network considers the implicit high-order interrelations among attributes. The framework automatically balances the trade-off between two components. NN-MCDA is more interpretable than a full complexity model and maintains similar predictability.\nWe present simulation experiments to demonstrate the effectiveness of NNMCDA. The experiments show that (1) polynomial of higher degrees do not always improve on accuracy; (2) There is a trade-off between the interpretability and the predictability of the model. NN-MCDA can achieve a good balance between them; (3) Given simple data, NN-MCDA performs as good as interpretable model, while given more complex data, NN-MCDA outperforms an interpretable model. We also present how to apply the NN-MCDA framework to real-world decision making problems. These experiments with real data\ndemonstrate the good prediction performance of NN-MCDA and its ability in capturing the detailed contributions of individual attributes.\nWe envisage the following directions for future researches based on the NNMCDA framework. First, we can further enhance the interpretability of the model through proposing algorithms to approximate the attribute interactions after obtaining the marginal value functions. Second, further studies are needed to validate the effectiveness of the NN-MCDA variants that introduced in the discussion section. Last, but not the least, applying the proposed framework to a variety of real-world decision making and prediction problems constitutes another interesting direction for future work."
    },
    {
      "heading": "Appendix A. Proof of Proposition 1",
      "text": "Proof. Assume the activation function has the following linear form al(x) = ATl x + b \u2032 l. The output of the l-th layer is\nzl(x) = al(W T l zl\u22121 + bl) = A T l (W T l zl\u22121 + bl) + b \u2032 l (A.1)\n= (WlAl) T zl\u22121 + (A T l bl + b \u2032 l) (A.2)\nleading to a linear combination at each unit. The global value of the i-th attribute vector is\nU(xi) = \u03b1z linear i + (1\u2212 \u03b1)znonlineari (A.3)\n= \u03b1wTPi + (1\u2212 \u03b1)hT ((WL\u22121WL\u22122 . . .W1A1 . . .AL\u22121)T\u03a6(xi) + b) (A.4)\nis apparently in an additive form."
    },
    {
      "heading": "Appendix B. Proof of Proposition 2",
      "text": "Proof. Without loss of generality, assume the activation function is a sigmoid function. When L = 1, assume there are K1 units in the first layer and the output of k-th unit in the nonlinear component is zk1 (xi) = \u03c3(w T k xi + bk). The output of this layer is\nznonlineari = \u2211K1\nk=1 hk\u03c3\n( wk1xi + b k 1 ) (B.1)\n= f(xi,1, xi,2, . . . , xi,n) (B.2)\nwhere f(xi,1, xi,2, . . . , xi,n) is a function of each attribute value. For example, when K1 = 2, we have\nf(xi,1, . . . , xi,n) = h1 + h2 + h2e\n\u2212(wT1 xi+b1) + h1e \u2212(wT2 xi+b2)\n1 + e\u2212(w T 1 xi+b1) + e\u2212(w T 2 xi+b2) + e\u2212(w T 1 xi+w T 2 xi+b1+b2)\n(B.3)\nwhere wTk xi = \u2211n j=1 w j kxi,j and k = {1, 2}. When L \u2265 2, let fkl\u22121(xi,1, . . . , xi,n) be the output of the k-th unit in the previous layer. We have\ninputu2 = \u2211K1\nk=1 wu,k1 f k 1 (xi,1, . . . xi,n), (B.4)\noutputu2 = \u03c3 (input u 2 ) = f u 2 ( f11 (xi,1, . . . xi,n) , . . . f K1 1 (xi,1, . . . xi,n) ) (B.5)\n...\noutputuL = f u L(f 1 L\u22121 ( \u00b7 \u00b7 \u00b7 fK22 ( f11 (xi,1, . . . xi,n) , . . . f K1 1 (xi,1, . . . xi,n) )) ,\n(B.6)\nf2L\u22121 (\u00b7 \u00b7 \u00b7 ) , . . . , f KL\u22121 L\u22121 (\u00b7 \u00b7 \u00b7 )), znonlineari = \u2211KL\nu=1 huoutput\nu L, (B.7)\nwhere inputul is the input of the u-th unit at the l-th layer. The form of Eq.(B.6) is complex and it is difficult to simulate its explicit form. Apparently, there are higher-order transformations of attribute interactions, which are more complicated than the forms in Eq.(B.2)."
    }
  ],
  "title": "A hybrid machine learning framework for analyzing human decision making through learning preferences",
  "year": 2019
}
