{
  "abstractText": "The paradigm of pretrained deep learning models has recently emerged in artificial intelligence practice, allowing deployment in numerous societal settings with limited computational resources, but also embedding biases and enabling unintended negative uses. In this paper, we treat pretrained models as objects of study and discuss the ethical impacts of their sociological position. We discuss how pretrained models are developed and compared under the common task framework, but that this may make self-regulation inadequate. Further how pretrained models may have a performative effect on society that exacerbates biases. We then discuss how pretrained models move through actor networks as a kind of computationally immutable mobile, but that users also act as agents of technological change by reinterpreting them via fine-tuning and transfer. We further discuss how users may use pretrained models in malicious ways, drawing a novel connection between the responsible innovation and user-centered innovation literatures. We close by discussing how this sociological understanding of pretrained models can inform AI governance frameworks for fairness, accountability, and transparency.",
  "authors": [
    {
      "affiliations": [],
      "name": "Lav R. Varshney"
    },
    {
      "affiliations": [],
      "name": "Nitish Shirish Keskar"
    }
  ],
  "id": "SP:3e113eadc80b64d38d6beb9cf6f16c856903792d",
  "references": [
    {
      "authors": [
        "Sam S. Adams",
        "Guruduth Banavar",
        "Murray Campbell"
      ],
      "title": "Iathlon: Towards A Multidimensional Turing Test",
      "venue": "AI Magazine 37,",
      "year": 2016
    },
    {
      "authors": [
        "R. McNeill Alexander"
      ],
      "title": "Optima for Animals (revised ed.)",
      "year": 1996
    },
    {
      "authors": [
        "Matthew Arnold",
        "Rachel K.E. Bellamy",
        "Michael Hind",
        "Stephanie Houde",
        "Sameep Mehta",
        "Aleksandra Mojsilovic",
        "Ravi Nair",
        "Karthikeyan Natesan Ramamurthy",
        "Darrell Reimer",
        "Alexandra Olteanu",
        "David Piorkowski",
        "Jason Tsay",
        "Kush R. Varshney"
      ],
      "title": "FactSheets: Increasing Trust in AI Services through Supplier\u2019s Declarations of Conformity",
      "year": 2018
    },
    {
      "authors": [
        "Kenneth Arrow"
      ],
      "title": "The Theory of Discrimination. InDiscrimination in Labor Markets, Orley Ashenfelter and Albert Rees (Eds.)",
      "year": 1973
    },
    {
      "authors": [
        "J.L. Austin"
      ],
      "title": "Philosophical Papers (2nd ed.)",
      "year": 1970
    },
    {
      "authors": [
        "Carliss Baldwin",
        "Christoph Hienerth",
        "Eric von Hippel"
      ],
      "title": "How user innovations become commercial products: A theoretical investigation and case study",
      "venue": "Research Policy 35,",
      "year": 2006
    },
    {
      "authors": [
        "Barry Barnes"
      ],
      "title": "Social Life as Bootstrapped Induction",
      "venue": "Sociology 17,",
      "year": 1983
    },
    {
      "authors": [
        "Rachel K.E. Bellamy",
        "Kuntal Dey",
        "Michael Hind",
        "Samuel C. Hoffman",
        "Stephanie Houde",
        "Kalapriya Kannan",
        "Pranay Lohia",
        "Jacquelyn Martino",
        "Sameep Mehta",
        "Aleksandra Mojsilovic",
        "Seema Nagar",
        "Karthikeyan Natesan Ramamurthy",
        "John Richards",
        "Diptikalyan Saha",
        "Prasanna Sattigeri",
        "Moninder Singh",
        "Kush R. Varshney",
        "Yunfeng Zhang"
      ],
      "title": "AI Fairness 360: An Extensible Toolkit for Detecting, Understanding, and Mitigating Unwanted Algorithmic Bias. arXiv:1810.01943 [cs.AI",
      "year": 2018
    },
    {
      "authors": [
        "Yochai Benkler"
      ],
      "title": "The Wealth of Networks: How Social Production Transforms Markets and Freedom",
      "year": 2006
    },
    {
      "authors": [
        "Miles Brundage"
      ],
      "title": "Artificial Intelligence and Responsible Innovation",
      "venue": "In Fundamental Issues of Artificial Intelligence, Vincent C. Mu\u0308ller (Ed.). Springer,",
      "year": 2016
    },
    {
      "authors": [
        "Miles Brundage",
        "Shahar Avin",
        "Jack Clark",
        "Helen Toner",
        "Peter Eckersley",
        "Ben Garfinkel",
        "Allan Dafoe",
        "Paul Scharre",
        "Thomas Zeitzoff",
        "Bobby Filar",
        "Hyrum Anderson",
        "Heather Roff",
        "Gregory C. Allen",
        "Jacob Steinhardt",
        "Carrick Flynn",
        "Se\u00e1n \u00d3 h\u00c9igeartaigh",
        "Simon Beard",
        "Haydn Belfield",
        "Sebastian Farquhar",
        "Clare Lyle",
        "Rebecca Crootof",
        "Owain Evans",
        "Michael Page",
        "Joanna Bryson",
        "Roman Yampolskiy",
        "Dario Amodei"
      ],
      "title": "The Malicious Use of Artificial Intelligence: Forecasting, Prevention, and Mitigation",
      "year": 2019
    },
    {
      "authors": [
        "Miles Brundage",
        "David H. Guston"
      ],
      "title": "Understanding the movement(s) for responsible innovation",
      "venue": "In International Handbook on Responsible Innovation: A Global Resource, Rene\u0301 von Schomberg and Jonathan Hankins (Eds.). Edward Elgar Publishing, Cheltenham,",
      "year": 2019
    },
    {
      "authors": [
        "Michel Callon",
        "Pierre Lascoumes",
        "Yannick Barthe"
      ],
      "title": "Acting in an Uncertain World: An Essay on Technical Democracy",
      "year": 2009
    },
    {
      "authors": [
        "Barry Canton",
        "Anna Labno",
        "Drew Endy"
      ],
      "title": "Refinement and standardization of synthetic biological parts and devices",
      "venue": "Nature Biotechnology 26,",
      "year": 2008
    },
    {
      "authors": [
        "Peter Clark",
        "Oren Etzioni"
      ],
      "title": "My Computer Is an Honor Student \u2013 but How Intelligent Is It? Standardized Tests as a Measure of AI",
      "venue": "AI Magazine 37,",
      "year": 2016
    },
    {
      "authors": [
        "Stephen Coate",
        "Glenn C. Loury"
      ],
      "title": "Will Affirmative-Action Policies Eliminate Negative Stereotypes",
      "venue": "American Economic Review 83,",
      "year": 1993
    },
    {
      "authors": [
        "Samantha Cole"
      ],
      "title": "2019. This Horrifying App Undresses a Photo of Any Woman With a Single Click",
      "year": 2019
    },
    {
      "authors": [
        "Ruth Schwartz Cowan"
      ],
      "title": "The Consumption Junction: A Proposal for Research Strategies in the Sociology of Technology",
      "venue": "In The Social Construction of Technological Systems,",
      "year": 1987
    },
    {
      "authors": [
        "William Cronon"
      ],
      "title": "Nature\u2019s Metropolis: Chicago and the Great West",
      "year": 1991
    },
    {
      "authors": [
        "Jacob Devlin",
        "Ming-Wei Chang",
        "Kenton Lee",
        "Kristina Toutanova"
      ],
      "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
      "year": 2019
    },
    {
      "authors": [
        "Roel Dobbe",
        "Sarah Dean",
        "Thomas Gilbert",
        "Nitin Kohli"
      ],
      "title": "A Broader View on Bias in Automated Decision-Making: Reflecting on Epistemology and Dynamics",
      "year": 2018
    },
    {
      "authors": [
        "David Donoho"
      ],
      "title": "Comments on Michael Jordan\u2019s Essay \u201cThe AI Revolution Hasn\u2019t Happened Yet",
      "venue": "Harvard Data Science Review (June 2019)",
      "year": 2019
    },
    {
      "authors": [
        "Tewodros Eguale",
        "David L. Buckeridge",
        "Aman Verma",
        "Nancy E. Winslade",
        "Andrea Benedetti",
        "James A. Hanley",
        "Robyn Tamblyn"
      ],
      "title": "Association of Off-label Drug Use and Adverse Drug Events in an Adult Population",
      "venue": "JAMA Internal Medicine 176,",
      "year": 2016
    },
    {
      "authors": [
        "Madeleine Clare Elish"
      ],
      "title": "Moral Crumple Zones: Cautionary Tales in Human-Robot Interaction",
      "venue": "Engaging Science, Technology, and Society",
      "year": 2019
    },
    {
      "authors": [
        "Luciano Floridi"
      ],
      "title": "Faultless responsibility: on the nature and allocation of moral responsibility for distributed moral actions",
      "venue": "Philosophical Transactions of the Royal Society A 374,",
      "year": 2016
    },
    {
      "authors": [
        "Thilo Hagendorff"
      ],
      "title": "The Ethics of AI Ethics \u2013 An Evaluation of Guidelines. arXiv:1903.03425 [cs.AI",
      "year": 2019
    },
    {
      "authors": [
        "Maarten Hajer"
      ],
      "title": "Policy without polity? Policy analysis and the institutional void",
      "venue": "Policy Sciences 36,",
      "year": 2003
    },
    {
      "authors": [
        "Douglas Heaven"
      ],
      "title": "Should we fear an AI super-troll",
      "venue": "New Scientist",
      "year": 2019
    },
    {
      "authors": [
        "Christoph Hienerth",
        "Eric von Hippel",
        "Morten Berg Jensen"
      ],
      "title": "User community vs. producer innovation development efficiency: A first empirical study",
      "venue": "Research Policy 43,",
      "year": 2014
    },
    {
      "authors": [
        "Jeremy Howard",
        "Sebastian Ruder"
      ],
      "title": "Universal Language Model Fine-tuning for Text Classification",
      "venue": "In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL)",
      "year": 2018
    },
    {
      "authors": [
        "Lily Hu",
        "Yiling Chen"
      ],
      "title": "A Short-term Intervention for Longterm Fairness in the Labor Market",
      "venue": "In Proceedings of the 2018 World Wide Web Conference (WWW",
      "year": 2018
    },
    {
      "authors": [
        "Phillip Isola",
        "Jun-Yan Zhu",
        "Tinghui Zhou",
        "Alexei A. Efros"
      ],
      "title": "Image-to-Image Translation with Conditional Adversarial Nets",
      "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)",
      "year": 2017
    },
    {
      "authors": [
        "Mandar Joshi",
        "Danqi Chen",
        "Yinhan Liu",
        "Daniel S. Weld",
        "Luke Zettlemoyer",
        "Omer Levy"
      ],
      "title": "SpanBERT: Improving Pre-training by Representing and Predicting Spans",
      "year": 2019
    },
    {
      "authors": [
        "David Kaiser"
      ],
      "title": "Drawing Theories Apart: The Dispersion of Feynman Diagrams in Postwar Physics",
      "year": 2005
    },
    {
      "authors": [
        "David Kaiser",
        "Jonathan Moreno"
      ],
      "title": "Self-censorship is not enough",
      "venue": "Nature 492,",
      "year": 2012
    },
    {
      "authors": [
        "Sertac Karaman",
        "Emilio Frazzoli"
      ],
      "title": "High-speed Flight in an Ergodic Forest",
      "venue": "In Proceedings of the 2012 IEEE International Conference on Robotics and Automation (ICRA). 2899\u20132906",
      "year": 2012
    },
    {
      "authors": [
        "Risto Karinen",
        "David Guston"
      ],
      "title": "Toward Anticipatory Governance: The Experience with Nanotechnology",
      "year": 2010
    },
    {
      "authors": [
        "Anne Kerr",
        "Rosemary L. Hill",
        "Christopher Till"
      ],
      "title": "The limits of responsible innovation: Exploring care, vulnerability and precision medicine",
      "venue": "Technology in Society",
      "year": 2018
    },
    {
      "authors": [
        "Jon Kleinberg",
        "Jens Ludwig",
        "Sendhil Mullainathan",
        "Cass R. Sunstein"
      ],
      "title": "Discrimination in the Age of Algorithms",
      "venue": "Journal of Legal Analysis",
      "year": 2019
    },
    {
      "authors": [
        "Ronald R. Kline",
        "Trevor Pinch"
      ],
      "title": "Users as Agents of Technological Change: The Social Construction of the Automobile in the Rural United States",
      "venue": "Technology and Culture 37,",
      "year": 1996
    },
    {
      "authors": [
        "John Knowles",
        "Nicola Persico",
        "Petra Todd"
      ],
      "title": "Racial Bias in Motor Vehicle Searches: Theory and Evidence",
      "venue": "Journal of Political Economy 109,",
      "year": 2001
    },
    {
      "authors": [
        "Simon Kornblith",
        "Jonathon Shlens",
        "Quoc V. Le"
      ],
      "title": "Do Better ImageNet Models Transfer Better",
      "venue": "In IEEE Conference on Computer Vision and Pattern Recognition",
      "year": 2019
    },
    {
      "authors": [
        "Roberta Kwok"
      ],
      "title": "A field in flux: Can researchers find a balance between academia and industry",
      "venue": "Nature 568,",
      "year": 2019
    },
    {
      "authors": [
        "Hannah Landecker"
      ],
      "title": "Culturing Life: HowCells Became Technologies",
      "year": 2007
    },
    {
      "authors": [
        "Bruno Latour"
      ],
      "title": "Visualisation and Cognition: Drawing Things Together. In Knowledge and Society Studies in the Sociology of Culture Past and Present",
      "venue": "Henrika Kuklick (Ed.)",
      "year": 1986
    },
    {
      "authors": [
        "Connor Leahy"
      ],
      "title": "GPT2, Counting Consciousness and the Curious Hacker",
      "year": 2019
    },
    {
      "authors": [
        "Connor Leahy"
      ],
      "title": "The Hacker Learns to Trust. https://medium",
      "year": 2019
    },
    {
      "authors": [
        "Jinhyuk Lee",
        "Wonjin Yoon",
        "Sungdong Kim",
        "Donghyeon Kim",
        "Sunkyu Kim",
        "Chan Ho So",
        "Jaewoo Kang"
      ],
      "title": "BioBERT: a pre-trained biomedical language representation model for biomedical text mining",
      "year": 2019
    },
    {
      "authors": [
        "Jieh-Sheng Lee",
        "Jieh Hsiang"
      ],
      "title": "PatentBERT: Patent Classification with Fine-Tuning a pre-trained BERT Model. arXiv:1906.02124 [cs.CL",
      "year": 2019
    },
    {
      "authors": [
        "Claire Leibowicz",
        "Steven Adler",
        "Peter Eckersley"
      ],
      "title": "When Is It Appropriate to Publish High-Stakes AI Research",
      "year": 2019
    },
    {
      "authors": [
        "Kevin Liao",
        "Matthew A. Hammer",
        "Andrew Miller"
      ],
      "title": "ILC: a calculus for composable, computational cryptography",
      "venue": "In Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation",
      "year": 2019
    },
    {
      "authors": [
        "Yinhan Liu",
        "Myle Ott",
        "Naman Goyal",
        "Jingfei Du",
        "Mandar Joshi",
        "Danqi Chen",
        "Omer Levy",
        "Mike Lewis",
        "Luke Zettlemoyer",
        "Veselin Stoyanov"
      ],
      "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach. arXiv:1907.11692v1 [cs.CL",
      "year": 2019
    },
    {
      "authors": [
        "Edwin A. Locke",
        "Gary P. Latham"
      ],
      "title": "Building a practically useful theory of goal setting and task motivation: A 35-year odyssey",
      "venue": "American Psychologist 57,",
      "year": 2002
    },
    {
      "authors": [
        "Jiasen Lu",
        "Dhruv Batra",
        "Devi Parikh",
        "Stefan Lee"
      ],
      "title": "ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Visionand-Language Tasks. arXiv:1908.02265 [cs.CV",
      "year": 2019
    },
    {
      "authors": [
        "Donald MacKenzie"
      ],
      "title": "An Engine, Not a Camera: How Financial Models Shape Markets",
      "year": 2006
    },
    {
      "authors": [
        "Dhruv Mahajan",
        "Ross Girshick",
        "Vignesh Ramanathan",
        "Kaiming He",
        "Manohar Paluri",
        "Yixuan Li",
        "Ashwin Bharambe",
        "Laurens van der Maaten"
      ],
      "title": "Exploring the Limits of Weakly Supervised Pretraining",
      "venue": "In European Conference on Computer Vision (ECCV)",
      "year": 2018
    },
    {
      "authors": [
        "Gary Marcus",
        "Francesca Rossi",
        "Manuela Veloso"
      ],
      "title": "Beyond the Turing Test",
      "venue": "AI Magazine 37,",
      "year": 2016
    },
    {
      "authors": [
        "Chandler May",
        "Alex Wang",
        "Shikha Bordia",
        "Samuel R. Bowman",
        "Rachel Rudinger"
      ],
      "title": "On Measuring Social Biases in Sentence Encoders",
      "venue": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)",
      "year": 2019
    },
    {
      "authors": [
        "Bryan McCann",
        "James Bradbury",
        "Caiming Xiong",
        "Richard Socher"
      ],
      "title": "Learned in Translation: Contextualized Word Vectors",
      "venue": "In Advances in Neural Information Processing Systems 30,",
      "year": 2017
    },
    {
      "authors": [
        "Bryan McCann",
        "Nitish Shirish Keskar",
        "Caiming Xiong",
        "Richard Socher"
      ],
      "title": "The Natural Language Decathlon: Multitask Learning as Question Answering",
      "year": 2018
    },
    {
      "authors": [
        "Sameep Mehta",
        "Rakesh Pimplikar",
        "Amit Singh",
        "Lav R. Varshney",
        "Karthik Visweswariah"
      ],
      "title": "Efficient multifaceted screening of job applicants",
      "venue": "In Proceedings of the 16th International Conference on Extending Database Technology",
      "year": 2013
    },
    {
      "authors": [
        "Robert K. Merton"
      ],
      "title": "The Self-Fulfilling Prophecy",
      "venue": "Antioch Review 8,",
      "year": 1948
    },
    {
      "authors": [
        "Vinith Misra"
      ],
      "title": "Minimum Context Channel Decoding",
      "venue": "School of Information Theory",
      "year": 2010
    },
    {
      "authors": [
        "Margaret Mitchell",
        "Simone Wu",
        "Andrew Zaldivar",
        "Parker Barnes",
        "Lucy Vasserman",
        "Ben Hutchinson",
        "Elena Spitzer",
        "Inioluwa Deborah Raji",
        "Timnit Gebru"
      ],
      "title": "2019. Model Cards for Model Reporting",
      "venue": "In Proceedings of the Conference on Fairness, Accountability,",
      "year": 2019
    },
    {
      "authors": [
        "Hussein Mouzannar",
        "Mesrob I. Ohannessian",
        "Nathan Srebro"
      ],
      "title": "From Fair Decision Making To Social Equality",
      "venue": "In Proceedings of the Conference on Fairness,",
      "year": 2019
    },
    {
      "authors": [
        "Lisa D. Ord\u00f3\u00f1ez",
        "Maurice E. Schweitzer",
        "Adam D. Galinsky",
        "Max H. Bazerman"
      ],
      "title": "Goals Gone Wild: The Systematic Side Effects of Overprescribing Goal Setting",
      "venue": "Academy of Management Perspectives",
      "year": 2009
    },
    {
      "authors": [
        "Nelly Oudshoorn",
        "Trevor Pinch"
      ],
      "title": "How Users and Non-Users Matter. In How Users Matter: The Co-Construction of Users and Technology, Nelly Oudshoorn and Trevor Pinch (Eds.)",
      "year": 2003
    },
    {
      "authors": [
        "Charles Perrow"
      ],
      "title": "Normal Accidents: Living with High Risk Technologies",
      "venue": "Basic Books",
      "year": 1984
    },
    {
      "authors": [
        "MatthewE. Peters",
        "MarkNeumann",
        "Mohit Iyyer",
        "Matt Gardner",
        "Christopher Clark",
        "Kenton Lee",
        "Luke Zettlemoyer"
      ],
      "title": "Deep contextualized word representations",
      "venue": "In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)",
      "year": 2018
    },
    {
      "authors": [
        "William H. Pierce"
      ],
      "title": "Failure-Tolerant Computer Design",
      "year": 1965
    },
    {
      "authors": [
        "Alec Radford",
        "Jeffrey Wu",
        "Dario Amodei",
        "Daniela Amodei",
        "Jack Clark",
        "Miles Brundage",
        "Ilya Sutskever"
      ],
      "title": "Better Language Models and Their Implications",
      "year": 2019
    },
    {
      "authors": [
        "Alec Radford",
        "Jeffrey Wu",
        "Rewon Child",
        "David Luan",
        "Dario Amodei",
        "Ilya Sutskever"
      ],
      "title": "Language Models are Unsupervised Multitask Learners",
      "year": 2019
    },
    {
      "authors": [
        "Harvey M. Sapolsky"
      ],
      "title": "The Politics of Risk",
      "venue": "Daedalus 119,",
      "year": 1990
    },
    {
      "authors": [
        "Andrew D. Selbst",
        "danah boyd",
        "Sorelle A. Friedler",
        "Suresh Venkatasubramanian",
        "Janet Vertesi"
      ],
      "title": "Fairness and Abstraction in Sociotechnical Systems",
      "venue": "In Proceedings of the Conference on Fairness,",
      "year": 2019
    },
    {
      "authors": [
        "Sonali K. Shah"
      ],
      "title": "From Innovation to Firm Formation: Contributions by Sports Enthusiasts to the Windsurfing, Snowboarding & Skateboarding Industries",
      "venue": "In The Engineering of Sport 6, Eckehard Fozzy Moritz and Steve Haake (Eds.). Springer,",
      "year": 2006
    },
    {
      "authors": [
        "Claude E. Shannon"
      ],
      "title": "A Mathematical Theory of Communication",
      "venue": "The Bell System Technical Journal",
      "year": 1948
    },
    {
      "authors": [
        "Stuart M. Shieber"
      ],
      "title": "Principles for Designing an AI Competition, or Why the Turing Test Fails as an Inducement Prize",
      "venue": "AI Magazine 37,",
      "year": 2016
    },
    {
      "authors": [
        "Michael Spence"
      ],
      "title": "Job market signaling",
      "venue": "Quarterly Journal of Economics 87,",
      "year": 1973
    },
    {
      "authors": [
        "Michael A. Steel",
        "L\u00e1szl\u00f3 A. Sz\u00e9kely"
      ],
      "title": "Inverting Random Functions II: Explicit Bounds for Discrete Maximum Likelihood Estimation, with Applications",
      "venue": "SIAM Journal on Discrete Mathematics 15,",
      "year": 2002
    },
    {
      "authors": [
        "Jack Stilgoe",
        "Richard Owen",
        "Phil Macnaghten"
      ],
      "title": "Developing a framework for responsible innovation",
      "venue": "Research Policy 42,",
      "year": 2013
    },
    {
      "authors": [
        "Emma Strubell",
        "Ananya Ganesh",
        "Andrew McCallum"
      ],
      "title": "Energy and Policy Considerations for Deep Learning in NLP",
      "venue": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
      "year": 2019
    },
    {
      "authors": [
        "Lav R. Varshney"
      ],
      "title": "Block Diagrams in Information Theory: Drawing Things Closed",
      "venue": "In Society for the History of Technology Annual Meeting. Special Interest Group on Computers,",
      "year": 2014
    },
    {
      "authors": [
        "Lav R. Varshney"
      ],
      "title": "Fundamental Limits of Data Analytics in Sociotechnical Systems. Frontiers in ICT 3, 2 (Feb. 2016)",
      "year": 2016
    },
    {
      "authors": [
        "Lav R. Varshney"
      ],
      "title": "Mathematical limit theorems for computational creativity",
      "venue": "IBM Journal of Research and Development 63,",
      "year": 2019
    },
    {
      "authors": [
        "Eric von Hippel"
      ],
      "title": "2017. Free Innovation",
      "year": 2017
    },
    {
      "authors": [
        "Ren\u00e9 von Schomberg"
      ],
      "title": "Why Responsible Innovation? In International Handbook on Responsible Innovation: A Global Resource",
      "venue": "Rene\u0301 von Schomberg and Jonathan Hankins (Eds.). Edward Elgar Publishing, Cheltenham,",
      "year": 2019
    },
    {
      "authors": [
        "Alex Wang",
        "Amanpreet Singh",
        "Julian Michael",
        "Felix Hill",
        "Omer Levy",
        "Samuel R. Bowman"
      ],
      "title": "GLUE: A multi-task benchmark and analysis platform for natural language understanding",
      "venue": "In Proceedings of the International Conference on Learning Representations (ICLR)",
      "year": 2019
    },
    {
      "authors": [
        "Wei Yang",
        "Yuqing Xie",
        "Aileen Lin",
        "Xingyu Li",
        "Luchen Tan",
        "Kun Xiong",
        "Ming Li",
        "Jimmy Lin"
      ],
      "title": "End-to-End Open-Domain Question Answering with BERTserini. arXiv:1902.01718 [cs.CL]. Pretrained AI Models: Performativity, Mobility, and Change",
      "year": 2019
    },
    {
      "authors": [
        "Zhilin Yang",
        "Zihang Dai",
        "Yiming Yang",
        "Jaime Carbonell",
        "Ruslan Salakhutdinov",
        "Quoc V. Le"
      ],
      "title": "XLNet: Generalized Autoregressive Pretraining for Language Understanding",
      "year": 2019
    },
    {
      "authors": [
        "Jieyu Zhao",
        "Tianlu Wang",
        "Mark Yatskar",
        "Vicente Ordonez",
        "Kai- Wei Chang"
      ],
      "title": "Men Also Like Shopping: Reducing Gender Bias Amplification Using Corpus-Level Constraints",
      "venue": "In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
      "year": 2017
    }
  ],
  "sections": [
    {
      "heading": "1 INTRODUCTION",
      "text": "Large-scale deep learning models with billions of parameters can now perform a variety of natural language and vision tasks at or above human levels, but require significant computational\u2014and therefore energetic/monetary\u2014 resources to train. As such, the development of these models has largely been carried out by artificial intelligence (AI) researchers in large institutions (especially for-profit companies) [79], and is out of reach for researchers in smaller institutions/academia and other technically-skilled AI enthusiasts. For brevity, we will sometimes refer to these social groups as producers and lead users, following the user-based innovation [92] and social construction of technology (SCOT) [42] literatures.\nThe development and release of pretrained deep learning models by producers has recently emerged as a standard paradigm in AI practice, allowing lead users to then finetune and transfer them for use in a variety of research and societal settings. In natural language processing, examples\nof pretrained models include BERT [21], GPT-2 [76], ELMo [73], and XLnet [96]. Beyond the models themselves, producers may or may not also release the training dataset, the code implementing the learning rule, or descriptions of the computational infrastructure; this provides varying levels of transparency. Unfortunately, as we detail in the sequel, pretrained models may embed biases in unknown and immutable ways while also enabling unintended negative uses. In this paper, we treat pretrained models as objects of study and discuss the impact their sociological position has on fairness, accountability, and transparency in the larger sociotechnical systems in which they are embedded. We draw on analytical frameworks from science and technology studies (STS). Without taking a strong normative position, we especially focus on implications for AI governance processes.\nWhether considering nuclear reactions, recombinant DNA technology, or mutant flu strains, much scientific research and innovation can benefit the public but also be diverted to harmful uses. A typical reaction by scientists performing such dual-use research has been self-regulation and selfimposed moratoriums, yet careful historical study demonstrates the inadequacy of this. As Kaiser and Moreno argue, \u201cno matter the field of research, can anyone be expected to step outside the excitement and momentum of their own work to make objective decisions in risky situations?\u201d [37]. Here we suggest such momentum may be even stronger when entire research fields are oriented around a quest to achieve a singular objective\u2014Holy Grail performativity [89] in Austin\u2019s sense of concepts being performed in practice [5, 57]. The ascendancy of the so-called common task framework in AI [23] embodies exactly such performativity, yet the dual-use potential of pretrained models has led to recent attempts at self-imposed limits on open release [30, 75]. The growing responsible innovation literature within science and technology studies [86] has been discussed in relation to AI by Brundage [10, 12], but these self-regulation actions are seemingly not informed by understanding the position of pretrainedmodels that emerges from treating them as objects of sociological study, cf. [11]. We will discuss how insights from responsible innovation and broader STS discourse may inform AI governance policies. ar X iv :1 90 9. 03 29 0v 1\n[ cs\n.C Y\n] 7\nS ep\n2 01\n9\nThe responsible innovation literature has, as far as we can tell [40], remained unconcerned with user-driven innovation [92] and users as agents of technological change [42, 71]. Yet, user innovation is of central importance in AI, where innovation comes not just from producers of pretrained models but also lead users of pretrained models that fine-tune and transfer them to applications outside the control (and often outside the imagination) of the producers.1 When pretrained models are at the consumption junction (in the sense of Cowan [19]), they may be reinterpreted in malicious ways. Taking the case of AI, we will discuss how principles of governance from responsible innovation should be extended to consider the role of users as innovators. Of particular relevance for this extension is to understand how pretrained models are developed and evolve as they move among actors in the two social groups. Von Hippel and colleagues have noted a kind of division of labor in types of innovations pursued by producers and users for scientific instruments [77] and whitewater kayaking [31]. Producers pursue innovations of interest to the entire market, typically along a fixed dimension of merit such as faster, cheaper, or more reliable. Contrarily, users pursue innovations to do functionally new things without strong concern for the fraction of the market that may be interested, since they are self-rewarded through intrinsic motivations [6, 92]. A similar division is seen in AI where an initial general-purpose model like BERT [21] is developed by an industrial research lab (and improved by another industrial research lab as RoBERTa [54]) whereas academic researchers fine-tune/transfer such a model to have other more specific functionality such as BioBERT [50] (for medical text mining), ViLBERT [56] (for vision-and-language tasks), and BERTserini [95] (for question-answering). Although pretrained models are not diagrams or inscriptions having the possibility of optical consistency in the sense of Latour [47], they do move around among actors in the AI community and are a kind of computationally immutable mobile where the immutability stems from the computational costs in modification. Despite the consistency of pretrained models\u2014at the level of individual bits\u2014as they move, they remain interpretively flexible for users. Since these models are largely obtained anonymously from open source repositories rather than from personal instructional interaction (like Feynman diagrams [36]), further interpretive flexibility is maintained. In fact it is this plasticity of pretrained models in the hands of users\u2014who can fine-tune them for transfer to alternative tasks\u2014that has given them their staying power.\n1Note that in this paper, we do not consider the final consumers of AI inferences, which are another social group altogether.\nMoving from responsible innovation considerations due to intentional malicious use by users, we also consider unintentional ethical issues such as algorithmic unfairness that may be immutably embedded in pretrained models. Since these models are often abstracted by users as essentially black boxes with general intelligence ability that can be fine-tuned to transfer to any task, the biases in models and their training data are not considered [80]. Moreover, due to computational immutability, these biases are fixed. More troublingly, as we detail later, the descriptions of the world (including societal biases) embedded in pretrained models have Barnesian performativity [7, 57], in the sense they may act to shape the future evolution of the world. That is, as noted in classical economic theories of discrimination [4, 17, 84] and recent models of model retraining [22, 33, 67], populations might becomemore like what (biased) models predict. In fact, biases may even be amplified through the fine-tuning and transfer carried out by lead users [97] in a single stage of evolution. We will further discuss how this understanding of unfairness propagation can inform AI governance. To summarize, we revisit responsible innovation in the context of AI fairness, accountability, and transparency by characterizing the social position of pretrained models:\n\u2022 Holy Grail performativity in model development due to the common task framework, \u2022 Users as innovators and agents of technological change through fine-tuning and transfer, \u2022 Computational immutability but interpretive flexibility of pretrained models as they move among actors, and \u2022 Barnesian performativity of pretrainedmodels in terms of the evolution of algorithmic fairness."
    },
    {
      "heading": "2 SELF-REGULATION AND THE HOLY GRAIL OF PRETRAINED MODEL DEVELOPMENT",
      "text": "In February 2019, OpenAI developed a large-scale unsupervised language model called GPT-2 (Generative Pretrained Transformer 2) to generate several coherent sentences of realistic text by extending any given seed. This model further simultaneously performs well on a variety of language tasks including question answering, reading comprehension, summarization, and translation [76]. We should further note that in general, better pretrained models lead to better performance on fine-tuned or transfer tasks [44, 58]. Contrary to recent practice in the artificial intelligence community, OpenAI did not release the training data or the learned parameters of their largest neural network model, only smaller ones. This, due to concerns that large language models may be used to generate deceptive, biased, or abusive language at scale. In describing their decision to limit the transparency of the GPT-2 model, the producers described several positive and negative uses, which we quote here [75]:\n[+] AI writing assistants, more capable dialogue agents, unsupervised translation between languages, and better speech recognition systems [\u2013] Generate misleading news articles, impersonate others online, automate the production of abusive or faked content to post on social media, automate the production of spam/phishing content\nAs seen, the producers themselves did not specify too many functionally novel uses. Yet, lead users quickly transferred the model to multifarious settings; a positive example using a smaller version of GPT-2 that OpenAI did release, Deep TabNine is a software programming productivity tool2 to predict the next chunk of code, fine-tuned on open source files from GitHub capturing numerous programming languages.\nNotwithstanding numerous arguments against GPT-2 actually posing a societal threat [30], the self-regulation practiced by OpenAI is rather limited. It is not a self-moratorium but only a limitation on distributing detailed results (the pretrained model itself). Indeed, a student with significant computational resources provided by Google [48] purportedly reproduced OpenAI\u2019s GPT-2 model, though he also did not release his model for verification [49] citing similar concerns of malicious uses, especially with respect to setting social norms for future release of dual-use AI technology.\nAs noted in Section 1, several cases in the history of science have shown that self-moratoriums are ineffective, to say nothing of limited self-regulation that does not militate the pursuit of technological progress and may even encourage it. After all, knowing that something can be done is often a greater spur for future innovation than a detailed description of how it was done.\nIn the next subsections, we describe certain social norms among producers of pretrained models and then discuss why these norms render self-regulation inadequate."
    },
    {
      "heading": "Pursuing Holy Grails",
      "text": "In building engineering systems\u2014whether physical systems like engines or informational ones like AI\u2014benchmarking performance to understand how well one is doing is often cast as important. To do so, scientists try to both establish clear metrics of performance (oftenmeasured in standardized units) and have useful points of comparison. In this vein, the Scottish engineer James Watt developed the concept of horsepower to benchmark the output of steam engines by comparing to the power of draft horses. Indeed, comparing performance of new technologies with either humans or animals that have similar abilities is a typical strategy.\n2https://tabnine.com/blog/deep\nIn AI, the Turing test has been proposed as a way to measure a machine\u2019s ability to exhibit intelligent behavior by making a binary comparison to people. A machine is said to be intelligent if it exhibits behavior equivalent to, or indistinguishable from, that of a human. This is to be tested through a conversation with human judges [88]. There are well-known limitations of the Turing test in terms of gaming, cheating, and operational difficulty. It is also largely focused on language ability at the neglect of other facets of intelligence such as perception or creativity. As such, several alternatives have recently been proposed [59]. One basic property of these new test proposals is scoring intelligence in a graded manner, rather than just all-ornothing, cf. [1, 16]. This provides a refined characterization of system performance on a quantitative scale. Yet, these approaches still essentially use human performance as a benchmark for comparison, even though there is much variation in human intelligence not only within populations but even across the historical record [68] (and may therefore not be absolute milestones, contrary to [83]). A typical AI leaderboard oriented as the pursuit of human performance is shown in Figure 1, here measuring performance with standard evaluation data (an idea we will return to).\nAn alternative to judging performance relative to animals or humans is to establish fundamental theoretical limits. Whether considering the Carnot limit on the efficiency of engines [15] or the Shannon limit on reliable communication in\nthe presence of noise [82], engineering systems theories establishwhat is possible andwhat is impossible. The boundary between the two is what is optimal. Thus, such limit theorems provide absolute standards by which performance may be measured. If a communication scheme operates within 0.0045 dB of the Shannon limit, this is nearly as good as can be and is independent of how efficiently human communication operates in noise. Once an absolute scale anchored on fundamental limits is established, human performance can also be fixed as a statistical distribution on that spectrum.3 There are several AI settings for which non-constructive fundamental limits are known: by non-constructive we mean that although the limits can be computed, strategies that actually achieve those limits are not known. Examples include flying without crashing [38], combinatorial creativity [91], communicating with aliens [65], and reconstructing the tree of life [85]. In data-driven areas of AI, however, it is often not possible to define fully closed deductive systems [69] in which to reason about fundamental limits (even if in principle, Bayes risk is a fundamental limit). To emulate the kind of abstraction achieved in closed deductive systems, the common task framework has emerged as a prevailing paradigm for AI model development [23]. The idea is to pursue best task performance on a fixed dataset, split into training and testing portions. Figure 1 shows the common task framework in action where the standardized GLUE dataset [94] is used to assess performance of different AI models on a standard task set. As seen, producers aim to develop AI models that perform better than humans and each other, and the top results are common pretrained models such as RoBERTa and XLNet.\nMoreover, because ideals are data- and task-specific, there can be a progression of goals within the common task framework, different than information-theoretic or thermodynamic limits which are fixed by the closed deductive system. For example, the header of Figure 1 indicates that SuperGLUE has been developed as a successor to GLUE."
    },
    {
      "heading": "Inadequacy of Self-Governance",
      "text": "Drawing on the historical case of coding theory being organized as a quest to achieve information-theoretic limits, Varshney had argued that closed universes of deductive discourse and fundamental limits within them lead to Holy 3There are some settings where animals perform intelligent behavior nearly at the fundamental limits [2]. Examples include great tits (Parus major) nearly achieving optimal performance in feeding strategy, as described by two-armed bandit exploration/exploitation tradeoffs; moose (Alces alces) essentially achieving the optimal diet in a Michigan national park, as given by the solution of a linear program; and lions (Panthera leo) having hunting behavior over time that matches the solution to a dynamic programming optimization. In all examples, performance of particular behavioral strategies taken by given organisms has been compared to the best performance possible by any strategy.\nGrail performativity [89]. That is, introducing the concept of a limiting ideal is performative: the use in practice of a theoretical concept orients research and innovation more towards that theoretical concept. As Pierce described [74], again about coding as a quest to achieve information-theoretic limits, \u201cit may be true that communications theorists could have devised error-correcting schemes even if they never knew of the limit theorems of information theory, but it is doubtful that theywould have tried so hard and so well without limit theorems with which to compare their results (and occasionally to goad themselves).\u201d This strongly captures the central thesis of goal-setting theory, a well-established theory of motivation in psychology [55]. The idea is that the most effective performance seems to result when goals are specific and challenging. Further, psychological momentum in pursuing a set goal is difficult to attenuate. Indeed, in Holy Grail performative settings where entire social groups are pursuing the same specific goals, this behavioral momentum is strengthened by social comparison (as facilitated by leaderboards). When the goals also evolve to become more difficult, this allows actors in the social group to \u201clevel up\u201d, yielding greater motivation. A side effect of goal setting, however, may be a narrow focus that neglects non-goal areas [70].\nThese behavioral factors are redolent of Kaiser andMoreno\u2019s claim that innovators cannot be expected to step outside the momentum of their work to self-regulate. Developing AI models within the common task framework has Holy Grail performative social norms, much more so than, say, DNA recombination where innovators had disparate functional goals. As such, self-regulation is especially inadequate and alternative governance approaches developed within the responsible innovation literature should be considered."
    },
    {
      "heading": "3 USERS AS FINE-TUNERS OF PRETRAINED MODELS",
      "text": "As we have seen, the culture of AI model producers is very much Holy Grail performative, pursuing innovation along a dimension of merit like the GLUE score in Figure 1. In this section, we turn to the social group of lead users, who are concerned with functionally new applications of AI models [92]. As Cowan has argued [19], analysis focused on users allows for the possibility of unintended consequences, \u201cwithout which no sociological or historical explanation should be taken seriously\u201d. Yet, the responsible innovation literature has remained unconcerned with user-driven innovation, cf. [86].\nWithin AI governance, too, the distinct role of innovative users seems to be unconsidered, see e.g. a recent survey on AI ethics frameworks that does not consider the social group of users [28].\nFigure 2: In transferring a pretrained neural networkmodel to a functionally new task, the early layers may be frozen as is, and the last few layers retrained using new data for the new task."
    },
    {
      "heading": "Users as AI Innovators",
      "text": "User innovation is of central importance in AI, where innovative lead users of pretrained models fine-tune and transfer them to functionally new applications, often far beyond what producers may have imagined. Although we do not discuss it further here, a closely related setting is multi-tenant cloud provision of AI models where the model creator does not have access to the data or application scenario for which the customer is deploying the model.\nBefore proceeding, let us briefly describe the technological approach used for transferring a neural network model developed for one task to work on a second task by fine-tuning. In deep neural networks\u2014taking feedforward networks such as multilayer perceptrons or convolutional neural networks as examples shown in Figure 2\u2014it has been found that early layers of models produce features that capture general attributes of the training dataset whereas later layers of the model capture properties of the task it is trained on. As such, one approach for inductive transfer of a model for one task (the pretrained model) to become a model for a different task (the fine-tuned model) is to freeze the early layers from the pretrained model and retrain the last couple layers using a new data set and a new task. This is computationally much easier than training a new model from scratch: since most learned parameters are taken straight from the pretrained model, a much smaller number of parameters must be learned. In essence, this works by beneficially narrowing the scope of possible models for the new task. The basic idea is depicted in Figure 2, using the now-standard diagrammatic style for neural network architectures. Putting nuclear technology to new uses requires large, expensive facilities and using recombinant DNA technology requires specialized reagents, but fine-tuning and transfer of AI models does not require either. Fine-tuning AI models is feasible for a large social group of lead users. Even though\nproducers of pretrained models may have a particular meaning in mind, they do not control how these artifacts are used once in the hands of users. As noted in the social construction of technology literature, \u201cusers precisely as users can embed new meanings into the technology\u201d [42]. Indeed, it is well-established in the user-driven innovation literature [92] that lead users come up with numerous functionally new applications. In the early twentieth century, Ford had built the Model T with the singular interpretation as a passenger vehicle, but rural American users put it to use as a power source for washing machines, butter churns, cream separators, corn shellers, water pumps, hay balers, fodder and ensilage cutters, wood saws, hay and grain hoists, cider presses, and corn grinders, as well as in mobile form as a snowmobile, tractor, and agricultural transport vehicle. This interpretive flexibility of users later pushed Ford Motor Company itself to create modification kits for the Model T [42]. Although perhaps not quite as general-purpose technology as a car engine, pretrained AI models have also been put to use in numerous settings. Taking the example of the BERT language model [21], it has been used for clinical medicine (clinicalBERT), scientific research (SciBERT), story generation (TransBERT), and intellectual property law (PatentBERT), among many other language task settings just within a year of its release. Besides these functionally new innovations that are putatively societally beneficial, lead users of AI models have also reinterpreted them perniciously to innovate in societally harmful ways. A typical example is DeepNude, an app that removes clothing from the images of women, making them look realistically nude [18] and is based on the previous pix2pix image transfer model [34]. Katelyn Bowden, founder and CEO of revenge porn activism organization Badass, was quoted as saying \u201cNow anyone could find themselves a victim of revenge porn, without ever having taken a nude photo. This tech should not be available to the public\u201d [18].\nUsers act as agents of technological change [71] not only in changing the interpretation of artifacts as we have described, but also in shaping the future design of artifacts themselves [42]. The relationships among social groups both constrain and enable the design and usage of technology, and the social groups in turn get shaped in designing/using the technology.\nThe design and use of skateboards took place as communitybased innovation, with significant back-and-forth among producers and users [81]. AI models are also developed largely within a tightly knit community. Academically-inclined producers and users publish papers in the same scholarly conferences; individual innovators may be users during their training at universities and then become producers when they join large companies (while also doing industrial internships in between). Senior researchers may frequently move between universities and industry, even simultaneously having\ndual appointments in both [45]. Moreover, notwithstanding Section 2, deep learning is largely an open source community which further enables interactions among actors.\nIn general, pretrained models that perform better on their benchmarks also perform better after fine-tuning on transfer tasks [44, 58]. Yet, the strong interaction of users with producers through community links has led to pretrained models that are specifically designed to be better at inductive transfer to other tasks [32, 61]. As a notable example, consider SpanBERT (not a fine-tuning of BERT, but a new pretrained model inspired by BERT) designed to be better at inductive transfer to new language tasks [35]; its developers are primarily from Facebook but also have participants from academia. The basic idea is to design the pretrained model to better represent and predict spans of text, which arise in several functionally novel language tasks; BERT was concerned with individual words rather than spans of words. As a variation, pretrained models can specifically be trained for multiple tasks simultaneously [62], aiming to generalize well to any task.\nWe have seen that the flexible interpretation and needs of lead users of pretrained models both influence the design of future pretrained models and lead to functionally new innovations through fine-tuning."
    },
    {
      "heading": "Inadequacy of Producer-Focused Governance",
      "text": "Although producers and lead users are coupled within the AI community, there is a division of labor between the two social groups, which imply distinct considerations for AI governance. As such, a focus solely on governance for producers would neglect the network of social relations among actors in the AI ecosystem, and the nature of accountability propagating through the actor network [19, 27].\nPretrained models at the consumption junction, as Cowan describes it [19], may be interpreted in both beneficent and maleficent ways and therefore yield both putatively positive and negative unintended consequences. Prima facie, beneficence and non-maleficence are desirable, but these must be balanced in AI governance, as embedded in the interaction network of producers and users.\nThe case of pretrained AI models suggests that responsible innovation should be expanded to include the role of users."
    },
    {
      "heading": "4 PRETRAINED MODELS AS (IM)MUTABLE MOBILES",
      "text": "We have seen in the previous section that users interpret pretrained AI models in various ways, and transfer them to numerous functionally new uses through fine-tuning. In this section, we look more at how the models themselves move through the relevant social groups and how an understanding of such information spreading may inform AI\ngovernance. A recent survey indicates that the spreading dynamics, fine-tuning, and recombination of AI models do not enter into existing AI governance frameworks [28], which instead focus only on initial development and release. Just as in Section 3, this omission suggests the value in expanding the scope of responsible AI innovation from a static focus on release to considering network dynamics."
    },
    {
      "heading": "Pretrained Models in Action",
      "text": "Pretrained models are mathematical objects that specify particular neural network architectures and learned synaptic weights: they are functional and can be used directly to perform inference when deployed as AI services or as part of larger AI services in sociotechnical systems, cf. [3, 90]. Although they are formalisms, they are not abstractions (under common definitions), but the thing itself. Abstractions such as neural network architecture diagrams of the type in Figure 2 also move around\u2014with Latourian optical consistency [47]\u2014among actors in the AI social network, but we focus on the pretrained AI models themselves.4\nPretrained models move around with not just optical consistency, but mathematically precise identicality. Indeed, the raison d\u2019\u00eatre of pretrainingmodels is to move with no change, due to the computational cost in training large AI models. In this sense, they are computationally immutable mobiles; yet, as we saw in Section 3, they are interpretively flexible. They are essentially physically immutable like car engines, which are difficult to modify without specialized equipment, rather than mutable like paper tools that are inherently plastic [36]. Although there are local, personal instructional interactions (including academic training relationships) among actors in AI, the primary way pretrained models are disseminated is through postings to open source repositories such as GitHub, see e.g. Figure 3. Insights into design ideas and detailed performance characterization are disseminated through preprint servers such as arXiv, together with more informal explanations as blog posts, which may further spread through social media such as Twitter. In such a technologymediated open source community [9], models circulatewidely from their original point of dispersion. One can see more than 4000 forks of the BERT model in Figure 3 by a wide variety of users, to say nothing of downloads that were then fine-tuned. Despite limited institutional gate keepers in open source settings (like journal editors, as in some branches of science), cultural norms do lead to a kind of file drawer problem [26] where only effective (with respect to producers\u2019 benchmarks) models and approaches are put into circulation by producers. 4Neural network architecture search and hyperparameter tuning is even more computationally intensive than training single neural networks [87] and so architecture diagrams may take a similar sociological position as pretrained models.\nIneffective ideas only spread by local instructional interaction through personal contact.\nIn addition to models, datasets, and code spreading, there have been several suggestions to create model cards for pretrained models [66] or fact sheets for larger compositions of models as AI services [3] that move along with them. Like nutrition labels for food or parts sheets for electronics, they are meant to be documentation listing performance characterization, contexts for intended usage, as well as properties such as safety (including fairness/explainability), security, and provenance. By listing contexts for intended usage, such documentation is meant to avoid the portability trap arising from abstraction [80], as pretrained models move around. As far as we know, such documentation approaches have not been put into widescale practice. Although Mitchell et al. [66] suggest model cards may inform users on \u201cdifferent options for fine-tuning, model combination,\u201d such documentation does not capture how performance, appropriateness, or safety properties may change under fine-tuning and transfer to other tasks, even though this is a primary mode of use. Moreover, model cards or fact sheets do not include an expiration date for validity in an ever-changing world.\nArnold et al. [3] argue that \u201csystems composed of safe components may be unsafe and, conversely, it may be possible to\nbuild safe systems out of unsafe components,\u201d and therefore focus on specific larger AI services composed of AI models. The danger of compositions may be especially pernicious when AI models are sociotechnically coupled in complex and tight ways [72]. Fact sheets do not consider recombinations of models or novel combinations of models. Indeed, there is as yet, no compositional calculus for the properties of AI models, like there is in cryptography [53]; unlike other part sheets [14], proposed AI documentation does not even indicate how to put pieces from a library together to build more complicated AI services (in sociotechnical systems [90]).\nTo summarize, AI models are computationally immutable but interpretively flexible. They become dissociated from context as they move around\u2014despite attempts to counter this using detailed documentation. This is especially the case since dissemination is largely technology-mediated and disconnected from personal interactions."
    },
    {
      "heading": "Inadequacy of Static Governance",
      "text": "Once AI models are developed, they move around. Indeed, much of the action is in this spreading and reinterpretation. Since there is a decentralized, technology-mediated network of dissemination and change, there are no Latourian \u201ccenters of calculation\u201d that maintain their scientific prominence and authority by having people continually return to them from hinterlands. Prominence within the common task framework may come directly from good performance on specifically stated criteria (achieving which often requires significant resources). As such, AI governance that only considers existing centers of production and their initial act of dissemination will be inadequate.\nMoreover, the fine-tuning and combining of AImodels that happens as they move is not mere bricolage, but is governed by the interpretive flexibility that users have for AI models. An understanding of the changing and combining dynamics of AI models is essential to effective AI governance.\nThe case of pretrained AI models suggests that responsible innovation should be expanded to consider the mechanisms and dynamics of spreading throughout the actor network."
    },
    {
      "heading": "5 BARNESIAN PERFORMATIVITY OF UNFAIR PRETRAINED MODELS",
      "text": "As AI models move through actor networks, become reinterpreted, and are fine-tuned to transfer to contexts previously unimagined, they remain computationally immutable. One particular property of pretrained AI models and services that model cards and fact sheets aim to capture is fairness, which necessarily also remains computationally fixed. When fine-tuned or composed into larger AI services, pretrained models are often interpreted as abstract black boxes of intelligence dissociated from context, much like grain is abstracted when put into a grain elevator, dissociated\nfrom its source [20], or gamete cells are abstracted as reagents when carrying out long-term freezing, dissociated from space and time [46]. For example, in describing PatentBERT, Lee and Hsiang [51] only say:\nIn this work, we leverage the released BERTBase pre-trained model (Uncased: 12-layer, 768- hidden, 12-heads, 110M parameters) . . .Our implementation follows the finetuning example released in the BERT project . . .We intentionally keep the code change as minimal as possible\nand never discuss any further properties of the BERT model or the dataset it was trained on. Cast as black boxes, the internal properties of pretrained models are not of central interest to many lead users. Even if there were model cards that specifically call out properties such as fairness and these are brought to the attention of users, these characterizations may fall into a formalism trap of abstraction, since summary statistics would not capture e.g. contextual or contestable aspects of fairness [80].\nYet, there is (appropriately defined) unfairness along many socially observable dimensions embedded within pretrained models [60]. Despite no animus\u2014only apathy\u2014on the part of actors in the community, this implies unfairness in pretrained models can spread widely. Moreover, unfairness in AI models can actually exacerbate unfairness in society itself through a kind of Barnesian performativity, as we describe next. Recall that Barnesian performativity is the effect that using a model in practice makes a societal process more like its depiction by that model [57]. Controlling such feedback may require a feedback-based strategy. Indeed, these kinds of unfairness dynamics for the case of AI suggest the need to expand responsible innovation to consider feedback-based governance."
    },
    {
      "heading": "Amplifying Bias in Pretrained Models",
      "text": "Algorithmic unfairness may be immutably embedded in pretrained models, and further this unfairness may not be evident to users as they often abstract pretrained models as black boxes [80], dissociated and decontextualized from the training data used to develop them. When users fine-tune pretrained models to transfer for alternative tasks, recent empirical analyses suggest that they may in fact amplify biases in the original model [97]. From a societal perspective, this is similar to adverse drug events from off-label prescribing of drugs [24], where side effects may be amplified when transferring a drug to a clinical setting for which it was not initially developed or tested.\nOf greater concern, however, is that societies can perform models, exacerbating the societal bias that was originally\npresent in the training data. AI models do not stand outside of society; rather they are part of the infrastructure of modern society [41]. Therefore AI algorithms do not just passively capture the properties of society, but in fact shape their evolution as intrinsic parts of societal processes. As noted by MacKenzie in his study of financial models and markets [57], \u201cthe sociologist Barry Barnes has emphasized the central role in social life of self-validating feedback loops.\u201d As such, he refers to the form of performativity where the use of a model make the model \u201cmore true\u201d as Barnesian, a term we also adopt. We observe that the use of a biased AI model makes a difference and may significantly alter society to conform more to the model, a self-fulfilling prophecy [64]. The basic mechanisms by which a biased AI model can be Barnesian performative is well-understood in economic theories of discrimination [4, 17, 43, 84]. Let us describe the two primary dynamic mechanisms in the context of human resource management, where AI models have been used for many years, e.g. [63]. First, a worker in a disadvantaged groupmay fail to invest in her human capital if she knows the employer\u2019s AI model is unlikely to suggest she be promoted. Second, an employer itself may invest less (e.g. for training) in a worker from a disadvantaged group if an AI model indicates that the worker will not benefit. This leads to a selffulfilling prophecy when new training data is used to update models that capture this under investment by disadvantaged subpopulations.\nIn the AI context, recent mathematical models of AI model retraining [33, 67] capture this dynamic phenomenon of populations becoming more like what (biased) models predict, a kind of positive feedback."
    },
    {
      "heading": "Inadequacy of Dead-Reckoned Governance",
      "text": "In control theory, there are two main approaches: feedforward and feedback. Under feedforward control, a system responds to a control signal in a predefined way, whereas under feedback control, the system adjusts the control signal based on how the plant reacts. In navigation, feedforward is called dead reckoning and requires advanced calculation of the exact direction, magnitude, and timing of all actions. This is nearly impossible to implement for complex systems whose dynamics are uncertain.\nAs we argued, the unfairness of pretrained models may amplify as time progresses and as they are transferred to other tasks\u2014a kind of positive feedback. Given these complex dynamics, AI governance may take inspiration from control theory, which suggests the use of feedback control either alone or in combination with feedforward control. In particular, positive feedback can be reduced by feedforward damping supplemented by adding negative feedback.\nIn law and economics, both regulation and litigation are used to mitigate market failures; one dimension of distinction is that regulation is ex ante whereas litigation is ex post. Strong ex ante approaches are often inspired by the precautionary principle [78]. In many ways, ex ante governance is analogous to feedforward control whereas ex post governance is analogous to feedback control. Note that regulatory approaches can be ex post; for example the Food and Drug Administration performs postmarket surveillance of drug safety and issues recall notices when a drug is found unsafe. When considering unintentional ethical issues such as algorithmic unfairness rather than intentional malicious use by users as in previous sections, we still find that responsible innovation should be expanded with a network-centric viewpoint and further allow the possibility of ex post governance based on feedback, rather than just ex ante governance."
    },
    {
      "heading": "6 RESPONSIBLE INNOVATION IN AI",
      "text": "Discourse in science and technology ethics, and responsible innovation in particular, have put forth general principles and frameworks for thinking about technology governance. Stilgoe et al. [86] suggest that \u201cresponsible innovation means taking care of the future through collective stewardship of science and innovation in the present.\u201d This essentially involves asking what kind of future is desired and then asking what kinds of actions should be taken, given there is much uncertainty about the future. In this approach, ethical governance moves from consequentialism to a question of process. Thus far, insights from the responsible innovation literature have played a limited role in AI practice [10, 12]. As noted by von Schomberg, definitions in technology governance are usually initially made by using analogies, which serve to normalize the new technology. As understanding of the technology grows, the force of analogies weakens and distinct governance responses can be made [93]. Here, making analogies between AI and other potentially dual-use technologies such as nuclear and DNA recombination have allowed us to understand the inadequacy of self-regulation. Moreover, we will see the analogy also suggests an alternative governance approach. Distinct approaches, however, may be needed to address the inadequacies of producerfocused, static, and dead-reckoned governance that we have identified through an STS (and especially SCOT)-based analysis of how pretrained models move and change through the actions of distinct social groups. The case of AI suggests that to pursue care for the future, responsible innovation must expand to consider dynamics, feedback, and networks of users.\nDrawing on the insight that consequentialist governance premised on formal risk assessment has done little to predict many of the most profound impacts of innovation [86], we take a more expansive viewpoint. Focusing on process to\nexpand beyond current AI governance approaches (without taking a strong normative stand), we suggest the following possibilities to address the inadequacies discussed in the previous sections. In doing so, we specifically recognize that the social world acts to fundamentally shape technical development at every level.\nSelf-Governance As detailed in the responsible innovation literature [86, 93], contrary to self-governance by innovators, an alternative is deliberative and inclusive governance with broad stakeholder involvement. This aims to diversify the inputs to and the delivery of governance [13]. A process of inclusion forces consideration of questions of power. One goal is to achieve a consensus set of norms and governance processes that are based on a broad set of values, standardized across the AI community. In fact, the Partnership on AI has been pursuing exactly this goal [52], though this effort may be enhanced by greater understanding of Holy Grail performativity.\nProducer-Focused Governance We have argued that innovation by lead users into functionally new application areas is a key process in AI, outside the control of pretrained model producers. Extant discussions of AI governance, however, have focused only on producers. An alternative is an ethics of co-responsibility, where producers and lead users assume shared responsibility [27] for intended and unintended consequences, rather than producers being cast as a kind of moral crumple zone [25]. Such a network-centric view recognizes the fact that lead users are agents embedded in a network of social relations that limits and controls the technological choices they are capable of making [19]. More specifically, mechanisms such as codes of conduct and ethical technology review conversations may build greater reflexivity in both users and producers.\nStatic Governance Since pretrained models are interpretively flexible as they move, a static view of governance does not capture the dynamics of change and recombination as the models are put to numerous innovative uses. Moreover, a risk-based assessment does not capture the desire to balance the beneficence and non-maleficence of users. An alternative possibility is governance built on a compositional calculus for pretrained models, paired with anticipation through technology foresight that specifically considers their mobility and change. Note that the responsible innovation literature has developed structured ways of performing technology foresight [39], but grounding in the (im)mutability of pretrained models would only enhance this process.\nDead-Reckoned GovernanceWhereas fixing an ex ante governance approach for anticipated malicious uses may be prudent, it also seems incomplete in the face of complex sociotechnical systems involving AI. Recognizing that most innovations are unexpected and hard to forecast (especially functionally new applications) suggests the need\nfor ex post surveillance too, much like ongoing monitoring of drug safety. Such a feedback-based approach is responsive to the power of innovative technology to create the future. For fairness specifically, there are even batteries of statistical tests that could be administered as pretrained models move into new applications [8, 43], but their deployment may be improved by greater understanding of the selffulfilling prophecy of Barnesian performativity. Moreover such feedback-based governance enable social learning."
    },
    {
      "heading": "7 CONCLUSION",
      "text": "Our estimates suggest that the cost to train (not considering architecture search or hyperparameter tuning) XLNet was $50,000, to train RoBERTa was $60,000, and to train GPT-2 was $250,000. On the other hand, the cost to fine-tune BERT on the SQuAD dataset is estimated to cost only $3. That is, it is at least tens of thousands of times more costly to initially develop a pretrained model than to fine-tune it. This technological distinction has several social consequences. We have described here, how large-scale AI models are developed, how they are used, how they move around among agents, and what unfairness properties may be embedded and exacerbated in them as they move.\nWhen closed stoves were developed in the eighteenth century, there were various interpretations about their safety. Their predecessor technology, open hearths, were also dangerous but their \u201cdangerswere dangers that people had coped with for centuries; the risks of stoves were new and thus potentially more worrisome\u201d [19]. Such worry (now for AI) often yields a desire for governance, but emerging technologies typically fall into an institutional void, where there are few agreed upon governance structures [29] and analogies to old technologies may be inadequate. Here we have argued that analyzing the sociological position of pretrained AI models suggests expanding responsible innovation to several new factors that may yield more responsive and effective AI governance."
    }
  ],
  "title": "Pretrained AI Models: Performativity, Mobility, and Change",
  "year": 2019
}
