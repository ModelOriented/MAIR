{"abstractText": "Machine learning models have been successfully applied to a wide range of applications including computer vision, natural language processing, and speech recognition. A successful implementation of these models however, usually relies on deep neural networks (DNNs) which are treated as opaque black-box systems due to their incomprehensible complexity and intricate internal mechanism. In this work, we present a novel algorithm for explaining the predictions of a DNN using adversarial machine learning. Our approach identifies the relative importance of input features in relation to the predictions based on the behavior of an adversarial attack on the DNN. Our algorithm has the advantage of being fast, consistent, and easy to implement and interpret. We present our detailed analysis that demonstrates how the behavior of an adversarial attack, given a DNN and a task, stays consistent for any input test data point proving the generality of our approach. Our analysis enables us to produce consistent and efficient explanations. We illustrate the effectiveness of our approach by conducting experiments using a variety of DNNs, tasks, and datasets. Finally, we compare our work with other well-known techniques in the current literature.", "authors": [{"affiliations": [], "name": "Arash Rahnama"}, {"affiliations": [], "name": "Modzy arash.rahnama@modzy.com"}, {"affiliations": [], "name": "Andrew Tseng"}], "id": "SP:de799887cc5883d37d603f9682b6815a27a84183", "references": [{"authors": ["D. Alvarez-Melis", "T.S. Jaakkola"], "title": "A causal framework for explaining the predictions of black-box sequenceto-sequence models", "venue": "Proceedings of the Conference on Empirical Methods in Natural Language Processing.", "year": 2017}, {"authors": ["D. Alvarez-Melis", "T.S. Jaakkola"], "title": "On the robustness of interpretability methods", "venue": "arXiv preprint arXiv:1806.08049.", "year": 2018}, {"authors": ["M. Ancona", "E. Ceolini", "C. \u00d6ztireli", "M. Gross"], "title": "Towards better understanding of gradient-based attribution methods for deep neural networks", "venue": "arXiv preprint arXiv:1711.06104.", "year": 2017}, {"authors": ["S. Bach", "A. Binder", "G. Montavon", "F. Klauschen", "K.-R. M\u00fcller", "W. Samek"], "title": "On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation", "venue": "PloS one 10(7).", "year": 2015}, {"authors": ["S. Becker", "M. Ackermann", "S. Lapuschkin", "K.-R. M\u00fcller", "W. Samek"], "title": "Interpreting and explaining deep neural networks for classification of audio signals", "venue": "arXiv preprint arXiv:1807.03418.", "year": 2018}, {"authors": ["M.G. Bulmer"], "title": "Principles of Statistics", "venue": "Courier Corporation.", "year": 1979}, {"authors": ["A. Datta", "S. Sen", "Y. Zick"], "title": "Algorithmic transparency via quantitative input influence: Theory and experiments with learning systems", "venue": "IEEE Symposium on Security and Privacy (SP), 598\u2013617. IEEE.", "year": 2016}, {"authors": ["J. Deng", "W. Dong", "R. Socher", "L.-J. Li", "K. Li", "L. FeiFei"], "title": "ImageNet: A large-scale hierarchical image database", "venue": "IEEE Conference on Computer Vision and Pattern Recognition, 248\u2013255. IEEE.", "year": 2009}, {"authors": ["D. Erhan", "Y. Bengio", "A. Courville", "P. Vincent"], "title": "Visualizing higher-layer features of a deep network", "venue": "University of Montreal 1341(3):1.", "year": 2009}, {"authors": ["G. Fidel", "R. Bitton", "A. Shabtai"], "title": "When Explainability Meets Adversarial Learning: Detecting Adversarial Examples using SHAP Signatures", "venue": "arXiv preprint arXiv:1909.03418.", "year": 2019}, {"authors": ["L.H. Gilpin", "D. Bau", "B.Z. Yuan", "A. Bajwa", "M. Specter", "L. Kagal"], "title": "Explaining Explanations: An Overview of Interpretability of Machine Learning", "year": 2018}, {"authors": ["I.J. Goodfellow", "J. Shlens", "C. Szegedy"], "title": "Explaining and harnessing adversarial examples", "venue": "International Conference on Learning Representations (ICLR).", "year": 2015}, {"authors": ["A. Ilyas", "S. Santurkar", "D. Tsipras", "L. Engstrom", "B. Tran", "A. Madry"], "title": "Adversarial examples are not bugs, they are features", "venue": "Advances in Neural Information Processing Systems, 125\u2013136.", "year": 2019}, {"authors": ["A. Jacovi", "O.S. Shalom", "Y. Goldberg"], "title": "Understanding convolutional neural networks for text classification", "venue": "the EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP.", "year": 2018}, {"authors": ["P. Kaur"], "title": "Convolutional neural networks (CNN) for cifar-10 dataset", "venue": "parneetk. github. io/blog/cnn-cifar10/, 2017.", "year": 2018}, {"authors": ["Y. Kim"], "title": "Convolutional Neural Networks for Sentence Classification", "year": 2014}, {"authors": ["B. Letham", "C. Rudin", "T.H. McCormick", "D Madigan"], "title": "Interpretable classifiers using rules and bayesian analysis: Building a better stroke prediction model", "venue": "The Annals of Applied Statistics", "year": 2015}, {"authors": ["J. Li", "X. Chen", "E. Hovy", "D. Jurafsky"], "title": "Visualizing and Understanding Neural Models in NLP", "year": 2015}, {"authors": ["S.M. Lundberg", "Lee", "S.-I."], "title": "A unified approach to interpreting model predictions", "venue": "Advances in Neural Information Processing Systems, 4765\u20134774.", "year": 2017}, {"authors": ["A. Madry", "A. Makelov", "L. Schmidt", "D. Tsipras", "A. Vladu"], "title": "Towards deep learning models resistant to adversarial attacks", "venue": "arXiv preprint arXiv:1706.06083.", "year": 2017}, {"authors": ["J.H. McDonald"], "title": "Handbook of Biological Statistics, volume 2", "venue": "Sparky House Publishing Baltimore, MD.", "year": 2009}, {"authors": ["S. Mishra", "B.L. Sturm", "S. Dixon"], "title": "Local interpretable model-agnostic explanations for music content analysis", "venue": "ISMIR.", "year": 2017}, {"authors": ["T. Miyato", "A.M. Dai", "I. Goodfellow"], "title": "Adversarial Training Methods for Semi-Supervised Text Classification", "year": 2016}, {"authors": ["S. Mohseni", "N. Zarei", "E.D. Ragan"], "title": "A survey of evaluation methods and measures for interpretable machine learning", "venue": "arXiv preprint arXiv:1811.11839.", "year": 2018}, {"authors": ["B. Pang", "L. Lee"], "title": "Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales", "venue": "Proceedings of the ACL.", "year": 2005}, {"authors": ["M.T. Ribeiro", "S. Singh", "C. Guestrin"], "title": "Why should I trust you?\u201d Explaining the predictions of any classifier", "venue": "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 1135\u20131144.", "year": 2016}, {"authors": ["M. Robnik-ikonja", "M. Bohanec"], "title": "PerturbationBased Explanations of Prediction Models", "venue": "159\u2013175.", "year": 2018}, {"authors": ["R.R. Selvaraju", "M. Cogswell", "A. Das", "R. Vedantam", "D. Parikh", "D. Batra"], "title": "Grad-Cam: Visual explanations from deep networks via gradient-based localization", "venue": "Proceedings of the IEEE International Conference on Computer Vision, 618\u2013626.", "year": 2017}, {"authors": ["A. Shrikumar", "P. Greenside", "A. Kundaje"], "title": "Learning important features through propagating activation differences", "venue": "Proceedings of the International Conference on Machine Learning, 3145\u20133153. JMLR. Org.", "year": 2017}, {"authors": ["K. Simonyan", "A. Vedaldi", "A. Zisserman"], "title": "Deep inside convolutional networks: Visualising image classification models and saliency maps", "venue": "arXiv preprint arXiv:1312.6034.", "year": 2013}, {"authors": ["D. Smilkov", "N. Thorat", "B. Kim", "F. Vi\u00e9gas", "M. Wattenberg"], "title": "Smoothgrad: Removing noise by adding noise", "venue": "arXiv preprint arXiv:1706.03825.", "year": 2017}, {"authors": ["E. \u0160trumbelj", "I. Kononenko"], "title": "Explaining prediction models and individual predictions with feature contributions", "venue": "Knowledge and Information Systems 41(3):647\u2013 665.", "year": 2014}, {"authors": ["M. Sundararajan", "A. Taly", "Q. Yan"], "title": "Axiomatic attribution for deep networks", "venue": "Proceedings of the International Conference on Machine Learning, 3319\u20133328. JMLR. Org.", "year": 2017}, {"authors": ["D. Tsipras", "S. Santurkar", "L. Engstrom", "A. Turner", "A. Madry"], "title": "Robustness may be at odds with accuracy", "venue": "International Conference on Learning Representations (ICLR).", "year": 2019}, {"authors": ["A. Van Etten", "D. Lindenbaum", "T.M. Bacastow"], "title": "SpaceNet: A remote sensing dataset and challenge series", "venue": "arXiv preprint arXiv:1807.01232.", "year": 2018}, {"authors": ["A. Vedaldi", "S. Soatto"], "title": "Quickshift and kernel methods for mode seeking", "venue": "European Conference on Computer Vision, 705\u2013718. Springer.", "year": 2008}, {"authors": ["P. Warden"], "title": "Speech Commands: A Dataset for LimitedVocabulary Speech Recognition", "year": 2018}, {"authors": ["G. Zhao", "B. Zhou", "K. Wang", "R. Jiang", "M. Xu"], "title": "Respond-Cam: Analyzing deep models for 3D imaging data by visualizations", "venue": "International Conference on Medical Image Computing and Computer-Assisted Intervention, 485\u2013492. Springer.", "year": 2018}, {"authors": ["Vedaldi", "Soatto"], "title": "A QuickShift Segmentation QuickShift is a mode seeking clustering algorithm", "year": 2008}, {"authors": ["Tsipras"], "title": "2019) introduced the concept of robust and non-robust features, where the authors indicated that there are features that humans ignore but the models are sensitive to. They call these the non-robust features. Non-robust features are the features can easily be manipulated by the attacker in order to fool the model. Robust features are features that are both important to the model and also humans and at the same time invincible to small adversarial manipulations", "year": 2019}], "sections": [{"text": "1 Introduction Explaining the outcomes of complex machine learning models is a prerequisite for establishing trust between the machines and users. As humans increasingly rely on DNNs to process large amounts of data and make decisions, it is crucial to develop solutions that can interpret the predictions of DNNs in a user-friendly manner. Explaining the outcomes of a model can help reduce bias and contribute to improvements in model design, performance, and accountability by providing beneficial insights into how models behave Fidel, Bitton, and Shabtai (2019). Consequently, the field of explainable artificial intelligence systems, XAI, has gained traction in recent years, where researchers from different disciplines have come together to define, design and evaluate explainable systems S\u030ctrumbelj and Kononenko (2014); Datta, Sen, and Zick (2016); Mohseni, Zarei, and Ragan (2018). The majority of current explainability algorithms for DNNs produce an explanation for a single input-output pair: an input data point fed into the DNN and the respective\nCopyright c\u00a9 2021, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nprediction made by the DNN. The algorithm usually finds the most important features in the input contributing the most to the model\u2019s predictions and selects those as explanations for the model\u2019s behavior Alvarez-Melis and Jaakkola (2018). The majority of these algorithms find the important features using either a perturbation-based approach or a saliency-based approach Lundberg and Lee (2017). The saliency-based approaches rely on gradients of the outputs in relation to the inputs to find the important features Simonyan, Vedaldi, and Zisserman (2013); Selvaraju et al. (2017). Perturbation-based methods on the other hand apply small local changes to the input, track the changes in the output, and find and rank the important input features Ribeiro, Singh, and Guestrin (2016); Alvarez-Melis and Jaakkola (2017).\nOne main problem with current state-of-the-art explainability tools is their reliance on a large set of hyperparameters. This leads to local instability of explanations and can negatively affect the user\u2019s experience AlvarezMelis and Jaakkola (2018). An explainability algorithm should satisfy 3 properties: 1- It has to produce humanunderstandable explanations which are loyal to the decision making process of the DNN, 2- It has to be locally consistent and efficient, 3- It should be user-friendly, easy to apply and quick in providing explanations. In this work, we propose a new algorithm, explanations via adversarial attacks, which satisfies these 3 important properties and more. We call our method Adversarial Explanations for Artificial Intelligence systems or AXAI 1. AXAI inherits from the nature of adversarial attacks to automatically find and select important features affecting the model\u2019s prediction to produce explanations. The idea behind our work comes from the natural behavior of adversarial attacks. The attacks tend to manipulate important features in the input to deceive a DNN. The logic is simple, rather than trying to build a model that learns to explain the DNN\u2019s behavior, why dont we utilize the nature of attacks to learn this behavior? One who knows how to fool a model, certainly knows what the model may be thinking. Another benefit of our approach is that certain attacks, such as the Projected Gradient Descent (PGD) method Madry et al. (2017), are fast, efficient, and consistent in their adversarial behavior. Our work further aims to solve at least\n1Code will be readily available.\nar X\niv :2\n00 5.\n10 28\n4v 4\n[ cs\n.L G\n] 2\n8 Se\np 20\n20\n2 problems: 1- Provide fast explanations without a need for model training, 2- Reduce the need for selecting a large set of hyper-parameters to produce consistent results.\nObviously, one needs to first show how adversarial attacks link to explainbility, i.e., how an attack can point to the important features in the input and how one can filter out the unimportant ones to produce explanations. Further, one needs to show how an adversary behaves similarly in its approach across models, tasks and datasets so that the explanations are consistent, stable, and applicable to a large group of models. Here, we present a novel algorithm for explaining the DNN\u2019s predictions in multiple domains including text, audio and image. In particular, this paper makes the following contributions:\n\u2022 We show that given an `2 PGD attack and a trained DNN, the distribution of attack magnitudes vs. frequency across all unseen test inputs follows a beta distribution, regardless of the task and dataset. We also show that these distributions are symmetric and the differences between their means, medians, and quantiles are not statistically significant.\n\u2022 We show that the most important input features, i.e., features with the largest effect on the model\u2019s predictions, can be found using a consistent rule across different DNN architectures, datasets, and tasks. This rule leverages the properties of the distributions explained above.\n\u2022 We propose a novel algorithm for explaining the outcomes of DNNs and provide a detailed analysis of our algorithm\u2019s performance for different DNN architectures, datasets and tasks.\n\u2022 We benchmark our algorithm against methods such as LIME and SHAP Lundberg and Lee (2017); Ribeiro, Singh, and Guestrin (2016) and show that our algorithm performs faster while producing similar or better explainability results.\n2 Related Work One of the popular explainability solutions called LIME Ribeiro, Singh, and Guestrin (2016) assumes that DNNs are linear locally. LIME trains weighted linear models on the top of the DNN for perturbed samples around a target input to produce explanations. The computational bottleneck in LIME is caused by the training part where a selected number of perturbed samples are sent through the DNN for learning the explanation. Certain combination of LIME\u2019s hyperparameters can produce unstable results Alvarez-Melis and Jaakkola (2018). DeepLIFT produces explanations by modeling the slope of gradient changes of output with respect to the input Shrikumar, Greenside, and Kundaje (2017). GradCAM is a saliency-based method that uses the gradients of the input at the final convolutional layer to produce coarse localization maps pointing to important regions in the input Selvaraju et al. (2017). The majority of approaches based on sensitivity maps fail to produce explanations that only rely on important features. Creators of DeepLIFT associate this lack of stability to the behavior of activation functions such as ReLU. Smilkov et al. (2017) proposed Smooth Grad\nwhich uses gradients and Gaussian based de-noising methods to produce stable explanations. The authors of the paper mention that large outlier values in the gradient maps produced by gradient differentiation may cause instability. In our algorithm, we overcome the problem of instability by utilizing the density of attacks, which are created iteratively on segments. Some other important works in this area are given in Sundararajan, Taly, and Yan (2017); Jacovi, Shalom, and Goldberg (2018); Zhao et al. (2018); Bach et al. (2015); Becker et al. (2018); Erhan et al. (2009); Letham et al. (2015).\nDNNs are vulnerable to subtle adversarial perturbations applied to their input. The basic idea behind most adversarial attacks revolves around solving a maximization problem with a constraint that keeps the distance between the original input and adversarial input small, so that the adversarial input, while capable of fooling the DNN, is not perceptually recognizable by humans. The connection between model interpretation and attacks has recently gravitated the interest of researchers. Ilyas et al. (2019) and Tsipras et al. (2019) showed that one benefit of adversarial examples is that they reveal useful insights into the salient features of input data and their effects on DNNs\u2019 predictions. Our solution relies on the nature of adversarial attacks to select and produce important and explainable features given a specific input and DNN. Our work puts more emphasis on model interpretability, where we make use of the information obtained from an adversarial attack on a DNN to de-noise the sensitivity maps and produce stable explanations. We de-noise the gradient map by utilizing the iterative nature of the PGD attack and by considering only a minimum number of highly influential gradients that contribute the most to the predictions. We use the density of gradients in a number of segments to remove the noise that was not filtered out in the previous steps and produce human-interpretable explanations.\n3 Main Results The core idea behind our approach, AXAI, is to utilize the knowledge gained from an adversarial attack on a DNN and an input, to find the important features in the input in order to produce good explanations. This is done by mapping \u201ccarefully filtered attacked inputs\u201d onto predefined segments and filtering out the unimportant features. This will be discussed in more detail in later sections. First let\u2019s look at an example in Fig. 1 to see how our approach works. Given an image classification DNN, the `2 adversarial attack changes the pixels in the entire image, as seen in Fig. 1c. The reason for this is simple: each pixel value is changed by the adversary so that the accumulated loss value can increase enough to fool the DNN. Fig. 1b shows the distribution of the attack on this image. The x-axis represents the magnitude of the pixel changes and the y-axis represents the number of pixels given each value on x-axis. AXAI maps the strongly attacked pixels to the image segments of the original image and filters out the segments with highest density of attacked pixels which meet certain criteria to produce explanations. Fig. 1c shows the value changes for the important attacked pixels. As we will show, the important features used for explanations are located at specific sections in the tails of the\ndistribution given in Fig. 1b. These are the pixels that directly affect the classification decision made by the model. We use QuickShift Vedaldi and Soatto (2008) for segmenting the input image (Fig. 1d). It is important to note that the segmentation step in our algorithm is general and any type of input segmentation method may be utilized for this step depending on the model and input type, e.g., language, signal or imagery. Fig. 1e shows the explanation produced by our algorithm.\nAlgorithm 1 details the steps taken by AXAI to produce an explanation E for the output of a selected model f . Suppose that input X is segmented into p groups using a segmentation method and that the attack magnitudes for the input X and DNN f are obtained. Let Xdiff be the difference between the original X and adversarial X \u2032. We filter out the low intensity attack magnitudes Xdiff and create a Boolean array Xdifft, where values larger than a threshold, are only set to True. Let Su be the set of unique segments, Su = {Su1, , Sup}. Next, we map the filtered attackXdifft to the segments Su, and create a new list of filtered attack groups, Sux = {Sux1 , , Suxp}. The mapping function, Map in Algorithm 1, simply stacks the filtered attacks on the segments and groups the filtered attack Xdifft based on the segments. Finally, the attack density of each unique segment can be written as Sud = { card(Sux1 )\ncard(Su1) , , card(Suxp ) card(Sup) } (Calcu-\nlate density in Algorithm 1). We then extract the indices j\u2019s of the top K maximum values in Sud (TopK indices in Algorithm 1), and produce Su(j) as explanation E for the input X . In next sections, we explain each step in details."}, {"heading": "3.1 White-box adversarial attacks", "text": "Adversary can attack a DNN by adding engineered noise to the input to increase the associated loss value, if it has some prior knowledge of the DNN including the weights and biases. AXAI utilizes Projected Gradient Descend (PGD) attack Madry et al. (2017), although any `2 adversarial attack can replace PGD in our algorithm (Appendix B). However,\nAlgorithm 1 AXAI Require: Model f , input X\n1: X \u2032 \u2190 Attack(f,X) . i.e. PGD attack 2: Xdiff = x\u2032 \u2212 x . The attack magnitudes 3: Xdifft \u2190 Threshold(Xdiff ) . Filtered attack\nmagnitudes 4: Su\u2190 Segment(X)) 5: Sux \u2190Map(Xdifft, Su) . Group attack magnitudes\nbased on segmentation 6: Sud \u2190 Calculate density(Sux) . Calculate attacks\nper segment 7: return Su(TopK indices(Sud))\nPGD provides specific benefits such as stability and gradient smoothness that other attacks do not. PGD can be thought of as an iterative version of `2 Fast Gradient Method (FGM) attack Goodfellow, Shlens, and Szegedy (2015), where in each iteration, the adversarial changes are clipped into an `2 ball of some value. PGD is generally considered a strong stable attack and is defined as,\nxt+1 = ux+S(xt + \u2207xL(\u0398, x, y)), (1)\nwhere for t iterations, x and y are the inputs and outputs, and \u0398 are the weights and biases."}, {"heading": "3.2 Statistical analysis of attack magnitudes vs. frequency distributions", "text": "Here, we briefly report our statistical analysis of attack magnitudes vs. frequency distributions for a fixed DNN, dataset and an adversarial attack. We can show that the distributions are similar in their \u201cshapes,\u201d \u201cmeans,\u201d \u201cmean ranks,\u201d \u201cmedians,\u201d and \u201cquantiles,\u201d and follow a Beta distribution with specific parameters. Given that there is no significant difference in the distributions, we can provide a universal threshold using quantiles which separates the important features from the rest to produce explanations.\nTo be able to show that highly perturbed regions can be chosen to produce explanations for a single input, we should first show analytically that the results are consistent for all inputs, i.e., adversarial attacks are consistent in their adversarial behavior and the manner in which they attack the most influential input segments . Our analyses prove this point and consequently we can show that our proposed rule to find the important input segments for a single input holds. Finally, we empirically show that these segments are indeed the most important parts of the input by analyzing the effects of them on the test error rate.\nWe can measure the symmetricity of distributions using the Fisher-Pearson coefficient of skewness. We present the results for AlexNet on CIFAR10 Kaur (2018), VGG16 on CIFAR100 Krizhevsky, Nair, and Hinton (2009) and ResNet34 on ImageNet Deng et al. (2009). The FisherPearson coefficients of the attack magnitudes vs. frequency distributions for all cases are shown in Fig. 2. It is seen that the skewness of all distributions falls within the [\u22120.5, 0.5] range showing strong evidence that they are approximately symmetric Bulmer (1979). Only 0.9% of CIFAR10, 3.3% of\nCIFAR100 and 1.9% of ImageNet test datasets lie outside of [\u22120.5, 0.5] range.\nQuantile-Quantile (Q-Q) plot allows us to understand how the quantiles of a distribution deviate from a specified theoretical distribution. The theoretical distribution selected is the normal distribution. The x-axis and y-axis represent the quantile values of the theoretical and sample distributions, respectively. While it is unlikely to have identical distributions that perfectly match, one can look at different parts of the Q-Q plot to distinguish between the similar and dissimilar locations in the distributions. Fig. 3 shows the Q-Q plots for random subsets of ImageNet and CIFAR10 test datasets each containing 1000 images. It is seen that the distributions follow a fairly straight line in the middle portion of the curve, while deviating at the upper and lower parts. This provides some evidence supporting the hypothesis that distributions are symmetrical with heavier tails.\nt-test (CIFAR10) Mann-Whitney (CIFAR10) t-test (ImageNet) Mann-Whitney (ImageNet) p-value 0.70 0.58 0.64 0.55\nTable 1: p-values for the mean similarity statistical tests at significance level 0.05.\nAlexNet, CIFAR10, PGD VGG16, CIFAR100, PGD ResNet34, ImageNet, PGD 15th Quantile (\u22121.807e\u2212 02,\u22121.805e\u2212 02) (\u22121.419e\u2212 02,\u22121.414e\u2212 02) (\u22121.785e\u2212 03,\u22121.777e\u2212 03) 25th Quantile (\u22121.145e\u2212 02,\u22121.071e\u2212 02) (\u22128.153e\u2212 03,\u22128.110e\u2212 03) (\u22121.015e\u2212 03,\u22121.101e\u2212 03) Mean (1.775e\u2212 05, 2.295e\u2212 05) (\u22126.850e\u2212 06,\u22123.624e\u2212 06) (\u22121.090e\u2212 07,\u22126.000e\u2212 08) Median (2.115e\u2212 06, 1.127e\u2212 05) (\u22122.842e\u2212 06, 4.467e\u2212 06) (\u22122.155e\u2212 07,\u22129.381e\u2212 08) 75th Quantile (1.071e\u2212 02, 1.073e\u2212 02) (8.102e\u2212 03, 8.146e\u2212 03) (1.011e\u2212 03, 1.016e\u2212 03) 85th Quantile (1.809e\u2212 02, 1.812e\u2212 02) (1.413e\u2212 02, 1.418e\u2212 02) (1.777e\u2212 03, 1.785e\u2212 03)\nTable 2: Estimations for mean, median, 15th , 25th, 75th and 85th quantiles at 95% confidence level.\nWe perform the two-sample location t-test and MannWhitney U test to determine if there is a significant difference between the hypotheses where the null hypothesis is the equality of the means. Carrying out pair t-tests on all samples allows us to be conservative in confirming the mean similarity of the distributions. A sample here is defined as the attack magnitudes vs. frequency distribution for a data point in the test adversarial dataset created by the PGD attack on a DNN trained on the training dataset. The results reported in Table 1 indicate no significant difference between the means. Further, the Mann-Whitney U test results indicate that all pairs are similar to each other on the mean ranks. Under the assumption of two distributions having similar shapes, one could further state that Mann-Whitney test can be considered as a test of medians McDonald (2009). Since, we have shown that the shapes are similar, we can conclude that there are no significant difference between the medians of the distributions. Further details in addition to the results for the ANOVA test are given in Appendix C.\nNext, to show consistency across distributions for a given model, dataset and attack, we estimate the values of quantiles, means and medians. We do this by estimating the statistics of the distributions and constructing confidences intervals. For each experiment, we estimate the mean, median, 15th, 25th, 75th and 85th quantiles of each attack magnitude vs. frequency distribution for the entire test dataset. The statistical confidence interval estimations at confidence level of 95% are reported in Table 2. Our results show that the confidence intervals have narrow ranges and the estimations are consistent. The estimates for the 15th, 25th, 75th and 85th quantiles indicate a strong symmetricity with respect to the origin in all cases. This matches the results of the skewness test in Fig. 2. Another observation is that the confidence interval of the mean and medians are pretty narrow, supporting the results of the t-tests and Mann-Whitney U test. Finally, we can show with high confidence that the distributions consistently follow a beta distribution. The beta distribution is a family of distributions defined by two positive shape pa-\nrameters, denoted by p and q. The estimated p and q of the beta distribution are reported in Table 3. Further technical details on our analyses presented in this section, in addition to further experiments with audio and text input types, are provided in Appendix C."}, {"heading": "3.3 Quantile selection for the explanations", "text": "Our algorithm produces explanations that rely only on the features in the input that have the largest effect on the predictions. While the majority of the input is attacked, our belief is that only important features are strongly attacked. We show how one can select the boundary threshold between \u201cexplainable features\u201d and the rest based on attack magnitudes. We demonstrate this with 2 experiments: 1) AlexNet trained on CIFAR10, 2) ResNet34 trained on ImageNet, both attacked by PGD with 20 iterations. In each case, we select the successfully attacked inputs from the adversarial test dataset, i.e., the inputs that fool the DNN. We then only reattack specific features of the original clean inputs within the [0%, \u03b1%] and [(100\u2212\u03b1)%, 100%] percentile of the distributions, where \u03b1 is the percentage threshold. The re-attacking process starts from \u03b1 = 0, where none of the input features are attacked, and then we gradually increase the value of \u03b1 until the attack successfully changes the prediction, and then we save the value of \u03b1 (Fig. 4a). We repeat this for every input. The probability density distribution of \u03b1\u2019s are given in Fig. 4b and Fig. 4c with an estimated mean of \u03b1 = 15.\nFurther, we report the test accuracies of the DNNs on the adversarial test datasets that are created based on different attack percentiles. Given an attack percentile range, the adversarial test dataset consists of adversarial test inputs which are created by attacking only portions of the input features that lie withing a specific percentile range of the attack magnitudes vs. frequency distributions similar to above. This allows us to understand how the features lying in the middle area, tails and outliers of the distributions affect the DNN\u2019s predictions. Our findings are reported in Table 4. Our re-\nsults show that the majority of the input features including those within the first two standard deviations and the outliers of the distributions do not have a strong effect on the predictions. A smaller portion of the input features which are also those attacked with the highest intensity, i.e., within the [0%, 15%] and [85%, 100%] percentiles of the distributions have the largest effect on the DNN\u2019s predictions, confirming our hypothesis. We see the same trend across different DNNs and datasets (Appendix C).\n4 Experiment Results Earlier, we provided a sample explanation created by AXAI for an image classifier. Appendix E contains more experiments for image classification and object detection DNNs. Further, Appendix E contains an ablation study and an interesting comparison between explanations produced by a non-robust model and an adversarially robust model. Here, we provide sample explanations produced by our algorithm for speech recognition and language-based tasks."}, {"heading": "4.1 Explaining a speech recognition model", "text": "The Speech Commands Dataset Warden (2018) is an audio dataset of short spoken words. Here, we have converted the audio files to spectrograms and used them to train a LeNet model to identify \u201cspeech commands.\u201d We have created time-frequency segments by dividing the spectrogram into time-frequency grids similar to Mishra, Sturm, and Dixon (2017). The x-axis and y-axis indicate the time-scale and log-scale frequency of the spectograms respectively, and the color bar indicates the magnitude. This kind of segmentation results in equal sized rectangular blocks where the height of the segment covers the range of frequencies (y-axis) and the width of the segment covers the range of the time (xaxis) associated with the spoken word. The spectrogram of the first word \u201dRight\u201d and its explanation are shown in Fig. 5a and Fig. 5b. The explanation shows that the first and last character in the spoken word \u201cRight\u201d stand out as important features ([0.4s, 0.6s] and [1.0s, 1.2s] intervals). This is reasonable because \u201cFive\u201d is the neighboring class of \u201cRight\u201d in the dataset (Appendix D) and \u201cRight\u201d and \u201cFive\u201d differ in the pronunciation of \u201cr\u201d and \u201cf\u201d and \u201ct\u201d and \u201cv.\u201d The second example is for the word \u201cThree\u201d (Fig. 5c and Fig. 5d). The produced explanation indicates the importance of \u201cThr\u201d ([1.4s, 1.7s] interval). This is reasonable because \u201cThree\u201d and its neighbor \u201cTree\u201d differ in the letter \u201ch\u201d in \u201cThr,\u201d and this difference is learned by the model during training to identify the two words correctly. More examples are shown in 8. Details on this experiment are given in Appendix E."}, {"heading": "4.2 Explaining a text classification model", "text": "The Sentence Polarity Dataset Pang and Lee (2005) is a collection of movie-review documents labeled with respect to their overall sentiment polarity. Here, we will look at a negative and positive example (Fig. 6a and Fig. 6b) where the rows are the word tokens in the sentence, and the columns are the embedding dimensions. The NLP model used in our experiment is taken from Kim (2014) and trained on the dataset. As part of the pre-processing, the words in the\ndataset are tokenized and mapped to an embedding matrix. The word embedding matrix is also used as the segments in our algorithm. Li et al. (2015) mentions that the saliency map of an NLP model can be visualized using the embedding layer similar to saliency maps used for image-based models. Consequently, one can apply our algorithm to NLP models in a similar manner, i.e., we can utilize the first order derivative of the loss with respect to the word embedding. This technique is similar to what was used in Miyato, Dai, and Goodfellow (2016). The first example, \u201cit\u2019s a glorified sitcom, and a long, unfunny one at that.\u201d is classified as a negative review by the model. Fig. 6a shows that the word \u201cunfunny\u201d is strongly highlighted as the main explanation for this prediction. For the positive example \u201ca work of astonishing delicacy and force,\u201d it is seen that the word \u201castonishing\u201d has the most significant influence on model\u2019s prediction. More examples are shown in Fig. 9."}, {"heading": "4.3 Benchmark tests", "text": "We test our algorithm against LIME and SHAP (Gradient Explainer). It is important to note that SHAP subsumes a number of prior approaches and provides a fair baseline. To show the consistency of our approach, we present visualizations for 3 cases: 1) AlexNet, CIFAR10, 2) VGG16, CIFAR100, 3) ResNet34, ImageNet using the 3 explainability tools and provide more experiments in Appendix F. The algorithms produce similar explanations where AXAI has fewer tune-able parameters and performs faster. LIME fails to produce good explanations for low-resolution CIFAR10 images. In Appendix F, we provide examples showing that AXAI outperforms LIME for low-resolution inputs. We benchmark the running-time performance of AXAI, LIME and SHAP for ResNet34 trained on ImageNet on a single\n0 10 20 30 40 Embedding Dimension\nit 'saglorifiedsitcom,andalong,unfunnyoneatthat\n0.00000 0.00025 0.00050 0.00075 0.00100 0.00125 0.00150 0.00175\n(a) Text example 1\n0 10 20 30 40 Embedding Dimension\naworkofastonishingdelicacyandforce\n0.0000 0.0002 0.0004 0.0006 0.0008 0.0010 0.0012\n(b) Text example 2\nFigure 6: The AXAI explanations for the sentence classification model.\nSingle CPU (Intel Core i5-7360U) Single GPU (Tesla V100-SXM2) LIME 105s 5.8s SHAP (Gradient Explainer) 35s 3.8s AXAI (PGD with 20 iters) 6.6s 1.7s\nTable 5: Benchmark running-time experiments.\nCPU (Intel Core i5-7360U) and single GPU (Tesla V100SXM2) on the entire test dataset. The results are given in Table. 5. LIME is the slowest to produce explanations. This is because LIME needs to forward propagate the perturbed inputs through the DNN several times. SHAP is also slower to generate the results in comparison to AXAI. LIME works better on a GPU. AXAI maintains its relative performance on the CPU and GPU. This is because the segmentation step which mainly uses the CPU is the main computational bottleneck for the algorithms (Appendix A). A few comparisons between AXAI, LIME, and SHAP are shown in Fig. 7.\n5 Final Remarks and Conclusion In this paper, we proposed a new approach for explaining the predictions of DNNs. Interpretability is directly related to the readability of an explanation Gilpin et al. (2018). An explanation relying on thousands of features is not interpretable. AXAI, similar to LIME, uses input segmentation to create human-readable explanations focused on important input features. Further, AXAI has the following properties,\nProperty 1 (Robustness): Our approach is more robust to the changes in segmentation hyper-parameters in comparison to other segmentation based approaches such as LIME. This is because AXAI does not require a surrogate model trained on \u201crandomly perturbed inputs.\u201d AXAI uses the deterministic attack magnitudes as \u201cbase explanations\u201d for a given DNN and dataset, and uses segments as an \u201caid\u201d to visualize the results. The segmentation affects the visualizations. We further explain this in Appendix A. Robustness is identical to stability of explanations as defined in Robnik-ikonja and Bohanec (2018). A lower number of nondeterministic steps in the algorithm enhances stability. A carefully filtered explanation based on our approach simply removes the features that have a low impact on predictions.\nOne can interpret this process as a de-noising step to create a sparse representation of explanations.\nProperty 2 (Local attribution): Our algorithm is locally stable and uses local attributes to produce explanations. This is because an adversarial attack uses the most minimal amount of noise within an `2 ball of some small to fool the DNN. Given the un-targeted nature of the attack used in AXAI, the distributions can be interpreted as estimations of the boundaries among neighboring classes. Thus, one can conclude that the attack magnitudes are a representation of feature contributions to the predictions on a local scale. A similar conclusion is made in Ancona et al. (2017), where it is argued that gradients can in fact point to important local attributions of a DNN. We explore this in details in Appendix D.\nProperty 3 (Completeness): Completeness as a property is described as the ability to accurately explain the operations of a DNN Gilpin et al. (2018). An explanation is more complete when it can explain the behavior of the DNN for a larger set of inputs. Sundararajan, Taly, and Yan (2017) and Smilkov et al. (2017) mention the problem of sensitivity and lack of stability in gradient-based algorithms. In the literature, if a solution can reduce the gradient \u201csensitivity\u201d problem, it can be described as having the completeness property Gilpin et al. (2018). AXAI with PDG attack is complete in the same sense as SmoothGrad is Smilkov et al. (2017). SmoothGrad takes the average of saliency maps with added Gaussian noise to reduce sensitivity. The PGD attack behaves in a similar manner by adding adversarial noise at each iteration. Both solutions add perturbations to the input\n0 0.5 1 1.5 2 Time (seconds)\n0\n64\n128\n256\n512\n1024\n2048\n4096\n8192\nFr eq\nue nc\ny (H\nz)\n0\n2\n4\n6\n8\n10\n(a) \u201dHappy\u201d\n0 0.5 1 1.5 2 Time (seconds)\n0\n64\n128\n256\n512\n1024\n2048\n4096\n8192\nFr eq\nue nc\ny (H\nz)\n0\n2\n4\n6\n8\n10\n(b) Explanation\n0 0.5 1 1.5 2 Time (seconds)\n0\n64\n128\n256\n512\n1024\n2048\n4096\n8192\nFr eq\nue nc\ny (H\nz)\n0\n2\n4\n6\n8\n10\n12\n14\n(c) \u201dsix\u201d\n0 0.5 1 1.5 2 Time (seconds)\n0\n64\n128\n256\n512\n1024\n2048\n4096\n8192\nFr eq\nue nc\ny (H\nz)\n0\n2\n4\n6\n8\n10\n12\n14\n(d) Explanation\nto smooth gradient fluctuations. While further research can be done on the power of iterative attacks in their gradient smoothing effects, we argue that AXAI with iterative PGD does have the desirable characteristic and produces stable sharpen visualizations of sensitivity maps for robust explanations.\nLastly, as shown in Section 3, our explainability algorithm exhibits a high-level of fidelity where the explainability outputs are both interpretable and also loyal to the decision making process of the DNN. The produced explainability segments directly point to the places in the input that affect the decision of the DNN. As a result, our solution can be used to explore the relationship between input features and predictions and to understand issues related to the training of DNNs, bias and robustness against adversarial attacks (Appendix E).\nPotential Ethical Impact Our work in this paper contributes to the fields of adversarial machine learning and artificial intelligence explainability (AI Explainability). There is still a huge gap between building a model in Jupyter notebook and shipping it as a standalone product to the users. Advances in these two fields directly relate to the deployment of AI systems that behave in a robust and user-friendly manner after deployment. Building AI systems is hard. AI explainability can provide insights into how AI models behave, why they make the decision they make and the reasoning behind their incorrect predictions. Additionally, explaining the outcomes of a model can help reduce bias and contribute to improvements in accountability and ethics by providing beneficial insights into how AI models think and make their decisions.\nDespite the hype, AI engineers struggle with deploying models which meet the users\u2019 performance expectations. A lack of robustness in the performance of trained model is a major impediment. We need to be able to design AI systems that both perform well and are robust. A robust model not only makes correct predictions in expected environment, but it also maintains an acceptable level of performance in unpredictable situations. Our work gives insights into how the adversary attacks an AI system trained to perform a specific task. Understanding how adversarial attacks behave can help AI engineers in development of AI systems that perform as expected while maintaining some level of robustness in presence of external disturbances and adversarial noise. This type of information can help AI engineers in developing AI models that perform better. In short our paper can help AI researchers in their endeavor to design, develop and deploy explainable ethical AI systems that are robust and reliable.\nReferences Alvarez-Melis, D., and Jaakkola, T. S. 2017. A causal frame-\nwork for explaining the predictions of black-box sequenceto-sequence models. Proceedings of the Conference on Empirical Methods in Natural Language Processing.\nAlvarez-Melis, D., and Jaakkola, T. S. 2018. On the robustness of interpretability methods. arXiv preprint arXiv:1806.08049.\nAncona, M.; Ceolini, E.; O\u0308ztireli, C.; and Gross, M. 2017. Towards better understanding of gradient-based attribution methods for deep neural networks. arXiv preprint arXiv:1711.06104.\nBach, S.; Binder, A.; Montavon, G.; Klauschen, F.; Mu\u0308ller, K.-R.; and Samek, W. 2015. On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation. PloS one 10(7).\nBecker, S.; Ackermann, M.; Lapuschkin, S.; Mu\u0308ller, K.-R.; and Samek, W. 2018. Interpreting and explaining deep neural networks for classification of audio signals. arXiv preprint arXiv:1807.03418.\nBulmer, M. G. 1979. Principles of Statistics. Courier Corporation.\nDatta, A.; Sen, S.; and Zick, Y. 2016. Algorithmic transparency via quantitative input influence: Theory and experiments with learning systems. In IEEE Symposium on Security and Privacy (SP), 598\u2013617. IEEE.\nDeng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and FeiFei, L. 2009. ImageNet: A large-scale hierarchical image database. In IEEE Conference on Computer Vision and Pattern Recognition, 248\u2013255. IEEE.\nErhan, D.; Bengio, Y.; Courville, A.; and Vincent, P. 2009. Visualizing higher-layer features of a deep network. University of Montreal 1341(3):1.\nFidel, G.; Bitton, R.; and Shabtai, A. 2019. When Explainability Meets Adversarial Learning: Detecting Adversarial Examples using SHAP Signatures. arXiv preprint arXiv:1909.03418.\nGilpin, L. H.; Bau, D.; Yuan, B. Z.; Bajwa, A.; Specter, M.; and Kagal, L. 2018. Explaining Explanations: An Overview of Interpretability of Machine Learning.\nGoodfellow, I. J.; Shlens, J.; and Szegedy, C. 2015. Explaining and harnessing adversarial examples. International Conference on Learning Representations (ICLR).\nIlyas, A.; Santurkar, S.; Tsipras, D.; Engstrom, L.; Tran, B.; and Madry, A. 2019. Adversarial examples are not bugs, they are features. In Advances in Neural Information Processing Systems, 125\u2013136.\nJacovi, A.; Shalom, O. S.; and Goldberg, Y. 2018. Understanding convolutional neural networks for text classification. the EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP.\nKaur, P. 2018. Convolutional neural networks (CNN) for cifar-10 dataset. parneetk. github. io/blog/cnn-cifar10/, 2017.\nKim, Y. 2014. Convolutional Neural Networks for Sentence Classification.\nKrizhevsky, A.; Nair, V.; and Hinton, G. 2009. Cifar10 and cifar-100 datasets. URl: https://www. cs. toronto. edu/kriz/cifar. html 6.\nLetham, B.; Rudin, C.; McCormick, T. H.; Madigan, D.; et al. 2015. Interpretable classifiers using rules and bayesian analysis: Building a better stroke prediction model. The Annals of Applied Statistics 9(3):1350\u20131371.\nLi, J.; Chen, X.; Hovy, E.; and Jurafsky, D. 2015. Visualizing and Understanding Neural Models in NLP.\nLundberg, S. M., and Lee, S.-I. 2017. A unified approach to interpreting model predictions. In Advances in Neural Information Processing Systems, 4765\u20134774.\nMadry, A.; Makelov, A.; Schmidt, L.; Tsipras, D.; and Vladu, A. 2017. Towards deep learning models resistant to adversarial attacks. arXiv preprint arXiv:1706.06083.\nMcDonald, J. H. 2009. Handbook of Biological Statistics, volume 2. Sparky House Publishing Baltimore, MD.\nMishra, S.; Sturm, B. L.; and Dixon, S. 2017. Local interpretable model-agnostic explanations for music content analysis. In ISMIR.\nMiyato, T.; Dai, A. M.; and Goodfellow, I. 2016. Adversarial Training Methods for Semi-Supervised Text Classification.\nMohseni, S.; Zarei, N.; and Ragan, E. D. 2018. A survey of evaluation methods and measures for interpretable machine learning. arXiv preprint arXiv:1811.11839.\nPang, B., and Lee, L. 2005. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Proceedings of the ACL.\nRibeiro, M. T.; Singh, S.; and Guestrin, C. 2016. \u201dWhy should I trust you?\u201d Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 1135\u20131144.\nRobnik-ikonja, M., and Bohanec, M. 2018. PerturbationBased Explanations of Prediction Models. 159\u2013175.\nSelvaraju, R. R.; Cogswell, M.; Das, A.; Vedantam, R.; Parikh, D.; and Batra, D. 2017. Grad-Cam: Visual explanations from deep networks via gradient-based localization. In Proceedings of the IEEE International Conference on Computer Vision, 618\u2013626.\nShrikumar, A.; Greenside, P.; and Kundaje, A. 2017. Learning important features through propagating activation differences. In Proceedings of the International Conference on Machine Learning, 3145\u20133153. JMLR. Org.\nSimonyan, K.; Vedaldi, A.; and Zisserman, A. 2013. Deep inside convolutional networks: Visualising image classification models and saliency maps. arXiv preprint arXiv:1312.6034.\nSmilkov, D.; Thorat, N.; Kim, B.; Vie\u0301gas, F.; and Wattenberg, M. 2017. Smoothgrad: Removing noise by adding noise. arXiv preprint arXiv:1706.03825.\nS\u030ctrumbelj, E., and Kononenko, I. 2014. Explaining prediction models and individual predictions with feature contributions. Knowledge and Information Systems 41(3):647\u2013 665.\nSundararajan, M.; Taly, A.; and Yan, Q. 2017. Axiomatic attribution for deep networks. In Proceedings of the International Conference on Machine Learning, 3319\u20133328. JMLR. Org.\nTsipras, D.; Santurkar, S.; Engstrom, L.; Turner, A.; and Madry, A. 2019. Robustness may be at odds with accuracy. International Conference on Learning Representations (ICLR).\nVan Etten, A.; Lindenbaum, D.; and Bacastow, T. M. 2018. SpaceNet: A remote sensing dataset and challenge series. arXiv preprint arXiv:1807.01232.\nVedaldi, A., and Soatto, S. 2008. Quickshift and kernel methods for mode seeking. In European Conference on Computer Vision, 705\u2013718. Springer.\nWarden, P. 2018. Speech Commands: A Dataset for LimitedVocabulary Speech Recognition.\nZhao, G.; Zhou, B.; Wang, K.; Jiang, R.; and Xu, M. 2018. Respond-Cam: Analyzing deep models for 3D imaging data by visualizations. In International Conference on Medical Image Computing and Computer-Assisted Intervention, 485\u2013492. Springer.\nA QuickShift Segmentation QuickShift is a mode seeking clustering algorithm proposed by Vedaldi and Soatto (2008). QuickShift creates segments by repeatedly moving each data point to its closest neighbor point that has higher density calculated by a Parzen Estimator. The Kernel size argument in the QuickShift function controls the width of the gaussian kernel of the estimator. The path of moving points can be seen as a tree that connects data points. Eventually, the algorithm connects all data points into a single tree. To balance between under and over fragmentation of the image, a threshold, \u03c4 , is served as a breaking point that limits the length of the branches in the QuickShift trees. The threshold, \u03c4 , is the Max distance argument in the QuickShift function. Finally, the pre-processing step of QuickShift projects a given image into a 5D space, including color space (r, g, b) and location (x, y). A hyper-parameter, \u03bb, takes a value between 0 and 1 and serves as a weight assigned to the color space, such that the feature space can be presented as {\u03bbr, \u03bbg, \u03bbb, \u03bbx, \u03bby}.\nLIME uses QuickShift for image segmentation where the default Kernel Size is 4, the Max distance is 200, and the threshold \u03c4 is 0.2. This combination prevents generating too many image segments. Even-though the image segmentation process is only performed once per image, we would like to point out that the parameter selection does change the explanation results slightly. First, increasing the kernel size increases the computation time while decreasing the number of image segments, making this parameter the major computational bottleneck in image segmentation. Second, extra care should be taken when it comes to lowresolution images, when the image is coarse and the number of image segments are low, because important and unimportant features can easily be merged together, as demonstrated in Fig. 10. From the perspective of explainability, both accuracy and human-readability are needed. This is achieved as long as the important segments are not merged with unimportant ones. This problem can be solved by selecting a small kernel size. In our algorithm, we introduce a user tunable hyper-parameter, called explainabilty length, K, that allows users to decide the number of explainable segments. Human-readability is subjective, so we let the user decide the explainable length, Fig. 11. We see that in Fig. 11, the wall of the castle on the left most side of the image is merged with the sky due to the similarity between colors. In both case, we picked the top 10 segments as explanations, i.e., explainabilty length=10. It is important to note that unlike LIME and other explainability algorithms, the choice of a longer explainabilty length (more segments) does not increase the computational time of our algorithm.\nDeciding the tradeoff between the importance of the color (r,g,b) and spatial components (x,y) of the feature space, is especially important for high resolution images. Take a castle image in the ImgeNet dataset as an example (given in Fig. 12). We choose two different parameter combinations for comparison. The only difference between the two combinations is the \u03bb parameter. For the first combination, we used 0.2 (Fig. 12b), for the second combination, we used 0.8 (Fig. 12c). One can see that using a lower \u03bb prevents details from merging with irrelevant background information. In Fig. 12b and Fig. 12c, the total number of segments are nearly the same (73 and 81) but the explanations have different qualities.\nB Convergence of Explanations across Adversarial Attacks As a tool for explainability, efficiency, accuracy and consistency are of top priority. Our experiments show that `2 PGD attacks with different iterations create explanations similar to `2 FGM attack. This points to consistency in explanations produced by our algorithm. PGD attack is an iterative version of FGM, while both attacks are subjected to an `2 norm. Note that the distribution of the attacks can influence the explanation results. This also means that since the attack distributions of the first iteration and later iterations of the PGD attack are nearly identical, the overall explanations remain the same. In Fig. 13, we provide an example from the ImageNet dataset to show the convergence of the attacks and consistency of our explanations. Fig. 13b shows the explanation results for an FGM based algorithm. Fig. 13c and Fig. 13d show the explanation results based on\nthe PGD attack with different number of iterations. They both look exactly the same. This is because the slight changes on the attack distribution for different number of iterations, do not affect the overall density of pixel changes in each segment, thus the final explainability results do not change. This point to stability and consistency of our algorithm. To further explore the stability and consistency of our approach, we can segment the image into much smaller segments, as given in Fig. 13e and Fig. 13f, in this case using 50 times more segments than the previous case and then produce the explanations. In this case, we do see small differences between an explanation produced with a PGM attack with 10 iterations and one based on a PGM attack with 40 iterations. These small differences are caused by small differences in the attack distributions in each segment. While it is interesting to further explore how different types of attacks can lead to more \u201csuitable\u201d explanations, it is important to note that one could explain the outcomes using our algorithm and with both types of attacks. Further, we can conclude that using FGM or PGD attacks in our algorithm satisfies consistency, accuracy and efficiency conditions for producing explanations.\nC Further Details on the Statistical Analyses given in Subsection 3.2 C.1 Further details on the statistical tests The Fisher-Pearson coefficient g1 of a distribution x with a sample size N is calculated using the third moment m3 and the second moment m2 of the distribution,\ng1 = m3\nm 3 2 2\n, (2)\nwhere,\nmi = 1\nN N\u2211 n=1 (x[n]\u2212 x\u0304)i (3)\nIf skewness is 0, the data is perfectly symmetrical, if skewness is positive, then one interprets the distribution as skewed right, if skewness is negative, then the distribution is skewed left. Bulmer (1979) pointed out that there are three levels of symmetricity,\na) when skewness is between -0.5 to 0.5, the distribution is \u201capproximately symmetric,\u201d b) when skewness is within -1 and -0.5 or 0.5 and +1, the distribution is \u201cmoderately skewed,\u201d c) when skewness falls out of the mentioned range, then the distribution is highly skewed. The Fisher-Pearson coefficient of all attack magnitudes are shown in Fig. 18. It is seen that the skewness of all attack magnitudes falls within -0.5 an 0.5 showing the strong evidence that the distributions are approximately symmetric.\nThe t-statistic test is represented as follows,\nt = X\u03041 \u2212 X\u03042 sp \u221a 2 n\n(4)\nwhere,\nsp =\n\u221a s2X1 + s 2 X2\n2 (5)\nHere X\u03041, X\u03042 and s2X1 , s 2 X2 are the means and variances of the two distributions with size n. The t-statistic can be interpreted as a kind of measurement for the ratio of the \u201cdifference between groups\u201d over the \u201cdifference within groups.\u201d Carrying out pair t-tests on all samples allows us to further be conservative on the similarity on means between the distributions. The results are shown in Table 4. Overall, there is no significant differences between the distributions.\nTo show the similarity between the distributions produced for a dataset, we also use the one-way ANOVA test on all the samples to show that the means across different distributions are the same. Samples here are defined as intensity vs. frequency distributions for all adversarial test samples created by attacking a model trained on a specific dataset. For CIFAR10, we get the p-value of 0.9, and for a random subset of ImageNet test dataset we get the p-value of 0.94, indicating no significant differences between the distribution means. Similarly, a two-sample location t-test is used to determine if there is a significant difference between two groups where the null hypothesis is the equality of the means. Even-though ANOVA and t-tests are known for being robust on non-normal data, we further performed pair wise MannWhitney U test on all pair of distributions to test whether the mean ranks are similar.\nMannWhitney U test is a nonparametric test of the null hypothesis that two independent samples selected from population have the same distribution. The statistic U is calculated as following,\nU1 = R1 \u2212 n1(n1 \u2212 1)\n2 , U2 = R2 \u2212\nn2(n2)\n2 (6)\nWhere subscripts \u201c1\u201d and \u201c2\u201d denote the two distributions being compared. In the case of comparing two distributions \u201csample 1\u201d and \u201csample 2.\u201d One first combines \u201csample 1\u201d and \u201csample 2\u201d together to form an ordered set, and then one assigns ranks to the members of this set. Next, one adds up the ranks for the members of the set coming from \u201csample 1\u201d and \u201csample 2\u201d respectively. This is called the rank sum ofR1 andR2. Once the rank sums are calculated The U statistic of the two distributions (U1 and U2) are calculated as above. Finally, the U statistic is determined by the lower value between U1 and U2. If U1 is lower than U2, then U1 is the U statistic of the Mann Whitney test between \u201csample1\u201d and \u201csample 2\u201d\u201d and vice versa. We further perform the pair-wise MannWhitney U test on all pair of distributions to test whether the mean ranks are similar as well. If U is 0, it means that the two distributions are far away from each other where there are no overlaps between them. If the Rank sums are close enough, one can say the two distributions are highly overlapped. Thus, one can say the MannWhitney U test is a test comparing the Rank sums (or the mean ranks, calculated by dividing the Rank sums over the size of samples) of two distributions. The smaller values of U1 and U2 is the one used when consulting significance tables."}, {"heading": "C.2 Quantile-Quantile plot", "text": "Quantile-Quantile (Q-Q) plot allows us to show how the quantiles of a distribution deviates from a specified theoretical distribution. The theoretical distribution selected here is the normal distribution. Quantiles are cut points dividing the range of a probability distribution into continuous intervals with equal probabilities. A Q-Q plot is then a scatter-plot showing two sets of quantiles (a sample distribution and a theoretical distribution) against one another. The x-axis are the quantile values of the theoretical distribution while the y-axis are the quantile values of the sample distribution, i.e., the distribution of attack intensities vs. pixel frequencies. One can see that if the quantiles of the sample distribution perfectly match the theoretical quantiles, then one can see all the quantiles located on a straight line. While it is unlikely to have identical distributions that perfectly match the theoretical distribution, one can look at different sections of the Q-Q curves to distinguish the parts that two distributions share similarity and parts that they differ. Compared to a normal distribution, if the sample distribution has heavy or light tails, the Q-Q curve bends at the upper or lower portion based on side of the tails that deviates from the normal distribution. One can say that one purpose of Q-Q plots is to look at the straightness of the Q-Q curve. We took a subset that contains 1000 images from both ImageNet and CIFAR10 and plotted the distributions against a normal distribution as given in Fig. 3. It is seen that all attack distributions plotted against the normal distribution have fairly straight lines at the middle portion of the Q-Q curve, while the curve bends at the upper part and the lower part. One can interpret this result as the attack magnitudes are similar to a normal distribution but differ in a way that the distributions have\u201cheavy tails\u201d thus the upper part of the curve bends \u201cup\u201d and the lower part of the curve bends\u201cdown.\u201d"}, {"heading": "C.3 The beta distribution", "text": "The beta distribution is a family of distributions defined on the interval [a, b] parametrized by two positive shape parameters, denoted by p and q. The general formula for the probability density function of the beta distribution can be written as,\nf(x) = (x\u2212 a)p\u22121(b\u2212 x)q\u22121\nB(p, q)(b\u2212 x)p+q\u22121 (7)\nwhere,\nB(\u03b1, \u03b2) = \u222b 1 0 t\u03b1\u22121(1\u2212 t)\u03b2\u22121dt (8)\nThe beta distribution is often used to describe different types of data, such as rainfall, traffic and financial data. In this paper, estimate the parameters of a beta distribution for our distributions. The method of moments estimation is employed to calculate the shape parameters, p,q, of the two-parameter beta distribution. As the interval [a, b] is known, the method of moments estimates of p and q are\np = x\u0304( x\u0304(1\u2212 x\u0304)\ns2 \u2212 1) (9)\nq = (1\u2212 x\u0304)( x\u0304(1\u2212 x\u0304) s2 \u2212 1) (10)\nWhen the interval [a, b] is [0, 1]. This is called the standard beta distribution. Since in most cases the interval [a, b] is not bounded between [0, 1], one can replace x\u0304 with x\u0304\u2212ab\u2212a and s 2 with s 2\n(b\u2212a)2 . Finally the estimated p and q of the beta distribution is listed in Table 3.\nC.4 Statistical analysis of distributions for DNNs with text or audio input types We test the symmetricity of distributions by calculating the Fisher-Pearson coefficient of skewness for LeNet trained on Speech Commands dataset, and a convolutional neural network (CNN) given in Kim (2014) on Polarity dataset. The Fisher-Pearson coefficients of the attack magnitudes vs. frequency distributions for all 3 cases are shown in Fig. 14. It is seen that the skewness of all distributions falls within the [\u22120.5, 0.5] range showing strong evidence that they are approximately symmetric Bulmer (1979).\nWe perform the two-sample location t-test and Mann-Whitney U test to determine if there is a significant difference between two groups where the null hypothesis is the equality of the means. The results reported in Table 6 indicate no significant difference between the means. Further, the Mann-Whitney U test results indicate that all pairs are similar to each other on the mean ranks. Under the assumption of two distributions having similar shapes, one could further state that Mann-Whitney test can be considered as a test of medians McDonald (2009). Since, we have shown that the shapes are similar, we can conclude that there are no significant difference between the medians of the distributions.\nNext, to show consistency across distributions for a given model, dataset and attack, we estimate the values of quantiles, means and medians. We do this by estimating the statistics of the distributions and constructing confidences intervals. For each experiment, we estimate the mean, median, 15th, 25th, 75th and 85th quantiles of each attack magnitude vs. frequency distribution for the entire test dataset. The statistical confidence interval estimations at confidence level of 95% are reported in Table 7. Our results show that the confidence intervals have narrow ranges and the estimations are consistent. The estimates for the 15th, 25th, 75th and 85th quantiles indicate a strong symmetricity with respect to the origin in all cases. Another\nobservation is that the confidence interval of the mean and medians are pretty narrow, supporting the results of the t-tests and Mann-Whitney U test. Finally, we can show with high confidence that the distributions consistently follow a beta distribution. The beta distribution is a family of distributions defined by two positive shape parameters, denoted by p and q. The estimated p and q of the beta distribution are reported in Table 8.\nD Explanations and Class Boundaries Explaining how important features affect the predictions made by the model depends on the set of classes the model was trained to predict. Un-targeted attacks change the prediction label of an input to the label of its closest neighbor. Based on the different datasets that a model may have been trained on, the label changes after attack may be significantly different. For example, given an image of a \u201cBeagle\u201d and a model that is trained on a dataset consisting of labels {Cats and Dogs}, after attacking the model, the label of the image can change from \u201cDog\u201d to \u201cCat.\u201d But if the same model is trained on a dataset composing of \u201cBeagle, Golden retriever, and Egyptian Cat\u201d, the label of the image can change from \u201cBeagle\u201d to \u201cGolden retriever,\u201d which is a more granule change. When an image is attacked, the features of the image will be directed to the nearest class with a similar probability distribution in the decision layer. Lets look at an example from ImageNet where the input image is classified as a \u201cconvertible\u201d by ResNet34 trained on ImageNet (given in Fig. 15). There are multiple classes such as minivan, sports car, race car etc., under the \u201ccar\u201d category in ImageNet. After attacking the model, the label changes from \u201cconvertible\u201d to \u201csports car.\u201d This indicates that \u201csports car\u201d may be the nearest neighbor class to the \u201cconvertible\u201d class. If we look at the produced explanations we see that segments including the door are intensely attacked as given in Fig. 15b. The fact is that the model thinks that the doors are the most important features for switching the label from \u201cconvertible\u201d to \u201csports car.\u201d Both classes, \u201cconvertible\u201d and \u201csports car,\u201d have similar wheels but different doors. In order to fool the model, attacking the wheels is not of top priority, its the doors that makes the difference between two classes. The fact is that the model thinks that the doors are the most important features for classifying the original image as \u201cconvertible\u201d and not \u201csports car.\u201d Both classes, have similar wheels but different doors. In order to fool the model, attacking the wheels is not of top priority, its the doors that make the difference between two classes. After bluring the segments of interest to the model, i.e. the door segment\u2014Fig. 15c, and feeding the image to the model, the predicted label changes from \u201cconvertible\u201d to \u201csports car\u201d which proves that the doors are the major features supporting the predictions made by the model. Using adversarial attacks as the force behind producing the explanations helps with finding the important features that are not only globally important to the model (doors are important features of cars, other classes do not have doors similar to cars), but also locally important to the model (within the car class, doors are the important features that make a difference between a convertible and a sports car).\nThere are also some explainable features that humans hardly understand but models do, these can be called \u201cnon-robust features.\u201d Tsipras et al. (2019) introduced the concept of robust and non-robust features, where the authors indicated that there are features that humans ignore but the models are sensitive to. They call these the non-robust features. Non-robust features are the features can easily be manipulated by the attacker in order to fool the model. Robust features are features that are both important to the model and also humans and at the same time invincible to small adversarial manipulations.\nE Further Experiment Results E.1 Explaining an image classification model Fig. 16 shows two examples of the explanations produced using AXAI for image samples from ImageNet Deng et al. (2009) test dataset for a Resnet34 trained on ImageNet training dataset. In the first example, Fig. 16a, the explanation results clearly show that the round control panel on an iPod is an important feature that helps the model identify an IPod in the image. The second example, Fig. 16c, shows how the model recognizes that there are two cats in the image (one is the reflection of the cat in the mirror).\nCIFAR10 dataset Kaur (2018) consists of images of size 32 \u00d7 32 pixels, compared to ImageNet, these images are lowresolution images. Fig. 17 shows the explanations produced by AXAI for sample images from CIFAR10 dataset for an AlexNet image classification model trained on CIFAR10 training dataset. For CIFAR10, our explanations clearly separate the background and capture the target object. The explanation given in Fig. 17b shows that the head of the horse with the leather halter is recognized by the model, and the white fence behind the horse is completely ignored by the model. This indicates that the model is well-trained. Similarly in Fig. 17d the ear and head of deer in the image helps the model to classify the image correctly into the deer class. Images from CIFAR10 dataset are easily explained due to the nature of the dataset with most objects in the images being located in the middle of the image and the lack of noisy background in most images.\nE.2 Explaining an object detection model We present two examples of explanations produced by our algorithm for a YOLOv3 object detection model trained on the SpaceNet Building Dataset Van Etten, Lindenbaum, and Bacastow (2018) to detect buildings in overhead imagery. The produced explanation are clearly focused on areas where buildings are located and ignore empty spaces in the images such as the top left\ncorner of Fig. 18b. Further, as seen in Fig. 18d, the roads are ignored and only buildings and their contours affect the predictions made by the object detector.\nE.3 Further details on the speech recognition experiment The Speech Commands Dataset Warden (2018) is an audio dataset of short spoken words, such as \u201cRight,\u201d \u201cThree,\u201d \u201cBed,\u201d etc. The audio files are converted to spectograms and are used to train a LeNet for a command recognition task. A spectrogram is a visual representation of the spectrum of frequencies of a signal as it varies with time. Fig. 5a is an example of a spectrogram. The y-axis, the frequency, of the spectograms are presented on a log-scale, the x-axis represent the time-scale, and the color bar shows the magnitude. Fig. 5a is the frequency spectrum of a human speaking the word \u201cRight.\u201d It is seen that in the time interval 0.4s to 1.1s, high magnitude is presented in the spectrum. In other words, the speaker pronounces the word \u201dRight\u201d around 0.4s to 1.1s into the recorded audio file. This is how one reads a spectrogram. Our explainable solution uses audio files as input, converts them into spectrograms, and then generates the corresponding explanations. So if one feeds AXAI with an audio file of a human speaking \u201cRight,\u201d AXAI first transforms the audio into a spectrogram shown in Fig. 5a, and produces the explanations in Fig. 5b. The explanation will have the exact same scale as the input, and simply masks out the unimportant parts of the spectrogram. To read the explanations, one can refer to the original spectrogram input Fig. 5a and find where the audio is located in the spectrogram (for example looking at the magnitudes), and then look at the corresponding location of the explanations in Fig. 5b.\nThe explanations of two examples are presented in Fig. 5. The spectrogram of the first example \u201cRight\u201d and its explanation are shown in Fig. 5a and Fig. 5b. One can see from Fig. 5a that the spoken word \u201c\u2019Right\u201d appears between 0.4s to 1.1s in the spectrogram of the audio file. If one looks at its corresponding explanation, it is seen that only time-intervals of 0.4s to 0.5s, 0.5s to 0.6s and 1.0s to 1.2s are not masked out by AXAI. This means that these intervals in the audio have great importance for the prediction made by the model. if we look back at Fig. 5a, one then realizes that the explanation shows that the first few and the last few seconds of the spoken word \u201cRight\u201d are important to the model, and the middle part is not. Why is that? The neighboring class of \u201cRight\u201d is \u201cFive.\u201d \u201cRight\u201d and \u201cFive\u201d differ in how \u201dR\u201d & \u201dF\u201d and \u201dt\u201d & \u201dve\u201d are pronounced. The middle part of \u201cFive\u201d and \u201cRight\u201d is highly similar and does not affect the model\u2019s prediction on deciding whether the spoken word is \u201cFive\u201d or \u201cRight.\u201d The second example is \u201cThree.\u201d As seen in the spectrogram, Fig. 5c, \u201cThree\u201d is expressed around the timeinterval 1.4s to 2.2s in the spectrogram of the audio file. The corresponding explanation is shown in Fig. 5d. The explanation masks out almost everywhere except 1.4s to 1.6s and a small part in 1.6s to 1.7s and 1.9s to 2.2s. Now, let\u2019s look at the original\nspectrgram of \u201cThree\u201d and understand what the explanation means. Since The explanation highlights 1.4s to 1.6s, which is the first few seconds of the spoken word. To understand why, one can learn that if we attack the model, then \u201cThree\u201d is missclassified as \u201cTree.\u201d This indicates that the model has learned to recognize \u201cThree\u201d and not \u201cThree\u201d by learning the difference between \u201cThr\u201d and \u201cTr.\u201d The explanation tells us that the first few seconds of the audio are important (the utterance of \u201cThr\u201d)."}, {"heading": "E.4 Ablation study", "text": "If a feature or a group of features is important to a model, then completely removing those features from the input would decrease the probability of a correct prediction. Accordingly, we performed an ablation study confirming that the explanations produced by AXAI contain important features. This ablation method can be used to test the accuracy of an explainability solution. If the generated explanation is faithful to the model, then removing the explanations would decrease the accuracy of the predictions. In this section, we demonstrate a simple experiment to validate our algorithm. Our experiment is performed as follows: 1) Generate the explanation of a targeted image X via AXAI, where the explanation length K = 10 is selected in this experiment, 2) Blur the top 5 explanations/segments of the targeted image according to the produced explanations, feed the modified image to the model and obtain its label, 3) repeat this process throughout the test dataset 4) Calculate the total decrease in accuracy. We use a ResNet34 training on ImageNet for this experiment and report the results for the entire ImageNet test dataset. Our results show that the prediction accuracy of the DNN decreases to %43 after blurring the top 5 explanation/segments. To further investigate, instead of blurring the top 5 explanations, we blur only the 6th to10th explanations. This results in a %22 drop in total accuracy. Hence, we can conclude 1) AXAI generates faithful explanations so that blurring the top explanations (the 1st-5th explanations) lead to a strong decrease in model prediction accuracy, and 2) AXAI generates faithful explanations in order of importance, i.e., the generated 6th to 10th explanations are also important to the model but their influence on model predictions is relatively less than the first 5 generated explanations.\nE.5 AXAI explanations for a robust model trained with adversarial training In this subsection, we compare the explanations produced for a robust model to explanations produced for a non-robust model. In our experiment, a robust model is a model trained on an adversarial dataset in addition to the training dataset so that the final trained model is more robust against adversarial attacks. Hypothetically, a robust model should focus more on robust important input features when making predictions. We have trained a non-robust AlexNet and a robust AlexNet on CIFAR10 and produced the explanations using AXAI for test inputs. Fig. 19 shows the AXAI produced explanations for the DNN given a sample input. It is seen that a small part of the background is included in the explanations produced for the non-robust AlexNet. However, the AXAI generated explanations for the robust model includes only the important features pertaining to the object in the image. In addition, the leg of the deer is now included in the explanations as well. It is concluded that explanations produced for the robust DNN are sharper, clearer and more robust than the ones generated for the regularly trained DNN."}, {"heading": "E.6 Additional Examples", "text": "In this section, we provide additional explainability results from using AXAI on an AlexNet image classification model trained on CIFAR10, a VGG16 image classification model trained on CIFAR100, a ResNet34 image classification model trained on ImageNet, the LeNet speech recognition model and the sentence classification model, Fig. 20, Fig. 21, Fig. 22, Fig. 23, and Fig. 24.\nF Benchmark Tests We test our algorithm against LIME and SHAP. We use \u201cGradient Explainer\u201d in SHAP, which integrates the f Integrated gradients algorithm with SHAP. Fig. 25 shows some sample comparisons among the 3 algorithms for 3 cases: 1) AlexNet trained on CIFAR10, 2) ResNet34 trained on ImageNet, 3) VGG16 trained on CIFAR100. PGDM with 20 iterations is used in our algorithm. For ImageNet, explanations for a sample test picture belonging to \u201cEgyptian cat\u201d are shown in Fig. 25a, Fig. 25b, and Fig. 25c. One can see the similarity between the explanations. The explanations produced by the 3 algorithms focused on the upper left of the image which contains the eyes of the \u201cEgyptian cat.\u201d Both LIME and our algorithms point to the same segment as explanations. SHAP (Gradient Explainer) locates pixels of interest. The important pixels shown in this case aligns with the results of LIME and AXAI. Since the default image segmentation parameters LIME chooses do not allow for a suitable number of segments for explanation for CIFAR10 and CIFAR100 due to the resolutions of images, we lowered the Kernel size parameter to 1. The default Kernel size parameters LIME uses for QuickShift is too large for low-resolution images. As we mentioned before, this leads to a few very large segments in the image and neglects all the granular details in the image. For CIFAR10, both our approach and LIME capture the upper portion of the head of the horse including the ears and eyes (Fig. 25d, Fig. 25e). The results of SHAP point out the important pixels located on the head, the nose and some pixels in the background (Fig. 25f). For CIFAR100, the explanations produced by the 3 algorithms are once again highly similar (Fig. 25g, Fig. 25h, and Fig. 25i). One can see that in many cases, pixel explanations do not serve as the best solution. Without the segments, it is hard to grasp the meaning behind explanations, this is because the human brain tends to comprehend image segments better than individual pixels."}], "title": "An Adversarial Approach for Explaining the Predictions of Deep Neural Networks", "year": 2020}