{"abstractText": "Since most machine learning models provide no explanations for the predictions, their predictions are obscure for the human. The ability to explain a model\u2019s prediction has become a necessity in many applications including Twitter mining. In this work, we propose a method called Explainable Twitter Mining (Ex-Twit) combining Topic Modeling and Local Interpretable Model-agnostic Explanation (LIME) to predict the topic and explain the model predictions. We demonstrate the effectiveness of Ex-Twit on Twitter health-related data.", "authors": [{"affiliations": [], "name": "Tunazzina Islam"}], "id": "SP:e77fa399ef5b6c145af5d147bca6918cca6b5d58", "references": [{"authors": ["Rubayyi Alghamdi", "Khalid Alfalqi. A survey of topic modeling in text mining. Int. J. Adv. Comput"], "title": "Sci", "venue": "Appl.(IJACSA), 6(1),", "year": 2015}, {"authors": ["Mehdi Allahyari", "Seyedamin Pouriyeh", "Mehdi Assefi", "Saied Safaei", "Elizabeth D Trippe", "Juan B Gutierrez", "Krys Kochut"], "title": "A brief survey of text mining: Classification", "venue": "clustering and extraction techniques. arXiv preprint arXiv:1707.02919,", "year": 2017}, {"authors": ["Bian et al", "2012] Jiang Bian", "Umit Topaloglu", "Fan Yu"], "title": "Towards large-scale twitter mining for drug-related adverse events", "venue": "In Proceedings of the 2012 international", "year": 2012}, {"authors": ["David M Blei", "Andrew Y Ng", "Michael I Jordan. Latent dirichlet allocation"], "title": "Journal of machine Learning research", "venue": "3:993\u20131022,", "year": 2003}, {"authors": ["Nathan K Cobb", "Amanda L Graham. Health behavior interventions in the age of facebook"], "title": "American journal of preventive medicine", "venue": "43(5):571\u2013572,", "year": 2012}, {"authors": ["Munmun De Choudhury", "Michael Gamon", "Scott Counts", "Eric Horvitz"], "title": "Predicting depression via social media", "venue": "Seventh international AAAI conference on weblogs and social media,", "year": 2013}, {"authors": ["Scott Deerwester", "Susan T Dumais", "George W Furnas", "Thomas K Landauer", "Richard Harshman. Indexing by latent semantic analysis"], "title": "Journal of the American society for information science", "venue": "41:391\u2013 407,", "year": 1990}, {"authors": ["Eichstaedt et al", "2018] Johannes C Eichstaedt", "Robert J Smith", "Raina M Merchant", "Lyle H Ungar", "Patrick Crutchley", "Daniel Preo\u0163iuc-Pietro", "David A Asch", "H Andrew Schwartz"], "title": "Facebook language predicts depression in medical records", "year": 2018}, {"authors": ["Gilpin et al", "2018] Leilani H Gilpin", "David Bau", "Ben Z Yuan", "Ayesha Bajwa", "Michael Specter", "Lalana Kagal"], "title": "Explaining explanations: An overview of interpretability of machine learning", "venue": "IEEE 5th International Conference on Data Science and Advanced Analytics (DSAA),", "year": 2018}, {"authors": ["David Gunning"], "title": "Explainable artificial intelligence (xai)", "venue": "Defense Advanced Research Projects Agency (DARPA), nd Web,", "year": 2017}, {"authors": ["Tunazzina Islam"], "title": "Yoga-veganism: Correlation mining of twitter health data", "venue": "arXiv preprint arXiv:1906.07668,", "year": 2019}, {"authors": ["Been Kim", "Julie A Shah", "Finale Doshi-Velez"], "title": "Mind the gap: A generative approach to interpretable feature selection and extraction", "venue": "Advances in Neural Information Processing Systems, pages 2260\u2013 2268,", "year": 2015}, {"authors": ["Lee", "Seung", "2001] Daniel D Lee", "H Sebastian Seung"], "title": "Algorithms for non-negative matrix factorization", "year": 2001}, {"authors": ["Tao Lei", "Regina Barzilay", "Tommi Jaakkola"], "title": "Rationalizing neural predictions", "venue": "arXiv preprint arXiv:1606.04155,", "year": 2016}, {"authors": ["Benjamin Letham", "Cynthia Rudin", "Tyler H McCormick", "David Madigan"], "title": "et al", "venue": "Interpretable classifiers using rules and bayesian analysis: Building a better stroke prediction model. The Annals of Applied Statistics, 9(3):1350\u20131371,", "year": 2015}, {"authors": ["Tomas Mikolov", "Kai Chen", "Greg Corrado"], "title": "and Jeffrey Dean", "venue": "Efficient estimation of word representations in vector space.", "year": 2013}, {"authors": ["Alexander Pak", "Patrick Paroubek. Twitter as a corpus for sentiment analysis", "opinion mining. In LREc"], "title": "volume 10", "venue": "pages 1320\u20131326,", "year": 2010}, {"authors": ["Rafeeque Pandarachalil", "Selvaraju Sendhilkumar", "GS Mahalakshmi"], "title": "Twitter sentiment analysis for large-scale data: an unsupervised approach", "venue": "Cognitive computation, 7(2):254\u2013262,", "year": 2015}, {"authors": ["Martin F Porter. An algorithm for suffix stripping"], "title": "Program", "venue": "14(3):130\u2013137,", "year": 1980}, {"authors": ["Kyle W Prier", "Matthew S Smith", "Christophe Giraud-Carrier", "Carl L Hanson. Identifying health-related topics on twitter. In International conference on social computing"], "title": "behavioral-cultural modeling", "venue": "and prediction, pages 18\u201325. Springer,", "year": 2011}, {"authors": ["Prieto et al", "2014] V\u0131\u0301ctor M Prieto", "Sergio Matos", "Manuel Alvarez", "Fidel Cacheda", "Jos\u00e9 Lu\u0131\u0301s Oliveira"], "title": "Twitter: a good place to detect health conditions", "venue": "PloS one,", "year": 2014}, {"authors": ["Andrew G Reece", "Andrew J Reagan", "Katharina LM Lix", "Peter Sheridan Dodds", "Christopher M Danforth", "Ellen J Langer. Forecasting the onset", "course of mental illness with twitter data"], "title": "Scientific reports", "venue": "7(1):13006,", "year": 2017}, {"authors": ["Marco Tulio Ribeiro", "Sameer Singh", "Carlos Guestrin"], "title": "Why should i trust you?: Explaining the predictions of any classifier", "venue": "Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pages 1135\u20131144,", "year": 2016}, {"authors": ["Carson Sievert", "Kenneth Shirley"], "title": "Ldavis: A method for visualizing and interpreting topics", "venue": "Proceedings of the workshop on interactive language learning, visualization, and interfaces, pages 63\u201370,", "year": 2014}, {"authors": ["Son et al", "2017] Youngseo Son", "Anneke Buffone", "Joe Raso", "Allegra Larche", "Anthony Janocko", "Kevin Zembroski", "H Andrew Schwartz", "Lyle Ungar"], "title": "Recognizing counterfactual thinking in social media texts", "venue": "In Proceedings of the 55th Annual Meeting of the Association for Compu-", "year": 2017}, {"authors": ["Zhengkui Wang", "Guangdong Bai", "Soumyadeb Chowdhury", "Quanqing Xu", "Zhi Lin Seow"], "title": "Twiinsight: Discovering topics and sentiments from social media datasets", "venue": "arXiv preprint arXiv:1705.08094,", "year": 2017}, {"authors": ["Yaden et al", "2018] David B Yaden", "Johannes C Eichstaedt", "Margaret L Kern", "Laura K Smith", "Anneke Buffone", "David J Stillwell", "Michal Kosinski", "Lyle H Ungar", "Martin EP Seligman", "H Andrew Schwartz"], "title": "The language of religious affiliation: social, emotional, and cognitive", "year": 2018}, {"authors": ["Sunmoo Yoon", "No\u00e9mie Elhadad", "Suzanne Bakken. A practical approach for content mining of tweets"], "title": "American journal of preventive medicine", "venue": "45(1):122\u2013129,", "year": 2013}], "sections": [{"heading": "1 Introduction", "text": "With the increasing availability and efficiency of modern computing, machine learning systems have become increasingly powerful and complex. Dramatic success in machine learning has led to an explosion of AI applications [Gunning, 2017]. The problem with most of the machine learning algorithms is that they are designed to be trained over training examples and their predictions are obscure for the human. There is a vital concern regarding trust. Explainable AI (XAI) provides details explanations of the decisions in some level [Gilpin et al., 2018]. These explanations are important to ensure fairness of machine learning algorithm and identify potential bias in the training data.\nThe main motivation of this work has been started with few questions (1) What do people do to maintain their health?\u2013 some people do balanced diet, some do exercise. Among diet plans some people maintain vegetarian diet/vegan diet, among exercises some people do swimming, cycling or yoga. Now-a-days people usually share their interest, thoughts via discussions, tweets, status in social media (i.e. Facebook, Twitter, Instagram etc.). It\u2019s huge amount of data and it\u2019s not possible to go through all the data manually. This motivates us to do Twitter mining on health-related data using Topic Modeling. (2) To what extend we could trust the model\nIn Proceedings of 7th International Workshop on Natural Language Processing for Social Media (SocialNLP 2019) at IJCAI 2019 in Macao, China.\npredictions? This research question is one of the main motivations of our work to explain the prediction of model.\nTwitter has been growing in popularity and now-a-days, it is used everyday by people to express opinions about different topics, such as products, movies, health, music, politicians, events, among others. Twitter data constitutes a rich source that can be used for capturing information about any topic imaginable. This data can be used in different use cases such as finding trends related to a specific keyword, measuring brand sentiment, and gathering feedback about new products and services. In this work, we use text mining to mine the Twitter health-related data. Text mining is the application of natural language processing techniques to derive relevant information [Allahyari et al., 2017]. This is getting a lot attention these last years, due to an exponential increase in digital text data from web pages. In this paper, we use topic mod-\neling to infer semantic structure of the unstructured data (i.e Tweets). Topic Modeling is a text mining technique which automatically discovers the hidden themes from given documents. It is an unsupervised text analytic algorithm that is used for finding the group of words from the given document. In our work, Topic model predicts the topics of corresponding tweets. In order to trust black-box methods, we need explainability. We incorporate Local Interpretable Model-agnostic Explanation (LIME) [Ribeiro et al., 2016] method with topic modeling for the explanation of model predictions. We build a pipeline called Ex-Twit: Explainable Twitter Mining on Health Data to explain the prediction of topic modeling. Finally, we conduct experiment on real Twitter data and show\nar X\niv :1\n90 6.\n02 13\n2v 2\n[ cs\n.C L\n] 2\n2 Ju\nn 20\n19\nthat our approach successfully predicts the topic with explanations.\nFig. 1 shows the overall pipeline of Ex-Twit. The implication of our work is to explain the annotation of unlabeled data done by the model."}, {"heading": "2 Related Work", "text": "Use of data generated through social media is gradually increasing. Several works have been done on prediction of social media content [Son et al., 2017]; [Yaden et al., 2018]; [Eichstaedt et al., 2018]; [De Choudhury et al., 2013]; [Bian et al., 2012]; [Pandarachalil et al., 2015]; [Pak and Paroubek, 2010]; [Cobb and Graham, 2012]. Social media support the collection and analysis of data in real time in the real world.\nTwitter has been growing in popularity now-a-days. [Prieto et al., 2014] proposed a method to extract a set of tweets to estimate and track the incidence of health conditions in society. Discovering public health topics and themes in tweets had been examined by [Prier et al., 2011]. [Yoon et al., 2013] described a practical approach of content mining to analyze tweet contents and illustrate an application of the approach to the topic of physical activity. [Reece et al., 2017] developed computational models to predict the emergence of depression and Post-Traumatic Stress Disorder (PTSD) in Twitter users. [Wang et al., 2017] developed a system called TwiInsight to identify the insight from Twitter data. [Islam, 2019] did correlation mining to explore hidden patterns and unknown correlations in Twitter health-related data. In our work, we use Topic Modeling to extract topic from health-related tweets.\nMachine learning systems have become increasingly powerful and complex day by day. Developing sparse interpretable models is of considerable interest to the broader research community [Letham et al., 2015]; [Kim et al., 2015]; [Lei et al., 2016]. To justify predictions made by neural networks, [Lei et al., 2016] built modular neural framework to automatically generate concise yet sufficient text fragments. [Ribeiro et al., 2016] proposed a model agnostic framework (LIME) where the proxy model is learned ensuring locally valid approximations for the target sample and its neighborhood. In this paper, we incorporate LIME [Ribeiro et al., 2016] framework with topic modeling.\nTo justify the model\u2019s prediction on Twitter mining, we propose a method combining topic modeling and LIME model explanation named Ex-Twit to predict the healthrelated topic and explain the model predictions for unsupervised learning."}, {"heading": "3 Data Collection", "text": "We use Twitter health-related data for Ex-Twit. In Subsection 3.1, we show data crawling process from Twitter. Subsection 3.2 shows data pre-processing method."}, {"heading": "3.1 Data Extraction", "text": "The twitter data has been crawled using Tweepy which is a Python library for accessing the Twitter API. We use Twitter streaming API to extract 40k tweets (April 17-19, 2019). For the crawling, we focus on several keywords that are related to health. The keywords are processed in a non-case-\nsensitive way. We use filter to stream all tweets containing the word \u2018yoga\u2019, \u2018healthylife\u2019, \u2018healthydiet\u2019, \u2018diet\u2019,\u2018hiking\u2019, \u2018swimming\u2019, \u2018cycling\u2019, \u2018yogi\u2019, \u2018fatburn\u2019, \u2018weightloss\u2019, \u2018pilates\u2019, \u2018zumba\u2019, \u2018nutritiousfood\u2019, \u2018wellness\u2019, \u2018fitness\u2019, \u2018workout\u2019, \u2018vegetarian\u2019, \u2018vegan\u2019, \u2018lowcarb\u2019, \u2018glutenfree\u2019, \u2018calorieburn\u2019."}, {"heading": "3.2 Data Pre-processing", "text": "Data pre-processing is one of the key components in many text mining algorithms [Allahyari et al., 2017]. Data cleaning is crucial for generating a useful topic model. We have some prerequisites i.e. we download the stopwords from NLTK (Natural Language Toolkit) and spacy\u2019s en model for text preprocessing.\nIt is noticeable that the parsed full-text tweets have many emails, \u2018RT\u2019, newline and extra spaces that is quite distracting. We use Python Regular Expressions (re module) to get rid of them.\nAfter removing the emails and extra spaces, we tokenize each text into a list of words, remove punctuation and unnecessary characters. We use Python Gensim package for further processing. Gensim\u2019s simple preprocess() is used for tokenization and removing punctuation. We use Gensim\u2019s Phrases model to build bigrams. Certain parts of English speech, like conjunctions (\u201cfor\u201d, \u201cor\u201d) or the word \u201cthe\u201d are meaningless to a topic model. These terms are called stopwords and we remove them from the token list. We use spacy model for lemmatization to keep only noun, adjective, verb, adverb. Stemming words is another common NLP technique to reduce topically similar words to their root. For example, \u201cconnect\u201d, \u201cconnecting\u201d, \u201cconnected\u201d, \u201cconnection\u201d, \u201cconnections\u201d all have similar meanings; stemming reduces those terms to \u201cconnect\u201d. The Porter stemming algorithm [Porter, 1980] is the most widely used method."}, {"heading": "4 Methodology", "text": "In this paper, we show a method to explain unsupervised learning. Our approach called Ex-Twit combining topic modeling and LIME model explanation predicts and explains health-related topics extracted from Twitter. Subsections 4.1 and 4.2 elaborately present how we can infer the meaning of unstructured data. Subsection 4.3 shows how we do manual annotation for ground truth comparison. We show the\nmethod for explainability in Subsectuon 4.4. Fig. 1 shows the methodology of Ex-Twit."}, {"heading": "4.1 Construct bag-of-words Model", "text": "The result of the data cleaning stage is texts, a tokenized, stopped, stemmed and lemmatized list of words from a single tweet. To understand how frequently each term occurs within each tweet, we construct a document-term matrix using Gensim\u2019s Dictionary() function. Gensim\u2019s doc2bow() function converts dictionary into a bag-of-words. In the bagof-words model, each tweet is represented by a vector in a mdimensional coordinate space, where m is number of unique terms across all tweets. This set of terms is called the corpus vocabulary."}, {"heading": "4.2 Choose optimal number of Topics", "text": "Topic modeling is a text mining technique which provides methods for identifying co-occurring keywords to summarize collections of textual information. This is used to analyze collections of documents, each of which is represented as a mixture of topics, where each topic is a probability distribution over words [Alghamdi and Alfalqi, 2015]. Applying these models to a document collection involves estimating the topic distributions and the weight each topic receives in each document. A number of algorithms exist for solving this problem. We use three unsupervised machine learning algorithms to explore the topics of the tweets: Latent Semantic Analysis (LSA) [Deerwester et al., 1990], Non-negative Matrix Factorization (NMF) [Lee and Seung, 2001], and Latent Dirichlet Allocation (LDA) [Blei et al., 2003].\nTopic modeling is an unsupervised learning, so the set of possible topics are unknown. To find out the optimal number of topic, we build many LSA, NMF, LDA models with different values of number of topics (k) and pick the one that gives the highest coherence score. Choosing a \u2018k\u2019 that marks the end of a rapid growth of topic coherence usually offers meaningful and interpretable topics.\nWe use Gensim\u2019s coherencemodel to calculate topic coherence for topic models (LSA and LDA). For NMF, we use a topic coherence measure called TC-W2V. This measure relies on the use of a word embedding model constructed from the corpus. So in this step, we use the Gensim implementation of Word2Vec [Mikolov et al., 2013] to build a Word2Vec model based on the collection of tweets.\nWe achieve the highest coherence score = 0.4495 when the number of topics is 2 for LSA, for NMF the highest coherence value is 0.6433 for K = 4, and for LDA we also get number of topics is 4 with the highest coherence score which is 0.3871 (see Fig. 2).\nFor our dataset, we picked k = 2, 4, and 4 with the highest coherence value for LSA, NMF, and LDA correspondingly (Fig. 2). Table 1 shows the topics and top-10 keywords of the corresponding topic. We get more informative and understandable topics using LDA model than LSA. LSA decomposed matrix is a highly dense matrix, so it is difficult to index individual dimension. LSA unable to capture the multiple meanings of words. It offers lower accuracy than LDA.\nIn case of NMF, we observe same keywords are repeated in multiple topics. Keywords \u201cgo\u201d, \u201cday\u201d both are repeated in Topic 2, Topic 3, and Topic 4 (See 4th, 5th, and 6th columns of Table 1). In Table 1 keyword \u201cyoga\u201d has been found both in Topic 1 (3rd column) and Topic 4 (6th column). We also notice that keyword \u201ceat\u201d is in Topic 2 and Topic 3 (Table 1 4th and 5th columns). If the same keywords being repeated in multiple topics, it is probably a sign that the \u2018k\u2019 is large though we achieve the highest coherence score in NMF for k=4.\nWe choose topics generated by LDA model for our further analysis. Because LDA is good in identifying coherent topics where as NMF usually gives incoherent topics. However, in the average case NMF and LDA are similar but LDA is more consistent."}, {"heading": "4.3 Manual Annotation", "text": "To calculate the accuracy of model in comparison with ground truth label, we selected top 500 tweets from train dataset (40k tweets). We extracted 500 new tweets (22 April, 2019) as a test dataset. We did manual annotation both for train and test data by choosing one topic among the 4 topics generated from LDA model (7th, 8th, 9th, and 10th columns of Table 1) for each tweet based on the intent of the tweet. Consider the following two tweets:\nTweet 1: Learning some traditional yoga with my good friend.\nTweet 2: Why You Should #LiftWeights to Lose #BellyFat #Fitness #core #abs #diet #gym #bodybuilding #workout #yoga\nThe intention of Tweet 1 is yoga activity (i.e. learning yoga). Tweet 2 is more about weight lifting to reduce belly fat. This tweet is related to workout. When we do manual annotation, we assign Topic 2 in Tweet 1, and Topic 1 in Tweet 2. It\u2019s not wise to assign Topic 2 for both tweets based on the keyword \u201cyoga\u201d. We assign zero if we could not infer which topic goes well with the corresponding tweet. During annotation, we focus on functionality of tweets."}, {"heading": "4.4 Prediction Explanation", "text": "It\u2019s easier for human to infer the topic from the tweets and human can explain the reason of the inference. But machine learning models remain mostly black boxes. Explainable machine learning model can present textual or visual artifacts that provide qualitative understanding of the relationship between the instance\u2019s components (e.g. words in text) and the\nmodel\u2019s prediction [Ribeiro et al., 2016]. We incorporate LIME [Ribeiro et al., 2016] method for the explanation of model predictions of twitter mining for health data.\nThe LIME algorithm generates distorted versions of the tweets by removing some of the words from the tweets; predict probabilities for these distorted tweets using the blackbox classifier; train another classifier which tries to predict output of a black-box classifier on these tweets.\nWe evaluate the explanation by observing the accuracy score weighted by cosine distance between generated sample and original tweets and Kullback\u2013Leibler divergence (KL divergence) which is also weighted by distance. KL divergence 0.0 means a perfect match."}, {"heading": "2 2 0.026 1.0", "text": ""}, {"heading": "3 3 0.015 0.95", "text": ""}, {"heading": "2 2 0.024 0.95", "text": ""}, {"heading": "1 1 0.025 0.94", "text": ""}, {"heading": "1 1 0.03 0.97", "text": ""}, {"heading": "5 Results and Discussion", "text": ""}, {"heading": "5.1 Visualization", "text": "We use LDAvis [Sievert and Shirley, 2014], a web-based interactive visualization of topics estimated using LDA. Gensim\u2019s pyLDAVis is the most commonly used visualization tool to visualize the information contained in a topic model. In Fig. 3, each bubble on the left-hand side plot represents a topic. The larger the bubble, the more prevalent is that topic. A good topic model has fairly big, non-overlapping bubbles scattered throughout the chart instead of being clustered in one quadrant. A model with too many topics, is typically have many overlaps, small sized bubbles clustered in one region of the chart. In right hand side, the words represent the salient keywords.\nIf we move the cursor over one of the bubbles (Fig. 3b), the words and bars on the right-hand side have been updated and top-30 salient keywords that form the selected topic and their estimated term frequencies are shown."}, {"heading": "5.2 Comparison with Ground Truth", "text": "To compare with ground truth, we extracted 500 tweets from train data and test data (new tweets) and did manual annotation both for train/test data based on functionality of tweets (described in Subsection 4.3). We achieved approximately 93% train accuracy and 80% test accuracy."}, {"heading": "5.3 Explanation Observation", "text": "In Table 2, we show some observation of explanations of the twitter mining. Top two rows (1st and 2nd row) of Table 2 show the tweets, predicted topic, annotated topic, mean KL divergence, and score from train data. Rest of the rows (3rd, 4th, 5th, and 6th row) of Table 2 show the tweets from the test dataset.\nFor the tweets from train data (1st and 2nd row of Table 2), predicted topic and the annotated topic is same. Topic 2 has been inferred by human and predicted by the model\n(1st row of Table 2). In the 1st row, we have good accuracy (100%) with mean KL divergence 0.026. For the 2nd row of Table 2, Topic 3 has been inferred by human and predicted by the model with lower mean KL divergence (0.015) having 95% accuracy. In Table 2, we show the tweets from test data\nwhere human annotation matches with model prediction in 3rd, 4th, and 5th row. Fig. 4 shows the explainable output of model prediction for one of the tweets from test data (4th row of Table 2). This tweet is related to someone\u2019s weight loss journey by changing diet and going gym 5/6 times a week. We annotated this tweet with Topic 1 (7th column of Table 1 shows the corresponding keywords).\nIn Fig. 4, y represents topic number and y = 0 means incomprehensible topic (we assign zero). For y = 1, the words \u2018diet\u2019 and \u2018gym\u2019 (green marker) match with 2 topic keywords of Topic 1 (7th column of Table 1) containing the most frequent keyword (\u2018diet\u2019). For Topic 1, we achieved the highest prediction probability (\u2248 0.89) and the highest score (\u2248 2.4) in this tweet (4th row of Table 2).\nThe 6th row of Table 2, we have tweets (having mean KL divergence = 0.061 and accuracy score = 92%) from test data where the predicted topic and human annotated topic are not same.\nIn Fig. 5, we show why the model prediction and manual annotation differs for the tweet of 6th row of Table 2. Human inferred the tweet as a swimming related topic (Topic 3). 9th column of Table 1 shows the corresponding top-10 keywords of Topic 3. Our model predicted Topic 4 (10th column of Table 1 shows the corresponding top-10 keywords) as the topic of the tweet. Ex-Twit can provide a possible explanation.\nFrom Fig. 5, we observed that for y = 1, word \u2018workout\u2019 (green marker) matches with one of the topic keywords of Topic 1 (7th column of Table 1). There are 4 mismatches (3 darker red markers and 1 lighter red marker). We can see the probability of prediction is 0.023. There is no matching keyword in Topic 2 for the tweet (when y = 2). For y = 3, word \u2018swimming\u2019 (green marker) matches with the top frequent keyword of Topic 3 (9th column of Table 1) having higher prediction (prediction probability \u2248 0.16). This one also has 4 mismatches (red markers). It is noticeable that for y = 4, words \u2018fitness\u2019, \u2018wellness\u2019, and \u2018great\u2019 (green markers) match with 3 topic keywords of Topic 4 (10th column of Table 1) containing the top frequent keyword (\u2018fitness\u2019). Topic 4 achieved the highest prediction probability (\u2248 0.82) with the highest score (\u2248 1.3) among other topics for the tweet. Fig. 5 shows the explanation of model prediction (Topic 4) for this tweet (last row of Table 2)."}, {"heading": "6 Conclusions", "text": "We proposed a method called Explainable Twitter Mining (Ex-Twit) combining LDA and LIME. We successfully showed the effectiveness of Ex-Twit in explanation of model predictions on Twitter health-related data. We believe that Ex-Twit can be applied more generally for the explanation of predictions in other NLP related applications. In the future, we would like to add more tweets to further validate our approach and observe the scalability."}], "title": "Ex-Twit: Explainable Twitter Mining on Health Data", "year": 2019}