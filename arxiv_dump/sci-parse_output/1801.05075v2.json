{
  "abstractText": "Research in interpretable machine learning proposes different computational and human subject approaches to evaluate model saliency explanations. These approaches measure different qualities of explanations to achieve diverse goals in designing interpretable machine learning systems. In this paper, we propose a human attention benchmark for image and text domains using multi-layer human attention masks aggregated from multiple human annotators. We then present an evaluation study to evaluate model saliency explanations obtained using Grad-cam and LIME techniques. We demonstrate our benchmark\u2019s utility for quantitative evaluation of model explanations by comparing it with human subjective ratings and ground-truth single-layer segmentation masks evaluations. Our study results show that our threshold agnostic evaluation method with the human attention baseline is more effective than single-layer object segmentation masks to ground truth. Our experiments also reveal user biases in the subjective rating of model saliency explanations.",
  "authors": [
    {
      "affiliations": [],
      "name": "Sina Mohseni"
    },
    {
      "affiliations": [],
      "name": "Jeremy E. Block"
    },
    {
      "affiliations": [],
      "name": "Eric D. Ragan"
    }
  ],
  "id": "SP:17c04f3b42a015cf470a649d38d2d8da9bf9d5fd",
  "references": [
    {
      "authors": [
        "Adebayo"
      ],
      "title": "Sanity checks for saliency maps",
      "venue": "In Advances in Neural Information Processing Systems,",
      "year": 2018
    },
    {
      "authors": [
        "Alqaraawi"
      ],
      "title": "Evaluating saliency map explanations for convolutional neural networks: A user study",
      "venue": "arXiv preprint arXiv:2002.00772",
      "year": 2020
    },
    {
      "authors": [
        "Choe"
      ],
      "title": "Evaluating weakly supervised object localization methods right",
      "venue": "In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,",
      "year": 2020
    },
    {
      "authors": [
        "Das"
      ],
      "title": "Human attention in visual question answering: Do humans and deep networks look at the same regions? Computer Vision and Image Understanding 163:90\u2013100",
      "year": 2017
    },
    {
      "authors": [
        "Deng"
      ],
      "title": "ImageNet: A large-scale hierarchical image database",
      "year": 2009
    },
    {
      "authors": [
        "Doshi-Velez",
        "F. Kim 2017] Doshi-Velez",
        "B. Kim"
      ],
      "title": "Towards a rigorous science of interpretable machine learning",
      "venue": "arXiv preprint arXiv:1702.08608",
      "year": 2017
    },
    {
      "authors": [
        "Du"
      ],
      "title": "Towards explanation of dnn-based prediction with guided feature inversion",
      "year": 2018
    },
    {
      "authors": [
        "Everingham"
      ],
      "title": "The pascal visual object classes challenge: A retrospective",
      "year": 2015
    },
    {
      "authors": [
        "Hooker"
      ],
      "title": "A benchmark for interpretability methods in deep neural networks",
      "venue": "In Advances in Neural Information Processing Systems,",
      "year": 2019
    },
    {
      "authors": [
        "Amershi Kocielnik",
        "R. Bennett 2019] Kocielnik",
        "S. Amershi",
        "P.N. Bennett"
      ],
      "title": "Will you accept an imperfect ai? exploring designs for adjusting end-user expectations of ai systems",
      "venue": "In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems,",
      "year": 2019
    },
    {
      "authors": [
        "Lage"
      ],
      "title": "Human evaluation of models built for interpretability",
      "venue": "In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing,",
      "year": 2019
    },
    {
      "authors": [
        "Lakkaraju",
        "H. Bastani 2019] Lakkaraju",
        "O. Bastani"
      ],
      "title": " how do i fool you?\u201d: Manipulating user trust via misleading black box explanations",
      "venue": "arXiv preprint arXiv:1911.06473",
      "year": 2019
    },
    {
      "authors": [
        "Barzilay Lei",
        "T. Jaakkola 2016] Lei",
        "R. Barzilay",
        "T.S. Jaakkola"
      ],
      "title": "Rationalizing neural predictions",
      "year": 2016
    },
    {
      "authors": [
        "Lertvittayakumjorn",
        "P. Toni 2019] Lertvittayakumjorn",
        "F. Toni"
      ],
      "title": "Human-grounded evaluations of explanation methods for text classification",
      "venue": "In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the",
      "year": 2019
    },
    {
      "authors": [
        "Li"
      ],
      "title": "Tell me where to look: Guided attention inference",
      "year": 2018
    },
    {
      "authors": [
        "Zarei Mohseni",
        "S. Ragan 2019] Mohseni",
        "N. Zarei",
        "E.D. Ragan"
      ],
      "title": "A multidisciplinary survey and framework for design and evaluation of explainable ai systems. arXiv preprint arXiv:1811.11839",
      "year": 2019
    },
    {
      "authors": [
        "Nourani"
      ],
      "title": "The effects of meaningful and meaningless explanations on trust and perceived system accuracy in intelligent systems",
      "venue": "In Proceedings of the AAAI Conference on Human Computation and Crowdsourcing,",
      "year": 2019
    },
    {
      "authors": [
        "Arras Osman",
        "A. Samek 2020] Osman",
        "L. Arras",
        "W. Samek"
      ],
      "title": "Towards ground truth evaluation of visual explanations",
      "venue": "arXiv preprint arXiv:2003.07258",
      "year": 2020
    },
    {
      "authors": [
        "Pang",
        "B. Lee 2004] Pang",
        "L. Lee"
      ],
      "title": "A sentimental education: Sentiment analysis using subjectivity summarization based on minimum cuts",
      "venue": "In Proceedings of the 42nd annual meeting on Association for Computational Linguistics,",
      "year": 2004
    },
    {
      "authors": [
        "Englebienne Papenmeier",
        "A. Seifert 2019] Papenmeier",
        "G. Englebienne",
        "C. Seifert"
      ],
      "title": "How model accuracy and explanation fidelity influence user trust",
      "venue": "IJCAI Workshop on Explainable Artificial Intelligence",
      "year": 2019
    },
    {
      "authors": [
        "Poursabzi-Sangdeh"
      ],
      "title": "Manipulating and measuring model interpretability",
      "venue": "arXiv preprint arXiv:1802.07810",
      "year": 2018
    },
    {
      "authors": [
        "Singh Ribeiro",
        "M.T. Guestrin 2016] Ribeiro",
        "S. Singh",
        "C. Guestrin"
      ],
      "title": "Why should i trust you?: Explaining the predictions of any classifier",
      "venue": "In KDD. ACM",
      "year": 2016
    },
    {
      "authors": [
        "Schmidt",
        "P. Biessmann 2019] Schmidt",
        "F. Biessmann"
      ],
      "title": "Quantifying interpretability and trust in machine learning systems",
      "venue": "arXiv preprint arXiv:1901.08558",
      "year": 2019
    },
    {
      "authors": [
        "Schneider"
      ],
      "title": "Deceptive ai explanations: Creation and detection",
      "venue": "arXiv preprint arXiv:2001.07641",
      "year": 2020
    },
    {
      "authors": [
        "Selvaraju"
      ],
      "title": "Gradcam: Visual explanations from deep networks via gradientbased localization",
      "venue": "IEEE International Conference on Computer Vision (ICCV),",
      "year": 2017
    },
    {
      "authors": [
        "Simonyan",
        "K. Zisserman 2014] Simonyan",
        "A. Zisserman"
      ],
      "title": "Very deep convolutional networks for largescale image recognition",
      "venue": "arXiv preprint arXiv:1409.1556",
      "year": 2014
    },
    {
      "authors": [
        "Wu"
      ],
      "title": "Beyond sparsity: Tree regularization of deep models for interpretability",
      "venue": "In Thirty-Second AAAI Conference on Artificial Intelligence",
      "year": 2018
    },
    {
      "authors": [
        "Eisner Zaidan",
        "O. Piatko 2007] Zaidan",
        "J. Eisner",
        "C. Piatko"
      ],
      "title": "Using annotator rationales to improve machine learning for text categorization. In Human language technologies 2007: The conference of the North American chapter of the association for computational linguistics",
      "year": 2007
    },
    {
      "authors": [
        "Zhang"
      ],
      "title": "Dissonance between human and machine understanding",
      "venue": "Proceedings of the ACM on HumanComputer Interaction 3(CSCW):1\u201323",
      "year": 2019
    }
  ],
  "sections": [
    {
      "heading": "Introduction",
      "text": "With the recent and continuing advancements in robust deep neural networks (DNN), the prominence of machine learning techniques for automated decision-making is growing. In such cases, human experts, operators, and decisionmakers can also take advantage of advanced machine learning techniques to account for latent features and assist in taking real-world actions. However, because of the disparity between the sense-making process in humans and the computational feature learning of machine learning models, people require model transparency to be able to understand and trust machine learning models. Thus, for more effective human-AI collaboration, advancements in model explainability are needed to support human understanding. This is the primary goal of recent interdisciplinary research thrusts in Explainable Artificial Intelligence (XAI). While a multifaceted topic, the ultimate goal is for people to understand machine models, and it is therefore essential to involve human feedback and reasoning as a requisite component for design and evaluation of XAI systems (Mohseni, Zarei, and Ragan 2019).\nCopyright c\u00a9 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nResearch on interpretable algorithms has recently proposed various techniques to design inherently interpretable models (Wu et al. 2018) and generate explanations for black-box models (Selvaraju et al. 2017). Interpretability techniques enable human review of model reasoning and learning representations for their correctness in accordance to design goals, law and regulations, and safety requirements. Such evaluations could potentially prevent adverse outcomes of AI-based systems\u2014such as unfair and discriminatory decision-making when performing real-world tasks. However, with the complexity of interpretability techniques and human cognitive biases, the question remains: how should we choose effective and efficient methods for the evaluation of machine learning explanations?\nDifferent approaches have been proposed for evaluating interpretable models and XAI systems at different stages of system design (Doshi-Velez and Kim 2017). In machine learning research, various computational methods are used to measure the fidelity of interpretability techniques with respect to the underlying black-box model (Adebayo et al. 2018; Hooker et al. 2019). On the other hand, in the field of human-computer interaction, human-grounded evaluation approaches measure human factors such as user satisfaction, mental model, and trust in XAI systems designed for different tasks. However, there are fundamental differences between these evaluation approaches. Computational methods set a precedent to objectively evaluate the model against a baseline ground truth, yet they lack the ability to quantify human interpretations. On the other hand, while more descriptive in nature, human subject studies tend to be more costly, imprecise, and subjective to the task. Another major difference between these evaluation methods is that once the human user is exposed to the evaluation study setup, she can not unlearn the experience for another round of evaluation. These differences raise the need to study the tradeoff between objective ground-truth evaluation and subjective human-judgment of explanations.\nIn this paper, we propose a human-attention baseline to quantitatively evaluate model saliency explanations. Our publicly available 1 human-grounded benchmark enables\n1https://github.com/SinaMohseni/ML-InterpretabilityEvaluation-Benchmark\nar X\niv :1\n80 1.\n05 07\n5v 2\n[ cs\n.H C\n] 2\n8 Ju\nn 20\nfast, replicable, and objective execution of evaluation experiments for saliency explanations. To foster the interest of the machine learning community, we demonstrate our benchmark\u2019s utility for quantitative evaluation of model explanations and compare it with the single-layer feature mask ground truth and human judgment rating evaluations. Our study results reveal the efficiency of threshold-agnostic evaluation with a human-attention baseline as compared to previous methods with binary ground truth masks and labels. Our experiments also reveal user biases between different model error types in the subjective rating of explanations."
    },
    {
      "heading": "Background",
      "text": "The evaluation of model explanations and interpretability techniques can be categorized in different ways (DoshiVelez and Kim 2017; Mohseni, Zarei, and Ragan 2019). For instance, previous works have examined the fidelity of interpretability techniques to the black-box model (Hooker et al. 2019; Adebayo et al. 2018), evaluated correctness of model explanations with ground-truth (Du et al. 2018), as well as the usefulness of explanations in different tasks and domains (Kocielnik, Amershi, and Bennett 2019). In this paper, we focus on the trustworthiness of explanations with the assumption of having a high-fidelity ad-hoc explainer. In this section, we review two evaluation approaches, human judgment evaluation and ground-truth evaluation, for the trustworthiness of machine learning explanations and assess their advantages and limitations."
    },
    {
      "heading": "Human Judgment Evaluation",
      "text": "A common approach for evaluating machine learning explanations is the direct review of model explanations with endusers for their subjective feedback. Multiple papers have reported measurements of users\u2019 understanding of explanations as a proxy for usefulness and interpretability of explanations (Lage et al. 2019; Poursabzi-Sangdeh et al. 2018). Others have measured user-reported trust as a proxy for explanation goodness. For example, Nourani et al. (2019) and Papenmeier et al. (2019) studied the effects of explanation meaningfulness and ad-hoc explainer fidelity on user reliance. Both studies show that model accuracy and explanation fidelity impact users\u2019 trust in the model and conclude that providing nonsensical explanations (i.e., those that do not align with users\u2019 expectations) may harm users\u2019 reported trust and observed reliance on the system. With a crowdsourced evaluation approach, Schmidt and Biessmann (2019) present quantitative measures for system interpretability and human trust. They propose that analyzing user interaction time can serve as a proxy for users\u2019 understanding of the explanation and level of trust. They recommend that model explanations need to enhance the information transfer rate to users, help users establish an intuitive understanding of system performance and perform well independent from the user task.\nTaking a different perspective, Schneider et al. (2020) inspected the effects of deceptive model explanations in a user study. Their findings indicate that explanations that are unfaithful to the black-box model can fool users in accepting\nwrong predictions. Following a similar goal, Lakkaragu et al. (2019) present an approach to generate misleading explanations and a case study with law and criminal justice domain experts. Their study results found that misleading explanations were able to significantly increase users\u2019 trust. Conclusively, various research efforts have shown the limitations of human judgment for robust evaluation of machine learning explanations.\nDifferent projects have run user studies to evaluate the human understanding of saliency map explanations from DNNs. For example, Alqaraawi et al. (2020) showed that instance explanations carry new information to users, but model behavior remained largely unpredictable for participants. In other work, Zhang et al. (2019) compared saliency explanations from multiple networks with human explanations of objects in images. They performed a large crowdsourced study to directly compare machine learning and human explanations and human feedback on model explanations. Their results indicate that the features learned by some DNN models are more similar to human intuition. However, it is not clear from their study whether the model generalizability or the choice of interpretability technique was more effective on user satisfaction of explanations. To address the limitations in human judgment evaluation studies, Lertvittayakumjorn and Toni (2019) defined a set of objective evaluation tasks for quantitative evaluation of model explanations with respect to different explanatory purposes. They used three human-grounded tasks to evaluate local explanation methods for their ability to reveal model behavior, justify model predictions, and help users investigate uncertain predictions. The review of previous research indicates that the dissonance between machine learning models\u2019 goal to learn discriminant features and human expectation of logical and common sense explanations undermines the correctness and completeness of human judgment evaluation methods."
    },
    {
      "heading": "Evaluation with Ground Truth",
      "text": "An objective way to quantify the correctness of model explanation is to examine it against a ground truth baseline. Ground truth is often defined by human annotation of representative features (i.e., feature masks) and provide a baseline for quantitative evaluation of explanations quality. Examples include annotations of the object\u2019s \u201csegmentation mask\u201d in natural datasets, e.g., (Everingham et al. 2015), and synthesized datasets, e.g., (Osman, Arras, and Samek 2020), that represent specific features associated with the target class. Different similarity metrics, such as Intersection over Union (IoU) (also called Jaccard index) and mean Average Precision (mAP), are used to quantify the quality of model saliency explanations or bounding boxes compared to the ground truth. For instance, Li et al. (2018) used IoU, between the model saliency map from a Convolutional Neural Network (CNN) and the ground truth segmentation mask from the validation set, to measure their quality as a weaklysupervised semantic segmentation task. In another work, Du et al. (2018) calculate the mAP between the bounding boxes of an objects\u2019 saliency mask and the ground truth bounding boxes to evaluate their interpretability technique as an\nobject localization task. Similarly, in the text domain, direct comparison of model attention explanations with human annotated sentences, e.g., evidence supporting the target label (Zaidan, Eisner, and Piatko 2007), provides an explanation quality score (Lei, Barzilay, and Jaakkola 2016). However, the relationship between the evaluation of machine learning explanations and the auxiliary tasks, such as binary object localization and semantic segmentation, is not clear yet. Our research highlights the aspects of human feedback that are missing in the human annotation baselines but would complement the evaluation of machine learning explanations.\nIn a review of limitations in threshold-based evaluations for model saliency map, Choe et al. (2020) present an evaluation protocol to include a hyperparameter search for the \u03c4 threshold for generating objects\u2019 \u201cbinary mask\u201d from the saliency score map. However, unlike our proposed evaluation protocol, they do not consider the pixel-wise evaluation of saliency score maps in the first place. Aside from object segmentation mask baselines that annotate entire features associated with the target class, perhaps closest work to our human attention benchmark is Das et al.\u2019s (2017) VQA-HAT baseline for evaluating saliency maps in visual question and answering models. They test multiple game-inspired, attention annotation methods to ask participants to sharpen regions of a blurred image to answer a question. The resulting baseline is a human attention map that enables object identification but does not indicate whether the necessary or sufficient features are annotated by individual participants. In comparison to VQA-HAT, our benchmark assembles annotations from multiple unique participants to generate accurate and generalizable human-attention maps. In this paper, we present a series of evaluation experiments and argue that our proposed multi-layer human-attention baseline is able to evaluate the completeness (i.e., the existence of falsenegative explanation errors) and correctness (i.e., the existence of false-positive explanation errors) of model saliency maps."
    },
    {
      "heading": "Human-Attention Benchmark",
      "text": "We captured the human annotation of salient features in order to create a human-grounded benchmark to evaluate model explanations. Participants were prompted to select relevant regions in images and phrases in text documents that they felt most representative of the target subject or topic, respectively. Figure 1 show examples from the resulting multi-layer ground truth from aggregating annotation from multiple unique annotators for each image. In comparison to the single-layer object\u2019s segmentation map, the human-attention benchmark allows for a higher level of granularity in the evaluation of saliency maps and reflects human attention to features. Also, compared to human judgment rating evaluations, the human attention benchmark enables reproducible and cost-efficient evaluation. The following reviews the details of benchmark specification, annotation procedure, and data processing."
    },
    {
      "heading": "Benchmark Specifications",
      "text": "The benchmark presents multi-layer masks representing what features humans expect to be the most important representations of a particular class. For each sample, we collect annotations from 10 unique annotators from Amazon Mechanical Turk platform that were instructed to select areas (in images) or words (in documents) that they deem most relevant to the target class. The multi-layer mask generated by aggregating annotations for each individual sample provides more granular representation of attributed features compared to the single-layer mask. Note that our method\u2014 collecting multiple user annotations for human-attention masks\u2014balances the trade-off between objective annotation of precise feature-masks (i.e., segmentation mask) and subjective human judgment of the representative features. Also, it is important to mention that this human-attention baseline evaluates the explanations\u2019 correctness or trustworthiness of saliency explanations and does not intend to measure the fidelity of ad-hoc interpretability techniques to the black box models.\nThe development of this benchmark consists of a validation subset from ImageNet (Deng et al. 2009) and PASCAL VOC2012 (Everingham et al. 2015) image datasets and 20 Newsgroup (Lang 1995) and IMDB (Pang and Lee 2004) text datasets. Table 1 presents details for the number of classes and annotated samples from the four datasets in our explanation evaluation benchmark. For the PASCAL VOC dataset, 50 randomly selected samples from all 20 classes are annotated including Vehicles (airplane, bicycle, boat, boat, bus, car, motorbike, train), Households (bottle, chair, dining table, potted plant, sofa, TV/monitor), and Animals (bird, cat, cow, dog, horse, sheep) and other (person). To create a validation set from the ImageNet dataset, we randomly selected five images from 20 classes including living things (man, woman, cat, dog, bird, ant, elephant, shark, zebra, flower, tree), indoor objects (chair, computer, ball, book, phone), outdoor objects (car, ship, airplane, house). The set includes images covering broad considerations such as multi-object and complex scenes, co-occurrence of target object, target object in different scales, and lighting conditions.\nFor the text domain datasets, 100 randomly selected movie reviews from each positive and negative classes of IMDB dataset are selected. Similarly, 100 randomly selected text documents (with the headers removed from samples) from the 20 Newsgroup dataset are selected from two categories of medical (sci.med) and electronic (sci.elect).\nAnnotation Interface and Procedure In order to generate multi-layer human-attention explanations, we ask annotators to provide their interpretations of the salient features that are most meaningful for the specific class from the data set. Each sample is annotated with 10 unique annotators recruited from Amazon Mechanical Turk (AMT). Recruitment advertisement for Human Intelligence Task (HIT) required participants to have at least 1000 previously approved HITs in AMT platform with the HIT approval rate of above 95%. Recruited participants were walked through a training slideshow of the task instructions and interface controls at the beginning of their HIT. As a control, each training slide was displayed on screen for two seconds before participants were able to continue to the next slide. Afterward, they were asked to agree to the IRB approved information sheet for data collection, and continued to a set of 12 images or documents for annotation. Participants were paid $0.40 for the image and text annotation HITs to reach an average hourly pay rate of $10 an hour.\nWe designed two fundamentally similar human annotation interfaces to capture human feedback for all image and text datasets. Annotators were using an interface with basic annotation tools in which each document or image was presented individually. Each annotation HIT started with the same two samples to serve as attention check and help the annotator to get adjusted with the interface and task. These are then followed by 10 samples from the main validation set. Task instructions prompted participants to select relevant regions in images that they felt most representative to the target object that could be entire or parts of it but generally not the background scenery. For image annotations, the annotators were specifically asked to use their mouse to lasso \u201csalient area(s) that explain target \u201cobject\u201d in the image\u201d. Similarly, for text annotations, participants were\nprompted to select relevant words in text documents that they felt most representative of the target topic or class. For example, for the movie review IMDB dataset, the annotators were explicitly asked to \u201cselect words and phrases which explain the positive or negative sentiment of the movie review\u201d."
    },
    {
      "heading": "Data Processing and Storage",
      "text": "In order to generate multi-layer feature masks from multiple user annotations, we run a union operation on all individual annotation that displays what areas are most frequently selected by the annotators. Figure 1 presents examples of resulting human-attention masks from different images. Although specified in annotation task instructions, we also applied the exact segmentation mask of the target object\u2019s true pixels (only for image datasets) to remove the impact of participants\u2019 imprecision or hand jitter that might have included the background pixels. The exact segmentation masks for images are created by two authors and included in the benchmark. Human attention masks for image datasets are stored in the format of grayscale masks the same size as original images. The human attention masks for text datasets are JSON objects with lists of index-word pairs with humanattention scores in the range of 0 to 1.0. We did not perform any feature filtering for text annotation samples. The benchmark is stored in a public domain and free for research use."
    },
    {
      "heading": "Evaluation Experiments",
      "text": "In this section, we present multiple evaluation experiments to validate the proposed benchmark with empirical results. These experiments compare three baselines: 1) humanattention mask (our approach) as the ground truth, 2) segmentation mask as the ground truth, and 3) human-judgment\nrating for evaluating model saliency explanations. Our goal is to understand the relationship between the three evaluation methods and communicate the benefits of the proposed benchmark over other common evaluation methods in the literature. The series of experiments are based on saliency maps generated by the Grad-CAM (Selvaraju et al. 2017) technique for a VGG-19 (Simonyan and Zisserman 2014) image classifier on a subset of 100 validation samples from the two classes of cat and dog in PASCAL VOC dataset. The VGG network is pre-trained on ImageNet-1k 2 and tuned on PASCAL VOC 2007 for the purpose of this evaluation. All evaluation scores are based on pixel-wise Mean Absolute Error (MAE) between model saliency score map and the ground truth baseline.\nThe saliency map error is calculated as the pixel-wise Mean Absolute Error (MAE) between model saliency score map and the ground truth mask. We also looked into False Positive (FP) and False Negative (FN) saliency explanation errors individually. We calculate FP saliency error as pixelwise MAE for the model saliency map scores outside the object\u2019s segmentation mask (i.e., error in background pixels) and FN error as the pixel-wise MAE for model saliency map scores inside the ground truth mask (i.e., error in target pixels). In the following subsections, we review details and share evaluation results from the three methods."
    },
    {
      "heading": "Comparison to Segmentation Mask",
      "text": "In the first evaluation experiment, we compare our proposed human-attention benchmark (multi-layer feature mask) with the segmentation mask (single-layer feature mask) as the evaluation ground truth for the set of saliency maps from\n2https://pytorch.org/docs/master/torchvision/models.html\nGrad-CAM technique. Given the lack of granularity for distinguishing important features in the segmentation mask, we hypothesize that the two baselines would result in different evaluation scores for the same set of inputs.\nIntuitively, the difference between the two baselines is that unlike the segmentation mask, which scores all target features equally, the human-attention mask gradiates the \u201csalient\u201d features more than others. To identify the difference between two evaluation baselines, we calculate evaluation scores using both baselines for direct comparison. Specifically, we first normalize both ground truth masks and model saliency maps and then calculate the pixel-wise MAE error between model saliency map and the ground truth baseline. For example, a saliency map identical to its human attention mask results in zero MAE error. In the opposite situation, with cases having no overlap between the ground truth mask and the model saliency map, the MAE error would be 1.0. Note that MAE is a threshold agnostic metric that\u2014unlike Intersection over Union (IoU)\u2014does not require choosing the \u03c4 hyperparameter for generating objects\u2019 binary masks or bounding boxes, see (Choe et al. 2020) for more discussion. Also, evaluating the saliency score map (without converting to a binary mask) retains the granular information in the model explanation.\nResults Figure 2-(a) shows the scatter plot of evaluation scores (1.0 \u2212MAE) between human-attention and segmentation mask baselines. The two evaluation scores are statistically significantly (r = 0.896 , p < 0.001) correlated, as expected. Using a linear regression test, we find a regression slope of w = 0.896 and intercept of b = 0.48. As seen in Figure 2-(a), this weight and bias result in different evaluation scores between the two ground truth, especially in the\nhigher and lower range of scores. To examine the statistical significance of the difference between two ground truth evaluations, we use an ANCOVA test with a custom model to the test for homogeneity of regression slopes between the calculated regression model and the ideal of slope 1.0 with a zero intercept. The test for homogeneity of regression slopes fails with a significant difference (p < 0.001) between the two lines indicating that the two evaluation baselines are not equal. We also look into FP and FN saliency explanation errors individually. The results show that the difference between the two baselines is only due to FN errors being treated differently between the two baselines. This was expected since both baselines measure zero evaluation score for the saliency explanations outside the ground truth mask."
    },
    {
      "heading": "Comparison to Human Judgment",
      "text": "In the second evaluation experiment, we compare explanation evaluation scores using the two ground truth baselines with the human ratings of explanation goodness. Subjective human ratings of the model explanations are commonly used as a direct approach for evaluating machine learning explanations by providing a numerical rating of explanations goodness using a simple quantitative measure such Likert scales. However, subjective measures typically lack precision and may include user bias. We hypothesize that results from human-judgment scores will be significantly different for both (human-attention mask and object segmentation mask) ground truth evaluations. We use the same subset of images and saliency map explanations from Grad-CAM technique similar to the previous section for the purposes of this human-subjects study. Figure 3-(Top) shows examples of heatmap overlays from the Grad-CAM technique used in the user study.\nHuman Judgment Interface and Data Collection We designed a simple interface to collect user feedback about the quality of heatmap overlays from the Grad-CAM saliency explanation technique. The participants started by reading task instructions followed by a series of images for review and rate. Given an image from the test set, the target classification, and a heatmap overlay, participants were instructed to \u201creview and rate the heatmaps which explain what parts the AI used to make it\u2019s classification decision\u201d and were asked to rate the \u201cgoodness\u201d of the AI decision on the scale of 1 to 10. A total of 200 unique participants\u2019 were recruited from Amazon Mechanical Turk and paid $0.20 per HIT to review and rate 14 images. The first four image ratings (identical images were used for all participants) were used as training and attention check examples; these were disregarded for data collection.\nResults: Figure 2-b shows a scatterplot of the evaluation scores (1.0 \u2212 MAE) between human judgment ratings and ground truth scores from objects\u2019 segmentation masks and human attention masks. The two regression lines for humanattention ground truth (in orange) and segmentation mask (in blue) show both baselines produce different evaluation scores from the user rating scores. To test for the statistical significance of observed differences, we first normalize user ratings across participants by subtracting each participant\u2019s\nmean rating. Then, we use a Pearson\u2019s correlation test and linear regression test to compare the human judgment rating scores and the two ground truth scores. The user ratings show a moderate-strength correlation with object segmentation mask baseline (r = \u22120.121, p = 0.002) and small correlation with human-attention mask baseline (r = \u22120.306, p < 0.001). We also observe signs of user bias, noting that none of the participants rated any of the saliency map instances in the test set below 3-stars even though there are multiple examples with scores below 0.3 for both ground truth evaluation types. These cases were specifically from the examples with multiple occurrences of the target object in which the saliency map was only pointing to one of the target objects. This could potentially indicate a side effect of lower user attention in reviewing cases with incomplete saliency explanations.\nTo compare measurements between evaluation approaches, we run a linear regression analysis and find that the segmentation mask scores fit with a slope of w = 0.313 and intercept of b = 0.268 (Figure 2-b, blue trend line), and the fit for human-attention mask scores has a slope of w = 0.428 and intercept of b = 0.210 (Figure 2-b, orange trend line). Note that the difference between the two linear regression models\u2019 slopes with the ideal slope of 1.0 is higher with the segmentation-mask baseline. To examine the statistical significance difference between the measures, we use ANCOVA with a custom model to test for homogeneity of the regression slopes between the two regression models as well as between the calculated regression model and the ideal of slope 1.0 with zero bias. The homogeneity test fails with a significant difference of p < 0.001 between the two regression models and the ideal line. The analysis indicates the subjective measurement of explanations goodness produces significantly different results from both objective ground truth measures."
    },
    {
      "heading": "Discussion",
      "text": "In this section, we review and discuss the evaluation experiments and open problems around model explanation evaluation. The evaluation experiment results showed that the human-attention benchmark has allowed for a higher level of granularity in the evaluation of saliency maps and reflected human attention to the features in comparison to the singlelayer object\u2019s segmentation map. As compared to the human judgment rating evaluations, we observed signs of participants\u2019 bias in their ratings.\nImplications of Results We ran human-subject experiments to understand the differences between the subjective and objective evaluation of saliency explanations. Although the evaluation results from the three methods had positive correlations, the experimental results showed significant differences among all evaluation scores. The difference in scores was mainly due to the clear non-uniform distribution of weights in human attention masks while the segmentation mask weights are uniformly distributed for all features (e.g., pixels, words).\nWhile segmentation mask benchmarks are mainly used for object segmentation evaluation and weakly supervised object localization (Li et al. 2018; Choe et al. 2020), the human-attention baseline reflects human factors in feature attribution. For example, in annotations of living things, users were more likely to select facial features as important features while the segmentation mask offers a uniformly weighted single-layer mask. This is reflected in the evaluation results with human judgment with participants\u2019 ratings of explanations being closer to the human-attention baseline\nrather than the segmentation mask baseline. Due to the same effect, evaluation results with the human-attention baseline could be extended to better anticipate user acceptance and trust in model explanations when putting on different applications.\nHuman Biases in Rating We explored the human judgment evaluation results to find other possible external or internal factors that could affect participants\u2019 subjective ratings. For example, human judgment ratings may include user biases toward visual appearance or completeness of saliency maps resulting in incorrect ratings. We reviewed and compared the results from human judgment for GradCAM and LIME explanations to identify possible biases. Also, we reviewed the results to assess possible participants\u2019 biases toward model explanation FP and FN error types.\nTo evaluate the effect of visual appearance of saliency explanations, we compare participants\u2019 rating of saliency map explanations from LIME (Ribeiro, Singh, and Guestrin 2016) technique to Grad-CAM explanations on the same subset of images and the same classifier. The saliency explanations from the LIME technique (Figure 3-(Bottom)) are visually more chunky and pixelated (mainly due to use of super pixels in LIME\u2019s pipeline) compared to smooth concept activation maps from Grad-CAM technique (Figure 3- (Top)). We analyze results after running a new user study to collect participants\u2019 subjective ratings of LIME explanations.\nWe used two linear regression models to compare participants\u2019 ratings between the two groups, see 4-(a). We find the slope of w = \u22120.428 and intercept of b = 0.789 for\nthe user ratings on samples with LIME saliency map (Figure 4-(a) green trend line) and slope of w = \u22120.607 and intercept of b = 0.947 for samples with Grad-CAM saliency map (Figure 4-(a), yellow trend line). We would have expected to see the similar regression slopes between the two groups if the users were evaluating both saliency map explanation types similarly. However, the test for homogeneity between the two regression slopes shows a significant difference (p < 0.001) between the two model error types. This indicates that users rated the saliency maps differently, although ground truth evaluation score (Figure 4-(a), y axis) for both sets of samples.\nWe then analyze participants\u2019 rating behavior with respect to different explanation error types. We first divided the samples for the test set into two groups with high FP (when the model is looking at background pixels) explanation error and high FN explanation errors (when the model is missing foreground pixels). Using linear regression models, we find the slope of w = \u22120.121 and intercept of b = 0.265 for the samples with FP explanation error score (Figure 4-(b) yellow trend line) and slope of w = \u22120.306 and intercept of b = 0.525 for samples with high FN explanations error score (Figure 4-(b), green trend line). We would have expected to see the similar regression slopes between the two groups if the users were evaluating both saliency error types similarly. However, the test for homogeneity between the two regression slopes shows a significant difference (p < 0.001) between the two explanation error types. This indicates that users pay less attention to FP explanation errors and in turn, are more critical for FN explanation errors. Looking at image samples from the user study, these images included several examples in which the target object was on a smaller scale and the model saliency map was largely exceeded to the background pixels.\nReproducibility and Objectivity Trade-off One way to categorize different evaluation measures is by their objectivity and reproducibility of results. As implemented in this paper, users\u2019 subjective rating of explanations could collect results for correctness and goodness of model generated explanations. Ribeiro et al. (2016) presented a case for correction of model explanation in which users reject wrong features and add new features for quantitative evaluation of model explanations. A different method is to collect user feedback through the direct comparison of explanations from multiple interpretability techniques. For example, users could review several options to choose the best machine-generated explanation and provide justifications for their choices. However, although these methods can provide detailed insights, subjective user feedback is not reusable for new models and interpretability techniques. This limitation indeed exists in studies for evaluating XAI systems in different applications and domains (Doshi-Velez and Kim 2017), including tasks and scenarios concerned with the fairness of the decisionmaking system.\nOn the other hand, objective evaluation that utilizes ground truth, provides quantitative and reproducible results, yet lacks the guidance of human correctness and goodness scores that show which improvements would be most sig-\nnificant. Our benchmarks bridge the trade-off between objectivity and subjectivity of a baseline to satisfy both evaluation aspects.\nLimitations and Future Work A limitation of creating this human-attention benchmark is the annotation cost for multi-level human explanation masks. However, annotation cost could be justified when compared to repeated novel rounds of evaluations for subjective human judgments. As typically, the iterative design and evaluation of machine learning based systems require multiple rounds of training and test. Our human attention benchmark can significantly reduce evaluation costs over design cycles. Further, an open question in creating human-attention benchmarks is how to standardize all annotators\u2019 perception of explanation when performing the annotation task. In our future work, we plan to study annotators behavior on objects in different size and pose to learn general patterns in human attention. This could potentially help to optimize the number of annotators for each sample. Lastly, we are interested in examining the use case of the human-attention benchmark for tuning models to improve prediction rationale and its effects on explanation quality."
    },
    {
      "heading": "Conclusion",
      "text": "We present a new model-explanation evaluation benchmark for multiple datasets in image and text domains. Our benchmark is designed for quantitative evaluation of saliency map explanations based on human attention on features. This human-grounded benchmark enables fast, replicable, and objective execution of evaluation experiments for saliency explanations. We studied the relationships and trade-offs between two different human-grounded evaluation approaches (i.e., single-layer annotation mask and human subjective feedback) to present the efficiency of the proposed human attention baseline. Our study results indicated the difference between threshold-agnostic evaluation with a humanattention baseline as compared to previous methods with binary ground truth masks and labels. Our experiments also revealed user biases on different explanations\u2019 visual appearance and error types in the subjective rating of explanations."
    },
    {
      "heading": "Acknowledgements",
      "text": "This research is based on work supported by the DARPA XAI program under Grant #N66001-17-2-4031 and NSF award #1900767."
    }
  ],
  "title": "Quantitative Evaluation of Machine Learning Explanations: A Human-Grounded Benchmark",
  "year": 2020
}
