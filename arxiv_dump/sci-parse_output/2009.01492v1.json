{
  "abstractText": "The widespread use of modern machine learning methods in decision making crucially depends on their explainability. Human users of machine learning methods are often not only interested in getting accurate predictions or projections. Rather, as a decision maker, the user also needs a convincing answer (or explanation) to the question of why a particular prediction was delivered. Explainable machine learning becomes a legal requirement when used for decision making with an immediate effect on the health of human beings. We have recently proposed an information-theoretic approach to construct personalized explanations for predictions obtained from ML. This method was model-agnostic and only required some training samples of the model to be explained along with a user feedback signal. This paper uses an information-theoretic measure for the quality of an explanation to learn predictors that are intrinsically explainable to a specific user. Our approach is not restricted to a particular hypothesis space, such as linear maps or shallow decision trees, whose predictor maps are considered as explainable by definition. Rather, we regularize an arbitrary hypothesis space using a personalized measure for the explainability of a particular predictor.",
  "authors": [
    {
      "affiliations": [],
      "name": "Alexander Jung"
    }
  ],
  "id": "SP:fe74b59d3a729bd4b11e4e195f91a9201be6d338",
  "references": [
    {
      "authors": [
        "S. Bach",
        "A. Binder",
        "G. Montavon",
        "F. Klauschen",
        "K.-R. M\u00fcller",
        "W. Samek"
      ],
      "title": "On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation",
      "venue": "PLOS ONE, 10(7):1\u201346,",
      "year": 2015
    },
    {
      "authors": [
        "C.M. Bishop"
      ],
      "title": "Pattern Recognition and Machine Learning",
      "venue": "Springer",
      "year": 2006
    },
    {
      "authors": [
        "S. Boyd",
        "L. Vandenberghe"
      ],
      "title": "Convex Optimization",
      "venue": "Cambridge Univ. Press, Cambridge, UK",
      "year": 2004
    },
    {
      "authors": [
        "P. Cairney",
        "R. Kwiatkowski"
      ],
      "title": "How to communicate effectively with policymakers: combine insights from psychology and policy studies",
      "venue": "Palgrave Communications,",
      "year": 2017
    },
    {
      "authors": [
        "T.M. Cover",
        "J.A. Thomas"
      ],
      "title": "Elements of Information Theory",
      "venue": "Wiley, New Jersey, 2 edition",
      "year": 2006
    },
    {
      "authors": [
        "H. Hagras"
      ],
      "title": "Toward human-understandable, explainable",
      "venue": "ai. Computer,",
      "year": 2018
    },
    {
      "authors": [
        "T. Hastie",
        "R. Tibshirani",
        "J. Friedman"
      ],
      "title": "The Elements of Statistical Learning",
      "venue": "Springer Series in Statistics. Springer, New York, NY, USA",
      "year": 2001
    },
    {
      "authors": [
        "A. Holzinger"
      ],
      "title": "Explainable AI (ex-AI)",
      "venue": "Informatik Spektrum,",
      "year": 2018
    },
    {
      "authors": [
        "K. Jeong",
        "J. Choi",
        "G. Jang"
      ],
      "title": "Semi-local structure patterns for robust face detection",
      "venue": "IEEE Sig. Proc. Letters, 22(9)",
      "year": 2015
    },
    {
      "authors": [
        "A. Jung"
      ],
      "title": "Components of machine learning: Binding bits and flops",
      "venue": "arXiv preprint https://arxiv.org/pdf/1910.12387.pdf",
      "year": 2019
    },
    {
      "authors": [
        "A. Jung"
      ],
      "title": "Machine learning: Basic principles",
      "venue": "arXiv preprint arXiv:1805.05052",
      "year": 2020
    },
    {
      "authors": [
        "A. Jung",
        "P.H.J. Nardelli"
      ],
      "title": "An information-theoretic approach to personalized explainable machine learning",
      "venue": "IEEE Sig. Proc. Lett., 27:825\u2013 829",
      "year": 2020
    },
    {
      "authors": [
        "J. Kagan"
      ],
      "title": "Motives and development",
      "venue": "Journal of Personality and Social Psychology, 22(1):51\u201366",
      "year": 1972
    },
    {
      "authors": [
        "A.W. Kruglanski",
        "D.M. Webster"
      ],
      "title": "Motivated closing of the mind",
      "venue": "Psychol. Rev., 103(2)",
      "year": 1996
    },
    {
      "authors": [
        "B.D. Mittelstadt",
        "P. Allo",
        "M. Taddeo",
        "S. Wachter",
        "L. Floridi"
      ],
      "title": "The ethics of algorithms: Mapping the debate",
      "venue": "Big Data and Society, 3(2)",
      "year": 2016
    },
    {
      "authors": [
        "C. Molnar"
      ],
      "title": "Interpretable Machine Learning - A Guide for Making Black Box Models Explainable",
      "venue": "[online] Available: https://christophm.github.io/interpretable-ml-book/.",
      "year": 2019
    },
    {
      "authors": [
        "G. Montavon",
        "W. Samek",
        "K. M\u00fcller"
      ],
      "title": "Methods for interpreting and understanding deep neural networks",
      "venue": "Digital Signal Processing, 73:1\u201315",
      "year": 2018
    },
    {
      "authors": [
        "M.T. Ribeiro",
        "S. Singh",
        "C. Guestrin"
      ],
      "title": "Why should i trust you?\u201d: Explaining the predictions of any classifier",
      "venue": "In Proc. 22nd ACM SIGKDD,",
      "year": 2016
    },
    {
      "authors": [
        "Axel Gandyet"
      ],
      "title": "Estimating the number of infections and the impact of non-pharmaceutical interventions on covid-19 in 11 european countries",
      "venue": "Technical report, Imperial College London,",
      "year": 2020
    },
    {
      "authors": [
        "S. Wachter",
        "B. Mittelstadt",
        "L. Floridi"
      ],
      "title": "Why a right to explanation of automated decision-making does not exist in the general data protection regulation",
      "venue": "International Data Privacy Law, 7(2):76\u201399",
      "year": 2017
    }
  ],
  "sections": [
    {
      "text": "I. INTRODUCTION\nA key challenge for the wide-spread use of machine learning (ML) methods is the (lack of) explainability of its predictions [6], [8], [15], [20]. The predictions delivered by ML methods are often used to motivate decisions affecting humans.\nAs a point in case, consider the school closures during the Covid-19 pandemic throughout Europe [19]. The decision makers (politicians) often based their decisions on the predictions obtained by fitting models to data, which is exactly what ML methods do. For the policy-makers in European countries, it was customary that decisions on restrictions or school closures have been explained to the public [4].1\nExplanations of predictions, motivating decisions that affect humans, are increasingly becoming a legal obligation [20]. It also seems that humans have a basic need for understanding decision making processes [13], [14].\nMost machine learning (ML) methods are based on a simple principle: fit a predictor map to given training data points [2], [7], [10]. This empirical risk minimization paradigm is also driving deep learning methods []. The various ML methods differ merely in their choice for the hypothesis spaces which collects all feasible predictor maps to choose from and the loss function to measure the quality of a fit.\nWe consider explainable ML methods that not only take into account how good a predictor map fits the training data (empirical risk minimization) but also how comprehensible the predictions are to a specific user.\nAJ is with the Department of Computer Science, Aalto University, Finland. 1You can find the communication of decisions by the Finnish prime minister\nat https://vnk.fi/en/communications-on-decisions.\nExisting approaches to explainable (or interpretable) ML broadly form two categories [16]. The first category are methods that are considered as intrinsically interpretable [1], [6], [17]. A prominent example for this class of explainable methods is linear regression which many consider as interpretable due to the additive combination of weighted features. The weights for each feature are interpreted as a measure for the importance or relevance of that feature.\nThere is no widely accepted formal definition for when to consider a ML method as intrinsically interpretable. A ML method tends to be considered Interpretable if it uses a \"simple\" hypothesis space. Examples for a simple hypothesis space are linear maps or shallow decision trees. What sets our approach apart from previous work on intrinsically explainable ML methods is that we use an information-theoretic measure for the intrinsic explainability of a predictor map.\nA second class of explainable ML methods is based on providing post-hoc (after model training) explanations [18]. These methods are model-agnostic in the sense of only requiring some training examples of the model predictions. We have recently put forward an information-theoretic approach to computing optimal post-hoc explanations for a specific user [12].\nWe build on our previous work [12] and use an informationtheoretic measure for the explainability of a particular predictor. In contrast to [12], we use this measure to directly steer the learning process for finding a good predictor map. This leads naturally to a form of regularized empirical risk minimization [7], [11].\nOur main contribution is the concept of explainable empirical risk minimization (EERM). The idea of EERM is to use the mutual information between prediction and user summary to regularize empirical risk minimization.\nOutline. We introduce the main building blocks of our approach, empirical risk minimization, regularization and information-theoretic measure of explainability in Section II. Our main contribution is the formulation of EERM in Section III."
    },
    {
      "heading": "II. PROBLEM SETUP",
      "text": "We consider ML problems revolving around the prediction of some label value y which represents a higher-level fact about a data point. The prediction is based solely on certain low-level properties or features of the data point. We use a predictor (map)\ng(\u00b7) : Rn \u2192 R : x 7\u2192 y\u0302 = g(x). (1)\nto compute the predicted label y\u0302 = g(x) using the features x = ( x1, . . . , xn )T \u2208 Rn. Any ML method that can only use finite computational resources can only use a subset of (computationally) feasible\nar X\niv :2\n00 9.\n01 49\n2v 1\n[ cs\n.L G\n] 3\nS ep\n2 02\n0\n2 maps. We refer to this subset as the hypothesis space H of a ML method. Examples for such a hypothesis space are linear maps or spaces spanned by polynomials.\nFor a given data point with features x and label y, we measure the quality of a predictor h using some loss function L(g, (x, y)). Formally, a loss function maps a pair (g, (x, y)) consisting of hypothesis g and data point (x, y) to some number L(g, (x, y)).\nThe number L(g, (x, y)) is used as a measure for the error or loss incurred by predicting the label y of a data point using the prediction y\u0302 = g(x). Popular examples for loss functions are the squared error loss L = (y\u0302 \u2212 y)2 (for numeric labels y \u2208 R) or the logistic loss L = log(1 + exp(\u2212h(x)y)) (for binary labels y \u2208 {\u22121, 1})."
    },
    {
      "heading": "A. Empirical Risk Minimization",
      "text": "A widely used principle for learning a predictor map is empirical risk minimization (ERM). The idea of ERM is to learn a predictor map with small training error [2], [7], [10]\nE := (1/|T |) \u2211\n(x,y)\u2208T\nL(g, (x, y)). (2)\nThe training error is computed on a set of labelled data points T \u2286 D = {( x(1), y(1) ) , ( x(2), y(2) ) , . . . , ( x(m), y(m) )} , (3)\nfor which we know the true label values y(i). The set D denotes all available labeled data points.\nThe training error E(g) is the average loss incurred on some labelled (training) data T \u2286 D . The most direct implementation of the ERM principle is to learn a predictor map via solving\ng\u0302 \u2208 arg min g\u2208H E(g). (4)\nIn general we use a strict subset T \u2282 D of all labeled data points to compute the training error (2). The remaining data points D \\ T , which are not used for the training, are then used to validate the predictor learnt via (4) [7], [11]."
    },
    {
      "heading": "B. Regularization",
      "text": "The direct form (4) of ERM is rarely used in practice since it is prone to overfitting. Overfitting happens when the size of the hypothesis space is much larger than the number of training data points. As a rule of thumb, for ERM (4) to work we should have several times more training data points than tunable (learnable) parameters of the hypothesis space. Current deep neural networks involves billions of tunable parameters. However, it is often not feasible to aquire billions of labelled training examples.\nTo avoid overfitting we can use regularization strategies. Loosely speaking, regularization aims at estimating or anticipating how much the average loss would increase on new data which is different from the training data. We then add this estimated increase as an additive (\u201cregularization\u201d) term to the ERM (4),\ng\u0302 \u2208 arg min g\u2208H E(g) + \u03bbR(g). (5)\nThe regularization parameter \u03bb\u22650 in the regularized ERM (5) allows to trade smaller training error (small \u03bb) against weaker regularization. A different implementation of regularization is via adding a constraint to the ERM,\ng\u0302 \u2208 arg min g\u2208H E(g) s.t. R(g) \u2264 \u03b7. (6)\nFor the hypothesis space constituted by linear maps g(x) = wTx, we obtain ridge regression as the special case of (5) with the regularizer R(g) = \u2016w\u20162 = \u2211n i=1 w 2 i . Another special case using linear predictor maps is the least absolute shrinkage and selection operator (Lasso). We obtain the Lasso from (5) for the choice R(h) = \u2016w\u20161 = \u2211n i=1 |wi|.\nSomewhat different to ridge regression and Lasso, our choice for the regularizer R(g) is not directly motivated by robustness or sparsity. Instead, we propose a regularizer that measures, in a sense made precise below, the explainability of the predictions y\u0302 = g(x) to a specific user."
    },
    {
      "heading": "C. Explanations",
      "text": "Assume we have learnt a predictor g\u0302 via regularized ERM (5). We can then apply this predictor to new data points (outside the training data T ) to obtain the prediction y\u0302 = g\u0302(x). The prediction y\u0302 is often delivered to human users with varying background knowledge on ML principles. Indeed, some users might have a university degree in ML while other users might have no formal training in statistics.\nDepending on her background, the user often has some intuition for the relation between features x and label y of a data point. We can think of this intuition as some crude model that has been built up during past experiences. To characterize the user intuition we allow her to provide some \u201cuser summary\u201d u\u0302 \u2208 R for a given data point.\nFor ease of exposition, we assume that the user summary is obtained by a deterministic map\nu(\u00b7) : Rn \u2192 R : x 7\u2192 u\u0302 := u(x). (7)\nHowever, we expect our approach to be readily extended to user summaries obtained via stochastic maps (a conditional probability distribution) p(u\u0302|x). We interpret u\u0302 as a \u201csummary\u201d formed by the user about a data point based on its features x and the user knowledge. In what follows, we formally identify a user with the function u(\u00b7) that maps the features of a given data point to the user summary u\u0302 = u(x)\nLet us illustrate the concept of a user summary by two examples. Consider a user that is able to implement linear regression using a single hand-picked feature of a data point (spreadsheet row in Figure 1). Here, a user summary could be the prediction obtained from a linear regression using few features (see Figure 1). For facial pictures, a user summary u could be a course characterization of the eye-spacing [9] (see Figure 2).\nTo study the explainability of a prediction y\u0302 delivered to a user u(\u00b7), we use a simple probabilistic model (see Figure 3). We interpret the features x and label y of data points as realizations of random variables with joint distribution p(x, y). Our approach does not require knowledge of the\n3\njoint probability distribution. Rather, our approach requires the availability of sufficiently large set D of i.i.d. realizations.\nNote that the prediction y\u0302 = h(x) and user summary u\u0302 = u(x) are random variables as they are obtained by applying maps to the random features x. We tacitly assume the existence of a joint probability density function for the random variables y\u0302 and u(x).\nWe propose to measure the (lack of) explainability of y\u0302 = g\u0302(x) via the conditional (differential) entropy [5, Ch. 2 and 8]\nh(y\u0302|u\u0302) := \u2212E { log p(y\u0302|u\u0302) } . (8)\nThe conditional entropy (8) measures uncertainty about the prediction y\u0302 given the user summary u\u0302 = u(x). Smaller values h(y\u0302;u) correspond to smaller levels of uncertainty in the predictions y\u0302 that the user u experiences.\nWe aim at learning a predictor h \u2208 H such that the user has minimal uncertainty h(y\u0302|u\u0302) about the predictions y\u0302 = h(x) obtained for a random data point. The preference for predictors incurring a small h(y\u0302|u\u0302) serves the basic human need to understand observed phenomena, such as the predictions from a ML method [13]."
    },
    {
      "heading": "III. EXPLAINABLE EMPIRICAL RISK MINIMIZATION",
      "text": "We now combine the parts developed in Section II to learn a predictor that is intrinsically explainable to a given user u.\nThe idea is to use the explainability measure (8) to regularize empirical risk minimization. In particular, we define explainable empirical risk minimization (EERM) as\ng\u0302 \u2208 arg min g\u2208H E(g) s.t. h(y\u0302|u\u0302) \u2264 \u03b7. (9)\nNote that the random variable y\u0302 in the constraint of (9) is obtained by applying the predictor map h \u2208 H to the features, y\u0302 = h(x).\nThe constraint h(y\u0302|u\u0302) \u2264 \u03b7 of EERM (9) enforces the resulting predictor g\u0302 to be sufficiently explainable in the sense that the conditional entropy h(g\u0302|u\u0302) \u2264 \u03b7 does not exceed a prescribed level \u03b7.\nTo gain more insight into the properties of EERM, we consider the hypothesis space of linear maps\ng(w)(x) := wTx with some weight vector w \u2208 Rn. (10)\nMoreover, we assume that the features x of a data point and its user summary u are jointly Gaussian with mean zero and covariance matrix C,(\nxT , u\u0302 )T \u223c N (0,C). (11)\nUnder the assumptions (10) and (11) (see [5, Ch. 8]),\nh(u\u0302|y\u0302) = (1/2) log \u03c32y\u0302|u\u0302. (12)\nHere, we used the conditional variance \u03c32y\u0302|u\u0302 of y\u0302 given the random user summary u = u(x). Inserting (12) into the generic form of EERM (9),\ng\u0302 \u2208 arg min g\u2208H E(h) s.t. log \u03c32y\u0302|u\u0302 \u2264 \u03b7. (13)\nBy the monotonicity of the logarithm, (13) is equivalent to\ng\u0302 \u2208 arg min g\u2208H E(g) s.t. \u03c32y\u0302|u\u0302 \u2264 e \u03b7. (14)\nTo further develop (15), we use the identity\nmin \u03b1\u2208R\nE {( y\u0302 \u2212 \u03b1u )2} = \u03c32y\u0302|u\u0302. (15)\nThe identity (15) relates the conditional variance \u03c32y\u0302|u\u0302 to the minimum mean squared error that can be achieved by estimating y\u0302 using a linear estimator \u03b1u\u0302 with some \u03b1 \u2208 R.\n4 Using (15) and assumption (10), we can formulate (14) as\ng\u0302 \u2208 arg min w\u2208Rn,\u03b1\u2208R\nE(g(w)) s.t. E {( wTx\u2212\u03b1u\u0302 )2} \u2264 e\u03b7. (16)\nNote that the inequality constraint in (16) is convex [3, Ch. 4.2.]. For squared error loss, the objective function E(g(w)) is also convex. Thus, for linear least squares regression, we can reformulate (16) as a dual unconstrained problem [3, Ch. 5]\ng\u0302 \u2208 arg min w\u2208Rn,\u03b1\u2208R\nE(g(w)) + \u03bbE {( wTx\u2212 \u03b1u\u0302 )2} . (17)\nBy convex duality, for a given threshold e\u03b7 in (16), we can find a value for \u03bb in (17) such that (16) and (17) have the same solutions [3, Ch. 5].\nIn order to actually use (17) to learn an explainable predictor g\u0302 we need to replace the expectation E {( wTx \u2212 \u03b1u\u0302 )2} by\na sample average on i.i.d. training data ( x(i), y\u0302(i), u\u0302(i) ) for i = 1, . . . ,m.\nAlgorithm 1 Explainable Linear Least Squares Regression Input: explainability parameter \u03bb, training samples(\nx(i), y\u0302(i), u\u0302(i) )\nfor i = 1, . . . ,m 1: solve\nw\u0302\u2208 arg min \u03b1\u2208R,w\u2208Rn m\u2211 i=1 ( y\u0302(i)\u2212wTx(i) )2\ufe38 \ufe37\ufe37 \ufe38 empirical risk +\u03bb (wTx(i) \u2212 \u03b1u\u0302(i))2\ufe38 \ufe37\ufe37 \ufe38 explainablity\n(18) Output: weights w\u0302 of explainable linear predictor"
    },
    {
      "heading": "IV. CONCLUSION",
      "text": "We have proposed EERM to learn predictors that are explainable in a certain sense. In particular, we regularize empirical risk minimization using an information-theoretic measure for the explainability of the predictions to a given user. Thus, we learn predictors that are intrinsically explainable to individual users. We have spelled out our approach for the special case of linear least squares regression and using a simple probabilistic model for the data and user."
    }
  ],
  "title": "Explainable Empirical Risk Minimization",
  "year": 2020
}
