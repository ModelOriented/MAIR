{"abstractText": "Black-box explanation is the problem of explaining how a machine learning model \u2013 whose internal logic is hidden to the auditor and generally complex \u2013 produces its outcomes. Current approaches for solving this problem include model explanation, outcome explanation as well as model inspection. While these techniques can be beneficial by providing interpretability, they can be used in a negative manner to perform fairwashing, which we define as promoting the false perception that a machine learning model respects some ethical values. In particular, we demonstrate that it is possible to systematically rationalize decisions taken by an unfair black-box model using the model explanation as well as the outcome explanation approaches with a given fairness metric. Our solution, LaundryML, is based on a regularized rule list enumeration algorithm whose objective is to search for fair rule lists approximating an unfair black-box model. We empirically evaluate our rationalization technique on black-box models trained on real-world datasets and show that one can obtain rule lists with high fidelity to the black-box model while being considerably less unfair at the same time.", "authors": [{"affiliations": [], "name": "Ulrich A\u0131\u0308vodji"}, {"affiliations": [], "name": "Hiromi Arai"}, {"affiliations": [], "name": "Olivier Fortineau"}, {"affiliations": [], "name": "S\u00e9bastien Gambs"}, {"affiliations": [], "name": "Satoshi Hara"}, {"affiliations": [], "name": "Alain Tapp"}], "id": "SP:f44b0fab6a18b1b454109d6ff2b0874da8d76908", "references": [{"authors": ["J.A. Adebayo"], "title": "Fairml: Toolbox for diagnosing bias in predictive modeling", "venue": "Master\u2019s thesis, Massachusetts Institute of Technology,", "year": 2016}, {"authors": ["E. Angelino", "N. Larus-Stone", "D. Alabi", "M. Seltzer", "C. Rudin"], "title": "Learning certifiably optimal rule lists for categorical data", "venue": "Journal of Machine Learning Research,", "year": 2018}, {"authors": ["B. Berendt", "S. Preibusch"], "title": "Exploring discrimination: A user-centric evaluation of discrimination-aware data mining", "venue": "In Proceedings of the IEEE 12th International Conference on Data Mining Workshops", "year": 2012}, {"authors": ["R. Berk", "H. Heidari", "S. Jabbari", "M. Kearns", "A. Roth"], "title": "Fairness in criminal justice risk assessments: The state of the art", "venue": "Sociological Methods & Research,", "year": 2018}, {"authors": ["L. Breiman"], "title": "Classification and regression trees. Routledge, 2017", "year": 2017}, {"authors": ["T. Calders", "F. Kamiran", "M. Pechenizkiy"], "title": "Building classifiers with independency constraints", "venue": "In Proceedings of the IEEE International Conference on Data Mining Workshops", "year": 2009}, {"authors": ["M. Craven", "J.W. Shavlik"], "title": "Extracting tree-structured representations of trained networks", "venue": "In Proceedings of the Annual Conference on Neural Information Processing Systems", "year": 1996}, {"authors": ["F. Doshi-Velez", "B. Kim"], "title": "Towards a rigorous science of interpretable machine learning", "venue": "arXiv preprint arXiv:1702.08608,", "year": 2017}, {"authors": ["C. Dwork", "M. Hardt", "T. Pitassi", "O. Reingold", "R. Zemel"], "title": "Fairness through awareness", "venue": "In Proceedings of the 3rd Conference on Innovations in Theoretical Computer Science,", "year": 2012}, {"authors": ["L. Edwards", "M. Veale"], "title": "Slave to the algorithm: why a right to an explanation is probably not the remedy you are looking for", "venue": "Duke L. & Tech. Rev.,", "year": 2017}, {"authors": ["http://archive. ics"], "title": "uci. edu/ml]. irvine, ca: University of california", "venue": "School of information and computer science,", "year": 2010}, {"authors": ["B. Goodman", "S. Flaxman"], "title": "European union regulations on algorithmic decision-making and a \u201cright to explanation", "venue": "AI Magazine,", "year": 2017}, {"authors": ["R. Guidotti", "A. Monreale", "S. Ruggieri", "F. Turini", "F. Giannotti", "D. Pedreschi"], "title": "A survey of methods for explaining black box models", "venue": "ACM Computing Surveys (CSUR),", "year": 2018}, {"authors": ["S. Hara", "M. Ishihata"], "title": "Approximate and exact enumeration of rule models", "venue": "In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence (AAAI\u201918),", "year": 2018}, {"authors": ["S. Hara", "T. Maehara"], "title": "Enumerate Lasso solutions for feature selection", "venue": "In Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence,", "year": 2017}, {"authors": ["M. Hardt", "E. Price", "N Srebro"], "title": "Equality of opportunity in supervised learning", "venue": "In Proceedings of the Annual Conference on Neural Information Processing Systems (NIPS\u201916),", "year": 2016}, {"authors": ["J. Kleinberg", "H. Lakkaraju", "J. Leskovec", "J. Ludwig", "S. Mullainathan"], "title": "Human decisions and machine predictions", "venue": "The quarterly journal of economics,", "year": 2017}, {"authors": ["E.L. Lawler"], "title": "A procedure for computing the k best solutions to discrete optimization problems and its application to the shortest path problem", "venue": "Management science,", "year": 1972}, {"authors": ["B. Lepri", "N. Oliver", "E. Letouz\u00e9", "A. Pentland", "P. Vinck"], "title": "Fair, transparent, and accountable algorithmic decisionmaking processes", "venue": "Philosophy & Technology,", "year": 2017}, {"authors": ["B. Letham", "C. Rudin", "T.H. McCormick", "D. Madigan"], "title": "Interpretable classifiers using rules and Bayesian analysis: Building a better stroke prediction model", "venue": "The Annals of Applied Statistics,", "year": 2015}, {"authors": ["J. Li", "H. Shen", "R. Topor"], "title": "Mining the optimal class association rule set", "venue": "Knowledge-Based Systems,", "year": 2002}, {"authors": ["Z.C. Lipton"], "title": "The mythos of model interpretability", "venue": "Communications of the ACM,", "year": 2018}, {"authors": ["S.M. Lundberg", "Lee", "S.-I"], "title": "A unified approach to interpreting model predictions", "venue": "In Proceedings of the Annual Conference on Neural Information Processing Systems (NIPS\u201917),", "year": 2017}, {"authors": ["D.A. Melis", "T. Jaakkola"], "title": "Towards robust interpretability with self-explaining neural networks", "venue": "In Advances in Neural Information Processing Systems,", "year": 2018}, {"authors": ["G. Montavon", "W. Samek", "M\u00fcller", "K.-R"], "title": "Methods for interpreting and understanding deep neural networks", "venue": "Digital Signal Processing,", "year": 2018}, {"authors": ["A. Narayanan"], "title": "Translation tutorial: 21 fairness definitions and their politics", "venue": "In Proceedings of the Conference on Fairness Accountability Transparency (FAT*),", "year": 2018}, {"authors": ["B. Reddix-Smalls"], "title": "Credit scoring and trade secrecy: An algorithmic quagmire or how the lack of transparency in complex financial models scuttled the finance market", "venue": "UC Davis Bus. LJ,", "year": 2011}, {"authors": ["M.T. Ribeiro", "S. Singh", "C. Guestrin"], "title": "Why should I trust you?: Explaining the predictions of any classifier", "venue": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD\u201916),", "year": 2016}, {"authors": ["R.L. Rivest"], "title": "Learning decision lists", "venue": "Machine learning,", "year": 1987}, {"authors": ["C. Rudin"], "title": "Please stop explaining black box models for high stakes decisions", "venue": "arXiv preprint arXiv:1811.10154,", "year": 2018}, {"authors": ["N. Siddiqi"], "title": "Credit risk scorecards: developing and implementing intelligent credit scoring, volume 3", "year": 2012}, {"authors": ["S. Wachter", "B. Mittelstadt", "L. Floridi"], "title": "Why a right to explanation of automated decision-making does not exist in the general data protection regulation", "venue": "International Data Privacy Law,", "year": 2017}, {"authors": ["R. Wexler"], "title": "When a computer program keeps you in jail: How computers are harming criminal justice", "year": 2017}], "sections": [{"heading": "1. Introduction", "text": "In recent years, the widespread use of machine learning models in high stakes decision-making systems (e.g., credit scoring (Siddiqi, 2012), predictive justice (Kleinberg et al., 2017) or medical diagnosis (Caruana et al., 2015)) combined with proven risks of incorrect decisions \u2013 such as people being wrongly denied parole (Wexler, 2017a) \u2013 has considerably raised the public demand for being able to provide explanations to algorithmic decisions.\n1Universite\u0301 du Que\u0301bec a\u0300 Montre\u0301al 2RIKEN Center for Advanced Intelligence Project 3JST PRESTO 4ENSTA ParisTech 5Osaka University 6UdeM 7MILA. Correspondence to: Ulrich A\u0131\u0308vodji <aivodji.ulrich@courrier.uqam.ca>.\nProceedings of the 36 th International Conference on Machine Learning, Long Beach, California, PMLR 97, 2019. Copyright 2019 by the author(s).\nIn particular, to ensure transparency and explainability in algorithmic decision processes, several initiatives have emerged for regulating the use of machine learning models. For instance, in Europe, the new General Data Protection Regulation has a provision requiring explanations for the decisions of machine learning models that have a significant impact on individuals (Goodman & Flaxman, 2017).\nExisting methods to achieve explainability include transparent-box design and black-box explanation (also called post-hoc explanation) (Lipton, 2018; Lepri et al., 2017; Montavon et al., 2018; Guidotti et al., 2018). The former consists in building transparent models, which are inherently interpretable by design. This approach requires the cooperation of the entity responsible for the training and usage of the model. In contrast, the latter involves an adversarial setting in which the black-box model, whose internal logic is hidden to the auditor, is reversed-engineered to create an interpretable surrogate model.\nWhile many recent works have focused on black-box explanations, their use can be problematic in high stakes decision-making systems (Reddix-Smalls, 2011; Wexler, 2017b) as explanations can be unreliable and misleading (Rudin, 2018; Melis & Jaakkola, 2018). Current techniques for providing black-box explanations include model explanation, outcome explanation and model inspection (Guidotti et al., 2018). Model explanation consists in building an interpretable model to explain the whole logic of the black box while outcome explanation only cares about providing a local explanation of a specific decision. Finally, model inspection consists of all the techniques that can help to understand (e.g., through visualizations or quantitative arguments) the influence of the attributes of the input on the black-box decision.\nSince the right to explanation as defined in current regulations (Goodman & Flaxman, 2017) does not give precise directives on what it means to provide a \u201cvalid explanation\u201d (Wachter et al., 2017; Edwards & Veale, 2017), there is a legal loophole that can be used by dishonest companies to cover up the possible unfairness of their black-box models by providing misleading explanations. In particular, due to the growing importance of the concepts of fairness in machine learning (Barocas & Selbst, 2016), a company might be tempted to perform fairwashing, which we define\nar X\niv :1\n90 1.\n09 74\n9v 3\n[ cs\n.L G\n] 1\n5 M\nay 2\n01 9\nas promoting the false perception that the learning models used by the company are fair while it might not be so.\nIn this paper, our main objective is to raise the awareness of this issue, which we believe to be a serious concern related to the use of black-box explanation. In addition, to provide concrete evidence on this possible risk, we show that it is possible to forge a fairer explanation from a truly unfair black box through a process that we coin as rationalization.\nIn particular, we propose a systematic rationalization technique that, given black-box access to a model f , produces an ensemble C of interpretable models c \u2248 f that are fairer than the black-box according to a predefined fairness metric. From this set of plausible explanations, a dishonest entity can pick a model to achieve fairwashing. One of the strength of our approach is that it is agnostic to both the black-box model and the fairness metric. To demonstrate the genericity of our technique, we show that the rationalization can be used to explain globally the decision of a black-box model (i.e., rationalization of model explanation) as well as its decision for a particular input (i.e., rationalization of outcome explanation). While our approach is mainly presented in this paper as a proof-of-concept to raise awareness on this issue, we believe that the number of ways rationalization can be instantiated are endless.\nThe outline of the paper is as follows. First in Section 2, we review the related work on interpretability and fairness necessary to the understanding of our work. Afterwards in Section 3, we formalize the rationalization problem before introducing LaundryML, our algorithm for the enumeration of rule lists that can be used to perform fairwashing. Finally, in Section 4 we report on the evaluation obtained when applying on rationalization approach on two real datasets before concluding in Section 5."}, {"heading": "2. Related work", "text": ""}, {"heading": "2.1. Interpretability and explanation", "text": "In the context of machine learning, interpretability can be defined as the ability to explain or to provide the meaning in understandable terms to a human (Doshi-Velez & Kim, 2017). An explanation is an interface between humans and a decision process that is both an accurate proxy of the decision process and understandable by humans (Guidotti et al., 2018). Examples of such interfaces include linear models (Ribeiro et al., 2016), decision trees (Breiman, 2017), rule lists (Rivest, 1987; Letham et al., 2015; Angelino et al., 2018) and rule sets (Li et al., 2002). In this paper, we focus on two black-box explanation tasks, namely model explanation and outcome explanation (Guidotti et al., 2018).\nDefinition 1 (Model explanation). Given a black-box model b and a set of instances X , the model explanation problem consists in finding an explanation E \u2208 E belonging to a\nhuman-interpretable domain E , through an interpretable global predictor cg = f(b,X) derived from the black-box b and the instances X using some process f(\u00b7, \u00b7).\nFor example, if we choose the domain E to be the set of decision trees, model explanation amounts to identifying a decision tree that approximates well the black-box model. The identified decision tree can be interpreted as an explanation of the black-box model (Craven & Shavlik, 1996). Definition 2 (Outcome explanation). Given a black-box model b and an instance x, the outcome explanation problem consists in finding an explanation e \u2208 E , belonging to a human-interpretable domain E , through an interpretable local predictor cl = f(b, x) derived from the black-box b and the instance x using some process f(\u00b7, \u00b7).\nFor example, choosing the domain E to be the set of linear models, the outcome explanation amounts to identifying a linear model approximating well the black-box model in the neighbourhood of x. The identified linear model can be interpreted as a local explanation of the black-box on the instance x. Approaches such as LIME (Ribeiro et al., 2016) and SHAP (Lundberg & Lee, 2017) belong to this class."}, {"heading": "2.2. Fairness in machine learning", "text": "To explain the fairness of machine models, various metrics have been proposed in recent years (Narayanan, 2018; Berk et al., 2018). Roughly there are two distinct families of fairness definitions: group fairness and individual fairness, which requires decisions to be consistent for individuals that are similar (Dwork et al., 2012). In this work, we focus on group fairness that requires the approximate equalization of a particular statistical property across groups defined according to the value of a sensitive attribute. In particular, if the sensitive attribute is binary, its value splits the population into a minority group and a majority group.\nOne of the common group fairness metrics is demographic parity (Calders et al., 2009), which is defined as the equality of distribution of decisions conditioned to the sensitive attribute. Equalized odds (Hardt et al., 2016) is another common criterion requiring the false positive and false negative rates be equal across the majority and minority groups. In a nutshell, demographic parity quantifies biases in both training data and learning, while equalized odds focus on the learning process. In this paper, we adopt demographic parity to evaluate the overall unfairness in algorithmic decisionmaking.\nMost of the existing tools for quantifying the fairness of a machine learning model do not address the interpretability issue at the same time. Possible exceptions include AI Fairness 360 that provides visualizations of attribute bias and feature bias localization (Bellamy et al., 2018), DCUBE-GUI (Berendt & Preibusch, 2012) that has an in-\nterface for visualizing the unfairness scores of association rules resulted from data mining as well as FairML (Adebayo, 2016) that computes the relative importance of each attribute into the prediction outcome. While these tools actually provide a form of model inspection, they cannot be used directly for model or outcome explanations."}, {"heading": "2.3. Rule Lists", "text": "A rule list (Rivest, 1987; Letham et al., 2015; Angelino et al., 2018) d = (dp, \u03b4p, q0,K) of length K \u2265 0 is a (K + 1)\u2212tuple consisting of K distinct association rules rk = pk \u2192 qk, in which pk \u2208 dp is the antecedent of the association rule and qk \u2208 \u03b4p its corresponding consequent, followed by a default prediction q0. An equivalent way to represent a rule list is in the form of a decision tree whose shape is a comb. Making a prediction using a rule list means that its rules are applied sequentially until one rule triggers, in which case the associated outcome is reported. If no rules are trigerred, then the default prediction is reported."}, {"heading": "2.4. Optimal rule list and enumeration of rule lists", "text": "CORELS (Angelino et al., 2018) is a supervised machine learning algorithm which takes as input a training set with n predictor variables, all assumed to be binary. First, it represents the search space of the rule lists as a n-level trie whose root node has n children, formed by the n predictor variables, each of which has n \u2212 1 children, composed of all the predictor variables except the parent, and so on. Afterwards, it considers for a rule list d = (dp, \u03b4p, q0,K), an objective function defined as a regularized empirical risk:\nR(\u00b7) = misc(\u00b7) + \u03bbK,\nin which misc(\u00b7) is the misclassification error and \u03bb \u2265 0 the regularization parameter used to penalize longer rule lists. Finally, CORELS selects the rule list that minimizes the objective function. To realize this, it uses an efficient branch-and-bound algorithm to prune the trie. In addition, on high-dimensional datasets (i.e., with a large number of predictor variables), CORELS uses both the number of nodes to be evaluated in the trie and the upper bound on the remaining search space size as a stopping criterion to identify suboptimal rule lists.\nIn (Hara & Ishihata, 2018), the authors propose an algorithm based on the Lawler\u2019s framework (Lawler, 1972), which allows to successively compute the optimal solution and then construct sub-problems excluding the solution obtained. In particular, the authors have combined this framework with CORELS (Angelino et al., 2018) to compute the exact enumeration for rule lists. In a nutshell, this algorithm takes as input the set T of all antecedents. It maintains a heap H, whose priority is the objective function value of the rule lists and a list M for the enumer-\nated models. First, it starts by computing the optimal rule m = CORELS(T ) = (dp, \u03b4p, q0,K) for T using the CORELS algorithm. Afterward, it inserts the tuple (m,T, \u2205) into the heap. Finally, the algorithm repeats the following steps until the stopping criterion is met:\n\u2022 (Step 1) Extract the tuple (m,S, F ) with the maximum priority value fromH.\n\u2022 (Step 2) Output m as the i\u2212th model if m /\u2208M.\n\u2022 (Step 3) Branch the search space: compute and insert m\u2032 = CORELS(S \\ {tj}) intoH for all tj \u2208 \u03b4p."}, {"heading": "3. Rationalization", "text": "In this section, we first formalize the problem of the rationalization before introducing LaundryML, our regularized enumeration algorithm that can be used to perform fairwashing with respect to black-box machine learning models."}, {"heading": "3.1. Problem formulation", "text": "A typical scenario that we envision is the situation in which a company wants to perform fairwashing because it is afraid of a possible audit evaluating the fairness of the machine learning it uses to personalize the services provided to its users. In this context, rationalization consists in finding an interpretable surrogate model c approximating a black-box model b, such that c is fairer than b. To achieve fairwashing, the surrogate model obtained through rationalization could be shown to the auditor (e.g., an external dedicated entity or the users themselves) to convince him that the company is \u201cclean\u201d. We distinguish two types of rationalization problems, namely the model rationalization problem and the outcome rationalization problem, which we define hereafter.\nDefinition 3 (Model rationalization). Given a black-box model b, a set of instances X and a sensitive attribute s, the model rationalization problem consists in finding an explanation E \u2208 E belonging to a human-interpretable domain E , through an interpretable global predictor cg = f(b,X) derived from the black-box b and the instances X using some process f(\u00b7, \u00b7), such that (cg, X, s) > (b,X, s), for some fairness evaluation metric (\u00b7, \u00b7, \u00b7). Definition 4 (Outcome rationalization). Given a black-box model b, an instance x, a neighborhood V(x) of x and a sensitive attribute s, the outcome rationalization problem consists in finding an explanation e \u2208 E , belonging to a human-interpretable domain E , through an interpretable local predictor cl = f(b, x) derived from the black-box b and the instance x using some process f(\u00b7, \u00b7), such that (cl,V(x), s) > (b,V(x), s), for some fairness evaluation metric (\u00b7, \u00b7, \u00b7).\nIn this paper, we will refer to the set of instances for which\nthe rationalization is done as the suing group. For the sake of clarity, we also restrict the context to binary attributes and to binary classification. However, our approach could also be used to multi-valued attributes. For instance, multi-valued attributes can be converted to binary ones using a standard approach such as a one-hot encoding. For simplicity, we also assume that there is only one sensitive attribute, though our work could also be extended to multiple sensitive attributes."}, {"heading": "3.2. Performance metrics", "text": "We evaluate the performance of the surrogate by taking into account (1) its fidelity with respect to the original black-box model and (2) its unfairness with respect to a predefined fairness metric.\nFor the model rationalization problem, the fidelity of the surrogate cg is defined as its accuracy relative to b on X (Craven & Shavlik, 1996), which is\nfidelity(c) = 1 |X| \u2211 x\u2208X I(c(x) = b(x)).\nFor the outcome rationalization problem, the fidelity of the surrogate cl is 1 if cl(x) = b(x) and 0 otherwise.\nWith respect to fairness, among the numerous existing definitions (Narayanan, 2018), we rely on the demographic parity (Calders et al., 2009) as the fairness metric. Demographic parity requires the prediction to be independent of the sensitive attribute, which is\nP (y\u0302 = 1|s = 1) = P (y\u0302 = 1|s = 0),\nin which y\u0302 is the predicted value of cg for x \u2208 X for the rationalization problem (respectively, the predicted value of cl for x \u2208 V(x) for the outcome problem. Therefore, unfairness is defined as\n|P (y\u0302 = 1|s = 1)\u2212 P (y\u0302 = 1|s = 0)|.\nWe consider that rationalization leads to fairwashing when the fidelity of the interpretable surrogate model is high while at the same time its level of unfairness is significantly lower than the original black-box algorithm.\n3.3. LaundryML: a regularized model enumeration algorithm\nTo rationalize unfair decisions, we need to find a good surrogate model that has high fidelity and low unfairness at the same time. However, finding a single optimal model is difficult in general; indeed it is rarely the case that a single model achieves optimality with respect to two different criteria at the same time. To bypass this difficulty, we adopt a model enumeration approach. In a nutshell, our approach\nworks by first enumerating several models with sufficiently high fidelity to the original black-box model and low unfairness. Afterwards, the approach picks up the model that is convenient for rationalization based on some other criteria or through human inspection.\nIn this section, we introduce LaundryML, a regularized model enumeration algorithm that enumerates rule lists to achieve rationalization. More precisely, we propose two versions of LaundryML, one that solves the model rationalization problem and the other that solves the outcome rationalization.\nLaundryML is a modified version of the algorithm presented in Section 2.4, which considers both the fairness and fidelity constraints in its search. The algorithm is summarized in Algorithm 1.\nAlgorithm 1 LaundryML 1: Inputs: T , \u03bb, \u03b2 2: Output:M 3: obj(\u00b7) = (1\u2212 \u03b2)misc(\u00b7) + \u03b2unfairness(\u00b7) + \u03bbK 4: Compute m = CORELS(obj, T ) = (dp, \u03b4p, q0,K) 5: Insert (m,T, \u2205) into the heap 6: M\u2190 \u2205 7: for i = 1, 2, . . . do 8: Extract (m,S, F ) from the heap 9: if m /\u2208M then 10: M\u2190M\u222a {m} 11: end if 12: if Terminate(M) = true then 13: break 14: end if 15: for tj \u2208 dp and tj /\u2208 F do 16: Compute m\u2032 = CORELS(obj, S \\ {tj}) 17: Insert (m\u2032, S \\ {tj}, F ) into the heap 18: F \u2190 F \u222a {tj} 19: end for 20: end for\nOverall, LaundryML takes as inputs the set of antecedents T and the regularization parameters \u03bb and \u03b2, respectively for the rule lists length and the unfairness. First, it redefines the objective function for CORELS to penalize both the rule list\u2019s length and unfairness (line 3):\nobj(\u00b7) = (1\u2212 \u03b2)misc(\u00b7) + \u03b2unfairness(\u00b7) + \u03bbK, in which \u03b2 \u2208 [0, 1] and \u03bb \u2265 0. Afterwards, the algorithm runs the enumeration routine described in Section 2.4 using the new objective function (lines 4\u201320). Using LaundryML to solve the model rationalization problem as well as the outcome rationalization problem is straightforward as described hereafter.\nThe model rationalization algorithm depicted in Algorithm 2 takes as input a black-box access to the black box model b,\nthe suing group X as well as the regularization parameters \u03bb and \u03b2. First, the members of the suing group are labeled using the predictions of b. Afterwards, the labeled dataset T obtained is passed to the LaundryML algorithm.\nAlgorithm 2 LaundryML-global 1: Inputs: X , b, \u03bb, \u03b2 2: Output:M 3: y = b(X) 4: T = {X, y} 5: M = LaundryML(T, \u03bb, \u03b2)\nThe rationalized outcome explanation presented in Algorithm 3 is similar to the rationalized model explanation, but instead of using the labelled dataset, it directly computes for the considered subject x its neighbourhood Tx = neigh(x, TX) and uses it for the model enumeration.\nAlgorithm 3 LaundryML-local 1: Inputs: x, T , neigh(\u00b7), \u03bb, \u03b2 2: Output:Mx 3: Tx = neigh(x,T) 4: Mx = LaundryML(Tx, \u03bb, \u03b2)\nIn the next section, we report on the results that we have obtained with these two approaches on real-world datasets."}, {"heading": "4. Experimental evaluation", "text": "In this section, we describe the experimental setting used to evaluate our rationalization algorithm as well as the results obtained. More precisely, we evaluate the unfairness and fidelity of models produced using both model rationalization and outcome rationalization. The results obtained confirm that it is possible to systematically produce models that are fairer than the black-box models they approximate while still being faithful to the original model."}, {"heading": "4.1. Experimental setting", "text": "Datasets. We conduct our experiments on two real-world datasets that have been extensively used in the fairness literature due to their biased nature, namely Adult Income (Frank & Asuncion, 2010) and the ProPublica Recidivism (Angwin et al., 2016) datasets. These datasets have been chosen because they are widely used in the FAT community due to the gender bias (respectively racial bias) in the Adult (respectively the ProPublica Recidivism) dataset. The Adult Income dataset gathers information about individuals from the 1994 U.S. census, and the objective of the classification task is to predict whether an individual makes more or less than 50, 000$ per year. Overall, this dataset contains 32, 561 instances and 14 attributes. The ProPublica Recidivism dataset gathers criminal offenders records in Florida\nduring 2013 and 2014 with the classification task considered being to predict whether or not a subject will re-offend within two years after the screening. Overall, this dataset contains 6, 167 instances and 12 attributes.\nMetrics. We use both fidelity and unfairness (as defined in Section 3.2) to evaluate the performance of LaundryML-global and LaundryML-local. In addition to these metrics, we also assess the performance of our approach by auditing both the black-box classifier and the best-enumerated models using FairML (Adebayo, 2016). In a nutshell, FairML is an automated tool requiring black-box access to a classifier to determine the relative importance of each attribute into the prediction outcome.\nLearning the black-box models. We first split each dataset into three subsets, namely the training set, the suing group and the test set. After learning the black-box models on the training sets, we apply them to label the suing groups, which in turn are used to train the interpretable surrogate models. Finally, the test sets are used to evaluate the accuracy and the unfairness of the black-box models. In practice for both datasets, we rely on a model trained using random forest to play the role of the black-box models. Table 1 summarizes the performances of the black-box models that have been trained.\nScenarios investigated. As explained in Section 2, CORELS requires binary features. After the preprocessing, we obtain respectively 28 and 27 antecedents for the Adult Income and ProPublica Recidivism dataset. In the experiments, we considered the following two scenarios.\n(S1) Responding to a suing group. Consider the situation in which a suing group complained for unfair decisions and its members request explanations on how the decisions affecting them were made. In this scenario, a single explanation needs to consistently explain the entire group as much as possible. Thus, we rationalize the decision using model rationalization based on Algorithm 2 (LaundryML-global).\n(S2) Responding to individual claim. Imagine that a subject files a complaint following a decision that he deems unfair and that he requests an explanation on how the decision was made. In this scenario, we only need to respond to each subject independently. Thus, we rationalize the decision using outcome rationalization based on Algorithm 3 (LaundryML-local).\nExperiments were conducted on an Intel Core i7 (2.90 GHz, 16GB of RAM). The modified version of CORELS is implemented in C++ and based on the original source\ncode of CORELS1. Algorithm 1, Algorithm 2 as well as Algorithm 3 were all implemented in Python. To implement Algorithm 1, we modified the Lasso enumeration algorithm2 of (Hara & Maehara, 2017). All our experiments can be reproduced using the code provided in https://github.com/aivodji/LaundryML.\nFor the scenario (S1), we use regularization parameters with values within the following ranges \u03bb = {0.005, 0.01} and \u03b2 = {0.0, 0.1, 0.2, 0.5, 0.7, 0.9} for both datasets, yielding 12 experiments per dataset. For each of these experiments, we enumerate 50 models.\nFor the scenario (S2), we use the regularization parameters \u03bb = 0.005 and \u03b2 = {0.1, 0.3, 0.5, 0.7, 0.9} for both datasets. For each dataset, we produce a rationalized outcome explanation of each rejected subject belonging to the minority groups (i.e., female subjects of Adult Income\u2019s suing group and black subjects of ProPublica Recidivism\u2019s suing group), and for whom the unfairness as measured in their neighbourhood is greater than 0.05. Overall, they are 2, 634 (respectively 285) such subjects in the Adult Income\u2019s suing group (respectively ProPublica Recidivism\u2019s suing group), yielding a total of 13, 170 (respectively 1, 425) experiments for the two datasets. For each of these experiments, we also enumerate 50 models and select the one (if it exists) that has the same outcome as the black-box model and the lowest unfairness. To compute the neighbourhood neigh(\u00b7) of each subject, we apply the k-nearest neighbour algorithm with k set to 10% of the size of the suing group. Although it would be interesting to see the effect of the size of the neighbourhood on the performance of the algorithm, we leave this aspect to be explored in future works."}, {"heading": "4.2. Experimental results", "text": "Summary of the results. Across the experiments, we observed the fidelity-fairness trade-off by performing model enumeration LaundryML-global. By visualizing this trade-off, we can select the threshold of the fairness condition easily. In some cases, the best models we found out significantly decrease the unfairness while retaining a high fidelity with the black-box model. We have also checked that the sensitive attributes have a low impact on the surro-\n1https://goo.gl/qTeUAu 2https://goo.gl/CyEU4C\ngate models\u2019 predictions compared to the original black-box ones. This confirms the effectiveness of the model enumeration, in the sense that we can identify a model convenient for rationalization out of the enumerated models. In addition, by using LaundryML-local, we can obtain models explaining the outcomes for users in the minority groups while being fairer than the black-box model.\n(S1) Responding to a suing group. Figure 1 represents the results obtained on the scenario (S1) based on LaundryML-global, by depicting the changes of fidelity and unfairness of the models accompanied by the fairness regulation parameter \u03b2. From these results, we observe that as \u03b2 increases, both the unfairness and the fidelity of the enumerated models decrease. In addition, as the enumerated models get more complex (i.e., \u03bb decreases), their fidelity increases. On both datasets, we selected the best model using the following strategy. First, we select models whose unfairnesses are at least two times less than the original black-box model and then among those models, and we select the one with the highest fidelity as the best model. On Adult Income, the best model has a fidelity of 0.908 and an unfairness of 0.058. On ProPublica Recidivism, the best model has a fidelity of 0.748 and an unfairness of 0.080.\nIn addition, we compared the obtained best models with the corresponding black-box models using FairML. With FairML, we ranked the importance of each feature to the model\u2019s decision process. Figure 3 shows that the best obtained surrogate models were found to be fair by FairML: in particular the rankings of the sensitive attributes (i.e., gender:Male for Adult Income, and race:Caucasian for ProPublica Recidivism) got significantly lower in the surrogate models. In addition, we observe a shift from the 2nd position to the last one on Adult Income as well as a change from the 5th rank to the 13th rank on ProPublica Recidivism.\n(S2) Responding to individual claim. Table 2 as well as Figure 2 demonstrate the results on (S2) by using LaundryML-local. From these results, we observe that as the fairness regulation parameter \u03b2 increases, the unfairness of the enumerated models decreases. In particular, with \u03b2 = 0.9, a completely fair model (unfairness = 0.0) was found to explain the outcome for each rejected user of the\nminority group of Adult Income. For ProPublica Recidivism, the enumerated model found for each user is at least twice less unfair than the black-box models.\nRemark. Our preliminary experiments in (S1) show that the fidelity of the model rationalization on the test set tends to be slightly lower than the one on the suing group, which means that the explanation is customized specifically for the suing group. Thus, if the suing group gather additional members on which the existing explanation is applied, the explanation may not be able to rationalize as well those additional members. This opens the way for future research directions such as developing methods for detecting that an explanation is actually a rationalization or conversely obtaining rationalizations that are difficult to be detected."}, {"heading": "5. Conclusion", "text": "In this paper, we have introduced the rationalization problem and the associated risk of fairwashing in the machine learning context and shown how it can be achieved through model explanation as well as outcome explanation. We have also proposed a novel algorithm called LaundryML for regularized model enumeration of rules lists, which incorporate fairness as a constraint along with the fidelity. Experimental results obtained using real-world datasets demonstrate the feasibility of the approach. Overall, we hope that our work will raise the awareness of the machine learning community and inspire future research towards the ethical issues raised by the possibility of rationalizing, in particular with respect to the risk of performing fairwashing in this context.\nOur framework can be extended to other interpretable models or fairness metrics as we show in the additional experiments provided in appendices. As future works, along with ethicists and law researchers, we want to investigate the general social implications of fairwashing in order to develop a comprehensive framework to reason on this issue. Finally, as mentioned previously we also aim at developing approaches that can estimate whether or not an explanation is likely to be a rationalization in order to be able to detect fairwashing.\nc\u20dd 2018 Information Processing Society of Japan\nc\u20dd 2018 Information Processing Society of Japan"}, {"heading": "Acknowledgements", "text": "We would like to thank Daniel Alabi for the help with the CORELS code and Zachary C. Lipton for his thoughtful and actionable feedback on the paper. Se\u0301bastien Gambs is supported by the Canada Research Chair program as well as by a Discovery Grant and a Discovery Accelerator Supplement Grant from NSERC."}, {"heading": "A. Generalization to other fairness metrics", "text": "In this section, we present the performances of LaundryML with other fairness metrics are used. To control the rest of the parameters, the experiments are done only for the model rationalization algorithm LaundryML-global for a black-box model trained on the Adult Income dataset. We use the same black-box model trained with Random Forest, that we use in our previous experiments. In addition to the demographic parity metric, we use three different fairness metrics, namely the overall accuracy equality metric, the statistical parity metric and the conditional procedure accuracy metric. The definitions of all these additional fairness metrics can be found in (Berk et al., 2018). For each of these scenarios, we enumerate 50 models and use the regularization parameters \u03bb = 0.005 and \u03b2 = {0, 0.1, 0.2, 0.5, 0.7, 0.9}.\nFigures 4, 5, 6 and 7 show the performances of LaundryML-global when respectively overall accuracy equality, statistical parity, conditional procedure accuracy and demographic parity are used as fairness metrics, and the black-box model is a Random Forest classifier. Overall, for all these fairness metrics, LaundryML-global can find explanation models to use for fairwashing. In particular, when the unfairness of the black-box model is high (i.e., unfairness \u2265 0.1), a higher unfairness regularization (i.e., \u03b2 \u2265 0.5) allows to find more potential candidate models for fairwashing. In contrast, when the unfairness of the black-box model is low (i.e., unfairness < 0.1), a lower unfairness regularization (i.e., \u03b2 < 0.5) enables to find more potential candidate models for fairwashing. In general, the smaller the regularization, the better the fidelity. Overall, these results confirm that the possibility of performing fairwashing is agnostic to the fairness metric considered."}, {"heading": "B. Generalization to other black-Box models", "text": "In this section, we present the performances of LaundryML when other black-box models are used. To control the rest of the parameters, the experiments are done only for the model rationalization algorithm LaundryML-global with demographic parity as fairness metrics. In addition to the Random Forest, we use three different types of black-box models, namely a Support Vector Machine (SVM) classifier, a Gradient Boosting (XGBOOST) classifier and a Multi-Layer Perceptron (MLP) classifier. For each of these scenarios, we enumerate 50 models and use the regularization parameters \u03bb = 0.005 and \u03b2 = {0, 0.1, 0.2, 0.5, 0.7, 0.9}. Figures 7, 8, 9 and 10 show the performances of LaundryML-global when the black-box models are respectively a Random Forest classifier, a SVM classifier, a XGBOOST classifier and a MLP classifier, and the fairness metric is demographic parity. For each type of black-box model, LaundryML-global can find explanation models to use for fairwashing. Overall, these results confirm that the possibility of performing fairwashing is also agnostic to the black-box model."}], "title": "Fairwashing: the risk of rationalization", "year": 2019}