{"abstractText": "Machine learning becomes increasingly important to tune or even synthesize the behavior of safety-critical components in highly nontrivial environments, where the inability to understand learned components in general, and neural nets in particular, poses serious obstacles to their adoption. Explainability and interpretability methods for learned systems have gained considerable academic attention, but the focus of current approaches on only one aspect of explanation, at a fixed level of abstraction, and limited if any formal guarantees, prevents those explanations from being digestible by the relevant stakeholders (e.g., end users, certification authorities, engineers) with their diverse backgrounds and situation-specific needs. We introduce Fanoos, a flexible framework for combining formal verification techniques, heuristic search, and user interaction to explore explanations at the desired level of granularity and fidelity. We demonstrate the ability of Fanoos to produce and adjust the abstractness of explanations in response to user requests on a learned controller for an inverted double pendulum and on a learned CPU usage model.", "authors": [{"affiliations": [], "name": "David Bayani"}, {"affiliations": [], "name": "Stefan Mitsch"}], "id": "SP:8735c8fc9671d60755fada5494908b7e98d910be", "references": [{"authors": ["A. Adadi", "M. Berrada"], "title": "Peeking inside the black-box: A survey on explainable artificial intelligence (xai)", "venue": "IEEE Access 6, 52138\u201352160", "year": 2018}, {"authors": ["S.P. Adam", "D.A. Karras", "G.D. Magoulas", "M.N. Vrahatis"], "title": "Reliable estimation of a neural network\u2019s domain of validity through interval analysis based inversion", "venue": "2015 International Joint Conference on Neural Networks, IJCNN 2015, Killarney, Ireland, July 12-17, 2015. pp. 1\u2013 8", "year": 2015}, {"authors": ["R. Agrawal", "T. Imieli\u0144ski", "A. Swami"], "title": "Mining association rules between sets of items in large databases", "venue": "Acm sigmod record. vol. 22, pp. 207\u2013216. ACM", "year": 1993}, {"authors": ["D.W. Aha", "T. Darrell", "M. Pazzani", "D. Reid", "C. Sammut", "P. Stone"], "title": "Ijcai 2017 workshop on explainable artificial intelligence (xai)", "venue": "Melbourne, Australia, August", "year": 2017}, {"authors": ["R. Andrews", "J. Diederich", "A. Tickle"], "title": "Survey and critique of techniques for extracting rules from trained artificial neural networks", "venue": "Knowledge-Based Systems 6, 373\u2013389", "year": 1995}, {"authors": ["S. Anjomshoae", "A. Najjar", "D. Calvaresi", "K. Fr\u00e4mling"], "title": "Explainable agents and robots: Results from a systematic literature review", "venue": "Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems. pp. 1078\u20131088. International Foundation for Autonomous Agents and Multiagent Systems", "year": 2019}, {"authors": ["D. Ballard"], "title": "Generalising the hough transform to detect arbitary patterns", "venue": "Pattern Recognition 13", "year": 1981}, {"authors": ["A. Benz", "G. J\u00e4ger", "R. Van Rooij", "R. Van Rooij"], "title": "Game theory and pragmatics", "venue": "Springer", "year": 2005}, {"authors": ["A. Biere", "A. Cimatti", "E.M. Clarke", "O. Strichman", "Y Zhu"], "title": "Bounded model checking", "venue": "Advances in computers 58(11), 117\u2013148", "year": 2003}, {"authors": ["O. Biran", "C. Cotton"], "title": "Explanation and justification in machine learning: A survey", "venue": "IJCAI-17 workshop on explainable AI (XAI). vol. 8, p. 1", "year": 2017}, {"authors": ["T. Chakraborti", "A. Kulkarni", "S. Sreedharan", "D.E. Smith", "S. Kambhampati"], "title": "Explicability? legibility? predictability? transparency? privacy? security? the emerging landscape of interpretable agent behavior", "venue": "Proceedings of the International Conference on Automated Planning and Scheduling. vol. 29, pp. 86\u201396", "year": 2019}, {"authors": ["J. Chuang", "D. Ramage", "C. Manning", "J. Heer"], "title": "Interpretation and trust: Designing model-driven visualizations for text analysis", "venue": "Proceedings of the SIGCHI Conference on Human Factors in Computing Systems. pp. 443\u2013452. ACM", "year": 2012}, {"authors": ["E. Clarke", "A. Fehnker", "Z. Han", "B. Krogh", "O. Stursberg", "M. Theobald"], "title": "Verification of hybrid systems based on counterexample-guided abstraction refinement", "venue": "International Conference on Tools and Algorithms for the Construction and Analysis of Systems. pp. 192\u2013207. Springer", "year": 2003}, {"authors": ["E. Clarke", "O. Grumberg", "S. Jha", "Y. Lu", "H. Veith"], "title": "Counterexample-guided abstraction refinement", "venue": "International Conference on Computer Aided Verification. pp. 154\u2013169. Springer", "year": 2000}, {"authors": ["P. Cousot", "R. Cousot"], "title": "Abstract interpretation: a unified lattice model for static analysis of programs by construction or approximation of fixpoints", "venue": "Proceedings of the 4th ACM SIGACT-SIGPLAN symposium on Principles of programming languages. pp. 238\u2013252", "year": 1977}, {"authors": ["Q. David"], "title": "Design issues in adaptive control", "venue": "IEEE Transactions on Automatic Control 33(1)", "year": 1988}, {"authors": ["L. De Moura", "Bj\u00f8rner", "N.: Z"], "title": "An efficient smt solver. In: Proceedings of the Theory and Practice of Software, 14th International Conference on Tools and Algorithms for the Construction and Analysis of Systems", "year": 2008}, {"authors": ["A. Driescher", "U. Korn"], "title": "Checking stability of neural narx models: An interval approach", "venue": "IFAC Proceedings Volumes 30(6), 1005\u20131010", "year": 1997}, {"authors": ["H.B. Eom", "S.M. Lee"], "title": "A survey of decision support system applications (1971\u2013april 1988)", "venue": "Interfaces 20(3), 65\u201379", "year": 1990}, {"authors": ["S. Eom", "E. Kim"], "title": "A survey of decision support system applications (1995\u20132001)", "venue": "Journal of the Operational Research Society 57(11), 1264\u20131278", "year": 2006}, {"authors": ["S.B. Eom", "S.M. Lee", "E. Kim", "C. Somarajan"], "title": "A survey of decision support system applications (1988\u20131994)", "venue": "Journal of the Operational Research Society 49(2), 109\u2013 120", "year": 1998}, {"authors": ["U.M. Fayyad", "G. Piatetsky-Shapiro", "P. Smyth", "R Uthurusamy"], "title": "Advances in knowledge discovery and data mining, vol", "venue": "21. AAAI press Menlo Park", "year": 1996}, {"authors": ["L. Floridi"], "title": "The method of levels of abstraction", "venue": "Minds and machines 18(3), 303\u2013 329", "year": 2008}, {"authors": ["G. Friedrich", "M. Zanker"], "title": "A taxonomy for generating explanations in recommender systems", "venue": "AI Magazine 32(3), 90\u201398", "year": 2011}, {"authors": ["J. Garc\u0131a", "F. Fern\u00e1ndez"], "title": "A comprehensive survey on safe reinforcement learning", "venue": "Journal of Machine Learning Research 16(1), 1437\u20131480", "year": 2015}, {"authors": ["R. Guidotti", "A. Monreale", "S. Ruggieri", "F. Turini", "F. Giannotti", "D. Pedreschi"], "title": "A survey of methods for explaining black box models", "venue": "ACM computing surveys (CSUR) 51(5), 93", "year": 2019}, {"authors": ["D. Gunning"], "title": "Explainable artificial intelligence (xai). Defense Advanced Research Projects Agency (DARPA), nd Web 2 (2017), https://www.darpa.mil/ attachments/XAIProgramUpdate.pdf", "year": 2017}, {"authors": ["T. Hailesilassie"], "title": "Rule extraction algorithm for deep neural networks: A review", "venue": "arXiv preprint arXiv:1610.05267", "year": 2016}, {"authors": ["J. Han", "Y. Fu"], "title": "Discovery of multiple-level association rules from large databases", "venue": "VLDB. vol. 95, pp. 420\u2013431. Citeseer", "year": 1995}, {"authors": ["J. Han", "Y. Fu"], "title": "Mining multiple-level association rules in large databases", "venue": "IEEE Transactions on knowledge and data engineering 11(5), 798\u2013805", "year": 1999}, {"authors": ["B. Hayes", "B. Scassellati"], "title": "Autonomously constructing hierarchical task networks for planning and human-robot collaboration", "venue": "2016 IEEE International Conference on Robotics and Automation (ICRA). pp. 5469\u20135476. IEEE", "year": 2016}, {"authors": ["B. Hayes", "J.A. Shah"], "title": "Improving robot controller transparency through autonomous policy explanation", "venue": "2017 12th ACM/IEEE International Conference on Human-Robot Interaction (HRI. pp. 303\u2013312. IEEE", "year": 2017}, {"authors": ["J. Hipp", "U. G\u00fcntzer", "G. Nakhaeizadeh"], "title": "Algorithms for association rule mininga general survey and comparison", "venue": "SIGKDD explorations 2(1), 58\u201364", "year": 2000}, {"authors": ["S.H. Huang", "D. Held", "P. Abbeel", "A.D. Dragan"], "title": "Enabling robots to communicate their objectives", "venue": "Autonomous Robots 43(2), 309\u2013326", "year": 2019}, {"authors": ["G. Katz", "C.W. Barrett", "D.L. Dill", "K. Julian", "M.J. Kochenderfer"], "title": "Reluplex: An efficient SMT solver for verifying deep neural networks (2017)", "year": 2017}, {"authors": ["R.B. Kearfott"], "title": "Interval computations: Introduction, uses, and resources", "venue": "Euromath Bulletin 2(1), 95\u2013112", "year": 1996}, {"authors": ["S.H. Kellert"], "title": "In the wake of chaos: Unpredictable order in dynamical systems", "venue": "University of Chicago press", "year": 1993}, {"authors": ["J. Kim", "A. Rohrbach", "T. Darrell", "J. Canny", "Z. Akata"], "title": "Textual explanations for self-driving vehicles", "venue": "Proceedings of the European conference on computer vision (ECCV). pp. 563\u2013578", "year": 2018}, {"authors": ["J. Kim", "A. Rohrbach", "T. Darrell", "J.F. Canny", "Z. Akata"], "title": "Textual explanations for self-driving vehicles (2018)", "year": 2018}, {"authors": ["A. Koul", "A. Fern", "S. Greydanus"], "title": "Learning finite state representations of recurrent policy networks", "year": 2019}, {"authors": ["R. Levien", "S. Tan"], "title": "Double pendulum: An experiment in chaos", "venue": "American Journal of Physics 61(11), 1038\u20131044", "year": 1993}, {"authors": ["Z.C. Lipton"], "title": "The mythos of model interpretability", "venue": "arXiv preprint arXiv:1606.03490", "year": 2016}, {"authors": ["B. Liu", "W. Hsu", "Y. Ma"], "title": "Mining association rules with multiple minimum supports", "venue": "Proceedings of the fifth ACM SIGKDD international conference on Knowledge discovery and data mining. pp. 337\u2013341", "year": 1999}, {"authors": ["T. Miller"], "title": "Explanation in artificial intelligence: Insights from the social sciences", "venue": "Artificial Intelligence", "year": 2018}, {"authors": ["T. Miller", "P. Howe", "L. Sonenberg"], "title": "Explainable ai: Beware of inmates running the asylum or: How i learnt to stop worrying and love the social and behavioural sciences", "venue": "arXiv preprint arXiv:1712.00547", "year": 2017}, {"authors": ["A. Mohseni-Kabir", "C. Rich", "S. Chernova", "C.L. Sidner", "D. Miller"], "title": "Interactive hierarchical task learning from a single demonstration", "venue": "Proceedings of the Tenth Annual ACM/IEEE International Conference on Human-Robot Interaction. pp. 205\u2013212. ACM", "year": 2015}, {"authors": ["R.E. Moore"], "title": "Interval analysis, vol", "venue": "4. Prentice-Hall Englewood Cliffs, NJ", "year": 1966}, {"authors": ["S. Muggleton"], "title": "Inductive logic programming: issues, results and the challenge of learning language in logic", "venue": "Artificial Intelligence 114(1-2), 283\u2013296", "year": 1999}, {"authors": ["S. Neema"], "title": "Assured autonomy (2017), https://www.darpa.mil/attachments/ AssuredAutonomyProposersDay_Program%20Brief.pdf", "year": 2017}, {"authors": ["S. Palaniappan", "C. Ling"], "title": "Clinical decision support using olap with data mining", "venue": "International Journal of Computer Science and Network Security 8(9), 290\u2013296", "year": 2008}, {"authors": ["A. Papadimitriou", "P. Symeonidis", "Y. Manolopoulos"], "title": "A generalized taxonomy of explanations styles for traditional and social recommender systems", "venue": "Data Mining and Knowledge Discovery 24(3), 555\u2013583", "year": 2012}, {"authors": ["V. Perera", "S.P. Selveraj", "S. Rosenthal", "M. Veloso"], "title": "Dynamic generation and refinement of robot verbalization", "venue": "2016 25th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN). pp. 212\u2013218. IEEE", "year": 2016}, {"authors": ["L. Pulina", "A. Tacchella"], "title": "An abstraction-refinement approach to verification of artificial neural networks", "venue": "International Conference on Computer Aided Verification. pp. 243\u2013257. Springer", "year": 2010}, {"authors": ["M.T. Ribeiro", "S. Singh", "C. Guestrin"], "title": "why should I trust you?\u201d: Explaining the predictions of any classifier", "year": 2016}, {"authors": ["A. Richardson", "A. Rosenfeld"], "title": "A survey of interpretability and explainability in human-agent systems", "venue": "XAI Workshop on Explainable Artificial Intelligence. pp. 137\u2013143", "year": 2018}, {"authors": ["M. Roberts", "I. Monteath", "R. Sheh", "D. Aha", "P. Jampathom", "K. Akins", "E. Sydow", "V. Shivashankar", "C. Sammut"], "title": "What was i planning to do", "venue": "ICAPS workshop on explainable planning. pp. 58\u201366", "year": 2018}, {"authors": ["S. Rosenthal", "J. Biswas", "M. Veloso"], "title": "An effective personal mobile robot agent through symbiotic human-robot interaction", "venue": "Proceedings of the 9th International Conference on Autonomous Agents and Multiagent Systems: volume 1Volume 1. pp. 915\u2013922. International Foundation for Autonomous Agents and Multiagent Systems", "year": 2010}, {"authors": ["J. Schulman", "F. Wolski", "P. Dhariwal", "A. Radford", "O. Klimov"], "title": "Proximal policy optimization algorithms", "venue": "arXiv preprint arXiv:1707.06347", "year": 2017}, {"authors": ["S. Sreedharan", "M.P. Madhusoodanan", "S. Srivastava", "S. Kambhampati"], "title": "Plan explanation through search in an abstract model space pp", "venue": "67\u201375", "year": 2018}, {"authors": ["R. Srikant", "R. Agrawal"], "title": "Mining generalized association rules", "year": 1995}, {"authors": ["I. Standardization"], "title": "Iso/iec 7498-1: 1994 information technology\u2013open systems interconnection\u2013basic reference model: The basic model", "venue": "International Standard ISOIEC 74981, 59", "year": 1996}, {"authors": ["R.D. Tennent"], "title": "The denotational semantics of programming languages", "venue": "Commun. ACM 19(8), 437\u2013453", "year": 1976}, {"authors": ["S. Thrun"], "title": "Extracting rules from artificial neural networks with distributed representations", "venue": "Advances in neural information processing systems. pp. 505\u2013512", "year": 1995}, {"authors": ["J. Vanschoren", "J.N. van Rijn", "B. Bischl", "L. Torgo"], "title": "Openml: Networked science in machine learning", "venue": "SIGKDD Explorations 15(2), 49\u201360", "year": 2013}, {"authors": ["M.M. Veloso", "J. Biswas", "B. Coltin", "S. Rosenthal"], "title": "Cobots: Robust symbiotic autonomous mobile service robots", "venue": "IJCAI. p. 4423", "year": 2015}, {"authors": ["E. Ventocilla", "T. Helldin", "M. Riveiro", "J. Bae", "V. Boeva", "G. Falkman", "N. Lavesson"], "title": "Towards a taxonomy for interpretable and interactive machine learning", "venue": "XAI Workshop on Explainable Artificial Intelligence. pp. 151\u2013157", "year": 2018}, {"authors": ["E. Walter", "L. Jaulin"], "title": "Guaranteed characterization of stability domains via set inversion", "venue": "IEEE Transactions on Automatic Control 39(4), 886\u2013889", "year": 1994}, {"authors": ["S. Wang", "K. Pei", "J. Whitehouse", "J. Yang", "S. Jana"], "title": "Formal security analysis of neural networks using symbolic intervals", "venue": "USENIX Security Symposium, USENIX Security 2018,", "year": 2018}, {"authors": ["A.T.M. Wasylewicz", "Scheepers-Hoeks", "A.M.J.W."], "title": "Clinical Decision Support Systems, pp", "venue": "153\u2013169. Springer International Publishing, Cham", "year": 2019}, {"authors": ["H.M. Wellman", "K.H. Lagattuta"], "title": "Theory of mind for learning and teaching: The nature and role of explanation", "venue": "Cognitive development 19(4), 479\u2013497", "year": 2004}, {"authors": ["W. Wen", "J. Callahan"], "title": "Neuralware engineering: develop verifiable ann-based systems", "venue": "Proceedings IEEE International Joint Symposia on Intelligence and Systems. pp. 60\u201366. IEEE", "year": 1996}, {"authors": ["W. Wen", "J. Callahan", "M. Napolitano"], "title": "Towards developing verifiable neural network controller", "venue": "Department of Aerospace Engineering, NASA/WVU Software Research Laboratory", "year": 1996}, {"authors": ["W. Wen", "M. Napolitano", "J. Callahan"], "title": "Verifying stability of dynamic softcomputing systems", "year": 1997}, {"authors": ["W. Xiang", "T.T. Johnson"], "title": "Reachability analysis and safety verification for neural network control systems", "venue": "CoRR abs/1805.09944", "year": 2018}, {"authors": ["M. Yasmin", "M. Sharif", "S. Mohsin"], "title": "Neural networks in medical imaging applications: A survey", "venue": "World Applied Sciences Journal 22(1), 85\u201396", "year": 2013}], "sections": [{"heading": "1 Problem Overview", "text": "Explainability and safety in AI\u2014particularly in systems tuned or synthesized using Machine Learning (ML)\u2014are an increasing subject of academic and public concern. As machine learning continues to grow in success and adoption by wide-ranging industries, the impact of these algorithms\u2019 behavior on people\u2019s lives is becoming highly non-trivial. Unfortunately, many of the most performant contemporary ML algorithms\u2014neural networks (NNs) in particular\u2014are widely considered black-boxes, with the method by which they perform their\n? This material is based upon work supported by the United States Air Force and DARPA under Contract No. FA8750-18-C-0092. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the United States Air Force and DARPA. We would like to thank David Held for pointing us to the rl-baselines-zoo repository. Also, we would like to thank Nicholay Topin for supporting our spirits at some key junctures of this work.\nar X\niv :2\n00 6.\n12 45\n3v 3\n[ cs\n.A I]\n2 5\nO ct\nduties not being amenable to direct human comprehension. The inability to understand learned components as thoroughly as more traditional software poses serious obstacles to their adoption [6,1,11,27,78,26,79,46] due to safety concerns, difficulty to debug and maintain, and explicit legal requirements, such as the right to an explanation legislation adopted by the European Union[22]. Symbiotic human-machine interactions can lead to safer and more robust agents, but this task requires effective and versatile communication [68,60].\nInterpretability of learned systems has been studied in the context of computer science intermittently since at least the late 1980s, particularly in the area of rule extraction (e.g., [5]), inductive logic programming [49], association rule learning [3] and its predecessors (e.g., [55,23]), and in adaptive/non-linear control analysis (e.g., [16]). Interpretability is also motivation to fundamental formal analysis approaches (e.g., [13,75,76,70,37,48]), but gained more significant attention only recently owing in part to its increased impact on daily life [1] with initiatives, such as the DARPA XAI project [28] and the DARPA Assured Autonomy Program [50,51], the IJCAI-XAI workshop [4], and the ICAPS XAIP Workshop [77].\nDespite this attention, however, most explanatory-systems developed for ML are hard-coded to provide a single type of explanation with descriptions at a certain fixed level of abstraction and a fixed type of guarantee about the system behavior, if any. This not only prevents the explanations generated from being digestible by multiple audiences (the end-user, the intermediate engineers who are non-experts in the ML component, and the ML-engineer for instance), but in fact limits the use by any single audience since the levels of abstraction and formal guarantees needed are situation and goal specific, not just a function of the recipient\u2019s background. When using a microscope, one varies between low- and high- magnification in order to find what they are looking for and explore samples; these same capabilities are desirable for XAI for much the same reasons. For example, when determining the reaction of an autonomous vehicle when a person steps in front of it, most audiences may prefer to ask generally (e.g., \u201cWhen you detect a person in front of you, what do you do?\u201d) and receive a break-down of qualitatively different behaviors for different situations, such as braking when traveling slowly enough, and doing a sharp swerve when traveling too fast to brake. Sometimes, however, an engineer might still want to specify precise starting locations of the car and person and have the car report exact motor commands so to ensure actuators are compliant; the context of use and the audience-type determine which level of abstraction is best, and supporting multiple types of abstractions in turn supports more use-cases and audiences. Further, the explanations for such a component need to range from formal guarantees to rough tendencies\u2014one might want to formally guarantee that the car will avoid collisions always, while it might be sufficient that it usually (but perhaps not always) drives slowly when its battery is low.\nThe divide between formal and probabilistic explanations also relates to events that are imaginable versus events that may actually occur; formal methods may check every point in a space for conformance to a condition, but if bad\nbehavior only occurs on measure-zero sets, the system would be safe while not being provably so in formalizations lacking knowledge of statistics (e.g., when a car must keep distance >10 cm to obstacles, formally we can get arbitrarily close but not equal; in practice, the difference with \u226510 cm might be irrelevant). Explainable ML systems should enable these sorts of search and smooth variation in need\u2014but at the moment they do not in general.\nTo address these challenges, we propose a combination of formal verification, heuristic search, and user interaction capable of providing explanations for currently ubiquitous ML methods\u2014such as feed-forward neural networks (FFNNs) and high-dimensional polynomial kernels \u2014at varying levels of abstraction that can be curtailed to users\u2019 preferences, with tunable fidelity spanning from formal guarantees to general tendencies of behavior. We introduce Fanoos1, an algorithm and framework for querying broad classes of learned models."}, {"heading": "2 The Fanoos Approach", "text": "On a high-level, illustrated in the query process in Fig. 2, Fanoos is an interactive system that allows users to pose a variety of questions grounded in a domain specification (e.g., what environmental conditions cause a robot to swerve left), receive replies from the system, and, as an inner-loop steering the heuristic search and analysis of the learned system, request that explanations be made more or less abstract. An illustration of the process and component interactions can be found in Appendix A, with a fuller example of interaction located in Appendix B.\nCrucially, Fanoos provides explanations of high fidelity (being a decompositional approach; see Section 3) while considering whether the explanation should be formally sound or probabilistically reasonable (which removes the \u201cnoise\u201d incurred by measure-zero sets that can plague formal descriptions). To this end, we combine formal verification techniques, interactive systems, and heuristic search over knowledge domains in response to user questions and requests."}, {"heading": "2.1 Knowledge Domains and User Questions", "text": "In the following discussion, let L be the learned system under analysis (which we will assume is piece-wise continuous), q be the question posed by the user, SI be the (bounded) input-space to L, and So be the output space to L, SIO be the joint of the input and output space, and r be the response given by the system. In order to formulate question q and response r, a library listing basic domain information D is provided to Fanoos; D lists what SI and SO are and provides a set of predicates, P , expressed over the domain symbols in SIO = SI \u222a SO, i.e., for all p \u2208 P, freeVars(p) \u2286 varNames(SIO).\n1 ( Fanoos ) when do you usual ly and ( outputtorque low , \u21aa\u2192 s t a t e v a l u e e s t i m a t e h i g h ) ?\nListing 1.1. Question to illuminate input space SI\n1 \u201cFanoos\u201d means lantern in Farsi. Our approach shines a light on black-box AI.\nFor queries that formally guarantee behavior (see the first three rows in Table 1), we require that the relevant predicates in P be able to expose their internals as first-order formulas; this enables us to guarantee they are satisfied over all members of a given set2 via typical SAT-solvers (such as Z3 [17]). The other query types require only being able to evaluate question q on a variable assignment provided. The members of P can be generated in a variety of ways, e.g., by forming most predicates through procedural generation and then using a few hand-tailored predicates to capture particular cases 3. Further, since the semantics of the predicates are grounded, they have the potential to be generated from demonstration.\n2 The box abstractions we introduce in a moment to be more precise. 3 For example, operational definitions of \u201chigh\u201d, \u201clow\u201d, etc., might be derived from\nsample data by setting thresholds on quantile values\u2014e.g., 90% or higher might be considered \u201chigh\u201d."}, {"heading": "2.2 Reachability Analysis of L", "text": "Having established what knowledge the system is given, we proceed to explain our process. First, users select a question type qt and the content of the question qc to query the system. That is, q = (qt, qc), where qt is a member of the first column of Table 1 and qc is a sentence in disjunctive normal form over a subset of P that obeys the restrictions listed in Table 1. To ease discussion, we will refer to variables and sets of variable-assignments that p accepts (AC) and those that p illuminates (IL), with the intuition being that the user wants to know what configuration of illuminated variables result in the variable configurations accepted by qc; see Table 1 for example queries. To provide a convenient user experience, auto-completion for question-asking is available, limiting completions to those that obey restrictions imposed by Table 1.\nWith question q provided, we analyze the learned system L to find subsets in the inputs SI and outputs So that agree with configuration qc and may overapproximate the behavior of L. Specifically, we use CEGAR [14,13] with boxes (hyper-cubes) as abstractions and a random choice between a bisection and trisection along the longest axis as the refinement process to find the collect of box tuples, B, specified below:\nB = {(B(i)I ,B (i) O ) \u2208 Boxes(SI)\u00d7 Boxes(SO) |(( ACq(B (i) I ) \u2227 ILq(B (i) O ) ) \u2228 ( ACq(B (i) O ) \u2227 ILq(B (i) I ) ) \u2228(\nACq(B (i) IO) \u2227 ILq(B (i) IO) )) \u2227B(i)O \u2287 L(B (i) I )}\nwhere Boxes(X) is the set of boxes over space X. See Fig. 1 for an example drawn from analysis conducted on the model in Section 4.1. For feed-forward neural nets with non-decreasing activation functions, B may be found by covering the input space, propagating boxes through the network, testing membership to B of the resulting input- and output-boxes, and refining the input mesh as needed over input-boxes that produce output-boxes overlapping with B. The exact size of the boxes found by CEGAR are determined by a series of hyper-parameters, such as the maximum number of refinement iterations or the minimal size abstractions one is willing to consider; for details on such hyper-parameters of CEGAR and other bounded-model checking approaches the interested reader may refer to [14,13,9]."}, {"heading": "2.3 Generating Descriptions", "text": "Having generated B, we produce an initial response, r0, to the user\u2019s query in three steps as follows: (1) for each member of B, we extract the box tuple members that were illuminated by q (in the case where SIO is illuminated, we produce a joint box over both tuple members), forming a set of joint boxes, B\u2032; (2) next, we heuristically search over P for members that describe B\u2032 and compute a set of predicates covering all boxes; (3) finally, we format the box\ncovering for user presentation. A sample result answer is shown in listing 1.2, and details on steps (2) and (3) how to produce it follow below.\n1 (0 .45789160 , 0 .61440409 , \u2019 x Near Normal Levels \u2019 ) 2 (0 .31030792 , 0 .51991449 , \u2019 pole2Angle rateOfChange Near Normal \u21aa\u2192 Levels \u2019 ) 3 (0 .12008841 , 0 .37943400 , \u2019 pole1Angle rateOfChange High \u2019 ) 4 (0 .06128723 , 0 .22426058 , \u2019 pole2Angle Low \u2019 )\nListing 1.2. Initial answer to question in listing 1.1\nProducing a Covering of B\u2032 Our search over P for members covering B\u2032 is largely based around the greedy construction of a set covering using a carefully designed candidate score.\nFor each member b \u2208 B\u2032 we want to find a set of candidate predicates capable of describing the box and for which we would like to form a larger covering. We find a subset of Pb \u2286 P that is consistent with b in that each member of Pb passes the checks called-for by qt when evaluated on b (see the Description column of Table 1). This process is made fast by a feasibility check of each member of P on a vector randomly sampled from b, prior to the expensive check for inclusion in Pb. Having Pb, we filter the candidate set further for those members of Pb that appear most specific to b; notice that in our setting, where predicates of varying abstraction level co-mingle in P , it would be of no surprise that Pb contains many members that only loosely fit b. This subset of Pb, P \u2032 b, is formed by sampling\noutside of b at increasing radii (in the `\u221e sense), collecting those members of Pb that fail to hold true at the earliest radius (see the pseudo-code in Appendix D for further details). Importantly, looking ahead to forming a full covering of B, if none of the predicates fail prior to exhausting4 this sampling, we report P \u2032b as empty, allowing us to handle b downstream; this avoids having \u201cdifficult\u201d boxes force us to report weak predicates, that would \u201cwash out\u201d more granular details. Further notice that if we want a subset of Pb that was less specific to b than P \u2032b, we simply perform the CEGAR-analysis so to produce larger boxes\u2014in other words, we try to be specific at this phase under the assumption that the granularity we wanted to describe has been determined earlier.\nWe next leverage the P \u2032b sets to construct a covering of B \u2032, proceeding in an iterative greedy fashion. Specifically, if Ci is the covering established at iteration i, we increment to Ci+1 as follows:\nCi+1 = (Ci \u222a {pi+1})\\ {p\u2032 \u2208 Ci | {b \u2208 B\u2032 | p\u2032 \u2208 P \u2032b} \u2286 {b \u2208 B\u2032 | pi+1 \u2208 P \u2032b}} with\npi+1 = argmaxp\u2208P\\CiCoverScore(p, Ci) where\nCoverScore(p, Ci) =\u2211 b\u2208B\u2032 1(|uncoveredVars(b, Ci) \u2229 freeVars(p)| > 0)1(p \u2208 P \u2032b)\nand uncoveredVars is the set of variables in b that are not constrained by Ci\u2229Pb; since the boxes are multivariate and our predicates typically only constrain a subset of the variables, we select predicates based on how many boxes would have open variables covered by them. Let CF be the final covering produced by this process. Notice that CF is not necessarily an approximately minimal covering of B with respect to members of P\u2014by forcing p \u2208 P \u2032b when calculating CoverScore, we enforce additional specificity criteria that the covering should adhere to.\nAfter forming CF , any boxes that fail to be covered even partially (for example, because Pb or P \u2032 b happen to be empty) are reported with a box-range predicate: atomic predicates that simply list the variable range in the box. In other words, we extend CF to a set C \u2032 F by introducing new predicates specific to each completely uncovered box so that C \u2032F does cover all boxes in B \u2032.\nCleaning and Formatting Output for User Having produced C \u2032F , we collect the content of the covering into a series of conjuncts and disjuncts. Let\nd0 = \u22c3 b\u2208B\u2032 {c \u2286 C \u2032F | \u2200p \u2208 c. (p covers b)} .\n4 The operational meaning of \u201cexhausting\u201d, as well as the radii sampled, are all parameters stored in the state.\nThe information needed to compute d0 is easily gathered from bookkeeping while computing C \u2032F . Ultimately, the members of d0 are conjunctions of predicates\n5, with their membership to the set being a disjunction. Prior to actually converting d0 to disjunctive normal form, however, we do the trivial filter of removing any c \u2208 d0 such that there is c\u2032 \u2208 d0 where c\u2032 ( c, since the set over which and(c\u2032) holds is a superset of where and(c) holds. While there are a variety of methods to perform this filter, in practice d0 is sufficiently small at this stage to allow full member-to-member comparisons. Call d0 post-filtering d \u2032 0.\nFinally, r0 is constructed by listing each c that exists in d \u2032 0 sorted by two relevance scores: first, the proportion of the volume in B\u2032 uniquely covered by c, and second by the proportion of total volume c covers in B\u2032. These sorting-scores can be thought of similarly to recall measures. Specificity is more difficult to tackle, since it would require determining the volume covered by each predicate (which may be an arbitrary first-order formula) across the box bounding the universe, not just the hyper-cubes at hand; this can be approximated for each predicate using set-inversion, but requires non-trivial additional computation for each condition."}, {"heading": "2.4 User Feedback and Revaluation", "text": "Based on the initial response r0, users can request a more abstract or less abstract explanation. We view this alternate explanation generation as another heuristic search, where the system searches over a series of states to find those that are deemed acceptable by the user. The states primarily include algorithm hyper-parameters, the history of user interaction including the provided explanations, the question to be answered, and the set B. Abstraction and refinement operators take a current state and produce a new one, often by adjusting the system hyper-parameters and recomputing B. This state-operator model of user response allows for rich styles of interaction with the user, beyond and alongside of the three-valued responses of acceptance, increase, or decrease of the abstraction level show in listing 1.3.\n1 (0 .11771033 , 0 .12043966 , \u2019And( pole1Angle Near Normal Levels , \u21aa\u2192 pole1Angle rateOfChange Near Normal Levels , pole2Angle \u21aa\u2192 High , pole2Angle rateOfChange Low , vx Low) \u2019 ) 2 (0 .06948142 , 0 .07269412 , \u2019And( pole1Angle High , \u21aa\u2192 pole1Angle rateOfChange Near Normal Levels , \u21aa\u2192 pole2Angle rateOfChange High , vx Low , x Near Normal \u21aa\u2192 Leve l s ) \u2019 ) 3 (0 .04513659 , 0 .06282974 , \u2019And( endOfPole2 x Near Normal \u21aa\u2192 Levels , pole1Angle Low , pole1Angle rateOfChange High , \u21aa\u2192 pole2Angle High , pole2Angle rateOfChange Near Normal \u21aa\u2192 Levels , x High ) \u2019 ) q\n5 From here-on, when we refer to a conjuct, we assume it is not in reference to the degenerate 1-ary or 0-ary cases.\n4 type l e t t e r f o l l owed by ente r key : b \u2212 break and ask a \u21aa\u2192 d i f f e r e n t quest ion ,\nListing 1.3. Response to \u201cless abstract\u201d than listing 1.2\nFor instance, a history-travel operator allows the state (and thus r) to return to an earlier point in the interaction process, if the user feels that response was more informative; from there, the user may investigate an alternate path of abstractions. Other operators allow for refinements of specified parts of explanations as opposed to the entire reply; the simplest form of this is by regenerating the explanation without using a predicate that the user specified be ignored. To illustrate details, we describe abstraction operators for increasing or decreasing the abstraction level in the appendix. As a fundamental underlying concept, these operators use a notion of abstractness, as discussed below."}, {"heading": "2.5 Capturing the Concept of Abstractness", "text": "The exact bounds that delimit abstractness and what makes one thing more or less abstract than another in the lay-sense are often difficult to capture. We consider abstractness a diverse set of relations that subsume the part-of-whole relation, and thus also generally includes the subset relation. For our purposes, defining this notion is not necessary, since we simply wish to utilize the fact of its existence. We understand abstractness to be a semantic concept that shows itself by producing a partial ordering over semantic states (their \u201cabstractness\u201d level) which is in turn reflected in the lower-order semantics of the input-output boxes, and ultimately is reflected in our syntax via explanations of different granularity. Discussions of formalisms most relavent to computer science can be found in [15,65,64] 6 and an excellent discussion of the philosophical underpinnings and extensions can be found in [24].\nIn this work, the primary method of producing explanations at desired levels of abstraction is entirely implicit\u2014that is, without explicitly tracking what boxes or predicates are considered more or less abstract. This leverages the notion of abstractness inherent in the semantics and refinements of CEGAR in operators that adjust abstraction by adjusting the input-space mesh size, which extends to the verbalization process through the computed covering of boxes.\nOn the opposite end of the spectrum is explicit expert tuning of abstraction orderings to be used in the system. Fanoos can easily be adapted to leverage expert-labels (e.g., taxonomies as in [63], or simply type/grouping-labels without explicit hierarchical information) to preference subsets of predicates conditionally on user-responses, but for the sake of this paper, we reserve agreement with expert-labels as an independent metric of performance in our evaluation, prohibiting the free use of such knowledge in the algorithm. Further, by forgoing direct supervision, we demonstrate that the concept of abstractness is recoverable from the semantics and structure of the problem itself.\n6 [15] features abstraction in verification, [65] features abstraction at play in interpretating programs, and [64] is an excellent example of interfaces providng a notion of abstractness in network communications."}, {"heading": "3 Related Work and Discussion", "text": "Many methods are closely related to XAI, stemming from a diverse body of literature and various application domains, e.g., [16,5,3,32,62,57,36,73,8]. Various taxonomies of explanation families have been proposed [45,6,39,43,5,1,27,10,25,58,69,12,59,53,29,11], with popular divisions being (1) between explanations that leverage internal mechanics of systems to generate descriptions (decompositional approaches) versus those that exclusively leverage input-output relations (pedagogical) 7, (2) the medium that comprises the explanation (such as with most-predictive-features [57], summaries of internal states via finite-state-machines [41], natural language descriptions [32,40] or even visual representations [35,40]), (3) theoretical criteria for a good explanation (see, for instance, [46]), and (4) specificity and fidelity of explanation. Overall, most of these approaches advocate for producing human-consumable information\u2014whether it be in natural language, logic, or visual plots\u2014conveying the behavior of the learned system in situations of interest.\nRule-based systems such as expert systems, and work in the (high-level) planning community have a long history of producing explanations in various forms; notably, hierarchical planning [32,47] naturally lends itself to explanations of multiple abstraction levels. All these methods, however, canonically work on the symbolic level, making them inapplicable to most modern ML methods. High fidelity, comprehensible rules describing data points can also be discovered with weakly-consistent inductive logic programming [49] or association rule learning [34,3] typical in data-mining. However, these approaches are typically pedagogical, not designed to leverage access to the internals of the system, and do not offer a variety of descriptions abstractions or strengths. While some extentions of association rule learning (e.g., [63,31,30]) do consider descriptions at various abstraction levels, they only understand abstractness syntatically, requiring complete taxonomies be provided explicitly and a priori ; further, such approaches do not attempt to describe the full datasets they derive from, but only some aspects of them - while support and confidence thresholds may be set sufficaintly low to ensure each transaction is described by at least one rule, the result would be a deluge of highly redundant, low-precision rules lacking most practical value (this may be considered the most extreme case of the \u201drare itemset problem\u201d as dicussed in [44]). Our approach, by contrast leverages semantic information, attempts to efficiently describe all relevant data instances, and produces descriptions that are necessarly reflective of the mechanism under study. Decision support systems [52,72,20,21,19] typically allow users to interactively investigate data, with operations such as drill-ups in OLAP (OnLine Analytical Processing) cubes analogous to a simple form of abstraction in that setting. The typical notions of analysis, however, largely operate by truncating portions of data distributions and running analytics packages on selected subregions at user\u2019s requests, failing to leverage access to the data-generation mechanism when\n7 We have also found this to be referred to as \u201dintrospective\u201d explanations versus \u201crationalizations\u201d, such as in [40]\npresent, and failing to provide explicit abstractions or explicit guarantees about the material it presents.\nMore closely related to our work are approaches to formally analyze neural networks to extract rules, ensure safety, or determine decision stability, which we discuss in more detail below. Techniques related to our inner-loop reachability analysis have been used for stability or reachability analysis in systems that are otherwise hard to analyze analytically. Reachability analysis for FFNNs based on abstract interpretation domains, interval arithmetic, or set inversion has been used in rule extraction and neural net stability analysis [5,18,66,74] and continues to be relevant, e.g., for verification of multi-layer perceptrons [56], estimating the reachable states of closed-loop systems with multi-layer perceptrons in the loop [78], estimating the domain of validity of neural networks [2], and analyzing security of neural networks [71]. While these works provide methods to extract descriptions that faithfully reflect behavior of the network, they do not generally ensure descriptions are comprehensible by end-users, do not explore the practice of strengthening descriptions by ignoring the effects of measure-zero sets, and do not consider varying description abstraction.\nThe high-level components of our approach can be compared to [33], where hand-tunable rule-based methods with natural language interfaces encapsulate a module responsible for extracting information about the ML system, with explanation generation in part relying on minimal set-covering methods to find predicates capturing the model states. Extending this approach to generate more varying-resolution descriptions, however, does not seem like a trivial endeavor, since (1) it is not clear that the system can appropriately handle predicates that are not logically independent, and expecting experts to explicitly know and encode all possible dependencies can be unrealistic, (2) the system described does not have a method to vary the type of explanation provided for a given query when its initial response is unsatisfactory, and (3) the method produces explanations by first learning simpler models via MDPs. Learning simpler models by sampling behavior of more sophisticated models is an often-utilized, widely applicable method to bootstrap human understanding (e.g. [10,41,28]), but it comes at the cost of failing to leverage substantial information from the internals of the targeted learned system. Crucially, such a technique cannot guarantee the fidelity of their explanations in respect to the learned system being explained, in contrast to our approach.\nIn [54], the authors develop vocabularies and circumstance-specific human models to determine the parameters of the desired levels of abstraction, specificity and location in robot-provided explanations about the robot\u2019s specific, previous experiences in terms of trajectories in a specific environment, as opposed to the more generally applicable conditional explanations about the internals of the learned component generated by Fanoos. The particular notions of abstraction and granularity from multiple, distinct, unmixable vocabularies of [54] evaluate explanations in the context of their specific application and are not immediately applicable nor easily transferable to other domains. Fanoos, by contrast, does not require separate vocabularies and enables descriptions to include multiple\nabstraction levels (for example, mixing them as in the sentence \u201cHouse X and a 6m large patch on house Y both need to be painted\u201d).\nClosest in spirit to our work are planning-related explanations [62]8, providing multiple levels of abstraction with a user-in-the-loop refinement process, but with a focus on markedly different search-spaces, models of human interaction, algorithms for description generation and extraction, and experiments. Further, we attempt to tackle the difficult problem of extracting high-level symbolic knowledge from systems where such concepts are not natively embedded, in contrast to [62], who consider purely symbolic constructs.\nIn summary, current approaches focus on single aspects of explanations, fixed levels of abstraction, and inflexible guarantees about the explanations given. We argue that an interleaving between automated formal techniques, search heuristics, and user interaction is necessary to achieve the desired flexibility in explanations and the desired adjustable level of fidelity."}, {"heading": "4 Experiments and Results", "text": "In this section we discuss empirical demonstrations of Fanoos\u2019s ability to produce and adjust descriptions across two different domains. The code implementing our method, the models analyzed, the database of raw-results, and the analysis code used to generate the results presented will be released in the near future."}, {"heading": "4.1 Systems Analyzed", "text": "We analyze learned-systems from robotics control and more traditional ML predictions to demonstrate the applicability to diverse domains. Information on the predicates available for each domain can be found in Table 2.\nInverted Double Pendulum (IDP) The control policy for an inverted doublependulum\u2014similar to the basic inverted single pendulum example in control\u2014is\n8 We note that [62] was published after the core of our approach was developed; both of our thinkings developed independantly.\ntasked to keep a pole steady and upright; the pole consists of two segments, an under-actuated one attached to the end of the first, actuated one, where both are rotationally free in the same plane. This substantially complicates the controlproblem, since multi-pendulum systems are known to exhibit chaotic behavior [38,42]. The trained policy was taken from reinforcement learning literature9. The seven-dimensional observation space is [x, vx, pole2 endpoint, pole1angle, pole1angle rateOfChange, pole2angle, pole2angle rateOfChange], the bounding box for which can be found in Table 4 located in Appendix C. The output is a torque in [\u22121, 1]Nm and a state-value, which is not a priori bounded. Internal to the analyzed model is a transformation to convert the observations we provide to the form expected by the networks\u2014chiefly, the angles are converted to sines and cosines and observations are standardized in respect to the mean and standard deviation of the model\u2019s training runs. The values chosen for the input-space bounding box were inspired by the 5% and 95% quantile values over a test-run of the model in the rl-zoo framework. We expanded the input-box beyond this range to allow for the examination of rare inputs and observations the model was not necessarily trained on (e.g., while the train and test environments exit whenever the end of the second segment was below a certain height, in real applications, a user may want to know how the system attempts to recover in such an unseen situation). Whether or not the analysis stays in trained regions depends on the content of the user\u2019s question, which may either include or exclude these previously unseen regions.\nCPU Usage (CPU) We also analyze a more traditional ML algorithm for a non-control task \u2014 a polynomial kernel regression for modeling. Specifically, we use a three-degree fully polynomial basis over a 5-dimensional input space10 to linearly regress-out a three-dimensional vector. We trained our model using the publicly available data from [67]11. The observations are\n[lread, scall, sread, freemem, freeswap]\nand the response variables we predict are\n[lwrite, swrite, usr] .\nWe opted to analyze an algorithm with a degree-3 polynomial feature-set after normalizing the data in respect to the minimum and maximum of the training set since this achieved the highest performance\u2014over 90% accuracy\u2014on a 90%- 10% train-test split of the data compared to similar models with 1,2, or 4 degree kernels12. While the weights of the kernel may be interpreted in some sense (such\n9 https://github.com/araffin/rl-baselines-zoo, trained using PPO2 [61] which, as an actor-critic method, uses one network to produce the action, and one to estimate state-action values. 10 The input space includes cross-terms and the zero-degree element\u2014e.g., x2y and 1 are members. 11 Dataset available at https://www.openml.org/api/v1/json/data/562 12 Note that while we did do due-diligence in producing a performant and soundly-\ntrained model, the primary point is to produce a model worthy of analysis.\nas indicating which individual feature is, by itself, most influential), how the model behaves over the original input space is far from clear from these weights due to the joint correlation between the features and non-linear transformations of the input values. For analysis convenience, we transform the input space to be normalized in the same fashion as the observations the model was trained and evaluated on (alternatively, we could have kept the original observation space and put the normalization as part of the model-pipeline). The bounds of our input-space bounding-box were determined from the 5% and 95% quantiles for each input-variable over the full, normalized dataset; the exact values can be found in Table 5 located in Appendix C."}, {"heading": "4.2 Experiment Design", "text": "We tested Fanoos on the listed domains using synthetically generated interactions, with the goal of determining whether our approach properly changes the description abstractness in response to the user request. The domain and question type were randomly chosen among the options listed. The questions themselves were randomly generated to have up to four disjuncts, each with conjuncts of length no more than four; conjuncts were ensured to be distinct, and only predicates respecting the constraints of the question-type were used. Interaction with Fanoos post-question-asking was randomly selected from four alternatives (here, MA means \u201dmore abstract\u201d and LA means \u201dless abstract\u201d):\n\u2013 Initial refinement of 0.25 ; make LA; make MA; exit \u2013 Initial refinement of 0.125 ; make MA; make LA; exit \u2013 Initial refinement of 0.20 ; make LA; make MA; exit \u2013 Initial refinement of 0.10 ; make MA; make LA; exit\nFor the results presented here, over 130 of these interactions were held, resulting in several hundred question-answer-descriptions."}, {"heading": "4.3 Metrics", "text": "We evaluated the abstractness of each response Fanoos provided using several metrics across the following categories: reachability analysis, structural description, and expert labeling.\nReachability Analysis We compare the reachability analysis results when producing descriptions of different abstraction levels, which call for different levels of refinement. Specifically, we record statistics about the input-boxes generated during the CEGAR-like analysis, normalized to the input-space bounding box so that each axis is in [0, 1] to yield comparable results across domains. The metrics give a rough sense of the abstractness notion implicit in the size of boxes and how they relate to descriptions:\n\u2013 Volume of the box (product of its side lengths).\n\u2013 Sum of the box side lengths. Unlike the box volume, this measure is at least as much as the maximum side length. \u2013 Number of boxes used to form the description.\nThe volume and summed-side-lengths are distributions, reported in terms of the minimum, maximum, median, and sum of the values.\nDescription Structure Fanoos responds to the user with a weighed description in disjunctive normal form. This structure is summarized as follows to give a rough sense of how specific each description is by itself:\n\u2013 Number of disjuncts, including atomic predicates \u2013 Number of conjuncts, excluding atomic predicates13 \u2013 Number of named predicates: atomic user-defined predicates that occur anywhere in the description, i.e., excluding box-range predicates of conjuncts of atomic predicates. \u2013 Number of box-range predicates that occur anywhere (i.e., in conjuncts as well as stand-alone).\nThe Jaccard score and overlap coefficients below are used to measure similarity in the verbage used in two descriptions.\n\u2013 Jaccard score: general similarity between two descriptions, viewing the set of atomic predicates used in each description as a bag-of-words. \u2013 Overlap coefficient: measures whether one description is simply a more \u201cverbose\u201d variant of the other, in the sense that the set of atomic predicates of\none is a subset of the other using |S1\u2229S2|min(|S1|,|S2|) , where S1 and S2 are the sets of predicates used in the two descriptions.\nExpert Labeling As humans, we have a priori knowledge about which atomic predicates describe more abstract notions in the world than others, and as such can evaluate the responses based on usage of more vs. less abstract verbage. It is important to note that this approach\u2014on descriptions built from atomic predicates\u2014yields an informative approximation rather than a true measure of abstractness for the following reasons: it is not clear that the abstractness of a description\u2019s components translates in an obvious fashion to the abstractness of the whole (in a similar vein, we do not rule out the possibility that predicates of the same level in the partially ordered set of abstractness can be combined to descriptions of different abstractness14). This phenomenon becomes more pronounced in coarsely grained partitions, where nuances are hidden in the partitions. For simplicity we choose two classes, more abstract (MA) vs. less abstract (LA), in the measures below:\n13 By excluding atomic predicates, this provides some rough measure of the number of \u201dcomplex\u201d terms. 14 For example, just because two description use verbage from the same expert-labeled category of abstractness, it does not mean the two descriptions have the same level of abstractness.\n\u2013 Number of predicates accounting for multiplicity, i.e., if an atomic predicate q has label MA and occurs twice in a sentence, it contributes two to this score. \u2013 Number of unique predicates: e.g., if an atomic predicate q has label MA and occurs twice in a sentence, it contributes one to this score. \u2013 Prevalence: ratio of unique predicates to the total number of atomic predicates in a description. This measure is particularly useful when aggregating the results of multiple descriptions into one distribution, since the occurrence of predicates is statistically coupled with the length of descriptions; under a null hypothesis of random generation of content, one would expect longer sentences to contain more MA,LA predicates, but expect the proportion to remain constant.\nEach of the above measures have obvious counter-parts for predicates with MA/LA labels. We note that prevalence will not necessarily sum to 1, since box-range predicates are atomic predicates without either label."}, {"heading": "4.4 Results", "text": "Running the experiments described in Section 4.2, we collected a series of states and the summary statistics on them described in Section 4.3. Since we are chiefly interested in evaluating whether a description changes to reflect the abstraction requested by the user, we examine the relative change in response to user interaction. Specifically, for pre-interaction state St and post-interaction state St+1, we collect metrics m(St+1)\u2212m(St) for each domain-response combination. This same process is used for the Jaccard score and overlap coefficients, except the values in question are computed as m(St+1, St). Our results are summarized in Table 3 on the medians of these distributions.\nAs can be seen, the reachability and structural metrics follow the desired trends: when the user requests greater abstraction (MA), the boxes become larger, and the sentences become structurally less complex\u2014namely, they become shorter (fewer disjuncts), have disjuncts that are less complicated (fewer explicit conjuncts, hence more atomic predicates), use fewer unique terms overall (reduction in named predicates) and resort less often to referring to the exact values of a box (reduction in box-range predicates). Symmetric statements can be made for when requests for less abstraction (LA) are issued. From the overlap and Jaccard scores, we can see that the changes in response complexity are not simply due to increased verbosity\u2014simply adding or removing phrases to the descriptions from the prior steps\u2014but also the result of changes in the verbage used; this is appropriate since abstractness is not exclusively a function of description specificity.\nTrends for the expert labels are similar, though more subtle to interpret. We see that use of LA-related terms follows the trend of user requests with respect to multiplicative- and uniqueness-counts (increases for LA-requests, decreases for MA-requests), while being less clear with respect to prevalence (uniform 0 scores). For use of MA terms, we see that the prevalence is correlated with\nuser requests in the expected fashion (decrease on LA requests, increase on MA requests). Further, we see that this correlation is mirrored for the MA counts when taken relative to the same measures for LA terms. Specifically, when a user requests greater abstraction (MA), the counts for LA terms decrease far more than those of MA terms, and the symmetric situation occurs for requests of lower abstraction (LA), as expected. While they depict encouraging trends, we take these expert-label measures with caution, due to the fragility of reasoning about the complete description\u2019s abstractness based on its constituents (recall that the abstractness of a description is not necessarily directly linked to the abstractness of its components). Nevertheless, these results\u2014labelings coupled with the structural trends\u2014lend solid support to the notion that Fanoos can recover substantial elements of an expert\u2019s notions about abstractness by leveraging the grounded semantics of the predicates."}, {"heading": "5 Conclusions And Future Work", "text": "Fanoos is an explanation framework for ML components mixing technologies ranging from heuristic search to classic verification techniques. We have demonstrated that our approach is capable of producing and navigating explanations\nat multiple, adjustable levels of granularity and strength. We will continue to explore this direction\u2019s potential, and hope that the community finds inspiration in both the methodology and philosophical underpinnings presented here.\nAs future work, we are exploring the use of active learning leveraging user interactions to select from a collection of operators, with particular interest in bootstrapping the learning process using operationally defined oracles to approximate users. In addition to this, we plan to explore more advanced data-driven predicate generation to accelerate construction of knowledge bases; an early candidate is learning generalized Hough transforms [7] given their representational flexibility, amenability to human review, and intuitiveness of the extrapolations that may be necessary. Finally, this style of work lends itself to engineering improvements on the reachability computations curtailed to ML systems."}, {"heading": "A Fanoos Structural Overview", "text": "Fig. 2 illustrates the component interactions in Fanoos. Sections detailing the component are italicized. Components requiring user interaction are oval, internal modules are rectangular, and the knowledge database cylindrical."}, {"heading": "B Example User Interactions", "text": "To demonstrate typical user-interactions with our system, we present here a sample of manual interactions in the spirit of other systems (e.g., [4,6]). In the interest of space, we do not list the meaning of all individual predicates. For those interested, the definition is provided with the code forthcoming. In practice, if users want to know more about exactly what each predicate means operationally (e.g., the exact conditions that each predicate tests for), they can look it up in the domain specification15\u2014a large part of the point of this system is to provide functionality beyond just cross-referencing code.\nWhenever we insert a comment in the interface-trace that was not originally there, we put // at the beginning of the line. Notice that our code uses a Unixstyle interaction in the spirit of the more command, so not to flood the screen.\n15 This is easily facilitated by open-on-click hyperlinks and/or hover text.\nC Input-Space Bounding Box Values\nIn this section, we list the input-space bounding boxes used in our experiments. We list the values here up to four significant figures. Listings with further precision can be found in the code bases.\n1 pole2 endpoint is a delta-value with respect to x. That is, in the observation given to the model to standardize, we add x to the value reported for pole2 endpoint. This choice is motivated by the fact that the model was trained on the pole2 endpoint position being measured in free-space, despite the fact that sensible values for this in an observation are highly dependent on x, the horizontal position of the cart\u2019s center."}, {"heading": "D Pseudo-Code for Specific-Selection Subroutine", "text": "Pseudo-code for our method of finding the most-specific conditions for a box are in Algorithm 1. In our code, we used ` = c exp(\u03b1\u00d7c) where c = [1.0, 1.01, 1.05, 1.1, 1.2, 1.4, 1.8, 2.6] and \u03b1 is a non-negative real-valued parameter we store and manipulate in the state. Similarly, n is stored in the states.\ninput : box to fit, b; number of random samples to try, n; list of predicates to try, P ; a list of strictly increase real numbers of length starting with 1.0, ` output: A set of indices into P of the most specific predicates, s\n1 s\u2190 {}; 2 dimsCovered\u2190 {}; 3 bCenter \u2190 getBoxCenter(b); 4 bDim = getDimension(b); 5 for i\u2190 0 to length(`)\u2212 1 do 6 lowerR\u2190 `[i]; 7 upperR\u2190 `[i + 1]; 8 innerB \u2190 ((b\u2212 bCenter(b))\u00d7 lowerR) + bCenter(b); 9 outterB \u2190 ((b\u2212 bCenter(b))\u00d7 upperR) + bCenter(b);\n10 randomSamples\u2190 {}; 11 for j \u2190 1 to n do 12 randomSamples\u2190 randomSamples \u222a {getRandV ecBetweenBoxes(innerBox, outterBox)}; 13 end 14 for pIndex\u2190 0 to length(P ) do 15 if pIndex \u2208 s then 16 continue; 17 end 18 else if freeVars(P [pIndex]) \u2286 dimsCovered then 19 continue; 20 end 21 foreach v \u2208 randomSamples do 22 /* We evaluate the predicate at index pIndex on v to see\nif it returns false */\n23 if \u00acP [pIndex].eval(v) then 24 s\u2190 s \u222a {pIndex}; 25 dimsCovered\u2190 dimsCovered \u222a freeVars(P [pIndex]); 26 break; 27 end 28 end 29 end 30 if length(dimsCovered) == bDim then 31 return s; 32 end 33 end 34 return s;\nAlgorithm 1: Find Most Specific Consistent Predicates"}, {"heading": "E Further Materials", "text": "Further materials and pointers to them can be found in the file https://github .com/DBay-ani/FanoosFurtherMaterials/blob/master/manifest.xml . The size of this file is 9529 bytes, with a sha512 hash of bc471b50c47b115d653739480541 f7549ec82687a622cf302b382835a4d01c7a37ce8c3a23b2bf221af4d49b043a449b2d 6e5db2c2f87723e4d7021203ab8737 ."}], "title": "Fanoos: Multi-Resolution, Multi-Strength, Interactive Explanations for Learned Systems", "year": 2020}