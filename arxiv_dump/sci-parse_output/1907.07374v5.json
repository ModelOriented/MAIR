{"abstractText": "Recently, artificial intelligence and machine learning in general have demonstrated remarkable performances in many tasks, from image processing to natural language processing, especially with the advent of deep learning. Along with research progress, they have encroached upon many different fields and disciplines. Some of them require high level of accountability and thus transparency, for example the medical sector. Explanations for machine decisions and predictions are thus needed to justify their reliability. This requires greater interpretability, which often means we need to understand the mechanism underlying the algorithms. Unfortunately, the blackbox nature of the deep learning is still unresolved, and many machine decisions are still poorly understood. We provide a review on interpretabilities suggested by different research works and categorize them. The different categories show different dimensions in interpretability research, from approaches that provide \u201cobviously\u201d interpretable information to the studies of complex patterns. By applying the same categorization to interpretability in medical research, it is hoped that (1) clinicians and practitioners can subsequently approach these methods with caution, (2) insights into interpretability will be born with more considerations for medical practices, and (3) initiatives to push forward data-based, mathematicallyand technically-grounded medical education are encouraged.", "authors": [{"affiliations": [], "name": "Erico Tjoa"}], "id": "SP:10b01da129edee65151190fbc52591bf0664b151", "references": [{"authors": ["Eun-Jae Lee", "Yong-Hwan Kim", "Namkug Kim", "Dong-Wha Kang"], "title": "Deep into the brain: Artificial intelligence in stroke imaging", "venue": "Journal of Stroke, 19:277\u2013285,", "year": 2017}, {"authors": ["Olaf Ronneberger", "Philipp Fischer", "Thomas Brox"], "title": "U-net: Convolutional networks for biomedical image segmentation", "venue": "CoRR, abs/1505.04597,", "year": 2015}, {"authors": ["Mary T. Dzindolet", "Scott A. Peterson", "Regina A. Pomranky", "Linda G. Pierce", "Hall P. Beck"], "title": "The role of trust in automation reliance", "venue": "Int. J. Hum.-Comput. Stud.,", "year": 2003}, {"authors": ["Liang Chen", "Paul Bentley", "Daniel Rueckert"], "title": "Fully automatic acute ischemic lesion segmentation in dwi using convolutional neural networks. NeuroImage: Clinical", "year": 2017}, {"authors": ["\u00d6zg\u00fcn \u00c7i\u00e7ek", "Ahmed Abdulkadir", "Soeren S. Lienkamp", "Thomas Brox", "Olaf Ronneberger"], "title": "3d u-net: Learning dense volumetric segmentation from sparse annotation", "year": 2016}, {"authors": ["Jeremy Irvin", "Pranav Rajpurkar", "Michael Ko", "Yifan Yu", "Silviana Ciurea- Ilcus", "Chris Chute", "Henrik Marklund", "Behzad Haghgoo", "Robyn L. Ball", "Katie S. Shpanskaya", "Jayne Seekins", "David A. Mong", "Safwan S. Halabi", "Jesse K. Sandberg", "Ricky Jones", "David B. Larson", "Curtis P. Langlotz", "Bhavik N. Patel", "Matthew P. Lungren", "Andrew Y. Ng"], "title": "Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison", "year": 1901}, {"authors": ["Fausto Milletari", "Nassir Navab", "Seyed-Ahmad Ahmadi"], "title": "V-net: Fully convolutional neural networks for volumetric medical image segmentation", "year": 2016}, {"authors": ["Liang-Chieh Chen", "George Papandreou", "Iasonas Kokkinos", "Kevin Murphy", "Alan L. Yuille"], "title": "Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs", "venue": "CoRR, abs/1606.00915,", "year": 2016}, {"authors": ["Christopher J. Kelly", "Alan Karthikesalingam", "Mustafa Suleyman", "Greg Corrado", "Dominic King"], "title": "Key challenges for delivering clinical impact with artificial intelligence", "venue": "BMC Medicine,", "year": 2019}, {"authors": ["Finale Doshi-Velez", "Been Kim"], "title": "Towards a rigorous science of interpretable machine learning, 2017", "year": 2017}, {"authors": ["Sana Tonekaboni", "Shalmali Joshi", "Melissa D. McCradden", "Anna Goldenberg"], "title": "What clinicians want: Contextualizing explainable machine learning for clinical end use", "year": 1905}, {"authors": ["Jonathan L. Herlocker", "Joseph A. Konstan", "John Riedl"], "title": "Explaining collaborative filtering recommendations", "venue": "In Proceedings of the 2000 ACM Conference on Computer Supported Cooperative Work, CSCW", "year": 2000}, {"authors": ["Sebastian Lapuschkin", "Stephan W\u00e4ldchen", "Alexander Binder", "Gr\u00e9goire Montavon", "Wojciech Samek", "Klaus-Robert M\u00fcller"], "title": "Unmasking clever hans predictors and assessing what machines really learn", "venue": "Nature Communications,", "year": 2019}, {"authors": ["Marco Tulio Ribeiro", "Sameer Singh", "Carlos Guestrin"], "title": "why should i trust you?: Explaining the predictions of any classifier", "venue": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD", "year": 2016}, {"authors": ["Zachary Chase Lipton"], "title": "The mythos of model interpretability", "venue": "CoRR, abs/1606.03490,", "year": 2016}, {"authors": ["F.K. Doilovi", "M. Bri", "N. Hlupi"], "title": "Explainable artificial intelligence: A survey", "venue": "41st International Convention on Information and Communication Technology, Electronics and Microelectronics (MIPRO),", "year": 2018}, {"authors": ["L.H. Gilpin", "D. Bau", "B.Z. Yuan", "A. Bajwa", "M. Specter", "L. Kagal"], "title": "Explaining explanations: An overview of interpretability of machine learning", "venue": "IEEE 5th International Conference on Data Science and Advanced Analytics (DSAA),", "year": 2018}, {"authors": ["Alejandro [Barredo Arrieta", "Natalia Daz-Rodrguez", "Javier [Del Ser", "Adrien Bennetot", "Siham Tabik", "Alberto Barbado", "Salvador Garcia", "Sergio Gil-Lopez", "Daniel Molina", "Richard Benjamins", "Raja Chatila", "Francisco Herrera"], "title": "Explainable artificial intelligence (xai): Concepts, taxonomies, opportunities and challenges toward responsible ai", "venue": "Information Fusion,", "year": 2020}, {"authors": ["Surjo R. Soekadar", "Niels Birbaumer", "Marc W. Slutzky", "Leonardo G. Cohen"], "title": "Brainmachine interfaces in neurorehabilitation of stroke", "venue": "Neurobiology of Disease,", "year": 2015}, {"authors": ["Andreas Holzinger", "Georg Langs", "Helmut Denk", "Kurt Zatloukal", "Heimo Mller"], "title": "Causability and explainability of artificial intelligence in medicine", "venue": "WIREs Data Mining and Knowledge Discovery,", "year": 2019}, {"authors": ["Yao Xie", "Ge Gao", "Xiang \u2019Anthony\u2019 Chen"], "title": "Outlining the design space of explainable intelligent systems for medical diagnosis", "year": 1902}, {"authors": ["Alfredo Vellido"], "title": "The importance of interpretability and visualization in machine learning for applications in medicine and health care", "venue": "Neural Computing and Applications,", "year": 2019}, {"authors": ["Eric J. Topol"], "title": "High-performance medicine: the convergence of human and artificial intelligence", "venue": "Nature Medicine,", "year": 2019}, {"authors": ["A. Fernandez", "F. Herrera", "O. Cordon", "M. Jose del Jesus", "F. Marcelloni"], "title": "Evolutionary fuzzy systems for explainable artificial intelligence: Why, when, what for, and where to", "venue": "IEEE Computational Intelligence Magazine,", "year": 2019}, {"authors": ["K. Kallianos", "J. Mongan", "S. Antani", "T. Henry", "A. Taylor", "J. Abuya", "M. Kohli"], "title": "How far have we come?: Artificial intelligence for chest radiograph interpretation", "venue": "Clinical Radiology,", "year": 2019}, {"authors": ["Grgoire Montavon", "Wojciech Samek", "Klaus-Robert Mller"], "title": "Methods for interpreting and understanding deep neural networks", "venue": "Digital Signal Processing,", "year": 2018}, {"authors": ["Wojciech Samek", "Thomas Wiegand", "Klaus-Robert M\u00fcller"], "title": "Explainable artificial intelligence: Understanding, visualizing and interpreting deep learning models", "year": 2017}, {"authors": ["Laura Rieger", "Pattarawat Chormai", "Gr\u00e9goire Montavon", "Lars Kai Hansen", "Klaus-Robert M\u00fcller"], "title": "Explainable and Interpretable Models in Computer Vision and Machine Learning, chapter Structuring Neural Networks for More Explainable Predictions, pages 115\u2013131", "year": 2018}, {"authors": ["Sofia Meacham", "Georgia Isaac", "Detlef Nauck", "Botond Virginas"], "title": "Towards explainable ai: Design and development for explanation of machine learning predictions for a patient readmittance medical application", "venue": "Intelligent Computing,", "year": 2019}, {"authors": ["J. Townsend", "T. Chaton", "J.M. Monteiro"], "title": "Extracting relational explanations from deep neural networks: A survey from a neuralsymbolic perspective", "venue": "IEEE Transactions on Neural Networks and Learning Systems,", "year": 2019}, {"authors": ["Bert Heinrichs", "Simon B. Eickhoff"], "title": "Your evidence? machine learning algorithms for medical diagnosis and prediction", "venue": "Human Brain Mapping,", "year": 2020}, {"authors": ["Miles Brundage", "Shahar Avin", "Jasmine Wang", "Haydn Belfield", "Gretchen Krueger", "Gillian Hadfield", "Heidy Khlaaf", "Jingying Yang", "Helen Toner", "Ruth Fong", "Tegan Maharaj", "Pang Wei Koh", "Sara Hooker", "Jade Leung", "Andrew Trask", "Emma Bluemke", "Jonathan Lebensold", "Cullen O\u2019Keefe", "Mark Koren", "Tho Ryffel", "JB Rubinovitz", "Tamay Besiroglu", "Federica Carugati", "Jack Clark", "Peter Eckersley", "Sarah de Haas", "Maritza Johnson", "Ben Laurie", "Alex Ingerman", "Igor Krawczuk", "Amanda Askell", "Rosario Cammarota", "Andrew Lohn", "David Krueger", "Charlotte Stix", "Peter Henderson", "Logan Graham", "Carina Prunkl", "Bianca Martin", "Elizabeth Seger", "Noa Zilberman", "Sen higeartaigh", "Frens Kroeger", "Girish Sastry", "Rebecca Kagan", "Adrian Weller", "Brian Tse", "Elizabeth Barnes", "Allan Dafoe", "Paul Scharre", "Ariel Herbert-Voss", "Martijn Rasser", "Shagun Sodhani", "Carrick Flynn", "Thomas Krendl Gilbert", "Lisa Dyer", "Saif Khan", "Yoshua Bengio", "Markus Anderljung"], "title": "Toward trustworthy ai development: Mechanisms for supporting verifiable claims, 2020", "year": 2020}, {"authors": ["Danding Wang", "Qian Yang", "Ashraf Abdul", "Brian Y. Lim"], "title": "Designing theory-driven user-centric explainable ai", "venue": "In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, CHI 19,", "year": 2019}, {"authors": ["D. Bau", "B. Zhou", "A. Khosla", "A. Oliva", "A. Torralba"], "title": "Network dissection: Quantifying interpretability of deep visual representations", "venue": "IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "year": 2017}, {"authors": ["Chris Olah", "Arvind Satyanarayan", "Ian Johnson", "Shan Carter", "Ludwig Schubert", "Katherine Ye", "Alexander Mordvintsev"], "title": "The building blocks of interpretability", "year": 2020}, {"authors": ["Scott M Lundberg", "Su-In Lee"], "title": "A unified approach to interpreting model predictions", "venue": "Advances in Neural Information Processing Systems", "year": 2017}, {"authors": ["Alon Jacovi", "Oren Sar Shalom", "Yoav Goldberg"], "title": "Understanding convolutional neural networks for text classification", "year": 2018}, {"authors": ["B. Zhou", "A. Khosla", "A. Lapedriza", "A. Oliva", "A. Torralba"], "title": "Learning deep features for discriminative localization", "venue": "In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "year": 2016}, {"authors": ["Ramprasaath R. Selvaraju", "Abhishek Das", "Ramakrishna Vedantam", "Michael Cogswell", "Devi Parikh", "Dhruv Batra"], "title": "Grad-cam: Why did you say that? visual explanations from deep networks via gradientbased localization", "year": 2016}, {"authors": ["Guannan Zhao", "Bo Zhou", "Kaiwen Wang", "Rui Jiang", "Min Xu"], "title": "Respond-cam: Analyzing deep models for 3d imaging data by visualizations", "venue": "Medical Image Computing and Computer Assisted Intervention \u2013 MICCAI", "year": 2018}, {"authors": ["Sebastian Bach", "Alexander Binder", "Grgoire Montavon", "Frederick Klauschen", "Klaus-Robert Mller", "Wojciech Samek"], "title": "On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation", "venue": "PLOS ONE, 10(7):1\u201346,", "year": 2015}, {"authors": ["W. Samek", "A. Binder", "G. Montavon", "S. Lapuschkin", "K. Mller"], "title": "Evaluating the visualization of what a deep neural network has learned", "venue": "IEEE Transactions on Neural Networks and Learning Systems,", "year": 2017}, {"authors": ["S\u00f6ren Becker", "Marcel Ackermann", "Sebastian Lapuschkin", "Klaus-Robert M\u00fcller", "Wojciech Samek"], "title": "Interpreting and explaining deep neural networks for classification of audio signals", "venue": "CoRR, abs/1807.03418,", "year": 2018}, {"authors": ["A.W. Thomas", "H.R. Heekeren", "K.R. M\u00fcller", "W. Samek"], "title": "Analyzing Neuroimaging Data Through Recurrent Deep Learning Models", "venue": "Front Neurosci,", "year": 2019}, {"authors": ["Leila Arras", "Franziska Horn", "Gr\u00e9goire Montavon", "Klaus-Robert M\u00fcller", "Wojciech Samek"], "title": "what is relevant in a text document?\u201d: An interpretable machine learning approach", "year": 2016}, {"authors": ["V. Srinivasan", "S. Lapuschkin", "C. Hellge", "K. Mller", "W. Samek"], "title": "Interpretable human action recognition in compressed domain", "venue": "IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),", "year": 2017}, {"authors": ["Oliver Eberle", "Jochen Bttner", "Florian Krutli", "Klaus-Robert Mller", "Matteo Valleriani", "Grgoire Montavon"], "title": "Building and interpreting deep similarity models, 2020", "year": 2020}, {"authors": ["Liam Hiley", "Alun Preece", "Yulia Hicks", "Supriyo Chakraborty", "Prudhvi Gurram", "Richard Tomsett"], "title": "Explaining motion relevance for activity recognition in video deep learning models, 2020", "year": 2020}, {"authors": ["Wojciech Samek", "Grgoire Montavon", "Alexander Binder", "Sebastian Lapuschkin", "Klaus-Robert Mller"], "title": "Interpreting the predictions of complex ml models by layer-wise relevance propagation, 2016", "year": 2016}, {"authors": ["Amirata Ghorbani", "James Wexler", "James Y Zou", "Been Kim"], "title": "Towards automatic concept-based explanations", "venue": "Advances in Neural Information Processing Systems", "year": 2019}, {"authors": ["Avanti Shrikumar", "Peyton Greenside", "Anshul Kundaje"], "title": "Learning important features through propagating activation", "venue": "differences. CoRR,", "year": 2017}, {"authors": ["Luisa M. Zintgraf", "Taco S. Cohen", "Tameem Adel", "Max Welling"], "title": "Visualizing deep neural network decisions: Prediction difference analysis", "year": 2017}, {"authors": ["Yanzhao Zhou", "Yi Zhu", "Qixiang Ye", "Qiang Qiu", "Jianbin Jiao"], "title": "Weakly supervised instance segmentation using class peak response", "year": 2018}, {"authors": ["Pieter-Jan Kindermans", "Kristof T. Schtt", "Maximilian Alber", "Klaus- Robert Mller", "Dumitru Erhan", "Been Kim", "Sven Dhne"], "title": "Learning how to explain neural networks: Patternnet and patternattribution, 2017", "year": 2017}, {"authors": ["Daniel Smilkov", "Nikhil Thorat", "Been Kim", "Fernanda Vigas", "Martin Wattenberg"], "title": "Smoothgrad: removing noise by adding noise, 2017", "venue": "https: //pair-code.github.io/saliency/", "year": 2017}, {"authors": ["Leila Arras", "Gr\u00e9goire Montavon", "Klaus-Robert M\u00fcller", "Wojciech Samek"], "title": "Explaining recurrent neural network predictions in sentiment analysis", "venue": "In Proceedings of the 8th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis,", "year": 2017}, {"authors": ["Andrej Karpathy", "Justin Johnson", "Li Fei-Fei"], "title": "Visualizing and understanding recurrent networks", "year": 2015}, {"authors": ["Magdalini Paschali", "Sailesh Conjeti", "Fernando Navarro", "Nassir Navab"], "title": "Generalizability vs. robustness: Investigating medical imaging networks using adversarial examples", "venue": "Medical Image Computing and Computer Assisted Intervention \u2013 MICCAI", "year": 2018}, {"authors": ["Heather D. Couture", "J.S. Marron", "Charles M. Perou", "Melissa A. Troester", "Marc Niethammer"], "title": "Multiple instance learning forheterogeneous images: Training acnn for histopathology", "venue": "Medical Image Computing and Computer Assisted Intervention \u2013 MICCAI", "year": 2018}, {"authors": ["Xiaoxiao Li", "Nicha C. Dvornek", "Juntang Zhuang", "Pamela Ventola", "James S. Duncan"], "title": "Brain biomarker interpretation in asd using deep learning and fmri", "venue": "Medical Image Computing and Computer Assisted Intervention \u2013 MICCAI", "year": 2018}, {"authors": ["Yao Qin", "Konstantinos Kamnitsas", "Siddharth Ancha", "Jay Nanavati", "Garrison W. Cottrell", "Antonio Criminisi", "Aditya V. Nori"], "title": "Autofocus layer for semantic segmentation", "year": 2018}, {"authors": ["Ziqi Tang", "Kangway V. Chuang", "Charles DeCarli", "Lee-Way Jin", "Laurel Beckett", "Michael J. Keiser", "Brittany N. Dugger"], "title": "Interpretable classification of alzheimer\u2019s disease pathologies with a convolutional neural network pipeline", "venue": "Nature Communications,", "year": 2019}, {"authors": ["Zachary Papanastasopoulos", "Ravi K. Samala", "Heang-Ping Chan", "Lubomir Hadjiiski", "Chintana Paramagul", "Mark A. Helvie M.D", "Colleen H. Neal M.D"], "title": "Explainable AI for medical imaging: deeplearning CNN ensemble for classification of estrogen receptor status from breast MRI", "venue": "Medical Imaging 2020: Computer-Aided Diagnosis,", "year": 2020}, {"authors": ["Hyebin Lee", "Seong Tae Kim", "Yong Man Ro"], "title": "Generation of multimodal justification using visual word constraint model for explainable computer-aided diagnosis", "venue": "Interpretability of Machine Intelligence in Medical Image Computing and Multimodal Learning for Clinical Decision Support,", "year": 2019}, {"authors": ["Pieter-Jan Kindermans", "Sara Hooker", "Julius Adebayo", "Maximilian Alber", "Kristof T. Sch\u00fctt", "Sven D\u00e4hne", "Dumitru Erhan", "Been Kim"], "title": "The (Un)reliability of Saliency Methods, pages 267\u2013280", "year": 2019}, {"authors": ["Matthew D. Zeiler", "Rob Fergus"], "title": "Visualizing and understanding convolutional networks", "venue": "CoRR, abs/1311.2901,", "year": 2013}, {"authors": ["Aravindh Mahendran", "Andrea Vedaldi"], "title": "Understanding deep image representations by inverting them", "venue": "CoRR, abs/1412.0035,", "year": 2014}, {"authors": ["Alexey Dosovitskiy", "Thomas Brox"], "title": "Inverting convolutional networks with convolutional networks", "venue": "CoRR, abs/1506.02753,", "year": 2015}, {"authors": ["Jost Tobias Springenberg", "Alexey Dosovitskiy", "Thomas Brox", "Martin Riedmiller"], "title": "Striving for simplicity: The all convolutional net", "year": 2014}, {"authors": ["Dumitru Erhan", "Yoshua Bengio", "Aaron Courville", "Pascal Vincent"], "title": "Visualizing higher-layer features of a deep network", "venue": "Technical Report 1341,", "year": 2009}, {"authors": ["Anh Mai Nguyen", "Jason Yosinski", "Jeff Clune"], "title": "Multifaceted feature visualization: Uncovering the different types of features learned by each neuron in deep neural networks", "year": 2016}, {"authors": ["Jason Yosinski", "Jeff Clune", "Anh Mai Nguyen", "Thomas J. Fuchs", "Hod Lipson"], "title": "Understanding neural networks through deep visualization", "venue": "CoRR, abs/1506.06579,", "year": 2015}, {"authors": ["C. Szegedy", "Wei Liu", "Yangqing Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "title": "Going deeper with convolutions", "venue": "In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "year": 2015}, {"authors": ["Richard Meyes", "Melanie Lu", "Constantin Waubert de Puiseau", "Tobias Meisen"], "title": "Ablation studies in artificial neural networks", "year": 1901}, {"authors": ["Richard Meyes", "Constantin Waubert de Puiseau", "Andres Posada- Moreno", "Tobias Meisen"], "title": "Under the hood of neural networks: Characterizing learned representations by functional neuron populations and network ablations, 2020", "year": 2020}, {"authors": ["Rich Caruana", "Yin Lou", "Johannes Gehrke", "Paul Koch", "Marc Sturm", "Noemie Elhadad"], "title": "Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission", "venue": "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD", "year": 2015}, {"authors": ["Benjamin Letham", "Cynthia Rudin", "Tyler H. McCormick", "David Madigan"], "title": "Interpretable classifiers using rules and bayesian analysis: Building a better stroke prediction model", "venue": "The Annals of Applied Statistics,", "year": 2015}, {"authors": ["Isaac Lage", "Emily Chen", "Jeffrey He", "Menaka Narayanan", "Been Kim", "Sam Gershman", "Finale Doshi-Velez"], "title": "An evaluation of the humaninterpretability of explanation", "year": 1902}, {"authors": ["Himabindu Lakkaraju", "Ece Kamar", "Rich Caruana", "Jure Leskovec"], "title": "Faithful and customizable explanations of black box models", "venue": "In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society,", "year": 2019}, {"authors": ["Tao Lei", "Regina Barzilay", "Tommi Jaakkola"], "title": "Rationalizing neural predictions", "venue": "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,", "year": 2016}, {"authors": ["Pei Guo", "Connor Anderson", "Kolten Pearson", "Ryan Farrell"], "title": "Neural network interpretation via fine grained textual summarization", "venue": "CoRR, abs/1805.08969,", "year": 2018}, {"authors": ["Aishwarya Agrawal", "Jiasen Lu", "Stanislaw Antol", "Margaret Mitchell", "C. Lawrence Zitnick", "Devi Parikh", "Dhruv Batra"], "title": "Vqa: Visual question answering", "venue": "Int. J. Comput. Vision,", "year": 2017}, {"authors": ["Jiasen Lu", "Jianwei Yang", "Dhruv Batra", "Devi Parikh"], "title": "Hierarchical question-image co-attention for visual question answering", "venue": "In Proceedings of the 30th International Conference on Neural Information Processing Systems,", "year": 2016}, {"authors": ["Mohammadhassan Izadyyazdanabadi", "Evgenii Belykh", "Claudio Cavallo", "Xiaochun Zhao", "Sirin Gandhi", "Leandro Borba Moreira", "Jennifer Eschbacher", "Peter Nakaji", "Mark C. Preul", "Yezhou Yang"], "title": "Weaklysupervised learning-based feature localization for confocal laser endomicroscopy glioma images", "venue": "Medical Image Computing and Computer Assisted Intervention \u2013 MICCAI", "year": 2018}, {"authors": ["Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio"], "title": "Neural machine translation by jointly learning to align and translate", "year": 2014}, {"authors": ["Peifei Zhu", "Masahiro Ogino"], "title": "Guideline-based additive explanation for computer-aided diagnosis of lung nodules", "venue": "Interpretability of Machine Intelligence in Medical Image Computing and Multimodal Learning for Clinical Decision Support,", "year": 2019}, {"authors": ["Mukund Sundararajan", "Ankur Taly", "Qiqi Yan"], "title": "Axiomatic attribution for deep networks", "venue": "In Proceedings of the 34th International Conference on Machine Learning - Volume 70,", "year": 2017}, {"authors": ["Amirata Ghorbani", "Abubakar Abid", "James Zou"], "title": "Interpretation of neural networks is fragile, 2017", "year": 2017}, {"authors": ["Been Kim", "Martin Wattenberg", "Justin Gilmer", "Carrie Cai", "James Wexler", "Fernanda B. Vigas", "Rory Sayres"], "title": "Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav)", "venue": "JMLR Workshop and Conference Proceedings,", "year": 2018}, {"authors": ["Maithra Raghu", "Justin Gilmer", "Jason Yosinski", "Jascha Sohl- Dickstein"], "title": "Svcca: Singular vector canonical correlation analysis for deep learning dynamics and interpretability", "venue": "In Proceedings of the 31st International Conference on Neural Information Processing Systems,", "year": 2017}, {"authors": ["N. Tishby", "N. Zaslavsky"], "title": "Deep learning and the information bottleneck principle", "venue": "In 2015 IEEE Information Theory Workshop (ITW),", "year": 2015}, {"authors": ["Ravid Shwartz-Ziv", "Naftali Tishby"], "title": "Opening the black box of deep neural networks via information", "year": 2017}, {"authors": ["Erdem Varol", "Aristeidis Sotiras", "Ke Zeng", "Christos Davatzikos"], "title": "Generative discriminative models for multivariate inference and statistical mapping in medical imaging", "venue": "Medical Image Computing and Computer Assisted Intervention \u2013 MICCAI", "year": 2018}, {"authors": ["Guillaume Alain", "Yoshua Bengio"], "title": "Understanding intermediate layers using linear classifier probes, 2016", "year": 2016}, {"authors": ["Trevor Hastie", "Robert Tibshirani"], "title": "Generalized additive models", "venue": "Statist. Sci., 1(3):297\u2013310,", "year": 1986}, {"authors": ["Yin Lou", "Rich Caruana", "Johannes Gehrke"], "title": "Intelligible models for classification and regression", "venue": "In Proceedings of the 18th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD", "year": 2012}, {"authors": ["Yin Lou", "Rich Caruana", "Johannes Gehrke", "Giles Hooker"], "title": "Accurate intelligible models with pairwise interactions", "venue": "In Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "year": 2013}, {"authors": ["Sercan \u00d6mer Arik", "Tomas Pfister"], "title": "Attention-based prototypical learning towards interpretable, confident and robust deep neural networks", "year": 1902}, {"authors": ["Nima Hatami", "Micha\u00ebl Sdika", "H\u00e9l\u00e8ne Ratiney"], "title": "Magnetic resonance spectroscopy quantification using deep learning", "year": 2018}, {"authors": ["Stefan Haufe", "Frank Meinecke", "Kai Grgen", "Sven Dhne", "John-Dylan Haynes", "Benjamin Blankertz", "Felix Biemann"], "title": "On the interpretation of weight vectors of linear models in multivariate neuroimaging", "year": 2014}, {"authors": ["Kristof T. Sch\u00fctt", "Farhad Arbabzadah", "Stefan Chmiela", "Klaus R. M\u00fcller", "Alexandre Tkatchenko"], "title": "Quantum-chemical insights from deep tensor neural networks", "venue": "Nature Communications,", "year": 2017}, {"authors": ["Kristof T. Sch\u00fctt", "Michael Gastegger", "Alexandre Tkatchenko", "Klaus-Robert M\u00fcller"], "title": "Quantum-Chemical Insights from Interpretable Atomistic Neural Networks, pages 311\u2013330", "year": 2019}, {"authors": ["Christos Liaskos", "Ageliki Tsioliaridou", "Shuai Nie", "Andreas Pitsillides", "Sotiris Ioannidis", "Ian F. Akyildiz"], "title": "An interpretable neural network for configuring programmable wireless environments", "year": 1905}, {"authors": ["Barnab\u00e1s Bede"], "title": "Fuzzy systems with sigmoid-based membership functions as interpretable neural networks", "venue": "Fuzzy Techniques: Theory and Applications,", "year": 2019}, {"authors": ["Markus Kaiser", "Clemens Otte", "Thomas A. Runkler", "Carl Henrik Ek"], "title": "Interpretable dynamics models for data-efficient reinforcement learning", "year": 1907}, {"authors": ["D.R. Hardoon", "S. Szedmak", "J. Shawe-Taylor"], "title": "Canonical correlation analysis: An overview with application to learning methods", "venue": "Neural Computation,", "year": 2004}, {"authors": ["Anil Hazarika", "Mausumi Barthakur", "Lachit Dutta", "Manabendra Bhuyan"], "title": "F-svd based algorithm for variability and stability measurement of bio-signals, feature extraction and fusion for pattern recognition", "venue": "Biomedical Signal Processing and Control,", "year": 2019}, {"authors": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "title": "Generative adversarial nets", "venue": "Advances in Neural Information Processing Systems", "year": 2014}, {"authors": ["Martin Arjovsky", "Soumith Chintala", "L\u00e9on Bottou"], "title": "Wasserstein generative adversarial networks", "venue": "In Proceedings of the 34th International Conference on Machine Learning - Volume 70,", "year": 2017}, {"authors": ["Jun-Yan Zhu", "Taesung Park", "Phillip Isola", "Alexei A. Efros"], "title": "Unpaired image-to-image translation using cycle-consistent adversarial networks", "venue": "In The IEEE International Conference on Computer Vision (ICCV),", "year": 2017}, {"authors": ["Y. Zhu", "S. Suri", "P. Kulkarni", "Y. Chen", "J. Duan", "C. . J. Kuo"], "title": "An interpretable generative model for handwritten digits synthesis", "venue": "IEEE International Conference on Image Processing (ICIP),", "year": 2019}, {"authors": ["Ryen Krusinga", "Sohil Shah", "Matthias Zwicker", "Tom Goldstein", "David W. Jacobs"], "title": "Understanding the (un)interpretability of natural image distributions using generative models", "year": 1901}, {"authors": ["Shan Carter", "Zan Armstrong", "Ludwig Schubert", "Ian Johnson", "Chris Olah"], "title": "Exploring neural networks with activation atlases, 2019", "year": 2019}, {"authors": ["Wei Ma", "Feng Cheng", "Yihao Xu", "Qinlong Wen", "Yongmin Liu"], "title": "Probabilistic representation and inverse design of metamaterials based on a deep generative model with semi-supervised learning strategy", "venue": "Advanced Materials,", "year": 2019}, {"authors": ["Yujun Yan", "Jiong Zhu", "Marlena Duda", "Eric Solarz", "Chandra Sripada", "Danai Koutra"], "title": "Groupinn: Grouping-based interpretable neural network for classification of limited, noisy brain data", "venue": "In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD", "year": 2019}, {"authors": ["Daniel Rueckert"], "title": "Learning interpretable anatomical features through deep generative models: Application to cardiac remodeling", "venue": "JOURNAL OF LATEX CLASS FILES,", "year": 2015}, {"authors": ["Mingliang Wang", "Daoqiang Zhang", "Jiashuang Huang", "Dinggang Shen", "Mingxia Liu"], "title": "Low-rank representation for multi-center autism spectrum disorder identification", "venue": "Medical Image Computing and Computer Assisted Intervention \u2013 MICCAI", "year": 2018}, {"authors": ["Ruth Fong", "Andrea Vedaldi"], "title": "Interpretable explanations of black boxes by meaningful perturbation", "year": 2017}, {"authors": ["David Alvarez-Melis", "Tommi Jaakkola"], "title": "A causal framework for explaining the predictions of black-box sequence-to-sequence models", "venue": "In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,", "year": 2017}, {"authors": ["David Baehrens", "Timon Schroeter", "Stefan Harmeling", "Motoaki Kawanabe", "Katja Hansen", "Klaus-Robert M\u00fcller"], "title": "How to explain individual classification decisions", "venue": "J. Mach. Learn. Res.,", "year": 2010}, {"authors": ["Pang Wei Koh", "Percy Liang"], "title": "Understanding black-box predictions via influence functions", "venue": "In Proceedings of the 34th International Conference on Machine Learning - Volume 70,", "year": 2017}, {"authors": ["Chih-Kuan Yeh", "Joon Sik Kim", "Ian E.H. Yen", "Pradeep Ravikumar"], "title": "Representer point selection for explaining deep neural networks", "venue": "In Proceedings of the 32nd International Conference on Neural Information Processing Systems,", "year": 2018}, {"authors": ["Cagdas Ulas", "Giles Tetteh", "Stephan Kaczmarz", "Christine Preibisch", "Bjoern H. Menze"], "title": "Deepasl: Kinetic model incorporated loss for denoising arterial spin labeled mri via deep residual learning", "venue": "Medical Image Computing and Computer Assisted Intervention \u2013 MICCAI", "year": 2018}, {"authors": ["C.J. Scott", "J. Jiao", "A. Melbourne", "N. Burgos", "D.M. Cash", "E. De Vita", "P.J. Markiewicz", "A. O\u2019Connor", "D.L. Thomas", "P.S. Weston", "J.M. Schott", "B.F. Hutton", "S. Ourselin"], "title": "Reduced acquisition time PET pharmacokinetic modelling using simultaneous ASL-MRI: proof of concept", "venue": "J. Cereb. Blood Flow Metab.,", "year": 2019}, {"authors": ["Maxim Pisov", "Mikhail Goncharov", "Nadezhda Kurochkina", "Sergey Morozov", "Victor Gombolevskiy", "Valeria Chernina", "Anton Vladzymyrskyy", "Ksenia Zamyatina", "Anna Chesnokova", "Igor Pronin", "Michael Shifrin", "Mikhail Belyaev"], "title": "Incorporating task-specific structural knowledge into cnns for brain midline shift detection", "venue": "Interpretability of Machine Intelligence in Medical Image Computing and Multimodal Learning for Clinical Decision Support,", "year": 2019}, {"authors": ["Feiyun Zhu", "Jun Guo", "Zheng Xu", "Peng Liao", "Liu Yang", "Junzhou Huang"], "title": "Group-driven reinforcement learning for personalized mhealth intervention", "venue": "Medical Image Computing and Computer Assisted Intervention \u2013 MICCAI", "year": 2018}, {"authors": ["Ozan Kocadagli", "Reza Langari"], "title": "Classification of eeg signals for epileptic seizures using hybrid artificial neural networks based wavelet transforms and fuzzy relations", "venue": "Expert Systems with Applications,", "year": 2017}, {"authors": ["Tao Zhang", "Wanzhong Chen", "Mingyang Li"], "title": "Classification of inter-ictal and ictal eegs using multi-basis modwpt, dimensionality reduction algorithms and ls-svm: A comparative study", "venue": "Biomedical Signal Processing and Control,", "year": 2019}, {"authors": ["Wentian Li", "Jane E. Cerise", "Yaning Yang", "Henry Han"], "title": "Application of t-sne to human genetic data", "venue": "Journal of Bioinformatics and Computational Biology,", "year": 2017}, {"authors": ["W. Xu", "X. Jiang", "X. Hu", "G. Li"], "title": "Visualization of genetic disease-phenotype similarities by multiple maps t-SNE with Laplacian regularization", "venue": "BMC Med Genomics,", "year": 2014}, {"authors": ["Noel C.F. Codella", "Chung-Ching Lin", "Allan Halpern", "Michael Hind", "Rogerio Feris", "John R. Smith"], "title": "Collaborative human-ai (chai): Evidence-based interpretable melanoma classification in dermoscopic images", "venue": "Understanding and Interpreting Machine Learning in Medical Image Computing Applications,", "year": 2018}, {"authors": ["Mara Graziani", "Vincent Andrearczyk", "Henning M\u00fcller"], "title": "Regression concept vectors for bidirectional explanations in histopathology", "venue": "Understanding and Interpreting Machine Learning in Medical Image Computing Applications,", "year": 2018}, {"authors": ["Hugo Yeche", "Justin Harrison", "Tess Berthier"], "title": "Ubs: A dimensionagnostic metric for concept vector interpretability applied to radiomics", "venue": "Interpretability of Machine Intelligence in Medical Image Computing and Multimodal Learning for Clinical Decision Support,", "year": 2019}, {"authors": ["Alvaro E. Ulloa Cerna", "Marios Pattichis", "David P. vanMaanen", "Linyuan Jing", "Aalpen A. Patel", "Joshua V. Stough", "Christopher M. Haggerty", "Brandon K. Fornwalt"], "title": "Interpretable neural networks for predicting mortality risk using multi-modal electronic health", "venue": "records. CoRR,", "year": 2019}, {"authors": ["Jean-Baptiste Lamy", "Boomadevi Sekar", "Gilles Guezennec", "Jacques Bouaud", "Brigitte Sroussi"], "title": "Explainable artificial intelligence for breast cancer: A visual case-based reasoning approach", "venue": "Artificial Intelligence in Medicine,", "year": 2019}, {"authors": ["Youngwon Choi", "Yongchan Kwon", "Hanbyul Lee", "Beom Joon Kim", "Myunghee Cho Paik", "Joong-Ho Won"], "title": "Ensemble of deep convolutional neural networks for prognosis of ischemic stroke", "venue": "Stroke and Traumatic Brain Injuries,", "year": 2016}, {"authors": ["Oskar Maier", "Heinz Handels"], "title": "Predicting stroke lesion and clinical outcome with random forests", "year": 2016}, {"authors": ["Been Kim", "Caleb M. Chacha", "Julie Shah"], "title": "Inferring robot task plans from human team meetings: A generative modeling approach with logic-based prior", "venue": "In Proceedings of the Twenty-Seventh AAAI Conference on Artificial Intelligence,", "year": 2013}, {"authors": ["Justin Cheng", "Michael S. Bernstein"], "title": "Flock: Hybrid crowd-machine learning classifiers", "venue": "In Proceedings of the 18th ACM Conference on Computer Supported Cooperative Work and Social Computing,", "year": 2015}, {"authors": ["L. Kuhlmann", "P. Karoly", "D.R. Freestone", "B.H. Brinkmann", "A. Temko", "A. Barachant", "F. Li", "G. Titericz", "B.W. Lang", "D. Lavery", "K. Roman", "D. Broadhead", "S. Dobson", "G. Jones", "Q. Tang", "I. Ivanenko", "O. Panichev", "T. Proix", "M. N?hl?k", "D.B. Grunberg", "C. Reuben", "G. Worrell", "B. Litt", "D.T.J. Liley", "D.B. Grayden", "M.J. Cook"], "title": "Epilepsyecosystem.org: crowd-sourcing reproducible seizure prediction with long-term human intracranial EEG", "venue": "Brain, 141(9):2619\u20132630,", "year": 2018}, {"authors": ["M. Wiener", "F.T. Sommer", "Z.G. Ives", "R.A. Poldrack", "B. Litt"], "title": "Enabling an Open Data Ecosystem for the Neurosciences", "venue": "Neuron, 92(4):929,", "year": 2016}, {"authors": ["F. Jiang", "Y. Jiang", "H. Zhi", "Y. Dong", "H. Li", "S. Ma", "Y. Wang", "Q. Dong", "H. Shen"], "title": "Artificial intelligence in healthcare: past, present and future", "venue": "Stroke Vasc Neurol,", "year": 2017}, {"authors": ["C.K. Cassel", "A.L. Jameton"], "title": "Dementia in the elderly: an analysis of medical responsibility", "venue": "Ann. Intern. Med.,", "year": 1981}, {"authors": ["S\u00e9rgio Pereira", "Raphael Meier", "Victor Alves", "Mauricio Reyes", "Carlos A. Silva"], "title": "Automatic brain tumor grading from mri data using convolutional neural networks and quality assessment", "venue": "JOURNAL OF LATEX CLASS FILES,", "year": 2015}, {"authors": ["A. Vilamala", "K.H. Madsen", "L.K. Hansen"], "title": "Deep convolutional neural networks for interpretable analysis of eeg sleep stage scoring", "venue": "IEEE 27th International Workshop on Machine Learning for Signal Processing (MLSP),", "year": 2017}, {"authors": ["Pieter Van Molle", "Miguel De Strooper", "Tim Verbelen", "Bert Vankeirsbilck", "Pieter Simoens", "Bart Dhoedt"], "title": "Visualizing convolutional neural networks to improve decision support for skin lesion classification", "venue": "Understanding and Interpreting Machine Learning in Medical Image Computing Applications,", "year": 2018}, {"authors": ["N. Prentzas", "A. Nicolaides", "E. Kyriacou", "A. Kakas", "C. Pattichis"], "title": "Integrating machine learning with symbolic reasoning to build an explainable ai model for stroke prediction", "venue": "IEEE 19th International Conference on Bioinformatics and Bioengineering (BIBE),", "year": 2019}, {"authors": ["H. Sun", "L. Paixao", "J.T. Oliva", "B. Goparaju", "D.Z. Carvalho", "K.G. van Leeuwen", "O. Akeju", "R.J. Thomas", "S.S. Cash", "M.T. Bianchi", "M.B. Westover"], "title": "Brain age from the electroencephalogram of sleep", "venue": "Neurobiol. Aging, 74:112\u2013120,", "year": 2019}, {"authors": ["Fabian Eitel", "Kerstin Ritter"], "title": "Testing the robustness of attribution methods for convolutional neural networks in mri-based alzheimer\u2019s disease classification", "venue": "Interpretability of Machine Intelligence in Medical Image Computing and Multimodal Learning for Clinical Decision Support,", "year": 2019}, {"authors": ["Christoph Jansen", "Thomas Penzel", "Stephan Hodel", "Stefanie Breuer", "Martin Spott", "Dagmar Krefting"], "title": "Network physiology in insomnia patients: Assessment of relevant changes in network topology with interpretable machine learning models", "venue": "Chaos: An Interdisciplinary Journal of Nonlinear Science,", "year": 2019}, {"authors": ["Kyle Young", "Gareth Booth", "Becks Simpson", "Reuben Dutton", "Sally Shrapnel"], "title": "Deep neural network or dermatologist", "venue": "Interpretability of Machine Intelligence in Medical Image Computing and Multimodal Learning for Clinical Decision Support,", "year": 2019}, {"authors": ["C. Zucco", "H. Liang", "G.D. Fatta", "M. Cannataro"], "title": "Explainable sentiment analysis with applications in medicine", "venue": "IEEE International Conference on Bioinformatics and Biomedicine (BIBM),", "year": 2018}, {"authors": ["Curtis P. Langlotz", "Bibb Allen", "Bradley J. Erickson", "Jayashree Kalpathy-Cramer", "Keith Bigelow", "Tessa S. Cook", "Adam E. Flanders", "Matthew P. Lungren", "David S. Mendelson", "Jeffrey D. Rudie", "Ge Wang", "Krishna Kandarpa"], "title": "A roadmap for foundational research on artificial intelligence in medical imaging: From the 2018 nih/rsna/acr/the academy workshop", "year": 2019}, {"authors": ["Arieh Gomolin", "Elena Netchiporouk", "Robert Gniadecki", "Ivan V. Litvinov"], "title": "Artificial intelligence applications in dermatology: Where do we stand", "venue": "Frontiers in Medicine,", "year": 2020}, {"authors": ["A.J. London"], "title": "Artificial Intelligence and Black-Box Medical Decisions: Accuracy versus Explainability", "venue": "Hastings Cent Rep,", "year": 2019}, {"authors": ["Sadid Hasan", "Yuan Ling", "Dimeji Farri", "Joey Liu", "Henning Mller", "Matthew Lungren"], "title": "Overview of imageclef 2018 medical domain visual question answering", "year": 2018}, {"authors": ["Asma Ben Abacha", "Sadid Hasan", "Vivek Datla", "Joey Liu", "Dina Demner- Fushman", "Henning Mller"], "title": "Vqa-med: Overview of the medical visual question answering task at imageclef 2019", "venue": "Lecture Notes in Computer Science,", "year": 2019}, {"authors": ["Ann-Kathrin Dombrowski", "Maximillian Alber", "Christopher Anders", "Marcel Ackermann", "Klaus-Robert M\u00fcller", "Pan Kessel"], "title": "Explanations can be manipulated and geometry is to blame", "venue": "Advances in Neural Information Processing Systems", "year": 2019}, {"authors": ["Himabindu Lakkaraju", "Osbert Bastani"], "title": "how do i fool you", "venue": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society,", "year": 2020}, {"authors": ["Yun Liu", "Krishna Gadepalli", "Mohammad Norouzi", "George E. Dahl", "Timo Kohlberger", "Aleksey Boyko", "Subhashini Venugopalan", "Aleksei Timofeev", "Philip Q. Nelson", "Gregory S. Corrado", "Jason D. Hipp", "Lily Peng", "Martin C. Stumpe"], "title": "Detecting cancer metastases on gigapixel pathology", "venue": "images. CoRR,", "year": 2017}], "sections": [{"text": "A Survey on Explainable Artificial Intelligence (XAI): towards Medical XAI\nErico Tjoa, and Cuntai Guan, Fellow, IEEE\nAbstract\u2014Recently, artificial intelligence and machine learning in general have demonstrated remarkable performances in many tasks, from image processing to natural language processing, especially with the advent of deep learning. Along with research progress, they have encroached upon many different fields and disciplines. Some of them require high level of accountability and thus transparency, for example the medical sector. Explanations for machine decisions and predictions are thus needed to justify their reliability. This requires greater interpretability, which often means we need to understand the mechanism underlying the algorithms. Unfortunately, the blackbox nature of the deep learning is still unresolved, and many machine decisions are still poorly understood. We provide a review on interpretabilities suggested by different research works and categorize them. The different categories show different dimensions in interpretability research, from approaches that provide \u201cobviously\u201d interpretable information to the studies of complex patterns. By applying the same categorization to interpretability in medical research, it is hoped that (1) clinicians and practitioners can subsequently approach these methods with caution, (2) insights into interpretability will be born with more considerations for medical practices, and (3) initiatives to push forward data-based, mathematically- and technically-grounded medical education are encouraged.\nIndex Terms\u2014Explainable Artificial Intelligence, Survey, Machine Learning, Interpretability, Medical Information System.\nI. INTRODUCTION\nMACHINE LEARNING (ML) has grown large in bothresearch and industrial applications, especially with the success of deep learning (DL) and neural networks (NN), so large that its impact and possible after-effects can no longer be taken for granted. In some fields, failure is not an option: even a momentarily dysfunctional computer vision algorithm in autonomous vehicle easily leads to fatality. In the medical field, clearly human lives are on the line. Detection of a disease at its early phase is often critical to the recovery of patients or to prevent the disease from advancing to more severe stages. While machine learning methods, artificial neural networks, brain-machine interfaces and related subfields have recently demonstrated promising performance in performing medical tasks, they are hardly perfect [1]\u2013[9].\nInterpretability and explainability of ML algorithms have thus become pressing issues: who is accountable if things go wrong? Can we explain why things go wrong? If things are working well, do we know why and how to leverage them further? Many papers have suggested different measures\nErico T. and Cuntai Guan were with the School of Computer Science and Engineering, Nanyang Technological University, Singapore.\nErico T. was also affiliated with HealthTech Division, Alibaba Group Holding Limited.\nand frameworks to capture interpretability, and the topic explainable artificial intelligence (XAI) has become a hotspot in ML research community. Popular deep learning libraries have started to include their own explainable AI libraries, such as Pytorch Captum and Tensorflow tf-explain. Furthermore, the proliferation of interpretability assessment criteria (such as reliability, causality and usability) helps ML community keep track of how algorithms are used and how their usage can be improved, providing guiding posts for further developments [10]\u2013[12]. In particular, it has been demonstrated that visualization is capable of helping researchers detect erroneous reasoning in classification problems that many previous researchers possibly have missed [13].\nThe above said, there seems to be a lack of uniform adoption of interpretability assessment criteria across the research community. There have been attempts to define the notions of \u201cinterpretability\u201d, \u201cexplainability\u201d along with \u201creliability\u201d, \u201ctrustworthiness\u201d and other similar notions without clear expositions on how they should be incorporated into the great diversity of implementations of machine learning models; consider [10], [14]\u2013[18]. In this survey, we will instead use \u201cexplainability\u201d and \u201cinterpretability\u201d interchangeably, considering a research to be related to interpretability if it does show any attempts (1) to explain the decisions made by algorithms, (2) to uncover the patterns within the inner mechanism of an algorithm, (3) to present the system with coherent models or mathematics, and we will include even loose attempts to raise the credibility of machine algorithms.\nIn this work, we survey through research works related to the interpretability of ML or computer algorithms in general, categorize them, and then apply the same categories to interpretability in the medical field. The categorization is especially aimed to give clinicians and practitioners a perspective on the use of interpretable algorithms that are available in diverse forms. The trade-off between the ease of interpretation and the need for specialized mathematical knowledge may create a bias in preference for one method compared to another without justification based on medical practices. This may further provide a ground for specialized education in the medical sector that is aimed to realize the potentials that reside within these algorithms. We also find that many journal papers in the machine learning and AI community are algorithm-centric. They often assume that the algorithms used are obviously interpretable without conducting human subject tests to verify their interpretability; see column HSI of table I and II. Note that assuming that a model is obviously interpretable is not necessarily wrong, and, in some cases human tests might be irrelevant (for example pre-defined ar X\niv :1\n90 7.\n07 37\n4v 5\n[ cs\n.L G\n] 1\n1 A\nug 2\n02 0\nmodels based on commonly accepted knowledge specific to the content-subject may be considered interpretable without human subject tests). In the tables, we also include a column to indicate whether the interpretability method applies for artificial NN, since the issue of interpretability is recently gathering attention due to its blackbox nature.\nWe will not attempt to cover all related works many of which are already presented in the research papers and survey we cite [1], [2], [15]\u2013[30]. We extend the so-called integrated interpretability [16] by including considerations for subject-content-dependent models. Compared to [17], we also overview the mathematical formulation of common or popular methods, revealing the great variety of approaches to interpretability. Our categorization draws a starker borderline between the different views of interpretability that seem to be difficult to reconcile. In a sense, our survey is more suitable for technically-oriented readers due to some mathematical details, although casual readers may find useful references for relevant popular items, from which they may develop interests in this young research field. Conversely, algorithm users that need interpretability in their work might develop an inclination to understand what is previously hidden in the thick veil of mathematical formulation, which might ironically undermine reliability and interpretability. Clinicians and medical practitioners already having some familiarity with mathematical terms may get a glimpse on how some proposed interpretability methods might be risky and unreliable. The survey [30] views interpretability in terms of extraction of relational knowledge, more specifically, by scrutinizing the methods under neuralsymbolic cycle. It presents the framework as a sub-category within the interpretability literature. We include it under verbal interpretability, though the framework does demonstrate that methods in other categories can be perceived under verbal interpretability as well. The extensive survey [18] provides a large list of researches categorized under transparent model and models requiring post-hoc analysis with multiple subcategories. Our survey, on the other hand, aims to overview the state of interpretable machine learning as applied to the medical field.\nThis paper is arranged as the following. Section II introduces generic types of interpretability and their sub-types. In each section, where applicable, we provide challenges and future prospects related to the category. Section III applies the categorization of interpretabilities in section II to medical field and lists a few risks of machine interpretability in the medical field. Before we proceed, it is also imperative to point out that the issue of accountability and interpretability has spawned discussions and recommendations [31]\u2013[33], and even entered the sphere of ethics and law enforcements [34], engendering movements to protect the society from possible misuses and harms in the wake of the increasing use of AI."}, {"heading": "II. TYPES OF INTERPRETABILITY", "text": "There has yet to be a widely-adopted standard to understand ML interpretability, though there have been works proposing frameworks for interpretability [10], [13], [35]. In fact, different works use different criteria, and they are justifiable in one\nway or another. Reference [36] suggests network dissection for the interpretability of visual representations and offers a way to quantify it as well. The interactive websites [37], [38] have suggested a unified framework to study interpretabilities that have thus-far been studied separately. The paper [39] defines a unified measure of feature importance in the SHAP (SHapley Additive exPlanations) framework. Here, we categorize existing interpretabilities and present a non-exhaustive list of works in each category.\nThe two major categories presented here, namely perceptive interpretability and interpretability by mathematical structures, as illustrated in fig. 1, appear to present different polarities within the notion of interpretability. An example of the difficulty with perceptive interpretability is as the following. When a visual \u201cevidence\u201d is given erroneously, the algorithm or method used to generate the \u201cevidence\u201d and the underlying mathematical structure sometimes do not offer any useful clues on how to fix the mistakes. On the other hand, a mathematical analysis of patterns may provide information in high dimensions. They can only be easily perceived once the pattern is brought into lower dimensions, abstracting some fine-grained information we could not yet prove is not discriminative with measurable certainty."}, {"heading": "A. Perceptive Interpretability", "text": "We include in this category interpretabilities that can be humanly perceived, often one that will be considered obvious. For example, as shown in fig. 2(A2), an algorithm that classifies an image into the cat category can be considered obviously interpretable if it provides segmented patch showing the cat as the explanation. We should note that this alone might on the other hand be considered insufficient, because (1) it still does not un-blackbox an algorithm and (2) it ignores the possibility of using background objects for its decision. The following are the sub-categories to perceptive interpretability. Refer to fig. 3 for the overview of the common sub-categories.\nA.1) Saliency Saliency method explains the decision of an algorithm by assigning values that reflect the importance of input components in their contribution to that decision. These values could take the forms of probabilities and super-pixels such as heatmaps etc. For example, fig. 2(A1) shows how a model predicts that the patient suffers from flu from a series of factors, but LIME [14] explains the choice by highlighting the importance of the particular symptoms that indicate that the illness should indeed be flu. Similarly, [40] computes the scores reflecting the n-grams activating convolution filters in NLP (Natural Language Processing). Fig. 2(A2) demonstrates the output that LIME will provide as the explanation for the choice of classifications cat and fig. 2(A3) demonstrates a kind of heatmap that shows the contribution of pixels to the segmentation result (segmentation result not shown, and this figure is only for demonstration). More formally, given that model f makes a prediction y = f(x) for input x, for some metric v, typically large magnitude of v(xi) indicates that the component xi is a significant reason for the output y.\nSaliency methods via decomposition have been developed. In general, they decompose signals propagated within their algorithm and selectively rearrange and process them to provide interpretable information. Class Activation Map (CAM) has been a popular method to generate heat/saliency/relevancemap (from now, we will use the terms interchangeably) that corresponds to discriminative features for classifications [41]\u2013 [43]. The original implementation of CAM [41] produces heatmaps using fk(x, y), the pixel-wise activation of unit k across spatial coordinates (x, y) in the last convolutional\nlayers, weighted by wck, the coefficient corresponding to unit k for class c. CAM at pixel (x, y) is thus given by Mc(x, y) = \u03a3kw c kfk(x, y).\nSimilarly, widely used Layer-wise Relevance Propagation (LRP) is introduced in [44]. Some papers that use LRP to construct saliency maps for interpretability include [13], [45]\u2013 [50]. It is also applicable for video processing [51]. A short summary for LRP is given in [52]. LRP is considered a decomposition method [53]. Indeed, the importance scores are decomposed such that the sum of the scores in each layer will be equal to the output. In short, the relevance score is the pixel-wise intensity at the input layer R(0) where\nR (l) i = \u03a3j\na (l) i w + ij \u03a3ia (l) i w + ij R (l+1) j is the relevance score of neuron\ni at layer l with the input layer being at l = 0. Each pixel (x, y) at the input layer is assigned the importance value R(0)(x, y), although some combinations of relevance scores {R(l)c } at inner layer l over different channels {c} have been demonstrated to be meaningful as well (though possibly less precise; see the tutorial in its website heatmapping.org). LRP can be understood in Deep Taylor Decomposition framework [54]. The code implementation can also be found in the aforementioned website.\nAutomatic Concept-based Explanations (ACE) algorithm [55] uses super-pixels as explanations. Other decomposition methods that have been developed include, DeepLIFT and gradient*input [56], Prediction Difference Analysis [57] and [40]. Peak Response Mapping [58] is generated by backpropagating peak signals. Peak signals are normalized and treated as probability, and the method can be seen as decomposition into probability transitions. In [59], Removed\ncorrelation \u03c1 is proposed as a metric to measure the quality of signal estimators. And then it proposes PatternNet and PatternAttribution that backpropagate parameters optimized against \u03c1, resulting in saliency maps as well. SmoothGrad [60] improves gradient-based techniques by adding noises. Do visit the related website that displays numerous visual comparison of saliency methods; be mindful of how some heatmaps highlight apparently irrelevant regions.\nFor natural language processing or sentiment analysis, saliency map can also take the form of heat scores over words in texts, as demonstrated by [61] using LRP and by [62]. In the medical field (see later section), [6], [43], [63]\u2013[69] have studied methods employing saliency and visual explanations. Note that we also sub-categorize LIME as a method that uses optimization and sensitivity as its underlying mechanisms, and many researches on interpretability span more than one subcategories.\nChallenges and Future Prospects. As seen, the formulas for CAM and LRP are given on a heuristic: certain ways of interaction between weights and the strength of activation of some units within the models will eventually produce the interpretable information. The intermediate processes are not amenable to scrutiny. For example, taking one of the weights and changing its value does not easily reveal any useful information. How these prescribed ways translate into interpretable information may also benefit from stronger evidences, especially evidences beyond visual verification of localized objects. Signal methods to investigate ML models (see later section) exist, but such methods that probe them with respect to the above methods have not been attempted systematically,\npossibly opening up a different research direction. A.2) Signal Method Methods of interpretability that observe the stimulation of neurons or a collection of neurons are called signal methods [70]. On the one hand, the activated values of neurons can be manipulated or transformed into interpretable forms. For example, the activation of neurons in a layer can be used to reconstruct an image similar to the input. This is possible because neurons store information systematically [71]: feature maps in the deeper layer activate more strongly to complex features, such as human face, keyboard etc while feature maps in the shallower layers show simple patterns such as lines and curves. Note: an example of feature map is the output of a convolutional filter in a Convolutional Neural Network (CNN). On the other hand, parameters or even the input data might be optimized with respect to the activation values of particular neurons using methods known as activation optimization (see a later section). The following are the relevant sub-categories.\nFeature maps and Inversions for Input Reconstructions. A feature map often looks like a highly blurred image with most region showing zero (or low intensity), except for the patch that a human could roughly discern as a detected feature. Sometimes, these discernible features are considered interpretable, as in [71]. However, they might be too distorted.\nThen, how else can a feature map be related to a humanlyperceptible feature? An inverse convolution map can be defined: for example, if feature map in layer 2 is computed in the network via y2 = f2(f1(x)) where x is the input, f1(.) consists of 7x7 convolutions of stride 2 followed by maxpooling and likewise f2(.). Then [71] reconstructs an image\nusing a deconvolution network by approximately inversing the trained convolutional network x\u0303 = deconv(y) = f\u0302\u221212 f\u0302 \u22121 1 (y) which is an approximation, because layers such as maxpooling have no unique inverse. It is shown that x\u0303 does appear like slightly blurred version of the original image, which is distinct to human eye. Inversion of image representations within the layers has also been used to demonstrate that CNN layers do store important information of an input image accurately [72], [73]. Guided backpropagation [74] modifies the way backpropagation is performed to achieve inversion by zeroing negative signals from both the output or input signals backwards through a layer. Indeed, inversion-based methods do use saliency maps for visualization of the activated signals.\nActivation Optimization. Besides transforming the activation of neurons, signal method also includes finding input images that optimize the activation of a neuron or a collection of neurons. This is called the activation maximization. Starting with a noise as an input x, the noise is slowly adjusted to increase the activation of a select (collection of) neuron(s) {ak}. In simple mathematical terms, the task is to find x0 = argmax ||{ak}|| where optimization is performed over input x and ||.|| is a suitable metric to measure the combined strength of activations. Finally the optimized input that maximizes the activation of the neuron(s) can emerge as something visually recognizable. For example, the image could be a surreal fuzzy combination of swirling patterns and parts of dog faces, as shown in fig. 2(B).\nResearch works on activation maximization include [75] on MNIST dataset, [76] and [77] that uses a regularization function. In particular, [37] provides an excellent interactive interface (feature visualization) demonstrating activationmaximized images for GoogLeNet [78]. GoogLeNet has a deep architecture, from which we can see how neurons in deeper layer stores complex features while shallower layer stores simple patterns; see fig. 2(B). To bring this one step further, the semantic dictionary is used [38] to provide a visualization of activations within a higher-level organization and semantically more meaningful arrangements.\nOther Observations of Signal Activations. Ablation studies [79], [80] also study the roles of neurons in shallower and deeper layers. In essence, some neurons are corrupted and the output of the corrupted neural network is compared to the original network.\nChallenges and Future Prospects. Signal methods might have revealed some parts of the black-box mechanisms. Many questions still remain. \u2022 What do we do with the (partially) reconstructed images\nand images that optimize activation? \u2022 We might have learned how to approximately inverse\nsignals to recover images, can this help improve interpretability further? \u2022 The components and parts in the intermediate process that reconstruct the approximate images might contain important information; will we be able to utilize them in the future? \u2022 How is explaining the components in this \u201cinverse space\u201d more useful than explaining signals that are forward propagated?\n\u2022 Similarly, how does looking at intermediate signals that lead to activation optimization help us pinpoint the role of a collection of neurons? \u2022 Optimization of highly parameterized functions notoriously gives non-unique solutions. Can we be sure that optimization that yields combination of surreal dog faces will not yield other strange images with minor alteration?\nIn the process of answering these questions, we may find hidden clues required to get closer to interpretable AI.\nA.3) Verbal Interpretability This form of interpretability takes the form of verbal chunks that human can grasp naturally. Examples include sentences that indicate causality, as shown in the examples below.\nLogical statements can be formed from proper concatenation of predicates, connectives etc. An example of logical statement is the conditional statement. Conditional statements are statements of the form A \u2192 B, in another words if A then B. An ML model from which logical statements can be extracted directly has been considered obviously interpretable. The survey [30] shows how interpretability methods in general can be viewed under such symbolic and relational system. In the medical field, see for example [81], [82].\nSimilarly, decision sets or rule sets have been studied for interpretability [83]. The following is a single line in a rule set \u201crainy and grumpy or calm \u2192 dairy or vegetables\u201d, directly quoted from the paper. Each line in a rule set contains a clause with an input in disjunctive normal form (DNF) mapped to an output in DNF as well. The example above is formally written (rainy\u2227grumpy)\u2228calm\u2192dairy\u2228vegetables. Comparing three different variables, it is suggested that interpretability of explanations in the form of rule sets is most affected by cognitive chunks, explanation size and little effected by variable repetition. Here, a cognitive chunk is defined as a clause of inputs in DNF and the number of (repeated) cognitive chunks in a rule set is varied. The explanation size is selfexplanatory (a longer/shorter line in a rule set, or more/less lines in a rule set). MUSE [84] also produces explanation in the form of decision sets, where interpretable model is chosen to approximate the black-box function and optimized against a number of metrics, including direct optimization of interpretability metrics.\nIt is not surprising that verbal segments are provided as the explanation in NLP problems. An encoder-generator framework [85] extracts segment like \u201ca very pleasant ruby redamber color\u201d to justify 5 out of 5-star rating for a product review. Given a sequence of words x = (x1, ..., xl) with xk \u2208 Rd, explanation is given as the subset of the sentence that gives a summary of why the rating is justified. The subset can be expressed as the binary sequence (z1, ..., zl) where zk = 1(0) indicates xk is (not) in the subset. Then z follows a probability distribution with p(z|x) decomposed by assuming independence to \u03a0kp(zk|x) where p(zk|x) = \u03c3z(W z[ \u2212\u2192 hk, \u2190\u2212 hk]+b z), with \u2212\u2192 ht , \u2190\u2212 ht being the usual hidden units in the recurrent cell (forward and backward respectively). Similar segments are generated using filter-attribute probability density function to improve the relation between the activation of certain filters and specific attributes [86]. Earlier works on Visual Question Answering (VQA) [87]\u2013[89] are concerned\nwith the generation of texts discussing objects appearing in images.\nChallenges and Future Prospects. While texts appear to provide explanations, the underlying mechanisms used to generate the texts are not necessarily explained. For example, NNs and the common variants/components used in text-related tasks such as RNN (recurrent NN), LSTM (long short term memory) are still black-boxes that are hard to troubleshoot in the case of wrong predictions. There have been less works that probe into the inner signals of LSTM and RNN neural networks. This is a possible research direction, although similar problem as mentioned in the previous sub-subsection may arise (what to do with the intermediate signals?). Furthermore, while word embedding is often optimized with the usual loss minimization, there does not seem to be a coherent explanation to the process and shape of the optimized embedding. There may be some clues regarding optimization residing within the embedding, and thus successfully interpreting the shape of embedding may help shed light into the mechanism of the algorithm.\nB. Interpretability via Mathematical Structure\nMathematical structures have been used to reveal the mechanisms of ML and NN algorithms. In the previous section, deeper layer of NN is shown to store complex information while shallower layer stores simpler information [71]. TCAV [95] has been used to show similar trend, as suggested by fig. 5(A2). Other methods include clustering such as t-SNE (t-Distributed Stochastic Neighbor Embedding) shown in fig. 5(B) and subspace-related methods, for example correlation-based Singular Vector Canonical Correlation Analysis (SVCCA) [96] is used to find the significant directions in the subspace of input for accurate prediction, as shown in figure 5(C). Information theory has been used to study interpretability by considering Information Bottleneck principle [97], [98]. The rich ways in which mathematical structures add to the interpretability pave ways to a comprehensive view of the interpretability of algorithms, hopefully providing a ground for unifying the different views under a coherent framework in the future. Fig. 4 provides an overview of ideas under this category.\nB.1) Pre-defined Model To study a system of interest, especially complex systems with not well-understood behaviour, mathematical formula such as parametric models can help simplify the tasks. With a proper hypothesis, relevant terms and parameters can be designed into the model. Interpretation of the terms come naturally if the hypothesis is either consistent with available knowledge or at least developed with good reasons. When the systems are better understood, these formula can be improved by the inclusion of more complex components. In the medical field (see later section), an example is kinetic modelling. Machine learning can be used to compute the parameters defined in the models. Other methods exist, such as integrating commonly available methodologies with subject specific contents etc. For example, Generative Discriminative Models [99], combining ridge regression and least square\nmethod to handle variables for analyzing Alzheimer\u2019s disease and schizophrenia.\nLinearity. The simplest interpretable pre-defined model is the linear combination of variables y = \u03a3iaixi where ai is the degree of how much xi contributes to the prediction y. A linear combination model with xi \u2208 {0, 1} has been referred to as the additive feature attribution method [39]. If the model performs well, this can be considered highly interpretable. However, many models are highly non-linear. In such cases, studying interpretability via linear properties (for example, using linear probe; see below) are useful in several ways, including the ease of implementation. When linear property appears to be insufficient, non-linearity can be introduced; it is typically not difficult to replace the linear component \u2212\u2192w \u00b7 \u2212\u2192a within the system with a non-linear version f(\u2212\u2192w ,\u2212\u2192a ).\nA linear probe is used in [100] to extract information from each layer in a neural network. More technically, assume we have deep learning classifier F (x) \u2208 [0, 1]D where Fi(x) \u2208 [0, 1] is the probability that input x is classified into class i out of D classes. Given a set of features Hk at layer k of a neural network, then the linear probe fk at layer k is defined as a linear classifier fk : Hk \u2192 [0, 1]D i.e. f(hk) = softmax(Whk+ b). In another words, the probe tells us how well the information from only layer k can predict the output, and each of this predictive probe is a linear classifier by design. The paper then shows plots of the error rate of the prediction made by each fk against k and demonstrates that these linear\nclassifiers generally perform better at deeper layer, that is, at larger k.\nGeneral Additive Models. Linear model is generalized by the Generalized Additive Model (GAM) [101], [102] with standard form g(E[y]) = \u03b20 + \u03a3fj(xj) where g is the link function. The equation is general, and specific implementations of fj and link function depend on the task. The familiar General Linear Model (GLM) is GAM with the specific implementation of linear fj and g is the identity. Modifications can be duly implemented. As a natural extension to the model, interaction terms between variables fij(xi, xj) are used [103]; we can certainly extend this indefinitely. ProtoAttend [104] uses probabilities as weights in the linear component of the NN. Such model is considered inherently interpretable by the authors. In the medical field, see [81], [99], [105], [106].\nContent-subject-specific model. Some algorithms are considered obviously interpretable within its field. Models are designed based on existing knowledge or empirical evidence, and thus interpretation of the models is innately embedded into the system. ML algorithms can then be incorporated in rich and diverse ways, for example, through parameter fitting. The following lists just a few works to illustrate the usage diversity of ML algorithms. Deep Tensor Neural Network is used for quantum many-body systems [107]. Atomistic neural network architecture for quantum chemistry is used in [108], where each atom is like a node in a graph with a set of feature vectors. The specifics depend on the neural network used, but this model is considered inherently interpretable. Neural network has been used for programmable wireless environments (PWE) [109]. TS approximation [110] is a fuzzy network approximation of other neural networks. The approximate fuzzy system is constructed with choices of components that can be adapted to the context of interpretation. The paper itself uses sigmoidbased membership function, which it considers interpretable.\nA so-called model-based reinforcement learning is suggested to be interpretable after the addition of high level knowledge about the system that is realized as Bayesian structure [111].\nChallenges and Future Prospects. The challenge of formulating the \u201ccorrect\u201d model exists regardless of machine learning trend. It might be interesting if a system is found that is fundamentally operating on a specific machine learning model. Backpropagation-based deep NN (DNN) itself is inspired by the brain, but they are not operating at fundamental level of similarity (nor is there any guarantee that such model exists). When interpretability is concerned, having fundamental similarity to real, existing systems may push forward our understanding of machine learning model in unprecedented ways. Otherwise, in the standard uses of machine learning algorithm, different optimization paradigms are still being discovered. Having optimization paradigm that is specialized for specific models may be contribute to a new aspect of interpretable machine learning.\nB.2) Feature Extraction We give an intuitive explanation via a hypothetical example of a classifier for heart-attack prediction. Given, say, 100- dimensional features including eating pattern, job and residential area of a subject. A kernel function can be used to find out that the strong predictor for heart attack is a 100- dimensional vector which is significant in the following axes: eating pattern, exercise frequency and sleeping pattern. Then, this model is considered interpretable because we can link heart-attack risk with healthy habits rather than, say sociogeographical factors. More information can be drawn from the next most significant predictor and so on.\nCorrelation. The methods discussed in this section include the use of correlation in a general sense. This will naturally include covariance matrix and correlation coefficients after transformation by kernel functions. A kernel function transforms high-dimensional vectors such that the transformed vectors better distinguish different features in the data. For example, the Principal Component Analysis transforms vectors into the principal components (PC) that can be ordered by the eigenvalues of singular-value-decomposed (SVD) covariance matrix. The PC with the highest eigenvalue is roughly the most informative feature. Many kernel functions have been introduced, including the Canonical Correlation Analysis (CCA) [112]. CCA provides the set of features that transforms the original variables to the pairs of canonical variables, where each pair is a pair of variables that are \u201cbest correlated\u201d but not correlated to other pairs. Quoted from [113], \u201csuch features can inherently characterize the object and thus it can better explore the insights and finer details of the problems at hand\u201d. In the previous sections, interpretability research using correlation includes [59].\nSVCCA combines CCA and SVD to analyze interpretability [96]. Given an input dataset X = {x1, ..., xm} where each input xi is possibly multi-dimensional. Denote the activation of neuron i at layer l as zli = (z l i(x1), ..., z l i(xm)). Note that one such output is defined for the entire input dataset. SVCCA finds out the relation between 2 layers of a network lk = {zlki |i = 1, ...,mk} for k = 1, 2 by taking l1 and l2 as the input (generally, lk does not have to be the entire\nlayer). SVCCA uses SVD to extract the most informative components l\u2032k and uses CCA to transform l \u2032 1 and l \u2032 2 such that l\u0304\u20321 = WX l \u2032 1 and l\u0304 \u2032 2 = WX l \u2032 2 have the maximum correlation \u03c1 = {\u03c11, ..., \u03c1min(m1,m2)}. One of the SVCCA experiments on CIFAR-10 demonstrates that only 25 most-significant axes in l\u2032k are needed to obtain nearly the full accuracy of a fullnetwork with 512 dimensions. Besides, the similarity between 2 compared layers is defined to be \u03c1\u0304 = 1min(m1,m2)\u03a3i\u03c1i.\nThe successful development of generative adversarial networks (GAN) [114]\u2013[116] for generative tasks have spawned many derivative works. GAN-based models have been able to generate new images not distinguishable from synthetic images and perform many other tasks, including transferring style from one set of images to another or even producing new designs for products and arts. Studies related to interpretabilities exist. For example [117] uses encoder-decoder system to perform multi-stage PCA. Generative model is used to show that natural image distribution modelled using probability density is fundamentally difficult to interpret [118]. This is demonstrated through the use of GAN for the estimation of image distribution density. The resulting density shows preferential accumulation of density of images with certain features (for examples, images featuring small object with few foreground distractions) in the pixel space. The paper then suggests that interpretability is improved once it is embedded in the deep feature space, for example, from GAN. In this sense, the interpretability is offered by better correlation between the density of images with the correct identification of the objects. Consider also the GAN-based works they cite.\nClustering. Algorithm such as t-SNE has been used to cluster input images based on their activation of neurons in a network [76], [119]. The core idea relies on the distance between objects being considered. If the distance between two objects are short in some measurement space, then they are similar. This possibly appeals to the notion of human learning by the Law of Association. It differs from correlation-based method which provides some metrics that relate the change of one variable with another, where the two related objects can originate from completely different domains; clustering simply presents their similarity, more sensibly in similar domain or in the subsets thereof. In [119], the activations {ffc7(x)} of 4096-dimensional layer fc7 in the CNN are collected over all input {x}. Then {ffc7(x)} is fed into t-SNE to be arranged and embedded into two-dimension for visualization (each point then is visually represented by the input image x). Activation atlases are introduced in [120], which similarly uses t-SNE to arrange some activations {fact(x)}, except that each point is represented by the average activations of feature visualization. In meta-material design [121], design pattern and optical responses are encoded into latent variables to be characterized by Variational Auto Encoder (VAE). Then, tSNE is used to visualize the latent space.\nIn the medical field (also see later section), we have [122], [123] (uses Laplacian Eigenmap for interpretability) [124] (introduces a low-rank representation method for Autistic Spectrum Diagnosis).\nChallenges and Future Prospects. This section exemplifies the difficulty in integrating mathematics and human intuition.\nHaving extracted \u201drelevant\u201d or \u201dsignificant\u201d features, sometimes we are left with still a combination of high dimensional vectors. Further analysis comes in the form of correlations or other metrics that attempt to show similarities or proximity. The interpretation may stay as mathematical artifact, but there is a potential that separation of concepts attained by these methods can be used to reorganize a black-box model from within. It might be an interesting research direction that lacks justification in terms of real-life application: however, progress in unraveling black-boxes may be a high-risk high-return investment.\nB.3) Sensitivity We group together methods that rely on localization, gradients and perturbations under the category of sensitivity. These methods rely on the notion of small changes dx in calculus and the neighborhood of a point in metric spaces.\nSensitivity to input noises or neighborhood of data points. Some methods rely on the locality of some input x. Let a model f(.) predicts f(x) accurately for some x. Denote x+ \u03b4 as a slightly noisy version of x. The model is locally faithful if f(x+ \u03b4) produces correct prediction, otherwise, the model is unfaithful and clearly such instability reduces its reliability. Reference [125] introduces meta-predictors as interpretability methods and emphasizes the importance of the variation of input x to neural network in explaining a network. They define explanation and local explanation in terms of the response of blackbox f to some input. Amongst many of the studies\nconducted, they provide experimental results on the effect of varying input such as via deletion of some regions in the input. Likewise, when random pixels of an image are deleted (hence the data point is shifted to its neighborhood in the feature space) and the resulting change in the output is tested [56], pixels that are important to the prediction can be determined. In text classification, [126] provides explanations in the form of partitioned graphs. The explanation is produced in three main steps, where the first step involves sampling perturbed versions of the data using VAE.\nTesting with Concept Activation Vectors (TCAV) has also been introduced as a technique to interpret the low-level representation of neural network layer [95]. First, the concept activation vector (CAV) is defined. Given input x \u2208 Rn and a feedforward layer l having m neurons, the activation at that layer is given by fl : Rn \u2192 Rm. If we are interested in the concept C, for example striped pattern, then, using TCAV, we supply a set PC of examples corresponding to striped pattern (zebra, clothing pattern etc) and the negative examples N . This collection is used to train a binary classifier vlC \u2208 Rm for layer l that partitions {fl(x) : x \u2208 PC} and {fl(x) : x \u2208 N}. In another words, a kernel function extracts features by mapping out a set of activations that has relevant information about the stripe-ness. CAV is thus defined as the normal vector to the hyperplane that separates the positive examples from the negative ones, as shown in fig. 5(A1). It then computes directional derivative Sv,k,l(x) = \u2207hl,k(fl(x)) \u00b7 vlC to obtain\nthe sensitivity of the model w.r.t. the concept C, where hl,k is the logit function for class k of C for layer l.\nLIME [14] optimizes over models g \u2208 G where G is a set of interpretable models G by minimizing locality-aware loss and complexity. In another words, it seeks to obtain the optimal model \u03be(x) = argming\u2208G L(f, g, \u03c0x) + \u2126(g) where \u2126 is the complexity and f is the true function we want to model. An example of the loss function is L(f, g, \u03c0x) = \u03a3z,z\u2032\u2208Z\u03c0x(z)[f(x) \u2212 g(z\u2032)]2 with \u03c0x(z) being, for example, Euclidean distance and Z is the vicinity of x. From the equation, it can be seen that the desired g will be close to f in the vicinity Z of x, because f(z) \u2248 g(z\u2032) for z, z\u2032 \u2208 Z. In another words, noisy inputs z, z\u2032 do not add too much losses.\nGradient-based explanation vector \u03be(x0) = \u2202\u2202xP (Y 6= g(x0)|X = x) is introduced by [127] for Bayesian classifier g(x) = argminc\u2208{i,...,C} P (Y 6= c|X = x), where x, \u03be are d-dimensional. For any i = 1, ..., d, high absolute value of [\u03be(x0)]i means that component i contributes significantly to the decision of the classifier. If it is positive, the higher the value is, the less likely x0 contributes to decision g(x0).\nACE algorithm [55] uses TCAV to compute saliency score and generate super-pixels as explanations. Grad-CAM [42] is a saliency method that uses gradient for its sensitivity measure. In [128], influence function is used. While theoretical, the paper also practically demonstrates how understanding the underlying mathematics will help develop perturbative training point for adversarial attack.\nSensitivity to dataset. A model is possibly sensitive to the training dataset {xi} as well. Influence function is also used to understand the effect of removing xi for some i and shows the consequent possibility of adversarial attack [128]. Studies on adversarial training examples can be found in the paper and its citations, where seemingly random, insignificant noises can degrade machine decision considerably. The representer theorem is introduced for studying the extent of effect xi has on a decision made by a deep NN [129].\nChallenges and Future Prospects. There seems to be a concern with locality and globality of the concepts. As mentioned in [95], to achieve global quantification for interpretability, explanation must be given for a set of examples or the entire class rather than \u201cjust explain individual data inputs\u201d. As a specific example, there may be a concern with the globality of TCAV. From our understanding, TCAV is a perturbation method by the virtue of stable continuity in the usual derivative and it is global because the whole subset of dataset with label k of concept C has been shown to be well-distinguished by TCAV. However, we may want to point out that despite their claim to globality, it is possible to view the success of TCAV as local, since it is only \u201cglobal within each label k rather than within all dataset considered at once.\nFrom the point of view of image processing, the neighborhood of a data point (an image) in the feature space poses a rather subtle question; also refer to fig. 4(C) for related illustration. For example, after rotating and stretching the image or deleting some pixels, how does the position of the image in the feature space change? Is there any way to control the effect of random noises and improve robustness of machine prediction in a way that is sensible to human\u2019s perception? The\ntransition in the feature space from one point to another point that belongs to different classes is also unexplored.\nOn a related note, gradients have played important roles in formulating interpretability methods, be it in image processing or other fields. Current trend recognizes that regions in the input space with significant gradients provide interpretability. Deforming these regions quickly degrades the prediction; conversely, the particular values at these regions are important to the reach a certain prediction. This is helpful, since calculus exists to help analyse gradients. However, this has shown to be disruptive as well. For example, imperceptible noises can degrade prediction drastically (see manipulation of explanations under the section Risk of Machine Interpretation in Medical Field). Since gradient is also in the core of loss optimization, it is a natural target for further studies.\nB.4) Optimization We have described several researches that seek to attain interpretability via optimization methods. Some have optimization at the core of their algorithm, but the interpretability is left to visual observation, while others optimize interpretability mathematically.\nQuantitatively maximizing interpretability. To approximate a function f , as previously mentioned, LIME [14] performs optimization by finding optimal model \u03be \u2208 G so that f(z) \u2248 \u03be(z\u2032) for z, z\u2032 \u2208 Z where Z is the vicinity of x, so that local fidelity is said to be achieved. Concurrently the complexity \u2126(\u03be) is minimized. Minimized \u2126 means the models interpretability is maximized. MUSE [84] takes in blackbox model, prediction and user-input features to output decision sets based on optimization w.r.t fidelity, interpretability and unambiguity. The available measures of interpretability that can be optimized include size, featureoverlap etc (refer to table 2 of its appendix).\nActivation Optimization. Activation optimizations are used in research works such as [37], [75]\u2013[77] as explained in a previous section. The interpretability relies on direct observation of the neuron-activation-optimized images. While the quality of the optimized images are not evaluated, the fact that parts of coherent images emerge with respect to a (collection of) neuron(s) does demonstrate some organization of information in the neural networks."}, {"heading": "C. Other Perspectives to Interpretability", "text": "There are many other concepts that can be related to interpretability. Reference [42] conducted experiments to test the improvements of human performance on a task after being given explanations (in the form of visualization) produced by ML algorithms. We believe this might be an exemplary form of interpretability evaluation. For example, we want to compare machine learning algorithms MLA with MLB . Say, human subjects are given difficult classification tasks and attain a baseline 40% accuracy. Repeat the task with different set of human subjects, but they are given explanations churned out by MLA and MLB . If the accuracies attained are now 50% and 80% respectively, then MLB is more interpretable.\nEven then, if human subjects cannot really explain why they can perform better with the given explanations, then\nthe interpretability may be questionable. This brings us to the question of what kind of interpretability is necessary in different tasks and certainly points to the possibility that there is no need for a unified version of interpretability.\nC.1) Data-driven Interpretability Data in catalogue. A large amount of data has been crucial to the functioning of many ML algorithms, mainly as the input data. In this section, we mention works that put a different emphasize on the treatment of these data arranged in catalogue. In essence, [10] suggests that we create a matrix whose rows are different real-world tasks (e.g. pneumonia detection), columns are different methods (e.g. decision tree with different depths) and the entries are the performance of the methods on some end-task. How can we gather a large collection of entries into such a large matrix? Apart from competitions and challenges, crowd-sourcing efforts will aid the formation of such database [147], [148]. A clear problem is how multidimensional and gigantic such tabulation will become, not to mention that the collection of entries is very likely uncountably many. Formalizing interpretability here means we pick latent dimensions (common criteria) that human can evaluate e.g. time constraint or time-spent, cognitive chunks (defined as the basic unit of explanation, also see the definition in [83]) etc. These dimensions are to be refined along iterative processes as more user-inputs enter the repository.\nIncompleteness. In [10], the problem of incompleteness of problem formulation is first posed as the issue in interpretability. Incompleteness is present in many forms, from the impracticality to produce all test-cases to the difficulty in justifying why a choice of proxy is the best for some scenarios. At the end, it suggests that interpretability criteria are to be born out of collective agreements of the majority, through a cyclical process of discoveries, justifications and rebuttals. In our opinion, a disadvantage is that there is a possibility that no unique convergence will be born, and the situation may aggravate if, say, two different conflicting factions are born, each with enough advocate. The advantage lies in the existence of strong roots for the advocacy of certain choice of interpretability. This prevents malicious intent from tweaking interpretability criteria to suit ad hoc purposes.\nC.2) Invariances Implementation invariance. Reference [93] suggests implementation invariance as an axiomatic requirement to interpretability. In the paper, it is stated as the following. Define two functionally equivalent functions as f1, f2 so that f1(x) = fx(x) for any x regardless of their implementation details. Given any two such networks using attribution method, then the attribution functional A will map the importance of each component of an input to f1 the same way it does to f2. In another words, (A[f1](x))j = (A[f2](x))j for any j = 1, , d where d is the dimension of the input. The statement can be easily extended to methods that do not use attribution as well.\nInput invariance. To illustrate using image classification problem, translating an image will also translate super-pixels demarcating the area that provides an explanation to the choice of classification correspondingly. Clearly, this property is desirable and has been proposed as an axiomatic invariance of a reliable saliency method. There has also been a study on\nthe input invariance of some saliency methods with respect to translation of input x\u2192 x+c for some c [70]. Of the methods studied, gradients/sensitivity-based methods [127] and signal methods [71], [74] are input invariant while some attribution methods, such as integrated gradient [93], are not.\nC.3) Interpretabilities by Utilities The following utilities-based categorization of interpretability is proposed by [10]. Application-based. First, an evaluation is applicationgrounded if human A gives explanation XA on a specific application, so-called the end-task (e.g. a doctor performs diagnosis) to human B, and B performs the same task. Then A has given B a useful explanation if B performs better in the task. Suppose A is now a machine learning model, then the model is highly interpretable if human B performs the same task with improved performance after given XA. Some medical segmentation works will fall into this category as well, since the segmentation will constitute a visual explanation for further diagnosis/prognosis [143], [144] (also see other categories of the grand challenge). Such evaluation is performed, for example, by [42]. They proposed Grad-CAM applied on guided backpropagation (proposed by [74]) of AlexNet CNN and VGG. The produced visualizations are used to help human subjects in Amazon Mechanical Turks identify objects with higher accuracy in predicting VOC 2007 images. The human subjects achieved 61.23% accuracy, which is 16.79% higher than visualization provided by guided backpropagation.\nHuman-based. This evaluation involves real humans and simplified tasks. It can be used when, for some reasons or another, having human A give a good explanation XA is challenging, possibly because the performance on the task cannot be evaluated easily or the explanation itself requires specialized knowledge. In this case, a simplified or partial problem may be posed and XA is still demanded. Unlike the application-based approach, it is now necessary to look at XA specifically for interpretability evaluation. Bigger pool of human subjects can then be hired to give a generic valuation to XA or create a model answer X\u0302A to compare XA with, and then a generic valuation is computed.\nNow, suppose A is a machine learning model, A is more interpretable compared to another ML model if it scores better in this generic valuation. In [145], a ML model is given a document containing the conversation of humans making a plan. The ML model produces a \u201dreport\u201d containing relevant predicates (words) for the task of inferring what the final plan is. The metric used for interpretability evaluation is, for example, the percentage of the predicates that appear, compared to human-made report. We believe the format of human-based evaluation needs not be strictly like the above. For example, hybrid human and interactive ML classifiers require human users to nominate features for training [146]. Two different standard MLs can be compared to the hybrid, and one can be said to be more interpretable than another if it picks up features similar to the hybrid, assuming they perform at similarly acceptable level.\nFunctions-based. Third, an evaluation is functionallygrounded if there exist proxies (which can be defined a priori) for evaluation, for example sparsity [10]. Some papers [2],\n[5], [41]\u2013[43], [95], [96], [143], [144] use metrics that rely on this evaluation include many supervised learning models with clearly defined metrics such as (1) Dice coefficients (related to visual interpretability), (2) attribution values, components of canonically transformed variables (see for example CCA) or values obtained from dimensionality reduction methods (such as components of principal components from PCA and their corresponding eigenvalues), where interpretability is related to the degree an object relates to a feature, for example, classification of a dog has high values in the feature space related to four limbs, shape of snout and paws etc. Which suitable metrics to use are highly dependent on the tasks at hand.\nIII. XAI IN MEDICAL FIELD\nML has also gained traction recently in the medical field, with large volume of works on automated diagnosis, prognosis [149]. From the grand-challenge.org, we can see many different challenges in the medical field have emerged and galvanized researches that use ML and AI methods. Amongst successful deep learning models are [2], [5], using U-Net for medical segmentation. However, being a deep learning neural network, U-Net is still a blackbox; it is not very interpretable. Other domain specific methods and special transformations (denoising etc) have been published as well; consider for example [130] and many other works in MICCAI publications.\nIn the medical field the question of interpretability is far\nfrom just intellectual curiosity. More specifically, it is pointed out that interpretabilities in the medical fields include factors other fields do not consider, including risk and responsibilities [21], [150], [151]. When medical responses are made, lives may be at stake. To leave such important decisions to machines that could not provide accountabilities would be akin to shirking the responsibilities altogether. Apart from ethical issues, this is a serious loophole that could turn catastrophic when exploited with malicious intent.\nMany more works have thus been dedicated to exploring explainability in the medical fields [11], [20], [43]. They provide summaries of previous works [21] including subfieldspecific reviews such as [25] for chest radiograph and sentiment analysis in medicine [160], or at least set aside a section to promote awareness for the importance of interpretability in the medical field [161]. In [162], it is stated directly that being a black-box is a \u201cstrong limitation\u201d for AI in dermatology, as it is not capable of performing customized assessment by certified dermatologist that can be used to explain clinical evidence. On the other hand, the exposition [163] argues that a certain degree of opaqueness is acceptable, i.e. it might be more important that we produce empirically verified accurate results than focusing too much on how to the unravel the blackbox. We recommend readers to consider them first, at least for an overview of interpretability in the medical field.\nWe apply categorization from the previous section to the ML and AI in the medical field. Table III shows categorization obtained by tagging (1) how interpretability method is incorporated: either through direct application of existing methods, methodology improvements or comparison between interpretability methods and (2) the organs targeted by the diseases e.g. brain, skin etc. As there is not yet a substantial number of significant medical researches that address interpretability, we will refrain from presenting any conclusive trend. However, from a quick overview, we see that the XAI research community might benefit from more studies comparing different existing methods, especially those with more informative conclusion on how they contribute to interpretability."}, {"heading": "A. Perceptive Interpretability", "text": "Medical data could come in the form of traditional 2D images or more complex formats such as NIFTI or DCOM\nwhich contain 3D images with multiple modalities and even 4D images which are time-evolving 3D volumes. The difficulties in using ML for these data include the following. Medical images are sometimes far less available in quantity than common images. Obtaining these data requires consent from the patients and other administrative barriers. High dimensional data also add complexity to data processing and the large memory space requirement might prevent data to be input without modification, random sampling or down-sizing, which may compromise analysis. Other possible difficulties with data collection and management include as left/rightcensoring, patients\u2019 death due to unrelated causes or other complications etc.\nWhen medical data is available, ground-truth images may not be correct. Not only do these data require some specialized knowledge to understand, the lack of comprehensive understanding of biological components complicates the analysis. For example, ADC modality of MR images and the isotropic version of DWI are in some sense derivative, since both are computed from raw images collected by the scanner. Furthermore, many CT or MRI scans are presented with skullstripping or other pre-processing. However, without a more complete knowledge of what fine details might have been accidentally removed, we cannot guarantee that an algorithm can capture the correct features.\nA.1) Saliency The following articles consist of direct applications of existing saliency methods. Chexpert [6] uses GradCAM for visualization of pleural effusion in a radiograph. CAM is also used for interpretability in brain tumour grading [152]. Reference [67] uses Guided Grad-CAM and feature occlusion, providing complementary heatmaps for the classification of Alzheimer\u2019s disease pathologies. Integrated gradient method and SmoothGrad are applied for the visualization of CNN ensemble that classifies estrogen receptor status using breast MRI [68]. LRP on DeepLight [47] was applied on fMRI data from Human Connectome Project to generate heatmap visualization. Saliency map has also been computed using primitive gradient of loss, providing interpretability to the neural network used for EEG (Electroencephalogram) sleep stage scoring [153]. There has even been a direct comparison between the feature maps within CNN and skin lesion images [154], overlaying the scaled feature maps on top of the images as a means to interpretability. Some images correspond to relevant features in the lesion, while others appear to explicitly capture artifacts that might lead to prediction bias.\nThe following articles are focused more on comparison between popular saliency methods, including their derivative/improved versions. Reference [158] trains an artificial neural network for the classification of insomnia using physiological network (PN). The feature relevance scores are computed from several methods, including DeepLIFT [56]. Comparison between 4 different visualizations is performed in [157]. It shows different attributions between different methods and concluded that LRP and guided backpropagation provide the most coherent attribution maps in their Alzheimer\u2019s disease study. Basic tests on GradCAM and SHAP on dermoscopy images for melanoma classification are conducted,\nconcluding with the need for significant improvements to heatmaps before practical deployment [159].\nThe following includes slightly different focus on methodological improvements on top of the visualization. RespondCAM [43] is derived from [41], [42], and provides a saliencymap in the form of heat-map on 3D images obtained from Cellular Electron Cryo-Tomography. High intensity in the heatmap marks the region where macromolecular complexes are present. Multi-layer class activation map (MLCAM) is introduced in [90] for glioma (a type of brain tumor) localization. Multi-instance (MI) aggregation method is used with CNN to classify breast tumour tissue microarray (TMA) image\u2019s for 5 different tasks [64], for example the classification of the histologic subtype. Super-pixel maps indicate the region in each TMA image where the tumour cells are; each label corresponds to a class of tumour. These maps are proposed as the means for visual interpretability. Also, see the activation maps in [65] where interpretability is studied by corrupting image and inspecting region of interest (ROI). The autofocus module from [66] promises improvements in visual interpretability for segmentation on pelvic CT scans and segmentation of tumor in brain MRI using CNN. It uses attention mechanism (proposed by [91]) and improves it with adaptive selection of scale with which the network \u201dsees\u201d an object within an image. With the correct scale adopted by the network while performing a single task, human observer analysing the network can understand that a neural network is properly identifying the object, rather than mistaking the combination of the object plus the surrounding as the object itself.\nThere is also a different formulation for the generation of saliency maps [69]. It defines a different softmax-like formula to extract signals from DNN for visual justification in classification of breast mass (malignant/benign). Textual justification is generated as well.\nA.2) Verbal In [81], a rule-based system could provide the statement has asthma \u2192 lower risk, where risk here refers to death risk due to pneumonia. Likewise, [82] creates a model called Bayesian Rule Lists that provides such statements for stroke prediction. Textual justification is also provided in the LSTM-based breast mass classifier system [69]. The argumentation theory is implemented in the machine learning training process [155], extracting arguments or decision rules as the explanations for the prediction of stroke based on the Asymptomatic Carotid Stenosis and Risk of Stroke (ACSRS) dataset.\nOne should indeed look closer at the interpretability in [81]. Just as many MLs are able to extract some humanly non-intuitive pattern, the rule-based system seems to have captured the strange link between asthma and pneumonia. The link becomes clear once the actual explanation based on real situation is provided: a pneumonia patient which also suffers from asthma is often sent directly to the Intensive Care Unit (ICU) rather than a standard ward. Obviously, if there is a variable ICU=0 or 1 that indicates admission to ICU, then a better model can provide more coherent explanation \u201dasthma\u2192ICU\u2192lower risk\u201d. In the paper, the model appears not to identify such variable. We can see that interpretability\nissues are not always clear-cut. Several researches on Visual Question Answering in the medical field have also been developed. The initiative by ImageCLEF [164], [165] appears to be at its center, though VQA itself has yet to gain more traction and successful practical demonstration in the medical sector before widespread adoption.\nChallenges and Future Prospects for perceptive interpretability in medical sector. In many cases, where saliency maps are provided, they are provided with insufficient evaluation with respect to their utilities within the medical practices. For example, when providing importance attribution to a CT scan used for lesion detection, are radiologists interested in heatmaps highlighting just the lesion? Are they more interested in looking for reasons why a haemorrhage is epidural or subdural when the lesion is not very clear to the naked eyes? There may be many such medically-related subtleties that interpretable AI researchers may need to know about.\nB. Interpretability via Mathematical Structure\nB.1) Pre-defined Model Models help with interpretability by providing a generic sense of what a variable does to the output variable in question, whether in medical fields or not. A parametric model is usually designed with at least an estimate of the working mechanism of the system, with simplification and based on empirically observed patterns. For example, [130] uses kinetic model for the cerebral blood flow in ml/100g/min with\nCBF = f(\u2206M) 6000\u03b2\u2206Mexp(PLDT1b )\n2\u03b1T1b(SIPD)(1\u2212 exp(\u2212 \u03c4T1b )) (1)\nwhich depends on perfusion-weighted image \u2206M obtained from the signal difference between labelled image of arterial blood water treated with RF pulses and the control image. This function is incorporated in the loss function in the training pipeline of a fully convolutional neural network. At least, an interpretation can be made partially: the neural network model is designed to denoise a perfusion-weighted image (and thus improve its quality) by considering CBF. How the network understands the CBF is again an interpretability problem of a neural network which has yet to be resolved.\nThere is an inherent simplicity in the interpretability of models based on linearity, and thus they have been considered obviously interpretable as well; some examples include linear combination of clinical variables [99], metabolites signals for MRS [105] etc. Linearity in different models used in the estimation of brain states is discussed in [106], including how it is misinterpreted. It compares what it refers to as forward and backward models and then suggested improvement on linear models. In [81], a logistic regression model picked up a relation between asthma and lower risk of pneumonia death, i.e. asthma has a negative weight as a risk predictor in the regression model. Generative Discriminative Machine (GDM) combines ordinary least square regression and ridge regression to handle confounding variables in Alzheimers disease and schizophrenia dataset [99]. GDM parameters are said to be interpretable, since they are linear combinations\nof the clinical variables. Deep learning has been used for PET pharmacokinetic (PK) modelling to quantify tracer target density [131]. CNN has helped PK modelling as a part of a sequence of processes to reduce PET acquisition time, and the output is interpreted with respect to the golden standard PK model, which is the linearized version of Simplified Reference Tissue Model (SRTM). Deep learning method is also used to perform parameters fitting for Magnetic Resonance Spectroscopy (MRS) [105]. The parametric part of the MRS signal model specified, x(t) = \u03a3amxm(t)e\u2206\u03b1mt+2\u03c0i\u2206fmt, consists of linear combination of metabolite signals xm(t). The paper shows that the error measured in SMAPE (symmetric mean absolute percentage error) is smallest for most metabolites when their CNN model is used. In cases like this, clinicians may find the model interpretable as long as the parameters are well-fit, although the neural network itself may still not be interpretable.\nThe models above use linearity for studies related to brain or neuro-related diseases. Beyond linear models, other brain and neuro-systems can be modelled with relevant subject-content knowledge for better interpretability as well. Segmentation task for the detection of brain midline shift is performed using using CNN with standard structural knowledge incorporated [132]. A template called model-derived age norm is derived from mean values of sleep EEG features of healthy subjects [156]. Interpretability is given as the deviation of the features of unhealthy subject from the age norm.\nOn a different note, reinforcement learning (RL) has been applied to personalized healthcare. In particular, [133] introduces group-driven RL in personalized healthcare, taking into considerations different groups, each having similar agents. As usual, Q-value is optimized w.r.t policy \u03c0\u03b8, which can be qualitatively interpreted as the maximization of rewards over time over the choices of action selected by many participating agents in the system.\nChallenges and Future Prospects. Models may be simplifying intractable system. As such, the full potential of machine learning, especially DNN with huge number of parameters, may be under-used. A possible research direction that taps onto the hype of predictive science is as the following: given a model, is it possible to augment the model with new, sophisticated components, such that parts of these components can be identified with (and thus interpreted as) new insights? Naturally, the augmented model needs to be comparable to previous models and shown with clear interpretation why the new components correspond to insights previously missed. Do note that there are critiques against the hype around the potential of AI which we will leave to the readers.\nB.2) Feature extraction Vanilla CNN is used in [141] but it is suggested that interpretability can be attained by using a separable model. The separability is achieved by polynomial-transforming scalar variables and further processing, giving rise to weights useful for interpretation. In [122], fMRI is analyzed using correlationbased functional graphs. They are then clustered into supergraph, consisting of subnetworks that are defined to be interpretable. A convolutional layer is then used on the supergraph. For more references about neural networks designed for\ngraph-based problems, see the papers citations. The following are further sub-categorization for methods that revolve around feature extraction and the evaluations or measurements (such as correlations) used to obtain the features, similar to the previous section.\nCorrelation. DWT-based method (discrete wavelet transform) is used to perform feature extraction before eventually feeding the EEG data (after a series of processings) into a neural network for epilepsy classification [134]. A fuzzy relation analogous to correlation coefficient is then defined. Furthermore, as with other transform methods, the components (the wavelets) can be interpreted component-wise. As a simple illustration, the components for Fourier transform could be taken as how much certain frequency is contained in a time series. Reference [135] mentioned a host of wavelet-based feature extraction methods and introduced maximal overlap discrete wavelet package transform (MODWPT) also applied on EEG data for epilepsy classification.\nFrame singular value decomposition (F-SVD) is introduced for classifications of electromyography (EMG) data [113]. It is a pipeline involving a number of processing that includes DWT, CCA and SVD, achieving around 98% accuracies on classifications between amyotrophic lateral sclerosis, myopathy and healthy subjects. Consider also CCA-based papers that are cited in the paper, in particular citations 18 to 21 for EMG and EEG signals.\nClustering. VAE is used to obtain vectors in 64-dimensional latent dimension in order to predict whether the subjects suffer from hypertrophic cardiomyopathy (HCM) [123]. A non-linear transformation is used to create Laplacian Eigenmap (LE) with two dimensions, which is suggested as the means for interpretability. Skin images are clustered [138] for melanoma classification using k-nearest-neighbour that is customized to include CNN and triplet loss. A queried image is then compared with training images ranked according to similarity measure visually displayed as query-result activation map pair.\nt-SNE has been applied on human genetic data and shown to provide more robust dimensionality reduction compared to PCA and other methods [136]. Multiple maps t-SNE (mm-tSNE) is introduced by [137], performing clustering on phenotype similarity data.\nSensitivity. Regression Concept Vectors (RCV) is proposed along with a metric Br score as improvements to TCAV\u2019s concept separation [139]. The method is applied on breast cancer histopathology classification problem. Furthermore, Unit Ball Surface Sampling metric (UBS) is introduced [140] to address the shortcoming of Br score. It uses neural networks for classification of nodules for mammographic images. Guidelinebased Additive eXplanation (GAX) is introduced in [92] for diagnosis using CT lung images. Its pipeline includes LIME-like perturbation analysis and SHAP. Comparisons are then made with LIME, Grad-CAM and feature importance generated by SHAP.\nChallenges and Future Prospects. We observe popular uses of certain methods ingrained in specific sectors on the one hand and, on the other hand, emerging applications of sophisticated ML algorithms. As medical ML (in particular\nthe application of recently successful DNN) is still a young field, we see fragmented and experimental uses of existing or customized interpretable methods. As medical ML research progresses, the trade-off between many practical factors of ML methods (such as ease of use, ease of interpretation of mathematical structure possibly regarded as complex) and its contribution to the subject matter will become clearer. Future research and application may benefit from a practice of consciously and consistently extracting interpretable information for further processing, and the process should be systematically documented for good dissemination. Currently, with feature selections and extractions focused on improving accuracy and performance, we may still have vast unexplored opportunities in interpretability research."}, {"heading": "C. Other Perspectives", "text": "Data-driven. Case-Based Reasoning (CBR) performs medical evaluation (classifications etc) by comparing a query case (new data) with similar existing data from a database. [142] combines CBR with an algorithm that presents the similarity between these cases by visually providing proxies and measures for users to interpret. By observing these proxies, the user can decide to take the decision suggested by the algorithm or not. The paper also asserts that medical experts appreciate such visual information with clear decision-support system."}, {"heading": "D. Risk of Machine Interpretation in Medical Field", "text": "Jumping conclusion. According to [81], logical statements such as has asthma\u2192lower risk are considered interpretable. However, in the example, the statement indicates that a patient with asthma has lower risk of death from pneumonia, which\nmight be strange without any clarification from the intermediate thought process. While human can infer that the lowered risk is due to the fact that pneumonia patients with asthma history tend to be given more aggressive treatment, we cannot always assume there is a similar humanly inferable reason behind each decision. Furthermore, interpretability method such as LRP, deconvolution and guided backpropagation introduced earlier are shown to not work for simple model, such as linear model, bringing into question their reliability [59]."}, {"heading": "IV. CONCLUSION", "text": "We present a survey on interpretability and explainability of ML algorithms in general, and place different interpretations suggested by different research works into distinct categories. From general interpretabilities, we apply the categorization into the medical field. Some attempts are made to formalize interpretabilities mathematically, some provide visual explanations, while others might focus on the improvement in task performance after being given explanations produced by algorithms. At each section, we also discuss related challenges and future prospects. Fig. 6 provides a diagram that summarizes all the challenges and prospects.\nManipulation of explanations. Given an image, a similar image can be generated that is perceptibly indistinguishable from the original, yet produces radically different output [94]. Naturally, its significance attribution and interpretable information become unreliable. Furthermore, explanation can even be manipulated arbitrarily [166]. For example, an explanation for the classification of a cat image (i.e. particular significant values that contribute to the prediction of cat) can be implanted into the image of a dog, and the algorithm could be fooled into classifying the dog image as a cat image. The risk in medical field is clear: even without malicious, intentional manipulation,\nnoises can render explanations wrong. Manipulation of algorithm that is designed to provide explanation is also explored in [167].\nIncomplete constraints. In [130], the loss function for the training of a fully convolutional network includes CBF as a constraint. However, many other constraints may play important roles in the mechanism of a living organ or tissue, not to mention applying kinetic model is itself a simplification. Giving an interpretation within limited constraints may place undue emphasis on the constraint itself. Other works that use predefined models might suffer similar problems [99], [105], [131].\nNoisy training data. The so-called ground truths for medical tasks, provided by professionals, are not always absolutely correct. In fact, news regarding how AI beats human performance in medical imaging diagnosis [168] indicates that human judgment could be brittle. This is true even of trained medical personnel. This might give rise to the classic garbagein-garbage-out situation.\nThe above risks are presented in large part as a reminder of the nature of automation. It is true that algorithms have been used to extract invisible patterns with some successes. However, one ought to view scientific problems with the correct order of priority. The society should not risk overallocating resources into building machine and deep learning models, especially since due improvements to understanding the underlying science might be the key to solving the root problem. For example, higher quality MRI scans might reveal key information not visible with current technology, and many models built nowadays might not be very successful because there is simply not enough detailed information contained in currently available MRI scans.\nFuture directions for clinicians and practitioners. Visual and textual explanation supplied by an algorithm might seem like the obvious choice; unfortunately, the details of decisionmaking by algorithms such as deep neural networks are still not clearly exposed. When an otherwise reliable deep learning model provides a strangely wrong visual or textual explanation, systematic methods to probe into the wrong explanations do not seem to exist, let alone methods to correct them. A specialized education combining medical expertise, applied mathematics, data science etc might be necessary to overcome this. For now, if \u201dinterpretable\u201d algorithms are deployed in medical practices, human supervision is still necessary. Interpretability information should be considered nothing more than complementary support for the medical practices before there is a robust way to handle interpretability.\nFuture directions for algorithm developers and researchers. Before the blackbox is un-blackboxed, machine decision always carries some exploitable risks. It is also clear that a unified notion of interpretability is elusive. For medical ML interpretability, more comparative studies between the performance of methods will be useful. The interpretability output such as heatmaps should be displayed and compared clearly, including poor results. In the best case scenario, clinicians and practitioners recognize the shortcomings of interpretable methods but have a general idea on how to handle them in ways that are suitable to medical practices.\nIn the worst case scenario, the inconsistencies between these methods can be exposed. The very troubling trend of journal publications emphasizing good results is precarious, and we should thus continue interpretability research with a mindset open to evaluation from all related parties. Clinicians and practitioners need to be given the opportunity for fair judgment of utilities of the proposed interpretability methods, not just flooded with performance metrics possibly irrelevant to the adoption of medical technology.\nAlso, there may be a need to shift interpretability study away from algorithm-centric studies. An authoritative body setting up the standard of requirements for the deployment of model building might stifle the progress of the research itself, though it might be the most efficient way to reach an agreement. This might be necessary to prevent damages, seeing that even corporate companies and other bodies non-academic in the traditional sense have joined the fray (consider health-tech start-ups and the implications). Acknowledging that machine and deep learning might not be fully mature for large-scale deployment, it might be wise to deploy the algorithms as a secondary support system for now and leave most decisions to the traditional methods. It might take a long time before humanity graduates from this stage, but it might be timely: we can collect more data to compare machine predictions with traditional predictions and sort out data ownership issues along the way."}, {"heading": "ACKNOWLEDGMENT", "text": "This research was supported by Alibaba Group Holding Limited, DAMO Academy, Health-AI division under AlibabaNTU Talent Program. The program is the collaboration between Alibaba and Nanyang Technological university, Singapore."}], "title": "A Survey on Explainable Artificial Intelligence (XAI): towards Medical XAI", "year": 2020}