{"abstractText": "The use of sophisticated machine learning models for critical decision making is faced with a challenge that these models are often applied as a \u2018black-box. This has led to an increased interest in interpretable machine learning, where post hoc interpretation presents a useful mechanism for generating interpretations of complex learning models. In this paper, we propose a novel approach underpinned by an extended framework of Bayesian networks for generating post hoc interpretations of a black-box predictive model. The framework supports extracting a Bayesian network as an approximation of the black-box model for a specific prediction. Compared to the existing post hoc interpretation methods, the contribution of our approach is three-fold. Firstly, the extracted Bayesian network, as a probabilistic graphical model, can provide interpretations about not only what input features, but also why these features contributed to a prediction. Secondly, for complex decision problems with many features, a Markov blanket can be generated from the extracted Bayesian network to provide interpretations with a focused view on those input features that directly contributed to a prediction. Thirdly, the extracted Bayesian network enables the identification of four different rules which can inform the decision-maker about the confidence level in a prediction, thus helping the decision-maker assess the reliability of predictions learned by a black-box model. We implemented the proposed approach, applied it in the context of two well-known public datasets and analysed the results, which are made available in an open-source repository.", "authors": [{"affiliations": [], "name": "Catarina Moreira"}, {"affiliations": [], "name": "Yu-Liang Chou"}, {"affiliations": [], "name": "Mythreyi Velmurugan"}, {"affiliations": [], "name": "Chun Ouyang"}, {"affiliations": [], "name": "Renuka Sindhgatta"}, {"affiliations": [], "name": "Peter Bruza"}], "id": "SP:b7bbae5c2213d26caa3536e2cc4bed0f0fa1a17e", "references": [{"authors": ["W.J. Murdoch", "C. Singh", "K. Kumbier", "R. Abbasi-Asl", "B. Yu"], "title": "Interpretable machine learning: definitions", "venue": "methods, and applications, CoRR abs/1901.04592 ", "year": 2019}, {"authors": ["Q.V. Liao", "D.M. Gruen", "S. Miller"], "title": "Questioning the AI: informing design practices for explainable AI user experiences", "venue": "CoRR abs/2001.02478 ", "year": 2020}, {"authors": ["R. Guidotti"], "title": "A survey of methods for explaining black box models, ACM Computing Survey", "year": 2018}, {"authors": ["H. Lakkaraju"], "title": "Faithful and customizable explanations of black box models, in", "venue": "Proceedings of the 2019 AAAI Conference on AIES 2019,", "year": 2019}, {"authors": ["Z.C. Lipton"], "title": "The mythos of model interpretability", "venue": "CACM 61 ", "year": 2018}, {"authors": ["F. Doshi-Velez", "B. Kim"], "title": "Towards a rigorous science of interpretable machine learning", "venue": "arxiv: 1702.08608 ", "year": 2017}, {"authors": ["A. Holzinger", "G. Langs", "H. Denk", "K. Zatloukal", "H. M\u00fcller"], "title": "Causability and explainability of artificial intelligence in medicine", "venue": "Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery 9 ", "year": 2019}, {"authors": ["M. Siering", "A.V. Deokar", "C. Janze"], "title": "Disentangling consumer recommendations: Explaining and predicting airline recommendations based on online reviews", "venue": "Decision Support Systems 107 ", "year": 2018}, {"authors": ["B. Kim", "J. Park", "J. Suh"], "title": "Transparency and accountability in ai decision support: Explaining and visualizing convolutional neural networks for text information", "venue": "Decision Support Systems 134 ", "year": 2020}, {"authors": ["M.T. Ribeiro", "S. Singh", "C. Guestrin"], "title": "Why Should I Trust You?\u201d: Explaining the predictions of any classifier", "venue": "in: Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining", "year": 2016}, {"authors": ["S. Lundberg", "S.-I. Lee"], "title": "A unified approach to interpreting model predictions", "venue": "in: Proceedings of the 31st Annual Conference on Neural Information Processing Systems (NIPS)", "year": 2017}, {"authors": ["M.A.-M. Radwa Elshawi"], "title": "Youssef Sherif", "venue": "S. Sakr, Interpretability in healthcare a comparative study of local machine learning interpretability techniques, in: Proceedings of IEEE Symposium on Computer-Based Medical Systems (CBMS)", "year": 2019}, {"authors": ["M. Stiffler", "A. Hudler", "E. Lee", "D. Braines", "D. Mott", "D. Harborne"], "title": "An analysis of the reliability of lime with deep learning models", "venue": "in: Proceedings of the Dstributed Analytics and Information Science International Technology Alliance", "year": 2018}, {"authors": ["H.F. Tan", "K. Song", "M. Udell", "Y. Sun"], "title": "Y", "venue": "Zhang, Why should you trust my interpretation? understanding uncertainty in lime predictions", "year": 2019}, {"authors": ["M.T. Ribeiro", "S. Singh", "C. Guestrin"], "title": "Anchors: High-precision model-agnostic explanations", "venue": "in: Proceedings of the 32nd AAAI International Conference on Artificial Intelligence", "year": 2018}, {"authors": ["L.S. Shapley"], "title": "A value for n-person games", "venue": "Rand coporation ", "year": 1952}, {"authors": ["E. Strumbelj", "I. Kononenko"], "title": "Explaining prediction models and individual predictions with feature contributions", "venue": "Knowledge and Information Systems 41 ", "year": 2013}, {"authors": ["A.C. Miller Janny Ariza-Garz\u00f3n", "Javier Arroyo", "M.-J"], "title": "Segovia-Vargas, Explainability of a machine learning granting scoring model in peer-to-peer lending", "venue": "in: Proceedings of IEEE Access,", "year": 2020}, {"authors": ["A.B. Parsa", "A. Movahedi", "H. Taghipour", "S. Derrible"], "title": "A", "venue": "(Kouros)Mohammadian, Toward safer highways, application of xgboost and shap for real-time accident detection and feature analysis, Accident Analysis & Prevention 136 ", "year": 2020}, {"authors": ["S. Wachter", "B. Mittelstadt"], "title": "C", "venue": "Russell, Counterfactual explanations without opening the black box: Automated decisions and the gdpr", "year": 2018}, {"authors": ["C.T. Ramaravind K. Mothilal", "Amit Sharma"], "title": "Examples are not enough, learn to criticize! criticism for interpretability", "venue": "in: Proceedings of the 2020 Conference on Fairness, Accountability, and TransparencyJanuary,", "year": 2020}, {"authors": ["J. Pearl"], "title": "The seven tools of causal inference", "venue": "with reflections on machine learning, Communications of ACM 62 ", "year": 2019}, {"authors": ["L. Bottou", "J. Peters", "J. Qui\u00f1onero-Candela", "D.X. Charles", "D.M. Chickering", "E. Portugaly", "D. Ray", "P. Simard", "E. Snelson"], "title": "Counterfactual reasoning and learning systems: The example of computational advertising", "venue": "Journal of Machine Learning Research 14 ", "year": 2013}, {"authors": ["F.D. Johansson", "U. Shalit"], "title": "D", "venue": "Sontag, Learning representations for counterfactual inference", "year": 2016}, {"authors": ["P. Schulam"], "title": "S", "venue": "Saria, Reliable decision support using counterfactual models", "year": 2017}, {"authors": ["E.C. Neto"], "title": "Towards causality-aware predictions in static machine learning tasks: the linear structural causal model case, 2020", "year": 2020}, {"authors": ["J. Pearl"], "title": "Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference", "venue": "Morgan Kaufmann Publishers", "year": 1988}, {"authors": ["S. Russel", "P. Norvig"], "title": "Artificial Intelligence: A Modern Approach", "venue": "Pearson Education (3rd Edition)", "year": 2010}, {"authors": ["D. Koller", "N. Friedman"], "title": "Probabilistic Graphical Models: Principles and Techniques", "venue": "The MIT Press", "year": 2009}, {"authors": ["M. Scutari", "C. Vitolo", "A. Tucker"], "title": "Learning bayesian networks from big data with greedy search: computational complexity and efficient implementation", "venue": "Statistics and Computing 29 ", "year": 2019}, {"authors": ["D. Heckerman", "D. Geiger", "D. Maxwell"], "title": "Learning bayesian networks: The combination of knowledge and statistical data", "venue": "Machine Learning 20 ", "year": 1995}, {"authors": ["D. Heckerman"], "title": "A Tutorial on Learning with Bayesian Networks", "venue": "Technical Report, Microsoft Research Advanced Technology Division, Microsoft Corporation", "year": 1995}, {"authors": ["D.M. Chickering", "D. Heckerman"], "title": "Learning Bayesian networks is NP-hard", "venue": "Technical Report, Tech. Rep. MSR-TR-94-17, Microsoft Corporation", "year": 1994}, {"authors": ["D.M. Chickering"], "title": "Learning Bayesian Networks is NP-Complete", "venue": "Springer New York", "year": 1996}, {"authors": ["D. Gabbay", "J. Woods"], "title": "Advice on abductive logic", "venue": "Logic Journal of the IGPL 14 ", "year": 2006}, {"authors": ["S. Piri", "D. Delen", "T. Liu"], "title": "A synthetic informative minority over-sampling (simo) algorithm leveraging support vector machine to enhance learning from imbalanced datasets", "venue": "Decision Support Systems 106 ", "year": 2018}], "sections": [{"text": "The use of sophisticated machine learning models for critical decision making is faced with a challenge that these models are often applied as a \u2018black-box. This has led to an increased interest in interpretable machine learning, where post hoc interpretation presents a useful mechanism for generating interpretations of complex learning models. In this paper, we propose a novel approach underpinned by an extended framework of Bayesian networks for generating post hoc interpretations of a black-box predictive model. The framework supports extracting a Bayesian network as an approximation of the black-box model for a specific prediction. Compared to the existing post hoc interpretation methods, the contribution of our approach is three-fold. Firstly, the extracted Bayesian network, as a probabilistic graphical model, can provide interpretations about not only what input features, but also why these features contributed to a prediction. Secondly, for complex decision problems with many features, a Markov blanket can be generated from the extracted Bayesian network to provide interpretations with a focused view on those input features that directly contributed to a prediction. Thirdly, the extracted Bayesian network enables the identification of four different rules which can inform the decision-maker about the confidence level in a prediction, thus helping the decision-maker assess the reliability of predictions learned by a black-box model. We implemented the proposed approach, applied it in the context of two well-known public datasets and analysed the results, which are made available in an open-source repository. Keywords: Interpretable machine learning, post hoc interpretation, probabilistic inference, Bayesian Network, predictive analytics\n\u2217Corresponding author Email address: catarina.pintomoreira@qut.edu.au (Catarina Moreira)\nPreprint submitted to Decision Support Systems July 22, 2020\nar X\niv :2\n00 7.\n10 66\n8v 1\n[ cs\n.A I]\n2 1\nJu l 2\n02 0"}, {"heading": "1. Introduction", "text": "The rapidly growing adoption of Artificial intelligence (AI) has led to the development of supervised machine learning, in particular, deep neural networks, for generating predictions of high accuracy [1]. While the advancement has the potential to make significant improvement to the state-of-the-art in operational decision making across various business domains and processes, the underlying models are often opaque and do not provide the decision-maker with any understanding of their internal predictive mechanisms. This opaqueness in machine learning models is known as the black-box problem. Immediate consequences of trusting predictions from opaque models might result in severe losses for businesses (and people), unfair job losses, or even lead to negative impacts in certain societal groups (for instance, racial and gender discrimination) [2]. This has posed an open challenge to data scientists and business analysts on how to endow machine intelligence with capabilities to explain the underlying predictive mechanisms in a way that helps decision-makers understand and scrutinize the machine learned predictions.\nThe recent body of literature in machine learning has emphasised the need to interpret and explain the (machine) learned predictions. Methods and techniques have been proposed for explaining black-box models which are known as interpretable machine learning [3] or, in a broader context, explainable AI (XAI) [4]. So far, there exist two different mechanisms to address model interpretability. One is to have an interpretable model that provides transparency at three levels: the entire model, the individual components and the learning algorithm [5]. For example, both linear regression models and decision tree models are interpretable models. Another mechanism to address model interpretability is via post hoc interpretation, in which case, explanations and visualisations are extracted from a learned model, that is, after the model has been trained, and as such they are model agnostic. This is particularly useful for generating model interpretations for those complex machine learning models (such as deep neural networks) that have low transparency and are hard to be transformed into an interpretable model (i.e., a \u2018white-box\u2019) due to their sophisticated internal representations. The existing post hoc interpretation techniques (see a review in [3]) present knowledge about the various levels of impact of individual input features on the corresponding prediction.\nIn this paper, we propose a novel approach underpinned by an extended framework of Bayesian networks for generating post hoc interpretations of a black-box predictive model, with a focus on providing interpretations for any instance of prediction learned by the model (known as local interpretations). We name this framework the Local Interpretation-Driven Abstract Bayesian Network (LINDA-BN), which supports extracting a Bayesian network as an approximation (or an abstraction) of a black-box model for a specific prediction learned from any given input. We implemented our approach, applied it in the context of two wellknown public datasets and analysed the results, which are made available in an open-source repository. Compared to the existing post hoc interpretation methods, the contribution of our approach is three-fold.\n\u2022 The extracted Bayesian network not only can provide interpretations about what in-\nput features contributed to the corresponding prediction. As a probabilistic graphical model, it also represents knowledge about dependencies (in form of conditional probabilities) between input features and prediction, thus generating interpretations about why certain input features contributed to the prediction.\n\u2022 For complex decision problems with a large number of features, the extracted Bayesian\nnetwork is often complicated to be analysed by human. In this case, LINDA-BN supports generating a Markov blanket from the extracted Bayesian network. The Markov blanket determines the boundaries of a decision system in a statistical sense, and presents a graph structure covering a decision (e.g., a prediction), its parents, children, and the parents of the children. As such, the Markov blanket of the extracted Bayesian network provides interpretation with a focused view on those input features that directly contributed to the corresponding prediction.\n\u2022 The extracted Bayesian network enables the identification of four different rules which\ncan inform the decision-maker about the confidence level in a given prediction. As such, the interpretations provided in our approach can help the decision-maker assess the reliability of predictions learned by a black-box model.\nIn the rest of the paper, we continue to introduce the relevant concepts and review the related research efforts in Section 2. We present our approach underpinned by the framework\nLINDA-BN in Section 3. Next, we report the experiments and discuss the results of analysis in Section 4. Finally, we conclude the paper with an outlook to future work (Section 5)."}, {"heading": "2. Background and Related Work", "text": "In this section, we present the main concepts that are used throughout our work, and\nreview research efforts that are related to the proposed framework."}, {"heading": "2.1. Concepts", "text": "Prior to discussing existing work that relates to our approach on providing intepretations\nof a black box machine learning model prediction, we note the following definitions:\n\u2022 Black box predictor: It is a machine learning opaque model, whose internals are\neither unknown to the observer or they are known but are not understandable by humans.\n\u2022 Interpretability: The ability to extract symbolic information out of a black box that\ncan provide the meaning in understandable terms to a human [6].\n\u2022 Explainability: The ability to highlight decision-relevant parts of the used representa-\ntions of the algorithms and active parts in the algorithmic model, that either contribute to the model accuracy on the training set, or a specific prediction for one particular observation [7].\nOne can see interpretability as the extraction of symbolic information from the black box (machine-level) that already needs some degree of semantics, and explainability as the conversion of this symbolic information to a human understandable way (human-level)."}, {"heading": "2.2. Related Work", "text": "Various approaches have been proposed in the literature to address the problem of interpretability. Generally, this problem can be classified into two major models: Interpretable models and model agnostic (post-hoc) models.\nInterpretable models are by design already interpretable, providing the decision-maker a transparent white box approach for prediction. Decision tree, logistic regression, and linear\nregression are commonly used interpretable models. These models have been used to explain predictions of specific prediction problems [8]. Model-agnostic approaches, on the other hand, refer to the deriving explanations from a black box predictor by extracting information about the underlying mechanisms of the system. In addition, studies have focused on providing model-specific post-hoc explanations [9]. The focus of our work is to build model-agnostic post-hoc methods as they have flexibility of being applied to any predictive model as compared to model-specific post-hoc approaches . To discover the demystifying predictive black box models, we focus on the widely cited post-hoc models that include LIME [10], SHAP [11], and Counterfactual explanation in this work."}, {"heading": "2.2.1. LIME", "text": "Local Interpretable Model-agnostic Explanations (LIME) [10] explains the predictions of any classifier by approximating it with an locally faithful interpretable model. Hence, LIME generates local interpretations by perturbing a sample around the input vector within a local decision boundary [12, 10]. Each feature is associated with a weight that is computed using a similarity function that measures the distances between the original instance prediction and the predictions of the sampled points in the local decision boundary. Linear regression is learned to determine the local importance of each feature.\nLIME has been extensively applied in the literature. For instance, Stiffler et al. [13] used LIME to generate salience maps of a certain region showing which parts of the image affect how the black box model reaches a classification for a given test image. Tan et al. [14] apply LIME to demonstrate the presence of uncertainty in the explanations that could raise concerns in the use of the black box model and diminish the value of the explanations. different sources of uncertainty in the explanation. Their work demonstrates the presence of three sources of uncertainty: randomness in the sampling procedure, variation with sampling proximity, and variation in explained model across different data points. Anchor [15] is an extension of LIME that attempts to address some of the limitations by maximizing likelihood on how a certain feature might contribute to a prediction. Anchor introduces IF-THEN rules as explanations as well as the notion of coverage, which allows the decision-maker to understand the boundaries in which the generated explanations are valid."}, {"heading": "2.2.2. SHAP", "text": "The SHAP (SHapley Additive exPlanations) is an explanation method which uses Shapley values [16] from coalitional game theory to fairly distribute the gain among players, where contributions of players are unequal [11]. Shapely values are a concept in economics and game theory and consist in a method to fairly distribute the payout of a game among a set of players. One can map these game theoretic concepts directly to an XAI approach: a game is the prediction task for a single instance; the players are the feature values of the instance that collaborate to receive the gain. This gain consists of the difference between the Shapley value of the prediction and the average of the Shapley values of the predictions among the feature values of the instance to be explained [17].\nStrumbelj and Kononenko [17] claim that in a coalition game, it is usually assumed that n players form a grand coalition that has a certain value. Given that we know how much each smaller (subset) coalition would have been worth, the goal is to distribute the value of the grand coalition among players fairly (that is, each player should receive a fair share, taking into account all sub-coalitions). Lundberg and Lee [11] on the other hand, present an explanation using SHAP values and the differences between them to estimate the gains of each feature.\nIn order to fairly distribute the payoff amongst players in a collaborative game, SHAP makes use of two fairness properties: (1) Additivity, which states that amounts must sum up to the final game result, and (2) Consistency, which states that if one player contributes more to the game, (s)he cannot get less reward.\nIn terms of related literature, Miller Janny Ariza-Garzo\u0301n and Segovia-Vargas [18] adopted SHAP values to assess logistic regression model and several machine learning algorithms for granting scoring in P2P (peer-to-peer) lending, the authors point out SHAP values can reflect dispersion, non-linearity and structural breaks in the relationships between each feature and the target variable. They concluded that the SHAP can provide accurate and transparent results on the credit scoring model. Parsa et al. [19] also highlight that SHAP could bring insightful meanings to interpret prediction outcomes. For instance, one of the techniques in the model, XGBoost, not only is capable of evaluating the global importance of the impacts of features on the output of a model, but it can also extract complex and non-linear joint\nimpacts of local features."}, {"heading": "2.2.3. Probabilistic graphical model", "text": "The literature of interpretable methods for explainable AI based on probabilistic graphical models (PGM) is mostly dominated by models based on counterfactual reasoning in order to derive explanations for a scpecic local datapoint.\nThe counterfactual explanation based on PGM comprises of a conditional assertion whose antecedent is false and whose consequent describes how the world would have been if the antecedent had occurred. It provides interpretations as a mean to point out which changes would be necessary to accomplish the desired goal, rather than supporting the understanding of why the current situation had a certain predictive outcome [20]. For instance, in a scenario where a machine learning algorithm assesses whether a person should be granted a loan or not, a counterfactual explanation of why a person did not have a loan granted could be in a form of a scenario if your income was greater than $15, 000 you would be granted a loan [21]. Unlike other explanation methods that depend on approximating an interpretable model within a perturbed decision boundary, counterfactual explanations have the strength that it is always truthful to the underlying model by providing direct outputs of the algorithms [10].\nCounterfactual explanations are part of causal inference methods, which are based on causal reasoning. and is focused on the estimation of the causal effects from treatments and actions [22]. In 2000, Pearl proposed a framework (the ladder of causation) that proposes different levels of causal relationships during causal inference. Level 1, Association, entails the sensing of regularities or patterns in the input data, expressed as relations; it focuses on the question what. Level 2, Intervention, predicts the effects of deliberate actions, expressed as causal relationships. And Level 3, Counterfactuals, involve constructing a theory of the world that explains why certain actions have specific effects and what happens is the absence of such actions [22]. A simple and naive approach for generating counterfactual explanations is searching by trial and error. In this approach the feature values are randomly changed for the instance of interest and stops searching when the desired output is predicted.\nThe notion of counterfactual model has been investigated by a few researchers. In 2013, the counterfactual approach has been proposed for the evolution of advertisement place-\nment in search engines [23]. Johansson et al. [24] claim that the counterfactual thinking has been adopted in the context of machine learning applications to predict the result of several different actions, policies, and interventions using non-experiment data. Moreover, the Counterfactual Gaussian Process (CGP) approach has been created by Schulam and Saria [25] for modelling the effects of sequences of actions on continuous time series data and facilitate the reliability of medical decisions [26].\nAlthough counterfactual explanation are useful, they do not explain why a certain prediction is made. On the contrary, they assume a hypothetical scenario where the prediction would be contrary to the output of that particular data point. Our approach aims to use probabilistic model to provide local explanations that provide insights into the features influencing a datapoint."}, {"heading": "3. The Local Interpretation-Driven Abstract Bayesian Network Framework", "text": "In this section, we present our framework built upon an extended framework of Bayesian networks that can generate post hoc interpretations for a single data point of prediction: the local interpretation-driven abstract Bayesian network (LINDA). We start with a brief introduction to Bayesian networks (Section 3.1) and structure learning (Section 3.2). Readers that are familiar with the knowledge can proceed directly to the proposed framework (Sections 3.3 to 3.5)."}, {"heading": "3.1. Bayesian Networks", "text": "A Bayesian Network (BN) is a directed acyclic graph in which each node represents a random variable, and each edge represents a direct influence from the source node to the target node. The graph represents (in)dependence relationships between variables, and each node is associated with a conditional probability table that specifies a distribution over the values of the node given each possible joint assignment of the values of its parents [27].\nBayesian networks can represent essentially any full joint probability distribution, which can be computed using the chain rule in probability theory [28]. Let G be a BN graph over the variables X1, \u00b7 \u00b7 \u00b7 , Xn. We say that a probability distribution, Pr, over the same space\nfactorizes according to G, if Pr can be expressed using the following equation [29]:\nPr(X1, . . . , Xn) = n\u220f\ni=1\nPr(Xi|PaXi). (1)\nIn Equation 1, PaXi corresponds to all the parent variables of Xi. The graph structure of the network, together with the associated factorization of the joint distribution allows the probability distribution to be used effectively for inference (i.e. answering queries using the distribution as our model of the world). For some query Y and some observed variable e, the exact inference in Bayesian networks is given by the following equation [29]:\nPr(Y |E = e) = \u03b1Pr(Y, e) = \u03b1 \u2211 w\u2208W Pr(Y, e, w), with \u03b1 = 1\u2211 y\u2208Y Pr(y, e). (2)\nEach instantiation of the expression Pr(Y = y, e) can be computed by summing up all joint entries that correspond to assignments consistent with y and the evidence variable e. The set of random variables W corresponds to variables that are neither query nor evidence. The \u03b1 parameter specifies the normalization factor for distribution Pr(Y, e), and this normalization factor is informed by certain assumptions made in Bayes rule [28]."}, {"heading": "3.2. Structure Learning in Bayesian Networks", "text": "Bayesian networks are made of two important components: a directed acyclic graph G representing the network structure, and a set of probability parameters \u0398 representing the conditional dependence relations. Learning a BN is a challenging problem when the network representation G is unknown. Given a dataset D with m observations, Pr (G,\u0398|D) is composed of two steps, structure learning and parameter learning, as follows [30]:\nPr (G,\u0398|D) = Pr (G|D)\ufe38 \ufe37\ufe37 \ufe38 structure learning \u00b7 Pr (\u0398|G,D) .\ufe38 \ufe37\ufe37 \ufe38 parameter learning\n(3)\nStructure learning aims to find the directed acyclic graph G by maximising Pr(G|D). Parameter learning, on the other hand, focuses on estimation of the parameters \u0398 given the graph G obtained from structure learning. According to [31, 32], considering that parameters \u0398 represent independent distributions (as assumed in Na\u0308\u0131ve Bayes), the learning process can\nbe formalised as follows [30]: Pr(\u0398|G,D) = \u220f i Pr(\u0398Xi |\u03a0Xi ,D). (4)\nIt is important to note that structure learning is well known to be both NP-hard [33] and\nNP-complete [34] due to the following equation:\nPr(G|D) \u221d Pr(G)Pr(D|G), (5)\nwhich can be decomposed into:\nPr(D|G) = \u222b Pr(D|G,\u0398)Pr(\u0398|G)d\u0398\n= \u220f i \u222b Pr(Xi|\u03a0Xi ,\u0398Xi)Pr(\u0398Xi |\u03a0Xi)d\u0398Xi\n(6)\nIn structure learning, it is often used the BIC score, a frequentist measure, to maximise,\nPr(G,\u0398|D), due to its simplicity.\nScore(G,D) = BIC(G, \u03b8|D) = \u2211 i logPr(Xi|\u03a0Xi ,\u0398Xi)\u2212 log(n) 2 |\u0398Xi | . (7)\nAccording to Scutari et al. [30], structure learning via score maximisation is performed using general-purpose optimisation techniques, typically heuristics, adapted to take advantage of these properties to increase the speed of structure learning. The most common are greedy search strategies that employ local moves designed to affect only few local distributions, to that new candidate DAGs can be scored without recomputing the full Pr(D|G). This can be done either in the space of the DAGs with hill climbing and tabu search [28]. In this paper, we opted for a greedy Hill Climbing approach to learn the structure G, due to its simplicity and effective results [31]."}, {"heading": "3.3. Local Interpretation-Driven Abstract Bayesian Network (LINDA-BN)", "text": "State-of-the-art techniques for constructing predictive models underpinned by machine intelligence usually adopt a \u2018black-box\u2019 approach, where the reasoning behind the predictions remains opaque (particularly in regard to deep learning models). Consequently, the underlying predictive mechanisms remain largely incomprehensible to the decision-maker.\nThe challenge is how to endow machine intelligence with capabilities to explain the underlying predictive mechanisms in a way that helps decision-makers understand and scrutinize the machine learned decisions. In the following, we propose an extended framework of Bayesian Networks for generating post hoc local interpretations of black-box predictive models. We name this framework the Local Interpretation-Driven Abstract Bayesian Network (LINDABN). It supports extracting a Bayesian network as an approximation (or an abstraction) of a black-box model for a specific prediction learned from any given input. Note that explanations can be constructed from the graphical representations of LINDA-BN, and we will address the explanation generation component as a direction for future work.\nThe basic idea behind the proposed framework LINDA-BN rests in three main steps: i) permutation generation, ii) Bayesian network learning, and iii) computation of the Markov Blanket of the class variable (representing result of a prediction). It is important to stress that the proposed model aims to augment a decision-maker\u2019s intelligence towards a specific decision problem, providing interpretations that can either reinforce the predictions of the black-box or lead to a complete distrust in these predictions (identification of misclassifications). Figure 1 shows a general illustration of the proposed framework.\nGiven a vector of input features ~X = {x1, x2, ..., xn} and a black-box predictor, y\u0302( ~X), the goal is to introduce a set of permutations ~Xi \u2032 in the features of ~X in a permutation variance\n\u2208 [0, 1] in such a way that each feature will be permuted using a uniform distribution over the interval [xi \u2212 , xi + ]. The goal is to analyse how introducing a small perturbation can impact the predictions of the black box prediction, y\u0302( ~Xi \u2032 ), generating a new statistical distribution describing small variations of the input vector ~X. The goal is to learn a Bayesian network structure out of this statistical sample using a Greedy Hill Climbing approach. Our hypothesis is the following: if the data point falls within the correct decision region of the black box predictor, leading to a correct class classification, c, then the predictions of all the permutations, y\u0302( ~Xi \u2032 ), should be close to certainty, i.e. favouring one of the assignments of the class variable with Pr(Class = c | X \u2032i) \u2248 1. This can strengthen the decision-maker\u2019s trust in the predictions of the black-box predictor. If, however, the data point, ~X, is very close to the black-box\u2019s decision boundary, then one would expect that the permutations will be spread around the different regions demarcated by the decision boundary, leading\nto a more diversified statistical distributions of predictions, and a higher uncertainty in the classification of the respective class Pr(Class = c | X \u2032i) << 1. Such situations have the potential to alert the decision-maker that the black-box predictor is not very certain about the classification of the given data point. Section 3.5 is centered in this topic.\nSince the network structure shows dependencies between the input features and the class variable, then it is possible to extract what features contributed to the prediction and why, allowing a deeper understanding about the impact that the features have in the class variable, or even provide the decision-maker additional insights about the decision problem.\nFor complex decision problems with a large amount of features, the local interpretable network that is learned from the generated permutations is extremely complicated to be analysed by a human, so a Markov Blanket is returned, instead, as a summarisation of what are the main variables influencing the class variable. The Markov blanket determines the\nAlgorithm 1: Local Interpretation-Driven Abstract Bayesian Network Generator\nInput: local vec, single vector from which we want to generate interpretations black box, a predictive model , variance range to permute the features (default = 0.1) n samples, number of permuted samples to generate (default = 300) class var, string with the name of the class variable Output: G, the Local Interpretable Abstract Bayesian Network\n1: /* Generate permutations via a uniform distribution within a permutation range */ 2: perms = GeneratePermutations( x, model, , n samples ) 3: 4: /* Discretise continuous features according to the number of quartiles */ 5: perms discr = DiscretisePermutations( perms, quartiles = 4 ) 6: 7: /* Learn BN from discrete permutations using a Greedy Hill Climbing Search */ 8: bn = LearnBN GreedyHillClimbing( perms discr ) 9:\n10: /* Compute BN\u2019s marginal distributions */ 11: bn inf = ComputeMarginalDistributions( bn ) 12: 13: /* Compute BN\u2019s Markov blanket */ 14: bn markov = ComputeMarkovBlanket( bn, class var ) 15: 16: if bn.nodes <= 10 then 17: return bn inf /* return full network */ 18: else 19: return bn markov /* return Markov blanket */ 20: end if 21:\nboundaries of a system in a statistical sense. It includes all its parents, children, and the parents of the children.\nIt can be shown that a node is conditionally independent of all other nodes given values for the nodes in its Markov blanket. Hence, if a node is absent from the class attribute\u2019s Markov blanket, its value is completely irrelevant to the classification [29]. Algorithm 1 describes the algorithm that we used for to generate the proposed local interpretable abstract Bayesian network."}, {"heading": "3.4. Interpreting Graphical Representations through Reasoning", "text": "This section analyses how to interpret the different situations where a random variable\ncan influence another in the local interpretable Bayesian network model.\nIn a common cause structure, Figure 2 (a), the local interpretable model approximates to a Na\u0308\u0131ve Bayes classifier, which means that having knowledge about the class variable will make the feature variables X1, X2, \u00b7 \u00b7 \u00b7 , XN conditionally independent, and consequently uncorrelated. This means that knowing about X1 does not bring any additional information to the decision-maker. Although human decision-makers tend to assess and interpret these structures as cause/effect relationships as a way to simplify and linearise the decision problem due to bounded rationality constraints, statistically, common cause structures do not imply causal effects in Bayesian networks [27]. The consideration of the class variable as a prior in interpretations for a single datapoint may indicate a high uncertainty obtained in the statistical sample of the permuted features, suggesting that the datapoint that is being interpreted may be very close to the predictive black-box decision boundary (Section 3.5 addresses this with a higher detail).\nThe other type of structure that one can often find in the local interpretable model is the v-structure, also called common effect (Figure 2 (b), which approximates to a linear regression representation. This means that, the features become conditionally independent of the class, if and only if one has knowledge about the class variable. Being uncertain about the class will lead to an influence from the features, X1, X2, \u00b7 \u00b7 \u00b7 , XN . In terms of the proposed local interpretable model, this means that the features have a direct effect in the class variable, and humans can interpret it through an abductive reasoning process.\nAbduction is a mode of human reasoning, which was brought into prominence by the the American philosopher C.S. Peirce [35]. In Peirce\u2019s view abduction is an inference of the form: \u201cThe surprising fact C is observed. But is A were true, C would be a matter of course. Hence, there is reason to suspect that A is true\u201d. Abductive inference is thus a process of justifying an assumption, hypothesis or conjecture in producing the class of interest. Peirce states that abduction might explain a given set of data, or might facilitate observationally valid predictions, or might permit the discounting of other hypotheses. By engaging in abduction, the decision maker interpreting the graph structure is afforded a simpler and more compact\naccount [35].\nAbduction is not a sound form of inference like deduction, and so even though the decision maker might suspect A, there is a degree of uncertainty. Abduction is sometimes termed \u201cinference to the best explanation\u201d where there is no guaranteed certainty in the explanation. In other words, given a set of observations, the decision maker uses abduction to find the simplest, most likely and compact explanation from the graph structure. The Markov blanket of the class variable is a way of supporting the decision maker\u2019s abductive reasoning process."}, {"heading": "3.5. Rules for Local Interpretations", "text": "The graphical nature of the proposed framework LINDA-BN enables the identification of certain parts that can help the decision-maker assess the reliability of the predictions of the black box for single datapoints. To this end, we propose a set of four rules that correspond to four different patterns that the proposed model can identify, depending on how close to the decision boundary a datapoint is. By analysing the confidence of the interpretable model\nwith regards to the class variable together with the structure of the network, one can provide useful guidelines to the decision-maker that can be later be used to generate human centric and understandable explanations (which is not the focus of this work).\nThe proposed rules to assess the confidence of the black box predictions using the proposed\nframework are the following:\n\u2022 Rule 1: High confidence in predictions. If the black box predicts a class c for a\ngiven datapoint, ~X, and the class variable is contained in a common-effect structure in G with a probability Pr(Class = c) \u2248 1, then the interpretable model, G, supports the prediction of ~X and its respective Markov blanket determines the most relevant features.\nAs mentioned in Section 3.4, common-effect structures in G approximate to a linear regression representation in which there is a direct influence from the features to the class. When Pr(Class = c) \u2248 1, then this means that the datapoint falls in a well defined decision region, as illustrated in Figure 3. Since the likelihood of the class is close to certainty, the decision-maker can make use of the class\u2019 respective Markov blanket for explanation and perform an abductive reasoning process in which the decision-maker will seek to find the simplest and most likely conclusion out of the Markov blanket.\n\u2022 Rule 2: Unreliable predictions. If the interpretable network, G, has a struc-\nture where the class variable is independent from all other feature variables, that is Class \u22a5 {X1, \u00b7 \u00b7 \u00b7 , XN}, then this corresponds to an unrealistic decision scenario, because the features are uncorrelated from the class variable and providing information\nabout them does not make any change in the probability Pr(Class = c). Thus, the classification y\u0302( ~X) is incorrect and it should be communicated to the decision-maker as an unreliable prediction.\nanalyse the Markov Blanket of the class variable representing ~X, and assess whether the relationships between the features justify the class. Figure 6 shows an example of an uncertain prediction. Although the likelihood of the variable Class is in accordance with ~\u0302X = c, the local interpretable abstract model shows full uncertainty in the prediction: the prediction is as good as flipping a coin."}, {"heading": "4. Evaluation", "text": "Given that there are no standard evaluation metrics for XAI [3], in this , we present a thorough analysis of the proposed LINDA-BN model in accordance to the rules that we put forward in Section 3.5. We performed an analysis in terms of two public well-known datasets from the literature, namely the Pima Indians diabetes dataset and the Breast Cancer Wisconsin [36], both from the UCI Machine Learning Repository1. We have made available a public repository with Jupyter notebooks with the proposed model and all the experiments that we made for this research work: https://github.com/catarina-moreira/LINDA_DSS.\nIn Section 4.1, we present the main experimental setup for our analysis. Section 4.2, presents an analysis of the impact of the permutation variance in the proposed LINDA model. In Section 4.3, we make a statistical analysis of the distribution of the interpretations generated by LINDA over both datasets and their the different rules together with existing interpretable approaches such as LIME [10] and SHAP [11]. Finally, Section 4.4, describes\n1https://archive.ics.uci.edu/ml/index.php\nhow the proposed interpretable model performs in more complex decision scenarios."}, {"heading": "4.1. Design of Experiments", "text": "In order to assess the performance and interpretations generated by the proposed LINDA model, we trained a deep learning neural network for two two public well-known datasets from the literature, namely the Pima Indians diabetes dataset and the Breast Cancer Wisconsin datasets. Both datasets are highly unbalanced, and for that reason, we had to balance the datasets in order to not have a biased predictive model.\nWe performed a grid search approach in order to find the best performing neural network model. The characteristics of the models can be found in Table 1. As such, the learned models apply sophisticated internal working mechanisms and run as a black-box when making predictions."}, {"heading": "4.2. Analysis of the Impact of Different Permutation Variances", "text": "As in other representative interpretable models in the literature (like LIME and SHAP), LINDA-BN performs permutations between an interval in the range [xi \u2212 , xi + ] on the input vector\u2019s features ~X = {x1, x2, \u00b7 \u00b7 \u00b7 , xN} in order to generate a statistical distribution of how the predictions of the black-box change with the features. To investigate the impact of the permutation variance, , we performed a set of experiments for both datasets, where we varied \u2208 [0, 1], and analysed how many times the proposed interpretable model returned a structure that is consistent with the rules proposed in Section 3.5. The obtained results are summarised in Figure 7.\nTaking a close look at the Diabetes dataset in Figure 7, results show that low variance makes the interpretable model very confident in its interpretations, with 92% of the datapoints (both from the training set and test set) falling in rule 1 (high confidence in the\nprediction). This is due to the fact that, for a very small permutation interval, the probability of hitting a decision boundary is very small. This is confirmed with the verification of the low amount of datapoints falling in rule 4 (uncertainty in the predictions) or rule 3 (contrast effects). When the permutation variance starts to increase, the model becomes less certain about the predictions. Consequently, rule 1 starts to decrease exponentially, while rule 4 starts to show a lot in uncertainty in the interpretations generated for the different datapoints. One can see that when the permutation variance reaches half of the feature space (note that we assume that the features of the black-box are scaled between 0 and 1 as in standard machine learning applications), then there comes a point where the uncertainty is so high that the interpretable model stops having confidence in almost 90% of its interpretations. Additionally, almost 80% of the datapoints start falling in rule 4.\nIn terms of the impact of the permutation variance for the breast cancer dataset, the\nscenario tends to be slightly different. Just like in the diabetes dataset, when the permutation variance interval is very small, the statistical distribution of the predictions learned by the proposed LINDA model majorly falls in rule 1. However, when the permutation variance starts to increase together with the uncertainty levels, we start to notice an increase of contrast effects (rule 3), rather than uncertainty in the predictions (rule 4). The reason for this is due to the effectiveness of the black-box. Note that the accuracy of the deep neural network model for the diabetes dataset was 73.80%, while the learned model for the breast cancer dataset achieved an accuracy of 98.40%. Since the majority of the datapoints fall in well defined decision regions, when the permutation variance increases, the statistical distribution of predictions will tend to show misclassifications (a statistical distribution more concentrated in the opposite region of the decision boundary). Figure 7 also shows that permutation variances superior to 0.2 do not decrease the certainty of the interpretability of correctly predicted datapoints, however it does increase the number of contrast effects (rule 3), reinforcing again the idea that bigger permutation intervals will make the distributions point towards opposite directions of the decision boundary.\nIn order to extract interpretable models that are both highly confident in the interpretations, but can also flag possible misclassifications, we decided to set the permutation variance to 0.1 for the remaining parts of this analysis."}, {"heading": "4.3. Analysis of Rules for Local Interpretations", "text": "In this section, we analyse the impact of the proposed rules in the different classifications: true positives, true negatives, false positives, and false negatives. The goal with this analysis is to understand if the proposed rules can provide the decision-maker some insights on whether or not, the decision-maker is facing a correct classification or a possible misclassification. Table 2 shows the percentage of datapoints over the different proposed rules for both the diabetes and breast cancer datasets, using = 0.1.\n\u2022 Correct classifications majorly coincide with rule 1, leading to highly confi-\ndent predictions. For the breast cancer dataset, for instance, all datapoints classified as true positives and true negatives were categorised as rule 1 with high confident interpretations. For the diabetes dataset, since the black box had a more poor performance, then the percentage of correctly classified datapoints is already smaller. Still, 86% of the true positives in the training set fell under the category of rule 1 and in the test set 75%. Regarding the true negatives, more than 90% of the datapoints had interpretations supporting a true classification of a non-diabetes case. When compared with interpretations from state of the art algorithms, one can see that both LIME and SHAP also tend to reinforce the features that are contributing positively to the class diabetes. Figure 8 shows an example of an interpretation of a correctly classified datapoint ( = 0.1) and the respective interpretations for LIME and SHAP. For the case of misclassifications, there was a significant amount of datapoints belonging to the set of the false positives and the false negatives that were also categorised as rule 1. In these examples, the interpretable model could not provide significant insights of why there was a misclassification.\n\u2022 Nonexistence of rule 2. As illustrated in Figure 7, rule 2, which corresponds to the\ncases where the class variable is independent of the features, is extremely rare. This rule only starts to emerge for permutations superior to 0.2 in the diabetes dataset, and are nonexistent in the cancer dataset. This suggests that permutation variances of 0.1 do not point towards unrealistic and erroneous classifications. Figure 9 shows an example of an unreliable prediction that was found in the testset of the diabetes dataset, using a variance of = 0.25 and the respective LIME and SHAP interpretations. Note that this specific datapoint corresponds to a misclassification (a false positive).\n\u2022 Contrast effects, rule 3, majorly coincide with misclassifications. When a dat-\napoint falls very close to the decision boundary, then when permuting that datapoint, there can be a significant statistical distribution of the predictions falling on the region\nsince there were almost no misclassified datapoints (accuracy of 0.9840), the percentage of datapoints falling in rule 4 are nearly zero. Figure 11 illustrates an example of a false positive datapoint, in which the interpretable model show maximum uncertainty in the class variable node. In terms of LIME and SHAP, it is hard to identify any principles that could point the decision-maker about a possible misclassification.\nIn the next section, we describe how more complex decision problems are addressed by\nthe proposed interpretable model."}, {"heading": "4.4. Interpretations for Complex Decision Scenarios", "text": "For small decision problems (at most 10 random variables), the proposed LINDA model displays the full interpretable network. For more complex decision problems, however, this would become unreadable for a human decision-maker. The breast cancer dataset is an example of such complex decision problem that contains a set of 30 features, which are mapped into random variables. This results in a graphical structure too complex for any human to analyse. For such datasets, the proposed LINDA model provides the decision-maker a Markov Blanket of the variable of interest, instead of the full local interpretable Bayesian\nnetwork, together with information about in which rule the network pattern corresponds to and respective marginal probabilities.\nThis representation enables the summarisation of information, enabling a fast and compact data driven interpretation of a local datapoint. Figure 12 shows an example of an interpretable network that was extracted out of a true positive datapoint. The complexity of\nthe network does not enable any human interpretations to take place. However, when looking at the Markov blanket together with the marginal probabilities of the random variables, then one can clearly identify a common-effect structure in which six features directly influence the class variable and contribute to its value. Moreover, the statistical distribution of the permutations shows a full confidence in the diagnosis, suggesting that the datapoint falls within rule 1 and consequently there is a high confidence that it is a correct prediction. The depth of this Markov blanket can potentially be extended to different depths, depending on the decision-maker\u2019s needs (for example. a normal person would be satisfied with the Markov blanket in Figure 12, however a medical doctor would probability explore other depths and analyse the relationship between more variables and their indirect influences towards the class variable)."}, {"heading": "5. Conclusions", "text": "In this paper, we proposed a new post hoc interpretable framework, the Local Interpretation-\nDriven Abstract Bayesian Network (LINDA-BN). This framework consists in learning a Bayesian network as an approximation of a black-box model from a statistical distribution of predictions from a local datapoint.\nThe major contribution of the proposed framework is the ability to identify four different rules which can inform the decision-maker about the confidence level in a given prediction of a specific datapoint. As such, the interpretations provided in our approach can help the decision-maker assess the reliability of predictions learned by a black-box model. These rules correspond to the different patterns that can be found in the learned Bayesian network, and they are summarised as follows:\n\u2022 Rule 1 - High confidence in predictions: a common-effect structure in which the features\nare directly influencing the class and the maximum likelihood of the class is close to one, suggesting a correct classification.\n\u2022 Rule 2 - Unreliable predictions: when the class variable is independent from the features,\nsuggesting a misclassification\n\u2022 Rule 3 - Contrast Effects: when the maximum likelihood of the class variable in the\nlearned Bayesian network favours a class opposite to the black-box model, suggesting a misclassification.\n\u2022 Rule 4 - Uncertainty in the predictions: when the likelihood of the class variable has\nvery high levels of uncertainty, suggesting that the decision-maker should assess the network in order to understand if the features are supporting the class.\nExperimental findings showed that rules 3 and 4 usually occurred in sets of false positives and false negatives, suggesting that the proposed framework might provide a possible approach to identify misclassifications in black-box models. On the other hand, the correct classifications (true positives and true negatives), were mostly associated with rule 1 with common-effect graph structures and maximum likelihood in the class variable close to one, again providing a potential method to identify correct classifications and promote trust in the decision-maker.\nFor future work, we would like to extend the proposed approach from an interpretable framework to an explainable one. This majorly consists in converting the symbolic rules proposed in this study into explainable arguments that could communicate the decisionmaker why a certain prediction was computed out of a black-box and the reasons of why / why not a decision-maker should trust in the predictions."}], "title": "An Interpretable Probabilistic Approach for Demystifying Black-box Predictive Models", "year": 2020}