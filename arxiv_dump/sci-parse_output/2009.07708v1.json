{"abstractText": "Feature importance aims at measuring how crucial each input feature is for model prediction. It is widely used in feature engineering, model selection and explainable artificial intelligence (XAI). In this paper, we propose a new tree-model explanation approach for model selection. Our novel concept leverages the Coefficient of Variation of a feature weight (measured in terms of the contribution of the feature to the prediction) to capture the dispersion of importance over samples. Extensive experimental results show that our novel feature explanation performs better than general cross validation method in model selection both in terms of time efficiency and accuracy performance.", "authors": [{"affiliations": [], "name": "Fan Fang"}, {"affiliations": [], "name": "Carmine Ventre"}, {"affiliations": [], "name": "Lingbo Li"}, {"affiliations": [], "name": "Leslie Kanthan"}, {"affiliations": [], "name": "Fan Wu"}, {"affiliations": [], "name": "Michail Basios"}], "id": "SP:218e7b5afc85e27be37b9e1bba4144fb0f980f85", "references": [{"authors": ["D.W. Apley", "J. Zhu"], "title": "Visualizing the effects of predictor variables in black box supervised learning models", "venue": "arXiv preprint arXiv:1612.08468 .", "year": 2016}, {"authors": ["S. Arlot", "A Celisse"], "title": "A survey of crossvalidation procedures for model selection. Statistics surveys", "year": 2010}, {"authors": ["R.R. Bies", "M.F. Muldoon", "B.G. Pollock", "S. Manuck", "G. Smith", "M.E. Sale"], "title": "A genetic algorithmbased, hybrid machine learning approach to model selection", "venue": "Journal of pharmacokinetics and pharmacodynamics 33(2): 195\u2013221.", "year": 2006}, {"authors": ["L. Breiman"], "title": "Random forests", "venue": "Machine learning 45(1): 5\u201332.", "year": 2001}, {"authors": ["C.E. Brown"], "title": "Applied multivariate statistics in geohydrology and related sciences", "venue": "Springer Science & Business Media.", "year": 2012}, {"authors": ["A. Fisher", "C. Rudin", "F. Dominici"], "title": "All Models are Wrong, but Many are Useful: Learning a Variable\u2019s Importance by Studying an Entire Class of Prediction Models Simultaneously", "venue": "Journal of Machine Learning Research 20(177): 1\u201381.", "year": 2019}, {"authors": ["R.A. Fisher"], "title": "The use of multiple measurements in taxonomic problems", "venue": "Annals of eugenics 7(2): 179\u2013188.", "year": 1936}, {"authors": ["M. Forina", "S. Lanteri", "C Armanino"], "title": "Parvusan extendible package for data exploration, classification and correlation, institute of pharmaceutical and food analysis and technologies, via brigata salerno, 16147 genoa, italy", "venue": "Av. Loss Av. O set Av. Hit-Rate", "year": 1991}, {"authors": ["J. Friedman", "T. Hastie", "R. Tibshirani"], "title": "The elements of statistical learning, volume 1", "venue": "Springer series in statistics New York.", "year": 2001}, {"authors": ["J.H. Friedman"], "title": "Greedy function approximation: a gradient boosting machine", "venue": "Annals of statistics 1189\u20131232.", "year": 2001}, {"authors": ["A. Goldstein", "A. Kapelner", "J. Bleich", "E. Pitkin"], "title": "Peeking inside the black box: Visualizing statistical learning with plots of individual conditional expectation", "venue": "Journal of Computational and Graphical Statistics 24(1): 44\u201365.", "year": 2015}, {"authors": ["B.M. Greenwell", "B.C. Boehmke", "A.J. McCarthy"], "title": "A simple and effective model-based variable importance measure", "venue": "arXiv preprint arXiv:1805.04755 .", "year": 2018}, {"authors": ["R. Guidotti", "A. Monreale", "S. Ruggieri", "F. Turini", "F. Giannotti", "D. Pedreschi"], "title": "A survey of methods for explaining black box models", "venue": "ACM computing surveys (CSUR) 51(5): 1\u201342.", "year": 2018}, {"authors": ["G. Hooker"], "title": "Discovering additive structure in black box functions", "venue": "Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, 575\u2013580.", "year": 2004}, {"authors": ["S. Khalid", "T. Khalil", "S. Nasreen"], "title": "A survey of feature selection and feature extraction techniques in machine", "year": 2014}, {"authors": ["R Kohavi"], "title": "A study of cross-validation and bootstrap for accuracy estimation and model selection", "venue": "In Ijcai,", "year": 1995}, {"authors": ["A. Kozak", "R. Kozak"], "title": "Does cross validation provide additional information in the evaluation of regression models", "venue": "Canadian Journal of Forest Research", "year": 2003}, {"authors": ["N. Lavesson", "P. Davidsson"], "title": "Quantifying the impact of learning algorithm parameter tuning", "venue": "AAAI, volume 6, 395\u2013400.", "year": 2006}, {"authors": ["G. Louppe", "L. Wehenkel", "A. Sutera", "P. Geurts"], "title": "Understanding variable importances in forests of randomized trees", "venue": "Advances in neural information processing systems, 431\u2013439.", "year": 2013}, {"authors": ["S.M. Lundberg", "G. Erion", "H. Chen", "A. DeGrave", "J.M. Prutkin", "B. Nair", "R. Katz", "J. Himmelfarb", "N. Bansal", "S.-I. Lee"], "title": "From local explanations to global understanding with explainable AI for trees", "venue": "Nature machine intelligence 2(1): 2522\u20135839.", "year": 2020}, {"authors": ["G.J. McLachlan", "K.-A. Do", "C. Ambroise"], "title": "Analyzing microarray gene expression data, volume 422", "venue": "John Wiley & Sons.", "year": 2005}, {"authors": ["C. Molnar"], "title": "Interpretable Machine Learning: A Guide for Making Black Box Models Explainable.(2019)", "venue": "URL https://christophm. github. io/interpretable-ml-book", "year": 2019}, {"authors": ["C. Molnar"], "title": "Interpretable Machine Learning", "venue": "Lulu. com.", "year": 2020}, {"authors": ["A. Rai"], "title": "Explainable AI: From black box to glass box", "venue": "Journal of the Academy of Marketing Science 48(1): 137\u2013 141.", "year": 2020}, {"authors": ["S. Raschka"], "title": "Model evaluation, model selection, and algorithm selection in machine learning", "venue": "arXiv preprint arXiv:1811.12808 .", "year": 2018}, {"authors": ["C. Rudin"], "title": "Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead", "venue": "Nature Machine Intelligence 1(5): 206\u2013215.", "year": 2019}, {"authors": ["C. Schaffer"], "title": "Selecting a classification method by cross-validation", "venue": "Machine Learning 13(1): 135\u2013143.", "year": 1993}, {"authors": ["J.W. Smith", "J. Everhart", "W. Dickson", "W. Knowler", "R. Johannes"], "title": "Using the ADAP learning algorithm to forecast the onset of diabetes mellitus", "venue": "Proceedings of the Annual Symposium on Computer Application in Medical Care, 261. American Medical Informatics Association.", "year": 1988}, {"authors": ["W.N. Street", "W.H. Wolberg", "O.L. Mangasarian"], "title": "Nuclear feature extraction for breast tumor diagnosis", "venue": "Biomedical image processing and biomedical visualization, volume 1905, 861\u2013870. International Society for Optics and Photonics.", "year": 1993}, {"authors": ["C. Strobl", "A.-L. Boulesteix", "T. Kneib", "T. Augustin", "A. Zeileis"], "title": "Conditional variable importance for random forests", "venue": "BMC bioinformatics 9(1): 307.", "year": 2008}, {"authors": ["I.-C. Yeh", "C.-h. Lien"], "title": "The comparisons of data mining techniques for the predictive accuracy of probability of default of credit card clients", "venue": "Expert Systems with Applications 36(2): 2473\u20132480. Zheng, A.; and Casari, A. 2018. Feature engineering for ma-", "year": 2009}], "sections": [{"heading": "Introduction", "text": "Model selection is the task of selecting a statistical model from a set of candidate models for given data. Cross validation is arguably the most used technique used to estimate the risk of an estimator or to perform model selection (Arlot, Celisse et al. 2010). But cross validation through data splitting provides little additional information during the evaluation of the model and, importantly, costs a long time in retraining the model (Kozak and Kozak 2003). Variable importance (a.k.a., feature importance) represents the statistical significance of the impact of each variable in the data on the generated machine learning models (Strobl et al. 2008). Variable importance also related to models\u2019 explanation. For example, it can be used to measure the increase of model prediction error after replacing the object features and, in turns, breaking the relationship between the features and the real results (Breiman 2001). Tree models are divided into black tree models (hard to explain to humans) like RandomForestTree and white box model tree models (easy to explain) like DecisionTree (Rudin 2019). Compared to other complex black box models, such as neural networks, it is relatively easy to understand (and explain) the contributions that each variable makes to the decision of tree models (Molnar 2019). Furthermore, \u201cfeatures\u201d are the core hyper-parameters in all tree models, which means it is important to combine \u201cfeatures\u201d with model selection and/or model explanation in tree models.\nCopyright c\u00a9 2021, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nRelated work on Explainable artificial intelligence (XAI) explains machine learning models from features\u2019 statistic and visualization like Partial Dependence Plot (Friedman 2001), Individual Conditional Expectation (Goldstein et al. 2015) and Accumulated Local Effects (Apley and Zhu 2016). As from above, the importance of the features is measured by the increase of the prediction error of the calculated model after arranging the features. If shuffling the values of a feature increases model error, the feature is \u201cimportant\u201d (Fisher, Rudin, and Dominici 2019). This method gives feature importance an interpretation that it is the increase in model error when the feature\u2019s information is destroyed (Molnar 2020). Related research in model selection usually involves appropriate criteria, usually based on an estimate of the generalization error, such as k-fold cross validation (McLachlan, Do, and Ambroise 2005). Other complex model selection methods, like model selection using combinatorial optimization and genetic algorithms have also been proposed (Bies et al. 2006). Our approach in the area is brand new in that it focuses on applying feature explanation in model selection, combining the comprehension (and more widely the explainability) of the models with their selection. We here initiate this research by looking at tree models.\nThis paper aims at proposing a new tree-model explanation approach and apply it to model selection. Specifically, we design a new explanation estimation of treestructure models (including DecisionTree, RandomForest, ExtraTree, GradientBoostingTree and XgboostTree) considering weighted contribution of features, the performance of contribution and feature quantity (i.e., the number of the features). This novel conceptual contribution connects common feature weighting techniques with the so-called Coefficient of Variation (Brown 2012), a statistical measure defined as the ratio of the standard deviation to the mean. We use this notion to define a new pipeline for model selection. We compare our new method with k-fold cross validation in model selection in five standard data sets. The results show that our method captures the dispersion of importance over samples and performs better than general cross validation method in model selection considering time efficiency and accuracy performance. The highlight is that we can generally maintain (and often improve) the performance of cross validation (in terms of test accuracy) whilst reducing computation time by at least a third. ar X iv :2\n00 9.\n07 70\n8v 1\n[ st\nat .M\nL ]\n1 6\nSe p\n20 20"}, {"heading": "Related Work", "text": "A large body of recent research has been devoted to the estimation of the performance of a machine learning model, including (i) estimating the generalization performance of models on future data; and, (ii) selecting the best performing model from a given hypothesis space (Raschka 2018). Basic selection/evaluation methods like Resubstitution validation, Stratified resampling and Holdout validation were proposed to be effective in selecting \u201cgood\u201d models. With the development of machine learning, a large numbers of settings (hyperparameters) need to be specified. Hyperparameter tuning allows to find the balance between bias and variance when optimizing the performance of these models. Cross validation is a great improvement based on holdout method in evaluating hyperparameters (Kohavi et al. 1995). Cross validation was proposed to help to select models with a better (average) generalization than just relying on the training score (Schaffer 1993). In particular, in k-fold cross validation (aka, repeated hold-out method) the data is split in k chunks and each chunk is used for testing the model trained on the remaining k\u22121 folds. Models selected by this method generally get results that are less biased and less optimistic than other methods.\nAs the core definition in machine learning research, \u201cfeatures\u201d refer to an multi-dimensional vector representing the (numerical) characteristics of an object. They are core to many fields of machine learning research including dimension reduction, relevance research, automation and model explanations (Zheng and Casari 2018). In Explainable AI (XAI), features are a medium for humans to understand the machine learning models that are hard to explain (commonly known as \u201cBlack box models\u201d) (Rai 2020). Feature interaction is a method to explain models by understanding whether features affect each other and to what extent they interact. Variable Interaction Networks are a tool proposed to decompose the prediction function into main effects and feature interactions and then visualize those as a network (Hooker 2004). Partial dependence based feature interaction is applied in measuring the feature importance by calculating the variance of the partial dependence function (Greenwell, Boehmke, and McCarthy 2018), which illustrates and explains interaction among features in machine learning models. But feature interaction is computationally expensive, and if we do not use all of the data points, the estimate has a non-negligible variance.\nPermutation feature importance (PFI) is a concept to explain models by calculating the increase of model prediction error after the feature values are permuted. The PFI method was introduced by Breiman (2001) for random forest first. Based on idea of PFI, a model-agnostic version of the feature importance was proposed (called model reliance) (Fisher, Rudin, and Dominici 2019). The PFI method considers both the influence of the main feature effect and the interaction effects on the performance of the model. But this approach has obvious drawbacks. Notably, if the features are related, the ranking of the importance of the features may be biased by unrealistic data instances. Moreover, the importance of the associated feature might be decreased by adding a correlated feature.\nRecently, a new interpretability of tree-based models is proposed. Lundberg et al. (2020) proposed to improve the explanation of tree along three dimensions: polynomial time algorithm for optimal interpretation of time based on game theory, direct interpretation of local feature interactions and understanding global model structure based on the combination of many local descriptions of each prediction. This research improves interpretability of tree-based machine learning models."}, {"heading": "Explanation of Tree-Based Models", "text": "Variable importance describes the contribution of covariates to the prediction and model accuracy (Fisher, Rudin, and Dominici 2019). While this works well in linear models, tree models have different variable importance calculation. Linear models\u2019 variable importance and explanation use loss functions which map the value of one or more event/variables to a real number that intuitively represents the \u201ccost\u201d associated with the event to optimize the original linear models. Tree-structure models have specific realization, which separates \u201cnodes\u201d and \u201cedges\u201d. Moreover, the splitting process will continue until no further revenue can be obtained or the preset rules are met (Friedman, Hastie, and Tibshirani 2001).\nThe variable importance of tree-structure models is calculated by Mean Decrease in Impurity (MDI) (Louppe et al. 2013) of the node, and the probability of the impurity reaching the node is obtained. The node probability can be calculated by dividing the number of samples arriving at the node by the total number of samples. The higher the value, the more important the node. A normal method is to arrange the value of each feature one by one and check how it changes the performance of the model or calculates the amount of \u201cimpurities\u201d. But experience tells us explanation through this method it is hard to understand and link decisions with insights into actual data. Alternative method is to iterate through all the splits that use this element, and measure the degree to which the variance or Gini coefficient is reduced compared to the parent node. The sum of all importance is scaled to 100. This means that each importance can be explained as part of the overall model importance (Molnar 2019).\nResearchers also consider using decision paths to explain tree-structure models, which consist of each decision path from the root of the tree to the leaf. Every decision path contributes to the final prediction (Guidotti et al. 2018). This decision function returns a value at the correct leaf of the tree, but it ignores the operational aspect of the decision tree, namely the path through the decision node and the information available there. Since each decision path is determined by features, and the decision will be added or subtracted from the value given in the parent node, the prediction can be defined as the sum of feature contributions plus \u201dbias\u201d covering the entire training set area. As defined in Tree-Interpreter library in Python, the prediction function can be defined as\nf(x) = cfull + K\u2211 k=1 contrib(x, k) (1)\nwhere K is the number of features, cfull is the value at the root node calculated according to information gain and contrib(x, k) is the contribution from the k-th feature in the feature vector x. In tree-structure models, contribution of each feature depends on the rest of the feature vector, which determines the decision path of traversing the decision tree, thereby determining the protection/contribution passed along.\nWe build upon (1) as follows. The specific feature f(x, k) can be defined\nf(x, k) = cfull + contrib(x, k) (2)\nTree-structure models use some hyper-parameters like \u201cdepth\u201d and \u201cmax leaf\u201d to control the complexity of tree models, given data set with features. To some extent, these parameters help tree models realize dimension reduction of decision rules to improve accuracy of the classifier, which is feature selection (Khalid, Khalil, and Nasreen 2014). Meanwhile, the quantity of features affects the explanation of machine learning models. We assume that good explanation combined with characteristics of features may lead to improvement of tree-structure models performance. In other words, we must combine the performance of feature performance with complexity of features (quantity) when we apply explanation in tree-structure models. We define the weight (contribution) of feature k in the model as\nweight(x, k) = f(x, k)\nf(x)\nWe then consider using Coefficient of Variation (Brown 2012) as explanation of model f with feature k, and obtain\nExplaincv(f) =\n\u2211K k=1(weight(x, k)\u2212 weight)2\nK \u2217 weight ,\nwhere weight is the mean of weights from 1 to K, i.e., weight = 1K \u2211K k=1 weight(x, k). From the definition of weight(x, k), we know that K\u2211\nk=1\nweight(x, k) = 1.\nWe then get\nExplaincv(f) = K\u2211 k=1 ( f(x, k) f(x) \u2212 1 K )2 . (3)\nEq 3 is defined as feature explanation of tree-structure model f . In this equation, f(x,k)f(x) is the weight (contribution) of specific feature k in the construction of tree-structure models; K is the quantity of features used in the model. Eq 3 is derived from coefficient of variation of feature weights\u2019 contribution to the entire tree model. From the theoretical analysis, Coefficient of Variation measures the dispersion of data point around the mean (Brown 2012). When we apply the Coefficient of Variation to the weight of features, we weigh contribution of features, the performance of contribution and quantity. Our hypothesis is that the original explanation combined with the coefficient of variation has better\nperformance in explaining models considering various features.\nConsidering the characteristics of feature explanation we defined, we design the workflow in model training (depicted in Figure 1). In particular, when compared to the state of the art, we substitute cross-validation with feature explanation. Cross validation costs a lot of time because the validation mechanism needs to train data set in a loop, which is computationally expensive. More importantly, feature explanation has the function of choosing best parameters considering the training accuracy, features\u2019 contribution and feature complexity (quantity).\nFigure 2 describes the comparison between cross validation and feature explanation. We use k-fold cross validation as an example. Considering the cross validation has k fold iterations and each iteration has an evaluation accuracy Ei, the final cross validation result is E = 1k \u2211k i=1 Ei. Each iteration needs a new training for original model. Meanwhile feature explanation splits data based on features. This method does not need a retrain of model we have trained before. We considered using feature explanation in Eq 3 to evaluate (even explain) models from training accuracy, features contribution and model complexity (quantity of features)."}, {"heading": "Experimental Study", "text": "We evaluate the performance of our feature explanation method using tree-structure models with standard data sets. In the test process, we simulate the workflow in Figure 1. Our results are evaluated by test accuracy and time efficiency\naccording to the models we selected. We considered k-fold as a benchmark cross validation method compared to feature explanation method."}, {"heading": "Experiment Subjects", "text": "\u201cFeatures\u201d are the core hyperparameter of all tree models, which means that it is important to combine \u201dfeatures\u201d with model selection and/or model interpretation in tree models. Considering characteristics of hyper-parameters in treestructure models, we designed explanation based on feature selection for tree-structure models. Specifically, we select Decision Tree, Random Forest, Extra Trees, Gradient Boosting and XGBClassifier for simulation. Hyper-parameters used in these models are through parameter tuning (Lavesson and Davidsson 2006) with available parameters in models. To tree-structure models, important features as \u201cmax depth\u201d, \u201cmin sample leaf\u201d and \u201ccriterion\u201d are included; \u201cmax features\u201d is limited by quantity of features."}, {"heading": "Datasets", "text": "We list details of data sets we have used in the experiment (cf. Table 1) for both binary and multi-object classification. We choose some classification datasets encompassing different areas, complexities and data size. The standard data sets contains Breast Cancer Wisconsin Dataset (Street, Wolberg, and Mangasarian 1993), Pima Indians Diabetes Database (Smith et al. 1988), Iris Data Set (Fisher 1936), Universal Bank Loan Data Set (from Kaggle), Wine Data Set (Forina et al. 1991). The dataset will be split into training dataset and test dataset (0.7/0.3) when using feature explanation as a method. When using cross validation, we will further split 20% of training dataset for validation."}, {"heading": "Evaluation Criteria", "text": "In this research, we focus on using feature explanation in model selection. When we compare performance of cross validation based selection and model selection by feature explanation, time efficiency and accuracy (ACC) performance are two factors we consider. Similar to normal model selection process, after we trained the tree models using parameter tuning with training datasets, we will use test datasets to evaluate the accuracy performance of selecting models according to accuracy matrix."}, {"heading": "Research Questions", "text": "To evaluate our method and compare it to cross validation in model selection, we explore the following research ques-\ntions (RQs, for short):\nRQ1. What is the relationship between feature explanation and accuracy matrix in model selection? How can we apply feature explanation method to select tree models? We know that feature explanation evaluates tree models from training accuracy, features contribution and model complexity. We need to experimentally evaluate what is the relationship between explanation and (test) accuracy. Depending on the empirical findings about the relationship, we could apply feature explanation method in tree models selection.\nRQ2. What is the effect to apply our feature explanation method in model selection compared to k-fold cross validation? Applying the results from RQ1 about selecting tree models using feature explanation, we need to experimentally compare the performance of our method with k-fold cross validation. The evaluation will look at accuracy performance and computational cost (time)."}, {"heading": "Experimental Results", "text": "In this section, we present the results of the experimental study, and interpret the research questions sequentially and separately to explain why the proposed approach is better than traditional cross-validation in model selection.\nRQ1: Relationship between feature explanations and accuracy matrix As discussed above, feature explanation explain tree models from training accuracy, features contribution and model complexity. When we apply feature explanation in model selection, we experiment the relationship between feature explanations and accuracy matrix using classification datasets. We have listed five of the results of relationship between feature explanations and accuracy matrix in Figure 3 from experiments of Decision tree model and Xgboost tree model. Each point in the relationship figure refers to a model evaluation based on hyperparameter tuning. In each experiment we present two figures, plotting the relationship between train accuracy and test accuracy. The experiment uses cross validation accuracy (CVaccuracy in figure) and Explaincv value (we defined in Eq 3) as control group. When we perform model selection, the results of cross-validation accuracy is the core evaluation to select \u201cgood\u201d model after training the model under most cases. The left part of this figure shows that cross validation cannot always select models with higher test accuracy. The right part of figure shows that when the value of feature explanation is smaller, the model is likely to achieve a higher test accuracy. It is worth noting that the distribution of models\u2019 test accuracy under cross validation and feature explanation is very similar. From Figure 3, cases (a) and (b) reflect cases in which it is hard to select models with best test accuracy by cross validation or feature explanation method, as shown by the sparse plots in test accuracy. The other three experiments \u2013 (c), (d), (e) \u2013 perform effectively. When the training accuracy is high and applying feature explanation method, smaller feature explanation\nvalues can lead to higher test accuracy. From the point distribution level, cross validation and feature explanation appear to aggregate when selecting models with high test accuracy, which might because cross validation and feature explanation have similar learning effects of features\u2019 characteristics. Figure 4 and Figure 5 are provided for completeness; the plots show more results for the relationship between features\nand accuracy matrix. In most cases, we find that low values of feature explanation lead to higher test accuracy (which means \u201cbetter\u201d models have been found). Some cases do not have clear distinction between accuracy of cross validation and feature explanation (e.g., cases (c) and (d) in Figure 5), which might be caused by model overfitting and datasets that are too simple for the task at hand.\nThe results show that compared to cross validation, in the majority of cases, the value of feature explanation in tree models show an inverse relationship with test accuracy. When we have a high test accuracy, feature explanation method and normal cross validation method have dense distribution. This means, feature explanation allows to find models with high test accuracy when the model\u2019s feature explanation value is low."}, {"heading": "RQ2: Feature Explanation in Model Selection", "text": "To evaluate the performance of feature explanation in model selection compared to cross validation method, we simulate the basic model selection on five datasets (cf. Table 2). \u201cTest Acc\u201d means the average test accuracy from the best three models selected from cross validation method (CV) and feature explanation method (FE). The results show that feature explanation method could select same or better models than cross validation method. In general, half of experiments show that FE achieve higher test accuracy of best three selected models than cross validation methods. In some cases (for example cases in GradientBoostingTree), FE achieves slightly worse test accuracy (no more than 1%). \u201cExec time\u201d refers to the execution time when applying CV and FE method in model selection respectively (including the whole process of model selection, cf. Figure 1). The results show that FE method saves at least 300% of execution time when selecting models in tree-structure models. Time efficiency of feature explanation is because the comparison of mechanism between CV and FE (cf. Figure 2). In model selection, the program needs to retrain and evaluate the model up to ten times (in 10-fold cross validation), which costs too much time. In feature explanation method instead, models that have been trained will be analysed by features contribution according to models\u2019 formation and features\u2019 contribution.\nThe results show that feature explanation performs at least as well as general cross validation method (k-fold) in treestructure model selection while feature explanation method has a notable advantages in time efficiency. It means that we can safely replace general cross validation with feature explanation method in tree-structure model selection in the vast majority of cases."}, {"heading": "Threats to Validity", "text": "The feature explanation method in this research is based on a novel notion in tree models (containing training accuracy, features contribution and model complexity). So this method is very sensitive to input features. Consider the noise of features (we consider the \u201cnoise\u201d as repeated or inappropriate features in datasets); selecting bias of features in datasets might impact the practical efficacy of our method. For example, in cases in which there is no benchmark feature collections (like financial market price prediction), the same model works better on datasets including more reasonable features than datasets with unreasonable ones. It means, the application of our method might take more time on feature selection in the data processing phase.\nAnother threat we must point out is about the settings of hyperparemeters in models. In our experimental study, hyperparemeters are selected from a reasonable search space. But we cannot cover all possible hyperparemeters in the experiments. In this research, we select most hyperparemeters that work for classification purpose.\nThe stochastic characteristics of model evaluation is also a threat. We are trying to mitigate this threat by running 20 times each of the experiments and choosing 5 different tree models to increase the diversity domain of all aspects."}, {"heading": "Conclusions", "text": "In this paper, we propose a new tree-model explanation approach for the model selection. According to our experiments, the results show that, in the vast majority of the cases, feature explanation allows to select models with high test accuracy when the models feature explanation value is low. Compared to general cross validation method (k-fold), our method performs better or similar in model selection performance while our method has a large efficiency improvement in model selection execution time. At the same time, we noticed that feature explanation based model selection would be impacted by features selection (eliminate noise generated by useless features).\nPossible future research directions might be the optimization of this explanation method and its application to other machine learning models."}], "title": "Better Model Selection with a new Definition of Feature Importance", "year": 2020}