{
  "abstractText": "Fair machine learning works have been focusing on the development of equitable algorithms that address discrimination of certain groups. Yet, many of these fairness-aware approaches aim to obtain a unique solution to the problem, which leads to a poor understanding of the statistical limits of bias mitigation interventions. We present the first methodology that allows to explore those limits within a multi-objective framework that seeks to optimize any measure of accuracy and fairness and provides a Pareto front with the best feasible solutions. In this work, we focus our study on decision tree classifiers since they are widely accepted in machine learning, are easy to interpret and can deal with nonnumerical information naturally. We conclude experimentally that our method can optimize decision tree models by being fairer with a small cost of the classification accuracy. We believe that our contribution will help stakeholders of socio-technical systems 1ana.valdivia@trilateralresearch.com (corresponding author) 2sanchez-monederoj@cardiff.ac.uk 3casillas@decsai.ugr.es Preprint submitted to arXiv June 23, 2020 ar X iv :2 00 6. 12 39 9v 1 [ cs .L G ] 2 2 Ju n 20 20 to assess how far they can go being fair and accurate, thus serving in the support of enhanced decision making where machine learning is used.",
  "authors": [
    {
      "affiliations": [],
      "name": "Ana Valdivia"
    },
    {
      "affiliations": [],
      "name": "Javier S\u00e1nchez-Monedero"
    },
    {
      "affiliations": [],
      "name": "Jorge Casillas"
    }
  ],
  "id": "SP:25890c1570a361aeb15517872f4c15d7d6cfeab2",
  "references": [
    {
      "authors": [
        "C. O\u2019Neil"
      ],
      "title": "Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy",
      "year": 2016
    },
    {
      "authors": [
        "V. Eubanks"
      ],
      "title": "Automating Inequality",
      "venue": "St. Martin\u2019s Press",
      "year": 2018
    },
    {
      "authors": [
        "J. Angwin",
        "J. Larson",
        "S. Mattu",
        "L. Kirchner"
      ],
      "title": "Machine bias",
      "venue": "ProPublica, May 23 ",
      "year": 2016
    },
    {
      "authors": [
        "T. Bolukbasi",
        "K.-W. Chang",
        "J.Y. Zou",
        "V. Saligrama",
        "A.T. Kalai"
      ],
      "title": "Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings",
      "venue": "in: 30th Conference on Neural Information Processing Systems ",
      "year": 2016
    },
    {
      "authors": [
        "M. Kearns",
        "S. Neel",
        "A. Roth",
        "Z.S. Wu"
      ],
      "title": "Preventing Fairness Gerrymandering: Auditing and Learning for Subgroup Fairness",
      "venue": "in: J. Dy, A. Krause (Eds.), Proceedings of the 35th International Conference on Machine Learning, Vol. 80 of Proceedings of Machine Learning Research, PMLR, Stockholmsmssan, Stockholm Sweden",
      "year": 2018
    },
    {
      "authors": [
        "J. Buolamwini",
        "T. Gebru"
      ],
      "title": "Gender shades: Intersectional accuracy disparities in commercial gender classification",
      "venue": "in: Conference on fairness, accountability and transparency",
      "year": 2018
    },
    {
      "authors": [
        "M.B. Zafar",
        "I. Valera",
        "M. Gomez Rodriguez",
        "K.P. Gummadi"
      ],
      "title": "Fairness beyond disparate treatment & disparate impact: Learning classification without disparate mistreatment",
      "venue": "in: Proceedings of the 26th International Conference on World Wide Web, International World Wide Web Conferences Steering Committee",
      "year": 2017
    },
    {
      "authors": [
        "B. Xia",
        "J. Yin",
        "J. Xu",
        "Y. Li"
      ],
      "title": "WE-Rec: A fairness-aware reciprocal recommendation based on Walrasian equilibrium",
      "venue": "Knowledge-Based Systems 182 ",
      "year": 2019
    },
    {
      "authors": [
        "M. Zehlike",
        "P. Hacker",
        "E. Wiedemann"
      ],
      "title": "Matching code and law: achieving algorithmic fairness with optimal transport",
      "venue": "Data Mining and Knowledge Discovery 34 (1) ",
      "year": 2020
    },
    {
      "authors": [
        "Z. Lipton",
        "J. McAuley",
        "A. Chouldechova"
      ],
      "title": "Does mitigating ml\u2019s impact disparity require treatment disparity",
      "venue": "in: Advances in Neural Information Processing Systems",
      "year": 2018
    },
    {
      "authors": [
        "R. Binns",
        "M. Van Kleek",
        "M. Veale",
        "U. Lyngs",
        "J. Zhao",
        "N. Shadbolt"
      ],
      "title": "it\u2019s reducing a human being to a percentage\u2019 perceptions of justice in algorithmic decisions",
      "venue": "in: Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems",
      "year": 2018
    },
    {
      "authors": [
        "M. Mitchell",
        "S. Wu",
        "A. Zaldivar",
        "P. Barnes",
        "L. Vasserman",
        "B. Hutchinson",
        "E. Spitzer",
        "I.D. Raji",
        "T. Gebru"
      ],
      "title": "Model Cards for Model Reporting",
      "venue": "in: Proceedings of the Conference on Fairness, Accountability, and Transparency, FAT* \u201919, ACM, New York, NY, USA",
      "year": 2019
    },
    {
      "authors": [
        "A.D. Selbst",
        "D. Boyd",
        "S.A. Friedler",
        "S. Venkatasubramanian",
        "J. Vertesi"
      ],
      "title": "Fairness and abstraction in sociotechnical systems",
      "venue": "in: Proceedings of the Conference on Fairness, Accountability, and Transparency, FAT* \u201919, ACM, New York, NY, USA",
      "year": 2019
    },
    {
      "authors": [
        "J. Snchez-Monedero",
        "L. Dencik",
        "L. Edwards"
      ],
      "title": "What Does It Mean to \u2019solve\u2019 the Problem of Discrimination in Hiring? Social",
      "venue": "Technical and Legal Perspectives from the UK on Automated Hiring Systems, in: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, FAT* 20, ACM, New York, NY, USA",
      "year": 2020
    },
    {
      "authors": [
        "S.A. Friedler",
        "C. Scheidegger",
        "S. Venkatasubramanian",
        "S. Choudhary",
        "E.P. Hamilton",
        "D. Roth"
      ],
      "title": "A comparative study of fairness-enhancing interventions in machine learning",
      "venue": "in: Proceedings of the Conference on Fairness, Accountability, and Transparency, FAT* \u201919, ACM, New York, NY, USA",
      "year": 2019
    },
    {
      "authors": [
        "A.K. Menon",
        "R.C. Williamson"
      ],
      "title": "The cost of fairness in binary classification",
      "venue": "in: S. A. Friedler, C. Wilson (Eds.), Proceedings of the 1st Conference on Fairness, Accountability and Transparency, Vol. 81 of Proceedings of Machine Learning Research, PMLR, New York, NY, USA",
      "year": 2018
    },
    {
      "authors": [
        "F. Kamiran",
        "T. Calders",
        "M. Pechenizkiy"
      ],
      "title": "Discrimination Aware Decision Tree Learning",
      "venue": "in: 2010 IEEE International Conference on Data Mining",
      "year": 2010
    },
    {
      "authors": [
        "A. Agarwal",
        "A. Beygelzimer",
        "M. Dudik",
        "J. Langford",
        "H. Wallach"
      ],
      "title": "A Reductions Approach to Fair Classification",
      "venue": "in: J. Dy, A. Krause (Eds.), Proceedings of the 35th International Conference on Machine Learning, Vol. 80 of Proceedings of Machine Learning Research, PMLR, Stockholmsmssan, Stockholm Sweden",
      "year": 2018
    },
    {
      "authors": [
        "A. Balashankar",
        "A. Lees",
        "C. Welty",
        "L. Subramanian"
      ],
      "title": "Pareto-Efficient Fairness for Skewed Subgroup Data",
      "venue": "in: International Conference on Machine Learning AI for Social Good Workshop, Long Beach, United States",
      "year": 2019
    },
    {
      "authors": [
        "M.B. Zafar",
        "I. Valera",
        "M. Gomez-Rodriguez",
        "K.P. Gummadi"
      ],
      "title": "Fairness constraints: A flexible approach for fair classification",
      "venue": "Journal of Machine Learning Research 20 (75) ",
      "year": 2019
    },
    {
      "authors": [
        "L. Hu",
        "Y. Chen"
      ],
      "title": "Fair classification and social welfare",
      "venue": "in: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, FAT* 20, Association for Computing Machinery, New York, NY, USA",
      "year": 2020
    },
    {
      "authors": [
        "N. Srinivasan",
        "K. Deb"
      ],
      "title": "Multi-objective function optimisation using nondominated sorting genetic algorithm",
      "venue": "Evolutionary Computation 2 (3) ",
      "year": 1994
    },
    {
      "authors": [
        "K. Deb",
        "A. Pratap",
        "S. Agarwal",
        "T. Meyarivan"
      ],
      "title": "A fast and elitist multiobjective genetic algorithm: NSGA-II",
      "venue": "IEEE Transactions on Evolutionary Computation 6 (2) ",
      "year": 2002
    },
    {
      "authors": [
        "E. Zitzler",
        "K. Deb",
        "L. Thiele"
      ],
      "title": "Comparison of multiobjective evolutionary algorithms: Empirical results",
      "venue": "Evolutionary Computation 8 (2) ",
      "year": 2000
    },
    {
      "authors": [
        "C. Rudin"
      ],
      "title": "Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead",
      "venue": "Nature Machine Intelligence 1 (5) ",
      "year": 2019
    },
    {
      "authors": [
        "A. Chouldechova"
      ],
      "title": "Fair prediction with disparate impact: A study of bias in recidivism prediction instruments",
      "venue": "Big data 5 (2) ",
      "year": 2017
    },
    {
      "authors": [
        "J.R. Foulds",
        "R. Islam",
        "K.N. Keya",
        "S. Pan"
      ],
      "title": "An intersectional definition of fairness",
      "venue": "in: 2020 IEEE 36th International Conference on Data Engineering (ICDE), IEEE",
      "year": 2020
    }
  ],
  "sections": [
    {
      "text": "Fair machine learning works have been focusing on the development of equitable algorithms that address discrimination of certain groups. Yet, many of these fairness-aware approaches aim to obtain a unique solution to the problem, which leads to a poor understanding of the statistical limits of bias mitigation interventions.\nWe present the first methodology that allows to explore those limits within a multi-objective framework that seeks to optimize any measure of accuracy and fairness and provides a Pareto front with the best feasible solutions. In this work, we focus our study on decision tree classifiers since they are widely accepted in machine learning, are easy to interpret and can deal with nonnumerical information naturally.\nWe conclude experimentally that our method can optimize decision tree models by being fairer with a small cost of the classification accuracy. We believe that our contribution will help stakeholders of socio-technical systems\n1ana.valdivia@trilateralresearch.com (corresponding author) 2sanchez-monederoj@cardiff.ac.uk 3casillas@decsai.ugr.es\nPreprint submitted to arXiv June 23, 2020\nar X\niv :2\n00 6.\n12 39\n9v 1\n[ cs\n.L G\nto assess how far they can go being fair and accurate, thus serving in the support of enhanced decision making where machine learning is used.\nKeywords: algorithmic fairness, group fairness, multi-objective optimization"
    },
    {
      "heading": "1. Introduction",
      "text": "Algorithmic and data-driven decision making has rapidly swept through several social, political and industry contexts. Beyond the possible misuses of technology, there is an increased awareness that these processes are not neutral and can reproduce and amplify past and current structural inequalities [1, 2]. Within this context, particular interest is paid to the role of machine learning (ML) with well known examples of models biased against historically discriminated groups [3, 4, 5] or the intersection of these groups [6, 7]. Fairness in ML has emerged as a community initially motivated to develop technological solutions to the disparate impact and treatment by biased algorithms [8, 9, 10, 11, 5] that also moves to a broader and multi-disciplinary understanding of the issues of socio-technological interventions [12, 13, 14, 15]. This work contribute to this field by studying how far bias mitigation can go whilst satisfying the accuracy and transparency of the models, thus providing a tool for a wider understanding of the technological boundaries of socio-technical proposals.\nBias mitigation techniques can broadly be divided into three non-exclusive categories [16]: (1) preprocessing, (2) inprocessing, and (3) postprocessing. The preprocessing techniques attempt to learn new representations of data to satisfy fairness definitions. The inprocessing methods involve modifying the classifier algorithm by adding a fairness constraint to the optimization problem. The postprocessing methods aim at removing discriminatory decisions after the model is trained. Normally, in inprocessing approaches the fairness criteria are used as an optimization constraint rather than as a guide to build a more equitable prediction model. As a result of the optimization process, those fixed restrictions will come out with a degree of equity that might not match the problem requirements whereas the space of solutions that can be reached remains\nunknown so that decision makers cannot observe the range of possibilities and their behaviour.\nThe main contribution of this paper is a methodology that explores optimal ML solutions and evaluates the boundaries of fairness in relation to other dimensions of the evaluation of an ML model. We claim that multi-objective evolutionary algorithms might be used to direct a meta-learning process for optimizing the hyperparameters of a classifier. In particular, we focus the study on the suitability of decision trees as base learners because of their properties of transparency and accuracy. Thus, we propose to use a genetic algorithm to tune the decision tree hyperparameters to find models that offer a wide repertoire of balances between precision and fairness. The architecture of this methodology can be applied on any type of classifier and hyperparameter set and the optimization is independent of the definition of fairness and precision. As a result of the meta-learning process, the method produces a Pareto front with a set of optimal feasible solutions. In this way, the method addresses the before mentioned issues of single constrained optimization proposals to build fair models.\nWe conduct an extensive set of experiments based on 5 real-world datasets which are widely used in the fairness literature. The solution space obtained by our approach indicates that there exists a wide number of optimal solutions (Pareto optimal) that are characterized by not being dominated by each other. We also evaluate the boundaries between accuracy and fairness that can be achieved on each problem, giving an empirical visualization of the limits between both measures. In addition, we assess how decision trees hyperparameters are affected by this tradeoff. Finally, a convergence analysis is also presented in order to evaluate the evolutionary approach of this methodology.\nAs far as we know, multi-objective optimization has not yet been used in the field of fairness in ML, so we believe that the proposal will open a very fruitful and beneficial research line, enriching the state-of-the-art."
    },
    {
      "heading": "2. Background",
      "text": "To ground our methodology, we begin by reviewing relevant related works. We then introduce evolutionary algorithms and briefly explain a type of algorithm belonging to this family."
    },
    {
      "heading": "2.1. Optimizing for fairness and accuracy",
      "text": "Bias mitigation algorithms often explicitly or implicitly add fairness constraints on model group performance. In this section, we introduce some related works that aim at optimizing for fairness and accuracy. For further information on the relation between accuracy and fairness measures we refer to [17].\nIn the context of decision trees, in [18] the information gain function is modified for splitting and pruning to add the entropy with respect to the sensitive attribute. The authors explored several options. The first one considers the entropy with respect to the class label, but it does not allow splitting if it introduces discrimination with respect to sensitive attribute. The second alternative implements a tradeoff between objectives by dividing the gain in accuracy by the gain in discrimination. This option did not achieve suitable results.\nMore recently, authors in [19] proposed to reduce fair classification to a sequence of cost-sensitive classification tasks to obtain Pareto optimality between overall accuracy and any fairness definition. In a related work, authors in [20] find a Pareto optimal point which maximizes multiple subgroup accuracy measures while satisfying equality of opportunity.\nZafar et. al [21] formulated the problem as a convex constrained optimization problem that allows a dual formulation in which accuracy is optimized under fairness constraints. In their formulation, fairness is introduced in terms of a measure of the decision boundary fairness that serves as a proxy to many fairness statistical metrics. The tradeoff between accuracy and fairness due to disparate mistreatment is expressed as a threshold parameter established by the user. Moreover, the formulation allows introducing several attributes as constraints, e.g. race and gender.\nHu et al. [22] transformed the constrained loss minimization problem into a social welfare maximization problem. Using SVM\u2019s regularization path and techniques from parametric programming, they show that always preferring more fair solutions does not abide by the Pareto Principle. They concluded that applying strict fairness criteria can lead to worse welfare outcomes for the groups.\nThus, there is an interest in exploring the simultaneous optimization of accuracy and fairness metrics. Some proposals obtain Pareto optimal solutions that implicitly set a tradeoff between objectives, whereas others relly this on a user parameter. As an alternative to this, our work aims to provide the whole Pareto front as a means to explore the impact of the ML models, or, in general, to understand the behaviour of the combination between a dataset and knowledge representation."
    },
    {
      "heading": "2.2. Evolutionary algorithms",
      "text": "Multi-objective optimization is a field of decision making which aims at optimizing simultaneously more than one objective function. This field of research has developed a large number of applications in engineering, economics, and logistics where optimal decisions need to be taken in the presence of tradeoffs between two or more competitive objectives. Maximizing comfort and energy saving in a climatization system is a practical example of multi-objective problem involving two objectives. Mathematically, this can be formulated as:\nmin (f1(x), . . . , fn(x)) s.t. x \u2208 X,\nwhere n > 1 is the number of objective functions and X is the set of feasible solutions.\nWhen multiple objective functions appear in a problem, no single solution exists that optimizes each function at once. Otherwise, the presence of multiple objectives gives a set of optimal solutions, possibly infinite. A solution is nondominated whether does not exist another solution that dominates the current\none, i.e., it does not improve one objective function without worsening other objective functions. Formally:\nDefinition 2.1. A solution x \u2208 X is said to dominate another solution x\u2032 \u2208 X, if it is better or equal in all the objectives and strictly better in at least on of them, i.e.:\n\u2022 fi(x) fi(x\u2032), \u2200i \u2208 {1, . . . , n} and,\n\u2022 fj(x) \u227a fj(x\u2032), for at least one index j \u2208 {1, . . . , n}.\nA solution is called Pareto optimal if there does not exist another solution that dominates it. Consequently, the set of all Pareto optimal solutions is defined as Pareto front or boundary. Assessing this frontier allows decision makers to select any efficient solution, depending on the worthiness of each objective function.\nEvolutionary algorithms (EAs) are often well-suited for solving optimization problems. They consist of meta-heuristic-based methods inspired by some aspects of natural evolution. The basic idea is that unfit members will die and not contribute to the gene pool of the offspring, while fitter individuals are allowed to survive and contribute to generate new solutions. Over the last decades, a number of multi-objective EAs have been developed, capable of searching for multiple Pareto optimal solutions concurrently in a single run."
    },
    {
      "heading": "2.3. NSGA-II",
      "text": "The non-dominated Sorting Genetic Algorithm (NSGA) [23] was one of the first EAs developed for multi-objective problem optimization. Yet this approach was criticized due to: (1) the high computational complexity, (2) the lack of elitism, and (3) the low spread of solutions. Then, the NSGA-II [24] was proposed as a modification to address these disadvantages. To solve (1) the authors proposed a non-dominated sorting procedure where all the individuals are sorted according to the level of non-dominance. In order to address the issue (2), they implemented elitism to store all non-dominated solutions and help to prevent\nthe loss of good solutions once they are found. This aspect also enhances the convergence property of EAs [25]. Finally, they adapted a suitable automatic mechanism based on the crowding distance to ensure diversity in a population and then solve (3). This distance function assigns a distance metric to all individuals within a population and then compares whether two solutions are close enough. A solution with a smaller value is more crowded by other solutions, therefore is more likely to not survive in further populations.\nThis approach starts by creating an initial parent population P of size N randomly. The population is evaluated by the objective functions and sorted following the non-dominance criteria 2.1. After that, a rank score is assigned to each solution where the first level corresponds to the best individuals, the second level is the next-best set of members, and so on. After that, the binary tournament selection, crossover, and mutation operators are used to create an offspring population. These children are also evaluated by the objective functions and combined together with the previous population. All individuals are then ranked and sorted by the non-domination rank and the crowding distance, which is considered the elitist step. The N -best members are then selected to form the next population. Finally, the algorithm finalizes when last generation is reached."
    },
    {
      "heading": "3. Multi-objective method for accurate and fair machine learning",
      "text": "Our proposed methodology is based on a multi-objective algorithm with a generational evolutionary approach. The goal is to guide a ML classifier to obtain the best tradeoffs between accuracy and fairness (Pareto optimal), by learning the best combination of hyperparameters given a dataset and its protected attribute that identifies a protected group. The selection mechanisms are inspired by the elitist NSGA-II method [24] which was described in the previous section. In particular, we propose decision trees as the ML classifiers to be optimized due to its comprehensive and transparency nature."
    },
    {
      "heading": "3.1. Meta-learning approach",
      "text": "The pseudo-code of the meta-learning approach is presented in Algorithm 1.\nAdditionally, Figure 1 presents a visual diagram of the process.\nSpecifically, the meta-learning consists of dividing the training set into two subsets (learning and validation) where the decision tree will be built from the first set, and the measures will be evaluated in the second one. The multiobjective algorithm will ensure that, iteration after iteration, the set of the best hyperparameter configurations will survive so that the NSGA-II will explore other settings around them. At the end, a set of optimal solutions is returned which will be tested in the testing set.\nIn this work, we propose decision trees as the base classifiers for the metalearning. Decision trees are considered white box models, since it is easy to analyze the steps taken to classify data [26]. They are easy to interpret, and they can be summarized in a set of rules. In addition, these kind of algorithms do not require data normalization or dummy variables creation, since they are able to use both numerical and categorical data. This fact simplifies the preprocessing step, which can directly affect the accuracy and fairness of the classifier [16]. Particularly, from these ML algorithms we are interested in the following hyperparameters:\n\u2022 criterion: This function measures the quality of a split. Decision trees\nsplit nodes as long as this value decreases. The purity of a node can be measured with the Gini index and the entropy.\n\u2022 max depth: The maximum depth of the tree. Deeper trees are more com-\nplex.\n\u2022 min samples split: The minimum number of samples required to split\nan internal node. In that case, a higher number of samples implies simpler trees.\n\u2022 max leaf nodes: Total number of leaves in a tree. The higher the number\nof leaves, the more complex the tree.\n\u2022 class weight: It is used to give weight to each class, which is considered\nwhen measuring the quality of the splits. It is very useful for unbalanced datasets where models usually misclassified the minority class. It takes values in [0, 1]. The positive class is weighted with class weight, while the negative one is 1\u2212class weight. A value of 0.5 means both classes are evenly considered.\nThe criterion, max depth, and min samples split adjust the size of the tree in different directions, which means that different balances between precision and complexity can be found. Moreover, if the search of the best set of\nhyperparameters is guided by any fairness metric, the structure of the tree can be regulated towards branches that do not generate disparities among groups. The class weight hyperparameter addresses disparity by the transferring instances between false positives and false negatives.\nThe main advantage of the proposed method is that it can obtain a wide number of optimal solutions in just one run. The design of this methodology also allows the use of any ML classifier without the need to modify it, unlike the inprocessing techniques. Moreover, our methodology supports any type of data since decision tree classifiers allow either numerical and categorical attributes. Finally, our proposal also supports another of the fairness community\u2019s claims, which is transparency. By using decision trees as classifiers, we allow decision makers to know all the decisions the model made once trained. Also, optimization functions do not need to be differentiable allowing a wider bank of fairness definitions.\nFormally, the jth-individual, Ikj , of the kth-population, Pk, is a trained decision tree. In turn, this tree is trained with a m-tuple gen, gkj , which contains the values of each hyperparameter h = {h1, . . . , hm} on each corresponding position, hence m = 5:\nIkj := decision tree(gkj)\nh := {criterion, max depth, min samples split, max leaf nodes,\nclass weight}.\nSince some of the tree hyperparameters are categorical or integer numbers, the chromosomes are decoded after its generation in order to obtain the proper value for the classifier.\nIn the following sections we extensively describe the evolutionary algorithm\ncomponents."
    },
    {
      "heading": "3.2. Pool initalization",
      "text": "The initialization step generates the first pool. The first individual generated\n(I11) is created with default values of hyperparameters: g11 = (Gini,\u221e, 2,\u221e, 0.5).\nAlgorithm 1: Meta-learning algorithm\nInput: objective function of accuracy and fairness (f1 and f2), number of\nhyperparameters of the ML classifier (m), intervals of hyperparameters (min (hi) and max (hi) \u2200i \u2208 {1, . . . ,m}), datasets, and the protected attribute\nOutput: Set of ML models with different accuracy-fairness tradeoffs Data: training (learning and validation) and testing dataset (Dlearn, Dval,\nand Dtest)\nParameters: number of generations (G), population size (N), crossover\nprobability (pc), mutation probability (pm), mutation parameter (\u00b5)\nbegin\ninitialize population P1 evaluate objective functions (P1, Dval) non-dominated rank individuals of P1 while k \u2264 G do\nP (1) k \u2190 elitist selection (Pk\u22121) P (2) k \u2190 crossover (P (1) k ) P (3) k \u2190 mutation (P (2) k ) while 1 \u2264 l \u2264 N do\ncreate Skl solution by training classifier (Ikl, Dlearn) evaluate objective functions (Skl, Dval)\nend while non-dominated rank individuals of population P (3) k Pk \u2190 elitist non-dominated replacement (P (3)k , Pk\u22121)\nend while return non-dominated solutions in Pk\nend\nThe purity of the node is measured with the Gini index; the tree can be widened and deepened as needed since the limits for the depth and number of leaves within a node is not fixed and the lowest minimum of samples to split is used; both positive and negative class have the same weight. After training the first tree with these hyperparameters, the remaining individuals are generated considering the actual values of depth and leaves of that first tree as limit. The second individual will be generated with entropy criterion and those limits, while the rest of individuals are generated with random hyperparameters within the limits fixed by the first individual.\nFor a better understanding of the previous paragraph, we propose a practical case. Given the first individual of the first generation of the meta-learning (I11), the first tree is trained with the specific values of the hyperparameters (Gini,\u221e, 2,\u221e, 0.5). Thereafter, the decision tree has a depth of value depth(I11) = D and a total number of leaves equals to leaves(I11) = L. The second individual (I12) is then trained with the following hyperparameter set: (entropy,D, 2, L, 0.5). These limits for the depth and number of leaves of the tree (D and L) will be preserved throughout the process until completion, i.e., I1j = (c, d, s, l, w) with c \u223c {Gini, entropy}, d \u223c U(1, D), s \u223c U(2, training set size), l \u223c U(1, L), and w \u223c U(0, 1). In this way, this adhoc modification will let the meta-learning to better adjust to dataset characteristics."
    },
    {
      "heading": "3.3. Crossover operator",
      "text": "The crossover generates two individuals (Ikj and Ik,j+1) that inherit the hyperparameters given by two parents (Ik\u22121,a and Ik\u22121,b), depending on the crossover probability (pc). Concretely, this match is based on a given parameter u \u223c U(0, 1) which follows a uniform distribution. If this value is u \u2264 pc, the crossover function assigns the same hyperparameter value of the parents to the children. Otherwise, it assigns a linear combination of parents\u2019 hyperparameters (gk\u22121,a and gk\u22121,b), where the parameter \u03b2 \u223c U(0, 1):\ngkj = gk\u22121,a + gk\u22121,b 2 + \u03b2 |gk\u22121,a \u2212 gk\u22121,b| 2\ngk,j+1 = gk\u22121,a + gk\u22121,b 2 \u2212 \u03b2 |gk\u22121,a \u2212 gk\u22121,b| 2\nAfter that, genes of the resulting offspring are rounded off and decoded in order to obtain the proper values for the hyperparameters. In integer genes, the rounded values replaces the decimal ones to ensure a more effective search space."
    },
    {
      "heading": "3.4. Mutation operator",
      "text": "The mutation operator changes the real membership function hyperparameter values encoded in the chromosome, according to the mutation probability (pm) per individual. The gene (hyperparameter) to be mutated is randomly selected over the five genes. Then, given u\u2032, u\u2032\u2032 \u223c U(0, 1), the chromosome is mutated as follows:\ngkj =  gkj + \u03b4(gkj \u2212min (hi)), u\u2032 < 0.5gkj + \u03b4(max (hi)\u2212 gkj), u\u2032 \u2265 0.5 where,\n\u03b4 =  \u22121 + 2u \u2032\u2032 1\u00b5+1 , u\u2032\u2032 \u2264 0.5\n1\u2212 2(1\u2212 u\u2032\u2032) 1 \u00b5+1 , u\u2032\u2032 > 0.5."
    },
    {
      "heading": "3.5. Multi-objective approach",
      "text": "The multi-objective optimization is based on two objective functions to be minimized: f1 evaluates the accuracy and f2 the fairness of the model. Thus, f1 is focused on improving the prediction performance while f2 is used to mitigate the discrimination of the ML algorithm.\nBoth concepts of accuracy and fairness can be defined in several ways, referring to different meanings. Although the proposed methodology is totally flexible for using any definition, in this work we focus on two of them. We define y as the binary class label vector where 1 is the positive outcome and 0 is\nthe negative outcome; y\u0302 is the predicted outcome of the ML classifier; z is the associated protected feature of each individual, where 1 is the privileged class."
    },
    {
      "heading": "3.5.1. Error",
      "text": "We consider the Geometric Mean (G-mean) to evaluate the performance of the assessment task. G-mean is also widely used for quantifying the classifier performance in class imbalanced problems, since it evaluates both positive and negative class. It combines True Positive Rate (TPR) (Pr(y\u0302 = 1 | y = 1)) and True Negative Rate (TNR) (Pr(y\u0302 = 0 | y = 0)):\nG-mean(y\u0302, y) = \u221a P (y\u0302 = 1 | y = 1) \u00b7 P (y\u0302 = 0 | y = 0).\nBy maximizing this measure, we ensure the cost of false positive and false negative to be low. Since our method is designed for a minimization problem, we consider the first objective function as the G-mean error, i.e. f1(y\u0302, y) = 1\u2212G-mean(y\u0302, y)."
    },
    {
      "heading": "3.5.2. Unfairness",
      "text": "We consider the difference of the unfairness measure proposed for avoiding disparate mistreatment, defined as False Positive Rate (FPR) [8, 27]. This definition ensures that missclassification rates are balanced across groups of the protected attribute z:\nf2(y\u0302, y) = FPRdiff(y\u0302, y) = |P (y\u0302 6= y | z = 0, y = 0)\u2212 P (y\u0302 6= y | z = 1, y = 0)|."
    },
    {
      "heading": "3.5.3. Domination criterion",
      "text": "Given X the genotype (hyperparameters) and Y the phenotype (decision trees), the f : X \u2192 Y map obtained by the proposed method is characterized by being a non-injective non-surjective function. It is not injective as different values of hyperparameters can lead to obtain exactly the same decision tree. It is not surjective as the image (set of all possible decision trees generated by our method) does not fill the whole codomain, i.e., it is not possible to obtain any decision tree, only those generated by the learner. The cardinality of Y is much more lesser than the cardinality of X.\nAs a result, there are many different individuals that generate exactly the same decision tree, and so the same objective functions. This impairs the search process as variations generated by crossover and mutation do not change the objective functions. To palliate this effect, we have improved the domination criterion as follows. Once two individuals have the same values for both objectives, we consider that the individual that generates the tree with the lowest number of leaves dominates the other one. In case of a tie also in this value, the individual with the lowest value of the hyperparameter max leaf nodes is considered to dominate the other one."
    },
    {
      "heading": "4. Experimental Analysis",
      "text": "In this section we first describe the datasets used for assessing the proposed methodology. After that, we define the parameter setup used in these experiments. Finally, the obtained results and its analysis is provided."
    },
    {
      "heading": "4.1. Datasets",
      "text": "We ran experiments based on five realworld datasets from different domains like salaries, recruitment processes, credit risks, or recidivism risk assessment. These datasets have been widely used as benchmarking in state-of-art in fairness [16]. They are freely available in a Github repository4. A brief description of the dataset context is given below:\n- Adults: This dataset contains demographic information about US citi-\nzens in 19945. There are 32,561 instances and 14 attributes. The prediction task is to asses whether an individual earns more (positive class) or less (negative class) than $50K per year. The protected attribute considered is race.\n4https://github.com/algofairness/fairness-comparison/tree/master/fairness/\ndata. Last date accessed: June 9, 2020 5http://archive.ics.uci.edu/ml/datasets/adult\n- German: It contains financial information about individuals6. There are\n1,000 instances and 20 attributes. The prediction task is to assess the credit risk of individuals. The protected attribute considered is age.\n- ProPublica: This dataset is about the performance of COMPAS algo-\nrithm, a statistical method for assigning risk scores within the US criminal justice system created by Northpointe. It was published by ProPublica in 2016 [4], claiming that this risk tool was biased against African-American individuals. In this dataset, they analyzed the COMPAS scores for \u201crisk of recidivism\u201d and checked to see how many were charged with new crimes over the next two years. It contains individuals from the Broward County (Florida) in 2013 and 2014. There are 7,214 individuals containing 52 attributes. From these attributes, we have used the following 12 in the experiments of this paper [16]: sex, age, age cat, race, juv fel count, juv misd count, juv other count, priors count, c charge degree, c charge desc, decile score, score text. The prediction variable is whether the individual will be rearrested in two years or not. The protected attribute is race.\n- ProPublica violent: This dataset describes the same scenario as the pre-\nvious one, but in this case the outcome is whether the rearrest happened within two years was for a violent crime [4]. It contains 4,743 individuals and also the 12 attributes. The protected attribute is also race.\n- Ricci: This dataset comes from labour law case from the United States,\nwhere several firefighters from New Haven (Connectitut, US) claimed for disparate impact on the promotion decision. It contains the scores obtained in the exam taken to be promoted [3]. There are a total number of 118 rows and 4 attributes. The protected attribute is race.\nEach dataset is preprocessed to assure that the input data satisfies the clas-\n6http://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)\nsifier requirements by removing features that should not be used for the classification task, imputing missing values or transforming features like dates, etc. We also transform all the protected attributes into binary (e.g., \u201cwhite\u201d-\u201cnot white\u201d, \u201cyounger than 25 years old\u201d-\u201colder than 25 years old\u201d, \u201ccaucasian\u201d-\u201cnot caucasian\u201d). Table 1 shows the number of features selected for each dataset and class distribution."
    },
    {
      "heading": "4.2. Parameter setup",
      "text": "The experiments are replicated 10 times with different seeds to ensure stability and reproducibility. In each seed, the learning (56.25%), validation (18.75%), and testing (25%) sets are randomly sampled. The parameters for the evolutionary method are set as follows:\n\u2022 300 generations (G = 300),\n\u2022 50 individuals (N = 50),\n\u2022 1 as crossover probability (pc = 1),\n\u2022 0.3 as mutation probability (pm = 0.3),\n\u2022 5 as mutation parameter (\u00b5 = 5).\nThe code is implemented in Python using libraries such as pandas for data processing, sklearn.DecisionTreeClassifier for machine learner (CART algorithm) and numpy for numerical processing. The original code of the NSGAII algorithm is available at github.com/baopng/NSGA-II (last date accessed:\nJune 9, 2020). This research complies with research reproducibility principles. Code and data are made open and available in a public repository: https://github.com/anavaldi/fairness_nsga (last date accessed: June 9, 2020)."
    },
    {
      "heading": "4.3. Analysis of results",
      "text": "In this section, we empirically study the limits of the accuracy-fairness tradeoff. We first analyze the properties of the Pareto optimal solutions obtained when optimizing both together. We also analyze the relationship between decision tree learner\u2019s hyperparameters and measures\u2019 values. Finally, we present the convergence properties of the meta-learning approach."
    },
    {
      "heading": "4.3.1. Analysis of accuracy-fairness tradeoff",
      "text": "The averaged results over 10 runs are shown in Table 2 for the five realworld problems. To represent the average distribution of the obtained results, we have computed the average of the ten runs at minimum value of error in validation dataset (Errorv), 25th percentile (Q1), 50th (Q2), 75th (Q3) and maximum value of error. As the set of inferred solutions are Pareto efficient, the corresponding values of unfairness are reversely sorted. In the case of the two ProPublica problems, the results obtained by COMPAS are also included to better understand the room for improvement in those cases.\nThe obtained results in Ricci are very particular. We found that this problem is very easy to be solved in terms of accuracy by a decision tree learner, i.e., it is possible to obtain solutions with almost zero error and, therefore, almost one unfairness. In fact, in some partitions the solution found was perfect. Consequently, the multi-objective optimization tends to obtain very spread Pareto solutions, so we decided to leave this problem out of the rest of the analysis.\nWhile the validation dataset is used to guide the meta-learning algorithm, the test dataset is never used. When comparing validation and test columns, we observe that, although the scores in test are slightly worse than validation (as expected), the Pareto efficiency in test also remains, which shows the robustness\nof our methodology. Yet the results are overfitted regarding the unfairness measure (i.e., strong differences between Unfairnessv and Unfairnesst in Table 2) when it comes to very low values.\nWhen comparing the average results that occupy the first (min) and 50th (Q2) positions of error (Q1 in Adult), we are able to estimate the percentage of accuracy that needs to be sacrificed to improve fairness. The accuracy lost in test (Errort) is 6%, 10%, 8% and 13% in Adult, German, ProPublica, and ProPublica Violent problems, respectively, whilst the fairness improvement is of 81%, 66%, 54% and 53%, respectively. This gives us an idea of how it is possible to optimize the ML process to generate fairer solutions without an excessive loss of precision, which should encourage ML designers to incorporate fairness criteria into these processes.\nFocusing on the two ProPublica problems, where the prediction made by COMPAS is widely known, we can analyze the accuracy and fairness achieved by the Northpointe\u2019s software when assessing a criminal defendant\u2019s likelihood to re-offend. We can observe that the most unfair solution got by our methodology is much fairer than the obtained by COMPAS. This demonstrates the improvement margin of fairness in these problems when guiding the ML process by unbiased measurements. If we interpolate the fairness scores got by our methodology for an accuracy equal to COMPAS\u2019s, the test results would be (Errort,Unfairnesst) = (0.3476, 0.0987) in ProPublica and (Errort,Unfairnesst) = (0.3349, 0.0992) in ProPublica Violent, showing that our method improves the fairness of COMPAS\u2019s solutions in 67% and 71%, respectively, without compromising accuracy.\nWhen analyzing the performance of solutions, we are additionally concerned with transparency of the classifiers. Indeed, in the problems considered in our experimental analysis, where wrong outcomes may discriminate unfavored social groups, to understand the reasoning behind a machine decision is critical. Therefore, we analyze in which degree the Pareto optimal models are also easy to interpret. The fact of being using a decision tree structure to represent the knowledge helps to understand the machine decision criteria compared with\nother black-box models, but the complexity of these trees will also influence on its interpretability, as an excessively fine-grain decision boundary (high number of leaves) and complex multivariate conditions (high depth of the tree) would be hardly understandable.\nAnalyzing the complexity results in Table 2, we observe that the number of leaves is relatively low in the most accurate solutions, but tends to increase as fairness improves. This effect shows that the method needs to use more leaves to improve fairness with a minimum loss of accuracy. This is an expected result since equalizing false positive rates between the two people groups forces a finer partitioning of data. The high depth with a relatively low number of leaves suggests the construction of unbalanced decision trees (keep in mind than a perfectly balanced binary tree would need 2depth leaves, which is very far from what we get). That is, some few leaves need a high depth (i.e., extensive multiple conditions) to be effective.\nAnalogously, it is well known that a lower error implies a higher complexity, so it is curious to observe that this relation is not shown in the obtained results. The reason is simply that the complexity (number of leaves and depth) is not considered as a criteria to be optimized by our methodology, so this variable is freely adapted to the two contradictory objectives (accuracy and fairness), both of them demanding higher complexity to be reached. It seems that the fairness objective ends up winning the battle. In other words, the algorithm finds it harder to improve fairness than accuracy with a reduced complexity. Nevertheless, this interesting effect deserves a deeper study that would divert us from the main goal of our research in this paper, so we leave it as a further research line.\nTo better understand the behavior of the proposed method, Figure 2 plots the obtained Pareto optimal solutions, with orange dots being the solutions of each run and dark gray dots connected by lines represent the average Pareto front. This average Pareto is obtained by firstly getting the rounded mean number of different solutions n (which corresponds to the number of dark gray dots) and then obtaining the average values at n different percentiles positions equally\ndistributed. For example, if we have three runs where we got 3, 5 and 7 Pareto optimal solutions, we would obtain the n = 5 evenly distributed percentiles (i.e., 1st, 25th, 50th, 75th and 100th) with linear interpolation between adjacent ranks in each run and then calculate the average value for each percentile. The interquartile range (Q3\u2212Q1) of the error is represented with the light blue area in the figures.\nThe spread of the dots (especially in fairness dimension) and the width of the interquartile range suggests us that the attainable levels of accuracy and fairness is quite sensitive to the dataset partitions into training and test. This is particularly serious in German. The exception is represented by Adult, where the solutions in different data partitions are very compact. This may be due the fact that Adult has a considerably high number of data, so that the bias of the data partitioning is mitigated. As our methodology splits the training data into learning and validation, it suffers when very little data is available, as in German.\nAs we can observe from the plotted Pareto fronts, the contradictory condition between accuracy and fairness is clear: more accuracy implies less fairness, and vice versa, as analyzed in previous works [19, 20]. Although what is really interesting to analyze is the shapes of the averaged Pareto fronts as they provide valuable information about how the combination dataset and decision tree is working. In fact, beyond generating a wide repertoire of solutions with different balances of accuracy and fairness, our methodology also returns a greater understanding of the problem by explaining how these contradictory criteria are related.\nLet us take as example the ProPublica problem. The accuracy-fairness relation is rather linear in the range [0.026, 0.125] of unfairness, i.e., range [0.328, 0.361] of error. Then, we see a clear knee of the curve below an unfairness of 0.026, meaning beyond this threshold, improving a bit the fairness has a relatively high cost in accuracy. Similar conclusion can be taken in the other problems, where the unfairness threshold is around 0.01 in Adult, and 0.02 in German and ProPublica Violent. This knowledge could be used by other researchers and practitioners to set different fairness requirements depending on the problem when employing decision trees."
    },
    {
      "heading": "4.3.2. Analysis of learner\u2019s hyperparameters",
      "text": "As we are proposing a meta-learning method that indirectly controls the generated decision tree by tuning the hyperparameters of the learner, we are\nalso interested in assessing the impact of learner\u2019s hyperparameters on the performance. We have already discussed in the previous section the effect of demanding optimal fairness in the complexity of the trees (good fairness needs higher number of leaves). Here we analyze the effect of two other hyperparameters: min samples split and class weight. We did not find significant results in the fifth hyperparameter (criterion). Figure 3 shows the values of these two hyperparameters in the obtained Pareto optimal solutions of ProPublica Violent. The mean values over all the runs is plotted as lines and dots, while the shaded areas and error bars correspond to the standard deviation. Blue color is used for error and red color for unfairness, both in test datasets (Errort and Unfairnesst).\nIn the case of min samples split, the results confirm our guess that in order to improve fairness it is necessary to deepen certain branches of the tree, so that a low value of the limit of samples needed to divide a node helps to generate fairer trees. It is interesting to see here how a high value of this limit hurts fairness a lot but does not influence accuracy.\nWith regard to class weight, which controls the importance of the positive class (and reversely the negative one), the effect is as follows. In accuracy, a higher weight of the positive class implies generating more accurate solutions in this imbalance dataset (there are five times more of the negative than the positive). This makes sense as G-mean measure rewards balanced predictive precision in the two classes, so making more important the minority (positive) class helps to this goal. This hyperparameter has the contrary effect in fairness. Here, fairer solutions are obtained when a positive class weight in [3, 5] is given (moreover, with a low variance that ensure statistical significance), i.e., to decrease the importance of the positive class (which in the analyzed problem means that the criminal defendant re-offends) reduces the false positives, which makes easier to generate decision trees with a better balance of false positive rates between the two groups (Caucasian vs. rest of ethnics). In other words, giving less credibility to the positive class (re-offend) allows for fairer classifiers. However, we cannot ignore that this could also be a side effect of the Pareto\nefficiency followed by the optimization process."
    },
    {
      "heading": "4.3.3. Analysis of convergence",
      "text": "An algorithm converges when there is no significant improvement in the values of the objective functions of the population from one to the next generation. This aspect is important to be studied in order to assess efficiency of the method. At the same time, its analysis can reveal the resistance of each problem to allow improvements of the accuracy and fairness measures.\nIn multi-objective optimization, convergence is more complex to analyze as many optimal solutions evolve at the same time. To summarize the behavior of the process, Figure 4 presents the mean, Q1 and Q3 of the two objectives (error and unfairness in validation set) for the obtained Pareto set at each generation (averaged results over 10 runs are plotted). In some way, the mean gives an idea about the quality of the solutions (the lower the better) while the interval [Q1,Q3] represents the diversity of the Pareto sets (the wider the better). In Ricci, the algorithm fully converges very quickly (these values do not change at all after 27 generations), so we omit this plot for the sake of clarity of the paper.\nWe observe that low unfairness is faster to get than low error, so in the\nfirst third of the evolution good fairness is reached in all the problems, while the accuracy is slowly improved until the end of the process. Adult has the most stable convergence of the four shown problems due to the reduced bias in data partitions as above said. German also converges very well, but with a slight improvement of accuracy in the last 40 generations at the expense of making fairness slightly worse. ProPublica shows the most continuous convergence where accuracy and fairness are persistently improved. In ProPublica Violent, good fairness is very quickly obtained while accuracy is continually enhanced."
    },
    {
      "heading": "5. Conclusions",
      "text": "In this work we propose a meta-learning multi-objective optimization algorithm to explore the boundaries of fairness in real world problems. We present a methodology that (1) enables standard ML algorithms to be fairness-aware, (2) obtains the experimental frontier of the accuracy-fairness tradeoff, (3) uses interpretable models as base learners to comply with transparency values, and (4) converges rapidly to optimal solutions. To the best of our knowledge, this is the first work that proposes both accuracy and fairness as objective functions for a multi-objective ML approach.\nAccuracy-Fairness: Throughout the experimental analysis, we show the optimal fitness that can be achieved by optimizing the geometric mean of the predictive precision of each class versus false positive rate equality of the groups, i.e., no disparate mistreatment as defined in [21]. The cost in accuracy when satisfying fairness criteria has been theoretically studied (e.g. [17, 21]). These studies demonstrate the existence of a unavoidable tradeoff between accuracy with respect to the target variable and fairness with respect to the sensitive attribute. That is, when one objective is improved by the model the second one is penalized. Or what is the same, these two objectives are contradictory. Based on this assertion, we design in this paper an optimization process able to push both objectives to the frontier where the mentioned Pareto efficiency is reached, thus returning a plethora of solutions with different accuracy-fairness balances. Besides, the experimental analysis shows how fair can we go in a specific problem by decision trees, providing further insight about the capability of standard ML algorithms to get good fairness and the flexibility of the problem (dataset) to allow this.\nFairness-Transparency : As it is well known in ML, decision trees can improve accuracy (at least, while the sweet spot without overfitting is reached) often by increasing the model complexity (i.e., tree depth and number of leaves). Moreover, we believe that, in order to improve fairness, the decision tree needs to be deeper for a fine-grain data partition to hold misclassification parity between\ndifferent groups having different values of the sensitive attribute. Therefore, both accuracy and fairness demand more complex decision trees. When optimizing accuracy and fairness together, we find that the process tends to solve the conflict by generating more complex trees in fairer solutions even when their accuracies are not so good. This may be due to the fact that, when optimizing learner\u2019s hyperparameters as our methodology do, fairness is mainly reached by more complex trees while there are other chances of improving accuracy by fine tuning the remaining hyperparameters.\nConvergence: Evolutionary algorithms are sharply criticized because of its low convergence in many problems. Nevertheless, we show that this methodology early achieves optimal solutions. Objective functions cooperate to generate good solutions in the first generations, but they compete to obtain optimal solutions at the end.\nFuture work : Although we know that technology interventions alone will not address social injustice, there are several interesting directions highlighted by our findings. From the obtained results, it is clear a further research is needed to understand the role of transparency (in terms of model complexity) in the accuracy-fairness tradeoff. Therefore, we propose to add the complexity of the trees as a third objective function (f3). Regarding the fact that fairness can be defined in multiple ways, we plan to develop further analysis with different measures of mistreatment. In relation to claims by [17], it would be interesting to study dataset properties, such as correlation of the sensitive attribute with the target variable. We are aware that the experiments presented in this work only include one binary sensitive attribute. We propose to consider more attributes in further experiments to analyze how convergence is affected. Differential fairness [28] is a growing concept highly related with this work, which addresses intersectionality. We propose to run new experiments of our meta-learning algorithm proposing this new fairness definition. Finally, it is worth mentioning that our approach is completely flexible, and its design allows the use of any type of classifier and hyperparameters, that serving as a tool to experimentally analyze several dimensions of the behavior of ML methods."
    }
  ],
  "title": "How fair can we go in machine learning? Assessing the boundaries of fairness in decision trees",
  "year": 2020
}
