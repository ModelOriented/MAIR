{
  "abstractText": "Neural networks are among the most accurate supervised learning methods in use today. However, their opacity makes them difficult to trust in critical applications, especially when conditions in training may differ from those in practice. Recent efforts to develop explanations for neural networks and machine learning models more generally have produced tools to shed light on the implicit rules behind predictions. These tools can help us identify when models are right for the wrong reasons. However, they do not always scale to explaining predictions for entire datasets, are not always at the right level of abstraction, and most importantly cannot correct the problems they reveal. In this thesis, we explore the possibility of training machine learning models (with a particular focus on neural networks) using explanations themselves. We consider approaches where models are penalized not only for making incorrect predictions but also for providing explanations that are either inconsistent with domain knowledge or overly complex. These methods let us train models which can not only provide more interpretable rationales for their predictions but also generalize better when training data is confounded or meaningfully different from test data (even adversarially so).",
  "authors": [
    {
      "affiliations": [],
      "name": "Andrew Slavin Ross"
    }
  ],
  "id": "SP:e39b25f02d450562a8bd532f8ab18aac41264ebc",
  "references": [
    {
      "authors": [
        "P. Abell"
      ],
      "title": "Narrative explanation: an alternative to variable-centered explanation? Annu",
      "venue": "Rev. Sociol., 30, 287\u2013310.",
      "year": 2004
    },
    {
      "authors": [
        "J. Adebayo",
        "J. Gilmer",
        "I. Goodfellow",
        "B. Kim"
      ],
      "title": "Local explanation methods for deep neural networks lack sensitivity to parameter values",
      "year": 2018
    },
    {
      "authors": [
        "J. Angwin",
        "J. Larson",
        "S. Mattu",
        "L. Kirchner"
      ],
      "title": "How we analyzed the compas recidivism algorithm",
      "venue": "ProPublica.",
      "year": 2016
    },
    {
      "authors": [
        "A. Athalye",
        "I. Sutskever"
      ],
      "title": "Synthesizing robust adversarial examples",
      "venue": "arXiv preprint arXiv:1707.07397.",
      "year": 2017
    },
    {
      "authors": [
        "J. Ba",
        "R. Caruana"
      ],
      "title": "Do deep nets really need to be deep? In Advances in neural information processing systems, pp",
      "venue": "2654\u20132662.",
      "year": 2014
    },
    {
      "authors": [
        "S. Bach",
        "A. Binder",
        "G. Montavon",
        "F. Klauschen",
        "M\u00fcller",
        "K.-R.",
        "W. Samek"
      ],
      "title": "On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation",
      "venue": "PloS one, 10 (7), e0130140.",
      "year": 2015
    },
    {
      "authors": [
        "S.H. Bach",
        "M. Broecheler",
        "B. Huang",
        "L. Getoor"
      ],
      "title": "Hinge-loss markov random fields and probabilistic soft logic",
      "venue": "Journal of Machine Learning Research, 18 (109), 1\u201367.",
      "year": 2017
    },
    {
      "authors": [
        "D. Baehrens",
        "T. Schroeter",
        "S. Harmeling",
        "M. Kawanabe",
        "K. Hansen",
        "M\u00c3\u017eller",
        "K.-R."
      ],
      "title": "How to explain individual classification decisions",
      "venue": "Journal of Machine Learning Research, 11 (Jun), 1803\u20131831.",
      "year": 2010
    },
    {
      "authors": [
        "D. Barber"
      ],
      "title": "Bayesian reasoning and machine learning",
      "venue": "Cambridge University Press.",
      "year": 2012
    },
    {
      "authors": [
        "D. Bau",
        "B. Zhou",
        "A. Khosla",
        "A. Oliva",
        "A. Torralba"
      ],
      "title": "Network dissection: Quantifying interpretability of deep visual representations",
      "venue": "Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on, IEEE, pp. 3319\u20133327.",
      "year": 2017
    },
    {
      "authors": [
        "C.M. Bishop"
      ],
      "title": "Curvature-driven smoothing: a learning algorithm for feedforward networks",
      "venue": "IEEE Transactions on Neural Networks, 4 (5), 882\u2013884.",
      "year": 1993
    },
    {
      "authors": [
        "Y. Butalov"
      ],
      "title": "The notMNIST dataset",
      "venue": "http://yaroslavvb.com/upload/ notMNIST/.",
      "year": 2011
    },
    {
      "authors": [
        "N. Carlini",
        "D. Wagner"
      ],
      "title": "Defensive distillation is not robust to adversarial examples",
      "venue": "arXiv preprint arXiv:1607.04311.",
      "year": 2016
    },
    {
      "authors": [
        "R. Caruana",
        "Y. Lou",
        "J. Gehrke",
        "P. Koch",
        "M. Sturm",
        "N. Elhadad"
      ],
      "title": "Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission",
      "venue": "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
      "year": 2015
    },
    {
      "authors": [
        "A. Chandrasekaran",
        "D. Yadav",
        "P. Chattopadhyay",
        "V. Prabhu",
        "D. Parikh"
      ],
      "title": "It takes two to tango: Towards theory of ai\u2019s mind",
      "venue": "arXiv preprint arXiv:1704.00717.",
      "year": 2017
    },
    {
      "authors": [
        "X. Chen",
        "Y. Duan",
        "R. Houthooft",
        "J. Schulman",
        "I. Sutskever",
        "P. Abbeel"
      ],
      "title": "Infogan: Interpretable representation learning by information maximizing generative adversarial nets",
      "venue": "Advances in Neural Information Processing Systems, pp. 2172\u20132180.",
      "year": 2016
    },
    {
      "authors": [
        "A. Chouldechova",
        "D. Benavides-Prado",
        "O. Fialko",
        "R. Vaithianathan"
      ],
      "title": "A case study of algorithm-assisted decision making in child maltreatment hotline screening decisions",
      "venue": "S. A. Friedler and C. Wilson (eds.), Proceedings of the 1st Conference on Fairness, Accountability and Transparency, New York, NY, USA: PMLR, Proceedings of Machine Learning Research, vol. 81, pp. 134\u2013148.",
      "year": 2018
    },
    {
      "authors": [
        "M. Craven",
        "J.W. Shavlik"
      ],
      "title": "Extracting tree-structured representations of trained networks",
      "venue": "Advances in neural information processing systems, pp. 24\u201330.",
      "year": 1996
    },
    {
      "authors": [
        "W.M. Czarnecki",
        "S. Osindero",
        "M. Jaderberg",
        "G. Swirszcz",
        "R. Pascanu"
      ],
      "title": "Sobolev training for neural networks",
      "venue": "Advances in Neural Information Processing Systems, pp. 4281\u20134290.",
      "year": 2017
    },
    {
      "authors": [
        "J. Donahue",
        "K. Grauman"
      ],
      "title": "Annotator rationales for visual recognition",
      "venue": "2011 International Conference on Computer Vision, IEEE, pp. 1395\u20131402.",
      "year": 2011
    },
    {
      "authors": [
        "F. Doshi-Velez",
        "B. Kim"
      ],
      "title": "Towards a rigorous science of interpretable machine learning",
      "venue": "arXiv preprint arXiv:1702.08608.",
      "year": 2017
    },
    {
      "authors": [
        "H.L. Dreyfus",
        "S.E. Dreyfus"
      ],
      "title": "What artificial experts can and cannot do",
      "venue": "AI & society, 6 (1), 18\u201326.",
      "year": 1992
    },
    {
      "authors": [
        "H. Drucker",
        "Y. Le Cun"
      ],
      "title": "Improving generalization performance using double backpropagation",
      "venue": "IEEE Transactions on Neural Networks, 3 (6), 991\u2013997.",
      "year": 1992
    },
    {
      "authors": [
        "B. Esmaeili",
        "H. Wu",
        "S. Jain",
        "S. Narayanaswamy",
        "B. Paige",
        "van de Meent",
        "J.-W."
      ],
      "title": "Hierarchical disentangled representations",
      "venue": "arXiv preprint arXiv:1804.02086.",
      "year": 2018
    },
    {
      "authors": [
        "R. Fong",
        "A. Vedaldi"
      ],
      "title": "Interpretable explanations of black boxes by meaningful perturbation",
      "venue": "arXiv preprint arXiv:1704.03296.",
      "year": 2017
    },
    {
      "authors": [
        "T. Gerstenberg",
        "J.B. Tenenbaum"
      ],
      "title": "Intuitive theories",
      "venue": "Oxford handbook of causal reasoning, pp. 515\u2013548.",
      "year": 2017
    },
    {
      "authors": [
        "A. Ghorbani",
        "A. Abid",
        "J. Zou"
      ],
      "title": "Interpretation of neural networks is fragile",
      "venue": "arXiv preprint arXiv:1710.10547.",
      "year": 2017
    },
    {
      "authors": [
        "I.J. Goodfellow",
        "J. Shlens",
        "C. Szegedy"
      ],
      "title": "Explaining and harnessing adversarial examples",
      "venue": "arXiv preprint arXiv:1412.6572.",
      "year": 2014
    },
    {
      "authors": [
        "M. 61 Grundland",
        "N.A. Dodgson"
      ],
      "title": "Decolorize: Fast, contrast enhancing, color to grayscale conversion",
      "venue": "Pattern Recognition,",
      "year": 2007
    },
    {
      "authors": [
        "S. Gu",
        "L. Rigazio"
      ],
      "title": "Towards deep neural network architectures robust to adversarial examples",
      "venue": "arXiv preprint arXiv:1412.5068.",
      "year": 2014
    },
    {
      "authors": [
        "Y. Hechtlinger"
      ],
      "title": "Interpretation of prediction models using the input gradient",
      "venue": "arXiv preprint arXiv:1611.07634.",
      "year": 2016
    },
    {
      "authors": [
        "M. Hein",
        "M. Andriushchenko"
      ],
      "title": "Formal guarantees on the robustness of a classifier against adversarial manipulation",
      "venue": "Advances in Neural Information Processing Systems, pp. 2263\u20132273.",
      "year": 2017
    },
    {
      "authors": [
        "I. Higgins",
        "L. Matthey",
        "A. Pal",
        "C. Burgess",
        "X. Glorot",
        "M. Botvinick",
        "S. Mohamed",
        "A. Lerchner"
      ],
      "title": "beta-vae: Learning basic visual concepts with a constrained variational framework",
      "year": 2016
    },
    {
      "authors": [
        "Kang",
        "H.-W.",
        "Kang",
        "H.-B."
      ],
      "title": "Prediction of crime occurrence from multi-modal data using deep learning",
      "venue": "PloS one, 12 (4), e0176244.",
      "year": 2017
    },
    {
      "authors": [
        "F.C. Keil"
      ],
      "title": "Explanation and understanding",
      "venue": "Annu. Rev. Psychol., 57, 227\u2013254.",
      "year": 2006
    },
    {
      "authors": [
        "B. Kim",
        "J. Gilmer",
        "F. Viegas",
        "U. Erlingsson",
        "M. Wattenberg"
      ],
      "title": "Tcav: Relative concept importance testing with linear concept activation vectors",
      "venue": "arXiv preprint arXiv:1711.11279.",
      "year": 2017
    },
    {
      "authors": [
        "C. Rudin",
        "J.A. Shah"
      ],
      "title": "The bayesian case model: A generative approach for case-based reasoning and prototype classification",
      "venue": "In Advances in Neural Information Processing Systems,",
      "year": 2014
    },
    {
      "authors": [
        "Kindermans",
        "P.-J.",
        "S. Hooker",
        "J. Adebayo",
        "M. Alber",
        "K.T. Sch\u00fctt",
        "S. D\u00e4hne",
        "D. Erhan",
        "B. Kim"
      ],
      "title": "The (un) reliability of saliency methods",
      "venue": "arXiv preprint arXiv:1711.00867.",
      "year": 2017
    },
    {
      "authors": [
        "D. Kingma",
        "J. Ba"
      ],
      "title": "Adam: A method for stochastic optimization",
      "venue": "arXiv preprint arXiv:1412.6980.",
      "year": 2014
    },
    {
      "authors": [
        "P.W. Koh",
        "P. Liang"
      ],
      "title": "Understanding black-box predictions via influence functions",
      "venue": "International Conference on Machine Learning, pp. 1885\u20131894.",
      "year": 2017
    },
    {
      "authors": [
        "A. Kurakin",
        "I. Goodfellow",
        "S. Bengio"
      ],
      "title": "Adversarial examples in the physical world",
      "venue": "arXiv preprint arXiv:1607.02533.",
      "year": 2016
    },
    {
      "authors": [
        "Y. LeCun",
        "C. Cortes",
        "C.J. Burges"
      ],
      "title": "The MNIST database of handwritten digits",
      "venue": "http://yann.lecun.com/exdb/mnist/.",
      "year": 2010
    },
    {
      "authors": [
        "T. Lei",
        "R. Barzilay",
        "T. Jaakkola"
      ],
      "title": "Rationalizing neural predictions",
      "venue": "arXiv preprint arXiv:1606.04155.",
      "year": 2016
    },
    {
      "authors": [
        "J. Li",
        "X. Chen",
        "E. Hovy",
        "D. Jurafsky"
      ],
      "title": "Visualizing and understanding neural models in NLP",
      "venue": "arXiv preprint arXiv:1506.01066.",
      "year": 2015
    },
    {
      "authors": [
        "W. Monroe",
        "D. Jurafsky"
      ],
      "title": "Understanding neural networks through representation erasure",
      "venue": "arXiv preprint arXiv:1612.08220",
      "year": 2016
    },
    {
      "authors": [
        "O. Li",
        "H. Liu",
        "C. Chen",
        "C. Rudin"
      ],
      "title": "Deep learning for case-based reasoning through prototypes: A neural network that explains its predictions",
      "venue": "arXiv preprint arXiv:1710.04806.",
      "year": 2017
    },
    {
      "authors": [
        "M. Lichman"
      ],
      "title": "UCI machine learning repository",
      "venue": "http://archive.ics.uci.edu/ ml.",
      "year": 2013
    },
    {
      "authors": [
        "Z.C. Lipton"
      ],
      "title": "The mythos of model interpretability",
      "venue": "CoRR, abs/1606.03490.",
      "year": 2016
    },
    {
      "authors": [
        "S. Lundberg",
        "Lee",
        "S.-I."
      ],
      "title": "An unexpected unity among methods for interpreting model predictions",
      "venue": "arXiv preprint arXiv:1611.07478.",
      "year": 2016
    },
    {
      "authors": [
        "D. Mclaurin",
        "D. Duvenaud",
        "M. Johnson"
      ],
      "title": "Autograd",
      "venue": "https://github.com/ HIPS/autograd.",
      "year": 2017
    },
    {
      "authors": [
        "G. Montavon",
        "S. Lapuschkin",
        "A. Binder",
        "W. Samek",
        "M\u00fcller",
        "K.-R."
      ],
      "title": "Explaining nonlinear classification decisions with deep taylor decomposition",
      "venue": "Pattern Recognition, 65, 211\u2013222.",
      "year": 2017
    },
    {
      "authors": [
        "A. Nayebi",
        "S. Ganguli"
      ],
      "title": "Biologically inspired protection of deep networks from adversarial attacks",
      "venue": "arXiv preprint arXiv:1703.09202.",
      "year": 2017
    },
    {
      "authors": [
        "Y. Netzer",
        "T. Wang",
        "A. Coates",
        "A. Bissacco",
        "B. Wu",
        "A.Y. Ng"
      ],
      "title": "Reading digits in natural images with unsupervised feature learning",
      "venue": "NIPS Workshop on Deep Learning and Unsupervised Feature Learning 2011.",
      "year": 2011
    },
    {
      "authors": [
        "C. Olah",
        "A. Mordvintsev",
        "L. Schubert"
      ],
      "title": "Feature visualization",
      "venue": "Distill, https://distill.pub/2017/feature-visualization.",
      "year": 2017
    },
    {
      "authors": [
        "A. Satyanarayan",
        "I. Johnson",
        "S. Carter",
        "L. Schubert",
        "K. Ye",
        "A. Mordvintsev"
      ],
      "title": "The building blocks of interpretability. Distill, https://distill.pub/2018/buildingblocks",
      "year": 2018
    },
    {
      "authors": [
        "N. Papernot",
        "I. Goodfellow",
        "R. Sheatsley",
        "R. Feinman",
        "P. McDaniel"
      ],
      "title": "2016a). cleverhans v1.0.0: an adversarial machine learning library. arXiv preprint arXiv:1610.00768",
      "year": 2016
    },
    {
      "authors": [
        "P. McDaniel"
      ],
      "title": "Deep k-nearest neighbors: Towards confident, interpretable and robust deep learning",
      "venue": "arXiv preprint arXiv:1803.04765",
      "year": 2018
    },
    {
      "authors": [
        "I. Goodfellow",
        "S. Jha",
        "Z.B. Celik",
        "A. Swami"
      ],
      "title": "Practical black-box attacks against machine learning",
      "venue": "In Proceedings of the 2017 ACM on Asia Conference on Computer and Communications Security,",
      "year": 2017
    },
    {
      "authors": [
        "S. Jha",
        "M. Fredrikson",
        "Z.B. Celik",
        "A. Swami"
      ],
      "title": "The limitations of deep learning in adversarial settings",
      "venue": "In Security and Privacy (EuroS&P),",
      "year": 2016
    },
    {
      "authors": [
        "X. Wu",
        "S. Jha",
        "A. Swami"
      ],
      "title": "Distillation as a defense to adversarial perturbations against deep neural networks",
      "venue": "In Security and Privacy (SP),",
      "year": 2016
    },
    {
      "authors": [
        "J. Pearl"
      ],
      "title": "Causal inference",
      "venue": "Causality: Objectives and Assessment, pp. 39\u201358.",
      "year": 2010
    },
    {
      "authors": [
        "A. Raghunathan",
        "J. Steinhardt",
        "P. Liang"
      ],
      "title": "Certified defenses against adversarial examples",
      "venue": "arXiv preprint arXiv:1801.09344.",
      "year": 2018
    },
    {
      "authors": [
        "A. Ratner",
        "S.H. Bach",
        "H. Ehrenberg",
        "J. Fries",
        "S. Wu",
        "C. R\u00e9"
      ],
      "title": "Snorkel: Rapid training data creation with weak supervision",
      "venue": "arXiv preprint arXiv:1711.10160.",
      "year": 2017
    },
    {
      "authors": [
        "M.T. Ribeiro"
      ],
      "title": "LIME",
      "venue": "https://github.com/marcotcr/lime.",
      "year": 2016
    },
    {
      "authors": [
        "S. Singh",
        "C. Guestrin"
      ],
      "title": "Why should I trust you?: Explaining the predictions of any classifier",
      "venue": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
      "year": 2016
    },
    {
      "authors": [
        "S. Rifai",
        "G. Mesnil",
        "P. Vincent",
        "X. Muller",
        "Y. Bengio",
        "Y. Dauphin",
        "X. Glorot"
      ],
      "title": "Higher order contractive auto-encoder",
      "venue": "Joint European Conference on Machine Learning and Knowledge Discovery in Databases, Springer, pp. 645\u2013660.",
      "year": 2011
    },
    {
      "authors": [
        "A. Ross",
        "F. Doshi-Velez"
      ],
      "title": "Improving the adversarial robustness and interpretability of deep neural networks by regularizing their input gradients",
      "venue": "Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence.",
      "year": 2018
    },
    {
      "authors": [
        "I. Lage",
        "F. Doshi-Velez"
      ],
      "title": "The neural lasso: Local linear sparsity for interpretable explanations",
      "venue": "In Workshop on Transparent and Interpretable Machine Learning in Safety Critical Environments,",
      "year": 2017
    },
    {
      "authors": [
        "A.S. Ross",
        "M.C. Hughes",
        "F. Doshi-Velez"
      ],
      "title": "Right for the right reasons: Training differentiable models by constraining their explanations",
      "venue": "Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, pp. 2662\u20132670.",
      "year": 2017
    },
    {
      "authors": [
        "R.M. Rustamov",
        "J.T. Klosowski"
      ],
      "title": "Interpretable graph-based semi-supervised learning via flows",
      "venue": "arXiv preprint arXiv:1709.04764.",
      "year": 2017
    },
    {
      "authors": [
        "E. Schulz",
        "J.B. Tenenbaum",
        "D. Duvenaud",
        "M. Speekenbrink",
        "S.J. Gershman"
      ],
      "title": "Compositional inductive biases in function learning",
      "venue": "Cognitive psychology, 99, 44\u201379.",
      "year": 2017
    },
    {
      "authors": [
        "R.R. Selvaraju",
        "A. Das",
        "R. Vedantam",
        "M. Cogswell",
        "D. Parikh",
        "D. Batra"
      ],
      "title": "Grad-CAM: Why did you say that? arXiv preprint arXiv:1611.07450",
      "year": 2016
    },
    {
      "authors": [
        "P. Sermanet",
        "S. Chintala",
        "Y. LeCun"
      ],
      "title": "Convolutional neural networks applied to house numbers digit classification",
      "venue": "Pattern Recognition (ICPR), 2012 21st International Conference on, IEEE, pp. 3288\u20133291.",
      "year": 2012
    },
    {
      "authors": [
        "A. Shrikumar",
        "P. Greenside",
        "A. Shcherbina",
        "A. Kundaje"
      ],
      "title": "Not just a black box: Learning important features through propagating activation differences",
      "venue": "arXiv preprint arXiv:1605.01713.",
      "year": 2016
    },
    {
      "authors": [
        "N. Siddharth",
        "B. Paige",
        "Van de Meent",
        "J.-W",
        "A. Desmaison",
        "P.H. Torr"
      ],
      "title": "Learning disentangled representations with semi-supervised deep generative models",
      "year": 2017
    },
    {
      "authors": [
        "K. Simonyan",
        "A. Vedaldi",
        "A. Zisserman"
      ],
      "title": "Deep inside convolutional networks: Visualising image classification models and saliency maps",
      "venue": "arXiv preprint arXiv:1312.6034.",
      "year": 2013
    },
    {
      "authors": [
        "S. Singh",
        "M.T. Ribeiro",
        "C. Guestrin"
      ],
      "title": "Programs as black-box explanations",
      "venue": "arXiv preprint arXiv:1611.07579.",
      "year": 2016
    },
    {
      "authors": [
        "D. Smilkov",
        "N. Thorat",
        "B. Kim",
        "F. Vi\u00e9gas",
        "M. Wattenberg"
      ],
      "title": "Smoothgrad: removing noise by adding noise",
      "venue": "arXiv preprint arXiv:1706.03825.",
      "year": 2017
    },
    {
      "authors": [
        "M. Sundararajan",
        "A. Taly",
        "Q. Yan"
      ],
      "title": "Axiomatic attribution for deep networks",
      "venue": "arXiv preprint arXiv:1703.01365.",
      "year": 2017
    },
    {
      "authors": [
        "C. Szegedy",
        "W. Zaremba",
        "I. Sutskever",
        "J. Bruna",
        "D. Erhan",
        "I. Goodfellow",
        "R. Fergus"
      ],
      "title": "Intriguing properties of neural networks",
      "venue": "arXiv preprint arXiv:1312.6199.",
      "year": 2013
    },
    {
      "authors": [
        "F. Tram\u00e8r",
        "A. Kurakin",
        "N. Papernot",
        "D. Boneh",
        "P. McDaniel"
      ],
      "title": "Ensemble adversarial training: Attacks and defenses",
      "venue": "arXiv preprint arXiv:1705.07204.",
      "year": 2017
    },
    {
      "authors": [
        "N. Papernot",
        "I. Goodfellow",
        "D. Boneh",
        "P. McDaniel"
      ],
      "title": "The space of transferable adversarial examples. arXiv preprint arXiv:1704.03453",
      "year": 2017
    },
    {
      "authors": [
        "M. Wu",
        "M.C. Hughes",
        "S. Parbhoo",
        "M. Zazzi",
        "V. Roth",
        "F. Doshi-Velez"
      ],
      "title": "Beyond sparsity: Tree regularization of deep models for interpretability",
      "venue": "arXiv preprint arXiv:1711.06178.",
      "year": 2017
    },
    {
      "authors": [
        "W. Xu",
        "D. Evans",
        "Y. Qi"
      ],
      "title": "Feature squeezing: Detecting adversarial examples in deep neural networks",
      "venue": "arXiv preprint arXiv:1704.01155.",
      "year": 2017
    },
    {
      "authors": [
        "L.A. Zadeh"
      ],
      "title": "Toward a theory of fuzzy information granulation and its centrality in human reasoning and fuzzy logic",
      "venue": "Fuzzy sets and systems, 90 (2), 111\u2013127.",
      "year": 1997
    },
    {
      "authors": [
        "O. Zaidan",
        "J. Eisner",
        "C.D. Piatko"
      ],
      "title": "Using \"annotator rationales\" to improve machine learning for text categorization",
      "venue": "HLT-NAACL, Citeseer, pp. 260\u2013267.",
      "year": 2007
    },
    {
      "authors": [
        "M.D. Zeiler",
        "R. Fergus"
      ],
      "title": "Visualizing and understanding convolutional networks",
      "venue": "European conference on computer vision, Springer, pp. 818\u2013833.",
      "year": 2014
    },
    {
      "authors": [
        "Y. Zhang",
        "I. Marshall",
        "B.C. Wallace"
      ],
      "title": "Rationale-augmented convolutional neural networks for text classification",
      "venue": "arXiv preprint arXiv:1605.04469.",
      "year": 2016
    }
  ],
  "sections": [
    {
      "heading": "Training Machine Learning Models by Regularizing",
      "text": "their Explanations\nA thesis presented\nby\nAndrew Slavin Ross\nto\nThe Institute for Applied Computational Science\nin partial fulfillment of the requirements\nfor the degree of\nMaster of Engineering\nin the subject of\nComputational Science and Engineering\nHarvard University\nCambridge, Massachusetts\nMay 2018\nar X\niv :1\n81 0.\n00 86\n9v 1\n[ cs\n.L G\n] 2\n9 Se\np 20\n18\nc\u00a9 2018 Andrew Slavin Ross\nAll rights reserved.\nThesis Advisor: Finale Doshi-Velez\nAuthor: Andrew Slavin Ross"
    },
    {
      "heading": "Training Machine Learning Models by Regularizing their Explanations",
      "text": ""
    },
    {
      "heading": "Abstract",
      "text": "Neural networks are among the most accurate supervised learning methods in use today. However, their opacity makes them difficult to trust in critical applications, especially when conditions in training may differ from those in practice. Recent efforts to develop explanations for neural networks and machine learning models more generally have produced tools to shed light on the implicit rules behind predictions. These tools can help us identify when models are right for the wrong reasons. However, they do not always scale to explaining predictions for entire datasets, are not always at the right level of abstraction, and most importantly cannot correct the problems they reveal. In this thesis, we explore the possibility of training machine learning models (with a particular focus on neural networks) using explanations themselves. We consider approaches where models are penalized not only for making incorrect predictions but also for providing explanations that are either inconsistent with domain knowledge or overly complex. These methods let us train models which can not only provide more interpretable rationales for their predictions but also generalize better when training data is confounded or meaningfully different from test data (even adversarially so).\niii"
    },
    {
      "heading": "Contents",
      "text": "Abstract . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . iii\nIntroduction 1"
    },
    {
      "heading": "1 Introduction 2",
      "text": "1.1 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4"
    },
    {
      "heading": "2 Right for the Right Reasons 6",
      "text": "2.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n2.1.1 Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7 2.1.2 Background: Input Gradient Explanations . . . . . . . . . . . . . . . . 9\n2.2 Our Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 2.2.1 Loss Functions that Constrain Explanations . . . . . . . . . . . . . . . 10 2.2.2 Find-Another-Explanation: Discovering Many Possible Rules without Annotations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 2.3 Empirical Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n2.3.1 Toy Color Dataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 2.3.2 Real-world Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 2.3.3 Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n2.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 Appendix 2.A Cross-Validation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 Appendix 2.B Learning with Less Data . . . . . . . . . . . . . . . . . . . . . . . . 24 Appendix 2.C Simultaneous Find-Another-Explanation . . . . . . . . . . . . . . . 25"
    },
    {
      "heading": "3 Interpretability and Robustness 26",
      "text": "3.1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 3.2 Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n3.2.1 Attacks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29 3.2.2 Defenses . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30\n3.3 Gradient Regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 3.4 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33\niv\n3.4.1 Accuracy Evaluations (FGSM and TGSM) . . . . . . . . . . . . . . . . 35 3.4.2 Human Subject Study (JSMA and Iterated TGSM) . . . . . . . . . . . . 38 3.4.3 Connections to Interpretability . . . . . . . . . . . . . . . . . . . . . . . 42\n3.5 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43"
    },
    {
      "heading": "4 General Explanation Regularization 46",
      "text": "4.1 Alternative Input Gradient Penalties . . . . . . . . . . . . . . . . . . . . . . . . 46\n4.1.1 L1 Regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 4.1.2 Higher-Order Derivatives . . . . . . . . . . . . . . . . . . . . . . . . . . 48\n4.2 Heftier Surrogates . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48 4.3 Examples and Exemplars . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50 4.4 Emergent Abstractions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51 4.5 Interpretability Interfaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54 4.6 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56"
    },
    {
      "heading": "5 Conclusion 58",
      "text": "References 60\n1\nChapter 1"
    },
    {
      "heading": "Introduction",
      "text": "The motivation for this thesis is easiest to express with a story:\nA father decides to teach his young son what a sports car is. Finding it difficult to explain in words, he decides to give some examples. They stand on a motorway bridge and as each car passes underneath, the father cries out \u201cthat\u2019s a sports car!\u201d when a sports car passes by. After ten minutes, the father asks his son if he\u2019s understood what a sports car is. The son says, \u201csure, it\u2019s easy\u201d. An old red VW Beetle passes by, and the son shouts \u2013 \u201cthat\u2019s a sports car!\u201d. Dejected, the father asks \u2013 \u201cwhy do you say that?\u201d. \u201cBecause all sports cars are red!\u201d, replies the son. (Barber, 2012)\nThere is another popular version that pokes fun at the Department of Defense:\nIn the early days of the perceptron the army decided to train an artificial neural network to recognize tanks partly hidden behind trees in the woods. They took a number of pictures of a woods without tanks, and then pictures of the same woods with tanks clearly sticking out from behind trees. They then trained a net to discriminate the two classes of pictures. The results were impressive, and the army was even more impressed when it turned out that the net could generalize its knowledge to pictures from each set that had not been used in training the net. Just to make sure that the net had indeed learned to recognize partially hidden tanks, however, the researchers took some more pictures in the same woods and showed them to the trained net. They were shocked and depressed to find that with the new pictures the net totally failed to discriminate between pictures of trees with partially concealed tanks behind them and just plain trees. The mystery was finally solved when someone noticed that the training pictures of the woods without tanks were taken on a cloudy day, whereas those with tanks\n2\nwere taken on a sunny day. The net had learned to recognize and generalize the difference between a woods with and without shadows! (Dreyfus and Dreyfus, 1992)\nThe first story is a parable and the second is apocryphal1, but both illustrate an inherent limitation in learning by example, which is how we currently train machine learning systems: we only provide them with inputs (questions, X) and outputs (answers, y). When we train people to perform tasks, however, we usually provide them with explanations, since without them many problems are ambiguous. In machine learning, model developers usually circumvent such ambiguities via regularization, inductive biases (e.g. using CNNs when you need translational invariance), or simply acquiring vast quantities of data (such that the problem eventually becomes unambiguous, as if the child had seen every car in the world). But there is still a risk our models will be right for the wrong reasons \u2013 which means that if conditions change, they will simply be wrong.\nAs we begin to use ML in sensitive domains such as healthcare, this risk has highlighted\nthe need for interpretable models, as this final story illustrates:\nAlthough models based on rules were not as accurate as the neural net models, they were intelligible, i.e., interpretable by humans. On one of the pneumonia datasets, the rule-based system learned the rule \u201cHasAsthma(x) \u2192 LowerRisk(x)\u201d, i.e., that patients with pneumonia who have a history of asthma have lower risk of dying from pneumonia than the general population. Needless to say, this rule is counterintuitive. But it reflected a true pattern in the training data: patients with a history of asthma who presented with pneumonia usually were admitted not only to the hospital but directly to the ICU (Intensive Care Unit). The good news is that the aggressive care received by asthmatic pneumonia patients was so effective that it lowered their risk of dying from pneumonia compared to the general population. The bad news is that because the prognosis for these patients is better than average, models trained on the data incorrectly learn that asthma lowers risk, when in fact asthmatics have much higher risk (if not hospitalized). (Caruana et al., 2015)\nIn this case, a pneumonia risk prediction model learned an unhelpful rule because its training outcomes didn\u2019t actually represent medical risk. Had the model been put into\n1https://www.gwern.net/Tanks\n3\nproduction, it would have endangered lives. The fact that they used an interpretable model let them realize and avoid this danger (by not using the neural network at all). But clearly, the dataset they used still contains information that, say, a human analyst could use to draw useful conclusions about how to treat pneumonia. How can machine learning models utilize it despite its flaws?\nThis thesis seeks to provide both concrete methods for addressing these types of problems in specific cases and more abstract arguments about how they should be solved in general. The main strategy we will consider is explanation regularization, which means jointly optimizing a machine learning model to make correct predictions and to explain those predictions well. Quantifying the quality of an explanation may seem difficult (especially if we would like it to be differentiable), but we will delve into cases where it is straightforward and intuitive, as well as strategies for making it so."
    },
    {
      "heading": "1.1 Contributions",
      "text": "The major contributions of this thesis are as follows:\n\u2022 It presents a framework for encoding domain knowledge about a classification problem\nas local penalties on the gradient of the model\u2019s decision surface, which can be incorporated into the loss function of any differentiable model (e.g. a neural network). Applying this framework in both supervised and unsupervised formulations, it trains models that generalize to test data from different distributions, which would otherwise be unobtainable by traditional optimization methods. (Chapter 2)\n\u2022 It applies a special case of this framework (where explanations are regularized to be\nsimple) to the problem of defending against adversarial examples. It demonstrates increased robustness of regularized models to white- and black-box attacks, at a level comparable or better than adversarial training. It also demonstrates both increased transferability and interpretability of adversarial examples created to fool regularized models, which we evaluate in a human subject experiment. (Chapter 3)\n4\n\u2022 It considers cases where we can meaningfully change what models learn by regulariz-\ning more general types of explanations. We review literature and suggest directions for explanation regularization, using sparse gradients, input Hessians, decision trees, nearest neighbors, and even abstract concepts that emerge or that we encourage to emerge in deep neural networks. It concludes by outlining an interface for interpretable machine teaching. (Chapter 4)\n5\nChapter 2\nRight for the Right Reasons1"
    },
    {
      "heading": "2.1 Introduction",
      "text": "High-dimensional real-world datasets are often full of ambiguities. When we train classifiers on such data, it is frequently possible to achieve high accuracy using classifiers with qualitatively different decision boundaries. To narrow down our choices and encourage robustness, we usually employ regularization techniques (e.g. encouraging sparsity or small parameter values). We also structure our models to ensure domain-specific invariances (e.g. using convolutional neural nets when we would like the model to be invariant to spatial transformations). However, these solutions do not address situations in which our training dataset contains subtle confounds or differs qualitatively from our test dataset. In these cases, our model may fail to generalize no matter how well it is tuned.\nSuch generalization gaps are of particular concern for uninterpretable models such as neural networks, especially in sensitive domains. For example, Caruana et al. (2015) describe a model intended to prioritize care for patients with pneumonia. The model was trained to predict hospital readmission risk using a dataset containing attributes of patients hospitalized at least once for pneumonia. Counterintuitively, the model learned that the\n1Significant portions of this chapter also appear in Ross, A. S., Hughes, M. C. and Doshi-Velez, F. (2017b). Right for the right reasons: Training differentiable models by constraining their explanations. In Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, pp. 2662\u20132670.\n6\npresence of asthma was a negative predictor of readmission, when in reality pneumonia patients with asthma are at a greater medical risk. This model would have presented a grave safety risk if used in production. This problem occurred because the outcomes in the dataset reflected not just the severity of patients\u2019 diseases but the quality of care they initially received, which was higher for patients with asthma.\nThis case and others like it have motivated recent work in interpretable machine learning, where algorithms provide explanations for domain experts to inspect for correctness before trusting model predictions. However, there has been limited work in optimizing models to find not just the right prediction but also the right explanation. Toward this end, this work makes the following contributions:\n\u2022 We confirm empirically on several datasets that input gradient explanations match\nstate of the art sample-based explanations (e.g. LIME, Ribeiro (2016)).\n\u2022 Given annotations about incorrect explanations for particular inputs, we efficiently\noptimize the classifier to learn alternate explanations (to be right for better reasons).\n\u2022 When annotations are not available, we sequentially discover classifiers with similar\naccuracies but qualitatively different decision boundaries for domain experts to inspect for validity."
    },
    {
      "heading": "2.1.1 Related Work",
      "text": "We first define several important terms in interpretable machine learning. All classifiers have implicit decision rules for converting an input into a decision, though these rules may be opaque. A model is interpretable if it provides explanations for its predictions in a form humans can understand; an explanation provides reliable information about the model\u2019s implicit decision rules for a given prediction. In contrast, we say a machine learning model is accurate if most of its predictions are correct, but only right for the right reasons if the implicit rules it has learned generalize well and conform to domain experts\u2019 knowledge about the problem.\n7\nExplanations can take many forms (Keil, 2006) and evaluating the quality of explanations or the interpretability of a model is difficult (Lipton, 2016; Doshi-Velez and Kim, 2017). However, within the machine learning community recently there has been convergence (Lundberg and Lee, 2016) around local counterfactual explanations, where we show how perturbing an input x in various ways will affect the model\u2019s prediction y\u0302. This approach to explanations can be domain- and model-specific (e.g. \u201cannotator rationales\u201d used to explain text classifications by Li et al. (2016); Lei et al. (2016); Zhang et al. (2016)). Alternatively, explanations can be model-agnostic and relatively domain-general, as exemplified by LIME (Local Interpretable Model-agnostic Explanations, Ribeiro et al. (2016); Singh et al. (2016)) which trains and presents local sparse models of how predictions change when inputs are perturbed.\nThe per-example perturbing and fitting process used in models such as LIME can be computationally prohibitive, especially if we seek to explain an entire dataset during each training iteration. If the underlying model is differentiable, one alternative is to use input gradients as local explanations (Baehrens et al. (2010) provides a particularly good introduction; see also Selvaraju et al. (2016); Simonyan et al. (2013); Li et al. (2015); Hechtlinger (2016)). The idea is simple: the gradients of the model\u2019s output probabilities with respect to its inputs literally describe the model\u2019s decision boundary (see Figure 2.1). They are similar in spirit to the local linear explanations of LIME but much faster to compute.\nInput gradient explanations are not perfect for all use-cases\u2014for points far from the decision boundary, they can be uniformatively small and do not always capture the idea of salience (see discussion and alternatives proposed by Shrikumar et al. (2016); Bach et al. (2015); Montavon et al. (2017); Sundararajan et al. (2017); Fong and Vedaldi (2017)). However, they are exactly what is required for constraining the decision boundary. In the past, Drucker and Le Cun (1992) showed that applying penalties to input gradient magnitudes can improve generalization; to our knowledge, our application of input gradients to constrain explanations and find alternate explanations is novel.\nMore broadly, none of the works above on interpretable machine learning attempt to\n8\noptimize explanations for correctness. For SVMs and specific text classification architectures, there exists work on incorporating human input into decision boundaries in the form of annotator rationales (Zaidan et al., 2007; Donahue and Grauman, 2011; Zhang et al., 2016). Unlike our approach, these works are either tailored to specific domains or do not fully close the loop between generating explanations and constraining them."
    },
    {
      "heading": "2.1.2 Background: Input Gradient Explanations",
      "text": "Consider a differentiable model f parametrized by \u03b8 with inputs X \u2208 RN\u00d7D and probability vector outputs f (X|\u03b8) = y\u0302 \u2208 RN\u00d7K corresponding to one-hot labels y \u2208 RN\u00d7K. Its input gradient is given by fX(Xn|\u03b8) or \u2207X y\u0302n, which is a vector normal to the model\u2019s decision boundary at Xn and thus serves as a first-order description of the model\u2019s behavior near Xn. The gradient has the same shape as each vector Xn; large-magnitude values of the input gradient indicate elements of Xn that would affect y\u0302 if changed. We can visualize explanations by highlighting portions of Xn in locations with high input gradient magnitudes.\n9"
    },
    {
      "heading": "2.2 Our Approach",
      "text": "We wish to develop a method to train models that are right for the right reasons. If explanations faithfully describe a model\u2019s underlying behavior, then constraining its explanations to match domain knowledge should cause its underlying behavior to more closely match that knowledge too. We first describe how input gradient-based explanations lend themselves to efficient optimization for correct explanations in the presence of domain knowledge, and then describe how they can be used to efficiently search for qualitatively different decision boundaries when such knowledge is not available."
    },
    {
      "heading": "2.2.1 Loss Functions that Constrain Explanations",
      "text": "When constraining input gradient explanations, there are two basic options: we can either constrain them to be large in relevant areas or small in irrelevant areas. However, because input gradients for relevant inputs in many models should be small far from the decision boundary, and because we do not know in advance how large they should be, we opt to shrink irrelevant gradients instead.\nFormally, we define an annotation matrix A \u2208 {0, 1}N\u00d7D, which are binary masks indicating whether dimension d should be irrelevant for predicting observation n. We would like \u2207X y\u0302 to be near 0 at these locations. To that end, we optimize a loss function L(\u03b8, X, y, A) of the form\nL(\u03b8, X, y, A) = N\n\u2211 n=1\nK\n\u2211 k=1 \u2212ynk log(y\u0302nk)\ufe38 \ufe37\ufe37 \ufe38\nRight answers\n+ \u03bb1 N\n\u2211 n=1\nD\n\u2211 d=1\n( And \u2202\n\u2202xnd\nK\n\u2211 k=1 log(y\u0302nk) )2 \ufe38 \ufe37\ufe37 \ufe38\nRight reasons\n+ \u03bb2 \u2211 i \u03b82i\ufe38 \ufe37\ufe37 \ufe38 Regular ,\nwhich contains familiar cross entropy and \u03b8 regularization terms along with a new regularization term that discourages the input gradient from being large in regions marked by A. This term has a regularization parameter \u03bb1 which should be set such that the \u201cright answers\u201d and \u201cright reasons\u201d terms have similar orders of magnitude; see Appendix 2.A for more details. Note that this loss penalizes the gradient of the log probability, which performed best in practice, though in many visualizations we show fX, which is the gradient\n10\nof the predicted probability itself. Summing across classes led to slightly more stable results than using the predicted class log probability max log(y\u0302k), perhaps due to discontinuities near the decision boundary (though both methods were comparable). We did not explore regularizing input gradients of specific class probabilities, though this would be a natural extension.\nBecause this loss function is differentiable with respect to \u03b8, we can easily optimize it with gradient-based optimization methods. We do not need annotations (nonzero An) for every input in X, and in the case A = 0N\u00d7D, the explanation term has no effect on the loss. At the other extreme, when A is a matrix of all 1s, it encourages the model to have small gradients with respect to its inputs; this can improve generalization on its own (Drucker and Le Cun, 1992). Between those extremes, it biases our model against particular implicit rules.\nThis penalization approach enjoys several desirable properties. Alternatives that specify a single Ad for all examples presuppose a coherent notion of global feature importance, but when decision boundaries are nonlinear many features are only relevant in the context of specific examples. Alternatives that simulate perturbations to entries known to be irrelevant (or to determine relevance as in Ribeiro et al. (2016)) require defining domain-specific perturbation logic; our approach does not. Alternatives that apply hard constraints or completely remove elements identified by And miss the fact that the entries in A may be imprecise even if they are human-provided. Thus, we opt to preserve potentially misleading features but softly penalize their use."
    },
    {
      "heading": "2.2.2 Find-Another-Explanation: Discovering Many Possible Rules without An-",
      "text": "notations\nAlthough we can obtain the annotations A via experts as in Zaidan et al. (2007), we may not always have this extra information or know the \u201cright reasons.\u201d In these cases, we propose an approach that iteratively adapts A to discover multiple models accurate for qualitatively different reasons; a domain expert could then examine them to determine which is the right for the best reasons. Specifically, we generate a \u201cspectrum\u201d of models with different decision\n11\nboundaries by iteratively training models, explaining X, then training the next model to differ from previous iterations:\nA0 = 0, \u03b80 = arg min \u03b8 L(\u03b8, X, y, A0), A1 = Mc [ fX|\u03b80] , \u03b81 = arg min \u03b8 L(\u03b8, X, y, A1), A2 = Mc [ fX|\u03b81] \u222a A1, \u03b82 = arg min \u03b8 L(\u03b8, X, y, A2),\n. . .\nwhere the function Mc returns a binary mask indicating which gradient components have a magnitude ratio (their magnitude divided by the largest component magnitude) of at least c and where we abbreviated the input gradients of the entire training set X at \u03b8i as\nfX|\u03b8i. In other words, we regularize input gradients where they were largest in magnitude previously. If, after repeated iterations, accuracy decreases or explanations stop changing (or only change after significantly increasing \u03bb1), then we may have spanned the space of possible models.2 All of the resulting models will be accurate, but for different reasons; although we do not know which reasons are best, we can present them to a domain expert for inspection and selection. We can also prioritize labeling or reviewing examples about which the ensemble disagrees. Finally, the size of the ensemble provides a rough measure of dataset redundancy."
    },
    {
      "heading": "2.3 Empirical Evaluation",
      "text": "We demonstrate explanation generation, explanation constraints, and the find-anotherexplanation method on a toy color dataset and three real-world datasets. In all cases, we used a multilayer perceptron with two hidden layers of size 50 and 30, ReLU nonlinearities with a softmax output, and a \u03bb2 = 0.0001 penalty on \u2016\u03b8\u201622. We trained the network using Adam (Kingma and Ba, 2014) with a batch size of 256 and Autograd (Mclaurin et al., 2017).\n2Though one can design simple pathological cases where we do not discover all models with this method; we explore an alternative version in Appendix 2.C that addresses some of these cases.\n12\nFor most experiments, we used an explanation L2 penalty of \u03bb1 = 1000, which gave our \u201cright answers\u201d and \u201cright reasons\u201d loss terms similar magnitudes. More details about cross-validation are included in Appendix 2.A. For the cutoff value c described in Section 2.2.2 and used for display, we often chose 0.67, which tended to preserve 2-5% of gradient components (the average number of qualifying elements tended to fall exponentially with c). Code for all experiments is available at https://github.com/dtak/rrr."
    },
    {
      "heading": "2.3.1 Toy Color Dataset",
      "text": "We created a toy dataset of 5\u00d7 5\u00d7 3 RGB images with four possible colors. Images fell into two classes with two independent decision rules a model could implicitly learn: whether their four corner pixels were all the same color, and whether their top-middle three pixels were all different colors. Images in class 1 satisfied both conditions and images in class 2 satisfied neither. Because only corner and top-row pixels are relevant, we expect any faithful explanation of an accurate model to highlight them.\nIn Figure 2.2, we see both LIME and input gradients identify the same relevant pixels, which suggests that (1) both methods are effective at explaining model predictions, and (2) the model has learned the corner rather than the top-middle rule, which it did consistently across random restarts.\nHowever, if we train our model with a nonzero A (specifically, setting And = 1 for\n13\ncorners d across examples n), we were able to cause it to use the other rule. Figure 2.3 shows how the model transitions between rules as we vary \u03bb1 and the number of examples penalized by A. This result demonstrates that the model can be made to learn multiple rules despite only one being commonly reached via standard gradient-based optimization methods. However, it depends on knowing a good setting for A, which in this case would still require annotating on the order of 103 examples, or 5% of our dataset (although always including examples with annotations in Adam minibatches let us consistently switch rules with only 50 examples, or 0.2% of the dataset).\nFinally, Figure 2.4 shows we can use the find-another-explanation technique from Sec. 2.2.2 to discover the other rule without being given A. Because only two rules lead to high accuracy on the test set, the model performs no better than random guessing when prevented from using either one (although we have to increase the penalty high enough that this accuracy number may be misleading - the essential point is that after the first iteration, explanations stop changing). Lastly, though not directly relevant to the discussion on interpretability and explanation, we demonstrate the potential of explanations to reduce the amount of data required for training in Appendix 2.B.\n14"
    },
    {
      "heading": "2.3.2 Real-world Datasets",
      "text": "To demonstrate real-world, cross-domain applicability, we test our approach on variants of three familiar machine learning text, image, and tabular datasets:\n\u2022 20 Newsgroups: As in Ribeiro et al. (2016), we test input gradients on the alt.atheism\nvs. soc.religion.christian subset of the 20 Newsgroups dataset Lichman (2013). We used the same two-hidden layer network architecture with a TF-IDF vectorizer with 5000 components, which gave us a 94% accurate model for A = 0.\n\u2022 Iris-Cancer: We concatenated all examples in classes 1 and 2 from the Iris dataset\nwith the the first 50 examples from each class in the Breast Cancer Wisconsin dataset (Lichman, 2013) to create a composite dataset X \u2208 R100\u00d734, y \u2208 {0, 1}. Despite the dataset\u2019s small size, our network still obtains an average test accuracy of 92% across 350 random 23 - 1 3 training-test splits. However, when we modify our test set to remove the 4 Iris components, average test accuracy falls to 81% with higher variance, suggesting the model learns to depend on Iris features and suffers without them. We verify that\n15\nour explanations reveal this dependency and that regularizing them avoids it.\n\u2022 Decoy MNIST: On the baseline MNST dataset (LeCun et al., 2010), our network obtains\n98% train and 96% test accuracy. However, in Decoy MNIST, images x have 4\u00d7 4 gray swatches in randomly chosen corners whose shades are functions of their digits y in training (in particular, 255\u2212 25y) but are random in test. On this dataset, our model has a higher 99.6% train accuracy but a much lower 55% test accuracy, indicating that the decoy rule misleads it. We verify that both gradient and LIME explanations let users detect this issue and that explanation regularization lets us overcome it.\nInput gradients are consistent with sample-based methods such as LIME, and faster. On 20 Newsgroups (Figure 2.5), input gradients are less sparse but identify all of the same words in the document with similar weights. Note that input gradients also identify words outside the document that would affect the prediction if added.\n16\nOn Decoy MNIST (Figure 2.6), both LIME and input gradients reveal that the model predicts 3 rather than 7 due to the color swatch in the corner. Because of their finegrained resolution, input gradients sometimes better capture counterfactual behavior, where extending or adding lines outside of the digit to either reinforce it or transform it into another digit would change the predicted probability (see also Figure 2.10). LIME, on the other hand, better captures the fact that the main portion of the digit is salient (because its super-pixel perturbations add and remove larger chunks of the digit).\nOn Iris-Cancer (Figure 2.7), input gradients actually outperform LIME. We know from the accuracy difference that Iris features are important to the model\u2019s prediction, but LIME only identifies a single important feature, which is from the Breast Cancer dataset (even when we vary its perturbation strategy). This example, which is tabular and contains\n17\ncontinuously valued rather categorical features, may represent a pathological case for LIME, which operates best when it can selectively mask a small number of meaningful chunks of its inputs to generate perturbed samples. For truly continuous inputs, it should not be surprising that explanations based on gradients perform best.\nThere are a few other advantages input gradients have over sample-based perturbation methods. On 20 Newsgroups, we noticed that for very long documents, explanations generated by the sample-based method LIME are often overly sparse, and there are many words identified as significant by input gradients that LIME ignores. This may be because the number of features LIME selects must be passed in as a parameter beforehand, and it may also be because LIME only samples a fixed number of times. For sufficiently long documents, it is unlikely that sample-based approaches will mask every word even once, meaning that the output becomes increasingly nondeterministic\u2014an undesirable quality for explanations. To resolve this issue, one could increase the number of samples, but that would increase the computational cost since the model must be evalutated at least once per sample to fit a local surrogate. Input gradients, on the other hand, only require on the order of one model evaluation total to generate an explanation of similar quality (generating gradients is similar in complexity to predicting probabilities), and furthermore, this complexity is based on the vector length, not the document length. This issue (underscored by Table 2.1) highlights some inherent scalability advantages input gradients enjoy over sample-based perturbation methods.\n18\nGiven annotations, input gradient regularization finds solutions consistent with domain knowledge. Another key advantage of using an explanation method more closely related to our model is that we can then incorporate explanations into our training process, which are most useful when the model faces ambiguities in how to classify inputs. We deliberately constructed the Decoy MNIST and Iris-Cancer datasets to have this kind of ambiguity, where a rule that works in training will not generalize to test. When we train our network on these confounded datasets, their test accuracy is better than random guessing, in part because the decoy rules are not simple and the primary rules not complex, but their performance is still significantly worse than on a baseline test set with no decoy rules. By penalizing\n19\nexplanations we know to be incorrect using the loss function defined in Section 2.2.1, we are able to recover that baseline test accuracy, which we demonstrate in Figures 2.8 and 2.9.\nWhen annotations are unavailable, our find-another-explanation method discovers diverse classifiers. As we saw with the Toy Color dataset, even if almost every row of A is 0, we can still benefit from explanation regularization (meaning practitioners can gradually incorporate these penalties into their existing models without much upfront investment). However, annotation is never free, and in some cases we either do not know the right explanation or cannot easily encode it. Additionally, we may be interested in exploring the structure of our model and dataset in a less supervised fashion. On real-world datasets, which are usually overdetermined, we can use find-another-explanation to discover \u03b8s in\n20\nshallower local minima that we would normally never explore. Given enough models right for different reasons, hopefully at least one is right for the right reasons.\nFigure 2.10 shows find-another-explanation results for our three real-world datasets, with example explanations at each iteration above and model train and test accuracy below. For Iris-Cancer, we find that the initial iteration of the model heavily relies on the Iris features and has high train but low test accuracy, while subsequent iterations have lower train but higher test accuracy (with smaller gradients in Iris components). In other words, we spontaneously obtain a more generalizable model without a predefined A alerting us that the first four features are misleading.\nFind-another-explanation also overcomes confounds on Decoy MNIST, needing only one iteration to recover baseline accuracy. Bumping \u03bb1 too high (to the point where its term is a few orders of magnitude larger than the cross-entropy) results in more erratic behavior. Interestingly, in a process remniscent of distillation (Papernot et al., 2016c), the gradients themselves become more evenly and intuitively distributed at later iterations. In many cases they indicate that the probabilities of certain digits increase when we brighten pixels along or extend their distinctive strokes, and that they decrease if we fill in unrelated dark areas, which seems desirable. However, by the last iteration, we start to revert to using decoy swatches in some cases.\nOn 20 Newsgroups, the words most associated with alt.atheism and\nsoc.religion.christian change between iterations but remain mostly intuitive in their associations. Train accuracy mostly remains high while test accuracy is unstable.\nFor all of these examples, accuracy remains high even as decision boundaries shift significantly. This may be because real-world data tends to contain significant redundancies."
    },
    {
      "heading": "2.3.3 Limitations",
      "text": "Input gradients provide faithful information about a model\u2019s rationale for a prediction but trade interpretability for efficiency. In particular, when input features are not individually meaningful to users (e.g. for individual pixels or word2vec components), input gradients\n21\nmay be difficult to interpret and A may be difficult to specify. Additionally, because they can be 0 far from the decision boundary, they do not capture the idea of salience as well as other methods (Zeiler and Fergus, 2014; Sundararajan et al., 2017; Montavon et al., 2017; Bach et al., 2015; Shrikumar et al., 2016). However, they are necessarily faithful to the model and easy to incorporate into its loss function. Input gradients are first-order linear approximations of the model; we might call them first-order explanations."
    },
    {
      "heading": "2.4 Discussion",
      "text": "In this chapter, we showed that:\n\u2022 On training sets that contain confounds which would fool any model trained just to\nmake correct predictions, we can use gradient-based explanation regularization to learn models that still generalize to test. These results imply that gradient regularization actually changes why our model makes predictions.\n\u2022 When we lack expert annotations, we can still use our method in an unsupervised\nmanner to discover models that make predictions for different reasons. This \u201cfindanother-explanation\u201d technique allowed us to overcome confounds on Decoy MNIST and Iris-Cancer, and even quantify the ambiguity present in the Toy Color dataset.\n\u2022 Input gradients are consistent with sample-based methods such as LIME but faster to\ncompute and sometimes more faithful to the model, especially for continuous inputs.\nOur consistent results on several diverse datasets show that input gradients merit further investigation as building blocks for optimizable explanations; there exist many options for further advancements such as weighted annotations A, different penalty norms, and more general specifications of whether features should be positively or negatively predictive of specific classes for specific inputs.\nFinally, our \u201cright for the right reasons\u201d approach may be of use in solving related problems, e.g. in integrating causal inference with deep neural networks or maintaining\n22\nrobustness to adversarial examples (which we discuss in Chapter 3). Building on our find-another-explanation results, another promising direction is to let humans in the loop interactively guide models towards correct explanations. Overall, we feel that developing methods of ensuring that models are right for better reasons is essential to overcoming the inherent obstacles to generalization posed by ambiguities in real-world datasets.\n2.A Cross-Validation\nMost regularization parameters are selected to maximize accuracy on a validation set. However, when your training and validation sets share the same misleading confounds, validation accuracy may not be a good proxy for test accuracy. Instead, we recommend increasing the explanation regularization strength \u03bb1 until the cross-entropy and \u201cright reasons\u201d terms have roughly equal magnitudes (which corresponds to the region of highest test accuracy below). Intuitively, balancing the terms in this way should push our optimization away from cross-entropy minima that violate the explanation constraints specified in A and towards ones that correspond to \u201cbetter reasons.\u201d Increasing \u03bb1 too much makes the cross-entropy term negligible. In that case, our model performs no better than random guessing.\n23\n2.B Learning with Less Data\nIt is natural to ask whether explanations can reduce data requirements. Here we explore that question on the Toy Color dataset using four variants of A (with \u03bb1 chosen to match loss terms at each N).\nWe find that when A is set to the Pro-Rule 1 mask, which penalizes all pixels except the corners, we reach 95% accuracy with fewer than 100 examples (as compared to A = 0, where we need almost 10000). Penalizing the top-middle pixels (Anti-Rule 2) or all pixels except the top-middle (Pro-Rule 2) also consistently improves accuracy relative to data. Penalizing the corners (Anti-Rule 1), however, reduces accuracy until we reach a threshold N. This may be because the corner pixels can match in 4 ways, while the top-middle pixels can differ in 4 \u00b7 3 \u00b7 2 = 24 ways, suggesting that Rule 2 could be inherently harder to learn from data and positional explanations alone.\n24\n2.C Simultaneous Find-Another-Explanation\nIn Section 2.2.2, we introduced a method of training classifiers to make predictions for different reasons by sequentially augmenting A to penalize more features. However, as our ensemble grows, A can saturate to 1N\u00d7D, and subsequent models will be trained with uniform gradient regularization. While these models may have desirable properties (which we explore in the following chapter), they will not be diverse.\nAs a simple example, consider a 2D dataset with one class confined to the first quadrant and the other confined to the third. In theory, we have a full degree of decision freedom; it should be possible to learn two perfect and fully orthogonal boundaries (one horizontal, one vertical). However, when we train our first MLP, it learns a diagonal surface; both features have large gradients everywhere, so A = 1N\u00d72 immediately. To resolve this, we propose a simultaneous training procedure:\n\u03b8\u22171 , . . . , \u03b8 \u2217 M = arg min\n\u03b81,...,\u03b8M\nM\n\u2211 a=1 L(y, f (X|\u03b8a)) +\nM\n\u2211 a=1\nM\n\u2211 b=a+1 sim( fX(X|\u03b8a), fX(X|\u03b8b)), (2.1)\nwhere L refers to our single-model loss function, and for our similarity measure we use the squared cosine similarity sim(v, w) = (v Tw)2\n(vTv)(wTw)+e , where we add e = 10 \u22126 to the\ndenominator for numerical stability. Squaring the cosine similarity ensures our penalty is positive, is minimized by orthogonal boundaries, and is soft for nearly orthogonal boundaries. We show in Figure 2.13 that this lets us obtain the two desired models.\n25\nChapter 3\nInterpretability and Robustness1"
    },
    {
      "heading": "3.1 Introduction",
      "text": "In the previous chapter, we used input gradient penalties to encourage neural networks to make predictions for specific reasons. We demonstrated this on \u201cdecoy\u201d datasets deliberately designed to deceive models making decisions for different reasons. This philosophy of testing \u2013 that we should measure generalization by testing on data from a different distribution than we trained on \u2013 can be taken to its extreme by testing models in an adversarial setting, where neural networks have known vulnerabilities (Szegedy et al., 2013). In this chapter, we consider whether a domain knowledge-agnostic application of explanation regularization (a uniform L2 penalty on input gradients, similar in spirit to Ridge regression on the model\u2019s local linear approximations) could help defend against adversarial examples.\nAdversarial examples pose serious obstacles for the adoption of neural networks in settings which are security-sensitive or have legal ramifications (Kang and Kang, 2017). Although many techniques for generating these examples (which we call \u201cattacks\u201d) require access to model parameters, Papernot et al. (2017) have shown that it is possible and even practical to attack black-box models in the real world, in large part because of transferability;\n1Significant portions of this chapter also appear in Ross, A. and Doshi-Velez, F. (2018). Improving the adversarial robustness and interpretability of deep neural networks by regularizing their input gradients. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence.\n26\nexamples generated to fool one model tend to fool all models trained on the same dataset. Particularly for images, these adversarial examples can be constructed to fool models across a variety of scales and perspectives (Athalye and Sutskever, 2017), which poses a problem for the adoption of deep learning models in systems like self-driving cars.\nAlthough there has recently been a great deal of research in adversarial defenses, many of these methods have struggled to achieve robustness to transferred adversarial examples (Tram\u00e8r et al., 2017b). Some of the most effective defenses simply detect and reject them rather than making predictions (Xu et al., 2017). The most common, \u201cbrute force\u201d solution is adversarial training, where we include a mixture of normal and adversarially-generated examples in the training set (Kurakin et al., 2016b). However, Tram\u00e8r et al. (2017a) show that the robustness adversarial training provides can be circumvented by randomizing or transferring perturbations from other models (though ensembling helps).\nAs we noted in Chapter 2, domain experts are also often concerned that DNN predictions are uninterpretable. The lack of interpretability is particularly problematic in domains where algorithmic bias is often a factor (Angwin et al., 2016) or in medical contexts where safety risks can arise when there is mismatch between how a model is trained and used (Caruana et al., 2015). For computer vision models (the primary target of adversarial attacks), the most common class of explanation is the saliency map, either at the level of raw pixels, grid chunks, or superpixels (Ribeiro et al., 2016).\nThe local linear approximation provided by raw input gradients (Baehrens et al., 2010) is sometimes used for pixel-level saliency maps (Simonyan et al., 2013). However, computer vision practitioners tend not to examine raw input gradients because they are noisy and difficult to interpret. This issue has spurred the development of techniques like integrated gradients (Sundararajan et al., 2017) and SmoothGrad (Smilkov et al., 2017) that generate smoother, more interpretable saliency maps from noisy gradients. The rationale behind these techniques is that, while the local behavior of the model may be noisy, examining the gradients over larger length scales in input space provides a better intution about the model\u2019s behavior.\n27\nHowever, raw input gradients are exactly what many attacks use to generate adversarial examples. Explanation techniques which smooth out gradients in background pixels may be inappropriately hiding the fact that the model is quite sensitive to them. We consider that perhaps the need for these smoothing techniques in the first place is indicative of a problem with our models, related to their adversarial vulnerability and capacity to overfit. Perhaps it is fundamentally hard for adversarially vulnerable models to be interpretable.\nOn the other hand, perhaps it is hard for interpretable models to be adversarially vulnerable. Our hypothesis is that by training a model to have smooth input gradients with fewer extreme values, it will not only be more interpretable but also more resistant to adversarial examples. In the experiments that follow we confirm this hypothesis using uniform gradient regularization, which optimizes the model to have smooth input gradients with respect to its predictions during training. Using this technique, we demonstrate robustness to adversarial examples across multiple model architectures and datasets, and in particular demonstrate robustness to transferred adversarial examples: gradient-regularized models maintain significantly higher accuracy on examples generated to fool other models than baselines. Furthermore, both qualitatively and in human subject experiments, we find that adversarial examples generated to fool gradient-regularized models are, in a particular sense, more \u201cinterpretable\u201d: they fool humans as well."
    },
    {
      "heading": "3.2 Background",
      "text": "In this section, we will (re)introduce notation, and give a brief overview of the baseline attacks and defenses against which we will test and compare our methods. The methods we will analyze again apply to all differentiable classification models f\u03b8(X), which are functions parameterized by \u03b8 that return predictions y\u0302 \u2208 RN\u00d7K given inputs X \u2208 RN\u00d7D. These predictions indicate the probabilities that each of N inputs in D dimensions belong to each of K class labels. To train these models, we try to find sets of parameters \u03b8\u2217 that minimize the total information distance between the predictions y\u0302 and the true labels y (also\n28\n\u2208 RN\u00d7K, one-hot encoded) on a training set:\n\u03b8\u2217 = arg min \u03b8\nN\n\u2211 n=1\nK\n\u2211 k=1 \u2212ynk log f\u03b8(Xn)k, (3.1)\nwhich we will sometimes write as\narg min \u03b8 H(y, y\u0302),\nwith H giving the sum of the cross entropies between the predictions and the labels."
    },
    {
      "heading": "3.2.1 Attacks",
      "text": ""
    },
    {
      "heading": "Fast Gradient Sign Method (FGSM)",
      "text": "Goodfellow et al. (2014) introduced this first method of generating adversarial examples by perturbing inputs in a manner that increases the local linear approximation of the loss function:\nXFGSM = X + e sign (\u2207x H(y, y\u0302)) (3.2)\nIf e is small, these adversarial examples are indistinguishable from normal examples to a human, but the network performs significantly worse on them.\nKurakin et al. (2016a) noted that one can iteratively perform this attack with a small e to induce misclassifications with a smaller total perturbation (by following the nonlinear loss function in a series of small linear steps rather than one large linear step)."
    },
    {
      "heading": "Targeted Gradient Sign Method (TGSM)",
      "text": "A simple modification of the Fast Gradient Sign Method is the Targeted Gradient Sign Method, introduced by Kurakin et al. (2016a). In this attack, we attempt to decrease a modified version of the loss function that encourages the model to misclassify examples in a specific way:\nXTGSM = X\u2212 e sign ( \u2207x H(ytarget, y\u0302) ) , (3.3)\n29\nwhere ytarget encodes an alternate set of labels we would like the model to predict instead. In the digit classification experiments below, we often picked targets by incrementing the labels y by 1 (modulo 10), which we will refer to as y+1. The TGSM can also be performed iteratively."
    },
    {
      "heading": "Jacobian-based Saliency Map Approach (JSMA)",
      "text": "The final attack we consider, the Jacobian-based Saliency Map Approach (JSMA), also takes an adversarial target vector ytarget. It iteratively searches for pixels or pairs of pixels in X to change such that the probability of the target label is increased and the probability of all other labels are decreased. This method is notable for producing examples that have only been changed in several dimensions, which can be hard for humans to detect. For a full description of the attack, we refer the reader to Papernot et al. (2016b)."
    },
    {
      "heading": "3.2.2 Defenses",
      "text": "As baseline defenses, we consider defensive distillation and adversarial training. To simplify comparison, we omit defenses (Xu et al., 2017; Nayebi and Ganguli, 2017) that are not fully architecture-agnostic or which work by detecting and rejecting adversarial examples."
    },
    {
      "heading": "Distillation",
      "text": "Distillation, originally introduced by Ba and Caruana (2014), was first examined as a potential defense by Papernot et al. (2016c). The main idea is that we train the model twice, initially using the one-hot ground truth labels but ultimately using the initial model\u2019s softmax probability outputs, which contain additional information about the problem. Since the normal softmax function tends to converge very quickly to one-hot-ness, we divide all of the logit network outputs (which we will call z\u0302k instead of the probabilities y\u0302k) by a temperature T (during training but not evaluation):\nfT,\u03b8(Xn)k = ez\u0302k(Xn)/T\n\u2211Ki=1 ez\u0302i(Xn)/T , (3.4)\n30\nwhere we use fT,\u03b8 to denote a network ending in a softmax with temperature T. Note that as T approaches \u221e, the predictions converge to 1K . The full process can be expressed as\n\u03b80 = arg min \u03b8\nN\n\u2211 n=1\nK\n\u2211 k=1 \u2212ynk log fT,\u03b8(Xn)k,\n\u03b8\u2217 = arg min \u03b8\nN\n\u2211 n=1\nK\n\u2211 k=1 \u2212 fT,\u03b80(Xn)k log fT,\u03b8(Xn)k.\n(3.5)\nDistillation is usually used to help small networks achieve the same accuracy as larger DNNs, but in a defensive context, we use the same model twice. It has been shown to be an effective defense against white-box FGSM attacks, but Carlini and Wagner (2016) have shown that it is not robust to all kinds of attacks. We will see that the precise way it defends against certain attacks is qualitatively different than gradient regularization, and that it can actually make the models more vulnerable to attacks than an undefended model."
    },
    {
      "heading": "Adversarial Training",
      "text": "In adversarial training (Kurakin et al., 2016b), we increase robustness by injecting adversarial examples into the training procedure. We follow the method implemented in Papernot et al. (2016a), where we augment the network to run the FGSM on the training batches and compute the model\u2019s loss function as the average of its loss on normal and adversarial examples without allowing gradients to propogate so as to weaken the FGSM attack (which would also make the method second-order). We compute FGSM perturbations with respect to predicted rather than true labels to prevent \u201clabel leaking,\u201d where our model learns to classify adversarial examples more accurately than regular examples."
    },
    {
      "heading": "3.3 Gradient Regularization",
      "text": "We defined our \u201cright for the right reasons\u201d objective in Chapter 2 using an L2 penalty on the gradient of the model\u2019s predictions across classes with respect to input features marked irrelevant by domain experts. We encoded their domain knowledge using an annotation matrix A. If we set A = 1, however, and consider only the log-probabilities of\n31\nthe predicted classes, we recover what Drucker and Le Cun (1992) introduced as \u201cdouble backpropagation\u201d, which trains neural networks by minimizing not just the \u201cenergy\u201d of the network but the rate of change of that energy with respect to the input features. In their formulation the energy is a quadratic loss, but we can reformulate it almost equivalently using the cross-entropy:\n\u03b8\u2217 = arg min \u03b8\nN\n\u2211 n=1\nK\n\u2211 k=1 \u2212ynk log f\u03b8(Xn)k + \u03bb\nD\n\u2211 d=1\nN\n\u2211 n=1\n( \u2202\n\u2202xd\nK\n\u2211 k=1 \u2212ynk log f\u03b8(Xn)k\n)2 , (3.6)\nwhose objective we can write a bit more concisely as\narg min \u03b8\nH(y, y\u0302) + \u03bb||\u2207x H(y, y\u0302)||22,\nwhere \u03bb is again a hyperparameter specifying the penalty strength. The intuitive objective of this function is to ensure that if any input changes slightly, the divergence between the predictions and the labels will not change significantly (though including this term does not guarantee Lipschitz continuity everywhere). Double backpropagation was mentioned as a potential adversarial defense in the same paper which introduced defensive distillation (Papernot et al., 2016c), but at publish time, its effectiveness in this respect had not yet been analyzed in the literature \u2013 though Gu and Rigazio (2014) previously and Hein and Andriushchenko (2017); Czarnecki et al. (2017) concurrently consider related objectives, and Raghunathan et al. (2018) derive and minimze an upper bound on adversarial vulnerability based on the maximum gradient norm in a ball around each training input. These works also provide stronger theoretical explanations for why input gradient regularization is effective, though they do not analyze its relationship to model interpretability. In this work, we interpret gradient regularization as a quadratic penalty on our model\u2019s saliency map.\n32"
    },
    {
      "heading": "3.4 Experiments",
      "text": ""
    },
    {
      "heading": "Datasets and Models",
      "text": "We evaluated the robustness of distillation, adversarial training, and gradient regularization to the FGSM, TGSM, and JSMA on MNIST (LeCun et al., 2010), Street-View House Numbers (SVHN) (Netzer et al., 2011), and notMNIST Butalov (2011). On all datasets, we test a simple convolutional neural network with 5x5x32 and 5x5x64 convolutional layers followed by 2x2 max pooling and a 1024-unit fully connected layer, with batch-normalization after all convolutions and both batch-normalization and dropout on the fully-connected layer. All models were implemented in Tensorflow and trained using Adam (Kingma and Ba, 2014) with \u03b1 = 0.0002 and e = 10\u22124 for 15000 minibatches of size of 256. For SVHN, we prepare training and validation set as described in Sermanet et al. (2012), converting the images to grayscale following Grundland and Dodgson (2007) and applying both global and local contrast normalization."
    },
    {
      "heading": "Attacks and Defenses",
      "text": "For adversarial training and JSMA example generation, we used the Cleverhans adversarial example library (Papernot et al., 2016a). For distillation, we used a softmax temperature of T = 50, and for adversarial training, we trained with FGSM perturbations at e = 0.3, averaging normal and adversarial losses. For gradient regularized models, we use double backpropagation, which provided the best robustness, and train over a spread of \u03bb values. We choose the \u03bb with the highest accuracy against validation black-box FGSM examples but which is still at least 97% as accurate on normal validation examples (though accuracy on normal examples tended not to be significantly different). Code for all models and experiments has been open-sourced2.\n2https://github.com/dtak/adversarial-robustness-public\n33"
    },
    {
      "heading": "Evaluation Metrics",
      "text": "For the FGSM and TGSM, we test all models against adversarial examples generated for each model and report accuracy. Testing this way allows us to simultaneously measure whiteand black-box robustness.\nOn the JSMA and iterated TGSM, we found that measuring accuracy was no longer a good evaluation metric, since for our gradient-regularized models, the generated adversarial examples often resembled their targets more than their original labels. To investigate this, we performed a human subject experiment to evaluate the legitimacy of adversarial example misclassifications.\n34"
    },
    {
      "heading": "3.4.1 Accuracy Evaluations (FGSM and TGSM)",
      "text": ""
    },
    {
      "heading": "FGSM Robustness",
      "text": "Figure 3.1 shows the results of our defenses\u2019 robustness to the FGSM on MNIST, SVHN, and notMNIST for our CNN at a variety of perturbation strengths e. Consistently across datasets, we find that gradient-regularized models exhibit strong robustness to black-box transferred FGSM attacks (examples produced by attacking other models). Although adversarial training sometimes performs slightly better at e \u2264 0.3, the value we used in training, gradient regularization generally surpasses it at higher e (see the green curves in the leftmost plots).\nThe story with white-box attacks is more interesting. Gradient-regularized models are generally more robust to than undefended models (visually, the green curves in the rightmost plots fall more slowly than the blue curves in the leftmost plots). However, accuracy still eventually falls for them, and it does so faster than for adversarial training. Even though their robustness to white-box attacks seems lower, though, the examples produced by those white-box attacks actually fool all other models equally well. This effect is particularly pronounced on SVHN. In this respect, gradient regularization may\n35\nhold promise not just as a defense but as an attack, if examples generated to fool them are inherently more transferable.\nModels trained with defensive distillation in general perform no better and often worse than undefended models. Remarkably, except on SVHN, attacks against distilled models actually fail to fool all models. Closer inspection of distilled model gradients and examples themselves reveals that this occurs because distilled FGSM gradients vanish \u2013 so the examples are not perturbed at all. As soon as we obtain a nonzero perturbation from a different model, distillation\u2019s appearance of robustness vanishes as well.\nAlthough adversarial training and gradient regularization seem comparable in terms of accuracy, they work for different reasons and can be applied in concert to increase robustness, which we show in Figure 3.2. In Figure 3.3 we also show that, on normal and adversarially trained black-box FGSM attacks, models trained with these two defenses are fooled by different sets of adversarial examples. We provide intuition for why this might be the case in Figure 3.4.\n36"
    },
    {
      "heading": "TGSM Robustness",
      "text": "Against the TGSM attack (Figure 3.5), defensively distilled model gradients no longer vanish, and accordingly these models start to show the same vulnerability to adversarial attacks as others. Gradient-regularized models still exhibit the same robustness even at large perturbations e, and again, examples generated to fool them fool other models equally well.\nOne way to better understand the differences between gradient-regularized, normal, and distilled models is to examine the log probabilities they output and the norms of their loss function input gradients, whose distributions we show in Figure 3.6 for MNIST. We can see that the different defenses have very different statistics. Probabilities of non-predicted classes tend to be small but remain nonzero for gradient-regularized models, while they vanish\n37\non defensively distilled models evaluated at T = 0 (despite distillation\u2019s stated purpose of discouraging certainty). Perhaps because \u2207 log p(x) = 1p(x)\u2207p(x), defensively distilled models\u2019 non-predicted log probability input gradients are the largest by many orders of magnitude, while gradient-regularized models\u2019 remain controlled, with much smaller means and variances. The other models lie between these two extremes. While we do not have a strong theoretical argument about what input gradient magnitudes should be, we believe it makes intuitive sense that having less variable, well-behaved, and non-vanishing input gradients should be associated with robustness to attacks that consist of small perturbations in input space."
    },
    {
      "heading": "3.4.2 Human Subject Study (JSMA and Iterated TGSM)",
      "text": ""
    },
    {
      "heading": "Need for a Study",
      "text": "Accuracy scores against the JSMA can be misleading, since without a maximum distortion constraint it necessarily runs until the model predicts the target. Even with such a constraint,\n38\nthe perturbations it creates sometimes alter the examples so much that they no longer resemble their original labels, and in some cases bear a greater resemblance to their targets. Figure 3.7 shows JSMA examples on MNIST for gradient-regularized and distilled models which attempt to convert 0s and 1s into every other digit. Although all of the perturbations \u201csucceed\u201d in changing the model\u2019s prediction, in the gradient-regularized case, many of the JSMA examples strongly resemble their targets.\nThe same issues occur for other attack methods, particularly the iterated TGSM, for which we show confusion matrices for different models and datasets in Figure 3.8. For the gradient-regularized models, these psuedo-adversarial examples quickly become almost prototypical examples of their targets, which is not reflected in accuracies with respect to the original labels.\nTo test these intuitions more rigorously, we ran a small pilot study with 11 subjects\n39\nto measure whether they found examples generated by these methods to be more or less plausible instances of their targets."
    },
    {
      "heading": "Study Protocol",
      "text": "The pilot study consisted of a quantitative and qualitative portion. In the quantitative portion, subjects were shown 30 images of MNIST JSMA or SVHN iterated TGSM examples. Each of the 30 images corresponded to one original digit (from 0 to 9) and one model (distilled, gradient-regularized, or undefended). Note that for this experiment, we used \u2207x H( 1K , y\u0302) gradient regularization, ran the TGSM for just 10 steps, and trained models for 4 epochs at a learning rate of 0.001. This procedure was sufficient to produce examples with explanations similar to the longer training procedure used in our earlier experiments, and actually increased the robustness of the undefended models (adversarial accuracy tends to fall with training iteration). Images were chosen uniformly at random from a larger set of 45 examples that corresponded to the first 5 images of the original digit in the test set transformed using the JSMA or iterated TGSM to each of the other 9 digits (we ensured that all models misclassified all examples as their target). Subjects were not given the original\n40\nlabel, but were asked to input what they considered the most and second-most plausible predictions for the image that they thought a reasonable classifier would make (entering N/A if they thought no label was a plausible choice). In the qualitative portion that came afterwards, users were shown three 10x10 confusion matrices for the different defenses on MNIST (Figure 3.7 shows the first two rows) and were asked to write comments about the differences between the examples. Afterwards, there was a short group discussion. This study was performed in compliance with the institution\u2019s IRB."
    },
    {
      "heading": "Study Results",
      "text": "Table 3.1 shows quantitative results from the human subject experiment. Overall, subjects found gradient-regularized model adversarial examples most convincing. On SVHN and especially MNIST, humans were most likely to think that gradient-regularized (rather than distilled or normal) adversarial examples were best classified as their target rather than their original digit. Additionally, when they did not consider the target the most plausible label, they were most likely to consider gradient-regularized model mispredictions \u201creasonable\u201d (which we define in Table 3.1), and more likely to consider distilled model mispredictions unreasonable. p-values for the differences between normal and gradient regularized unreasonable error rates were 0.07 for MNIST and 0.08 for SVHN.\nIn the qualitative portion of the study (comparing MNIST JSMA examples), all of the written responses described significant differences between the insensitive model\u2019s JSMA\n41\nexamples and those of the other two methods. Many of the examples for the gradientregularized model were described as \u201cactually fairly convincing,\u201d and that the normal and distilled models \u201cseem to be most easily fooled by adding spurious noise.\u201d Few commentators indicated any differences between the normal and distilled examples, with several saying that \u201cthere doesn\u2019t seem to be [a] stark difference\u201d or that they \u201ccouldn\u2019t describe the difference\u201d between them. In the group discussion one subject remarked on how the perturbations to the gradient-regularized model felt \u201cmore intentional\u201d, and others commented on how certain transitions between digits led to very plausible fakes while others seemed inherently harder. Although the study was small, both its quantitative and qualitative results support the claim that gradient regularization, at least for the two CNNs on MNIST and SVHN, is a credible defense against the JSMA and the iterated TGSM, and that distillation is not."
    },
    {
      "heading": "3.4.3 Connections to Interpretability",
      "text": "Finally, we present a qualitative evaluation suggesting a connection between adversarial robustness and interpretability. In the literature on explanations, input gradients are frequently used as explanations (Baehrens et al., 2010), but sometimes they are noisy and\n42\nnot interpretable on their own. In those cases, smoothing techniques have been developed (Smilkov et al., 2017; Shrikumar et al., 2016; Sundararajan et al., 2017) to generate more interpretable explanations, but we have already argued that these techniques may obscure information about the model\u2019s sensitivity to background features.\nWe hypothesized that if the models had more interpretable input gradients without the need for smoothing, then perhaps their adversarial examples, which are generated directly from their input gradients, would be more interpretable as well. That is, the adversarial example would be more obviously transformative away from the original class label and towards another. The results of the user study show that our gradient-regularized models have this property; here we ask if the gradients are more interpretable as explanations.\nIn Figure 3.9 we visualize input gradients across models and datasets, and while we cannot make any quantitative claims, there does appear to be a qualitative difference in the interpretability of the input gradients between the gradient-regularized models (which were relatively robust to adversarial examples) and the normal and distilled models (which were vulnerable to them). Adversarially trained models seem to exhibit slightly more interpretable gradients, but not nearly to the same degree as gradient-regularized models. When we repeatedly apply input gradient-based perturbations using the iterated TGSM (Figure 3.8), this difference in interpretability between models is greatly magnified, and the results for gradient-regularized models seem to provide insight into what the model has learned. When gradients become interpretable, adversarial images start resembling feature visualizations Olah et al. (2017); in other words, they become explanations."
    },
    {
      "heading": "3.5 Discussion",
      "text": "In this chapter, we showed that:\n\u2022 Gradient regularization slightly outperforms adversarial training (the SOTA) as a\ndefense against black-box transferred FGSM examples from undefended models.\n\u2022 Gradient regularization significantly increases robustness to white-box attacks, though\n43\nnot quite as much as adversarial training.\n\u2022 Adversarial examples generated to fool gradient-regularized models are more \u201cuniver-\nsal;\u201d they are more effective at fooling all models than examples from unregularized models.\n\u2022 Adversarial examples generated to fool gradient-regularized models are more in-\nterpretable to humans, and examples generated from iterative attacks quickly come to legitimately resemble their targets. This is not true for distillation or adversarial training.\nThe conclusion that we would like to reach is that gradient-regularized models are right for better reasons. Although they are not completely robust to attacks, their correct predictions and their mistakes are both easier to understand. To fully test this assertion, we would need to run a larger and more rigorous human subject evaluation that also tests adversarial training and other attacks beyond the JSMA, FGSM, and TGSM.\nConnecting what we have done back to the general idea of explanation regularization, we saw in Equation 3.6 that we could interpret our defense as a quadratic penalty on our CNN\u2019s saliency map. Imposing this penalty had both quantitative and qualitative effects; our gradients became smaller but also smoother with fewer high-frequency artifacts. Since gradient saliency maps are just normals to the model\u2019s decision surface, these changes suggest a qualitative difference in the \u201creasons\u201d behind our model\u2019s predictions. Many techniques for generating smooth, simple saliency maps for CNNs not based on raw gradients have been shown to vary under meaningless transformations of the model Kindermans et al. (2017) or, more damningly, to remain invariant under extremely meaningful ones (Adebayo et al., 2018) \u2013 which suggests that many of these methods either oversimplify or aren\u2019t faithful to the models they are explaining. Our approach in this chapter was, rather than simplifying our explanations of fixed models, to optimize our models to have simpler explanations. Their increased robustness can be thought of as a useful side effect.\nAlthough the problem of adversarial robustness in deep neural networks is still very\n44\nmuch an open one, these results may suggest a deeper connection between it and interpretability. No matter what method proves most effective in the general case, we suspect that any progress towards ensuring either interpretability or adversarial robustness in deep neural networks will likely represent progress towards both.\n45\nChapter 4"
    },
    {
      "heading": "General Explanation Regularization",
      "text": "In the previous two chapters, we introduced the idea of explanation regularization, and showed that we could use this to obtain models that were both simpler and more robust to differences between training and test conditions. However, we obtained all of those results just with L2 input gradient penalties. Although gradients have special importance in differentiable models such as neural networks, they have major limitations, especially when the kind of constraints we would like to impose on an explanation are abstract in a way we cannot easily relate back to input features. So in this chapter, we outline promising avenues towards more general forms of explanation regularization."
    },
    {
      "heading": "4.1 Alternative Input Gradient Penalties",
      "text": "Before we leave input gradients behind altogether, it is worth considering what else we can do with them besides simple L2 regularization."
    },
    {
      "heading": "4.1.1 L1 Regularization",
      "text": "In Chapter 3, we saw that penalizing the L2 norm of our model\u2019s input gradients encouraged gradient interpretability and prediction robustness to adversarial examples, and drew an analogy to Ridge regression. One natural question to ask is how penalizing the L1 norm\n46\ninstead would compare, which we could understand as a form of local linear LASSO.\nFor a discussion of this question with application to sepsis treatment, we refer the reader to Ross et al. (2017a), which includes a case-study showing how L1 gradient regularization can help us obtain mortality risk models that are locally sparse and more consistent with clinical knowledge.\nOn image datasets (where input features are not individually meaningful), we do find that L1 gradient regularization is effective in defending against adversarial examples, perhaps more so than L2 regularization. To that end, in Figure 4.1 we present results for VGG-16 models on CIFAR-10, which bode favorably for L1 regularization against both white- and black-box attacks. However, although the gradients of these models change qualitatively compared to normal models, they are not significantly sparser than gradients of models trained with L2 gradient regularization. These results suggest that sparsity with respect to input features may not be a fully achievable or desirable objective for complex image classification tasks.\n47"
    },
    {
      "heading": "4.1.2 Higher-Order Derivatives",
      "text": "Bishop (1993) introduced the idea of limiting the curvature of the function learned by a neural network by imposing an L2 penalty on the network\u2019s second input derivatives. They note, however, that evaluating these second derivatives increases the computational complexity of training by a factor of D, the number of input dimensions. This scaling behavior poses major practical problems for datasets like ImageNet, whose inputs are over 150,000-dimensional. Rifai et al. (2011) develop a scalable workaround by estimating the Frobenius norm of the input Hessian as 1 \u03c32 E [ ||\u2207X f (x)\u2212\u2207X f (x + e)||22 ] for e iid\u223c N (0, \u03c32), which converges to the true value as \u03c3\u2192 0. They then train autoencoders whose exact gradient and approximate Hessian norms are both L2-penalized, and find that the unsupervised representations they learn are more useful for downstream classification tasks. Czarnecki et al. (2017) also regularize using estimates of higher-order derivatives.\nHessian regularization may be desirable for adversarial robustness and interpretability as well. The results in Figure 4.2 suggest that exact Hessian regularization for an MLP on a simple 2D problem encourages the model to learn flatter and wider decision boundaries than gradient regularization, which could be useful for interpretability and robustness. Hessian regularization also appears to behave more sensically even when the penalty term is much larger than the cross entropy. By contrast, in this regime, gradient regularization starts pathologically seeking areas of the input space (usually near the edges of the training distribution) where it can set gradients to 0."
    },
    {
      "heading": "4.2 Heftier Surrogates",
      "text": "While input gradient-based methods are appealing because of their close relationship to the shape and curvature of differentiable models\u2019 decision surfaces, they are limited by their locality and humans\u2019 inability to express abstract desiderata in terms of input features. This second limitation in particular prevents us from optimizing for the kind of simplicity or diversity humans find intuitive. Therefore, in the next sections we explore ways of training\n48\nmodels using more complex forms of explanation.\nOne common way of explaining complicated models like neural networks is by distilling them into surrogate models; decision trees are a particularly popular choice Craven and Shavlik (1996). However, these decision trees must sometimes be quite deep in order to accurately explain the associated networks, which defeats the purpose of making predictions interpretable. To address this problem, Wu et al. (2017) optimize the underlying neural networks to be accurately approximatable by shallow decision trees. Performing such an optimization is difficult because the process of distilling a network into a decision tree cannot be expressed analytically, much less differentiated. However, they approximate it by training a second neural network to predict the depth of the decision tree that would result from the first neural network\u2019s parameters. They then use this learned function as a differentiable surrogate of the true approximating decision tree depth. Crucially, they find a depth regime where their networks can outperform decision trees while remaining explainable by them. Although they only try to minimize the approximating decision tree depth, in principle one could train the second network to estimate other characteristics of\n49\nthe decision tree related to simplicity or consistency with domain knowledge (and optimize the main network accordingly)."
    },
    {
      "heading": "4.3 Examples and Exemplars",
      "text": "Another popular way of explaining predictions is with inputs themselves. k-Nearest Neighbors (kNN) algorithms are easy to understand since one can simply present the neighbors, and techniques have recently been proposed to perform kNN using distance metrics derived from pretrained neural networks (Papernot and McDaniel, 2018). More general methods involve sparse graph flows between labeled and unlabeled inputs (Rustamov and Klosowski, 2017) or optimization to find small sets of prototypical inputs that can be used for cluster characterization or classification (Kim et al., 2014), even within neural networks (Li et al., 2017). There has also been recent work on determining which points would most affect a prediction if removed from the training set (Koh and Liang, 2017). These approaches have both advantages and disadvantages. Justifying predictions based on input similarity and difference can seem quite natural, though it can also be confusing or misleading when the metric used to quantify distance between points does not correspond to human intuition. Influence functions shed light on model sensitivities that are otherwise very hard to detect, but they are also very sensitive to outliers, leading to sometimes inscrutable explanations.\nHowever, it seems straightforward at least in principle to implement example-based explanation regularization. For example, we could train neural networks with annotations indicating that certain pairs of examples should be similar or dissimilar, and penalize the model when their intermediate representations are relatively distant or close (which might require altering minibatch sampling to keep paired examples together if annotations are sparse). Although influence functions may be too computationally expensive to incorporate into the loss functions of large networks, it seems useful in principle to specify that certain examples should be particularly representative or influential in deciding how to classify others.\n50"
    },
    {
      "heading": "4.4 Emergent Abstractions",
      "text": "Stepping back, the level of abstraction at which we communicate the reason behind a decision significantly affects its utility, as Keil (2006) notes:\nExplanations... suffer if presented at the wrong level of detail. Thus, if asked why John got on the train from New Haven to New York, a good explanation might be that he had tickets for a Broadway show. An accurate but poor explanation at too low a level might say that he got on the train because he moved his right foot from the platform to the train and then followed with his left foot. An accurate but poor explanation at too high a level might say that he got on the train because he believed that the train would take him to New York from New Haven.\nThe explanations we have considered so far have been in terms of input features, entire inputs, or simple surrogates. However, sometimes humans seek to know the reasons behind predictions at levels of abstraction these forms cannot capture. If we really want to create interpretable interfaces for training and explaining machine learning models, humans and models will need to speak a common language that permits abstraction.\nThis may seem like a daunting task, but there has been important recent progress in interpreting neural networks in terms of abstractions that emerge during training. Bau et al. (2017) introduce a densely labeled image dataset. They train convolutional neural networks on a top-level classification task, but also include lower-level sublabels that indicate other features in the image. They measure the extent to which different intermediate nodes in their top-level label classifiers serve as exclusive \u201cdetectors\u201d for particular sublabels, and compare the extent to which different networks learn different numbers of exclusive detectors. They also categorize their sublabels and look at differences in which kinds of sublabels each network learns to detect (and when these detectors emerge during training).\nKim et al. (2017) provide a method of testing networks\u2019 sensitivity to concepts as defined by user-provided sets of examples. Concretely, they train a simple linear classifer at each layer to distinguish between examples in the concept set and a negative set. They reinterpret the weights of this linear classifier as a \u201cconcept activation vector,\u201d and take directional derivatives of the class logits with respect to these concept activations. Repeated across the\n51\nfull dataset for many different concepts, this procedure outputs a set of concept sensitivity weights for each prediction, which can be used for explanation or even image retrieval.\nThe previous two methods require manual human selection of images corresponding to concepts, and they do not guarantee meaningful correspondence between these concepts and what the network has learned. Feature visualization (Olah et al., 2017) takes a different approach and attempts to understand what the network has learned on its own terms. In particular, it tries to explain what (groups of) neuron(s) learn by optimizing images to maximize (or minimize) their activations. It can also optimize sets of images to jointly maximize activations while encouraging diversity. This process can be useful for obtaining an intuitive sense of (some of) what the model has learned, especially if the neurons being explained are class logits. However, it also leads to an information overload, since modern networks contain millions of neurons and an effectively infinite number of ways to group them. To that end, Olah et al. (2018) use non-negative matrix factorization (NMF) to learn a small number of groups of neurons whose feature visualizations best summarize the entire set. Feature visualizations of neuron groups obtained by NMF tend to correspond more cleanly to human-interpretable concepts, though again there is no guarantee this will occur. Olah et al. (2018) also suggest that incorporating human feedback into this process could lead to a method to train models to make decisions \u201cfor the right reasons.\u201d\nThe above cases either take humans concepts and try to map them to network representations or take network \u201cconcepts\u201d and try to visualize them so humans can map them to their own concepts. But they do not actually try to align network representations with human concepts. However, there has been significant recent interest in training models to learn disentangled representations (Chen et al., 2016; Higgins et al., 2016; Siddharth et al., 2017). Disentangled representations are often described as separating out latent factors that concisely characterize important aspects of the inputs but which cannot be easily expressed in terms of their component features. Generally, disentangled representations tend to be much easier to relate to human-intuitive concepts than what models learn when only trained to minimize reconstruction or prediction error.\n52\nThese advances in bridging human and neural representations could have major payoffs in terms of interpreting models or optimizing them to make predictions for specific reasons. Suppose we are interested in testing a classifier\u2019s sensitivity to an abstract concept entangled with our input data. If we have an autoencoder whose representation of the input disentangles the concept into a small set of latent factors, then for a specific input, we can encode it, decode it, and pass the decoded input through the classifier, taking the gradient of the network\u2019s output with respect to the latent factors associated with the concept. If we fix the autoencoder weights but not the classifier weights, we can use this differentiable concept sensitivity score to apply our \u201cright for the right reasons\u201d technique from Chapter 2 to encourage the classifier to be sensitive or insensitive to the concept.\nWe present a preliminary proof of concept of this idea in Figure 4.3. In this experiment, we construct a toy dataset of images of white squares with four true latent factors of variation: the size of the square, its x and y position, and the background color of the image. In training, background color and square size are confounded; images either have dark backgrounds and small squares or light backgrounds and large squares (and either one can be used to predict the label). However, we create two versions of the test set where these latent factors are decoupled (and only one predicts the label). This is analogous to the parable in our introduction with squares representing tanks and background colors\n53\nrepresenting light. When we train a one-hidden layer MLP normally, it learns to implicitly use both factors, and obtains suboptimal accuracies of about 75% on each test set. To circumvent this issue, we first train a convolutional autoencoder that disentangles square size from background color (which we do with supervision here, but in principle this can be unsupervised) and then prepend the autoencoder to our MLP with fixed weights. We then simultaneously train two instantiations of this network with the find-another-explanation penalty we introduced in Section 2.C. These two networks learn to perform nearly perfectly on one test set and do no better than random guessing on the other, which suggests they are making predictions for different conceptual reasons. Obtaining these networks would have been very difficult using only gradient penalties in the input space."
    },
    {
      "heading": "4.5 Interpretability Interfaces",
      "text": "Olah et al. (2018) describe a space of \u201cinterpretability interfaces\u201d and introduce a formal grammar for expressing explanations of neural networks (and a systematic way of exploring designs). They visualize this design space in a grid of relationships between different \u201csubstrates\u201d of the design, which include groups of neurons, dataset examples, and model parameters \u2013 the latter of which presents an opportunity \u201cto consider interfaces for taking action in neural networks.\u201d If human-defined concepts, disentangled representations, or other forms of explanation are included as additional substrates, one can start to imagine a very general framework for expressing priors or constraints on relationships between them. These would be equivalent to optimizing models to make predictions for specific reasons.\nHow would humans actually express these kinds of objectives? One interface worth emulating could be that introduced by recent but popular libraries for weak supervision (Ratner et al., 2017) or probabilistic soft logic (Bach et al., 2017), which is related to the well-studied topic of fuzzy logic, a method noted for its compatibility with human reasoning (Zadeh, 1997). In these frameworks, users can specify \u201csoft\u201d logical rules for labeling datasets or constraining relationships between atoms (or substrates) of a system. Though users can sometimes specify that certain rules are inviolable or highly-weighted, in general\n54\nthese systems assume that rules are not always correct and attempt to infer weights for each. While these inference problems are nontrivial, and in general there may be complex, structured interactions between rules that are difficult to capture, the interface it exposes to users is expressive and potentially worth emulating in an interpretability interface. For example, we could imagine writing soft rules relating:\n\u2022 dataset examples to each other (e.g. these examples should be conceptually similar\nwith respect to a task)\n\u2022 dataset examples to concepts (e.g. these are examples of a concept)\n\u2022 features to concepts (e.g. this set of features is related to this concept, this other set is\nnot; in this specific case, these features contribute positively)\n\u2022 concepts to predictions (e.g. the presence of this concept makes this prediction more\nor less likely, except when this other concept is present)\nThese rules could be \u201ccompiled\u201d into additional energy terms in the model\u2019s loss function, possibly with thresholding if we expect them to be incorrect some percentage of the time (though rules defined for specific examples may be more reliable). We present a schematic diagram of how a system like this might work in Figure 4.4.\n55\nSuch a system would strongly depend on being able to define rules in terms of abstract concepts, but such rules might not be enforcible until the model has a differentiable, stable representations of them. However, one could imagine pre-learning static, disentangled concept representations that could be related back to input features. If 1:1 mappings between human concepts and latent representations do not emerge naturally, even allowing for hierarchical relationships (Esmaeili et al., 2018), steps could be taken to optimize model representations to better match human understanding (e.g. using partial supervision) or to help humans better understand model representations (e.g. using feature visualization). This process of reaching user-model intersubjectivity might require multiple stages of identification and refinement, but seems possible in principle. And perhaps arriving at a shared conceptual framework for understanding a problem is where the work of teaching and learning ought to lie, regardless of whether the teachers and learners are human."
    },
    {
      "heading": "4.6 Discussion",
      "text": "In this chapter, we discussed a number of strategies for explanation regularization beyond the methods we used in the previous chapters. We described simple extensions of gradientbased methods (imposing L1 and Hessian penalties), strategies in terms of interpretable surrogates (regularizing distilled decision trees, nearest neighbors, and exemplars), and strategies in terms of concepts (concept activation vectors, disentangled representations, and feature visualization). We then combined many of these strategies into a design for an \u201cinterpretability interface\u201d that could be used to simultaneously improve neural network interpretability and incorporate domain knowledge.\nOne limitation of this discussion is that we only considered classification models and traditional ways of explaining their predictions. However, there is a much larger literature on alternative forms of explanation and prediction like intuitive theories (Gerstenberg and Tenenbaum, 2017) or causal inference (Pearl, 2010) that is highly relevant, especially if we want to apply these techniques to problems like sequential decisionmaking. We started this thesis by making a point that was \u201ceasiest to express with a story;\u201d even with arbitrarily\n56\nhuman-friendly compositional abstraction (Schulz et al., 2017), flat sets of concepts may never be sufficient in cases where users think in terms of narratives (Abell, 2004).\nHowever, despite these limitations, we think the works we have outlined in this chapter have started to map a rich design space for interpreting and training machine learning models with more than just xes and ys.\n57\nChapter 5"
    },
    {
      "heading": "Conclusion",
      "text": "Building on Keil (2006), Doshi-Velez and Kim (2017) argue that explanations are necessary when our models have a certain incompleteness. In the context of this thesis, we have considered the kind of incompleteness that results from ambiguity in a dataset. When datasets do not fully specify their decision boundaries, we have freedom to learn many models that could accurately classify them.1 We generate explanations to determine which model we learned; we regularize explanations to choose which model to learn.\nIn Chapter 2, we provided our first formulation of explanation regularization using input gradients, a particular kind of local explanation that is low-level but faithful and differentiable. We then used this method to solve classification problems despite the presence of challenging confounding factors, find diverse solutions, and achieve high accuracy with significantly fewer training examples. In Chapter 3, we applied explanation regularization to CNN saliency maps and showed we could make our CNNs both more interpretable and more robust to adversarial attacks. In both chapters, we found that explanations were reliable indicators of generalizability. In Chapter 4, we reviewed other potential ways of implementing explanation regularization, including penalties based on alternative gradient penalties, complex surrogates, nearest neighbors, and even abstract concepts (realized via\n1Finding a way to quantify the degree of freedom a dataset affords its classifiers is an interesting topic for future research.\n58\ndisentangled representations). Finally, we presented a vision for an interface that would allow users and machine learning models to explain predictions to each other, using a common conceptual framework they define together.\nOne methodological point we would like to stress before closing is about the importance of ground truth in evaluating explanation techniques. Throughout this thesis, we tried to ensure that whenever we evaluated explanations, we also had some way of determining what those explanations should be. For example, in Chapter 2, we knew which features in our synthetic dataset actually mattered for prediction, and we knew that models fooled by decoy datasets were sensitive to the corresponding confounds. Regardless of whether explanations are used in human-, application-, or functionally-grounded tasks (Doshi-Velez and Kim, 2017), having ground truth is critical \u2013 especially given recent criticisms of many explanation methods for being unhelpful in detecting generalization issues (Chandrasekaran et al., 2017), being sensitive to meaningless changes to models (Kindermans et al., 2017; Ghorbani et al., 2017), or being invariant to meaningful changes to models (Adebayo et al., 2018). Otherwise, we may end up rationalizing our predictions rather than explaining them.\nMachine learning is being deployed in more and more critical domains, including the automotive industry, medicine, lending, bail determination (Angwin et al., 2016), and even child maltreatment hotlines (Chouldechova et al., 2018). As we move forward, it is essential that we develop better diagnostic tools for understanding when our models are wrong and how to right them. The promise of machine learning to improve human lives is great, but so is its peril. To use machine learning models responsibly, regardless of whether they seem right, we must have methods of making sure they are reasonable.\n59"
    }
  ],
  "title": "Training Machine Learning Models by Regularizing their Explanations",
  "year": 2018
}
