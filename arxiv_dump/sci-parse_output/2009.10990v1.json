{
  "abstractText": "Health insurance companies cover half of the United States population through commercial employer-sponsored health plans and pay 1.2 trillion US dollars every year to cover medical expenses for their members. The actuary and underwriter roles at a health insurance company serve to assess which risks to take on and how to price those risks to ensure profitability of the organization. While Bayesian hierarchical models are the current standard in the industry to estimate risk, interest in machine learning as a way to improve upon these existing methods is increasing. Lumiata, a healthcare analytics company, ran a study with a large health insurance company in the United States. We evaluated the ability of machine learning models to predict the per member per month cost of employer groups in their next renewal period, especially those groups who will cost less than 95% of what an actuarial model predicts (groups with \u201cconcession opportunities\u201d). We developed a sequence of two models, an individual patient-level and an employer-group-level model, to predict the annual per member per month allowed amount for employer groups, based on a population of 14 million patients. Our models performed 20% better than the insurance carrier\u2019s existing pricing model, and identified 84% of the concession opportunities. This study demonstrates the application of a machine learning system to compute an accurate and fair price for health insurance products and analyzes how explainable machine learning models can exceed actuarial models\u2019 predictive accuracy while maintaining interpretability.",
  "authors": [
    {
      "affiliations": [],
      "name": "Rohun Kshirsagar"
    },
    {
      "affiliations": [],
      "name": "Li-Yen Hsu"
    },
    {
      "affiliations": [],
      "name": "Charles H. Greenberg"
    },
    {
      "affiliations": [],
      "name": "Matthew McClelland"
    },
    {
      "affiliations": [],
      "name": "Anushadevi Mohan"
    },
    {
      "affiliations": [],
      "name": "Wideet Shende"
    },
    {
      "affiliations": [],
      "name": "Nicolas P. Tilmans"
    },
    {
      "affiliations": [],
      "name": "Min Guo"
    },
    {
      "affiliations": [],
      "name": "Ankit Chheda"
    },
    {
      "affiliations": [],
      "name": "Meredith Trotter"
    },
    {
      "affiliations": [],
      "name": "Shonket Ray"
    },
    {
      "affiliations": [],
      "name": "Miguel Alvarado"
    }
  ],
  "id": "SP:5aa7282e4ddc05548c1536f3cb41638e60ab01a8",
  "references": [
    {
      "authors": [
        "D.B. Atkinson"
      ],
      "title": "Credibility Methods Applied to Life, Health, and Pensions: Credibility Applications for Life and Health Insurers and Pension Plans",
      "venue": "Society of Actuaries .",
      "year": 2019
    },
    {
      "authors": [
        "W. Bluhm",
        "D. Skwire",
        "S. Kaczmarek",
        "K. Bohn"
      ],
      "title": "Group Insurance 6th Edition",
      "venue": "ACTEX Publications, Inc.",
      "year": 2007
    },
    {
      "authors": [
        "T. Buchmueller",
        "C. Carey",
        "H. Levy"
      ],
      "title": "Will employers drop Health Insurance Coverage because of the Affordable Care Act",
      "venue": "Health Affairs",
      "year": 2013
    },
    {
      "authors": [
        "B. Callaghan",
        "E. Reynolds",
        "M. Banerjee",
        "K. Kerber",
        "L. Skolarus",
        "B. Magliocco",
        "G. Esper",
        "J. Burke"
      ],
      "title": "Outof-pocket costs are on the rise for commonly prescribed neurologic medications",
      "venue": "Neurology 92: e2604\u2013e2613.",
      "year": 2019
    },
    {
      "authors": [
        "CBO."
      ],
      "title": "Congressional Budget Office: Federal Subsidies for Health Insurance Coverage for People Under Age 65: 2019 to 2029",
      "venue": "Technical report.",
      "year": 2019
    },
    {
      "authors": [
        "CMS."
      ],
      "title": "Centers for Medicare and Medicaid Office of the Actuary - Claims Credibility Guidelines",
      "venue": "Technical report.",
      "year": 2018
    },
    {
      "authors": [
        "P. Finn",
        "A. Gupta",
        "S. Lin",
        "E. Onitskansky"
      ],
      "title": "Growing employer interest in innovative ways to control healthcare costs",
      "venue": "Technical report.",
      "year": 2017
    },
    {
      "authors": [
        "E. Frees",
        "G. Meyers",
        "A.D. Cummings"
      ],
      "title": "Summarizing Insurance Scores Using a Gini Index",
      "venue": "Journal of the American Statistical Association 106: 1085\u20131098.",
      "year": 2011
    },
    {
      "authors": [
        "C. Fuhrer"
      ],
      "title": "A Practical Approach to Assigning Credibility for Group Medical Insurance Pricing",
      "venue": "Society of Actuaries \u2013 Health Section Research Committee .",
      "year": 2015
    },
    {
      "authors": [
        "J.V. Guttag"
      ],
      "title": "Introduction to Computation and Programming Using Python: With Application to Understanding Data",
      "year": 2016
    },
    {
      "authors": [
        "N. Henke",
        "J. Levine",
        "P. McInerney"
      ],
      "title": "Analytics Translator: The new must-have role",
      "venue": "Technical report.",
      "year": 2018
    },
    {
      "authors": [
        "G. Hileman",
        "S. Steele"
      ],
      "title": "Accuracy of ClaimsBased Risk Scoring Model",
      "venue": "Society of Actuaries \u2013 Health Section Research Committee .",
      "year": 2016
    },
    {
      "authors": [
        "IBC."
      ],
      "title": "Independence Blue Cross Underwriting Department Large Group Underwriting Guidelines",
      "venue": "Technical report.",
      "year": 2016
    },
    {
      "authors": [
        "G. Ke",
        "Q. Meng",
        "T. Finley",
        "T. Wang",
        "W. Chen",
        "W. Ma",
        "Q. Ye",
        "T. Liu"
      ],
      "title": "LightGBM: A Highly Efficient Gradient Boosting Decision Tree",
      "venue": "Neural Information Processing Systems .",
      "year": 2017
    },
    {
      "authors": [
        "KFF."
      ],
      "title": "Kaiser Family Foundation 2019 Employer Health Benefits Survey",
      "venue": "Technical report.",
      "year": 2019
    },
    {
      "authors": [
        "A. Kirzinger",
        "C. Munana",
        "B. Wu",
        "M. Brodie"
      ],
      "title": "Data Note: Americans Challenges with Health Care Costs",
      "venue": "Technical report.",
      "year": 2019
    },
    {
      "authors": [
        "J. Lee",
        "M. Majerol",
        "J.D. Burke"
      ],
      "title": "Addressing the social determinants of health for Medicare and Medicaid enrollees",
      "year": 2019
    },
    {
      "authors": [
        "S. Lundberg",
        "S. Lee"
      ],
      "title": "A Unifed Approach to Interpreting Model Predictions",
      "venue": "Neural Information Processing Systems .",
      "year": 2017
    },
    {
      "authors": [
        "H. Lyons",
        "S.E. Shaw",
        "J. Kittredge",
        "G. Watson",
        "J. Moran",
        "R. Millman"
      ],
      "title": "Persistency of Group Health Insurance",
      "venue": "Transactions of Society of Actuaries 13.",
      "year": 1961
    },
    {
      "authors": [
        "Z. Obermeyer",
        "B. Powers",
        "C. Vogeli",
        "S. Mullainathan"
      ],
      "title": "Dissecting racial bias in an algorithm used to manage the health of populations",
      "venue": "Science 366: 447\u2013453.",
      "year": 2019
    },
    {
      "authors": [
        "D. Powers"
      ],
      "title": "Evaluation: From Precision, Recall and FFactor to ROC, Informedness, Markedness and Correlation",
      "venue": "Journal of Machine Learning Technologies 2: 3763.",
      "year": 2008
    },
    {
      "authors": [
        "M. Pearson",
        "S. Madabushi",
        "N. Shah",
        "A. Butte",
        "M. Howell",
        "C. Cui",
        "J. Dean"
      ],
      "title": "Scalable and accurate deep learning with electronic health records",
      "venue": "Nature Digital Medicine",
      "year": 2018
    },
    {
      "authors": [
        "N. Razavian",
        "S. Blecker",
        "A.M. Schmidt",
        "A. Smith-McLallen",
        "S. Nigam",
        "D. Sontag"
      ],
      "title": "Population-level Prediction of Type 2 Diabetes From Claims Data and Analysis of Risk Factors",
      "venue": "Big Data 3: 277\u2013287.",
      "year": 2015
    },
    {
      "authors": [
        "C. Rudin"
      ],
      "title": "Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead",
      "venue": "Nature Machine Intelligence 1: 206\u2013215.",
      "year": 2019
    },
    {
      "authors": [
        "F. Schaudel",
        "F. Niedermann",
        "S. Kumar",
        "K. Reinecke"
      ],
      "title": "Underwriting excellence: The foundation for sustainable growth in health insurance",
      "venue": "Technical report.",
      "year": 2018
    },
    {
      "authors": [
        "H. Schmidli"
      ],
      "title": "Lecture notes on Risk Theory",
      "venue": "Technical report. URL https://web.archive.org/web/20130811041617/ http://www.math.ku.dk/\u223cschmidli/rt.pdf.",
      "year": 2013
    },
    {
      "authors": [
        "SOA."
      ],
      "title": "Society of Actuaries: Pricing, Reserving and Forecasting Module",
      "venue": "Society of Actuaries - FSA Group and Health Track 3.",
      "year": 2017
    },
    {
      "authors": [
        "J.V. Steenwyk"
      ],
      "title": "Rating and Underwriting for Health Plans",
      "venue": "Xlibris Corporation.",
      "year": 2007
    },
    {
      "authors": [
        "S. Tamang",
        "A. Milstein",
        "H. Sorensen",
        "L. Pedersen",
        "L. Mackey",
        "J. Betterton",
        "L. Janson",
        "N. Shah"
      ],
      "title": "Predicting patient cost blooms in Denmark: a longitudinal population-based study",
      "venue": "BMJ Open .",
      "year": 2017
    },
    {
      "authors": [
        "J. White"
      ],
      "title": "The Tax Exclusion for Employer-Sponsored Insurance Is Not Regressive-But What Is It",
      "venue": "J Health Polit Policy Law",
      "year": 2017
    },
    {
      "authors": [
        "H. Zhou",
        "Y. Yang",
        "W. Qian"
      ],
      "title": "Tweedie Gradient Boosting for Extremely Unbalanced Zero-inflated Data",
      "venue": "arXiv preprint arXiv:1811.10192 1811.",
      "year": 2018
    }
  ],
  "sections": [
    {
      "heading": "Introduction",
      "text": "The recent explosion of available electronic health record (EHR) and insurance claims data sets, coupled with the democratization of statistical learning algorithms, has set the stage for machine learning (ML) applications to fundamentally transform the healthcare industry. Employer-sponsored health insurance (ESI) currently covers 150 million Americans (Kirzinger et al. 2019). With numerous subsidies in place to make ESI more affordable (Buchmueller, Carey, and Levy 2013; White 2017), it is by far the most popular option for obtaining health insurance in the United States (CBO 2019). Since the passage of the Affordable Care Act (ACA) in 2009, premiums for single and family ESI plans have increased 50% and deductibles have doubled, making affordability of care a major issue for many Americans. Thirty four percent of patients on ESI plans are reportedly unable to pay\nfor an unexpected bill of $500 and over 50% skip or postpone medical care and prescription fills due to cost (KFF 2019). By making healthcare more affordable, it increases the chance patients will receive needed medical care and refill medications in a timely manner. Increasing affordability improves patient health outcomes and quality of life, reducing familial strain of medical debt, postponing major household spending, and obviating the need to hold multiple jobs (KFF 2019).\nHowever, ESI premiums are the dominating source of revenue for US-based health insurance companies, many of which are among the Fortune 500 companies1. As such, accurate rate-setting is a crucial component of revenue and membership growth for insurance companies; setting inadequate rates can mean the difference between profitability and unprofitability (Steenwyk 2007). Traditionally, health insurance companies set rates using a combination of actuarial science and underwriting judgement. Actuaries apply statistical modeling to claims data to set premiums for each employer-group; underwriters use the actuary\u2019s predicted rate alongside non-claims data (e.g. health questionnaires) to decide which groups to cover and what their rates will be. Accurate rate setting is essential to balance customer retention against business viability because the insurer needs to retain a group for several years before the account becomes profitable. Hence, insurers are willing to reduce rates in the near-term, in exchange for the chance of a longer term relationship (i.e. greater persistency) (Lyons et al. 1961). Good financial standing allows insurers to focus on growing their business, influencing patient health outcomes, improving customer experience, and increasing efficiency (Schaudel et al. 2018). Reduced renewal premiums can align patients\u2019 financial interests and the insurers strategic interests.\nWithin the ESI market, the <500 employer group segment (employer groups with fewer than 500 enrollees) is highly transactional, particularly during peak season (near January 1st of each year). The largest insurance carriers process tens of thousands of new and renewal business quotes. For instance, the carrier in this pilot study averaged 130 presale quotes and 70 renewal quotes per underwriter. Some aspects unique to the <500 market are the: (1) Complex array of funding arrangements, including: fullyinsured, level-funding Administrative Services Only (ASO,\nar X\niv :2\n00 9.\n10 99\n0v 1\n[ cs\n.C Y\n] 2\n3 Se\np 20\n20\ni.e. self-insured), pay-as-you-go and monthly shared-risk models (Finn et al. 2017; IBC 2016) (2) high-risk, highreward nature of this segment: the <500 market comprises one third of medical insurance customers but yields one half of earnings, providing opportunities to yield higher profit margins, and (3) lower persistency compared to larger clients (the average account length is 5 years for the <500 market vs >7 years for larger groups).\nDue to the large volume of cases in the < 500 market, underwriters could benefit from an additional highly-accurate signal to increase the efficiency of their work. This signal would need to be easily interpretable and broadly applied for measurability. The unique circumstances of the <500 market provide an opportunity for improved renewal rate development, driving price optimization and increasing persistency in a risk-rich environment focused on strong, sustainable customer-base growth.\nAlthough ML-based approaches on claims data are widely applied in clinical contexts (e.g. Razavian et al. 2015; Tamang et al. 2017; Rajkomar et al. 2018), few studies incorporate ML approaches for underwriting group health insurance. To our knowledge, there are no models of individual health risk used to form more competitive pricing for a group. Here, we show that ML approaches in medical underwriting can provide: (1) improved accuracy and broader applicability, facilitated by modeling cost and risk at the individual and group levels; and (2) interpretability for nonexperts to use.\nWe jointly ran our study between Lumiata, a healthcare analytics company, and a large insurance company in the US, referred to here with the pseudonym \u201cDelphi\u201d."
    },
    {
      "heading": "Materials and Methods",
      "text": ""
    },
    {
      "heading": "Participants and Setting",
      "text": "We drew from a 14 million member population representative of Delphi\u2019s entire fully-insured employer-group customer base (i.e. their book of business) from 2015 to 2017. We used this population to train and tune our models and then predicted the annual cost of groups in a separate \u201choldout\u201d set. The holdout set consisted of 648 employer groups (referred to here simply as \u201cgroup\u201d) with renewal dates varying from 05/01/2016 to 04/01/2017. There were a total of 349,715 members still actively enrolled as of their respective groups renewal date. Using the holdout set, we evaluated our model by its performance of predicted cost incurred during the 12-month period starting from their group\u2019s renewal date (the \u201cprojection period\u201d).\nIn the holdout set, Delphi censored the data recorded four months prior to the renewal date for each group (the \u201cblackout period\u201d); group underwriting is usually done several months before the new contract year in order to create the renewal quotes presented to employer groups. The 12 months before the blackout period comprise the \u201cexperience period\u201d, ending on the \u201cslice date\u201d. For example, if a group\u2019s renewal date was 05/01/2017, the experience period was the entire 2016 calendar year and 05/01/2017 to 04/30/2018 was the cost projection period (Figure 1).\nFor this pilot study, we sliced the training data to select\ngroups and members in a similar, but simpler way. Instead of dynamically slicing the groups, we imposed a fixed renewal date of 01/01/2017 and used only the data recorded until 08/31/2016 for feature extraction. Therefore, the members and groups for which we predicted cost were those eligible as of the 08/31/2016 slice date. After filtering the training data for eligibility,\u223c 7.4 million patients were enrolled as of the slice date. In order to fine-tune and evaluate models, we split these eligible groups and members into \u201ctrain\u201d, \u201ctest\u201d, and \u201cevaluate\u201d sets. The data were split using a 70:20:10 ratio (Figure 2)."
    },
    {
      "heading": "Data Sources",
      "text": "We built our models using medical, capitation and pharmacy claims, and lab and eligibility tables for Delphi\u2019s patients and groups. The medical claims tables contained cost information, International Classification of Disease (ICD-9 and ICD-10) diagnostic codes, and Current Procedural Technology (CPT) procedure codes at the claim-level. The capitation tables contained only cost information. All tables reported each claim\u2019s care setting: inpatient, outpatient, ancillary, emergency, primary care, or specialty care. The pharmacy claims tables contained National Drug Code (NDC) medication codes and cost for each drug prescribed to a patient and the written and fill dates for the drug prescriptions. The cost associated to claims were given as \u201callowed\u201d amounts (the amount paid by the insurer plus the members cost share). Lab tables contained Logical Observation Identifiers Names and Codes (LOINC) lab test codes. Eligibility tables captured each patient\u2019s health plans, enrollment time periods, and plan benefits."
    },
    {
      "heading": "Models",
      "text": "Actuarial Models To compare model performance between Delphi\u2019s actuarial models and Lumiata\u2019s ML models, we followed the best practices of actuarial science. Actuarial models estimate group cost on a per member per month (pmpm) basis. The normalization unit is called a \u201cmember month\u201d, which is defined as one month of enrollment for one member. We used member months to normalize cost because predicted pmpm cost for a group translates to the monthly premium charged to each member in this group. Actuar-\nial models estimate group-level cost, treating each member month of medical history as independent across members and within a member.\nThe following example shows the utility and limitation of this perspective. Consider three hypothetical groups X, Y, and Z, each having 100 members and costing $1 million during the 2017 calendar year:\n\u2022 Group X: each member is enrolled for 10 months - the pmpm cost equals $1 million/(10 months \u00d7 100 members) = $1,000 pmpm \u2022 Group Y: each member is enrolled for five months - the pmpm cost equals $1 million/(5 months \u00d7 100 members) = $2,000 pmpm \u2022 Group Z: each member is enrolled for five months and one member costs $900,000, while the rest of the members cost $100,000 total - the pmpm cost equals $1 million/(5 months \u00d7 100 members) = $2,000 pmpm\nAs a result of the pmpm formulation, two groups with the same cost but different member months will have a different pmpm cost (Group X vs Y or Z). If the group cost is highly concentrated on one individual, versus being evenly distributed amongst the members in the group, it does not necessarily reflect in the pmpm cost, (Group Y vs Z). In contrast, a member-level cost prediction model views Group Y and Z differently, therefore better modeling cost at the group level.\nActuarial predictive models used for quoting renewal business rely on dozens of rating factors (input variables) to build a predicted rate for a given group. The factors rely on pre-computed demographic, medical trend, pharmacy trend and other actuarial coefficients derived from patients across a large (usually exogenous) population using regressionbased methods. These rating factor \u201cpriors\u201d are then used\nto assemble a predicted trend value for a particular group (called the \u201cmanual rate\u201d, MR). Historical claims for the group are used to create the \u201cexperience rate\u201d (ER), which is then blended with the manual rate to create the final prediction. The proportion of blending between the two quantities is called \u201ccredibility\u201d (c, where 0 \u2264 c \u2264 1; Atkinson 2019). Thus, the predicted total cost of a group can be expressed as:\npredicted cost = c \u00b7 ER+ (1\u2212 c) \u00b7MR (1) This formula is a linear Bayesian hierarchical model, and is the optimal linear least-squares solution for estimating the annual pmpm cost of an employer group, called the Buhlmann-Straub method (Schmidli 2013). This approach is the industry standard, underlying most models in production at insurance companies; applications include pricing, plan design, and reserve setting (Bluhm et al. 2007; Fuhrer 2015). As a group becomes more credible, the actuarial model can rely more on the medical claims history of the group as an indicator of future expenses. In the absence of credibility, the safer bet is to rely on population-level cost estimations using only age and sex (the manual rate). Actuaries use a group\u2019s member months to parameterize credibility (Atkinson 2019; CMS 2018). A larger group enrolled for a shorter period of time (e.g. 1000 members \u00d7 6 months = 6000 member months) can have the same number of member months as a smaller group enrolled for a longer period of time (e.g. 200 members \u00d7 30 months = 6000 member months), making them equally credible.\nThe two equations below are examples of the type of experience rating and manual rating models used by Delphi:\nER = (TC \u2212 TSC)(1 +AT )m12 \u00b7 xmxbxd+ nsxp +BCp(1 +ATL) m 12 \u00b7 xphxgpxdpxip \u00b7mm\nMR = [BCmed(1 +ATmed) m 12 \u00b7 xgmxdmximxudm+\nBCcap(1 +ATmed) m 12+\nBCph(1 +ATph) m 12 \u00b7 xgphxdphxiphxudph] \u00b7mm\nThe definitions of independent variables in these two equations are shown in Table 1. The experience rate is linear in terms of the total claims (TC), and is combined with the manual rate and the group\u2019s member months (Eqn. 1).\nTo improve accuracy, we modeled cost at both the individual and group levels. We used a sequence of two models: (1) Individual-level model predicting per month cost for a given member and (2) Group-level model predicting per member per month (pmpm) cost for a given group. Our approach contrasts with traditional actuarial methods which are heavily focused on group-level cost and lack individuallevel information within each group.\nFeature Engineering We reshaped the claims and eligibility tables into longitudinal patient records per our proprietary data format, the \u201cLumiata Data Model\u201d (LDM - see Appendix). From the LDM, we created member-level features (input variables) using information before the blackout period, based on techniques from the literature (e.g.\nRazavian et al. 2015; Tamang et al. 2017). Our demographic features were age and sex; all other features were time-dependent. Our time windows to compute a variety of features (e.g. diagnosis, medication, procedure, lab, and revenue codes, and cost and coverage) were: \u201clast three months\u201d, \u201clast six months\u201d, \u201clast one year\u201d, and \u201canytime\u201d prior to the blackout date.\nIn addition to ICD-9, ICD-10, CPT, NDC, and LOINC codes, we transformed the codes into their grouped counterparts based on organ-type (SNOMED), condition categories (HCUP, CMS-HCC and HHS-HCC), drug molecule (RxNorm and ATC), and our proprietary clinical grouper (Lumiata disease code). We derived features from these codes by calculating the log count of every unique code in each time window, and the summary statistics of each system (total count, unique code count, minimum count, maximum count, mean count, etc) in the \u201canytime\u201d window. The presence of revenue codes (binary) was computed in the \u201canytime\u201d window.\nWe computed features using their observed lab interpretations or values from the LOINC codes. These include (1) log counts of interpretations, i.e. \u201chigh\u201d, \u201clow\u201d, \u201cabnormal\u201d, and \u201cnormal\u201d, (2) whether the value was increasing, decreasing or flat across time points for the same test (one-hot encoded), and (3) if the interpretation was fluctuating across time (binary). For (2), the calculation was based on a t-test\u2019s p-value of a simple linear regression\u2019s slope.\nCost features are the most powerful features to predict future cost. In addition to the cumulative allowed cost in different time windows, we computed cost attributed to different care settings. The length of coverage was computed for all time windows. In total, we constructed more than 5 million possible features per patient. The resulting feature matrix was very sparse; most of the columns had no values at all. We reduced the dimension of the feature space before training a model using feature selection techniques discussed below.\nIndividual-Level Model We regressed our first model on the allowed amount per month during the projection period for each member. We trained a gradient boosting tree that optimized the mean squared error (MSE) using the LightGBM package (Ke et al. 2017) in the Python programming language (Guttag 2016). To speed up training time and reduce over-fitting, we tested a variety of feature prevalence thresholds to reduce the features set to \u2264100,000. We defined the prevalence of a feature as the fraction of non-zero values for this feature across patients in the training set. The model implicitly selected useful features to split on; the typical number of features was \u223c 4000 (see Table 2).\nWe ran the model recursively, using a different threshold (thus a different number of features) in each iteration. Evaluating on the test set reached a performance plateau at a threshold of \u223c 0.001, which we used for all subsequent models. We used the test set data for hyper-parameter tuning and early-stopping. Using the trained model, we made cost predictions for all the individuals in the train, test, evaluate, and holdout sets.\nMembers enrolled as of 08/31/2016 were included in our\ntrain, test, and evaluate sets. Members who dropped out during the blackout period were filtered out before training; we did not train on a member\u2019s data if this member was not enrolled in a group on 01/01/2017. We only omitted these members during training, not during inference.\nAggregation Our individual model predicts the per month cost in the projection period for the members who were enrolled in groups at the end of experience period. For our train, test, and evaluate sets, this date was 08/31/2016. For the holdout set, the slice date depended on the groups renewal date. We then aggregated these predictions based on the members active on this date to obtain the mean of member-level cost predictions for each group. This quantity has a unit of pmpm and became the input of our group-level model described below.\nGroup-Level Adjustment The aggregated mean prediction of a group can be thought of as predicted pmpm cost. However, this quantity only considers the members enrolled at the end of the experience period and assumes enrollment remains constant throughout the projection period. In re-\nality, a group can grow or shrink during the blackout and projection periods, affecting the true pmpm. We regressed a second model on the true group-level pmpm cost to adjust the aggregated predictions. We experimented with different group-level features in this model and use the following (per group): (i) mean cost of member-level predictions, (ii) mean member age, (iii) total number of member months for the group during the experience period, (iv) \u201cgrowth\u201d feature, defined as the change in the number of members during the experience period divided by the total number of member months, (v) average length of member coverage, (vi) fraction of experience period costs that were incurred during the final four months, before the blackout period (vii) fraction of high-cost members, defined as someone whose cost falls within the top 10% of all members in the training set. We then trained a LightGBM model that optimized pmpm Mean Absolute Error (MAE) and used the test set to perform hyper-parameter tuning and earlystopping. The mean of member-level cost predictions for each group highly correlates with the overall target pmpm cost for the group. With the additional features, the grouplevel model improved pmpm MAE by \u223c 10% compared to the individual-level model alone.\nSimilar to the individual model, we trained the group model on groups still active during the projection period because only the groups that remained active will be included when evaluating the results. This operation was only done in model training and was not performed when doing inference using the trained model."
    },
    {
      "heading": "Model Evaluation",
      "text": "ML Metrics Evaluation We adapted the standard metrics R-squared (R2), MAE, and Gini index (Frees, Meyers, and Cummings 2011) to predicted pmpm cost; evaluation metrics are measured by comparing \u201ctrue pmpm cost\u201d and \u201cpredicted pmpm cost\u201d. Lumiata received claims data at the allowed amount level to preserve Delphi\u2019s pharmaceutical and provider reimbursement rates, whereas Delphi\u2019s production models predict the \u201cpaid amount\u201d for employer groups. As a result, Delphi and Lumiata modeled two different types of cost, which are not directly comparable because the allowed amount is usually greater than the paid amount (though they are correlated; see Hileman and Steele 2016). Both teams agreed on two solutions to make the predictions comparable. First, we computed \u201cnormalized\u201d pmpm MAE, which equals pmpm MAE divided by the \u201cglobal pmpm cost\u201d. The global pmpm cost is the total (allowed or paid) amount divided by the total number of member months across all groups. Second, we computed the trend versions of our predictions. The allowed trend was defined as the pmpm allowed amount in the projection period divided by the pmpm allowed amount in the experience period; the \u201cpaid trend was defined similarly. Trend calculations are ubiquitous in actuarial science (SOA 2017).\nLift Plot and Concession Opportunities While ML performance metrics like MAE and R2 are ubiquitous in the tech industry, they often lack direct connection to concrete business-level Key Performance Indicators (KPIs) in other\nfields (Henke, Levine, and McInerney 2018). To address this gap, we computed a KPI called a \u201clift plot\u201d, illustrating the implications of Lumiata\u2019s model in a production context. The lift plot is made using the following steps: (1) compute Lumiata\u2019s predicted allowed trend divided by Delphi\u2019s predicted paid trend (the \u201ctrend ratio\u201d), (2) rank groups according to the trend ratio, (3) segment the ranked groups into deciles, (4) compute the actual-to-expected (A/E) ratios within each decile, where \u201cA\u201d is the true paid amount of a given decile, and \u201cE\u201d is Delphi\u2019s predicted paid amount of the same decile, (5) normalize the A/E of all the ten deciles by the global A/E of the holdout set, and (6) plot the \u201coracle\u201d model (a model that perfectly predicts pmpm allowed trend). The predicted paid amount of a given decile is the sum of the predicted pmpm paid amount times the projected member months for all groups in this decile. The projected member months of the projection period is 12 months times the number of members at the end of the experience period.\nA group is considered to be a \u201cconcession opportunity at the 5% level\u201d if the group\u2019s rates can be reduced by 5% while maintaining profitability (A/E remains <1). In other words, the true paid trend ratio (true pmpm paid trend divided by Delphis predicted pmpm paid trend) is < 0.95. We used precision and recall to evaluate concession opportunity identification performance (Powers 2008). In order to identify concession opportunities at the 5% level, we applied a decision rule of < 1 using a predicted \u201ctrend ratio\u201d (Lumiata\u2019s predicted pmpm allowed trend divided by Delphis predicted pmpm paid trend). That is, if Lumiata\u2019s predicted pmpm allowed trend was lower than Delphis predicted pmpm paid trend for a group, we asserted that group was a concession opportunity at the 5% level. We used a decision rule of < 1 instead of < 0.95 in order to achieve a higher recall."
    },
    {
      "heading": "Results",
      "text": "Model Performance Lumiata\u2019s model was 20% better in normalized pmpm MAE, 26% better in pmpm R2 than Delphis model, and 2% lower in Gini index than Delphi\u2019s model (Table 3). Lumiata correctly identified 84% of the groups in the holdout set that had concession opportunities at the 5% level.\nLumiata\u2019s predicted pmpm allowed trend had a 65% precision and 84% recall in identifying concession opportunities at the 5% level, and a 56% precision and 85% recall in identifying concession opportunities at the 10% level. For comparison, an \u201coracle\u201d model had an 84% precision and 96% recall at the 5% level, and a 73% precision and 97% recall at the 10% level. This similarity in precision and recall\nindicates Lumiata\u2019s model was near optimal for predicting concession opportunities at the 5% level.\nPractical Application Operationalizing Lumiata\u2019s model relied on the \u201cstop-light principle\u201d to make the model output interpretable to non-data scientists. The decision process derived from Lumiata\u2019s model is: (1) If Lumiata\u2019s predicted trend is less than Delphi\u2019s predicted trend, then the model suggests an underwriter give a concession of at least 5% on that group\u2019s renewal quote (Green). (2) If Lumiata\u2019s predicted trend is equal to or greater than Delphi\u2019s predicted trend, no action is taken (Yellow or Red, see Figure 3).\nThe lift plot in Figure 4 shows the result of using Lumiata\u2019s predicted pmpm allowed trend in this decision process. The average A/E per decile of Delphi\u2019s model varies with the decile of Lumiata trend ratio (Lumiata\u2019s predicted allowed trend divided by Delphi\u2019s predicted paid trend) and true allowed trend ratio (true allowed trend divided by Delphi\u2019s predicted paid trend), respectively. The A/E of the bottom five Lumiata trend ratio deciles are below 0.95, meaning Lumiata\u2019s trend ratio can select groups for a rate drop of \u2265 5% with good accuracy. The median trend ratio was 0.89 which yielded higher precision than a decision rule of 1.0.\nHad Delphi implemented our model for this pricing period, their underwriters could have dropped the renewal quote 5% or more for approximately half of the groups while retaining profitability. An \u201coracle\u201d model (blue line in Figure 4) identified the same number of decile concession opportunities at the 5% level as our model (five deciles)."
    },
    {
      "heading": "Deployment",
      "text": "The goals of Lumiata\u2019s pilot study with Delphi were to prove Lumiata\u2019s (1) ML could improve over an established industry methodology, and (2) tech stack could deliver monthly predictions for Delphi\u2019s renewal business groups. We designed a Kaggle-style competition2 between the two companies, with two holdout sets - a preliminary holdout set (658 groups) and a final holdout set (which we call \u201choldout set\u201d throughout this paper - 648 groups). We had one chance to compare our predictions to Delphi\u2019s on the final holdout set; the success or failure of the pilot study was predicated on whose model had the best group-level cost predictions.\nQuality Assurance Due to several delicate calculations needed to assemble the predicted/true allowed trend predictions, we computed non-prediction fields to rule out nondata science confounding factors before running the results analysis. These fields included (per group): (i) number of members enrolled at the end of experience period, (ii) number of member months in the experience period, (iii) true allowed amount in the experience period, and (iv) predicted allowed amount in the projection period. After receiving the censored information, we computed the: (i) number of members at the beginning of projection period, (ii) number of member months in the projection period, and (iii) true allowed amount in the projection period. Due to a strong data model built off of FHIR3 and solid compute infrastructure, we were able to iterate and fix bugs quickly, until our calculations of the experience period non-prediction columns matched Delphi\u2019s within 5%. Completion of this analysis allowed quick and self-evident comparison of Lumiatas and Delphis models performance metrics. Additionally, the nonprediction fields found their way into roll-out plan below.\nData Challenges For model comparison, we had to address discrepancies between Lumiata\u2019s and Delphi\u2019s claims data sets. Two major differences in the claims data were: (1) Delphis models used paid amount and Lumiata used allowed amount, and (2) Delphi used the \u201cpaid date\u201d, but requested Lumiata use the \u201cencounter date\u201d for allowed amounts (Delphi felt \u201cencounter date\u201d was more appropriate for patientlevel cost predictions). To solve these issues, we computed the \u201callowed trend\u201d compared to the \u201cpaid trend\u201d (Figure 4). Calculating the \u201callowed trend\u201d required we compute three other quantities per group (\u201cnumber of members at the end of experience period\u201d, \u201cmember months in the experience period\u201d, and \u201callowed amount in the experience period\u201d), and the predicted allowed amount for the group. Accurately\ncomputing these quantities was more difficult than expected due to the consequences of dual paid/allowed conventions and time-dependent patient enrollment:\n(i) Calculating the \u201callowed amount in the experience period\u201d differs depending on whether \u201cpaid date\u201d or \u201cencounter date\u201d is used. Using the paid vs encounter date segmented claims differently into a groups experience, blackout, and projection periods. For example, some claims were denied or paid claims could be reversed (see Appendix Figure A1). Hence, claims data filtered on encounter date in the holdout set experience period were sometimes absent from the unblinded holdout set\u2019s experience period.\n(ii) The number of members enrolled in a group at the end of the experience period could change during the blackout and projection periods. For example, members can shift between different groups due to job or spousal health plan changes. Also, enrollment was updated on the 15th of the month, but we calculated \u201cnumber of members at end of experience period\u201d and \u201cmember months in the experience period\u201d for the first of each month, making our enrollment data two weeks out of date.\nRoll-out Strategy Delphi requested Lumiata provide monthly group-level cost predictions and concession opportunities for groups up for renewal within the next four months (Delphi renews groups throughout the year). Delphi asked for a seven day cost-prediction turn around time and fixed model feature set each quarter. In response, we built a platform that can create LDMs for 1 TB of claims data in under an hour and produce highly optimized group-level cost predictions in under four hours.\nTo streamline deployment, we proposed the following plan to Delphi: (1a) Each month, Delphi sends a moving three year snapshot of their claims and eligibility tables. (1b) Delphi sends their non-prediction field calculations and their paid trend group cost predictions for groups up for renewal. (2) Lumiata updates the existing patients LDMs and adds new patients from the eligibility file. (3) Lumiata updates the group cost prediction model, training using the prior 12 months as the projection period, four months prior as the blackout, and prior 14 months as the experience period. (4) Lumiata creates feature vectors including the new projection/blackout period claims information. (5) Lumiata applies the updated member- and group-level models to the claims data to produce group-level allowed trend predictions. (6) Using Delphi\u2019s paid trend predictions from 1b and the \u201cstoplight\u201d principle (Figure 3), Lumiata recommends whether to drop the rate by 5% for each group. (7) Lumiata sends its non-prediction attributes and the allowed trend predictions to Delphi with one recommendation per group up for renewal. All non-prediction fields must agree within \u2264 5% between Delphi and Lumiata\u2019s calculations. (8) Delphi verifies receipt of the data and the results are consumed by their actuarial and underwriting teams. All files are transferred to and from Lumiata\u2019s platform (hosted on Google Cloud Platform) using sFTP.\nTransparent Rate Setting Actuarial models have an essential property: they are \u201cexplainable\u201d because a prediction can be decomposed into discrete multiplicative factors with\nan inherent interpretation. For example, \u201cgeographic area factor\u201d = 0.9 means people in a particular zip code cost 10% less than the mean, so the base rate is adjusted (multiplied) by 0.9 for members from this zip code. This degree of explainability is crucial because actuaries need to file rates annually for individual/small group markets with the state insurance commissioner to ensure the factors used to produce the rate are compliant with legal guidelines. Furthermore, an underwriter needs to be able to explain how she arrived at a particular rate to a customer. A critique of ML models is that they lack explainability in terms of what input variables may have contributed to a particular prediction (Rudin 2019). However, explainable ML in healthcare is must-have, touching upon fundamental issues of bias, transparency, and reasonableness of ML model predictions.\nShapely values, a game theoretic algorithm, was developed to enable common machine-learning algorithms to output a set of feature weights specific to a prediction (Lundberg and Lee 2017). We applied the Python package SHAP4 to our LightGBM models to yield member-level explanations. The weights are interpreted as the dollar value pmpm amounts and the sum of the values equals the prediction made by the original model. Similar to an actuarial formula, the rate predicted by the group-level model can be expressed in terms of aggregated member-level SHAP values for member-level features. We often found that \u2264500 features\u2019 SHAP values account for 95% of a cost prediction, for each group. However, the specific features involved varied by group.\nThe transparency afforded by the group-level SHAP values provides the opportunity to explain a rate adjustment to a customer in dollar pmpm amounts using the specific risk drivers for that group and modify a rate given by the ML model by the expected change in cost for specific drugs and services. For instance, if the price of Glipizide, a drug to treat type-2 diabetes, will drop for an insurer next year by 20%, the insurer can multiply the SHAP values corresponding to Glipizide-related pharmacy costs by 0.80 for all the members on Glipizide, thus lowering the projected rate. These mechanics are similar to current actuarial methods, making them easy to implement. Furthermore, greater model transparency could increase patient adherence to prescribed medications. As drug prices rise and more patients purchase high-deductible plans, patients have to pay higher out of pocket costs for drug treatment, and patient medication adherence declines (Callaghan et al. 2019). Insurers can use the SHAP values from the patient-level model to identify drugs driving up projected cost for that group, and suggest the prescribing doctor offer a lower cost alternative drug with similar efficacy. This provides a win-win opportunity, lowering drug cost for the payer, and improving patient adherence to the cheaper drug through increased affordability."
    },
    {
      "heading": "Discussion",
      "text": "Here, we demonstrate that: (1) ML approaches can significantly improve the accuracy and efficiency of group health insurance underwriting and (2) ML models can offer comparable interpretability to traditional actuarial methods. Our contributions provide clear direction for how to improve the\nefficiency and predictive performance of underwriting for employer-based insurance and how to lower the cost for members in groups of any size. Our ML-based approach improved MAE over actuarial models across the book of business: >500-member groups showed an improved performance (see Figure 5).\nOur model shows the most improvement over actuarial models in situations where the group size is \u2264500 and/or the group claims experience is relatively short (<8 months). In these situations, the groups are not yet credible, so actuarial models perform sub-optimally. We believe the success of our model was due to modeling costs: (1) at the individual level; by contrast, actuarial methods aggregate medical history across group members, (2) with models that perform well with skewed distributions; the cost of care in an insurance population is often gamma distributed, making the ML method of gradient boosting trees, like LightGBM, highly effective (Zhou, Yang, and Qian 2018), (3) using a modelagnostic approach to select features relevant for predictions, and (4) by combining individual- and group-level models to produce the final predictions.\nAs ML models are increasingly compared to more traditional statistical techniques, the most appropriate study design and model evaluation metrics should be examined. For example, the holdout set data was \u201cout of sample\u201d (i.e. using patients/groups unseen before by the model) but not \u201cout of time\u201d (i.e. projecting costs for time periods subsequent to the 2015-2017 data years). Furthermore, claims data are not time-stationary (e.g. new drugs and treatments will be developed), so the expected model performance may not be perfectly realized in practice, but the relative difference between the models should hold. Also, we obtained a slightly worse Gini index than Delphi, despite our much better R2, MAE, and lift plot (Table 3 and Figure 4). This discrepancy can occur because the Gini index is a ranking-based metric, whereas a regression model minimizes the prediction errors. One difficulty is the Gini index is not a differentiable quantity. Future work should develop algorithms to address this problem.\nData quality was crucial to our success. Alignment on non-prediction fields between Lumiata and Delphi ruled out errors in the data, pipeline, or output, improving communication across teams and increasing efficiency. These calculations must be automated for developing and productionalizing a medical underwriting ML application, due to the large size of data sets and rapid turnaround of results.\nDue to its highly applied nature, some operational realities limit our study\u2019s evaluation. A challenge for validating our predictions is the long feedback cycle (20 months). Also, not all of Lumiata\u2019s concession recommendations could be granted due to a variety of quantitative and judgement factors under the underwriter and insurer\u2019s discretion.\nAdditionally, we could not determine if our individuallevel model was racially biased, because we did not receive patient ethnicity data. Avoiding racial bias is important as previous studies have found evidence of racial bias in commercial cost prediction models used for clinical management (Obermeyer et al. 2019). Historically, poorer minorities under-utilized healthcare services due to mistrust of the system and confusion about how to navigate it (Obermeyer et al. 2019). However, because our response variable is not a clinical outcome but a financial one, we think this effect on pricing may be less significant. More work will be needed to better understand the effect of pricing insurance more affordably for minority patients, predicated on their less frequent utilization of the healthcare system.\nIn practice, ML approaches can help insurers be more competitive, avoiding adverse risk. It can result in the design of more \u201cexotic\u201d funding arrangements due to better predictive power of patient health, following the industry trend towards capitated payments (Lee, Majerol, and Burke 2019).\nUnlike previous ML models in healthcare (Rudin 2019), our model output is interpretable by a non-technical user, simplifying operationalization (Figure 3). A user does not need to understand the inner workings of our algorithm to apply our output as a multiplicative adjustment factor to their existing actuarial models and can output the most important group-specific risk factors."
    },
    {
      "heading": "Conclusions",
      "text": "Machine learning on insurance claims data provides a powerful tool to improve the efficiency and affordability of plans and care offered to patients enrolled in employer-sponsored health plans. With more accurate rate-setting, health insurance companies can design nuanced plan attributes, reducing the cost of care for their members. Our ML model achieved 20% improved accuracy in absolute predictive performance over traditional actuarial methods and was able to identify over 80% of new concession opportunities available to Delphi. This allows underwriters to better price and retain <500 employer group customers. This study can be used by payers to give underwriters improved pricing guidance, retaining business and giving a better and more affordable experience to members."
    },
    {
      "heading": "Acknowledgements",
      "text": "We thank our counterparts at Delphi for collaboratively working with us to validate our model against a productiongrade model with real data. Additionally, we thank the following people for their contributions: Kim Branson, Vatshank Chaturvedi, Hilaf Hasson, Renzo Frigato, Dilawar Syed, Laika Kayani, Wil Yu, Shahab Hassani, Alexandra Pettet, Derek Gordon, Arnold Lee, Ash Damle, Thomas Watson, Leon Barovic, Diana Rypkema and our investors at Blue Cross Blue Shield Venture Fund, and Khosla Ventures.\nNotes 1https://fortune.com/fortune500/2019/ 2https://www.kaggle.com/ 3https://wiki.hl7.org/FHIR 4https://github.com/slundberg/shap"
    },
    {
      "heading": "Appendix",
      "text": ""
    },
    {
      "heading": "Data Quality",
      "text": "The following describes some implications of modeling individual-level cost when the data model, from which the data is derived, considers the employer group as the primary entity.\n\u2022 The cost of a claim depends entirely on what date you are inspecting the claim. The example in Figure A1 shows hows, depending on the time period, a claim which has been reversed has a material impact on the total allowed amount we calculate. A corollary is that calculations done to a claims dataset which has been filtered on a date (so that all the claims occur prior to that date) can change as data after the filter date is added back in.\n\u2022 Must consider \u201calteration of training set\u201d implications due to modeling at group-level. Members can move between groups because they change jobs or move to their partner\u2019s health plan. This raises the potential of a member being in both the training and testing data. Of course, the solution here, as above, is to remember that enrollment is only relevant with respect to a particular date, since enrollment can change.\n\u2022 Finally, a rigorous accounting of the (1) count of member, (2) count of groups, and (3) total cost per group, from the insurance carrier\u2019s side, to receipt of raw data, to generating cost predictions from the pipeline, and finally to sending predictions back to the insurance carrier, is crucial to ensure the credibility of the performance estimates asserted by the model training process.\n08/01/2015 08/01/2016\nClaim ID Allowed Amount Start Date Paid Date\n1 $3000 06/25/2016 07/20/2016\n1 -$3000 06/25/2016 08/15/2015\nexperience period censored\nFigure A1: An example of a reversed claim that is seen differently due to censored data. In this example, the blackout period begins on 08/01/2016, so all the claims with paid dates on or after this date are removed before we received the data. In reality, claim \u201c1\u201d was originally incurred and paid before 08/01/2016 but was reversed on a paid date after 08/01/2016. However, because the data were censored based on paid dates, we saw that claim \u201c1\u201d had an allowed amount of $3000. Once we receive the uncensored full claims data, we would see that claim \u201c1\u201d had actually been reversed and therefore cost $0."
    },
    {
      "heading": "Lumiata Data Model",
      "text": "The Lumiata Data Model (LDM) is the standard format in which all Lumiata client data is formatted. The purpose of this format is to consolidate all relevant patient information into a single place, with one record per patient. The schema of LDM is shown below: root |-- MBR: string (nullable = true) |-- patient: struct (nullable = true) | |-- patient_id: string (nullable = true) | |-- birthday: date (nullable = true) | |-- gender: string (nullable = true) | |-- marital_status: string (nullable = true) | |-- postal_code: string (nullable = true) | |-- dead: boolean (nullable = true) | |-- extensions: array (nullable = true) | | |-- element: struct (containsNull = true) | | | |-- url: string (nullable = true) | | | |-- valueString: string (nullable = true) | | | |-- valueDate: date (nullable = true) | | | |-- valueDecimal: float (nullable = true) | |-- population_type: string (nullable = true) |-- practitioners: array (nullable = true) | |-- element: struct (containsNull = true) | | |-- resource_id: string (nullable = true) | | |-- npi: string (nullable = true) | | |-- specialty_codes: array (nullable = true) | | | |-- element: string (containsNull = true) |-- terms: array (nullable = true) | |-- element: struct (containsNull = true) | | |-- resource_id: string (nullable = true) | | |-- date: date (nullable = true) | | |-- resource_type: string (nullable = true) | | |-- code: struct (nullable = true) | | | |-- code: string (nullable = true) | | | |-- system: string (nullable = true) | | |-- day_supply: integer (nullable = true) | | |-- display: string (nullable = true) | | |-- value: string (nullable = true) | | |-- interpretation: string (nullable = true) | | |-- units: string (nullable = true) | | |-- original_code: string (nullable = true) | | |-- practitioners: array (nullable = true) | | | |-- element: string (containsNull = true) | | |-- resource_id_list: array (nullable = true) | | | |-- element: string (containsNull = true) | | |-- lab_info: struct (nullable = true) | | | |-- actual_value: string (nullable = true) | | | |-- description: string (nullable = true) | | | |-- comparator: string (nullable = true) | | | |-- reference_range: string (nullable = true) | | | |-- low_val: float (nullable = true) | | | |-- high_val: float (nullable = true) | | | |-- abnormal_result_description: string\n(nullable = true) |-- claims: array (nullable = true) | |-- element: struct (containsNull = true) | | |-- resource_id: string (nullable = true) | | |-- paid_date: date (nullable = true) | | |-- start_date: date (nullable = true) | | |-- end_date: date (nullable = true) | | |-- fill_date: date (nullable = true) | | |-- billed_amount: float (nullable = true) | | |-- allowed_amount: float (nullable = true) | | |-- member_cost_share: float (nullable = true) | | |-- cob_amount: float (nullable = true) | | |-- paid_amount: float (nullable = true) | | |-- capitation_type: string (nullable = true) | | |-- major_service_category: string (nullable = true) | | |-- service_major: string (nullable = true) | | |-- coverage: string (nullable = true) | | |-- place_type: string (nullable = true) | | |-- surcharge_amount: float (nullable = true) | | |-- terms: array (nullable = true) | | | |-- element: struct (containsNull = true) | | | | |-- resource_id: string (nullable = true) | | | | |-- date: date (nullable = true) | | | | |-- resource_type: string (nullable = true) | | | | |-- code: struct (nullable = true) | | | | | |-- code: string (nullable = true) | | | | | |-- system: string (nullable = true) | | | | |-- day_supply: integer (nullable = true) | | | | |-- display: string (nullable = true) | | | | |-- value: string (nullable = true) | | | | |-- interpretation: string (nullable = true) | | | | |-- units: string (nullable = true) | | | | |-- original_code: string (nullable = true) | | | | |-- practitioners: array (nullable = true)\n| | | | | |-- element: string (containsNull = true) | | | | |-- resource_id_list: array (nullable = true) | | | | | |-- element: string (containsNull = true) | | | | |-- lab_info: struct (nullable = true) | | | | | |-- actual_value: string (nullable = true) | | | | | |-- description: string (nullable = true) | | | | | |-- comparator: string (nullable = true) | | | | | |-- reference_range: string (nullable = true) | | | | | |-- low_val: float (nullable = true) | | | | | |-- high_val: float (nullable = true) | | | | | |-- abnormal_result_description: string (nullable = true) | | |-- prescription_written_date: date (nullable = true) | | |-- group_number: string (nullable = true) | | |-- specialty_codes: string (nullable = true) | | |-- revenue_codes: string (nullable = true) | | |-- is_capitation: boolean (nullable = true) | | |-- claim_id: string (nullable = true) |-- coverage: array (nullable = true) | |-- element: struct (containsNull = true) | | |-- resource_id: string (nullable = true) | | |-- group_number: string (nullable = true) | | |-- branch_code: string (nullable = true) | | |-- start_date: date (nullable = true) | | |-- end_date: date (nullable = true) | | |-- account_effective_date: date (nullable = true) | | |-- account_termination_date: date (nullable = true) | | |-- medical_plan_type: string (nullable = true) | | |-- beneficiary_id: string (nullable = true) | | |-- account_state: string (nullable = true) | | |-- account_coverage_type: string (nullable = true) | | |-- account_renewal_month: string (nullable = true) | | |-- account_name: string (nullable = true) | | |-- scp_copay: float (nullable = true) | | |-- pcp_copay: float (nullable = true) | | |-- pcp_coinsurance: float (nullable = true) | | |-- scp_coinsurance: float (nullable = true) | | |-- individual_deductible: float (nullable = true) | | |-- collective_deductible_indicator: string (nullable = true) | | |-- inpatient_coinsurance: float (nullable = true) | | |-- outpatient_coinsurance: float (nullable = true) | | |-- individual_oop_maximum: float (nullable = true) | | |-- medical_member_cost_share_percentage: float (nullable = true) | | |-- pharmacy_member_cost_share_percentage: float (nullable = true) | | |-- total_member_cost_share_percentage: float (nullable = true) | | |-- product_type: string (nullable = true) | | |-- relationship: string (nullable = true) | | |-- industry_code: string (nullable = true) | | |-- pharmacy_coverage_indicator: string (nullable = true) | | |-- effective_date: date (nullable = true) | | |-- cancel_date: date (nullable = true)"
    }
  ],
  "title": "Accurate and Interpretable Machine Learning for Transparent Pricing of Health Insurance Plans",
  "year": 2020
}
