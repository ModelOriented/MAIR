{"abstractText": "Motivated by concerns surrounding the fairness effects of sharing and transferring fair machine learning tools, we propose two algorithms: Fairness Warnings and Fair-MAML. The first is a modelagnostic algorithm that provides interpretable boundary conditions for when a fairly trained model may not behave fairly on similar but slightly different tasks within a given domain. The second is a fair meta-learning approach to train models that can be quickly fine-tuned to specific tasks from only a few number of sample instances while balancing fairness and accuracy. We demonstrate experimentally the individual utility of each model using relevant baselines and provide the first experiment to our knowledge of K-shot fairness, i.e. training a fair model on a new task with only K data points. Then, we illustrate the usefulness of both algorithms as a combined method for training models from a few data points on new tasks while using Fairness Warnings as interpretable boundary conditions under which the newly trained model may not be fair.", "authors": [{"affiliations": [], "name": "Dylan Slack"}, {"affiliations": [], "name": "Sorelle A. Friedler"}, {"affiliations": [], "name": "Emile Givental"}], "id": "SP:2f5379e0974c16f43f9728dd387eb8fe8f7403f7", "references": [{"authors": ["Solon Barocas", "Moritz Hardt", "Arvind Narayanan"], "title": "Fairness and Machine Learning. fairmlbook.org", "year": 2018}, {"authors": ["Solon Barocas", "Andrew D"], "title": "Selbst. 2016", "venue": "Big data\u2019s disparate impact. Calif. L. Rev", "year": 2016}, {"authors": ["Richard Berk", "Hoda Heidari", "Shahin Jabbari", "Matthew Joseph", "Michael Kearns", "Jamie Morgenstern", "Seth Neel", "Aaron Roth"], "title": "A Convex Framework for Fair Regression", "year": 2017}, {"authors": ["Steffen Bickel", "Michael Br\u00fcckner", "Tobias Scheffer"], "title": "Discriminative Learning Under Covariate Shift", "venue": "J. Mach. Learn. Res", "year": 2009}, {"authors": ["Toon Calders", "Sicco Verwer"], "title": "Three naive Bayes approaches for discrimination-free classification", "venue": "Data Mining and Knowledge Discovery 21,", "year": 2010}, {"authors": ["Alexandra Chouldechova"], "title": "Fair prediction with disparate impact: A study of bias in recidivism prediction instruments", "venue": "Big data 5,", "year": 2017}, {"authors": ["Alexandra Chouldechova", "Aaron Roth"], "title": "The Frontiers of Fairness in Machine Learning", "year": 2018}, {"authors": ["Ang\u00e8le Christin"], "title": "Algorithms in practice: Comparing web journalism and criminal justice", "venue": "Big Data & Society 4,", "year": 2017}, {"authors": ["Amanda Coston", "Karthikeyan Natesan Ramamurthy", "Dennis Wei", "Kush R Varshney", "Skyler Speakman", "Zairah Mustahsan", "Supriyo Chakraborty"], "title": "Fair transfer learning with missing protected attributes", "venue": "In Proceedings of the AAAI/ACM Conference on Artificial Intelligence,", "year": 2019}, {"authors": ["Cynthia Dwork", "Moritz Hardt", "Toniann Pitassi", "Omer Reingold", "Richard Zemel"], "title": "Fairness Through Awareness", "venue": "In Proceedings of the 3rd Innovations in Theoretical Computer Science Conference (ITCS \u201912)", "year": 2012}, {"authors": ["Cynthia Dwork", "Nicole Immorlica", "Adam Tauman Kalai", "Max Leiserson"], "title": "Decoupled classifiers for group-fair and efficient machine learning", "venue": "In Conference on Fairness, Accountability and Transparency", "year": 2018}, {"authors": ["Michael Feldman", "Sorelle A Friedler", "JohnMoeller", "Carlos Scheidegger", "Suresh Venkatasubramanian"], "title": "Certifying and removing disparate impact", "venue": "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining", "year": 2015}, {"authors": ["Chelsea Finn", "Pieter Abbeel", "Sergey Levine"], "title": "Model-Agnostic Meta- Learning for Fast Adaptation of Deep Networks", "venue": "In Proceedings of the 34th International Conference on Machine Learning (Proceedings of Machine Learning Research), Doina Precup and Yee Whye Teh (Eds.),", "year": 2017}, {"authors": ["Friedler", "Scheidegger", "Venkatasubramanian", "Choudhary", "Hamilton", "Roth"], "title": "A comparative study of fairness-enhancing interventions in machine learning", "venue": "In ACM Conference on Fairness, Accountability and Transparency (FAT*). ACM", "year": 2019}, {"authors": ["Moritz Hardt", "Eric Price", "Nathan Srebro"], "title": "Equality of Opportunity in Supervised Learning", "venue": "In Proceedings of the 30th International Conference on Neural Information Processing Systems (NIPS\u201916). Curran Associates Inc.,", "year": 2016}, {"authors": ["Lingxiao Huang", "Nisheeth Vishnoi"], "title": "Stable and Fair Classification", "venue": "In Proceedings of the 36th International Conference on Machine Learning (Proceedings of Machine Learning Research),", "year": 2019}, {"authors": ["Nathan Kallus", "Angela Zhou"], "title": "Residual Unfairness in Fair Machine Learning from Prejudiced Data", "venue": "In Proceedings of the 35th International Conference on Machine Learning (Proceedings of Machine Learning Research), Jennifer Dy and Andreas Krause (Eds.),", "year": 2018}, {"authors": ["Toshihiro Kamishima", "Shotaro Akaho", "Jun Sakuma"], "title": "Fairness-aware classifier with prejudice remover regularizer", "venue": "In Joint European Conference on Machine Learning and Knowledge Discovery in Databases", "year": 2012}, {"authors": ["Chao Lan", "Jun Huan"], "title": "Discriminatory Transfer. Workshop on Fairness, Accountability, and Transparency in Machine Learning (2017)", "year": 2017}, {"authors": ["Zachary C. Lipton", "Yu-Xiang Wang", "Alexander J. Smola"], "title": "Detecting and Correcting for Label Shift with Black Box Predictors", "year": 2018}, {"authors": ["David Madras", "Elliot Creager", "Toniann Pitassi", "Richard Zemel"], "title": "Learning Adversarially Fair and Transferable Representations", "venue": "International Conference on Machine Learning", "year": 2018}, {"authors": ["Marco Tulio Ribeiro", "Sameer Singh", "Carlos Guestrin"], "title": "Why Should I Trust You?\": Explaining the Predictions of Any Classifier", "venue": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data", "year": 2016}, {"authors": ["Andrea Romei", "Salvatore Ruggieri"], "title": "A multidisciplinary survey on discrimination analysis", "venue": "The Knowledge Engineering Review 29,", "year": 2014}, {"authors": ["Candice Schumann", "Xuezhi Wang", "Alex Beutel", "Jilin Chen", "Hai Qian", "Ed H. Chi"], "title": "Transfer of Machine Learning Fairness across Domains", "year": 2019}, {"authors": ["Dylan Slack", "Sorelle A. Friedler", "Carlos Eduardo Scheidegger", "Chitradeep Dutta Roy"], "title": "Assessing the Local Interpretability of Machine Learning Models. Workshop on Human-Centric Machine Learning, NeurIPS (2019)", "year": 2019}, {"authors": ["Megan T. Stevenson"], "title": "Assessing Risk Assessment in Action", "venue": "Minnesota Law Review", "year": 2017}, {"authors": ["Adarsh Subbaswamy", "Peter G. Schulam", "Suchi Saria"], "title": "Preventing Failures Due to Dataset Shift: Learning Predictive Models That Transport", "venue": "In AISTATS", "year": 2018}, {"authors": ["Berk Ustun", "Cynthia Rudin"], "title": "Supersparse linear integer models for optimized medical scoring systems", "venue": "Machine Learning", "year": 2015}, {"authors": ["Berk Ustun", "Cynthia Rudin"], "title": "Learning Optimized Risk Scores", "venue": "Journal of Machine Learning Research 20,", "year": 2019}, {"authors": ["Oriol Vinyals", "Charles Blundell", "Timothy P. Lillicrap", "Koray Kavukcuoglu", "Daan Wierstra"], "title": "Matching Networks for One Shot Learning", "venue": "NeurIPS", "year": 2016}, {"authors": ["Muhammad Bilal Zafar", "Isabel Valera", "Manuel Gomez Rodriguez", "Krishna P Gummadi"], "title": "Fairness beyond disparate treatment & disparate impact: Learning classification without disparate mistreatment", "venue": "In Proceedings of the 26th International Conference on World Wide Web", "year": 2017}, {"authors": ["Muhammad Bilal Zafar", "Isabel Valera", "Manuel Gomez Rogriguez", "Krishna P Gummadi"], "title": "Fairness Constraints: Mechanisms for Fair Classification", "venue": "In Artificial Intelligence and Statistics", "year": 2017}, {"authors": ["Indre Zliobaite"], "title": "A survey on measuring indirect discrimination in machine learning", "venue": "January 27\u201330,", "year": 2015}], "sections": [{"text": "CCS CONCEPTS \u2022 Computing methodologies\u2192 Machine learning.\nKEYWORDS machine learning, fairness, meta-learning, covariate shift ACM Reference Format: Dylan Slack, Sorelle A. Friedler, and Emile Givental. 2020. Fairness Warnings and Fair-MAML: Learning Fairly with Minimal Data. In Conference on Fairness, Accountability, and Transparency (FAT* \u201920), January 27\u201330, 2020, Barcelona, Spain. ACM, New York, NY, USA, 11 pages. https://doi.org/10. 1145/3351095.3372839"}, {"heading": "1 INTRODUCTION", "text": "As machine learning tools become more responsible for decision making in sensitive domains such as credit, employment, and criminal justice, developing methods that are both fair and accurate become critical to the success of such tools. \u2217Partially supported by the NSF under grant IIS-1633387. Code can be found at: https://github.com/dylan-slack/fairness-warnings-fair-maml. Thanks to Deirdre Mulligan, Charles Marx, and the other participants of the 2019 Summer Cluster on Fairness at the Simons Institute for the Theory of Computing for interesting conversations that helped to shape this work.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. FAT* \u201920, January 27\u201330, 2020, Barcelona, Spain \u00a9 2020 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-6936-7/20/02. . . $15.00 https://doi.org/10.1145/3351095.3372839\nCorrespondingly, there has been an increasing amount of academic interest in the field of fair machine learning (for surveys, see [4, 10, 28, 39]). Research on fairness is often concerned with identifying a notion of fairness, developing an approach that mitigates the notion of fairness, and applying the approach to a variety of data sets in a supervised learning setting (see, e.g., [16, 19, 37, 38]).\nHowever, we ask where this leaves fairness-concerned practitioners who are interested in using fair tools for their particular applications but have access to minimal or no training data. In particular, we introduce the following questions:\n\u2022 When can a practitioner rule out the use of a fair tool trained in a similar but slightly different context? \u2022 How can a practitioner who has access to only a few labeled training points successfully train a fair machine learning model?\nWe suggest the relevance of these questions through the motivating scenario of recidivism prediction. There have been calls for and extensive action towards the proliferation of criminal risk assessment tools in the United States [31]. However, there is often a disconnect between the intended use of these tools and how they are used in practice, which can lead to undesirable or ineffective results [11, 31]. The LJAF, a foundation that focuses on addressing societal issues through data driven approaches, argues for a risk assessment tool that \u201c[can] be adopted by judges and jurisdictions anywhere in America\u201d and has released such a tool that has been widely used [2, 31]. We observe that minor demographic differences in the distribution of data can lead to broad effects on statistical notions of group fairness on fairly trained machine learning models. These results could impact the ways in which fair risk prediction tools are used because such results suggest the proliferation and transfer of such methods between precincts can lead to their unreliability.\nThe most related work to the proposed questions is fairness applied to transfer learning and the covariate shift problem inmachine learning. Covariate shift deals with situations where the distribution of data in application differs from the distribution of data in training. Covariate shift is a well studied field, and there are numerous methods that attempt to train supervised learning classifiers that are robust to test distribution shifts with respect to accuracy [7, 25, 32]. Related methods have been developed to address fairness in the covariate shift setting. Kallus et. al. address the problem of systematic bias in data collection and use covariate shift methods to better compute fairness metrics under such conditions [21]. Coston et. al. consider the situation where there are sensitive labels available in only the source or target domain and propose covariate shift methods to solve such problems [12].\nAdditional work focuses on transferring fair machine learning models across domains. Madras et. al. propose a solution called\nar X\niv :1\n90 8.\n09 09\n2v 2\n[ cs\n.L G\n] 5\nD ec\n2 01\n9\nLAFTR that uses an adversarial approach to create an encoder that can be used to generate fair representations of data sets and demonstrate the utility of the encoder for fair transfer learning [26]. Similarly, Schumman et. al. provide theoretical guarantees surrounding transfer fairness related to equalized odds and opportunity and suggest another adversarial approach aimed at transferring into new domains with different sensitive attributes [29]. Lan and Huan observe that the predictive accuracy of transfer learning across domains can be improved at the cost of fairness [23]. Related to fair transfer learning, Dwork et. al. use a decoupled classifier technique to train a selection of classifiers fairly for each sensitive group in a data set [14].\nWe argue that our proposed questions are different than the existing work in the following ways. While methods exist that address fairness and covariate shift, such methods do not address the problem of communicating to practitioners and policy makers what domain specific factors might cause a fairly trained model to fail to be fair in practice.\nAdditionally, the problem of training fair machine learning models with very little task specific training data is relatively unstudied. Practitioners might have access to minimal training data in one task and sufficient data from other related tasks. This data might be minimal or skewed in terms of which sensitive attribute or label the data belongs to because of data collection issues associated with sensitive data sets like those discussed in Kallus et. al [21]. Though LAFTR offers a way to transfer machine learning models between tasks, we observe it is unsuccessful in very data light situations.\nIn this paper, we propose two different methods to address the proposed problems. First, we discuss the situation where a practitioner has no training data and must decide whether to use a fair machine learning tool trained in another similar but slightly different context. We introduce Fairness Warnings \u2014 a model agnostic approach that provides interpretable boundary conditions on fairness for when not to apply a fair model in a different but related context because the model may behave unfairly. Fairness Warnings provide an interpretable model that indicates what distribution shifts to a data set\u2019s features may cause a fairly trained classifier to act unfairly in terms of a user specified notion of group fairness. While the covariate shift problem setting allows for arbitrary changes to the testing distribution, we only consider mean shifts in this paper. We discuss the limitations imposed by this problem restriction in section 3.1.2.\nTo provide intuition, if Fairness Warnings were trained on a recidivism classifier with respect to the 80% rule of demographic parity [5, 16], the model would provide conditions such as what mean shifts to the features age and priors count would cause the model to score demographic parity lower than 80%. Because law enforcement agencies report general covariate crime information [1], it is likely the case that precinct specific practitioners have access to these high level details and can effectively use fairness warnings to assess whether unfairly trained machine learning model may behave fairly in their application.\nSecond, we consider a better route to transfer a fair machine learningmodel throughmeta-learning.We introduce ameta-learning approach, Fair-MAML, to quickly train models that are both accurate and fair with respect to different notions of fairness with very minimal task specific data. Fair-MAML is based on a meta-learning\nalgorithm called Model Agnostic Meta Learning or MAML [17] that has shown success in reinforcement learning and image recognition. Fair-MAML encourages the learning of more general notions of fairness and accuracy that allow it to achieve strong results on new tasks with only minimal data available. In this way, Fair-MAML can escape the negative inter-distributional effects of sharing a fair machine learning model by providing a model that can be quickly fine-tuned a specific task. We connect Fairness-Warnings and FairMAML by applying Fairness Warnings as boundary conditions on the fine-tuned fair meta-model.\nBesides offering better ways for practitioners to implement fair machine learning models, these methods also provide ways for involved parties to question and assess the results of fair machine learning models. Considering recidivism prediction, the absence of success surrounding the adoption of recidivism tools in the United States has been explained in part by judges\u2019 lack of trust in algorithm recidivism tools [11, 31]. Indeed, such scores are sometimes given to judges without any context [31]. By including Fairness Warnings in assessments, users may be able to better understand under what conditions the algorithmwill fail to be fair. This could help increase judge understanding of such tools, as well as defense attorneys\u2019 ability to challenge its results. Additionally, by fine-tuning recidivism prediction to specific precincts using Fair-MAML, users may more readily trust that such algorithms are delivering relevant predictions than if they were only trained on disparate localities."}, {"heading": "2 BACKGROUND", "text": ""}, {"heading": "2.1 Fairness", "text": "We consider a binary fair classification setting with featuresX \u2208 Rn , labels Y \u2208 {0, 1}, and sensitive attributes A \u2208 {0, 1}. Our goal is to train a model that outputs predictions Y\u0302 \u2208 {0, 1} such that the predictions are both accurate with respect toY and fair with respect to the groups defined by A. We consider 1 the \u201cpositive\" outcome (being labeled at low risk of recidivism) and 0 the \u201cnegative\" outcome (being labeled high risk). Within the sensitive attribute, one label is protected (denoted 0) and the other unprotected (denoted 1). The protected group might be historically disadvantaged groups such as women or African-Americans.\nThere are three often-used ways to define group fairness in this setting.\nThe first, demographic parity (or statistical parity [13]), can be formalized as:\nP(Y\u0302 = 1|A = 0) P(Y\u0302 = 1|A = 1)\n(1)\nThis is also known as a lack of disparate impact [5, 16] or discrimination [8]. A value closer to 1 indicates fairness.\nThe second group fairness definition, equalized odds [19], requires that Y\u0302 have equal true positive rates and false positive rates between groups, where values closer to 1 indicate fairness:\nP(Y\u0302 = 1|A = 0,Y = y) P(Y\u0302 = 1|A = 1,Y = y) y \u2208 {0, 1} (2)\nThis is also known as error rate balance [9] or disparate mistreatment [37]. Equal opportunity (or equal true positive rates) introduces relaxed constraints on 2 and requires the equivalence to hold\nonly on the positive outcome in Y . As compared to equalized odds, equal opportunity often allows for increased accuracy [19]."}, {"heading": "2.2 Meta-Learning", "text": "Meta-learning is concerned with training models such that they can be trained on new tasks using only minimal data and few training iterations within a domain. Meta-learning can be phrased as \u201clearning how to learn\u201d because such methods are trained on a range of tasks with the goal of being able to adapt to new tasks quickly [35]. Metaphorically, this can be likened to finding a base camp (meta-model) from which you can quickly ascend to multiple nearby peaks (optimized per-task models).\nIn the supervised learning setting, each task T = {D,L} where D is a data set containing pairs (X ,Y ) and L is a loss function. We consider a distribution over tasks P(T ) which we train the metamodel to adapt to. Supposing the meta-model is a parameterized function f\u03b8 with parameters \u03b8 , its optimal parameters are:\n\u03b8\u2217 = argmin \u03b8 ET\u223cp(T)LT (f\u03b8 ) (3)\nThis states that the optimal parameters of the model are those that minimize the loss with respect to both L and D. Intuitively, the model parameters should be such that they are nearly optimal for a range of tasks. Ideally, this will mean that optimizing for any new task is quick and requires minimal data.\nIn the meta-learning scenario used in this paper, we train f\u03b8 to learn a new task T \u223c P(T ) using K examples drawn from T . Additionally, we assume f\u03b8 can be optimized through gradient descent. During themeta-training procedure,K examples are drawn from T . The model is trained with respect to K and L and the test performance is evaluated with K new examples. The use of only K training examples for learning a new task is often referred to as K-shot learning and such methods have generally been applied to image recognition and reinforcement learning [36]. Based on the test performance, f\u03b8 is improved. Themeta-model f\u03b8 is evaluated at the end of meta-training through a set of tasks that are not included in the meta-training procedure."}, {"heading": "3 METHODS", "text": ""}, {"heading": "3.1 Fairness Warnings", "text": "3.1.1 Framework. Similar to the formalization of LIME in Ribeiro et. al. [27], we define fairness warnings as an interpretable model\u0434 \u2208 G where G is a class of interpretable models such as decision trees or logistic regression [30]. Further, \u0434 is a function \u0434 : Rd \u2192 {0, 1} where Rd is a set of distribution shifts applied to the features, labels, and sensitive values of some test data set D = {X ,Y ,A} under which a fair model is evaluated. We assume f is fair with respect to some notion of group fairness such as equation 1, and the codomain of \u0434 represents whether the potential shift may result in fair classifications according to that notion of group fairness. Additionally, we assume that group fairness can be evaluated as fair or unfair according to some binary notion of fairness success such as the 80% rule of demographic parity [5, 15, 16]. We assume access to a functionUf : D \u2192 {0, 1} that maps between a data set and whether f acts fairly on that data set according to the binary notion of group fairness.\n3.1.2 Problem Restrictions. In typical covariate shift settings, the testing distribution can be changed in any number of ways \u2014 including being drawn from an entirely different distribution altogether. In this application, we only consider shifts to the mean of the distribution of data that is available for training. Under this assumption there could be more complex changes to the distribution that affect the mean but are not captured by this summary statistic and that may affect fairness. Because we only consider a subset of the possible changes to the testing distribution, Fairness Warnings only indicate what mean shifts may lead a classifier to not be fair and do not strongly indicate fairness if no warning is issued. Additionally, it could be the case that Fairness Warnings predict unfairness for certain mean shifts but due to other changes to the testing distribution the classifier actually behaves fairly. Because of these challenges, Fairness Warnings are just that\u2014warnings that there is some evidence that suggests the model may behave unfairly with respect to a notion of group fairness.\n3.1.3 SLIM. In practice, we use Supersparse Linear Integer Models or SLIM as the interpretable model \u0434 [33]. SLIM creates a linear perceptron that reduces the magnitudes of the coefficients, removes unnecessary coefficients, and forces the coefficients to be integers. SLIM is a highly interpretable method that is well suited to trading off between model complexity in presentation and accuracy. SLIM has been used in sensitive applications such as risk scoring [34]. It has hyperparameters C and \u03f5 . C controls the marginal accuracy a coefficient must add to stay in the model while \u03f5 does the same except for the magnitude of the coefficients.\n3.1.4 Fairness Warnings Algorithm. In order to train \u0434, we generate some user specified number of perturbed versions of D using mean shifts. We generate shifts for numerical features by randomly sampling from a Gaussian distribution with the standard deviation of the feature and mean zero. The number sampled is the mean shift across the feature. To perform the shift, we simply add the number to all the values in the feature. We assume categorical features are one-hot encoded and thus only have two binary categorical features in {0, 1}. We shift each categorical feature by assuming each feature is drawn from a binomial distribution and use the percentage of features labeled 1 as p. We shift the feature vector by drawing a new p from a Gaussian distribution p \u223c N(p, 1) and randomly sample a new vector according to p. If p is less than 0 or greater than 1, we adjust p to 0 or 1 respectively. Doing this a user specified number of times, we create a set of shifted variations D \u2032 of the original D.\nFor each shifted data set, we generate a fairness label F using the binary notion of group fairnessUf . We create a data set of mean shifted data sets and their group fairness behavior with respect to f , Zf = (D \u2032,F ). Finally, we train \u0434 onZf using D \u2032 as the features and F as the labels. Intuitively, we train \u0434 so that it learns to predict what mean shifts may result in unfairness. Assuming shi f t(; ) is some function that computes the mean shifting scheme above, the algorithm for generating fairness warnings is given as Algorithm 1."}, {"heading": "3.2 Fair Meta-Learning", "text": "3.2.1 K-shot Fairness. In order to address the problem of learning fairly from minimal data on a new task, we introduce the notion of K-shot fairness. Given K training examples, K-shot fairness aims to\nRequire: D: data set Require: Uf : fairness notion Require: \u0434: interpretable model Require: N : number of shifts to perform Zf \u2190 [] for i = 1 : N do D \u2032 \u2190 shi f t(D) F \u2190 Uf (D \u2032) Zf \u2190 (D \u2032,F )\n\u22c3Zf end for \u0434\u2190 Train \u0434 withZf using D \u2032 as features and F as labels return \u0434\nAlgorithm 1: Fairness Warnings\nRequire: p(T ): distribution over tasks Require: \u03b1 , \u03b2 : step size hyperparameters\nrandomly initialize \u03b8 while not done do\nSample batch of tasks Ti \u223c p(T ) for all Ti do Sample K datapoints D = {x(j), y(j), a(j)} from Ti Evaluate \u2207\u03b8LTi (f\u03b8 ) using D and LTi Compute updated parameters: \u03b8 \u2032i = \u03b8 \u2212 \u03b1\u2207\u03b8 [LTi (f\u03b8 ) + \u03b3TiRTi (f\u03b8 )] Sample K new datapoints D \u2032i = {x\n(j), y(j), a(j)} from Ti to be used in the meta-update\nend for Update \u03b8 \u2190 \u03b8 \u2212 \u03b2\u2207\u03b8 \u2211 Ti\u223cp(T)[LTi (f\u03b8 \u2032i ) + \u03b3TiRTi (f\u03b8 \u2032i )]\nusing each D \u2032i end while\nAlgorithm 2: Fair-MAML\n(1) quickly train a model that is both fair and accurate on a given task. Additionally, because the relationship between fairness and accuracy is often understood as a trade-off [18], an additional aim is to (2) allow tuning of such a model so that it achieves different balances between accuracy and fairness using justK training points.\nThe language used in this paper surrounding K-shot learning differs slightly from the language used in typical K-shot learning scenarios such as image recognition. In K-shot image recognition, the goal is to learn how to distinguish between N different image labels using only K training examples of each type. The training set size is then KN examples. Because we assume all the tasks to be binary labeled, all of our tasks are 2-way. In referencing K-shot fairness, we will mean that we are using K training examples total\u2014 irrespective of class label, with the assumption that all tasks are 2-way.\n3.2.2 Fair-MAML Framework. We expand the meta learning framework from section 2.2 such that each task includes a fairness regularization term R and fairness hyperparameter \u03b3 . Additionally, we require that D have a protected feature A such that D = (X ,Y ,A). The goal of R is to minimize some notion of group fairness and \u03b3 dictates the trade off between R and L. A task is defined as T = {D,L,R,\u03b3 }. We adjust equation 3 such that the optimal parameters are now:\n\u03b8\u2217 = argmin \u03b8 ET\u223cp(T)[LT (f\u03b8 ) + \u03b3TRT (f\u03b8 )] (4)\nIn order to train a fair meta-learning model, we adapt ModelAgnostic Meta-Learning or MAML [17] to our fair meta-learning framework and introduce Fair-MAML. MAML is trained by optimizing performance of f\u03b8 across a variety of tasks after one gradient step. MAML is particularly well suited to easy fairness adaption because it works with any model that can be trained with gradient descent. The core assumption of MAML is that some internal representations are better suited to transfer learning. The loss function used by MAML is effectively the loss across a batch of task losses. Thus, the MAML learning configuration encourages learning representations that encode more general features than a traditional learning approach. The MAML algorithm works by first sampling a batch of tasks, computing the updated parameters \u03b8 after one gradient step of training on K data points sampled from each task, and finally updating f\u03b8 based on the performance of \u03b8 on a new sample of K points.\nWe modify MAML to Fair-MAML by including a fairness regularization term R in the task losses. The algorithm for Fair-MAML is given in algorithm 2. By including a regularization term, we hope to encourage MAML to learn generalizable internal representations that strike a desirable balance between accuracy and fairness."}, {"heading": "3.3 Fairness Regularizers", "text": "A variety of fairness regularizers have been proposed to handle various definitions of group fairness [6, 20, 22]. Because MAML has shown success with the use of deep neural networks [17], we require regularization terms compatible with neural networks. Methods that require the model to be linear are clearly not applicable. In addition, Fair-MAML requires that second derivatives be computed through a Hessian-vector product in order to calculate the metaloss function which can be computationally intensive and timeconsuming. Thus, it is critical that our fairness regularization term be quick to compute in order to allow for reasonable Fair-MAML training times.\nWe propose two simple regularization terms aimed at achieving demographic parity and equal opportunity that are easy to implement and extremely quick to compute. LetD0 denote the protected instances in X and Y . The demographic parity regularizer is:\nRdp (f\u03b8 ,D) = 1 \u2212 P(Y\u0302 = 1|A = 0) \u2248 1 \u2212 1|D0 | \u2211 x \u2208D0 P(f\u03b8 (x) = 1) (5)\nThis regularizer incurs a penalty if the probability that the protected group receives positive outcomes is low. Our value assumption here is that we want to adjust the likelihood of the protected class receiving a positive outcome upwards. Namely, we do not reduce the rate at which the unprotected class receives positive outcomes and instead adjust upwards the rate at which the protected class receives positive outcomes.\nAdditionally, we consider a regularizer aimed at improving equal opportunity. Let D10 denote the instances within X that are both protected and have the positive outcome in Y .\nReop (f\u03b8 ,D) = 1 \u2212 P(Y\u0302 = 1|A = 0,Y = 1)\n\u2248 1 \u2212 1 |D10 | \u2211 x \u2208D10 P(f\u03b8 (x) = 1) (6)\nWe have a similar value assumption using this regularizer as the one for demographic parity. We adjust the true positive rate of the protected class upwards and do not decrease the true positive rate of the unprotected class. In the case of recidivism prediction, our value system could be likened to the belief that it is better not to classify more non-black defendants as high likelihood for recidivism and instead classify black defendants at a lower rate of likelihood to recidivate."}, {"heading": "4 EXPERIMENTS", "text": "We first demonstrate the individual utility of both Fairness Warnings and Fair-MAML. We then show their usefulness as a combined method."}, {"heading": "4.1 Fairness Warnings", "text": "4.1.1 COMPAS Recidivism Experiment Setup. We initially consider applying Fairness Warnings to the COMPAS recidivism data set. The COMPAS recidivism data set consists of data from over 10, 000 criminal defendants from Broward County, Florida. It includes attributes such as the sex, age, race, and priors for the defendants. We pre-process the data set as described in Angwin et. al. [3]. We create a binary sensitive column for whether the defendant is AfricanAmerican. We predict the ProPublica collected label of whether the defendant was rearrested within two years.\nWe trained a neural network as the model, f , to use with Fairness Warnings. We trained two models\u2014one regularized for demographic parity and the other equal opportunity using the regularization terms from equations 5 and 6 respectively. The demographic parity regularized model scored 58% accuracy and 81% demographic parity on a 20% test set. The equal opportunity regularized model scored 54% accuracy and 68% equal opportunity using the same test set. For the demographic parity fairness warnings, we set the fairness warnings demographic parity threshold at 80%. Meaning, if the classifier scored demographic parity above 80%, it was deemed fair. In the equal opportunity setting, we set the threshold to 60%. We generated 2, 000 perturbed data sets, 800 of which were classified unfairly according to demographic parity. We set \u03f5 to 1e \u2212 3 and C to 1e \u2212 3. We found that \u0434 was able to classify whether the shifts applied to the perturbed data sets would result in unfair group fairness behavior with 88% accuracy on a 10% test set. Using the same perturbed set, the equal opportunity regularized network was found to be unfair in 550 of the 2, 000 perturbed examples. Using the same hyperparameters as before, \u0434 was able to classify whether the shifts would result in unfairness with respect to equal opportunity with 86% accuracy. The Fairness Warnings for the COMPAS data set is given in figure 1.\n4.1.2 COMPAS Recidivism Experiment Analysis. The COMPAS Fairness Warnings both rely on priors_count and age to determine what mean shifts to the data set may result in unfairness. In the demographic parity warning for instance, if the mean group age\napplied to f were to increase by 3 years and mean priors were to remain unchanged, the fairness warning would predict unfairness because the score total would be (0 \u00b7 20) + (3 \u00b7 \u22122) < \u22121. However, in the equal opportunity case, the same shift would not yield unfairness because (0 \u00b7 24) + (3 \u00b7 \u22122) \u226e \u221219. A case that would result in unfairness in the equal opportunity setting would be a decrease in mean priors count by one charge and for age to remain level, i.e. (\u22121 \u00b7 24) + (0 \u00b7 \u22122) < \u221219.\nOverall, the SLIM implementation of fairness warnings showed good ability to classify whether certain mean shifts applied to the feature values of the COMPAS data set would result in unfairness. Because SLIM is tunable with respect to the importance threshold of features shown in the presentation of the model, the classifier only outputs 2 of a possible 8 feature values in both warnings. The presentation is simple. A practitioner would only have to perform a few arithmetic operations in order to compute the fairness warning outcome.\nAdditionally, we were able to train a random forest classifier using 200 estimators from the Scikit-learn implementation which scored 94% and 89% accuracy on the demographic parity and equal opportunity fairness warnings tasks respectively. This suggests that more robust models could serve as much more accurate fairness warnings than SLIM. Presenting a random forest of such size in a digestible way to a user would be difficult. However, the success of the random forest to perform this task indicates that improved interpretable methods that achieve equal levels of interpretability to SLIM but higher levels of accuracy on the fairness warnings task could serve as more desirable fairness warnings."}, {"heading": "4.2 Fair-MAML", "text": "4.2.1 Synthetic Experiment Setup. We illustrate the usefulness of Fair-MAML as opposed to a regularized pre-trained model in fair few-shot classification through a synthetic example based on Zafar et. al [38]. We generate two Gaussian distributions using the means and covariances from Zafar et. al. The first distribution (1) is set to p(x) = N ([2; 2], [5, 1; 1, 5]) and the second (2) is set to p(x) = N ([\u22122;\u22122], [10, 1; 1, 3]). During training, we simulate a variety of tasks by dividing the class labels along a line with y-intercept of (0, 0) and a slope randomly selected on the range [\u22125, 5]. All points above the line in terms of their y-coordinate receive a positive outcome while those below are negative. Using the formulation from Zafar et. al., we create a sensitive feature by drawing from a Bernoulli distribution where the probability of the example being in the protected class is:p(a = 0) = p(x \u2032 |y = 1)/(p(x \u2032 |y = 1)+p(x \u2032 |y = 0)) where x \u2032 = [cos(\u03d5),\u2212sin(\u03d5); sin(\u03d5), cos(\u03d5))]x . Here, \u03d5 controls the correlation between the sensitive attribute and class labels. The lower \u03d5, the more correlation and unfairness. We randomly select \u03d5 from the range [2, 4, 8, 16] to simulate a variety in fairness between tasks.\nIn order to assess the fine-tuning capacity of Fair-MAML and the pre-trained neural network, we introduced a more difficult finetuning task. During training, the two classes were separated clearly by a line. For fine-tuning, we set each of the binary class labels to a distribution. The positive class was set to distribution (1) and the negative class was set to distribution (2). In this scenario, a straight line cannot clearly divide the two classes. We assigned\nsensitive attributes using the same strategy as above and used a \u03d5 of 4. Additionally, we only gave 5 positive-outcome examples from the protected class. We hoped to simulate a situation where a fair classifier is needed on a new task, but there are only a few protected examples in the positive outcome to learn from\u2014simulating the situation where the distribution of fine-tuning task data is biased. An example of such a scenario could be if a practitioner needed to train a new recidivism tool and had access to only a few examples of African-Americans who had previously been labeled as low risk.\nWe randomly generated 100 synthetic tasks that we cached before training. We sampled 5 examples from each task during metatraining, used a meta-batch size of 32 for Fair-MAML, and performed a single epoch of optimization within the internal MAML loop. We trained Fair-MAML for 5, 000 meta-iterations. For the pre-trained neural network, we performed a single epoch of optimization for each task. We trained over 5, 000 batches of 32 tasks per batch to match the training set size used by Fair-MAML.\nThe loss used is the cross-entropy loss between the prediction f (x) and the true value using the demographic parity regularizer from equation 5. We use a neural network with two hidden layers consisting of 20 nodes and the ReLU activation function. We used the softmax activation function on the last layer. When training with Fair-MAML, we used K = 5 examples and performed one gradient step update. We set the step size \u03b1 to 0.3, used the Adam optimizer to update the meta-loss with learning rate \u03b2 set to 1e \u2212 3. We pre-trained a baseline neural network on the same architecture as Fair-MAML. To one-shot update the pre-trained neural network we experimentedwith step sizes of [0.01, 0.1, 0.2, 0.3] and ultimately found that 0.3 yielded the best trade offs between accuracy and fairness. Additionally, we tested \u03b3 values during training and finetuning of [0, 10]. We present an example task in figure 2 using 5\nfine-tuning points from the positive outcome and protected class. When \u03b3 = 0, Fair-MAML does not incur any fairness regularization, so the model is just MAML.\n4.2.2 Synthetic Experiment Analysis. In the new task, there is an unseen configuration of positively labeled points. It was not possible for positively labeled points to fall below y = 0 during training. Fair-MAML is able to performwell with respect to both fairness and accuracy on the fine-tuning task when only biased fine-tuning data is available. The pre-trained neural network fails at performing the new task when the fine-tuning data does not come from the original distribution of data. This example suggests that Fair-MAML has learned a more useful internal representation for both fairness and accuracy than the pre-trained neural network.\n4.2.3 Communities and Crime Experiment. Next we consider an example using the Communities and Crime data set [24]. The Communities and Crime data set includes information relevant to crime (e.g., police per population, income) as well as demographic information (such as race and sex) in different communities across the United States. The goal is to predict the violent crime rate in the community. We convert this data set to a few-shot fairness setting by using each state as a different task.\nBecause the violent crime rate is a continuous value, we convert it into a binary label based on whether the community is in the top 50% in terms of violent crime rate within a state. Additionally, we add a binary sensitive column that receives a protected label if African-Americans are the highest or second highest population in a community in terms of percentage racial makeup.\nThe Communities and Crime data set has data from 46 states ranging in number of communities from 1 to 278 communities per state. We only used states with 20 or more communities leaving\n30 states. We held out 5 randomly selected states for testing and trained using 25 states. We set K = 10 and cached 100meta-batches of size 8 states for training. For testing, we randomly selected 10 communities from the hold out task that we used for fine-tuning and evaluated on whatever number of communities were left over. The number of evaluation communities is guaranteed to be at least 10 because we only included states with 20 or more communities.\nWe trained two Fair-MAML models\u2014one with the demographic parity regularizer from equation 5 and another with the equal\nopportunity regularizer from equation 6. For both models, we used a neural network with two hidden layers of 20 nodes. We trained the model with one gradient step using a step size of 1e \u2212 2 and a meta-learning rate of 1e \u2212 4 using the Adam optimizer. We trained the model for 2, 000 meta-iterations.\nIn order to assess Fair-MAML, we trained a neural network regularized for fairness using the same architecture and training data. We fine-tuned the neural network for each of the assessment tasks. We used a learning rate of 1e \u2212 3 for training and assessed learning\nrates of [1e \u2212 4, 1e \u2212 3, 1e \u2212 2, 1e \u2212 1] for fine-tuning. We found the fine-tuning rate of 1e \u2212 1 to perform the best trade offs between accuracy and fairness and present results using this learning rate. We varied \u03b3 over [0, 4] incremented by 1 for the demographic parity regularizer. We found higher \u03b3 \u2019s to work better for the equal opportunity regularizer and varied \u03b3 from [0, 40] incremented by 10.\nAdditionally, we trained two LAFTR models on the transfer tasks as comparisons for demographic parity and equal opportunity. LAFTR is not intended to be compatible with our proposed K-shot fairness experiments because training on fine-tuning tasks with a minimal number of epochs and training points is not expected. However, we find that it is the most relevant fair transfer learning method to use as comparison. We used the same transfer methodology and hyperparameters as described in Madras et. al. [26] and used a neural network with a hidden layer of 20 nodes as the encoder. We used another neural network with a hidden layer of 20 nodes as the multilayer perception (MLP) to be trained on the fairly encoded representation. We used the demographic parity and equal opportunity adversarial objectives for the first and second LAFTR model respectively. We trained each encoder for 1, 000 epochs and swept over a range of \u03b3 \u2032s : [0, 0.5, 1.0, 2.0, 4.0]. We trained with all the data not held out as one of the 5 testing tasks. When training a MLP from the encoder on each of the transfer tasks, we found that LAFTR struggled to produce useful results with only 10 training points from the new task over any number of training epochs. We found that we were able to get reasonable results from LAFTR using 30 fine-tuning points and 100 epochs of optimization\u2014using a minimal number of epochs was unsuccessful. It makes sense that a minimal number of training epochs for the new task is unsuccessful because the MLP trained on the fairly encoded data is trained from scratch. The results are presented in figure 3. We were able to generate similar results with LAFTR to Fair-MAML using 50 training points from the new task after 100 epochs of optimization. These results are given in the appendix.\nWe observe that Fair-MAML achieves the best trade off between fairness and accuracy both in terms of demographic parity and equal opportunity. In our proposed problem setting, LAFTR was not successful at learning with minimal data and a small number of fine-tuning epochs for the new task. The pre-trained neural network shows some ability to learn the new task using little data and fine-tuning epochs. At low \u03b3 \u2019s, Fair-MAML is able to achieve higher accuracy than the pre-trained neural network and LAFTR. Crucially, Fair-MAML is able to learn more accurate representations that are also fairer for a range of \u03b3 \u2019s than both of the baselines. In order to generalize to new states, only 10 communities are needed in order to achieve strong predictive accuracy and fairness using Fair-MAML."}, {"heading": "4.3 Fair-MAML with Fairness Warnings", "text": "4.3.1 Motivation. We next consider Fairness Warnings applied to Fair-MAML. We argue that Fairness Warnings can serve as a complementary tool to Fair-MAML. Because we expect Fair-MAML to be used in situations with minimal data available, it is possible that testing data given to a fine-tuned Fair-MAML model is unrepresentative of the true distribution of data for a particular task.\nWhile in section 4.2.1, we empirically demonstrate that Fair-MAML can still achieve good results when training data is available from one value in a sensitive attribute or label, it still may be useful for practitioners to have indication surrounding situations in which their model may fail to be fair in testing.\n4.3.2 Communities and Crime Fairness Warning/Fair-MAML Experiment. We apply fairness warnings to Fair-MAML on the communities and crimes experimental setup from section 4.2.3 using demographic parity as our notion of fairness. We randomly chose an evaluation state to apply Fairness Warnings and left the rest for meta-training. We trained two Fair-MAML models as f in fairness warnings using the demographic parity regularizer for the first model and equal opportunity regularizer for the second model. We used \u03b3 = 5 for the demographic parity Fair-MAML model and \u03b3 = 30 for the equal opportunity Fair-MAML model. We trained for 2, 000 meta-iterations in a 1-step optimization setting, with the update learning rate set to 1e \u2212 2 and the meta learning rate set to 1e \u2212 4. The demographic parity Fair-MAML model scored 87% demographic parity on the test set of the fine-tuning task and accuracy of 69%. The equal opportunity Fair-MAML model scored 64% accuracy and equal opportunity of 63%.\nTo train Fairness Warnings on the fine-tuning task, we created 2, 000 shifted data sets of the fine-tuning test data. We trained a Fairness Warning for both demographic parity and equal opportunity. We used the 80% rule of demographic parity in the demographic parity warning and a 60% equal opportunity threshold in the equal opportunity warning. We found that 1, 034 or close to 50% of the shifted data sets were classified fairly according to f with respect to demographic parity and that 1, 248 of the shifted data sets were classified fairly according to equal opportunity. We trained SLIM using \u03f5 of 1e \u2212 3 andC of 1e \u2212 5 for the demographic parity fairness warning. We adjustedC to 1e \u2212 3 for the equal opportunity fairness warning.\nSLIM was able to predict whether the mean shifts across the features in the communities and crime data set would result in demographic parity unfairness with 71% accuracy on a 10% test set. A random forest with 200 estimators was able to predict the same task with 88% accuracy. In the equal opportunity setting, SLIM predicted the task with 68% accuracy. A random forest with 200 estimators was able to perform the same task with 77% accuracy. The fairness warnings are presented in figure 4.\n4.3.3 Communities and Crime Fairness Warning/Fair-MAML Analysis. The Fairness Warning trained on the fine-tuned Fair-MAML model is able to perform reasonable prediction accuracy and generates informative results. Particularly, it is interesting to consider that the demographic parity fine-tuned model behaves unfairly when the testing data set changes according to features such as number people living under the poverty line, in urban areas, and number of police officer. A similar result is found in the equal opportunity setting with police operating budget. In both the demographic parity and equal opportunity cases, the fairness warnings demonstrate that seemingly small and perhaps innocuous differences between states where Fair-MAML is trained and applied could result in unfair behavior. For instance, the addition of a couple dozen additional police officers across communities in a state\nin the demographic parity case could lead to the classifier behaving unfairly. The same is true for equal opportunity and a slight increase to the mean police operating budget. As we see in this example, reasonable real world changes to the testing distribution can result in negative changes to the group fairness of the fine-tuned Fair-MAML model. Providing Fairness Warnings to accompany\nthe fine-tune meta model could lend additional guidance to a practitioner and help them better understand if their model will not behave fairly in application."}, {"heading": "5 LIMITATIONS AND CONCLUSIONS", "text": "In this paper, we introduced Fairness Warnings and Fair-MAML. Fairness Warnings provides an interpretable model that predicts which changes to the testing distribution will cause a model to behave unfairly. Fair-MAML is a method that \u201clearns to learn\" fairly and can be used to train a fair model quickly from minimal data. We demonstrate empirically the usefulness of both methods through multiple examples on both synthetic and real data sets.\nIn this work, we explore Fairness Warnings applied to mean shifts in the testing distribution. It is a relatively straight forward extension to apply Fairness Warnings to other distribution shifts such as changes to the standard deviation. Though we are able to generate Fairness Warnings that show useful results, they ultimately are only applied to summary statistics. Meaning, changes to the distribution that are not captured by such statistics could affect fairness in unpredictable ways. Thus, we only propose fairness warnings as boundary conditions under which the model may not be fair. In this regard, receiving a non-unfair score in fairness warnings does not guarantee that the model will behave fairly in the new domain. We emphasize the importance of this directionality to any lawmakers or practitioners who would be interested in using Fairness Warnings and advise that they be used only to decide against the use of certain models instead of verify that models will behave fairly. A final limitation to our work is that we assess Fair-MAML when there are many related training tasks to learn from. In reality, there may only be a few related training tasks available. We leave assessing how useful Fair-MAML is on domains with only a few related training tasks to future work."}], "title": "Fairness Warnings and Fair-MAML: Learning Fairly with Minimal Data", "year": 2019}