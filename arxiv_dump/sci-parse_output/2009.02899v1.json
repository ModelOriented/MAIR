{"abstractText": "One way to achieve eXplainable artificial intelligence (XAI) is through the use of post-hoc analysis methods. In particular, methods that generate heatmaps have been used to explain black-box models, such as deep neural network. In some cases, heatmaps are appealing due to the intuitive and visual ways to understand them. However, quantitative analysis that demonstrates the actual potential of heatmaps have been lacking, and comparison between different methods are not standardized as well. In this paper, we introduce a synthetic data that can be generated adhoc along with the ground-truth heatmaps for better quantitative assessment. Each sample data is an image of a cell with easily distinguishable features, facilitating a more transparent assessment of different XAI methods. Comparison and recommendations are made, shortcomings are clarified along with suggestions for future research directions to handle the finer details of select post-hoc analysis methods.", "authors": [{"affiliations": [], "name": "Erico Tjoa"}, {"affiliations": [], "name": "Cuntai Guan"}], "id": "SP:70382d34adcee626947dff03e5ac4a4310b5d48d", "references": [{"authors": ["B. Zhou", "A. Khosla", "A. Lapedriza", "A. Oliva", "A. Torralba"], "title": "Learning deep features for discriminative localization", "venue": "In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "year": 2016}, {"authors": ["Ramprasaath R. Selvaraju", "Abhishek Das", "Ramakrishna Vedantam", "Michael Cogswell", "Devi Parikh", "Dhruv Batra"], "title": "Grad-cam: Why did you say that? visual explanations from deep networks via gradient-based localization", "venue": "CoRR, abs/1610.02391,", "year": 2016}, {"authors": ["Avanti Shrikumar", "Peyton Greenside", "Anshul Kundaje"], "title": "Learning important features through propagating activation", "venue": "differences. CoRR,", "year": 2017}, {"authors": ["Sebastian Bach", "Alexander Binder", "Grgoire Montavon", "Frederick Klauschen", "Klaus-Robert Mller", "Wojciech Samek"], "title": "On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation", "venue": "PLOS ONE, 10(7):1\u201346,", "year": 2015}, {"authors": ["Daniel Smilkov", "Nikhil Thorat", "Been Kim", "Fernanda B. Vi\u00e9gas", "Martin Wattenberg"], "title": "Smoothgrad: removing noise by adding noise", "venue": "CoRR, abs/1706.03825,", "year": 2017}, {"authors": ["Marco Tulio Ribeiro", "Sameer Singh", "Carlos Guestrin"], "title": "why should i trust you?: Explaining the predictions of any classifier", "venue": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD", "year": 2016}, {"authors": ["Been Kim", "Martin Wattenberg", "Justin Gilmer", "Carrie Cai", "James Wexler", "Fernanda B. Vigas", "Rory Sayres"], "title": "Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav)", "venue": "ICML, volume 80 of JMLR Workshop and Conference Proceedings,", "year": 2018}, {"authors": ["Cynthia Rudin"], "title": "Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead", "venue": "Nature Machine Intelligence,", "year": 2019}, {"authors": ["Fabian Eitel", "Kerstin Ritter"], "title": "Testing the robustness of attribution methods for convolutional neural networks in mri-based alzheimer\u2019s disease classification", "venue": "Interpretability of Machine Intelligence in Medical Image Computing and Multimodal Learning for Clinical Decision Support,", "year": 2019}, {"authors": ["Peifei Zhu", "Masahiro Ogino"], "title": "Guideline-based additive explanation for computer-aided diagnosis of lung nodules", "venue": "Interpretability of Machine Intelligence in Medical Image Computing and Multimodal Learning for Clinical Decision Support,", "year": 2019}, {"authors": ["A.W. Thomas", "H.R. Heekeren", "K.R. M\u00fcller", "W. Samek"], "title": "Analyzing Neuroimaging Data Through Recurrent Deep Learning Models", "venue": "Front Neurosci,", "year": 2019}, {"authors": ["Jeremy Irvin", "Pranav Rajpurkar", "Michael Ko", "Yifan Yu", "Silviana Ciurea-Ilcus", "Chris Chute", "Henrik Marklund", "Behzad Haghgoo", "Robyn L. Ball", "Katie S. Shpanskaya", "Jayne Seekins", "David A. Mong", "Safwan S. Halabi", "Jesse K. Sandberg", "Ricky Jones", "David B. Larson", "Curtis P. Langlotz", "Bhavik N. Patel", "Matthew P. Lungren", "Andrew Y. Ng"], "title": "Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison", "venue": "URL http://arxiv.org/abs/ 1901.07031", "year": 1901}, {"authors": ["Guannan Zhao", "Bo Zhou", "Kaiwen Wang", "Rui Jiang", "Min Xu"], "title": "Respond-cam: Analyzing deep models for 3d imaging data by visualizations", "venue": "Medical Image Computing and Computer Assisted Intervention \u2013 MICCAI", "year": 2018}, {"authors": ["Xiaoxiao Li", "Nicha C. Dvornek", "Juntang Zhuang", "Pamela Ventola", "James S. Duncan"], "title": "Brain biomarker interpretation in asd using deep learning and fmri", "venue": "Medical Image Computing and Computer Assisted Intervention \u2013 MICCAI", "year": 2018}, {"authors": ["Magdalini Paschali", "Sailesh Conjeti", "Fernando Navarro", "Nassir Navab"], "title": "Generalizability vs. robustness: Investigating medical imaging networks using adversarial examples", "venue": "Medical Image Computing and Computer Assisted Intervention \u2013 MICCAI", "year": 2018}, {"authors": ["Heather D. Couture", "J.S. Marron", "Charles M. Perou", "Melissa A. Troester", "Marc Niethammer"], "title": "Multiple instance learning forheterogeneous images: Training acnn for histopathology", "venue": "Medical Image Computing and Computer Assisted Intervention \u2013 MICCAI", "year": 2018}, {"authors": ["Ziqi Tang", "Kangway V. Chuang", "Charles DeCarli", "Lee-Way Jin", "Laurel Beckett", "Michael J. Keiser", "Brittany N. Dugger"], "title": "Interpretable classification of alzheimer\u2019s disease pathologies with a convolutional neural network pipeline", "venue": "Nature Communications,", "year": 2041}, {"authors": ["Zachary Papanastasopoulos", "Ravi K. Samala", "Heang-Ping Chan", "Lubomir Hadjiiski", "Chintana Paramagul", "Mark A. Helvie M.D", "Colleen H. Neal M.D"], "title": "Explainable AI for medical imaging: deep-learning CNN ensemble for classification of estrogen receptor status from breast MRI", "venue": "Medical Imaging 2020: Computer-Aided Diagnosis,", "year": 2020}, {"authors": ["Yao Qin", "Konstantinos Kamnitsas", "Siddharth Ancha", "Jay Nanavati", "Garrison W. Cottrell", "Antonio Criminisi", "Aditya V. Nori"], "title": "Autofocus layer for semantic segmentation", "venue": "CoRR, abs/1805.08403,", "year": 2018}, {"authors": ["Hyebin Lee", "Seong Tae Kim", "Yong Man Ro"], "title": "Generation of multimodal justification using visual word constraint model for explainable computer-aided diagnosis", "venue": "Interpretability of Machine Intelligence in Medical Image Computing and Multimodal Learning for Clinical Decision Support,", "year": 2019}, {"authors": ["Alex Krizhevsky"], "title": "One weird trick for parallelizing convolutional neural networks", "venue": "CoRR, abs/1404.5997,", "year": 2014}, {"authors": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "title": "Deep residual learning for image recognition", "venue": "In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "year": 2016}, {"authors": ["S. Liu", "W. Deng"], "title": "Very deep convolutional neural network based image classification using small training sample size", "venue": "In 2015 3rd IAPR Asian Conference on Pattern Recognition (ACPR),", "year": 2015}, {"authors": ["Karen Simonyan", "Andrea Vedaldi", "Andrew Zisserman"], "title": "Deep inside convolutional networks: Visualising image classification models and saliency maps", "venue": "In Workshop at International Conference on Learning Representations,", "year": 2014}, {"authors": ["Pieter-Jan Kindermans", "Kristof Schtt", "Klaus-Robert Mller", "Sven Dhne"], "title": "Investigating the influence of noise and distractors on the interpretation of neural networks, 2016", "year": 2016}, {"authors": ["Jost Tobias Springenberg", "Alexey Dosovitskiy", "Thomas Brox", "Martin Riedmiller"], "title": "Striving for simplicity: The all convolutional net", "year": 2014}, {"authors": ["Matthew D. Zeiler", "Rob Fergus"], "title": "Visualizing and understanding convolutional networks", "venue": "Computer Vision \u2013 ECCV", "year": 2014}, {"authors": ["Scott M Lundberg", "Su-In Lee"], "title": "A unified approach to interpreting model predictions", "venue": "Advances in Neural Information Processing Systems", "year": 2017}, {"authors": ["Mukund Sundararajan", "Ankur Taly", "Qiqi Yan"], "title": "Axiomatic attribution for deep networks", "venue": "In Proceedings of the 34th International Conference on Machine Learning - Volume 70,", "year": 2017}, {"authors": ["Avanti Shrikumar", "Peyton Greenside", "Anshul Kundaje"], "title": "Learning important features through propagating activation differences. volume 70 of Proceedings of Machine Learning Research, pages 3145\u20133153", "venue": "International Convention Centre, Sydney, Australia,", "year": 2017}, {"authors": ["Ashwin Srinivasan"], "title": "Note on the location of optimal classifiers in n-dimensional roc space", "venue": "Technical report,", "year": 1999}, {"authors": ["C\u00e9sar Ferri", "Jos\u00e9 Hern\u00e1ndez-Orallo", "Miguel Angel Salido"], "title": "Volume under the roc surface for multi-class problems", "venue": "Machine Learning: ECML", "year": 2003}], "sections": [{"text": "Index Terms\u2014Explainable AI, Machine Learning, Interpretability, Black-box\nI. INTRODUCTION EXplainable artificial intelligence (XAI) has been gathering attention in the artificial intelligence (AI) and machine learning (ML) community recently. The recent trend was propelled by the success of deep neural network (DNN), especially convolutional neural network (CNN) in image processing. DNN has been considered a blackbox because the mechanism underlying its remarkable performance is not well understood. XAI research has thus developed in many different directions. Among them is the saliency method, where heatmaps are generated and used to give explanations on where AI model is \u201clooking at\u201d when it is making a decision or prediction. The heatmaps are compatible with human\u2019s visual comprehension, easy to read and interpret and thus they are desirable. However, many of the formulas used to generate the heatmaps are given using heuristics, and hence are not revealing enough of the underlying mechanism to help us debug, fix or improve the AI model in meaningful ways.\nRegardless, the development of heatmap methods have continued without correspondingly reliable ways to evaluate how one heatmap is better than another. The metrics used to quantify the quality of heatmaps are sometimes indirect, and at other times, qualitative assessment of the quality of heatmaps appear to be possibly given in hind-sight to fit natural reasoning. This often occurs due to the lack of groundtruth heatmaps to verify the correctness of the generated\nheatmaps. Under such situation, the quality and effectiveness of interpretable heatmaps have nevertheless been demonstrated in several ways. CAM [1] and GradCAM [2] heatmaps were shown to improve the localization on ILSVRC datasets. By observing the change in the log odd scores after deleting image pixels, the relevance of image pixels to the decision or prediction of a model can be determined as well [3]. The earlier paper [4] on the development of layerwise relevance propagation (LRP) shows heatmaps generated on many sample data, although many heatmaps do not appear to demonstrate good consistency in their pixel-wise assignment of values (different improvements have since been suggested). Tests were conducted on the effect of transformation on the images, for example, by flipping MNIST digits, and mean prediction is defined to assess the method after interchanging pixels systematically based on relevance computed by LRP. Still, the paper itself mentions that the analysis is semi-quantitative. The paper that introduced SmoothGrad [5] mentioned that, at the time, there was no ground-truth to allow for quantitative evaluation of heatmaps. It then proceeded with 2 qualitative evaluations instead. As of now, even though there are many different datasets available for AI and ML researches, the corresponding ground-truth explanations (such as heatmaps) are typically not available. Note: though heatmaps are sometimes interchangeably called saliency map, we only refer to them as heatmaps here because we want to distinguish them from XAI method whose name is Saliency.\nXAI methods that are not focused on generating heatmaps have also been developed. This paper is mainly concerned about how to quantitatively compare heatmaps, but we may still benefit from different types of evaluations of XAI performance. Local interpretable model-agnostic explanation (LIME) [6] is introduced to find a locally faithful interpretable\nar X\niv :2\n00 9.\n02 89\n9v 1\n[ cs\n.C V\n] 7\nS ep\n2 02\n0\nmodel that represents well the model under inspection, regardless of the latter\u2019s architecture (i.e. is agnostic). By comparing LIME with obviously interpretable models such as decision trees and sparse logistic regression, in particular using recall value, the quality of feature importance obtained using LIME can be assessed. Experiments on Concept Activation Vectors (section 4.3 of [7]) include quantitative comparison of the information used by a model when a ground-truth caption is embedded into the image. In some cases, the caption is used by the model for decision-making, but in other cases, only the image concept is used. Furthermore, human-subject experiments are also conducted to test the importance of the saliency mask, showing that heatmaps help only marginally for human to make decision and that heatmaps can even be misleading. There has also been other similar sentiment that doubts the usefulness of heatmaps, for example in the caption of fig. 2 in [8].\nOn the other hand, applications of XAI methods have emerged in other fields, where evaluation of heatmaps has been performed in different ways. Still, one should be careful that the evaluations may not always clearly indicate the relevant\nusefulness of the heatmaps themselves. A study on MRIbased Alzheimer\u2019s disease classification [9] computes the L2 norm between average heatmaps generated by different XAI methods and compares the performance of three other different metrics. Ground-truth heatmaps are sometimes available, for example in the diagnosis of lung nodules [10] where recall values can be directly computed between the reference features (ground-truth) and the heatmaps generated by different XAI methods. Different kinds of ground-truth have been obtained using specialized method, such as NeuroSynth in [11] for analyzing neuroimaging data. Some parts of the evaluation appears qualitative (such as group-level evaluation), though the paper uses F1-score to evaluate the heatmap, thus naturally including recall and precision concepts in the evaluation. Other applications of XAI methods, especially heatmaps, in the medical field are for example [12\u201320].\nIn this paper, in section II-A we first introduce a synthetic dataset containing containing images with simple features with ground-truth heatmaps that can be generated on demand. The aim is to provide a standardized dataset to compare the viability and effectiveness of heatmaps generated by XAI\nmethods in providing explanations. Ground-truth heatmaps are automatically generated alongside the image data and labels, avoiding the need to manually mark heatmap features, which is a very laborious process. In this 10-class dataset, each data sample consists of an object with a simple shape and its corresponding heatmap designed to be unambiguous, which is the core feature intended to address the problems mentioned above. In short, we provide a dataset where heatmaps can be verified in a more objective way. The rest of section II describes the implementation of neural network training, validation and evaluation processes, followed by the description of five-band-score, a metric defined to capture quantities such as recall and precision that take into account the distinct meaningful regions in heatmaps. Section III discusses the recall-precision results and ROC curves we obtained. Finally, we conclude with recommendations on which methods are possibly useful on specific cases and provide some caveats."}, {"heading": "II. DATA AND METHODOLOGY", "text": "This section describes the workflow starting from data generation, network training, network performance evaluation, heatmap generation and evaluating generated heatmaps with common quantities. The workflow is shown in fig. 1, closely following the sequence of commands run in the package of python codes 1 provided. Some details, such as the algorithms needed to generate each sample data, can be traced from the tutorials available as jupyter notebook included in the package of codes."}, {"heading": "A. Dataset", "text": "Algorithm 1: build basic ball body(x0, y0, r, t etc) for type 0 cell. ys is a multiplicative factor for modifying object\u2019s elliptical shape. t is the thickness of cell border. Subscript ex stands for \u201cexplanation\u201d, which will be the heatmap parts. Thresholds thd, thl = 0.05 are suitably chosen to create binary arrays.\nx, y \u2190 meshgrid d\u2190 \u221a x2 + (y/ys)2 + noise border \u2190 (d \u2264 r) \u2217 (d \u2265 r \u2212 t) innerball\u2190 (d < r \u2212 t) rotate by \u03b8 shift center to (x0, y0) ball\u2190 border + innerball make explanation(){ for all pixels (i, j) do borderex,ij \u2190 13 \u221a \u03a3c(border)2ij,c \u2265 thd\nbodyex,ij \u2190 13 \u221a\n\u03a3c(inner)2ij,c \u2265 thl bodyex,ij \u2190 bodyex,ij \u2217 (1\u2212 borderex,ij) heatmapij \u2190 borderex,ij \u2217 0.9 + bodyex,ij \u2217 0.4\nend for }\n1https://github.com/etjoa003/explainable ai/tree/master/xai basic\nAlgorithm 2: build ccell body(x0, y0, r, t, tb etc) for type 1 or 2 cell. vs is a multiplicative factor for stretching. tb, tp are bar and pole thicknesses to form minus- and plusshaped skeletons of the cells.\nbuild basic ball body(x0, y0, r, t etc) x, y \u2190 x+ noise, y + noise create skeleton(){\nbar = (x\u2212 x0 \u2264 r) \u2217 (x\u2212 x0 \u2265 \u2212r)\u2217 (y \u2212 y0 \u2264 tb/2) \u2217 (y \u2212 y0 \u2265 \u2212tb/2) if type 2 then pole = (y \u2212 y0 \u2264 r \u2217 vs) \u2217 (y \u2212 y0 \u2265 r \u2217 vs)\u2217\n(x\u2212 x0 \u2264 tp/2) \u2217 (x\u2212 x0 \u2265 \u2212tp/2) polepos = (pole \u2265 0) \u2217 (1\u2212 bar \u2265 0) bar = bar + pole \u2217 polepos\nend if } rotate ball and bar by \u03b8 shift center of ball and bar to (x0, y0) make explanation()\nWe provide algorithms that can generate dataset as shown in fig. 2 on demand, where the top three rows are the images and the last three rows are the corresponding ground-truth heatmaps. The ten different classes of cells are shown along the columns. Types 0,1,2 are circular cells with border (algo. 1), with a bar (or minus sign) and with a plus sign (algo. 2) respectively. Types 3,4,5 are rectangular cells with different dominant colors. Types 6,7,8 following are circular cells with one, three and eight tails respectively. The last class does not contain any cell. Three types of backgrounds are given to increase the variation of dataset, as shown separately in the first three rows of the same figure.\nThe ground-truth heatmaps h0 have been designed to mark features that distinguish all the classes in a way that is as unambiguous as possible, subject to human judgment. Admittedly, there may not exist a unique unambiguous way of defining them. Where appropriate, the heatmaps could be readjusted by editing the heatmap generator classes in the package of codes. The heatmaps are shown in fig. 2 row 4 to 6. With this dataset, fair comparison between heatmaps generated by different XAI methods can be performed. In this particular implementation, each h0 is normalized to [\u22121, 1], and thus heatmaps to be compared to h0 are expected to be normalized to [\u22121, 1] as well. Each ground-truth h0 consists of an array of values of size (H, W) with three distinct regions (1) regions of value 0 (shown as white background) for regions that should not contribute to the neural network prediction, (2) regions of value 0.4 for localization (shown as light red region) and (3) regions of value 0.9 for discriminative feature (shown as dark red region), where discriminative feature is also qualitatively considered part of localization. In this work, two other distinct regions are defined symmetrically. They are \u22120.4 and \u22120.9 regions, to accommodate the fact that some heatmap methods have been interpreted in such a way that negative regions\n(shown as blue color in this paper) are considered as regions contributing against a given decision or prediction. For our dataset, the ground-truth does not contain any such information that contributes negatively, although, as will be shown later, some XAI methods still do generate negative values.\nFor this paper, training, validation and evaluation datasets are prepared in 32, 8 and 8 shards respectively, each shard containing 200 samples uniform randomly drawn from the 10 classes. In another words, in total, the datasets contain 6400, 1600 and 1600 samples respectively. The dataset is prepared in shards for practical purposes, for example, to prevent full restart in case of interruption of data downloading and caching, and to facilitate more efficient process of training in evaluation mode indicated in fig. 1 as part 2.\nB. Training\nAfter the data is cached or saved, the process starts with training in continuous mode, indicated as part 1 of the workflow fig. 1. In this mode, pre-trained models are first downloaded from Torchvision and modified for compatibility with pytroch Captum API. The three pre-trained models used are AlexNet [21], ResNet34 [22] and VGG [23], corresponding to workflow 1, 2 and 3 in the codes. In this phase, training proceeds continuously for the purpose of fine-tuning the models to our current data. The number of epochs and batch size are specified in table I. Adam optimizer is used with learning rate lr = 0.001 for ResNet but lr = 0.0001 for AlexNet and VGG, and the same weight decay 10\u22125 is used for all. Plot of losses against training iterations (not shown in this paper) is saved as a figure in part 1.2 of the workflow. We refer to the model trained after this phase as the base model.\nThe next phase is the training in regular evaluation mode, indicated as part 2 in fig. 1. The training uses the same optimizer as the previous phase, and the number of epochs used are also shown in table I. Evaluation is performed every 4 training iterations; more accurately, this part is known as validation in machine learning community, separate from the final evaluation. Each validation is performed on a shard randomly drawn from the 8 shards of the validation dataset. We set the target accuracy to 0.96. If during validation, the accuracy computed on that single shard exceeds the target accuracy, the training is stopped and evaluation on all validation data shards is performed. The total validation accuracy is used to ensure that the validation accuracy on a single shard is not high by pure chance. While the total validation accuracy can\nbe slightly lower, our experiments so far indicate that there is no such problem. Furthermore, only ResNet attained the target accuracy within the specified setting. For AlexNet and VGG, 0.96 is not exceeded throughout and early stopping mechanism is triggered to prevent unnecessarily long, unfruitful training; note that, fortunately, the total accuracy when evaluated on the final evaluation dataset is still very high, as shown in table I. The early stopping mechanism is as the following. Whenever validation on a single shard does not achieve the target accuracy but (1) if there is no improvement in the validation accuracy, then early stopping counter nearly is increased by one (2) if there is improvement in the validation accuracy, then nearly \u2192 nearly \u00d7 rf , where rf < 1 is the refresh fraction, so that the process is given more chance to train longer. If nearly becomes equal to the early stopping limit, nl, training is stopped.\nWe repeat the above process of training in regular evaluation mode 4 other times starting from the base model, and thus we have a total of 5 branch models. Note that rf and nl are set so that AlexNet and VGG can be trained for longer period (shown in table I), since they both achieve lower accuracy performance than ResNet if given the same number of epochs, nl and rf . This is possibly because (1) larger batch size means fewer iterations per epoch and (2) improvement in accuracy is inherently slower, considering that ResNet has been known to generally perform better. Here, comparing accuracy of prediction in a precise manner is not very meaningful, as we are focusing on the heatmaps later. No attempt is made to train the models to perfect accuracy, as a few erroneous predictions are kept so that their heatmaps can be compared with heatmaps from correct predictions. There is no need for k-fold validation here since the validation dataset is completely separate from the training dataset."}, {"heading": "C. Evaluation and XAI implementation", "text": "This part corresponds to part 3 of fig. 1, where heatmaps h are computed using the following XAI methods available in pytorch Captum API: Saliency [24], Input*Gradient [25], DeepLift [3], GuidedBackprop [26], GuidedGradCam [2], Deconvolution [27], GradientShap [28], DeepLiftShap [28]. Integrated-Gradients [29] has been excluded as it is comparatively inefficient with ResNet. Note also that the original implementation of Layerwise Relevance Propagation (LRP) [4] has been shown to be equivalent to gradient*input or DeepLIFT depending on a few conditions [30]. For all heatmaps, we compute the heatmaps derived from the predicted values, not the true values (for some XAI methods, explanation can be extracted from the probability of predicting not only the correct class, but also other classes). The following is the sequence of processing leading to the final results.\nChannel adjustments. Each heatmap h, which has (C, H, W) shape (C=3 for 3 color channels), is compressed along the channels to (H, W) by sum-pixel-over-channels, where the values are summed pixel-wise along all channel, i.e. sc(hij) = \u03a3c=1,2,3hij,c when written component-wise. This\nis so that it can be compared with h0 of shape (H, W). Normalization to [\u22121, 1] is also performed by absolute-maxbefore-sum scheme, so the overall channel adjustment process is h \u2192 h/max(|h|) \u2192 sc(h) \u2192 h/max(|h|). max(|h|) is the maximum absolute value over all pixels in that single heatmap. The practice of summing over channels can be seen, for example, in LRP tutorial site [31].\nFive-band stratification. Adjusted heatmaps h will subsequently be evaluated using five-band score, where each pixel needs to be assigned one of the five values that have been previously described. The value 2 is designated for discriminative feature, 1 for localization, 0 for irrelevant background, while -1 and -2 are symmetrically defined for negative contribution to model prediction or decision. Recall that our ground-truth heatmaps h0 pixels have been assigned one of the following values 0, 0.4 and 0.9. Regardless of the intermediate processing of the heatmap h, the mapping for h0 is always such that 0.9\u2192 2, 0.4\u2192 1 and 0. To map h, which has been normalized to [\u22121, 1] by now, a threshold of the form t = [\u2212t(2),\u2212t(1), t(1), t(2)] is used, so that for each pixel hij , a transformation we refer to as five-band stratification is performed in the following manner: hij \u2192 2 if hij > t(2), to 1 if hij \u2208 (t(1), t(2)], to 0 if hij \u2208 (\u2212t(1), t(1)], to \u22121 if hij \u2208 (\u2212t(2),\u2212t(1)] and to \u22122 if hij \u2264 \u2212t(2). Bracketed sub-script here is used to denote the component of t if it is regarded as a vector for notational convenience later. Up to this point, we have S5(h0), S5t (h) where S\n5 denotes the five-band stratification.\nFive-band score. After stratification, for each heatmap, we compute accuracy A, precision P = TPTP+FP+ , R =\nTP TP+FN+ , where accuracy is the fraction of correctly assigned hij pixel over the total number of pixels, TP is the number of true positive pixels, FP false positives, FN false negatives and = 10\u22126 for smoothing. TP is slightly different from TP used in binary case. We only count TP when h0,ij 6= 0 and hij = h0,ij , i.e. we use the stringent condition where the labels for localization and features must be correctly hit to achieve a true positive. Likewise, FP is counted when h0,ij = 0 and hij 6= 0 plus h0,ij 6= 0 and hij 6= h0,ij whereas FN when h0,ij 6= 0 and hij = 0. To plot receiver operating characteristics (ROC), false positive rate FPR = FPFP+TN+ is also computed, where TN is the number of true negatives hij = h0,ij = 0.\nSoft five-band scores. As seen, the threshold defined above is sharp, and the value near any of the thresholds \u00b1t(i) might not be properly accounted for. We thus instead use soft five-band scores, where the metrics are collected for different thresholds. More precisely, for the k-th data sample, we obtain (A,R, P )(k)tm for tm = [\u22120.5+md,\u22120.3+md, 0.3\u2212 md, 0.5 \u2212 md] where d = 0.005, m = 0, 1, \u00b7 \u00b7 \u00b7 , nsoft, and nsoft = 55 after comparing the stratified ground-truth S5(h0) with S5tm(h), where h has undergone channel adjustment process previously described. The best and average values of X = A,R, P for sample k over the different thresholds, X (k) avg =\n1 nsoft \u03a3tmX (k) tm and X (k) best = maxtm{X (k) tm } respec-\ntively, are then saved sample by sample into a csv file in the XAI result folder for analysis in the discussion section. These values are identified by their positions among the shards, the predicted class and the true class.\nReceiver operating characteristic. To compare the performances of different XAI methods mentioned above, ROC is also obtained as shown in fig. 4. For each threshold tm, mean values of {FPR(k)tm } and {R (k) tm } over all samples in the evaluation datasets contribute to a single point in the figure. Unlike the usual binary ROC, changing thresholds in the multidimensional space we defined does not guarantee the change from FP to TP (or vice versa). For example, a point that begins as FN that predicts label 0 can become TP or FP if the true value are 1 and 2 respectively when the 0 thresholds are lowered. Hence, we will not always obtain a curve that starts with (0, 0) and ends with (1, 1) in the ROC space, unlike the usual ROC curve. Regardless, by simple understanding of rate of change of FPR and recall, the usual rule of thumb that assigns steeper increase in recall to better ROC quality should still hold. Mathematically, the more optimal ROC curve lies nearer the top-left vertices of the convex hull formed by the points. There has been studies on multi-dimensional ROC curve with its \u201carea under volume\u201d [32, 33], though the difficulty of observing them makes them unsuitable for visual comparison here. With the definition of TP, FP, TN, FN above, we have instead created pseudo-binary conditions."}, {"heading": "III. DISCUSSION", "text": ""}, {"heading": "A. Recall vs Prediction", "text": "We provide recall vs precision scores as shown in fig. 3. Each point in the plot corresponds to an XAI method, for example Saliency, applied on a single branch of the corresponding model trained from the base model. There are 5 points per method as we have trained 5 branches per architecture. Naturally, the higher P and R are, the better is the XAI method. Each point can be denoted by (Rstat, Pstat), where Xstat = 1 N\u03a3 N k=1X (k) stat, X = R,P , stat = avg, best and N is the number of data sample in the evaluation dataset. Thus fig. 3(A) is a plot of Ravg vs Pavg for ResNet, AlexNet and VGG respectively. Likewise, fig. 3(B) is Rbest vs Pbest. After qualitative assessment of some of the generated heatmaps, we perform similar analysis by applying clamping to heatmap values after the first normalization process, following roughly the idea in [34]. In other words, the channel adjustment process described in the previous section is changed to h \u2192 h/max(|h|) \u2192 C[c1,c2](h) \u2192 sc(h) \u2192 h/max(|h|) where C[c1,c2](hij) = c2 if hij \u2265 c2, C[c1,c2](hij) = c1 if hij \u2264 c1 and otherwise C[c1,c2](hij) = hij . A different set of soft thresholds has been used to match the clamping process as well, with tm = [\u22120.9 +md,\u22120.5 +md, 0.5\u2212md, 0.9\u2212md] where d = 0.01, m = 0, 1, \u00b7 \u00b7 \u00b7 , nsoft, and nsoft = 40 with the clamping threshold given by [c1, c2] = [\u22120.1, 0.1]. Fig. 3(C) is thus the same as fig. 3(B), except with clamping process applied, where we do observe some changes in the precision and recall scores, more notably for AlexNet and VGG, though not necessarily better.\nRecall scores are generally low in most of the points in fig. 3, indicating high FN. The first obvious cause is the fact that most XAI methods in all architectures appear to assign 0 values to regions that contain either localization pixels or discriminative features. For example, fig. 5 shows the heatmaps from different channels R, G and B (extracted before summing pixel over channels). The heatmaps generally appear granular and non-continuous, having many white pixels in between the red pixels, thus contributing to false negatives. Furthermore, most of the inner body of the cells (represented by light red color in the ground-truth h0) is completely unmarked by most of the XAI methods, contributing to very large amount of false negatives. The highest recall values in fig. 3(A) are attained by Saliency applied on AlexNet. This is consistent with visual inspection of the heatmaps across different methods and architectures, because Saliency assigns a lot more red pixels in relevant regions while other methods often assign blue pixels (negative values) in unpredictable manner and highlight only the edges.\nSimilar to the heatmaps shown in fig. 5 produced by Guided GradCAM applied on VGG, many of the XAI methods only highlight the edges of the cell borders, sometimes faintly. As such, comparatively high recall values for Saliency can be qualitatively accounted for by the halo of high-valued heatmap encompassing the relevant area, although not in a very precise\nand compact manner. Deconvolution, on the other hand, has relatively higher recall scores due to the large amount of artifact pixels. The quality of its heatmaps has been therefore undermined, reflected as low precision score. Other methods such as guided GradCAM are still capable of highlighting some of the relevant regions, and, to reiterate, many of them tend to highlight the edges as seen in the heatmaps in the supp. material. Also, AlexNet tends to produce denser heatmaps than the other two, giving rise to slightly higher recall scores than VGG, while the average scores for ResNet are very low. Depending on the context, different XAI methods can be the\nbetter choice based on their strength and weaknesses, although adjustment to existing interpretations may be necessary.\nDifferences in responses to color channels are also observed. Saliency method appears as positive values (red) in all channels as shown in fig. 5, although type 3 cell has only 1 color channel whose input signal is strong because it has border whose pre-dominant color is red; in the implementation, the normalized border color is roughly (0.8, 0.1, 0.1) with small uniform random perturbation. On the other hand, Guided GradCAM marks green and blue channels with negative values. If they are to be interpreted as negative contribution, the interpretation will be consistent. But when the heatmaps are summed over channels as we have done, the offsetting effect of the negative values become questionable. In other methods, such color responses are variable. For example, input*gradient for AlexNet do appear to exhibit color responses as well (not shown), although the quality is highly variable too. It is thus difficult to strongly recommend any one method specializing on color-detection, even for guided GradCAM.\nInterpretation of heatmap values. Fig. 6 shows in column h the heatmaps obtained after summing pixel over channels, one of the earlier processes in the previous section. The figure shows the effect of soft five-band stratification as well, which demonstrates that the appropriate selection thresholding does affect the scores. In previous section, we addressed this by distinguishing between the best and average of recall and precision values over the soft thresholds, which is the main purpose of fig. 3(B). The effect of threshold change is variable across different XAI methods. If we focus on recall scores, from visual inspection of fig. 6(B), the XAI community may need to revise the idea of negative values in heatmaps. Clearly, DeepLift and DeepLiftShap examples show that they will score much better recall if we take the absolute values of the heatmaps and apply the same process from stratification to the computation of five-band scores.\nSHAP, DeepLift and background effect. When SHAP is applied to DeepLift, the effect appears to be background artifact removals, thus confining non-zero heatmap pixel values to more relevant regions (fig. 6(B) and supp. materials). Still, we need to point out that the heatmaps could be inconsistent even from the correct predictions of the same classes, as shown in fig. 6(C). The figure shows two heatmaps of different qualities generated by DeepLift for VGG for cell type 8 that are correctly predicted. It may be tempting to make guesses regarding possible reasons, such as the backgrounds. More investigation on the signals activated by the background may be necessary.\nFrom observing fig. 3 and many heatmaps, for examples the figures in the appendix, it is tempting to deduce that deeper networks (AlexnNet shallowest, followed by VGG, then ResNet deepest) tend to produce heatmaps that are more sensitive to the edges but cover less thoroughly the bulk of discriminative features and localization regions. To test this, we conduct a test on AlexNet modified by systematically adding more and more convolutional layers, trained and then evaluated for its precision vs recall in the same manner as\nbefore. The number of layers added are 1, 2, ..., 8, and the plot is made by computing mean values of recall and precision like before, except that the points are collected separately based on the predicted values (whereas in the previous section, averages are taken over all test samples regardless of predicted values). The expectation is for the precision and recall values to be nearer to 1 (more towards top right of the plot) for the modified AlexNet with less additional layers. However, as shown in appendix fig. 1, this does not appear to be the case."}, {"heading": "B. ROC curve", "text": "ROC plot in fig. 4 shows that most heatmap methods tested lie on traditionally poor ROC regions. There appears to be trade-offs between higher recall values (which is good) and higher FPR (which is bad), most prominently shown by Saliency method. Deconvolution appears to be the best, as it has the greatest rate of increasing recall compared to FPR. However, this is misleading, since deconvolution starts with many FP predictions in all three architecture, as shown by the grid-like artifacts in fig. 5. This causes FP to change more quickly, and the ROC fails to make good comparison between deconvolution and other methods. Saliency tends to \u201cover-assign\u201d the heatmap pixels around the correct region; consider fig. 5, 6 and appendix, compare it to, for example, Guided GradCAM, DeepLift. Unlike DeepLift and DeepLiftShap, Saliency ROC shows higher recall because of more correct assignments of positive (red) values, but also higher FPR because of the assignment of positive values in supposedly white regions. Guided GradCAM appears to have some difficulty improving through the change of thresholds. Considering the way sum-pixel-over-channels is performed and its color-channel sensitivity, its performance might have suffered through incompatible heatmap pre-processing. ROC for other methods do not provide sufficiently distinct trends that favor the adoption of one method over another. There may be a need to investigate the different ways soft-thresholding can be performed for specific XAI methods to at least bring the ROC curves to traditionally favorable regions."}, {"heading": "C. Other observations", "text": "For images of type 9 (no cell present), generally we see heatmaps in the form of artifacts appearing as well-spaced spots, forming lattice (see appendix fig. 40 etc), similar to the heatmaps from deconvolution method in fig. 5. In some cases, for example appendix fig. 17 row 2, we can see that DeepLift is able to \u201cprovide\u201d the correct reasoning for wrong prediction. In that figure, type 0 cell that has shape that almost looked like a single tail was mistaken as type 6, though some similar wrong predictions are not highlighted in similar manner. In many other cases of wrong predictions (see the heatmaps in the appendix), it is unclear what the highlighted regions mean."}, {"heading": "IV. CONCLUSION", "text": "Recommendations and Caveats. Regardless of the imperfect performance, relative comparisons between the XAI methods can be made.\n\u2022 Saliency method appears to highlight the relevant regions in the most conservative way, which is more suitable for localization in the case where false positives are not important. In particular, AlexNet is scoring the highest recall. \u2022 If only the edge of the features are needed, VGG and ResNet with input*grad, DeepLift, DeepLiftShap seem to be the reasonable choices, while the same heatmap methods for Alexnet seem to produce heatmaps that go beyond capturing just the edges in rather inconsistent ways. Compared to Saliency, they may be more useful to detect small, hard to observe discriminative features, e.g. from medical images and other dense images. \u2022 The heatmaps produced by ResNet appear to be sparsest, followed by VGG then AlexNet. Input size and depth of networks may be the reasons. \u2022 A research into the role of negative values in the heatmaps may be necessary. If we continue with the interpretation that negative values correspond to negative contribution of prediction, some XAI methods such as DeepLift and DeepLiftShap may be completely incomprehensible. \u2022 More investigation may be needed to find the best channel adjustment, to handle the phenomenon where large continuous patches or areas are ignored by many of the tested methods and the activation of signals caused by the background.\nWe have provided an algorithm to produce synthetic data that we hope can be a baseline for testing XAI method, especially in the form of saliency maps or heatmaps. Some XAI methods appear to be more suitable for localization, while others are more responsive to the edges of the features. Modifications required to boost the explainability power of XAI methods might differ across the methods, making fair comparison a possibly difficult task. At least, for each application of XAI method, we should attempt to find a clear, consistent interpretation under the same context of study. For example, if negative values need to be treated with absolute values in some application, at least an accompanying experiment is needed to show the effect and implication of performing such transformation. As of now, XAI still remains a challenging problem. However, it does exhibit good potential to improve the reliability of black-box models in the future."}, {"heading": "ACKNOWLEDGMENT", "text": "This research was supported by Alibaba Group Holding Limited, DAMO Academy, Health-AI division under AlibabaNTU Talent Program. The program is the collaboration between Alibaba and Nanyang Technological university, Singapore."}, {"heading": "A. Heatmaps arranged according to XAI methods", "text": "The following are heatmaps arranged according to the XAI methods. They are similar to fig. 6 in the main text, and are obtained from all the correct predictions of the respective class, unless specified otherwise."}, {"heading": "B. Saliency", "text": ""}, {"heading": "C. DeepLift", "text": "Fig. 15. VGG, DeepLift.\nFig. 17. AlexNet, DeepLift. All predictions above are wrong; the predicted class is y, ground-truth is y0."}, {"heading": "D. DeepLiftShap", "text": ""}, {"heading": "E. GradientShap", "text": "F. InputXGrad\nFig. 34. ResNet, Input*Gradient.\nFig. 35. ResNet, Input*Gradient.\nFig. 37. AlexNet, Input*Gradient."}, {"heading": "G. Cell type 9", "text": ""}], "title": "Quantifying Explainability of Saliency Methods in Deep Neural Networks", "year": 2020}