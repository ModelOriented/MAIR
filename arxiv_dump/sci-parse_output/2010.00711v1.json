{
  "abstractText": "Recent years have seen important advances in the quality of state-of-the-art models, but this has come at the expense of models becoming less interpretable. This survey presents an overview of the current state of Explainable AI (XAI), considered within the domain of Natural Language Processing (NLP). We discuss the main categorization of explanations, as well as the various ways explanations can be arrived at and visualized. We detail the operations and explainability techniques currently available for generating explanations for NLP model predictions, to serve as a resource for model developers in the community. Finally, we point out the current gaps and encourage directions for future work in this important research area.",
  "authors": [
    {
      "affiliations": [],
      "name": "Marina Danilevsky"
    },
    {
      "affiliations": [],
      "name": "Kun Qian"
    },
    {
      "affiliations": [],
      "name": "Ranit Aharonov"
    },
    {
      "affiliations": [],
      "name": "Yannis Katsis"
    },
    {
      "affiliations": [],
      "name": "Ban Kawas"
    },
    {
      "affiliations": [],
      "name": "Prithviraj Sen"
    }
  ],
  "id": "SP:7f62cc5ad8d0bd8eaafe538d696681bfa937e82a",
  "references": [
    {
      "authors": [
        "Abdalghani Abujabal",
        "Rishiraj Saha Roy",
        "Mohamed Yahya",
        "Gerhard Weikum."
      ],
      "title": "Quint: Interpretable question answering over knowledge bases",
      "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing: Sys-",
      "year": 2017
    },
    {
      "authors": [
        "A. Adadi",
        "M. Berrada."
      ],
      "title": "Peeking inside the black-box: A survey on explainable artificial intelligence (xai)",
      "venue": "IEEE Access, 6:52138\u201352160.",
      "year": 2018
    },
    {
      "authors": [
        "Zakaria Aldeneh",
        "Emily Mower Provost."
      ],
      "title": "Using regional saliency for speech emotion recognition",
      "venue": "2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 2741\u20132745. IEEE.",
      "year": 2017
    },
    {
      "authors": [
        "David Alvarez-Melis",
        "Tommi Jaakkola."
      ],
      "title": "A causal framework for explaining the predictions of black-box sequence-to-sequence models",
      "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 412\u2013",
      "year": 2017
    },
    {
      "authors": [
        "Aida Amini",
        "Saadia Gabriel",
        "Shanchuan Lin",
        "Rik Koncel-Kedziorski",
        "Yejin Choi",
        "Hannaneh Hajishirzi."
      ],
      "title": "MathQA: Towards interpretable math word problem solving with operation-based formalisms",
      "venue": "Proceedings of the 2019 Conference",
      "year": 2019
    },
    {
      "authors": [
        "Sattigeri",
        "Karthikeyan Shanmugam",
        "Moninder Singh",
        "Kush R. Varshney",
        "Dennis Wei",
        "Yi Zhang."
      ],
      "title": "One explanation does not fit all: A toolkit and taxonomy of ai explainability techniques",
      "venue": "ArXiv, abs/1909.03012.",
      "year": 2019
    },
    {
      "authors": [
        "M. Aubakirova",
        "M. Bansal."
      ],
      "title": "Interpreting neural networks to improve politeness comprehension",
      "venue": "Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (Austin, Texas, 2016), page 2035\u20132041.",
      "year": 2016
    },
    {
      "authors": [
        "AmirAli Bagher Zadeh",
        "Paul Pu Liang",
        "Soujanya Poria",
        "Erik Cambria",
        "Louis-Philippe Morency."
      ],
      "title": "Multimodal language analysis in the wild: CMUMOSEI dataset and interpretable dynamic fusion graph",
      "venue": "Proceedings of the 56th Annual Meeting of",
      "year": 2018
    },
    {
      "authors": [
        "Dzmitry Bahdanau",
        "Kyunghyun Cho",
        "Yoshua Bengio."
      ],
      "title": "Neural machine translation by jointly learning to align and translate",
      "venue": "ICLR.",
      "year": 2015
    },
    {
      "authors": [
        "Francesco Barbieri",
        "Luis Espinosa-Anke",
        "Jose Camacho-Collados",
        "Steven Schockaert",
        "Horacio Saggion."
      ],
      "title": "Interpretable emoji prediction via label-wise attention LSTMs",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in",
      "year": 2018
    },
    {
      "authors": [
        "Dimitris Bertsimas",
        "Arthur Delarue",
        "Patrick Jaillet",
        "S\u00e9bastien Martin."
      ],
      "title": "The price of interpretability",
      "venue": "ArXiv, abs/1907.03419.",
      "year": 2019
    },
    {
      "authors": [
        "Nikita Bhutani",
        "Kun Qian",
        "Yunyao Li",
        "H.V. Jagadish",
        "Mauricio Hernandez",
        "Mitesh Vasa."
      ],
      "title": "Exploiting structure in representation of named entities using active learning",
      "venue": "Proceedings of the 27th International Conference on Computational Linguis-",
      "year": 2018
    },
    {
      "authors": [
        "Samuel Carton",
        "Qiaozhu Mei",
        "Paul Resnick."
      ],
      "title": "Extractive adversarial networks: High-recall explanations for identifying personal attacks in social media posts",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Process-",
      "year": 2018
    },
    {
      "authors": [
        "Diogo V. Carvalho",
        "Eduardo M. Pereira",
        "Jaime S. Cardoso."
      ],
      "title": "Machine Learning Interpretability: A Survey on Methods and Metrics",
      "venue": "Electronics, 8(8):832. Number: 8 Publisher: Multidisciplinary Digital Publishing Institute.",
      "year": 2019
    },
    {
      "authors": [
        "Danilo Croce",
        "Daniele Rossini",
        "Roberto Basili."
      ],
      "title": "Explaining non-linear classifier decisions within kernel-based deep architectures",
      "venue": "Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for",
      "year": 2018
    },
    {
      "authors": [
        "Danilo Croce",
        "Daniele Rossini",
        "Roberto Basili."
      ],
      "title": "Auditing deep learning processes through kernel-based explanatory models",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
      "year": 2019
    },
    {
      "authors": [
        "Yue Dong",
        "Zichao Li",
        "Mehdi Rezagholizadeh",
        "Jackie Chi Kit Cheung."
      ],
      "title": "EditNTS: An neural programmer-interpreter model for sentence simplification through explicit editing",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Com-",
      "year": 2019
    },
    {
      "authors": [
        "Sahibsingh A Dudani."
      ],
      "title": "The distance-weighted k-nearest-neighbor rule",
      "venue": "IEEE Transactions on Systems, Man, and Cybernetics, (4):325\u2013327.",
      "year": 1976
    },
    {
      "authors": [
        "Shi Feng",
        "Eric Wallace",
        "Alvin Grissom II",
        "Mohit Iyyer",
        "Pedro Rodriguez",
        "Jordan Boyd-Graber."
      ],
      "title": "Pathologies of neural models make interpretations difficult",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,",
      "year": 2018
    },
    {
      "authors": [
        "Yaroslav Ganin",
        "Evgeniya Ustinova",
        "Hana Ajakan",
        "Pascal Germain",
        "Hugo Larochelle",
        "Fran\u00e7ois Laviolette",
        "Mario Marchand",
        "Victor Lempitsky"
      ],
      "title": "Domain-adversarial training of neural networks. JMLR",
      "year": 2016
    },
    {
      "authors": [
        "Nicolas Garneau",
        "Jean-Samuel Leboeuf",
        "Luc Lamontagne."
      ],
      "title": "Predicting and interpreting embeddings for out of vocabulary words in downstream tasks",
      "venue": "Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neu-",
      "year": 2018
    },
    {
      "authors": [
        "Reza Ghaeini",
        "Xiaoli Fern",
        "Prasad Tadepalli."
      ],
      "title": "Interpreting recurrent and attention-based neural models: a case study on natural language inference",
      "venue": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,",
      "year": 2018
    },
    {
      "authors": [
        "Fr\u00e9deric Godin",
        "Kris Demuynck",
        "Joni Dambre",
        "Wesley De Neve",
        "Thomas Demeester"
      ],
      "title": "Explaining character-aware neural networks for word-level prediction: Do they discover linguistic rules",
      "venue": "In Proceedings of the 2018 Conference on Empirical Meth-",
      "year": 2018
    },
    {
      "authors": [
        "Riccardo Guidotti",
        "Anna Monreale",
        "Salvatore Ruggieri",
        "Franco Turini",
        "Fosca Giannotti",
        "Dino Pedreschi."
      ],
      "title": "A survey of methods for explaining black box models",
      "venue": "ACM Comput. Surv., 51(5).",
      "year": 2018
    },
    {
      "authors": [
        "Pankaj Gupta",
        "Hinrich Sch\u00fctze."
      ],
      "title": "LISA: Explaining recurrent neural network judgments via layer-wIse semantic accumulation and example to pattern transformation",
      "venue": "Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and In-",
      "year": 2018
    },
    {
      "authors": [
        "Joseph Y. Halpern."
      ],
      "title": "Actual Causality",
      "venue": "MIT Press.",
      "year": 2016
    },
    {
      "authors": [
        "David Harbecke",
        "Robert Schwarzenberg",
        "Christoph Alt."
      ],
      "title": "Learning explanations from language data",
      "venue": "Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 316\u2013318, Brus-",
      "year": 2018
    },
    {
      "authors": [
        "Sepp Hochreiter",
        "J\u00fcrgen Schmidhuber."
      ],
      "title": "Long short-term memory",
      "venue": "Neural Computation.",
      "year": 1997
    },
    {
      "authors": [
        "Shiou Tian Hsu",
        "Changsung Moon",
        "Paul Jones",
        "Nagiza Samatova."
      ],
      "title": "An interpretable generative adversarial approach to classification of latent entity relations in unstructured sentences",
      "venue": "AAAI Conference on Artificial Intelligence.",
      "year": 2018
    },
    {
      "authors": [
        "Sarthak Jain",
        "Byron C. Wallace."
      ],
      "title": "Attention is not Explanation",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Pa-",
      "year": 2019
    },
    {
      "authors": [
        "Yichen Jiang",
        "Mohit Bansal."
      ],
      "title": "Self-assembling modular networks for interpretable multi-hop reasoning",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natu-",
      "year": 2019
    },
    {
      "authors": [
        "Yichen Jiang",
        "Nitish Joshi",
        "Yen-Chun Chen",
        "Mohit Bansal."
      ],
      "title": "Explore, propose, and assemble: An interpretable model for multi-hop reading comprehension",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguis-",
      "year": 2019
    },
    {
      "authors": [
        "Dongyeop Kang",
        "Varun Gangal",
        "Ang Lu",
        "Zheng Chen",
        "Eduard Hovy."
      ],
      "title": "Detecting and explaining causes from text for a time series event",
      "venue": "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2758\u2013",
      "year": 2017
    },
    {
      "authors": [
        "Sweta Karlekar",
        "Tong Niu",
        "Mohit Bansal."
      ],
      "title": "Detecting linguistic characteristics of alzheimer\u2019s dementia by interpreting neural models",
      "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Lin-",
      "year": 2018
    },
    {
      "authors": [
        "Shun Kiyono",
        "Sho Takase",
        "Jun Suzuki",
        "Naoaki Okazaki",
        "Kentaro Inui",
        "Masaaki Nagata."
      ],
      "title": "Unsupervised token-wise alignment to improve interpretation of encoder-decoder models",
      "venue": "Proceedings of the 2018 EMNLP Workshop BlackboxNLP:",
      "year": 2018
    },
    {
      "authors": [
        "Freddy Lecue",
        "Krishna Gade",
        "Sahin Cem Geyik",
        "Krishnaram Kenthapadi",
        "Varun Mithal",
        "Ankur Taly",
        "Riccardo Guidotti",
        "Pasquale Minervini."
      ],
      "title": "Explainable ai: Foundations, industrial applications, practical challenges, and lessons learned",
      "venue": "AAAI",
      "year": 2020
    },
    {
      "authors": [
        "Piyawat Lertvittayakumjorn",
        "Francesca Toni."
      ],
      "title": "Human-grounded evaluations of explanation methods for text classification",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
      "year": 2019
    },
    {
      "authors": [
        "Jiwei Li",
        "Xinlei Chen",
        "Eduard Hovy",
        "Dan Jurafsky."
      ],
      "title": "Visualizing and understanding neural models in nlp",
      "venue": "arXiv preprint arXiv:1506.01066.",
      "year": 2015
    },
    {
      "authors": [
        "Qiuchi Li",
        "Benyou Wang",
        "Massimo Melucci."
      ],
      "title": "CNM: An interpretable complex-valued network for matching",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language",
      "year": 2019
    },
    {
      "authors": [
        "Chao-Chun Liang",
        "Shih-Hong Tsai",
        "Ting-Yun Chang",
        "Yi-Chung Lin",
        "Keh-Yih Su."
      ],
      "title": "A meaningbased English math word problem solver with understanding, reasoning and explanation",
      "venue": "Proceedings of COLING 2016, the 26th International Conference",
      "year": 2016
    },
    {
      "authors": [
        "Wang Ling",
        "Dani Yogatama",
        "Chris Dyer",
        "Phil Blunsom."
      ],
      "title": "Program induction by rationale generation: Learning to solve and explain algebraic word problems",
      "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics",
      "year": 2017
    },
    {
      "authors": [
        "Hui Liu",
        "Qingyu Yin",
        "William Yang Wang."
      ],
      "title": "Towards explainable NLP: A generative explanation framework for text classification",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5570\u20135581, Florence,",
      "year": 2019
    },
    {
      "authors": [
        "Ninghao Liu",
        "Xiao Huang",
        "Jundong Li",
        "Xia Hu."
      ],
      "title": "On interpretation of network embedding via taxonomy induction",
      "venue": "Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, KDD \u201918, page",
      "year": 2018
    },
    {
      "authors": [
        "Junyu Lu",
        "Chenbin Zhang",
        "Zeying Xie",
        "Guang Ling",
        "Tom Chao Zhou",
        "Zenglin Xu."
      ],
      "title": "Constructing interpretive spatio-temporal features for multiturn responses selection",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computa-",
      "year": 2019
    },
    {
      "authors": [
        "Ling Luo",
        "Xiang Ao",
        "Feiyang Pan",
        "Jin Wang",
        "Tong Zhao",
        "Ningzi Yu",
        "Qing He"
      ],
      "title": "Beyond polarity: Interpretable financial sentiment analysis with hierarchical query-driven attention",
      "year": 2018
    },
    {
      "authors": [
        "Seungwhan Moon",
        "Pararth Shah",
        "Anuj Kumar",
        "Rajen Subba."
      ],
      "title": "OpenDialKG: Explainable conversational reasoning with attention-based walks over knowledge graphs",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational",
      "year": 2019
    },
    {
      "authors": [
        "James Mullenbach",
        "Sarah Wiegreffe",
        "Jon Duke",
        "Jimeng Sun",
        "Jacob Eisenstein."
      ],
      "title": "Explainable prediction of medical codes from clinical text",
      "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computa-",
      "year": 2018
    },
    {
      "authors": [
        "An Nguyen",
        "Aditya Kharosekar",
        "Matthew Lease",
        "Byron Wallace."
      ],
      "title": "An interpretable joint graphical model for fact-checking from crowds",
      "venue": "AAAI Conference on Artificial Intelligence.",
      "year": 2018
    },
    {
      "authors": [
        "Alexander Panchenko",
        "Fide Marten",
        "Eugen Ruppert",
        "Stefano Faralli",
        "Dmitry Ustalov",
        "Simone Paolo Ponzetto",
        "Chris Biemann."
      ],
      "title": "Unsupervised, knowledge-free, and interpretable word sense disambiguation",
      "venue": "Proceedings of the 2017 Confer-",
      "year": 2017
    },
    {
      "authors": [
        "Nikolaos Pappas",
        "Andrei Popescu-Belis."
      ],
      "title": "Explaining the stars: Weighted multiple-instance learning for aspect-based sentiment analysis",
      "venue": "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages",
      "year": 2014
    },
    {
      "authors": [
        "Pouya Pezeshkpour",
        "Yifan Tian",
        "Sameer Singh."
      ],
      "title": "Investigating robustness and interpretability of link prediction via adversarial modifications",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Compu-",
      "year": 2019
    },
    {
      "authors": [
        "Pouya Pezeshkpour",
        "Yifan Tian",
        "Sameer Singh."
      ],
      "title": "Investigating robustness and interpretability of link prediction via adversarial modifications",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for Computa-",
      "year": 2019
    },
    {
      "authors": [
        "Nina Poerner",
        "Hinrich Sch\u00fctze",
        "Benjamin Roth."
      ],
      "title": "Evaluating neural network explanation methods using hybrid documents and morphosyntactic agreement",
      "venue": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguis-",
      "year": 2018
    },
    {
      "authors": [
        "Nicolas Pr\u00f6llochs",
        "Stefan Feuerriegel",
        "Dirk Neumann."
      ],
      "title": "Learning interpretable negation rules via weak supervision at document level: A reinforcement learning approach",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the",
      "year": 2019
    },
    {
      "authors": [
        "Reid Pryzant",
        "Sugato Basu",
        "Kazoo Sone."
      ],
      "title": "Interpretable neural architectures for attributing an ad\u2019s performance to its writing style",
      "venue": "Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP,",
      "year": 2018
    },
    {
      "authors": [
        "Reid Pryzant",
        "Kelly Shen",
        "Dan Jurafsky",
        "Stefan Wagner."
      ],
      "title": "Deconfounded lexicon induction for interpretable social science",
      "venue": "Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics:",
      "year": 2018
    },
    {
      "authors": [
        "Nazneen Fatema Rajani",
        "Bryan McCann",
        "Caiming Xiong",
        "Richard Socher."
      ],
      "title": "Explain yourself! leveraging language models for commonsense reasoning",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguis-",
      "year": 2019
    },
    {
      "authors": [
        "Nazneen Fatema Rajani",
        "Bryan McCann",
        "Caiming Xiong",
        "Richard Socher."
      ],
      "title": "Explain yourself! leveraging language models for commonsense reasoning",
      "venue": "arXiv preprint arXiv:1906.02361.",
      "year": 2019
    },
    {
      "authors": [
        "Ehud Reiter",
        "Robert Dale."
      ],
      "title": "Building applied natural language generation systems",
      "venue": "Natural Language Engineering, 3(1):57\u201387.",
      "year": 1997
    },
    {
      "authors": [
        "Marco Tulio Ribeiro",
        "Sameer Singh",
        "Carlos Guestrin."
      ],
      "title": "Why should i trust you?\u201d: Explaining the predictions of any classifier",
      "venue": "Proceedings of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
      "year": 2016
    },
    {
      "authors": [
        "Andrew Slavin Ross",
        "Michael C. Hughes",
        "Finale Doshi-Velez."
      ],
      "title": "Right for the right reasons: Training differentiable models by constraining their explanations",
      "venue": "Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelli-",
      "year": 2017
    },
    {
      "authors": [
        "A. Saltelli",
        "M. Ratto",
        "T. Andres",
        "F. Campolongo",
        "J. Cariboni",
        "D. Gatelli",
        "M. Saisana",
        "S. Tarantola."
      ],
      "title": "Global Sensitivity Analysis: The Primer",
      "venue": "John Wiley & Sons.",
      "year": 2008
    },
    {
      "authors": [
        "Robert Schwarzenberg",
        "David Harbecke",
        "Vivien Macketanz",
        "Eleftherios Avramidis",
        "Sebastian M\u00f6ller."
      ],
      "title": "Train, sort, explain: Learning to diagnose translation models",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Asso-",
      "year": 2019
    },
    {
      "authors": [
        "Prithviraj Sen",
        "Yunyao Li",
        "Eser Kandogan",
        "Yiwei Yang",
        "Walter Lasecki."
      ],
      "title": "HEIDL: Learning linguistic expressions with deep learning and human-in-theloop",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: Sys-",
      "year": 2019
    },
    {
      "authors": [
        "Sofia Serrano",
        "Noah A. Smith."
      ],
      "title": "Is attention interpretable? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2931\u20132951, Florence, Italy",
      "venue": "Association for Computational Linguistics.",
      "year": 2019
    },
    {
      "authors": [
        "Karen Simonyan",
        "Andrea Vedaldi",
        "Andrew Zisserman."
      ],
      "title": "Deep inside convolutional networks: Visualising image classification models and saliency maps",
      "venue": "arXiv preprint arXiv:1312.6034.",
      "year": 2013
    },
    {
      "authors": [
        "Mukund Sundararajan",
        "Ankur Taly",
        "Qiqi Yan."
      ],
      "title": "Axiomatic attribution for deep networks",
      "venue": "International Conference on Machine Learning, Sydney, Australia.",
      "year": 2017
    },
    {
      "authors": [
        "Alona Sydorova",
        "Nina Poerner",
        "Benjamin Roth."
      ],
      "title": "Interpretable question answering on knowledge bases and text",
      "venue": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4943\u20134951, Florence, Italy. Asso-",
      "year": 2019
    },
    {
      "authors": [
        "James Thorne",
        "Andreas Vlachos",
        "Christos Christodoulopoulos",
        "Arpit Mittal."
      ],
      "title": "Generating token-level explanations for natural language inference",
      "venue": "Proceedings of the 2019 Conference of the North American Chapter of the Association for",
      "year": 2019
    },
    {
      "authors": [
        "Martin Tutek",
        "Jan \u0160najder."
      ],
      "title": "Iterative recursive attention model for interpretable sequence classification",
      "venue": "Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 249\u2013257, Brussels, Bel-",
      "year": 2018
    },
    {
      "authors": [
        "Ashish Vaswani",
        "Noam Shazeer",
        "Niki Parmar",
        "Jakob Uszkoreit",
        "Llion Jones",
        "Aidan N. Gomez",
        "\u0141ukasz Kaiser",
        "Illia Polosukhin."
      ],
      "title": "Attention is all you need",
      "venue": "NeurIPS.",
      "year": 2017
    },
    {
      "authors": [
        "Nikos Voskarides",
        "Edgar Meij",
        "Manos Tsagkias",
        "Maarten de Rijke",
        "Wouter Weerkamp."
      ],
      "title": "Learning to explain entity relationships in knowledge graphs",
      "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Lin-",
      "year": 2015
    },
    {
      "authors": [
        "Eric Wallace",
        "Shi Feng",
        "Jordan Boyd-Graber."
      ],
      "title": "Interpreting neural networks with nearest neighbors",
      "venue": "Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 136\u2013144, Brussels, Belgium.",
      "year": 2018
    },
    {
      "authors": [
        "Sarah Wiegreffe",
        "Yuval Pinter."
      ],
      "title": "Attention is not not explanation",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-",
      "year": 2019
    },
    {
      "authors": [
        "Qizhe Xie",
        "Xuezhe Ma",
        "Zihang Dai",
        "Eduard Hovy."
      ],
      "title": "An interpretable knowledge transfer model for knowledge base completion",
      "venue": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),",
      "year": 2017
    },
    {
      "authors": [
        "Yang Yang",
        "Deyu Zhou",
        "Yulan He",
        "Meng Zhang."
      ],
      "title": "Interpretable relevant emotion ranking with event-driven attention",
      "venue": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International",
      "year": 2019
    },
    {
      "authors": [
        "Mantong Zhou",
        "Minlie Huang",
        "Xiaoyan Zhu."
      ],
      "title": "An interpretable reasoning network for multirelation question answering",
      "venue": "Proceedings of the 27th International Conference on Computational Linguistics, pages 2010\u20132022.",
      "year": 2018
    }
  ],
  "sections": [
    {
      "heading": "1 Introduction",
      "text": "Traditionally, Natural Language Processing (NLP) systems have been mostly based on techniques that are inherently explainable. Examples of such approaches, often referred to as white box techniques, include rules, decision trees, hidden Markov models, logistic regressions, and others. Recent years, though, have brought the advent and popularity of black box techniques, such as deep learning models and the use of language embeddings as features. While these methods in many cases substantially advance model quality, they come at the expense of models becoming less interpretable. This obfuscation of the process by which a model arrives at its results can be problematic, as it may erode trust in the many AI systems humans interact with daily (e.g., chatbots, recommendation systems, information retrieval algorithms, and many others). In the broader AI community, this growing understanding of the importance of explainability has created an emerging field called Explainable AI (XAI). However, just as tasks in different fields are more amenable to particular approaches, explainability\nmust also be considered within the context of each discipline. We therefore focus this survey on XAI works in the domain of NLP, as represented in the main NLP conferences in the last seven years. This is, to the best of our knowledge, the first XAI survey focusing on the NLP domain.\nAs will become clear in this survey, explainability is in itself a term that requires an explanation. While explainability may generally serve many purposes (see, e.g., Lertvittayakumjorn and Toni, 2019), our focus is on explainability from the perspective of an end user whose goal is to understand how a model arrives at its result, also referred to as the outcome explanation problem (Guidotti et al., 2018). In this regard, explanations can help users of NLP-based AI systems build trust in these systems\u2019 predictions. Additionally, understanding the model\u2019s operation may also allow users to provide useful feedback, which in turn can help developers improve model quality (Adadi and Berrada, 2018).\nExplanations of model predictions have previously been categorized in a fairly simple way that differentiates between (1) whether the explanation is for each prediction individually or the model\u2019s prediction process as a whole, and (2) determining whether generating the explanation requires post-processing or not (see Section 3). However, although rarely studied, there are many additional characterizations of explanations, the most important being the techniques used to either generate or visualize explanations. In this survey, we analyze the NLP literature with respect to both these dimensions and identify the most commonly used explainability and visualization techniques, in addition to operations used to generate explanations (Sections 4.1-Section 4.3). We briefly describe each technique and point to representative papers adopting it. Finally, we discuss the common evaluation techniques used to measure the quality of explanations (Section 5), and conclude with a discussion of gaps and challenges in developing successar X iv :2 01 0.\n00 71\n1v 1\n[ cs\n.C L\n] 1\nO ct\n2 02\n0\nful explainability approaches in the NLP domain (Section 6). Related Surveys: Earlier surveys on XAI include Adadi and Berrada (2018) and Guidotti et al. (2018). While Adadi and Berrada provide a comprehensive review of basic terminology and fundamental concepts relevant to XAI in general, our goal is to survey more recent works in NLP in an effort to understand how these achieve XAI and how well they achieve it. Guidotti et al. adopt a four dimensional classification scheme to rate various approaches. Crucially, they differentiate between the \u201cexplanator\u201d and the black-box model it explains. This makes most sense when a surrogate model is used to explain a black-box model. As we shall subsequently see, such a distinction applies less well to the majority of NLP works published in the past few years where the same neural network (NN) can be used not only to make predictions but also to derive explanations. In a series of tutorials, Lecue et al. (2020) discuss fairness and trust in machine learning (ML) that are clearly related to XAI but not the focus of this survey. Finally, we adapt some nomenclature from Arya et al. (2019) which presents a software toolkit that can help users lend explainability to their models and ML pipelines.\nOur goal for this survey is to: (1) provide the reader with a better understanding of the state of XAI in NLP, (2) point developers interested in building explainable NLP models to currently available techniques, and (3) bring to the attention of the research community the gaps that exist; mainly a lack of formal definitions and evaluation for explainability. We have also built an interactive website providing interested readers with all relevant aspects for every paper covered in this survey. 1"
    },
    {
      "heading": "2 Methodology",
      "text": "We identified relevant papers (see Appendix A) and classified them based on the aspects defined in Sections 3 and 4. To ensure a consistent classification, each paper was individually analyzed by at least two reviewers, consulting additional reviewers in the case of disagreement. For simplicity of presentation, we label each paper with its main applicable category for each aspect, though some papers may span multiple categories (usually with varying degrees of emphasis.) All relevant aspects for every\n1https://xainlp2020.github.io/xainlp/ (we plan to maintain this website as a contribution to the community.)\npaper covered in this survey can be found at the aforementioned website; to enable readers of this survey to discover interesting explainability techniques and ideas, even if they have not been fully developed in the respective publications."
    },
    {
      "heading": "3 Categorization of Explanations",
      "text": "Explanations are often categorized along two main aspects (Guidotti et al., 2018; Adadi and Berrada, 2018). The first distinguishes whether the explanation is for an individual prediction (local) or the model\u2019s prediction process as a whole (global). The second differentiates between the explanation emerging directly from the prediction process (selfexplaining) versus requiring post-processing (posthoc). We next describe both of these aspects in detail, and provide a summary of the four categories they induce in Table 1."
    },
    {
      "heading": "3.1 Local vs Global",
      "text": "A local explanation provides information or justification for the model\u2019s prediction on a specific input; 46 of the 50 papers fall into this category.\nA global explanation provides similar justification by revealing how the model\u2019s predictive process works, independently of any particular input. This category holds the remaining 4 papers covered by this survey. This low number is not surprising given the focus of this survey being on explanations that justify predictions, as opposed to explanations that help understand a model\u2019s behavior in general (which lie outside the scope of this survey)."
    },
    {
      "heading": "3.2 Self-Explaining vs Post-Hoc",
      "text": "Regardless of whether the explanation is local or global, explanations differ on whether they arise as part of the prediction process, or whether their generation requires post-processing following the model making a prediction. A self-explaining approach, which may also be referred to as directly interpretable (Arya et al., 2019), generates the explanation at the same time as the prediction, using information emitted by the model as a result of the process of making that prediction. Decision trees and rule-based models are examples of global self-explaining models, while feature saliency approaches such as attention are examples of local self-explaining models.\nIn contrast, a post-hoc approach requires that an additional operation is performed after the predictions are made. LIME (Ribeiro et al., 2016) is\nan example of producing a local explanation using a surrogate model applied following the predictor\u2019s operation. A paper might also be considered to span both categories \u2013 for example, (Sydorova et al., 2019) actually presents both self-explaining and post-hoc explanation techniques."
    },
    {
      "heading": "4 Aspects of Explanations",
      "text": "While the previous categorization serves as a convenient high-level classification of explanations, it does not cover other important characteristics. We now introduce two additional aspects of explanations: (1) techniques for deriving the explanation and (2) presentation to the end user. We discuss the most commonly used explainability techniques, along with basic operations that enable explainability, as well as the visualization techniques commonly used to present the output of associated explainability techniques. We identify the most common combinations of explainability techniques, operations, and visualization techniques for each of the four high-level categories of explanations presented above, and summarize them, together with representative papers, in Table 2.\nAlthough explainability techniques and visualizations are often intermixed, there are fundamental differences between them that motivated us to treat them separately. Concretely, explanation derivation - typically done by AI scientists and engineers - focuses on mathematically motivated justifications of models\u2019 output, leveraging various explainability techniques to produce \u201craw explanations\u201d (such as attention scores). On the other hand, explanation presentation - ideally done by UX engineers - focuses on how these \u201craw explanations\u201d are best presented to the end users using suitable visualization techniques (such as saliency heatmaps)."
    },
    {
      "heading": "4.1 Explainability Techniques",
      "text": "In the papers surveyed, we identified five major explainability techniques that differ in the mechanisms they adopt to generate the raw mathematical justifications that lead to the final explanation presented to the end users.\nFeature importance. The main idea is to derive explanation by investigating the importance scores of different features used to output the final prediction. Such approaches can be built on different types of features, such as manual features obtained from feature engineering (e.g., Voskarides et al., 2015), lexical features including word/tokens and n-gram (e.g., Godin et al., 2018; Mullenbach et al., 2018), or latent features learned by NNs (e.g., Xie et al., 2017). Attention mechanism (Bahdanau et al., 2015) and first-derivative saliency (Li et al., 2015) are two widely used operations to enable feature importance-based explanations. Text-based features are inherently more interpretable by humans than general features, which may explain the widespread use of attention-based approaches in the NLP domain.\nSurrogate model. Model predictions are explained by learning a second, usually more explainable model, as a proxy. One well-known example is LIME (Ribeiro et al., 2016), which learns surrogate models using an operation called input perturbation. Surrogate model-based approaches are model-agnostic and can be used to achieve either local (e.g., Alvarez-Melis and Jaakkola, 2017) or global (e.g., Liu et al., 2018) explanations. However, the learned surrogate models and the original models may have completely different mechanisms to make predictions, leading to concerns about the fidelity of surrogate model-based approaches.\nExample-driven. Such approaches explain the prediction of an input instance by identifying and presenting other instances, usually from available labeled data, that are semantically similar to the input instance. They are similar in spirit to nearest neighbor-based approaches (Dudani, 1976), and have been applied to different NLP tasks such as text classification (Croce et al., 2019) and question answering (Abujabal et al., 2017).\nProvenance-based. Explanations are provided by illustrating some or all of the prediction derivation process, which is an intuitive and effective explainability technique when the final prediction is the result of a series of reasoning steps. We observe several question answering papers adopt such ap-\nproaches (Abujabal et al., 2017; Zhou et al., 2018; Amini et al., 2019).\nDeclarative induction. Human-readable representations, such as rules (Pro\u0308llochs et al., 2019), trees (Voskarides et al., 2015), and programs (Ling et al., 2017) are induced as explanations.\nAs shown in Table 2, feature importance-based and surrogate model-based approaches have been in frequent use (accounting for 29 and 8, respectively, of the 50 papers reviewed). This should not come as a surprise, as features serve as building blocks for machine learning models (explaining the proliferation of feature importance-based approaches) and most recent NLP papers employ NNbased models, which are generally black-box models (explaining the popularity of surrogate modelbased approaches). Finally note that a complex NLP approach consisting of different components\nmay employ more than one of these explainability techniques. A representative example is the QA system QUINT (Abujabal et al., 2017), which displays the query template that best matches the user input query (example-driven) as well as the instantiated knowledge-base entities (provenance)."
    },
    {
      "heading": "4.2 Operations to Enable Explainability",
      "text": "We now present the most common set of operations encountered in our literature review that are used to enable explainability, in conjunction with relevant work employing each one.\nFirst-derivative saliency. Gradient-based explanations estimate the contribution of input i towards output o by computing the partial derivative of o with respect to i. This is closely related to older concepts such as sensitivity (Saltelli et al., 2008). First-derivative saliency is particularly con-\nvenient for NN-based models because these can be computed for any layer using a single call to auto-differentiation, which most deep learning engines provide out-of-the-box. Recent work has also proposed improvements to first-derivative saliency (Sundararajan et al., 2017). As suggested by its name and definition, first-derivative saliency can be used to enable feature importance explainability, especially on word/token-level features (Aubakirova and Bansal, 2016; Karlekar et al., 2018).\nLayer-wise relevance propagation. This is another way to attribute relevance to features computed in any intermediate layer of an NN. Definitions are available for most common NN layers including fully connected layers, convolution layers and recurrent layers. Layer-wise relevance propagation has been used to, for example, enable feature importance explainability (Poerner et al., 2018) and example-driven explainability (Croce et al., 2018).\nInput perturbations. Pioneered by LIME (Ribeiro et al., 2016), input perturbations can explain the output for input x by generating random perturbations of x and training an explainable model (usually a linear model). They are mainly used to enable surrogate models (e.g., Ribeiro et al., 2016; Alvarez-Melis and Jaakkola, 2017).\nAttention (Bahdanau et al., 2015; Vaswani et al., 2017). Less an operation and more of a strategy to enable the NN to explain predictions, attention layers can be added to most NN architectures and, because they appeal to human intuition, can help indicate where the NN model is \u201cfocusing\u201d. While previous work has widely used attention layers (Luo et al., 2018; Xie et al., 2017; Mullenbach et al., 2018) to enable feature importance explainability, the jury is still out as to how much explainability attention provides (Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019).\nLSTM gating signals. Given the sequential nature of language, recurrent layers, in particular LSTMs (Hochreiter and Schmidhuber, 1997), are commonplace. While it is common to mine the outputs of LSTM cells to explain outputs, there may also be information present in the outputs of the gates produced within the cells. It is possible to utilize (and even combine) other operations presented here to interpret gating signals to aid feature importance explainability (Ghaeini et al., 2018).\nExplainability-aware architecture design. One way to exploit the flexibility of deep learning is to devise an NN architecture that mimics the process\nhumans employ to arrive at a solution. This makes the learned model (partially) interpretable since the architecture contains human-recognizable components. Implementing such a model architecture can be used to enable the induction of human-readable programs for solving math problems (Amini et al., 2019; Ling et al., 2017) or sentence simplification problems (Dong et al., 2019). This design may also be applied to surrogate models that generate explanations for predictions (Rajani et al., 2019a; Liu et al., 2019).\nPrevious works have also attempted to compare these operations in terms of efficacy with respect to specific NLP tasks (Poerner et al., 2018). Operations outside of this list exist and are popular for particular categories of explanations. Table 2 mentions some of these. For instance, Pro\u0308llochs et al. (2019) use reinforcement learning to learn simple negation rules, Liu et al. (2018) learns a taxonomy post-hoc to better interpret network embeddings, and Pryzant et al. (2018b) uses gradient reversal (Ganin et al., 2016) to deconfound lexicons."
    },
    {
      "heading": "4.3 Visualization Techniques",
      "text": "An explanation may be presented in different ways to the end user, and making the appropriate choice is crucial for the overall success of an XAI approach. For example, the widely used attention mechanism, which learns the importance scores of a set of features, can be visualized as raw attention scores or as a saliency heatmap (see Figure 1a). Although the former is acceptable, the latter is more user-friendly and has become the standard way to visualize attention-based approaches. We now present the major visualization techniques identified in our literature review.\nSaliency. This has been primarily used to visualize the importance scores of different types of elements in XAI learning systems, such as showing input-output word alignment (Bahdanau et al., 2015) (Figure 1a), highlighting words in input text (Mullenbach et al., 2018) (Figure 1b) or displaying extracted relations (Xie et al., 2017). We observe a strong correspondence between feature importancebased explainability and saliency-based visualizations; namely, all papers using feature importance to generate explanations also chose saliency-based visualization techniques. Saliency-based visualizations are popular because they present visually perceptive explanations and can be easily understood by different types of end users. They are there-\nfore frequently seen across different AI domains (e.g., computer vision (Simonyan et al., 2013) and speech (Aldeneh and Provost, 2017)). As shown in Table 2, saliency is the most dominant visualization technique among the papers covered by this survey.\nRaw declarative representations. As suggested by its name, this visualization technique directly presents the learned declarative representations, such as logic rules, trees, and programs (Figure 1c and 1d). Such techniques assume that end users can understand specific representations, such as firstorder logic rules (Pezeshkpour et al., 2019a) and reasoning trees (Liang et al., 2016), and therefore may implicitly target more advanced users.\nNatural language explanation. The explanation is verbalized in human-comprehensible natural language (Figure 2). The natural language can be generated using sophisticated deep learning models, e.g., by training a language model with human natural language explanations and coupling with a deep generative model (Rajani et al., 2019a). It can also be generated by using simple templatebased approaches (Abujabal et al., 2017). In fact, many declarative induction-based techniques can use template-based natural language generation (Reiter and Dale, 1997) to turn rules and programs into human-comprehensible language, and this minor extension can potentially make the explanation more accessible to lay users.\nTable 2 references some additional visualiza-\ntion techniques, such as using raw examples to present example-driven approaches (Jiang et al., 2019; Croce et al., 2019) (e.g., Figure 1e), and dependency parse trees to represent input questions (Abujabal et al., 2017)."
    },
    {
      "heading": "5 Explanation Quality",
      "text": "Following the goals of XAI, a model\u2019s quality should be evaluated not only by its accuracy and performance, but also by how well it provides explanations for its predictions. In this section we discuss the state of the field in terms of defining and measuring explanation quality."
    },
    {
      "heading": "5.1 Evaluation",
      "text": "Given the young age of the field, unsurprisingly there is little agreement on how explanations should be evaluated. The majority of the works reviewed (32 out of 50) either lack a standardized evaluation or include only an informal evaluation, while a smaller number of papers looked at more formal evaluation approaches, including leveraging ground truth data and human evaluation. We\nnext present the major categories of evaluation techniques we encountered (summarized in Table 3).\nInformal examination of explanations. This typically takes the form of high-level discussions of how examples of generated explanations align with human intuition. This includes cases where the output of a single explainability approach is examined in isolation (Xie et al., 2017) as well as when explanations are compared to those of other reference approaches (Ross et al., 2017) (such as LIME, which is a frequently used baseline).\nComparison to ground truth. Several works compare generated explanations to ground truth data in order to quantify the performance of explainability techniques. Employed metrics vary based on task and explainability technique, but commonly encountered metrics include P/R/F1 (Carton et al., 2018), perplexity, and BLEU (Ling et al., 2017; Rajani et al., 2019b). While having a quantitative way to measure explainability is a promising direction, care should be taken during ground truth acquisition to ensure its quality and account for cases where there may be alternative valid explanations. Approaches employed to address this issue involve having multiple annotators and reporting inter-annotator agreement or mean human performance, as well as evaluating the explanations at different granularities (e.g., token-wise vs phrasewise) to account for disagreements on the precise value of the ground truth (Carton et al., 2018).\nHuman evaluation. A more direct way to assess the explanation quality is to ask humans to evaluate the effectiveness of the generated explanations. This has the advantage of avoiding the assumption that there is only one good explanation that could serve as ground truth, as well as sidestepping the need to measure similarity of explanations. Here as well, it is important to have multiple annotators, report inter-annotator agreement, and correctly deal with subjectivity and variance in the responses. The approaches found in this survey vary in several dimensions, including the number of humans involved (ranging from 1 (Mullenbach et al., 2018) to\n25 (Sydorova et al., 2019) humans), as well as the high-level task that they were asked to perform (including rating the explanations of a single approach (Dong et al., 2019) and comparing explanations of multiple techniques (Sydorova et al., 2019)).\nOther operation-specific techniques. Given the prevalence of attention layers (Bahdanau et al., 2015; Vaswani et al., 2017) in NLP, recent work (Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019) has developed specific techniques to evaluate such explanations based on counterfactuals or erasure-based tests (Feng et al., 2018). Serrano and Smith repeatedly set to zero the maximal entry produced by the attention layer. If attention weights indeed \u201cexplain\u201d the output prediction, then turning off the dominant weights should result in an altered prediction. Similar experiments have been devised by others (Jain and Wallace, 2019). In particular, Wiegreffe and Pinter caution against assuming that there exists only one true explanation to suggest accounting for the natural variance of attention layers. On a broader note, causality has thoroughly explored such counterfactualbased notions of explanation (Halpern, 2016).\nWhile the above overview summarizes how explainability approaches are commonly evaluated, another important aspect is what is being evaluated. Explanations are multi-faceted objects that can be evaluated on multiple aspects, such as fidelity (how much they reflect the actual workings of the underlying model), comprehensibility (how easy they are to understand by humans), and others. Therefore, understanding the target of the evaluation is important for interpreting the evaluation results. We refer interested readers to (Carvalho et al., 2019) for a comprehensive presentation of aspects of evaluating approaches.\nMany works do not explicitly state what is being evaluated. As a notable exception, (Lertvittayakumjorn and Toni, 2019) outlines three goals of explanations (reveal model behavior, justify model predictions, and assist humans in investigating uncertain predictions) and proposes human evaluation experiments targeting each of them."
    },
    {
      "heading": "5.2 Predictive Process Coverage",
      "text": "An important and often overlooked aspect of explanation quality is the part of the prediction process (starting with the input and ending with the model output) covered by an explanation. We have observed that many explainability approaches explain\nonly part of this process, leaving it up to the end user to fill in the gaps.\nAs an example, consider the MathQA task of solving math word problems. As readers may be familiar from past education experience, in math exams, one is often asked to provide a step-by-step explanation of how the answer was derived. Usually, full credit is not given if any of the critical steps used in the derivation are missing. Recent works have studied the explainability of MathQA models, which seek to reproduce this process (Amini et al., 2019; Ling et al., 2017), and have employed different approaches in the type of explanations produced. While (Amini et al., 2019) explains the predicted answer by showing the sequence of mathematical operations leading to it, this provides only partial coverage, as it does not explain how these operations were derived from the input text. On the other hand, the explanations produced by (Ling et al., 2017) augment the mathematical formulas with text describing the thought process behind the derived solution, thus covering a bigger part of the prediction process.\nThe level of coverage may be an artifact of explainability techniques used: provenance-based approaches tend to provide more coverage, while example-driven approaches, may provide little to no coverage. Moreover, while our math teacher would argue that providing higher coverage is always beneficial to the student, in reality this may depend on the end use of the explanation. For instance, the coverage of explanations of (Amini et al., 2019) may be potentially sufficient for advanced technical users. Thus, higher coverage, while in general a positive aspect, should always be considered in combination with the target use and audience of the produced explanations."
    },
    {
      "heading": "6 Insights and Future Directions",
      "text": "This survey showcases recent advances of XAI research in NLP, as evidenced by publications in major NLP conferences in the last 7 years. We have discussed the main categorization of explanations (Local vs Global, Self-Explaining vs Post-Hoc) as well as the various ways explanations can be arrived at and visualized, together with the common techniques used. We have also detailed operations and explainability techniques currently available for generating explanations of model predictions, in the hopes of serving as a resource for developers interested in building explainable NLP models.\nWe hope this survey encourages the research community to work in bridging the current gaps in the field of XAI in NLP. The first research direction is a need for clearer terminology and understanding of what constitutes explainability and how it connects to the target audience. For example, is a model that displays an induced program that, when executed, yields a prediction, and yet conceals the process of inducing the program, explainable in general? Or is it explainable for some target users but not for others? The second is an expansion of the evaluation processes and metrics, especially for human evaluation. The field of XAI is aimed at adding explainability as a desired feature of models, in addition to the model\u2019s predictive quality, and other features such as runtime performance, complexity or memory usage. In general, trade-offs exist between desired characteristics of models, such as more complex models achieving better predictive power at the expense of slower runtime. In XAI, some works have claimed that explainability may come at the price of losing predictive quality (Bertsimas et al., 2019), while other have claimed the opposite (Garneau et al., 2018; Liang et al., 2016). Studying such possible trade-offs is an important research area for XAI, but one that cannot advance until standardized metrics are developed for evaluating the quality of explanations. The third research direction is a call to more critically address the issue of fidelity (or causality), and to ask hard questions about whether a claimed explanation is faithfully explaining the model\u2019s prediction.\nFinally, it is interesting to note that we found only four papers that fall into the global explanations category. This might seem surprising given that white box models, which have been fundamental in NLP, are explainable in the global sense. We believe this stems from the fact that because white box models are clearly explainable, the focus of the explicit XAI field is in explaining black box models, which comprise mostly local explanations. White box models, like rule based models and decision trees, while still in use, are less frequently framed as explainable or interpretable, and are hence not the main thrust of where the field is going. We think that this may be an oversight of the field since white box models can be a great test bed for studying techniques for evaluating explanations."
    },
    {
      "heading": "A Appendix A - Methodology",
      "text": "This survey aims to demonstrate the recent advances of XAI research in NLP, rather than to provide an exhaustive list of XAI papers in the NLP community. To this end, we identified relevant papers published in major NLP conferences (ACL, NAACL, EMNLP, and COLING) between 2013 and 2019. We filtered for titles containing (lemmatized) terms related to XAI, such as \u201cexplainability\u201d, \u201cinterpretability\u201d, \u201ctransparent\u201d, etc. While this may ignore some related papers, we argue that representative papers are more likely to include such terms in their titles. In particular, we assume\nthat if authors consider explainability to be a major component of their work, they are more likely to use related keywords in the title of their work. Our search criteria yielded a set of 107 papers.\nTop 3 NLP Topics\n1 2 3\nQuestion Answering\n(9)\nComputational Social Science &\nSocial Media (6)\nSyntax: Tagging, Chunking\n& Parsing (6)\nTop 3 Conferences\nDuring the paper review process we first verified whether each paper truly fell within the scope of the survey; namely, papers with a focus on explainability as a vehicle for understanding how a model arrives at its result. This process excluded 57 papers, leaving us with a total of 50 papers. Table 4 lists the top three broad NLP topics (taken verbatim from the ACL call for papers) covered by these 50 papers, and the top three conferences of the set.\nTo ensure a consistent classification, each paper was individually reviewed by at least two reviewers, consulting additional reviewers in the case of disagreement."
    }
  ],
  "title": "A Survey of the State of Explainable AI for Natural Language Processing",
  "year": 2020
}
