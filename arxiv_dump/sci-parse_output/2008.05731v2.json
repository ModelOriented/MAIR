{"abstractText": "In recent years, Artificial Intelligence (AI) has proven its relevance for medical decision support. However, the \u201cblack-box\u201d nature of successful AI algorithms still holds back their wide-spread deployment. In this paper, we describe an eXplanatory Artificial Intelligence (XAI) that reaches the same level of performance as black-box AI, for the task of classifying Diabetic Retinopathy (DR) severity using Color Fundus Photography (CFP). This algorithm, called ExplAIn, learns to segment and categorize lesions in images; the final image-level classification directly derives from these multivariate lesion segmentations. The novelty of this explanatory framework is that it is trained from end to end, with image supervision only, just like black-box AI algorithms: the concepts of lesions and lesion categories emerge by themselves. For improved lesion localization, foreground/background separation is trained through self-supervision, in such a way that occluding foreground pixels transforms the input image into a healthy-looking image. The advantage of such an architecture is that automatic diagnoses can be explained simply by an image and/or a few sentences. ExplAIn is evaluated at the image level and at the pixel level on various CFP image datasets. We expect this new framework, which jointly offers high classification performance and explainability, to facilitate AI deployment.", "authors": [{"affiliations": [], "name": "Gwenol\u00e9 Quelleca"}, {"affiliations": [], "name": "Hassan Al Hajjb"}, {"affiliations": [], "name": "Mathieu Lamardb"}, {"affiliations": [], "name": "Pierre-Henri Conzec"}, {"affiliations": [], "name": "Pascale Massind"}, {"affiliations": [], "name": "B\u00e9atrice Cochenerb"}], "id": "SP:8c04a47e43ba4c9ebcbb5a92a40b3c64d83a1348", "references": [{"authors": ["M.D. Abr\u00e0moff", "Y. Lou", "A. Erginay", "W. Clarida", "R. Amelon", "J.C. Folk", "M. Niemeijer", "Oct."], "title": "Improved automated detection of diabetic retinopathy on a publicly available dataset through integration of deep learning", "venue": "Invest Ophthalmol Vis Sci 57 (13), 5200\u20135206.", "year": 2016}, {"authors": ["J. Ahn", "S. Cho", "S. Kwak", "Jun."], "title": "Weakly supervised learning of instance segmentation with inter-pixel relations", "venue": "In: Proc IEEE CVPR. Long Beach, CA, USA, pp. 2204\u20132213.", "year": 2019}, {"authors": ["T. Ara\u00fajo", "G. Aresta", "L. Mendon\u00e7a", "S. Penas", "C. Maia", "A. Carneiro", "A.M. Mendon\u00e7a", "A. Campilho", "Jul."], "title": "DR|GRADUATE: Uncertaintyaware deep learning-based diabetic retinopathy grading in eye fundus images", "venue": "Med Image Anal 63, 101715.", "year": 2020}, {"authors": ["F. Arcadu", "F. Benmansour", "A. Maunz", "J. Willis", "Z. Haskova", "M. Prunotto", "Sep."], "title": "Deep learning algorithm predicts diabetic retinopathy progression in individual patients", "venue": "NPJ Digit Med 2 (1), 1\u20139.", "year": 2019}, {"authors": ["M.S. Ayhan", "L. K\u00fchlewein", "G. Aliyeva", "W. Inhoffen", "F. Ziemssen", "P. Berens", "Aug."], "title": "Expert-validated estimation of diagnostic uncertainty for deep neural networks in diabetic retinopathy detection", "venue": "Med Image Anal 64, 101724.", "year": 2020}, {"authors": ["S. Bach", "A. Binder", "G. Montavon", "F. Klauschen", "M\u00fcller", "K.-R.", "W. Samek", "Jul."], "title": "On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation", "venue": "PLoS One 10 (7).", "year": 2015}, {"authors": ["S.R. Benoit", "B. Swenor", "L.S. Geiss", "E.W. Gregg", "J.B. Saaddine", "Mar"], "title": "Eye care utilization among insured people with diabetes in the U.S., 2010-2014", "venue": "Diabetes Care", "year": 2019}, {"authors": ["L. Chan", "M.S. Hosseini", "K.N. Plataniotis", "Dec."], "title": "A comprehensive analysis of weakly-supervised semantic segmentation in different image domains", "venue": "Tech. Rep. arXiv:1912.11186 [cs].", "year": 2019}, {"authors": ["Conze", "P.-H.", "A.E. Kavur", "Gall", "E.C.-L.", "N.S. Gezer", "Y.L. Meur", "M.A. Selver", "F. Rousseau", "Jan."], "title": "Abdominal multi-organ segmentation with cascaded convolutional and adversarial deep networks", "venue": "Tech. Rep. arXiv:2001.09521 [cs, eess].", "year": 2020}, {"authors": ["J. Cuadros", "G. Bresnick", "May"], "title": "EyePACS: an adaptable telemedicine system for diabetic retinopathy screening", "venue": "J Diabetes Sci Technol 3 (3), 509\u2013 516.", "year": 2009}, {"authors": ["T. Durand", "T. Mordan", "N. Thome", "M. Cord", "Jul."], "title": "WILDCAT: Weakly supervised learning of deep ConvNets for image classification, pointwise localization and segmentation", "venue": "In: Proc IEEE CVPR. Honolulu, HI, USA, pp. 5957\u20135966.", "year": 2017}, {"authors": ["H.", "D. Hassabis", "G. Rees", "T. Back", "P.T. Khaw", "M. Suleyman", "J. Cornebise", "P.A. Keane", "O. Ronneberger", "Sep."], "title": "Clinically applicable deep learning for diagnosis and referral in retinal disease", "venue": "Nat Med 24 (9), 1342.", "year": 2018}, {"authors": ["N. Frosst", "G. Hinton", "Nov."], "title": "Distilling a neural network into a soft decision tree", "venue": "In: Proc AI*IA Works CEX. Bari, Italy.", "year": 2017}, {"authors": ["B. Goodman", "S. Flaxman", "Oct."], "title": "European Union regulations on algorithmic decision-making and a right to explanation", "venue": "AI Magazine 38 (3), 50\u201357.", "year": 2017}, {"authors": ["T.M. Hehn", "J.F.P. Kooij", "F.A. Hamprecht", "Apr."], "title": "End-to-end learning of decision trees and forests", "venue": "Int J Comput Vis 128 (4), 997\u20131011.", "year": 2020}, {"authors": ["J.C. Javitt", "L.P. Aiello", "Jan."], "title": "Cost-effectiveness of detecting and treating diabetic retinopathy", "venue": "Ann Intern Med 124 (1 Pt 2), 164\u2013169.", "year": 1996}, {"authors": ["Jiang", "P.-T.", "Q. Hou", "Y. Cao", "Cheng", "M.-M.", "Y. Wei", "H. Xiong", "Oct."], "title": "Integral object mining via online attention accumulation", "venue": "In: Proc IEEE ICCV. Seoul, Korea, pp. 2070\u20132079.", "year": 2019}, {"authors": ["H. Kervadec", "J. Dolz", "M. Tang", "E. Granger", "Y. Boykov", "I.B. Ayed", "May"], "title": "Constrained-CNN losses for weakly supervised segmentation", "venue": "Medical Image Analysis 54, 88\u201399.", "year": 2019}, {"authors": ["A. Kolesnikov", "C.H. Lampert", "May"], "title": "Improving weakly-supervised object localization by micro-annotation", "venue": "Tech. Rep. arXiv:1605.05538 [cs].", "year": 2016}, {"authors": ["P. Kontschieder", "M. Fiterau", "A. Criminisi", "S.R. Bul", "Jul."], "title": "Deep neural decision forests", "venue": "In: Proc IJCAI. New York, NY, USA, pp. 4190\u20134194.", "year": 2016}, {"authors": ["S. Kwak", "S. Hong", "B. Han", "Feb."], "title": "Weakly supervised semantic segmentation using superpixel pooling network", "venue": "In: Proc AAAI. San Francisco, CA, USA, pp. 4111\u20134117.", "year": 2017}, {"authors": ["I.H. Laradji", "D. Vazquez", "M. Schmidt", "Sep."], "title": "Where are the masks: Instance segmentation with image-level supervision", "venue": "In: Proc BMVC. Cardiff, UK.", "year": 2019}, {"authors": ["J. Lee", "E. Kim", "S. Lee", "J. Lee", "S. Yoon", "Jun."], "title": "FickleNet: Weakly and semi-supervised semantic image segmentation using stochastic inference", "venue": "In: Proc IEEE CVPR. Long Beach, CA, USA, pp. 5262\u20135271.", "year": 2019}, {"authors": ["Lin", "T.-Y.", "P. Doll\u00e1r", "R. Girshick", "K. He", "B. Hariharan", "S. Belongie", "Jul."], "title": "Feature pyramid networks for object detection", "venue": "In: Proc CVPR. pp. 936\u2013944.", "year": 2017}, {"authors": ["P. Massin", "A. Chabouis", "A. Erginay", "C. Viens-Bitker", "A. Lecleire-Collet", "T. Meas", "Guillausseau", "P.-J.", "G. Choupot", "B. Andr", "P. Denormandie", "Jun."], "title": "OPHDIAT: a telemedical network screening system for diabetic retinopathy in the Ile-de-France", "venue": "Diabetes Metab 34 (3), 227\u2013234.", "year": 2008}, {"authors": ["K. Ogurtsova", "J.D. da Rocha Fernandes", "Y. Huang", "U. Linnenkamp", "L. Guariguata", "N.H. Cho", "D. Cavan", "J.E. Shaw", "L.E. Makaroff", "Jun"], "title": "2017. IDF Diabetes Atlas: Global estimates for the prevalence of diabetes", "venue": "Diabetes Res Clin Pract", "year": 2015}, {"authors": ["G. Papandreou", "Chen", "L.-C.", "K. Murphy", "A.L. Yuille", "Oct."], "title": "Weaklyand semi-supervised learning of a DCNN for semantic image segmentation", "venue": "Tech. Rep. arXiv:1502.02734 [cs].", "year": 2015}, {"authors": ["D. Pathak", "P. Krahenbuhl", "T. Darrell", "Dec."], "title": "Constrained convolutional neural networks for weakly supervised segmentation", "venue": "In: Proc IEEE ICCV. Washington, DC, USA, pp. 1796\u20131804.", "year": 2015}, {"authors": ["S.", "Y. Shen", "L. Dai", "O. Saha", "R. Sathish", "T. Melo", "T. Arajo", "B. Harangi", "B. Sheng", "R. Fang", "D. Sheet", "A. Hajdu", "Y. Zheng", "A.M. Mendona", "S. Zhang", "A. Campilho", "B. Zheng", "D. Shen", "L. Giancardo", "G. Quellec", "F. M\u00e9riaudeau", "Oct."], "title": "IDRiD: Diabetic retinopathy segmentation and grading challenge", "venue": "Medical Image Analysis, 101561.", "year": 2019}, {"authors": ["G. Quellec", "K. Charri\u00e8re", "Y. Boudi", "B. Cochener", "M. Lamard", "Jul."], "title": "Deep image mining for diabetic retinopathy screening", "venue": "Med Image Anal 39, 178\u2013193.", "year": 2017}, {"authors": ["G. Quellec", "M. Lamard", "B. Lay", "A. Le Guilcher", "A. Erginay", "B. Cochener", "P. Massin", "Jun."], "title": "Instant automatic diagnosis of diabetic retinopathy", "venue": "Tech. Rep. arXiv:1906.11875 [cs, eess].", "year": 2019}, {"authors": ["O. Ronneberger", "P. Fischer", "T. Brox", "Oct."], "title": "U-Net: Convolutional networks for biomedical image segmentation", "venue": "In: Proc MICCAI. Munich, Germany, pp. 234\u2013241.", "year": 2015}, {"authors": ["S. Russell", "D. Dewey", "M. Tegmark", "Dec."], "title": "Research priorities for robust and beneficial artificial intelligence", "venue": "AI Magazine 36 (4), 105\u2013114.", "year": 2015}, {"authors": ["W. Samek", "A. Binder", "G. Montavon", "S. Lapuschkin", "K.R. Mller", "Nov."], "title": "Evaluating the visualization of what a deep neural network has learned", "venue": "IEEE Trans Neural Netw Learn Syst 28 (11), 2660\u20132673.", "year": 2017}, {"authors": ["R. D"], "title": "Using a deep learning algorithm and integrated gradients explanation to assist grading for diabetic", "venue": "retinopathy. Ophthalmology", "year": 2019}, {"authors": ["R.R. Selvaraju", "M. Cogswell", "A. Das", "R. Vedantam", "D. Parikh", "D. Batra", "Oct."], "title": "Grad-CAM: Visual explanations from deep networks via gradient-based localization", "venue": "In: Proc IEEE ICCV. Venice, Italy, pp. 618\u2013 626.", "year": 2017}, {"authors": ["W. Shimoda", "K. Yanai", "Oct."], "title": "Distinct class-specific saliency maps for weakly supervised semantic segmentation", "venue": "In: Proc ECCV. Lecture Notes in Computer Science. Springer International Publishing, Amsterdam, The Netherlands, pp. 218\u2013234.", "year": 2016}, {"authors": ["K. Simonyan", "A. Vedaldi", "A. Zisserman", "Apr."], "title": "Deep inside convolutional networks: Visualising image classification models and saliency maps", "venue": "In: ICLR Workshop. Calgary, Canada.", "year": 2014}, {"authors": ["M. Tan", "Q.V. Le", "Jun."], "title": "EfficientNet: Rethinking model scaling for convolutional neural networks", "venue": "In: Proc ICML. Long Beach, CA, USA.", "year": 2019}, {"authors": ["J.B. Tenenbaum", "V. de Silva", "J.C. Langford", "Dec"], "title": "A global geometric framework for nonlinear dimensionality reduction", "venue": "Science", "year": 2000}, {"authors": ["D.S.W. Ting", "L.R. Pasquale", "L. Peng", "J.P. Campbell", "A.Y. Lee", "R. Raman", "G.S.W. Tan", "L. Schmetterer", "P.A. Keane", "T.Y. Wong", "Feb."], "title": "Artificial intelligence and deep learning in ophthalmology", "venue": "Br J Ophthalmol 103 (2), 167\u2013175.", "year": 2019}, {"authors": ["D.S.W. Ting", "L. Peng", "A.V. Varadarajan", "P.A. Keane", "P. Burlina", "M.F. Chiang", "L. Schmetterer", "L.R. Pasquale", "N.M. Bressler", "D.R. Webster", "M. Abramoff", "T.Y. Wong", "Sep."], "title": "Deep learning in ophthalmology: The technical and clinical considerations", "venue": "Prog Retin Eye Res 72, 100759.", "year": 2019}, {"authors": ["Y. Wang", "J. Zhang", "M. Kan", "S. Shan", "X. Chen", "Jun."], "title": "Self-supervised Scale Equivariant Network for Weakly Supervised Semantic Segmentation", "venue": "In: Proc IEEE CVPR.", "year": 2020}, {"authors": ["C.P. Wilkinson", "F.L. Ferris", "R.E. Klein", "P.P. Lee", "C.D. Agardh", "M. Davis", "D. Dills", "A. Kampik", "R. Pararajasegaram", "J.T. Verdaguer", "Sep."], "title": "Proposed international clinical diabetic retinopathy and diabetic macular edema disease severity scales", "venue": "Ophthalmology 110 (9), 1677\u20131682.", "year": 2003}, {"authors": ["S. Xie", "Z. Tu", "Dec."], "title": "Holistically-nested edge detection", "venue": "In: Proc IEEE ICCV. pp. 1395\u20131403.", "year": 2015}, {"authors": ["Y. Yang", "I.G. Morillo", "T.M. Hospedales", "Jul."], "title": "Deep neural decision trees", "venue": "In: Proc ICML Works WHI. Stockholm, Sweden, pp. 34\u201340.", "year": 2018}, {"authors": ["R. Varma", "J.J. Wang", "N. Wang", "S. West", "L. Xu", "M. Yasuda", "X. Zhang", "P. Mitchell", "T.Y. Wong"], "title": "Global prevalence and major risk factors of diabetic retinopathy", "venue": "Diabetes Care", "year": 2012}, {"authors": ["M.D. Zeiler", "R. Fergus", "Sep."], "title": "Visualizing and understanding convolutional networks", "venue": "In: Proc ECCV. Zurich, Switzerland, pp. 818\u2013833.", "year": 2014}, {"authors": ["B. Zhang", "J. Xiao", "Y. Wei", "M. Sun", "K. Huang", "Feb."], "title": "Reliability does matter: An end-to-end weakly supervised semantic segmentation approach", "venue": "In: Proc AAAI. New York, NY, USA.", "year": 2020}, {"authors": ["B. Zhou", "A. Khosla", "A. Lapedriza", "A. Oliva", "A. Torralba", "Jun."], "title": "Learning deep features for discriminative localization", "venue": "In: Proc IEEE CVPR. Las Vegas, NV, USA, pp. 2921\u20132929.", "year": 2016}, {"authors": ["Y. Zhou", "Y. Zhu", "Q. Ye", "Q. Qiu", "J. Jiao", "Jun."], "title": "Weakly supervised instance segmentation using class peak response", "venue": "In: Proc IEEE CVPR. Salt Lake City, UT, USA, pp. 3791\u20133800.", "year": 2018}], "sections": [{"text": "In recent years, Artificial Intelligence (AI) has proven its relevance for medical decision support. However, the \u201cblack-box\u201d nature of successful AI algorithms still holds back their wide-spread deployment. In this paper, we describe an eXplanatory Artificial Intelligence (XAI) that reaches the same level of performance as black-box AI, for the task of classifying Diabetic Retinopathy (DR) severity using Color Fundus Photography (CFP). This algorithm, called ExplAIn, learns to segment and categorize lesions in images; the final image-level classification directly derives from these multivariate lesion segmentations. The novelty of this explanatory framework is that it is trained from end to end, with image supervision only, just like black-box AI algorithms: the concepts of lesions and lesion categories emerge by themselves. For improved lesion localization, foreground/background separation is trained through self-supervision, in such a way that occluding foreground pixels transforms the input image into a healthy-looking image. The advantage of such an architecture is that automatic diagnoses can be explained simply by an image and/or a few sentences. ExplAIn is evaluated at the image level and at the pixel level on various CFP image datasets. We expect this new framework, which jointly offers high classification performance and explainability, to facilitate AI deployment.\nKeywords: explanatory artificial intelligence, self-supervised learning, diabetic retinopathy diagnosis"}, {"heading": "1. Introduction", "text": "Diabetic Retinopathy (DR) is a leading and growing cause of vision impairment and blindness: by 2040, around 600 million people throughout the world will have diabetes (Ogurtsova et al., 2017), a third of whom will have DR (Yau et al., 2012). Early diagnosis is key to slowing down the progression of DR and therefore preventing the occurrence of blindness. Annual retinal screening, generally using Color Fundus Photography (CFP), is thus recommended for all diabetic patients (Javitt and Aiello, 1996). However, the goal of annual screening for all diabetic patients represents a huge burden on ophthalmologists and it is far from being achieved (Benoit et al., 2019). In order to improve DR screening programs, numerous Artificial Intelligence (AI) systems were thus developed to automate DR diagnosis using CFP (Ting et al., 2019b). However, due to the \u201cblack-box\u201d nature of state-of-the-art AI, these systems still need to gain the trust of clinicians and patients.\nTo gain this trust, one solution investigated by Arau\u0301jo et al. (2020) and Ayhan et al. (2020) is to design AI systems able to reliably estimate the uncertainty level of their predictions. This feature is expected to help clinicians know when AI predictions should be carefully reviewed and when they can be trusted. Another solution, investigated by Abra\u0300moff et al. (2016), is to\n\u2217LaTIM - IBRBS - 22, avenue Camille Desmoulins - 29200 Brest, France - Tel.: +33 2 98 01 81 29\nEmail address: gwenole.quellec@inserm.fr (Gwenole\u0301 Quellec)\ndevelop two-stage AI systems that 1) learn to detect or segment lesions considered relevant by ophthalmologists (microaneurysms, exudates, etc.) and 2) base the AI predictions on these detections. Because they mimic ophthalmologists\u2019 reasoning, clinicians are more likely to adopt them. A similar approach was investigated by Fauw et al. (2018) for the classification of optical coherence tomography images. However, these approaches cannot generalize easily to new imaging modalities or new decision problems, such as DR progression prediction (Arcadu et al., 2019), since relevant patterns are not fully known to ophthalmologists. Alternatively, another solution investigated by Quellec et al. (2017) and Sayres et al. (2019) is to help clinicians interpret AI predictions by highlighting image regions supposedly involved in AI predictions. If clinicians agree with highlighted areas, they will more likely trust the AI and eventually adopt it. Note that a similar approach was recently investigated for medical image segmentation (Wickstr\u00f8m et al., 2020). However, these visualization methods provide limited information: they tell us which pixels seem to play a role in the decision process, but they do not tell us precisely how. Although interpretability is an interesting feature, it may not be enough to gain the trust of clinicians. And it is certainly not enough to gain the trust of patients, which would rather have an explanation.\nGilpin et al. (2018) differentiate interpretability and explainability as follows: interpretability is the science of comprehending what a model did or might have done, while explainability is the ability to summarize the reasons for an AI behav-\nPreprint submitted to arXiv September 2, 2020\nar X\niv :2\n00 8.\n05 73\n1v 2\n[ cs\n.C V\n] 1\nS ep\n2 02\n0\nior. Explainability implies interpretability, but the reverse is not always true. To gain the trust of patients and clinicians, explainability is desirable. EXplanatory Artificial Intelligence (XAI) is a growing field of research (Gilpin et al., 2018) motivated by potential AI users, worried about safety (Russell et al., 2015). It is also pushed by European regulations and others: the goal is to grant users the right for an explanation about algorithmic decisions that were made about them (Goodman and Flaxman, 2017). An Explanatory AI system, called ExplAIn, is presented and evaluated in this paper. Unlike visualization methods above Quellec et al. (2017); Sayres et al. (2019), ExplAIn does not attempt to retrospectively analyze a complex classification process. Instead, it modifies the classification process in such a way that it can be understood directly.\n\u201cBlack-box\u201d image classification AI algorithms are usually defined as Convolutional Neural Networks (CNNs) or, more generally, as ensembles of multiple CNNs (Ting et al., 2019a; Quellec et al., 2019). Each of these CNNs is supervised at the image level: given an image, one or several experts indicate which labels should be assigned to this image. To enable explainability, we propose to include a pixel-level classification step into the neural network. Pixel-level classification, also known as image segmentation, is generally performed by an Encoder-Decoder Network (EDN) (Ronneberger et al., 2015; Lin et al., 2017; Conze et al., 2020). EDNs, however, are supervised at the pixel level: given an image, one or several experts assign a label to each pixel in the image. ExplAIn bridges the gap between the two paradigms: pixel-level classification and image-level classification are trained simultaneously, using image-level supervision only.\nThe paper is organized as follows. Related machine learning frameworks are presented in section 2. The proposed ExplAIn solution is described in section 3. This framework is applied to DR diagnosis in section 4. We end up with a discussion and conclusions in section 5."}, {"heading": "2. Related Machine Learning Frameworks", "text": "In terms of purpose, ExplAIn is related to existing algorithms for visualizing/interpreting what image classification CNNs have learnt. Given a trained classification CNN and an input image, these algorithms compute the influence of each pixel on CNN predictions. In the occlusion method, patches are occluded in the input image and the occluded image is run through the CNN: a drop in classification performance indicates that the occluded patch is relevant (Zeiler and Fergus, 2014). In sensitivity analysis, the backpropagation algorithm is used to compute the gradient of CNN predictions with respect to the value of each input pixel (Simonyan et al., 2014). Various improvements on sensitivity analysis, including layer-wise relevance propagation (Bach et al., 2015), also rely on backpropagated quantities to build a high-resolution heatmap showing the relevance of each pixel (Samek et al., 2017; Quellec et al., 2017). Next, Class Activation Mapping (CAM) was proposed for image classification CNNs containing a Global Average Pooling (GAP) layer after the last convolutional layer:\nCAM computes CNN predictions for each of the GAP\u2019s input locations rather than for the GAP\u2019s output, thus providing coarse-resolution class-specific activation maps (Zhou et al., 2016). Grad-CAM generalizes this idea to any classification CNN architecture (Selvaraju et al., 2017). These visualization methods attempt to retrospectively analyze a complex classification process. Another approach, investigated in this paper, is to replace the classification process with one that can be understood directly.\nBecause deep neural networks are hard to interpret, several authors have proposed to train deep neural decision trees (Yang et al., 2018) or deep neural decision forests (Kontschieder et al., 2016; Hehn et al., 2020) instead. These architectures are indeed based on rules that can be interpreted more easily by humans. Frosst and Hinton (2017) thus proposed a training procedure to derive a deep neural decision tree from a deep neural network, so that it can be interpreted. However, understanding a deep decision tree is still a challenging task for machine learning agnostics (Hehn et al., 2020): such an algorithm is interpretable, but not explainable. To enable explanations, the solution investigated in ExplAIn is rather to base the classification process on a segmentation and a categorization of the pathological signs, obtained solely through weak supervision.\nIn that sense, ExplAIn is also related to Weakly-Supervised Semantic Segmentation (WSSS). In WSSS, the goal is to predict pixel classes using image labels only for supervision. In particular, no positional information, like manual segmentations or bounding boxes, is required for supervision. Note that segmentation is the end goal of WSSS, while it is an intermediate step in ExplAIn. WSSS solutions can be classified into four categories (Chan et al., 2019). (1) ExpectationMaximization solutions use image annotations to initialize prior assumptions about the class distribution in images. Next, an EDN is trained to meet those constraints. Then, the prior assumption model is updated based on the EDN features, and the training cycle is repeated again until convergence (Papandreou et al., 2015; Pathak et al., 2015; Kervadec et al., 2019). (2) Multiple-Instance Learning (MIL) solutions train an image classification CNN with image-level supervision and then infer the image locations responsible for each class prediction: inference relies on the MIL assumption that an image belongs to one class if and only if at least one of its pixels does (Shimoda and Yanai, 2016; Durand et al., 2017). (3) Self-supervised learning solutions train an image classification CNN with image-level supervision to obtain a coarse-resolution CAM. Next, a segmentation EDN is trained using the CAM as ground truth to obtain a higher-resolution segmentation (Kolesnikov and Lampert, 2016; Zhang et al., 2020). Interestingly, the Reliable Region Mining (RRM) solution by Zhang et al. (2020) merges the two steps (CAM computation and segmentation) into one. (4) In a final category, object proposals are extracted first and the most probable class is assigned to each of them, using coarseresolution CAMs obtained as above (Kwak et al., 2017; Zhou et al., 2018; Laradji et al., 2019). The self-supervised learning approach seems to be the most popular nowadays (Ahn et al., 2019; Lee et al., 2019; Jiang et al., 2019; Zhang et al., 2020; Wang et al., 2020). It should be noted that ExplAIn is more\ngeneral than WSSS solutions since the number of pixel labels can be different from the number of image labels; in particular, it can be larger. As a result, ExplAIn is more semantically rich and it can lead to better image classification, as it allows for more general \u201cpixel to image\u201d label inference rules.\nNote that the proposed framework is related to the GP-Unet solution by Dubost et al. (2020): both solutions include an EDN inside an image classification CNN, in order to generate high-resolution attention maps. ExplAIn differs in that it generates multiple, complementary maps: one map is generated per type of discriminant patterns in images. It also differs in that image classification derives very simply from the pixel classifications in order to allow explainability, rather than interpretability. Finally, ExplAIn introduces a new criterion, namely the generalized occlusion method, in order to optimize foreground/background separation in the pixel classification maps."}, {"heading": "3. Explanatory Artificial Intelligence", "text": ""}, {"heading": "3.1. Overview and Notations", "text": "This paper addresses multilabel image classification: given an input image I and N image-level labels, the goal is to predict whether or not experts would assign the n-th label to image I, \u2200n \u2208 {1, ...,N}. Let pn \u2208 [0; 1] denote the probabilistic prediction of ExplAIn and let \u03b4I,n \u2208 {0, 1} denote the ground truth: did experts actually assign the n-th label to image I? Unlike multiclass classification, multilabel classification does not assume image-level labels to be mutually exclusive:\u2211N\nn=1 \u03b4I,n \u2208 {0, 1, ...,N}. Images where \u2211N\nn=1 \u03b4I,n = 0 are referred to as \u201cbackground images\u201d, i.e. images where experts did not annotate anything.\nAs an intermediate step, ExplAIn also assigns a label to each (color or grayscale) pixel Ix,y for explainability purposes. Let M denote the number of pixel-level labels:\n\u2022 M can be smaller than, equal to, or larger than the number N of image-level labels,\n\u2022 the first of these pixel-level labels represents \u201cbackground pixels\u201d.\nA pixel-level prediction tensor P is thus calculated, where Pm,x,y \u2208 [0; 1] indicates the probability that pixel Ix,y should be assigned the m-th pixel-level label, with m \u2208 {1, ...,M}. Unlike image-level labels, pixel-level labels are assumed to be mutually exclusive: \u2211M m=1 Pm,x,y = 1 (multiclass classification). Since our framework is trained with image supervision only, we assume that no ground truth is available for pixel-level labels.\nAs illustrated in Fig. 1, the pixel-level and image-level classification problems are solved jointly as follows:\n\u2022 an Encoder-Decoder Network (EDN) s predicts P from I,\n\u2022 a classification head c predicts p = {pn,\u2200n \u2208 {1, ...,N}} from P,\n\u2022 additional branches are included during training, to compute auxiliary losses improving explainability."}, {"heading": "3.2. Pixel-Level Label Prediction", "text": "In order to predict pixel-level labels Pm,x,y for each pixel Ix,y, an EDN s is used: P = s(I). This EDN is composed of an encoder network e and a decoder network d such that s = d \u25e6 e.\nThe encoder part e of s was defined as an EfficientNet (Tan and Le, 2019). EfficientNet is a family of CNN architectures of increasing complexity. The smallest model, namely EfficientNet-B0, was obtained through neural architecture search. Larger models, EfficientNet-B1 to -B7, were then obtained by up-scaling EfficientNet-B0: depth, width and resolution were increased proportionally. Although any classification CNN may be used as backbone for s, EfficientNets were chosen for their clearly superior tradeoff between accuracy and complexity, compared to traditional deep networks.\nThe overall architecture of s was defined as a Feature Pyramid Network (FPN) by Lin et al. (2017). Like the well-know U-Net (Ronneberger et al., 2015), an FPN is a top-down architecture with skip-connections between e and d. Unlike UNet, high-level semantic feature maps are produced at multiple scales: these multi-scale feature maps are then 1) up-scaled to match the size of I, 2) concatenated and 3) processed by a final convolutional layer to obtain the output tensor P, following a deep supervision strategy (Xie and Tu, 2015). Although any EDN architecture may be used, FPN was chosen for its faster convergence.\nBecause pixel-level labels are mutually exclusive, a softmax operator \u00b5 was used as activation function for the last convolutional layer:\n\u00b5(z) =  ezm\u2211M\ni=1 ezi ,\u2200m \u2208 {1, ...,M}  , (1) where z are the outputs of the M neurons at a given pixel location."}, {"heading": "3.3. Image-Level Label Prediction", "text": "In order to predict the vector p of image-level labels from the tensor P of pixel-level labels, a classification head c is used: p = c(P) = c \u25e6 s(I). c has to be very simple to enable explainability. In practice, it consists of two layers only."}, {"heading": "3.3.1. Summary Layer", "text": "The first layer, \u03a0, summarizes each label prediction map Pm by a single scalar value. Two easily understandable summaries may be considered:\n1. the average value, which is proportional to the surface covered by each pixel-level label,\n2. the maximal value, which represents the strongest clue of presence for each pixel-level label.\nIn theory, both solutions enable explainability. However, in practice, the first option has one major drawback: it favors oversegmentation in foreground images, which limits explainability. Indeed, to improve separation between background and foreground images, the EDN is encouraged to increase the average predictions for foreground pixel-level labels in foreground images (and decrease them in background images): one way is to\nL2-norm loss\nincrease the number of foreground pixels in foreground images, i.e. to oversegment. Therefore, the second option was chosen. \u03a0 is thus defined as the global max pooling layer:\n\u03a0(P) = {\nmax x,y\nPm,x,y,\u2200m \u2208 {2, ...,M} } . (2)\nNote that the background probability map P1 is not used for image-level prediction."}, {"heading": "3.3.2. Classification Layer", "text": "The second layer, \u2206, is a special class of dense layers, where neural weights are all positive. The positivity constraint improves explainability: the image-level prediction is defined as a sum of pixel-level clues, each clue being associated with a (positive) confidence level. Layer \u2206 is defined as follows:\n\u2206(z) = \u03c3  M\u2211\nm=2\nzmw2m,n + bn  ,\u2200n \u2208 {1, ...,N}  , (3)\nwhere w2m,n are positive neural weights, bn are biases and \u03c3 is an activation function. Because image-level labels are not mutually exclusive, \u03c3 was defined as a sigmoid function:\n\u03c3(z) = 1\n1 + exp(\u2212z) . (4)"}, {"heading": "3.4. Learning to Detect Background Pixels", "text": "In ExplAIn, pixel-level labels are not necessarily related to image-level labels. However, for localization purposes, \u201cbackground pixels\u201d are related to \u201cbackground images\u201d. We remind\nthat: 1) background pixels are those associated with the first pixel-level label and 2) background images are those associated with no image-level label ( \u2211N n=1 \u03b4I,n = 0). To improve explainability, we propose that background images only contain background pixels.\nTo ensure this property, a generalization of the occlusion method (Zeiler and Fergus, 2014) is proposed. The original occlusion method generates multiple occluded versions of the input image I by zeroing all pixel intensities inside a sliding square window. These occluded versions are then run through a previously trained CNN in order to detect background (or conversely foreground) pixels. Instead, we propose to generate a single occluded version I\u0302 of the input image. Let P1 = { P1,x,y,\u2200(x, y) } denote the background probability map.\nThe occluded image I\u0302 is defined as follows:\nI\u0302 = I \u00d7 P1 = { Ix,yP1,x,y,\u2200(x, y) } , (5)\nwhere \u00d7 is the element-wise product. If I is a color image, each color component of pixel Ix,y is multiplied by the same value P1,x,y. Unlike the original occlusion method, this generalized occlusion method is performed during training, in order to optimize the background probability map P1. The following two properties are optimized jointly:\nOcclusion sensitivity: the occluded image I\u0302 = I \u00d7 P1 should always be perceived as a background image, regardless of the ground-truth image-level labels. This indicates that all relevant pixels have been successfully occluded: occlusion is sensitive.\nOcclusion specificity: the background P1 should be as extended as possible or, conversely, the foreground image 1 \u2212 P1 should be as sparse as possible. This indicates that occlusion is specific to relevant pixels."}, {"heading": "3.5. Auxiliary Classification Branch", "text": "In order to optimize the first property above, namely occlusion sensitivity, the occluded image I\u0302 = I \u00d7 P1 should run through a classifier, and the background image P1 should be optimized in such a way that I\u0302 is predicted as a background image. The c\u25e6 s classifier may be used for that purpose. However, optimizing c \u25e6 s(I\u0302) would not only alter background pixel detection, it will potentially alter the entire classifier. Therefore, an independent classification branch should be used instead of c \u25e6 s.\nFor the related problem of CAM computation in WSSS, Zhang et al. (2020) used a completely independent classifier. However, we assume that foreground/background separation is mainly performed by the decoder part d of the EDN s = d \u25e6 e (see section 3.2). Therefore, we propose to reuse the encoder part e of s for this auxiliary classification branch. It has the advantage of significantly reducing training complexity, while alllowing a more generic feature extraction. In that purpose, an auxiliary classification head c\u2032 is connected to the L-channel tensor T = e(I), i.e. to the \u201ctop activation\u201d layer of s. Following common practice, this classification head consists of a global average pooling layer, followed by a regular dense layer. Like c, this classification head has N non-mutually exclusive outputs:\nc\u2032(T ) = \u03c3  L\u2211\nl=1\n\u2211 x,y Tl,x,y\u2211\nx,y 1 w\u2032l,n + b \u2032 n  ,\u2200n \u2208 {1, ...,N}  , (6)\nwhere \u03c3 is the sigmoid function of Eq. (4), and where w\u2032l,n and b\u2032n are neural weights and biases, respectively.\nTo summarize, c\u2032 \u25e6 e is the classification branch used to classify occluded images, for the purpose of optimizing background images."}, {"heading": "3.6. Loss Functions", "text": "The entire neural architecture has been described. This section enumerates the loss functions to optimize both pixel-level and image-level classification performance."}, {"heading": "3.6.1. Cross-Entropy Losses", "text": "The main goal of the proposed framework is to correctly classify image-level labels. Given network predictions p = c \u25e6 s(I) and ground truth labels \u03b4I = { \u03b4I,n,\u2200n \u2208 {1, ...,N} } , the primary loss function Lprimary is thus defined as a cross-entropy loss function:\nLprimary = \u2212 1 N \u2211\nI\n1 \u2211 I N\u2211 n=1 [ \u03b4I,n log(c \u25e6 s(I)n)+\n(1 \u2212 \u03b4I,n) log(1 \u2212 c \u25e6 s(I)n) ] , (7)\nwhere c \u25e6 s(I)n = pn, n \u2208 {1, ...,N}. Because an auxiliary classification branch c\u2032 is defined in section 3.5, N auxiliary image-level predictions p\u2032 = c\u2032 \u25e6 e(I) should also be optimized. An auxiliary loss function Lauxiliary is thus defined similarly toLprimary, by replacing c\u25e6 s with c\u2032 \u25e6e in equation 7."}, {"heading": "3.6.2. Occlusion Loss", "text": "An additional loss function Locclusion is defined to optimize the first property of the background probability map P1, namely that the occluded image I\u0302 = I \u00d7 P1 should always be perceived as a background image (see section 3.4). Given the definition of a background image, namely \u2211N n=1 \u03b4I,n = 0, Locclusion is defined\nas the squared L2-norm of c\u2032 \u25e6 e ( I\u0302 ) predictions:\nLocclusion =\n\u2211 I N\u2211 n=1 [ c\u2032 \u25e6 e ( I\u0302 ) n ]2 N\n\u2211 I 1 . (8)"}, {"heading": "3.6.3. Sparsity Loss", "text": "A final loss function Lsparsity is defined to optimize the second property of the background probability map P1, namely that the foreground map 1 \u2212 P1 should be sparse (see section 3.4). Lsparsity is defined as the L1-norm of 1\u2212P1 maps. Because a softmax activation function is used at the end of s,Lsparsity can be expressed as follows (see Eq. (1)):\nLsparsity =\n\u2211 I \u2211 x \u2211 y \u2223\u2223\u2223\u2223\u2223\u2223\u2223 M\u2211 m=2 s(I)m,x,y \u2223\u2223\u2223\u2223\u2223\u2223\u2223\u2211 I \u2211 x \u2211 y 1 . (9)\nNote that the modulus operator can be dropped since 0 \u2264\u2211M m=2 s(I)m,x,y \u2264 1,\u2200(x, y), due to the use of a softmax activation function in section 3.2."}, {"heading": "3.6.4. Total Loss", "text": "Those four loss functions are combined linearly to obtain the total loss function L that should be minimized during training:\nL = Lprimary + \u03b1Lauxiliary + \u03b2Locclusion + \u03b3Lsparsity , (10)\nwhere \u03b1 \u2265 0, \u03b2 \u2265 0 and \u03b3 \u2265 0."}, {"heading": "3.6.5. Loss Function Competition", "text": "Ideally, convergence to a suitable classifier should not depend critically on the choice of \u03b1, \u03b2 and \u03b3 weights. In other words, the four basic loss functions should not compete with one other.\nFirst, Lauxiliary does not compete with Lprimary: whether we want to segment and classify pathological signs (through Lprimary) or directly classify images without segmentation (through Lauxiliary), the objective of the shared encoder e is to encode the presence of pathological signs in images. Locclusion and Lsparsity, on the other hand, are competing. By design, these loss functions optimize occlusion sensitivity and\nspecificity (see section 3.4), two metrics that always need to be traded off. Lsparsity and Lprimary are also competing: if all pixels are assigned to the background (high foreground sparsity), then no pathological sign can be detected and the classifier will always predict background images, leading to poor classification performance. Fortunately, assuming that decoder d ensures foreground/background separation, Lauxiliary and Lsparsity are independent. Therefore, encoder e, the largest part of the network, can always be trained through Lauxiliary, even if the foreground is temporarily too sparse. Besides training complexity, this property also motivates the use of a partly independent classification branch for occlusion sensitivity maximization, over a completely independent classification branch (see section 3.5).\nFinally, Locclusion has no reason to compete with Lprimary and Lauxiliary. In conclusion, \u03b3, the weight controlling Lsparsity, is the most critical weight. It can be used to trade-off image-level classification performance and pixel-level classification quality (i.e. explainability)."}, {"heading": "3.7. Explanation Generation", "text": "Once training has converged, the proposed system can be used to infer automatic diagnoses for an unseen image I. In addition to N image-level predictions, M \u2212 1 pixel-level probability maps are obtained (see Fig. 1). The following procedure is proposed to explain automatic diagnoses for I:\n1. The M\u22121 pixels maximizing pixel-level prediction in each of the M \u2212 1 foreground maps can be highlighted in I (see Eq. (2)).\n2. Let i2, i3, ..., iM denote the (positive) intensity of those pixels. To explain the n-th image-level decision, those intensities should be multiplied, respectively, by the (positive) weights w22,n, w 2 3,n, ..., w 2 M,n involved in c (see Eq. (3)).\nProduct imw2m,n indicates the strength of clue number m in the n-th decision.\n3. Additionally, if experts can associate a keyword (typically lesion names) with each pixel-level label, then the previous step can be converted into a set of sentences."}, {"heading": "4. Application to Diabetic Retinopathy Diagnosis", "text": "The proposed framework has been applied to the diagnosis of DR through the analysis of CFP images. The goal was to grade DR severity in one eye according to the ETDRS scale (Wilkinson et al., 2003): \u2018absence of DR\u2019, \u2018mild non-proliferative DR (NPDR)\u2019, \u2018moderate NPDR\u2019, \u2018severe NPDR\u2019 and \u2018proliferative DR (PDR)\u2019. This multiclass classification problem can be translated into a four-label multilabel classification problem (N = 4): \u2018at least mild NPDR\u2019, \u2018at least moderate NPDR\u2019, \u2018at least severe NPDR\u2019 and \u2018PDR\u2019. Each of these labels triggers a specific course of action. Note that 1) background images correspond to \u2018absence of DR\u2019 and that 2) detecting the \u2018absence of DR\u2019 is the exact opposite of detecting \u2018at least mild NPDR\u2019."}, {"heading": "4.1. Image Datasets", "text": "A model was developed using training and validation datasets with image-level labels. Next, this model was evaluated on two datasets: one with image-level labels (Ei) and one with pixel-level labels (Ep). In practice, image-level labels are not assigned to single images but, more generally, to small sets of images (CFPs) from the same eye. These datasets have various origins: USA, France and India."}, {"heading": "4.1.1. OPHDIAT Image-Level Evaluation Dataset (Ei)", "text": "The image-level evaluation dataset originates from the OPHDIAT DR screening network, which consists of 40 screening centers located in 22 diabetic wards of hospitals, 15 primary health-care centers and 3 prisons in the Ile-de-France area (Massin et al., 2008). This dataset consists of 21,576 CFPs from 9,734 eyes of 4,996 patients (\u223c 2 CFPs per eye). As part of the quality-assurance program of OPHDIAT, DR severity in these randomly-selected patients was graded by two ophthalmologists. In case of disagreement, images were read a third time by a senior ophthalmologist."}, {"heading": "4.1.2. IDRiD Pixel-Level Evaluation Dataset (Ep)", "text": "The pixel-level evaluation dataset is part of the Indian Diabetic Retinopathy Image Dataset (IDRiD)1, which originates from an eye clinic located in Nanded, India. A total of 143 CFPs with publicly-available manual annotations was used in this study (corresponding to the official IDRiD training set). Experts manually segmented four types of abnormalities related to DR in those images: microaneurysms, hemorrhages, soft exudates and hard exudates (Porwal et al., 2019). For each CFP, one binary segmentation map is thus available per lesion type."}, {"heading": "4.1.3. Training and Validation Datasets", "text": "The training and validation datasets originate from two DR screening programs: EyePACS (Cuadros and Bresnick, 2009) and OPHDIAT.\nImages from EyePACS were acquired in multiple primary care sites throughout California and elsewhere. A total of 88,702 CFPs from 88,702 eyes of 44,351 patients (one CFP per eye), released for Kaggle\u2019s Diabetic Retinopathy Detection challenge2, was used. Training and validation images from OPHDIAT consist of all images from patients that were not included in the image-level evaluation dataset: a total of 610,748 CFPs from 275,236 eyes of 142,145 patients (\u223c 2 CFPs per eye) were included. In both subsets, DR severity in each eye was graded by a single human reader.\nImages were distributed as follows: 90% were used for training and 10% were used for validation. The same proportion of eyes from EyePACS and OPHDIAT (24.4% / 75.6%) was used in the training and validation datasets.\n1https://idrid.grand-challenge.org/Segmentation/ 2https://www.kaggle.com/c/diabetic-retinopathy-detection/\ndata"}, {"heading": "4.2. Image Pre-processing", "text": "The image pre-processing procedure proposed by B. Graham for the \u201cmin-pooling\u201d solution, which ranked first in the Kaggle Diabetic Retinopathy competition, was followed.3 First, to focus the analysis on the camera\u2019s field of view, images were adaptively cropped and resized to 448\u00d7448 pixels. Second, to attenuate intensity variations throughout and across images, image intensity was normalized. Normalization was performed in each color channel independently: the background was estimated using a large Gaussian kernel and subtracted from the image (Quellec et al., 2017).\nData augmentation was performed during training. Before feeding a pre-processed image to a CNN, the image was randomly rotated (range: [0\u25e6, 360\u25e6]), translated (range: [-15%, 15%]), scaled (range: [80%, 120%]) and horizontally flipped. Additionally, Gaussian noise (standard deviation: 5% of the intensity range) was added and the contrast was randomly scaled (range: [75%, 125%])."}, {"heading": "4.3. Managing Eye-Level Labels", "text": "In OPHDIAT, which is used for training, validation and image-level evaluation, labels (DR severity) are assigned to eyes. To facilitate training, a single image was selected per eye at the beginning of each epoch: the one maximizing the sum of image-level predictions ( \u2211N n=1 pn), i.e. the most pathological image of each eye. For image-level evaluation, the maximal prediction among images of the same eye was considered for each label n \u2208 {1, ...,N}."}, {"heading": "4.4. Baseline Methods", "text": ""}, {"heading": "4.4.1. Reliable Region Mining (RRM)", "text": "The self-supervised WSSS solution by (Zhang et al., 2020), named Reliable Region Mining (RRM), was used as baseline. Like the proposed solution, RRM jointly trains a classification branch and a segmentation branch. The purpose of the classification branch in RRM is to compute the CAMs, which are used to supervise the segmentation branch, after post-processing by a dense Conditional Random Field (CRF). However, unlike the proposed solution, segmentations are not used for classification, so only segmentation results can be compared. RRM also use FPN as EDN architecture. For a fair comparison, the same family of classification CNNs (EfficientNet) was used as backbones."}, {"heading": "4.4.2. ExplAIn-CAM", "text": "In order to evaluate the proposed occlusion strategy (see section 3.4), a variation on ExplAIn and RRM, called ExplAInCAM, was also evaluated. In ExplAIn-CAM, the background probability map P1 is not trained to maximize occlusion sensitivity and specificity. Instead, what is maximized is the Dice similarity coefficient between the foreground probability map (1 \u2212 P1) and the element-wise maximum of class-specific CAMs. The CAMs are computed as in RRM, using the classification branch.\n3https://www.kaggle.com/c/diabetic-retinopathydetection/discussion/15801"}, {"heading": "4.4.3. \u201cBlack-Box\u201d AI", "text": "Finally, in order to evaluate the impact of explainability on image-level performance, ExplAIn was compared to a previous \u201cblack-box\u201d AI solution from our group, relying on an ensemble of multiple CNNs (Quellec et al., 2019)."}, {"heading": "4.5. Image-Level Performance", "text": ""}, {"heading": "4.5.1. Performance Metrics", "text": "Classification performance at the image level was evaluated using Receiver-Operating Characteristics (ROC) curves in the evaluation dataset Ei: one ROC curve was obtained per imagelevel label n \u2208 {1, ...,N}. Each curve was obtained by varying a cutoff on the output probabilities pn and by measuring the classification sensitivity and specificity for each cutoff; ROC curves were obtained by connecting (1-specificity, sensitivity) pairs. Classification performance for one label can be summarized by the Area Under the ROC Curve (AUC)."}, {"heading": "4.5.2. Hyperparameter Optimization", "text": "Hyperparameters of each algorithm were optimized at the image level using the validation dataset. A ROC analysis was performed: hyperparameters maximizing the average per-label AUC on the validation subset were selected. For ExplAIn, the following hyperparameters had to be set:\n\u2022 the number M of pixel-level labels (see section 3.1),\n\u2022 the CNN backbone (see section 3.2),\n\u2022 training weights \u03b1, \u03b2 and \u03b3 (see section 3.6).\nTraining weight \u03b3, which trades off image-level and pixel-level classification quality (see section 3.6.5), was empirically set to 0.1. Then, M, \u03b1 and \u03b2 were chosen to maximize the average per-backbone and per-label AUC. Finally, the best CNN backbone, given the optimal M, \u03b1 and \u03b2, was selected. A similar procedure was followed for each algorithm (\u03b3 or equivalent set to 0.1, CNN backbone chosen last)."}, {"heading": "4.5.3. Results", "text": "The hyperparameters of ExplAIn maximizing image-level classification performance on the validation dataset are as follows:\n\u2022 M = 6 pixel-level labels (see section 3.1),\n\u2022 EfficientNet-B5 backbone (see section 3.2),\n\u2022 training weights \u03b1 = \u03b2 = 0.1 (see section 3.6).\nROC curves obtained with these hyperparameters in the OPHDIAT evaluation dataset Ei are reported in Fig. 2. For comparison purposes, image-level performance achieved with different EfficientNet backbones and with the ExplAIn-CAM and \u201cblack-box\u201d AI baselines is summarized in Table 1."}, {"heading": "4.6. Pixel-Level Performance", "text": ""}, {"heading": "4.6.1. Performance Metrics", "text": "Classification performance at the pixel level can also be evaluated by ROC curves, in the evaluation dataset Ep. However, because positive and negative pixels are highly unbalanced, a second evaluation method was used: Precision-Recall (PR)\ncurves. PR curves are built similarly to ROC curves, but (recall, precision) pairs are used in place of (1-specificity, sensitivity) pairs; note that recall and sensitivity are synonyms. A PR curve can be summarized by the mean Average Precision (mAP), defined as the area under the PR curve. Two types of pixel-level evaluations were performed:\nlesion-specific each foreground probability map Pm, m \u2208 {2, ...,M}, is compared to each lesion-specific segmentation map. The label m \u2208 {2, ...,M} maximizing the AUC is retained. We measure how well each lesion type can be detected by a foreground probability map.\ncombined the combined foreground probability map (1 \u2212 P1) is compared to the union of all lesion segmentation maps. Here, we evaluate foreground/background separation."}, {"heading": "4.6.2. Qualitative Pixel-Level Evaluation", "text": "For improved visualization, pixel labeling can be summarized by color codes. In ExplAIn, each pixel Ix,y is labeled by an M-dimensional vector: Pm,x,y, m \u2208 {1, ...,M}: this vector can be summarized by a 3-dimensional color vector in CIE L*a*b* color space. The lightness component L* represents foreground probability 1 \u2212 P1,x,y. Chromaticity components a* and b* represent the normalized (N \u2212 1)-dimensional label vectors P\u2217m,x,y:\nP\u2217m,x,y = {\nPm,x,y 1 \u2212 P1,x,y\n,\u2200m \u2208 {2, ...,M} } . (11)\nThis normalized vector is summarized by a 2-D vector (a*, b*) using isometric mapping (Tenenbaum et al., 2000). Isometric mapping, or Isomap, is a manifold learning algorithm that seeks a lower-dimensional embedding maintaining geodesic distances between all input vectors."}, {"heading": "4.6.3. Results", "text": "Classification performance at the pixel level in the evaluation dataset Ep (IDRiD) is summarized in Fig. 3 (PR curves) and 4 (ROC curves) for all architectures.\nTypical pixel-level classification maps, obtained with the optimal hyperparameters, are reported in Fig. 5. From a visual inspection, it appears that hemorrhages are targeted by map P3, exudates are targeted by map P4, signs of advanced DR (such as vascular abnormalities or laser scars) are targeted by map P5 and microaneurysms are targeted by map P6. This is confirmed by the lesion-specific quantitative evaluation described in section 4.6.1, the results of which are summarized in Fig. 4 (e). Unfortunately, advanced DR signs are not manually segmented in IDRiD, so quantitative evaluation is impossible in this case. For improved visualization, color-coded pixel-level classification maps are reported in Fig. 6. For comparison purposes, typical pixel-level classification maps obtained with different EfficientNet backbones and with the baseline methods are reported in Fig. 7."}, {"heading": "5. Discussion and Conclusions", "text": "We have presented ExplAIn, a novel eXplanatory Artificial Intelligence (XAI) framework for multilabel image classification. In addition to labeling images, ExplAIn also classifies each pixel within images. Simple rules link pixel-level classification to image-level labeling. Consequently, image-level labeling can be explained simply by pixel-level classification. Unlike image-level labeling, pixel-level classification is selfsupervised; a novel occlusion method is presented to ensure satisfactory foreground/background pixel separation and therefore meaningful explanations. This framework was applied to the diagnosis of Diabetic Retinopathy (DR) using Color Fundus Photography (CFP). Classification performance was evaluated both at the image level and at the pixel level.\nExplAIn models were trained to classify DR at the image level, using data from French and American DR screening programs (OPHDIAT and EyePACS): the goal was to grade DR severity in one eye according to the ETDRS scale. On an Indian dataset (IDRiD), we found that manually-segmented DR lesions (microaneurysms, hemorrhages, soft exudates and hard exudates) could be detected well (see Fig. 4). Overall, these lesions could also be differentiated correctly (see Fig. 5). Using the optimal number of pixel-level labels (M = 6) and the optimal CNN backbone (EfficientNet-B5), unsupervised pixellevel classification can be schematized as follows. By design, label m = 1 was assigned to the background (see section 3.4). One pixel-level label was assigned to microaneurysms (m = 6), another one was assigned to hemorrhages (m = 3), another one to exudates (m = 4) and another one to advanced DR signs, such as vascular abnormalities or laser scars (m = 5). The two types of exudates (hard and soft), however, were not separated\nwell using this optimal CNN backbone. Assigning pixel-level labels to lesion types makes sense, since the ETDRS scale relies primarily on the types of DR lesions present in images (Wilkinson et al., 2003). Regarding the last label, m = 2, it seems to group together false alarms of the occlusion method: pixels irrelevant for DR classification but nevertheless unusual for background pixels. For easier visualization, we propose to analyze all pixel-level labels jointly, using color codes obtained through dimension reduction (see section 4.6.2). One advantage is a more compact representation: one color-coded image is generated per input image, as opposed of M = 6 grayscale images.\nIt appears that changing model hyperparameters does not change image-level classification radically, as illustrated in Fig. 7, where different CNN backbones are used. We note, however, that large CNN backbones (EfficientNet models B4 and higher) lead to more semantically rich pixel-level classification than smaller backbones: color codes are more diverse. Nevertheless, this observation only holds when training has converged: pixel-level classification varies significantly during the course of training. In the first training iterations, classification is essentially binary. The system mostly performs foreground/background separation: a single pixel-level foreground label (m > 1) is used, while others are set to zero everywhere. In the following iterations, large patterns (large lesions or lesion clusters) are progressively stressed. To achieve this goal, each binary object is automatically subdivided into M \u2212 1 regions (e.g. center, north, south, east and west) and each region is associated with one pixel-level foreground label. Large patterns (regardless of the lesion types) receive a large response in each region, thus receiving more importance in the final DR classification decision. In a third stage, pixel-level foreground labels are assigned to lesion types (see Fig. 7) which, for DR classification, is more relevant than the size of patterns. The same evolution was observed for the ExplAIn-CAM baseline, except that the third stage was not fully reached, as illustrated in Fig. 7 (j) and (t).\nAt the pixel level, ExplAIn is clearly superior to ExplAInCAM and RRM (see Fig. 7). In particular, precision is about 10 times higher (see Fig. 3). In ExplAIn, increasing \u03b3, the weight assigned to the Lsparsity loss in Eq. (10), increases pixel-level precision without dramatically impacting image-level classification. On the other hand, enforcing the sparsity of CAM, like in ExplAIn-CAM, largely impacts image-level classification. This is due to the low resolution of CAM: a sparse CAM implies that large regions of images have to be completely ignored. This suggests that CAM is not precise enough for WeaklySupervised Semantic Segmentation (WSSS) of DR-related lesions, even though this is currently the most frequently used technique for WSSS (see section 2).\nAlthough explainability is a desirable feature, it should not come at the expense of decreased classification performance. Fortunately, for the same task on the same evaluation dataset, image-level classification performance of ExplAIn (see Fig. 2) is similar to previously reported results for a \u201cblack-box\u201d AI solution from our group (Quellec et al., 2019). Clearly, this \u201cblack-box\u201d solution was designed without explainability con-\nstraints: in particular, it consisted of an ensemble of multiple CNNs while, for explainability purposes, ExplAIn necessarily consists of a single CNN. In details, two classification criteria have improved in ExplAIn (see Table 1):\n\u2022 \u201cmoderate NPDR or more\u201d, the most important criterion in screening applications: AUC increases from 0.9859 to 0.9939. Besides, for a 100% sensitivity, specificity increases from 12.5% to 88.1%,\n\u2022 \u201csevere NPDR or more\u201d: AUC increases from 0.9969 to\n0.9978. Besides, for a 100% sensitivity, specificity increases from 92.4% to 95.6%.\nFor the remaining two criteria, performance in terms of AUC decreases overall. However, the optimal sensitivity/specificity endpoint is improved or unchanged:\n\u2022 \u201cmild NPDR or more\u201d: sensitivity = 90%, specificity = 93.3% (\u201cblack-box\u201d AI) or 94.1% (ExplAIn),\n\u2022 \u201cPDR\u201d: sensitivity = 100%, specificity = 98.9% (in both\nsolutions).\nWe note that ExplAIn is also more efficient than ExplAIn-CAM for classification at the image level (see Table 1), although the difference is more subtle than at the pixel level.\nThe very good classification performance of ExplAIn at the image level was somehow unexpected. One possible explanation for this success is that the proposed generalized occlusion method acts as a regularization operator. Enforcing foreground sparsity encourages the AI to look for local patterns in images rather than to analyze the image globally. More generally, by solving a multi-task problem (image-level and pixel-level classification), we favor the extraction of more general and relevant features.\nIn this paper, ExplAIn has been applied to a well-known classification problem (DR severity classification). It allowed us to evaluate the relevance of the identified local patterns (namely\nDR lesions). However, ExplAIn would be even more useful for a totally new classification problem (disease progression prediction, diagnosis of a new disease, etc.): it would allow knowledge acquisition. In medicine, for instance, it would help clinicians quickly identify new useful markers in images.\nOne limitation of the proposed solution for foreground/background separation (the generalized occlusion method) is that it is limited to binary or multilabel classification (zero, one or multiple labels per image): it is not directly applicable to multiclass classification (exactly one label per image). This is because it relies on the concept of \u201cbackground image\u201d, which does not generally apply to multiclass classification. Another limitation is that pixel-level evaluation relies on incomplete lesion segmentations. Only four types of lesions were manually-segmented in IDRiD: microaneurysms, hemorrhages, soft exudates and hard exudates (Porwal et al., 2019). This results in pessimistic precisions in Fig. 3.\nIn conclusion, we have presented a novel explanatory AI framework for multilabel image classification. Given an image, this framework 1) localizes and categorizes relevant patterns in the image, 2) classifies the image and 3) explains how exactly each pattern contributes to the classification. If experts can assign keywords to these patterns, then a textual report can also be generated. For the task of DR diagnosis using CFP, the new explainability feature comes without loss of classification performance. Thanks to this new feature, we expect healthcare AI systems to gain the trust of clinicians and patients more easily, which would facilitate their deployment."}], "title": "ExplAIn: Explanatory Artificial Intelligence for Diabetic Retinopathy Diagnosis", "year": 2020}