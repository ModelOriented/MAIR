{"abstractText": "Research at the intersection of machine learning and the social sciences has provided critical new insights into social behavior. At the same time, a variety of critiques have been raised ranging from technical issues with the data used and features constructed, problematic assumptions built into models, their limited interpretability, and their contribution to bias and inequality. We argue such issues arise primarily because of the lack of social theory at various stages of the model building and analysis. In the first half of this paper, we walk through how social theory can be used to answer the basic methodological and interpretive questions that arise at each stage of the machine learning pipeline. In the second half, we show how theory can be used to assess and compare the quality of different social learning models, including interpreting, generalizing, and assessing the fairness of models. We believe this paper can act as a guide for computer and social scientists alike to navigate the substantive questions involved in applying the tools of machine learning to social data.", "authors": [{"affiliations": [], "name": "Jason Radford"}, {"affiliations": [], "name": "Kenneth Joseph"}], "id": "SP:2791512d6c4235f12f1cfb7134949c12cd3a8f48", "references": [{"authors": ["Andrew Abbott"], "title": "Sequence Analysis: New Methods for Old Ideas", "venue": "Annual Review of Sociology,", "year": 1995}, {"authors": ["Blaise Aguera y Arcas", "Margaret Mitchell", "Alexander Todorov"], "title": "Physiognomy\u2019s New Clothes, 2017", "year": 2017}, {"authors": ["David Bamman", "Ted Underwood", "Noah A. Smith"], "title": "A bayesian mixed effects model of literary character", "venue": "In Proceedings of the 52st Annual Meeting of the Association for Computational Linguistics", "year": 2014}, {"authors": ["Solon Barocas", "danah boyd", "Sorelle Friedler", "Hanna Wallach"], "title": "Social and Technical Trade-Offs in Data Science", "venue": "Big Data,", "year": 2017}, {"authors": ["S. Bauer", "A. Noulas", "D.O. Seaghdha", "S. Clark", "C. Mascolo"], "title": "Talking Places: Modelling and Analysing Linguistic Content in Foursquare", "venue": "In Privacy, Security, Risk and Trust (PASSAT),", "year": 2012}, {"authors": ["Paul C Beatty", "Gordon B Willis"], "title": "Research synthesis: The practice of cognitive interviewing", "venue": "Public opinion quarterly,", "year": 2007}, {"authors": ["Nicholas Beauchamp"], "title": "Predicting and Interpolating State-Level Polls Using Twitter Textual Data", "venue": "American Journal of Political Science,", "year": 2017}, {"authors": ["Emily M Bender"], "title": "Linguistic typology in natural language processing", "venue": "Linguistic Typology,", "year": 2016}, {"authors": ["Sebastian Benthall", "Bruce D. Haynes"], "title": "Racial categories in machine learning", "venue": "In Proceedings of the Conference on Fairness, Accountability, and Transparency,", "year": 2019}, {"authors": ["Lin Bian", "Sarah-Jane Leslie", "Andrei Cimpian"], "title": "Gender stereotypes about intellectual ability emerge early and influence children\u2019s interests", "year": 2017}, {"authors": ["David M. Blei", "Andrew Y. Ng", "Michael I. Jordan"], "title": "Latent dirichlet allocation", "venue": "J. Mach. Learn. Res.,", "year": 2003}, {"authors": ["Su Lin Blodgett", "Lisa Green", "Brendan O\u2019Connor"], "title": "Demographic dialectal variation in social media: A case study of African-American English", "year": 2016}, {"authors": ["Tolga Bolukbasi", "Kai-Wei Chang", "James Y. Zou", "Venkatesh Saligrama", "Adam T. Kalai"], "title": "Man is to computer programmer as woman is to homemaker? debiasing word embeddings", "venue": "In Advances in Neural Information Processing Systems,", "year": 2016}, {"authors": ["Adam Bonica"], "title": "Mapping the Ideological Marketplace", "venue": "American Journal of Political Science,", "year": 2014}, {"authors": ["Dallas Card", "Chenhao Tan", "Noah A. Smith"], "title": "A Neural Framework for Generalized Topic Models", "venue": "arXiv preprint arXiv:1705.09296,", "year": 2017}, {"authors": ["Jonathan Chang", "Jordan L. Boyd-Graber", "Sean Gerrish", "Chong Wang", "David M. Blei"], "title": "Reading Tea Leaves: How Humans Interpret Topic Models", "venue": "In NIPS,", "year": 2009}, {"authors": ["Raviv Cohen", "Derek Ruths"], "title": "Classifying political orientation on Twitter: It\u2019s not easy", "venue": "In ICWSM,", "year": 2013}, {"authors": ["Justin Cranshaw", "Raz Schwartz", "Jason I. Hong", "Norman Sadeh"], "title": "The Livehoods Project: Utilizing Social Media to Understand the Dynamics of a City", "venue": "In Proceedings of the Sixth International AAAI Conference on Weblogs and Social Media,", "year": 2012}, {"authors": ["Kate Crawford"], "title": "Can an Algorithm be Agonistic? Ten Scenes from Life in Calculated Publics", "venue": "Science, Technology, & Human Values,", "year": 2016}, {"authors": ["G Roy"], "title": "d\u2019Andrade. The development of cognitive anthropology", "year": 1995}, {"authors": ["Thomas Davidson", "Dana Warmsley", "Michael Macy", "Ingmar Weber"], "title": "Automated hate speech detection and the problem of offensive language", "venue": "In Eleventh international aaai conference on web and social media,", "year": 2017}, {"authors": ["Daniel DellaPosta", "Yongren Shi", "Michael Macy"], "title": "Why Do Liberals Drink Lattes", "venue": "American Journal of Sociology,", "year": 2015}, {"authors": ["Jacob Devlin", "Ming-Wei Chang", "Kenton Lee", "Kristina Toutanova. Bert"], "title": "Pre-training of deep bidirectional transformers for language understanding", "venue": "arXiv preprint arXiv:1810.04805,", "year": 2018}, {"authors": ["Carroll Doherty"], "title": "Key takeaways on americans\u2019 growing partisan divide over political values", "venue": "Pew Research Center,", "year": 2017}, {"authors": ["Jacob Eisenstein", "Amr Ahmed", "Eric P. Xing"], "title": "Sparse additive generative models of text", "venue": "In Proceedings of the 28th International Conference on Machine Learning", "year": 2011}, {"authors": ["Justin Farrell"], "title": "Corporate funding and ideological polarization about climate change", "venue": "Proceedings of the National Academy of Sciences,", "year": 2016}, {"authors": ["Sarah Florini"], "title": "Tweets, tweeps, and signifyin\u2019 communication and cultural performance on \u201cblack twitter", "venue": "Television & New Media,", "year": 2014}, {"authors": ["Michel Foucault"], "title": "The history of sexuality: An introduction", "venue": "Vintage,", "year": 1990}, {"authors": ["James Foulds", "Shimei Pan"], "title": "An Intersectional Definition of Fairness", "venue": "arXiv preprint arXiv:1807.08362,", "year": 2018}, {"authors": ["Jerome Friedman", "Trevor Hastie", "Rob Tibshirani"], "title": "glmnet: Lasso and elastic-net regularized generalized linear models", "venue": "R package version,", "year": 2009}, {"authors": ["Yarin Gal", "Zoubin Ghahramani"], "title": "Dropout as a bayesian approximation: Representing model uncertainty in deep learning", "venue": "In international conference on machine learning,", "year": 2016}, {"authors": ["R Stuart Geiger", "Kevin Yu", "Yanlai Yang", "Mindy Dai", "Jie Qiu", "Rebekah Tang", "Jenny Huang"], "title": "Garbage in, garbage out? do machine learning application papers in social computing report where human-labeled training data comes from", "venue": "arXiv preprint arXiv:1912.08320,", "year": 2019}, {"authors": ["Matthew Gentzkow", "Jesse Shapiro", "Matt Taddy"], "title": "Measuring polarization in high-dimensional data: Method and application to congressional speech", "venue": "Technical report,", "year": 2016}, {"authors": ["Bruce Glymour", "Jonathan Herington"], "title": "Measuring the Biases That Matter: The Ethical and Casual Foundations for Measures of Fairness in Algorithms", "venue": "In Proceedings of the Conference on Fairness, Accountability, and Transparency,", "year": 2019}, {"authors": ["Sharad Goel", "Ashton Anderson", "Jake Hofman", "Duncan J. Watts"], "title": "The Structural Virality of Online Diffusion", "venue": "Management Science,", "year": 2015}, {"authors": ["Yoav Goldberg"], "title": "A primer on neural network models for natural language processing", "venue": "Journal of Artificial Intelligence Research,", "year": 2016}, {"authors": ["Stephen Jay Gould", "Steven James Gold"], "title": "The mismeasure of man", "venue": "WW Norton & Company,", "year": 1996}, {"authors": ["Ben Green"], "title": "Fair\u201d Risk Assessments: A Precarious Approach for Criminal Justice Reform", "venue": "In 5th Workshop on Fairness, Accountability, and Transparency in Machine Learning,", "year": 2018}, {"authors": ["Alex Hanna", "Emily Denton", "Andrew Smart", "Jamila Smith-Loud"], "title": "Towards a Critical Race Methodology in Algorithmic Fairness", "venue": "arXiv preprint arXiv:1912.03593,", "year": 2019}, {"authors": ["Donna Haraway"], "title": "Situated knowledges: The science question in feminism and the privilege of partial perspective", "venue": "Feminist studies,", "year": 1988}, {"authors": ["Sandra G Harding"], "title": "The feminist standpoint theory reader: Intellectual and political controversies", "year": 2004}, {"authors": ["Trevor Hastie", "Robert Tibshirani", "Jerome Friedman"], "title": "The elements of statistical learning: data mining, inference, and prediction", "venue": "Springer Science & Business Media,", "year": 2009}, {"authors": ["John R. Hipp", "Robert W. Faris", "Adam Boessen"], "title": "Measuring \u2018neighborhood\u2019: Constructing network neighborhoods", "venue": "Social Networks,", "year": 2012}, {"authors": ["Anna Lauren Hoffmann"], "title": "Where fairness fails: On data, algorithms, and the limits of antidiscrimination discourse. Information, Communication, and Society, 2019", "year": 2019}, {"authors": ["Jake M. Hofman", "Amit Sharma", "Duncan J. Watts"], "title": "Prediction and explanation", "venue": "in social systems. Science,", "year": 2017}, {"authors": ["Carl I Hovland", "Walter Weiss"], "title": "The influence of source credibility on communication effectiveness", "venue": "Public opinion quarterly,", "year": 1951}, {"authors": ["Dirk Hovy", "Tommaso Fornaciari"], "title": "Increasing In-Class Similarity by Retrofitting Embeddings with Demographic Information", "venue": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,", "year": 2018}, {"authors": ["Ben Hutchinson", "Margaret Mitchell"], "title": "Years of Test (Un)fairness: Lessons for Machine Learning", "venue": "Proceedings of the Conference on Fairness, Accountability, and Transparency - FAT* \u201919,", "year": 2019}, {"authors": ["Sergey Ioffe", "Christian Szegedy"], "title": "Batch normalization: Accelerating deep network training by reducing internal covariate shift", "venue": "arXiv preprint arXiv:1502.03167,", "year": 2015}, {"authors": ["Panagiotis G. Ipeirotis", "Foster Provost", "Victor Sheng", "Jing Wang"], "title": "Repeated Labeling Using Multiple Noisy Labelers", "venue": "SSRN eLibrary,", "year": 2010}, {"authors": ["Kenneth Joseph", "Lisa Friedland", "William Hobbs", "David Lazer", "Oren Tsur"], "title": "ConStance: Modeling Annotation Contexts to Improve Stance Classification", "venue": "In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,", "year": 2017}, {"authors": ["Kenneth Joseph", "Wei Wei", "Kathleen M. Carley"], "title": "Exploring patterns of identity usage in tweets: A new problem, solution and case study", "venue": "In Proceedings of the 25th International Conference on World Wide Web,", "year": 2016}, {"authors": ["Soon-Gyo Jung", "Jisun An", "Haewoon Kwak", "Joni Salminen", "Bernard J Jansen"], "title": "Inferring social media users demographics from profile pictures: A face++ analysis on twitter users", "venue": "In Proceedings of 17th International Conference on Electronic Business,", "year": 2017}, {"authors": ["Toshihiro Kamishima", "Shotaro Akaho", "Jun Sakuma"], "title": "Fairness-aware learning through regularization approach", "venue": "IEEE 11th International Conference on Data Mining Workshops,", "year": 2011}, {"authors": ["Matthew Kay", "Cynthia Matuszek", "Sean A Munson"], "title": "Unequal representation and gender stereotypes in image search results for occupations", "venue": "In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems,", "year": 2015}, {"authors": ["Michael Kearns", "Seth Neel", "Aaron Roth", "Zhiwei Steven Wu"], "title": "Preventing fairness gerrymandering: Auditing and learning for subgroup fairness", "venue": "arXiv preprint arXiv:1711.05144,", "year": 2017}, {"authors": ["Ryan Kennedy", "Stefan Wojcik", "David Lazer"], "title": "Improving election prediction", "venue": "internationally. Science,", "year": 2017}, {"authors": ["Norbert L. Kerr"], "title": "HARKing: Hypothesizing after the results are known", "venue": "Personality and Social Psychology Review,", "year": 1998}, {"authors": ["Jon Kleinberg"], "title": "Inherent trade-offs in algorithmic fairness", "venue": "In ACM SIGMETRICS Performance Evaluation Review,", "year": 2018}, {"authors": ["Klaus Krippendorff"], "title": "Reliability in content analysis", "venue": "Human communication research,", "year": 2004}, {"authors": ["Jeff Larson", "Julia Angwin"], "title": "How We Analyzed the COMPAS Recidivism Algorithm", "venue": "ProPublica,", "year": 2016}, {"authors": ["David Lazer", "Ryan Kennedy", "Gary King", "Alessandro Vespignani"], "title": "The parable of Google flu: Traps in big data analysis", "year": 2014}, {"authors": ["David Lazer", "Alex Sandy Pentland", "Lada Adamic", "Sinan Aral", "Albert Laszlo Barabasi", "Devon Brewer", "Nicholas Christakis", "Noshir Contractor", "James Fowler", "Myron Gutmann"], "title": "Computational social science", "year": 2009}, {"authors": ["David Lazer", "Jason Radford"], "title": "Data ex Machina: Introduction to Big Data", "venue": "Annual Review of Sociology,", "year": 2017}, {"authors": ["Matthew Levendusky"], "title": "The partisan sort: How liberals became Democrats and conservatives became Republicans", "year": 2009}, {"authors": ["Zachary C. Lipton"], "title": "The mythos of model interpretability", "venue": "arXiv preprint arXiv:1606.03490,", "year": 2016}, {"authors": ["Yan Liu", "Alexandru Niculescu-Mizil", "Wojciech Gryc"], "title": "Topic-link LDA: Joint models of topic and author community", "venue": "In Proceedings of the 26th Annual International Conference on Machine Learning,", "year": 2009}, {"authors": ["Christopher Lucas", "Richard A. Nielsen", "Margaret E. Roberts", "Brandon M. Stewart", "Alex Storer", "Dustin Tingley"], "title": "Computer-Assisted Text Analysis for Comparative Politics", "venue": "Political Analysis,", "year": 2015}, {"authors": ["Marco Lui", "Timothy Baldwin"], "title": "langid. py: An off-the-shelf language identification tool", "venue": "In Proceedings of the ACL 2012 system demonstrations,", "year": 2012}, {"authors": ["Ian Lundberg", "Arvind Narayanan", "Karen Levy", "Matthew J Salganik"], "title": "Privacy, ethics, and data access: A case study of the fragile families challenge", "venue": "Socius,", "year": 2019}, {"authors": ["Peter V Marsden", "Noah E Friedkin"], "title": "Network studies of social influence", "venue": "Sociological Methods & Research,", "year": 1993}, {"authors": ["Emily Martin"], "title": "The Egg and the Sperm: How Science Has Constructed a Romance Based on Stereotypical Male-Female Roles", "venue": "Signs: Journal of Women in Culture and Society,", "year": 1991}, {"authors": ["Lilliana Mason"], "title": "I Disrespectfully Agree\u201d: The Differential Effects of Partisan Sorting on Social and Issue Polarization", "venue": "American Journal of Political Science,", "year": 2015}, {"authors": ["M. McPherson", "Lynn Smith-Lovin", "J. Cook"], "title": "Birds of a Feather: Homophily in Social Networks", "venue": "Annual Review of Sociology,", "year": 2001}, {"authors": ["Shira Mitchell", "Eric Potash", "Solon Barocas"], "title": "Prediction-Based Decisions and Fairness: A Catalogue of Choices, Assumptions, and Definitions", "venue": "[stat],", "year": 2018}, {"authors": ["Fred Morstatter", "Huan Liu"], "title": "In search of coherence and consensus: Measuring the interpretability of statistical topics", "venue": "The Journal of Machine Learning Research,", "year": 2017}, {"authors": ["S Mukherjee"], "title": "Joint Author Sentiment Topic Model", "venue": "In SDM,", "year": 2014}, {"authors": ["Laura K. Nelson"], "title": "Computational grounded theory: A methodological framework", "venue": "Sociological Methods & Research,", "year": 2017}, {"authors": ["B. O\u2019Connor", "D. Bamman", "N.A. Smith"], "title": "Computational Text Analysis for Social Science: Model Assumptions and Complexity", "venue": "public health,", "year": 2011}, {"authors": ["Alexandra Olteanu", "Carlos Castillo", "Fernando Diaz", "Emre Kiciman"], "title": "Social Data: Biases, Methodological Pitfalls, and Ethical Boundaries", "venue": "SSRN Scholarly Paper ID 2886526,", "year": 2016}, {"authors": ["Michael Omi", "Howard Winant"], "title": "Racial formation in the United States", "year": 2014}, {"authors": ["Rebecca J. Passonneau", "Bob Carpenter"], "title": "The benefits of a model of annotation", "venue": "Transactions of the Association for Computational Linguistics,", "year": 2014}, {"authors": ["Judea Pearl"], "title": "The seven tools of causal inference, with reflections on machine learning", "venue": "Communications of the ACM,", "year": 2019}, {"authors": ["Vikas C. Raykar", "Shipeng Yu", "Linda H. Zhao", "Gerardo Hermosillo Valadez", "Charles Florin", "Luca Bogoni", "Linda Moy"], "title": "Learning from crowds", "venue": "Journal of Machine Learning Research,", "year": 2010}, {"authors": ["Marco Tulio Ribeiro", "Sameer Singh", "Carlos Guestrin"], "title": "Why Should I Trust You?\u201d: Explaining the Predictions of Any Classifier", "year": 2016}, {"authors": ["John R Rickford", "William Labov"], "title": "African American vernacular English: Features, evolution, educational implications", "year": 1999}, {"authors": ["Margaret E. Roberts", "Brandon M. Stewart", "Dustin Tingley", "Edoardo M. Airoldi"], "title": "The structural topic model and applied social science", "venue": "In Advances in Neural Information Processing Systems Workshop on Topic Models: Computation,", "year": 2013}, {"authors": ["Margaret E. Roberts", "Brandon M. Stewart", "Dustin Tingley", "Christopher Lucas", "Jetson Leder-Luis", "Shana Kushner Gadarian", "Bethany Albertson", "David G. Rand"], "title": "Structural topic models for openended survey responses", "venue": "American Journal of Political Science,", "year": 2014}, {"authors": ["Julia M. Rohrer"], "title": "Thinking Clearly About Correlations and Causation: Graphical Causal Models for Observational Data", "venue": "Advances in Methods and Practices in Psychological Science,", "year": 2018}, {"authors": ["Michal Rosen-Zvi", "Thomas Griffiths", "Mark Steyvers", "Padhraic Smyth"], "title": "The author-topic model for authors and documents", "venue": "In Proceedings of the 20th conference on Uncertainty in artificial intelligence,", "year": 2004}, {"authors": ["Matthew J Salganik", "Ian Lundberg", "Alexander T Kindel", "Sara McLanahan"], "title": "Introduction to the special collection on the fragile families challenge", "year": 2019}, {"authors": ["H. Andrew Schwartz", "Johannes C. Eichstaedt", "Margaret L. Kern", "Lukasz Dziurzynski", "Stephanie M. Ramones", "Megha Agrawal", "Achal Shah", "Michal Kosinski", "David Stillwell", "Martin E.P. Seligman", "Lyle H. Ungar"], "title": "Personality, Gender, and Age in the Language of Social Media: The Open-Vocabulary Approach", "venue": "PLoS ONE,", "year": 2013}, {"authors": ["Andrew D. Selbst", "Danah Boyd", "Sorelle Friedler", "Suresh Venkatasubramanian", "Janet Vertesi"], "title": "Fairness and Abstraction in Sociotechnical Systems", "venue": "SSRN Scholarly Paper ID 3265913,", "year": 2018}, {"authors": ["Mario Luis Small"], "title": "Someone to Talk To", "year": 2017}, {"authors": ["Lynn Smith-Lovin"], "title": "The Strength of Weak Identities: Social Structural Sources of Self, Situation and Emotional Experience", "venue": "Social Psychology Quarterly,", "year": 2007}, {"authors": ["Rion Snow", "Brendan O\u2019Connor", "Daniel Jurafsky", "Andrew Y. Ng"], "title": "Cheap and fast\u2014but is it good?: Evaluating non-expert annotations for natural language tasks", "venue": "In Proceedings of the Conference on Empirical Methods in Natural Language Processing,", "year": 2008}, {"authors": ["Christian Szegedy", "Sergey Ioffe", "Vincent Vanhoucke", "Alexander A Alemi"], "title": "Inception-v4, inceptionresnet and the impact of residual connections on learning", "venue": "In Thirty-First AAAI Conference on Artificial Intelligence,", "year": 2017}, {"authors": ["Iddo Tavory", "Stefan Timmermans"], "title": "Abductive analysis: Theorizing qualitative research", "year": 2014}, {"authors": ["Alexander Todorov", "Chris P Said", "Andrew D Engell", "Nikolaas N Oosterhof"], "title": "Understanding evaluation of faces on social dimensions", "venue": "Trends in cognitive sciences,", "year": 2008}, {"authors": ["Toole Jameson L", "Lin Yu-Ru", "Muehlegger Erich", "Shoag Daniel", "Gonz\u00e1lez Marta C", "Lazer David"], "title": "Tracking employment shocks using mobile phone data", "venue": "Journal of The Royal Society Interface,", "year": 2015}, {"authors": ["Zeynep Tufekci"], "title": "Big Questions for Social Media Big Data: Representativeness, Validity and Other Methodological Pitfalls", "venue": "Proceedings of the 8th International AAAI Conference on Weblogs and Social Media.,", "year": 2014}, {"authors": ["Zeynep Tufekci"], "title": "Big Questions for Social Media Big Data: Representativeness, Validity and Other Methodological Pitfalls", "venue": "Proceedings of the 8th International AAAI Conference on Weblogs and Social Media.,", "year": 2014}, {"authors": ["Jay J. Van Bavel", "Andrea Pereira"], "title": "The Partisan Brain: An Identity-Based Model of Political Belief", "venue": "Trends in Cognitive Sciences,", "year": 2018}, {"authors": ["Ashish Vaswani", "Noam Shazeer", "Niki Parmar", "Jakob Uszkoreit", "Llion Jones", "Aidan N Gomez", "Lukasz Kaiser", "Illia Polosukhin"], "title": "Attention is all you need", "venue": "In Advances in neural information processing systems,", "year": 2017}, {"authors": ["Eric Wallace", "Shi Feng", "Nikhil Kandpal", "Matt Gardner", "Sameer Singh"], "title": "Universal adversarial triggers for nlp", "venue": "arXiv preprint arXiv:1908.07125,", "year": 2019}, {"authors": ["Hanna Wallach"], "title": "Computational social science 6= computer science + social data", "venue": "Communications of the ACM,", "year": 2018}, {"authors": ["Wei Wang", "David Rothschild", "Sharad Goel", "Andrew Gelman"], "title": "Forecasting elections with nonrepresentative polls", "venue": "International Journal of Forecasting,", "year": 2015}, {"authors": ["Yilun Wang", "Michal Kosinski"], "title": "Deep neural networks are more accurate than humans at detecting sexual orientation from facial images", "venue": "Journal of personality and social psychology,", "year": 2018}, {"authors": ["Zeerak Waseem"], "title": "Are You a Racist or Am I Seeing Things? Annotator", "venue": "Influence on Hate Speech Detection on Twitter. NLP+ CSS 2016,", "year": 2016}, {"authors": ["Duncan J. Watts"], "title": "Everything Is Obvious:* Once You Know the Answer", "venue": "Crown Business,", "year": 2011}, {"authors": ["Xiaolin Wu", "Xi Zhang"], "title": "Automated inference on criminality using face images", "venue": "arXiv preprint arXiv:1611.04135,", "year": 2016}, {"authors": ["Xiaohui Yan", "Jiafeng Guo", "Yanyan Lan", "Xueqi Cheng"], "title": "A biterm topic model for short texts", "venue": "In Proceedings of the 22nd international conference on World Wide Web,", "year": 2013}, {"authors": ["Sergey Zagoruyko", "Nikos Komodakis"], "title": "Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer", "venue": "arXiv preprint arXiv:1612.03928,", "year": 2016}, {"authors": ["Tukufu Zuberi", "Eduardo Bonilla-Silva"], "title": "White logic, white methods: Racism and methodology", "venue": "Rowman & Littlefield Publishers,", "year": 2008}], "sections": [{"heading": "1 Introduction", "text": "Machine learning is increasingly being applied to vast quantities of social data generated from and about people [69]. Much of this work has been fruitful. For example, research using machine learning approaches on large social datasets has allowed us to provide accurate forecasts of state-level polls in U.S. elections [8], study character development in novels [4], and to better understand the structure and demographics of city neighborhoods [19, 48]. The increasing application of machine learning to social data has thus seen important success stories advancing our understanding of the social world.\nAt the same time, many social scientists have noted fundamental problems with a range of research that uses machine learning on social data [70, 21]. Machine learning on social data often does not account for myriad biases that arise during the analysis pipeline that can undercut the validity of study claims [87]. Attempts to identify criminality [120] and sexuality [117] from people\u2019s faces and predicting recidivism using criminal justice records [67] have led to critiques that current attempts to apply machine learning to social data represent a new form of physiognomy [3]. Physiognomy was the attempt to explain human behavior through body types and was characterized by poor theory and sloppy measurement [40]. It ultimately served to merely re-enforce the racial, gender, and class privileges of scientists and other elites. Today it is considered pseudoscience.\nAcknowledging these misappropriations of machine learning on social data, researchers have largely sought out technical solutions, both old and new. For example, in response to claims that algorithms embedded in policy decisions often provide unfair advantages and disadvantages across social groups, scholars in the Fairness, Accountability and Transparency (FAT*) community have proposed new algorithms to make decisions more fair. Similarly, researchers in natural language processing have proposed several new methods to \u201cde-bias\u201d word embeddings\u2019 representation of gender, race, and other social identities and statuses. [14].\nThe primary contribution of this paper is to argue and show that at each step of the machine learning pipeline, problems arise which cannot be solved using a technical solution alone, but with the use of social\n\u2217Both authors contributed equally to this manuscript\nar X\niv :2\n00 1.\n03 20\n3v 3\n[ cs\n.C Y\n] 1\n5 Ja\nn 20\ntheory. Social theory is the set of scientifically-defined constructs like race, gender, social class, inequality, family, and institution and their empirically-identified causes and consequences. Using social theory in machine learning means engaging these constructs as they are defined and described scientifically and accounting for the established mechanisms and patterns of behavior engendered by these constructs. For example, Omi and Winant\u2019s [88] racial formation theory argues that race is a social identity that is constantly being constructed by political, economic, and social forces. What makes someone \u201cBlack\u201d or \u201cWhite\u201d in the United States and the opportunities and inequities associated with this distinction have changed dramatically throughout history and continues to change today. While there are other scientific definitions of race and active debates about its causes and consequences, engaging with them at each stage in the pipeline allows us to answer critical questions about the models we should use, features we should select, and generalizations we can make.\nIn this paper, we explain how social theory helps us solve problems arising at every step in the machine learning for social data pipeline. This is outlined in Figure 1.\nThe paper is structured into two broad section. In the Theory In section, we discuss how social theory can help us model social data by informing how we identify important problems and determine the right class of models. In the Theory Out section, we talk about different desiderata that we have for the models and results we produce, like generalizability, and discuss how social theory can help to improve these outputs of our work.\nBoth the Theory In and Theory Out sections have multiple subsections. Although each subsection addresses a unique step in the pipeline, solutions problems identified in one step often enable solutions to problems in others. For example, a model that is parsimonious is often more interpretable. A lack of a solution to a problem in one step can also prohibit a solution to issues that might arise downstream. At the highest level, a lack of social theory going into the model is likely to stymy drawing theory out. These overlaps are a strength, rather than a weakness, of the structure of this article. Like Olteanu et al. [87], we believe that by emphasizing both the uniqueness and the critical relationships between different pieces of the pipeline, we can understand how failure to address critical problems can propagate from one step to\nthe next."}, {"heading": "2 Related Work", "text": "Social scientists have long established that theory can solve methodological and analytic issues that new techniques cannot. For example, Mario Small has argued that theory alone can address questions of how best to measure what it means for one to have a close social tie [102]. In the present work, we regularly draw on this literature, seeing many parallels between prior methodological innovations like linear models and sequence analysis [2, 1].\nOther scholars working at the intersection of machine learning and social science have also proposed important critiques upon which we draw throughout the paper. These critiques fall into four broad categories. First, Hanna Wallach and her colleagues have argued that many machine learning papers focus too heavily on prediction relative to explanation [115] or measurement [56]. The prioritization of prediction leads to models that perform well for unknown reasons, leading to ad hoc justifications for model decisions and performance. It also leads to models which output parameters that cannot be used to directly measure real-world quantities we are interested in. A lack of focus on measurement leads, as we will also discuss, to a failure to acknowledge the always imperfect association between what we are trying to measure and what we are able to quantify.\nOthers, like Laura Nelson [85], argue that machine learning applied to social data must come hand-inhand with the development of new theory. From their perspective, we do not necessarily know what model will work for what data, nor do we typically have theory to tell us what to expect. Consequently, we need to create new theory as we develop and run methods on our data. This approach helps us to understand why machine learning models might present results that at first seem unintuitive, but that do reflect genuine patterns that should force us to reconsider our understanding of the social world. However, it also requires an a priori understanding of the potential theories that could apply, and seeks to adapt this existing theory, rather than create new theory entirely ex post facto.\nStill others have taken specific studies or sets of studies to task, arguing that they fail to understand the socio-technical context in which their data are produced [110, 69, 87]. For example, Tufecki [110] argues that, despite generic claims above universal social behavior in many papers, research using Twitter data is unlikely to tell us much about social interaction on Facebook because the two have very different rules, data, and norms. Similarly, Olteanu et al. [87] provide a dizzying array of potential pitfalls in analyzing social data, emphasizing the need for time-tested ideas from the social sciences, like matching and analyses of sample bias, to address these issues. These critiques point to the fact that machine learning is often done with one eye closed to the peculiarities of the data.\nFinally, with the advent of algorithms that are making increasingly high-impact decisions, the FATE community1 has arisen to study how these algorithms can serve to reflect social biases embedded in the complex sociotechnical systems within which they are embedded, and how we might be able to address these issues. However, recent critiques of the fairness literature argue that far too much emphasis has been placed on technical \u201csolutions\u201d to unfair and/or \u201cbiased\u201d algorithms, relative to the structural causes and consequences of those algorithms [49, 41, 21]. Such scholarship has argued that social science disciplines need be at the forefront of our understanding of how to address these root causes.\nEach of these critiques\u2014that prediction does not equal understanding, that we must be ready to construct new theory to interpret our results, that myriad biases lurk in the data and methods we use, and that these methods can result in discriminatory outcomes with systemic causes\u2014 is critical in pushing us towards a better, more responsible application of machine learning towards social data. Further, the many works reviewed below that apply machine learning to social data with these issues in mind provide a further glimpse into the potential for this version of the science.\nIn the present work, we seek to unify these critiques, arguing that each of them are levied at different pieces of the same underlying problem\u2014attempts to use technology, or ad hoc, post ex facto reasoning, to address problems only social theory can solve. We argue below that theory alone can lead us to the valid explanatory models sought by Wallach [115], to ensure we draw correct conclusions from initially unintuitive\n1http://fatconference.org\nresults, to help us characterize dangerous assumptions in our data collection processes, and to help us understand and address discriminatory, biased, or unfair model behavior."}, {"heading": "3 Theory In", "text": "In this section, we discuss the pipeline for studies that use machine learning to analyze social data, from problem conception through model selection. At each step, we identify critical issues that arise which can only be addressed with theory.\nThroughout the section, two broad themes arise. First, given the breadth of data available to us, we sometimes act opportunistically; we use what we have to jump quickly on new problems. This push to rapidly solve pressing social problems by using widely available data and methods leads us to, for example, use a dataset to answer a research question that the dataset is ill-suited for. Problems can arise from these decisions that entirely undermine the utility of the research\u2014for example, selecting a bad sample of data can undermine the external validity of a study.\nSecond, we often rely on intuition to make decisions about research design. For example, when constructing annotation tasks, intuition can lead to a overly simplified designs, when many other potential approaches could also be equally, or more, valid [57]. Often, these intuitions are good enough for the task at hand. However, when out intuitions are wrong, the results can be problematic. For example, following misguided intuitions about sexuality and its causes can lead to incorrect claims, made on top of poor research design decisions, about the biological nature of sexuality [117].\nThis combination of opportunism and intuition can be particularly pernicious when combined with a lack of theory. While social scientists also often rely on intuition [106], they can rely on the scaffolding provided by previous theoretical work, guiding them towards a better research design and/or understanding of their data. In Section 3, we discuss how we can use social theory to help us constrain our opportunism and intuitions to existing knowledge of the social world provided to us by social theory. This increases our chance at producing new, lasting science that helps move forward our understanding of society."}, {"heading": "3.1 Problem Selection and Framing", "text": "As researchers, we are constantly asking ourselves, \u201cwhat problem should we be studying?\u201d2 In core machine learning research, problems of interest revolve around technical innovation \u2014 how can we develop new statistical or mathematical theory or approaches to improve the quality of our methods? In the social sciences, we often focus instead on research that can help us gain new insights about the social world.\nUnfortunately, while technical approaches can sometimes help identify oddities in social data worth investigating, there is no technical solution to identifying good social science research questions. These insights require an understanding of what is known already about the social world, and where gaps in this knowledge lie. However, with the onslaught of big data, we all too often optimize for convenience, using the data we have on hand to solve problems just because they seem solvable, and because they seem to have real-world relevance. For example, we use publicly available Twitter data to predict people\u2019s movements within cities [6] or aggregated search data from Google Trends to predict the prevalence of the flu [68].\nThis convenience approach to problem selection and framing leads to two problems. First, it can lead us to formulate and tackle problems that seem important but in reality serve chiefly as an exercise in prediction, providing little new insight into the social world. Second, it can lead us to address problems that our intuitions accurately assume are important, but leave us struggling to frame the reasons why the problem is important. Social theory can help to alleviate these issues.\nFirst, theory tells us which problems are worth solving. For example, election prediction is an essential research tool because it provides a model for understanding political processes [8, 63]. However, theory tells us that because of polarization, gerrymandering, and campaign finance laws, most American elections today are very predictable with only one piece of data \u2013 knowing who the incumbent is. Theory also tells us, however, that in nominally competitive races, polling provides the next best predictor, because politics is driven by opinion. However, polling is expensive, and s only available for the most high-profile races.\n2Importantly, not all researchers ask this question. Others, for example, may seek to understand the consequences of the practices they are studying. Thank you to stef shuster for pointing this out.\nTheory thus suggests that within the domain of elections, the correct problem to study is modeling opinion in competitive and under-polled elections.\nSecond, theory can help us to motivate and frame problems that seem intuitively important. It may be apparent that predicting the prevalence of the flu can help save lives. However, less obvious is what problem is being solved when predicting, for example, a person\u2019s political affiliation based on their social media behavior (e.g. based on their tweets) [18]. Without theory, this prediction is simply an exercise in engineering. However, recent work on political polarization urges us to study affiliation as a function of partisan identity [71], and shows that such identities are rapidly undermining social and cultural stability in the United States [26]. Social theory therefore explains why predicting political affiliation is important \u2013 in order to study its association with cultural polarization [24].\nThere are many examples where scholars using machine learning on social data have used theory to identify and frame important problems. For example, several scholars have addressed precisely the problem of opinion polling in competitive and under-polled elections using big data [8, 116]. And Cranshaw et al. [19] take an intuitively interesting task\u2014clustering venues on foursquare by the patrons they have in common\u2014 and ground it in theory from urban studies and design to motivate and frame their work as addressing an unsolved problem of how neighborhood boundaries should be defined."}, {"heading": "3.2 Outcome Definition", "text": "Having established a problem of interest, we turn to the task of defining and measuring our outcome. Our outcome measure is ideally meant to be the ground truth for our phenomenon we\u2019re modeling, i.e. an observation of the phenomenon itself. For example, if we are interested in studying partisanship, we can establish ground truth through a variety of means\u2014whether someone votes for a single party [91], who they donate money to [15], or what topics they tweet about [109]. However, this data is often not easily available. The easiest \u201ctechnical solution\u201d to this is simply to use a variable available in our data as ground truth, or, as we discuss in Section 3.5, to construct the variable through a rapid crowdsourced annotation task.\nHowever, no technical tool we are aware of can tell you whether or not, for example, a) voting behavior or b) political sentiment in tweets are valid or better measures of whether one is a liberal or conservative [18]. Answering this requires a theoretical understanding of what we mean by liberal and conservative. In turn, we must approach ground truth as something being theorized by researchers. Consequently, when we define an outcome, we have to do so using these theories to argue that the variable validly captures the construct we seek to measure [43].\nWith respect to liberalism and conservatism, for example, theory shows that people and sociotechnical systems shape the \u201cground truth\u201d. Only recently have liberal and conservative ideologies aligned with the Democratic and Republican parties in the United States \u2013 what is called partisan sorting [80]. For example, [35] show that partisan ideology has only become distinguishable in Congressional floor speeches since 1980. That is, language has only become partisan in the past forty years.\nThese theoretical insights, in turn, helps us create valid outcomes by grounding its construction in a definition of what the outcome should be. Instead of predicting liberalism/conservatism, we likely would prefer to focus on if someone is a democrat or a republican. This is because partisan identity theory [112] claims that party membership is driven by party identification. What makes someone a democrat is not that they support public health care or market regulation but that they identify with democrats. Thus, if we want to infer someone\u2019s political party identification from their tweets, we should look to who\u2019s side they take in a debate, rather than the specific issues they support. In his campaign, Donald Trump famously supported liberal policies like public health care and criticized the war in Iraq. These stances did not make him a moderate conservative. They made him a populist republican as opposed to an establishment republican."}, {"heading": "3.3 Data Selection", "text": "The process of data selection is defined as the identification of one or more datasets that can be used to address the problem under study.Data selection is typically carried out using either precedent or convenience as a heuristic. Where possible, we seek out existing and established datasets that we can simply adopt for our own use. Where existing data do not exist, we often seek data that can be easily and scalably collected.\nThis use of precedence and convenience stems from our interest not only in answering questions about the social world, but in desiring to do so via novel methodologies. For example, when constructing novel solutions to (or models for) existing problems, we tend to reach for established datasets for which prior results exist for comparison. And for novel data collection, our methods often require large datasets, and so convenient collection of this data is almost a prerequisite.\nBut relying on either convenience or precedent can cause issues for social science questions, because all data contain both inclusions and exclusions that manifest in varying forms of bias [87]. By taking shortcuts with data selection, we often choose to ignore or brush over these inclusions and exclusions. For example, Blodgett et al. [13] show that language identification tools shown to perform well on a wide array of text corpora [76] suffer significantly at distinguishing African-American English as English in social media texts. Because scholars have often used this tool to filter out non-English tweets, the result is a set of studies on social media data where the voices of African Americans are diminished relative to white Americans.\nAs Blodgett et al. [13] suggest, socio-linguistic theory could have helped us to anticipate the potential issues in using the convenient language classifier they studied to identify English versus non-English content. Specifically, theoretical models of how dialects form emphasize that variations of written English may not readily align in terms of the primary features used by the language classification model, n-grams of characters [94]. Further, socio-linguistic theory emphasizing the importance of African American English and its distinctions from other English dialects in the presentation of the self online for Americans [29] would have emphasized the need for social media scholars to reconsider the notion that there is a single definition of English that they wish to study.\nMore broadly, then, social theory helps us to understand the implications of how we make our decisions on what data to include or not include in our sample. This is especially critical when we expect others will reuse our data or the models we construct from them. For example, a significant amount of research has used pre-trained word vectors from an unreleased Google news corpus, meaning the biases in the data are both unclear and unknown. On the contrary, Matt Salganik and colleagues [77] use the statistical sampling and survey measurement theories baked into the Fragile Families Wellbeing Study to create the Fragile Families Challenge - a common data set computational social scientists can use to develop models predicting critical social factors like income, health, and housing. The use of theory to identify and explain important inclusion and exclusion variables have allowed research conducted during the challenge to contribute successfully to social scientific knowledge on child development [99]."}, {"heading": "3.4 Feature Engineering", "text": "Feature engineering encompasses the process of converting our raw data to quantities that can be input into a machine learning model. The key question is, of course, how do we know that we have engineered the right features? A technical solution to this question has typically privileged model performance. In supervised learning, we assess our feature engineering strategy by comparing predictive performance metrics (e.g. precision) across various potential feature sets. In unsupervised learning, we assess our strategy both by looking to other kinds of automatically-calculated or annotation-based performance metrics [83].\nThe problem with this approach to feature engineering is that the features we select might boost our performance but may not help us distinguish genuine from spurious signal. Overfitting to noise is one way in which injudicious feature selection can inflate performance. Another is to include features that are spuriously related with our outcome of interest or exclude features that are directly related. Take the case of recidivism prediction as an example. To predict who will return to prison, not only do we need features that signal a person\u2019s propensity to commit a crime, but also features that capture police and judicial processes around who is likely to be arrested and convicted of a crime. Omitting features capturing both crime commission and arrest yields a poorly-specified model that performs well.\nAutomated causal inference at scale is an as-yet unattained holy grail in machine learning. Thus, without theory, we cannot enumerate which features we should include and which we should exclude. Specifying the theoretical model in advance is the only way to enumerate what features we should generate [90, 97]. Building theoretical models allow us to identify which features should be included, and if they are deemed important by a model, what they might mean. More concretely, Wallach [115] argues that we should always be informing our selection of features and understanding of the problem with theory-based causal models in mind. This argument is, of course, at odds with claims of \u201cfeatureless\u201d models, as many claim deep learning models\nto be. For example, where before we may have needed to provide a model for Named Entity Recognition with part-of-speech tags for each input word, modern deep learning architectures do not require this feature engineering step [39]. However, even with such models, we are still making implicit decisions about our features, for example, by deciding whether to use words or characters as input to the model [25]. Further, the causal processes of interest often lay beyond decisions on whether or not to use words or characters. For example, regardless of what deep NLP model we choose to model an individual\u2019s language, word choices are often driven by more difficult-to-capture variables, like age and gender [100]."}, {"heading": "3.5 Annotation", "text": "Oftentimes we cannot identify some critical feature we want to model from our data. For example, Twitter does not provide data on the gender or religious affiliation of their users. In such cases, we often ask humans, be it ourselves or others, to go through the data and identify the feature of interest by hand. This process of annotation has been understood as identifying some ground truth label that can then be used for model building.\nWhen annotating data, a primary goal is to ensure that annotators agree. Due to both noise and intrinsic variation amongst individuals, different people look at the same data and come up with different labels. Our interest, particularly when searching for some objective ground truth, is to ensure that despite these differences, we can identify some annotated value on which most annotators roughly agree. Scholars in the social sciences have long established statistical measures of agreement in annotation [66], which are readily used in the machine learning pipeline. However, machine learning researchers have also sought to increase agreement in various ways [104]. These technical efforts to increase agreement largely rely on either trying to find the best annotators (i.e. those that tend to agree most frequently with others [55]), finding better aggregation schemes [89, 92], or simply by increasing the amount of data labeled [104].\nAt the core of many disagreements between annotators, however, is that the constructs we are seeking to annotate are often difficult to define. For example, Joseph et al. [58] built a classifier to identify social identities in tweets, a concept that is notoriously varied in its meaning in the social sciences. Thus, even experts disagree on exactly what a social identity constitutes. Unsurprisingly, then, Joseph and his colleagues found that non-expert annotators provided unreliable annotations, even after a discussion period. Annotations of hate speech have seen similar struggles, with limited agreement across annotators [23] and with significant differences across annotators with different demographics [118].\nIn such cases where the construct is difficult to define, technical solutions like adding more annotators or performing different aggregation schemes are unlikely to increase agreement. This is because, as with outcome definition,technical solutions cannot address the fundamental issue\u2014defining the construct itself. In other words, technical solutions cannot be used to answer the questions, \u201cwhat is a social identity?\u201d Or, \u201cwhat is hate speech?\u201d Instead, we must rely on theory to provide a definition. For example, Affect Control Theory in sociology focuses not on the general idea of social identity, but rather on \u201ccultural identity labels\u201d, defined as \u201c(1) the role-identities indicating positions in the social structure, (2) the social identities indicating membership in groups, and (3) the category memberships that come from identification with some characteristic, trait, or attribute\u201d ([103], pg. 110). Upon using this definition, and annotations from Affect Control theorists, Joseph et al. [58] noted a significant increase in annotation quality.\nAnnotation, particularly with complex phenomena like identity, hate speech, or fake news [42], therefore requires starting with a theory of the construct we wish to measure and its intersection with the subjective processes of our annotators. One additional tool worth noting for this task that social scientists have developed is cognitive interviewing [7]. Cognitive interviewing involves talking to potential annotators about how they think of the construct, its potential labels, how they would identify those labels, and then having them actually try to apply our task to some test data. While similar to the idea of a pilot annotation task that machine learning researchers are likely familiar with, cognitive interviewing outlines specific ways in which theory can be applied before, during, and after the pilot to help shape the definition of the construct. Finally, although beyond the scope of the present work, it is also critical that annotation follows best methodological practices for structured content analysis in the social sciences [34]."}, {"heading": "3.6 Model Construction", "text": "In building a machine learning model for social data, our goal is to predict, describe, and/or explain some social phenomenon. Our job, then, is to identify the model that best accomplishes this goal, under some definition of best. Our challenge is to determine which of the many modeling approaches (e.g. a deep neural network versus a Random Forest) we can take, and which specific model(s) (e.g. which model architecture with what hyperparameters) within this broad array we will use for analysis.\nIt can be, and often is, overwhelming to select which model to use for a given analysis. Consider, for example, the goal of understanding the topics in a corpora of text. The early topic modeling work of Blei et al. [12], has been cited over 28,000 times. Many of these citations are from extensions of the original model. For example, there are topic models for incorporating author characteristics [98], author characteristics and sentiment [84], author community [74], that deal specifically with short text [121], that incorporate neural embeddings of words [16], and that emphasize sparsity [27]. How do we construct a model that is best, or right, for our analysis?\nBrendan O\u2019Connor, David Bamman, and Noah Smith [86] describe this kind of modeling choice as occurring along two axes - computational complexity and domain assumptions. Computational complexity is used loosely to represent complexity in computational time and \u201chorsepower\u201d. Domain assumptions vary from few assumptions, essentially assuming \u201cthe model will learn everything,\u201d to cases where we explicitly model theory. However, O\u2019Connor et al. leave open the question of where in this space the \u201cright\u201d model for a particular problem is likely to fall, or how to define the right domain assumptions.\nThis is where theory comes in. By defining the goal of the model \u2013 prediction, explanation, description, and so on; and providing clear expectations for what our domain assumptions are, theory helps us navigate the computation/domain space. In the context of topic modeling, the Structural Topic Model (STM) [95, 96] provides a generic framework for defining our domain assumptions based on the factors we expect to be important for shaping the topics that appear in a document. By incorporating covariates into the modeling process that we theorize to be relevant, we can leverage theory both to create a model that \u201cfits the data better,\u201d and get outputs of the model that we can use to directly test extensions to our theory. The right model, then, is defined by theory. For example, Farrell [28] uses theories of polarization through \u201ccontrarian campaigns\u201d that originate in well-funded organizations to determine a particular instantiation of the Structural Topic Model that they use to study how polarization has emerged on the topic of climate change.\nThe STM is therefore useful in that, given an established set of generic modeling assumptions and a defined level of computational complexity, we can use theory to define the specific model we construct. Similar efforts have been made in other areas of text analysis as well. For example, Hovey and Fornaciari [52] use the concept of homophily, that people with similar social statuses use similar language, to retrofit their word embedding model. This theory-driven change allowed the model to leverage new information, yielding a more performant model. As such, the use of theory to guide natural language processing models can serve as a blueprint for the application of theory in other domains of social data."}, {"heading": "4 Theory Out", "text": "Machine learning has traditionally concerned itself with maximizing predictive performance. This means that the first results reported in machine learning papers, those in \u201cTable 1\u201d, are often a report on the model\u2019s predictive performance relative to some baselines. However, scholars are increasingly interested in other aspects of model output, like interpretability and fairness. In applied research, it is important for scholars to demonstrate that their model helps us understand the data and explains why particular predictions are made. These new demands for the output of machine learning models create problems for which technical solutions have been proposed. In this section, we argue that this technical innovation must be supplemented with social theory if we are to truly interpret, explain, and use our models to learn about and improve the social world."}, {"heading": "4.1 Interpretability, Explainability, and Theory-building", "text": "Few criticisms have been leveled against machine learning models more than the charge that they are uninterpretable. While a concrete definition of interpretability has been elusive [73], the general critique has been that machine learning models are often\u201cblack boxes\u201d, performing complex and unobservable procedures that produce outputs we are expected to trust and use. In trying to open the black box and account for our models, three distinct questions are often treated interchangeably:\n\u2022 What did the model learn, and how well did it learn it? Meaning, given a particular input, how does the model translate this to an output and how accurately does this output match what we expect? We refer to this as the question of interpretability.\n\u2022 Why did the model learn this? What is it about the (social) world that led to the model learning these particular relationships between inputs and outputs? We will refer to this as the question of explainability.\n\u2022 What did we learn about the world from this model? What new knowledge about the social world can be gleaned from the results of our model? We refer to this as the question of theory-building.\nInterpretability, explainability, and theory-building get lumped together in the technical solutions that have been developed to open the black box. For example, sparsity-inducing mechanisms like regularization [32] and attention (in neural networks; [113]) increase interpretability by minimizing the number of parameters to inspect. In turn, these technical solutions are used help us explain how the parameters relate to the data generating process (explainability) [122]. Similarly, qualitative analysis of what the model got right and wrong through case studies and error analysis sheds light on both the limits of what the model has learned (interpretability) and why it makes incorrect predictions (explainability). We also use model-based simulations, tweaking inputs to show how they produce different outputs [93] and adversarial examples that fool the model to explore its performance (interpretability) the limits of its understanding about the world (theory-building) [114].\nHowever, while there are many methodological overlaps; interpretation, explanation, and theory-building are distinct research questions that require distinct practices with distinct roles for social theory.\nWhen interpreting models\u2013 that is, when trying to understand what the model learned and how well it learned it\u2014 social theory enables us to go beyond the technical question of how to look at the model to what to look at. In order to choose what model parameters, characteristics, and cases to visualize, we need to have theory-driven expectations about how the model is supposed to work and what it is supposed to do. For example, social theory tells us that racial bias is tied to visual cues beyond just skin color. Bias driven by skin tone, called \u2018colorism,\u2019 is different from bias driven by cultural codes around race. For example, the color of someone\u2019s skin biases people in ways that are different from how they are dressed, their accent, and hair style [107]. Consequently, if we want to interpret the biases of a computer vision model through the lens of color and race, social theory tells us we should look to see whether our machine vision models are more sensitive to skin tone or cultural indicators. This can help differentiate, for example, whether biases in the model derive from cultural norms embedded in the population that make up the training data or from under-representation of individuals with certain skin tones in the training data (or both) [44, 10].\nA good example of how theory can be used to guide interpretation is the work from Bamman et al. [4], who identify tropes of literary characters. They validate their model by testing specific parameters against a slate of theory-based hypotheses. These hypotheses, derived from theory on the writing styles of authors during the time period of study, took the form of \u201ccharacter X is more similar to character Y than either X or Y is to a distractor character Z.\u201d Good models were those that accurately predicted these theorized character similarities. By combining visualization, summarization, and other interpretive techniques to test different theories, we develop an account of the world that is consistent with established research rather than what is obvious [119].\nOftentimes, we take our interpretation of model behavior and develop an account of why the model did what it did based on that interpretation. This often serves as an explanation of what the model did and an effort to build new theory. However, when we build explanations and new theory only on insights gleaned from examining model behavior, we are at risk of developing folk theory [22]. Folk theory involves leaning on common understanding to \u201cread the tea leaves\u201d [17] characterizing human behavior as simply \u201cmaking\nsense.\u201d This is dangerous, however [64]. Models will always output something and some model will always outperform others on some metrics. Building theory only from numerical outputs serves only to reinforce common myths and biases.\nIf we would like for our explanations of our model to contribute to a broader understanding of the social world, we need to not only find the right explanation for each model, but to also integrate many models and explanations into a coherent account of the world. Nelson\u2019s work on the development of second wave feminism is a prime example. She used social network and feminist theory to build different machine-learning based models for the structure of feminist communities in New York and Chicago. She then compared the structures of the social and idea networks to show that the ideas central to feminist community in New York were more aligned with what we understand today to be \u201csecond wave\u201d feminism and that their community was more densely connected than that in Chicago. She argues this dense connectivity enabled feminists in New York to set the agenda for feminism in the 1960s and 70s."}, {"heading": "4.2 Generalizability", "text": "Generalizability refers to the goal of understanding how well results apply to the cases that were not tested. For example, if we develop a model to predict unemployment using mobile phone data in Europe [108], an analysis of generalizability might involve assessing whether the same approach would work in Algeria, Canada, or Mexico. We might also like to know how likely it is to work on mobile phone data from five years ago or five years in the future. And, we might want to know whether a similar kind of approach would work on other data sets like social media posts, internet searches, or transit data.\nIn machine learning, generalizability is often addressed technically by reapplying the same methodology to other data to see whether it performs similarly to the original. For example, generalizability of a particular modeling approach like topic models might be tested by applying a fitted model to different kinds of data and different problems. We also test the generalizability of a particular analytic approach by reapplying it in different domains. For example, Lucas et al. [75] use machine translation across multiple languages to study whether politics in different countries were constituted by the same issues being discussed in the same ways.\nBeyond mechanical tests, machine learning researchers have developed automated procedures to try to encourage generalizability of their models. Many of these approaches are ways to mitigate overfitting, which can improve model generalizability by reducing the extent to which a model learns from patterns unique to the given data and/or feature set [47]. Recently, efforts have been made to train a model that learns representations of some universal input which can then be fine-tuned to apply to a variety of problems. For example, ResNet [105] and BERT [25] learn generic representations for images and sentences, respectively, and can then be fine-tuned for various classification tasks.\nHowever, while these technical solutions can make individual models more generalizable, they cannot help us establish why a result on one dataset can be generalized (or not) to others. For this, we need theory to tell us what similarities and differences are salient. Zeynep Tufekci [111] makes this point when arguing that we cannot treat one online platform (i.e. Twitter) as a stand-in for all others \u2013 as a model organism for society. The platform rules, social dynamics, and population that make Twitter worth engaging in for its users also distinguish it fundamentally from services like Facebook, Instagram, and WhatsApp. For example, theories of homophily suggests that, on any platform, people will associate with others like them. Yet, the commonalities on which we build connections depend on the platform itself. Our friends, colleagues, and public figures are on Twitter and our family is on Facebook. Following Goffman\u2019s theory of presentation of self, these differences in audiences drive people to behave differently on different platforms [38].\nMoving forward then, we see theory as a way to develop paradigms for understanding for particular kinds of data, like data from different social media platforms, and for how models might be tweaked to generalize beyond the data they were trained on."}, {"heading": "4.3 Parsimony", "text": "Parsimony refers to the goal of building a model with as few parameters as possible while still maximizing performance. Machine learning models benefit from parsimony because it decreases complexity and cost, limits the danger of overfitting, and makes it easier to visualize [47].\nA variety of technical approaches for constructing parsimonious models exist. For example, we can use topics, clustering, or factoring to reduce feature dimensionality, and regularization to remove weak predictors. In the case of neural networks, we also use techniques like drop-out [33] or batch normalization [54].\nThere are, however, three flaws with these approaches to addressing parsimony. First, because many features are correlated with one another and the outcome, technical approaches often arbitrarily select certain correlated features and not others. This arbitrary selection can make it difficult to differentiate between truly irrelevant features and those that are simply correlated with other relevant features. Second, decisions on when the model is \u201cparsimonious enough\u201d rely largely on heuristic comparisons between model performance on training and validation data (e.g. the \u201c1-Standard Error rule\u201d used in the popular glmnet package in R [32]). Such approaches cannot help guide our intuitions on what number of predictors \u201cmakes sense\u201d. Finally, the standard machine learning assumption that we need many features can be incorrect even at relatively low values. It is often the case in social science problems that a small set of variables can easily explain a large part of the variance. A regularizer may select 1,000 features out of 10,000 while the best model may only need 50.\nSocial theory provides a solution to these issues by helping us define small sets, or \u201cbuckets,\u201d of variables that we expect to explain a large portion of the variance in the outcome. Instead of starting with many features and trying to weed out irrelevant ones, theory points us in the direction of the most important variables, helping establish a baseline level of predictability from which we can assess whether additional features provide additional performance. Similarly, because theory provides us with the features we expect to be important, we may be able to identify cases in which regularization removes important, stable predictors due to correlation amongst variables.\nThe idea of identifying parsimonious, theoretically-informed baseline models for comparison has been shown to work well in practice. In their study of Twitter cascades, Goel et al. [37] show that a simple model which accounts only for popularity of the user is an extremely strong baseline for predicting the size of a retweet cascade. These ideas align with theories of source credibility [51] and the importance of influential nodes in information spread [78] that emphasize the centrality of social network structure in information flow. Similarly, social homophily has also long provided simple yet powerful baselines for recommendation systems [81].\nIn linguistics, simple theory-driven models of have routinely been shown to have high levels of predictive validity. For example, as Bender notes [9], theories of linguistic variation across languages are critical for the expansion of existing English-only techniques to a broader array of languages for which the same level of training data and/or knowledge is not available. Combined, these efforts have informed the development of more formal social theory on the limits of predictability in social systems [50], which may further extend our ability to estimate the degree of parsimony expected for particular problems."}, {"heading": "4.4 Fairness", "text": "In both popular media [72] and academic literature [82], significant attention has turned to the question of how machine learning models may lead to increased discrimination against certain social groups. This has translated into two areas of work. First, scholars have focused on the construction of fair machine learning models which purport to alleviate discrimination in model outputs altogether. Second, a significant amount of research has focused on measuring fairness of the output. In both, fairness is defined at either the group level, such that there is no difference in outcome between groups, or at the individual level, such that individuals are treated similarly regardless of group status [53].\nA host of technical approaches have been developed to construct fair models and measure of fairness. For a recent overview on the relationship of this research to decades-old discussions of fairness in statistical models, see [53] or [82]. The bulk of the work to ensure fair outcomes has focused on batch classification and various manipulations of either data input to the model (e.g. de-biasing data according to groups [62]) or modifications to existing models (e.g. via a regularization term ensuring individual fairness [60]) to ensure fair outcomes. On the metric construction side, scholars have recently focused on developing measures that account for sociologically relevant phenomena like intersectionality3 [31], on the tradeoffs between existing measures [65], and on a better understanding of the causal assumptions of different measures [36] amongst other tasks.\n3Although see the critique from Hoffmann of this work [49]\nHowever, an increasing number of scholars have identified important complications to defining fairness technically. This work has emphasized 1) that different people have different views on what is fair, 2) that the views of those in power are the views that are most likely to be used, and 3) that models emerge from a vast and complex sociotechnical landscape where discrimination emerges from many other places beyond the models themselves [20, 101, 5, 41, 49]. One conclusion has been that a fair algorithm cannot fix a discriminatory process. For example, recidivism prediction algorithms will almost certainly be used in a discriminatory fashion, regardless of whether or not the models themselves are fair [41]. We need social theory to better understand the social processes in which these algorithms are embedded. Social theory enables us to distinguish discrimination caused by the algorithm from that originating in the social system itself.\nPerhaps equally important, theory can also help us to understand the consequences of unfair and/or biased algorithms. For example, take recent work showing that search algorithms return gender and race stereotypical images for various occupations [61]. Social psychology theories focusing on representation emphasize that from a young age, we internalize representations of occupations and skills that cause us to shift towards those that are stereotypical of our own perceived gender [11]. Thus, while technical solutions may help us to identify such problems, they cannot explain the impacts of these biases and thus why they should be addressed and how.\nFinally, social theory helps to identify how unfair machine learning impacts our knowledge about the world. Biased algorithms, such as those that detect gender and race for demographic comparisons [59], can bias the science we produce. Standpoint theory and other critical epistemological theories have shown how who does science and who\u2019s data are used for what analysis affects what we know about the social world [46, 45]. We do not want to replicate the patterns of exclusion and stigmatization found in the history of medicine [79], psychology [30], and sociology [123] by throwing out data from marginalized people, only studying marginalized people as the Other, or not allowing marginalized people speak for themselves about their data."}, {"heading": "5 Conclusion", "text": "The combination of machine learning methods and big social data offers us an exciting array of scientific possibilities. However, work in this area too often privileges machine learning models that perform well over models that are founded in a deeper understanding of the society under study. At best, this trade-off puts us in danger of advancing only computer science rather than both computer science and social science. At worst, these efforts push the use of machine learning for social data towards pseudoscience, where misappropriated algorithms are deployed to make discriminatory decisions and baseless social scientific claims are made.\nHowever, as the many positive examples we have highlighted here show, machine learning and big social data can be used to produce important, ground-breaking research. To do so, the examples we highlight have baked social theory into each step of the machine learning pipeline. These works do not cherry-pick one theory, ex post facto, to support their claims. Instead, they use multiple, potentially competing theories, at every step of the pipeline, to justify their inputs and help validate their outputs. In using, or at least acknowledging, competing theories, we can elucidate where disagreements exist and therefore which technical trade-offs are most important.\nThe positive examples we highlight, our review of the negative cases, and the related work we draw on pave the way forward for the scientifically-grounded, ethical application of machine learning to social data. But our efforts must move beyond the way we produce research to the ways we review it, consume it, and encourage it as a research community. As reviewers, for example, we must ask ourselves if the work we are looking at is justified not only by statistical theory, but by social theory as well. And as a community, we must find ways to feature and promote papers that may not have the flashiest \u201cTable 1,\u201d but that provide a careful and well-grounded social scientific study.\nMachine learning can and should become a critical piece of social science. The solution does not necessarily require a computer scientist to \u201cgo find a social scientist,\u201d or vice versa. There is already a wealth of knowledge to draw from, and we should not allow ourselves or others to avoid delving into it simply because it is \u201cout of our field.\u201d For those who do not know where to start, we hope this paper is a guide to anyone for how to use that knowledge to address specific questions in the research. Similarly, social science should\nbecome an increasingly important part of machine learning. In incorporating social theory into their work, machine learning researchers need not reliquish model performance as the ultimate goal; we have argued here that, instead, theory can help guide the path to even better models and predictive performance."}, {"heading": "6 Acknowledgements", "text": "We greatly appreciate the assistance of Laura Nelson, Celeste Campos-Casillo, stef shuster, Atri Rudra, and David Lazer, who all provided invaluable feedback on earlier versions of this work. That said, these individuals of course bear no responsibility for the current content; all issues, errors, and omissions are the fault of the authors alone."}], "title": "Theory In, Theory Out: The uses of social theory in machine learning for social science", "year": 2020}