{
  "abstractText": "There is a growing concern that the recent progress made in AI, especially regarding the predictive competence of deep learning models, will be undermined by a failure to properly explain their operation and outputs. In response to this disquiet, counterfactual explanations have become massively popular in eXplainable AI (XAI) due to their proposed computational, psychological, and legal benefits. In contrast however, semi-factuals, which are a similar way humans commonly explain their reasoning, have surprisingly received no attention. Most counterfactual methods address tabular rather than image data, partly due to the latter\u2019s non-discrete nature making good counterfactuals difficult to define. Additionally, generating plausible looking explanations which lie on the data manifold is another issue which hampers progress. This paper advances a novel method for generating plausible counterfactuals (and semi-factuals) for black-box CNN classifiers doing computer vision. The present method, called PlausIble Exceptionality-based Contrastive Explanations (PIECE), modifies all \u201cexceptional\u201d features in a test image to be \u201cnormal\u201d from the perspective of the counterfactual class (hence concretely defining a counterfactual). Two controlled experiments compare this method to others in the literature, showing that PIECE not only generates the most plausible counterfactuals on several measures, but also the best semi-factuals.",
  "authors": [
    {
      "affiliations": [],
      "name": "Eoin M. Kenny"
    },
    {
      "affiliations": [],
      "name": "Mark T. Keane"
    }
  ],
  "id": "SP:89d60c79707af1a1a4f8002d7749352235561a8f",
  "references": [
    {
      "authors": [
        "A. Adadi",
        "M. Berrada"
      ],
      "title": "Peeking inside the blackbox: A survey on Explainable Artificial Intelligence (XAI)",
      "venue": "IEEE Access 6: 52138\u201352160.",
      "year": 2018
    },
    {
      "authors": [
        "R.M. Byrne"
      ],
      "title": "Counterfactual thought",
      "venue": "Annual review of psychology 67: 135\u2013157.",
      "year": 2016
    },
    {
      "authors": [
        "R.M. Byrne"
      ],
      "title": "Counterfactuals in explainable artificial intelligence (XAI): evidence from human reasoning",
      "venue": "Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI-19, 6276\u20136282.",
      "year": 2019
    },
    {
      "authors": [
        "A. Dhurandhar",
        "P.-Y. Chen",
        "R. Luss",
        "C.-C. Tu",
        "P. Ting",
        "K. Shanmugam",
        "P. Das"
      ],
      "title": "Explanations based on the missing: Towards contrastive explanations with pertinent negatives",
      "venue": "Advances in neural information processing systems, 592\u2013603.",
      "year": 2018
    },
    {
      "authors": [
        "J. Dodge",
        "Q.V. Liao",
        "Y. Zhang",
        "R.K. Bellamy",
        "C. Dugan"
      ],
      "title": "Explaining models: an empirical study of how explanations impact fairness judgment",
      "venue": "Proceedings of the 24th International Conference on Intelligent User Interfaces, 275\u2013285.",
      "year": 2019
    },
    {
      "authors": [
        "Y. Gal",
        "Z. Ghahramani"
      ],
      "title": "Dropout as a bayesian approximation: Representing model uncertainty in deep learning",
      "venue": "international conference on machine learning, 1050\u2013 1059.",
      "year": 2016
    },
    {
      "authors": [
        "I. Goodfellow",
        "J. Pouget-Abadie",
        "M. Mirza",
        "B. Xu",
        "D. Warde-Farley",
        "S. Ozair",
        "A. Courville",
        "Y. Bengio"
      ],
      "title": "Generative adversarial nets",
      "venue": "Advances in neural information processing systems, 2672\u20132680.",
      "year": 2014
    },
    {
      "authors": [
        "I.J. Goodfellow",
        "J. Shlens",
        "C. Szegedy"
      ],
      "title": "Explaining and harnessing adversarial examples",
      "venue": "arXiv preprint arXiv:1412.6572 .",
      "year": 2014
    },
    {
      "authors": [
        "R.M. Grath",
        "L. Costabello",
        "C.L. Van",
        "P. Sweeney",
        "F. Kamiab",
        "Z. Shen",
        "F. Lecue"
      ],
      "title": "Interpretable credit application predictions with counterfactual explanations",
      "venue": "arXiv preprint arXiv:1811.05245 .",
      "year": 2018
    },
    {
      "authors": [
        "R. Guidotti",
        "A. Monreale",
        "S. Ruggieri",
        "F. Turini",
        "F. Giannotti",
        "D. Pedreschi"
      ],
      "title": "A survey of methods for explaining black box models",
      "venue": "ACM computing surveys (CSUR) 51(5): 1\u201342.",
      "year": 2018
    },
    {
      "authors": [
        "D. Gunning"
      ],
      "title": "Explainable artificial intelligence (xai)",
      "venue": "Defense Advanced Research Projects Agency (DARPA), nd Web 2.",
      "year": 2017
    },
    {
      "authors": [
        "D. Hendrycks",
        "K. Gimpel"
      ],
      "title": "A baseline for detecting misclassified and out-of-distribution examples in neural networks",
      "venue": "arXiv preprint arXiv:1610.02136 .",
      "year": 2016
    },
    {
      "authors": [
        "M.T. Keane",
        "B. Smyth"
      ],
      "title": "Good Counterfactuals and Where to Find Them: A Case-Based Technique for Generating Counterfactuals for Explainable AI (XAI)",
      "venue": "International Conference on Case-Based Reasoning. Springer.",
      "year": 2020
    },
    {
      "authors": [
        "E.M. Kenny",
        "M.T. Keane"
      ],
      "title": "Twin-systems to explain artificial neural networks using case-based reasoning: comparative tests of feature-weighting methods in ANNCBR twins for XAI",
      "venue": "Twenty-Eighth International Joint Conferences on Artifical Intelligence (IJCAI), Macao, 10-16",
      "year": 2019
    },
    {
      "authors": [
        "D.P. Kingma",
        "M. Welling"
      ],
      "title": "Auto-encoding variational bayes",
      "venue": "arXiv preprint arXiv:1312.6114 .",
      "year": 2013
    },
    {
      "authors": [
        "T. Laugel",
        "M.-J. Lesot",
        "C. Marsala",
        "X. Renard",
        "M. Detyniecki"
      ],
      "title": "The dangers of post-hoc interpretability: Unjustified counterfactual explanations",
      "venue": "arXiv preprint arXiv:1907.09294 .",
      "year": 2019
    },
    {
      "authors": [
        "Y. LeCun",
        "C. Cortes",
        "C. Burges"
      ],
      "title": "MNIST handwritten digit database",
      "venue": "ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist 2.",
      "year": 2010
    },
    {
      "authors": [
        "Z.C. Lipton"
      ],
      "title": "The mythos of model interpretability",
      "venue": "Queue 16(3): 31\u201357.",
      "year": 2018
    },
    {
      "authors": [
        "S. Liu",
        "B. Kailkhura",
        "D. Loveland",
        "H. Yong"
      ],
      "title": "Generative Counterfactual Introspection forExplainable Deep Learning",
      "venue": "Technical report, Lawrence Livermore National Lab.(LLNL), Livermore, CA (United States).",
      "year": 2019
    },
    {
      "authors": [
        "A. Lucic",
        "H. Haned",
        "M. de Rijke"
      ],
      "title": "Why does my model fail? contrastive local explanations for retail forecasting",
      "venue": "In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency,",
      "year": 2020
    },
    {
      "authors": [
        "D. Mahajan",
        "C. Tan",
        "A. Sharma"
      ],
      "title": "Preserving Causal Constraints in Counterfactual Explanations for Machine Learning Classifiers",
      "venue": "arXiv preprint arXiv:1912.03277 .",
      "year": 2019
    },
    {
      "authors": [
        "A. Malinin",
        "M. Gales"
      ],
      "title": "Predictive uncertainty estimation via prior networks",
      "venue": "Advances in Neural Information Processing Systems, 7047\u20137058.",
      "year": 2018
    },
    {
      "authors": [
        "R. McCloy",
        "R.M. Byrne"
      ],
      "title": "Semifactual even if thinking",
      "venue": "Thinking & Reasoning 8(1): 41\u201367.",
      "year": 2002
    },
    {
      "authors": [
        "T. Miller"
      ],
      "title": "Contrastive explanation: A structural-model approach",
      "venue": "arXiv preprint arXiv:1811.03163 .",
      "year": 2018
    },
    {
      "authors": [
        "T. Miller"
      ],
      "title": "Explanation in artificial intelligence: Insights from the social sciences",
      "venue": "Artificial Intelligence 267: 1\u201338.",
      "year": 2019
    },
    {
      "authors": [
        "C. Nugent",
        "D. Doyle",
        "P. Cunningham"
      ],
      "title": "Gaining insight through case-based explanation",
      "venue": "Journal of Intelligent Information Systems 32(3): 267\u2013295.",
      "year": 2009
    },
    {
      "authors": [
        "M. Pawelczyk",
        "K. Broelemann",
        "G. Kasneci"
      ],
      "title": "Learning Model-Agnostic Counterfactual Explanations for Tabular Data",
      "venue": "Proceedings of The Web Conference 2020, 3126\u20133132.",
      "year": 2020
    },
    {
      "authors": [
        "J. Pearl"
      ],
      "title": "Causality: Models, reasoning and inference cambridge university press",
      "venue": "Cambridge, MA, USA, 9: 10\u201311.",
      "year": 2000
    },
    {
      "authors": [
        "S. Pichai"
      ],
      "title": "AI at Google: our principles",
      "venue": "https://www. blog.google/technology/ai/ai-principles/. [Online; accessed 01-June-2020].",
      "year": 2018
    },
    {
      "authors": [
        "R. Poyiadzi",
        "K. Sokol",
        "R. Santos-Rodriguez",
        "T. De Bie",
        "P. Flach"
      ],
      "title": "FACE: Feasible and actionable counterfactual explanations",
      "venue": "Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society, 344\u2013350.",
      "year": 2020
    },
    {
      "authors": [
        "A. Radford",
        "J. Wu",
        "R. Child",
        "D. Luan",
        "D. Amodei",
        "I. Sutskever"
      ],
      "title": "Language models are unsupervised multitask learners",
      "venue": "OpenAI Blog 1(8): 9.",
      "year": 2019
    },
    {
      "authors": [
        "M.T. Ribeiro",
        "S. Singh",
        "C. Guestrin"
      ],
      "title": " Why should i trust you?\u201d Explaining the predictions of any classifier",
      "venue": "Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, 1135\u20131144.",
      "year": 2016
    },
    {
      "authors": [
        "C. Russell"
      ],
      "title": "Efficient search for diverse coherent explanations",
      "venue": "Proceedings of the Conference on Fairness, Accountability, and Transparency, 20\u201328.",
      "year": 2019
    },
    {
      "authors": [
        "P. Samangouei",
        "A. Saeedi",
        "L. Nakagawa",
        "N. Silberman"
      ],
      "title": "ExplainGAN: Model Explanation via Decision Boundary Crossing Transformations",
      "venue": "Proceedings of the European Conference on Computer Vision (ECCV), 666\u2013 681.",
      "year": 2018
    },
    {
      "authors": [
        "J.C. Seah",
        "J.S. Tang",
        "A. Kitchen",
        "F. Gaillard",
        "A.F. Dixon"
      ],
      "title": "Chest radiographs in congestive heart failure: visualizing neural network learning",
      "venue": "Radiology 290(2): 514\u2013 522.",
      "year": 2019
    },
    {
      "authors": [
        "S. Singla",
        "B. Pollack",
        "J. Chen",
        "K. Batmanghelich"
      ],
      "title": "Explanation by Progressive Exaggeration",
      "venue": "International Conference on Learning Representations.",
      "year": 2019
    },
    {
      "authors": [
        "A. Van Looveren",
        "J. Klaise"
      ],
      "title": "Interpretable counterfactual explanations guided by prototypes",
      "venue": "arXiv preprint arXiv:1907.02584 .",
      "year": 2019
    },
    {
      "authors": [
        "S. Wachter",
        "B. Mittelstadt",
        "C. Russell"
      ],
      "title": "Counterfactual explanations without opening the black box: Automated decisions and the GDPR",
      "venue": "Harv. JL & Tech. 31: 841.",
      "year": 2017
    },
    {
      "authors": [
        "L. Xu",
        "M. Skoularidou",
        "A. Cuesta-Infante",
        "K. Veeramachaneni"
      ],
      "title": "Modeling tabular data using conditional gan",
      "venue": "Advances in Neural Information Processing Systems, 7333\u20137343.",
      "year": 2019
    }
  ],
  "sections": [
    {
      "heading": "Introduction",
      "text": "In the last few years, emerging issues around the the interpretability of machine learning models have elicited a major, on-going response from government (Gunning 2017), industry (Pichai 2018), and academia (Miller 2019) on eXplainable AI (XAI) (Guidotti et al. 2018; Adadi and Berrada 2018). As opaque, black-box deep learning models are increasingly being used in the \u201creal world\u201d for high-stakes decision making (e.g., medicine and law), there is a pressing need to give end-users some insight into how these models achieve their predictions. In this paper, we advance a new technique for XAI using counterfactual and semi-factual explanations, applied to deep learning models [i.e., convolutional neural networks (CNNs)]. These \u201ccontrastive explanations\u201d have attracted massive interest in AI (Miller 2018; Wachter, Mittelstadt, and Russell 2017), but have never directly examined semi-factual explanations. This is important\n*Corresponding Author.\nbecause counterfactual explanations appear to offer computational, psychological, and legal advantages over other explanation strategies, and semi-factuals should also. In this introduction, we review the importance of contrastive explanation and related work."
    },
    {
      "heading": "Contrastive Explanation",
      "text": "To understand what makes counterfactuals important, consider the difference between factual and counterfactual explanations. An AI loan application system could explain its decision factually saying \u201cYou were refused because a previous customer with your profile asked for this amount, and was also refused\u201d. In contrast, a counterfactual explanation of the same refusal might say \u201cIf you applied for a slightly lower amount, you would have been accepted\u201d. The proponents of counterfactuals argue that they have distinct computational, psychological, and legal benefits for XAI. Computationally, counterfactuals provide explanations without having to \u201copen the black box\u201d (Grath et al. 2018). Psychologically, counterfactuals elicit spontaneous, causal thinking in\nar X\niv :2\n00 9.\n06 39\n9v 1\n[ cs\n.L G\n] 1\n0 Se\np 20\n20\npeople, thus making explanations that use them more engaging (Byrne 2019; Miller 2019). Legally, it is argued that counterfactual explanations are GDPR compliant (Wachter, Mittelstadt, and Russell 2017).\nSimilar arguments for counterfactuals can be also made for semi-factual explanations, were humans typically begin an explanation with the words \u201cEven if...\u201d. For example, the previous AI loan system might say \u201cEven if you had asked for a slightly lower amount, you still would have been refused\u201d. Semi-factuals are a common form of human explanation and have been researched in psychology for decades (McCloy and Byrne 2002), they offer the benefits of contrastive explanations (e.g., counterfactuals) without having to cross a decision boundary, which in turn decreases the amount of featural changes needed to convey an explanation. This is important because the less featural changes there are, the more interpretable the explanation likely is (Keane and Smyth 2020). This issue is one of the main drawbacks of counterfactual explanations, which semi-factuals can conceivably help correct. Despite this however, semi-factual reasoning has been largely ignored in the AI community, they sit between factuals and counterfactuals (see Fig. 1), offering causal justifications for same-class predictions. Additionally, semi-factuals have the advantage of decreasing negative emotions in people when compared to counterfactuals (McCloy and Byrne 2002), which may have a notable use when giving explanations for bad news such as a loan rejection, or a devastating medical diagnosis. Lastly, semifactuals can make a prediction seem incontestable (Byrne 2019), which is highly effective for convincing people a classifier is correct (Nugent, Doyle, and Cunningham 2009).\nThese explanation strategies for interpreting AI models \u2013 factual, counterfactual, and semi-factual \u2013 are typically used for post-hoc explanation-by-example (Lipton 2018). In general, post-hoc explanations provide after-the-fact justifications for why a prediction was made using nearest-neighbor training instances (Kenny and Keane 2019), generated synthetic instances (Wachter, Mittelstadt, and Russell 2017), or feature contributions (Ribeiro, Singh, and Guestrin 2016)."
    },
    {
      "heading": "Related Work",
      "text": "Most post-hoc explanation-by-example research on counterfactuals has focused on discrete data such as tabular datasets [e.g., see (Grath et al. 2018)]. These methods aim to generate minimally-different counterfactual instances that can plausibly explain test instances [i.e., instances from a \u201cpossible world\u201d (Pawelczyk, Broelemann, and Kasneci 2020)].1 These counterfactual explanation techniques can be divided into \u201cblind perturbation\u201d and \u201cexperienceguided\u201d methods (Keane and Smyth 2020). Blind perturbation methods generate candidate counterfactual explanations by perturbing feature values of the test instance to find minimally-different instances from a different/opposing class [e.g., (Wachter, Mittelstadt, and Russell 2017)], using distance metrics to select \u201cclose\u201d instances. Experience-\n1There is a literature using Causal Bayesian Networks to assess fairness of AI systems (Pearl 2000). This is a different use of counterfactuals for another aspect of XAI.\nguided methods rely more directly on the training data by justifying counterfactual selection using training instances (Laugel et al. 2019), analyzing features of the training data (Grath et al. 2018), or by directly adapting training instances (Keane and Smyth 2020). At present, it is unclear which works best, as there is no agreed standard for computational evaluation, and few papers perform user evaluations [but see (Dodge et al. 2019; Lucic, Haned, and de Rijke 2020)]. With respect to semi-factual explanations, there is only one relevant paper, a case-based reasoning work detailing a fortiori reasoning (Nugent, Doyle, and Cunningham 2009), which follows a similar explanation paradigm to semi-factuals, but this focused only on tabular data.\nThe applicability of the above techniques to image data remains an open question, largely due to the difference between discrete (e.g., tabular and text) and non-discrete domains (i.e., images). In image datasets, a separate literature examines counterfactuals for adversarial attacks, rather than generating them for XAI. In adversarial attacks, small changes are made (i.e., at the pixel level of an image) to generate synthetic instances to induce misclassifications (Goodfellow, Shlens, and Szegedy 2014). Typically, these micro-level perturbations are constructed to be human-undetectable. In XAI however, counterfactual feature changes need to be human detectable, comprehensible, and plausible (see Fig. 1). With this in mind, some recent work has notably used variational autoencoders (VAEs) (Kingma and Welling 2013) and generative adversarial networks (GANs) (Goodfellow et al. 2014) to produce counterfactual images with large featural-changes for XAI. Within this literature, the most relevant research to ours are those which utilize GANs to produce explanations (Samangouei et al. 2018; Seah et al. 2019; Singla et al. 2019; Liu et al. 2019), but only one of these methods is able to offer explanations for pre-trained CNNs in multi-class classification (Liu et al. 2019), which we compare our method to here (see Expt. 1). This preference for binary classification is partly because choosing a counterfactual class in multiclass classification is non-trivial, and optimization to arbitrary classes is susceptible to local minima, but PIECE overcomes these issues and automates the process. In addition, none of this previous research has considered modifying exceptional features to generate explanations, or semi-factuals.\nPresent Contribution. This paper reports PlausIble Exceptionality-based Contrastive Explanations (PIECE), a novel algorithm for generating contrastive explanations for any CNN. PIECE automatically models the distributions of learned latent features to detect \u201cexceptional features\u201d in a test instance, modifying them to be \u201cnormal\u201d in explanation generation. PIECE automates the counterfactual generation process in multi-class classification, and is applicable to any pre-trained CNN. Experimental tests show that this method advances the state-of-the-art for counterfactual explanations in quantitative measurements (see Expt. 1). Additionally, semi-factual explanations are considered here for the first time in deep learning, and PIECE is shown to produce them appreciably better than other methods (see Expt. 2). So,\npost-hoc explanation in XAI is significantly advanced by this work."
    },
    {
      "heading": "PlausIble Exceptionality-based Contrastive Explanations (PIECE)",
      "text": "Plausibility is the major challenge facing contrastive explanations for XAI. A good counterfactual explanation needs to be plausible, informative, and actionable (Poyiadzi et al. 2020; Byrne 2019). For example, good counterfactual explanations in a loan application system should not propose implausible feature-changes (e.g., \u201cIf you earned $1M more, you would get the loan\u201d). For images, plausible counterfactuals need to modify human-detectable features (see Fig. 2); indeed, some methods can generate synthetic instances that are not even within the data distribution (Laugel et al. 2019). Accordingly, an explanation-instance\u2019s proximity to the data distribution is now commonly used as a proxy for evaluating plausibility (Van Looveren and Klaise 2019; Samangouei et al. 2018), which we use as our approach for evaluation.\nFig. 2 illustrates some of PIECE\u2019s plausible contrastive explanations for a CNN\u2019s classifications on MNIST (LeCun, Cortes, and Burges 2010), alongside a factual explanation for completeness. In Fig. 2c, the test image of an \u201c8\u201d misclassified as a \u201c3\u201d, is shown alongside its counterfactual explanation, showing feature changes that would cause the CNN to classify it as an \u201c8\u201d (i.e., the cursive stroke making the plausible \u201c8\u201d image). An implausible counterfactual, generated by a minimal-edit method (i.e., the Min-Edit method in Expt. 1), is also shown, with human-undetectable feature-changes that would also cause the CNN to classify the image as an \u201c8\u201d. Fig 2b shows a semi-factual, with meaningful changes to the test image that do not change the CNN\u2019s prediction. That is, even if the \u201c9\u201d had a very open loop, so it looked more like a \u201c4\u201d, the CNN would still classify it as a \u201c9\u201d. This type of explanation has potential to convince people the original classification was definitely correct (Byrne 2019; Nugent, Doyle, and Cunningham 2009). Finally, though these examples show two explana-\ntions for incorrect predictions (factual and counterfactual), and one for a correct prediction (semi-factual), it should be noted that these three explanation types may be generated for either predictive outcome.\nPIECE uses an experience-guided approach, exploiting the distributional properties of the training data. The algorithm generates counterfactuals and semi-factuals by identifying \u201cexceptional\u201d features in the test image, and then modifying these to be \u201cnormal\u201d. This idea is inspired by people\u2019s spontaneous use of counterfactuals, specifically the exceptionality effect, were people change exceptional events into what would normally have occurred (Byrne 2019). For example, when people are told that \u201cBill died in a car crash taking an unusual route home from work\u201d, they typically respond counterfactually, saying \u201cif only he had taken his normal route home, he might have lived\u201d (Byrne 2016). So, PIECE identifies probabilistically-low feature-values in the test image (i.e., exceptional features) and modifies them to be their expected values in the counterfactual class (i.e., normal features)."
    },
    {
      "heading": "The Algorithm: PIECE",
      "text": "PIECE involves two distinct systems, a CNN with predictions to be explained, and a GAN that helps generate counterfactual or semi-factual explanatory images (see Section S1 supplement for model architectures). This algorithm will work with any CNN post-training, provided there is a GAN trained on the same dataset as the CNN. PIECE has three main steps: (i) \u201cexceptional\u201d features are identified in the CNN for a test image from the perspective of the counterfactual class, (ii) these are then modified to be their expected values, and (iii) the resulting latent-feature representation of the explanatory counterfactual is visualized in the pixelspace with help from the GAN. To produce semi-factuals, the algorithm is identical, but the feature modifications in step two are stopped prematurely before the model\u2019s prediction crosses the counterfactual decision boundary.\nSetup and Notation. Allow all layers in the CNN up to the penultimate extracted feature layer X be C, and its output classifier S (see Fig. 3). The extracted features from a test image I at layer X will be denoted as x, this connects to an output SoftMax layer to give a probability vector Y which predicts a class c. To denote that c is the class in Y with the largest probability (i.e., the predicted class), Yc will be used. Let the generator in the GAN be G, and its latent input z, which together produce a given image. The counterfactuals to a test image I , in class c, with latent features x, are denoted as I \u2032, c\u2032 and x\u2032, respectively.\nIdentify the Counterfactual Class. The initial steps involve locating a given test image I inG, and then identifying the counterfactual class c\u2032. First, to find the input vector z for G, such that G(z) \u2248 I , we solve the following optimization with gradient descent: z = argmin\nz0\n\u2016C(G(z0))\u2212 C(I)\u201622 + \u2016G(z0)\u2212 I\u201622 (1)\nwhere z0 is a sample from the standard normal distribution. More efficient methods exist to do this involving encoders (Seah et al. 2019), but Eq. (1) was sufficient here, and our focus is on more novel questions. Secondly, the counterfactual class c\u2032 for I may need to be generated for an incorrect or correct prediction. When the CNN incorrectly classifies I , c\u2032 is trivially selected as being the actual label (see Fig. 3). However, when the CNN\u2019s classification is correct for I , identifying c\u2032 becomes non-trivial. We use a novel method here involving gradient ascent to solve this problem and run:\nargmax z \u2016S(C(G(z)))\u2212 Yc\u201622 (2)\nwhere Yc is binary encoded as all 0s, and a 1 for the class c. During this optimization process, the first time a decision boundary is crossed, the new class is selected as c\u2032. Whilst hard-coding c\u2032 can result in the optimization becoming \u201cstuck\u201d (Liu et al. 2019), our method never failed to generate the desired counterfactual, and required no human intervention.\nStep 1: Identifying Exceptional Features Here, when the CNN classifies a test image I as class c, we identify its exceptional features in x by considering the statistical probability that each took its respective value, but from the perspective of c\u2032. So, assuming the use of ReLU activations in X, we can model each neuron Xi for c\u2032, as a hurdle model with:\np(xi) = (1\u2212 \u03b8i)\u03b4(xi)(0) + \u03b8ifi(xi), s.t. xi \u2265 0 (3)\nwhere xi is the neuron activation value, \u03b8i is the probability of the neuron i activating for the class c\u2032 (i.e., Bernoulli trial success), fi is the subsequent probability density function (PDF) modelled for when xi > 0 (i.e., when the \u201churdle\u201d is passed), the constraint of xi \u2265 0 refers to the ReLU activations, and \u03b4(xi)(0) is the Kronecker delta function, returning 0 for xi > 0, and 1 for xi = 0. Moving forward, Xi will signify the random variable associated with fi.\nTo model this, x is gathered from all training data into the latent dataset L, and considering the n output classes, we divide L into {Li}ni=1 where \u2200x \u2208 Li, S(x) = Yi. Now considering the counterfactual class data Lc\u2032 , let all data for some neuron Xi be {xj}mj=1 \u2208 Lc\u2032 , where m is the number of instances. If we let the number of thesem instances where xj 6= 0 be q, the probability of success \u03b8i in the Bernoulli trail can be modelled as \u03b8i = q/m, and the probability of failure as 1\u2212\u03b8i. The subsequent PDF fi from Eq. (3) is modelled with {xj}mj=1 \u2208 Lc\u2032 ,\u2200xj > 0. Importantly, the hurdle models use what S predicted each instance to be (rather than the label), because we wish to model what the CNN has learned, irrespective of whether it is objectively correct or incorrect.\nWe found empirically that the PDFs will typically approximate a Gaussian, Gamma, or Exponential distribution. Hence, we automated the modelling process by fitting the data with all three distributions (with and without a fixed location parameter of 0) using maximum likelihood estimation. Then, using the Kolmogorov-Smirnov test for goodness of fit across all these distributions, we chose the one\nof best fit. In all generated explanations, the average p-value for goodness of fit was p > 0.3 across all features. With the modelling process finished, a feature value xi is considered an exceptional feature xe for the test image I if:\nxi = 0 | p(1\u2212 \u03b8i) < \u03b1 (4)\nxi > 0 | p(\u03b8i) < \u03b1 (5) Glossed, Eq. (4) dictates that it is exceptional if a neuron Xi does not activate, given the probability of it not activating is less than \u03b1 for c\u2032 typically. Eq.(5) illustrates that it is exceptional if a neuron activates, given that the probability of it activating is less than \u03b1 for c\u2032 typically. The other two exceptional feature events are:\n\u03b8iFi(xi) < \u03b1 | xi > 0 (6)\n(1\u2212 \u03b8i) + \u03b8iFi(xi) > 1\u2212 \u03b1 | xi > 0 (7) where Fi is the cumulative distribution function for fi. Eq. (6) dictates that, given the neuron has activated, it is exceptional (i.e., a probability < \u03b1) to have such a low activation value for c\u2032. Eq. (7) relays that, given the neuron has activated, it is exceptional to have such a high activation value for c\u2032. In defining the \u03b1 threshold, the statistical hypothesistesting standard was adopted, categorizing any feature value which has a probability less than \u03b1 = 0.05 as being exceptional in both experiments.\nStep 2: Changing the Exceptional to the Expected The exceptional features {xe}ne=1 \u2208 x (where n is the number of exceptional features identified) divide into those that negatively or positively affect the classification of c\u2032 in I , PIECE only modifies the former (see Algorithm 1). Importantly, features are only modified if they meet the criteria regarding their connection weight, and identification process (i.e., found using Eq. (4)/(5)/(6) or (7)). Glossed, the algorithm only modifies the exceptional feature values to their expected values if doing so brings the CNN closer to modifying the classification to c\u2032. These exceptional features are ordered from the lowest probability to the highest, which is important in semi-factual explanations where the modification of features is stopped short of the decision boundary.\nStep 3: Visualizing the Explanation Finally, having constructed x\u2032, the explanation is visualized by solving the following optimization problem with gradient descent:\nz\u2032 = argmin z \u2016C(G(z))\u2212 x\u2032\u201622 (8)\nand inputting z\u2032 into G to visualize the explanation I \u2032.\nExperiment 1: Counterfactuals In this experiment, PIECE\u2019s performance is compared against other known methods for counterfactual explanation generation. The tests compare PIECE against other sufficiently general methods which are applicable to color datasets (Liu et al. 2019; Wachter, Mittelstadt, and Russell 2017) [here we use CIFAR-10 (Krizhevsky, Nair, and\nAlgorithm 1: Modify exceptional features in x to produce x\u2032\nInput: x: The latent features of the test image I Input: w: The weight vector connecting X to c\u2032\n1 foreach xe in {xe}ne=1 \u2208 x do . Ordered lowest probability to highest 2 if we > 0 and xe discovered with Eq. (4), Eq. (5), or Eq. (6) then 3 xe \u2190 E[Xe] . Using PDF modelled for c\u2032 in Eq. (3) 4 else if we < 0 and xe discovered with Eq. (5) or Eq. (7) then 5 xe \u2190 E[Xe] . Using PDF modelled for c\u2032 in Eq. (3) 6 end 7 return x (now modified to be x\u2032)\nHinton)], and then with the addition of other relevant works which focused on MNIST (Dhurandhar et al. 2018; Van Looveren and Klaise 2019). The methods compared in Expt. 1 are:\n\u2022 PIECE. The present algorithm, using Eq. (8), where all exceptional features were categorized with \u03b1 = 0.05, and subsequently modified.\n\u2022 Min-Edit. A simple minimal-edit perturbation method based on a direct optimization towards c\u2032, where the optimization used gradient descent and was immediately stopped when the decision boundary was crossed, defined by: z\u2032 = argmin\nz \u2016S(C(G(z)))\u2212 Yc\u2032\u201622.\n\u2022 Constrained Min-Edit (C-Min-Edit). A modified version of (Liu et al. 2019),2, and inspired by (Wachter, Mittelstadt, and Russell 2017), this optimized with gradient descent and stopped when the decision boundary was crossed, defined as: z\u2032 = argmin\nz max \u03bb \u03bb\u2016S(C(G(z))) \u2212 Yc\u2032\u201622 + d(C(G(z)), x).\n\u2022 Contrastive Explanations Method (CEM). Pertinent negatives from (Dhurandhar et al. 2018), which are a form of counterfactual explanation, implemented here using (Klaise et al.).\n\u2022 Interpretable Counterfactual Explanations Guided by Prototypes (Proto-CF). The method by (Van Looveren and Klaise 2019), implemented here using (Klaise et al.).\nHyperparameter choices are presented in Section S2 of the supplementary material. Although other similar techniques are reported in the literature (Singla et al. 2019; Samangouei et al. 2018; Seah et al. 2019), they are not applicable as they cannot explain CNNs which are pre-trained on multi-class classification problems.\n2They used the pixel rather than latent-space in d(.). We tested both but found no significant difference. However, the latent-space required a smaller \u03bb to find z\u2032, and was more stable (Russell 2019).\nSetup, Test Set, and Evaluation Metrics. For MNIST, a test-set of 163 images classified by the CNN was used which divided into: (i) correct classifications (N=60) with six examples per number-class, (ii) close-correct classifications (N=62), that had an output SoftMax probability < 0.8, where the CNN \u201cjust\u201d got the classification right,3 and (iii) incorrect classifications (N=41) by the CNN (i.e., every instance misclassified by the CNN). For CIFAR-10, the testset was divided into: (i) correct classifications (N=30) with three examples per class, and (ii) incorrect classifications (N=30) with three examples per class. All instances were randomly selected, with the obvious exception of MNIST\u2019s incorrect classifications.\nAlthough many measures have been proposed to quantitatively evaluate an explanation\u2019s plausibility, there are no agreed benchmark measures, but most researchers use some measure of proximity to the data distribution. One related work proposed IM1 and IM2, based on training multiple autoencoders (AEs) to test the generated counterfactual\u2019s relative reconstruction error (Van Looveren and Klaise 2019). However, as there can be issues interpreting IM2 (Mahajan, Tan, and Sharma 2019), we replaced it with Monte Carlo Dropout (Gal and Ghahramani 2016) (MC Dropout), a commonly used method for out-of-distribution detection (Malinin and Gales 2018), with 1000 forward passes. Additionally, we use R%-Substitutability (Samangouei et al. 2018) which measures how well generated explanations can substitute the actual training data. As there is relatively few explanations generated compared to the actual training datasets (163 compared to 60,000), we use k-NN on the pixel space of MNIST, as the classifier works well with small amounts of training data, and the centred nature of the MNIST dataset means it performs well normally (i.e., \u223c 97% accuracy). In the current experiment, the measures used were:\n\u2022 MC-Mean. Posterior mean of MC Dropout on the generated counterfactual image (higher is better).\n\u2022 MC-STD. Posterior standard deviation of MC Dropout on the generated counterfactual (lower is better).\n\u2022 NN-Dist. The distance of the counterfactual\u2019s latent representation at layer X from the nearest training instance measured with the L2 norm [i.e., the closest \u201cpossible\n3We understand SoftMax probability is not considered reliable for CNN certainty, but it\u2019s a good baseline (Hendrycks and Gimpel 2016).\nworld\u201d (Wachter, Mittelstadt, and Russell 2017)].\n\u2022 IM1. From (Van Looveren and Klaise 2019), an AE is trained on class c (i.e., AEc) and c\u2032 (i.e., AEc\u2032 ) to compute IM1 = \u2016I \u2032\u2212AEc\u2032 (I \u2032)|\u201622\n\u2016I\u2032\u2212AEc(I\u2032)\u201622 , where a lower score is con-\nsidered better.\n\u2022 Substitutability (R%-Sub). Inspired by (Samangouei et al. 2018), the method\u2019s generated counterfactuals are fit to a k-NN classifier (in pixel space) which predicts the MNIST test set. The original training set gives \u223c 97% accuracy with k-NN, if a method produces half that accuracy, its R%-Sub score is 50%.\nResults and Discussion. PIECE generates counterfactual explanations that are more plausible compared to the other methods in all tests, analysis using the Anderson-Darling test (AD) showed AD> 22, p < .001 significance to all these results (except IM1 on CIFAR-10). Notably, Proto-CF/CEM were the only methods that failed to find a counterfactual explanation for 20/25 images out of a total of 163 on MNIST, respectively. Interestingly, for all results on MNIST, a plot of the NN-Dist measure against the MC-Mean/MC-STD scores show a significant linear relationship r = -0.8/0.82. So, the more a generated counterfactual is grounded in the training data, the more likely it is to be plausible [as some have argued should be the case (Laugel et al. 2019)], see Section S4 of the supplementary material for these plots.\nExperiment 2: Semi-Factuals One paper (Nugent, Doyle, and Cunningham 2009) argued that semi-factual explanation (they called it a fortiori reasoning) should involve the largest possible feature modifications without changing the classification (e.g., \u201cEven if you trebled your salary, you would still not get the loan\u201d). However, they did not consider semi-factuals for image datasets, or perform controlled experiments. As such, a new evaluation method is needed to measure \u201cgood semi-factuals\u201d in terms of how far the generated semi-factual instance is from the test instance, without crossing the decision boundary into the counterfactual class c\u2032. To accomplish this in an image domain, here we use the L1 distance between the test image and synthetic explanatory semi-factual in the pixelspace (n.b., the greater the distance the better the method). In the present experiment, PIECE is only compared to the\nminimal-edit methods from Expt. 1 (i.e., Min-Edit and CMin-Edit), as the other methods (i.e., CEM and Proto-CF) cannot generate semi-factuals. To thoroughly evaluate all methods, three distinct tests were carried out (see Fig. 4). First, a max-edit run was performed on a set of test images, where each of the three methods produced their \u201cbest semi-factual\u201d. Specifically, Min-Edit and C-Min-Edit were allowed optimize until the next step would push them over the decision boundary into the counterfactual class c\u2032, and PIECE followed its normal protocol, but stopped Algorithm 1 when the next exceptional feature modification to x would alter the CNN classification such that S(x) 6= Yc. Second, the performance of the methods, on the same test set, for different proportions of feature changes were recorded. Specifically, PIECE only modifies 25%, 50%, 75%, and 100% of the exceptional-features from the first test, whilst the minedit methods were allowed to optimize to the same distance as PIECE (measured using L2 distance) in the latent-space for each of these four distances. This second test allows us to view the full spectrum of results for semi-factuals. Third, and finally, all the plausibility measures used in Expt. 1 were applied to PIECE for the same proportional-increments of changes to the exceptional features used in the second test (measured at 25%, 50%, 75% and 100%) to get a full profile of its operation.\nSetup, Test-Set, and Evaluation Metrics. PIECE was run as in Expt. 1, with the counterfactual class c\u2032 being selected in the same way, and with all exceptional features being identified using \u03b1 = 0.05. For full details on hyperparameter choices see Section S2 of the supplementary material. A test set of 60 test images were used (i.e., the \u201ccorrect\u201d set from MNIST in Expt. 1), with the plausibility of PIECE being evaluated using the same metrics from Expt. 1 [but we add IM2 here since it has not been tested on semi-factuals (Van Looveren and Klaise 2019)]. The semifactual\u2019s goodness was measured using the L1 pixel distance between the test image and the semi-factual image generated, the larger this distance, the better the semi-factual.\nResults and Discussion. Fig. 4 shows the results of the first comparative tests of semi-factual explanations in XAI. First, PIECE produces the best semi-factuals, with significantly higher L1 distance scores than the min-edit methods (see Fig. 4a; AD > 2.5, p < .029). Second, all methods produce better semi-factuals at every distance measured (see Fig. 4b), but PIECE\u2019s semi-factuals are significantly better at every distance tested (AD > 3.3, p < .015). Third, when different plausibility measures are applied to progressive incremental changes of the exceptional features by PIECE, there are significant changes across some (i.e., MC-Mean, MCSTD, and NN-Dist), but not all measures (i.e., IM1/IM2), perhaps suggesting the former metrics are more sensitive than the latter (see Fig. 4c). Notably, there is a clear tradeoff between plausibility (measured in MC-Dropout measures), and NN-Dist for semi-factuals, showing that as semifactuals get better, they may sacrifice some plausibility."
    },
    {
      "heading": "Conclusion",
      "text": "A novel method, PlausIble Exceptionality-based Contrastive Explanations (PIECE), has been proposed that produces plausible counterfactuals to provide post-hoc explanations for a CNN\u2019s classifications. Competitive tests have shown that PIECE adds significantly to the collection of tools currently proposed to solve this XAI problem. Future work will extend this effort to more complex image datasets. In addition, another obvious direction would be to use recent advances in text and tabular generative models (e.g., see (Xu et al. 2019; Radford et al. 2019)) to extend the framework into these domains, alongside pursuing semi-factual explanations more extensively, as there remains a rich, substantial, untapped research area involving them."
    },
    {
      "heading": "Ethics Statement",
      "text": "As AI systems are increasingly used in human decision making (e.g., health and legal applications), there are significant issues around the fairness and accountability of these algorithms, in addition to whether or not people have reasonable grounds to trust them. One aim of explainable AI research is\nto create techniques and task scenarios that support people in making these fairness, accountability, and trust judgements. The present work is part of this aforementioned research effort. In providing people with counterfactual/semi-factual explanations, there is a risk of revealing \u201ctoo much\u201d about how a system operates (e.g., they potentially convey exactly how a proprietary algorithm works). Notably, the balance of this risk is more on the side of the algorithm-proprietors than on algorithm-users, which may be where we want it to be in the interests of fairness and accountability. Indeed, these methods have the potential to reveal biases in datasets and algorithms as they reveal how data is being used to make predictions (i.e., they could also be used to debug models). The psychological evidence shows that counterfactual and semi-factual explanations elicit spontaneous causal thinking in people; hence, they may have the benefit of reducing the passive use of AI technologies, enabling better human-inthe-loop systems, in which people have appropriate (rather than inappropriate) trust."
    },
    {
      "heading": "Acknowledgements",
      "text": "This paper emanated from research funded by (i) Science Foundation Ireland (SFI) to the Insight Centre for Data Analytics (12/RC/2289 P2), (ii) SFI and DAFM on behalf of the Government of Ireland to the VistaMilk SFI Research Centre (16/RC/3835), and (iii) the SFI Centre for Research Training in Machine Learning (18/CRT/6183)."
    }
  ],
  "title": "On Generating Plausible Counterfactual and Semi-Factual Explanations for Deep Learning",
  "year": 2020
}
