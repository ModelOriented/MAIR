{"abstractText": "Explainable machine learning (ML) enables human learning from ML, human appeal of automated model decisions, regulatory compliance, and security audits of ML models.1,2,3 Explainable ML (i.e. explainable artificial intelligence or XAI) has been implemented in numerous open source and commercial packages and explainable ML is also an important, mandatory, or embedded aspect of commercial predictive modeling in industries like financial services.4,5,6 However, like many technologies, explainable ML can be misused, particularly as a faulty safeguard for harmful blackboxes, e.g. fairwashing or scaffolding, and for other malevolent purposes like stealing models and sensitive training data [1], [38], [40], [42], [45]. To promote best-practice discussions for this already in-flight technology, this short text presents internal definitions and a few examples in Section 2 before covering the proposed guidelines in Subsections 3.1 \u2013 3.4. This text concludes in Section 4 with a seemingly natural argument for the use of interpretable models and explanatory, debugging, and disparate impact testing methods in lifeor mission-critical ML systems.", "authors": [{"affiliations": [], "name": "Patrick Hall"}, {"affiliations": [], "name": "Navdeep Gill"}], "id": "SP:78e943d2ee5a9b29a0b7b354ac43056e01591ca7", "references": [{"authors": ["Ulrich A\u00efvodji", "Hiromi Arai", "Olivier Fortineau", "S\u00e9bastien Gambs", "Satoshi Hara", "Alain Tapp"], "title": "Fairwashing: the Risk of Rationalization", "venue": "arXiv preprint arXiv:1901.09749,", "year": 2019}, {"authors": ["Saleema Amershi", "Max Chickering", "Steven M. Drucker", "Bongshin Lee", "Patrice Simard", "Jina Suh"], "title": "Modeltracker: Redesigning Performance Analysis Tools for Machine Learning", "venue": "In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems,", "year": 2015}, {"authors": ["Marco Ancona", "Enea Ceolini", "Cengiz Oztireli", "Markus Gross"], "title": "Towards Better Understanding of Gradient-based Attribution Methods for Deep Neural Networks", "venue": "In 6th International Conference on Learning Representations (ICLR 2018),", "year": 2018}, {"authors": ["Julia Angwin", "Jeff Larson", "Surya Mattu", "Lauren Kirchner"], "title": "Machine Bias: There\u2019s Software Used Across the Country to Predict Future Criminals", "venue": "And It\u2019s Biased Against Blacks. ProPublica,", "year": 2016}, {"authors": ["Daniel W. Apley"], "title": "Visualizing the Effects of Predictor Variables in Black Box Supervised Learning Models", "venue": "arXiv preprint arXiv:1612.08468,", "year": 2016}, {"authors": ["Solon Barocas", "Moritz Hardt", "Arvind Narayanan"], "title": "Fairness and Machine Learning", "venue": "fairmlbook.org,", "year": 2019}, {"authors": ["Osbert Bastani", "Carolyn Kim", "Hamsa Bastani"], "title": "Interpreting Blackbox Models via Model Extraction", "venue": "arXiv preprint arXiv:1705.08504,", "year": 2017}, {"authors": ["Osbert Bastani", "Yewen Pu", "Armando Solar-Lezama"], "title": "Verifiable Reinforcement Learning Via Policy Extraction", "venue": "In Advances in Neural Information Processing Systems,", "year": 2018}, {"authors": ["Mark W. Craven", "Jude W. Shavlik"], "title": "Extracting Tree-Structured Representations of Trained Networks", "venue": "Advances in Neural Information Processing Systems,", "year": 1996}, {"authors": ["Finale Doshi-Velez", "Been Kim"], "title": "Towards a Rigorous Science of Interpretable Machine Learning", "venue": "arXiv preprint arXiv:1702.08608,", "year": 2017}, {"authors": ["Cynthia Dwork", "Moritz Hardt", "Toniann Pitassi", "Omer Reingold", "Richard Zemel"], "title": "Fairness Through Awareness", "venue": "In Proceedings of the 3rd Innovations in Theoretical Computer Science Conference,", "year": 2012}, {"authors": ["Michael Feldman", "Sorelle A. Friedler", "John Moeller", "Carlos Scheidegger", "Suresh Venkatasubramanian"], "title": "Certifying and Removing Disparate Impact", "venue": "In Proceedings of the 21 ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "year": 2015}, {"authors": ["Anthony W. Flores", "Kristin Bechtel", "Christopher T. Lowenkamp"], "title": "False Positives, False Negatives, and False Analyses: A Rejoinder to Machine Bias: There\u2019s Software Used Across the Country to Predict Future Criminals", "venue": "And It\u2019s Biased against Blacks. Fed. Probation,", "year": 2016}, {"authors": ["Sorelle A. Friedler", "Chitradeep Dutta Roy", "Carlos Scheidegger", "Dylan Slack"], "title": "Assessing the Local Interpretability of Machine Learning Models", "venue": "arXiv preprint arXiv:1902.03501,", "year": 2019}, {"authors": ["Jerome Friedman", "Trevor Hastie", "Robert Tibshirani"], "title": "The Elements of Statistical Learning", "year": 2001}, {"authors": ["Leilani H. Gilpin", "David Bau", "Ben Z. Yuan", "Ayesha Bajwa", "Michael Specter", "Lalana Kagal"], "title": "Explaining Explanations: An Approach to Evaluating Interpretability of Machine Learning", "venue": "arXiv preprint arXiv:1806.00069,", "year": 2018}, {"authors": ["Alex Goldstein", "Adam Kapelner", "Justin Bleich", "Emil Pitkin"], "title": "Peeking Inside the Black Box: Visualizing Statistical Learning with Plots of Individual Conditional Expectation", "venue": "Journal of Computational and Graphical Statistics,", "year": 2015}, {"authors": ["Krishna M. Gopinathan", "Louis S. Biafore", "William M. Ferguson", "Michael A. Lazarus", "Anu K. Pathria", "Allen Jost"], "title": "Fraud Detection using Predictive Modeling, October", "venue": "US Patent 5,819,226", "year": 1998}, {"authors": ["Alicja Gosiewska", "Aleksandra Gacek", "Piotr Lubon", "Przemyslaw Biecek"], "title": "SAFE ML: Surrogate Assisted Feature Extraction for Model Learning", "venue": "arXiv preprint arXiv:1902.11035,", "year": 2019}, {"authors": ["Riccardo Guidotti", "Anna Monreale", "Salvatore Ruggieri", "Franco Turini", "Fosca Giannotti", "Dino Pedreschi"], "title": "A Survey of Methods for Explaining Black Box Models", "venue": "ACM Computing Surveys (CSUR),", "year": 2018}, {"authors": ["Patrick Hall"], "title": "On the Art and Science of Machine Learning Explanations", "venue": "In KDD \u201919 XAI Workshop Proceedings,", "year": 2019}, {"authors": ["Moritz Hardt", "Eric Price", "Nati Srebro"], "title": "Equality of Opportunity in Supervised Learning", "venue": "In Advances in Neural Information Processing Systems,", "year": 2016}, {"authors": ["Linwei Hu", "Jie Chen", "Vijayan N. Nair", "Agus Sudjianto"], "title": "Locally Interpretable Models and Effects Based on Supervised Partitioning (LIME-SUP)", "venue": "arXiv preprint arXiv:1806.00663,", "year": 2018}, {"authors": ["Faisal Kamiran", "Toon Calders"], "title": "Data Preprocessing Techniques for Classification Without Discrimination", "venue": "Knowledge and Information Systems,", "year": 2012}, {"authors": ["Daniel Kang", "Deepti Raghavan", "Peter Bailis", "Matei Zaharia"], "title": "Debugging Machine Learning Models via Model Assertions, 2019", "venue": "URL: https://www-cs.stanford.edu/~matei/papers/2018/mlsys_ model_assertions.pdf", "year": 2018}, {"authors": ["Alon Keinan", "Ben Sandbank", "Claus C. Hilgetag", "Isaac Meilijson", "Eytan Ruppin"], "title": "Fair Attribution of Functional Contribution in Artificial and Biological Networks", "venue": "Neural Computation,", "year": 2004}, {"authors": ["Stan Lipovetsky", "Michael Conklin"], "title": "Analysis of Regression in Game Theory Approach", "venue": "Applied Stochastic Models in Business and Industry,", "year": 2001}, {"authors": ["Zachary C. Lipton"], "title": "The Mythos of Model Interpretability", "venue": "arXiv preprint arXiv:1606.03490,", "year": 2016}, {"authors": ["Yin Lou", "Rich Caruana", "Johannes Gehrke", "Giles Hooker"], "title": "Accurate Intelligible Models with Pairwise Interactions", "venue": "In Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "year": 2013}, {"authors": ["Scott M. Lundberg", "Su-In Lee"], "title": "A Unified Approach to Interpreting Model Predictions", "venue": "Advances in Neural Information Processing Systems", "year": 2017}, {"authors": ["Scott M. Lundberg", "Gabriel G. Erion", "Su-In Lee"], "title": "Consistent Individualized Feature Attribution for Tree Ensembles", "venue": "Proceedings of the 2017 ICML Workshop on Human Interpretability in Machine Learning (WHI", "year": 2017}, {"authors": ["Christoph Molnar"], "title": "Interpretable Machine Learning. christophm.github.io, 2018", "venue": "URL: https:// christophm.github.io/interpretable-ml-book/", "year": 2018}, {"authors": ["Christoph Molnar", "Giuseppe Casalicchio", "Bernd Bischl"], "title": "Quantifying Interpretability of Arbitrary Machine Learning Models Through Functional Decomposition", "venue": "arXiv preprint arXiv:1904.03867,", "year": 2019}, {"authors": ["W. James Murdoch", "Chandan Singh", "Karl Kumbier", "Reza Abbasi-Asl", "Bin Yu"], "title": "Interpretable Machine Learning: Definitions, Methods, and Applications", "venue": "arXiv preprint arXiv:1901.04592,", "year": 2019}, {"authors": ["Anh Nguyen", "Jason Yosinski", "Jeff Clune"], "title": "Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images", "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "year": 2015}, {"authors": ["Marco Tulio Ribeiro", "Sameer Singh", "Carlos Guestrin"], "title": "Why Should I Trust You?: Explaining the Predictions of Any Classifier", "venue": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "year": 2016}, {"authors": ["Cynthia Rudin"], "title": "Please Stop Explaining Black Box Models for High Stakes Decisions", "venue": "arXiv preprint arXiv:1811.10154,", "year": 2018}, {"authors": ["Lloyd S. Shapley", "Alvin E. Roth"], "title": "The Shapley value: Essays in Honor of Lloyd S", "year": 1988}, {"authors": ["Reza Shokri", "Marco Stronati", "Congzheng Song", "Vitaly Shmatikov"], "title": "Membership Inference Attacks Against Machine Learning Models", "venue": "IEEE Symposium on Security and Privacy (SP),", "year": 2017}, {"authors": ["Reza Shokri", "Martin Strobel", "Yair Zick"], "title": "Privacy Risks of Explaining Machine Learning Models", "venue": "arXiv preprint arXiv:1907.00164,", "year": 2019}, {"authors": ["Dylan Slack", "Sophie Hilgard", "Emily Jia", "Sameer Singh", "Himabindu Lakkaraju"], "title": "How Can We Fool LIME and SHAP? Adversarial Attacks on Post-hoc Explanation Methods", "venue": "arXiv preprint arXiv:1911.02508,", "year": 2019}, {"authors": ["Erik Strumbelj", "Igor Kononenko"], "title": "An Efficient Explanation of Individual Classifications using Game Theory", "venue": "Journal of Machine Learning Research,", "year": 2010}, {"authors": ["Christian Szegedy", "Wojciech Zaremba", "Ilya Sutskever", "Joan Bruna", "Dumitru Erhan", "Ian Goodfellow", "Rob Fergus"], "title": "Intriguing Properties of Neural Networks", "venue": "arXiv preprint arXiv:1312.6199,", "year": 2013}, {"authors": ["Florian Tram\u00e8r", "Fan Zhang", "Ari Juels", "Michael K. Reiter", "Thomas Ristenpart"], "title": "Stealing Machine Learning Models via Prediction APIs", "venue": "In 25th {USENIX} Security Symposium ({USENIX} Security", "year": 2016}, {"authors": ["Berk Ustun", "Cynthia Rudin"], "title": "Supersparse Linear Integer Models for Optimized Medical Scoring Systems", "venue": "Machine Learning,", "year": 2016}, {"authors": ["Joel Vaughan", "Agus Sudjianto", "Erind Brahimi", "Jie Chen", "Vijayan N. Nair"], "title": "Explainable Neural Networks", "venue": "Based on Additive Index Models. arXiv preprint arXiv:1806.01933,", "year": 2018}, {"authors": ["Adrian Weller"], "title": "Challenges for Transparency", "venue": "arXiv preprint arXiv:1708.01870,", "year": 2017}, {"authors": ["Mike Williams"], "title": "Interpretability. Fast Forward Labs, 2017", "venue": "URL: https://www.cloudera.com/ products/fast-forward-labs-research.html", "year": 2017}, {"authors": ["Hongyu Yang", "Cynthia Rudin", "Margo Seltzer"], "title": "Scalable Bayesian Rule Lists", "venue": "In Proceedings of the 34th International Conference on Machine Learning (ICML),", "year": 2017}, {"authors": ["Brian Hu Zhang", "Blake Lemoine", "Margaret Mitchell"], "title": "Mitigating Unwanted Biases with Adversarial Learning", "venue": "In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society,", "year": 2018}], "sections": [{"text": "Proposed Guidelines for the Responsible Use of Explainable Machine Learning\nPatrick Hall H2O.ai\nWashington, DC phall@h2o.ai\nNavdeep Gill H2O.ai\nMountain View, CA navdeep@h2o.ai\nNicholas Schmidt BLDS, LLC\nPhiladelphia, PA nschmidt@bldsllc.com"}, {"heading": "1 Introduction", "text": "Explainable machine learning (ML) enables human learning from ML, human appeal of automated model decisions, regulatory compliance, and security audits of ML models.1,2,3 Explainable ML (i.e. explainable artificial intelligence or XAI) has been implemented in numerous open source and commercial packages and explainable ML is also an important, mandatory, or embedded aspect of commercial predictive modeling in industries like financial services.4,5,6 However, like many technologies, explainable ML can be misused, particularly as a faulty safeguard for harmful blackboxes, e.g. fairwashing or scaffolding, and for other malevolent purposes like stealing models and sensitive training data [1], [38], [40], [42], [45]. To promote best-practice discussions for this already in-flight technology, this short text presents internal definitions and a few examples in Section 2 before covering the proposed guidelines in Subsections 3.1 \u2013 3.4. This text concludes in Section 4 with a seemingly natural argument for the use of interpretable models and explanatory, debugging, and disparate impact testing methods in life- or mission-critical ML systems."}, {"heading": "2 Definitions and Examples", "text": "While the explainable ML community has apparently not yet adopted a clear taxonomy of concepts or a precise vocabulary, many authors have grappled with ideas related to interpretability and explanations. Some of these efforts include: \u201cA Survey of Methods for Explaining Black Box Models\u201d by Guidotti et al. [20], \u201cThe Mythos of Model Interpretability\u201d by Lipton [29], Interpretable Machine Learning by Molnar [33], \u201cInterpretable Machine Learning: Definitions, Methods, and Applications\u201d by Murdoch et al. [35], and \u201cChallenges for Transparency\u201d by Weller [48]. To decrease ambiguity herein, this section uses the review and survey corpus and practical examples to address the terms and phrases interpretable, explanation, explainable ML, interpretable models, model debugging techniques, unwanted sociological bias, and fairness techniques before proposing guidelines."}, {"heading": "2.1 Interpretable and Explanation", "text": "Doshi-Velez and Kim [10] define interpretability in ML as, \u201cthe ability to explain or to present in understandable terms to a human.\u201d Professor Sameer Singh of the University of California at\n1This text and associated software are not, and should not be construed as, legal advice or requirements for regulatory compliance. 2In the U.S., interpretable models, explanations, disparate impact testing, and the model documentation they enable may be required under the Civil Rights Acts of 1964 and 1991, the Americans with Disabilities Act, the Genetic Information Nondiscrimination Act, the Health Insurance Portability and Accountability Act, the Equal Credit Opportunity Act (ECOA), the Fair Credit Reporting Act (FCRA), the Fair Housing Act, Federal Reserve SR 11-7, and the European Union (E.U.) Greater Data Privacy Regulation (GDPR) Article 22 [49].\n3For various security applications, see: \u201cProposals for Model Vulnerability and Security\u201d. 4E.g. open source software listed here: \u201cAwesome machine learning interpretability\u201d. 5E.g., Datarobot, H2O Driverless AI, SAS Visual Data Mining and Machine Learning, Zest AutoML. 6E.g., \u201cDeep Insights into Explainability and Interpretability of Machine Learning Algorithms and Applications to Risk Management\u201d.\nRobust AI in Financial Services Workshop at the 33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.\nar X\niv :1\n90 6.\n03 53\n3v 3\n[ st\nat .M\nL ]\n2 9\nN ov\n2 01\n9\nIrvine (UCI), co-inventor of the seminal local interpretable model-agnostic explanation (LIME) technique, defines explanation as a, \u201ccollection of visual and/or interactive artifacts that provide a user with sufficient description of a model\u2019s behavior to accurately perform tasks like evaluation, trusting, predicting, or improving a model.\u201d7 And Gilpin et al. [16] posit that a good explanation occurs when modelers or consumers \u201ccan no longer keep asking why\u201d in regards to some ML model behavior. These three thoughtful characterizations link explainability to interpretability, give clarity on explanation, and provide an abstract goal for any explainability task."}, {"heading": "2.2 Explainable ML and Interpretable Models", "text": "Herein explainable ML means mostly post-hoc analysis and techniques used to understand trained model mechanisms or predictions. Examples of common explainable ML techniques include:\n\u2022 Local and global feature importance, e.g., Shapley and derivative-based feature attribution [3] [26], [31], [39], [43].\n\u2022 Local and global model-agnostic surrogate models, e.g., surrogate decision trees and LIME [7], [8], [9], [23], [37], [47].\n\u2022 Local and global visualizations of model predictions, e.g., accumulated local effect (ALE) plots, 1- and 2-dimensional partial dependence plots, and individual conditional expectation (ICE) plots [5], [15], [17].\nAlthough difficult to quantify, credible research efforts into scientific measures of interpretability are underway [14], [34], and the ability to measure degrees of interpretability implies it is not a binary, on-off quantity. Here, unconstrained, traditional black-box ML models, such as multilayer perceptron (MLP) neural networks and gradient boosting machines (GBMs), are said to be difficult to interpret, potentially unsafe for use in life- or mission-critical applications, but not necessarily completely unexplainable. In this text, interpretable models (i.e., white-box models) will include linear models, decision trees, rule-based models, constrained or Bayesian variants of traditional black-box ML models, or novel types of models designed to be directly interpretable. Examples of newer, highly interpretable ML modeling techniques include explainable neural networks (XNNs), explainable boosting machines (EBMs, GA2Ms), monotonically constrained GBMs, scalable Bayesian rule lists, or super-sparse linear integer models (SLIMs), [30], [46], [47], [50].8,9,10"}, {"heading": "2.3 Model Debugging Techniques", "text": "Herein model debugging techniques test ML models to increase trust in mechanisms and predictions. Debugging techniques include model assertions, security audits, variants of sensitivity (i.e., what-if?) analysis, variants of residual analysis and residual explanation, and unit tests to verify the accuracy or security of ML models [2], [25].11 Model debugging should also include remediating any discovered errors or vulnerabilities."}, {"heading": "2.4 Unwanted Sociological Bias and Fairness Techniques", "text": "In this text, unwanted sociological bias encompasses several forms of discrimination that may manifest in ML, including overt discrimination, disparate treatment, and disparate impact (DI), i.e., unintentional discrimination. DI may be caused by model misspecification, inaccurate or incomplete data, or data that has differing correlations or dependencies among demographic groups of individuals, driving differences in favorable model outcomes. A model is said to be biased here if, (1) group membership is not independent of the likelihood of a favorable outcome, or (2) under certain circumstances, membership in a subset of a group is not independent of the likelihood of a favorable outcome (i.e., local bias). Underlying discrimination that causes bias may or may not be illegal, depending on how it arises and applicable discrimination laws. Herein fairness techniques are used to diagnose and remediate unwanted sociological bias in ML models. Diagnosis approaches include\n7From: \u201cProposed Guidelines for the Responsible Use of Explainable Machine Learning\u201d (presentation only). 8EBM, as implemented in the Microsoft interpret package. 9Monotonic GBM, as implemented in XGBoost or h2o.\n10And similar methods, e.g.: https://users.cs.duke.edu/~cynthia/papers.html. 11And similar methods, e.g.: https://debug-ml-iclr2019.github.io/.\nDI testing and other tests for bias [12]. Remediation methods tend to involve model selection by minimization of bias, preprocessing training data, e.g., reweighing [24], training unbiased models, e.g., adversarial de-biasing [51], or post-processing model predictions, e.g., by equalized odds [22].12"}, {"heading": "3 Proposed Guidelines for the Responsible Use of Explainable ML", "text": "Four guidelines are proposed and discussed in Subsections 3.1 \u2013 3.4 to assist practitioners in avoiding unintentional misuse or identifying intentional abuse of explainable ML. The guidelines are:\n1. Use explanations to enable understanding. 2. Learn how explainable ML is used for nefarious purposes. 3. Augment surrogate models with direct explanations. 4. Use highly interpretable mechanisms for life- or mission-critical ML.\nImportant corollaries to the guidelines are also highlighted and simple, reproducible software examples accompany the guidelines to avoid hypothetical reasoning whenever possible."}, {"heading": "3.1 Guideline: Use Explanations to Enable Understanding", "text": "(a) Consistent global Shapley feature importance values for gGBM. (b) gGBM deviance residuals and predictions by PAY_0.\nFigure 1: An unconstrained GBM probability of default model, gGBM, generally over-emphasizes the importance of the input feature PAY_0, a customer\u2019s most recent repayment status, in the UCI credit card data. gGBM often produces large positive residuals when PAY_0 indicates on-time payments (PAY_0 \u2264 1) and large negative residuals when PAY_0 indicates late payments (PAY_0 > 1). Combining explanatory and debugging techniques shows that gGBM is explainable, but probably not trustworthy.\nExplanations are often discussed in the context of trust (e.g., Ribeiro et al. [37]), but explanations alone are not sufficient for trust in ML models. Explanation, as a general concept, is related more directly to understanding and transparency than to trust.13 Simply put, one can understand and explain a model without trusting it. One can also trust a model and not be able to understand or explain it. Consider the following example scenarios.\n\u2022 Explanation and understanding without trust: In Figure 1, global Shapley explanations and residual analysis identify a pathology in an unconstrained GBM model, gGBM, trained on the UCI credit card dataset [27].14 gGBM over emphasizes the input feature PAY_0, or a\n12And similar methods, e.g.: http://www.fatml.org/resources/relevant-scholarship. 13The Merriam-Webster definition of explain, accessed Sept. 8th 2019, does not mention trust. 14Code to replicate Figure 1: https://bit.ly/2m58Lxl.\ncustomer\u2019s most recent repayment status. Due to over-emphasis of PAY_0, gGBM is often unable to predict on-time payment if recent payments are delayed (PAY_0 > 1), causing large negative residuals. gGBM is also often unable to predict default if recent payments are made on-time (PAY_0 \u2264 1), causing large positive residuals. In this example scenario, gGBM is explainable, but likely untrustworthy.\n\u2022 Trust without explanation and understanding: Years before reliable explanation techniques were widely acknowledged and available, black-box predictive models, such as autoencoder and MLP neural networks, were used for fraud detection in the financial services industry [18]. When these models performed well, they were trusted.15,16 However, they were not explainable or well-understood by contemporary standards.\nExplanations typically increase trust in models as a side-effect when they are acceptable to human users by various criteria. As illustrated in Figure 4, in an ideal scenario, explanation techniques should be used to directly increase understanding in ML models, while debugging and DI testing methods should be used to directly promote trust."}, {"heading": "3.2 Guideline: Learn How Explainable ML is Used for Nefarious Purposes", "text": "When used disingenuously, explainable ML methods can provide cover for misused or intentionally abusive black-boxes [1], [38], [42]. Explainable ML methods can also enable hacking or stealing of models or data through public prediction APIs or other endpoints [40], [45]. Moreover, explainable ML methods are likely to be used for other nefarious purposes in the future and may be used for unknown destructive purposes now. Responsible practitioners need to understand the malevolent side of this technology to better detect and correct misuse and abuse."}, {"heading": "3.2.1 Corollary: Use Explainable ML for Security Audits", "text": "Use explainable ML techniques to test ML systems for vulnerabilities to model stealing, inversion, and membership inference attacks."}, {"heading": "3.2.2 Corollary: Explainable ML Can be Used to Crack Nefarious Black-boxes", "text": "Used as white-hat hacking tools, explainable ML can help draw attention to accuracy or unwanted sociological bias problems in proprietary black-boxes. See Angwin et al. [4] for evidence that cracking proprietary black-box models for oversight purposes is possible.17"}, {"heading": "3.2.3 Corollary: Explainable ML is a Privacy Vulnerability", "text": "Recent research shows that providing explanations along with predictions eases attacks that can compromise sensitive training data [41]."}, {"heading": "3.3 Guideline: Augment Surrogate Models with Direct Explanations", "text": "Models of models, or surrogate models, can be helpful explanatory tools, but they are usually approximate, low-fidelity explainers. Aside from (1) a global or local summary of a complex model provided by a surrogate model can be helpful sometimes and (2) much work in explainable ML has been directed toward improving the fidelity and usefulness of surrogate models [7], [8], [9], [23], [47], many explainable ML techniques have nothing to do with surrogate models. One of the most exciting breakthroughs for supervised learning problems in explainable ML is the application of a coalitional game theory concept, Shapley values, to compute feature attributions which are consistent globally and accurate locally using the trained model itself [31], [43]. An extension of this idea, called Tree SHAP, has already been implemented for popular tree ensemble methods [32].\nThere are many other explainable ML methods that operate on trained models directly such as partial dependence, ALE, and ICE plots [5], [15], [17]. Surrogate models and explanatory techniques that\n15E.g., \u201cReduce Losses from Fraudulent Transactions\u201d. 16E.g., \u201cSAS Secures Technology Patent for Better Fraud Detection Performance\u201d. 17This text makes no claim on the quality of the analysis in Angwin et al. (2016), which has been criticized [13]. This now infamous analysis is presented only as evidence that motivated activists can crack proprietary black-boxes using surrogate models and other explanatory techniques. Moreover, such analyses would likely improve with established best-practices for explainable ML.\n(a) Na\u00efve htree, a surrogate model, forms an approximate overall flowchart for the explained model, gGBM.\n(b) Partial dependence and ICE curves generated directly from the explained model, gGBM.\nFigure 2: htree displays known interactions in f = Xnum1 \u2217 Xnum4 + \u2223Xnum8\u2223 \u2217 X2num9 for \u223c \u22121 < Xnum9 <\u223c 1. Modeling of the known interactions in f by gGBM is also highlighted by the divergence of partial dependence and ICE curves for \u223c \u22121 <Xnum9 <\u223c 1. Explanations from a surrogate model have augmented and confirmed findings from a direct model visualization technique.\noperate directly on trained models can also be combined, for instance by using partial dependence, ICE, and surrogate decision trees to investigate and confirm modeled interactions [21]. In Figure 2, an unconstrained GBM, gGBM, models a known signal generating function f :\nf(X) = { 1 if Xnum1 \u2217Xnum4 + \u2223Xnum8\u2223 \u2217X 2 num9 + e \u2265 0.42\n0 if Xnum1 \u2217Xnum4 + \u2223Xnum8\u2223 \u2217X2num9 + e < 0.42 (1)\nwhere e signifies the injection of random noise in the form of label switching for roughly 15% of the training and validation observations.18 gGBM is then trained such that gGBM(X) \u2248 f(X) in training and validation data. htree, displayed in Figure 2a, is extracted such that htree(X) \u2248 gGBM(X) \u2248 f(X) in validation data. Partial dependence and ICE plots are generated directly for gGBM in the same validation data and overlaid in Figure 2b. The parent-child node relationships displayed in htree for \u223c \u22121 < Xnum9 <\u223c 1 in 2a and the divergence of ICE and partial dependence curves in 2b for \u223c \u22121 <Xnum9 <\u223c 1 help confirm and explain how gGBM learned the interactions in f . As in Figure 1, combining different approaches provided additional, beneficial information about a ML model."}, {"heading": "3.3.1 Corollary: Augment LIME with Direct Explanations", "text": "LIME is important, imperfect (like every other ML technique), and vulnerable to adversarial manipulation [42]. LIME, in its most popular implementation, uses local linear surrogate models fit to perturbed, locally weighted samples to explain regions of machine-learned decision boundaries or response functions [37]. Like other surrogate models, LIME can be combined with model-specific methods for validation and to yield deeper insights. Consider that Tree SHAP can provide locally accurate and consistent point estimates for local feature importance as in 3b below. LIME can then provide approximate information about modeled local linear trends around the same point. Table 1 contains LIME hGLM coefficients for a local region of a validation set sampled from the UCI credit card data defined by PAY_0 > 1, or customers with a fairly high risk of default due to late most recent payments.19 hGLM models the predictions of a simple interpretable decision tree model, gtree, displayed in 3a. hGLM coefficients show linear trends between features in the sampled set XPAY_0>1 and gtree(XPAY_0>1). Because hGLM is relatively well-fit (0.73 R2) and has a logical intercept (0.77), it can be used along with Shapley values to reason about the modeled average behavior for risky customers, to differentiate the behavior of any one specific risky customer from their peers under the model, or to validate LIME results.\n18Code to replicate Figure 2: https://bit.ly/2kSuAQD. 19Code to replicate Table 1: https://bit.ly/2miCPpo."}, {"heading": "3.4 Guideline: Use Highly Interpretable Mechanisms for Mission- or Life-Critical ML", "text": "Given the known difficulties with explaining black-boxes [38], the existence of unwanted social bias in data and ML models [6], the security vulnerabilities of ML (e.g., Shokri et al. [40], Tram\u00e8r et al. [45]), and the potentially surprising behavior of black-boxes (e.g., Nguyen et al. [36], Szegedy et al. [44]), it appears prudent today to use highly transparent ML mechanisms for applications that make life-altering or high-value decisions. Interpretability, as enabled by interpretable models and post-hoc explanations (see Corollary 3.4.1), may be mandated by regulation for some life- or mission-critical applications, but interpretability is also recommended for any ML application in which inevitable wrong decisions should be appealable. This subsection discusses a few details and examples regarding regulated ML applications and appeal, and also advocates for trust-enhancing DI testing (see Corollaries 3.4.2 \u2013 3.4.4) in high-stakes or human-centered applications.\nInterpretable ML mechanisms are required under numerous regulatory statutes in the U.S., and explainable ML tools like LIME and other surrogate models, partial dependence plots, and global and local feature importance are already used to document, understand, and validate some predictive models in the financial services industry [23], [47].2, 6 Moreover, adverse action notices are mandated under the Equal Credit Opportunity Act (ECOA) and the Fair Credit Reporting Act (FCRA) for many credit lending, employment, and insurance decisions in the U.S.20 If ML is used for such decisions, it must be explained in terms of adverse action notices.21 Shapley values, and other local feature importance approaches, provide a convenient methodology to rank the direct contribution of input features to final model decisions and potentially generate customer-specific adverse action notices.\nAside from regulatory mandates, interpretable models and explanations enable logical appeal processes for automated decisions made by ML models. Consider being negatively impacted by an erroneous black-box model decision, say for instance being mistakenly denied a loan or parole. How would you argue your case for appeal without knowing how model decisions were made? According to the New York Times, a man named Glenn Rodr\u00edguez found himself in this unfortunate position in a penitentiary in Upstate New York in 2016.22"}, {"heading": "3.4.1 Corollary: Use Interpretable Models Along with Explanation Techniques", "text": "Some well-known publications have focused either on interpretable models (e.g., Ustun and Rudin [46], Yang et al. [50]) or on post-hoc explanations (e.g., Lundberg and Lee [31], Ribeiro et al. [37]), but the two can be used together in the context of a holistic ML workflow, illustrated in Figure 4. Consider the seemingly useful example case of augmenting globally interpretable models with local post-hoc explanations. A practitioner could train a single decision tree, a globally interpretable model, then apply local explanations in the form of Shapley feature importance as in Figure 3.23 This enables practitioners to see accurate numeric feature contributions for each prediction and the entire directed graph of the decision tree. Even for interpretable models, such as linear models and decision\n20See: \u201cAdverse Action Notice Requirements Under the ECOA and the FCRA\u201d. 21This is apparently already happening: \u201cNew Patent-Pending Technology from Equifax Enables Configurable AI Models\u201d. 22See: \u201cWhen a Computer Program Keeps You in Jail\u201d. 23Code to replicate Figure 3: https://bit.ly/2miCPpo.\n(a) Simple decision tree, gtree, trained on the UCI credit card data to predict default with validation AUC of 0.74. The decision policy for a high-risk individual is highlighted in red.\n(b) Locally-accurate Shapley contributions for the highlighted individual\u2019s probability of default.\nFigure 3: The decision-policy for a high-risk customer is highlighted in 3a and the locally-accurate Shapley contributions for this same individual\u2019s predicted probability are displayed in 3b. Due to their consistency properties, Shapley values highlight the local importance of features not on the decision path in this particular encoding, i.e., gtree, of the unknown signal-generating function, which is likely helpful for the generation of consistent adverse action notices across similar data and models.\ntrees, Shapley values present accuracy and consistency advantages over standard feature attribution methods [28], [31], [32]. Shapley values also enable the consistent ranking of input features for each model decision, which is likely helpful for FCRA and ECOA compliance. Another twist on the idea of combining explainable ML methods and interpretable models is described by Gosiewska et al. [19] in \u201cSurrogate Assisted Feature Extraction for Machine Learning (SAFE ML)\u201d. In the SAFE ML approach, features learned by more complex models are extracted and used in an explainable fashion to increase the accuracy of more interpretable models. Aren\u2019t either of these augmented processes more desirable than either an interpretable model or post-hoc explanations by themselves?"}, {"heading": "3.4.2 Corollary: Use Explanations Along with DI Testing", "text": "Like interpretable models, fairness methods are often presented in different articles than post-hoc explanatory methods. However, in banks for example, using post-hoc explanatory tools along with DI testing is often necessary to comply with model documentation guidance and with fair lending regulations.24,25 To clarify, explanatory techniques should not replace DI testing for bias detection purposes, but in general, explanations increase transparency and understanding of model mechanisms and predictions, while DI auditing and remediation increases trust that model predictions are as fair as possible. As in previous sections, trust and understanding are different but complimentary goals achieved by combining multiple approaches.\nTable 2 displays basic group disparity metrics for a monotonic GBM model, gmono, trained on the UCI credit card data.26 In this example, gmono displays group parity for adverse impact with male as the reference level according to the four-fifths rule, but also presents unwanted false omissions rate bias against females, indicating that males may be receiving too much credit they cannot repay, potentially preventing females from receiving that credit.27 This disparity can be remedied by gently increasing the decision cutoff for gmono, and Shapley values can also explain each gmono prediction.26\n24See: \u201cInteragency Fair Lending Examination Procedures\u201d. 25See: \u201cCFPB Consumer Laws and Regulations: ECOA\u201d. 26Code to replicate Table 2: https://bit.ly/2lZUlyN. 27The four-fifths rule was delineated by the Equal Employment Opportunity Commission (EEOC) as a measure of DI that would be of concern to regulators. This threshold has been associated with a specific measure of DI, the adverse impact ratio (AIR). While it may be applied to other measures of fairness, as in Table 2, this is often irrelevant in real-world compliance and litigation settings in the U.S.\nBeyond explaining predictions, Explainable ML can assist in the difficult problem of determining input features within wide training sets that drive DI. For example, weighted average Shapley values can be analyzed by demographic segment, highlighting the features that have the largest deleterious impact on the protected segment. Explainable ML techniques, especially when paired with clustering, can also be useful for isolating instances of local bias."}, {"heading": "3.4.3 Corollary: Explanation is Not a Frontline Fairness Tool", "text": "Demographic attributes cannot currently be used in predictive models for high-stakes and commercially viable uses of explainable ML in credit lending, insurance, and employment in the U.S. that fall under FCRA, ECOA, or other applicable regulations. Thus their contribution to models cannot be assessed using accurate, direct explainable ML techniques. Even when demographic attributes can be used in models, it has been shown that explanations may not detect unwanted sociological bias [1]. Given these drawbacks, it is recommended that fairness techniques are used to test for and remediate bias, and explanations are used to understand bias when appropriate (see Corollary 3.4.2)."}, {"heading": "3.4.4 Corollary: Use DI Testing Along with Constrained Models", "text": "Unconstrained ML models can treat similar individuals differently due to small differences in input data values, causing local bias that is not detectable with standard DI testing methods that measure group fairness [11]. To mitigate local bias when using ML, and to ensure standard bias or DI testing methods are most effective, pair such testing with constrained models."}, {"heading": "4 Conclusion: a Holistic Approach for Life- or Mission-Critical ML", "text": "ML systems are used today to make life-altering decisions about employment, bail, parole, and lending,28 and the scope of decisions delegated to ML systems seems likely to expand in the future. Many researchers and practitioners are tackling DI, inaccuracy, privacy violations, and security vulnerabilities with a number of brilliant, but sometimes siloed, approaches. By proposing some straightforward explainable ML guidelines, this short text also gives examples of combining innovations from several sub-disciplines of ML research to train understandable and trustworthy predictive modeling systems. As illustrated in Figure 4, these innovations can be used together, and this combination may be better-suited than conventional ML methods for use in business- and life-critical applications.\n28See: \u201cDebugging Machine Learning Models\u201d."}, {"heading": "Acknowledgements", "text": "The authors thank Przemyslaw Biecek, Pramit Choudhary, Benjamin Cox, Lingyao Meng, Christoph Molnar, Sameer Singh, and Bryce Stephens for their helpful input and insights."}], "title": "Proposed Guidelines for the Responsible Use of Explainable Machine Learning", "year": 2019}