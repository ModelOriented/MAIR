{"abstractText": "Unbiased data collection is essential to guaranteeing fairness in artificial intelligence models. Implicit bias, a form of behavioral conditioning that leads us to attribute predetermined characteristics to members of certain groups and informs the data collection process. This paper quantifies implicit bias in viewer ratings of TEDTalks, a diverse social platform assessing social and professional performance, in order to present the correlations of different kinds of bias across sensitive attributes. Although the viewer ratings of these videos should purely reflect the speaker\u2019s competence and skill, our analysis of the ratings demonstrates the presence of overwhelming and predominant implicit bias with respect to race and gender. In our paper, we present strategies to detect and mitigate bias that are critical to removing unfairness in AI.", "authors": [{"affiliations": [], "name": "Rupam Acharyya"}, {"affiliations": [], "name": "Shouman"}, {"affiliations": [], "name": "Das"}, {"affiliations": [], "name": "Ankani Chattoraj"}, {"affiliations": [], "name": "Oishani Sengupta"}, {"affiliations": [], "name": "Md"}, {"affiliations": [], "name": "Iftekar Tanveer"}], "id": "SP:e868a184530f96c81d0a64bd59f96a1c479cc6d7", "references": [{"authors": ["Dimitrios Alikaniotis", "Helen Yannakoudakis", "Marek Rei"], "title": "Automatic Text Scoring Using Neural Networks", "venue": "In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),", "year": 2016}, {"authors": ["Rachel K.E. Bellamy", "Kuntal Dey", "Michael Hind", "Samuel C. Hoffman", "Stephanie Houde", "Kalapriya Kannan", "Pranay Lohia", "Jacquelyn Martino", "Sameep Mehta", "Aleksandra Mojsilovic", "Seema Nagar", "Karthikeyan Natesan Ramamurthy", "John Richards", "Diptikalyan Saha", "Prasanna Sattigeri", "Moninder Singh", "Kush R. Varshney", "Yunfeng Zhang"], "title": "AI Fairness 360: An Extensible Toolkit for Detecting, Understanding, and Mitigating Unwanted Algorithmic Bias", "venue": "https:", "year": 2018}, {"authors": ["Dan Biddle"], "title": "Adverse impact and test validation: A practitioner\u2019s guide to valid and defensible employment testing", "venue": "Gower Publishing,", "year": 2006}, {"authors": ["Toon Calders", "Sicco Verwer"], "title": "Three naive Bayes approaches for discrimination-free classification", "venue": "Data Mining and Knowledge Discovery 21,", "year": 2010}, {"authors": ["Flavio Calmon", "Dennis Wei", "Bhanukiran Vinzamuri", "Karthikeyan Natesan Ramamurthy", "Kush R Varshney"], "title": "Optimized pre-processing for discrimination prevention", "venue": "In Advances in Neural Information Processing Systems", "year": 2017}, {"authors": ["Lei Chen", "Ru Zhao", "Chee Wee Leong", "Blair Lehman", "Gary Feng", "Mohammed Ehsan Hoque"], "title": "Automated video interview judgment on a large-sized corpus collected online", "venue": "Seventh International Conference on Affective Computing and Intelligent Interaction (ACII)", "year": 2017}, {"authors": ["Lei Chen", "Ru Zhao", "Chee Wee Leong", "Blair Lehman", "Gary Feng", "Mohammed Ehsan Hoque"], "title": "Automated video interview judgment on a large-sized corpus collected online", "venue": "ACII", "year": 2017}, {"authors": ["Brian d\u2019Alessandro", "Cathy O\u2019Neil", "Tom LaGatta"], "title": "Conscientious classification: A data scientist\u2019s guide to discrimination-aware classification", "venue": "Big data 5,", "year": 2017}, {"authors": ["Cynthia Dwork", "Moritz Hardt", "Toniann Pitassi", "Omer Reingold", "Richard Zemel"], "title": "Fairness through awareness", "venue": "In Proceedings of the 3rd innovations in theoretical computer science conference", "year": 2012}, {"authors": ["Michael Feldman", "Sorelle A Friedler", "John Moeller", "Carlos Scheidegger", "Suresh Venkatasubramanian"], "title": "Certifying and removing disparate impact", "venue": "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining", "year": 2015}, {"authors": ["Nina Grgic-Hlaca", "Muhammad Bilal Zafar", "Krishna P Gummadi", "Adrian Weller"], "title": "The case for process fairness in learning: Feature selection for fair decision making", "venue": "In NIPS Symposium on Machine Learning and the Law,", "year": 2016}, {"authors": ["Kenneth Holstein", "Jennifer Wortman Vaughan", "Hal Daum\u00e9 III", "Miro Dudik", "Hanna Wallach"], "title": "Improving fairness in machine learning systems: What do industry practitioners need", "venue": "In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems", "year": 2019}, {"authors": ["Faisal Kamiran", "Toon Calders"], "title": "Data preprocessing techniques for classification without discrimination", "venue": "Knowledge and Information Systems", "year": 2012}, {"authors": ["Toshihiro Kamishima", "Shotaro Akaho", "Hideki Asoh", "Jun Sakuma"], "title": "Fairness-aware classifier with prejudice remover regularizer", "venue": "In Joint European Conference on Machine Learning and Knowledge Discovery in Databases", "year": 2012}, {"authors": ["Toshihiro Kamishima", "Shotaro Akaho", "Jun Sakuma"], "title": "Fairness-aware learning through regularization approach", "venue": "In 2011 IEEE 11th International Conference on Data Mining Workshops", "year": 2011}, {"authors": ["Klaus Krippendorff"], "title": "Content analysis: An introduction to its methodology", "venue": "Sage publications,", "year": 2018}, {"authors": ["Quoc Le", "Tomas Mikolov"], "title": "Distributed representations of sentences and documents", "venue": "In International conference on machine learning. ACM, Beijing,", "year": 2014}, {"authors": ["Iftekhar Naim", "Md Iftekhar Tanveer", "Daniel Gildea", "Ehsan Hoque"], "title": "Automated analysis and prediction of job interview performance", "venue": "IEEE Transactions on Affective Computing", "year": 2016}, {"authors": ["Laurent Son Nguyen", "Daniel Gatica-Perez"], "title": "Hirability in the wild: Analysis of online conversational video resumes", "venue": "IEEE Transactions on Multimedia 18,", "year": 2016}, {"authors": ["Dino Pedreschi", "Salvatore Ruggieri", "Franco Turini"], "title": "Measuring discrimination in sociallysensitive decision records", "venue": "In Proceedings of the 2009 SIAM International Conference on Data Mining", "year": 2009}, {"authors": ["Dino Pedreshi", "Salvatore Ruggieri", "Franco Turini"], "title": "Discrimination-aware data mining", "venue": "In Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining", "year": 2008}, {"authors": ["Geoff Pleiss", "Manish Raghavan", "Felix Wu", "Jon Kleinberg", "Kilian Q Weinberger"], "title": "On fairness and calibration", "venue": "In Advances in Neural Information Processing Systems", "year": 2017}, {"authors": ["Chris Russell", "Matt J Kusner", "Joshua Loftus", "Ricardo Silva"], "title": "When worlds collide: integrating different counterfactual assumptions in fairness", "venue": "In Advances in Neural Information Processing Systems", "year": 2017}, {"authors": ["Kaveh Taghipour", "Hwee Tou Ng"], "title": "A neural approach to automated essay scoring", "venue": "In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP)", "year": 2016}, {"authors": ["M Iftekhar Tanveer", "Ji Liu", "M Ehsan Hoque"], "title": "Unsupervised Extraction of Human-Interpretable Nonverbal Behavioral Cues in a Public Speaking Scenario", "venue": "In 23rd Annual ACM Conference on Multimedia. ACM,", "year": 2015}, {"authors": ["M. Iftekhar Tanveer", "Samiha Samrose", "Raiyan Abdul Baten", "Mohammed Ehsan Hoque"], "title": "Awe the Audience: How the Narrative Trajectories Affect Audience Perception in Public Speaking", "venue": "In Proceedings of the Conference on Human Factors in Computing Systems (CHI). ACM,", "year": 2018}, {"authors": ["LLC. TED Conferences"], "title": "2019", "venue": "About Tedtalk organization. ", "year": 2019}, {"authors": ["Rich Zemel", "Yu Wu", "Kevin Swersky", "Toni Pitassi", "Cynthia Dwork"], "title": "Learning fair representations", "venue": "In International Conference on Machine Learning", "year": 2013}, {"authors": ["Brian Hu Zhang", "Blake Lemoine", "Margaret Mitchell"], "title": "Mitigating unwanted biases with adversarial learning", "venue": "In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society", "year": 2018}, {"authors": ["Indre Zliobaite"], "title": "A survey on measuring indirect discrimination in machine learning", "venue": "arXiv preprint arXiv:1511.00148", "year": 2015}], "sections": [{"heading": "1 Introduction", "text": "Machine-learning techniques are being used to evaluate human skills in areas of social performance, such as automatically grading essays [1, 28], outcomes of video based job interviews [6, 20], hirability [21], presentation performance [29, 7, 30] etc. These algorithms automatically quantify the relative skills and performances by assessing large quantities of human annotated data. Companies and organizations worldwide are increasingly using commercial products that utilize machine learning techniques to assess these areas of social interaction. However, the presence of implicit bias in society reflected in the annotators and a combination of several other unknown factors (e.g. demographics of the subjects in the datasets, demographics of the annotators) creates systematic imbalances in human datasets. Machine learning algorithms (neural networks in most cases) trained on such biased datasets automatically replicate the imbalance [23] naturally present in the data and result in producing unfair predictions.\nExamining the impact of implicit bias in social behavior requires extensive, diverse human data that is spontaneously generated and reveals the perception of success in social performance. In this paper, we analyze ratings of TED Talk videos to quantify the amount of social bias in viewer opinions. TED Talks present a platform where speakers are given a short time to present inspiring and socially transformative ideas in an innovating and engaging way. In its mission statement, the TED organization describes itself as a \u201cglobal community, welcoming people from every discipline and culture\u201d and makes an explicit commitment to \u201cchange attitudes, lives, and ultimately the world\u201d [31].\nSince TED Talks offer a platform to speakers from diverse backgrounds trying to convince people of their professional skills and achievements, the platform lends itself to a discussion of several critical issues regarding fairness and implicit bias: How can we determine the fairness of viewer ratings of TED Talk videos? Can we detect implicit bias in the ratings dataset? Are ratings influenced by the race and gender of the speaker? Ideally, these ratings should depend on the perception of the speaker\u2019s success and communicative performance; not on the speaker\u2019s gender or ethnicity. For instance, our findings show that while a larger proportion of viewers rate white speakers in a confidently positive manner, speaker of other gender identities and ethnic backgrounds receive a greater number of mixed ratings and elicit wider differences of opinion. In addition, men and women are rated as positive or negative with more consistency,\nar X\niv :2\n00 3.\n00 68\n3v 1\n[ cs\n.A I]\n2 M\nar 2\n02 0\nwhile speakers identifying with other gender identities are rated less consistently in either direction. With this assumption, we conducted computational analysis to detect and mitigate bias present in our data. We utilize a state of the art metric \u201cDisparate Impact\u201d as in [10] for measuring fairness, and three popular methods of bias correction\u2014\n1. pre-processing [5, 14],\n2. in-processing [4, 16], and\n3. post-processing [12]\n. We compared such predictions of the ratings with the actual ratings provided by the viewers of the TED talks and found that our model prediction performs better w.r.t. a standard fairness metric.\nOur experiments show that if the traditional machine learning models are trained on a dataset without any consideration of the data bias, the model will make decision in an unwanted way which could be highly unfair to an unprivileged group of the society. In short, major contributions of the paper are as follows,\n1. We show that public speaking ratings can be biased depending on the race and gender of a speaker. We utilize the state-of-the-art fairness measuring metric to identify the biases present in the TED talk public speaking rating.\n2. We propose a systematic procedure to detect unfairess in the TEDTalk public speaking ratings. This method can be adopted by any machine learning practitioner or data scientist at industry who are concerned with fairness in their datasets or machine learning models."}, {"heading": "2 Related Works", "text": "With the increased availability of huge amount of data, data-driven decision making has emerged as a fundamental practice to all sorts of industries. In recent years, data scientists and the machine learning community put conscious effort to detect and mitigate bias from data sets and respective models. Over the years, researchers have used multiple notions of fairness as the tools to get rid of bias in data that are outlined below:\n\u2022 \u2018individual fairness\u2019, which means that similar individuals should be treated similarly [9]\n\u2022 \u2018group fairness\u2019, which means that underprivileged groups should be treated same as privileged groups [24, 25].\n\u2022 \u2018fairness through awareness\u2019, which assumes that an algorithm is fair as long as its outcome or prediction is not dependent on the use of protected or sensitive attributes in decision making [11].\n\u2022 \u2018equality of opportunity\u2019, mainly used in classification task which assumes that the probability of making a decision should be equal for groups with same attributes [12].\n\u2022 \u2018counterfactual fairness, very close to equality of opportunity but the probability is calculated from the sample of counterfactuals [27, 18] which ensures that the predictor probability of a particular label should be same even if the protected attributes change to different values .\nThe fairness measures mentioned above can be characterized as both, manipulation to data and implementation of a supervised classification algorithm. One can employ strategies of detecting unfairness in a machine learning algorithm, observe [34] and removing them by,\n\u2022 Pre-processing: this strategy involves processing the data to detect any bias and mitigating unfairness before training any classifiers [5, 14].\n\u2022 In-processing: this technique adds a regularizer term in the loss function of the classifier which gives a measurement of the unfairness of the classifier [4, 16].\n\u2022 Post-processing: this strategy manipulates predictor output which makes the classifier fair under the measurement of a specific metric [12].\nFor our analysis, we follow this well established paradigm and use an open-source toolkit AIF360 [2] to detect and mitigate bias present in the data set and classifiers at all three stages: the pre-processing, the in-processing and the post-processing step."}, {"heading": "3 Data Collection", "text": "We analysed the TedTalk data collected from the ted.com website. We crawled the website and gathered information about TedTalk videos which have been published on the website for over a decade (2006-2017). These videos cover a wide range of topics, from contemporary political, social issues to modern technological advances. The speakers who delivered talks at the TedTalk platform are also from a diverse background; including but not limited to, scientists, education innovators, celebrities, environmentalists, philosophers, filmmakers etc. These videos are published on the ted.com website and are watched by millions of people around the world who can give ratings to the speakers. The rating of each talk is a collection of fourteen labels such as beautiful, courageous, fascinating etc. In this study we try to find if there is any implicit bias in the rating of the talks with respect to the race and gender of the speaker. Some properties of the full dataset is given in table 1. Each viewer can assign three out of fourteen labels to a talk and we use the total count for each label of rating for our analysis. In figure 1, average number of ratings in each of the fourteen categories is shown as a bar plot. Our preliminary observation reveals some disparities among the rating labels e.g. the label \u2018inspiring\u2019 has significantly higher count than other labels."}, {"heading": "3.1 Data Annotation and Normalization", "text": "Our raw data includes information about the topic of the talk, number of views, date of publications, the transcripts used by the speakers, count of rating labels given by the viewers etc. However, this raw data does not come with the protected features such as gender and race of the speakers. We identified gender and race as potential sensitive attributes from our preliminary analysis. Thereby, to annotate the data for these protected features, we used amazon mechanical turk. We assigned three turkers for each talk, and annotated the race and gender of the speaker. To estimate the reliability of the annotation, we performed the Krippendorff\u2019s alpha reliability test [17] on the annotations. We found a value of 93% for the Krippendorff\u2019s alpha on our annotations. If there is a disagreement among the annotations of different turkers, we take the majority vote whenever possible or, we manually investigate to do the annotations. For gender, we used three categories: male, female, others, and for race, we used four categories, White, African American or Black, Asian, others. In our analysis, we used the total number of views (V ), the transcripts of the talks (T ) and the rating (Y ) given by the viewers. For preprocessing we employ the following steps,\nGender Count Race Count Female 768 White 1901 Male 1596 Asian 210 Other 19 Black 169\nOther 103"}, {"heading": "4 Observation in Data", "text": "There is also a clear and visible difference in the distribution of the ratings for different groups corresponding to the protected attributes: race and gender. Figure 2 and figure 3 represents smoothed histogram across different groups for several ratings where we observed substantial difference in the distribution. Interestingly we find one expected bias and one counter intuitive bias when considering ratings based on race. The expected bias is that talks of white speakers are rated to be beautiful and courageous with high confidence as compared to speakers of other races, the sharp blue distributions in top row of Figure 2 are indicative of that. The wider green and red distributions on the top row of Figure 2 confirm that the society is more confused and less confident while rating the talk of a non-white speaker as beautiful or courageous. On the other hand, counter intuitively, we find that viewers rate talks of speakers of other race as more fascinating than other races (sharpness of red distribution in 2 in bottom row, left panel). Though counter intuitive, such bias is also not acceptable and clearly not fair. This highlights the diversity and difficulty of the issues of fairness and bias, since bias may not always stand out to be against the\n\u201dexpected\u201d unprivileged class. Our work hence highlights the need to be careful when accounting for fairness as in fair society all types of biases should be removed. Furthermore we also looked at the way viewers rate speakers based on gender. Even under this category we observed that male speakers are confidently rated to have given beautiful and courageous talks when compared to any other gender (sharpness, and less width of blue distributions in figure 3 indicates that). Though the confidence and tendency of rating in favor of male speakers drops substantially under informative and inspiring category (comparable width of green blue and red distributions) but is still slightly more than that for the speakers that are female (bi-modality of the distribution is an indication of confused rating behaviour) and of other gender. Besides highlighting the prevalence of biases with respect to gender and race, these results point out the need consider both expected and unexpected biases in data. To dive deep into the nature of prevalent bias with respect to gender of a speaker we further divided the rating labels into positive and negative, where positive includes say, \u201cbeautiful\u201d, \u201ccourageous\u201d etc. and negative includes say, \u201cconfusing\u201d, \u201cunconvincing\u201d etc. Now if we carefully notice 4, we will see that there is lot more structure in ratings for male and female speakers and a lot more variability in the matrix for other speakers when positive and negative rating labels are considered. Note that, in our dataset a viewer can choose to rate with any of three possible labels. The chaos in the third matrix (rightmost) of figure 4 indicates that, for speakers of other genders, viewers tend to choose a mix of positive and negative labels. However for male and female speakers viewers are more sure and choose either all positive or all negative labels depending on their liking of the talk. This indicates that in general there is extreme confusion among viewers about their decision of whether or not they like a talk given by a speaker of other gender. This can also be identified as an indicator for the viewer\u2019s double minded intention when they rate those talks."}, {"heading": "5 Methodology", "text": "In this section we will describe the methods we used to design a bias free rating prediction. We explored all three methods as suggested in [8], 1) Pre-Processing, 2) In-Processing and 3) Post Processing. We utilized the aforementioned toolkit AIF360 to implement these methods. This is explained in Figure 5 which is adopted from [2].For each of the three steps, we quantify the amount of fairness with respect to a well defined metric, in this case \u2018Disparate Impact\u2019 [3]. In our TedTalk dataset, the fourteen rating categories are transformed to binary labels as described in Data section, i.e the category \u2019beautiful\u2019 can be 0 or 1 depending on if it was rated true or not. Using these binary labels we calculate the disparate impact in the\nrating of a video. Disparate impact can be understood with an example, let\u2019s say we consider the rating category \u2018beautiful\u2019. For female speakers, we can estimate how likely are they to get a beautiful rating. Disaprate impact compares such probabilities for all possible groups like male with female, female with other gender, male with other gender and so on. Disparate impact equals to 1 identifies as a fair case since it means both groups that are compared are equally likely to be rated beautiful."}, {"heading": "5.1 Pre Processing", "text": "As a first step we modify the original data before training the rating classifier. We then feed the modified and bias free data to the classifier for training as in 5 top row. The main driving force for this step is the fact that raw data is inherently biased in almost all problems pertaining to social science. The pre-processing step attempts to make the data bias free, so that the classifier trained with this unbiased data makes fair predictions. When we look into our data in details, we see from Table 2 that the there is a strong imbalance of bias in the data with respect to race and gender. We observe that our data has a stronger bias related to race than for gender. So with the goal to make fair predictions, we first preprocess the data and attempt to remove the dominant bias with respect to race. There are several ways to remove bias from the training data before feeding it to the classifier [5, 10, 14, 32]. We have chosen the disparate impact remover method [10] as the metric we have used is disparate impact metric. We use the disparate impact remover from AIF360 for this purpose. The result of the preprocesing step is shown in Figure 6. This figure is showing the disparate impact across all rating labels of the dataset. We computed the disparate impact metric for the original label of the dataset and observed that it is far from 1 for most of the rating categories as shown by the blue bars. This exhibits the amount of unfairness\npresent in the original dataset. The huge existing bias in original data highlights the need to design a fair predictor of the rating. Hence, we trained a logistic regression model with both the original dataset and the pre-processed dataset. We observed that prediction on preprocessed data has disparate impact closer to 1 (Figure 6) than when it is trained with the orginal biased data. Our result hence shows that it is important to identify the cause of dominant bias in the data and remove it by preprocessing to gain substantial improvement in fair predictions."}, {"heading": "5.2 In Processing", "text": "In the last section we showed that it is possible to build a fair model by removing the bias present in the original data before training a classification model. However it is not always possible to do preprocessing, because there may not be access to the original data or it might be hard to identify the cause of bias in the data or it may be time consuming to re-annotate data and so on. Under such scenarios the solution is to design a fair classifier that uses a fair algorithm instead. So this allows us to still train the model with a biased dataset as input but the predictions employed by the fair algorithm are fair. The algorithm and the classifier in this case identifies the presence of bias in the dataset used for training and adjusts appropriately (based on the amount of bias) while predicting corresponding labels. Examples of such in processing techniques for bias mitigation can be found in [22, 15, 33]. For example, in our case, we hope that even if there is some bias in our dataset in favor of male, our rating classifier will make sure it accounts for that and weighs females and speakers of other genders equally to get rid of such unwanted unfairness. This will then generate fair rating predictions. For the TED talk data we have used the \u201cPrejudice Remover\u201d to do in processing [15]. The intuition is that we attempt to make the dependence of rating on sensitive attributes as strong as the nonsensitive attributes. For example, assume that female speaker is unprivileged and male speaker is privileged, that is ratings are heavily biased by male speakers in the real data. In that case the goal of the classifier and the learning algorithm is to make sure that rating predictions depend equally on male and female speaker and is not strongly influenced by male speakers only instead even if that is the case in the biased data. The result of in-processing is shown in Figure 7. This figure is showing the disparate impact across all rating labels of the dataset. Similar to the pre-processing part we computed the disparate impact metric (in this case we compute the metric for gender bias) for the original label of the dataset and observed that it is far from 1 for most of the rating categories as shown by the blue bars,. This exhibits the amount of unfairness present in the original dataset w.r.t gender. We then trained a logistic regression model and a Prejudice Remover model [15] with the original dataset. We observed that prediction of the Prejudice Remover model has disparate impact closer to 1 (Figure 6) than the prediction of the logistic regression model for most of the categories."}, {"heading": "5.3 Post Processing", "text": "We tried many popular methods [12, 26] available in literature to employ post processing in our data. However, none of the methods gained significant improvement in making fair predictions. This highlights two important issues: 1) Nature of bias strongly drives what technique works best in terms of making fair predictions, in our case gender and race bias cannot be removed by just post processing technique 2) Not all types of dataset can used to make fair prediction with a fixed processing technique, in our case, as shown, post processing is unsuitable for the TED talk data videos."}, {"heading": "5.4 Conclusion from analysis", "text": "We have employed a principled technique for removing bias and making fair rating predictions for TED talk videos. We first identify the dominant cause of the bias (in our case it is racial bias) in the data and remove it by pre processing, we then remove the other non-dominant causes of bias (gender bias in our\ncase) by in-processing and we show that post processing does not make any improvement in fair predictions for our data. This highlights that choice of processing to achieve fair predictions heavily depends of types of bias and the dataset being used. This establishes the need to explore all possible biases and all possible techniques to obtain best results in terms of fairness."}, {"heading": "6 Discussion", "text": "Ensuring fairness in a machine learning model is a complex, multi-faceted issue which depends not only on the challenges presented by the immediate task but also on the definition of fairness that is in use. Unfairness or bias may arise at any stage of a machine learning pipeline. Thus, while developing machine learning algorithms which can make fair predictions for a classification task is important, creating a data collection process that is free from bias is essential for the model\u2019s ultimate success. For example, Holstein et al. [13] show that commercial machine-learning product teams express the need for tools that can actively guide the data collection process to ensure fairness in the downstream machine learning task. In our study, we demonstrate a systematic method of detecting bias using the AIF360 toolkit and show that various bias mitigation techniques can be applied to our data set at various stages of a machine learning pipeline (i.e. pre-processing, in-processing, post-processing). This procedure can be followed by any machine learning practitioner or data scientist who has to deal with real world data to make machine learning models for decision making. We have applied a state of the art fairness measuring metric \u2018Disparate impact\u2019 to a new and diverse dataset \u2018TedTalk\u2019, revealing sharp differences in the perception of speakers based on their race and gender. Based on our findings, we demonstrate that in TED Talk ratings, viewers rate \u2018white male\u2019 speakers confidently, while all other speakers are rated in a weaker manner. On the other hand, viewers rating shows a great deal variability when rating any other groups combined. Our study shows that detecting and erasing bias in the data collection process is essential to resolving issues related to fairness in AI. While the existing literature about fairness in AI [9, 24, 27] have focused almost exclusively on detecting bias in specific domains such as recidivism prediction, admission decision-making,\nface detection etc, the analysis of implicit bias and . In all these scenarios, researchers come up with a quantifiable measurement of fairness and train the model which ensures the model is fair. However, in a more diverse domain where the users and system interacts in a complex way (e.g. recommendation system, chatbots etc), there is a lot of work left to do to detect and erase unfairness.\nWhile we have evaluated the Ted Talk dataset with specific metrics that are appropriate to it, there is an urgent need to develop metrics that are adapted just as uniquely to the needs of each dataset and its possible implementations. Identifying different kinds of social bias that affects in applications directly engaging with social influence and opinion formation is absolutely necessary for creating fair and balanced artificial intelligence systems."}], "title": "Detection and Mitigation of Bias in Ted Talk Ratings", "year": 2020}