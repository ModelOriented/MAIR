{"abstractText": "Model interpretability is a requirement in many applications in which crucial decisions are made by users relying on a model\u2019s outputs. The recent movement for \u201calgorithmic fairness\u201d also stipulates explainability, and therefore interpretability of learning models. And yet the most successful contemporary Machine Learning approaches, the Deep Neural Networks, produce models that are highly non-interpretable. We attempt to address this challenge by proposing a technique called CNN-INTE to interpret deep Convolutional Neural Networks (CNN) via meta-learning. In this work, we interpret a specific hidden layer of the deep CNN model on the MNIST image dataset. We use a clustering algorithm in a two-level structure to find the meta-level training data and Random Forest as base learning algorithms to generate the meta-level test data. The interpretation results are displayed visually via diagrams, which clearly indicates how a specific test instance is classified. Our method achieves global interpretation for all the test instances on the hidden layers without sacrificing the accuracy obtained by the original deep CNN model. This means our model is faithful to the original deep CNN model, which leads to reliable interpretations.", "authors": [{"affiliations": [], "name": "Xuan Liu"}, {"affiliations": [], "name": "Xiaoguang Wang"}, {"affiliations": [], "name": "Stan Matwin"}], "id": "SP:ec28cfe6d4e8cf599020d9403fb231b13ad406f1", "references": [{"authors": ["J.L. Herlocker", "J.A. Konstan", "J. Riedl"], "title": "Explaining collaborative filtering recommendations", "venue": "Proceedings of the 2000 ACM conference on Computer supported cooperative work, pp. 241\u2013250,December 2000.", "year": 2000}, {"authors": ["M.T. Dzindolet", "S.A. Peterson", "R.A. Pomranky", "L.G. Pierce", "H.P. Beck"], "title": "The role of trust in automation reliance", "venue": "Int. J. Hum.-Comput. Stud., vol.58, no.6, pp.697\u2013718, 2003.", "year": 2003}, {"authors": ["J. Ross Quinlan"], "title": "5: programs for machine", "year": 2014}, {"authors": ["R.L. Rivest"], "title": "Learning decision lists", "venue": "Machine learning, vol.2, no.3, pp.229\u2013246, 1987.", "year": 1987}, {"authors": ["H. Lakkaraju", "S.H. Bach", "J. Leskovec"], "title": "interpretable decision sets: A joint framework for description and prediction", "venue": "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 1675\u20131684, ACM, August, 2016.", "year": 2016}, {"authors": ["M.T.Ribeiro", "S.Singh", "C.Guestrin"], "title": "Why should i trust you? : Explaining the predictions of any classifier", "venue": "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 1135\u20131144, ACM, August, 2016.", "year": 2016}, {"authors": ["H. Lakkaraju", "E.Kamar", "R.Caruana", "J.Leskovec"], "title": "Interpretable & Explorable Approximations of Black Box Models", "venue": "KDD\u201917 workshop, 2017.", "year": 2017}, {"authors": ["J.J.Thiagarajan", "B.Kailkhura", "P. Sattigeri", "K.N.Ramamurthy"], "title": "TreeView: Peeking into Deep Neural Networks Via Feature-Space Partitioning", "venue": "30th Conference on Neural Information Processing Systems (NIPS), 2016.", "year": 2016}, {"authors": ["M.Abadi"], "title": "TensorFlow: A System for Large-Scale Machine Learning", "venue": "OSDI, Vol. 16, pp. 265\u2013283, 2016.", "year": 2016}, {"authors": ["M.T.Ribeiro", "S.Singh", "C.Guestrin"], "title": "Nothing Else Matters: Model- Agnostic Explanations By Identifying Prediction Invariance", "venue": "30th Conference on Neural Information Processing Systems (NIPS), 2016.", "year": 2016}, {"authors": ["M. Craven", "J.W. Shavlik"], "title": "Extracting tree-structured representations of trained networks", "venue": "Advances in neural information processing systems, pp. 24\u201330, 1996.", "year": 1996}, {"authors": ["P.K. Chan", "S.J. Stolfo"], "title": "Experiments on multistrategy learning by meta-learning", "venue": "Proceedings of the second international conference on information and knowledge management, pp. 314\u2013323, ACM, December, 1993.", "year": 1993}, {"authors": ["G. Montavon", "W. Samek", "K.R. M\u00fcller"], "title": "Methods for interpreting and understanding deep neural networks", "venue": "Digital Signal Processing, 2017.", "year": 2017}, {"authors": ["V. Nair", "G.E. Hinton"], "title": "Rectified linear units improve restricted boltzmann machines", "venue": "Proceedings of the 27th international conference on machine learning (ICML-10), pp. 807\u2013814, 2010.", "year": 2010}, {"authors": ["Y. LeCun", "L. Bottou", "Y. Bengio", "P. Haffner"], "title": "Gradient-based learning applied to document recognition", "venue": "Proceedings of the IEEE, 86(11), pp. 2278\u20132324, November, 1998.", "year": 1998}, {"authors": ["N. Srivastava", "G. Hinton", "A. Krizhevsky", "I. Sutskever", "R. Salakhutdinov"], "title": "Dropout: A simple way to prevent neural networks from overfitting", "venue": "The Journal of Machine Learning Research, 15(1), pp. 1929\u20131958, 2014.", "year": 1929}, {"authors": ["D.P. Kingma", "J. Ba"], "title": "Adam: A method for stochastic optimization", "venue": "arXiv preprint arXiv:1412.6980, 2014.", "year": 2014}, {"authors": ["X. Liu", "X. Wang", "S. Matwin", "N. Japkowicz"], "title": "Meta-learning for large scale machine learning with MapReduce", "venue": "Big Data, 2013 IEEE International Conference on, pp. 105\u2013110, IEEE, October, 2013.", "year": 2013}, {"authors": ["J.A. Hartigan", "M.A. Wong"], "title": "Algorithm AS 136: A k-means clustering algorithm", "venue": "Journal of the Royal Statistical Society, Series C (Applied Statistics), 28(1), pp. 100-108, 1979.", "year": 1979}, {"authors": ["A. Liaw", "M. Wiener"], "title": "Classification and regression by randomForest", "venue": "R news, 2(3), pp. 18\u201322, 2002.", "year": 2002}, {"authors": ["M. Ester", "H.P. Kriegel", "J. Sander", "X. Xu"], "title": "A density-based algorithm for discovering clusters in large spatial databases with noise", "venue": "Kdd, Vol. 96, No. 34, pp. 226-231, August, 1996.", "year": 1996}, {"authors": ["J. Yosinski", "J. Clune", "A. Nguyen", "T. Fuchs", "H. Lipson"], "title": "Understanding neural networks through deep visualization", "venue": "arXiv preprint arXiv:1506.06579, 2015. Fig. 7. Example of a correctly classified test instance: True1. Fig. 8. Example of a correctly classified test instance: True2. Fig. 9. Example of a wrongly classified test instance: Wrong1.", "year": 2015}], "sections": [{"text": "Index Terms\u2014interpretability, Meta-learning, deep learning, Convolutional Neural Network, TensorFlow, big data\nI. INTRODUCTION\nWith the fast development of sophisticated machine learning algorithms, artificial intelligence has been gradually penetrating a number of brand new fields with unprecedented speed. One of the outstanding problems hampering further progress is the interpretability challenge. This challenge arises when the models built by the machine learning algorithms are to be used by humans in their decision making, particularly when such decisions are subject to legal consequences and/or administrative audits. For human decision makers operating in those circumstances, to accept the professional and legal responsibility ensuing from decisions assisted by machine learning, it is critical to comprehend the models. This is generally true for areas like criminal justice, health care, terrorism detection, education system and financial markets.\nTo trust the model, decision makers need to first understand the model\u2019s behavior, and then evaluate and refine the model using their domain knowledge. Even for areas like\nbook or movie recommendations [1] and automated aids [2], explanations for a recommendation and an error made could increase the trust and reliance on these systems. Furthermore, the European General Data Protection Regulation, forthcoming in June, 2018, stipulates the explainability of all automatically made decisions concerning individuals, and that includes the decisions made with or assisted by machine learning models. Hence, there is a growing demand for interpretability of the machine learning algorithms.\nIn this paper, we define interpretability of a model as the ability to provide visual or textual presentation of the connections between input features and the output predictions.\nTo realize the goal of interpretability, there are usually two approaches. One is to design an algorithm that is inherently interpretable, while achieving competitive accuracy of a complex model. The examples are Decision Trees [3], Decision Lists [4], and Decision Sets [5], etc. The disadvantage of this approach is that there is a trade off between interpretability and accuracy: it is not easy to learn an interpretable (so presumably simple) model expressing a complex process with a very high accuracy. The other approach which does not sacrifice accuracy takes the opposite approach: it first builds a highly accurate model without worrying about interpretabilty, and subsequently uses a separate set of re-representation techniques to assist the user in understanding the behavior of the algorithm. One of the techniques could be to use the aforementioned relatively simple and interpretable algorithms to explain the behavior of a complex model and the reasons why a given classifier, treated as a black box, classifies a given instance in a particular way, e.g. LIME [6], BETA [7], TREPAN [11].\nDeep learning methods have been lately very successful in image processing and natural language processing. It could be categorized as a representation learning approach [12], which learns refined features that could improve a model\u2019s generalization ability. Deep learning, however, is highly noninterpretable.\nIn this paper we are reporting a work in progress where we try to interpret the inner mechanisms of deep learning.\nar X\niv :1\n80 2.\n00 56\n0v 2\n[ cs\n.L G\n] 1\n9 A\nug 2\nOur method: CNN-INTE is inspired by [8]. We design and implement a tool that helps the user understand how the hidden layers in a deep CNN model work to classify examples. And the results are expressed in graphs which indicate sequential separations of the true class and the hypothesis. The main contributions of our method is as follows:\n\u2022 Compared to LIME [6] which provides local interpretations for the entire model in specific regions of the feature space, our method provides global interpretation for any test instances on the hidden layers in the whole feature space. \u2022 Compared to models which apply inherently interpretable algorithms, e.g. [5], our method has the advantage of not compromising the accuracy of the model to be interpreted. This produces more reliable interpretation. \u2022 In contrary to [6] and [7] which treat the model to be interpreted as black box, we interpret the inner mechanisms of deep CNN models. \u2022 The experiments are implemented in the TensorFlow [9] platform, which makes our model scalable to big datasets more easily. Scalability is an issue pointed out as future work in [6] and [7] but not realized yet."}, {"heading": "II. RELATED WORK", "text": "To resolve the problems for \u201ctrusting a prediction\u201d and \u201ctrusting a model\u201d, two methods are proposed in [6] to explain individual predictions and understand a model\u2019s behavior respectively: Local Interpretable Model-agnostic Explanations (LIME) and Submodular Pick LIME (SP-LIME). The main idea for LIME is to use inherently interpretable models g to interpret complex models f locally. They designed an objective function to minimize the unfaithfulness (when g is approximating f in a local area) and the complexity of g. Although it was stated in their paper that in the objective function g could be any interpretable models, they set g as sparse linear models in their paper. Based on the individual explanations generated by LIME, they design an submodular pick algorithm: SP-LIME to explain the model as a whole by picking a number of representative and non-redundant instances.\nIt was suggested in [10] that coverage, precision and effort should be used to evaluate the results of the model interpretation. Although LIME achieves high precision and low effort, the coverage is not clear. In other words, LIME is able to explain why a specific prediction is made using the weights of the local model g, but can\u2019t indicate to what local region the explanation is faithful. To solve this problem, the Anchor Local Interpretable Model-Agnostic Explanations method (aLIME) was introduced in [10]. In aLIME, the if-then rules are used instead of using the weights in a linear model to explain a specific prediction (as was executed in LIME). The idea is based on the Decision Sets algorithm from [5]. These if-then rules are easy to comprehend and has good coverage.\nIt was pointed out that there is a trade off between interpretability and accuracy for machine learning algorithms [5]. In terms of inherently interpretable models, rule-based models,\ne.g. Decision Trees and Decision Lists are often preferred, as they can find a balance between these two factors. Decision lists are usually considered more interpretable than decision trees, as they use the if-then-else statements with a hierarchy structure. But this structure reduces to some extent the interpretability, as to interpret an additional rule all previous rules should be reasoned about. Also new rules down the list are applied to much narrow feature spaces, which makes the multi-class classification difficult where the minority classes deserves equally good rules. This motivates the proposal of the Decision Sets algorithm in [5], which produces the isolated ifthen rules, where each rule could be an independent prediction. To realize this, an objective function takes into account both interpretability (expressed by precision and recall of rules) and accuracy (expressed by size, length, cover and overlap). They showed that solving the objective function is a NPhard problem, and finds near-optimal solutions of it. However, Decision set\u2019 accuracy only approaches random forest, and its expressive power just catches up with decision tree.\nAnother model agnostic explanation approach is the Black Box Explanations through Transparent Approximations (BETA), introduced in [7]. Different from LIME which aims for local interpretation, BETA is a framework which attempts to produce global interpretation for any classifier which are treated as black box classifiers. Based on their previous work on Decision Sets, the authors designed a framework with two level decision sets to taking into account fidelity (faithfulness to the black box model), unambiguity (single and deterministic explanations for each instance), interpretability (complexity minimized) and interactivity (user specified explorations of the feature\u2019s subspace). In this two level structure, the outer if-then rules are the \u201cneighborhood descriptors\u201d and the inner if-then rules are \u201cdecision logic rules\u201d (how the black box model labels an instance under the outer if-then rules). Similar to [5], an objective function is built and near-optimal solutions are found.\nTwo tools are introduced in [23] to provide intuitive understandings about the inner workings of deep neural networks (DNNs). The first tool directly plots the activations on each layer of a specific trained DNN. The second tool visualizes the features computed by each neurons on each layer of a DNN. Although the tools introduced in [23] also interpret DNNs, they are totally different approachs from ours. They visualize the values of the intermediate results directly within a DNNs while we visualize the behavior of a hidden layer via an inherently interpretable algorithm: Decision Tree. Therefore, they aims to improve the architecture of DNNs and provide inspirations for transfer learning, discriminative networks and generative models. And we focus on interpreting how the hidden layers classify test instances to ensure trust on a trained DNN."}, {"heading": "III. METHODOLOGY", "text": "Our methodology could be classified as the post-hoc interpretation [14], where a trained model is given and the main task is to interpret it. This method is close to the second\napproach mentioned in the fourth paragraph of the introduction section, but is also different in many ways. First, the model to be interpreted here is not treated as a black box as we directly interpret the hidden layers of a deep CNN. Second, compared to LIME [6] which only has local interpretability, our method achieves global interpretability. Similar to LIME, we also provide qualitative interpretation with graphs to visualize the results. As our method interprets deep CNN via Meta-learning, we first briefly introduces deep CNN, meta-learning and then discuss our framework in details."}, {"heading": "A. Deep Convolutional Neural Network", "text": "This section introduces the deep CNN model we are going to interpret. As we implement our program using TensorFlow, we use its TensorBoard function to draw the structure of the deep CNN we construct in Fig. 1. Deep CNN is now the most advanced machine learning algorithm for image classification. It takes advantage of the two-dimensional structure of the input images. It uses a set of filters to filter the pixels of the raw input images to generate higher level representations to be learnt by the model in order to improve the performance.\nThere are three major components of deep CNN: convolutional layer, pooling layer and fully connected layer (same as in regular neural networks). A deep CNN model is usually a stack of these layers. In the convolutional layer, a filter is used to compute dot products between the pixels of the input image at specific position and the values of the filter, producing one single value in the output feature map. The convolution operation is completed after the filter is slided across the width and height of the input image. Following the convolutional layer, an activation function, often a rectified linear unit (ReLU) [15], is applied to inject nonlinearities into the model and speed up the training process. Following ReLU is the pooling layer which is a non-linear down-sampling layer. A common algorithm for pooling is the max pooling algorithm. In this algorithm, each sub-region of the previous feature map is turned into a single maximum value in this region. Max pooling reduces computation and controls overfitting. In order to calculate the predicted class, after performing max pooling, the feature map needs to be flattened and feed into a fully connected layer. In the last layer: the output layer, a softmax classifier is applied for prediction.\nThe structure of the deep CNN model we designed is illustrated in Fig. 1, \u201cPlaceholder\u201d represents for the interface to input the training data. \u201cReshape\u201d is needed first to convert the input one-dimensional image data into two dimensional data. In our experiment, we use the MNIST dataset [16]. The 784 input features are converted into a two-dimensional 28 \u00d7 28 image. Our model has two series of a convolutional layer followed by a pooling layer: \u201cconv1\u201d-\u201cpool1\u201d-\u201cconv2\u201d\u201cpool2\u201d, which are followed by one fully connected layer \u201cfc1\u201d. As a fully connected network is susceptible to suffer from overfitting, the \u201cdropout\u201d operation [17] applied after \u201cfc1\u201d aims to reduce it. In this operation, a probability parameter p is set to keep a specific neuron with probability p (or drop it with probability 1-p). The \u201cAdam optimizer\u201d [18],\nrather than a standard Stochastic Gradient Descent optimizer is used to train the model via modifying the variables and reducing the loss. \u201cfc2\u201d is the output layer with 10 neurons: each represents one of the classes: 0-9."}, {"heading": "B. Meta-learning", "text": "Meta-learning is an ensemble learning method which learns from the results of the base classifiers. It has a two-level structure, where the algorithms used in the first level are called base-learners and the algorithm in the second level is called the meta-learner. The base-learners are trained on the original training data. The meta-learner is trained by the predictions of the base classifiers and the true class of the original training data. When training the meta-learner, the \u201cClass-combiner\u201d strategy [13] is applied here, where the predictions includes just the predicted class (instead of all classes, as in the \u201cBinary-class-combiner\u201d).\nTo understand the meta-learning algorithm intuitively, Fig. 2 illustrates a simplified training process for meta-learning [19].\nThe numbers 1, 2, 3, 4 represent the four steps of training. In the 1st step, the base learning algorithms 1 to m are trained on the training data. In the 2nd step, a validation dataset is used to test the trained classifiers 1 to m. In the 3rd step, the predictions generated in step 2 and the true labels of the validation dataset are used to train a meta-learner. Finally, in the 4th step, a meta-classifier is produced and the whole metalearning training process is completed.\nOnce the training process is accomplished, the test process is much easier to execute. Fig. 3 presents a simplified test process [19]. In the 1st step, the test data is applied on the base classifiers to generate predictions which combined with the true labels of the test data comprises the meta-level test data in 2nd step. In the 3rd step, the final predictions are generated by testing the meta-level classifier with the predictions in the 2nd step and the accuracy could be calculated."}, {"heading": "C. Framework", "text": "Our framework is named as CNN-INTE which stands for Convolutional Neural Network Interpretation. It is similar to meta-learning, but different in a few ways. In this work, we interpret the first fully connected layer \u201cfc1\u201d of the deep CNN model illustrated in Fig. 1.\nThe training process is shown in Fig. 4. In the 1st step, the original training data is used to train a CNN model. In the 2nd step, the parameters generated in the 1st step are used to calculate the values for the activations of the first fully connected layer: fc1. In the 3rd step, a clustering algorithm is used to cluster the data generated in step 2 into a number of groups which we define as factors henceforth. In the 4th step, the data belonging to each of the factors are clustered again generating a number of clusters each assigned a unique ID. In the 5th step, these IDs are grouped together as the training features in the meta-level, using the labels of the original training data as label for the meta-learner. In the 6th step, the features of the original training data and the IDs (set as labels) in step 4 are used to train a number of random forests [21].\nNow we discuss the training process in more details. Assume the training data T has N numbers of instances and layer \u201cfc1\u201d has H neurons. The labels of the training data are Ty = {l1, l2, ..., lN}. Once the deep CNN model is trained, for\neach training instance ti, we calculate the activations at each hidden neuron on this layer. Hence, we obtain a matrix S with size H \u00d7N . To construct the meta-level training data, we use a clustering algorithm to cluster S along the hidden layer axis into several factors F = {f1, f2, ..., fK}. How to set the value of K is tricky. In our experiments, this value is determined as the one which produces the best accuracy performance for the meta-learning algorithm. We also discussed how to avoid this problem in section V. Then within each of the factors, we cluster the data again, this time along the axis of the instances. The clustering results are the IDs each instance belongs to. For instance, if the number of clusters is 10, after the second level clustering each instance will have an ID number between 0-9. All the IDs combined with the true labels of the training data builds up the meta-level training data.\nTo present the technical details of the CNN-INTE training process, we provide the pseudo code in Algorithm 1. Line 1-3 is the initialization of the algorithm. In line 4, the activations S are clustered into K factors, where K is the number of clusters set in the first level clustering algorithm Cl. In lines 5-7 the same clustering algorithm Cl is applied on all the factors to generate K sets of ID numbers. Lines 8-9 uses the generated ID numbers and the true labels of the original training data to train the meta-learner: Cm. Till now, the training process is not done yet. We still need to generate the base models to be used in the test process. Lines 10-12 uses the features of the original training data and the ID numbers to train K base models. The output of the training process would be the meta-lever classifier: M\u0303 and K base models: B = {M1,M2, \u00b7 \u00b7 \u00b7 ,MK}.\nFig. 5 is a toy example that illustrates the above process. In this example, there are 5 hidden neurons and 6 training instances. We set the number of clusters for both the first and\nAlgorithm 1 CNN-INTE Training Process 1: Input: activations: S; training data: T ; Meta learning\nalgorithm: Cm; Clustering algorithm: Cl; Base learning algorithms: {C1, C2, \u00b7 \u00b7 \u00b7 , CK}\n2: E = \u2205 3: Scv = \u2205 4: {f1, f2, \u00b7 \u00b7 \u00b7 , fK} = Cl(S) 5: for k = 1 \u00b7 \u00b7 \u00b7K do 6: IDsk = C\nl(fk) 7: end for 8: Scv = {IDs1, IDs2, \u00b7 \u00b7 \u00b7 , IDsK , Ty} 9: M\u0303 = Cm(Scv)\n10: for k = 1 \u00b7 \u00b7 \u00b7K do 11: Mk = Ck(Tx, IDsk) 12: end for 13: E = ({M1,M2, \u00b7 \u00b7 \u00b7 ,MK} , M\u0303) 14: Output: Ensemble E\nsecond level clustering as 3. Hence, the matrix S with size 5\u00d7 6 is first clustered into 3 factors {f1, f2, f3} horizontally. For each factor, the activations are again clustered into three clusters vertically, e.g. F1 is clustered into {C11, C12, C13}. If we set the ID numbers for these cluster as {0, 1, 2}, then the corresponding ID numbers for t1 to t6 in factor f1 according to Fig. 5 are {0, 0, 1, 1, 2, 2}. Hence, the meta-level training features are expressed as0 0 1 1 2 20 0 0 1 1 2\n0 1 1 1 2 2  This data combined with the corresponding training labels of the original training data is used to train the meta-learner. Here the meta-learner we used is the Decision Tree [3], an inherently interpretable algorithm. Its tree structure provides an excellent visual explanation of the predictions.\nThe test process of the meta-model is exactly the same as the meta-learning test process, which is shown in Fig. 6.\nIn the test process, we use the original test data to test the base classifiers generated in the meta-level training process to obtain the meta-level test data\u2019s features. The base-learner we applied is random forest. The number of base models is equal to the number of factors. Hence, we have K base models: B = {M1,M2, ...,MK}. In the toy example, there are three factors which lead to three base models. The training data for the first base model corresponding to f1 are\n(t1\u2212 l1) 0 (t2\u2212 l2) 0 (t3\u2212 l3) 1 (t4\u2212 l4) 1 (t5\u2212 l5) 2 (t6\u2212 l6) 2\nHere ti\u2212li represents for the features of the original training instance i. Once we obtain the K base models, we can use the original test data to test them to produce the meta-level test data. These data are then feed into the trained decision tree model to interpret individual test predictions."}, {"heading": "IV. EXPERIMENTS", "text": "The dataset we use is the MNIST database of handwritten digits from 0 to 9 [16]. We extracted 55,000 examples (the original dataset has 60,000 examples for training) as the training data and 10,000 examples as the test data. Each of the examples represents for the 28 \u00d7 28 images with pixels flattened as 784 features. The experiments are performed on the TensorFlow platform."}, {"heading": "A. Experimental Setup", "text": "First of all, we need to train a nice deep CNN model. We first reshape the input training data into 55000 images each with size 28 \u00d7 28. Training all the data on every epoch is expensive, which requires lots of resources of the computer and may lead to the termination of the program. Here we apply stochastic training: on the first epoch, we select a minibatch of the training data and perform optimization on this batch; once we loop through all the batches, we randomize the training data and start a new epoch. In our experiment, we set the epoch e = 1000, batch size b = 50. Stochastic training is cheap and achieves similar performance to using the whole training data in every epoch. For each mini-batch, in the first convolutional layer, we apply 32 filters (or kernels) each with size 5\u00d75, which generates 32 feature maps. In the first pooling layer we apply filters with size 2\u00d7 2. The stride size is set as\n2. The second convolutional layer use 64 filters with the same size as the first convolutional layer. The second pooling layer has the same parameters as the previous one. Immediately after this pooling layer is the first fully connected layer: fc1. We set the number of neurons for this layer as 128. To reduce overfitting we also set the dropout [17] parameter d = 0.5, which means a neuron\u2019s output has 50% probability to be dropped. The last layer is the second fully connected layer (or the \u201creadout layer\u201d), which has 10 neurons with each neuron outputs the probability of the corresponding digits 0-9. The test accuracy of this trained CNN model on the test data is 93.9%.\nNow comes the key part for setting up interpretation. we define interpretability of a model as the ability to provide visual or textual presentation of the connections between input features and the output predictions. We first feed the trained fully connected layer fc1 with the original training data, which would produce a data S with size of 128 \u00d7 55000. We then cluster S into several factors. The clustering algorithm we applied is the k-means algorithm [20]. The number of factors is equal to the number of clusters which we set as 8 in this level. Hence, S is now turned into a list F = {f1, f2, ..., f8} with size 8 \u00d7 55000 having each row representing the data belonging to each factor. In the second level clustering, for each factor in F we use the k-means algorithm again to cluster them into a number of clusters. We set the number as 10 in our experiment as the number of classes for the original training data is 10. Hence each cluster will be assigned a unique ID number between 0 and 9. Then we use the IDs belonging to each training instances and the true labels of the original training data to train a decision tree algorithm. Due to the limitation of the space, we are unable to show the structure of the trained decision tree here. We set the maximum depth of the decision tree as 5. Although deeper decision tree would generate better accuracy, it makes it harder to interpret with too many tree levels.\nTo obtain the test data for decision tree, we first use the original training data\u2019s feature as features and the IDs for each factor in F as labels to train the corresponding random forest algorithm [21], generating 8 base models. For random forest, we set the number of trees as 20 and the maximum nodes as 2000. Finally we use the original test data to test the 8 trained base models. The generated predictions become the features of meta-level test data with sizes of 10000 \u00d7 8. Using the meta-level test data on the trained decision tree produces an accuracy of 92.8% with tree depth=5. This value is comparable to the test accuracy on the trained deep CNN model: 93.9%. It should be noted that the decision tree\u2019s accuracy could be further improved by increasing the depth of tree and tuning other related parameters."}, {"heading": "B. Experimental Results", "text": "To interpret the deep CNN model\u2019s behavior on the test data, we intend to use diagrams generated by our tool: CNN-INTE to examine individual predictions on the test data. Hence, we provide qualitative interpretations visually. We arbitrarily\nselected two test instances that were correctly classified by the decision tree and one test instance that was wrongly classified. It should be noted that this tool could be used on any test instances globally and not just limited to the three cases we provide. The details of the selected test instances are shown in Table I. Here \u201cf0-f7\u201d represents the features of the metalevel test data, \u201clabel\u201d is the test label in the original test data, \u201cpred\u201d is the prediction generated by the decision tree on the meta-level test data. \u201cTrue1\u201d and \u201cTrue2\u201d represents for the two correctly classified instances and \u201cWrong1\u201d is the wrongly classified instance.\nIn order to examine the classification process visually, we check each feature values according to the trained structure of the decision tree and plot the graphs of the activations corresponding to the true label and the hypothesis. The interpretation result for instance \u201cTrue1\u201d is shown in Fig. 7. As the true label for this instance is 3, all other classes could be regarded as hypothesis and this is why there are no graphs for \u201cHypothesis: 3\u201d in Fig. 7. Each row represents the examination of the feature values corresponding to different factors in different levels of the trained decision tree, e.g. the first row represents the root level of the decision tree. Since we set the depth of the decision tree as 5, there are 5 rows in all. Each column stands for the query of if the test instance belongs to the corresponding hypothesis over the nodes visited.\nTake the column of \u201cHypothesis:0\u201d as an example, the goal is to find if the label of the test instance is 0. In the 1st row we extract the activations corresponding to \u201cf6\u201d which satisfies the condition that f6 6 4.5 (this is determined by the trained decision tree) and draw a graph between activations that belongs to label=0 (hypothesis) and label=3 (true). Then we check the graph to evaluate if the data corresponding to the true class could be separated from the hypothesis. The answer is no because the hypothesis represented as blue points overlaps with the true class shown as red points. Hence, we need to query the trained decision tree further. The values of the factors we need to check is: f6 6 2.5 for 2nd row; f6 6 0.5 for 3rd row; f7 6 0.5 for 4th row; f1 6 0.5 for 5th row. In this process, we noticed that in the 4th row the true class and the hypothesis class are successfully separated as only the red points corresponding to the true label are left. Therefore, we don\u2019t need to examine further and that\u2019s why the graph for the 5th row is not displayed. We highlight the graph with green rectangles if the final results are separable and red vice versa. The same idea is applied on other hypothesis. We also draw the graphs for instances \u201cTrue2\u201d and \u201cWrong1\u201d in Fig. 8 and Fig. 9 respectively."}, {"heading": "V. CONCLUSION AND FUTURE WORK", "text": "In this work, we present an interpretation tool CNN-INTE, which interprets a hidden layer of a deep CNN model: to find out how the learned hidden layer classifies new test instances. Although we just show the results for the first fully connected layer before the read-out layer, the approach could be applied on any hidden layers. The interpretation is realized by finding the relationships between the original training data and the trained hidden layer \u201cfc1\u201d via meta-learning. We used two-level k-means clustering algorithm to find the metalevel training data and random forests as base models for generating meta-level test data. The visual results generated by our program clearly indicate why a test instance is truly or wrongly classified by checking if there are any overlaps of the corresponding activations. For future work, we plan to initiate quantification of the interpreted results. In our experiments, one of the things we find tricky is the setting of the number of clusters for the k-means algorithm. In the future, we plan to replace the k-means algorithm with DBSCAN [22] which doesn\u2019t need specifying the number of clusters. As stated in [5], \u201cdecision sets\u201d seems to be a better option than decision tree as a inherently interpretable algorithm, so we also plan to replace decision tree with decision sets. Last but not least, it would be quite meaningful to apply this tool on real world applications with more complex data where interpretations are demanded either between the training data and the hidden layer or between the hidden layer and the predictions."}, {"heading": "ACKNOWLEDGMENT", "text": "The authors acknowledge the support of the Province of Nova Scotia, of Dalhousie University, and of the the Natural Sciences and Engineering Research Council of Canada under the CREATE program grant."}], "title": "Interpretable Deep Convolutional Neural Networks via Meta-learning", "year": 2018}