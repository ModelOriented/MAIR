{"abstractText": "Shapley values underlie one of the most popular model-agnostic methods within explainable artificial intelligence. These values are designed to attribute the difference between a model\u2019s prediction and an average baseline to the different features used as input to the model. Being based on solid game-theoretic principles, Shapley values uniquely satisfy several desirable properties, which is why they are increasingly used to explain the predictions of possibly complex and highly non-linear machine learning models. Shapley values are well calibrated to a user\u2019s intuition when features are independent, but may lead to undesirable, counterintuitive explanations when the independence assumption is violated. In this paper, we propose a novel framework for computing Shapley values that generalizes recent work that aims to circumvent the independence assumption. By employing Pearl\u2019s do-calculus, we show how these \u2018causal\u2019 Shapley values can be derived for general causal graphs without sacrificing any of their desirable properties. Moreover, causal Shapley values enable us to separate the contribution of direct and indirect effects. We provide a practical implementation for computing causal Shapley values based on causal chain graphs when only partial information is available and illustrate their utility on a real-world example.", "authors": [{"affiliations": [], "name": "Tom Heskes"}, {"affiliations": [], "name": "Evi Sijben"}], "id": "SP:b570d1da3c8a5e9b7debf5e0550f8757e58933a7", "references": [{"authors": ["Kjersti Aas", "Martin Jullum", "Anders L\u00f8land"], "title": "Explaining individual predictions when features are dependent: More accurate approximations to Shapley values", "year": 1903}, {"authors": ["Patrick Forr\u00e9", "Joris M Mooij"], "title": "Causal calculus in the presence of cycles, latent confounders and selection bias", "venue": "In Proceedings of the 35th Annual Conference on Uncertainty in Artificial Intelligence,", "year": 2019}, {"authors": ["Christopher Frye", "Ilya Feige", "Colin Rowat"], "title": "Asymmetric Shapley values: Incorporating causal knowledge into model-agnostic explainability", "venue": "arXiv preprint arXiv:1910.06358,", "year": 2019}, {"authors": ["Dominik Janzing", "Lenon Minorics", "Patrick Bl\u00f6baum"], "title": "Feature relevance quantification in explainable AI: A causal problem", "venue": "In International Conference on Artificial Intelligence and Statistics,", "year": 2020}, {"authors": ["Judea Pearl"], "title": "The do-calculus revisited", "venue": "arXiv preprint arXiv:1210.4852,", "year": 2012}, {"authors": ["Xinpeng Shen", "Sisi Ma", "Prashanthi Vemuri", "Gyorgy Simon"], "title": "Challenges and opportunities with causal discovery algorithms: Application to Alzheimer\u2019s pathophysiology", "venue": "Scientific reports,", "year": 2020}], "sections": [{"heading": "1 Introduction", "text": "Complex machine learning models like deep neural networks and ensemble methods like random forest and gradient boosting machines may well outperform simpler approaches such as linear regression or single decision trees, but are noticeably harder to interpret. This can raise practical, ethical, and legal issues, most notably when applied in critical systems, e.g., for medical diagnosis or autonomous driving. The field of explainable AI aims to address these issues by enhancing the interpretability of complex machine learning models.\nThe Shapley-value approach has quickly become one of the most popular model-agnostic methods within explainable AI. It can provide local explanations, attributing changes in predictions for individual data points to the model\u2019s features, that can be combined to obtain better global understanding of the model structure [17]. Shapley values are based on a principled mathematical foundation [27] and satisfy various desiderata (see also Section 2). They have been applied for explaining statistical and machine learning models for quite some time, see e.g., [15, 31]. Recent interests have been triggered by Lundberg and Lee\u2019s breakthrough paper [19] that introduces efficient computational procedures and unifies Shapley values and other popular local model-agnostic approaches such as LIME [26].\nAccepted at 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.\nar X\niv :2\n01 1.\n01 62\n5v 1\n[ cs\n.A I]\n3 N\nHumans have a strong tendency to reason about their environment in causal terms [28], where explanation and causation are intimately related: explanations often appeal to causes, and causal claims often answer questions about why or how something occurred [16]. The specific domain of causal responsibility studies how people attribute an effect to one or more causes, all of which may have contributed to the observed effect [29]. Causal attributions by humans strongly depend on a subject\u2019s understanding of the generative model that explains how different causes lead to the effect, for which the relations between these causes are essential [7].\nMost explanation methods, however, tend to ignore such relations and act as if features are independent. Even so-called counterfactual approaches, that strongly rely on a causal intuition, make this simplifying assumption (e.g., [33]) and ignore that, in the real world, a change in one input feature may cause a change in another. This independence assumption also underlies early Shapley-based approaches, such as [31, 3], and is made explicit as an approximation for computational reasons in [19]. We will refer to these as marginal Shapley values.\nAas et al. [1] argue and illustrate that marginal Shapley values may lead to incorrect explanations when features are highly correlated, motivating what we will refer to as conditional Shapley values. Janzing et al. [8], following [3], discuss a causal interpretation of Shapley values, in which they replace conventional conditioning by observation with conditioning by intervention, as in Pearl\u2019s do-calculus [24]. They argue that, when the goal is to causally explain the prediction algorithm, the inputs of this algorithm can be formally distinguished from the features in the real world and \u2018interventional\u2019 Shapley values simplify to marginal Shapley values. This argument is also picked up by [17] when implementing interventional Tree SHAP. Going in a different direction, Frye et al. [6] propose asymmetric Shapley values as a way to incorporate causal knowledge in the real world by restricting the possible permutations of the features when computing the Shapley values to those consistent with a (partial) causal ordering. In line with [1], they then apply conventional conditioning by observation to make sure that the explanations respect the data manifold.\nThe main contributions of our paper are as follows. (1) We derive causal Shapley values that explain the total effect of features on the prediction, taking into account their causal relationships. This makes them principally different from marginal and conditional Shapley values. At the same time, compared to asymmetric Shapley values, causal Shapley values provide a more direct and robust way to incorporate causal knowledge. (2) Our method allows for further insights into feature relevance by separating out the total causal effect into a direct and indirect contribution. (3) Making use of causal chain graphs [13], we propose a practical approach for computing causal Shapley values and illustrate this on a real-world example."}, {"heading": "2 Causal Shapley values", "text": "In this section, we will introduce causal Shapley values and contrast them to other approaches. We assume that we are given a machine learning model f(\u00b7) that can generate predictions for any feature vector x. Our goal is to provide an explanation for an individual prediction f(x), that takes into account the causal relationships between the features in the real world.\nAttribution methods, with Shapley values as their most prominent example, provide a local explanation of individual predictions by attributing the difference between f(x) and a baseline f0 to the different features i \u2208 N with N = {1, . . . , n} and n the number of features:\nf(x) = f0 + n\u2211\ni=1\n\u03c6i , (1)\nwhere \u03c6i is the contribution of feature i to the prediction f(x). For the baseline f0 we will take the average prediction f0 = Ef(X) with expectation taken over the observed data distribution P (X). Equation (1) is referred to as the efficiency property [27], which appears to be a sensible desideratum for any attribution method and we therefore take here as our starting point.\nTo go from knowing none of the feature values, as for f0, to knowing all feature values, as for f(x), we can add feature values one by one, actively setting the features to their values in a particular order \u03c0. We define the contribution of feature i given permutation \u03c0 as the difference in value function before and after setting the feature to its value:\n\u03c6i(\u03c0) = v({j : j \u03c0 i})\u2212 v({j : j \u227a\u03c0 i}) , (2)\nwith j \u227a\u03c0 i if j precedes i in the permutation \u03c0 and where we choose the value function\nv(S) = E [f(X)|do(XS = xS)] = \u222b dXS\u0304 P (XS\u0304 |do(XS = xS))f(XS\u0304 ,xS) . (3)\nHere S is the subset of \u2018in-coalition\u2019 indices with known feature values xS . To compute the expectation, we average over the \u2018out-of-coalition\u2019 or dropped features XS\u0304 with S\u0304 = N \\ S, the complement of S. To explicitly take into account possible causal relationships between the \u2018incoalition\u2019 features and the \u2018out-of-coalition\u2019 features, we propose to condition \u2018by intervention\u2019 for which we resort to Pearl\u2019s do-calculus [23]. In words, the contribution \u03c6i(\u03c0) now measures the relevance of feature i through the (average) prediction obtained if we actively set feature i to its value xi compared to (the counterfactual situation of) not knowing its value.\nSince the sum over features i in (2) is telescoping, the efficiency property (1) holds for any permutation \u03c0. Therefore, for any distribution over permutations w(\u03c0) with \u2211 \u03c0 w(\u03c0) = 1, the contributions\n\u03c6i = \u2211\n\u03c0\nw(\u03c0)\u03c6i(\u03c0) (4)\nstill satisfy (1). An obvious choice would be to take a uniform distribution w(\u03c0) = 1/n!. We then arrive at (with shorthand i for the singleton {i}):\n\u03c6i = \u2211\nS\u2286N\\i\n|S|!(n\u2212 |S| \u2212 1)! n! [v(S \u222a i)\u2212 v(S)] .\nBesides efficiency, the Shapley values uniquely satisfy three other desirable properties [27].\nLinearity: for two value functions v1 and v2, we have \u03c6i(\u03b11v1 + \u03b12v2) = \u03b11\u03c6i(v1) + \u03b12\u03c6i(v2). This guarantees that the Shapley value of a linear ensemble of models is a linear combination of the individual models\u2019 Shapley values.\nNull player (dummy): if v(S \u222a i) = v(S) for all S \u2286 N \\ i, then \u03c6i = 0. A feature that never contributes to the prediction (directly nor indirectly, see below) receives zero Shapley value.\nSymmetry: if v(S \u222a i) = v(S \u222a j) for all S \u2286 N \\ {i, j}, then \u03c6i = \u03c6j . Symmetry in this sense holds for marginal, conditional, and causal Shapley values alike.\nNote that symmetry here is defined w.r.t. to the contributions \u03c6i, not the function values f(x). As discussed in [8] (Section 3), conditioning by observation or intervention then does not break the symmetry property. For a non-uniform distribution of permutations as in [6], symmetry is lost, but efficiency, linearity, and null player still apply.\nReplacing conditioning by intervention with conventional conditioning by observation, i.e., averaging over P (XS\u0304 |xS) instead of P (XS\u0304 |do(XS = xS)) in (3), we arrive at the conditional Shapley values of [1, 18]. A third option is to ignore the feature values xS and take the unconditional, marginal distribution P (XS\u0304), which leads to the marginal Shapley values.\nUp until here, our active interventional interpretation of Shapley values coincides with that in [3, 8, 17]. However, from here on Janzing et al. [8] choose to ignore any dependencies between the features in the real world, by formally distinguishing between true features (corresponding to one of the data points) and the features plugged as input into the model. This leads to the conclusion that, in our notation, P (XS\u0304 |do(XS = xS)) = P (XS\u0304) for any subset S. As a result, any expectation under conditioning by intervention collapses to a marginal expectation and, in the interpretation of [3, 8, 17], interventional Shapley values simplify to marginal Shapley values. As we will see below, marginal Shapley values can only represent direct effects, which makes that \u2018root causes\u2019 with strong indirect effects (e.g. genetic markers) are ignored in the attribution, which is quite different from how humans tend to attribute causes [29]. In this paper, we choose not to make this distinction between the features in the real world and the inputs of the prediction model, but to explicitly take into account the causal relationships between the data in the real world to enhance the explanations. Since the term \u2018interventional\u2019 Shapley values has been coined for causal explanations of the prediction algorithm, ignoring causal relationships between the features in the real world, we will use the term \u2018causal\u2019 Shapley values for those that do attempt to incorporate these relationships using Pearl\u2019s do-calculus.\nThe asymmetric Shapley values introduced in [6] have the same objective: enhancing the explanation of the Shapley values by incorporating causal knowledge about the features in the real world. In\n[6], this knowledge is incorporated by choosing w(\u03c0) 6= 0 in (4) only for those permutations \u03c0 that are consistent with the causal structure between the features, i.e., are such that a known causal ancestor always precedes its descendants. On top of this, Frey et al. [6] apply standard conditioning by observation. In this paper we show that there is no need to resort to asymmetric Shapley values to incorporate causal knowledge: applying conditioning by intervention instead of conditioning by observation is sufficient. Choosing asymmetric Shapley values instead of symmetric ones can be considered orthogonal to choosing conditioning by observation versus conditioning by intervention. We will therefore refer to the approach of [6] as asymmetric conditional Shapley values, to contrast them with asymmetric causal Shapley values that implement both ideas."}, {"heading": "3 Decomposing Shapley values into direct and indirect effects", "text": "Our causal interpretation allows us to distinguish between direct and indirect effects of each feature on a model\u2019s prediction. This decomposition then also helps to understand the difference between marginal, symmetric, and asymmetric Shapley values. Going back to the contribution \u03c6i(\u03c0) for a permutation \u03c0 and feature i in (2) and using shorthand notation S = {j : j \u227a\u03c0 i} and S\u0304 = {j : j \u03c0 i}, we can decompose the total effect for this permutation into a direct and an indirect effect: \u03c6i(\u03c0) = E[f(XS\u0304 ,xS\u222ai)|do(XS\u222ai = xS\u222ai)]\u2212 E[f(XS\u0304\u222ai,xS)|do(XS = xS)] (total effect)\n= E[f(XS\u0304 ,xS\u222ai)|do(XS = xS)]\u2212 E[f(XS\u0304\u222ai,xS)|do(XS = xS)] + (direct effect) E[f(XS\u0304 ,xS\u222ai)|do(XS\u222ai = xS\u222ai)]\u2212 E[f(XS\u0304 ,xS\u222ai)|do(XS = xS)] (indirect effect)\n(5)\nThe direct effect measures the expected change in prediction when the stochastic feature Xi is replaced by its feature value xi, without changing the distribution of the other \u2018out-of-coalition\u2019 features. The indirect effect measures the difference in expectation when the distribution of the other \u2018out-of-coalition\u2019 features changes due to the additional intervention do(Xi = xi). The direct and indirect parts of Shapley values can then be computed as in (4): by taking a, possibly weighted, average over all permutations. Conditional Shapley values can be decomposed similarly by replacing conditioning by intervention with conditioning by observation in (5). For marginal Shapley values, there is no conditioning and hence no indirect effect: by construction marginal Shapley values can only represent direct effects. We will make use of this decomposition in the next section to clarify how different causal structures lead to different Shapley values."}, {"heading": "4 Shapley values for different causal structures", "text": "To illustrate the difference between the various Shapley values, we consider four causal models on two features. They are constructed such that they have the same P (X), with E[X2|x1] = \u03b1x1 and E[X1] = E[X2] = 0, but with different causal explanations for the dependency between X1 and X2. In the causal chain X1 could, for example, represent season, X2 temperature, and Y bike rental. The fork inverts the arrow between X1 and X2, where now Y may represent hotel occupation, X2 season, and X1 temperature. In the chain and the fork, different data points correspond to different days. For the confounder and the cycle, X1 and X2 may represent obesity and sleep apnea, respectively, and Y hours of sleep. The confounder model implements the assumption that obesity and sleep apnea have a common confounder Z, e.g., some genetic predisposition. The cycle, on the other hand, represents the more common assumption that there is a reciprocal effect, with obesity affecting sleep apnea and vice versa [22]. In the confounder and the cycle, different data points correspond to different subjects. We assume to have trained a linear model f(x1, x2) that happens to largely, or even completely to simplify the formulas, ignore the first feature, and boils down to the prediction function f(x1, x2) = \u03b2x2. Figure 1 shows the explanations provided by the various Shapley values for each of the causal models in this extreme situation. Derivations can be found in the supplement.\nSince in all cases there is no direct link between X1 and the prediction, the direct effect of X1 is always equal to zero. Similarly, any indirect effect ofX2 can only go throughX1 and hence must also be equal to zero. So, all we can expect is a direct effect from X2, proportional to \u03b2, and an indirect effect from X1 through X1, proportional to \u03b1 times \u03b2. Because of the sufficiency property, the direct and indirect effect always add up to the output \u03b2x2. This makes that, for all different combinations of causal structures and types of Shapley values, we end up with just three different explanation patterns, referred to as D, E, and R in Figure 1.\nD E R direct indirect direct indirect direct indirect\n\u03c61 0 0 0 12\u03b2\u03b1x1 0 \u03b2\u03b1x1 \u03c62 \u03b2x2 0 \u03b2x2 \u2212 12\u03b2\u03b1x1 0 \u03b2x2 \u2212 \u03b2\u03b1x1 0\nY\nX2\nX1\nChain\n\u03b2\n\u03b1\nY\nX2\nX1\nFork\n\u03b2\nY\nX2\nX1\nZ\nConfounder\n\u03b2\nY\nX2\nX1\nCycle\n\u03b2\nmarginal conditional causal\nsy m\nm et\nri c\nas ym\nm et\nri c\nsy m\nm et\nri c\nas ym\nm et\nri c\nChain D E R E R Fork D E D D D\nConfounder D E E D D Cycle D E E E E\nFigure 1: Direct and indirect Shapley values for four causal models with the same observational distribution over features (such that E[X1] = E[X2] = 0 and E[X2|x1] = \u03b1x1), yet a different causal structure. We assume a linear model that happens to ignore the first feature: f(x1, x2) = \u03b2x2. The bottom table gives for each of the four causal models on the left the marginal, conditional, and causal Shapley values, where the latter two are further split up in symmetric and asymmetric. Each letter in the bottom table corresponds to one of the patterns of direct and indirect effects detailed in the top table: \u2018direct\u2019 (D, only direct effects), \u2018evenly split\u2019 (E, credit for an indirect effect split evenly between the features), and \u2018root cause\u2019 (R, all credit for the indirect effect goes to the root cause). Shapley values that, we argue, do not provide a proper causal explanation, are underlined and indicated in red.\nTo argue which explanations make sense, we call upon classical norm theory [9]. It states that humans, when asked for an explanation of an effect, contrast the actual observation with a counterfactual, more normal alternative. What is considered normal, depends on the context. Shapley values can be given the same interpretation [20]: they measure the difference in prediction between knowing and not knowing the value of a particular feature, where the choice of what\u2019s normal translates to the choice of the reference distribution to average over when the feature value is still unknown.\nIn this perspective, marginal Shapley values as in [3, 8, 17] correspond to a very simplistic, counterintuitive interpretation of what\u2019s normal. Consider for example the case of the chain, with X1 representing season, X2 temperature, and Y bike rental, and two days with the same temperature of 13 degrees Celsius, one in fall and another in winter. Marginal Shapley values end up with the same explanation for the predicted bike rental on both days, ignoring that the temperature in winter is higher than normal for the time of year and in fall lower. Just like marginal Shapley values, symmetric conditional Shapley values as in [1] do not distinguish between any of the four causal structures. They do take into account the dependency between the two features, but then fail to acknowledge that an intervention on feature X1 in the fork and the confounder, does not change the distribution of X2.\nFor the confounder and the cycle, asymmetric Shapley values put X1 and X2 on an equal footing and then coincide with their symmetric counterparts. Asymmetric conditional Shapley values from [6] have no means to distinguish between the cycle and the confounder, unrealistically assigning credit to X1 in the latter case. Asymmetric and symmetric causal Shapley values do correctly treat the cycle and confounder cases.\nIn the case of a chain, asymmetric and symmetric causal Shapley values provide different explanations. Which explanation is to be preferred may well depend on the context. In our bike rental example, asymmetric Shapley values first give full credit to season for the indirect effect (here \u03b1\u03b2x1), subtracting this from the direct effect of the temperature to fulfill the sufficiency property (\u03b2x2 \u2212 \u03b1\u03b2x1). Symmetric causal Shapley values consider both contexts \u2013 one in which season is intervened upon before temperature, and one in which temperature is intervened upon before season \u2013\nand then average over the results in these two contexts. This symmetric strategy appears to better appeal to the theory dating back to [14], that humans sample over different possible scenarios (here: different orderings of the features) to judge causation. However, when dealing with a temporal chain of events, alternative theories (see e.g. [30]) suggest that humans have a tendency to attribute credit or blame foremost to the root cause, which seems closer in spirit to the explanation provided by asymmetric causal Shapley values.\nBy dropping the symmetry property, asymmetric Shapley values do pay a price: they are sensitive to the insertion of causal links with zero strength. As an example, consider a neural network trained to perfectly predict the XOR function on two binary variables X1 and X2. With a uniform distribution over all features and no further assumption w.r.t. the causal ordering ofX1 andX2, the Shapley values are \u03c61 = \u03c62 = 1/4 when the prediction f equals 1, and \u03c61 = \u03c62 = \u22121/4 for f = 0: completely symmetric. If we now assume that X1 preceeds X2 (and a causal strength of 0 to maintain the uniform distribution over features), all Shapley values stay the same, except for the asymmetric ones: these suddenly jump to \u03c61 = 0 and \u03c62 = 1/2 for f = 1, and \u03c61 = 0 and \u03c62 = \u22121/2 for f = 0. More details on this instability of asymmetric Shapley values can be found in the supplement, where we compare Shapley values of trained neural networks for varying causal strengths.\nTo summarize, unlike marginal and (both symmetric and asymmetric) conditional Shapley values, causal Shapley values provide sensible explanations that incorporate causal relationships in the real world. Asymmetric causal Shapley values may be preferable over symmetric ones when causality derives from a clear temporal order, whereas symmetric Shapley values have the advantage of being much less sensitive to model misspecifications."}, {"heading": "5 A practical implementation with causal chain graphs", "text": "In the ideal situation, a practitioner has access to a fully specified causal model that can be plugged in (3) to compute or sample from every interventional probability of interest. In practice, such a requirement is hardly realistic. In fact, even if a practitioner could specify a complete causal structure (including potential confounding) and has full access to the observational probability P (X), not every causal query need be identifiable (see e.g., [24]). Furthermore, requiring so much prior knowledge could be detrimental to the method\u2019s general applicability. In this section, we describe a pragmatic approach that is applicable when we have access to a (partial) causal ordering plus a bit of additional information to distinguish confounders from mutual interactions, and a training set to estimate (relevant parameters of) P (X). Our approach is inspired by [6], but extends it in various aspects: it provides a formalization in terms of causal chain graphs, applies to both symmetric and asymmetric Shapley values, and correctly distinguishes between dependencies that are due to confounding and mutual interactions.\nIn the special case that a complete causal ordering of the features can be given and that all causal relationships are unconfounded, P (X) satisfies the Markov properties associated with a directed acyclic graph (DAG) and can be written in the form\nP (X) = \u220f\nj\u2208N P (Xj |Xpa(j)) ,\nwith pa(j) the parents of node j. With no further conditional independences, the parents of j are all nodes that precede j in the causal ordering. For causal DAGs, we have the interventional formula [13]:\nP (XS\u0304 |do(XS = xS)) = \u220f\nj\u2208S\u0304 P (Xj |Xpa(j)\u2229S\u0304 ,xpa(j)\u2229S) , (6)\nwith pa(j) \u2229 T the parents of j that are also part of subset T . The interventional formula can be used to answer any causal query of interest.\nWhen we cannot give a complete ordering between the individual variables, but still a partial ordering, causal chain graphs [13] come to the rescue. A causal chain graph has directed and undirected edges. All features that are treated on an equal footing are linked together with undirected edges and become part of the same chain component. Edges between chain components are directed and represent causal relationships. See Figure 2 for an illustration of the procedure. The probability distribution\npartial causal ordering: {(1, 2), (3, 4, 5), (6, 7)} \u21d2 \u21d2\nX1 X2\nX3 X4\nX5\nX6 X7\n\u03c41\n\u03c42\n\u03c43\nX1 X2\nX3 X4\nX5\nX6 X7\nZ1\nZ2\n\u03c41\n\u03c42\n\u03c43\nFigure 2: From partial ordering to causal chain graph. Features on an equal footing are combined into a fully connected chain component. How to handle interventions within each component depends on the generative process that best explains the (surplus) dependencies. In this example, the dependencies in chain components \u03c41 and \u03c43 are assumed to be the result of a common confounder, and those in \u03c42 of mutual interactions.\nP (X) in a chain graph factorizes as a \u201cDAG of chain components\u201d:\nP (X) = \u220f\n\u03c4\u2208T P (X\u03c4 |Xpa(\u03c4)) ,\nwith each \u03c4 a chain component, consisting of all features that are treated on an equal footing.\nHow to compute the effect of an intervention depends on the interpretation of the generative process leading to the (surplus) dependencies between features within each component. If we assume that these are the consequence of marginalizing out a common confounder, intervention on a particular feature will break the dependency with the other features. We will refer to the set of chain components for which this applies as Tconfounding. The undirected part can also correspond to the equilibrium distribution of a dynamic process resulting from interactions between the variables within a component [13]. In this case, setting the value of a feature does affect the distribution of the variables within the same component. We refer to these sets of components as Tconfounding. Any expectation by intervention needed to compute the causal Shapley values can be translated to an expectation by observation, by making use of the following theorem (see the supplement for a more detailed proof and some corollaries linking back to other types of Shapley values as special cases).\nTheorem 1. For causal chain graphs, we have the interventional formula\nP (XS\u0304 |do(XS = xS)) = \u220f\n\u03c4\u2208Tconfounding\nP (X\u03c4\u2229S\u0304 |Xpa(\u03c4)\u2229S\u0304 ,xpa(\u03c4)\u2229S)\u00d7\n\u220f\n\u03c4\u2208Tconfounding\nP (X\u03c4\u2229S\u0304 |Xpa(\u03c4)\u2229S\u0304 ,xpa(\u03c4)\u2229S ,x\u03c4\u2229S) . (7)\nProof.\nP (XS\u0304 |do(XS = xS)) (1) =\n\u220f \u03c4\u2208T P (X\u03c4\u2229S\u0304 |Xpa(\u03c4)\u2229S\u0304 , do(XS = xS))\n(3) =\n\u220f \u03c4\u2208T P (X\u03c4\u2229S\u0304 |Xpa(\u03c4)\u2229S\u0304 , do(Xpa(\u03c4)\u2229S = xpa(\u03c4)\u2229S), do(X\u03c4\u2229S = x\u03c4\u2229S))\n(2) =\n\u220f \u03c4\u2208T P (X\u03c4\u2229S\u0304 |Xpa(\u03c4)\u2229S\u0304 ,xpa(\u03c4)\u2229S , do(X\u03c4\u2229S = x\u03c4\u2229S)) ,\n0 2500\n5000\n7500\n0 200 400 600 Days since 1 January 2011\nN um\nbe r\nof b\nik es\nr en\nte d\n0\n10\n20\n30\ntemp\n\u2212500\n0\n500\nM S\nV c\nos ye\nar\n\u2212500\n0\n500\n\u22121000 0 1000 MSV temp\nC S\nV te\nm p\n\u22121000 0 1000 CSV cosyear\nhum\nwindspeed\natemp\ntemp\nsinyear\ncosyear\ntrend\n\u22122000 \u22121000 0 1000 2000\nCausal Shapley value (impact on model output)\nLow High Scaled feature value\nFigure 3: Bike shares in Washington, D.C. in 2011-2012 (top left; colorbar with temperature in degrees Celsius). Sina plot of causal Shapley values for a trained XGBoost model, where the top three date-related variables are considered to be a potential cause of the four weather-related variables (right). Scatter plots of marginal (MSV) versus causal Shapley values (CSV) for temperature (temp) and one of the seasonal variables (cosyear) show that MSVs almost purely explain the predictions based on temperature, whereas CSVs also give credit to season (bottom left).\nwhere the number above each equal sign refers to the standard do-calculus rule from [24] that is applied. For a chain component with dependencies induced by a common confounder, rule (3) applies once more and yields P (X\u03c4\u2229S\u0304 |Xpa(\u03c4)\u2229S\u0304 ,xpa(\u03c4)\u2229S), whereas for a chain component with dependencies induced by mutual interactions, rule (2) again applies and gives P (X\u03c4\u2229S\u0304 |Xpa(\u03c4)\u2229S\u0304 ,xpa(\u03c4)\u2229S ,x\u03c4\u2229S)).\nTo compute these observational expectations, we can rely on the various methods that have been proposed to compute conditional Shapley values [1, 6]. Following [1], we will assume a multivariate Gaussian distribution for P (X) that we estimate from the training data. Alternative proposals include assuming a Gaussian copula distribution, estimating from the empirical (conditional) distribution (both from [1]) and a variational autoencoder [6]."}, {"heading": "6 Illustration on real-world data", "text": "To illustrate the difference between marginal and causal Shapley values, we consider the bike rental dataset from [5], where we take as features the number of days since January 2011 (trend), two cyclical variables to represent season (cosyear, sinyear), the temperature (temp), feeling temperature (atemp), wind speed (windspeed), and humidity (hum). As can be seen from the time series itself (top left plot in Figure 3), the bike rental is strongly seasonal and shows an upward trend. Data was randomly split in 80% training and 20% test set. We trained an XGBoost model for 100 rounds.\nWe adapted the R package SHAPR from [1] to compute causal Shapley values, which essentially boiled down to an adaptation of the sampling procedure so that it draws samples from the interventional conditional distribution (7) instead of from a conventional observational conditional distribution. The sina plot on the righthand side of Figure 3 shows the causal Shapley values calculated for the trained XGBoost model on the test data. For this simulation, we chose the partial order ({trend}, {cosyear, sinyear}, {all weather variables}), with confounding for the second component and no confounding for the third, to represent that season has an effect on weather, but that we have no clue how to represent the intricate relations between the various weather variables. The sina plot clearly shows the relevance of the trend and the season (in particular cosine of the year, which is -1 on January 1 and +1 on July 1). The scatter plots on the left zoom in on the causal (CSV) and marginal Shapley values (MSV) for cosyear and temp. The marginal Shapley values for cosyear vary\nover a much smaller range than the causal Shapley values for cosyear, and vice versa for the Shapley values for temp: where the marginal Shapley values explain the predictions predominantly based on temperature, the causal Shapley values give season much more credit for the higher bike rental in summer and the lower bike rental in winter. Sina plots for marginal and asymmetric conditional Shapley values can be found in the supplement.\nAsymmetric Causal Marginal\ncosyear temp cosyear temp cosyear temp\n\u22121000\n\u2212500\n0\n500\nS ha\npl ey\nv al\nue\ndate\n2012\u221210\u221209 2012\u221212\u221203\nFigure 4: Asymmetric (conditional), (symmetric) causal and marginal Shapley values for two different days, one in October (brown) and one in December (gray) with more or less the same temperature of 13 degrees Celsius. Asymmetric Shapley values focus on the root cause, marginal Shapley values on the more direct effect, and symmetric causal Shapley consider both for the more natural explanation. The difference between asymmetric (conditional, from [6]), (symmetric) causal, and marginal Shapley values clearly shows when we consider two days, October 10 and December 3, 2012, with more or less the same temperature of 13 and 13.27 degrees Celsius, and predicted bike counts of 6117 and 6241, respectively. The temperature and predicted bike counts are relatively low for October, yet high for December. The various Shapley values for cosyear and temp are shown in Figure 4. The marginal Shapley values provide more or less the same explanation for both days, essentially only considering the more direct effect temp. The asymmetric conditional Shapley values, which are almost indistinguishable from the asymmetric causal Shapley values in this case, put a huge emphasis on the \u2018root\u2019 cause cosyear. The (symmetric) causal Shapley values nicely balance the two extremes, giving credit to both season and temperature, to provide a sensible, but still different explanation for the two days."}, {"heading": "7 Discussion", "text": "In real-world systems, understanding why things happen typically implies a causal perspective. It means distinguishing between important, contributing factors and irrelevant side effects. Similarly, understanding why a certain instance leads to a given output by a complex algorithm asks for those features that carry a significant amount of information contributing to the final outcome. Our insight was to recognize the need to properly account for the underlying causal structure between the features in order to derive meaningful and relevant attributive properties in the context of a complex algorithm.\nFor that, this paper introduced causal Shapley values, a model-agnostic approach to split a model\u2019s prediction of the target variable for an individual data point into contributions of the features that are used as input to the model, where each contribution aims to estimate the total effect of that feature on the target and can be decomposed into a direct and an indirect effect. We contrasted causal Shapley values with (interventional interpretations of) marginal and (asymmetric variants of) conditional Shapley values. We proposed a novel algorithm to compute these causal Shapley values, based on causal chain graphs. All that a practitioner needs to provide is a partial causal order (as for asymmetric Shapley values) and a way to interpret dependencies between features that are on an equal footing. Existing code for computing conditional Shapley values is easily generalized to causal Shapley values, without additional computational complexity. Computing conditional and causal Shapley values can be considerably more expensive than computing marginal Shapley values due to the need to sample from conditional instead of marginal distributions, even when integrated with computationally efficient approaches such as KernelSHAP [19] and TreeExplainer [17].\nOur approach should be a promising step in providing clear and intuitive explanations for predictions made by a wide variety of complex algorithms, that fits well with natural human understanding and expectations. Additional user studies should confirm to what extent explanations provided by causal Shapley values align with the needs and requirements of practitioners in real-world settings. Similar ideas may also be applied to improve current approaches for (interactive) counterfactual explanations [33] and properly distinguish between direct and total effects of features on a model\u2019s prediction. If successful, causal approaches that better match human intuition may help to build much needed trust in the decisions and recommendations of powerful modern-day algorithms.\nBroader Impact\nOur research, which aims to provide an explanation for complex machine learning models that can be understood by humans, falls within the scope of explainable AI (XAI). XAI methods like ours can help to open up the infamous \u201cblack box\u201d of complicated machine learning models like deep neural networks and decision tree ensembles. A better understanding of the predictions generated by such models may provide higher trust [26], detect flaws and biases [12], higher accuracy [2], and even address the legal \u201cright for an explanation\u201d as formulated in the GDPR [32].\nDespite their good intentions, explanation methods do come with associated risks. Almost by definition, any sensible explanation of a complex machine learning system involves some simplification and hence must sacrifice some accuracy. It is important to better understand what these limitations are [11]. Model-agnostic general purpose explanation tools are often applied without properly understanding their limitations and over-trusted [10]. They could possibly even be misused just to check a mark in internal or external audits. Automated explanations can further give an unjust sense of transparency, sometimes referred to as the \u2018transparency fallacy\u2019 [4]: overestimating one\u2019s actual understanding of the system. Last but not least, tools for explainable AI are still mostly used as an internal resource by engineers and developers to identify and reconcile errors [2].\nCausality is essential to understanding any process and system, including complex machine learning models. Humans have a strong tendency to reason about their environment and to frame explanations in causal terms [28, 16] and causal-model theories fit well to how humans, for example, classify objects [25]. In that sense, explanation approaches like ours, that appeal to a human\u2019s capability for causal reasoning should represent a step in the right direction [21].\nAcknowledgments and Disclosure of Funding\nThis research has been partially financed by the Netherlands Organisation for Scientific Research (NWO), under project 617.001.451."}, {"heading": "2 Shapley values for linear models", "text": "We will make use of the do-calculus rules above to derive the causal Shapley values for the four different models in Figure 1 in the main text. To this end, we consider the three models in Figure 1 that predict f (x1, x2) = \u03b21x1 + \u03b22x2 for general values of \u03b21 and \u03b22. All three models have the same observational probability distribution, with E[Xi] = x\u0304i and E[X3\u2212i|Xi = xi] = \u03b1ixi, for i = 1, 2, yet different causal structures. We will arrive at the main text\u2019s results for the \u2018chain\u2019, \u2018confounder\u2019, and \u2018cycle\u2019 by setting \u03b21 = 0, whereas for the \u2018fork\u2019 we set \u03b22 = 0 and swap the two indices. We then further need to take x\u03041 = x\u03042 = 0, and \u03b1 = \u03b12.\nFollowing the definitions in the main text, the contribution of feature i given permutation \u03c0 is the difference in value function before and after setting the feature to its value:\n\u03c6i(\u03c0) = v({ j : j \u03c0 i}) \u2212 v({ j : j \u227a\u03c0 i}) , (1) with value function\nv(S ) = E [ f (X)|do(XS = xS )] = \u222b dXS\u0304 P(XS\u0304 |x\u0302S ) f (XS\u0304 , xS ) , (2)\nwhere we use shorthand x\u0302 for do(X = x). Combining these two definitions and substituting f (x) =\u2211 i \u03b2ixi, we obtain\n\u03c6i(\u03c0) = \u03b2i ( xi \u2212 E[Xi|x\u0302 j: j\u227a\u03c0i] ) + \u2211\nk \u03c0i \u03b2k\n( E[Xk |x\u0302 j: j \u03c0i] \u2212 E[Xk |x\u0302 j: j\u227a\u03c0i] ) .\nThe first term corresponds to the direct effect, the second one to the indirect effect. Symmetric causal Shapley values will follow by averaging these contributions for the two possible permutations \u03c0 = (1, 2) and \u03c0 = (2, 1). Conditional Shapley values result when replacing conditioning by intervention with conventional conditioning by observation, marginal Shapley values by not conditioning at all.\nTo start with the latter, we immediately see that for marginal Shapley values the indirect effect vanishes and the direct effect simplifies to\n\u03c6i = \u03c6i(\u03c0) = \u03b2i(xi \u2212 E[Xi]) = \u03b2i(xi \u2212 x\u0304i) , as also derived in [1].\nFor symmetric conditional Shapley values, we do get different contributions for the two different permutations, but by definition still the same for the three different models:\n\u03c61(1, 2) = \u03b21(x1 \u2212 E[X1]) + \u03b22(E[X2|x1] \u2212 E[X2]) = \u03b21(x1 \u2212 x\u03041) + \u03b22\u03b11(x1 \u2212 x\u03041) \u03c62(1, 2) = \u03b22(x2 \u2212 E[X2|x1]) = \u03b22(x2 \u2212 x\u03042) \u2212 \u03b22\u03b11(x1 \u2212 x\u03041) . (3)\nHere the first term in the contribution for the first feature corresponds to the direct effect and the second term to the indirect effect. The contribution for the second feature only consists of a direct\neffect. The contributions for the other permutation follow by swapping the indices and the final Shapley values by averaging to arrive at the symmetric conditional Shapley values\n\u03c61 = \u03b21(x1 \u2212 x\u03041) \u2212 12\u03b21\u03b12(x2 \u2212 x\u03042) + 1 2 \u03b22\u03b11(x1 \u2212 x\u03041) \u03c62 = \u03b22(x2 \u2212 x\u03042) \u2212 12\u03b22\u03b11(x1 \u2212 x\u03041) + 1 2 \u03b21\u03b12(x2 \u2212 x\u03042) , (4)\nwhere now the first two terms constitute the direct effect and the third term the indirect effect.\nThe asymmetric conditional Shapley values consider both permutations for the confounder and the cycle, and hence are equivalent to the symmetric Shapley values for those models. Yet for the chain, they only consider the permutation \u03c0(1, 2) and thus yield \u03c6 = \u03c6(1, 2) from (3).\nTo go from the symmetric conditional Shapley values to the causal symmetric Shapley values, we follow the same line of reasoning, but have to replace E[X2|x1] by E[X2|x\u03021] and E[X1|x2] by E[X1|x\u03022]. Table 1 tells whether the expectations under conditioning by intervention reduce to expectations under conditioning by observation (because of the second rule of do-calculus above) or to marginal expectations (because of the third rule). For the chain we have\nP(X2|x\u03021) = P(X2|x1) since X2 \u03c3 y G IX1 | X1 (rule 2), yet P(X1|x\u03022) = P(X1) since X1 \u03c3 y G IX2 (rule 3),\nfor the confounder\nP(X2|x\u03021) = P(X2) since X2 \u03c3 y G IX1 and P(X1|x\u03022) = P(X1) since X1 \u03c3 y G IX2 (rule 3),\nand for the cycle\nP(X2|x\u03021) = P(X2|x1) since X2 \u03c3 y G IX1 | X1 and P(X1|x\u03022) = P(X1|x2) since X1 \u03c3 y G IX2 | X2 (rule 2). Consequently, for the confounder the symmetric and asymmetric causal Shapley values coincide with the marginal Shapley values (consistent with [4]) and for the cycle with the symmetric conditional Shapley values from (4). For the chain, the causal symmetric Shapley values become\n\u03c61(1, 2) = \u03b21(x1 \u2212 x\u03041) + 12\u03b22\u03b11(x1 \u2212 x\u03041)\n\u03c62(1, 2) = \u03b22(x2 \u2212 x\u03042) \u2212 12\u03b22\u03b11(x1 \u2212 x\u03041) , (5) where the asymmetric causal Shapley values coincides with the asymmetric conditional Shapley values from (5).\nCollecting all results and setting x\u03041 = x\u03042 = \u03b21 = 0, \u03b22 = \u03b2, and \u03b11 = \u03b1 (after swapping the indices for the \u2018fork\u2019), we arrive at the Shapley values reported in Figure 1 in the main text. Note that for most Shapley values, the indirect effect for the second feature vanishes because we chose to set \u03b21 = 0. The exceptions, apart from the marginal Shapley values, are the causal Shapley values for the chain and the confounder, as well as the asymmetric conditional Shapley values for the chain: these show no indirect effect for the second feature even for nonzero \u03b21."}, {"heading": "3 Proofs and corollaries on causal chain graphs", "text": "In this section we expand on the proof of Theorem 1 in the main text and add some corollaries to link back to other approaches for computing Shapley values.\nThe probability distribution for a causal chain graph boils down to a directed acyclic graph of chain components:\nP(X) = \u220f\n\u03c4\u2208T P(X\u03c4|Xpa(\u03c4)) . (6)\nFor each (fully connected) chain component, we further need to specify whether (surplus) dependencies within the component are due to confounding or due to mutual interactions. Given this information, we can turn any causal query into an observational distribution with the following interventional formula. Theorem 1. For causal chain graphs, we have the interventional formula\nP(XS\u0304 |do(XS = xS )) = \u220f\n\u03c4\u2208Tconfounding P(X\u03c4\u2229S\u0304 |Xpa(\u03c4)\u2229S\u0304 , xpa(\u03c4)\u2229S ) \u00d7\n\u220f\n\u03c4\u2208Tconfounding\nP(X\u03c4\u2229S\u0304 |Xpa(\u03c4)\u2229S\u0304 , xpa(\u03c4)\u2229S , x\u03c4\u2229S ) . (7)\nProof. Plugging in (6) and using shorthand x\u0302 = do(X = x), we obtain\nP(XS\u0304 |x\u0302S ) = P(X|x\u0302S ) = \u220f\n\u03c4\u2208T P(X\u03c4|X\u03c4\u2032\u227aG\u03c4, x\u0302S ) (1) =\n\u220f \u03c4\u2208T P(X\u03c4|Xpa(\u03c4), x\u0302S ) = \u220f \u03c4\u2208T P(X\u03c4\u2229S\u0304 |Xpa(\u03c4)\u2229S\u0304 , x\u0302S ) ,\nwhere in the second step we made use of do-calculus rule (1): the conditional independencies in the causal chain graph G are preserved when we intervene on some of the variables. Now rule (3) tells us that we can ignore any interventions from nodes in components further down the causal chain graph as well as those from higher up that are shielded by the direct parents:\nP(X\u03c4\u2229S\u0304 |Xpa(\u03c4)\u2229S\u0304 , x\u0302S ) (3)= P(X\u03c4\u2229S\u0304 |Xpa(\u03c4)\u2229S\u0304 , x\u0302pa(\u03c4)\u2229S , x\u0302\u03c4\u2229S ) . Rule (2) then states that conditioning by intervention upon variables higher up in the causal chain graph is equivalent to conditioning by observation:\nP(X\u03c4\u2229S\u0304 |Xpa(\u03c4)\u2229S\u0304 , x\u0302pa(\u03c4)\u2229S , x\u0302\u03c4\u2229S ) (2)= P(X\u03c4\u2229S\u0304 |Xpa(\u03c4)\u2229S\u0304 , xpa(\u03c4)\u2229S , x\u0302\u03c4\u2229S ) . For a chain component with dependencies induced by a common confounder, rule (3) applies once more and makes that we can ignore the interventions:\nP(X\u03c4\u2229S\u0304 |Xpa(\u03c4)\u2229S\u0304 , xpa(\u03c4)\u2229S , x\u0302\u03c4\u2229S ) = P(X\u03c4\u2229S\u0304 |Xpa(\u03c4)\u2229S\u0304 , xpa(\u03c4)\u2229S ) . For a chain component with dependencies induced by mutual interactions, rule (2) again applies and allows us to replace conditioning by intervention with conditioning by observation:\nP(X\u03c4\u2229S\u0304 |Xpa(\u03c4)\u2229S\u0304 , xpa(\u03c4)\u2229S , x\u0302\u03c4\u2229S ) = P(X\u03c4\u2229S\u0304 |Xpa(\u03c4)\u2229S\u0304 , xpa(\u03c4)\u2229S , x\u03c4\u2229S )) .\nAlgorithm 1 provides pseudocode on how to estimate the value function v(S ) by drawing samples from the interventional probability (7). It assumes that a user has specified a partial causal ordering of the features, which is translated to a chain graph with components T , and for each (non-singleton) component \u03c4 whether or not surplus dependencies are the result of confounding. Other prerequisites include access to the model f (), the feature vector x, (a procedure to sample from) the observational probability distribution P(X), and the number of samples Nsamples.\nTheorem 1 connects to observations made and algorithms proposed in recent papers. Corollary 1. With all features combined in a single component and all dependencies induced by confounding, as in [4], causal Shapley values are equivalent to marginal Shapley values.\nProof. With just a single confounded component \u03c4, pa(\u03c4) = \u2205 and (7) reduces to P(XS\u0304 ). Corollary 2. With all features combined in a single component and all dependencies induced by mutual interactions, causal Shapley values are equivalent to conditional Shapley values as proposed in [1].\nProof. With just a single non-confounded component \u03c4, pa(\u03c4) = \u2205 and (7) reduces to P(XS\u0304 |xS ). Corollary 3. When we only take into account permutations that match the causal ordering and assume that all dependencies within chain components are induced by mutual interactions, the resulting asymmetric causal Shapley values are equivalent to the asymmetric conditional Shapley values as defined in [3].\nAlgorithm 1 Compute the value function v(S ) under conditioning by intervention. 1: function ValueFunction(S ) 2: for k \u2190 1 to Nsamples do 3: for all j\u2190 1 to |T | do . run over all chain components in their causal order 4: if confounding(\u03c4 j) then 5: for all i \u2208 \u03c4 j \u2229 S\u0304 do 6: Sample x\u0303(k)i \u223c P(Xi|x\u0303(k)pa(\u03c4 j)\u2229S\u0304 , xpa(\u03c4 j)\u2229S\u0304 ) . can be drawn independently 7: end for 8: else 9: Sample x\u0303(k)\n\u03c4 j\u2229S\u0304 \u223c P(X\u03c4 j\u2229S\u0304 |x\u0303 (k) pa(\u03c4 j)\u2229S\u0304 , xpa(\u03c4 j)\u2229S\u0304 , x\u03c4 j\u2229S ) . e.g., Gibbs sampling\n10: end if 11: end for 12: end for\n13: v\u2190 1 Nsamples\nNsamples\u2211\nk=1\nf (xS , x\u0303(k)S\u0304 )\n14: return v 15: end function\nProof. Following [3], asymmetric Shapley values only include those permutations \u03c0 for which i \u227a\u03c0 j for all known ancestors i of descendants j in the causal graph. For those permutations, we are guaranteed to have \u03c4 \u227aG \u03c4\u2032 for all \u03c4, \u03c4\u2032 \u2208 T such that \u03c4 \u2229 S , \u2205, \u03c4\u2032 \u2229 S\u0304 , \u2205. That is, the chain components that contain features from S are never later in the causal ordering of the chain graph G than those that contain features from S\u0304 . We then have\nP(XS\u0304 |xS ) = \u220f\n\u03c4\u2208T P(X\u03c4\u2229S\u0304 |Xpa(\u03c4)\u2229S\u0304 , xS ) =\n\u220f \u03c4\u2208T P(X\u03c4\u2229S\u0304 |Xpa(\u03c4)\u2229S\u0304 , xpa(\u03c4)\u2229S , x\u03c4\u2229S ) = P(XS\u0304 |x\u0302S ) ,\nwhere in the last step we used interventional formula (7) in combination with the fact thatTconfounding = \u2205."}, {"heading": "4 Additional illustrations on the bike rental data", "text": "Figure 2 shows sina plots for asymmetric conditional Shapley values (left) and marginal Shapley values (right), to be compared with the sina plots for symmetric causal Shapley values in Figure 3 of the main text. In this case, the sina plots for asymmetric causal Shapley values are virtually indistinguishable from those for the asymmetric conditional Shapley values.\nIt can be seen that the marginal Shapley values strongly focus on temperature, largely ignoring the seasonal variables. The asymmetric Shapley values do the opposite: they focus on the seasonal variables, in particular cosyear and put much less emphasis on the temperature variables."}, {"heading": "5 Comparing symmetric and asymmetric Shapley values on the XOR problem", "text": "We consider the standard XOR problem with binary features X1 and X2 and binary output Y:\nX1 X2 Y 0 0 0 0 1 1 1 0 1 1 1 0\nWe generate a dataset of n samples by drawing features and corresponding outputs with probabilities pi j = P(X1 = i, X2 = j). We will choose p00 = p11 = 14 (1 + ) and p01 = p10 = 1 4 (1 \u2212 ). With\n> 0, the probability of the two features having the same values is larger than the probability of them having different values. p\u0302i j is the same probability estimated from the data, e.g., by computing the\nfrequencies of the four input combinations. We train a neural network on the generated data, which yields a function f\u0302 (X1, X2) hopefully closely approximating the correct XOR function. The parameter captures the dependency between the two features and can be interpreted as a measure of the causal strength when the two features are causally related.\nWe will now compute the various Shapley values for the data point (X1, X2) = (0, 0). The value functions with all features either \u2018out-of-coalition\u2019 or \u2018in-coalition\u2019 are the same for all Shapley values:\n\u03bd({}) = E [ f (X)] = \u2211\ni, j\np\u0302i j f\u0302 (i, j) \u2248 12(1 \u2212 )\n\u03bd({1, 2}) = f\u0302 (0, 0) \u2248 0 , where we use the convention that the Shapley values computed from the fitted probabilities and learned neural network appear before the \u2248-sign, and those that we obtain when the fitted probabilities equal the probabilities used to generate the data and when the learned neural network equals the XOR function after the \u2248-sign. The value functions for the case that one input is \u2018in-coalition\u2019 and the other \u2018out-of-coalition\u2019 does depend on the type of Shapley value under consideration. For the marginal Shapley values we get\n\u03bd({1}) = E [ f (0, X2)] = \u2211\nj\n \u2211\ni\np\u0302i j  f\u0302 (0, j) \u2248 1 2\n\u03bd({2}) = E [ f (X1, 0)] = \u2211\ni\n \u2211\nj\np\u0302i j  f\u0302 (i, 0) \u2248 1 2 , (8)\nyet for the conditional Shapley values\n\u03bd({1}) = E [ f (0, X2)|X1 = 0] = \u2211\nj\np\u03020 j\u2211 i p\u0302i j f\u0302 (0, j) \u2248 1 2 (1 \u2212 )\n\u03bd({2}) = E [ f (X1, 0)|X2 = 0] = \u2211\ni\np\u0302i0\u2211 j p\u0302i j f\u0302 (i, 0) \u2248 1 2 (1 \u2212 ) . (9)\nThe value functions for the causal Shapley values depend on the presumed causal model that generates the dependencies. In case the dependencies are assumed to be the result of confounding, we get the value functions in (8) as for the marginal Shapley values and when the dependencies are assumed to\nbe the result of mutual interaction the value functions in (9) as for the conditional Shapley values. The more interesting case is when we assume a causal chain, e.g., X1 \u2192 X2:\n\u03bd({1}) = E [ f (0, X2)|do(X1 = 0)] = E [ f (0, X2)|X1 = 0] = \u2211\nj\np\u03020 j\u2211 i p\u0302i j f\u0302 (0, j) \u2248 1 2 (1 \u2212 )\n\u03bd({2}) = E [ f (X1, 0)|do(X2 = 0)] = E [ f (X1, 0)] = \u2211\ni\n \u2211\nj\np\u0302i j  f\u0302 (i, 0) \u2248 1 2 , (10)\nand the same with indices 1 and 2 interchanged for the causal chain X2 \u2192 X1. Given these value functions, we can now compute the various Shapley values. For marginal and symmetric Shapley values we have\n\u03c61 = 1 2 [\u03bd({1}) \u2212 \u03bd({})] + 1 2 [\u03bd({1, 2}) \u2212 \u03bd({2})] \u03c62 = 1 2 [\u03bd({2}) \u2212 \u03bd({})] + 1 2\n[\u03bd({1, 2}) \u2212 \u03bd({1}]) , whereas for asymmetric Shapley values, assuming the causal chain X1 \u2192 X2,\n\u03c61 = \u03bd({1}) \u2212 \u03bd({}) \u03c62 = \u03bd({1, 2}) \u2212 \u03bd({1}) ,\nand the same with indices 1 and 2 interchanged for the causal chain X2 \u2192 X1. With the expressions above, we can compute the various Shapley values based on a learned neural network and the actual frequencies of the generated feature combinations and compare those with the theoretical values obtained when the estimated frequencies equal the probabilities used to generate the\ndata and the neural network indeed managed to learn the XOR function. For the latter we distinguish the following cases.\nidentical: \u03c61 = \u03c62 \u2248 14 \u2212 14 . This applies to marginal, symmetric conditional, symmetric causal assuming confounding, symmetric causal assuming mutual interaction.\nsymmetric causal: \u03c61 \u2248 \u2212 14 and \u03c62 \u2248 12 \u2212 14 assuming the causal chain X1 \u2192 X2 and vice versa for X1 \u2192 X2.\nasymmetric: \u03c61 \u2248 0 and \u03c62 \u2248 12 \u2212 12 assuming the causal chain X1 \u2192 X2 and vice versa for X1 \u2192 X2. These apply both to asymmetric conditional and asymmetric causal.\nIn this example, symmetric causal Shapley values are clearly to be preferred over asymmetric causal Shapley values for small causal strengths. Inserting a causal link with zero strength ( = 0), asymmetric Shapley values jump from the symmetric \u03c61 = \u03c62 \u2248 \u2212 14 to the completely asymmetric \u03c61 \u2248 0 and \u03c62 \u2248 \u2212 12 , assigning all credit to the second feature, even though the first feature in reality does not affect the second feature at all. Symmetric Shapley values, on the other hand, are insensitive to the insertion of a causal link with zero strength: in the limit \u2192 0 symmetric causal Shapley values correctly converge to marginal Shapley values.\nFigure 3 shows the results of a series of simulations, computing different Shapley values for trained neural networks and comparing these to the theoretical values. The discontinuity of asymmetric Shapley values (conditional and causal asymmetric Shapley values are identical in this example) is most clearly seen in the third row, showing the difference between the Shapley values for X1 and X2. Symmetric conditional Shapley values do not distinguish between the Shapley values for X1 and X2 for any causal strength , whereas the symmetric causal Shapley values are identical for = 0 and then slowly start to deviate for larger values of ."}, {"heading": "6 Shapley values for predicting dementia", "text": "We consider the Alzheimer\u2019s disease data set obtained from the Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI) database (http://adni.loni.usc.edu). The primary goal of ADNI has been to test whether serial magnetic resonance imaging (MRI), positron emission tomography (PET), other biological markers, and clinical and neuropsychological assessment can be combined to measure the progression of mild cognitive impairment and early Alzheimer\u2019s disease. As features we consider age (age), gender (gender), education level (pteducat), fudeoxyglucose (FDG), amyloid beta (ABETA), phosphorylated tau (PTAU), and the number of apolipoprotein alleles (APOE4). Data was normalised and randomly split in 80% training and 20% test set. We consider a binary classification problem, where we grouped together patients with mild cognitive impairment and early Alzheimer\u2019s disease, to distinguish these from the healthy \u201ccognitive normal\u201d subjects. We trained a multi-layered perceptron with five hidden units.\nTo compute the causal Shapley values, we chose the partial order ({gender, APOE4, age, pteducat}, {ABETA}, {FDG, PTAU}), in line with the \u201cgold standard\u201d causal graph from [6]. We assume confounding in the first and third component. Since interventional expectations over variables in the first component simplify to marginal expectations, we can sample the discrete variables gender and APOE4 from their empirical distributions. All other variables are sampled from conditional Gaussian distributions.\nThe sina plots in Figure 4 show the marginal, symmetric causal, and asymmetric causal Shapley values, respectively, for the predicted probability of having dementia or mild cognitive impairment. As the dependencies among the features are relatively weak, the marginal and symmetric causal Shapley values are quite similar. The asymmetric causal Shapley values for different values of APOE4, a known risk factor of ABETA, are more clearly separated than those for the marginal and symmetric causal Shapley values. The asymmetric Shapley values provide an explanation with a relatively strong focus on the root cause APOE4, which first gets all the credit for the indirect effect through ABETA.\nAcknowledgments and Disclosure of Funding\nADNI data collection and sharing was funded by the Alzheimer\u2019s Disease Neuroimaging Initiative (ADNI) (National Institutes of Health Grant U01 AG024904) and DOD ADNI (Department of Defense award number W81XWH-12-2-0012). See http://adni.loni.usc.edu/wp-content/ themes/freshnews-dev-v2/documents/policy/ADNI_Data_Use_Agreement.pdf, section 12, for further contributions and http://adni.loni.usc.edu/wp-content/uploads/how_to_ apply/ADNI_Acknowledgement_List.pdf for a complete listing of ADNI investigators."}], "title": "Causal Shapley Values: Exploiting Causal Knowledge to Explain Individual Predictions of Complex Models", "year": 2020}