{"abstractText": "With machine learning models being used for more sensitive applications, we rely on interpretability methods to prove that no discriminating attributes were used for classification. A potential concern is the so-called \"fair-washing\" manipulating a model such that the features used in reality are hidden and more innocuous features are shown to be important instead. In our work we present an effective defence against such adversarial attacks on neural networks. By a simple aggregation of multiple explanation methods, the network becomes robust against manipulation. This holds even when the attacker has exact knowledge of the model weights and the explanation methods used.", "authors": [{"affiliations": [], "name": "Laura Rieger"}, {"affiliations": [], "name": "Lars Kai Hansen"}], "id": "SP:63d93c83e44e4a7286845a91b084831c08dd9864", "references": [{"authors": ["J. Adebayo", "J. Gilmer", "M. Muelly", "I. Goodfellow", "M. Hardt", "B. Kim"], "title": "Sanity checks for saliency maps", "venue": "In Advances in Neural Information Processing Systems,", "year": 2018}, {"authors": ["U. A\u00efvodji", "H. Arai", "O. Fortineau", "S. Gambs", "S. Hara", "A. Tapp"], "title": "Fairwashing: the risk of rationalization", "year": 1901}, {"authors": ["M. Ancona", "E. Ceolini", "C. Oztireli", "M. Gross"], "title": "Towards better understanding of gradient-based attribution methods for deep neural networks", "venue": "In 6th International Conference on Learning Representations (ICLR", "year": 2018}, {"authors": ["S. Bach", "A. Binder", "G. Montavon", "F. Klauschen", "M\u00fcller", "K.-R", "W. Samek"], "title": "On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation", "venue": "PloS one,", "year": 2015}, {"authors": ["U. Bhatt", "A. Weller", "J.M. Moura"], "title": "Evaluating and aggregating feature-based model explanations", "venue": "arXiv preprint arXiv:2005.00631,", "year": 2020}, {"authors": ["Chang", "C.-H", "E. Creager", "A. Goldenberg", "D. Duvenaud"], "title": "Explaining image classifiers by counterfactual generation", "year": 2018}, {"authors": ["Dombrowski", "A.-K", "M. Alber", "C.J. Anders", "M. Ackermann", "M\u00fcller", "K.-R", "P. Kessel"], "title": "Explanations can be manipulated and geometry is to blame", "year": 1906}, {"authors": ["S. Geman", "E. Bienenstock", "R. Doursat"], "title": "Neural networks and the bias/variance dilemma", "venue": "Neural computation,", "year": 1992}, {"authors": ["A. Ghorbani", "A. Abid", "J. Zou"], "title": "Interpretation of neural networks is fragile", "venue": "In Proceedings of the AAAI Conference on Artificial Intelligence,", "year": 2019}, {"authors": ["L.K. Hansen", "L. Rieger"], "title": "Interpretability in intelligent systems\u2013a new concept? In Explainable AI: Interpreting", "venue": "Explaining and Visualizing Deep Learning,", "year": 2019}, {"authors": ["J. Heo", "S. Joo", "T. Moon"], "title": "Fooling neural network interpretations via adversarial model manipulation", "venue": "arXiv preprint arXiv:1902.02041,", "year": 2019}, {"authors": ["B. Kim", "M. Wattenberg", "J. Gilmer", "C. Cai", "J. Wexler", "F Viegas"], "title": "Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav)", "venue": "In International Conference on Machine Learning,", "year": 2018}, {"authors": ["Kindermans", "P.-J", "S. Hooker", "J. Adebayo", "G. Brain", "M. Alber", "K.T. Sch\u00fctt", "S. D\u00e4hne", "D. Erhan", "B. Kim"], "title": "The (un)reliability of saliency methods", "venue": "In Proceedings Workshop on Interpreting, Explaining and Visualizing Deep Learning (at NIPS),", "year": 2017}, {"authors": ["F. Liao", "M. Liang", "Y. Dong", "T. Pang", "X. Hu", "J. Zhu"], "title": "Defense against adversarial attacks using high-level representation guided denoiser", "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "year": 2018}, {"authors": ["S.M. Lundberg", "Lee", "S.-I"], "title": "A unified approach to interpreting model predictions", "venue": "In Advances in Neural Information Processing Systems,", "year": 2017}, {"authors": ["S. Mohseni", "E.D. Ragan"], "title": "A human-grounded evaluation benchmark for local explanations of machine learning", "venue": "arXiv preprint arXiv:1801.05075,", "year": 2018}, {"authors": ["G. Montavon", "S. Lapuschkin", "A. Binder", "W. Samek", "M\u00fcller", "K.-R"], "title": "Explaining nonlinear classification decisions with deep taylor decomposition", "venue": "Pattern Recognition,", "year": 2017}, {"authors": ["T. Pang", "K. Xu", "C. Du", "N. Chen", "J. Zhu"], "title": "Improving adversarial robustness via promoting ensemble diversity", "year": 1901}, {"authors": ["M.T. Ribeiro", "S. Singh", "C. Guestrin"], "title": "Why should i trust you?: Explaining the predictions of any classifier", "venue": "In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining,", "year": 2016}, {"authors": ["L. Rieger", "P. Chormai", "G. Montavon", "L.K. Hansen", "M\u00fcller", "K.-R"], "title": "Structuring Neural Networks for More Explainable Predictions", "year": 2018}, {"authors": ["R.R. Selvaraju", "M. Cogswell", "A. Das", "R. Vedantam", "D. Parikh", "D. Batra"], "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization", "venue": "IEEE International Conference on Computer Vision (ICCV),", "year": 2017}, {"authors": ["A. Shrikumar", "P. Greenside", "A. Kundaje"], "title": "Learning important features through propagating activation differences", "venue": "In International Conference on Machine Learning,", "year": 2017}, {"authors": ["K. Simonyan", "A. Zisserman"], "title": "Very deep convolutional networks for large-scale image recognition", "venue": "arXiv preprint arXiv:1409.1556,", "year": 2014}, {"authors": ["K. Simonyan", "A. Vedaldi", "A. Zisserman"], "title": "Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps", "year": 2013}, {"authors": ["J. Springenberg", "A. Dosovitskiy", "T. Brox", "M. Riedmiller"], "title": "Striving for simplicity: The all convolutional net", "venue": "In ICLR (workshop track),", "year": 2014}, {"authors": ["M. Sundararajan", "A. Taly", "Q. Yan"], "title": "Axiomatic attribution for deep networks", "venue": "In International Conference on Machine Learning,", "year": 2017}, {"authors": ["F. Tram\u00e8r", "A. Kurakin", "N. Papernot", "I. Goodfellow", "D. Boneh", "P. McDaniel"], "title": "Ensemble adversarial training: Attacks and defenses", "venue": "arXiv preprint arXiv:1705.07204,", "year": 2017}, {"authors": ["Yeh", "C.-K", "Hsieh", "C.-Y", "A. Suggala", "D.I. Inouye", "P.K. Ravikumar"], "title": "On the (in) fidelity and sensitivity of explanations", "venue": "In Advances in Neural Information Processing Systems,", "year": 2019}, {"authors": ["M.D. Zeiler", "R. Fergus"], "title": "Visualizing and understanding convolutional networks", "venue": "In European Conference on Computer Vision,", "year": 2014}, {"authors": ["Q. Zhang", "Y. Nian Wu", "Zhu", "S.-C"], "title": "Interpretable convolutional neural networks", "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "year": 2018}, {"authors": ["X. Zhang", "N. Wang", "H. Shen", "S. Ji", "X. Luo", "T. Wang"], "title": "Interpretable deep learning under fire", "venue": "arXiv preprint arXiv:1812.00891,", "year": 2018}, {"authors": ["B. Zhou", "A. Khosla", "A. Lapedriza", "A. Oliva", "A. Torralba"], "title": "Learning deep features for discriminative localization", "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "year": 2016}, {"authors": ["L.M. Zintgraf", "T.S. Cohen", "T. Adel", "M. Welling"], "title": "Visualizing deep neural network decisions: Prediction difference analysis", "venue": "In ICLR,", "year": 2017}], "sections": [{"text": "In our work we present an effective defence against such adversarial attacks on neural networks. By a simple aggregation of multiple explanation methods, the network becomes robust against manipulation. This holds even when the attacker has exact knowledge of the model weights and the explanation methods used."}, {"heading": "1. Introduction", "text": "In recent years machine learning algorithms have become more complex and are used for more important decisions. Since models, especially neural networks, are trained with large amounts of data, it is hard to oversee just what is hidden in the data and what correlations the model picks up on. Explainability methods present a solution for this (Hansen & Rieger, 2019). By looking at what features of the input were important for a classification, we can make sure that the classification is aligned with our ethical convictions and understanding of the task.\nIt follows that there are many reasons why someone might want to manipulate an explanation, referred to as \"fairwashing\" (A\u00efvodji et al., 2019). For example, a company may want to hide that they use discriminatory practices in their hiring or someone may want to hide adversarial attacks on machine learning algorithms. Before explainability methods can be used and relied on in practice, we need to evaluate the the risk for this and find effective defences. Previous works\n1DTU Compute, Technical University Denmark, 2800 Kgs. Lyngby, Denmark. Correspondence to: Laura Rieger <lauri@dtu.dk>."}, {"heading": "2020 ICML Workshop on Human Interpretability in Machine", "text": "Learning (WHI 2020). Copyright 2020 by the author(s).\nFigure 1. Explanation methods (here Guided Backpropagation) are very vulnerable to adversarial attacks. Our method, AGG-Mean, presents a simple but effective defence.\nhave shown that explainability methods are remarkably brittle against adversarial attacks. In practice, an attacker can effectively manipulate the explanation at will without any visual changes to the input that a human would pick up on (Dombrowski et al., 2019; Ghorbani et al., 2019).\nWe propose a simple way to ward against this potential security risk and make explainability methods more viable in deployment. Our approach is motivated by a key insight in machine learning: Ensemble models can reduce both bias and variance compared to applying a single model. A related approach was pursued for functional visualization in neuroimaging (Hansen et al., 2001). Based on this insight, we propose a way to aggregate explanation methods, AGGMean. This approach is analysed theoretically and evaluated empirically. In experiments on Imagenet, the aggregate is more robust to adversarial attacks than any single method. Even when the attacker has complete knowledge of the model weights and the explanation methods to be used as well as complete control over the input, the explanation stays robust as shown in Fig. 1."}, {"heading": "2. Related Work", "text": ""}, {"heading": "2.1. Explanation methods", "text": "The open problem of explainability is reflected in a lot of recent work (Kindermans et al., 2017; Selvaraju et al., 2017; Bach et al., 2015; Zhang et al., 2018a; Zhou et al., 2016; Ancona et al., 2018; Ribeiro et al., 2016; Rieger et al., 2018; Kim et al., 2018; Lundberg & Lee, 2017; Zintgraf et al.,\nar X\niv :2\n00 7.\n06 38\n1v 1\n[ cs\n.L G\n] 1\n3 Ju\nl 2 02\n0\n2017; Simonyan et al., 2013; Zeiler & Fergus, 2014; Selvaraju et al., 2017; Smilkov et al., 2017; Sundararajan et al., 2017; Shrikumar et al., 2017; Montavon et al., 2017; Chang et al., 2018). We focus on generating visual explanations for single samples. To our knowledge the first work in this direction was Simonyan et al. (2013) with Saliency Maps (SM) that proposed backpropagating the output onto the input to gain an understanding of a neural network decision. The relevance for each input dimension is extracted by taking the gradient of the output w. r. t. to the input. This idea was extended by Springenberg et al. (2014) into Guided Backpropagation (GM) by applying ReLU non-linearities after each layer during the backpropagation. Compared to Saliency, this removes visual noise in the explanation. GradCAM (GC) from Selvaraju et al. (2017) is an explanation method, developed for use with convolutional neural networks. By backpropagating relevance through the dense layers and up-sampling the evidence for the convolutional part of the network, the method obtains coarse heatmaps that highlight relevant parts of the input image. Integrated Gradients (IG) Sundararajan et al. (2017) sums up the gradients from linearly interpolated pictures between a baseline, e.g. a black image, and the actual image. SmoothGrad (SG) filters out noise from a basic saliency map by creating many samples of the original input with Gaussian noise (Smilkov et al., 2017). The final saliency map is the average over all samples. In concurrent work, (Bhatt et al., 2020) also proposed aggregating explanation methods, albeit with the goal of decreasing complexity rather than vulnerability. Finally, (Yeh et al., 2019) showed that a combination of two popular explanation methods is optimal in terms of fidelity."}, {"heading": "2.2. Adversarial attacks on explanation methods", "text": "While adversarial examples for classification are wellknown, recently there has been growing interest in adversarial manipulation of explanations (Ghorbani et al., 2019; Heo et al., 2019; Dombrowski et al., 2019). Attacks on explanation can serve multiple purposes including \"fairwashing\" (A\u00efvodji et al., 2019). All of these methods exploit the fully differentiable nature of neural networks and iteratively update the input (or the model weights) to change the explanation while only minimally changing the input and output. The goal is to manipulate the explanation while keeping the input and output (visually) similar. It is assumed that the network architecture and weights are known and that either the input ((Ghorbani et al., 2019; Zhang et al., 2018b; Dombrowski et al., 2019)) or the network weights (Heo et al., 2019) can be changed by the attacker.\nFocussing on changing the input, Ghorbani et al. (2019),Zhang et al. (2018b) and Dombrowski et al. (2019) attack the explanation by manipulating the image, not changing the network weights. Interestingly, Zhang et al. (2018b) discuss the transferability of attacks and conclude that at-\ntacks are not that transferable. If the attacker is allowed to modify networks weights, as in Heo et al. (2019), the attacks generalize to all the considered explanation methods. We are interested in the more realistic situation where the attacker can modify the input but not the network. We investigate the transferability, c.f., Zhang et al. (2018b), and hypothesise that the limited transferability leads to improved robustness of the ensemble explanation. While ensemble methods have been proposed earlier as a defense for attacks on the label (Tram\u00e8r et al., 2017), they have not previously been investigated as a defense mechanism against attacks on explanations."}, {"heading": "3. Averaging explanation methods to reduce vulnerability", "text": ""}, {"heading": "3.1. Averaging explanations", "text": "All currently available explanation methods have weaknesses that are inherent to the approach and include significant uncertainty in the resulting heatmap (Kindermans et al., 2017; Adebayo et al., 2018; Smilkov et al., 2017). A natural way to mitigate this issue and reduce noise is to combine multiple explanation methods. Ensemble methods have been used for a long time to reduce the variance and bias of machine learning models. We apply the same idea to explanation methods and build an ensemble of explanation methods. Ensemble methods have also been previously used to defend against adversarial attacks on neural network outputs (Pang et al., 2019; Liao et al., 2018), motivating the usage of an explanation ensemble to defend against attacks on the explanation.\nWe assume a neural network F : X 7\u2192 y with X \u2208 Rm\u00d7m and a set of explanation methods {ej}Jj=1 with ej : X, y, F 7\u2192 E with E \u2208 Rm\u00d7m. We write Ej,n for the explanation obtained for Xn with method ej and denote the mean aggregate explanation as e\u0304 with E\u0304n = 1J \u2211J j=1 Ej,n. While we assume the input to be an image \u2208 Rm\u00d7m, this method is generalizable to inputs of other dimensionalities as well.\nTo get a theoretical understanding of the aggregation, we hypothesize the existence of a \u2019true\u2019 explanation E\u0302n. This allows us to quantify the error of an explanation method as the mean squared difference between the \u2019true\u2019 explanation and an explanation procured by an explanation method, i.e. the MSE.\nFor clarity we subsequently omit the notation for the neural network. We write the error of explanation method j on image Xn as errj,n = ||Ej,n \u2212 E\u0302n||2 with\nMSE(Ej) = 1\nN \u2211 n errj,n\nand MSE(E\u0304) = 1N \u2211 n ||E\u0304n \u2212 E\u0302n||2 is the MSE of the aggregate. The typical error of an explanation method is the mean error over all explanation methods\nMSE = 1\nJ \u2211 j MSE(Ej).\nWith these definitions we can do a standard bias-variance decomposition (Geman et al., 1992). Accordingly we can show the error of the aggregate will be less that the typical error of explanation methods,\nMSE = 1N \u2211 n 1 J \u2211 j ||E\u0302n \u2212 Ej,n||2 (1)\n= 1N \u2211 n ||E\u0302n \u2212 E\u0304n||2 (2)\n+ 1NJ \u2211 n,j ||E\u0304n \u2212 Ej,n||2,\nhence, MSE = 1J \u2211 j 1 N \u2211 n ||E\u0304n \u2212 Ej,n||2 + MSE(E\u0304) (3)\n\u2265 MSE(E\u0304).\nA detailed calculation is given in the appendix. The error of the aggregate MSE(E\u0304) is less than the typical error of the participating methods. The difference - a \u2018variance\u2019 term - represents the epistemic uncertainty and only vanishes if all methods produce identical maps. By taking the average over all available explanation methods, we reduce the variance of the explanation compared to using a single method. To obtain this average, we normalize all input heatmaps such that the relevance over all pixels sum up to one. This reflects our initial assumption that all individual explanation methods are equally good estimators. We refer to this approach as AGG-Mean.\nEAGG-Mean,n = 1\nJ J\u2211 j=1 Ej,n (4)"}, {"heading": "3.2. Adversarial scenarios", "text": "With the increasing interest and practical importance of explainability of neural networks the interest in methods for manipulation and control of explanations is also increasing. A typical scenario is to make imperceptible changes to the input of the neural network such that the output/label is unchanged while the explanation changes according to a given goal. Such effort could, e.g., be used to hide bias or other fairness issues a given classifier might have.\nDombrowski et al. (2019) showed that explanations can be made more robust by replacing the ReLU nonlinearity with a Softplus function. However, this requires changing the network and using a different architecture for classification and explanation, which is highly undesirable as it defeats the purpose of the explanation. The analysis of Zhang et al. (2018b) showed that transferability of attacks is limited,\nhence, our ensemble of multiple explanations may offer robustness also towards certain types of adversarials.\nIn the following we will assume an attacker who has full knowledge of the neural network, including the architecture and weights. In contrast to Heo et al. (2019), however, we will assume that the attacker cannot change the neural network, following Dombrowski et al. (2019); Ghorbani et al. (2019). Furthermore the attacker has full control over the input to the neural network. The goal is to adversarially manipulate the image according to a predefined objective.\nIn the following, x will refer to the original image. x\u0302 is the \u2019target\u2019 image. The objective is to produce an adversarial input x\u2032 with x\u2032 \u2248 x but the explanation Ex\u2032 \u2248 Ex\u0302. While we focus on assimilating the explanation map of another input as in Dombrowski et al. (2019), all techniques introduced can be readily adapted to other objectives, f.e. to move the mass center of the explanation.\nExploring the robustness of aggregates of multiple explanation methods we concentrate on the following two scenarios:\nArsenal of explanation methods In this scenario we have a pool of potential explanation methods. The attacker does not have knowledge of what explanation method is used and optimizes for a different explanation method than is used by the defender. The success of the attack depends on how readily an attack of one explanation method translates to another method.\nWe hypothesize that attacks on explanation methods are fragile and do not translate well across explanation methods, as they exploit locally high variances in the gradient landscape. This hypothesis is examined in Section 3.2.\nAggregation of explanation methods In this more challenging scenario we aggregate multiple explanation methods as described in Eq. (4). The attacker knows the exact explanation methods and ratio going into the mixture and attacks this aggregation.\nMany attribution-based methods are utilizing the gradient \u03b4y \u03b4x (x) of the output to create an explanation. Due to the non-linearity of the neural network, the gradient can change rapidly with small distances in input space (Dombrowski et al., 2019; Ghorbani et al., 2019). Attacks on explanation methods exploit this vulnerability."}, {"heading": "4. Experiments", "text": "We evaluate how robust aggregated methods are against adversarial attacks, compared to unaggregated methods. In all cases we assume that the attacker has full knowledge of the network architecture and weights (white box attack) but cannot change them. However, the attacker has full control\nover the input.\nFollowing (Dombrowski et al., 2019) we run experiments on a pretrained VGG161 We consider Layerwise Relevance Propagation (LRP), Saliency Mapping (SM), Guided Backprop (GB) and Integrated Gradients (IG) as explanation methods. The latter was not used in the aggregation.\nUnless otherwise noted we followed (Dombrowski et al., 2019) in the choice of hyperparameters for attacking explanation methods. In the appendix we show that our defence also works against the attack as proposed in Ghorbani et al. (2019).\nSince the ReLU function used in neural networks is not twice differentiable, we replace it with a differentiable approximation, SoftPlus for the iterative creation of the adversarial input. The final manipulated heatmaps are created with the ReLU non-linearity. Further details about the experiments are in the appendix.\nWe consider the two scenarios introduced in Section 3.2. In all cases, the objective of the attacker is to make the explanation of input Ex\u2032 \u2248 Ex\u0302 while keeping x\u2032 \u2248 x. To do this, the attacker manipulates x\u2032.\nWe visually confirmed that the adversarial images look similar to the input images and provide examples in the appendix. We measure the difference between the start explanation Ex, target explanation Ex\u0302 and adversarial explanation Ex\u2032 with the MSE (Mean Square Error), the PCC (Pearson Correlation Coefficient) and the top-k intersection with k being ad-hoc set to 10% (Dombrowski et al., 2019; Ghorbani et al., 2019).\nIn all metrics, explanations obtained with different methods have different \u2019base\u2019 values (similarity between the explanations of two randomly chosen images) due to structural differences between explanation methods. To account for this, we consider for each similarity metric msim the difference msim(Ex\u0302, Ex\u2032)\u2212msim(Ex\u0302, Ex), i.e. how much more similar the attack makes Ex\u2032 look to Ex\u0302. For the MSE, this results in a negative score, since the difference between the target and the attack is less than between the target and the starting point. For all metrics, the ideal score is 0, i.e. the attack did not change the explanation at all. Thus, for MSE a high value is desirable, for PCC and top-k union a low value is desirable.\nTransferability of attacks on explanation methods Visually comparing the success of an attack on AGG-Mean on the y-axis compared to unaggregated method (Guided Backprop) on the x-axis. Similarity metrics (topK and PCC) should be low, MSE should be high for less similarity between target and adversarial. Since for most samples topK\n1Models retrieved from https://github.com/keras-team/keras.\nand PCC are lower for AGG-Mean than for Guided Backprop, AGG-Mean is more robust than Guided Backprop. The red dot is the sample visualized in Fig. 4.\nThe lack of transferability results are in line with the findings of (Zhang et al., 2018b).\nWe consider a case where the attacker does not know what explanation method is used, i.e. we attack a different explanation method than the one that is used. This would be the case if the defender has not made the specific explanation method used public or is choosing one at random to ward off attacks. If the attack translates well, i.e. the image manipulation fools both methods, similarity metrics should be similar for both explanation methods.\nIn Fig. 2 we provide results for attacking Guided Backpropagation and extracting an explanation with LRP. For a hundred samples we visualize for each sample the respective similarity metrics for both explanation methods in Fig. 2. If the attack translates well, the points should lie on the identity line in Fig. 2. Samples below the identity line for PCC and topK and above for MSE indicate that the attack does not generalize to other explanation methods.\nAs visible in Fig. 2 and anecdotally in Fig. 3 (red data point in Fig. 2), attacks perform much worse on other methods (here LRP) than the targeted one (here GB). We provide statistics for other combinations in the appendix.\nAttacking aggregations of explanation methods In the second scenario the attacker knows that the explanations are aggregated and attacks the aggregation. We aggregate LRP, GB and SM and compare against those methods as well as Integrated Gradient. IG was not included in the aggregation as it requires sampling for each step, making it computationally much more expensive than the other methods.\nIn Table 1 we provide metrics averaged over a hundred samples. AGG-Mean outperforms unaggregated methods. We also provide a direct comparison to GuidedBackprop in Fig. 5. To give an intuition on what differences in the metrics look like, we visualize a sample (red dot in Fig. 5) in Fig. 4. We see that AGG-Mean opposed to the unaggregated methods largely preserves the original heatmap after the attack. More examples are provided in the appendix.\nThe resilience of the aggregate to attacks can be understood in terms of averaging induced smoothness. In (Dombrowski et al., 2019) the beneficial effects of averaging in the SmoothGrad method are described. As noted in (Dombrowski et al., 2019) SmoothGrad is computationally expensive. We conjecture that the diversity of the methods involved in the present aggregate implies that smoothing can be achieved at less computational effort."}, {"heading": "5. Conclusion", "text": "In recent times, attacks on explanation method have received increased attention as the so-called \"fairwashing\", manipulating explanations to more innocuous ones, has become a concern.\nWe provide a simple and intuitive approach to defend against such attacks that does not require the model to be changed in any way and is computationally inexpensive. This approach is explored theoretically. We then provided experimental"}, {"heading": "SM -0.92 0.74 0.40", "text": "evidence that aggregations are a more robust to adversarial manipulation than individual explanation methods.\nPerhaps surprisingly, a simple average with the originally attacked method included induces a more robust explanation than replacing the explanation method with a different one. In (Dombrowski et al., 2019) arguments are presented that the observed vulnerability is due to non-smoothness of contemporary networks. It is also argued that averaging as in SmoothGrad increases robustness. We theorize that the averaging of the diverse set of explanation methods involved in the aggregate creates similar smoothness. We noted that in contrast to (Dombrowski et al., 2019), the aggregate does not require modification (smoothing) of the network.\nWe hope that our approach will be useful to make neural networks more transparent and increase their credibility as they are applied in real-life scenarios."}, {"heading": "A. Appendix", "text": "A.1. Aggregating explanation methods to reduce variance - detailed derivation\nAll currently available explanation methods have weaknesses that are inherent to the approach and include significant noise in the heatmap (Kindermans et al., 2017; Adebayo et al., 2018; Smilkov et al., 2017). A natural way to mitigate this issue and reduce noise is to combine multiple explanation methods. Ensemble methods have been used for a long time to reduce the variance and bias of machine learning models. We apply the same idea to explanation methods and build an ensemble of explanation methods.\nWe assume a neural network F : X 7\u2192 y with X \u2208 Rmxm and a set of explanation methods {ej}Jj=1 with ej : X, y, F 7\u2192 E with E \u2208 Rmxm. We write Ej,n for the explanation obtained for Xn with method ej and denote the mean aggregate explanation as e\u0304 with E\u0304n = 1J \u2211J j=1 Ej,n. While we assume the input to be an image \u2208 Rmxm, this method is generalizable to inputs of other dimensions as well.\nWe define the error of an explanation method as the mean squared difference between a hypothetical \u2019true\u2019 explanation and an explanation procured by the explanation method, i.e. the MSE. For this definition we assume the existence of the hypothetical \u2019true\u2019 explanation E\u0302n for image Xn.\nFor clarity we subsequently omit the notation for the neural network.\nWe write the error of explanation method j on image Xn as errj,n = ||Ej,n \u2212 E\u0302n||2 with\nMSE(Ej) = 1\nN \u2211 n errj,n\nand MSE(E\u0304) = 1N \u2211 n ||E\u0304n\u2212 E\u0302n||2 is the MSE of the aggregate. The typical error of an explanation method is represented by the mean\nMSE = 1\nN \u2211 n 1 J \u2211 j ||E\u0302n \u2212 Ej,n||2\n= 1\nNJ \u2211 n,j ||E\u0302n \u2212 Ej,n + E\u0304n \u2212 E\u0304n||2\n= 1\nNJ \u2211 n,j ||(E\u0302n \u2212 E\u0304n) + (E\u0304n \u2212 Ej,n)||2\n= 1\nNJ \u2211 n,j ||E\u0302n \u2212 E\u0304n||2 + ||E\u0304n \u2212 Ej,n||2 + 1 NJ \u2211 n,j ( 2Tr [ (E\u0302n \u2212 E\u0304n)(E\u0304n \u2212 Ej,n) ])\n= 1\nN \u2211 n ||E\u0302n \u2212 E\u0304n||2 + 1 NJ \u2211 n,j ||E\u0304n \u2212 Ej,n||2 + 2 1 N \u2211 n Tr (E\u0302n \u2212 E\u0304n)  1 J \u2211 j (E\u0304n \u2212 Ej,n) \n= 1\nN \u2211 n ||E\u0302n \u2212 E\u0304n||2 + 1 NJ \u2211 n,j ||E\u0304n \u2212 Ej,n||2 + 2 1 N \u2211 n Tr (E\u0302n \u2212 E\u0304n) 1J \u2211 j\n(E\u0304n \u2212 Ej,n)\ufe38 \ufe37\ufe37 \ufe38 =0  = 1\nN \u2211 n ||E\u0302n \u2212 E\u0304n||2 + 1 NJ \u2211 n,j ||E\u0304n \u2212 Ej,n||2,\nhence,\nMSE = MSE(E\u0304) + 1\nNJ \u2211 n,j ||E\u0304n \u2212 Ej,n||2\ufe38 \ufe37\ufe37 \ufe38 epistemic uncertainty \u2265 MSE(E\u0304)\nThe error of the aggregate MSE(E\u0304) is less than the typical error of the participating methods. The difference - a \u2018variance\u2019 term - represents the epistemic uncertainty and only vanishes if all methods produce identical maps."}, {"heading": "A.2. Experimental setup", "text": ""}, {"heading": "A.2.1. GENERAL", "text": "For AGG-Var, we add a constant to the denominator. We set this constant to 10 times the mean std, a value chosen empirically after trying values in the range of [1, 10, 100] times the mean. Evaluations were run with a set random seed for reproducibility. SE were reported either for each individual result or if they were non-significant in the caption to avoid cluttering the results. All experiments were done on a Titan X.\nA.2.2. IMAGENET\nWe downloaded the data from the ImageNet Large Scale Visual Recognition Challenge website and used the validation set only. No images were excluded. The images were preprocessed to be within [\u22121, 1] unless a custom range was used for training (indicated by the preprocess function of keras)."}, {"heading": "A.2.3. DETAILS ABOUT ATTACKING EXPLANATION METHODS", "text": "For a range of explanation methods we chose to compare against LRP, Gradient, Guided Backpropagation and Integrated Gradients, a range of well-known and well-established explanation methods (Sundararajan et al., 2017; Bach et al., 2015; Springenberg et al., 2014; Simonyan et al., 2013). Since Integrated Gradients is thirty times more computationally expensive than other methods, we did not include it in the aggregation as it would have slowed down experiments considerably.\nUnless otherwise noted, all metrics are computed as the average of a hundred data samples with mean and SE. Informally, we also found that the MSE does not align well with perceived changes in the explanations, likely due to it being susceptible to outliers.\nWe used a pretrained VGG16 for all experiments attacking explanation methods (Simonyan & Zisserman, 2014).\nA.3. Alignment between human attribution and explanation methods\nWe want to quantify whether an explanation method agrees with human judgement on which parts of an image should be important. While human annotation is expensive, there exists a benchmark for human evaluation introduced in (Mohseni & Ragan, 2018). The benchmark includes ninety images of categories in the ImageNet Challenge (ten images were excluded\ndue to the category not being in the ImageNet challenge) and provides annotations of relevant segments that ten human test objects found important. Example images are shown in Fig. 6.\nWhile human evaluation is not a precise measure, we still expect some correlation between neural network and human judgement.\nTo test the alignment, we calculate the cosine similarity,\nsimilarity(ej) = \u2211N n=1 AnEj,n\u221a\u2211N\nn=1 A 2 n \u221a\u2211N n=1 E 2 j,n\nbetween the human annotation and the explanations produced by the respective explanation methods. An is the human annotation of what is important for image Xn\nSince the images in this dataset are 224x224 pixel large, we only compute the cosine similarity for the network architectures where pretrained networks with this input size were available.\nWe see that AGG-Mean and AGG-Var perform on-par with the best methods (SmoothGrad and GradCAM). While the aggregated methods perform better than the average explanation method, they do not surpass the best method.\nWhen we combine the two best-performing single methods, SmoothGrad and GradCAM, we surpass each individual method. We hypothesize that this is because the epistemic uncertainty is reduced by the aggregate.\nA.4. Details about attacking explanation methods\nChoice of explanation methods We focused on explanation methods that have previously been shown to be susceptible to adversarial attacks. As such, we did not include GradCAM in the experiments, neither as a comparison or in the aggregation.\nDifferent explanation methods have different computational loads. Notably, SmoothGrad and IntegratedGradients involve the sampling of many explanations for a single pass, increasing computation times by the number of samples () and were not included in the aggregation but as a comparison.\nChoice of hyperparameters We followed (Dombrowski et al., 2019) for the choice of hyperparameters in learning rate and beta growth. For AGG-Mean we chose a learning rate of 10\u22123 and 1500 iterations for the attack.\nhyperparameter choice when attacking explanation methods, using a learning rate of 10\u22123."}, {"heading": "A.4.1. MORE EXAMPLES", "text": "We provide more examples showing different explanation methods being attacked in Figs. 7 to 10. An abridged version of Fig. 7 is also shown in the main text."}, {"heading": "A.4.2. TRANSFERABILITY OF ATTACKS", "text": "In the main text we show similarity metrics differences between the method being attacked and not being attacked for Guided Backprop and LRP. Here we provide scatter plots for the rest of the considered methods in Figs. 11 to 14:"}, {"heading": "A.4.3. SIMILARITY OF THE ATTACKED IMAGES TO THE STARTING IMAGES", "text": "We provide the average distance of the adversarial images to the original images in Table 3 (calculated in RGB space, average over all pixels). As can be seen in Figs. 7 to 9 and 10, there is no visual difference to the input images for any of the attacks. Attacking AGG-Mean has the smallest distance to the input image, supporting our hypothesis that aggregating explanation methods removes vulnerabilities to adversarial manipulation."}, {"heading": "A.4.4. OTHER ATTACKS", "text": "In the main text we mainly concern ourselves with making the explanation of one image look like a pre-specified target explanation, as this is a use case where the motivation of an attacker is apparent. However, as introduced in (Ghorbani et al., 2019) other attack objectives are also conceivable.\nWe show results when following the objective of making a specified area of the explanation not relevant, i.e. a blank space in the explanation as introduced in (Ghorbani et al., 2019). A square (in size a quarter of the image) centered on the middle should not contain any relevance for the explanation. Size and position of the blank space were chosen ad-hoc, we assume that the center of the image generally contains useful information for the classification. We show quantitative results in Table 4, computing how much percentage of the original relevance is preserved and qualitative results in Figs. 15 and 16. While an aggregation is not completely robust to the attack, far more of the original explanation is preserved."}], "title": "A simple defense against adversarial attacks on heatmap explanations", "year": 2020}