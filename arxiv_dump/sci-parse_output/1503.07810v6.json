{
  "abstractText": "We investigate a long-debated question, which is how to create predictive models of recidivism that are sufficiently accurate, transparent, and interpretable to use for decision-making. This question is complicated as these models are used to support different decisions, from sentencing, to determining release on probation, to allocating preventative social services. Each case might have an objective other than classification accuracy, such as a desired true positive rate (TPR) or false positive rate (FPR). Each (TPR, FPR) pair is a point on the receiver operator characteristic (ROC) curve. We use popular machine learning methods to create models along the full ROC curve on a wide range of recidivism prediction problems. We show that many methods (SVM, SGB, Ridge Regression) produce equally accurate models along the full ROC curve. However, methods that designed for interpretability (CART, C5.0) cannot be tuned to produce models that are accurate and/or interpretable. To handle this shortcoming, we use a recent method called Supersparse Linear Integer Models (SLIM) to produce accurate, transparent, and interpretable scoring systems along the full ROC curve. These scoring systems can be used for decision-making for many different use cases, since they are just as accurate as the most powerful black-box machine learning models for many applications, but completely transparent, and highly interpretable.",
  "authors": [
    {
      "affiliations": [],
      "name": "Jiaming Zeng"
    },
    {
      "affiliations": [],
      "name": "Berk Ustun"
    },
    {
      "affiliations": [],
      "name": "Cynthia Rudin"
    }
  ],
  "id": "SP:907212d50de6cf62f50e93e4505631b190d008f8",
  "references": [
    {
      "authors": [
        "Andrade",
        "Joel T"
      ],
      "title": "Handbook of violence risk assessment and treatment: New approaches for mental health professionals",
      "year": 2009
    },
    {
      "authors": [
        "Andrews",
        "Donald A",
        "James Bonta"
      ],
      "title": "The level of service inventory-revised",
      "venue": "Multi-Health Systems,",
      "year": 2000
    },
    {
      "authors": [
        "Baradaran",
        "Shima"
      ],
      "title": "Race, prediction, and discretion",
      "venue": "Geo. Wash. L. Rev.,",
      "year": 2013
    },
    {
      "authors": [
        "Barnes",
        "Geoffrey C",
        "Jordan M Hyatt"
      ],
      "title": "Classifying adult probationers by forecasting future offending",
      "venue": "Technical report, National Institute of Justice, U.S. Department of Justice,",
      "year": 2012
    },
    {
      "authors": [
        "Belfrage",
        "Henrik",
        "Ran Fransson",
        "Susanne Strand"
      ],
      "title": "Prediction of violence using the hcr-20: A prospective study in two maximum-security correctional institutions",
      "venue": "The Journal of Forensic Psychiatry,",
      "year": 2000
    },
    {
      "authors": [
        "Berk",
        "Richard"
      ],
      "title": "The role of race in forecasts of violent crime",
      "venue": "Race and social problems,",
      "year": 2009
    },
    {
      "authors": [
        "Berk",
        "Richard"
      ],
      "title": "Balancing the costs of forecasting errors in parole decisions",
      "venue": "Alb. L. Rev.,",
      "year": 2010
    },
    {
      "authors": [
        "Zeng",
        "Ustun",
        "Rudin Berk",
        "Richard"
      ],
      "title": "Asymmetric loss functions for forecasting in criminal justice settings",
      "venue": "Journal of Quantitative Criminology,",
      "year": 2011
    },
    {
      "authors": [
        "Berk",
        "Richard",
        "Justin Bleich"
      ],
      "title": "Forecasts of violence to inform sentencing decisions",
      "venue": "Journal of Quantitative Criminology,",
      "year": 2014
    },
    {
      "authors": [
        "Berk",
        "Richard",
        "Lawrence Sherman",
        "Geoffrey Barnes",
        "Ellen Kurtz",
        "Lindsay Ahlman"
      ],
      "title": "Forecasting murder within a population of probationers and parolees: a high stakes application of statistical learning",
      "venue": "Journal of the Royal Statistical Society: Series A (Statistics in Society),",
      "year": 2009
    },
    {
      "authors": [
        "Berk",
        "Richard A",
        "Justin Bleich"
      ],
      "title": "Statistical procedures for forecasting criminal behavior",
      "venue": "Criminology & Public Policy,",
      "year": 2013
    },
    {
      "authors": [
        "Berk",
        "Richard A",
        "Susan D. Sorenson"
      ],
      "title": "Machine learning forecasts of domestic violence to help inform release decisions at arraignment",
      "venue": "Technical report, University of Pennsylvania,",
      "year": 2014
    },
    {
      "authors": [
        "Berk",
        "Richard A",
        "Yan He",
        "Susan B Sorenson"
      ],
      "title": "Developing a practical forecasting screener for domestic violence incidents",
      "venue": "Evaluation Review,",
      "year": 2005
    },
    {
      "authors": [
        "Berk",
        "Richard A",
        "Brian Kriegler",
        "Jong-Ho Baek"
      ],
      "title": "Forecasting dangerous inmate misconduct: An application of ensemble statistical procedures",
      "venue": "Journal of Quantitative Criminology,",
      "year": 2006
    },
    {
      "authors": [
        "Bhati",
        "Avinash Singh"
      ],
      "title": "Estimating the number of crimes averted by incapacitation: an information theoretic approach",
      "venue": "Journal of Quantitative Criminology,",
      "year": 2007
    },
    {
      "authors": [
        "Bhati",
        "Avinash Singh",
        "Alex R Piquero"
      ],
      "title": "Estimating the impact of incarceration on subsequent offending trajectories: Deterrent, criminogenic, or null effect",
      "venue": "The Journal of Criminal Law and Criminology,",
      "year": 2007
    },
    {
      "authors": [
        "Borden",
        "Howard G"
      ],
      "title": "Factors for predicting parole success",
      "venue": "Journal of the American Institute of Criminal Law and Criminology,",
      "year": 1928
    },
    {
      "authors": [
        "Borum",
        "Randy"
      ],
      "title": "Manual for the structured assessment of violence risk in youth (SAVRY)",
      "venue": "Odessa, Florida: Psychological Assessment Resources,",
      "year": 2006
    },
    {
      "authors": [
        "Breiman",
        "Leo"
      ],
      "title": "Statistical modeling: The two cultures",
      "venue": "Statistical Science,",
      "year": 2001
    },
    {
      "authors": [
        "Breiman",
        "Leo",
        "Jerome Friedman",
        "Charles J Stone",
        "Richard A Olshen"
      ],
      "title": "Classification and regression trees",
      "venue": "CRC press,",
      "year": 1984
    },
    {
      "authors": [
        "Burgess",
        "Ernest W"
      ],
      "title": "Factors determining success or failure on parole",
      "venue": "Illinois Committee on IndeterminateSentence Law and Parole Springfield,",
      "year": 1928
    },
    {
      "authors": [
        "Bushway",
        "Shawn D"
      ],
      "title": "Is there any logic to using logit",
      "venue": "Criminology & Public Policy,",
      "year": 2013
    },
    {
      "authors": [
        "Bushway",
        "Shawn D",
        "Anne Morrison Piehl"
      ],
      "title": "The inextricable link between age and criminal history in sentencing",
      "venue": "Crime & Delinquency,",
      "year": 2007
    },
    {
      "authors": [
        "Clements",
        "Carl B"
      ],
      "title": "Offender classification two decades of progress",
      "venue": "Criminal Justice and Behavior,",
      "year": 1996
    },
    {
      "authors": [
        "Copas",
        "John",
        "Peter Marshall"
      ],
      "title": "The offender group reconviction scale: a statistical reconviction score for use by probation officers",
      "venue": "Journal of the Royal Statistical Society: Series C (Applied Statistics),",
      "year": 1998
    },
    {
      "authors": [
        "Cristianini",
        "Nello",
        "John Shawe-Taylor"
      ],
      "title": "An introduction to support vector machines and other kernel-based learning methods",
      "year": 2000
    },
    {
      "authors": [
        "Crow",
        "Matthew S"
      ],
      "title": "The complexities of prior record, race, ethnicity, and policy: Interactive effects in sentencing",
      "venue": "Criminal Justice Review,",
      "year": 2008
    },
    {
      "authors": [
        "Dawes",
        "Robyn M",
        "David Faust",
        "Paul E Meehl"
      ],
      "title": "Clinical versus actuarial",
      "venue": "judgment. Science,",
      "year": 1989
    },
    {
      "authors": [
        "Freitas",
        "Alex A"
      ],
      "title": "Comprehensible classification models: a position paper",
      "venue": "ACM SIGKDD Explorations Newsletter,",
      "year": 2014
    },
    {
      "authors": [
        "Freund",
        "Yoav",
        "Robert E Schapire"
      ],
      "title": "A decision-theoretic generalization of on-line learning and an application to boosting",
      "venue": "Journal of computer and system sciences,",
      "year": 1997
    },
    {
      "authors": [
        "Friedman",
        "Jerome",
        "Trevor Hastie",
        "Robert Tibshirani"
      ],
      "title": "Regularization paths for generalized linear models via coordinate descent",
      "venue": "Journal of Statistical Software,",
      "year": 2010
    },
    {
      "authors": [
        "Friedman",
        "Jerome H"
      ],
      "title": "Greedy function approximation: a gradient boosting machine",
      "venue": "Annals of statistics,",
      "year": 2001
    },
    {
      "authors": [
        "Friedman",
        "Jerome H"
      ],
      "title": "Stochastic gradient boosting",
      "venue": "Computational Statistics  Data Analysis,",
      "year": 2002
    },
    {
      "authors": [
        "Goel",
        "Sharad",
        "Justin M Rao",
        "Ravi Shroff"
      ],
      "title": "Precinct or prejudice? understanding racial disparities in new york city\u2019s stop-and-frisk policy. Understanding Racial Disparities in New York City\u2019s Stop-and-Frisk",
      "venue": "Policy (March",
      "year": 2015
    },
    {
      "authors": [
        "Goh",
        "Siong Thye",
        "Cynthia Rudin"
      ],
      "title": "Box drawings for learning with imbalanced data",
      "venue": "In Proceedings of the 20th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD),",
      "year": 2014
    },
    {
      "authors": [
        "Gottfredson",
        "Don M",
        "Howard N Snyder"
      ],
      "title": "The mathematics of risk classification: Changing data into valid instruments for juvenile courts. ncj 209158",
      "venue": "Office of Juvenile Justice and Delinquency Prevention,",
      "year": 2005
    },
    {
      "authors": [
        "Grove",
        "William M",
        "Paul E Meehl"
      ],
      "title": "Comparative efficiency of informal (subjective, impressionistic) and formal (mechanical, algorithmic) prediction procedures: The clinical\u2013statistical controversy",
      "venue": "Psychology, Public Policy,",
      "year": 1996
    },
    {
      "authors": [
        "Hahsler",
        "Michael",
        "Christian Buchta",
        "Bettina Gruen",
        "Kurt Hornik",
        "Christian Borgelt"
      ],
      "title": "Package \u2018arules\u2019: Mining Association Rules and Frequent Itemsets",
      "venue": "URL http://cran.r-project. org/web/packages/arules/arules.pdf",
      "year": 2014
    },
    {
      "authors": [
        "Hannah-Moffat",
        "Kelly"
      ],
      "title": "Actuarial sentencing: An \u201cunsettled",
      "venue": "proposition. Justice Quarterly,",
      "year": 2013
    },
    {
      "authors": [
        "RK Hanson",
        "D Thornton"
      ],
      "title": "Notes on the development of static-2002",
      "venue": "Ottawa, Ontario: Department of the Solicitor General of Canada,",
      "year": 2003
    },
    {
      "authors": [
        "Hesterberg",
        "Tim",
        "Nam Hee Choi",
        "Lukas Meier",
        "Chris Fraley"
      ],
      "title": "Least angle and `1 penalized regression: A review",
      "venue": "Statistics Surveys,",
      "year": 2008
    },
    {
      "authors": [
        "Hoffman",
        "Peter B"
      ],
      "title": "Twenty years of operational use of a risk prediction instrument: The United States parole commission\u2019s salient factor score",
      "venue": "Journal of Criminal Justice,",
      "year": 1994
    },
    {
      "authors": [
        "Hoffman",
        "Peter B",
        "Sheldon Adelberg"
      ],
      "title": "The salient factor score: A nontechnical overview",
      "venue": "Fed. Probation,",
      "year": 1980
    },
    {
      "authors": [
        "Holte",
        "Robert C"
      ],
      "title": "Very simple classification rules perform well on most commonly used datasets",
      "venue": "Machine learning,",
      "year": 1993
    },
    {
      "authors": [
        "Holte",
        "Robert C"
      ],
      "title": "Elaboration on Two Points Raised in \u201cClassifier Technology and the Illusion of Progress",
      "venue": "Statistical Science,",
      "year": 2006
    },
    {
      "authors": [
        "Howard",
        "Philip",
        "Brian Francis",
        "Keith Soothill",
        "Leslie Humphreys"
      ],
      "title": "OGRS 3: The revised offender group reconviction scale",
      "venue": "Technical report, Ministry of Justice,",
      "year": 2009
    },
    {
      "authors": [
        "Kropp",
        "P Randall",
        "Stephen D Hart"
      ],
      "title": "The spousal assault risk assessment (sara) guide: reliability and validity in adult male offenders",
      "venue": "Law and human behavior,",
      "year": 2000
    },
    {
      "authors": [
        "Kuhn",
        "Max",
        "Kjell Johnson"
      ],
      "title": "Applied predictive modeling",
      "year": 2013
    },
    {
      "authors": [
        "Kuhn",
        "Max",
        "Steve Weston",
        "Nathan Coulter",
        "R. Quinlan"
      ],
      "title": "C50: C5.0 decision trees and rule-based models, 2012. URL http://CRAN.R-project.org/package=C50. R package version 0.1.0-013",
      "year": 2012
    },
    {
      "authors": [
        "Langan",
        "Patrick A",
        "David J Levin"
      ],
      "title": "Recidivism of prisoners",
      "venue": "Federal Sentencing Reporter,",
      "year": 1994
    },
    {
      "authors": [
        "Langton",
        "Calvin M",
        "Howard E Barbaree",
        "Michael C Seto",
        "Edward J Peacock",
        "Leigh Harkins",
        "Kevin T Hansen"
      ],
      "title": "Actuarial assessment of risk for reoffense among adult sex offenders evaluating the predictive accuracy of the static-2002 and five other instruments",
      "venue": "Criminal Justice and Behavior,",
      "year": 2007
    },
    {
      "authors": [
        "Liaw",
        "Andy",
        "Matthew Wiener"
      ],
      "title": "Classification and regression by randomforest",
      "venue": "R News,",
      "year": 2002
    },
    {
      "authors": [
        "Lim",
        "Tjen-Sien",
        "Wei-Yin Loh",
        "Yu-Shan Shih"
      ],
      "title": "A comparison of prediction accuracy, complexity, and training time of thirty-three old and new classification algorithms",
      "venue": "Machine learning,",
      "year": 2000
    },
    {
      "authors": [
        "Lowenkamp",
        "Christopher T",
        "Edward J Latessa"
      ],
      "title": "Understanding the risk principle: How and why correctional interventions can harm low-risk offenders",
      "venue": "Topics in community corrections,",
      "year": 2004
    },
    {
      "authors": [
        "Maloof",
        "Marcus A"
      ],
      "title": "Learning when data sets are imbalanced and when costs are unequal and unknown. In ICML-2003 workshop on learning from imbalanced data sets II, volume",
      "year": 2003
    },
    {
      "authors": [
        "McCord",
        "Joan"
      ],
      "title": "A thirty-year follow-up of treatment effects",
      "venue": "American psychologist,",
      "year": 1978
    },
    {
      "authors": [
        "McCord",
        "Joan"
      ],
      "title": "Cures that harm: Unanticipated outcomes of crime prevention programs",
      "venue": "The Annals of the American Academy of Political and Social Science,",
      "year": 2003
    },
    {
      "authors": [
        "Meyer",
        "David",
        "Evgenia Dimitriadou",
        "Kurt Hornik",
        "Andreas Weingessel",
        "Friedrich Leisch"
      ],
      "title": "e1071: Misc Functions of the Department of Statistics (e1071)",
      "venue": "TU Wien,",
      "year": 2012
    },
    {
      "authors": [
        "Milgram",
        "Anne"
      ],
      "title": "Why smart statistics are the key to fighting crime",
      "venue": "Ted Talk,",
      "year": 2014
    },
    {
      "authors": [
        "Miller",
        "George"
      ],
      "title": "The magical number seven, plus or minus two: Some limits on our capacity for processing information",
      "venue": "The Psychological Review,",
      "year": 1956
    },
    {
      "authors": [
        "Nafekh",
        "Mark",
        "Laurence Louis Motiuk"
      ],
      "title": "The Statistical Information on Recidivism, Revised 1 (SIR-R1) Scale: A Psychometric Examination",
      "venue": "Correctional Service of Canada. Research Branch,",
      "year": 2002
    },
    {
      "authors": [
        "Netter",
        "Brian"
      ],
      "title": "Using group statistics to sentence individual criminals: an ethical and statistical critique of the virginia risk assessment program",
      "venue": "The Journal of Criminal Law and Criminology,",
      "year": 2007
    },
    {
      "authors": [
        "Neuilly",
        "Melanie-Angela",
        "Kristen M Zgoba",
        "George E Tita",
        "Stephen S Lee"
      ],
      "title": "Predicting recidivism in homicide offenders using classification tree analysis",
      "venue": "Homicide studies,",
      "year": 2011
    },
    {
      "authors": [
        "Petersilia",
        "Joan",
        "Susan Turner"
      ],
      "title": "Guideline-based justice: Prediction and racial minorities",
      "venue": "Crime & Justice,",
      "year": 1987
    },
    {
      "authors": [
        "R R Core Team"
      ],
      "title": "A Language and Environment for Statistical Computing",
      "venue": "R Foundation for Statistical Computing, Vienna,",
      "year": 2015
    },
    {
      "authors": [
        "Ricardo H. Hinojosa"
      ],
      "title": "A comparison of the federal sentencing guidelines criminal history category and the U.S. parole commission salient factor score",
      "venue": "Technical report, U.S. Sentencing Commission,",
      "year": 2005
    },
    {
      "authors": [
        "Ridgeway",
        "Greg"
      ],
      "title": "gbm: Generalized boosted regression models",
      "venue": "R package version,",
      "year": 2006
    },
    {
      "authors": [
        "Ridgeway",
        "Greg"
      ],
      "title": "The pitfalls of prediction",
      "venue": "NIJ Journal, National Institute of Justice,",
      "year": 2013
    },
    {
      "authors": [
        "Ritter",
        "Nancy"
      ],
      "title": "Predicting recidivism risk: New tool in philadelphia shows great promise",
      "venue": "NIJ Journal,",
      "year": 2013
    },
    {
      "authors": [
        "Rubin",
        "Paul A"
      ],
      "title": "Mixed integer classification problems",
      "venue": "In Encyclopedia of Optimization,",
      "year": 2009
    },
    {
      "authors": [
        "Sherman",
        "Lawrence W"
      ],
      "title": "The power few: experimental criminology and the reduction of harm",
      "venue": "Journal of Experimental Criminology,",
      "year": 2007
    },
    {
      "authors": [
        "Simon",
        "Jonathan"
      ],
      "title": "Reversal of fortune: the resurgence of individual risk assessment in criminal justice",
      "venue": "Annu. Rev. Law Soc. Sci.,",
      "year": 2005
    },
    {
      "authors": [
        "Steinhart",
        "David"
      ],
      "title": "Juvenile detention risk assessment: A practice guide to juvenile detention reform",
      "venue": "Annie E. Casey Foundation,",
      "year": 2006
    },
    {
      "authors": [
        "Tibbitts",
        "Clark"
      ],
      "title": "Success or failure on parole can be predicted: A study of the records of 3,000 youths paroled from the illinois state reformatory",
      "venue": "Journal of Criminal Law and Criminology",
      "year": 1931
    },
    {
      "authors": [
        "Tibshirani",
        "Robert"
      ],
      "title": "Regression shrinkage and selection via the lasso",
      "venue": "Journal of the Royal Statistical Society. Series B (Methodological),",
      "year": 1996
    },
    {
      "authors": [
        "Tollenaar",
        "Nikolaj",
        "P.G.M. van der Heijden"
      ],
      "title": "Which method predicts recidivism best?: a comparison of statistical, machine learning and data mining predictive models",
      "venue": "Journal of the Royal Statistical Society: Series A (Statistics in Society),",
      "year": 2013
    },
    {
      "authors": [
        "Turner",
        "Susan",
        "James Hess",
        "Jesse Jannetta"
      ],
      "title": "Development of the California Static Risk Assessment Instrument (CSRA). University of California, Irvine, Center for Evidence-Based Corrections",
      "year": 2009
    },
    {
      "authors": [
        "Ustun",
        "Berk"
      ],
      "title": "slim for matlab v0.1, March 2016",
      "venue": "URL http://dx.doi.org/10.5281/zenodo",
      "year": 2016
    },
    {
      "authors": [
        "Ustun",
        "Berk",
        "Cynthia Rudin"
      ],
      "title": "Supersparse linear integer models for optimized medical scoring systems",
      "venue": "Machine Learning, pages 1\u201343,",
      "year": 2015
    },
    {
      "authors": [
        "Wang",
        "Fulton",
        "Cynthia Rudin"
      ],
      "title": "Falling rule lists",
      "venue": "In Proceedings of Artificial Intelligence and Statistics (AISTATS),",
      "year": 2015
    },
    {
      "authors": [
        "Zeng",
        "Ustun",
        "Rudin Webster",
        "Christopher D"
      ],
      "title": "HCR-20: Assessing risk for violence. Technical report, Mental Health, Law, and Policy Institute, Simon Fraser University, in cooperation with the British Columbia",
      "venue": "Forensic Psychiatric Services Commission,",
      "year": 1997
    },
    {
      "authors": [
        "Wolfgang",
        "Marvin E"
      ],
      "title": "Delinquency in a birth cohort",
      "year": 1987
    },
    {
      "authors": [
        "Wroblewski",
        "Jonathan J"
      ],
      "title": "Annual letter, U.S. department of justice",
      "venue": "Criminal division,",
      "year": 2014
    },
    {
      "authors": [
        "Wu",
        "Xindong",
        "Vipin Kumar",
        "Ross Quinlan",
        "Joydeep Ghosh",
        "Qiang Yang",
        "Hiroshi Motoda",
        "Geoffrey Mclachlan",
        "Angus Ng",
        "Bing Liu",
        "Philip Yu",
        "Zhi-Hua Zhou",
        "Michael Steinbach",
        "David Hand",
        "Dan Steinberg"
      ],
      "title": "Top 10 algorithms in data mining",
      "venue": "Knowledge and Information Systems,",
      "year": 2008
    },
    {
      "authors": [
        "Yang",
        "Min",
        "Stephen CP Wong",
        "Jeremy Coid"
      ],
      "title": "The efficacy of violence prediction: a meta-analytic comparison of nine risk assessment tools",
      "venue": "Psychological bulletin,",
      "year": 2010
    },
    {
      "authors": [
        "Zadrozny",
        "Bianca",
        "Charles Elkan"
      ],
      "title": "Transforming classifier scores into accurate multiclass probability estimates",
      "venue": "In Proceedings of the eighth ACM SIGKDD international conference on Knowledge discovery and data mining,",
      "year": 2002
    },
    {
      "authors": [
        "Zhang",
        "Yan",
        "Lening Zhang",
        "Michael S Vaughn"
      ],
      "title": "Indeterminate and determinate sentencing models: A state-specific analysis of their effects on recidivism",
      "venue": "Crime & Delinquency,",
      "year": 2009
    }
  ],
  "sections": [
    {
      "text": "Interpretable Classification Models for Recidivism Prediction\nJiaming Zeng\u2020, Berk Ustun\u2020, Cynthia Rudin\n\u2020These authors contributed equally to this work.\nSummary. We investigate a long-debated question, which is how to create predictive models of recidivism that are sufficiently accurate, transparent, and interpretable to use for decision-making. This question is complicated as these models are used to support different decisions, from sentencing, to determining release on probation, to allocating preventative social services. Each case might have an objective other than classification accuracy, such as a desired true positive rate (TPR) or false positive rate (FPR). Each (TPR, FPR) pair is a point on the receiver operator characteristic (ROC) curve. We use popular machine learning methods to create models along the full ROC curve on a wide range of recidivism prediction problems. We show that many methods (SVM, SGB, Ridge Regression) produce equally accurate models along the full ROC curve. However, methods that designed for interpretability (CART, C5.0) cannot be tuned to produce models that are accurate and/or interpretable. To handle this shortcoming, we use a recent method called Supersparse Linear Integer Models (SLIM) to produce accurate, transparent, and interpretable scoring systems along the full ROC curve. These scoring systems can be used for decision-making for many different use cases, since they are just as accurate as the most powerful black-box machine learning models for many applications, but completely transparent, and highly interpretable.\nKeywords: recidivism, machine learning, interpretability, scoring systems, binary classification"
    },
    {
      "heading": "1. Introduction",
      "text": "Forecasting has been used for criminology applications since the 1920s (Borden, 1928; Burgess, 1928) when various factors derived from age, race, prior offense history, employment, grades, and neighborhood background were used to estimate success of parole. Many things have changed since then, including the fact that we have developed machine learning methods that can produce accurate predictive models, and have collected large high-dimensional datasets on which to apply them.\nRecidivism prediction is still extremely important. In the United States, for example, a minority of individuals commit the majority of the crimes (Wolfgang, 1987): these are the \u201cpower few\u201d of Sherman (2007) on which we should focus our efforts. We want to ensure that public resources are directed effectively, be they correctional facilities or preventative social services. Milgram (2014) recently discussed the critical importance of accurately predicting if an individual who is released on bail poses a risk to public safety, pointing out that high-risk individuals are being released 50% of the time while low-risk individuals are being released less often then they should be. Her observations are in line with longstanding work on clinical versus actuarial judgment, which shows that humans, on their own, are not as good at risk assessment as statistical models (Dawes et al., 1989; Grove and Meehl, 1996). This is the reason that several U.S. states have mandated the use of predictive models for sentencing decisions (Pew Center of the States, Public Safety Performance Project, 2011; Wroblewski, 2014).\nThere has been some controversy as to whether sophisticated machine learning methods (such as random forests, see e.g., Breiman, 2001b; Berk et al., 2009; Ritter, 2013) are necessary to produce accurate predictive models of recidivism, or if traditional approaches such as logistic regression or linear discriminant analysis would suffice (see e.g., Tollenaar and van der Heijden, 2013; Berk and Bleich, 2013; Bushway, 2013). Random forests may produce accurate predictive models, but these models effectively operate as black-boxes, which make it difficult to understand how the input variables are producing a predicted outcome. If a simpler, more transparent, but equally accurate predictive model could be developed, it would be more usable and defensible for many decision-making applications. There is a precedent for using such models in criminology (Steinhart, 2006; Andrade, 2009); Ridgeway (2013) argues that a \u201cdecent transparent model that is actually used will outperform a sophisticated system that predicts better but sits on a shelf.\u201d This discussion is captured nicely by Bushway (2013), who contrasts the works of Berk and Bleich (2013) and Tollenaar and van der Heijden\nar X\niv :1\n50 3.\n07 81\n0v 6\n[ st\nat .M\nL ]\n8 J\nul 2\n01 6\n(2013). Berk and Bleich (2013) claim we need sophisticated machine learning methods due to their substantial benefits in accuracy, whereas Tollenaar and van der Heijden (2013) claim that \u201cmodern statistical, data mining and machine learning models provides no real advantage over logistic regression and LDA,\u201d assuming that humans have done appropriate pre-processing. In this work, we argue that the answer to the question is far more subtle than a simple yes or no.\nIn particular, the answer depends on how the models will be used for decision-making. For each use case (e.g., sentencing, parole decisions, policy interventions), one might need a decision point at a different level of true positive rate (TPR) and false positive rate (FPR) (see also Ritter, 2013). Each (TPR, FPR) pair is a point on the receiver operator characteristic (ROC) curve. To determine if one method is better than another, one must consider the appropriate point along the ROC curve for decision-making. As we show, for a wide range of recidivism prediction problems, many machine learning methods (support vector machines, random forests) produce equally accurate predictive models along the ROC curve. However, there are trade-offs between accuracy, transparency, and interpretability: methods that are designed to yield transparent models (CART, C5.0) cannot be tuned to produce as accurate models along the ROC curve, and do not always yield models that are interpretable. This is not to say that interpretable models for recidivism prediction do not exist. The fact that many machine learning methods produce models with similar levels of predictive accuracy indicates that there is a large class of approximately-equally-accurate predictive models (called the \u201cRashomon\u201d effect by Breiman 2001a). In this case, there may exist interpretable models that also attain the same level of accuracy. Finding models that are accurate and interpretable, however, is computationally challenging.\nIn this paper, we explore whether such accurate-yet-interpretable models exist and how to find them. To this end, we use a new machine learning method known as a Supersparse Linear Integer Model (SLIM; Ustun and Rudin, 2015) to learn scoring systems from data. Scoring systems that have used for many criminal justice applications because they let users make quick predictions by adding, subtracting and multiplying a few small numbers (see e.g., Hoffman and Adelberg, 1980; U.S. Sentencing Commission, 1987; Pennsylvania Commission on Sentencing, 2012). In contrast to existing tools, which have been built using heuristic approaches (see e.g., Gottfredson and Snyder, 2005), the models built by SLIM are fully optimized for accuracy and sparsity, and can handle additional constraints (e.g., bounds on the false positive rate, monotonicity properties for the coefficients). We use SLIM to produce a set of simple scoring systems at different decision points across the full ROC curve, and provide a comparison with other popular machine learning methods. Our findings show that the SLIM scoring systems are often just as accurate as the most powerful black-box machine learning models, but transparent and highly interpretable."
    },
    {
      "heading": "1.1. Structure",
      "text": "The remainder of this paper is structured as follows. In Section 1.2, we discuss related work. In Section 2, we describe how we derived 6 recidivism prediction problems. In Section 3, we provide a brief overview of SLIM and describe several new techniques that can reduce the computation required to produce scoring systems. In Section 4, we compare the accuracy and interpretability of models produced by the 9 machine learning methods on the 6 recidivism prediction problems. We include additional results related to the accuracy and interpretability of models from different methods in the Appendix."
    },
    {
      "heading": "1.2. Related Work",
      "text": "Predictive models for recidivism have been in widespread use in different countries and different areas of the criminal justice system since the early 1920s (see e.g., Borden, 1928; Burgess, 1928; Tibbitts, 1931). The use of these tools has been spurred on by continued research into the superiority of actuarial judgment (Dawes et al., 1989; Grove and Meehl, 1996) as well as a desire to efficiently use limited public resources (Clements, 1996; Simon, 2005; McCord, 1978, 2003). In the U.S., federal guidelines currently mandate the use of a predictive recidivism measure known as the Criminal History Category for sentencing (U.S. Sentencing Commission, 1987). Besides the U.S., countries that currently use risk assessment tools include Canada (Hanson and Thornton, 2003), the Netherlands (Tollenaar and van der Heijden, 2013), and the U.K. (Howard et al., 2009). Applications of these tools can be seen in evidence-based sentencing (Hoffman, 1994), corrections and prison administration (Belfrage et al., 2000), informing release on parole (Pew Center of the States, Public Safety Performance Project, 2011), determining the level of supervision during parole (Barnes\nand Hyatt, 2012; Ritter, 2013), determining appropriate sanctions for parole violations (Turner et al., 2009), and targeted policy interventions (Lowenkamp and Latessa, 2004).\nOur paper focuses on binary classification models to predict general recidivism (i.e., recidivism of any type of crime) as well as crime-specific recidivism (i.e., recidivism for drug, general violence, domestic violence, sexual violence, and fatal violence offenses). Risk assessment tools for general recidivism include: the Salient Factor Score (Hoffman and Adelberg, 1980; Hoffman, 1994), the Offender Group Reconviction Scale (Copas and Marshall, 1998; Maden et al., 2006; Howard et al., 2009), the Statistical Information of Recidivism scale (Nafekh and Motiuk, 2002), and the Level of Service/Case Management Inventory (Andrews and Bonta, 2000). Crime-specific applications include risk assessment tools for domestic violence (see e.g., the Spousal Abuse Risk Assessment of Kropp and Hart, 2000), sexual violence (see e.g., Hanson and Thornton, 2003; Langton et al., 2007), and general violence (see e.g., Historical Clinical and Risk Management tool of Webster et al. 1997, or the Structured Assessment of Violence Risk in Youth tool of Borum 2006).\nThe scoring systems that we present in this paper are designed to mimic the form of risk scores that are currently used throughout the criminal justice system \u2013 that is, linear classification models that only require users to add, subtract and multiply a few small numbers to make a prediction (Ustun and Rudin, 2015). These tools are unique in that they allow users make quick predictions by hand, without a computer, calculator, or nomogram (which is a visualization tool for more difficult calculations). Current examples of such tools include: the Salient Factor Score (SFS) (Hoffman and Adelberg, 1980), the Criminal History Category (CHC) (U.S. Sentencing Commission, 1987), and the Offense Gravity Score (OGS) (Pennsylvania Commission on Sentencing, 2012). Our approach aims to produce scoring systems that are fully optimized for accuracy and sparsity without any post-processing. In contrast, current tools are produced through heuristic approaches that primarily involve logistic regression with some ad-hoc post processing to ensure that the models are sparse and use integer coefficients (see e.g., the methods described in Gottfredson and Snyder, 2005).\nOur scoring systems differ from existing tools in that they directly output a predicted outcome (i.e., prisoner i will recidivate) as opposed to an predicted probability of the outcome (i.e. the predicted probability that prisoner i will recidivate is 90%). The predicted probabilities from existing tools are typically converted into an outcome by imposing a threshold (i.e., classify a prisoner as \u201chigh-risk\u201d if the predicted probability of arrest > 70%). In practice, users arbitrarily pick several thresholds to translate predicted probabilities into an ordinal outcome (e.g., prisoner i is \u201clow risk,\u201d if the predicted probability is < 30%, \u201cmedium risk\u201d if the predicted probability is< 60%, and \u201chigh risk\u201d otherwise). These arbitrary threshholds make it difficult, if not impossible, to effectively assess the predictive accuracy of the tools (Hannah-Moffat, 2013). Netter (2007), for instance, mentions that \u201cthe possibility of making a prediction error (false positive or false negative) using a risk tool is probable, but not easily determined.\u201d In contrast to existing tools, the scoring systems let users assess accuracy in a straightforward way (i.e., through the true positive rate and true negative rate). Further, our approach has the advantage that is can yield a scoring system that optimizes the class-based accuracy at a particular decision point (i.e., produce the model that maximizes the true positive rate, given a false-positive rate of at most 30%).\nOur work is related to a stream of research that has aimed to leverage new methods for predictive modeling in criminology. In contrast to our work, much of the research to date has focused on improving predictive accuracy by training powerful black-box models such as random forests (Breiman, 2001b) and stochastic gradient boosting Friedman (2002). Random forests (Breiman, 2001b), in particular, have been used for several criminological applications, including: predicting homicide offender recidivism (Neuilly et al., 2011); predicting serious misconduct among incarcerated prisoners (Berk et al., 2006); forecasting potential murders for criminals on probation or parole (Berk et al., 2009); forecasting domestic violence and help inform court decisions at arraignment (Berk and Sorenson, 2014). We note that not all studies in used black-box models: Berk et al. (2005), for instance, help the Los Angeles Sheriff\u2019s Department develop a simple and practical screener to forecast domestic violence using decision trees. More recently, (Goel et al., 2015), developed a simple scoring system to help the New York Police Department address stop and frisk by first running logistic regression, and then rounding the coefficients."
    },
    {
      "heading": "2. Data and Prediction Problems",
      "text": "Each problem is a binary classification problem with N = 33, 796 prisoners and P = 48 input variables. The goal is to predict whether a prisoner will be arrested for a certain type of crime within 3 years of being\nreleased from prison. In what follows, we describe how we created each prediction problem."
    },
    {
      "heading": "2.1. Database Details",
      "text": "We derived the recidivism prediction problems in our paper from the \u201cRecidivism of Prisoners Released in 1994\u201d database, assembled by the U.S. Department of Justice, Bureau of Justice Statistics (2014). It is the largest publicly available database on prisoner recidivism in the United States. The study tracked 38,624 prisoners for 3 years following their release from prison in 1994. These prisoners were randomly sampled from the population of all prisoners released from 15 U.S. states (Arizona, California, Delaware, Florida, Illinois, Maryland, Michigan, Minnesota, New Jersey, New York, North Carolina, Ohio, Oregon, Texas, and Virginia). The sampled population accounts for roughly two-thirds of all prisoners that were released from prison in the U.S. in 1994. Other studies that use this database include: Bhati and Piquero (2007); Bhati (2007); Zhang et al. (2009).\nThe database is composed of 38,624 rows and 6,427 columns, where each row represents a prisoner and each column represents a feature (i.e. a field of information for a given prisoner). The 6,427 columns consist of 91 fields that were recorded before or during release from prison in 1994 (e.g., date of birth, effective sentence length), and 64 fields that were repeatedly recorded for up to 99 different arrests in the 3 year follow-up period (e.g., if a prisoner was rearrested three times with 3 years, there would be three record cycles recorded). The information for each prisoner is sourced from record-of-arrest-and-prosecution (RAP) sheets kept by state law enforcement agencies and/or the FBI. A detailed descriptive analysis of the database was carried out by statisticians at the U.S. Bureau of Justice Statistics (Langan and Levin, 2002). This study restricted its attention to 33,796 of the 38,624 prisoners to exclude extraordinary or unrepresentative release cases. To be selected for the analysis of Langan and Levin (2002), a prisoner had to be alive during the 3 year follow-up period, and had to have been released from prison in 1994 for an original sentence that was at least 1 year or longer. Prisoners with certain release types \u2013 release to custody/detainer/warrant, absent without leave, escape, transfer, administrative release, and release on appeal \u2013 were excluded. To mirror the approach of Langan and Levin (2002), we restricted our attention to the same subset of prisoners.\nThis dataset has some serious flaws which we point out below. To begin, many important factors that could be used to predict recidivism are missing, and many included factors are noisy enough to be excluded from our preliminary experiments. The information about education levels is extremely minimal; we do not even know whether each prisoner attended college, or completed high school. The information about courses in prison is only an indicator of whether the inmate took any education or vocation courses at all. Also, there is no family history for each prisoner (e.g., foster care) and no record of visitors while in prison (e.g., indicators of caring family members or friends). There is no information about reentry programs or employment history. While some of these factors exist, such as drug or alcohol treatment and in-prison vocational programs, the data is highly incomplete and therefore excluded from our analysis. For example, for drug treatment, less than 14% of the prisoners had a valid entry. The rest were \u201cunknown.\u201d To include as many prisoners as possible, we chose to exclude factors with extremely sparse information."
    },
    {
      "heading": "2.2. Deriving Input Variables",
      "text": "We provide a summary of the P = 48 input variables derived from the database in Table 1. We encoded each input variable as a binary rule of the form xij \u2208 {0, 1}, j = 1 . . . , P , where xij = 1 if condition j holds true about prisoner i. This allows a linear model to encode nonlinear functions of the original variables. We refer to input variables in the text using italicized font (e.g., female). All prediction problems in Table 2 and all machine learning methods in Table 4 use these same input variables.\nThe final set of input variables are representative of well-known risk factors for recidivism (Bushway and Piehl, 2007; Crow, 2008) and have been used in risk assessment tools since 1928 (see e.g., Borden, 1928; Ricardo H. Hinojosa et al., 2005; Berk et al., 2006; Baradaran, 2013). They include: 1) information about prison release in 1994 (e.g., time served, age at release, infraction in prison); 2) information from past arrests, sentencing, and convictions (e.g., prior arrests\u22651, any prior jail time);1 3) history of substance abuse (e.g., alcohol abuse) 4) gender (e.g., female). These input variables are advantageous because: a) the\n1The prior arrest variable does not count the original crime for which they were released from prison in 1994; thus, about 12% of the prisoners have no prior arrests =1 even though they were arrested at least once.\ninformation is easily accessible to law enforcement officials (all above information can be found in state RAP sheets); b) they do not include socioeconomic factors such as race, which would directly eliminate the potential to use these tools in applications such as sentencing.\nWe note that encoding the input variables as binary values presents many advantages. They produce models that are easier to understand (removing the wide range presented by continuous variables), and they avoid potential confusion stemming from coefficients of normalized inputs (for instance, after undoing the normalization for normalized coefficients, a small coefficient might be highly influential if it applies to a variable taking large values). Binarization is especially useful for SLIM as we can fit SLIM models by solving a slightly easier discrete optimization problem when the data only contains binary input variables (as discussed in Section 3.3). In Appendix E, we explore the change in predictive accuracy if continuous variables are included and show that the changes in performance are minor for most methods. There are some exceptions; for example, CART and C5.0T experienced an improvement of 4.6% for drug and SVM RBF experienced a 7.7% improvement for fatal violence. Yet even for these methods, no clear improvement is seen across all problems."
    },
    {
      "heading": "2.3. Deriving Outcome Variables",
      "text": "We created a total of 6 recidivism prediction problems by encoding a binary outcome variable yi \u2208 {\u22121,+1} such that yi = +1 if a prisoner is arrested for a particular type of crime within 3 years after being released from prison. For clarity, we refer to each prediction problem in the text using typewriter font (e.g., arrest). We provide details on each recidivism prediction problems in Table 2. These include: an arrest for any crime (arrest); an arrest for a drug-related offense (drug); or an arrest for a certain type of violent offense (general violence, domestic violence, sexual violence, fatal violence).\nIn the dataset, all crime types can be broken down into smaller subcategories (e.g., fatal violence can be broken into 6 subcategories such as murder, vehicular manslaughter, etc.). We chose to use the broader crime categories for the sake of conciseness and clarity. Indeed, the study by Langan and Levin (2002) also split the crimes into the same major categories. We note that the outcomes of violent offenses are mutually exclusive, as different types of violence are treated differently within the U.S. legal system. In other words, yi = +1 for general violence does not necessarily imply yi = +1 for domestic violence, sexual violence, fatal violence)."
    },
    {
      "heading": "2.4. Relationships between Input and Output Variables",
      "text": "Table 3 lists the conditional probabilities P (y = 1|xj = 1) between the outcome variable y and each input variable xj for all prediction problems. Using this table, we can identify strong associations between the input and output for each prediction problem. These associations can help uncover insights into each problem and also help qualitatively validate predictive models in Section 4.4.\nConsider, for instance, the arrest problem. Here, we can see that prisoners who are released from prison at a later age are less likely to be arrested (as the probability for arrest decreases monotonically as age at release increases). This also appears to be the case for prisoners who were first confined (i.e., sent to prison or jail) at an older age (see e.g., age of first confinement). In addition, we can also see that prisoners with more prior arrests have a higher likelihood of being arrested (as the probability for arrest increases monotonically with prior arrest).\nSimilar insights can be made for crime-specific prediction problems. In drug, for instance, we see that prisoners who were previously arrested for a drug-related offense are more likely to be rearrested for a drugrelated offense (32%) than those who were previously arrested for any other type of offense. Likewise, looking at domestic violence, we see that the prisoners with the greatest probability of being arrested for a domestic violence crime are those with a history of domestic violence (13%)."
    },
    {
      "heading": "3. Supersparse Linear Integer Models",
      "text": "A Supersparse Linear Integer Model (SLIM) is a new machine learning method for creating scoring systems \u2013 that is, binary classification models that only require users to add, subtract and multiply a few small numbers to make a prediction (Ustun and Rudin, 2015). Scoring systems are widely used because they allow users to make quick predictions, without the use of a computer, and without extensive training in statistics. These models are also useful because their high degree of sparsity and integer coefficients let users easily gauge the influence of multiple input variables on the predicted outcome (see Section 4.4 for an example). In what follows, we provide a brief overview of SLIM, and provide several new techniques to reduce the computation for problems with binary input variables."
    },
    {
      "heading": "3.1. Framework and Optimization Problem",
      "text": "SLIM scoring systems are linear classification models of the form:\ny\u0302i =  +1 if P\u2211 j=1 \u03bbjxij > \u03bb0\n\u22121 if P\u2211 j=1 \u03bbjxij \u2264 \u03bb0.\nHere, \u03bb1, . . . , \u03bbP represent the coefficients (i.e. the \u201cpoints\u201d for input variables j = 1, . . . , P ), and \u03bb0 represents an intercept (i.e. the \u201cthreshold score\u201d that has to be surpassed to predict y\u0302i = +1).\nThe values of the coefficients are determined from data by solving a discrete optimization problem that has the following form:\nmin \u03bb\n1\nN N\u2211 i=1 1 [yi 6= y\u0302i] + C0 P\u2211 j=1 1 [\u03bbj 6= 0] + P\u2211 j=1 |\u03bbj |\ns.t. (\u03bb0, \u03bb1, ..., \u03bbP ) \u2208 L.\n(1)\nHere, the objective directly minimizes the error rate 1N \u2211N\ni=1 1 [yi 6= y\u0302i] and directly penalizes the number of non-zero terms \u2211P j=1 1 [\u03bbj 6= 0]. The constraints restrict coefficients to a finite set such as L = {\u221210, . . . , 10}P+1. Optionally, one could include additional operational constraints on the accuracy and interpretability of the desired scoring system.\nThe objective includes a tiny penalty on the absolute value of the coefficients to restrict coefficients to coprime values without affecting accuracy or sparsity. To illustrate the use of this penalty, consider a classifier such as y\u0302 = sign (x1 + x2). If SLIM only minimized the misclassification rate and the number of terms (the first two terms of the objective), then y\u0302 = sign (2x1 + 2x2) would have the same objective value as y\u0302 = sign (x1 + x2) because it makes the same predictions and has the same number of non-zero coefficients. Since coefficients are restricted to a discrete set, we use this tiny penalty on the absolute value of these coefficients so that SLIM chooses the classifier with the smallest (coprime) coefficients, y\u0302 = sign (x1 + x2).\nThe C0 parameter represents the maximum accuracy that SLIM is willing to sacrifice to remove a feature from the optimal scoring system. If, for instance, C0 is set within the range (1/N, 2/N), we would sacrifice the accuracy of one observation to have a model with one fewer feature. Given C0, we can set the `1-penalty parameter to any value\n0 < < min(1/N,C0) max{\u03bbj}j\u2208L \u2211P j=1 |\u03bbj |\nso that it does not affect the accuracy or sparsity of the optimal classifier, but only induces the coefficients to be coprime for the features that are selected.\nSLIM differs from traditional machine learning methods because it directly optimizes accuracy and sparsity without making approximations that other methods make for scalability (e.g., controlling for accuracy using convex surrogate loss functions). By avoiding these approximations, SLIM sacrifices the ability to fit a model in seconds or in a way that scales to extremely large datasets. In return, however, it gains the ability to\nfit models that are highly customizable, since one could directly encode a wide range of operational constraints into its integer programming formulation. In this paper, we primarily make use of a simple constraint to limit the number of non-zero coefficients, however, it is also natural to incorporate constraints on class-specific accuracy, structural sparsity, and prediction (see Ustun and Rudin, 2015).\nIn this paper we trained the following version of SLIM, which is different than (1) in that it includes class weights, and has specific constraints on the coefficients:\nmin \u03bb\nW+\nN \u2211 i\u2208I+ 1 [yi 6= y\u0302i] + W\u2212 N \u2211 i\u2208I\u2212 1 [yi 6= y\u0302i] + C0 P\u2211 j=1 1 [\u03bbj 6= 0] + P\u2211 j=1 |\u03bbj |\ns.t. P\u2211 j=1 1 [\u03bbj 6= 0] \u2264 8\n\u03bbj \u2208 {\u221210, . . . , 10} for j = 1...P \u03bb0 \u2208 {\u2212100, . . . , 100}.\n(2)\nIn the formulation above, the constraints restrict each coefficient \u03bbj to an integer between \u221210 and 10, the threshold \u03bb0 to an integer between \u2212100 and 100, the number of non-zero to at most 8 (i.e., within the range of cognitive entities humans could handle, as per Miller, 1956). The parameters W+ and W\u2212 are class-based weights that control the accuracy on positive and negative examples. We typically choose values of W+ and W\u2212 such thatW++W\u2212 = 2, so that we recover an error-minimizing formulation by settingW+ = W\u2212 = 1. The C0 parameter was set to a sufficiently small value so that SLIM would not sacrifice accuracy for sparsity: given W+ and W\u2212, we can set C0 to any value\n0 < C0 < min{W\u2212,W+}/(N \u00d7 P )\nto ensure this condition. The parameter was set to a sufficiently small value so that SLIM would produce a model with coprime coefficients without affecting accuracy or sparsity: given W+, W\u2212 and C0, we can set to any value 0 < < C0/max \u2211P j=1 |\u03bbj | to ensure this condition."
    },
    {
      "heading": "3.2. General SLIM IP Formulation",
      "text": "Training a SLIM scoring system requires solving an integer programming (IP) problem using a solver such as CPLEX, Gurobi, or CBC. In general, we use the following IP formulation to recover the solution to the optimization problem (2):\nmin \u03bb,z,\u03a6,\u03b1,\u03b2\n1\nN N\u2211 i=1 zi + P\u2211 j=1 \u03a6j\ns.t. Mizi \u2265 \u03b3 \u2212 P\u2211 j=0 yi\u03bbjxi,j i = 1...N error on i (3a)\n\u03a6j = C0\u03b1j + \u03b2j j = 1...P penalty for coef j (3b) \u2212\u039bj\u03b1j \u2264 \u03bbj \u2264 \u039bj\u03b1j j = 1...P `0-norm (3c) \u2212\u03b2j \u2264 \u03bbj \u2264 \u03b2j j = 1...P `1-norm (3d) \u03bbj \u2208 Z \u2229 [\u2212\u039bj ,\u039bj ] j = 0...P coefficient set zi \u2208 {0, 1} i = 1...N loss variables\n\u03a6j \u2208 R+ j = 1...P penalty variables \u03b1j \u2208 {0, 1} j = 1...P `0 variables \u03b2j \u2208 R+ j = 1...P. `1 variables\nThe constraints in (3a) compute the error rate by setting the loss variables zi = 1 [ yi\u03bb Txi \u2264 0 ]\nto 1 if a linear classifier with coefficients \u03bb misclassifies example i (or is close to misclassifying it, depending on the margin \u03b3). This is a Big-M constraint for the error rate that depends on scalar parameters \u03b3 and Mi (see e.g., Rubin, 2009). The value of Mi represents the maximum score when example i is misclassified, and can be set as Mi = max\u03bb\u2208L(\u03b3 \u2212 yi\u03bbTxi) which is easy to compute since L is finite. The value of \u03b3 represents the margin, and the objective is penalized when points are either incorrectly classified, or within\n\u03b3 of the decision boundary. How close a point is to the decision boundary (or whether it is misclassified) is determined by yi\u03bbTxi. When the features are binary, and since the coefficients are integers, \u03b3 can naturally be set to any value between 0 and 1. (In other cases, we can set \u03b3 = 0.5 for instance, which makes an implicit assumption on the values of the features.) The constraints in (3b) set the total penalty for each coefficient to \u03a6j = C0\u03b1j + \u03b2j , where \u03b1j := 1 [\u03bbj 6= 0] is defined by Big-M constraints in (3c), and \u03b2j := |\u03bbj | is defined by the constraints in (3d). We denote the largest absolute value of each coefficient as \u039bj := max\u03bbj\u2208Lj |\u03bbj |.\nRestricting coefficients to a finite set results in significant practical benefits for the SLIM IP formulation, especially in comparison to other IP formulations that minimize the 0\u20131-loss and/or penalize the `0-norm. Without the restriction of \u03bb to a bounded set, we would not have a natural choice for the Big-M constant, which means the user chooses one that is very large, leading to a less efficient formulation (see e.g., Wolsey, 1998). For SLIM, the Big-M constants used to compute the 0\u20131 loss in constraint (3a) is bounded as Mi \u2264 max\u03bb\u2208L(\u03b3\u2212 yi\u03bbTxi), and the Big-M constant used to compute the `0-norm in constraints (3c) is bounded as \u039bj \u2264 max\u03bbj\u2208Lj |\u03bbj |. Bounding these constants lead to a tighter LP relaxation, which narrows the integrality gap, and improves the ability of commercial IP solvers to obtain a proof of optimality more quickly."
    },
    {
      "heading": "3.3. Improved SLIM IP Formulation",
      "text": "The following formulation provides a tighter relaxation of the IP which reduces computation. It relies on the fact that when the input variables are binary, we are likely to get repeated feature values among observations.\nmin \u03bb,z,\u03a6,\u03b1,\u03b2\nW+\nN \u2211 s\u2208S nszs + W\u2212 N \u2211 t\u2208T ntzt + P\u2211 j=1 \u03a6j\ns.t. Mszs \u2265 1\u2212 P\u2211 j=0 \u03bbjxs,j s \u2208 S error on s(4a)\nMtzt \u2265 P\u2211 j=0 \u03bbjxt,j t \u2208 T error on t(4b)\n1 = zs + zt \u2200s, t : xs = xt, ys = \u2212yt conflicting labels(4c) \u03a6j = C0\u03b1j + \u03b2j j = 1...P penalty for coef j(4d) \u2212\u039bj\u03b1j \u2264 \u03bbj \u2264 \u039bj\u03b1j j = 1...P `0-norm(4e) \u2212\u03b2j \u2264 \u03bbj \u2264 \u03b2j j = 1...P `1-norm (4f) \u03bbj \u2208 Z \u2229 [\u2212\u039bj ,\u039bj ] j = 0...P coefficient set\nzs, zt \u2208 {0, 1} s \u2208 S t \u2208 T loss variables \u03a6j \u2208 R+ j = 1...P penalty variables \u03b1j \u2208 {0, 1} j = 1...P `0 variables \u03b2j \u2208 R+ j = 1...P. `1 variables\nThe main difference between this formulation and the one in (3) is that we compute the error rate of the classifier using loss constraints that are expressed in terms of the number of distinct points in the dataset. Here, the set S represents the set of distinct points with positive labels, and the set T represents the set of distinct points with negative examples. The parameters ns (and nt) count the number of times a point of type s (or t) are found in the original dataset so that \u2211 s ns = \u2211N i=1 1 [yi = +1], \u2211 t nt = \u2211N i=1 1 [yi = \u22121], and\nN = \u2211 s ns + \u2211\nt nt. The main computational benefits of this formulation are due to the fact that: (i) we can reduce the number of loss constraints by counting the number of repeated rows in the dataset; and (ii) we can directly encode a lower bound on the error rate by counting the number of points s, t with identical feature but opposite labels (i.e., xs = xt but ys = \u2212yt). Here (i) reduces the size of the problem that we pass to an IP solver, and (ii) produces a much stronger lower bound on the 0\u20131 loss (in comparison to the LP relaxation), which speeds up the progress of branch-and-bound type algorithms. Note that it would be possible to use this formulation on a dataset without binary input variables, though it would not necessarily be effective because it could be much less likely for a dataset to contain repeated rows in such a setting.\nAnother subtle benefit of this formulation is that the margin for the negative points is 0 while the margin for the positive points is 1. This means that for positive points, we have a correct prediction if and only if the\nscore \u2265 1. For negative points, we have a correct prediction if and only if the score \u2264 0. This provides a slight computational advantage since the negative points do not need to have scores below -1 to be correctly classified, which reduces the size of the Big-M parameter and the coefficient set. For instance, say we would to produce a linear model that encode: \u201cpredict rearrest unless a1 or a2 are true.\u201d Using the previous formulation with the margin of \u03b3 \u2208 (0, 1) on both positives and negatives, the optimal SLIM classifier would be: \u201crearrest = sign(1\u2212 2a1 \u2212 2a2).\u201d In contrast, the margin of the current formulation is: \u201crearrest = sign(1\u2212 a1 \u2212 a2)\u201d, which uses smaller coefficients, and produces a slightly simpler model."
    },
    {
      "heading": "3.4. Active Set Polishing",
      "text": "On large datasets, IP solvers may take long time to produce an optimal solution or provider users with a certificate of optimality. Here, we present a polishing procedure that can be used to improve the quality of solutions locally. For a fixed set of features, this procedure optimizes the values of coefficients.\nThe polishing procedure takes as input a feasible set of coefficients from the SLIM IP \u03bbfeasible, and returns a polished set of coefficients \u03bbpolished by solving a a simpler IP formulation shown in (5). The polishing IP only optimizes the coefficients of features that belong to the active set of \u03bbfeasible: that is, the set of features with nonzero coefficients A := { j : \u03bbfeasiblej 6= 0 } . The coefficients for features that do not belong to the active set are fixed to zero so that \u03bbj = 0 for j /\u2208 A. In this way, the optimization no longer involves feature selection, and the formulation becomes much easier to solve.\nmin \u03bb,z,\u03a6,\u03b1,\u03b2\nW+\nN \u2211 s\u2208S nszs + W\u2212 N \u2211 t\u2208T ntzt (5a)\ns.t. Mszs \u2265 1\u2212 \u2211 j\u2208A \u03bbjxs,j s \u2208 S error on s (5b)\nMtzt \u2265 \u2211 j\u2208A \u03bbjxt,j t \u2208 T error on t (5c)\n1 = zs + zt \u2200s, t : xs = xt, ys = \u2212yt conflicting labels (5d) \u03bbj \u2208 Z \u2229 [\u2212\u039bj ,\u039bj ] j \u2208 A coefficient set\nzs, zt \u2208 {0, 1} s \u2208 S t \u2208 T . loss variables\nThe polishing IP formulation is especially fast to solve to optimality for classification problems with binary input variables because this limits the number of loss constraints. Say for instance that we wish to polish a set of coefficients with only 5 nonzero variables, then there are at most |{\u22121,+1}| \u00d7 |{0, 1}5| = 64 possible unique data points, and thus the same number of possible loss constraints.\nIn our experiments in Section 4, we use the polishing procedure on all of the feasible solutions we find from the earlier formulation. In all cases, we can solve the polishing IP to optimality within a few seconds (i.e. a MIPGAP of 0.0%)."
    },
    {
      "heading": "4. Experimental Results",
      "text": "In this section, we compare the accuracy and interpretability of recidivism prediction models from SLIM to models from 8 other popular classification methods. In Section 4.1, we explain the experimental setup used for all the methods. In Section 4.2, we compare the predictive accuracy of the methods with the AUC values and ROC curves. In Section 4.3 and 4.4, we evaluate the interpretability of the models. Finally, in Section 4.5, we present the scoring systems generated by SLIM."
    },
    {
      "heading": "4.1. Methodology",
      "text": "In what follows we discuss cost-sensitive classification for imbalanced problems, provide an overview of techniques."
    },
    {
      "heading": "4.1.1. Evaluating Predictive Accuracy for Imbalanced Problems",
      "text": "The majority of classification problems that we consider are imbalanced, where the data contain a relatively small number of examples from one class and a relatively large number of examples from the other.\nImbalanced problems necessitate changes in the way that we evaluate the performance of classification models. Consider, for instance, a heavily imbalanced problem such as fatal violence where only P(yi = +1) = 0.7% of individuals are arrested within 3 years of being released from prison. In this case, a method that maximizes overall classification accuracy is likely to produce a trivial model that predicts no one will be arrested for fatal offenses \u2013 a result that is not surprising given that the trivial model is 99.3% accurate on the overall population. Unfortunately, this model will never be able to identify individuals that will be arrested for a fatal offense, and therefore be 0% accurate on the population of interest.\nTo provide a measure of classification model performance on imbalanced problems, we assess the accuracy of a model on the positive and negative classes separately. In our experiments, we report the class-based accuracy of each model using the true positive rate (TPR), which reflects the accuracy on the positive class, and the false positive rate (FPR), which reflects the error rate on negative class. For a given classification model, we compute these quantities as\nTPR = 1\nN+ \u2211 i\u2208I+ 1 [y\u0302i = +1] and FPR = 1 N\u2212 \u2211 i\u2208I\u2212 1 [y\u0302i = +1] ,\nwhere y\u0302i denotes the predicted outcome for example i, N+ denotes the number of examples in the positive class I+ = {i : yi = +1}, and N\u2212 denotes the number of examples from the negative class I\u2212 = {i : yi = \u22121}. Ideally, a classification model should have high TPR and low FPR (i.e., TPR close to 1 and FPR = 0).\nMost classification methods can be adapted to yield a model that is more accurate on the positive class, but only if we are willing to sacrifice some accuracy on examples from the negative class, and vice-versa. To illustrate the trade-off of classification accuracy between positive and negative classes, we plot all models produced by a given method as points on a receiver operating characteristic (ROC) curve, which plots the TPR on the vertical axis and the FPR on the horizontal axis. Having constructed an ROC curve, we then assess the overall performance of each method by calculating the area under the ROC curve (AUC).2 A detailed discussion of ROC analysis in recidivism prediction can be found in the work of Maloof (2003)."
    },
    {
      "heading": "4.1.2. Fitting Models over the Full ROC Curve using a Cost-Sensitive Approach",
      "text": "Different applications require predictive models at different points of the ROC curve. Models for sentencing, for example, need low FPR in order to avoid predicting that a low-risk individual will reoffend. Models for screening, however, need high TPR in order to capture as many high-risk individuals as possible. In our experiments, we use a cost-sensitive approach to produce classification models at different points of the ROC curve (see e.g., Berk, 2010, 2011). This approach involves controlling the accuracy on the positive and negative classes by tuning the misclassification costs for examples in each class. In what follows, we denote the misclassification cost on examples from the positive and negative classes as W+ and W\u2212, respectively. As we increase W+, the cost of making a mistake on a positive example increases, and we expect to obtain a model that classifies the positive examples more accurately (i.e. with higher TPR). We choose W+ and W\u2212 so that W+ + W\u2212 = 2. Thus, when W+ = 2, we obtain a trivial model that predicts y\u0302i = +1 and attains TPR = 1. When W+ = 0, we obtain a trivial model that predicts y\u0302i = \u22121 that attains FPR = 0."
    },
    {
      "heading": "4.1.3. Choice of Classification Methods",
      "text": "We compared SLIM scoring systems to models produced by eight popular classification methods, including those previously used for recidivism prediction (see Section 1.2) or those that ranked among the \u201ctop 10 algorithms in data mining\u201d (Wu et al., 2008). In choosing these methods, we restricted our attention to methods that have publicly-available software packages, and allow users to specify misclassification costs for positive and negative classes. Our final choice of methods includes:\n\u2022 C5.0 Trees and C5.0 Rules: C5.0 is an updated version of the popular C4.5 algorithm (Quinlan, 2014; Kuhn and Johnson, 2013) that can create decision trees and rule sets.\n2We note that AUC is a summary statistic that is frequently misused in the context of classification problems. It is true that a method that with AUC = 1 always produces models that are more accurate than a method with AUC = 0. Other than this simple case, however, it is not possible to state that a method with high AUC always produces models that are more accurate than a method with low AUC.\n\u2022 Classification and Regression Trees (CART): CART is a popular method to create decision trees through recursive partitioning of the input variables (Breiman et al., 1984).\n\u2022 L1 and L2-Penalized Logistic Regression: Variants of logistic regression that penalize the coefficients to prevent overfitting (Friedman et al., 2010). L1-penalized methods are typically used to create linear models that are sparse (Tibshirani, 1996; Hesterberg et al., 2008). The L2 regularized methods are called \u201cridge\u201d and are not generally sparse.\n\u2022 Random Forests: A popular black-box method that makes predictions using a large ensemble of weak classification trees. The method was originally developed by Breiman (2001b) but is widely used for recidivism prediction (see e.g., Berk et al., 2009; Ritter, 2013).\n\u2022 Support Vector Machines: A popular black-box method for non-parametric linear classification. The Radial Basis Function (RBF) kernel lets the method to handle classification problems where the decisionboundary may be non-linear (see e.g., Cristianini and Shawe-Taylor, 2000; Berk and Bleich, 2014).\n\u2022 Stochastic Gradient Boosting: A popular black-box method that create prediction models in the form of an ensemble of weaker prediction models (Friedman, 2001; Freund and Schapire, 1997)."
    },
    {
      "heading": "4.1.4. Details on Experimental Design, Parameter Tuning, and Computation",
      "text": "We summarize the methods, software, and settings that we used in our experiments in Table 4.\nFor each of the 6 recidivism prediction problems and each of the 9 methods, we constructed ROC curves by running the algorithm with 19 values ofW+. The values ofW+ were chosen to produce models across the full ROC curves. By default, we chose values of W+ \u2208 {0.1, 0.2, . . . , 1.9} and set W\u2212 = 2\u2212W+. These values ofW+ were inappropriate for problems with a significant class imbalance as all methods produced trivial models. Thus, for significantly imbalanced problems, such as domestic violence and sexual violence, we used values of W+ \u2208 {1.815, 1.820, . . . , 1.995}. For fatal violence, which was extremely imbalanced, we used W+ \u2208 {1.975, 1.976, . . . , 1.995}.\nThis setup requires us to produce a total of 1,026 recidivism prediction models (6 recidivism problems\u00d7 9 methods \u00d7 19 imbalance ratios). Each of the 1,026 models were built on a training set and their performance was assessed out-of-sample. In particular, 1/3 of the data was reserved as the test set. The remaining 2/3 of the data was the training set. During training, we used 5-fold nested cross-validation (5-CV) for parameter tuning. Explicitly, the training data were split into 5 folds, and one of those 5 was reserved as the validation fold. The validation fold was rotated in order to select free parameter values, and a final model was trained on the full training set (2/3) with the selected parameter values and its performance was assessed on the test set (1/3). The folds were generated once to allow for comparisons across methods and prediction problems. The parameters were chosen during nested cross validation to minimize the mean weighted 5-CV validation error on the training set. Having obtained a set of 19 different models for each method and each problem, we then constructed an ROC curve for that method on that problem by plotting the test TPR and test FPR of the 19 final models.\nWe trained all baseline methods using publicly available packages in R 3.2.2 (R Core Team, 2015) without imposing any time constraints. In comparison, we trained SLIM by solving integer programming problems (IP) with the CPLEX 12.6 API in MATLAB 2013a. We solved each IP through the following procedure: (i) we trained the solver on the formulation in Section 3.3 for a total of 4 hours on a local computing cluster with 2.7GHz CPUs. Each time we solved a IP we kept 500 feasible solutions, and polished them using the formulation in Section 3.4. We then used the same nested cross-validation procedure as the other methods to tune the number of terms in the final model. Polishing all 500 solutions took less than one minute of computing time. Thus, the total number of optimization problems we solved were 500 polishing IP\u2019s \u00d7 (5 folds + 1 final model) \u00d7 6 problems \u00d7 19 values of W+ = 342,000 integer programming problems."
    },
    {
      "heading": "4.2. Observations on Predictive Accuracy",
      "text": "We show ROC curves for all methods and prediction problems in Figure 1 and summarize the test AUC of each method in Table 5. Tables with the training and 5-CV validation AUC\u2019s for all methods are included in Appendix A.\nWe make the following important observations, which we believe carry over to a large class of problems beyond recidivism prediction:\n\u2022 All methods did well on the general recidivism prediction problem arrest. In this case, we observe only small differences in predictive accuracy of different methods: all methods other than CART attain a test AUC above 0.72; the highest test AUC of 0.73 was achieved by SGB, Ridge, and RF. This multiplicity of good models reflects the Rashomon effect of Breiman (2001b).\n\u2022 Major differences between methods appeared in their performance on imbalanced prediction problems. We expected different methods to respond differently to changes in the misclassification costs, and therefore trained each method over a large range of possible misclassification costs. Even so, it was difficult (if not impossible) to tune certain methods to produce models at certain points of the ROC curve (see e.g., problems with significant imbalance, such as fatal violence).\n\u2022 SVM RBF, SGB, Lasso and Ridge were able to produce accurate models at different points on the ROC curve for most problems. SGB usually achieved the highest AUC on most problems (e.g., arrest, drug, general violence, domestic violence, fatal violence). Lasso, Ridge, and SVM RBF often produce comparable AUCs. We find that these methods respond well to cost-sensitive tuning, but it is difficult to tune the misclassification costs for highly imbalanced problems, such as fatal violence, to get models at specific points on the ROC curve.\n\u2022 C5.0T, C5.0R and CART were unable to produce accurate models at different points on the ROC curve on any imbalanced problems. We found that these methods do not respond well to cost-sensitive tuning. The issue becomes markedly more severe as problems become more imbalanced. For drug and general violence, for instance, these methods could not produce models with high TPR. For fatal violence, sexual violence, and domestic violence, these methods almost always produced trivial models that predict y = \u22121 (resulting in AUCs of 0.5). This result may be attributed to the greedy nature of the algorithms used to fit the trees, as opposed to the use of tree models in general. The issue is unlikely to be software-related as it affects both C5.0 and CART, and has been observed by others (see e.g., Goh and Rudin, 2014). This problem might not occur if trees were better optimized.\n\u2022 In general, SLIM produced models that are close to or on the efficient frontier of the ROC curve, despite being restricted to a relatively small class of simple linear models (at most 8 non-zero coefficients from - 10 to 10). Even on highly imbalanced problems such as domestic violence and sexual violence, it responds well to changes in misclassification costs (as expected, by nature of its formulation).\nIn addition to predictive accuracy, we also examine the risk calibration of the models. Figure 2 show the risk calibration for arrest, constructed using the binning method from Zadrozny and Elkan (2002). We include calibration plots for all other problems in Appendix B. We see that SLIM is well-calibrated, even though there is no reason it should be; it is a decision-making tool, not a risk assessment tool. For arrest, Lasso and Ridge are well-calibrated; however, they lose this quality once we consider only sparse models (see Appendix D). This property would also be lost if the Lasso and Ridge coefficients were rounded."
    },
    {
      "heading": "4.3. Trade-offs Between Accuracy and Interpretability",
      "text": "In Appendix C, we show that the baseline methods are unable to maintain the same level of accuracy as they have in Section 4.2 when their model size was constrained. For Lasso, Ridge and SLIM, model size is defined as the number of features in the model. For CART and C5.0, model size is the number of leaves or rules. In fact, we find the only methods that can consistently produce accurate models along the full ROC curve and also have the potential for interpretability are SLIM and (non-sparse) Lasso.\nTree and rule-based methods such as CART, C5.0T and C5.0R were generally unable to produce models that attain high degrees of accuracy. Worse, even for balanced problems such as arrest, where these methods did produce accurate models, the models are complicated and use a very large number of rules or leaves (similar behavior for C5.0T/C5.0R is also observed by, for instance, Lim et al., 2000). As we show in Appendix C, it was not reasonably possible to obtain a C5.0R/C5.0T/CART model with at most 8 rules or 8 leaves for almost every prediction problem."
    },
    {
      "heading": "4.4. On the Interpretability of Equally Accurate Transparent Models",
      "text": "To assess the interpretability of different models, we provide a comparison of predictive models produced by SLIM, Lasso and CART for the arrest problem in Figures 3\u20135. This setup provides a nice basis for comparison as all three methods produce models at roughly the same decision point, and with the same degree of sparsity. For this comparison, we considered any transparent model with at most 8 coefficients (Lasso), 8 rules (C5.0R) or 8 leaves (C5.0T, CART) and had a test FPR of below 50%. We report the models with the minimum weighted test error. Here, neither C5.0R nor C5.0T could produce an acceptable model with at most 8 rules or 8 leaves, so only models from SLIM, CART and Lasso could be displayed. As described before, it is rare for Lasso and CART to produce models with a similar degree of accuracy to SLIM when model size is constrained. We make the following observations:\n\u2022 All three models attain similar levels of predictive accuracy. Test TPR values ranged between 70-79% and test FPR values ranged between 43-48%. There may not exist a classification model that can attain substantially higher accuracy.\n\u2022 The SLIM model uses 5 input variables and small integer coefficients (see e.g., Figure 3). There is a natural rule-based interpretation. In this case, the model implies that if the prisoner is young (age at release of 18 to 24) or has a history of arrests (prior arrests\u22655), he is highly likely to be rearrested. On the other hand, if he is relatively older (age at release\u226540) or has no history of arrests (no prior arrests), he is unlikely to commit another crime.\n\u2022 The CART model also allows users to make predictions without a calculator. In comparison to the SLIM model, however, the hierarchical structure of the CART model makes it difficult to gauge the relationship of each input variable on the predicted outcome. Consider, for instance, the relationship between age at release and the outcome. In this case, users are immediately aware that there is an effect, as the model branches on the variables age at release\u226540 and age at release 18 to 24. However, the effect is difficult to comprehend since it depends on prior arrests for misdemeanor: if prior arrests\u22655 = 1 and age at release 18 to 24 = 1 then the model predicts y\u0302 = +1; if prior arrests\u22655 = 0 and age at release\u226540 = 0 then y\u0302 = +1; however, if prior arrests\u22655 = 0 and age at release\u226540 = 1 then y\u0302 = +1 only if prior arrest for misdemeanor = 1. Such issues do not affect linear models such as SLIM and Lasso, where users can immediately gauge the direction and strength of the relationship between a input variable and the predicted outcome by the size and sign of a coefficient. The literature on interpretability in machine learning indicates that interpretability is domain-specific; there are some domains where logical models are preferred over linear models, and vice versa (e.g., Freitas, 2014)."
    },
    {
      "heading": "4.5. Scoring Systems for Recidivism Prediction",
      "text": "We show a SLIM scoring system for each of the prediction problems that we consider in Figures 6\u201310. The models are chosen at specific decision points, with the constraint that 5-CV FPR\u2264 50% except for sexual violence, which is chosen at 5-CV FPR\u2264 20%. The models presented here may be suitable for screening tasks. To obtain a model suitable for sentencing, a point on the ROC curve with a much higher TPR would be needed. We note that these models generalize well from the dataset, evident by the close match between test TPR/FPR (Table 5) and training TPR/FPR (Table 6).\nMany of these models exhibit the same \u201crule-like\u201d tendencies discussed in Section 4.4. For example, the model for drug in Figure 6 predicts that a person will be arrested for a drug-related offense if he/she has ever had any prior drug offenses. Similarly, model for sexual violence in Figure 9 effectively states that a person will be rearrested for a sexual offense if and only if he/she has prior history of sexual crimes. For completeness, we include comparisons with other models in Appendix B. Additional risk calibration plots for models with constrained model size are included in Appendix D."
    },
    {
      "heading": "5. Discussion",
      "text": "Our paper merges two perspectives on recidivism modeling: the first is to obtain accurate predictive models using the most powerful machine learning tools available, and the second is to create models that are easy to use and understand.\nWe used a set of features that are commonly accessible to police officers and judges, and compared the ability of different machine learning methods to produce models at different decision points across the ROC curve. Our results suggest that it is possible for traditional methods, such as Ridge Regression, to perform just as well as more modern methods, such as Stochastic Gradient Boosting \u2013 a finding that is in line with the work of Tollenaar and van der Heijden (2013) and Yang et al. (2010). Further, we found that even simple models may perform surprisingly well, even when they are fitting from a heavily constrained space \u2013 a finding that is in line with work on the surprising performance of simple models (see e.g., Dawes, 1979; Holte, 1993, 2006).\nOur study shows that there may be major advantages of using SLIM for recidivism prediction, as it can dependably produce a simple scoring system that is accurate and interpretable on any decision point along the ROC curve. Interpretability is crucial for many of the high-stakes applications where recidivism prediction models are being used. In such applications, it is not enough for the decision-maker to know what input variables are being used to train the model, or how individual input variables are related to the outcome; decision-makers should know how the model combines all the input variables to generate its predictions, and whether this mechanism aligns with their ethical values. SLIM not only shows this mechanism, but also accommodates constraints that are designed to align the prediction model with the ethical values of the decision-maker.\nIn comparison to current machine learning methods, the main drawback of running SLIM is increased computation involved in solving an integer programming problem. To this end, we proposed two new techniques to reduce computation involved in training high quality SLIM scoring systems: (i) a polishing procedure that improves the quality of feasible solutions found by an IP solver; and (ii) an IP formulation that makes it easier for an IP solver to provide a certificate of optimality. In our experiments, the time required to train SLIM was ultimately comparable to the time required to train random forests or stochastic gradient boosting. However, it was still significant compared to the time required for other methods such as CART, C5.0 and penalized logistic regression. In theory, the computation required to find an optimal solution to the SLIM integer program is NP-hard, meaning that the runtime increases exponentially with the number of features. In practice, the runtime depends on several factors: such as the number of samples, the number of dimensions, the underlying ease of the classification, and how the data are encoded. Since most criminological problems cannot by nature involve massive datasets (since each observation is a person), and since computer speed of solving MIPs is also increasing exponentially, it is possible that mathematical programming techniques like SLIM are well-suited for criminological problems that are substantially larger and more complex than the one discussed in this work."
    },
    {
      "heading": "A. Additional Results on Predictive Accuracy",
      "text": "To supplement the experimental results in Section 4.2, we include the training and 5-CV validation results. Table 6 shows the training AUC performance for all methods on all prediction problems, and Table 7 shows the 5-CV validation AUC performance for all methods. A table of test AUC for all methods on all prediction problems can be found in Table 5."
    },
    {
      "heading": "B. Model-Based Comparisons",
      "text": "In Section 4, we included a comparison of transparent models produced for the arrest problem. Here, we include a similar comparison for all other recidivism prediction problems.\nThe models and calibration plots shown here correspond to the best models we produced using Lasso and Ridge (i.e., the ones that were plotted as points in Figure 1). We omit CART and C5.0 models are shown because all models that were produced were either trivial or contained too many leaves to be printed. For any given problem, the models operate at similar decision points (TPR), and are constrained to the same FPR criteria as in Section 4.5.\nNote that the calibration plots will appear to be flat for problems with significant class imbalance. Typically, a well-calibrated classifier on a problem without class imbalance should fall on the x = y line. However, because the y-axis is defined as P (y = +1|s(x) = s), where s is predicted score of a model, the slope of the graph will be less than P (y = +1) by definition. Therefore, for a highly imbalanced problem such as fatal violence, where P (y = +1) = 0.7%, the plot will be flat.\nB.1. drug This is the SLIM model for drug. This model has a test TPR/FPR of 85.7%/51.1%, and a mean 5-CV validation TPR/FPR of 82.3%/49.7%.\n9.00 prior arrest for drugs + 5.00 age at release 18 to 24 + 4.00 age at release 25 to 29 + 3.00 prior arrest for multiple types of crime + 1.00 prior arrest for property \u2212 6.00 no prior arrests \u2212 1.00 age at release 30 to 39 \u2212 7.00\nThis is the best Lasso model for drug. This model has a test TPR/FPR of 82.0%/45.9%, and a mean 5-CV validation TPR/FPR of 81.2%/45.9%.\n1.14 prior arrest for drugs + 0.27 prior arrest for property + 0.26 time served\u22646mo + 0.19 prior arrest for other violence + 0.18 prior arrest for multiple types of crime + 0.17 prior arrest for misdemeanor + 0.16 age at release 18 to 24 + 0.14 prior arrests\u22655 + 0.13 age 1st confinement 18 to 24 + 0.12 prior arrest for public order + 0.10 prior arrest with firearms involved + 0.08 any prior jail time + 0.06 age 1st arrest\u226417 + 0.04 multiple prior jail time + 0.04 drug abuse + 0.03 multiple prior prison time + 0.03 any prior prb or fine \u2212 0.62 age at release\u226540 \u2212 0.25 prior arrest for sexual \u2212 0.23 age at release 30 to 39 \u2212 0.12 time served 25 to 60mo \u2212 0.11 prior arrest with child involved \u2212 0.08 alcohol abuse \u2212 0.07 age 1st confinement\u226540 \u2212 1.11\u00d7 10\u221203 time served\u226561mo \u2212 1.01\nThis is the best Ridge model for drug. This model has a test TPR/FPR of 84.0%/48.2%, and a mean 5-CV validation TPR/FPR of 83.1%/48.4%.\n0.91 prior arrest for drugs + 0.25 time served\u22646mo + 0.24 age at release 18 to 24 + 0.21 prior arrest for multiple types of crime + 0.20 prior arrest for property + 0.17 prior arrest for misdemeanor + 0.17 prior arrest for other violence + 0.17 age 1st confinement 18 to 24 + 0.14 prior arrests\u22655 + 0.13 prior arrest with firearms involved + 0.12 age at release 25 to 29 + 0.11 drug abuse + 0.11 prior arrest for public order + 0.09 age 1st arrest\u226417 + 0.08 age 1st confinement\u226417 + 0.08 any prior jail time + 0.07 multiple prior jail time + 0.07 age at release\u226417 + 0.06 multiple prior prison time + 0.06 released unconditonal + 0.05 any prior prb or fine + 0.05 prior arrests\u22652 + 0.04 time served 7 to 12mo + 0.04 multiple prior prb or fine + 0.02 prior arrests\u22651 + 0.01 age 1st confinement 25 to 29 + 0.01 released conditonal + 2.52\u00d7 10\u221203 prior arrest for felony + 1.76\u00d7 10\u221203 age 1st arrest 18 to 24 + 9.58\u00d7 10\u221204 prior arrest for fatal violence \u2212 0.33 age at release\u226540 \u2212 0.25 prior arrest for sexual \u2212 0.19 age 1st confinement\u226540 \u2212 0.16 prior arrest with child involved \u2212 0.15 time served 25 to 60mo \u2212 0.14 alcohol abuse \u2212 0.13 time served\u226561mo \u2212 0.10 prior arrest for domestic violence \u2212 0.09 age at release 30 to 39 \u2212 0.05 age 1st arrest\u226540 \u2212 0.04 female \u2212 0.04 infraction in prison \u2212 0.03 age 1st arrest 30 to 39 \u2212 0.02 age 1st confinement 30 to 39 \u2212 0.02 no prior arrests \u2212 4.71\u00d7 10\u221203 prior arrest for local ord \u2212 4.45\u00d7 10\u221203 time served 13 to 24mo \u2212 2.23\u00d7 10\u221203 age 1st arrest 25 to 29 \u2212 1.09\nB.2. general violence SLIM model for general violence. This model has a test TPR/FPR of 76.7%/45.4%, and a mean 5-CV validation TPR/FPR of 76.8%/47.6%.\n8 prior arrest for other violence + 5 prior arrest for misdemeanor + 3 infraction in prison + 3 prior arrest for local ord + 2 prior arrest for property + 2 prior arrest for fatal violence + prior arrest with firearms involved \u2212 7 age at release\u226540 \u2212 7\nThis is the best Lasso model for general violence. This model has a test TPR/FPR of 79.7%/45.5%, and a mean 5-CV validation TPR/FPR of 77.3%/45.7%.\n0.90 prior arrest for other violence + 0.35 prior arrest for property + 0.28 prior arrest for misdemeanor + 0.28 age at release 18 to 24 + 0.24 prior arrest for public order + 0.20 age 1st arrest\u226417 + 0.20 released unconditonal + 0.17 age 1st confinement 18 to 24 + 0.16 alcohol abuse + 0.14 prior arrest for fatal violence + 0.14 age 1st confinement\u226417 + 0.10 prior arrest for felony + 0.10 prior arrests\u22655 + 0.10 prior arrest with firearms involved + 0.10 age 1st arrest 18 to 24 + 0.09 infraction in prison + 0.04 time served\u22646mo + 0.03 time served 7 to 12mo + 2.89\u00d7 10\u221203 prior arrest for drugs \u2212 0.72 age at release\u226540 \u2212 0.41 female \u2212 0.27 age at release 30 to 39 \u2212 0.15 prior arrest with child involved \u2212 0.07 age 1st confinement\u226540 \u2212 0.05 age 1st arrest\u226540 \u2212 0.01 time served 25 to 60mo \u2212 1.84\u00d7 10\u221203 age 1st confinement 30 to 39 \u2212 1.19\nThis is the best Ridge model for general violence. This model has a test TPR/FPR of 81.4%/48.1%, and a mean 5-CV validation TPR/FPR of 80.0%/48.5%.\n0.62 prior arrest for other violence + 0.27 age at release 18 to 24 + 0.24 prior arrest for property + 0.23 prior arrest for misdemeanor + 0.19 age 1st confinement 18 to 24 + 0.18 prior arrest for public order + 0.17 age 1st arrest\u226417 + 0.14 prior arrest for multiple types of crime + 0.13 released unconditonal + 0.13 prior arrests\u22655 + 0.13 prior arrest for felony + 0.12 prior arrest with firearms involved + 0.11 age 1st confinement\u226417 + 0.11 alcohol abuse + 0.10 age at release 25 to 29 + 0.10 prior arrest for fatal violence + 0.09 infraction in prison + 0.08 age 1st arrest 18 to 24 + 0.07 prior arrest for domestic violence + 0.05 drug abuse + 0.05 time served\u22646mo + 0.05 prior arrest for local ord + 0.04 time served 7 to 12mo + 0.04 age at release\u226417 + 0.03 prior arrests\u22652 + 0.03 multiple prior prb or fine + 0.02 multiple prior jail time + 0.01 prior arrest for drugs + 3.41\u00d7 10\u221203 no prior arrests \u2212 0.32 age at release\u226540 \u2212 0.20 female \u2212 0.18 age 1st confinement\u226540 \u2212 0.12 prior arrest with child involved \u2212 0.12 age 1st arrest\u226540 \u2212 0.11 age 1st arrest 30 to 39 \u2212 0.09 age 1st confinement 30 to 39 \u2212 0.08 age at release 30 to 39 \u2212 0.05 age 1st arrest 25 to 29 \u2212 0.04 prior arrest for sexual \u2212 0.04 time served 25 to 60mo \u2212 0.03 time served\u226561mo \u2212 0.03 released conditonal \u2212 0.03 age 1st confinement 25 to 29 \u2212 0.02 any prior prb or fine \u2212 0.02 time served 13 to 24mo \u2212 5.89\u00d7 10\u221203 multiple prior prison time \u2212 3.60\u00d7 10\u221203 any prior jail time \u2212 3.47\u00d7 10\u221203 prior arrests\u22651 \u2212 1.13\nB.3. domestic violence This is the SLIM model for domestic violence. This model has a test TPR/FPR of 85.5%/46.0%, and a mean 5-CV validation TPR/FPR of 81.4%/48.0%.\n4 prior arrest for misdemeanor + 3 prior arrest for felony + 2 prior arrest for domestic violence + age 1st confinement 18 to 24 \u2212 5 infraction in prison \u2212 3\nThis is the best Lasso model for domestic violence. This model has a test TPR/FPR of 87.0%/45.8%, and a mean 5-CV validation TPR/FPR of 84.5%/45.8%.\n0.88 prior arrest for misdemeanor + 0.73 prior arrest for domestic violence + 0.73 prior arrest for felony + 0.66 prior arrest for other violence + 0.54 released unconditonal + 0.32 age 1st confinement 18 to 24 + 0.24 multiple prior prb or fine + 0.21 alcohol abuse + 0.17 prior arrest for sexual + 0.16 prior arrests\u22655 + 0.16 prior arrest with firearms involved + 0.08 age at release 18 to 24 + 0.06 no prior arrests + 0.05 time served 7 to 12mo + 0.03 prior arrest for property + 0.01 age 1st arrest 18 to 24 + 0.01 prior arrest for public order \u2212 1.09 infraction in prison \u2212 0.54 age at release\u226540 \u2212 0.47 drug abuse \u2212 0.40 multiple prior prison time \u2212 0.31 prior arrest with child involved \u2212 0.28 multiple prior jail time \u2212 0.26 female \u2212 0.20 age 1st confinement\u226540 \u2212 0.16 any prior jail time \u2212 0.07 age 1st arrest 30 to 39 \u2212 0.07 any prior prb or fine \u2212 0.06 prior arrest for drugs \u2212 0.06 time served\u226561mo \u2212 4.48\u00d7 10\u221204 time served 25 to 60mo \u2212 1.04\nThis is the best Ridge model for domestic violence. This model has a test TPR/FPR of 87.0%/47.7%, and a mean 5-CV validation TPR/FPR of 85.2%/47.5%.\n0.76 prior arrest for misdemeanor + 0.59 prior arrest for other violence + 0.57 prior arrest for domestic violence + 0.54 prior arrest for felony + 0.40 released unconditonal + 0.27 age 1st confinement 18 to 24 + 0.27 multiple prior prb or fine + 0.21 prior arrest for sexual + 0.19 prior arrest with firearms involved + 0.18 alcohol abuse + 0.18 prior arrests\u22655 + 0.17 age at release 18 to 24 + 0.15 prior arrest for local ord + 0.12 age at release 25 to 29 + 0.11 time served 7 to 12mo + 0.10 prior arrest for property + 0.10 prior arrest for fatal violence + 0.10 no prior arrests + 0.08 age at release 30 to 39 + 0.07 prior arrest for multiple types of crime + 0.07 age 1st arrest\u226417 + 0.07 age 1st arrest 18 to 24 + 0.07 prior arrest for public order + 0.05 age 1st arrest 25 to 29 + 0.05 time served\u22646mo + 0.05 time served 13 to 24mo + 0.05 prior arrests\u22652 + 3.08\u00d7 10\u221203 age 1st confinement 30 to 39 \u2212 0.86 infraction in prison \u2212 0.40 drug abuse \u2212 0.39 multiple prior prison time \u2212 0.36 age at release\u226540 \u2212 0.26 prior arrest with child involved \u2212 0.25 multiple prior jail time \u2212 0.25 female \u2212 0.24 age 1st confinement\u226540 \u2212 0.19 any prior jail time \u2212 0.14 time served\u226561mo \u2212 0.12 age 1st arrest 30 to 39 \u2212 0.10 any prior prb or fine \u2212 0.10 age 1st arrest\u226540 \u2212 0.10 prior arrests\u22651 \u2212 0.08 prior arrest for drugs \u2212 0.06 age 1st confinement 25 to 29 \u2212 0.05 time served 25 to 60mo \u2212 0.04 released conditonal \u2212 0.04 age at release\u226417 \u2212 0.02 age 1st confinement\u226417 \u2212 1.01\nB.4. sexual violence This is the SLIM model for sexual violence. This model has a test TPR/FPR of 44.3%/17.7%, and a mean 5-CV validation TPR/FPR of 43.7%/19.9%.\n3 prior arrest for sexual + prior arrests\u22655 + multiple prior jail time \u2212 2 no prior arrests \u2212 prior arrest for multiple types of crime \u2212 2\nThis is the best Lasso model for sexual violence. This model has a test TPR/FPR of 46.9%/18.1%, and a mean 5-CV validation TPR/FPR of 43.7%/17.9%.\n1.10 prior arrest for sexual + 0.40 prior arrest for other violence + 0.27 age 1st confinement 18 to 24 + 0.27 prior arrest for felony + 0.19 prior arrest with child involved + 0.19 infraction in prison + 0.12 prior arrest for property + 0.09 prior arrest for public order + 0.07 prior arrests\u22655 + 0.03 age 1st confinement\u226417 + 0.02 age 1st arrest\u226417 + 8.11\u00d7 10\u221204 prior arrest for fatal violence \u2212 0.58 female \u2212 0.25 age at release\u226540 \u2212 0.23 prior arrest for drugs \u2212 0.05 any prior prb or fine \u2212 0.05 drug abuse \u2212 0.01 time served 25 to 60mo \u2212 0.01 prior arrest for misdemeanor \u2212 5.85\u00d7 10\u221203 age 1st confinement 30 to 39 \u2212 1.63\nThis is the best Ridge model for sexual violence. This model has a test TPR/FPR of 48.6%/19.3%, and a mean 5-CV validation TPR/FPR of 44.9%/19.4%.\n0.92 prior arrest for sexual + 0.35 prior arrest for other violence + 0.30 prior arrest for felony + 0.28 prior arrest with child involved + 0.20 age 1st confinement 18 to 24 + 0.18 infraction in prison + 0.14 prior arrest for property + 0.14 prior arrest for public order + 0.13 age 1st confinement\u226417 + 0.12 prior arrests\u22655 + 0.10 prior arrest for fatal violence + 0.07 age at release 18 to 24 + 0.07 time served\u226561mo + 0.07 age 1st arrest\u226417 + 0.07 prior arrest for local ord + 0.06 any prior jail time + 0.05 age at release 30 to 39 + 0.04 age at release 25 to 29 + 0.04 multiple prior prb or fine + 0.03 time served 13 to 24mo + 0.03 released conditonal + 0.03 released unconditonal + 0.02 age 1st arrest 18 to 24 + 9.63\u00d7 10\u221203 age 1st arrest 30 to 39 + 7.60\u00d7 10\u221203 prior arrests\u22651 + 6.27\u00d7 10\u221203 age at release\u226417 \u2212 0.37 female \u2212 0.25 prior arrest for drugs \u2212 0.16 age at release\u226540 \u2212 0.11 age 1st confinement\u226540 \u2212 0.11 any prior prb or fine \u2212 0.11 age 1st confinement 30 to 39 \u2212 0.09 drug abuse \u2212 0.09 age 1st arrest\u226540 \u2212 0.07 prior arrest for misdemeanor \u2212 0.06 multiple prior jail time \u2212 0.05 time served 25 to 60mo \u2212 0.04 prior arrests\u22652 \u2212 0.04 alcohol abuse \u2212 0.04 time served 7 to 12mo \u2212 0.03 prior arrest for multiple types of crime \u2212 0.02 prior arrest for domestic violence \u2212 0.02 time served\u22646mo \u2212 0.02 age 1st confinement 25 to 29 \u2212 0.02 multiple prior prison time \u2212 7.46\u00d7 10\u221203 no prior arrests \u2212 5.79\u00d7 10\u221203 age 1st arrest 25 to 29 \u2212 4.60\u00d7 10\u221203 prior arrest with firearms involved \u2212 1.47\nB.5. fatal violence This is the SLIM model for fatal violence. This model has a test TPR/FPR of 55.4%/35.5%, and a mean 5-CV validation TPR/FPR of 64.2%/42.4%.\n5 age 1st confinement\u226417 + 3 prior arrest with firearms involved + 2 age 1st confinement 18 to 24 + 2 prior arrest for felony + age at release 18 to 24 + prior arrest for drugs \u2212 4\nThis is the best Lasso model for fatal violence. This model has a test TPR/FPR of 68.9%/44.5%, and a mean 5-CV validation TPR/FPR of 67.6%/42.4%.\n1.52 age 1st confinement\u226417 + 1.47 age at release\u226417 + 1.12 prior arrest for felony + 0.73 age at release 18 to 24 + 0.69 alcohol abuse + 0.66 prior arrests\u22655 + 0.60 prior arrest for fatal violence + 0.54 age 1st confinement 18 to 24 + 0.47 prior arrest with firearms involved + 0.39 prior arrest for drugs + 0.38 age 1st confinement 25 to 29 + 0.35 prior arrest for other violence + 0.35 age 1st arrest\u226417 + 0.34 prior arrest for public order + 0.31 prior arrest for multiple types of crime + 0.28 no prior arrests + 0.26 age 1st arrest 25 to 29 + 0.24 age 1st confinement 30 to 39 + 0.20 multiple prior prison time + 0.19 prior arrest for property + 0.18 prior arrest for sexual + 0.11 any prior prb or fine + 0.07 time served 7 to 12mo + 0.07 time served\u22646mo + 0.04 age 1st arrest 18 to 24 \u2212 2.69 age 1st arrest\u226540 \u2212 1.68 female \u2212 0.70 drug abuse \u2212 0.55 infraction in prison \u2212 0.50 time served\u226561mo \u2212 0.42 released conditonal \u2212 0.39 prior arrests\u22652 \u2212 0.36 age at release\u226540 \u2212 0.34 prior arrest for misdemeanor \u2212 0.33 prior arrest with child involved \u2212 0.29 multiple prior prb or fine \u2212 0.24 multiple prior jail time \u2212 0.16 released unconditonal \u2212 0.13 time served 13 to 24mo \u2212 0.08 age at release 30 to 39 \u2212 0.08 prior arrest for domestic violence \u2212 0.02 prior arrests\u22651 \u2212 2.00\nThis is the best Ridge model for fatal violence. This model has a test TPR/FPR of 62.2%/34.0%, and a mean 5-CV validation TPR/FPR of 60.1%/33.0%.\n0.55 prior arrest for felony + 0.54 age 1st confinement\u226417 + 0.45 age at release 18 to 24 + 0.39 age 1st arrest\u226417 + 0.39 prior arrest for fatal violence + 0.35 prior arrests\u22655 + 0.35 prior arrest with firearms involved + 0.29 prior arrest for other violence + 0.29 prior arrest for drugs + 0.26 prior arrest for public order + 0.25 alcohol abuse + 0.24 prior arrest for multiple types of crime + 0.19 age at release\u226417 + 0.16 multiple prior prison time + 0.16 prior arrest for property + 0.15 time served 7 to 12mo + 0.14 time served\u22646mo + 0.12 age 1st confinement 18 to 24 + 0.10 any prior prb or fine + 0.08 prior arrest for sexual + 0.06 released unconditonal + 0.06 no prior arrests + 0.06 time served 25 to 60mo + 0.05 age 1st arrest 25 to 29 + 0.03 prior arrest for local ord \u2212 0.51 female \u2212 0.42 age at release\u226540 \u2212 0.35 drug abuse \u2212 0.30 infraction in prison \u2212 0.29 age 1st arrest\u226540 \u2212 0.28 age 1st confinement\u226540 \u2212 0.25 time served\u226561mo \u2212 0.20 multiple prior prb or fine \u2212 0.19 multiple prior jail time \u2212 0.17 prior arrest with child involved \u2212 0.16 prior arrest for misdemeanor \u2212 0.16 age at release 30 to 39 \u2212 0.15 released conditonal \u2212 0.14 prior arrests\u22652 \u2212 0.14 age 1st confinement 30 to 39 \u2212 0.12 age 1st arrest 30 to 39 \u2212 0.07 time served 13 to 24mo \u2212 0.06 age at release 25 to 29 \u2212 0.06 age 1st confinement 25 to 29 \u2212 0.06 prior arrests\u22651 \u2212 0.01 prior arrest for domestic violence \u2212 0.01 any prior jail time \u2212 8.27\u00d7 10\u221203 age 1st arrest 18 to 24 \u2212 1.33"
    },
    {
      "heading": "C. Additional Results on the Trade-off between Accuracy and Interpretability",
      "text": "In the experiments in Section 4, we used SLIM to fit models from a highly constrained space (i.e., models with at most 8 non-zero integer coefficients between -10 and 10). Here, we present evidence to show that baseline methods cannot attain the same level of accuracy or risk calibration when they are used to fit models from a slightly less constrained model space (i.e, model with at most 8 non-zero coefficients, 8 leaves or 8 rules).\nTable 8 shows the test AUC of each method when they are used to fit a models with a model size of 8 or less. Trivial models of size 1 are also omitted. Table 9 shows the percentage change in test AUC for the methods due to the model size restriction. For all models other than SLIM, the predictive accuracy was compromised with the size constraint. We see that C5.0R and C5.0T are unable to produce a suitably sparse model for some of the problems since their implementation does not provide control over model sparsity. Note that we have omitted results for Ridge because it could not produce a model with fewer than 8 coefficients for all prediction problems (see Section 4.4 for explanation)."
    },
    {
      "heading": "D. Trade-off between Risk Calibration and Interpretability",
      "text": "Figure 16 shows the risk calibration plots of Lasso, Ridge, and SLIM for transparent models with model size constrained to 8 or less, chosen under the same decision criteria as Appendix B. Ridge is not included because no such models are achievable, as discussed also in Appendix C. For Lasso, the risk calibration performance is worse in comparison to Figures 11\u201315. For fatal violence, there was no Lasso model available at the desired decision point."
    },
    {
      "heading": "E. On the Predictive Accuracy of Baseline Methods with Continuous Input Variables",
      "text": "In our experiments in Section 4, we ran all methods with a dataset composed exclusively of binary input variables. That is, for each feature in the original database (e.g., prior arrests), we derived binary variables (e.g., no prior arrests, prior arrests \u2265 1 and so on) and trained each method using these binary variables. It is possible that machine learning methods could potentially be hindered by this removal of information. Here, we investigate how the predictive accuracy of the baseline method would have been affected had we run these methods using continuous input variables (Appendix E.1) or both binary and continuous input variables (Appendix E.2). In both cases, we find that the change in variable encoding results in a minor difference in performance.\nE.1. Change in Predictive Accuracy using Only Continuous Input Variables Instead of using 48 input variables, we now have 25 continuous variables. Table 10 summarizes the test AUC for all methods on all prediction problems when we use only continuous input variables. Table 11 shows the percentage change in test AUC due to this change in encoding (i.e. from binary input variables to continuous input variables). The largest increases in predictive accuracy are 4.6% for CART and 7.7% for SVM RBF, while the biggest decrease in accuracy is \u221219.6% for RF.\nOur results suggest that there is no uniform gain/loss in performance for most of the methods: for any given method, the test AUC increased slightly for at least one problem, and decreased slightly for at least another. Among the methods, CART saw the most uniform improvement, performing slightly better on 5 out of the 6 problems when continuous variables are used (though CART still performs poorly compared to other methods).\nE.2. Change in Predictive Accuracy using Both Binary and Continuous Input Variables Instead of the original 48 variables, we now use a combination of 66 binary and continuous variables. Table 12 summarizes the test AUC for all methods on all prediction problems when we used both binary and continuous input variables. Table 13 shows the percentage change in test AUC due to this change in encoding (i.e., from binary input variables to both binary and continuous input variables). Most methods saw a slight AUC increase due to the addition of continuous variables, ranging from 0.2\u20136.3%. The most significant increases are 3.3% for CART and 6.3% for C5.0T, while the largest decrease is \u221216.0% for RF. In addition to RF, Ridge and SVM RBF all saw slight decreases with the inclusion. Similar to Appendix E.1, no uniform gain/loss in performance is seen."
    },
    {
      "heading": "F. Association Rules",
      "text": "We produce insights more extensive than those in Section 2.4 by mining association rules. Association rules, also known as \u201cIF-THEN\u201d rules, are small predictive models that can be produced using search techniques or optimization techniques.\nF.1. Terminology High quality association rules are characterized by large values of support, confidence, and lift. To define this terminology, consider a rule such as \u201cIF a THEN b.\u201d We denote this rule also as a \u2192 b. The support of a \u2192 b is the empirical probability P\u0302 (a and b), that is, the proportion of observations where the conditions a and b are both satisfied. The confidence of a \u2192 b is the empirical probability P\u0302 (b|a), that is, the proportion of observations for which condition b is satisfied given a is satisfied. The lift of a \u2192 b is the ratio P\u0302 (b|a)\nP\u0302 (b) .\nLift measures the ability of condition a to \u201ctarget\u201d the population where condition b is satisfied: if the lift of a \u2192 b is equal to 1, then outcome b could be predicted equally well if we had assumed that a and b were independent; if the lift of a\u2192 b greater than 1 then event a has some effect on predicting event b.\nTo illustrate these concepts, consider the following association rule:\nIF age at release 18 to 24 AND prior arrests\u22655 THEN y = +1. The support of this rule is 0.07, which means that 7% of prisoners were released from prison between the ages of 18 to 24, had at least 5 prior arrests, and were arrested within 3 years of being released from prison. The confidence of this rule is 0.83, which means that if a prisoner was released from prison between the ages of 18 to 24 and had at least 5 prior arrests, then there was an 83% chance that this person would be arrested within 3 years of being released from prison. Lastly, the lift of this rule is 1.41, which means that prisoners released from prison between the ages of 18 to 24 and had at least 5 prior arrests have a higher chance of being rearrested than other prisoners, i.e., the prisoners age at release and arrest history makes the conditional probability of arrest 1.41 times higher than if arrest was independent of these conditions.\nF.2. Rule Mining We list 24 interesting association rules for the arrest problem in Table 14. These rules were generated with the apriori method in the arules package in R 3.1.1 (Hahsler et al., 2014). Note that the choice of package does not matter, as mining rules through search techniques is deterministic, so all packages produce the same rules.\nHere, the IF conditions are formulated using combinations of input variables (i.e. xj = 1 and xk = 1) and the THEN condition is that a prisoner is arrested within 3 years of being released from prison (i.e. a positive outcome y = +1). The rules in Table 14 have the highest levels of lift and confidence with a minimum support of 5% (i.e., the rule applied to at least 1690 of the 33796 prisoners in our dataset). This threshold value was chosen so the rules do not reflect spurious correlations. Rules A \u2013 E were produced by mining the most powerful single-variable predictors for arrest. Rules A \u2013 E attain the highest lift among one-variable rules with a support of at least 5% and a confidence of at least 0.70. Rules F \u2013 X were produced by mining twovariable rules that use at least one of the input variables from Rules A \u2013 E that attain the highest possible lift, as well as support at least 5% and confidence at least 0.75. Out of all these rules, Rule F performs the best with a confidence of 0.83 and a lift of 1.41. As it turns out, Rule F is often exploited by some of the best models we find for arrest, as we often find patterns similar to \u201cage at release 18 to 24 AND prior arrests\u22655\u201d in our predictive models (see e.g., Figure 5 in Section 4.4).\nInteresting observations can also be made from the discovered rules. Recall that jail is a much less severe punishment than prison. Considering Rule L and Rule M in Table 14, we can see that prisoners with multiple jail time and have any past probations or fines are just as likely to be arrested as those with multiple jail time and multiple prior prison records \u2013 despite multiple prior prison time being a indicator of much more severe past actions than any prior probation or fine.\nF.3. Falling Rule Lists for Imbalanced Problems As we discuss in Section 4.2, it is difficult to use traditional tree and rule-based methods to create non-trivial models on imbalanced classification problems such as sexual violence. This is possibly because these\nalgorithms employ greedy splitting and pruning procedures. Here, we aim to show that there exist rule-based models that perform well on such problems by training Falling Rule Lists (Wang and Rudin, 2015).\nFalling Rule Lists are ordered lists of IF-THEN rules. The confidence of each rule decreases as we go down the list. In this way, the highest rule applies to the group of individuals that have the highest risk, the second highest rule applies to a group of individuals with the second highest risk, and so on. The algorithm that produces Falling Rule List globally optimizes the list, without greedy splitting and pruning.\nWe present a Falling Rule List for the arrest problem in Table 15, learned from the algorithm of Wang and Rudin (2015). This model was trained using rules with at most two input variables and a support of at least 5%. The rules listed within this model have the form \u201cIF a THEN b\u201d where b denotes a positive outcome y = +1. In Table 15, support refers to the percentage of remaining examples that satisfy the IF conditions and probability refers to percentage of these examples where the outcome variable is positive. This model shows that the highest risk prisoners are those who were released between ages 18 and 24, and who have at least 5 prior arrests \u2013 this is aligned with the association rule (Rule F) that we found in Section F.2. Once those individuals are removed, the second highest risk prisoners are 25\u201329 year olds with at least 5 prior arrests, etc. The risk of each group decreases as one moves down the rules. Rule 15 represents the default rule. If an individual does not fall under any of risk groups determined by Rules 1-14, then his/her risk of arrest is 0.21."
    },
    {
      "heading": "G. The Impact of Race",
      "text": "As discussed earlier, we chose not to include race as an input variable in our prediction problems. Some studies have shown that race is important for accurate recidivism prediction (Petersilia and Turner, 1987; Berk, 2009).\nWe wanted to know the answers to two questions. First, whether including race as a feature would lead to more accurate predictions. Second, whether we could predict race from the features that we already had. If we could predict race well from our current set of features, this would show that race information could be implicitly included in any model we might construct. The results that follow show: (i) including race does not substantially increase prediction accuracy for our problems, and (ii) race can be predicted fairly well from the features we already have. These results indicate that most of the information necessary to predict recidivism is already included in the features we have, and these features also include relevant information for predicting race.\nTo address whether race provided an increase in accuracy for predicting recidivism, we re-ran all methods other than SLIM on all new versions of each prediction problem that included three additional race-related input variables: white, black, hispanic. An overview of these variables can be seen in Table 16. Table 17 presents the models\u2019 test AUC when race-related indicator variables are included. Table 18 represent the percentage increase in AUC when compared to 5. As shown, the differences for most methods are negligible. In the cases of SVM RBF and Ridge, the accuracy increased slightly. In the case of RF, including race decreases accuracy (most likely because it exacerbates the overfitting problem).\nTo determine whether race could be predicted from the current variables, we used three different race options (white, black, and hispanic) as outcomes and predicted each race as a function of our features. ROC plots are provided in Figure 17, showing that race can be predicted much better than random guessing. This is not a surprise, as we already know that blacks tend to have longer criminal histories than whites. On the other hand, we remark that we could not predict race perfectly with the features we have - in fact, our predictions (for all methods) were far from perfect. This means that not all of the information about race is contained in the features we have."
    }
  ],
  "title": "Interpretable Classification Models for Recidivism Prediction",
  "year": 2018
}
