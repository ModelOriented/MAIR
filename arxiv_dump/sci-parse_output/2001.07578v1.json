{
  "abstractText": "Explaining sophisticated machine-learning based systems is an important issue at the foundations of AI. Recent efforts [Ribeiro et al., 2016; Ribeiro et al., 2018; Wachter et al., 2017; Ignatiev et al., 2019; Bachoc et al., 2018] have shown various methods for providing explanations. These approaches can be broadly divided into two schools: those that provide a local and human interpreatable approximation of a machine learning algorithm, and logical approaches that exactly characterise one aspect of the decision. In this paper we focus upon the second school of exact explanations with a rigorous logical foundation. There is an epistemological problem with these exact methods. While they can furnish complete explanations, such explanations may be too complex for humans to understand or even to write down in human readable form. Interpretability requires epistemically accessible explanations, explanations humans can grasp. Yet what is a sufficiently complete epistemically accessible explanation still needs clarification. We do this here in terms of counterfactuals, following [Wachter et al., 2017]. With counterfactual explanations, many of the assumptions needed to provide a complete explanation are left implicit. To do so, counterfactual explanations exploit the properties of a particular data point or sample, and as such are also local as well as partial explanations. We explore how to move from local partial explanations to what we call complete local explanations and then to global ones. But to preserve accessibility we argue for the need for partiality. This partiality makes it possible to hide explicit biases present in the algorithm that may be injurious or unfair. We investigate how easy it is to uncover these biases in providing complete and fair explanations by exploiting the structure of the set of counterfactuals providing a complete local explanation.",
  "authors": [],
  "id": "SP:bfaea84bf53217e2416a711c56ef766bac72a892",
  "references": [
    {
      "authors": [
        "P. Achinstein"
      ],
      "title": "The Nature of Explanation",
      "venue": "Oxford University Press",
      "year": 1980
    },
    {
      "authors": [
        "N. Asher",
        "S. Paul"
      ],
      "title": "Evaluating conversational success: Weighted message exchange games",
      "venue": "J. Hunter, M. Simons, and M. Stone, editors, 20th workshop on the semantics and pragmatics of dialogue (SEMDIAL), New Jersey, USA, July",
      "year": 2016
    },
    {
      "authors": [
        "Fran\u00e7ois Bachoc",
        "Fabrice Gamboa",
        "Max Halford",
        "Jean-Michel Loubes",
        "Laurent Risser"
      ],
      "title": "Entropic variable projection for explainability and intepretability",
      "venue": "arXiv preprint arXiv:1810.07924,",
      "year": 2018
    },
    {
      "authors": [
        "S. Bromberger"
      ],
      "title": "An approach to explanation",
      "venue": "R. Butler, editor, Analytical Philsophy, pages 72\u2013105. Oxford University Press",
      "year": 1962
    },
    {
      "authors": [
        "Simant Dube. High dimensional spaces"
      ],
      "title": "deep learning and adversarial examples",
      "venue": "arXiv preprint arXiv:1801.00634,",
      "year": 2018
    },
    {
      "authors": [
        "Matthew L Ginsberg. Counterfactuals"
      ],
      "title": "Artificial intelligence",
      "venue": "30(1):35\u201379,",
      "year": 1986
    },
    {
      "authors": [
        "A. Ignatiev",
        "J. Narodytska",
        "J. Marques-Silva"
      ],
      "title": "On relating explanations and adversarial examples",
      "venue": "Advances in Neural Information Processing Systems",
      "year": 2019
    },
    {
      "authors": [
        "David S Johnson",
        "Christos H Papadimitriou",
        "Mihalis Yannakakis"
      ],
      "title": "How easy is local search? Journal of computer and system sciences",
      "venue": "37(1):79\u2013100,",
      "year": 1988
    },
    {
      "authors": [
        "Matt J Kusner",
        "Joshua Loftus",
        "Chris Russell",
        "Ricardo Silva. Counterfactual fairness"
      ],
      "title": "In Advances in Neural Information Processing Systems",
      "venue": "pages 4066\u20134076,",
      "year": 2017
    },
    {
      "authors": [
        "David Lewis. Counterfactuals"
      ],
      "title": "Basil Blackwell",
      "venue": "Oxford,",
      "year": 1973
    },
    {
      "authors": [
        "Tim Miller"
      ],
      "title": "Explanation in artificial intelligence: Insights from the social sciences",
      "venue": "Artificial Intelligence, pages 1\u201338,",
      "year": 2019
    },
    {
      "authors": [
        "Judea Pearl"
      ],
      "title": "System z: A natural ordering of defaults with tractable applications to nonmonotonic reasoning",
      "venue": "Proceedings of the 3rd Conference on Theoretical Aspects of Reasoning about Knowledge, pages 121\u2013 135. Morgan Kaufmann Publishers Inc.,",
      "year": 1990
    },
    {
      "authors": [
        "Marco T\u00falio Ribeiro",
        "Sameer Singh",
        "Carlos Guestrin"
      ],
      "title": "why should I trust you?\u201d: Explaining the predictions of any classifier",
      "venue": "KDD, pages 1135\u2013 1144,",
      "year": 2016
    },
    {
      "authors": [
        "Marco T\u00falio Ribeiro",
        "Sameer Singh",
        "Carlos Guestrin"
      ],
      "title": "Anchors: High-precision modelagnostic explanations",
      "venue": "AAAI, pages 1527\u20131535,",
      "year": 2018
    },
    {
      "authors": [
        "Lloyd S. Shapley. Stochastic games"
      ],
      "title": "Proceedings of the National Academy of Sciences of the United States of America",
      "venue": "39(10):1095\u20131100,",
      "year": 1953
    },
    {
      "authors": [
        "Sandra Wachter",
        "Brent Mittelstadt",
        "Chris Russell"
      ],
      "title": "Counterfactual explanations without opening the black box: Automated decisions and the gpdr",
      "venue": "Harv. JL & Tech., 31:841,",
      "year": 2017
    }
  ],
  "sections": [
    {
      "text": "ar X\niv :2\n00 1.\n07 57\n8v 1\n[ cs\n.A I]\n2 1\nJa n\n20 20\nExplaining sophisticated machine-learning based systems is an important issue at the foundations of AI. Recent efforts [Ribeiro et al., 2016; Ribeiro et al., 2018; Wachter et al., 2017; Ignatiev et al., 2019; Bachoc et al., 2018] have shown various methods for providing explanations. These approaches can be broadly divided into two schools: those that provide a local and human interpreatable approximation of a machine learning algorithm, and logical approaches that exactly characterise one aspect of the decision. In this paper we focus upon the second school of exact explanations with a rigorous logical foundation.\nThere is an epistemological problem with these exact methods. While they can furnish complete explanations, such explanations may be too complex for humans to understand or even to write down in human readable form. Interpretability requires epistemically accessible explanations, explanations humans can grasp. Yet what is a sufficiently complete epistemically accessible explanation still needs clarification. We do this here in terms of counterfactuals, following [Wachter et al., 2017].\nWith counterfactual explanations, many of the assumptions needed to provide a complete explanation are left implicit. To do so, counterfactual explanations exploit the properties of a particular data point or sample, and as such are also local as well as partial explanations. We explore how to move from local partial explanations to what we call complete local explanations and then to global ones. But to preserve accessibility we argue for the need for partiality. This partiality makes it possible to hide explicit biases present in the algorithm that may be injurious or unfair. We investigate how easy it is to uncover these biases in providing complete and fair explanations by exploiting the structure of the set of counterfactuals providing a complete local explanation."
    },
    {
      "heading": "1 Introduction",
      "text": "Explaining the predictions of sophisticated machine-learning algorithms is an important issue for the foundations of AI. Recent efforts [Ribeiro et al., 2016; Ribeiro et al., 2018; Wachter et al., 2017; Ignatiev et al., 2019; Bachoc et al., 2018] have shown various methods for providing explanations. These approaches can be broadly divided into two schools: those that provide a local and human interpretable approximation of a machine learning algorithm, and logical approaches that completely characterise one aspect of the decision. In this paper we investigate a comparison between complete explanations and partial, epistemically accessible ones.\nThere is an epistemological problem with these complete methods. While they can furnish complete explanations, such explanations may be too complex for humans to understand or even to write down in human readable form. Interpretability requires epistemically accessible explanations, explanations humans can grasp. Yet what is a sufficiently complete or adequate epistemically accessible explanation still needs analysis. We provide such an analysis in terms of counterfactuals, following [Wachter et al., 2017].\nWith counterfactual explanations, many of the assumptions needed to provide a complete explanation are left implicit. To do so, counterfactual explanations exploit the properties of a particular data point or sample, and as such are also local as well as partial explanations. We explore how to move from local partial explanations to what we call complete local explanations and then to global ones. But to preserve accessibility we argue for the need for partiality. This partiality makes it possible to hide explicit biases present in the algorithm that may be injurious or unfair. We investigate how easy it is to uncover these biases in providing complete and fair explanations by exploiting the structure of the set of counterfactuals providing a complete local explanation.\nTo make the point about biases in counterfactual explanations concrete, consider the following scenario. An ML program judges A\u2019s application for a loan. A is turned down. When A asks the bank for an explanation of the decision, the bank returns with the following.\n(1) Your income is 50K euro per year.\n(2) If your income had been 100K euro per year, you would have gotten the loan.\nThe counterfactual in (2) might be true but it also might be misleading, hiding a bias that one might find unfair. Suppose that (1)-(2) is the explanation the bank gives A. But suppose also that there is another more morally repugnant explanation: A black. If A had been white he would have gotten the loan with your current income of 50K per year. There\u2019s also a question of features that indirectly code bias. For example, tying the loan availability to the postal code of A\u2019s residence could be a way of encoding racial bias.\nThe problem of dissimulating a bias when giving an explanation is a direct outcome of the partiality of epistemically accessible explanations."
    },
    {
      "heading": "2 Background on explanations",
      "text": "Suppose that f : Xn \u2192 Y is the \u201cideal\u201d function taking data encoded in an n-dimensional space of features Xn into the representations in Y and that f\u0302 : Xn \u2192 Y is the function that the algorithm has encoded. For this paper, we\u2019ll assume that f\u0302 is some sort of classifier and thus we can assume Y to be a set of classes. We want to have an explanation of why f\u0302 outputs the predictions it does. We might want to know its behavior over the total space Xn; this would be a complete explanation.\nBut for many purposes, we might only need to know how f\u0302 behaves on a data point of interest or focal point, like A\u2019s profile that was submitted to the loan program. Note that, we are\nimplicitly assuming that f\u0302 is too complex or opaque for its be-\nhaviour to be analyzed statically. For instance, f\u0302 might be a neural network with multiple layers/parameters/variables etc. Or, it might be be case that we have access only to the bina-\nries of f\u0302 , from which the actual algorithm cannot be reverseengineered.\nThere are two sorts of explanations of program behavior. Internal explanations involve the internal states of the program\u2014if these are linked by logic then we can have a deductive explanation for a particular response in Y to input data from Xn. However, there are also external explanations that involve linking features of X with output Y (we can also have a deductive link or something else). These are initially attractive because they do not involve unpacking the algorithms\u2019 internal states and assigning them a meaning, which in the case of deep learning networks with multiple hidden layers can be a very complicated affair. Even partial explanations that exploit internal states of a complex neural architecture may be epistemically inaccessible.\n[Ignatiev et al., 2019] provide a definition of complete ex-\nplanations. They assume a classifier f\u0302 can be represented as a set of logic formulas, which we assume here too. In addition,\nwe will assume that f\u0302 has a constant set of features with binary values making the encoding into logic transparent.1 For [Ignatiev et al., 2019], an explanation, or what we call\nand MS explanation, of a prediction \u03c0 of a classifier f\u0302 given a feature space X is a subset minimal set of literals E (each one describing a value of a feature in the problem space) such that\n1By increasing the number of literals we can simulate nonbinary values, so this is not really a limitation as long as the features are finite.\nE |= \u2227 f\u0302 \u2192 \u03c0\nwhere \u2227\nranges over the set of formulas encoding f\u0302 . E |= f\u0302 \u2192 \u03c0 means that we can prove in the logic representation that f\u0302 predicts \u03c0 given any instance with features as described in E . A complete explanation of the behavior of f\u0302 in our sense would be the set of all possible MS explanations.\nAn instance in such a set up is a set of literals that assigns values to every feature in the feature space. The underlying logical form of explanations discussed in [Ignatiev et al., 2019] thus exploit universal generalizations and a deductive consequence relation. The explanations in [Ignatiev et al., 2019] thus explain in principle sets of instances, and they are known as global explanations and are a version of a deductive nomological explanation, where a relation of entailment holds between the explanans and the explanandum.2\nCounterfactuals offer a natural way to provide epistemically accessible, partial explanations geared toward properties of individuals or focal points. Such explanations directed to a particular case are often called local explanations in contrast to global ones [Ribeiro et al., 2016; Ribeiro et al., 2018]. One reason for using counterfactuals in explanations is that counterfactuals lend themselves to an attractive analysis of causation, as [Lewis, 1973] proposed. The reason why counterfactual explanations furnish natural candidates for partial epistemically accessible explanations is that they single out properties or features that would make a difference to a decision about an individual as in (2), other things being as equal as they can be given that the individual has the property described by the counterfactual\u2019s antecedent. This ceteris paribus property of counterfactuals means that many factors that would be mentioned in a complete explanation can remain implicit. They are thus more partial than MS explanations.\nThe canonical semantics for counterfactuals symbolised via \u2192 as outlined in [Lewis, 1973] exploits a possible worlds model for propositional logic and crucially a similarity relation: A \u2192 B is true at world w just in case for all worlds w\u2032 in which A is true and that are the closest worlds to w in which A is true, B is true.\nThe similarity relation in Lewis\u2019s semantics is used to model the complicated nature of causal laws, which are themselves often formulated with ceteris paribus assumptions. In particular, allows us to have consistent laws with conflicting consequents and antecedents ordered by entailment as in the following set of cascading counterfactuals:\n(3) a. If I were making 100K euro or more, I would have gotten the loan.\nb. If I were making 100K euro or more but were convicted of a serious financial fraud, I would not\n2Note however, such global generalizations may not offer a mathematical function or rule based reconstruction of f\u0302 \u2019s behavior. Thus, global explanations may be extensional as in the case [Ignatiev et al., 2019] or intensional in the form of a rule reconstruction of f\u0302 .\nget the loan. c. If If I were making 100K euro or more and were\nconvicted of a serious financial fraud but then the conviction was overturned and I was awarded a medal, I would get the loan.\nIn a cascading set of counterfactuals we can count how many times the value of the consequent changes as we move from one antecedent to a logically more specific one (e.g., does the prediction flip from A to A\u2227C or from A\u2227C to A\u2227C \u2227D). We will call the number of flips the degree of the cascading set. The counterfactual semantics with weak centering3 permits the counterfactuals in (3) to be satisfiable at a world without forcing the antecedents of (3)b or (3)c to be inconsistent. The reason for this is that strengthening of the antecedent fails for counterfactuals; the closest worlds in which I make 100k euro do not include a world w in which I make 100k euro but am also convicted of fraud. Counterfactuals share this property with other conditionals that have been used as the basis for nonmonotonic reasoning [Ginsberg, 1986; Pearl, 1990]. However, if the actual world turns out to be like w, then by weak centering (3)a turns out to be false, because the ceteris paribus assumption in (3)a is that the actual world is one in which I\u2019m not convicted of fraud.\nIn adapting counterfactuals to provide explanations of a learning algorithm\u2019s behavior around a focal point, it is natural to interpret the similarity relation appealed to in the semantics of counterfactuals as a distance function over the feature space X used to describe data points\u2014in effect identifying the latter as the relevant \u201c worlds\u201d for the semantics of the counterfactuals to be defined over. To find the relevant counterfactuals to explain the behavior of f\u0302 around a focal point xp \u2208 X\nn where X has n dimensions, we exploit a linear map \u2206i : X\nn \u2192 Xn where \u2206i is defined such that there is a subset i \u2286 {1,2, . . . ,n} such that (i) for any x \u2208 X , \u2206i(x) differs from x only concerning values for dimensions in i, (ii) f\u0302 (xp) = \u03b7; (iii) f\u0302 (\u2206i(xp)) = \u03c0, with \u03b7,\u03c0 two incompatible predictions in Y and (iv) \u2200x\u2208X such that f\u0302 (x)= \u03c0 and x differs from xp only on the values of dimensions i, \u2016x\u2212 xp\u2016X \u2265 \u2016\u2206i(xp)\u2212 xp\u2016X , where \u2016.\u2016X is a natural norm on X like the Euclidean norm. Thus, \u2206i represents a minimal change in the features of xp (values of xp in the dimensions i) needed to shift the predictions of f\u0302 to \u03c0 from the unwanted prediction f\u0302 (xp). Exploiting the translation from feature values to literals, the antecedents of our target counterfactuals will express these features a conjunction of literals.\nAs discussed in [Kusner et al., 2017], these linear maps can be generated via techniques of adversarial perturbations.4 A typical definition of an adversarial perturbation of an image x, given a classifier, is that it is a smallest change to x such that the classification changes. Essentially, this is a counterfactual by a different name. Finding a closest possible world to x such that the classification changes is, under the right\n3Weak centering requires that w \u2264w w \u2032 for all w\u2032.\n4[Kusner et al., 2017] explore how to uncover these biased explanations and reveal the ones that are damaging. He uses a Pearl causal system to generate counterfactuals, where the causal variables are exogenous descriptions of input data for the ML algorithm.\nchoice of distance function, the same as finding the smallest change to x to get the classifier to make a different prediction. Adversarial perturbation has been the locus of a lot of recent research activity and can be computed quite efficiently [Dube, 2018]. Proofs for minimal perturbations can be found using optimal transport theory [Bachoc et al., 2018].\nThe fact that counterfactuals are closely tied to adversarial examples relative to some focal point invites a comparison to recent work by [Ignatiev et al., 2019] on explanations and adversarial examples in a deductive framework. The discussion in [Ignatiev et al., 2019] of counterexamples and adversarial examples builds a bridge between deductive nomological explanations and local counterfactual explanations based on a particular focal point. A counterexample to a prediction \u03c0 is a subset minimal set of literals A such that\nA |= \u2227 f\u0302 \u2192 \u2228\n\u03b46=\u03c0,\u03b4\u2208Y\n\u03b4.\n[Ignatiev et al., 2019] show that counterexamples and explanations are incompatible in that every explanation and every counterexample to a prediction \u03c0 contain literals ei and c j such that ei is inconsistent with c j. An adversarial example to a predication \u03c0 for x is then an instance that has all the features of a counterexample to \u03c0 and otherwise has the features of the instance x. An adversarial example for a learning\nmodel f\u0302 thus is a closest element y in X to a focal point xp, in which certain features are shifted so that f\u0302 (y) is a different prediction from f\u0302 (x). An adversarial example then can be defined in terms of a linear map \u2206i on X , and this map links the adversarial example with a counterfactual.\nCounterexamples can also serve as the basis of explanations of properties of a focal element.\n\u2022 Why not \u03c0 for x?\n\u2022 x has features of A and A is a counterexample to \u03c0 (A |= f\u0302 \u2192\u00ac\u03c0).\nNote that this deductive explanation is distinct from counterfactual explanations. When a counterfactual is used to give an explanation, the relationship between the explanans and the explanandum is not logical consequnce but a more pragmatic relation based on a Lewisian analysis of causation. The counterfactual in (2) gives a sufficient reason for A\u2019s getting the loan, all other factors of my situation being equal or being as equal as possible given the assumption of a different salary for me. Deductive explanations specify those ceteris paribus conditions; this makes them more complex but also invariant with respect to the choice of focal point. Counterfactual explanations depend on the nature of the focal point.\nThis (relative) simplicity comes at a cost. Counterfactuals may offer only a partial explanation in some cases [Wachter et al., 2017]. In fact there are two sorts of partiality in a counterfactual explanation. First a counterfactual explanation doesn\u2019t specify the ceteris paribus conditions and so doesn\u2019t specify what is necessary for the prediction\u2014call this partiality. On the other hand counterfactual explanations are also partial in the sense that they don\u2019t specify all the sufficient conditions for the prediction; they are hence what are called local explanations. A given counterfactual might give\nonly a partial picture of the behavior of the program or agent. It might not give a complete local explanation of a decision, as such a decision might be over determined for the given point at hand. A prediction \u03c0 is over determined for a focal point x0 iff the following set of linear maps contains at least two elements\n{\u2206i : i \u2286 n\u2227 f\u0302 (\u2206i(x0)) = \u03c0\u2227 f\u0302 (x0) = \u03b7,\u03b7 6= \u03c0}\nMany real world applications like our bank loan example will have this feature."
    },
    {
      "heading": "3 From partial to complete explanations",
      "text": "In principle, we can move from a partial picture of the behav-\nior of f\u0302 to a more complete one. The linear maps \u2206i associated with counterfactuals permit us to plot the local behavior of f\u0302 around a focal point xp. If we look at all possible \u2206i using all the possible combinations of dimensions of X , we can plot a neighborhood around xp, N f\u0302 ,xp , where for all points z in the interior N f\u0302 ,xp , f\u0302 (z) = f\u0302 (xp) and for points w on the boundary of N f\u0302 ,xp , f\u0302 (w) 6= f\u0302 (xp). We call the collection of such linear maps a complete local explanation of the decision at xp.\nN f\u0302 ,xp captures the fact that there may be several distinct conditions the lack of which would be causally responsible for a particular prediction, like my not getting the loan in our motivating example. Let us call the set of counterfactuals corresponding to N f\u0302 ,xp , S(N f\u0302 ,xp). S(N f\u0302 ,xp) may contain many cascading counterfactuals, and its cascading degree may be high.\nThere are some cases in which the geometry of the prediction space allow us to move from complete local to global\nexplanations of the behavior of f\u0302 . Suppose f\u0302 changes values only once for each feature/dimension di moving out from a focal point xp.\n5 In such a case N f\u0302 ,xp forms a convex sub-\nspace of f\u0302 [X ] and a complete local explanation provides a full global explanation.\nThere is an important connection between the cascad-\ning degree of S(N f\u0302 ,xp) and the geometry of f\u0302 on the feature space. If f\u0302 changes values only once for each feature/dimension di, S(N f\u0302 ,xp), has cascading degree 1. In addition, S(N f\u0302 ,xp), has cascading degree 1 iff the feature/dimensions are pairwise independant of each other with\nrespect to f\u0302 \u2019s predictions.\nProposition 1. Suppose that the feature space is Boolean valued. Then S(N f\u0302 ,xp), has a cascading degree \u2264 2 iff N f\u0302 ,xp is convex.\nProof: Note that a cascading set of counterfactuals exhibits an entailment relation between antecedents. Thus, if \u03c6 \u2192 \u03c8 and \u03c7 \u2192 \u00ac\u03c8 are counterfactuals of degrees n and n-1, \u03c6 entails \u03c7. This means that if we have a set S of cascading counterfactuals of degree 3 or more, we will have antecedents \u03c6, \u03c7, \u03b4 which are conjunctions of feature values, such that if\n5By complicating the language of counterfactuals, we can have probability estimates of literals and so approximate continuous feature spaces.\n\u03c6 |= \u03c7 |= \u03b4 that in the Manhattan space of Boolean features puts the points in the feature space corresponding to \u03c6, \u03c7, \u03b4 are all on a same line. But if S has degree 3 or greater, this forcibly means that one of the points will not be in the space of predictions made f\u0302 at the other two points. So N f\u0302 ,xp is non convex. Conversely, suppose N f\u0302 ,xp is non convex. Using the construction of counterfactuals from N f\u0302 ,xp will immediately yield a cascading set of degree 3 or higher. The cascading degree of S(N f\u0302 ,xp) for nonconvex N f\u0302 ,xp thus gives a measure of the degree of non-convexity of N f\u0302 ,xp , and a measure of the complexity of an explanation.\nRemark 1. : Suppose N f\u0302 ,xp gives rise to a set of cascading counterfactuals of degree 2. In this case two complete local explanations can determine a full global explanation (and de-\ntermine the behavior of f\u0302 ).\nThe problem with complete local explanation is that they may be still too complex for any human to understand. It is not unusual for AI applications to encode data via hundreds even thousands of features. The complete local explanation would involve too many counterfactuals for humans to grasp."
    },
    {
      "heading": "4 Pragmatic constraints on explanations",
      "text": "Because the set of causal factors can be so complex, it becomes apparent that explanations must have an important pragmatic component. All explanations have an \u201cexplainee\u201d who asks for the explanation, and an explanation for an explainee must respond to the particular conundrum that brought the explainee to ask for one [Bromberger, 1962; Achinstein, 1980].6 For instance, our loan seeker A may wonder why the bank refused her a loan when she has what she thinks is an adequate qualifying income and other qualities. An appropriate explanation for A would then explain which of his assumptions was faulty or incomplete, thus solving the conundrum. In essence, what is going on here is that A has in mind the \u201cideal\u201d function f and is confused about why the\nvalue of f\u0302 on her data xp is not that of f ; i.e. her conundrum is that f (xp) 6= f\u0302 (xp). The conundrum can come about for two reasons: either 0 is simply mistaken about the nature of\nf\u0302 (perhaps she is also mistaken about f or if not, she is mis-\ntaken about how f\u0302 differs from f ), or her understanding of f is incomplete.\nFor the incompleteness sense, suppose xp is decomposed into \u3008x~d1 ,x~d2 \u3009; for A f only pays attention to the values of dimensions ~d1 in the sense that for her f (\u3008x~d1 ,x~d2 \u3009) = f (\u3008x~d1 ,x\u2032~d2 \u3009), for any values x\u2032~d2 . An adequate explanation will then point out that for some \u2206 where \u2206(\u3008x~d1 ,x~d2 \u3009= \u3008x~d1 ,y~d2 \u3009, f\u0302 (\u2206(\u3008x~d1 ,y~d2 \u3009)) = \u03c0 while f\u0302 (xp) = \u03b7.\nSo we have, simplifying, conundra resulting from incompleteness and conundra resulting from mistaken information. We then stipulate:\nCI Suppose we have a conundrum based on incompleteness. An adequate explanation for explainee A who re-\n6A more contemporary view aligned with this is [Miller, 2019].\nquests an explanation why f\u0302 (xp) = \u03b7 must resolve A\u2019s conundrum arising from attending only to some dimensions ~d1 of xp = \u3008x~d1 ,x~d2 \u3009. More precisely, the explanation must provide a \u2206 such that \u2206(\u3008x~d1 ,x~d2 \u3009) = \u3008x~d1 ,y~d2 \u3009 and f\u0302 (\u2206(xp)) = f\u0302 (\u3008x~d1 ,y~d2 \u3009) = \u03c0 while f\u0302 (xp) = \u03b7.\nCM Suppose A\u2019s conundrum is based on error. An adequate explanation for explainee A who requests an explana-\ntion why f\u0302 (xp) = \u03b7 must resolve A\u2019s conundrum by providing the values for the dimensions ~d2 of xp on which A is mistaken. More precisely, supposing a decomposition of xp = \u3008x~d1 ,x~d2 \u3009, the explanation must provide a \u2206 such that \u2206(\u3008x~d1 ,x~d2 \u3009) = \u3008y~d1 ,x~d2 \u3009 such that f\u0302 (\u3008y~d1 ,x~d2 \u3009) = f\u0302 (\u2206(xp)) = \u03c0 while f\u0302 (xp) = \u03b7."
    },
    {
      "heading": "5 The fairness / bias problem",
      "text": "Partial explanations seem good epistemologically, but they can also be dangerous. We remarked that several counterfactuals that point to a difference in behavior may be simultaneously true. But this means that without the complete local\nexplanation, f\u0302 may act in ways unknown to the agent x0 or the public that is biased or unfair. Worse, the constructor or\nowner of f\u0302 will be able to conceal this fact if the decision for x0 is overdetermined, by offering counterfactual explanations using maps \u2206i that don\u2019t mention the biased feature.\nSo we need another constraint on satisfactory explanations: a satisfactory explanation must make clear the biases of the system which may account for 0\u2019s incomplete understanding\nof f or f\u0302 . To be more precise, we say that f\u0302 exhibits a biased dependency on particular preducial factor P, which we can also take to be a map on X , just in case for some \u2206i, and for some incompatible predictions \u03b4 and \u03c0\nf\u0302 (\u2206i(xp)) = \u03b4 (1)\nf\u0302 (P(\u2206i(xp))) = \u03c0 (2)\nNote that the incompleteness condition for a conundrum mirrors the notion of a biased dependency. As a person with data xp might reasonably want to know whether such biases were the result of a particular decision, we put one additional constraint on appropriate explanations:\nCB an appropriate explanation for explainee A must lay bare any prejudicial factors P that affect A. That is, where \u03c0, \u03b4 and P are defined as above, the explanation must provide\nf\u0302 (\u2206(xp)) = \u03b4 and f\u0302 (P(\u2206(xp))) = \u03c0\nIn our loan example, explanations that obey (CB) might not be in the interest of the bank that owns the ML algorithm. For instance, the bias of the bank against loans to people of color is unfair to A. But the bank might not want to have this bias exposed. This is the ethical problem for explanations.\nTo attain an appropriate explanation meeting CB, we cannot simply rely on the bank\u2019s proferred explanation, as the bank may have something to hide. In similar fashion, even if\nwe have access to all of f\u0302 , we cannot rest content with just one counterfactual explanation that satisfies CM and CI. We\nhave to ensure that CB is met as well. We will say that a set of counterfactuals provides an adequate local explanation just in case it obeys CM, CI and CB.\nLet us suppose that we do not have access to f\u0302 . To find an appropriate explanation, we imagine a game played between the bank and the would be loan taker, in which the loan taker can ask questions of the bank (or owner/ developer of the algorithm) about the algorithm\u2019s decisions. More particularly, we propose to use a two player game, an explanation game to get at appropriate explanations for A.\nTo define an explanation game, we first fix a set of two players {0,1}. (1\u2212 i) denotes the opponent of i.\nThe moves or actions in the game for 0, denoted A0, are to request an explanation from 1 about the behavior of f\u0302 at xp, which means requesting a particular linear map \u2206i. 0 may accept an explanation provided by 1 or reject 1\u2019s explanation and request an explanation different from explanations previously offered by 1; that is request a \u2206 j where j and i are different sets of dimensions of X (this move is known as restriction. Alternatively, 0 may force 1 to provide a counterfactual explanation generating linear map for a particular set of dimensions. If 0 is allowed to use forcing, then she can name which dimensions she wants varied to find a boundary point of N f\u0302 ,xp .\n1\u2019s moves A1 consists of the following: 1 may offer a \u2206 j generating a counterfactual explanation; 1 may also claim that a particular map \u2206 j for dimensions j does generate a counterfactual explanation\u2014that is, for no values in dimensions j do we get the requested prediction; 1 may also insist that a previously given \u2206 solve 0\u2019s conundra. The game terminates when (a) 0 is convinced that her conundra are resolved or (b) when all possible \u2206 have been examined or (c) 0 does not want to continue anymore.\nWe assume that in an explanation game it is common knowledge among the players that one can check the truth of 1\u2019s responses and whether 1\u2019s counterfactuals address the constraints we have mentioned. Thus, we do not address issues of deceit here.\nWe can now specify an explanation game and its winning condition for player 0.\nDefinition 1. An Explanation game, G , concerning a polynomially computable function f\u0302 : Xn \u2192 Y , where Xn is a space of data and Y a set of predictions, is a tuple ((A0 \u222a A1, f\u0302 : X n \u2192 Y,xp,C) where:\ni. 1, but not 0 has access to the behavior of f\u0302 .\nii. xp \u2208 X n is the starting position, and 0 opens G with a\nrequest for an explanation.\niii. 1 responds to 0\u2019s requests (forcing or restriction) or claims an adequate explanation is already provided.\niv. C \u2286 Xn is a local minimum of f\u0302 with respect to the neighbourhood N f\u0302 ,x, where x is the current position in the game. Every c \u2208 C resolves one or more of 0\u2019s conundra CM or CI.\nWe say that 0 wins G just in case in G she acquires a set of counterfactuals about the behavior of f\u0302 concerning features C that answer her conundrum.\nIf the dimensions encoding data in X are common knowledge, then 0 always has a winning strategy in an explanation game. We will assume this, and so the real question is how quickly 0 can achieve her winning condition. This depends on three parameters. The first has to do with whether 0 knows\nwhich are the prejudicial dimensions for f\u0302 \u2019s behavior. In gen-\neral this is not known; an algorithm f\u0302 may be shown to be insensitive say to an explicit gender dimension in the data in the sense that it makes the same predictions whether this fea-\nture is taken into account or not. Nevertheless, f\u0302 might still be gender sensitive, because other dimensions of X encode gender information, making the gender dimension itself superfluous. In general we do not assume that 0 knows which dimensions are prejudicial\u2014i.e., which dimensions encode prejudicial factors.\nThe second parameter governing complexity is the degree of the set C of cascading counterfactuals associated with N f\u0302 ,xp . The third has to do whether 0 is allowed forcing moves or only restriction moves.7 If we assume that 1 has already calculated the boundary of N f\u0302 ,xp , then the following results are in order.\nProposition 2. Suppose in an explanation game G involving f\u0302 and where 0 has focal point xp \u2208 X n and the cascading degree of S(N f\u0302 ,xp) < 3. If 0 knows which dimensions encode prejudicial factors, she has an at worst linear time winning strategy using forcing. If she does not know know which dimensions encode prejudicial factors, 0 has an worst n2 time winning strategy.\nTo see that this, note that 0 needs only to force 1 to provide the counterfactuals that vary the known prejudicial factors together with at most one other feature value. The set of counterfactuals uncovered is a linear function of n.\nIf we do not assume 0 knows the prejudicial factors, then:\nProposition 3. Suppose in an explanation game G involving f\u0302 with 0\u2019s data as focal point xp \u2208 X n and the cascading degree of S(N f\u0302 ,xp) = k \u2265 3. Then 0 has a winning strategy using forcing in G that is in the complexity class Polynomial Local Search (PLS) in the worst case [Johnson et al., 1988].\nGiven that f\u0302 is polynomial and the cascading degree of S(N f\u0302 ,xp) is fixed, finding the neighbors of a boundary point of N f\u0302 ,xp is poly-time. Thus, 0\u2019s winning strategy for the game meets the conditions for a PLS problem. What happens when 0 cannot force 1 to provide certain counterfactuals, but can restrict 1 to shift a single dimension in his explanation?\nProposition 4. Suppose that for X \u2282 Rn in an explanation game G, with f\u0302 and 0 with focal point xp \u2208 X n and the cascading degree of S(N f\u0302 ,xp)< 2. Then 0 has a worst case linear time with respect to n winning strategy in G.\n7The complexity of the winning condition is important in a realistic model where finding an explanation is time sensitive. If 0 takes too long to find the explanation, she loses. The standard way to do this is to exploit discounting [Shapley, 1953]. See also [Asher and Paul, 2016] for a setting more similar to this one.\nGiven that 0 can only ask for a single dimension to be shifted, her strategy is just to go through all the dimensions by continuing to ask for a new explanation. Eventually 0 will have gone through all of the dimensions of Xn. As the cascading degree of S(N f\u0302 ,xp) < 2, she will have determined S(N f\u0302 ,xp).\nOnce the cascading degree associated with N f\u0302 ,xp \u2265 2, Player 1 can hide, if he wishes, prejudicial factors for exponential time.\nProposition 5. Suppose that for X \u2282 Rn in an explanation game G, with f\u0302 and 0 with focal point xp \u2208 X n and the cascading degree of S(N f\u0302 ,xp \u2265 2. Then 0 has a worst case exponential time with respect to n winning strategy in G.\nPlayer 1 in this case is free to provide counterfactuals that satisfy constraint (CI) and (CM) but possibly not (CB) and can provide counterfactuals with arbitrarily complex antecedents. Without forcing, 0 cannot make 1 provide counterfactuals that take satisfy (CB). However, given that the boundary N f\u0302 ,xp is computable, 0 still has a winning strategy with restriction in an explanation game. She will eventually compute the boundary of N f\u0302 ,xp in exponential time wrt to n in the worst case .\nWe draw from this the moral that in order to be effective an explanation game must allow 0 to force 1 to vary certain dimensions that 0 picks and S(N f\u0302 ,xp) must have a fixed cascading degree. We also claim that having such a fixed cascading degree might be an important aspect of good explanations, as there is an important connection between cascading degrees and the generality of laws. A low cascading degree means a general set of laws, which a priori is preferable scientifically."
    },
    {
      "heading": "6 Conclusions and outlook",
      "text": "We have used the semantics and logic of counterfactuals to explore epistemically accessible and adequate but partial explanations. Counterfactual explanations are promising vehicles for epistemic accessibility, and we have shown that they can algorithmically provide adequate explanations where all biases are made clear if certain conditions (forcing) obtain and the cascading degree of the set of counterfactuals describing the local neighborhood around the focal point is fixed. However, we have not explored here another important parameter for epistemic adequacy. Adequacy also depends on the characterization of the input data X for a learning algo-\nrithm f\u0302 : X \u2192 Y . If the dimensionality of X is too high or if the dimensions don\u2019t correspond to intuitive concepts, then even partial explanations may not be satisfactory. So to get\nfully explanatory counterfactuals for the behavior of f\u0302 , it will be important in such cases to find the right representation of the data for the explainee.\nCounterfactual explanations may also express adversarial examples. These typically aren\u2019t good explanations of the\nphenomenon f\u0302 is trying to model. Nevertheless such coun-\nterfactuals explain the behavior of f\u0302 and can be as such very valuable."
    }
  ],
  "year": 2020
}
