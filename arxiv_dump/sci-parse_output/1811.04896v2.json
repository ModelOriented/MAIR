{
  "abstractText": "Artificial intelligence systems are being increasingly deployed due to their potential to increase the efficiency, scale, consistency, fairness, and accuracy of decisions. However, as many of these systems are opaque in their operation, there is a growing demand for such systems to provide explanations for their decisions. Conventional approaches to this problem attempt to expose or discover the inner workings of a machine learning model with the hope that the resulting explanations will be meaningful to the consumer. In contrast, this paper suggests a new approach to this problem. It introduces a simple, practical framework, called Teaching Explanations for Decisions (TED), that provides meaningful explanations that match the mental model of the consumer. We illustrate the generality and effectiveness of this approach with two different examples, resulting in highly accurate explanations with no loss of prediction accuracy for these two examples.",
  "authors": [
    {
      "affiliations": [],
      "name": "Michael Hind"
    },
    {
      "affiliations": [],
      "name": "Dennis Wei"
    },
    {
      "affiliations": [],
      "name": "Murray Campbell"
    },
    {
      "affiliations": [],
      "name": "Noel C. F. Codella"
    },
    {
      "affiliations": [],
      "name": "Amit Dhurandhar"
    },
    {
      "affiliations": [],
      "name": "Aleksandra Mojsilovi\u0107"
    },
    {
      "affiliations": [],
      "name": "Karthikeyan Natesan Ramamurthy"
    },
    {
      "affiliations": [],
      "name": "Kush R. Varshney"
    }
  ],
  "id": "SP:6b2fed2b08a1522bec61f595acd8cf0684735c48",
  "references": [
    {
      "authors": [
        "Yessenalina Ainur",
        "Yejin Choi",
        "Claire Cardie"
      ],
      "title": "Automatically Generating Annotator Rationales to Improve Sentiment Classification",
      "venue": "In Proceedings of the ACL 2010 Conference Short Papers",
      "year": 2010
    },
    {
      "authors": [
        "Osbert Bastani",
        "Carolyn Kim",
        "Hamsa Bastani"
      ],
      "title": "Interpreting Blackbox Models via Model Extraction",
      "venue": "arXiv preprint arXiv:1705.08504",
      "year": 2018
    },
    {
      "authors": [
        "Or Biran",
        "Courtenay Cotton"
      ],
      "title": "Explanation and Justification in Machine Learning: A Survey",
      "venue": "Workshop on Explainable AI (XAI)",
      "year": 2017
    },
    {
      "authors": [
        "Rich Caruana",
        "Yin Lou",
        "Johannes Gehrke",
        "Paul Koch",
        "Marc Sturm",
        "Noemie Elhadad"
      ],
      "title": "Intelligible Models for HealthCare: Predicting Pneumonia Risk and Hospital 30-day Readmission",
      "venue": "In Proc. ACM SIGKDD Int. Conf. Knowl. Disc. Data Min. Sydney,",
      "year": 2015
    },
    {
      "authors": [
        "Sanjeeb Dash",
        "Oktay Gunluk",
        "Dennis Wei"
      ],
      "title": "Boolean Decision Rules via Column Generation",
      "venue": "In Advances in Neural Information Processing Systems",
      "year": 2018
    },
    {
      "authors": [
        "Amit Dhurandhar",
        "Vijay Iyengar",
        "Ronny Luss",
        "Karthikeyan Shanmugam"
      ],
      "title": "A Formal Framework to Characterize Interpretability of Procedures",
      "venue": "In Proc. ICML Workshop Human Interp. Mach. Learn. Sydney,",
      "year": 2017
    },
    {
      "authors": [
        "Jeff Donahue",
        "Kristen Grauman"
      ],
      "title": "Annotator Rationales for Visual Recognition",
      "venue": "In ICCV",
      "year": 2011
    },
    {
      "authors": [
        "Finale Doshi-Velez",
        "Been Kim"
      ],
      "title": "Towards A Rigorous Science of Interpretable Machine Learning",
      "year": 2017
    },
    {
      "authors": [
        "Finale Doshi-Velez",
        "Mason Kortz",
        "Ryan Budish",
        "Chris Bavitz",
        "Sam Gershman",
        "David O\u2019Brien",
        "Stuart Schieber",
        "James Waldo",
        "David Weinberger",
        "Alexandra Wood"
      ],
      "title": "Accountability of AI Under the Law: The Role of Explanation",
      "year": 2017
    },
    {
      "authors": [
        "Kun Duan",
        "Devi Parikh",
        "David Crandall",
        "Kristen Grauman"
      ],
      "title": "Discovering Localized Attributes for Fine-grained Recognition",
      "year": 2012
    },
    {
      "authors": [
        "Bryce Goodman",
        "Seth Flaxman"
      ],
      "title": "EU Regulations on Algorithmic Decision-Making and a \u2018Right to Explanation",
      "venue": "In Proc. ICML Workshop Human Interp. Mach. Learn. New York,",
      "year": 2016
    },
    {
      "authors": [
        "Lisa Anne Hendricks",
        "Zeynep Akata",
        "Marcus Rohrbach",
        "Jeff Donahue",
        "Bernt Schiele",
        "Trevor Darrell"
      ],
      "title": "Generating Visual Explanations",
      "venue": "In European Conference on Computer Vision",
      "year": 2016
    },
    {
      "authors": [
        "Been Kim"
      ],
      "title": "Tutorial on Interpretable machine learning. \"http://people.csail",
      "year": 2017
    },
    {
      "authors": [
        "Todd Kulesza",
        "Simone Stumpf",
        "Margaret Burnett",
        "Sherry Yang",
        "Irwin Kwan",
        "Weng-Keen Wong"
      ],
      "title": "Too Much, Too Little, or Just Right? Ways Explanations Impact End Users",
      "venue": "Mental Models. In Proc. IEEE Symp. Vis. Lang. Human-Centric Comput. San Jose,",
      "year": 2013
    },
    {
      "authors": [
        "Tao Lei",
        "Regina Barzilay",
        "Tommi Jaakkola"
      ],
      "title": "Rationalizing neural predictions",
      "venue": "In EMNLP",
      "year": 2016
    },
    {
      "authors": [
        "Zachary C Lipton"
      ],
      "title": "The mythos of model interpretability",
      "venue": "In ICML Workshop on Human Interpretability of Machine Learning",
      "year": 2016
    },
    {
      "authors": [
        "Tania Lombrozo"
      ],
      "title": "Simplicity and probability in causal explanation",
      "venue": "Cognitive Psychol. 55,",
      "year": 2007
    },
    {
      "authors": [
        "Scott Lundberg",
        "Su-In Lee"
      ],
      "title": "A unified approach to interpreting model predictions",
      "venue": "In Advances of Neural Inf. Proc. Systems",
      "year": 2017
    },
    {
      "authors": [
        "T. McDonnell",
        "M. Lease",
        "M. Kutlu",
        "T. Elsayed"
      ],
      "title": "Why Is That Relevant? Collecting Annotator Rationales for Relevance Judgments",
      "venue": "In Proc. AAAI Conf. Human Comput. Crowdsourc",
      "year": 2016
    },
    {
      "authors": [
        "Tim Miller"
      ],
      "title": "Explanation in Artificial Intelligence: Insights from the Social Sciences",
      "venue": "arXiv preprint arXiv:1706.07269",
      "year": 2017
    },
    {
      "authors": [
        "Tim Miller",
        "Piers Howe",
        "Liz Sonenberg"
      ],
      "title": "Explainable AI: Beware of Inmates Running the Asylum Or: How I Learnt to Stop Worrying and Love the Social and Behavioural Sciences",
      "venue": "In Proc. IJCAI Workshop Explainable Artif. Intell. Melbourne, Australia",
      "year": 2017
    },
    {
      "authors": [
        "Gr\u00e9goireMontavon",
        "Wojciech Samek",
        "Klaus-Robert M\u00fcller"
      ],
      "title": "Methods for interpreting and understanding deep neural networks",
      "venue": "Digital Signal Processing",
      "year": 2017
    },
    {
      "authors": [
        "P. Peng",
        "Y. Tian",
        "T. Xiang",
        "Y.Wang",
        "T. Huang"
      ],
      "title": "Joint Learning of Semantic and Latent Attributes",
      "venue": "In ECCV 2016, Lecture Notes in Computer Science,",
      "year": 2016
    },
    {
      "authors": [
        "Marco Tulio Ribeiro",
        "Sameer Singh",
        "Carlos Guestrin"
      ],
      "title": "Why Should I Trust You?\u201d: Explaining the Predictions of Any Classifier",
      "venue": "In Proc. ACM SIGKDD Int. Conf. Knowl. Disc. Data Min. San Francisco,",
      "year": 2016
    },
    {
      "authors": [
        "A.D. Selbst",
        "J. Powles"
      ],
      "title": "Meaningful Information and the Right to Explanation",
      "venue": "Int. Data Privacy Law 7,",
      "year": 2017
    },
    {
      "authors": [
        "Qiang Sun",
        "Gerald DeJong"
      ],
      "title": "Explanation-Augmented SVM: an Approach to Incorporating Domain Knowledge into SVM Learning",
      "venue": "In 22nd International Conference on Machine Learning",
      "year": 2005
    },
    {
      "authors": [
        "James Vacca"
      ],
      "title": "A Local Law in relation to automated decision systems used by agencies. Technical Report",
      "year": 2018
    },
    {
      "authors": [
        "Kush R. Varshney"
      ],
      "title": "Engineering Safety in Machine Learning",
      "venue": "In Information Theory and Applications Workshop",
      "year": 2016
    },
    {
      "authors": [
        "S. Wachter",
        "B. Mittelstadt",
        "L. Floridi"
      ],
      "title": "Transparent, explainable, and accountable AI for robotics. Science Robotics 2 (May 2017)",
      "year": 2017
    },
    {
      "authors": [
        "Sandra Wachter",
        "Brent Mittelstadt",
        "Luciano Floridi"
      ],
      "title": "Why a Right to Explanation of Automated Decision-Making Does Not Exist in the General Data Protection Regulation",
      "venue": "Int. Data Privacy Law 7,",
      "year": 2017
    },
    {
      "authors": [
        "Omar F. Zaidan",
        "Jason Eisner"
      ],
      "title": "Using \u2019annotator rationales\u2019 to improve machine learning for text categorization",
      "venue": "NAACL-HLT",
      "year": 2007
    },
    {
      "authors": [
        "Omar F. Zaidan",
        "Jason Eisner"
      ],
      "title": "Modeling Annotators: A Generative Approach to Learning from Annotator Rationales",
      "venue": "In Proceedings of EMNLP",
      "year": 2008
    },
    {
      "authors": [
        "Ye Zhang",
        "Iain James Marshall",
        "Byron C.Wallace"
      ],
      "title": "Rationale-Augmented Convolutional Neural Networks for Text Classification",
      "venue": "InConference on Empirical Methods in Natural Language Processing (EMNLP)",
      "year": 2016
    }
  ],
  "sections": [
    {
      "text": "KEYWORDS Explainable AI; Meaningful Explanation; AI Ethics; Elicitation; Supervised Classification; Machine Learning\nACM Reference Format: Michael Hind, Dennis Wei, Murray Campbell, Noel C. F. Codella, Amit Dhurandhar, and Aleksandra Mojsilovi\u0107, Karthikeyan Natesan Ramamurthy, Kush R. Varshney . 2019. TED: Teaching AI to Explain its Decisions. In AAAI/ACM Conference on AI, Ethics, and Society (AIES \u201919), January 27\u2013 28, 2019, Honolulu, HI, USA. ACM, New York, NY, USA, 7 pages. https: //doi.org/10.1145/3306618.3314273"
    },
    {
      "heading": "1 INTRODUCTION",
      "text": "Machine learning based systems have proven to be quite effective for producing highly accurate results in several domains. This effectiveness is leading to wider adoption in higher stakes domains, which has the potential to lead to more accurate, consistent, and fairer decisions and the resulting societal benefits. However, given the higher stakes of these domains, there is a growing demand that these systems provide explanations for their decisions, so that necessary oversight can occur, and a citizen\u2019s due process rights are respected [4, 5, 10, 15, 17, 33\u201336].\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. AIES \u201919, January 27\u201328, 2019, Honolulu, HI, USA \u00a9 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-6324-2/19/01. . . $15.00 https://doi.org/10.1145/3306618.3314273\nThe demand for explanation has manifested itself in new regulations that call for automated decision making systems to provide \u201cmeaningful information\u201d on the logic used to reach conclusions [15, 31, 36]. Selbst and Powles [31] interpret the concept of \u201cmeaningful information\u201d as information that should be understandable to the audience (potentially individuals who lack specific expertise), is actionable, and is flexible enough to support various technical approaches.\nUnfortunately, the advance in effectiveness of machine-learning techniques has coincided with increased complexity in the inner workings of these techniques. For some techniques, like deep neural networks or large random forests, even experts cannot explain how decisions are reached. Thus, we have a stronger need for explainable AI, just when we have a greater gap in achieving it.\nThis has sparked a growing research community focused on this problem [18, 19]. Most of this research attempts to explain the inner workings of a machine learning model either directly, indirectly via a simpler proxy model, or by probing the model with related inputs. This paper proposes a different approach that requires a model to jointly produce both a decision as well as an explanation, rather than exposing the inner details of how the model produces a decision. The explanation is not constrained to any particular format and can vary to accommodate user needs.\nThe main contributions of this work are as follows:\n\u2022 A description of the challenges in providing meaningful explanations for machine learning systems. \u2022 Anew framework, called TED, that enables machine learning algorithms to provide meaningful explanations that match the complexity and domain of consumers. \u2022 A simple instantiation of the framework that demonstrates the generality and simplicity of the approach. \u2022 Two illustrative examples and results that demonstrate the effectiveness of the instantiation in providing meaningful explanations. \u2022 A discussion on several possible extensions and open research opportunities that this framework enables.\nThe rest of this paper is organized as follows. Section 2 explores the challenges presented by the problem statement of providing explanations for AI decisions. Section 3 discusses related work. Section 4 describes our general approach, TED, and a simple instantiation for providing explanations that are understandable by the consumer and discusses the advantages of the approach. Section 5 presents results from two examples that demonstrate the effectiveness of the simple instantiation. Section 6 discusses future directions and open issues for the TED approach. Section 7 draws conclusions.\nar X\niv :1\n81 1.\n04 89\n6v 2\n[ cs\n.A I]\n1 5\nJu n\n20 19"
    },
    {
      "heading": "2 CHALLENGES TO PROVIDING AI EXPLANATIONS",
      "text": "This section explores the challenges in providing meaningful explanations, which provide the motivation for the TED framework.\nThe concept of an explanation is probably as old as human communication. Intuitively, an explanation is the communication from one person (A) to another (B) that provides justification for an action or decision made by person A. Mathematicians use proofs to formally provide explanations. These are constructed using agreedupon logic and formalism, so that any person trained in the field can verify if the proof/explanation is valid. Unfortunately, we do not have such formalism for non-mathematical explanations. Even in the judicial system, we utilize nonexpert jurors to determine if a defendant has violated a law, relying on their intuition and experience in weighing (informal) arguments made by prosecution and defense.\nSince we do not have a satisfying formal definition for valid human-to-human explanations, developing one for system-to-human explanations is challenging [17, 22]. Motivated by the concept of meaningful information [15, 31, 36], we feel that explanations must have the following three characteristics:\nJustification: An explanation needs to provide justification for a decision that increases trust in the decision. This often includes some information that can be verified by the consumer. Complexity Match: The complexity of the explanation needs to match the complexity capability of the consumer [7, 20]. For example, an explanation in equation form may be appropriate for a statistician, but not for a nontechnical person [27]. Domain Match: An explanation needs to be tailored to the domain, incorporating the relevant terms of the domain. For example, an explanation for a medical diagnosis needs to use terms relevant to the physician (or patient) who will be consuming the prediction.\nThere are at least four distinct groups of people who are interested in explanations for an AI system, with varying motivations.\nGroup 1: End User Decision Makers: These are the people who use the recommendations of an AI system to make a decision, such as physicians, loan officers, managers, judges, social workers, etc. They desire explanations that can build their trust and confidence in the system\u2019s recommendations and possibly provide themwith additional insight to improve their future decisions and understanding of the phenomenon. Group 2: Affected Users: These are the people impacted by the recommendationsmade by anAI system, such as patients, loan applicants, employees, arrested individuals, at-risk children, etc. They desire explanations that can help them understand if they were treated fairly and what factor(s) could be changed to get a different result [10]. Group 3: Regulatory Bodies: Government agencies, charged to protect the rights of their citizens, want to ensure that decisions are made in a safe and fair manner, and that society is not negatively impacted by the decisions. Group 4: AI System Builders: Technical individuals (data scientists and developers) who build or deploy an AI system\nwant to know if their system is working as expected, how to diagnose and improve it, and possibly gain insight from its decisions.\nUnderstanding the motivations and expectations behind each group\u2019s needs for an explanation will help to ensure a solution that satisfies these expectations. For example, Group 4 is likely to desire a more complex explanation of the system\u2019s inner workings to take action. Group 3\u2019s needs may be satisfied by showing the overall process, including training data, is fair and free of negative societal impact and they may not be able to consume the same level of complexity as Group 4. Group 1 will have a high need for domain sophistication, but will also have less tolerance for complex explanations. Finally, Group 2 will have the lowest threshold for both complexity and domain information. These are affected users, such as loan applicants, and need to have the reasons for their outcomes such as loan denials explained in a simplemanner without industry terms or complex formulas.\nIn summary, outside of a logical proof, there is no clear definition of a valid explanation; it seems to be subjective to the consumer and circumstances. Furthermore, there is a wide diversity of potential consumers of explanations, with different needs, different levels of sophistication, and different levels of domain knowledge. This seems to make it impossible to produce a single meaningful explanation without any information about the consumer."
    },
    {
      "heading": "3 RELATEDWORK",
      "text": "Prior work in providing explanations can be partitioned into three areas:\n(1) Making existing or enhanced models interpretable, i.e. to provide a precise description of how the model determined its decision (e.g., [24, 28, 30]). (2) Creating a second, simpler-to-understand model, such as a small number of logical expressions, that mostly matches the decisions of the deployed model (e.g., [2, 5]). (3) Work in the natural language processing and computer vision domains that generate rationales/explanations derived from input text (e.g., [1, 16, 21]).\nThe first two groups attempt to precisely describe how amachine learning decision was made, which is particularly relevant for AI system builders (Group 4). The insight into the inner workings of a model can be used to improve the AI system and may serve as the seeds for an explanation to a non-AI expert. However, work still remains to determine if these seeds are sufficient to satisfy the needs of the diverse collection of non-AI experts (Groups 1\u20133). Furthermore, when the underlying features are not human comprehensible, these approaches are inadequate for providing human consumable explanations.\nThe third group seeks to generate textual explanations with predictions. For text classification, this involves selecting the minimal necessary content from a text body that is sufficient to trigger the classification. For computer vision [16], this involves utilizing textual captions in training to automatically generate new textual captions of images that are both descriptive as well as discriminative. Although promising, it is not clear how these techniques generalize to other domains and if the explanations will be meaningful to the variety of explanation consumers described in Section 2.\nDoshi-Velez et al. [10] discuss the societal, moral, and legal expectations of AI explanations, provide guidelines for the content of an explanation, and recommend that explanations of AI systems be held to a similar standard as humans. Our approach is compatible with their view. Biran and Cotton [3] provide an excellent overview and taxonomy of explanations and justifications in machine learning.\nMiller [26] and Miller, Howe, and Sonenberg [27] argue that explainable AI solutions need to meet the needs of the users, an area that has been well studied in philosophy, psychology, and cognitive science. They provides a brief survey of the most relevant work in these fields to the area of explainable AI. They, along with Doshi-Velez and Kim [9], call for more rigor in this area."
    },
    {
      "heading": "4 TEACHING EXPLANATIONS",
      "text": "Given the challenges to developing meaningful explanations for the diversity of consumers described in Section 2, we advocate a non-traditional approach. We suggest a high-level framework, with one simple instantiation, that we see as a promising complementary approach to the traditional \u201cinside-out\u201d approach to providing explanations.\nTo understand the motivation for the TED approach, consider the common situation when a new employee is being trained for their new job, such as a loan approval officer. The supervisor will show the new employee several example situations, such as loan applications, and teach them the correct action: approve or reject, and explain the reason for the action, such as \u201cinsufficient salary\u201d. Over time, the new employee will be able to make independent decisions on new loan applications and will give explanations based on the explanations they learned from their supervisor. This is analogous to how the TED framework works. We ask the training dataset to teach us, not only how to get to the correct answer (approve or reject), but also to provide the correct explanation, such as \u201cinsufficient salary\u201d, \u201ctoo much existing debt\u201d, \u201cinsufficient job stability\u201d, \u201cincomplete application\u201d, etc. From this training information, we will generate a model that, for new input, will predict answers and provide explanations based on the explanations it was trained on. Because these explanations are the ones that were provided by the training, they are relevant to the target domain and meet the complexity level of the explanation consumer.\nPrevious researchers have demonstrated that providing explanations with the training dataset may not add much of a burden to the training time and may improve the overall accuracy of the training data [25, 37\u201339]."
    },
    {
      "heading": "4.1 TED Framework and a Simple Instantiation",
      "text": "The TED framework leverages existing machine learning technology in a straightforward way to generate a classifier that produces explanations along with classifications. To review, a supervised machine learning algorithm takes a training dataset that consists of a series of instances with the following two components:\nX, a set of features (feature vector) for the particular entity, such as an image, a paragraph, loan application, etc. Y, a label/decision/classification for each feature vector, such an image description, a paragraph summary, or a loan-approval decision.\nThe TED framework requires a third component: E, an explanation for each decision, Y , which can take any\nform, such as a number, text string, an image, a video file, etc. Unlike traditional approaches, E does not necessarily need to be expressed in terms of X . It could be some other high-level concept specific to the domain that applies with some domain-specific combination of X , such as \u201cscary image\u201d or \u201cloan information is not trustworthy\u201d. Regardless of the format, we represent each unique value of E with an identifier.\nThe TED framework takes this augmented training set and produces a classifier that predicts both Y and E. There are several ways that this can be accomplished. The instantiation we explore in this work is a simple Cartesian product approach. This approach encodes Y and E into a new classification, called YE, which, along with the feature vector, X , is provided as the training input to any machine learning classification algorithm to produce a classifier that predicts YE\u2019s. After the model produced by the classification algorithm makes a prediction, we apply a decoding step to partition a YE prediction into its components, Y and E, to return to the consumer. Figure 1 illustrates the algorithm. The boxes in dashed lines are new TED components that encode Y and E into YE and decode a predicted YE into its individual components, Y and E. The solid boxes represent 1) any machine learning algorithm that takes a normal training dataset: features and labels, and 2) the resulting model produced by this algorithm."
    },
    {
      "heading": "4.2 Example",
      "text": "Let\u2019s assume we are training a system to recommend cancer treatments. A typical training set for such a system would be of the following form, where Pi is the feature vector representing patient i and Tj , represents various treatment recommendations.\n(P1, TA), (P2, TA), (P3, TA), (P4, TA) (P5, TB ), (P6, TB ), (P7, TB ), (P8, TC )\nThe TED approach would require adding an additional explanation component to the training dataset as follows:\n(P1, TA, E1), (P2, TA, E1), (P3, TA, E2), (P4, TA, E2) (P5, TB, E3), (P6, TB, E3), (P7, TB, E4), (P8, TC , E5)\nEach Ei would be an explanation to justify why a feature vector representing a patient would map to a particular treatment. Some treatments could be recommended formultiple reasons/explanations. For example, treatment TA is recommended for two different reasons, E1 and E2, but treatment TC is only recommended for reason E5.\nGiven this augmented training data, the Cartesian product instantiation of the TED framework transforms this triple into a form that any supervised machine learning algorithm can use, namely (feature, class) by combining the second and third components into a unique new class as follows:\n(P1, TAE1), (P2, TAE1), (P3, TAE2), (P4, TAE2) (P5, TBE3), (P6, TBE3), (P7, TBE4), (P8, TCE5)\nFigure 2 shows how the training dataset would change using the TED approach for the above example. The left picture illustrates how the original 8 training instances in the example are mapped into the 3 classes. The right picture shows how the training data is changed, with explanations added. Namely, Class A was decomposed to Classes A1 and A2. Class B was transformed into Classes B3 and B4 and Class C became C5.\nAs Figure 2 illustrates, adding explanations to training data implicitly creates a 2-level hierarchy in that the transformed classes are members of the original classes, e.g., Classes A1 and A2 are a decomposition of the original Class A. This hierarchical property could be exploited by employing hierarchical classification algorithms when training to improve accuracy."
    },
    {
      "heading": "4.3 Advantages",
      "text": "Although this approach is simple, there are several nonobvious advantages that are particularly important in addressing the requirements of explainable AI for groups 1, 2, and 3 discussed in Section 2. Complexity/Domain Match: Explanations provided by the algorithm are guaranteed to match the complexity and mental model of the domain, given that they are created by the domain expert who is training the system. Dealing with Incomprehensible Features: Since the explanation format can be of any type, they are not limited to being a function of the input features, which is useful when the features are not comprehensible. Accuracy: Explanations will be accurate if the training data explanations are accurate and representative of production data. Generality: This approach is independent of the machine learning classification algorithm; it can work with any supervised classification algorithm, including neural networks, making this technique widely deployable. Preserves Intellectual Property: There is no need to expose details of machine learning algorithm to the consumer. Thus, proprietary technology can remain protected by their owners. Easy to incorporate: The Cartesian product approach does not require a change to the current machine learning algorithm, just the addition of pre- and post-processing components: encoder and decoder. Thus, an enterprise does not need to adopt a new machine learning algorithm, just to get explanations.\nEducates Consumer: The process of providing good training explanations will help properly set expectations for what kind of explanations the system can realistically provide. For example, it is probably easier to explain in the training data why a particular loan application is denied than to explain why a particular photo is a cat. Setting customer expectations correctly for what AI systems can (currently) do is important to their satisfaction with the system. Improved Auditability: After creating a TED dataset, the domain expert will have enumerated all possible explanations for a decision. (The TED system does not create any new explanations.) This enumeration can be useful for the consumer\u2019s auditability, i.e., to answer questions such as \u201cWhat are the reasons why you will deny a loan?\u201d or \u201cWhat are the situations in which you will prescribe medical treatment X?\u201d May Reduce Bias: Providing explanations will increase the likelihood of detecting bias in the training data because 1) a biased decision will likely be harder for the explanation producer to justify, and 2) one would expect that training instances with the same explanations cluster close to each other in the feature space. Anomalies from this property could signal a bias or a need for more training data."
    },
    {
      "heading": "5 EVALUATION",
      "text": "To evaluate the ideas presented in this work, we focus on two fundamental questions:\n(1) How useful are the explanations produced by the TED approach? (2) How is the prediction accuracy impacted by incorporating explanations into the training dataset?\nSince the TED framework has many instantiations, can be incorporated into many kinds of learning algorithms, tested against many datasets, and used in many different situations, a definitive answer to these questions is beyond the scope of this paper. Instead we try to address these two questions using the simple Cartesian product instantiation with two different machine learning algorithms (neural nets and random forest), on two use cases to show that there is justification for further study of this approach.\nDetermining if any approach provides useful explanations is a challenge and no consensus metric has yet to emerge [10]. However, since the TED approach requires explanations be provided for the target dataset (training and testing), one can evaluate the accuracy of a model\u2019s explanation (E) in a similar way that one evaluates the accuracy of a predicted label (Y ).\nThe TED approach requires a training set that contains explanations. Since such datasets are not yet readily available, we evaluate the approach on two synthetic datasets described below: tic-tac-toe and loan repayment."
    },
    {
      "heading": "5.1 Tic-Tac-Toe",
      "text": "The tic-tac-toe example tries to predict the best move given a particular board configuration. A tic-tac-toe board is represented by two 3 \u00d7 3 binary feature planes, indicating the presence of X and O, respectively. An additional binary feature indicates the side to move, resulting in a total of 19 binary input features. Each legal nonterminal board position (4,520) is labeled with a preferred move,\nalong with the reason the move is preferred. The labeling is based on a simple set of rules that are executed sequentially:1\n(1) If a winning move is available, completing three in a row for the side to move, choose that move with reasonWin (2) If a blocking move is available, preventing the opponent from completing three in a row on their next turn, choose that move with reason Block (3) If a threatening move is available, creating two in a row with an empty third square in the row, choose that move with reason Threat (4) Otherwise, choose an empty square, preferring center over corners over middles, with reason Empty\nTwo versions of the dataset were created, one with only the preferred move (represented as a 3 \u00d7 3 plane), the second with the preferred move and explanation (represented as a 3 \u00d7 3 \u00d7 4 stack of planes). A simple neural network classifier was built on each of these datasets, with one hidden layer of 200 units using ReLU and a softmax over the 9 (or 36) outputs. We use a 90%/10% split of the legal non-terminal board positions for the training/testing datasets. This classifier obtained an accuracy of 96.5% on the baseline moveonly prediction task, i.e., when trained with just X (the 19 features) and Y it was highly accurate.\nTo answer the first question, does the approach provide useful explanations, we calculated the accuracy of the predicted explanation. Although there are only 4 rules, each rule applies to 9 different preferred moves, resulting in 36 possible explanations. Our classifier was able to generate the correct explanation 96.3% of the time, i.e., very rarely did it get the correct move and not the correct rule.\nThe second question asks how the accuracy of the classifier is impacted by the addition of E\u2019s in the training dataset. Given the increase in number of classes, one might expect the accuracy to decrease. However, for this example, the accuracy of predicting the preferred move actually increases to 97.4%. This illustrates that the approach works well in this domain; it is possible to provide accurate explanations without impacting the Y prediction accuracy. Table 1 summarizes the results for both examples."
    },
    {
      "heading": "5.2 Loan Repayment",
      "text": "The second example is closer to an industry use case and is based on the FICO Explainable Machine Learning Challenge dataset [13]. The dataset contains around 10,000 applications for Home Equity Line of Credit (HELOC), with the binaryY label indicating payment performance (any 90-day or longer delinquent payments) over 2 years.\n1These rules do not guarantee optimal play.\nSince the dataset does not come with explanations (E),2 we generated them by training a rule set on the training data, resulting in the following two 3-literal rules for the \u201cgood\u201d class Y = 1 (see [13] for a data dictionary):\n(1) NumSatisfactoryTrades \u2265 23 AND ExternalRiskEstimate \u2265 70 AND NetFractionRevolvingBurden \u2264 63; (2) NumSatisfactoryTrades \u2264 22 AND ExternalRiskEstimate \u2265 76 AND NetFractionRevolvingBurden \u2264 78.\nThese two rules, from researchers at IBM Research, predict Y with 72% accuracy and were the winning entry to the challenge [6]. Since the TED approach requires 100% consistency between explanations and labels, we modified the Y labels in instances where they disagree with the rules. We then assigned the explanation E to one of 8 values: 2 for the good class, corresponding to which of the two rules is satisfied (they are mutually exclusive), and 6 for delinquent, corresponding first to which of the rules should apply based on NumSatisfactoryTrades, and then to which of the remaining conditions (ExternalRiskEstimate, NetFractionRevolvingBurden, or both) are violated.\nWe trained a Random Forest classifier (100 trees, minimum 5 samples per leaf) on first the dataset with just X and (modified) Y and then on the enhanced dataset with E added. The accuracy of the baseline classifier (predicting binary label Y ) was 99.2%. The accuracy of TED in predicting explanations E was 99.4%, despite the larger class cardinality of 8. In this example, Y predictions can be derived from E predictions through the mapping mentioned above, and doing so resulted in an improved Y accuracy of 99.6%. While these accuracies may be artificially high due to the data generation method, they do show two things as in Section 5.1: (1) To the extent that user explanations follow simple logic, very high explanation accuracy can be achieved; (2) Accuracy in predicting Y not only does not suffer but actually improves. The second result has been observed by other researchers who have suggested adding \u201crationales\u201d to improve classifier performance, but not for explainability [8, 11, 25, 29, 32, 37\u201339]."
    },
    {
      "heading": "6 EXTENSIONS AND OPEN QUESTIONS",
      "text": "The TED framework assumes a training dataset with explanations and uses it to train a classifier that can predict Y and E. This work described a simple way to do this, by taking the Cartesian product of Y and E and using any suitable machine learning algorithm to train a classifier. Another instantiation would be to bring together the labels and explanations in a multitask setting. Yet another option is to learn feature embeddings using labels and explanation similarities in a joint and aligned way to permit neighbor-based explanation prediction.\nUnder the Cartesian product approach, adding explanations to a dataset increases the number of classes that the classification algorithm will need to handle. This could stress the algorithm\u2019s effectiveness or training time performance, although we did not observe this in our two examples. However, techniques from the \u201cextreme classification\u201d community [12] could be applicable.\n2The challenge asks participants to provide explanations along with predictions, which will be judged by the organizers.\nAlthough the flexibility of allowing any format for an explanation, provided the set of explanations can be enumerated, is quite general, it could encourage a large number of explanations that differ in only unintended ways, such as \u201cinsufficient salary\u201d vs. \u201csalary too low\u201d. Providing more structure via a domain-specific language (DSL) or good tooling could be useful. If free text is used, we could leverage word embeddings to provide some structure and to help reason about similar explanations.\nAs there are many ways to explain the same phenomenon, it may be useful to explore having more than one version of the same base explanation for different levels of consumer sophistication. Applications already do this formultilingual support, but in this case it would be multiple levels of sophistication in the same language for, say, a first time borrower vs. a loan officer or regulator. This would be a postprocessing step once the explanation is predicted by the classifier.\nProviding explanations for the full training set is ideal, but may not be realistic. Although it may be easy to add explanations while creating the training dataset, it may be more challenging to add explanations after a dataset has been created because the creator may not available or may not remember the justification for a label. One possibility is to use an external knowledge source to generate explanations, such as WebMD in a medical domain. Another possibility is to request explanations on a subset of the training data and apply ideas from few-shot learning [14] to learn the rest of the training dataset explanations. Another option is to use active learning to guide the user where to add explanations. One approach may be to first ask the user to enumerate the classes and explanations and then to provide training data (X ) for each class/explanation until the algorithm achieves appropriate confidence. At a minimum one could investigate how the performance of the explanatory system changes as more training explanations are provided. Combinations of the above may be fruitful."
    },
    {
      "heading": "7 CONCLUSIONS",
      "text": "This paper introduces a new paradigm for providing explanations for machine learning model decisions. Unlike existing methods, it does not attempt to probe the reasoning process of a model. Instead, it seeks to replicate the reasoning process of a human domain user. The two paradigms share the objective to produce a reasoned explanation, but the model introspection approach is more suited to AI system builders who work with models directly, whereas the teaching explanations paradigm more directly addresses domain users. Indeed, the European Union GDPR guidelines say: \u201cThe controller should find simple ways to tell the data subject about the rationale behind, or the criteria relied on in reaching the decision without necessarily always attempting a complex explanation of the algorithms used or disclosure of the full algorithm.\u201d\nWork in social and behavioral science [23, 26, 27] has found that people prefer explanations that are simpler, more general, and coherent, even over more likely ones. Miller writes that in the context of Explainable AI: \u201cGiving simpler explanations that increase the likelihood that the observer both understands and accepts the explanation may be more useful to establish trust [27].\u201d\nOur two examples illustrate promise for this approach. They both showed highly accurate explanations and no loss in prediction\naccuracy.We hope this workwill inspire other researchers to further enrich this paradigm."
    }
  ],
  "title": "TED: Teaching AI to Explain its Decisions",
  "year": 2019
}
