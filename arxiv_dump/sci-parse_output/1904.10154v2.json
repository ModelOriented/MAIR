{
  "abstractText": "Device-free Wi-Fi indoor localization has received significant attention as a key enabling technology for many Internet of Things (IoT) applications. Machine learning-based location estimators, such as the deep neural network (DNN), carry proven potential in achieving high-precision localization performance by automatically learning discriminative features from the noisy wireless signal measurements. However, the inner workings of DNNs are not transparent and not adequately understood especially in the indoor localization application. In this paper, we provide quantitative and visual explanations for the DNN learning process as well as the critical features that DNN has learned during the process. Toward this end, we propose to use several visualization techniques, including: 1) dimensionality reduction visualization, to project the high-dimensional feature space to the 2D space to facilitate visualization and interpretation, and 2) visual analytics and information visualization, to quantify relative contributions of each feature with the proposed feature manipulation procedures. The results provide insightful views and plausible explanations of the DNN in device-free Wi-Fi indoor localization using channel state information (CSI) fingerprints.",
  "authors": [
    {
      "affiliations": [],
      "name": "Shing-Jiuan Liu"
    },
    {
      "affiliations": [],
      "name": "Ronald Y. Chang"
    },
    {
      "affiliations": [],
      "name": "Feng-Tsun Chien"
    }
  ],
  "id": "SP:41af2b2ea9783f5a267d1512e3b12d778a7d9d17",
  "references": [
    {
      "authors": [
        "D. Macagnano",
        "G. Destino",
        "G. Abreu"
      ],
      "title": "Indoor positioning: A key enabling technology for IoT applications",
      "venue": "Proc. IEEE World Forum Internet Things, Mar. 2014, pp. 117\u2013118.",
      "year": 2014
    },
    {
      "authors": [
        "J. Wyffels",
        "J.D. Brabanter",
        "P. Crombez",
        "P. Verhoeve",
        "B. Nauwelaers",
        "L.D. Strycker"
      ],
      "title": "Distributed signal strength-based indoor localization algorithm for use in healthcare environments",
      "venue": "IEEE J. Biomed. Health Informat., vol. 18, no. 6, pp. 1887\u20131893, Jun. 2014.",
      "year": 1887
    },
    {
      "authors": [
        "K. Zheng",
        "H. Wang",
        "H. Li",
        "W. Xiang",
        "L. Lei",
        "J. Qiao",
        "X.S. Shen"
      ],
      "title": "Energy-efficient localization and tracking of mobile devices in wireless sensor networks",
      "venue": "IEEE Trans. Veh. Technol., vol. 66, no. 3, pp. 2714\u2013 2726, Mar. 2017.",
      "year": 2017
    },
    {
      "authors": [
        "S. Alletto",
        "R. Cucchiara",
        "G.D. Fiore",
        "L. Mainetti",
        "V. Mighali",
        "L. Patrono",
        "G. Serra"
      ],
      "title": "An indoor location-aware system for an IoT-based smart museum",
      "venue": "IEEE Internet Things J., vol. 3, no. 2, pp. 244\u2013253, Apr. 2016.",
      "year": 2016
    },
    {
      "authors": [
        "J. Xiao",
        "Z. Zhou",
        "Y. Yi",
        "L.M. Ni"
      ],
      "title": "A survey on wireless indoor localization from the device perspective",
      "venue": "ACM Comput. Surv., vol. 49, no. 2, pp. 25:1\u201325:31, Jun. 2016.",
      "year": 2016
    },
    {
      "authors": [
        "B. Mager",
        "P. Lundrigan",
        "N. Patwari"
      ],
      "title": "Fingerprint-based device-free localization performance in changing environments",
      "venue": "IEEE J. Sel. Areas Commun., vol. 33, no. 11, pp. 2429\u20132438, Nov. 2015.",
      "year": 2015
    },
    {
      "authors": [
        "Z. Wu",
        "Q. Xu",
        "J. Li",
        "C. Fu",
        "Q. Xuan",
        "Y. Xiang"
      ],
      "title": "Passive indoor localization based on CSI and Naive Bayes classification",
      "venue": "IEEE Trans. Syst., Man, Cybern., Syst., vol. 48, no. 9, pp. 1566\u20131577, Sep. 2018.",
      "year": 2018
    },
    {
      "authors": [
        "R. Zhou",
        "X. Lu",
        "P. Zhao",
        "J. Chen"
      ],
      "title": "Device-free presence detection and localization with SVM and CSI fingerprinting",
      "venue": "IEEE Sensors J., vol. 17, no. 23, pp. 7990\u20137999, Dec. 2017.",
      "year": 2017
    },
    {
      "authors": [
        "S. Saha",
        "K. Chaudhuri",
        "D. Sanghi",
        "P. Bhagwat"
      ],
      "title": "Location determination of a mobile device using IEEE 802.11b access point signals",
      "venue": "Proc. IEEE Wireless Commun. Netw. Conf., vol. 3, Mar. 2003, pp. 1987\u20131992.",
      "year": 2003
    },
    {
      "authors": [
        "W. Zhang",
        "K. Liu",
        "W. Zhang",
        "Y. Zhang",
        "J. Gu"
      ],
      "title": "Deep neural networks for wireless localization in indoor and outdoor environments",
      "venue": "Neurocomputing, vol. 194, pp. 279\u2013287, Jun. 2016.",
      "year": 2016
    },
    {
      "authors": [
        "X. Wang",
        "L. Gao",
        "S. Mao",
        "S. Pandey"
      ],
      "title": "CSI-based fingerprinting for indoor localization: A deep learning approach",
      "venue": "IEEE Trans. Veh. Technol., vol. 66, no. 1, pp. 763\u2013776, Jan. 2017.",
      "year": 2017
    },
    {
      "authors": [
        "R.Y. Chang",
        "S.-J. Liu",
        "Y.-K. Cheng"
      ],
      "title": "Device-free indoor localization using Wi-Fi channel state information for Internet of things",
      "venue": "Proc. IEEE GLOBECOM, Dec. 2018.",
      "year": 2018
    },
    {
      "authors": [
        "J. Wang",
        "X. Zhang",
        "Q. Gao",
        "H. Yue",
        "H. Wang"
      ],
      "title": "Device-free wireless localization and activity recognition: A deep learning approach",
      "venue": "IEEE Trans. Veh. Technol., vol. 66, no. 7, pp. 6258\u20136267, Jul. 2017.",
      "year": 2017
    },
    {
      "authors": [
        "Q. Gao",
        "J. Wang",
        "X. Ma",
        "X. Feng",
        "H. Wang"
      ],
      "title": "CSI-based devicefree wireless localization and activity recognition using radio image features",
      "venue": "IEEE Trans. Veh. Technol., vol. 66, no. 11, pp. 10 346\u201310 356, Nov. 2017.",
      "year": 2017
    },
    {
      "authors": [
        "M. Craven",
        "J. Shavlik"
      ],
      "title": "Visualizing learning and computation in artificial neural networks",
      "venue": "Int. J. Artif. Intell. Tools, vol. 1, no. 3, pp. 399\u2013425, 1992.",
      "year": 1992
    },
    {
      "authors": [
        "A. Mohamed",
        "G. Hinton",
        "G. Penn"
      ],
      "title": "Understanding how deep belief networks perform acoustic modelling",
      "venue": "Proc. IEEE Int. Conf. Acoust. Speech Signal Process., 2012, pp. 4273\u20134276.",
      "year": 2012
    },
    {
      "authors": [
        "P.E. Rauber",
        "S.G. Fadel",
        "A.X. Falco",
        "A.C. Telea"
      ],
      "title": "Visualizing the hidden activity of artificial neural networks",
      "venue": "IEEE Trans. Vis. Comput. Graphics, vol. 23, no. 1, pp. 101\u2013110, Jan. 2017.",
      "year": 2017
    },
    {
      "authors": [
        "S. Bach",
        "A. Binder",
        "G. Montavon",
        "F. Klauschen",
        "K.-R. M\u00fcller",
        "W. Samek"
      ],
      "title": "On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation",
      "venue": "PLOS ONE, vol. 10, no. 7, p. e0130140, Jan. 2015.",
      "year": 2015
    },
    {
      "authors": [
        "W. Samek",
        "A. Binder",
        "G. Montavon",
        "S. Lapuschkin",
        "K.R. M\u00fcller"
      ],
      "title": "Evaluating the visualization of what a deep neural network has learned",
      "venue": "IEEE Trans. Neural Netw. Learn. Syst, vol. 28, no. 11, pp. 2660\u20132673, Nov. 2017.",
      "year": 2017
    },
    {
      "authors": [
        "D. Halperin",
        "W. Hu",
        "A. Sheth",
        "D. Wetherall"
      ],
      "title": "Tool release: Gathering 802.11n traces with channel state information",
      "venue": "ACM SIGCOMM Comput. Commun. Review., vol. 41, no. 1, pp. 53\u201353, Jan. 2011.",
      "year": 2011
    },
    {
      "authors": [
        "Z. Wu",
        "K. Fu",
        "E. Jedari",
        "S.R. Shuvra",
        "R. Rashidzadeh",
        "M. Saif"
      ],
      "title": "A fast and resource efficient method for indoor positioning using received signal strength",
      "venue": "IEEE Trans. Veh. Technol., vol. 65, no. 12, pp. 9747\u2013 9758, Dec. 2016.",
      "year": 2016
    },
    {
      "authors": [
        "L. van der Maaten",
        "G. Hinton"
      ],
      "title": "Visualizing data using t-SNE",
      "venue": "J. Mach. Learn. Res., vol. 9, pp. 2579\u20132605, Nov. 2008.",
      "year": 2008
    },
    {
      "authors": [
        "P.J. Rousseeuw"
      ],
      "title": "Silhouettes: A graphical aid to the interpretation and validation of cluster analysis",
      "venue": "J. Comput. Appl. Math., vol. 20, no. 1, pp. 53\u201365, 1987.",
      "year": 1987
    }
  ],
  "sections": [
    {
      "text": "1 Analysis and Visualization of Deep Neural Networks in Device-Free Wi-Fi Indoor Localization\nShing-Jiuan Liu, Ronald Y. Chang, Member, IEEE, and Feng-Tsun Chien, Member, IEEE\nAbstract\u2014Device-free Wi-Fi indoor localization has received significant attention as a key enabling technology for many Internet of Things (IoT) applications. Machine learning-based location estimators, such as the deep neural network (DNN), carry proven potential in achieving high-precision localization performance by automatically learning discriminative features from the noisy wireless signal measurements. However, the inner workings of DNNs are not transparent and not adequately understood especially in the indoor localization application. In this paper, we provide quantitative and visual explanations for the DNN learning process as well as the critical features that DNN has learned during the process. Toward this end, we propose to use several visualization techniques, including: 1) dimensionality reduction visualization, to project the high-dimensional feature space to the 2D space to facilitate visualization and interpretation, and 2) visual analytics and information visualization, to quantify relative contributions of each feature with the proposed feature manipulation procedures. The results provide insightful views and plausible explanations of the DNN in device-free Wi-Fi indoor localization using channel state information (CSI) fingerprints.\nIndex Terms\u2014Wireless indoor localization, fingerprinting, channel state information (CSI), machine learning, deep neural networks (DNN), Internet of Things (IoT), visual analytics.\nI. INTRODUCTION\nIndoor localization is an essential component of many Internet of Things (IoT) applications [1]. Applications such as healthcare management [2], object tracking [3], and IoTbased smart environment [4] all require precise indoor location information. From the device perspective, wireless indoor localization can be classified as either device-based or devicefree, depending on whether or not a tracking device needs to be attached to the target to be localized [5]. Device-based methods generally have the advantages of higher accuracy and robustness against environmental interferences and dynamics as compared to device-free methods. However, device-free methods have the advantages of less hardware costs, less power demands, better privacy, and provision of real-time positioning and tracking, and find a wide array of IoT applications such as intrusion detection, elderly care, etc.\nThis work was supported in part by the Center for mmWave Smart Radar Systems and Technologies, under the Featured Areas Research Center Program within the framework of the Higher Education Sprout Project by the Ministry of Education (MOE), Taiwan, and in part by the Ministry of Science and Technology (MOST), Taiwan, under Grants MOST 106-2628-E-001-001MY3, MOST 107-3017-F-009-001, and MOST 107-2221-E-009-037-MY2.\nS.-J. Liu and R. Y. Chang are with the Research Center for Information Technology Innovation, Academia Sinica, Taipei, Taiwan (e-mail: {cindy1445858, rchang}@citi.sinica.edu.tw).\nF.-T. Chien is with the Institute of Electronics, National Chiao Tung University, Hsinchu, Taiwan (e-mail: ftchien@mail.nctu.edu.tw). From the mathematical techniques perspective, wireless indoor localization can be classified as using triangulation/trilateration or fingerprinting for position calculation. The triangulation/trilateration technique calculates the location of a target based on distances and/or angles of the target with respect to signal sources. The fingerprinting technique matches the online testing measurements with the database created from offline site survey to determine the location of a target. For fingerprinting indoor localization, machine learningbased approaches have been studied [6]\u2013[14]. In [6], machine learning classifiers such as k-nearest neighbor (k-NN), support vector machine (SVM), linear discriminant analysis (LDA), and random forests for localization in changing environments were studied and compared. Wu et al. [7] proposed an indoor localization system using the Naive Bayes classifier and compared it with k-NN and SVM. Zhou et al. [8] applied SVM to device-free localization based on channel state information (CSI) fingerprinting. In [9]\u2013[14], deep neural network (DNN)based approaches to indoor localization have been proposed. Saha et al. [9] proposed a localization scheme based on the received signal strength (RSS) of Wi-Fi using a neural networkbased classifier, which was shown to outperform the nearest neighbor classifier and the histogram matching method. Zhang et al. [10] developed an RSS-based Wi-Fi localization scheme that combines DNN and hidden Markov model (HMM). It was shown that DNN outperforms other machine learning approaches including k-NN, SVM, and locally linear embedding (LLE) each in combination with HMM, in terms of root mean square error (RMSE). Wang et al. [11] proposed a novel deep learning-based fingerprinting indoor localization system based on the CSI of Wi-Fi. A multilayer framework of a deep network was developed for learning discriminative features so as to effectively reduce the distance error in comparison with the probabilistic methods. Chang et al. [12] proposed a devicefree DNN-based Wi-Fi fingerprinting localization system. CSI pre-processing and data augmentation (including noise injection and interperson interpolation) were incorporated into the DNN framework for enhanced robustness and performance. Wang et al. [13] designed a machine learning framework for simultaneous location, activity, and gesture recognition by integrating a sparse autoencoder network with the softmaxregression-based classification module. The learning process can automatically learn the discriminative features from the RSS measurements and achieve an accuracy higher than 85%, which is better than the system without utilizing the learning process. Gao et al. [14] used a multilayer deep learningbased image processing framework to learn the optimized deep features from the radio images (transformed from the ar X iv :1 90 4. 10 15 4v 2 [ cs .N I] 2 2 M ay 2 01 9\n2 amplitude and phase information of the measured CSI) to estimate the location and activity of a target person. Note that [9]\u2013[11] are device-based and [6]\u2013[8], [12]\u2013[14] are devicefree indoor localization schemes.\nDNNs have achieved extraordinary results in many areas. While the DNN can perform automatic feature extraction without much human intervention, it is largely conceived as a \u201cblack box,\u201d and the design of a neural network is typically a trial-and-error process. The transparency issue of the DNN has received increasing attention lately, where both machine learning and visualization communities have attempted to understand the training process and interpret the machine cognition in a visually conceivable way. Craven and Shavlik [15] surveyed several visualization techniques, including Hinton diagrams, bond diagrams, hyperplane diagrams, responsefunction plots, and trajectory diagrams, which help provide visual evidence for understanding the learning and decisionmaking processes of neural networks. Mohamed et al. [16] introduced a dimension-reduction technique which shows that pre-training, fine-tuning, and nonlinear hidden layers are three major components that contribute to the superb acoustic recognition performance of deep belief networks (DBNs). Rauber et al. [17] analyzed the DNNs by visualizing the low-dimensional projections of hidden layer activations, and provided insights into how the learned representations of samples have evolved during training. Bach et al. [18] introduced the layer-wise relevance propagation (LRP) algorithm for identifying important pixels linked to a particular DNN prediction in a given input image. The significant features extracted by the training process were visualized in the pixel/input space. Samek et al. [19] showed that LRP, as compared to the sensitivity analysis and the deconvolution method, can more accurately identify important pixels based on which the DNN prediction was made.\nApplying the visual analytics techniques to understand the working mechanism of DNNs in wireless indoor localization has not been studied. The significance of this endeavor is twofold. First, providing visual evidence and interpretation of DNN workings in wireless indoor localization is by itself valuable, as the wireless signal patterns generally lack visual evidence and human intuition to interpret important features. Making DNN classifier interpretable also opens up new ways for model improvement and allows for analysis and comparison of various machine learning models for wireless indoor localization. Second, since the features in wireless localization applications are physical signatures of multipleinput multiple-output (MIMO) wireless links, an investigation and discussion from specifically wireless perspectives provides new insights on DNNs which cannot be directly inferred from other domain applications. Specifically, in this paper, we address the following issues: 1) How well has the DNN learned and been trained in device-free Wi-Fi indoor localization? 2) What critical features (wireless signatures) has the DNN learned to distinguish different classes in device-free WiFi indoor localization? Toward these ends, we adopt several visualization techniques to provide quantitative and plausible explanations of DNN workings in this specific application. Our main contributions are summarized as follows:\n\u2022 We model the device-free wireless indoor localization as a classification problem using the DNN, which yields a better average precision and recall rate than the k-NN and SVM techniques. \u2022 To the best of our knowledge, this paper presents the first study that provides visual analysis to understand the mechanism of DNN-based device-free wireless indoor localization. We employ two visual analytics techniques, namely, dimensionality reduction visualization and visual analytics and information visualization, to analyze the resultant clustering effect among the data after DNN and visualize the critical channels of a given input relative to a particular output decision. \u2022 We exploit the relevance score of a channel in an input CSI relative to a specific output class by regarding it as a correlation coefficient between two channels: one associated with the location class of the input CSI and the other associated with the specific output class that is learned by the DNN. Particularly, we develop the progressive channel nullification and channel modification procedures to evaluate the impacts of the channels with higher absolute values of relevance scores on the output prediction of each class. With extensive experiments using these two channel manipulation procedures, we justify that relevance scores are truly reliable quantitative indicators of critical features that distinguish classes, which allows for a better understanding of the critical features in wireless signals learned by the DNN which cannot be well interpreted by human perceptions.\nThe rest of the paper is organized as follows. Section II provides preliminaries on the system and experiments of DNN-based device-free Wi-Fi indoor localization. Section III presents the visual analytics techniques. Sections IV and V present the results and discussion. Finally, Section VI concludes the paper."
    },
    {
      "heading": "II. EXPERIMENTS AND DATASETS",
      "text": "Two real-world experiments for device-free Wi-Fi indoor localization were conducted, in a conference room and in a lounge, respectively, at the Research Center for Information Technology Innovation, Academia Sinica. The two environments exemplify different but common, realistic indoor settings (open space in the conference room and space with furniture/obstructions in the lounge) for our analysis."
    },
    {
      "heading": "A. Experimental Settings",
      "text": "The experimental settings in the conference room are depicted in Fig. 1. The conference room has dimensions 5.8\u00d78.6 m2. There are M = 16 target locations (p1, p2, . . . , p16) which are 1.2 m apart between any two nearest locations. A single Wi-Fi access point (AP) of the model Zyxel NBG-419N as the transmitter (Tx) and a single ASUS laptop equipped with Ubuntu 10.04 LTS and Intel Wi-Fi Wireless Link 5300 802.11n MIMO-OFDM radios [20] as the receiver (Rx) are both placed at fixed locations with heights 88 cm and 82 cm, respectively. No tracking device is attached to the target to be localized (i.e., device-free). The receiver pings the packets\n3 Table\nRx\nTx\n1.2m\n1.2m1.2m\n1.2m\n8 6\n0 c\nm 580 cm\nC o\nn c r e\nte w\na ll\nConcrete wall\nC o\nn c r e\nte w\na ll\nWooden wall\nDoor\ncabinet\nScreen\n! \" # $\n% & ' (\n\"\" ) \"* \"!\n\"% \"# \"& \"$\nTable\nTable\n(a)\nTx\nRx\nDoor (b)\nFig. 1. Experiment 1 (conference room). A single Wi-Fi AP (Tx) and a single fixed-location receiver (Rx) are deployed for device-free indoor localization. (a) Floor plan. (b) Photograph.\n6 6\n0 c\nm\n540 cm Concrete wall\nConcrete wall\nW o\no d\ne n\nw a\nll\nDoor\nT a b le\nC o\nn c r e\nt e\nw a\nll\nS o fa\nTx\n1 .2\nm\n1.2m\n!\n\"\n#$\n#!\n%\n&\n#%\n'\n(\n#'\n#\n)\n*\n##\nRx\n(a)\nTX\nRX\nDoor\n(b)\nFig. 2. Experiment 2 (lounge). A single Wi-Fi AP (Tx) and a single fixedlocation receiver (Rx) are deployed for device-free indoor localization. (a) Floor plan. (b) Photograph.\nsent from the transmitter and collects channel state information (CSI) during packet transmissions. The CSI data are used to train a DNN-based classification model for localization.\nThe experimental settings in the lounge are depicted in Fig. 2. The lounge has dimensions 5.4 \u00d7 6.6 m2. There are M = 14 target locations (p1, p2, . . . , p14) which are 1.2 m apart between two nearest locations. Unlike the conference room experiment, the target locations here are not uniformly arranged due to obstructions. The Wi-Fi transmitter and receiver are placed at fixed locations with heights 76 cm and 69 cm, respectively. All other settings are the same as in the conference room experiment."
    },
    {
      "heading": "B. Datasets",
      "text": "In the conference room experiment, CSI data for both training and testing phases were collected. In the offline training phase, the same person stood at each of the 16 locations and then the fixed-location receiver pings the packets from the transmitter. The same procedure was repeated eight times, in two days and at four different times each day (morning, noon,\nHidden layer\n0\n\u2026 \u2026\n\u2026 \u2026\n\u2026 \u2026\n\u2026 \u2026\n\u2026 \u2026\n! (\")\n! (#)\n! ($)\nInput layer Output layer\n%& '*\nFig. 3. The architecture of the DNN.\nafternoon, evening). Overall, 200 CSI samples were collected for each location (3200 samples for all 16 locations). Each CSI sample is a K-dimensional vector, where K = 120 is specified by the number of OFDM subcarriers (30) multiplied by two transmit and two receive antennas. The elements of the K-dimensional CSI sample, termed CSI channels or simply channels in this paper, are indexed in the order of 30 subcarriers for transmit antenna 1 and receive antenna 1 pairing (denoted by Tx1\u2013Rx1), 30 subcarriers for Tx1\u2013Rx2, 30 subcarriers for Tx2\u2013Rx1, and 30 subcarriers for Tx2\u2013Rx2. In the online testing phase, a similar task was performed, which is independent of the training phase. Each of the 16 locations was tested twice, at different times of the same day to incorporate environmental variations. In each test, 100 CSI samples were collected for each location (1600 for all 16 locations).\nIn the lounge experiment, in the offline training phase, the same person stood at each of the 14 locations with four different orientations at each location (facing forward, backward, right, and left with respect to the transmitter), and the fixed-location receiver pings the packets from the transmitter. The same procedure was repeated 12 times, in two days with six times of two hours apart each day. Overall, 384 CSI samples were collected for each location (5376 samples for all 14 locations). In the online testing phase, the same task was performed, independently of the training phase. Each of the 14 locations was tested twice, at different times of the same day. In each test, 92 CSI samples were collected for each location (1288 for all 14 locations)."
    },
    {
      "heading": "C. Deep Neural Networks",
      "text": "A fully connected feedforward network consisting of an input layer, L = 3 hidden layers (from first to third, 300, 280, and 260 neurons, respectively)1, and an output layer is adopted, as illustrated in Fig. 3. The input to the input layer is the K-dimensional raw CSI data, denoted by\n1It is known that by increasing the number of hidden neurons we can improve the approximation of the desired function, yet with a higher computational cost. The selection of the number of hidden layers and the number of neurons in each hidden layer here is based on both conventional wisdom and our experiments to strike a reasonable tradeoff between complexity and performance.\n4 x = [x1, x2, . . . , xK ] T , where xk denotes the input data of CSI channel k. The input to neuron i in the first hidden layer is z(1)i = \u2211 j w (1) i,j xj + b (1) i , where w (1) i,j and b (1) i are the weight and bias with self-explanatory indices. The output of neuron i in the first hidden layer is given by a (1) i = ReLU ( z (1) i ) , where ReLU(\u00b7) is the rectified linear unit (ReLU) activation function defined by ReLU(x) = max(0, x). The input of neuron i in the lth (l = 2, . . . , L) hidden layer is similarly given by z(l)i = \u2211 k=1 w (l) i,ka (l\u22121) k + b (l) i ,\nwith the corresponding output a(l)i = ReLU ( z (l) i ) . The softmax output of neuron i in the output layer is described by yi = exp ( z (L+1) i ) / \u2211M m=1 exp ( z (L+1) m ) . The prediction is location pm\u2032 , where m\u2032 = argmax m\nym , f(x), with f(\u00b7) denoting the decision function of the DNN. The network is trained in a supervised manner with the cross-entropy objective function, with 30 iterations of pre-training and 1500 iterations of backpropagation."
    },
    {
      "heading": "D. Performance",
      "text": "Table I and Table II summarize the location-specific statistical measures, i.e., precision and recall, for the conference room experiment and lounge experiment, respectively, for DNN-based device-free indoor localization system in comparison with k-NN (k = 5, the best-performing configuration2) and SVM. k-NN classification is based on the raw CSI data and is adopted to benchmark the analysis of DNN in Sections IV and V. SVM with a Gaussian radial basis function (RBF) kernel and the one-against-all technique [8], [21] for solving the multi-classification problem is adopted. The precision and recall measure the percentages of correct classification for each predicted and true class in the testing, respectively.\nIII. VISUAL ANALYTICS TECHNIQUES\nTwo visual analytics techniques are employed to facilitate the analysis, visualization, and evaluation of the neural network in the application of device-free Wi-Fi indoor localization."
    },
    {
      "heading": "A. Dimensionality Reduction Visualization",
      "text": "A dimensionality reduction technique called t-distributed stochastic neighbor embedding (t-SNE) [22] is employed to project the high-dimensional data to the low-dimensional space for ease of visualization. In the projection, t-SNE aims to preserve the pairwise relationship of the high-dimensional data in the low-dimensional space. Specifically, t-SNE first converts the pairwise distances between datapoints in the highdimensional space to joint probabilities, i.e., the joint probabilities of high-dimensional datapoints xi and xj are defined as pij = exp(\u2212\u2016xi \u2212 xj\u20162)/ \u2211 k 6=l exp(\u2212\u2016xk \u2212 xl\u2016 2 ) for i 6=\n2More precisely, k = 5 achieves the highest macro-average recall and second-highest macro-average precision in the conference room experiment, and the highest macro-average recall and precision in the lounge experiment, among commonly used k = 1, 3, 5, 7 values. Besides, k = 5 achieves the highest F1-score, which is the harmonic mean of the macro-average precision and recall, in both experiments.\nj, and pii = 0,\u2200i. Then, t-SNE finds the low-dimensional data representation such that the low-dimensional counterparts have similar joint probabilities as the high-dimensional data. The joint probabilities qij for the low-dimensional counterparts \u03c7i and \u03c7j are defined as qij = (1 +\n\u2225\u2225\u03c7i \u2212 \u03c7j\u2225\u22252)\u22121/\u2211k 6=l(1 + \u2016\u03c7k \u2212 \u03c7l\u2016 2 )\u22121 for i 6= j, and qii = 0,\u2200i, which is based on the heavy-tailed Student t-distribution instead of Gaussian as in the high-dimensional space to avoid the \u201ccrowding problem\u201d [22] in the low-dimensional representation. t-SNE finds the low-dimensional data representation with such qij\u2019s that the Kullback-Leibler divergence between pij\u2019s and qij\u2019s, i.e., \u2211 i \u2211 j pij log(pij/qij), is minimized.\nIn our application, the high-dimensional input or hiddenlayer activations are projected to the 2D space via t-SNE and presented as scatterplots. In the scatterplot, the points are colored or labeled according to the corresponding classes (i.e., locations p1, . . . , p16). We adopt the silhouette score [23] to visually assess the clustering of points into different classes in the 2D representation. The silhouette score s(i) of the ith point is defined as s(i) = (dnearest(i) \u2212 dintra(i))/max{dintra(i), dnearest(i)}, where dintra(i) is the mean distance between the ith point and all other points within the same cluster (i.e., the mean intra-cluster distance), and dnearest(i) is the minimum mean distance between the ith point and all points in any other cluster (i.e., the mean nearestcluster distance). The silhouette score lies in the interval [\u22121, 1]. The score is close to 1 if dnearest(i) dintra(i), i.e., the data are well-clustered, close to \u22121 if dnearest(i) dintra(i), i.e., the data are not well-clustered in the sense that the data would have been better clustered if clustered to the neighboring cluster, and near zero if dnearest(i) \u2248 dintra(i), i.e., the data are in between two clusters. We calculate the average silhouette score over all points for each scatterplot.\nB. Visual Analytics and Information Visualization\nLayer-wise relevance propagation (LRP) [18] is developed to quantify the contribution of the data in each input neuron to a specific output prediction. This is achieved by decomposing a specific pre-softmax value, i.e., z(L+1)m , in the output layer into relevance scores of the neurons in the previous hidden layer which are then propagated backward toward the input layer. The back-propagation rule in LRP is constrained by the conservation principle [18], i.e.,\u2211\ni hi(pn \u2192 pm) = \u2211 i R (l) i (pn \u2192 pm) = z (L+1) m ,\nm, n = 1, . . . ,M ; l = 1, . . . , L (1)\nwhere hi(pn \u2192 pm) and R(l)i (pn \u2192 pm) are the relevance scores of CSI channel i in the input layer and of neuron i in the lth hidden layer, respectively, corresponding to the input CSI data of location pn and the output DNN decision of location pm. The conservation principle forces the sum of the relevance scores to be preserved in all layers and equal to z(L+1)m , the output score before softmax. The conservation principle can be applied repeatedly backward in all layers to\n5\nobtain the relevance scores. The relevance score of neuron i in the Lth hidden layer is given by\nR (L) i (pn \u2192 pm) =\na (L) i w (L+1) m,i\u2211\nj a (L) j w (L+1) m,j + b (L+1) m\nz(L+1)m (2)\nfor some m,n, which approximates the conservation principle. The relevance scores of neuron i in the lth (l = 1, . . . , L\u2212 1) hidden layer and of CSI channel i in the input layer are given respectively by\nR (l\u22121) i (pn \u2192 pm) = \u2211 k\na (l\u22121) i w (l) k,i\u2211\nj a (l\u22121) j w (l) k,j + b (l) k\nR (l) k (pn \u2192 pm),\nl = 2, . . . , L, (3)\nhi(pn \u2192 pm) = \u2211 k xiw (1) k,i\u2211 j xjw (1) k,j + b (1) k R (1) k (pn \u2192 pm)\n(4)\nfor some m,n. The relevance score hi(pn \u2192 pm) is normalized to [\u22121, 1] by h\u2032i(pn \u2192 pm) = hi(pn\u2192pm) maxj=1,...,K(|hj(pn\u2192pm)|) . A positive valued h\u2032i(pn \u2192 pm) suggests that CSI channel i provides positive evidence in support of the DNN prediction of location pm given the input CSI data of location pn, a negative valued h\u2032i(pn \u2192 pm) suggests that CSI channel i provides negative evidence against such a prediction, and a near-zero h\u2032i(pn \u2192 pm) suggests that CSI channel i provides little evidence for or against such a prediction.\nTo examine how each input CSI channel has contributed to a specific output DNN decision through the relevance scores, we propose two channel manipulation processes, inspired by the approach in [18] where the process is associated with flipping the pixel value of a binary image. We first define, for some m,n, the following channel ordering sequences, where each (r1, . . . , rK) represents a permutation of the natural channel\nordering (1, . . . ,K) according to a certain ordering of the normalized relevance scores:\n1) O1(pn\u2192pm) = (r1, . . . , rK) such that the corresponding sequence of relevance scores is in descending order, i.e., h\u2032r1(pn \u2192 pm) \u2265 h \u2032 r2(pn \u2192 pm) \u2265 \u00b7 \u00b7 \u00b7 \u2265 h \u2032 rK (pn \u2192\npm). 2) O2(pn\u2192pm) = (r1, . . . , rK) such that the corresponding\nsequence of relevance scores is in ascending order, i.e., h\u2032r1(pn \u2192 pm) \u2264 h \u2032 r2(pn \u2192 pm) \u2264 \u00b7 \u00b7 \u00b7 \u2264 h \u2032 rK (pn \u2192\npm). 3) O3(pn\u2192pm) = (r1, . . . , rK) such that the corresponding\nsequence of relevance scores satisfies |h\u2032r1(pn \u2192 pm)| \u2265 |h\u2032r2(pn \u2192 pm)| \u2265 \u00b7 \u00b7 \u00b7 \u2265 |h \u2032 rK (pn \u2192 pm)|. 4) O4(pn\u2192pm) = (r1, . . . , rK) such that the corresponding sequence of relevance scores satisfies |h\u2032r1(pn \u2192 pm)| \u2264 |h\u2032r2(pn \u2192 pm)| \u2264 \u00b7 \u00b7 \u00b7 \u2264 |h \u2032 rK (pn \u2192 pm)|.\nThen, the proposed channel manipulation processes to facilitate information visualization are described below.\n1) Channel Nullification: The process is to progressively nullify or remove the channel in the input CSI data of location pn, denoted by xpn = [xpn,1, xpn,2, . . . , xpn,K ]\nT , one channel at a time according to a specific channel ordering sequence, and measure the impact of the nullified channel on the classification result. The tth (t = 1, . . . ,K) step in the removal process returns x(t)pn = gnull(x (t\u22121) pn , rt), where x (0) pn , xpn , and the function gnull sets the rt-th element of x(t\u22121)pn to zero. The influence of nullified CSI channels is reflected by the classification decision f(x(t)pn ) of the DNN.\n2) Channel Modification: The process is to progressively modify the channel in the input CSI data of location pn \u201ctoward\u201d that of location pm, one channel at a time according to a specific channel ordering sequence, given the input CSI data of location pn and the output DNN decision of location pm. The tth (t = 1, . . . ,K) step in the modification process returns x(t)pn = gmod(x (t\u22121) pn , rt), where x (0) pn , xpn ,\n6 and the function gmod sets the rt-th element of x (t\u22121) pn to\nmax ( 0,E [xpm,rt ] +h\u2032rt(pn\u2192pm) \u03c3xpm,rt \u03c3xpn,rt (xpn,rt\u2212E [xpn,rt ]) )\n, which is based on the linear minimum mean square error (LMMSE) estimation of unknown X given Y , i.e., X\u0302 = E[X] + \u03c1\u03c3X\u03c3Y (Y \u2212 E[Y ]), with correlation coefficient \u03c1 and standard deviations \u03c3X , \u03c3Y . The influence of modified CSI channels is reflected by the classification decision f(x(t)pn ) of the DNN.\nWe examine the percentage of correct classification for each true class pn in the testing (i.e., recall), and also the percentage of classification as location pm given the true class pn, after channel nullification/modification. More specifically, we calculate, for t = 1, . . . ,K, the number of f(x(t)pn ) = n and f(x (t) pn ) = m, respectively, divided by the number of testing samples for location pn. The proposed technique can be extended to examine the contribution of each subcarrier to the DNN decision. We can calculate\ns\u2032i(pn \u2192 pm)\n= 1\n4\n( h\u2032i(pn \u2192 pm) + h\u2032i+30(pn \u2192 pm)\n+ h\u2032i+60(pn \u2192 pm) + h\u2032i+90(pn \u2192 pm) ) ,\ni = 1, . . . , 30 (5)\nto represent the average normalized relevance scores of subcarrier i corresponding to the input CSI data of location pn and output DNN decision of location pm. Once we have the relevance scores of all the subcarriers corresponding to input pn and output pm, we can define subcarrier ordering sequences similar to O1\u2013O4 to rank the importance of subcarriers according to s\u2032i(pn \u2192 pm). By a slight abuse of notations, we use the same O1\u2013O4 to denote the subcarrier ordering sequences and clearly make the distinction from channel ordering sequences in the context. Subcarrier nullification/modification procedures are also similarly defined. For example, nullification of subcarrier i represents setting the ith, (i+ 30)th, (i+ 60)th, and (i+ 90)th elements of the CSI sample to zero."
    },
    {
      "heading": "IV. ANALYSIS AND VISUALIZATION OF EXPERIMENTAL RESULTS: EXPERIMENT 1 (CONFERENCE ROOM)",
      "text": "The results for experiment 1 (conference room) and experiment 2 (lounge) are analyzed and discussed in this section and next section, respectively."
    },
    {
      "heading": "A. Training Effects",
      "text": "The 2D visualization (via t-SNE) of 1) the raw CSI samples, 2) the last DNN hidden layer activations (i.e., a(L)i \u2019s) before training (with random initializations, i.e., all the weights and biases are randomly generated from a Gaussian distribution with mean zero and standard deviation one), and 3) the last DNN hidden layer activations after training, are presented in Fig. 4(a)\u2013(c), respectively. 200 training samples and 100\u00d72 = 200 testing samples per location are plotted and color-coded. For each location, the training samples are shown with a darker shade to distinguish from the testing samples with a lighter\nshade of the same color. It is observed from Fig. 4(a) that the raw CSI samples are highly scattered, with a silhouette score of 0.22, showing that the indoor localization problem at hand is not an easy classification problem. The untrained DNN shows similar visual separation among different classes (silhouette score: 0.09) in Fig. 4(b). The trained DNN yields much improved visual separation (silhouette score: 0.66), as shown in Fig. 4(c), by automatically extracting important CSI features to distinguish among different classes. It is expected that, as more distinctive patterns are learned through DNN training, different locations can be better classified with DNN in the testing. We use location p13 as an example. The rectangular boxes in Fig. 4(a) and Fig. 4(c) enclose all the training samples for location p13. It is seen that after DNN training these training samples become more concentrated. This leads to reduced misclassification of other locations as location p13 in the testing. Specifically, for the DNN scheme, only locations p1, p5, and p13 are classified as location p13 in the testing, while for the k-NN scheme based on the raw CSI samples, locations p1, p4, p5, p9, p12, p13, and p15 are classified as location p13 in the testing. Quantitatively, DNN yields a higher precision than k-NN for location p13 (51% vs. 40%), as shown in Table I. Overall, DNN generally yields higher precision values, with a higher macro-average precision as compared to k-NN (86.06% vs. 82.25%). Thus, DNN training is effective in learning discriminative features from the original noisy wireless signals."
    },
    {
      "heading": "B. Testing Performance",
      "text": "Fig. 5 depicts the same figure as Fig. 4, but with the testing samples (instead of the training samples) highlighted in a darker shade of the same color. It is observed that Fig. 5(c) presents better visual separation than Fig. 5(a), with silhouette scores (calculated for the testing samples only) of 0.41 vs. \u22120.01. Since testing was performed in two different time slots, Fig. 5(c) suggests that the trained DNN model modifies the testing samples collected in different time slots to be more coherent, even though the raw testing data are more scattered in the data space due to environmental variations in two different time slots. The more concentrated distribution of the testing samples per location leads to the reduced misclassification of other locations as the true location in the testing, as shown by the higher macro-average recall for DNN than for k-NN (79.37% vs. 70.88% in Table I).\nConsider location p11 as an example. In Fig. 5(a), the testing samples for location p11 are located near the training samples for locations p11 and p13, as enclosed by the rectangular box. After the DNN operation, the distribution of the testing data is altered, where the testing samples for location p11 are now located near the training samples for location p11 only, as enclosed by the rectangular box in Fig. 5(c). The visual distribution matches the classification results where the recall for k-NN and DNN for location p11 are 85% and 100%, respectively."
    },
    {
      "heading": "C. Line-of-Sight Effects",
      "text": "It is observed from Table I that, for DNN, p1 and p5 have very small values of recall but large values of precision. This\n7 \u2704 \u2701 \u2702 \u260e \u2706 \u271d \u271e \u271f \u2704\u2720 \u2704\u2704 \u2704 \u2704\u2701 \u2704\u2702 \u2704\u260e \u2704\u2706 \u2721\u261b\u261e \u2721\u270c\u261e \u2721\u270d\u261e \u270e\u270f\u2711\u2712\u2713\u2712\u2713\u2714 \u2715\u2716 \u2715\u2717 \u2715\u2716\u2718 \u2715\u2719 \u2715\u2716 \u2715\u2716\u2718 \u2715\u2717 \u2715\u2716 \u2715\u271a \u2715\u2717 \u2715\u271b \u2715\u2719 \u2715\u271c\nFig. 4. The 2D visualization of (a) the raw CSI samples, (b) the last DNN hidden layer activations before training (with random initializations), and (c) the last DNN hidden layer activations after training. For each location, the training samples are shown with a darker shade to distinguish from the testing samples with a lighter shade of the same color. The silhouette scores (calculated for the training samples only) for (a)\u2013(c) are 0.22, 0.09, and 0.66, respectively. The rectangular boxes in (a) and (c) enclose all the training samples for location p13. Black location markers refer to training samples and colored location markers (in colors corresponding to respective locations) refer to testing samples collected at the respective locations.\n\u2704 \u2701 \u2702 \u260e \u2706 \u271d \u271e \u271f \u2704\u2720 \u2704\u2704 \u2704 \u2704\u2701 \u2704\u2702 \u2704\u260e \u2704\u2706\n\u2721\u261b\u261e \u2721\u270c\u261e \u2721\u270d\u261e\n\u270e\u270f\u2711\u2712\u2713\u2712\u2713\u2714\n\u2715\u2716\u2716\n\u2715\u2716\u2716\n\u2715\u2716\u2717\n\u2715\u2716\u2716\n\u2715\u2716\u2716\nFig. 5. Same plots as Fig. 4, but here, for each location, the testing samples are shown with a darker shade to distinguish from the training samples with a lighter shade of the same color. The silhouette scores (calculated for the testing samples only) for (a)\u2013(c) are \u22120.01, 0.16, and 0.41, respectively. The rectangular boxes in (a) and (c) enclose the testing samples for location p11 and the nearby training samples. Black location markers refer to training samples and colored location markers (in colors corresponding to respective locations) refer to testing samples collected at the respective locations.\nsuggests that location p1 (or p5) is largely misclassified as another location (small recall), and very few other locations are misclassified as location p1 (or p5) (large precision). A confusion matrix analysis reveals that the testing samples for location p1 are predominantly misclassified as locations p3 and p13, and the testing samples for location p5 are predominantly misclassified as locations p3, p9, and p13. Fig. 4(c) and Fig. 5(c) show that the testing samples for locations p1 and p5 are near the training samples for locations p3, p9, and p13. Fig. 4(c) also shows that the training samples for locations p1, p5, p9, and p13 are close to each other, which suggests that locations p1, p5, p9, and p13 are not discernable after DNN learning. Fig. 4(a) shows that the raw CSI training samples for these four locations are also not well separated, which indicates that these original CSI patterns in the radio map are not discernable, making it difficult for the DNN to distinguish the location-specific features in the learning process. This may be in part because p1, p5, p9, and p13 are farther away from the line-of-sight (LoS) between the Wi-Fi transmitter and receiver. However, since the propagation of wireless signals is sensitive to room layouts and object placement, the CSI patterns and the resulting classification results are not the same for p4, p8, p12, and p16, which are also farther away from the LoS, in this experiment."
    },
    {
      "heading": "D. Discriminative Features",
      "text": "Fig. 6 shows the effect of progressive channel nullification on the DNN prediction using locations p6 and p13 as an example of locations nearer and farther from the LoS, respectively. For location p6, channel nullification according to channel ordering sequences O3(p6 \u2192 p6) and O4(p6 \u2192 p6) represent, respectively, that the most relevant and irrelevant channels, as determined by LRP, leading to the correct DNN decision of location p6 are progressively nullified. As can be seen, the percentage of correct classification generally decreases, more rapidly for O3 than for O4, as more channels are nullified. For O3, when as few as 20 channels (out of 120) are nullified, the percentage of correct classification drops from 100% (the recall for location p6) to nearly 0%, showing that the channels with large absolute values of relevance scores\n8 0 10 20 30 40 50 60 70 80 90 100 110 120\nNumber of channels nullified (t)\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90 100 P er ce nt ag e of c or re ct c la ss ifi ca tio n (% )\nO 3 (p 6 p 6 ) O 4 (p 6 p 6 ) O 3 (p 13 p 13 ) O 4 (p 13 p 13 )\nFig. 6. The percentage of correct classification (true class: p6 (or p13)) after progressive channel nullification according to the channel ordering sequences O3(p6 \u2192 p6) and O4(p6 \u2192 p6) (or O3(p13 \u2192 p13) and O4(p13 \u2192 p13)).\n0 10 20 30 40 50 60 70 80 90 100 110 120\nNumber of channels modified/nullified (t)\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nP er\nce nt\nag e\nof c\nla ss\nifi ca\ntio n\n(% )\nChannel mod. O 2 (p 6 p 10 ) True class p 6 , Predicted class p 6 True class p 6 , Predicted class p 10 Channel null. O\n2 (p 6 p 10 )\nTrue class p 6 , Predicted class p 6 True class p 6 , Predicted class p 10 Channel mod. O\n2 (p 11 p 10 )\nTrue class p 11 , Predicted class p 11 True class p 11 , Predicted class p 10\nFig. 7. The percentage of classification as p6 and p10 (true class: p6) after progressive channel modification according to the channel ordering sequence O2(p6 \u2192 p10), in comparison with the same setting but after progressive channel nullification, and with a different pair of locations p11 and p10 (true class: p11) after progressive channel modification according to the channel ordering sequence O2(p11 \u2192 p10).\nare indeed important and indispensable features for correct classification. Also, this shows that DNN can extract a small fraction of channels as dominant features. For O4, when the most irrelevant channels are nullified progressively, the percentage of correct classification still generally decreases, albeit more slowly. This is because, while these channels have little impact on the classification of the true class, they could have a pronounced impact on the classification of other classes. For example, when the channel corresponding to the leading element r1 of O4(p6 \u2192 p6) has a large CSI amplitude and a negative valued h\u2032r1(p6 \u2192 pm) for some m 6= 6, nullifying this specific channel may erase the original negative evidence against the wrong class pm,m 6= 6, resulting in an increased\nchance of a wrong DNN decision of location pm,m 6= 6. For location p13, the same general trend is observed, even though the percentage of correct classification drops even more rapidly from 89% (the recall for location p13) to 0%, since locations farther from the LoS could easily mutually misclassify as each other, as discussed previously.\nFig. 7 shows how the percentages of classification change as channels of the true class (p6) are progressively modified toward another class (p10) according to the channel ordering sequence O2(p6 \u2192 p10). Note that the leading elements of O2(p6 \u2192 p10) are channels that have the most negative relevance scores, i.e., provide the most negative evidence against the prediction of location p10 given the true class being location p6. By modifying these channels of location p6 toward those of location p10, we gradually erase the evidence against the prediction of location p10 and enhance the evidence for the prediction of location p10 (analogous to pixel flipping in binary images). Thus, we expect that the percentage of predicting location p6 (p10) will decrease (increase), provided that the relevance scores are reliable indicators of critical features that distinguish between classes. This is confirmed in Fig. 7. In comparison, nullifying, instead of modifying, the same channels in the same order does not yield the same effect. Since channel nullification erases the evidence against, but does not enhance the evidence for, the prediction of location p10, misclassification favors not only location p10 but other locations as well. Fig. 7 also shows that channel nullification yields a significantly faster decreasing rate of correct classification than channel modification. This suggests that making radical changes to the channels such as nullification poses a more harmful effect on the correct classification than modifying the channels to another, possibly similar, pattern.\nAs will be illustrated (in Figs. 9(a) and 9(h)), the CSI patterns of locations p6 and p10 are somewhat similar (by visual inspection) and these two locations are easily misclassified as each other. The raw training samples of locations p6 and p10 are close to each other as seen in Fig. 4(a). The recall for location p10 for k-NN based on the raw CSI data is 96% from Table I, and the 4% misses have all been classified as location p6. This may explain the fact that when channels of location p6 are modified toward those of location p10, all misclassified samples are classified as location p10 (i.e., the two topmost curves in Fig. 7 exhibit symmetric trends). For a different pair of locations, e.g., locations p11 and p10, as compared in Fig. 7, we observe that the decrease in classification as the true class (p11) does not lead to the increase in classification as another class (p10) in a symmetric way. Eventually, though, as the majority of channels are such modified, the percentage of classification as the true class decreases to 0% and the percentage of classification as another class increases to 100%, regardless of the pairs of locations.\nFigs. 8 and 9 examine the specific channels progressively modified which lead to the result shown by the topmost curve in Fig. 7. The heatmaps of relevance scores h\u2032i(p6 \u2192 p6) and h\u2032i(p6 \u2192 p10) are shown on top of the CSI of location p6 in Fig. 8. Progressive channel modification is visualized in Fig. 9. The channel modification starts with the channels\n9 0 20 40 60 80 100 120\nChannel index\n0\n5\n10\n15\n20\n25\n30\n35 40 C S I a m pl itu de\n-1\n-0.8\n-0.6\n-0.4\n-0.2\n0\n0.2\n0.4\n0.6\n0.8\n1\n(a) 0 20 40 60 80 100 120 Channel index\n0\n5\n10\n15\n20\n25\n30\n35\n40\nC S\nI a m\npl itu\nde\n-1\n-0.8\n-0.6\n-0.4\n-0.2\n0\n0.2\n0.4\n0.6\n0.8\n1\n(b)\nwith the most negative valued h\u2032i(p6 \u2192 p10), i.e., the bluecolored channels in Fig. 8(b). When up to 10 channels are modified, the percentage of correct classification remains 100%. When more channels are modified, the percentage starts to decline. Comparing Figs. 9(b) and 9(c), the additional channels modified, roughly corresponding to the red-circled ones in Fig. 9(c), are evidence in support of predicting p6\n0 5 10 15 20 25 30\nNumber of subcarriers nullified (t)\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nP er\nce nt\nag e\nof c\nor re\nct c\nla ss\nifi ca\ntio n\n(% )\nO 3 (p 6 p 6 ) O 4 (p 6 p 6 ) O 3 (p 13 p 13 ) O 4 (p 13 p 13 )\nFig. 10. The percentage of correct classification (true class: p6 (or p13)) after progressive subcarrier nullification according to the subcarrier ordering sequences O3(p6 \u2192 p6) and O4(p6 \u2192 p6) (or O3(p13 \u2192 p13) and O4(p13 \u2192 p13)).\n0 10 20 30 Subcarrier index\n0\n20 40 C S I a m pl itu de\nTx1 - Rx1\n0 10 20 30 Subcarrier index\n0\n20\n40\nC S\nI a m\npl itu\nde\nTx1 - Rx2\n0 10 20 30 Subcarrier index\n0\n20\n40\nC S\nI a m\npl itu\nde\nTx2 - Rx1\n0 10 20 30 Subcarrier index\n0\n20\n40\nC S\nI a m\npl itu\nde\nTx2 - Rx2\n-1 -0.8 -0.6 -0.4 -0.2 0 0.2 0.4 0.6 0.8 1\nFig. 11. Heatmaps of relevance scores s\u2032i(p6 \u2192 p6) for i = 1, . . . , 30, superimposed on the 200 testing samples of CSI for four transmit-receive antenna pairings of location p6.\n(yellow/red in Fig. 8(a)) but neutral or against predicting p10 (cyan/blue in Fig. 8(b)). In Fig. 9(c), for the 25% misclassified samples, the channels with h\u2032i(p6 \u2192 p6) > 0.4 (i.e., strong evidence in support of predicting p6) have all been modified. By modifying these critical channels in support of predicting p6, the percentage of correct classification as p6 decreases rapidly (from 100% to about 50% when only 22 channels are modified, from Fig. 9(b) to Fig. 9(d)). As channel modification progresses, channels with near-zero and positive valued h\u2032i(p6 \u2192 p10), i.e., the cyan/yellow/red-colored channels in Fig. 8(b), start to be modified. Comparing Figs. 9(e) and 9(f), the additional channels modified, roughly corresponding to the red-circled ones in Fig. 9(f), are evidence neutral or mildly supporting predicting p6 (cyan/yellow in Fig. 8(a)) and supporting predicting p10 (yellow/red in Fig. 8(b)). Modifying these channels not critically supporting predicting p6 and against predicting p10 results in a slower decrease in the percentage of correct classification (from about 50% to 0% when as many as 66 channels are modified, from Fig. 9(d) to Fig. 9(f)). When all 120 channels of location p6 are modified (Fig. 9(g)), the resulting CSI pattern approaches that of location p10 (Fig. 9(h))."
    },
    {
      "heading": "E. Subcarrier Importance",
      "text": "From a wireless perspective, the logical 120 channels comprise 30 physical subcarriers in 2 \u00d7 2 MIMO wireless links.\n10\nBy breaking down the 120 channels in view of wireless links and by defining the relevance scores of subcarriers and subcarrier ordering sequences, the subcarrier importance can be examined. Fig. 10 plots the results of progressive subcarrier nullification according to the subcarrier ordering sequences O3 and O4 on the DNN prediction for the same two locations as in Fig. 6. Similar conclusions can be drawn here. Fig. 11 shows the heatmaps of subcarrier relevance scores s\u2032i(p6 \u2192 p6) on top of the CSI of 2 \u00d7 2 MIMO links for location p6. Two observations can be made. First, the CSI amplitude for a specific subcarrier varies greatly in 2\u00d72 MIMO links, because the four links are independent of each other. Second, the subcarrier relevance scores are more temperate (no extreme values near 1 or \u22121) than the channel relevance scores in Fig. 8(a). This suggests that the logical channels, i.e., the ith, (i + 30)th, (i + 60)th, and (i + 90)th CSI channels, which all correspond to the same physical subcarrier i, do not have relevance scores that are simultaneously high or low (otherwise, it would have led to extreme values after the averaging operation in (5)). In other words, the same physical subcarriers at different MIMO links provide distinct information to the DNN prediction. The DNN treats the 120 logical channels as individual features irrespective of their wireless origins or implications."
    },
    {
      "heading": "V. ANALYSIS AND VISUALIZATION OF EXPERIMENTAL RESULTS: EXPERIMENT 2 (LOUNGE)",
      "text": "The dataset collected in the lounge experiment is more challenging for classification than the dataset collected in the conference room experiment. The lounge environment has more obstructions that produce more disturbance to the wireless signals and drastically different multipath profiles. Specifically, the CSI is more noisy/dispersive across different samples, and each sample exhibits more strongly varying CSI across different subcarriers as a result of richer multipath effects in this environment."
    },
    {
      "heading": "A. Training Effects",
      "text": "The 2D visualization of raw CSI samples shows poor visual separation between classes in Fig. 12(a), with a silhouette score of \u22120.18. Comparing Figs. 12(b) and 12(c), the trained DNN yields improved visual separation than the untrained DNN (silhouette scores \u22120.21 vs. 0.09). The improvement, albeit much smaller as compared to the counterpart in the conference room experiment, suggests that DNN still manages to learn distinctive patterns in this more noisy environment, which leads to better classification results in the testing. Consider locations p1 and p4 as an example, as marked in Figs. 12(a)\u2013(c). It is seen that before training the samples of the same location are more scattered (as enclosed by the rectangular box) while after training they become better clustered. This leads to reduced misclassification as locations p1 and p4 in the testing, as reflected by a higher precision in the DNN scheme as compared to the k-NN scheme for locations p1 and p4, as shown in Table II. Overall, DNN yields a higher macro-average precision as compared to k-NN (73.29% vs. 62.29%)."
    },
    {
      "heading": "B. Testing Performance",
      "text": "The poorer visual separation for the raw testing samples, as shown in Fig. 13(a), results in a much lower macro-average recall for k-NN in the lounge experiment as compared to in the conference room experiment (47.5% vs. 70.88%). After DNN training, it is observed that a better clustering is achieved in Fig. 13(c). For example, the raw testing samples for locations p1 and p4, as enclosed by the rectangular box in Fig. 13(a), are highly scattered, resulting in a high misclassification rate of the testing samples of location p1 (p4) to other locations. In comparison, after DNN training, as shown in Fig. 13(c), the testing samples are better clustered and closer to the training samples of the same class. As a result, DNN yields a higher recall than k-NN for locations p1 and p4 from Table II. The macro-average recall for DNN is higher than for k-NN (63.71% vs. 47.5% in Table II).\nThe improvement in silhouette scores between untrained and trained DNN at the last DNN hidden layer (Figs. 13(b) and 13(c)) is smaller in the lounge experiment than in the conference room experiment. This is because there are still testing samples that cannot be well-clustered by DNN (e.g., those located in the lower left corner in Fig. 13(c)) in this more noisy environment. However, DNN training still has significant contribution to the classification accuracy, where the difference in the macro-average recall between k-NN and DNN is greater in the lounge experiment than in the conference room experiment (16.21% vs. 8.49%)."
    },
    {
      "heading": "C. Discriminative Features",
      "text": "Similar to the conference room experiment, we show the results of progressive channel nullification for a location near the LoS (p12) with a high recall (96%) and a location away from the LoS (p1) with a low recall (43%), in Fig. 14. The general observations agree with those in the conference room experiment.\nFig. 15 shows the effect of progressive channel modification from p12 toward p13. It is observed that, again, the decrease in classification as the true class (p12) generally leads to the increase in classification as p13. However, even when all channels are such modified, the percentage of classification as p13 does not reach 100%. This is because the CSI samples are apparently more noisy/dispersive in this lounge experiment than in the conference room experiment (the waveforms of different CSI samples collectively appear \u201cthicker\u201d in Fig. 17 than in Fig. 9), and thus there is a higher discrepancy between the modified CSI patterns toward p13 and the actual CSI patterns of p13 (comparing Figs. 17(g) and 17(h)). The crosscomparison between channel modification and nullification concludes similarly as in the conference room experiment.\nFigs. 16 and 17 examine the specific channels progressively modified which lead to the results in Fig. 15. Channel modification is more sensitive in this lounge experiment, as the percentage of predicting p12 declines rapidly as channels are modified. By modifying only six channels for each sample, the percentage of classifying as p12 decreases to 73.9% while the percentage of classifying as p13 increases to 18.48%. These channels initially modified, roughly corresponding to\n11\nthe black-circled ones in Fig. 17(b), represent evidence in support of predicting p12 (yellow/red in Fig. 16(a)) but neutral or against predicting p13 (cyan/blue in Fig. 16(b)). Comparing Figs. 17(d) and 17(e), the additional 51 channels modified, roughly corresponding to the black-circled ones in Fig. 17(e), are evidence neutral for predicting both p12 and p13, leading to a slow decrease and increase in the percentages of predicting p12 and p13, respectively. Comparing Figs. 17(g) and 17(h), there is a more pronounced discrepancy between the modified CSI toward the mean of CSI of p13 and the actual CSI of p13, due to a larger variation in the testing CSI samples in the lounge experiment."
    },
    {
      "heading": "D. Subcarrier Importance",
      "text": "Figs. 18 and 19 present results related to subcarrier importance. There is a richer multipath effect (larger delay spread) in the lounge environment than in the conference room environment, as shown by the generally smaller correlation coefficients between two adjacent subcarriers in the lounge experiment (comparing the same pairs of adjacent subcarriers,\nthe lounge environment yields smaller correlation coefficients in 26 out of the total 29 pairs of adjacent subcarriers). In the lounge experiment, the logical channels that correspond to the same physical subcarriers have even more greatly varying relevance scores. Thus, progressively nullifying a specific subcarrier in four wireless links simultaneously translates to channel nullification in neither the channel ordering sequence of O3 nor O4. Thus, Fig. 18 exhibits a slightly different trend as compared to Fig. 14. The richer multipath effect, which, in theory, provides more degrees of freedom which could aid feature selection for classification, may have led to a greater performance improvement of DNN over k-NN in the macroaverage recall and precision in the lounge experiment (see Tables I and II). Yet, in the meantime, the same environment induces more noisy CSI across different samples, which deteriorates the performance of DNN (as well as k-NN and SVM) in absolute terms in the lounge experiment.\n12"
    },
    {
      "heading": "VI. CONCLUSION",
      "text": "In this paper, we have performed a detailed analysis of the DNN in device-free Wi-Fi fingerprinting indoor localization through visual analytics techniques. By projecting the last DNN hidden layer activations to visualizable 2D representations via the t-SNE dimensionality reduction technique, we observed better clustering and visual separation among different classes after DNN training. The more concentrated distribution of the testing samples matches the reduced misclassification in the testing for DNN. DNN generally yields higher precision results as compared to k-NN based on the raw data, suggesting that DNN training is effective in learning discriminative features from the noisy wireless signals. The LRP visual analytics technique, coupled with the channel nullification/modification\n\u272d \u272e \u272f \u2730 \u2731 \u2732 \u2733\u2734 \u2735\u2736 \u2737 \u2738\nprocedures, revealed critical channels or features in wireless signals learned by the DNN which cannot be well interpreted by human perceptions. The relevance scores, determined by LRP, provided analyzable, interpretable, and reliable measures of how and why the percentages of classification change as specific channels or features are manipulated, and thereby allow for a better understanding of the mechanism of DNN-\n13\nbased device-free wireless indoor localization."
    }
  ],
  "title": "Analysis and Visualization of Deep Neural Networks in Device-Free Wi-Fi Indoor Localization",
  "year": 2019
}
