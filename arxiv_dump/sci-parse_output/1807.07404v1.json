{"abstractText": "Predictive geometric models deliver excellent results for many Machine Learning use cases. Despite their undoubted performance, neural predictive algorithms can show unexpected degrees of instability and variance, particularly when applied to large datasets. We present an approach to measure changes in geometric models with respect to both output consistency and topological stability. Considering the example of a recommender system using word2vec, we analyze the influence of single data points, approximation methods and parameter settings. Our findings can help to stabilize models where needed and to detect differences in informational value of data points on a large scale.", "authors": [{"affiliations": [], "name": "Michaela Regneri"}, {"affiliations": [], "name": "Malte Hoffmann"}, {"affiliations": [], "name": "Jurij Kost"}, {"affiliations": [], "name": "Niklas Pietsch"}, {"affiliations": [], "name": "Timo Schulz"}, {"affiliations": [], "name": "Sabine Stamm"}], "id": "SP:7549917c4382d2844dca11cf80f52451bd5998ba", "references": [{"authors": ["Saleh Alomari", "Sumari Putra", "Sadik Al-Taweel", "Anas J.A. Husain. Digital recognition using neural network"], "title": "5", "venue": "06", "year": 2009}, {"authors": ["Maria Antoniak", "David Mimno. Evaluating the stability of embedding-based word similarities"], "title": "Transactions of the Association for Computational Linguistics", "venue": "6:107\u2013119,", "year": 2018}, {"authors": ["James Bennett", "Stan Lanning", "Netflix Netflix"], "title": "The netflix prize", "venue": "In KDD Cup and Workshop in conjunction with KDD,", "year": 2007}, {"authors": ["Hugo Caselles-Dupr\u00e9", "Florian Lesaint", "Jimena Royo-Letelier"], "title": "Word2vec applied to recommendation: Hyperparameters matter", "venue": "CoRR, abs/1804.04212,", "year": 2018}, {"authors": ["Arjun Das", "Debasis Ganguly", "Utpal Garain. Named entity recognition with word embeddings", "wikipedia categories for a low-resource language. ACM Trans"], "title": "Asian & Low-Resource Lang", "venue": "Inf. Process., 16:18:1\u201318:19,", "year": 2017}, {"authors": ["Ester et al", "1996] Martin Ester", "Hans-Peter Kriegel", "J\u00f6rg Sander", "Xiaowei Xu"], "title": "A density-based algorithm for discovering clusters a density-based algorithm for discovering clusters in large spatial databases with noise", "venue": "In Proceedings of the Second International Conference", "year": 1996}, {"authors": ["Esteva et al", "2017] Andre Esteva", "Brett Kuprel", "Roberto A. Novoa", "Justin Ko", "Susan M. Swetter", "Helen M. Blau", "Sebastian Thrun"], "title": "Dermatologist-level classification of skin cancer", "year": 2017}, {"authors": ["William H. Guss", "Ruslan Salakhutdinov. On characterizing the capacity of neural networks using algebraic topology"], "title": "CoRR", "venue": "abs/1802.04443,", "year": 2018}, {"authors": ["Haixia Liu"], "title": "Sentiment analysis of citations using word2vec", "venue": "CoRR, abs/1704.00177,", "year": 2017}, {"authors": ["Germ\u00e1n Kruszewski Marco Baroni", "Georgiana Dinu"], "title": "Don\u2019t count", "venue": "predict! a systematic comparison of context-counting vs. context-predicting semantic vectors. Proceedings of ACL 2014, 1:238\u2013247,", "year": 2014}, {"authors": ["Diana McCarthy", "Roberto Navigli"], "title": "Semeval-2007 task 10: English lexical substitution task", "venue": "Proceedings of the 4th International Workshop on Semantic Evaluations, SemEval \u201907, pages 48\u2013 53, Stroudsburg, PA, USA,", "year": 2007}, {"authors": ["Melamud et al", "2015] Oren Melamud", "Omer Levy", "Ido Dagan"], "title": "A simple word embedding model for lexical substitution", "venue": "In Proceedings of the 1st Workshop on Vector Space Modeling for Natural Language Processing, VS@NAACL-HLT 2015, June", "year": 2015}, {"authors": ["Tomas Mikolov", "Kai Chen", "G.s Corrado", "Jeffrey Dean. Efficient estimation of word representations in vector space"], "title": "2013", "venue": "01", "year": 2013}, {"authors": ["Mikolov et al", "2013b] Tomas Mikolov", "Ilya Sutskever", "Kai Chen", "Greg S Corrado", "Jeff Dean"], "title": "Distributed representations of words and phrases and their compositionality", "year": 2013}, {"authors": ["Morin", "Bengio", "2005] Frederic Morin", "Yoshua Bengio"], "title": "Hierarchical probabilistic neural network language model", "venue": "Proceedings of the Tenth International Workshop on Artificial Intelligence and Statistics,", "year": 2005}, {"authors": ["An Thanh Nguyen", "Dan Xu", "Byron C. Wallace", "Maarten de Rijke", "Matthew Lease"], "title": "Neural information retrieval: at the end of the early years", "venue": "Information Retrieval Journal,", "year": 2017}, {"authors": ["Perozzi et al", "2014] Bryan Perozzi", "Rami Al-Rfou", "Steven Skiena"], "title": "Deepwalk: Online learning of social representations", "venue": "In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD", "year": 2014}, {"authors": ["Paul Resnick", "Hal R. Varian. Recommender systems. Commun. ACM"], "title": "40(3):56\u201358", "venue": "March", "year": 1997}, {"authors": ["Demis Hassabis"], "title": "Mastering the game of go without human knowledge", "venue": "Nature, 550(7676):354\u2013359,", "year": 2017}, {"authors": ["Jizhe Wang", "Pipei Huang", "Huan Zhao", "Zhibo Zhang", "Binqiang Zhao", "Dik Lun Lee. Billionscale commodity embedding for e-commerce recommendation in alibaba"], "title": "CoRR", "venue": "abs/1803.02349,", "year": 2018}, {"authors": ["L. Wendlandt", "J. Kummerfeld", "R. Mihalcea"], "title": "Factors influencing the surprising instability of word embeddings", "venue": "NAACL-HLT", "year": 2018}], "sections": [{"heading": "1 Introduction", "text": "Nowadays, Artificial Intelligences (AIs) beat humans in recognizing hand-written numbers [Alomari et al., 2009], playing complex games like Go! [Silver et al., 2017] and partially even in cancer diagnosis [Esteva et al., 2017]. Many algorithms become more performant and more necessary than ever with the steady growth of available data. On the one hand, automated decisions become more accurate with more information to learn from, on the other hand, humans cannot keep pace with the velocity at which new data arises.\nWith the advance of neural networks and Big Data sources, results and mechanisms of Machine Learning components seem to have become less explainable. AI enables us to access useful data in the first place. However, the complexity of AI algorithms increases the gap between users as information consumers and the full background knowledge about the digested input\u2019s formation. AI consumers thus cannot always verify reliability and objectivity of automatically compiled content.\nInstability in Machine Learning algorithms often exacerbates the problem of classifying AI output as trustworthy or questionable: The output of a learning algorithm might sometimes alter unexpectedly, even if the environment changes only marginally or not at all. Such instabilities can often be misinterpreted as unreliability of an algorithm, in particular for systems which require a high degree of guaranteed behaviour and reproducibility (e.g. a diagnostic assistant).\nWe take a step towards explaining output changes in geometric models in general, including neural models in particular. In an innovative experimental setup for sensitivity analysis, we apply topological and application-oriented metrics to detect differences between two recommender models. The experimental design disentangles two common sources of instability: We separate sensitivity that originates from minimal changes in the input data from variation originating from the algorithm itself, in particular from strategies that keep the approach compuationally tractable for large-scale applications.\nOur main contribution is as follows: We present a combination of scalable measures for sensitivity, drawn from both, output consistency and topological model change. We apply those metrics to analyze word2vec [Mikolov et al., 2013a], a widely used example for geometric models. As a result, we classify factors of instability originating from the algorithm itself and from properties of data points."}, {"heading": "2 Related Work", "text": "While the algorithm in our focus was designed for Natural Language Processing, there are many non-linguistic applications of related vector models. For instance, skip-gram algorithms exist for different types of data, like social media graphs [Perozzi et al., 2014], ordered products in e-commerce [Wang et al., 2018], or click-through data as in our case.\nPlenty of previous work is based on word2vec [Mikolov et al., 2013a], and its results are analyzed thoroughly, but only few studies explain instabilities and the impact of randomness on the learned vector model. Wendlandt et al. [2018] show that even the embeddings of frequent words are unstable. Semantic properties of natural language (in particular, semantic relations) account for some of the sensitivities they find, which are thus hard to generalize to use cases that operate on other data sources rather than text.\nAntoniak and Mimno [2018] show that inherent relations in the input data (like aforementioned semantic relations) can only partially explain the learned vector model. Some of the instability factors they find (like document length) might be use case generic. Other work [Caselles-Dupre\u0301 et al., 2018] highlights that instability can be also caused by default hyperparameters that are not tuned for a specific task.\nOne evaluation methodology we use resembles the lexical substitution task [McCarthy and Navigli, 2007], which is a popular benchmark for automatically generated semantic\nar X\niv :1\n80 7.\n07 40\n4v 1\n[ cs\n.L G\n] 1\n7 Ju\nl 2 01\n8\nsimilarity measures. Similar embedding methods to the one we employ have also been used for finding lexical substitutions [Melamud et al., 2015, among others]. In context of recommender systems, the Netflix challenge [Bennett et al., 2007] is a similar setting for predicting user-specific ratings.\nGuss and Salakhutdinov [2018] derive a topological metric for measuring data complexity. One part of this metric is the number of the connected components, which we also employ as a metric. However, our goal differs from Guss and Salakhutdinov since they derive the appropriate neural architecture from the input data\u2019s topology, whereas we analyze the resulting embeddings."}, {"heading": "3 Data and Algorithm", "text": "In this section, we briefly sketch recommender systems, introduce word2vec and describe our source data."}, {"heading": "3.1 Recommender Systems", "text": "Recommender systems [Resnick and Varian, 1997] use automated means that help users make decisions with insufficient information. In general, such systems either use collaborative filtering, content-based recommendations or hybrid approaches. Collaborative filtering means that the (implicit or explicit) recommendations of all users are aggregated to find the best recommendations for the current user (such algorithms are mostly employed in web-shops for unknown users). Content-based recommendations refer to products previously bought or viewed by the current user and generate recommendations based on similarities with those products. Such similarities can be visual, textual or computed from other metadata (such as prices or brands).\nLike many modern recommenders, the system we consider is a hybrid approach. Given a seed product (the product currently viewed by the user), we compute a sorted list of recommendations that are possible alternatives for the seed product (\u201cYou might also like...\u201d). Such item-to-item recommendations are widely used in e-commerce. They increase conversion rates by helping customers find a product that meets their current need, and by keeping them engaged on the site. Technically, the alternatives in our model share similar click embeddings with the seed product, i.e. other customers looked at the alternative in similar user journey contexts in which they visited the current product in focus. For computing such embeddings, we use the word2vec algorithm."}, {"heading": "3.2 Word2vec", "text": "Word2vec [Mikolov et al., 2013a] is an algorithm that computes word embeddings. Words are represented by vectors, and the computed embeddings represent the words in their predicted contexts. In contrast to count-based co-occurrence models, word2vec and other predictive models use neural networks to compute a vector space, deriving abstract concepts as dimensions for each word from a given corpus. Countbased models define the vector space by directly using context words as dimensions, resulting in many more dimensions that are potentially less effective. Neural models perform superior in numerous tasks [Marco Baroni, 2014].\nWord2vec applications comprise mostly tasks around Natural Language Processing, e.g. Sentiment Analysis [Liu,\n2017], Information Retrieval [Onal et al., 2017] and Named Entity Recognition [Das et al., 2017], among many others. Neural predictions deliver good quality, which, however, comes at the cost of explainability. The features of word2vec are mostly opaque, and the resulting models change to a surprising extent even with only minor input variations [Wendlandt et al., 2018]. The combination of high performance, frequent use, instability and unpredictability forges high interest in conditions that influence the algorithm, especially in corporate contexts."}, {"heading": "3.3 Source Data", "text": "Our training data consists of anonymous user interactions with product pages. We define the chronologically sorted series of clicks by one user with breaks shorter than a certain time a user session. While a user session contains events of various types, we only consider clicks on product pages. Adopting linguistic terminology, we call one click in a session a token, and one abstract product, that can have multiple click instances, a (product) type. We use the term product interchangeably with type. The vector model we compute contains one vector per product type.\nIn our data model, each product belongs to a product group (e.g. the product group coats consists of several product instances, which can be parkas, jackets, or other coats). Those semantic groupings constitute a hierarchical system (all coats also belong to the more general group clothing).\nIn an operational recommender system, we train the recommender model on six months (180 days) of user sessions. Our standard training set of six months of user sessions contains about 98.4 million sessions with 1.3 million distinct products, of which we consider the 1.1 million products with a frequency of at least 5. There are 850 product groups, and each group contains 1,587.85 products on average. We update the system on a daily basis, which entails that the source data shifts by one day. While guaranteeing up-to-date recommendations and computational tractability, this method produces a permanent instability factor. Although we retain 99% of the data on each re-training, the results can differ considerably."}, {"heading": "4 How to measure instability", "text": "We consider instability of an algorithm as the sum of unexpected changes, assessable in prediction changes of the resulting model. To measure instability, we thus need to be able to compare two models by quantifying and qualifying their differences. In the following, we describe the metrics to accomplish this kind of analyses, which are applied in the experimental setup outlined afterwards."}, {"heading": "4.1 Metrics", "text": "We compare two click embeddings (trained on either two identical or modified data samples) using two types of metrics. One type is application-driven and complies directly with changes in recommendations. For this measurement, we measure the neighborhood similarity of the product vectors. The other type of metric assesses the geometric differences in the vector model itself and analyzes the topological stability of the manifolds constituted of the product vectors.\nNeighborhood similarity: To compute neighborhood similarity of two vector models, we sample some vectors and compare their nearest neighbors according to cosine similarity in both models. The top k most similar vectors for a seed product represent the top k best recommendations for the seed product in our use case. For a given seed product p we define the top k overlap of two models as the fraction of the intersecting top k recommendations predicted by both models. In our experiment, we average over several pairwise model overlap scores, considering the intersection of a reference model and one test model at a time. For example, a top 15 overlap of 80% for a given seed product and a reference model means that on average, 12 of 15 recommendations predicted by the reference model are also predicted by the test models. Inversely, the same result quantifies instability as a difference of 20%. In our final evaluation, we compute the overlap for the 15 nearest neighbors, averaged over 5,000 seed products randomly sampled among the 10,000 most frequent products. While this excludes rare products, this sample reflects the instabilities we will mostly see in practice.\nTopological stability: We complement the applicationoriented neighborhood similarity with a model-driven measure which directly describes the geometric configuration of the vector model. As a tractable way to investigate properties and changes in vector constellation, we use a topological cluster analysis. Clusters arise from vectors according to the cosine similarity computed with the vector model. To determine the number of connected components, we use the DBScan clustering algorithm [Ester et al., 1996]. The parameters for DBScan (neighborhood distance and the minimum number of neighbors) were heuristically determined and fixed for all runs. We require a core point to have at least ten neighbors with a cosine similarity > 0.8. For our purpose, we do not require an optimal clustering, we just need a setting that reflects model changes deterministically.\nTo measure topological stability, we compute the number of connected components that is equivalent to the number of clusters. Further, we analyze the size and purity of clusters as well as the total amount of noise.\nAdditionally, we measure local density in the vector space, counting the number of vectors within a certain distance from the clusters\u2019 centroids. We set this radius threshold to the value of \u03b5-Parameter used for DBScan (0.8). To keep this analysis computationally tractable, we restrict the analysis to the 200 closest neighbors."}, {"heading": "4.2 Experimental Setup", "text": "We want to identify and separate random factors from minimal changes in input data. In order to keep the algorithm computationally tractable, we compile a comparably small and representative data sample consisting of 1,309,907 user sessions (2 consecutive days). The sample contains 503,936 distinct product types. Ignoring products that occur less than 5 times (\u2013min-count 5), the resulting word2vec output contains 253,889 product types. On average, one session consists of 6.93 product clicks, and the product tokens in one session belong to 2.26 distinct product groups.\nThe model computed from the whole sample dataset is our reference model. We analyze the impact of a single data point\n(cf. Sec. 5.3) with a leaving-one-out experiment. Based on the data sample described above, we create 500 subsamples by randomly leaving out one single session. For each subsample, we compute a vector model which we refer to as model\u22121. The 500 sessions under consideration are representative with respect to session length (7.29 clicks).\nWe compute the product embeddings using Google\u2019s word2vec WORD VECTOR estimation toolkit, version 0.1c. For our sensitivity analysis towards minimal data changes, we need to eliminate algorithm-induced instabilities, which requires some parameter settings deviating from commonly used defaults. We choose skip-gram (\u2013cbow 0), no subsampling of frequent words (\u2013sample 0), ten iterations (\u2013 iter 10) and only one thread due to thread-induced instabilities explained in Section 5.1. We stick to 100 dimensions (\u2013size 100) and calculate the word vectors for all 501 samples in two runs: One using Hierarchical Softmax (later on referred to as HS with \u2013hs 1 \u2013negative 0) and one with negative sampling with 5 samples (later on referred to as NEG with \u2013hs 0 \u2013negative 5). Additionally, we fix the context window size to exactly 5 and round all vector numbers to 4 rather than 8 digits (the latter shows no impact in our results)."}, {"heading": "5 Factors of Instability", "text": "We analyze two types of instability. We consider the instability due to random factors and describe the influence of the approximation method. To disentangle artifacts from model training from actual sensitivity to input changes, we design an experiment that measures the prediction models\u2019 instability due to minimal data changes."}, {"heading": "5.1 Random Factors out of the box", "text": "Using word2vec, we have learned that some seemingly random instabilities occur even for very simple, often small, datasets where, for instance, a single token that belongs to two sessions might introduce an opposing force that destabilizes the learned embedding. While our specific results are computed using Google\u2019s word2vec WORD VECTOR estimation toolkit v 0.1c, many findings probably carry over to other embedding algorithms, too.\nInitially, we suspect random weight initialization in word2vec\u2019s code to account for most of the resulting variation. However, the word2vec implementation employs a number generator for each thread with a fixed seed and thereby guarantees reproducibility.\nAnalyzing other potential sources of randomness, we find one hyperparameter that induces a considerable amount of instability in the resulting word vectors: The number of threads.\nUsing multiple threads for training is vital for large datasets, allowing to distribute all data equally over all threads. Since all threads update the same weight matrix without locking, the resulting weight updates are nondeterministic. To verify the impact, we train five models on the same input file with 30 threads1. The models\u2019 agreement using similarity of the five nearest neighbors (cf. Sec. 4.1) results in an average overlap of 75% .\n1Additional parameters: -window 5 -size 100 -sample 0 -mincount 5 -cbow 0 -hs 1 -negative 0 -iter 10\nAnother factor is subsampling that randomly omits single tokens from sessions with respect to the frequency of tokens, controlled by the hyperparameter \u2013sample. Furthermore, the definition of the context window size (that is at max \u2013window) depends on the same random numbers as the subsampling.\nWhile the subsampling can be deactivated in v 0.1c, there is no hyperparameter to keep the window size constant. When keeping the input data identical, these two factors will have no \u201crandom\u201d effect on the output vectors."}, {"heading": "5.2 Approximation Methods for Model Training", "text": "We use an algorithm that computes word embeddings from sequential data for all types in a vocabulary. In their most basic count-based form, such embeddings treat each type as a dimension. With a growing vocabulary, training time increases considerably, because there are more vectors to be updated with each processed training token.\nIn order to keep embedding computation tractable and maintain the resulting predictions\u2019 quality, many algorithms use approximation methods, which vastly influence the learned vector model as well as its sensitivity towards changes in the training data. The word2vec implementation we use comes with the built-in options to either apply negative sampling (NEG), or Hierarchical Softmax (HS).\nNegative Sampling: NEG saves runtime by reducing the number of vectors updated for each training token. The number is a new hyperparameter z. Instead of touching each vector in the vector space, only z vectors (and additionally the vector for the current training token) will change at a time. The vectors are chosen using some probability distribution D. D can, for instance, be set to be the unigram distribution, which ensures that vectors of frequent concepts are updated more often. Mikolov et al. show that this approach does not directly maximize the likelihood of correct tokens, but nevertheless it has been shown that this leads to useful embeddings and is thus a popular choice. In the implementation of word2vec that we use, z is by default set to 5.\nHierarchical Softmax: HS is inspired from Morin and Bengio (2005) and builds a binary tree to reduce the complexity from O(n) to O(log(n)), with n being the number of types in the training corpus. Assuming e.g. one million of types, building the model requires only log(n) \u223c 20 computation steps instead of n = 106 computations. The tree itself can be build in various ways. One common option is a Huffman tree that leads to binary codes where the length of each code is proportional to the frequency of the given type. In this tree, each node represents a unique type from the vocabulary with a unique code. Vector updates follow an order derived from root-to-leaf paths in the Huffman tree.\nChoice of Approximation Method: Recent literature reflects NEG as the more popular option, but since more stable embeddings for recommendations are favored in practice, we nevertheless decide to use HS. We study the difference of both sampling methods by evaluating how the resulting embeddings react to minimal data change with the experiment outlined in Sec. 4. The correlation between the results of HS and NEG is about 50%. We find that NEG shows lower variance, but produces more sensitive models with respect to both environment and data changes.\nHierarchical Softmax and Instabilities: For our experiments that require a deterministic approximation method, we modify HS\u2019s tree building mechanism by processing types in their lexical order whenever they have the same frequency.\nWhile this modification guarantees identical prediction models for identical input, the tree building algorithm still accounts for much of the resulting models\u2019 sensitivity to seemingly inconsequential data changes: Given that our input data follows a Zipf distribution, many of our types have similar frequencies, especially the long tail of rare products. If one single token of a type t is omitted from the input (which means one click less), many leaves shuffle around and large parts of the inner tree structure can change. Each leaf node whose type shares t\u2019s original frequency and each type node with t\u2019s new frequency might change their position.\nWe illustrate such changes in the tree with a minimal example: Table 1 shows the Huffman codes (Code 1 and 2) arising from an artificial setting of 9 product types (Pr. A\u2013I). We compare two settings (1 and 2) that differ only by one occurrence of G (in practice, this means a difference of one click). The Hamming distances (Dist.) between the two Huffman codes are shown in the last column. In this example, decrementing the frequency of one type by one results in an average Hamming distance of 1.67.\nWhen we scale this experiment to our sample data with 1,309,907 million sessions and 253,889 different products, one might assume that leaving out one data point becomes a trivial change. Leaving out one session from 1,309,907 does, however, impact the Huffman coding considerably: Of the 500 subsamples that differ by one session, only 13 share the same set of Huffman codes (and thus the same tree) with the reference model. Within the 487 models that have a different Huffman coding, on average the codes of 46,244\u00b159,015 products change in a range between 3 and 244,259 changed codes. As we will show in the next section, changes in the Huffman tree directly affect the resulting vector model, and thus the final predictions."}, {"heading": "5.3 Results: Minimal Data Changes", "text": "We evaluate the impact of single data points (user sessions) in two ways. First, we analyze the influence on neighborhood\nsimilarities in detail (cf. Section 4.1) using a linear regression model. This experiment quantifies associations between particular data point characteristics and the degree of overlap to the reference model. Additionally, we analyze topological changes via cluster analysis, which indicates how the model shape reflects sensitivity to missing data points.\nData Point Features & Neighborhood Similarity We compare the overlap of the 15 nearest neighbors for 5,000 vectors sampled from the 10,000 most frequent types. The average overlap is 93.6\u00b12.65 for HS and 80.5\u00b11.98 for NEG. With this measure as dependent variable, we analyze stability rather than instability, and hence draw the reverse conclusion. We consider a model sensitive or unstable, if omitting a data point results in a new model\u22121 that has only a small overlap with the reference model. As sampling with frequent types might be suspected to induce a biased view on the overlap, we validate this assumption drawing a random sample of types. This decreases the level of overlap slightly, but due to an almost perfect correlation to the used variable, analytical relations are virtually unaffected.\nTable 2 describes the explaining variables. From that set, session Length is the only property inherent to a data point, all other features describe data points in the training corpus\u2019 context (Frequency, Rank, Min Count). Associated with the sampling method, we also count the number of changes in the Huffman codes induced by leaving out this session (log10scaled for a better fit), and the maximum change in binary code as quantified by the Hamming distance.\nOur analysis also compares both approximation methods, Hierarchical Softmax (HS) and Negative Sampling (NEG), verifying results across both methods. Except for Rank, the features were selected such that the explanatory power for the HS-method is optimized; for the NEG-method choosing a different set of features would raise the degree of explicability. Table 3 summarizes the outcome: Constant shows the intercept of the regression model, Observations is the number of considered model\u22121-experiments and Adjusted R2 displays the adjusted coefficient of determination.\nAccording to the regression model\u2019s results, the features account for about 92% of the variance in overlap (cf. Adjusted R2) for HS, and about 67% for NEG. For the HS-\nmodel, the two measures reflecting changes in the Huffman tree are by far the most important ones. They alone explain 85% of the variation in the overlap, whereby stability is associated with fewer changes in the tree. A replicated leavingone-out experiment with exactly the same Huffman tree for all runs shows this assumption to be true resulting in an average overlap of more than 99%.\nInterestingly, both Huffman-related measures also show a weakly significant correlation with NEG-overlap, although Negative Sampling does not employ Huffman trees. Exploring this will be subject to future work.\nOther relevant features include the session Length, which correlates negatively with the overlap. Its significance is, however, bounded compared to changes in the Huffman tree. In a univariate regression model, session length alone can explain about 35% of the variation. This relates to findings on linguistic data [Antoniak and Mimno, 2018], with both results confirming the expectation that larger data chunks (documents or sessions) contain more information.\nFrequency is positively associated with overlap for HS. Omitting sessions with rare products decreases stability. Interestingly, rare items seem to affect NEG inversely.\nAnother feature, which has opposite associations for the two models, is Min Count. This binary variable indicates whether one or more products fall under the threshold for consideration. While it seems odd that the stability of HS increases if products are left out, the association is weakly significant.\nOne feature exclusively relevant for NEG is the position of the left-out session in the input data file (Rank). Sessions processed later have less influence on the models\u2019 stability.\nTopological stability We complement the analysis of neighborhood similarity with a study of model topologies. While the overlap of recommendations quantifies a change with respect to a reference model, topological figures might allow to directly benchmark models using their formal representations.\nAs an example, we illustrate the impact of a single data point on the number of clusters, which we calculate with the DBScan clustering algorithm (cf. Section 4.1). Figure 1\nshows a histogram of the 501 embeddings trained with the Hierarchical Softmax approximation. Each bar represents the number of model\u22121-experiments (y-axis) resulting in the specific number of clusters (x-axis). The number of clusters ranges from 399 to 441 and the bar marked in red includes the reference model consisting of 429 clusters. The mean difference in the number of clusters with respect to the reference model is -7.23\u00b16.21. In most cases, omitting a session reduces the number of clusters. Interestingly, the total number of noise points, which averages to 143,416\u00b199, is virtually constant. Associated therewith, the average cluster size increases with a decreasing number of clusters, because the same number of points is divided into fewer clusters.\nWe also analyze the purity of clusters, which we define as the proportion of products belonging to the clusters\u2019 prevalent product group. Purity is thus minimal if each product in a cluster belongs to a different group, and maximal if all products in a cluster belong to the same group. The mean cluster purity for each embedding ranges from 0.903 to 0.917 with a mean of 0.910\u00b10.002. The average purity depends on the number of small clusters, which contain fewer product groups and are thus purer in our experiment.\nTracing back the relationship of topological structure and model stability, we find mainly two bridges. First, differences in the number of clusters are significantly larger for two models with large maximum Hamming distances and many Huffman code changes, which is in line with the findings of our leaving-one-out experiment. Second, local density of the trained embeddings (cf. Section 4.1) correlates with the overlap measure. The larger the density of the embedding, the smaller the overlap. An intuitive explanation here is that there are many good recommendation candidates in a dense cluster, so the choice can vary more without sacrificing quality. Future research needs to show how exactly such topological changes relate to the omitted data points."}, {"heading": "6 Conclusion", "text": "We showed an approach to sensitivity analysis of geometric predictive models, compiling metrics that measure both the topological stability of the vector model itself and the similarity of the output when changing parameters or leaving out sin-\ngle data points. As a basis for such an analysis, we identified and eliminated sources of randomness inherent to word2vec."}, {"heading": "6.1 Main Contributions", "text": "Our main take-aways might help others to interpret and prevent instabilities in word2vec and similar algorithms:\n\u2022 Stable settings: We identified sources of instability in word2vec and showed how to achieve constant results on identical input data. In particular, we showed that fixing the Huffman tree for HS can prevent variance even when the input data changes.\n\u2022 Choice of Approximation Method: Choosing a different approximation method changes both, the result of a model and its sensitivity. In word2vec\u2019s case, Negative Sampling adopts new information more readily, while Hierarchical Softmax maintains more stability.\n\u2022 Data changes: Even a single data point can change the models\u2019 output considerably. While the results\u2019 quality might be stable, the actual outputs change depending on the actual data points added or removed.\n\u2022 Information Density: Data points, which hold more information, cause more changes in the final model. \u201cMore information\u201d does not only refer to longer sentences or user sessions, but in particular to data points containing surprising associations of concepts, or information on rare concepts, or information on concepts for which the model learned a \u201ccluttered\u201d environment."}, {"heading": "6.2 Discussion & Future Work", "text": "We presented a detailed way to measure model and output stability on an industrial scale. While certainly not comprehensive, our metrics can be a starting point for further analysis on model sensitivity of embeddings. We also believe that our methods and findings are applicable to linguistic and other applications. However, language as a data source behaves in a considerably different fashion than user interactions, thus possible differences concerning expressiveness and usefulness of our way to measure output consistency and quality can be expected. Comparing our results applied with linguistic experiments could show interesting differences.\nIn future work, we aim to additionally relate topological metrics to data point features. In the long run, we want to find a generic way to detect and predict sensitivity for all types of vector models, independently from particular use cases, data types or scale. Another goal is to highlight the relationship of model instability and business value of certain data points, assuming that data points which increase the model\u2019s output quality carry more valuable information. With such experiments, we could show not only data-point influence on model structure and predictions, but also measure the monetary value of model stability and the underlying data."}, {"heading": "Acknowledgments", "text": "We want to thank Julia Soraya Georgi and two anonymous reviewers for their helpful comments. All remaining errors are of course our own."}], "title": "Analyzing Hypersensitive AI: Instability in Corporate-Scale Machine Learning", "year": 2018}