{"abstractText": "Algorithmic bias is the systematic preferential or discriminatory treatment of a group of people by an artificial intelligence system. In this work we develop a random-effects based metric for the analysis of social bias in supervised machine learning prediction models where model outputs depend on U.S. locations. We define a methodology for using U.S. Census data to measure social bias on user attributes legally protected against discrimination, such as ethnicity, sex, and religion, also known as protected attributes. We evaluate our method on the Strategic Subject List (SSL) gun-violence prediction dataset, where we have access to both U.S. Census data as well as ground truth protected attributes for 224,235 individuals in Chicago being assessed for participation in future gun-violence incidents. Our results indicate that quantifying social bias using U.S. Census data provides a valid approach to auditing a supervised algorithmic decision-making system. Using our methodology, we then quantify the potential social biases of 100 million ridehailing samples in the city of Chicago. This work is the first large-scale fairness analysis of the dynamic pricing algorithms used by ridehailing applications. An analysis of Chicago ridehailing samples in conjunction with American Community Survey data indicates possible disparate impact due to social bias based on age, house pricing, education, and ethnicity in the dynamic fare pricing models used by ridehailing applications, with effect-sizes of 0.74, 0.70, 0.34, and -0.31 (using Cohen\u2019s d) for each demographic respectively. Further, our methodology provides a principled approach to quantifying algorithmic bias on datasets where protected attributes are unavailable, given that U.S. geolocations and algorithmic decisions are provided.", "authors": [{"affiliations": [], "name": "Akshat Pandey"}, {"affiliations": [], "name": "Aylin Caliskan"}], "id": "SP:0f206757ff6ed97e6c74d8412b4b3e6332b18941", "references": [{"authors": ["Julia Angwin", "Surya Mattu", "Jeff Larson"], "title": "Asian students are charged more for test prep", "year": 2015}, {"authors": ["Julia Angwin", "Robert Siegel"], "title": "Propublica reveals discriminatory pricing by computer algorithms, Oct 2016", "year": 2016}, {"authors": ["Iain Barclay", "Alun D. Preece", "Ian Taylor", "Dinesh C. Verma"], "title": "Quantifying transparency of machine learning systems through analysis of contributions", "year": 1907}, {"authors": ["Solon Barocas", "Moritz Hardt", "Arvind Narayanan"], "title": "Fairness and Machine Learning", "venue": "fairmlbook.org,", "year": 2019}, {"authors": ["Solon Barocas", "Andrew D. Selbst"], "title": "Big data\u2019s disparate impact", "venue": "California Law Review,", "year": 2016}, {"authors": ["Jacob Benesty", "Jingdong Chen", "Yiteng Huang", "Israel Cohen"], "title": "Pearson correlation coefficient", "venue": "In Noise reduction in speech processing,", "year": 2009}, {"authors": ["Michael Borenstein"], "title": "Introduction to meta-analysis", "year": 2012}, {"authors": ["Khristopher J. Brooks"], "title": "Redlining\u2019s legacy: Maps are gone, but the problem hasn\u2019t disappeared, Feb 2020", "year": 2020}, {"authors": ["Anne Elizabeth Brown"], "title": "Ridehail Revolution: Ridehail Travel and Equity in Los Angeles", "venue": "PhD thesis,", "year": 2018}, {"authors": ["Sylvia M Burwell"], "title": "Annual update of the hhs poverty", "year": 2015}, {"authors": ["Regina R. Clewlow", "Gouri S. Mishra"], "title": "Disruptive Transportation: The Adoption, Utilization, and Impacts of Ride-Hailing in the United States", "venue": "Institute of Transportation Studies,", "year": 2017}, {"authors": ["Jack Cohen"], "title": "Statistical power analysis for the behavioral sciences", "venue": "L. Erlbaum Associates,", "year": 1988}, {"authors": ["Harris M. Cooper", "Larry V. Hedges", "Jeff C. Valentine"], "title": "The handbook of research synthesis and meta-analysis", "venue": "Russell Sage Foundation,", "year": 2009}, {"authors": ["Sam Corbett-Davies", "Sharad Goel"], "title": "The measure and mismeasure of fairness: A critical review of fair machine learning", "venue": "arXiv preprint arXiv:1808.00023,", "year": 2018}, {"authors": ["Amit Datta", "Michael Carl Tschantz", "Anupam Datta"], "title": "Automated experiments on ad privacy settings: A tale of opacity, choice, and discrimination", "venue": "Proceedings on privacy enhancing technologies,", "year": 2015}, {"authors": ["Anupam Datta", "Shayak Sen", "Yair Zick"], "title": "Algorithmic transparency via quantitative input influence: Theory and experiments with learning systems", "venue": "IEEE symposium on security and privacy (SP),", "year": 2016}, {"authors": ["Michael Feldman", "Sorelle Friedler", "John Moeller", "Carlos Scheidegger", "Suresh Venkatasubramanian"], "title": "Certifying and removing disparate impact", "year": 2014}, {"authors": ["Yanbo Ge", "Christopher R Knittel", "Don MacKenzie", "Stephen Zoepf"], "title": "Racial and gender discrimination in transportation network companies", "venue": "Working Paper 22776,", "year": 2016}, {"authors": ["Arline T. Geronimus", "John Bound", "Lisa J. Neidert"], "title": "On the validity of using census geocode characteristics to proxy individual socioeconomic characteristics", "venue": "Journal of the American Statistical Association,", "year": 1996}, {"authors": ["Jeremy Hermann"], "title": "Scaling machine learning at uber with michelangelo, Oct 2019", "year": 2019}, {"authors": ["R. Laxhammar"], "title": "Anomaly detection for sea surveillance", "venue": "In 2008 11th International Conference on Information Fusion,", "year": 2008}, {"authors": ["Lishuai Li", "R. Hansman", "Rafael Palacios", "Roy Welsch"], "title": "Anomaly detection via a gaussian mixture model for flight operation and safety monitoring", "venue": "Transportation Research Part C: Emerging Technologies, 64:45\u201357,", "year": 2016}, {"authors": ["Weiwen Miao"], "title": "Did the results of promotion exams have a disparate impact on minorities? using statistical evidence in ricci v. destefano", "venue": "Journal of Statistics Education,", "year": 2010}, {"authors": ["Arvind Narayanan"], "title": "Translation tutorial: 21 fairness definitions and their politics", "venue": "In Proc. Conf. Fairness Accountability Transp.,", "year": 2018}, {"authors": ["Douglas Reynolds"], "title": "Gaussian Mixture Models, pages 659\u2013663", "year": 2009}, {"authors": ["Mah-jabeen Soobader", "Felicia B LeClere", "Wilbur Hadden"], "title": "Using aggregate geographic data to proxy individual socioeconomic status: does size matter", "venue": "American Journal of Public Health,", "year": 2001}, {"authors": ["Martin Strobel"], "title": "Aspects of transparency in machine learning", "venue": "Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems,", "year": 2019}, {"authors": ["Latanya Sweeney"], "title": "Discrimination in online ad delivery", "year": 2013}, {"authors": ["Nicol Turner-Lee", "Paul Resnick", "Genie Barton"], "title": "Algorithmic bias detection and mitigation: Best practices and policies to reduce consumer harms", "year": 2019}, {"authors": ["H Jerrold"], "title": "Zar. Spearman rank correlation", "venue": "Encyclopedia of Biostatistics,", "year": 2005}, {"authors": ["Rich Zemel", "Yu Wu", "Kevin Swersky", "Toni Pitassi", "Cynthia Dwork"], "title": "Learning fair representations", "venue": "Proceedings of the 30th International Conference on Machine Learning,", "year": 2013}], "sections": [{"heading": "1 Introduction", "text": "In 2015, the dynamic algorithmic pricing used by the Princeton Review was found to be charging Asian-Americans higher prices for their test-preparation services twice as often as non-Asian customers, often regardless of a customer\u2019s income [4, 5]. The use of geographic location in their dynamic pricing model, which adjusted prices based on where customers resided, was determined to be at the root of the change in prices due to ethnicity [4, 5]. While neither illegal nor intentional [4, 5], the bias exhibited by the Princeton Review\u2019s use of a dynamic pricing model had considerable financial consequences for their customers. As the use of algorithmic dynamic pricing increases, unintended consequences such as racially based price disparities, as in the case of the Princeton Review [4, 5], must be found and accounted for to ensure the fairness of service pricing.\nThe use of dynamic pricing models is becoming increasingly more common [4]. Another service where the use of algorithmic dynamic pricing can be regularly seen is in ridehailing services, such as Uber [3, 4]. Ridehailing applications are services that allow users to order rides to a specified location from the convenience of their phone. These services are provided by various Transportation Network\nar X\niv :2\n00 6.\n04 59\n9v 4\n[ cs\n.C Y\n] 2\n2 Ju\nn 20\nCompanies, such as Uber and Lyft, and can be found all over the world. In major U.S. cities, 21% of adults have used ridehailing services, and in the first five years of adoption (beginning in 2009) there were a total of 250 million estimated users of ridehailing services [17].\nUnlike traditional taxi services, fare prices for ridehailing services are dynamic, calculated using both the length of the requested trip as well as the demand for ridehailing services in the area [3, 41]. Uber determines demand for rides using machine learning models, using forecasting based on prior demand to determine which areas drivers will be needed most at a given time [31]. While the use of machine learning to forecast demand may improve ridehailing applications\u2019 ability to provide services to their riders, machine learning methods have been known to adopt policies that display demographic disparity in online recruitment, online advertisements, and recidivism prediction [7, 40]. In this work, bias will be used to refer to social bias, or prejudice in favor of an attribute, rather than statistical bias, which is often of concern in machine learning. Current anti-discrimination laws do not account for bias as it may propagate in a supervised machine learning model [8], and neither the model nor the ground truth data used to train the model may be available for examination of bias [6, 38, 20]. The lack of extensive algorithmic regulation and the black-box nature of ridehailing fare pricing algorithms leads to the concern of whether they may be exhibiting bias towards riders based on their demographics. This concern is the primary motivation for this work, prompting us to examine a large-scale ridehailing dataset for evidence of possible bias. This is the first analysis of the fairness of ridehailing application dynamic fare pricing algorithms, performed using the most comprehensive ridehailing dataset currently available, consisting of over 100 million rides from the city of Chicago from November 2018 to December 2019 [1].\nOur analysis of Chicago ridehailing applications is the first to examine possible biases of ridehailing dynamic fare pricing algorithms, and our work provides the following 3 contributions: 1) We develop a bias summarization metric, Iterative Effect-Size Bias (IESB), combining iterative bias magnitudes with a random-effect approach on census demographic thresholds to obtain a fine-grained analysis of dynamic pricing bias based on model per-mile ride prices and pickup and dropoff locations, and apply it to ridehailing data in the city of Chicago. 2) We provide an approach to measuring bias in supervised algorithmic decision-makers where decisions are tied to U.S. locations. 3) We show that ridehailing fare pricing algorithms may be developing biases based on the age, house pricing, and education statistics of riders\u2019 pickup neighborhoods (with effect-sizes of 0.72, 0.68, and 0.34 respectively using Cohen\u2019s d) and biases based on the age, house pricing, and ethnicity statistics of riders\u2019 dropoff neighborhoods (with effect-sizes of 0.74, 0.70, and -0.31 respectively using Cohen\u2019s d), and provide evidence that these biases may be reflected in increased fare prices. Conventional small, medium and large values of Cohen\u2019s d are 0.2, 0.5, and 0.8 - our results indicate that fares increase for neighborhoods with a lower percentage of people above 40, a lower percentage of below median house price homes, a lower percentage of individuals with a high-school diploma or less, or a higher percentage of non-white individuals.\nData from the city of Chicago was chosen because of a new city-wide law requiring ridehailing applications to disclose fare prices [18], making the Chicago ridehailing data the most complete dataset for understanding ridehailing fare pricing models currently available. The dataset includes not only fare prices but neighborhood pickup and dropoff locations, which can be used to connect the dataset to American Community Survey (ACS) [13] data. ACS is an annual survey performed by the federal government of the U.S. which collects various demographic statistics about people living in the U.S. This work shows how ACS\u2019 census data can be used to analyze biased outcomes of geotagged datasets from the U.S. in case location information becomes a proxy to protected attributes. Prior work on quantitative input influence and redlining have uncovered bias in algorithmic decision-making when seemingly innocuous features such as an individual\u2019s zipcode act as a proxy to protected attributes [25, 23]. This work shows that in cases where location is a proxy to protected attributes similar to redlining [11], as Datta et al. [25] shows, geotagged datasets can then be analyzed for potential proxies for bias. Figure 1 displays by neighborhood, the distribution of mean fare prices in the city of Chicago, indicating that ridehailing is more expensive in neighborhoods that have higher house prices with younger and more educated populations. These census tract properties also correlate highly with ridehailing demand. Nevertheless, raw correlation information doesn\u2019t provide fine-grained analysis of how individual neighborhoods might be affected by algorithmic pricing. As a result, in this work, analysis is performed using a newly defined metric based on multiple effect-size calculations, which does not necessarily assume a linear relationship between variables, as in the case of Pearson correlation [9]. The rest of our work explores the relationships between ridehailing fare pricing, pickup and dropoff frequencies, and various different available demographics from ACS, including ethnicity, income, housing prices, and age. Implementation details, datasets, results and source code are available in supplementary materials or in our public repository at www.gitRepo.com."}, {"heading": "2 Related Work", "text": "We examine two different categories of related work in the space of algorithmic bias: research examining bias specifically in ridehailing and taxi service pickup rates [12, 29], and research detailing how to ensure the fairness of supervised learning models [27, 28, 44]. We also examine two papers on the use of ACS demographic data rather than individual demographic attributes to make inferences about an individual [30, 37].\nGe et al. [29] examine bias in Uber and Lyft pickups in the cities of Boston and Seattle by requesting human subjects to call for rides using Uber and Lyft, evaluating wait times and cancellation rates based on subject ethnicity through a user study. Their findings suggest male riders with African-American names are three times more likely to have rides cancelled and wait as much as 35% longer for rides. Brown [12] performs an extensive analysis of ridehailing applications and their effect on mobility in the city of Los Angeles, including an analysis of the different socioeconomic characteristics of the neighborhoods served by Lyft. Unlike Ge et al. [29], our work does not rely on a user study, and unlike both Ge et al. [29] and Brown [12], our work is the first to provide an analysis on ridehailing trip fare pricing rather than frequency.\nFeldman et al. [28] and Zemel et al. [44] detail methodologies for testing and removing bias from datasets, on the same datasets with different perspectives on fairness, disparate impact and statistical parity respectively. While the debiasing of a supervised model is not relevant to this work directly, both works describe methods for quantifying bias, which are compared to the method for quantifying bias introduced in this work. Generally public access to white-box models making decisions algorithmically or the data used to train them is not granted due to privacy regulations [20], as in prior cases of bias in online recruitment, online advertisements, and recidivism prediction [24, 40, 39]. The focus of our work is on measuring bias with observable model outcomes and a partial set of input features, which can be a necessity if models and their training data cannot be made publicly available.\nGeronimus et al. [30] and Soobader et al. [37] test how the use of ACS demographic data to make inferences about individuals compares to the use of individual demographic data to make inferences in the context of predicting health outcomes. Geronimus et al.\u2019s [30] and Soobader et al.\u2019s [37] findings conclude that the results of inferences based upon aggregates can not be interpreted the same as the results of inferences based on individual demographic attributes. However our methodology does not attempt to make inferences based on neighborhood-wide demographic data, but rather uses bias found across neighborhoods to test bias on individual demographics. The role of our methodology\nfor auditing algorithmic decision-makers rather than making further inferences using their outcomes provides the justification for the use of ACS data in our analyses. In addition, without access to training data, this approach is currently the best possible option for a fairness audit."}, {"heading": "3 Datasets", "text": "Measuring Bias. Our method, IESB is tested using both the Ricci Firefighter Promotion Exam dataset [34] and German credit dataset [26]. These two datasets were chosen because of their use in other works regarding algorithmic bias, including Feldman et al. [28] and Zemel et al. [44]. The Ricci dataset contains 118 test taker entries for a firefighter promotion exam that is examined for racial bias with regard to passing rates. The German credit dataset is a set of individuals with corresponding \u201cgood\u201d or \u201cbad\u201d credit ratings, with an age feature that is examined for bias. The dataset is tested to determine if younger individuals disproportionately receive better or worse credit ratings then older individuals. Both the Ricci dataset and German credit datasets contain a protected attribute, ethnicity and age respectively, as well as a decision-making outcome, test promotion outcomes and credit ratings, and have been tested for bias in prior works in algorithmic bias [28, 44], so they are used to validate how well IESB is able to find bias on outcomes based on protected attributes.\nNeighborhood vs. Individual Demographics. Our method for using bias on ACS census data as an estimate for bias on individual demographic data is validated using a dataset where we have access to both ACS census data through location and individual demographic data. For this validation we use the Strategic Subject List (SSL) dataset, a list of individuals in Chicago with prior arrests, which were then given a score of being involved in a gun violence incident in the future using the Strategic Subject Algorithm, created by the Illinois Institute of Technology [16]. The SSL dataset contains location information of each individual\u2019s latest arrest as well as their underlying protected race attribute. There are a total of 224,235 samples in the dataset, each sample a unique individual, representing data from 801 U.S. neighborhoods in the Chicago area. We calculate how much the SSL score is biased by an individual\u2019s race, comparing the bias calculated using ground truth protected attributes and the bias calculated using ACS census data. Ground truth bias is calculated using Cohen\u2019s d, and bias using ACS census data is calculated using our metric IESB. The SSL dataset was chosen because it is the output of an algorithmic decision-maker where we have access to U.S. locations as well as ground truth protected attributes for each data point. Due to the disclosure of protected attributes as well as location in the dataset, there are not many comparable datasets available for a similar analysis.\nBias in Ridehailing and Taxi Rides. In order to examine possible biases in fare pricing for ridehailing and taxis we use ridehailing and taxi data provided by the city of Chicago\u2019s data portal [1]. Prior to 2019, there was no comparable comprehensive ridehailing dataset available for analysis - this work began very shortly after the dataset\u2019s release. The ridehailing data contains data from 100,717,117 rides in the city of Chicago from November of 2018 to December of 2019, and the taxi data contains data for 19,496,933 rides in Chicago during the same time period. Times are rounded to the nearest 15 minutes and fares are rounded to the nearest $2.50. Because Uber takes roughly 72% of the ridehailing market share in Chicago [18], the dataset likely is most representative of Uber\u2019s fare-pricing algorithm. Lyft and Via take up 27% and 1% of the remaining market share [18]. No rider characteristics are reported in order to preserve the privacy of the riders [15]. The dataset contains information about the time, charges, and pickup and dropoff locations for each ride, with location being given in the form of ACS census tracts. ACS census tracts are subdivisions of a county designated by the U.S. Census Bureau consisting of about 1200-8000 people, whose boundaries are updated every 10 years. After removing ridehailing rides with missing data as well as shared rides, we are left with data for ~68 million rides1. Due to the self-reported nature of the taxi dataset, we find many of the fare prices to be inconsistent with rates charged by Taxi cab companies in Chicago. We detect and remove taxi rides with outlier fare prices using a Gaussian Mixture Model (GMM) [36]. With outliers removed, we are left with data for ~10 million taxi rides2.\nACS demographic data is split based on census tracts, which are approximations of neighborhoods developed by the American Census Bureau. We use demographic data to represent minority and majority groups for each demographic type: ethnicity is split into white and non-white populations, which include Black or African American, American Indian and Alaska Native, Asian, Hispanic, and\n1Further details regarding ridehailing preprocessing steps can be found in appendices 2Further details regarding taxi data preprocessing steps can be found in appendices\nNative Hawaiian or Pacific Islander. Age is split using a threshold of 40, education is split into those with a high-school diploma or less and those with further education, income is split into those below and above the poverty line, citizenship status into citizens and non-citizens, and home valuations into below and above the median house price (calculated from Zillow\u2019s data3) in the city of Chicago4."}, {"heading": "4 Methods", "text": "Measuring Social Bias. Cohen\u2019s d [19] is a widely-used method for calculating effect-size which compares the difference of means between two groups to determine the magnitude of the effect of a treatment. Conventional small, medium, and large values of Cohen\u2019s d are 0.2, 0.5, and 0.8 respectively. Cohen\u2019s d can be used to measure the effect of a discrete binary characteristic by using one subset of data where all members have a positive label and one subset where all members have a negative label. In our analysis we examine continuous real-valued characteristics, such as the percentage of non-white individuals and non-U.S. citizens in a neighborhood, so we introduce a new method of quantifying bias, Iterative Effect-Size Bias (IESB).\nIn the context of social bias in machine learning, we calculate the standardized magnitude of the effect of having a feature X with a value below a threshold t on our target variable Y . Comparing the groups Xt\u2212 and Xt+, the data with values below and above t respectively, will tell us how much having an X value below t influences target values. Examining effect-sizes rather than computing correlation coefficients provides the advantage of being able to pinpoint bias at particular thresholds (t values) when necessary, and is also free of any assumption about the linearity or monotonicity of the data, when compared to Pearson [9] or Spearman [43] correlation. In addition other methods of determining feature importance, like information gain, cannot be used due to the continuous nature of X and Y in the context of this work. IESB summarizes the effect-sizes calculated between groups Xt\u2212 and Xt+ as t is increased from the minimum to the maximum observed values of X using a weighted mean of effect-sizes. Weights are assigned using a random-effects model [10] since the expected effect-size for each intervention is random and not fixed. In meta-analysis, random-effects models are used to compute a combined effect given a set of effect calculations, where weights are assigned to effect-sizes given the amount of information they contain [10]. A random-effects model is used to summarize the effect-sizes rather than a mean of effect-sizes because as t increases, the sizes of groups Xt\u2212 and Xt+ change, which effects the variance of each effect-size calculation. Using a random-effects model approach to summarize the effect-sizes helps account for this variance [10].\nFormally, let continuous variable Y be the target variable that we test for bias and continuous variable X be the feature we test for influence on Y . Let t be the X value used to separate the two groups being compared with each other, Yxt\u2212 the target values with a corresponding X feature less than t, and Yxt+ the target values with a corresponding X feature greater than t. Equation 1 displays how we calculate bias using IESB. The weight w(X,Y, t) of each effect-size calculation d(X,Y, t) is a sum of the overall variance of all effect-sizes (var(X,Y )) and the variance of each effect-size calculation (var(X,Y, t)) [22]. For example, when calculating bias on fare price based on the percentage of the neighborhood below the poverty line, Y are fare price values, X is the percentage of a census tract below the poverty line, Yx50+ designates the Y values where the corresponding X value is greater than 50, and Yx50\u2212 designates the Y values where the corresponding X value is less than 50.\nThe weighted average effect-size measures the overall bias of the target Y towards attribute X . The sign of slope of the best fit line on the effect-sizes gives us a measure of the direction of the bias, where a positive value means Y generally increases with X , and a negative value means Y generally decreases with X , assuming variance is not high. In the instance that we are using IESB to evaluate whether fare pricing is biased due to rider age, a positive slope means that fare prices generally increase as age increases, and a negative slope means that fare prices generally decrease as age increases. In order to validate our metric we compare our measure with bias measures mentioned in Feldman et al. [28] and Zemel et al. [44], which measure disparate impact and statistical parity respectively. These works were used for validation because both works are concerned with fairness in supervised learning, but use different approaches for measuring bias with an overlap in datasets, making results easy to compare with each other as well as using IESB.\n3American online real estate database company: www.zillow.com 4Further details regarding census data preprocessing can be found in appendices\nIESB(X,Y ) =\n\u2211 t=[tmin...tmax]\nd(X,Y, t)\u00d7 w(X,Y, t)\u2211 t=[tmin...tmax] w(X,Y, t) (1)\nd(X,Y, t) = Y xt\u2212 \u2212 Y xt+\n\u03c3(Y ) [Cohen\u2019s d [19]]\nw(X,Y, t) = var(X,Y ) + vart(X,Y, t)\nvar(X,Y ) = \u03c32 t=[tmin...tmax] [es(Yxt\u2212 , Yxt+)]\nvart(X,Y, t) = ( |Xt\u2212|+ |Xt+| |Xt\u2212||Xt+| + es(Yxt\u2212 , Yxt+) 2(|Xt\u2212|+ |Xt+| \u2212 2) ) \u00d7 ( |Xt\u2212|+ |Xt+| |Xt\u2212|+ |Xt+| \u2212 2 )\nIn Feldman et al. [28], disparate impact is measured using a generalization of the 80% rule as specified by the US Equal Employment Opportunity Commission regarding fair hiring practices. The 80% rule, or the 4/5ths rule is a rule defined by the U.S. Equal Employment Opportunity Commission which states that disparate impact is occurring when one race, sex, or ethnic group is being selected with less than 80% of the selection rate of the group with the highest selection rate [21]. In Zemel at al. [44], bias is measured by taking the difference between the probability of being given a positive value for Y and a positive label for binary characteristic X and the probability of being given a positive value for Y and a negative label for binary characteristic X .\nValidating the Use of ACS Census Data in Bias Measurement. When examining models for bias, it is often the case that neither the model nor the ground truth training data used to train the model is available for both the protection of proprietary software and data privacy [6, 38]. In these cases, model bias can only be examined using model outputs and an incomplete set of features used in prediction. Ridehailing algorithms fall into this category. Because ridehailing users\u2019 protected attributes are not available, ridehailing data cannot be used to test how well ACS census data performs as a proxy for individual demographic data. Since ridehailing applications might also not have access to protected attributes, our approach tries to accomplish the most accurate measure of fairness given data availability constraints.\nWe validate our use of ACS census data with SSL scores, which measure the likelihood that an individual will be involved in a future gun violence incident [16], and protected race attributes using Cohen\u2019s d. We measure bias on SSL scores due to the percentage of non-white members of a census tract with ACS data using IESB scores as a measurement. Because IESB uses repeated applications of Cohen\u2019s d, our bias score outcomes are in the same scale as a single Cohen\u2019s d analysis, where conventional small, medium, and large values are 0.2, 0.5, and 0.8. We compare bias between ACS census data and SSL score, to the bias between protected attributes and SSL score to validate the use of ACS census data in algorithmic bias analysis when protected attributes are not available to us.\n5 Results Dataset Feldmanet al. [28] Zemel et al. [44] IESB\nRicci 0.50 0.30 0.70 German Credit 0.35 0.15 0.08\nTable 1: Bias Metric Comparison - Bias scores are provided for the Ricci and German Credit datasets using IESB, as well as bias scores from Feldman et al. [28] and Zemel et al. [44]. All scores are scaled to range between 0-1, so that we can compare them.\nMeasuring Social Bias. We compare our bias metric, IESB to those specified by Feldman et al. [28] and Zemel et al. [44] using the Ricci firefighter promotion exam dataset, as well as the German credit dataset, which are both examined in Feldman et al. [28]. The bias scores calculated using IESB as well as those from Feldman et al. [28] and Zemel et\nal. [44] are displayed in Table 1, where IESB shows bias with large effect-size in the Ricci dataset and bias with small effect-size in the German credit dataset. There are more than 20 fairness notions measuring different types of social biases leading to slightly different results. Nevertheless all indicate significant biases [35].\nUse of ACS Census Data in SSL Gun-Violence Prediction Algorithm Bias Measurement. Figure 2 displays the population density of SSL scores, a score from 0-500 designating how likely an individual was determined to be part of a future gun violence incident [16]. Score densities are\nshown split by the white and non-white populations. From Figure 2, we can see that the non-white population distribution is shifted to the right of the white population distribution, indicating that non-white individuals in this dataset generally receive higher SSL scores. Systematically scoring one group higher than another is indicative of bias in the ground truth SSL data.\nThis significant bias is also shown in the effect-size calculation for the relationship between race and SSL score, shown in Table 2. This effect-size, d = 2.61 indicates a very strong relationship between race and SSL score - a Cohen\u2019s d value of 0.8 is considered large. Table 2 shows IESB scores on ACS census data and the ground truth bias, or the bias (Cohen\u2019s d) calculated on the individual demographic data. Using IESB on census attributes indicates a relationship between race and SSL scores, matching the relationship shown between race and SSL scores calculated using individual demographics, though IESB\u2019s underestimates bias.\nProtected Attributes (effect-size) Census Data (IESB)\n2.61 0.40\nTable 2: Census Data vs Ground Truth Bias - This table shows bias calculated on the SSL dataset on protected attributes and census data. All scores are in standard deviation units.\nRidehailing and Taxi Bias. We calculate the bias score on the fare price per mile for ethnicity, age, education level, citizenship status, and median house price in Chicago. These bias scores for each demographic are shown in Table 3. A positive direction indicates an increase in fare price as the respective attribute increases in a census tract. A negative direction indicates a decrease in fare price as the respective attribute increases in a census tract."}, {"heading": "6 Discussion", "text": "Figure 2: SSL Score Distribution by Ethnicity - SSL scores designate how likely an individual is to be part of a gun violence incident in the future and range between 0-500.\nTable 3 suggests that there is an increase in ridehailing fare prices when riders are picked up or dropped off in neighborhoods with a low percentage of individuals over 40 or a low percentage of individuals with a high school education or less. Table 3 also suggests that there is an increase in ridehailing fare prices when riders are picked up in a neighborhood with a low percentage of individuals with a high school education or less, or dropped off in a neighborhood with a high non-white population. The highest bias scores on fare pricing are seen for the number of pickups and the number of dropoffs in a census tract, suggesting that these pickup and dropoff frequency have the largest impact on fare price compared to neighborhood demographic statistics. The bias scores on neighborhoods in the city of Chicago may point to the existence\nof bias on riders based on individual age and education demographics. Comparing bias scores between ridesharing and taxi services in table 3 also suggests that the static pricing model used by taxis generally leads to less bias with regard to fare pricing. Moreover, the p-values for taxi bias scores suggest that neighborhood demographic attributes have little correlation to fare pricing for taxi rides.\nIn Section 4, we show how bias scores calculated across neighborhoods can be indicative of bias on individual riders\u2019 protected attributes. Our analysis of the gun-violence dataset, shown in Table 2 suggests that biases found at the neighborhood level may be an underestimation of individual level bias. IESB may be underestimating bias in the SSL dataset, which contains predictions for whether individuals will be involved in future gun violence incidents, because arrest locations are used for demographic data rather than home census tracts, which we do not have access to. This is consistent with previous work on the use of ACS data to estimate individual socioeconomic attributes [30]. Accordingly, if our results in Table 3 reflect a similar underestimation of bias on individual demographic attributes, ridehailing applications in Chicago may be exhibiting large bias scores on fare prices with regard to rider age, poverty level and ethnicity. Regardless of an individual rider\u2019s age, education, ethnicity or poverty level, our results show that neighborhood-wide statistics for these demographics in pickup and dropoff neighborhoods have an affect on fare pricing.\nScaling IESB pickup location bias scores for age, education and house price between 0 and 1, the resulting bias scores, .36, .17, and .34 are comparable to the Pearson correlation values from Figure 1 - however we can examine bias at different percentage thresholds of each demographic using our approach, and avoid assuming a linear relationship between demographic and fare price variables. As a result, we can provide an accurate and comprehensive measurement of impact of continuous variables on continuous outcomes.\nWhile we can estimate bias on protected attributes using ACS data, we do not know how strong the relationship is between the estimate and the underlying ground truth without access to the data. Currently, our methodology provides the only way to measure the bias of fare pricing on ridehailing applications, using the observational data that is available about rides. While we are able to provide an estimate of bias on protected attributes, and are able to show that additional (non-protected) attributes, pickup and dropoff counts, essentially demand, have the strongest relationship with fare pricing, our current analysis does not explicitly provide us with proof that the relationship is causal. IESB analysis of pickup and dropoff frequencies shows that while neighborhood demographic statistics generally have a larger bias score on fare pricing than for taxis, demographics do not have a significantly different effect on pickup and dropoff frequencies for ridehailing or taxi services.6 The results of this work pertain specifically to ridehailing applications in the city of Chicago.\nIESB cannot determine how much a change in the bias scores exhibited by one feature (pickup counts) would affect bias scores of other features (ethnicity). We can compare the magnitude of bias in two datasets but not the actual raw values, such as ridehailing vs. taxi fare pricing. Multi-variable cross-dataset comparison and inter-feature relationship analysis is left to future work."}, {"heading": "7 Conclusion", "text": "We introduce a new method, IESB for social datasets, that measures the effect of continuous variables on prediction outcomes to measure social bias on populations. While demand and speed have the highest correlation with ridehailing fares, analysis shows that users of ridehailing applications in the city of Chicago may be experiencing social bias with regard to fare prices when they are picked up or dropped off in neighborhoods with a low percentage of individuals over 40 or a low percentage of individuals with a high school diploma or less. In addition users may be facing social bias if picked up in a neighborhood with a low percentage of houses priced less than the median house price of Chicago, or dropped off in a neighborhood with a low percentage of white people. This means that users in neighborhoods with lower percentages of these demographics may be experiencing disparate impact and have to pay more for rides than users in other neighborhoods. Our analysis of ACS census data and protected attributes also suggests that the difference in fare prices based on neighborhoods may also extend to individual attributes of age, education, home prices, and ethnicity.\n6Extended bias results can be found in appendices."}, {"heading": "8 Broader Impact", "text": "Any algorithmic decision making process based on geolocation data in the United States has the potential to learn and express demographic biases, due to demographic differences in locations throughout the United States. When geolocation is used in conjunction with dynamic pricing algorithms, applications may learn to charge people belonging to a particular demographic more than others [4, 5]. The methodologies proposed in this work can be used to audit applications and determine where biases may exist in a geolocation-based algorithmic decision maker, such as a dynamic pricing model. But an audit of algorithmic bias is only the first step to ensuring algorithmic fairness. Debiasing any discovered bias requires a separate concerted effort - one that can only be completed after an audit, but yet requires further detail of a model\u2019s inner workings. Performing an audit of algorithmic decision makers can help reduce the possible disparate impact of applications that result in unfair outcomes for users downstream given that any discovered bias is subsequently dealt with using a specifically tailored solution for each individual decision maker."}, {"heading": "A Datasets", "text": "Ricci Firefighter Promotion & German Credit Datasets Preprocessing: Iterative Effect-Size Bias (IESB) is tested using both the Ricci and German credit datasets, which were chosen because of their use in other works regarding algorithmic bias, including Feldman et al. [28] and Zemel et al. [44]. The Ricci dataset contains 118 test taker entries for a firefighter promotion exam that is examined for racial bias. From a machine learning perspective, the features of the data would be used to predict whether or not an individual passed the promotion exam. The dataset contains a race feature, with labels for White, Black, and Hispanic test-takers, a position listing, oral and written test scores, as well as a combined test score, ranging from 0 to 100%. This dataset was used to test whether there was an existing bias on test scores with regard to race. We binarize the race feature to white/non-white, the combined test score to pass/no-pass with a threshold of 70% as the minimum requirement for a passing test score, matching the preprocessing settings of Feldman et al. [28].\nThe German credit dataset is a set of \u201cgood\u201d or \u201cbad\u201d credit ratings for individuals, with an age feature that is examined for bias, as in Feldman et al. [28] and Zemel et al. [44]. The dataset is tested to determine if younger individuals disproportionately receive better or worse credit ratings than older individuals. The dataset contains 20 different features for each individual which could be used to predict credit rating labels in a supervised machine learning setting. When calculating bias using the methods specified by Feldman et al. [28] and Zemel et al. [44], we binarize age into young/old, splitting at age 25, similar to Feldman et al. [28], as both methods from Feldman et al. [28] and Zemel et al. [44] are unable to take continuous values into account when calculating bias. Because IESB is able to take continuous values, we do not binarize the age feature when calculating bias using our scoring metric. Both the Ricci dataset and German credit datasets contain a protected attribute, ethnicity and age as well as a decision-making outcome, test promotion outcomes and credit ratings, and have been tested for bias in prior works in algorithmic bias [28, 44], so they are used to validate how well IESB is able to find bias on outcomes based on protected attributes.\nChicago Ridehailing and Taxi Datasets Preprocessing: In order to examine possible biases in fare pricing for ridehailing and taxis we use ridehailing and taxi data provided by the city of Chicago\u2019s data portal [1]. The ridehailing data contains data from 100,717,117 rides in the city of Chicago from November of 2018 to December of 2019, and the taxi data contains data for 19,496,933 rides in Chicago during the same time period. Times are rounded to the nearest 15 minutes and fares are rounded to the nearest $2.50. Due to the size of both the ridehailing (~68 million rides) and taxi datasets (~20 million rides), the general trends reported in our analysis still account for evidence of bias from the perspective of an algorithmic audit, even though unrounded ride times and fare prices are unavailable to us. No rider characteristics are reported in order to preserve the privacy of the riders [15]. The dataset contains information about the time, charges, and pickup and dropoff locations for each ride, with location being given in the form of ACS census tracts. ACS census tracts are subdivisions of a county designated by the U.S. Census Bureau consisting of about 1,200-8,000 people, whose boundaries are updated every 10 years. Due to the self-reported nature of the taxi dataset, we find many of the fare prices to be inconsistent with rates charged by Taxi cab companies in Chicago. We detect and remove rides with outlier fare prices using a Gaussian Mixture Model (GMM) [36]. With outliers removed, we are left with data for 10,292,081 taxi rides.\nShared trips, or rides that involve the pickup and dropoff of multiple individuals, are excluded from the ridehailing dataset. These trips were excluded because they contain multiple riders, have multiple pickup and dropoff points as well as a different pricing model [42] than rides with only a single rider. In addition, including them would limit our ability to compare ridehailing services with taxi services, as the taxi services do not list a similar ride sharing option in the provided dataset. After excluding shared rides and removing any rows with missing data, we have at total of ~68 million ridehailing rides.\nWe found many of the fare prices in the Taxi dataset to be inconsistent with rates charged by Taxi cab companies in Chicago. For example, Yellow Cab Chicago charges a base fare of $3.25, with an additional charge of $2.25 per mile [2]. Given that our taxi dataset contains around 127 million rides, representative of approximately 37 million miles worth of driving, assuming all rides were taken using Chicago Yellow Cab, we would expect a total fare price of approximately $125 million. However, our observed overall fare price is closer to $143 million. Many rides cost much more in\nfare per mile than expected given listed cab prices, with over 1000 rides costing more than $1,000.00 per mile, and the maximum fare price per mile being approximately $90,000. We detect and remove rides with outlier fare prices using a Gaussian Mixture Model (GMM) [36]. With outliers removed, we are left with data for 10,292,081 taxi rides.\nGMMs are a probabilistic model which assume data is generated from a mixture of a number of Gaussian distributions. We use GMMs due to their effectiveness in detecting anomalies in prior works [32, 33]. GMMs require the specification of a number, k, of clusters to create. We cluster taxi rides assuming 2 Gaussian distributions, one corresponding to normal rides, and another for anomalous rides, meaning k is set to 2. Using GMMs for outlier detection, we remove about 2.5 million taxi rides from the dataset. After removing rides clustered in the anomalous cluster, our new total observed fare price is approximately $130 million and our new maximum observed fare price per mile is $17, which is more consistent to our expectations based on Yellow Cab rates [2] compared to the original max fare price per mile of about $90,000. With outliers removed, we are left with data for 10292081 taxi rides.\nACS census tract demographic information is retrieved from the United States Census Bureau from the year 2018, the latest year for which ACS census data is currently available. 2018 data was used as this is the closest year to the time frame of the Chicago ridehailing dataset for which ACS census data was available. The average age of ridehailing users as of 2017 is 37 [17]. We split age using a threshold of 40 because age is organized in multi-year brackets in ACS data (0-5 years old, 5-9 years old, etc), and the age 40 is the closest bracket age that can be used to divide the population into those above and those below the average ridehailing user age. A poverty threshold $25,000.00 is chosen based on the poverty line as defined by United States Department of Health and Human Services [14]. The threshold for median house prices is set at $250,000 based on a median house price estimate conducted by Zillow in 2018 [45]."}, {"heading": "B Methods", "text": "When calculating IESB scores, we determine direction using the slope of the magnitudes. This is because we wish to understand how the effect-size changes as we increase the percentage of a particular demographic attribute. For example, in Figure 3, when examining how bias magnitudes change as the percentage of non-white population is increased (first row, third column of Figure 3) we see that as the white population decreases and the non-white population increases, the bias magnitude on fare price increases - so the resulting IESB score is reported as having a positive direction, even though overall, all bias scores have negative values.\nIn Feldman et al. [28], disparate impact is measured using a generalization of the 80% rule as specified by the US Equal Employment Opportunity Commission [21] regarding fair hiring practices as shown in Equation 2. The 80% rule, or the 4/5ths rule is a rule defined by the U.S. Equal Employment Opportunity Commission which states that disparate impact is occurring when one race, sex, or ethnic group is being selected with less than 80% of the selection rate of the group with the highest selection rate [21]. Feldman et al. [28] implement the 80% rule using Equation 2. This equation measures the ratio of positively classified instances with a negative label for binary characteristic X to positively classified instances with a positive label for binary characteristic X . Disparate impact is an expression of unintentional bias [28], and is determined to exist when the value of Equation 2 is a probability less than or equal to 0.8. Disparate impact is a particular threshold for bias tested by Feldman et al. [28]. Our work is concerned with bias in general and not strictly disparate impact, so we do not include the constraint that the outcome must be less than 0.8 for the bias measurement detailed in Feldman et al. [28]. In addition, there are no comparable legally defined thresholds that can be used to delineate a disparate impact threshold for the bias score detailed by Zemel et al. [44] or IESB so instead we focus on the measurement of bias instead. D is the dataset being examined, Y is the binary target outcome variable for which we are testing bias, and binary variable X is the protected attribute which we are testing as the source of bias. Using this test for bias requires that both X and Y be binary categorical variables. Smaller values correspond to a larger amount of bias, and larger values correspond to a smaller amount of bias. In Zemel at al. [44], bias is measured by taking the difference between the probability of being given a positive classification with a positive label for binary characteristic X and the probability of being given a positive classification with a negative label for binary characteristic X as shown in Equation 3. D is the dataset being examined, Y is the binary target outcome variable for which we are testing bias, and binary variable X is the protected\nattribute which we are testing as the source of bias. Using this test, smaller values correspond to a smaller bias, and larger values correspond to a larger bias. This test also requires that X and Y be binary variables.\nFeldman et al. [28]: bias(D) = Pr(Y = Y ES|X = 0) Pr(Y = Y ES|X = 1)\n(2)\nZemel et al. [44]: bias(D) = \u2223\u2223\u2223\u2223\u2223 \u2211 n:xn=1 y\u0302n\u2211 n:xn=1 1 \u2212 \u2211 n:xn=0 y\u0302n\u2211 n:xn=0 1 \u2223\u2223\u2223\u2223\u2223 (3)"}, {"heading": "C Results", "text": "Measuring Social Bias: The metric proposed by Feldman et al. [28] can be any positive rational number between 0 and the total number of members of the dataset being examined, with smaller values corresponding to larger bias, and the metric from Zemel et al. [44] ranges from 0 to 1, with larger values corresponding to larger bias. IESB, as a composition of multiple calculations of Cohen\u2019s d, takes the same range as Cohen\u2019s d, generally appearing between 0 and 2, though magnitudes can get larger because Cohen\u2019s d a measure of standard deviation. Larger bias scores correspond to larger bias in the case of IESB. For the purpose of comparison, all bias scores are scaled between 0-1 in Table 1. IESB scores are consistent with the prior two methods with regard to both datasets. All metrics find more bias in the Ricci dataset than in the German credit dataset.\nThe bias metric defined by Feldman et al. [28], shown in Equation 2 can range between 0 and |D|, the size of the dataset being examined. As shown in Equation 2, the bias being measured is on the group X = 0. Using the reciprocal of the fraction will give us the bias measured on the group X = 1. In order to scale this metric for comparison with IESB as well as the metric defined by Zemel et al. [44], we use either Equation 2 or its reciprocal, depending upon which one is between 0 and 1. In this method, we are measuring general bias of the dataset rather than bias against a single group (X = 0 or X = 1). There are more than 20 different quantitative fairness notions that measure bias and fairness in different ways [35]. As all measurements of bias use different notions of fairness, the same exact scores cannot be expected across measurements. However, the bias scores as calculated by IESB and the bias scores detailed in Feldman et al. [28] and Zemel et al. [44] are in alignment in this case for both the Ricci and German credit datasets.\nRidehailing & Taxi Bias: Extended Results: We calculate the bias score on the fare price per mile for ethnicity, age, education level, citizenship status, and median house price in Chicago. These bias scores for each demographic are shown in Table 6. ACS data contains estimates for demographic statistics by census tract. Additionally, ACS provides a standard error margin for the demographic statistics estimates. In order to account for the possible error, we provide results calculated using the estimate with the error subtracted, as well as the estimate with the error added, the results of which are shown in the \u201cMin Est\u201d and \u201cMax Est\u201d columns in Table 6. Additionally, we calculate the bias score on pickup frequencies for ethnicity, age, education level, citizenship status, and median house price in Chicago, which shows how census demographics may effect neighborhood pickup frequencies for ridehailing applications as well as taxis. These bias scores for each demographic are shown in Table 4. Also we calculate the bias scores on seconds per mile for ethnicity, age, education level, citizenship status, and median house price in Chicago, which shows how the demographics of neighborhoods may relate to how long rides take. These bias scores for each demographic are shown in Table 5. P-values are calculated using the permutation test Pri[IESB(Xi, Yi) > IESB(X,Y )], where Xi and Yi are a randomly selected subset of X and Y , which account for census-wide data for 1159 census tracts for the ridehailing dataset and 820 census tracts for the taxi dataset. Xi and Yi subsets are limited to 500 random census tracts and 10000 iterations of the permutation test are run to calculate p-values. Figure 3 displays all bias magnitudes on fare price/per mile for census tract-wide demographics. This chart represents the magnitudes in IESB scores when their weighted average has not been taken. Figure 4 displays the raw difference in prices in neighborhoods with D values less than or equal to threshold t and greater than threshold t. Figure 3 displays all bias magnitudes on fare price/per mile for census tract-wide demographics. Figure 5 displays all bias magnitudes on ride frequency for census tract-wide demographics. Figure 6 displays all bias magnitudes on ride time per mile for census tract-wide demographics.\nR id\neh ai\nlin g\nPi ck\nup D\nro po ff M in E st E st M ax E st M in E st E\nst M\nax E st A tt ri bu te IE SB p IE SB p IE SB p IE SB p IE SB p IE SB p Pi ck up C ou nt -1 .4 6/ - 0. 00 -1 .4 6/ - 0. 00 -1 .4 6/ - 0. 00 -1 .5 2/ + 0. 00 -1 .5 2/ + 0. 00 -1 .5 2/ +\n0. 00\nD ro\npo ff\nC ou\nnt -1\n.4 8/\n+ 0.\n00 -1\n.4 8/\n+ 0.\n00 -1\n.4 8/\n+ 0.\n00 -1\n.4 9/\n+ 0.\n00 -1\n.4 9/\n+ 0.\n00 -1\n.4 9/\n+ 0. 00 Pi ck up Se c/ M i -1 .4 6/ - 0. 00 -1 .4 6/ - 0. 00 -1 .4 9/ - 0. 00 -1 .5 1/ - 0. 00 -1 .4 9/ - 0. 00 -1 .4 8/ - 0. 00 D ro po ff Se c/ M i -1 .2 2/ - 0. 00 -1 .2 1/ - 0. 00 -1 .2 6/ - 0. 00 -1 .2 5/ - 0. 00 -1 .2 1/ - 0. 00 -1 .2 4/ - 0. 00 N on -W hi te % -0 .1 4/ + 0. 16 -0 .1 9/ + 0. 07 -0 .2 3/ + <1 0 \u2212 3 -0 .3 3/ + 0. 01 4 -0 .3 1/ + <1 0 \u2212 3 -0 .3 5/ + 0. 00 A ge \u2265 40 % 0. 91 /+ 0. 00 0. 72 /- 0. 00 0. 61 /- 0. 00 0. 96 /+ 0. 00 0. 74 /- 0. 00 0. 64 /- 0. 00 H ig h sc ho ol E du ca tio n % 0. 52 /- 0. 00 0. 34 /- <1 0 \u2212 3 0. 27 /- 0. 00 0. 45 /- <1 0\u2212 2 0. 23 /- 0. 01 0. 17 /- 0. 10 B el ow Po ve rt y L in e % -0 .2 1/ - 0. 03 -0 .1 3/ - 0. 21 -0 .1 0/ - 0. 03 -0 .2 2/ - 0. 35 -0 .2 6/ - 0. 01 -0 .2 3/ - 0. 01 N ot a U .S .c iti ze n % 0. 01 /- 0. 90 0. 05 /- 0. 64 -0 .0 6/ - 0. 90 0. 01 /- 0. 55 0. 06 /- 0. 59 -0 .0 2/ - 0. 87 < M ed ia n H ou se Pr ic e % 0. 60 /+ 0. 00 0. 68 /- 0. 00 0. 52 /+ 0. 00 0. 56 /+ 0. 00 0. 70 /- 0. 00 0. 46 /+ 0. 00 Ta xi Pi ck up D ro po ff M in E st E st M ax E st M in E st E st M ax E st A tt ri bu te IE SB p IE SB p IE SB p IE SB p IE SB p IE SB p Pi ck up C ou nt -0 .0 3/ - 0. 78 -0 .0 3/ - 0. 79 -0 .0 3/ - 0. 78 -0 .9 5/ - 0. 00 -0 .9 5/ - 0. 00 -0 .9 5/ - 0. 00 D ro po ff C ou nt -0 .2 9/ - 0. 01 -0 .2 9/ - 0. 01 -0 .2 9/ - 0. 01 -0 .8 6/ - 0. 00 -0 .8 6/ - 0. 00 -0 .8 6/ - 0. 00 Pi ck up Se c/ M i -0 .2 4/ - 0. 03 -0 .0 7/ - 0. 52 -0 .2 1/ - 0. 06 -0 .9 2/ - 0. 00 -0 .8 9/ - 0. 00 -0 .8 5/ - 0. 00 D ro po ff Se c/ M i -0 .3 4/ + <1 0\u2212 3 -0 .1 3/ - 0. 22 -0 .4 0/ + 0. 00 -0 .7 6/ - 0. 00 -0 .8 1/ - 0. 00 -0 .7 4/ - 0. 00 N on -W hi te % 0. 02 /+ 0. 85 0. 16 /+ 0. 14 0. 17 /+ 0. 12 0. 08 /+ 0. 40 -0 .1 9/ + 0. 04 -0 .1 0/ + 0. 29 A ge \u2265 40 % -0 .1 5/ - 0. 18 -0 .1 0/ - 0. 36 -0 .0 4/ + 0. 70 0. 62 /- 0. 00 0. 31 /- 0. 00 0. 26 /- <1 0 \u2212 2 H ig h sc ho ol E du ca tio n % 0. 05 /+ 0. 66 0. 02 /+ 0. 84 0. 01 /+ 0. 89 0. 53 /- 0. 00 0. 23 /- 0. 01 0. 24 /- 0. 01 B el ow Po ve rt y L in e % 0. 12 /+ 0. 28 0. 03 /+ 0. 80 0. 06 /+ 0. 62 0. 03 /- 0. 74 -0 .2 4/ - 0. 01 -0 .1 7/ - 0. 07 N ot a U .S .c iti ze n % -0 .1 1/ + 0. 31 -0 .1 4/ + 0. 22 -0 .1 8/ + 0. 10 0. 05 /+ 0. 59 -0 .1 5/ + 0. 11 -0 .2 3/ + 0. 01 < M ed ia n H ou se Pr ic e % 0. 03 /- 0. 77 0. 07 /+ 0. 50 0. 08 /+ 0. 49 0. 55 /- 0. 00 0. 30 /- <1 0 \u2212 3 0. 29 /- <1 0 \u2212 2\nTa bl\ne 6:\nB ia\ns sc\nor es\non fa\nre pr\nic e\npe rm\nile by\nne ig\nhb or\nho od\nat tri\nbu te\ns -B\nia s\nsc or\nes an\nd th\nei rr\nes pe\nct iv\ne bi\nas di\nre ct\nio n\nar e\nsh ow\nn fo\nrt he\nfa re\npr ic\ne pe\nrm ile\ngi ve\nn a\nse t\nof ne\nig hb\nor ho\nod at\ntr ib\nut es\n.\u201c Pi\nck up\n\u201d an\nd \u201cD\nro po\nff \u201d\nco lu\nm ns\nde si\ngn at\ne fa\nre pr\nic e\npe rm\nile w\nhe n\nbe in\ng pi\nck ed\nup or\ndr op\npe d\nof in\na ne\nig hb\nor ho\nod an\nd \u201cp\n\u201d pr\nes en ts th e pva lu e fo rI ES B ca lc ul at io ns .A C S da ta co nt ai ns es tim at es fo rd em og ra ph ic st at is tic s by ce ns us tra ct .A dd iti on al ly ,A C S pr ov id es a st an da rd er ro rm ar gi n fo rt he de m og ra ph ic st at is tic s es tim at es .R es ul ts ca lc ul at ed us in g th e es tim at e w ith th e er ro rs ub tr ac te d, as w el la s th e es tim at e w ith th e er ro ra dd ed ,t he re su lts of w hi ch ar e sh ow n in th e \u201cM in E st \u201d an d \u201cM ax E st \u201d co lu m ns ."}], "title": "Iterative Effect-Size Bias in Ridehailing: Measuring Social Bias in Dynamic Pricing of 100 Million Rides", "year": 2020}