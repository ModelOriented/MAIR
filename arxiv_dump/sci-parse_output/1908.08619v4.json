{
  "abstractText": "Given a data set D containing millions of data points and a data consumer who is willing to pay for $X to train a machine learning (ML) model over D, how should we distribute this $X to each data point to reflect its \u201cvalue\u201d? In this paper, we define the \u201crelative value of data\u201d via the Shapley value, as it uniquely possesses properties with appealing real-world interpretations, such as fairness, rationality and decentralizability. For general, bounded utility functions, the Shapley value is known to be challenging to compute: to get Shapley values for all N data points, it requires O(2) model evaluations for exact computation and O(N logN) for ( , \u03b4)-approximation. In this paper, we focus on one popular family of ML models relying on K-nearest neighbors (KNN). The most surprising result is that for unweighted KNN classifiers and regressors, the Shapley value of all N data points can be computed, exactly, in O(N logN) time \u2013 an exponential improvement on computational complexity! Moreover, for ( , \u03b4)-approximation, we are able to develop an algorithm based on Locality Sensitive Hashing (LSH) with only sublinear complexity O(N ,K) logN) when is not too small and K is not too large. We empirically evaluate our algorithms on up to 10 million data points and even our exact algorithm is up to three orders of magnitude faster than the baseline approximation algorithm. The LSH-based approximation algorithm can accelerate the value calculation process even further. We then extend our algorithms to other scenarios such as (1) weighed KNN classifiers, (2) different data points are clustered by different data curators, and (3) there are data analysts providing computation who also requires proper valuation. Some of these extensions, although also being improved exponentially, are less practical for exact computation (e.g., O(N) complexity for weighted KNN). We thus propose a Monte Carlo approximation algorithm, which is O(N(logN)/(logK)) times more efficient than the baseline approximation algorithm. 1 ar X iv :1 90 8. 08 61 9v 4 [ cs .L G ] 2 9 M ar 2 02 0",
  "authors": [
    {
      "affiliations": [],
      "name": "RUOXI JIA"
    },
    {
      "affiliations": [],
      "name": "DAVID DAO"
    },
    {
      "affiliations": [],
      "name": "BOXIN WANG"
    },
    {
      "affiliations": [],
      "name": "FRANCES ANN HUBIS"
    },
    {
      "affiliations": [],
      "name": "NEZIHE MERVE GUREL"
    },
    {
      "affiliations": [],
      "name": "BO LI"
    },
    {
      "affiliations": [],
      "name": "CE ZHANG"
    },
    {
      "affiliations": [],
      "name": "COSTAS J. SPANOS"
    },
    {
      "affiliations": [],
      "name": "DAWN SONG"
    }
  ],
  "id": "SP:85b7674f6db454b55daea50db7860fa1927b3bd3",
  "references": [
    {
      "authors": [
        "G. AMATO",
        "F. FALCHI",
        "C. GENNARO"
      ],
      "title": "RABITTI, Yfcc100m-hnfc6: a large-scale deep features benchmark for similarity search",
      "venue": "in International Conference on Similarity Search and Applications,",
      "year": 2016
    },
    {
      "authors": [
        "E.J.J. BARTHOLDI"
      ],
      "title": "KEMAHLIO\u011eLU-ZIYA, Using shapley value to allocate savings in a supply chain, in Supply chain",
      "year": 2005
    },
    {
      "authors": [
        "M.J. BREMER"
      ],
      "title": "SONNENSCHEIN, Estimating shapley values for fair profit distribution in power planning smart grid coalitions, in German",
      "venue": "Conference on Multiagent System Technologies,",
      "year": 2013
    },
    {
      "authors": [
        "S. M"
      ],
      "title": "CHARIKAR, Similarity estimation techniques from rounding algorithms",
      "venue": "Proceedings of the thiry-fourth annual ACM symposium on Theory of computing,",
      "year": 2002
    },
    {
      "authors": [
        "L. CHEN",
        "P. KOUTRIS",
        "A. KUMAR"
      ],
      "title": "Model-based pricing for machine learning in a data marketplace, arXiv preprint",
      "year": 2018
    },
    {
      "authors": [
        "P.M. CHESSA"
      ],
      "title": "LOISEAU, A cooperative game-theoretic approach to quantify the value of personal data in networks",
      "venue": "Proceedings of the 12th workshop on the Economics of Networks, Systems and Computation,",
      "year": 2017
    },
    {
      "authors": [
        "D. DAO",
        "D. ALISTARH",
        "C. MUSAT",
        "C. ZHANG"
      ],
      "title": "Databright: Towards a global exchange for decentralized data ownership and trusted computation, arXiv preprint arXiv:1802.04780 (2018)",
      "year": 2018
    },
    {
      "authors": [
        "M. DATAR",
        "N. IMMORLICA",
        "P. INDYK",
        "V. S"
      ],
      "title": "MIRROKNI, Locality-sensitive hashing scheme based on p-stable distributions",
      "venue": "Proceedings of the twentieth annual symposium on Computational geometry,",
      "year": 2004
    },
    {
      "authors": [
        "S. DEEP",
        "P. KOUTRIS"
      ],
      "title": "BIDASARIA, Qirana demonstration: real time scalable query pricing, PVLDB",
      "year": 2017
    },
    {
      "authors": [
        "J. DENG",
        "W. DONG",
        "R. SOCHER",
        "L.-J. LI",
        "K. LI"
      ],
      "title": "FEI-FEI, ImageNet: A Large-Scale Hierarchical Image Database",
      "year": 2009
    },
    {
      "authors": [
        "C.H.X. DENG"
      ],
      "title": "PAPADIMITRIOU, On the complexity of cooperative solution concepts",
      "venue": "Mathematics of Operations Research",
      "year": 1994
    },
    {
      "authors": [
        "A. S"
      ],
      "title": "DUDANI, The distance-weighted k-nearest-neighbor rule",
      "venue": "IEEE Transactions on Systems, Man, and Cybernetics",
      "year": 1976
    },
    {
      "authors": [
        "S.S. FATIMA",
        "M. WOOLDRIDGE",
        "N. R"
      ],
      "title": "JENNINGS, A linear approximation method for the shapley value",
      "venue": "Artificial Intelligence",
      "year": 2008
    },
    {
      "authors": [
        "A. GHORBANI",
        "J. ZOU"
      ],
      "title": "Data shapley: Equitable valuation of data for machine learning, arXiv preprint arXiv:1904.02868 (2019)",
      "year": 2019
    },
    {
      "authors": [
        "A. GIONIS",
        "R.P. INDYK"
      ],
      "title": "MOTWANI, and OTHERS, Similarity search in high dimensions via hashing, in Vldb",
      "year": 1999
    },
    {
      "authors": [
        "S. HAR-PELED",
        "P. INDYK"
      ],
      "title": "MOTWANI, Approximate nearest neighbor: Towards removing the curse of dimensionality, Theory of computing",
      "year": 2012
    },
    {
      "authors": [
        "M. HARDT",
        "N.E. PRICE"
      ],
      "title": "SREBRO, and OTHERS, Equality of opportunity in supervised learning, in Advances in neural information processing",
      "year": 2016
    },
    {
      "authors": [
        "J. HE",
        "S. KUMAR",
        "S.-F. CHANG"
      ],
      "title": "On the difficulty of nearest neighbor search",
      "venue": "Proceedings of the 29th International Coference on International Conference on Machine Learning, Omnipress,",
      "year": 2012
    },
    {
      "authors": [
        "HE K",
        "ZHANG X",
        "REN S",
        "SUN J"
      ],
      "title": "Deep residual learning for image recognition",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition,",
      "year": 2016
    },
    {
      "authors": [
        "N. HYNES",
        "D. DAO",
        "D. YAN",
        "R. CHENG",
        "D. SONG"
      ],
      "title": "A demonstration of sterling: a privacypreserving data marketplace",
      "venue": "PVLDB",
      "year": 2018
    },
    {
      "authors": [
        "P.W. KOH",
        "P. LIANG"
      ],
      "title": "Understanding black-box predictions via influence",
      "venue": "functions, in International Conference on Machine Learning,",
      "year": 2017
    },
    {
      "authors": [
        "P. KOUTRIS",
        "P. UPADHYAYA",
        "M. BALAZINSKA",
        "B. HOWE",
        "D. SUCIU"
      ],
      "title": "Querymarket demonstration: Pricing for online data markets, PVLDB",
      "year": 2012
    },
    {
      "authors": [
        "P. KOUTRIS",
        "P. UPADHYAYA",
        "M. BALAZINSKA",
        "B. HOWE",
        "D. SUCIU"
      ],
      "title": "Query-based data pricing",
      "venue": "Journal of the ACM (JACM)",
      "year": 2015
    },
    {
      "authors": [
        "C. LI",
        "D.Y. LI",
        "G. MIKLAU",
        "D. SUCIU"
      ],
      "title": "A theory of pricing private data",
      "venue": "Communications of the ACM",
      "year": 2017
    },
    {
      "authors": [
        "LI G. C"
      ],
      "title": "MIKLAU, Pricing aggregate queries in a data marketplace",
      "venue": "WebDB,",
      "year": 2012
    },
    {
      "authors": [
        "C. LI",
        "S. ZHANG",
        "H. ZHANG",
        "L. PANG",
        "K. LAM",
        "C. HUI"
      ],
      "title": "Using the k-nearest neighbor algorithm for the classification of lymph node metastasis in gastric cancer, Computational and mathematical methods in medicine",
      "year": 2012
    },
    {
      "authors": [
        "S. MALEKI"
      ],
      "title": "Addressing the computational issues of the Shapley value with applications in the smart grid",
      "venue": "Ph.D. thesis, University of Southampton,",
      "year": 2015
    },
    {
      "authors": [
        "S.D.M. MOUNT"
      ],
      "title": "ARYA, Ann: library for approximate nearest neighbour searching",
      "year": 1998
    },
    {
      "authors": [
        "K. OGAWA",
        "Y. SUZUKI",
        "I. TAKEUCHI"
      ],
      "title": "Safe screening of non-support vectors in pathwise svm computation",
      "venue": "in International Conference on Machine Learning,",
      "year": 2013
    },
    {
      "authors": [
        "R. RASKAR",
        "P. VEPAKOMMA",
        "T. SWEDISH"
      ],
      "title": "SHARAN, Data markets to support ai for all: Pricing, valuation and governance",
      "year": 1905
    },
    {
      "authors": [
        "S. L"
      ],
      "title": "SHAPLEY, A value for n-person games, Contributions to the Theory of Games",
      "year": 1953
    },
    {
      "authors": [
        "C. SZEGEDY",
        "V. VANHOUCKE",
        "S. IOFFE",
        "J. SHLENS"
      ],
      "title": "WOJNA, Rethinking the inception architecture for computer vision",
      "venue": "Proceedings of the IEEE conference on computer vision and pattern recognition,",
      "year": 2016
    },
    {
      "authors": [
        "F. TOPSOK"
      ],
      "title": "Some bounds for the logarithmic function, Inequality theory and applications",
      "year": 2006
    },
    {
      "authors": [
        "P. UPADHYAYA",
        "M. BALAZINSKA",
        "D. SUCIU"
      ],
      "title": "Price-optimal querying with data apis",
      "venue": "PVLDB",
      "year": 2016
    },
    {
      "authors": [
        "Z. ZHENG",
        "Y. PENG",
        "F. WU",
        "S. TANG",
        "G. CHEN"
      ],
      "title": "Arete: On designing joint online pricing and reward sharing mechanisms for mobile data markets",
      "venue": "IEEE Transactions on Mobile Computing",
      "year": 2019
    }
  ],
  "sections": [
    {
      "text": "In this paper, we focus on one popular family of ML models relying on K-nearest neighbors (KNN). The most surprising result is that for unweighted KNN classifiers and regressors, the Shapley value of all N data points can be computed, exactly, in O(N logN) time \u2013 an exponential improvement on computational complexity! Moreover, for ( , \u03b4)-approximation, we are able to develop an algorithm based on Locality Sensitive Hashing (LSH) with only sublinear complexity O(Nh( ,K) logN) when is not too small and K is not too large. We empirically evaluate our algorithms on up to 10 million data points and even our exact algorithm is up to three orders of magnitude faster than the baseline approximation algorithm. The LSH-based approximation algorithm can accelerate the value calculation process even further.\nWe then extend our algorithms to other scenarios such as (1) weighed KNN classifiers, (2) different data points are clustered by different data curators, and (3) there are data analysts providing computation who also requires proper valuation. Some of these extensions, although also being improved exponentially, are less practical for exact computation (e.g., O(NK) complexity for weighted KNN). We thus propose a Monte Carlo approximation algorithm, which is O(N(logN)2/(logK)2) times more efficient than the baseline approximation algorithm.\nar X\niv :1\n90 8.\n08 61\n9v 4\n[ cs\n.L G\n] 2\n9 M\nar 2\n02 0\nContents\n1. Introduction 3 2. Preliminaries 7 2.1. Data Valuation based on the SV 7 2.2. A Baseline Algorithm 8 3. Valuing Data for KNN Classifiers 9 3.1. Exact SV Calculation 9 3.2. LSH-based Approximation 12 4. Extensions 13 5. Improved MC Approximation 16 6. Experiments 17 6.1. Experimental Setup 17 6.2. Experimental Results 18 7. Discussion 26 8. Related Work 29 9. Conclusion 30 Acknowledgement 31 References 31 Appendix A. Additional Experiments 34 A.1. Runtime Comparision for Computing the Unweighted KNN SV 34 Appendix B. Proof of Lemma 1 34 Appendix C. Proof of Theorem 2 35 Appendix D. Proof of Theorem 3 36 Appendix E. Detailed Algorithms and Proofs for the Extensions 37 E.1. Unweighted KNN Regression 37 E.2. Weighted KNN 41 E.3. Multiple Data Per Contributor 43 E.4. Valuing Computation 43 Appendix F. Generalization to Piecewise Utility Difference 45 Appendix G. Proof of Theorem 5 46 Appendix H. Derivation of the Approximate Lower Bound on Sample Complexity for the\nImproved MC Approximation 49"
    },
    {
      "heading": "1. Introduction",
      "text": "\u201cData is the new oil\u201d \u2014 large-scale, high-quality datasets are an enabler for business and scientific discovery and recent years have witnessed the commoditization of data. In fact, there are not only marketplaces providing access to data, e.g., IOTA [IOT], DAWEX [DAW], Xignite [xig], but also marketplaces charging for running (relational) queries over the data, e.g., Google BigQuery [BIG]. Many researchers start to envision marketplaces for ML models [CKK18]. Data commoditization is highly likely to continue and not surprisingly, it starts to attract interests from the database community. One series of seminal work is conducted by Koutris et al. [KUB+15,KUB+13] who systematically studied the theory and practice of \u201cquery pricing,\u201d the problem of attaching value to running relational queries over data. Recently, Chen et al. [CKK18, CKK17] discussed \u201cmodel pricing\u201d, the problem of valuing ML models. This paper is inspired by the prior work on query and model pricing, but focuses on a different scenario. In many real-world applications, the datasets that support queries and ML are often contributed by multiple individuals. One example is that complex ML tasks such as chatbot training often relies on massive crowdsourcing efforts. A critical challenge for building a data marketplace is thus to allocate the revenue generated from queries and ML models fairly between different data contributors. In this paper, we ask: How can we attach value to every single data point in relative terms, with respect to a specific ML model trained over the whole dataset? Apart from being inspired by recent research, this paper is also motivated by our current effort in building a data market based on privacy-preserving machine learning [HDY+18,DAMZ18] and an ongoing clinical trial at the Stanford Hospital, as illustrated in Figure 1. In this clinical trial, each patient uploads their encrypted medical record (one \u201cdata point\u201d) onto a blockchain-backed data store. A \u201cdata consumer\u201d, or \u201cbuyer\u201d, chooses a subset of patients (selected according to some non-sensitive information that is not encrypted) and trains a ML model. The buyer pays a certain amount of money that will be distributed back to each patient. In this paper, we focus on the data valuation problem that is abstracted from this real use case and propose novel, practical algorithms for this problem.\nSpecifically, we focus on the Shapley value (SV), arguably one of the most popular way of revenue sharing. It has been applied to various applications, such as power grids [BS13], supply chains [BKZ05], cloud computing [UBS12], among others. The reason for its wide adoption is that the SV defines a unique profit allocation scheme that satisfies a set of appealing properties, such as fairness, rationality, and decentralizability. Specifically, let D = {z1, ..., zN} be N data points and \u03bd(\u00b7) be the \u201cutility\u201d of the ML model trained over a subset of the data points; the SV of a given data point zi is\n(1) si = 1\nN \u2211 S\u2286D\\zi 1( N\u22121 |S| )[\u03bd(S \u222a {zi}) \u2212 \u03bd(S)] Intuitively, the SV measures the marginal improvement of utility attributed to the data point zi,\naveraged over all possible subsets of data points. Calculating exact SVs requires exponentially many utility evaluations. This poses a radical challenge to using the SV for data valuation\u2013how can we compute the SV efficiently and scale to millions or even billions of data points? This scale is rare to the previous applications of the SV but is not uncommon for real-world data valuation tasks. To tackle this challenge, we focus on a specific family of ML models which restrict the class of utility functions \u03bd(\u00b7) that we consider. Specifically, we study K-nearest neighbors (KNN) classifiers [Dud76], a simple yet popular supervised learning method used in image recognition [HE15], recommendation systems [AWY16], healthcare [LZZ+12], etc. Given a test set, we focus on a natural utility function, called the KNN utility, which, intuitively, measures the boost of the likelihood that KNN assigns the correct label to each test data point. When K = 1, this utility is the same as the test accuracy. Although some of our techniques also apply to a broader class of utility functions (See Section 4), the KNN utility is our main focus. The contribution of this work is a collection of novel algorithms for efficient data valuation within the above scope. Figure 2 summarizes our technical results. Specifically, we made four technical contributions: Contribution 1: Data Valuation for KNN Classifiers. The main challenge of adopting the SV for data valuation is its computational complexity \u2014 for general, bounded utility functions, calculating the SV requires O(2N) utility evaluations for N data points. Even getting an ( , \u03b4)-approximation (error bounded by with probability at least 1 \u2212 \u03b4) for all data points requires O(N logN) utility evaluations using state-of-the-art methods (See Section 2.2). For the KNN utility, each utility evaluation requires to sort the training data, which has asymptotic complexity O(N logN).\nC1.1 Exact Computation We first propose a novel algorithm specifically designed for KNN classifiers. We observe that the KNN utility satisfies what we call the piecewise utility difference property: the difference in the marginal contribution of two data points zi and zj over has a \u201cpiecewise form\u201d (See Section 3.1):\nU(S \u222a {zi}) \u2212U(S \u222a {zj}) = T\u2211 t=1 C (t) i,j 1[S \u2208 St], \u2200S \u2208 D\\{zi, zj}\nwhere St \u2286 2D\\{zi,zj} and C(t)i,j \u2208 R. This combinatorial structure allows us to design a very efficient algorithm that only has O(N logN) complexity for exact computation of SVs on all N data points. This is an exponential improvement over the O(2NN logN) baseline! C1.2 Sublinear Approximation The exact computation requires to sort the entire training set for each test point, thus becoming time-consuming for large and high-dimensional datasets. Moreover, in some applications such as document retrieval, test points could arrive sequentially and the values of each training point needs to get updated and accumulated on the fly, which makes it impossible to complete sorting offline. Thus, we investigate whether higher efficiency can be achieved by finding approximate SVs instead. We study the problem of getting ( , \u03b4)-approximation of the SVs for the KNN utility. This happens to be reducible to the problem of answering approximate max{K, 1/ }- nearest neighbor queries with probability 1\u2212 \u03b4. We designed a novel algorithm by taking advantage of LSH, which only requires O(Nh( ,K) logN) computation where h( , K) is dataset-dependent and typically less than 1 when is not too small and K is not too large. Limitation of LSH The h( , K) term monotonically increases with max{1 , K}. In experiments, we found that the LSH can handle mild error requirements (e.g., = 0.1) but appears to be less efficient than the exact calculation algorithm for stringent error requirements. Moreover, we can extend the exact algorithm to cope with KNN regressors and other scenarios detailed in Contribution 2; however, the application of the LSH-based approximation is still confined to the classification case. To our best knowledge, the above results are one of the very first studies of efficient SV evaluation designed specifically for utilities arising from ML applications. Contribution 2: Extensions. Our second contribution is to extend our results to different settings beyond a standard KNN classifier and the KNN utility (Section 4). Specifically, we studied: C2.1 Unweighted KNN regressors. C2.2 Weighted KNN classifiers and regressors. C2.3 One \u201cdata curator\u201d contributes multiple data points and has the freedom to delete all data points at the same time. C2.4 One \u201cdata analyst\u201d provides ML analytics and the system attaches value to both the analyst and data curators.\nThe connection between different settings are illustrated in Figure 3, where each vertical layer represents a different slicing to the data valuation problem. In some of these scenarios, we successfully designed algorithms that are as efficient as the one for KNN classifiers. In some other cases, including weigthed KNN and the multiple-data-per-curator setup, the exact computation algorithm is less practical although being improved exponentially. Contribution 3: Improved Monte Carlo Approximation for KNN. To further improve the efficiency in the less efficient cases, we strengthen the sample complexity bound of the state-of-the-art\napproximation algorithm, achieving an O(N log2N/ log2 K) complexity improvement over the stateof-the-art. Our algorithm requires in total O(N/ 2 log2 K) computation and is often practical for reasonable . Contribution 4: Implementation and Evaluation. We implement our algorithms and evaluate them on datasets up to ten million data points. We observe that our exact SV calculation algorithm can provide up to three orders of magnitude speed-up over the state-of-the-art Monte Carlo approximation approach. With the LSH-based approximation method, we can accelerate the SV calculation even further by allowing approximation errors. The actual performance improvement of the LSHbased method over the exact algorithm depends the dataset as well as the error requirements. For instance, on a 10M subset of the Yahoo Flickr Creative Commons 100M dataset, we observe that the LSH-based method can bring another 4.6\u00d7 speed-up. Moreover, to our best knowledge, this work is also one of the first papers to evaluate data valuation at scale. We make our datasets publicly available and document our evaluation methodology in details, with the hope to facilitate future research on data valuation. Relationship with Our Previous Work. Unlike this work which focuses on KNN, our previous work [JDW+19] considered some generic properties of ML models, such as boundedness of the utility functions, stability of the learning algorithms, etc, and studied their implications for computing the SV. Also, the algorithms presented in our previous work only produce approximation to the SV. When the desired approximation error is small, these algorithms may still incur considerable computational costs, thus not able to scale up to large datasets. In contrast, this paper presents a scalable algorithm that can calculate the exact SV for KNN. The rest of this paper is organized as follows. We provide background information in Section 2, and present our efficient algorithms for KNN classifiers in Section 3. We discuss the extensions in Section 4 and propose a Monte Carlo approximation algorithm in Section 5, which significantly boosts the efficiency for the extensions that have less practical exact algorithms. We evaluate our approach in Section 6. We discuss the integration with real-world applications in Section 7 and present a survey of related work in Section 8."
    },
    {
      "heading": "2. Preliminaries",
      "text": "We present the setup of the data marketplace and introduce the framework for data valuation based on the SV. We then discuss a baseline algorithm to compute the SV.\n2.1. Data Valuation based on the SV. We consider two types of agents that interact in a data marketplace: the sellers (or data curators) and the buyer. Sellers provide training data instances, each of which is a pair of a feature vector and the corresponding label. The buyer is interested in analyzing the training dataset aggregated from various sellers and producing an ML model, which can predict the labels for unseen features. The buyer pays a certain amount of money which depends on the utility of the ML model. Our goal is to distribute the payment fairly between the sellers. A natural way to tackle the question of revenue allocation is to view ML as a cooperative game and model each seller as a player. This game-theoretic viewpoint allows us to formally characterize the \u201cpower\u201d of each seller and in turn determine their deserved share of the revenue. For ease of exposition, we assume that each seller contributes one data instance in the training set; later in Section 4, we will discuss the extension to the case where a seller contributes multiple data instances. Cooperative game theory studies the behaviors of coalitions formed by game players. Formally, a cooperative game is defined by a pair (I, \u03bd), where I = {1, . . . ,N} denotes the set of all players and \u03bd : 2N \u2192 R is the utility function, which maps each possible coalition to a real number that describes the utility of a coalition, i.e., how much collective payoff a set of players can gain by forming the coalition. One of the fundamental questions in cooperative game theory is to characterize how important each player is to the overall cooperation. The SV [Sha53] is a classic method to distribute the total gains generated by the coalition of all players. The SV of player i with respect to the utility function \u03bd is defined as the average marginal contribution of i to coalition S over all S \u2286 I \\ {i}:\ns(\u03bd, i) = 1\nN \u2211 S\u2286I\\{i} 1( N\u22121 |S| )[\u03bd(S \u222a {i}) \u2212 \u03bd(S)](2) We suppress the dependency on \u03bd when the utility is self-evident and use si to represent the value allocated to player i.\nThe formula in (2) can also be stated in the equivalent form:\nsi = 1\nN! \u2211 \u03c0\u2208\u03a0(I) [ \u03bd(P\u03c0i \u222a {i}) \u2212 \u03bd(P\u03c0i ) ] (3)\nwhere \u03c0 \u2208 \u03a0(I) is a permutation of players and P\u03c0i is the set of players which precede player i in \u03c0. Intuitively, imagine all players join a coalition in a random order, and that every player i who has joined receives the marginal contribution that his participation would bring to those already in the coalition. To calculate si, we average these contributions over all the possible orders. Transforming these game theory concepts to data valuation, one can think of the players as training data instances and the utility function \u03bd(S) as a performance measure of the model trained on the set of training data S. The SV of each training point thus measures its importance to learning a performant ML model. The following desirable properties that the SV uniquely possesses motivate us to adopt it for data valuation.\ni Group Rationality: The value of the entire training dataset is completely distributed among all sellers, i.e., \u03bd(I) = \u2211 i\u2208I si.\nii Fairness: (1) Two sellers who are identical with respect to what they contribute to a dataset\u2019s utility should have the same value. That is, if seller i and j are equivalent in the sense that \u03bd(S \u222a {i}) = \u03bd(S \u222a {j}), \u2200S \u2286 I \\ {i, j}, then si = sj. (2) Sellers with zero marginal contributions to all subsets of the dataset receive zero payoff, i.e., si = 0 if \u03bd(S \u222a {i}) = 0 for all S \u2286 I \\ {i}. iii Additivity: The values under multiple utilities sum up to the value under a utility that is the sum of all these utilities: s(\u03bd1, i) + s(\u03bd2, i) = s(\u03bd1 + \u03bd2, i) for i \u2208 I.\nThe group rationality property states that any rational group of sellers would expect to distribute the full yield of their coalition. The fairness property requires that the names of the sellers play no role in determining the value, which should be sensitive only to how the utility function responds to the presence of a seller. The additivity property facilitates efficient value calculation when the ML model is used for multiple applications, each of which is associated with a specific utility function. With additivity, one can decompose a given utility function into an arbitrary sum of utility functions and compute value shares separately, resulting in transparency and decentralizability. The fact that the SV is the only value division scheme that meets these desirable criteria, combined with its flexibility to support different utility functions, leads us to employ the SV to attribute the total gains generated from a dataset to each seller. In addition to its theoretical soundness, our previous work [JDW+19] empirically demonstrated that the SV also coincides with people\u2019s intuition of data value. For instance, noisy images tend to have lower SVs than the high-fidelity ones; the training data whose distribution is closer to the test data distribution tends to have higher SVs. These empirical results further back up the use of the SV for data valuation. For more details, we refer the readers to [JDW+19].\n2.2. A Baseline Algorithm. One challenge of applying SV is its computational complexity. Evaluating the exact SV using Eq. (2) involves computing the marginal utility of every user to every coalition, which is O(2N). Such exponential computation is clearly impractical for valuating a large number of training points. Even worse, in many ML tasks, evaluating the utility function per se (e.g., testing accuracy) is computationally expensive as it requires training a ML model. For large datasets, the only feasible approach currently in the literature is Monte Carlo (MC) sampling [Mal15]. In this paper, we will use it as a baseline for evaluation. The central idea behind the baseline algorithm is to regard the SV definition in (3) as the expectation of a training instance\u2019s marginal contribution over a random permutation and then use the sample mean to approximate it. More specifically, let \u03c0 be a random permutation of I and each permutation has a probability of 1/N!. Consider the random variable \u03c6i = \u03bd(P\u03c0i \u222a {i}) \u2212 \u03bd(P\u03c0i ). By (3), the SV si is equal to E[\u03c6i]. Thus,\ns\u0302i = 1\nT T\u2211 t=1 \u03bd(P\u03c0ti \u222a {i}) \u2212 \u03bd(P \u03c0t i )(4)\nis a consistent estimator of si, where \u03c0t be tth sample permutation uniformly drawn from all possible permutations \u03a0(I). We say that s\u0302 \u2208 RN is an ( , \u03b4)-approximation to the true SV s = [s1, \u00b7 \u00b7 \u00b7 , sN]T \u2208 RN if P[maxi |s\u0302i \u2212 si| 6 ] > 1\u2212 \u03b4. Let r be the range of utility differences \u03c6i. By applying the Hoeffding\u2019s inequality, [MTTH+13] shows that for general, bounded utility functions, the number of permutations T needed to achieve an ( , \u03b4)-approximation is r 2\n2 2 log 2N\u03b4 . For each permutation, the baseline\nalgorithm evaluates the utility function for N times in order to compute the SV for N training\ninstances; therefore, the total utility evaluations involved in the baseline approach is O(N logN). In general, evaluating \u03bd(S) in the ML context requires to re-train the model on the subset S of the training data. Therefore, despite its improvements over the exact SV calculation, the baseline algorithm is not efficient for large datasets. Take the KNN classifier as an example and assume that \u03bd(\u00b7) represents the testing accuracy of the classifier. Then, evaluating \u03bd(S) needs to sort the training data in S according to their distances to the test point, which has O(|S| log |S|) complexity. Since on average |S| = N/2, the asymptotic complexity of calculating the SV for a KNN classifier via the baseline algorithm is O(N2 log2N), which is prohibitive for large-scale datasets. In the sequel, we will show that it is indeed possible to develop much more efficient algorithms to compute the SV by leveraging the locality of KNN models."
    },
    {
      "heading": "3. Valuing Data for KNN Classifiers",
      "text": "In this section, we present an algorithm that can calculate the exact SV for KNN classifiers in quasi-linear time. Further, we exhibit an approximate algorithm based on LSH that could achieve sublinear complexity.\n3.1. Exact SV Calculation. KNN algorithms are popular supervised learning methods, widely adopted in a multitude of applications such as computer vision, information retrieval, etc. Suppose the dataset D consisting of pairs (x1, y1), (x2, y2), . . ., (xN, yN) taking values in X\u00d7 Y, where X is the feature space and Y is the label space. Depending on whether the nearest neighbor algorithm is used for classification or regression, Y is either discrete or continuous. The training phase of KNN consists only of storing the features and labels in D. The testing phase is aimed at finding the label for a given query (or test) feature. This is done by searching for the K training features most similar to the query feature and assigning a label to the query according to the labels of its K nearest neighbors. Given a single testing point xtest with the label ytest, the simplest, unweighted version of a KNN classifier first finds the top-K training points (x\u03b11 , \u00b7 \u00b7 \u00b7 , x\u03b1K) that are most similar to xtest and outputs the probability of xtest taking the label ytest as P[xtest \u2192 ytest] = 1K \u2211K k=1 1[y\u03b1k = ytest], where \u03b1k is the index of the kth nearest neighbor. One natural way to define the utility of a KNN classifier is by the likelihood of the right label:\n\u03bd(S) = 1\nK min{K,|S|}\u2211 k=1 1[y\u03b1k(S) = ytest](5)\nwhere \u03b1k(S) represents the index of the training feature that is kth closest to xtest among the training examples in S. Specifically, \u03b1k(I) is abbreviated to \u03b1k.\nUsing this utility function, we can derive an efficient, but exact way of computing the SV.\nTHEOREM 1. Consider the utility function in (5). Then, the SV of each training point can be calculated recursively as follows:\ns\u03b1N = 1[y\u03b1N = ytest]\nN (6)\ns\u03b1i = s\u03b1i+1+ 1[y\u03b1i = ytest] \u2212 1[y\u03b1i+1 = ytest]\nK\nmin{K, i} i (7)\nNote that the above result for a single test point can be readily extended to the multiple-testpoint case, in which the utility function is defined by\n\u03bd(S) = 1\nNtest Ntest\u2211 j=1 1 K min{K,|S|}\u2211 k=1 1[y \u03b1 (j) k (S) = ytest,j](8)\nwhere \u03b1(j)k (S) is the index of the kth nearest neighbor in S to xtest,j. By the additivity property, the SV for multiple test points is the average of the SV for every single test point. The pseudo-code for calculating the SV for an unweighted KNN classifier is presented in Algorithm 1. The computational complexity is only O(N logNNtest) for N training data points and Ntest test data points\u2014this is simply to sort Ntest arrays of N numbers!\nAlgorithm 1: Exact algorithm for calculating the SV for an unweighted KNN classifier.\ninput :Training data D = {(xi, yi)}Ni=1, test data Dtest = {(xtest,i, ytest,i)} Ntest i=1\noutput :The SV {si}Ni=1 1 for j\u2190 1 to Ntest do 2 (\u03b11, ..., \u03b1N)\u2190 Indices of training data in an ascending order using d(\u00b7, xtest); 3 sj,\u03b1N \u2190 1[y\u03b1N=ytest]\nN ; 4 for i\u2190 N\u2212 1 to 1 do 5 sj,\u03b1i \u2190 sj,\u03b1i+1 + 1[y\u03b1i=ytest,j]\u22121[y\u03b1i+1=ytest,j] K min{K,i}\ni ; 6 end 7 end 8 for i\u2190 1 to N do 9 si \u2190 1Ntest \u2211Ntest j=1 sj,i;\n10 end\nThe proof of Theorem 1 relies on the following lemma, which states that the difference in the utility gain induced by either point i or point j translates linearly to the difference in the respective SVs.\nLEMMA 1. For any i, j \u2208 I, the difference in SVs between i and j is\nsi \u2212 sj = 1\nN\u2212 1 \u2211 S\u2286I\\{i,j} \u03bd(S \u222a {i}) \u2212 \u03bd(S \u222a {j})( N\u22122 |S|\n)(9) Proof of Theorem 1. W.l.o.g., we assume that x1, . . . , xn are sorted according to their similarity to xtest, that is, xi = x\u03b1i . For any given subset S \u2286 I \\ {i, i+ 1} of size k, we split the subset into two disjoint sets S1 and S2 such that S = S1 \u222a S2 and |S1|+ |S2| = |S| = k. Given two neighboring points with indices i, i+ 1 \u2208 I, we constrain S1 and S2 to S1 \u2286 {1, ..., i\u2212 1} and S2 \u2286 {i+ 2, ...,N}. Let si be the SV of data point xi. By Lemma 1, we can draw conclusions about the SV difference si \u2212 si+1 by inspecting the utility difference \u03bd(S \u222a {i}) \u2212 \u03bd(S \u222a {i + 1}) for any S \u2286 I \\ {i, i + 1}. We analyze \u03bd(S \u222a {i}) \u2212 \u03bd(S \u222a {i+ 1}) by considering the following cases. (1) |S1| > K. In this case, we know that i, i+1 > K and therefore \u03bd(S\u222a {i}) = \u03bd(S\u222a {i+1}) = \u03bd(S), hence \u03bd(S \u222a {i}) \u2212 \u03bd(S \u222a {i+ 1}) = 0.\n(2) |S1| < K. In this case, we know that i 6 K and therefore \u03bd(S \u222a {i}) \u2212 \u03bd(S) might be nonzero. Note that including a point i into S can only expel the Kth nearest neighbor from the original set of K nearest neighbors. Thus, \u03bd(S \u222a {i}) \u2212 \u03bd(S) = 1K(1[yi = ytest] \u2212 1[yK = ytest]). The same hold for the inclusion of point i+ 1: \u03bd(S \u222a {i+ 1}) \u2212 \u03bd(S) = 1K(1[yi+1 = ytest] \u2212 1[yK = ytest]). Combining the two equations, we have\n\u03bd(S \u222a {i}) \u2212 \u03bd(S \u222a {i+ 1}) = 1[yi = ytest] \u2212 1[yi+1 = ytest] K\nCombining the two cases discussed above and applying Lemma 1, we have\nsi \u2212 si+1\n= 1\nN\u2212 1 N\u22122\u2211 k=0 1( N\u22122 k ) \u2211 S1\u2286{1,...,i\u22121}, S2\u2286{i+2,...,N}:\n|S1|+|S2|=k,|S1|<K\n1[yi = ytest] \u2212 1[yi+1 = ytest]\nK\n= 1[yi = ytest] \u2212 1[yi+1 = ytest] K \u00d7 1 N\u2212 1 N\u22122\u2211 k=0 1( N\u22122 k ) min(K\u22121,k)\u2211 m=0 ( i\u2212 1 m )( N\u2212 i\u2212 1 k\u2212m ) (10)\nThe sum of binomial coefficients in (10) can be simplified as follows:\nN\u22122\u2211 k=0 1( N\u22122 k ) min{K\u22121,k}\u2211 m=0 ( i\u2212 1 m )( N\u2212 i\u2212 1 k\u2212m ) (11)\n= min{K\u22121,i\u22121}\u2211 m=0 N\u2212i\u22121\u2211 k \u2032=0 ( i\u22121 m )( N\u2212i\u22121 k \u2032 )( N\u22122 m+k \u2032\n)(12) =\nmin{K, i}(N\u2212 1) i (13)\nwhere the first equality is due to the exchange of the inner and outer summation and the second one is by taking v = N\u2212 i\u2212 1 and u = i\u2212 1 in the binomial identity \u2211v j=0 (ui)( v j)\n(u+vi+j ) = u+v+1u+1 .\nTherefore, we have the following recursion\nsi \u2212 si+1 = 1[yi = ytest] \u2212 1[yi+1 = ytest]\nK\nmin{K, i} i (14)\nNow, we analyze the formula for sN, the starting point of the recursion. Since xN is farthest to xtest among all training points, xN results in non-zero marginal utility only when it is added to the subsets of size smaller than K. Hence, sN can be written as\nsN = 1\nN K\u22121\u2211 k=0 1( N\u22121 k ) \u2211 |S|=k,S\u2286I\\{N} \u03bd(S \u222aN) \u2212 \u03bd(S)(15)\n= 1\nN K\u22121\u2211 k=0 1( N\u22121 k ) \u2211 |S|=k,S\u2286I\\{N} 1[yN = ytest] K (16)\n= 1[yN = ytest]\nN (17)\n3.2. LSH-based Approximation. The exact calculation of the KNN SV for a query instance requires to sort the entire training dataset, and has computation complexity O(Ntest(Nd+N log(N))), where d is the feature dimension. Thus, the exact method becomes expensive for large and highdimensional datasets. We now present a sublinear algorithm to approximate the KNN SV for classification tasks. The key to boosting efficiency is to realize that only O(1/ ) nearest neighbors are needed to estimate the KNN SV with up to error. Therefore, we can avert the need of sorting the entire database for every new query point.\nTHEOREM 2. Consider the utility function defined in (5). Consider {s\u0302i}Ni=1 defined recursively by\ns\u0302\u03b1i = 0 if i > K \u2217(18)\ns\u0302\u03b1i = s\u0302\u03b1i+1 + 1[y\u03b1i = ytest] \u2212 1[y\u03b1i+1 = ytest]\nK\nmin{K, i} i if i 6 K\u2217 \u2212 1(19) where K\u2217 = max{K, d1/ e} for some > 0. Then, [s\u0302\u03b11 ,. . ., s\u0302\u03b1N ] is an ( , 0)-approximation to the true SV [s\u03b11 ,. . ., s\u03b1N ] and s\u0302i \u2212 s\u0302i+1 = si \u2212 si+1 for i 6 K \u2217 \u2212 1.\nTheorem 2 indicates that we only need to find max{K, d1/ e}(, K\u2217) nearest neighbors to obtain an ( , 0)-approximation. Moreover, since s\u0302i \u2212 s\u0302i+1 = si \u2212 si+1 for i 6 K\u2217 \u2212 1, the approximation retains the original value rank for K\u2217 nearest neighbors. The question on how to efficiently retrieve nearest neighbors to a query in large-scale databases has been studied extensively in the past decade. Various techniques, such as the kd-tree [MA98], LSH [DIIM04], have been proposed to find approximate nearest neighbors. Although all of these techniques can potentially help improve the efficiency of the data valuation algorithms for KNN, we focus on LSH in this paper, as it was experimentally shown to achieve large speedup over several tree-based data structures [GIM+99,HPIM12,DIIM04]. In LSH, every training instance x is converted into codes in each hash table by using a series of hash functions hj(x), j = 1, . . . ,m. Each hash function is designed to preserve the relative distance between different training instances; similar instances have the same hashed value with high probability. Various hash functions have been proposed to approximate KNN under different distance metrics [Cha02,DIIM04]. We will focus on the distance measured in l2 norm; in that case, a commonly used hash function is h(x) = \u230a wTx+b r \u230b , where w is a vector with entries sampled from a p-stable distribution, and b is uniformly chosen from the range [0, r]. It is shown in [DIIM04]:\nP[h(xi) = h(xtest)] = fh(\u2016xi \u2212 xtest\u20162)(20) where the function fh(c) = \u222br 0 1 cf2( z c)(1\u2212 z r )dz is a monotonically decreasing with c. Here, f2 is the probability density function of the absolute value of a 2-stable random variable. We now present a theorem which relates the success rate of finding approximate nearest neighbors to the intrinsic property of the dataset and the parameters of LSH.\nTHEOREM 3. LSH with O(d log(N)Ng(CK) log K\u03b4 ) time complexity, O(Nd+N g(CK)+1 log K\u03b4 ) space\ncomplexity, and O(Ng(CK) log K\u03b4 ) hash tables can find the exact K nearest neighbors with probability 1\u2212 \u03b4, where g(CK) = log fh(1/CK)/ log fh(1) is a monotonically decreasing function. CK = Dmean/DK, where Dmean is the expected distance of a random training instance to a query xtest and DK is the expected distance between xtest to its Kth nearest neighbor denoted by x\u03b1i(xtest), i.e.,\nDmean = Ex,xtest [D(x, xtest)](21)\nDK = Extest [D(x\u03b1i(xtest), xtest](22)\nThe above theorem essentially extends the 1NN hardness analysis in Theorem 3.1 of [HKC12] to KNN. CK measures the ratio between the distance from a query instance to a random training instance and that to its Kth nearest neighbor. We will hereinafter refer to CK as Kth relative contrast. Intuitively, CK signifies the difficulty of finding the Kth nearest neighbor. A smaller CK implies that some random training instances are likely to have the same hashed value as the Kth nearest neighbor, thus entailing a high computational cost to differentiate the true nearest neighbors from the false positives. Theorem 3 shows that among the datasets of the same size, the one with higher relative contrast will need lower time and space complexity and fewer hash tables to approximate the K nearest neighbors. Combining Theorem 2 and Theorem 3, we obtain the following theorem that explicates the tradeoff between KNN SV approximation errors and computational complexity.\nTHEOREM 4. Consider the utility function defined in (8). Let x\u0302 \u03b1\n(j) k\ndenote the kth closest train-\ning point to xtest,j output by LSH with O(Ntestd log(N)Ng(CK\u2217)logNtestK \u2217\n\u03b4 ) time complexity, O(Nd + Ng(CK\u2217)+1logNtestK \u2217\n\u03b4 ) space complexity, and O(N g(CK\u2217)logNtestK\n\u2217\n\u03b4 ) hash tables, where K \u2217 = max(K, d1/ e).\nSuppose that {s\u0302i}Ni=1 is computed via s\u0302i = 1 Ntest \u2211Ntest j=1 s\u0302i,j and s\u0302i,j (j = 1, . . . ,Ntest) are defined recursively by\ns\u0302 \u03b1\n(j) i ,j\n= 0 if i > K\u2217(23)\ns\u0302 \u03b1\n(j) i ,j\n= s\u0302 \u03b1\n(j) i+1,j\n+ 1[y\u0302 \u03b1 (j) i = ytest,j] \u2212 1[y\u0302\u03b1(j)i+1 = ytest,j]\nK\nmin{K, i} i if i 6 K\u2217 \u2212 1(24)\nwhere y\u0302 \u03b1\n(j) i and ytest,j are the labels associated with x\u0302\u03b1(j)i and xtest,j, respectively. Let the true SV of x\u0302\u03b1k\nbe denoted by s\u03b1i . Then, [s\u0302\u03b11 , . . . , s\u0302\u03b1N ] is an ( , \u03b4)-approximation to the true SV [s\u03b11 , . . . , s\u03b1N ].\nThe gist of the LSH-based approximation is to focus only on the SV of the retrieved nearest neighbors and neglect the values of the rest of the training points since their values are small enough. For a error requirement not too small such that CK\u2217 > 1, the LSH-based approximation has sublinear time complexity, thus enjoying higher efficiency than the exact algorithm."
    },
    {
      "heading": "4. Extensions",
      "text": "We extend the exact algorithm for unweighted KNN to other settings. Specifically, as illustrated by Figure 3, we categorize a data valuation problem according to whether data contributors are valued in tandem with a data analyst; whether each data contributor provides a single data instance or multiple ones; whether the underlying ML model is a weighted KNN or unweighted; and whether the model solves a regression or a classification task. We will discuss the valuation algorithm for each of the above settings. Unweighted KNN Regression. For regression tasks, we define the utility function by the negative mean square error of an unweighted KNN regressor:\nU(S) = \u2212\n( 1\nK min{K,|S|}\u2211 k=1 y\u03b1k(S) \u2212 ytest )2 (25)\nUsing similar proof techniques to Theorem 1, we provide a simple iterative procedure to compute the SV for unweighted KNN regression in Appendix E.1.\nWeighted KNN. A weighted KNN estimate produced by a training set S can be expressed as y\u0302(S) =\u2211min{K,|S|} k=1 w\u03b1k(S)y\u03b1k , where w\u03b1k(S) is the weight associated with the kth nearest neighbor in S. The weight assigned to a neighbor in the weighted KNN estimate often varies with the neighbor-to-test distance so that the evidence from more nearby neighbors is weighted more heavily [Dud76]. Correspondingly, we define the utility function associated with weighted KNN classification and regression tasks as\nU(S) = min{K,|S|}\u2211 k=1 w\u03b1k(S)1[y\u03b1k(S) = ytest](26)\nand\nU(S) = \u2212 (min{K,|S|}\u2211 k=1 w\u03b1k(S)y\u03b1k(S) \u2212 ytest )2 .(27)\nFor weighted KNN classification and regression, the SV can no longer be computed exactly in O(N logN) time. In Appendix E.2, we present a theorem showing that it is however possible to compute the exact SV for weighted KNN in O(NK) time. Figure 4 illustrates the origin of the polynomial complexity result. When applying (2) to KNN, we only need to focus on the subsets whose utility might be affected by the addition of ith training instance. Since there are only NK possible distinctive combinations for K nearest neighbors, the number of distinct utility values for all S \u2286 I is upper bounded by NK. Multiple Data Per Contributor. We now study the case where each seller provides more than one data instance. The goal is to fairly value individual sellers in lieu of individual training points. In Appendix E.3, we show that for both unweighted/weighted classifiers/regressors, the complexity for computing the SV of each seller is O(MK), where M is the number of sellers. Particularly, when K = 1, even though each seller can provision multiple instances, the utility function only depends on the training point that is nearest to the query point. Thus, for 1NN, the problem of computing the multi-data-per-seller KNN SV reduces to the single-data-per-seller case; thus, the corresponding computational complexity is O(M logM).\nValuing Computation. Oftentimes, the buyer may outsource data analytics to a third party, which we call the analyst throughout the rest of the paper. The analyst analyzes the training dataset aggregated from different sellers and returns an ML model to the buyer. In this process, the analyst contributes various computation efforts, which may include intellectual property pertaining to data anlytics, usage of computing infrastructure, among others. Here, we want to address the problem of appraising both sellers (data contributors) and analysts (computation contributors) within a unified game-theoretic framework. Firstly, we extend the game-theoretic framework for data valuation to model the interplay between data and computation. The resultant game is termed a composite game. By contrast, the game discussed previously which involves only the sellers is termed a data-only game. In the composite game, there are M + 1 players, consisting of M sellers denoted by Is and one analyst denoted by C. We can express the utility function \u03bdc associated with the game in terms of the utility function \u03bd in the data-only game as follows. Since in the case of outsourced analytics, both contributions from data sellers and data analysts are necessary for building models, the value of a set S \u2286 Is \u222a {C} in the composite game is zero if S only contains the sellers or the analyst; otherwise, it is equal to \u03bd evaluated on all the sellers in S. Formally, we define the utility function \u03bdc by\n\u03bdc(S) =\n{ 0, if S = {C} or S \u2286 Is\n\u03bd(S \\ {C}), otherwise (28)\nThe goal in the composite game is to allocate \u03bdc({Is, C}) to the individual sellers and the analyst. s(\u03bdc, i) and s(\u03bdc, C) represent the value received by seller i and the analyst, respectively. We suppress the dependency of s on the utility function whenever it is self-evident, denoting the value allocated to seller i and the analyst by si and sc, respectively. In Appendix E.4, we show that one can compute the SV for both the sellers and the analyst with the same computational complexity as the one needed for the data-only game. Comments on the Proof Techniques. We have shown that we can circumvent the exponential complexity for computing the SV for a standard unweighted KNN classifier and its extensions. A natural question is whether it is possible to abstract the commonality of these cases and provide a general property of the utility function that one can exploit to derive efficient algorithms. Suppose that some group of S\u2019s induce the same \u03bd(S \u222a {i}) \u2212 \u03bd(S \u222a {j}) and there only exists T number of such groups. More formally, consider that \u03bd(S \u222a {i}) \u2212 \u03bd(S \u222a {j}) can be represented by a \u201cpiecewise\u201d form:\n\u03bd(S \u222a {i}) \u2212 \u03bd(S \u222a {j}) = T\u2211 t=1 C (t) ij 1[S \u2208 St](29)\nwhere St \u2286 2I\\{i,j} and C(t)i,j \u2208 R is a constant associated with tth \u201cgroup.\u201d An application of Lemma 1 to the utility functions with the piecewise utility difference form indicates that the SV difference between i and j is\nsi \u2212 sj = 1\nN\u2212 1 \u2211 S\u2286I\\{i,j} T\u2211 t=1 C (t) ij( N\u22122 |S|\n)1[S \u2208 St](30) = 1\nN\u2212 1 T\u2211 t=1 C (t) ij [N\u22122\u2211 k=0 |{S : S \u2208 St, |S| = k}|( N\u22122 k ) ](31)\nWith the piecewise property (29), the SV calculation is reduced to a counting problem. As long as the quantity in the bracket of (31) can be efficiently evaluated, the SV difference between any pair of training points can be computed in O(TN). Indeed, one can verify that the utility function for unweighted KNN classification, regression and weighted KNN have the aforementioned \u201cpiecewise\u201d utility difference property with T = 1,N\u2212 1, \u2211K k=0 ( N\u22122 k ) , respectively. More details can be found in Appendix F."
    },
    {
      "heading": "5. Improved MC Approximation",
      "text": "As discussed previously, the SV for unweighted KNN classification and regression can be computed exactly with O(N logN) complexity. However, for the variants including the weighted KNN and multiple-data-per-seller KNN, the complexity to compute the exact SV is O(NK) and O(MK), respectively, which are clearly not scalable. We propose a more efficient way to evaluate the SV up to provable approximation errors, which modifies the existing MC algorithm presented in Section 2.2. By exploiting the locality property of the KNN-type algorithms, we propose a tighter upper bound on the number of permutations for a given approximation error and exhibit a novel implementation of the algorithm using efficient data structures. The existing sample complexity bound is based on Hoeffding\u2019s inequality, which bounds the number of permutations needed in terms of the range of utility difference \u03c6i. This bound is not always optimal as it depends on the extremal values that a random variable can take and thus accounts for the worst case. For KNN, the utility does not change after adding training instance i for many subsets; therefore, the variance of \u03c6i is much smaller than its range. This inspires us to use Bennett\u2019s inequality, which bounds the sample complexity in terms of the variance of a random variable and often results in a much tighter bound than Hoeffding\u2019s inequality.\nTHEOREM 5. Given the range [\u2212r, r] of the utility difference \u03c6i, an error bound , and a confidence 1\u2212 \u03b4, the sample size required such that P[\u2016s\u0302\u2212 s\u2016\u221e > ] 6 \u03b4 is T > T\u2217. T\u2217 is the solution of\nN\u2211 i=1 exp(\u2212T\u2217(1\u2212 q2i )h( (1\u2212 q2i )r )) = \u03b4/2.(32)\nwhere h(u) = (1+ u) log(1+ u) \u2212 u and\nqi = { 0, i = 1, . . . , K i\u2212K i , i = K+ 1, . . . ,N (33)\nGiven , \u03b4, and r, the required permutation size T\u2217 derived from Bennett\u2019s bound can be computed numerically. For general utility functions the range r of the utility difference is twice the range of the utility function, while for the special case of the unweighted KNN classifier, r = 1K . Although determining exact T\u2217 requires numerical calculation, we can nevertheless gain insights into the relationship between N, , \u03b4 and T\u2217 through some approximation. We leave the detailed derivation to Appendix H, but it is often reasonable to use the following T\u0303 as an approximation of T\u2217:\nT\u0303 > r2\n2 log\n2K\n\u03b4 (34)\nAlgorithm 2: Improved MC Approach input :Training set - D = {(xi, yi)}Ni=1, utility function \u03bd(\u00b7), the number of measurements -\nM, the number of permutations - T output :The SV of each training point - s\u0302 \u2208 RN\n11 for t\u2190 1 to T do 12 \u03c0t \u2190 GenerateUniformRandomPermutation(D); 13 Initialize a length-K max-heap H to maintain the KNN; 14 for i\u2190 1 to N do 15 Insert \u03c0t,i to H; 16 if H changes then 17 \u03c6t\u03c0t,i \u2190 \u03bd(\u03c0t,1:i) \u2212 \u03bd(\u03c0t,1:i\u22121); 18 else 19 \u03c6t\u03c0t,i \u2190 \u03c6 t \u03c0t,i\u22121\n; 20 end 21 end 22 end 23 s\u0302i = 1 T \u2211T t=1\u03c6 t i for i = 1, . . . ,N;\nThe sample complexity bound derived above does not change with N. On the one hand, a larger training data size implies more unknown SVs to be estimated, thus requiring more random permutations. On the other hand, the variance of the SV across all training data decreases with the training data size, because an increasing proportion of training points makes insignificant contributions to the query result and results in small SVs. These two opposite driving forces make the required permutation size about the same across all training data sizes. The algorithm for the improved MC approximation is provided in Algorithm 2. We use a max-heap to organize the KNN. Since inserting any training data to the heap costs O(logK), incrementally updating the KNN in a permutation costs O(N logK). Using the bound on the number of permutations in (34), we can show that the total time complexity for our improved MC algorithm is O(N 2 logK log K\u03b4 )."
    },
    {
      "heading": "6. Experiments",
      "text": "We evaluate the proposed approaches to computing the SV of training data for various nearest neighbor algorithms.\n6.1. Experimental Setup. Datasets. We used the following popular benchmark datasets of different sizes: (1) dog-fish [KL17] contains the features of dog and cat images extracted from ImageNet, with 900 training examples and 300 test examples for each class. The features have 2048 dimensions, generated by the state-of-the-art Inception v3 network [SVI+16] with all but the top layer. (2) MNIST [LC10] is a handwritten digit dataset with 60000 training images and 10000 test images. We extracted 1024-dimensional features via a convolutional network. (3) The CIFAR-10 dataset consists of 60000 32\u00d7 32 color images in 10 classes, with 6000 images per class. The deep features have 2048 dimensions and were extracted via the ResNet-50 [HZRS16]. (4) ImageNet [DDS+09] is an image dataset with more than 1 million\nimages organized according to the WordNet hierarchy. We chose 1000 classes which have in total around 1 million images and extracted 2048-dimensional deep features by the ResNet-50 network. (5) Yahoo Flickr Creative Commons 100M that consists of 99.2 million photos. We randomly chose a 10-million subset (referred to as Yahoo10m hereinafter) for our experiment, and used the deep features extracted by [AFGR16]. Parameter selection for LSH. The three main parameters that affect the performance of the LSH are the number of projections per hash value (m), the number of hash tables (h), and the width of the project (r). Decreasing r decreases the probability of collision for any two points, which is equivalent to increasing m. Since a smaller m will lead to better efficiency, we would like to set r as small as possible. However, decreasing r below a certain threshold increases the quantity g(CK), thereby requiring us to increase h. Following [DIIM04], we performed grid search to find the optimal value of r which we used in our experiments. Following [GIM+99], we set m = \u03b1 logN/ log(fh(Dmean)\u22121). For a given value of m, it is easy to find the optimal value of h which will guarantee that the SV approximation error is no more than a user-specified threshold. We tried a few values for \u03b1 and reported the m that leads to lowest runtime. For all experiments pertaining to the LSH, we divided the dataset into two disjoint parts: one for selecting the parameters, and another for testing the performance of LSH for computing the SV.\n6.2. Experimental Results.\n6.2.1. Unweighted KNN Classifier. Correctness. We first empirically validate our theortical result. We randomly selected 1000 training points and 100 test points from MNIST. We computed the SV of each training point with respect to the KNN utility using the exact algorithm and the baseline MC method. Figure 5 shows that the MC estimate of the SV for each training point converges to the result of the exact algorithm. Performance. We validated the hypothesis that our exact algorithm and the LSH-based method outperform the baseline MC method. We take the approximation error = 0.1 and \u03b4 = 0.1 for\nboth MC and LSH-based approximations. We bootstrapped the MNIST dataset to synthesize training datasets of various sizes. The three SV calculation methods were implemented on a machine with 2.6 GHz Intel Core i7 CPU. The runtime of the three methods for different datasets is illustrated in Figure 6 (a). The proposed exact algorithm is faster than the baseline approximation by several orders magnitude and it produces the exact SV. By circumventing the computational complexity of sorting a large array, the LSH-based approximation can significantly outperform the exact algorithm, especially when the training size is large. Figure 6 (b) sheds light on the increasing performance gap between the LSH-based approximation and the exact method with respect to the training size. The relative contrast of these bootstrapped datasets grows with the number of training points, thus requiring fewer hash tables and less time to search for approximate nearest neighbors. We also tested the approximation approach proposed in our prior work [JDW+19], which achieves the-start-of-the-art performance for ML models that cannot be incrementally maintained. However, for models that have efficient incremental training algorithms, like KNN, it is less efficient than the baseline approximation, and the experiment for 1000 training points did not finish in 4 hours. Using a machine with the Intel Xeon E5-2690 CPU and 256 GB RAM, we benchmarked the runtime of the exact and the LSH-based approximation algorithm on three popular datasets, including CIFAR-10, ImageNet, and Yahoo10m. For each dataset, we randomly selected 100 test points, computed the SV of all training points with respect to each test point, and reported the average runtime across all test points. The results for K = 1 are reported in Figure 7. We can see that the LSHbased method can bring a 3\u00d7-5\u00d7 speed-up compared with the exact algorithm. The performance of LSH depends heavily on the dataset, especially in terms of its relative contrast. This effect will be thoroughly studied in the sequel. We compare the prediction accuracy of KNN (K = 1, 2, 5) with the commonly used logistic regression and the result is illustrated in Figure 8. We can see that KNN achieves comparable prediction power to logistic regression when using features extracted via deep neural networks. The runtime of the exact and the LSH-based approximation for K = 2, 5 is similar to the K = 1 case in Figure 7, so we will leave their corresponding results to Appendix A.1.\nEffect of relative contrast on the LSH-based method. Our theoretical result suggests that the K\u2217th relative contrast (K\u2217 = max{K, d1/ e}) determines the complexity of the LSH-based approximation. We verified the effect of relative contrast by experimenting on three datasets, namely, dog-fish, deep and gist. deep and gist were constructed by extracting the deep features and gist features [SI07] from MNIST, respectively. All of these datasets were normalized such that Dmean = 1. Figure 9 (a) shows that the relative contrast of each dataset decreases as K\u2217 increases. In this experiment, we take = 0.01 and K = 2, so the corresponding K\u2217 = 1/ = 100. At this value of K\u2217, the relative contrast is in the following order: deep (1.57) > gist (1.48) > dog-fish (1.17). From Figure 9 (b) and (c), we see that the number of hash tables and the number of returned points required to meet the error tolerance for the three datasets follow the reversed order of their relative contrast, as predicted by Theorem 4. Therefore, the LSH-based approximation will be less efficient if the K in the nearest neighbor algorithm is very large or the desired error is small. Figure 9 (d) shows that the LSH-based method can better approximate the true SV as the recall of the underlying nearest neighbor retrieval gets higher. For the datasets with high relative contrast, e.g., deep and gist, a moderate value of recall (\u223c 0.7) can already lead to an approximation error below the desired threshold. On the other hand, dog-fish, which has low relative contrast, will need fairly accurate nearest neighbor retrieval (recall \u223c 1) to obtain a tolerable approximation error. The reason for the different retrieval accuracy requirements is that for the dataset with higher relative contrast, even if the retrieval of the nearest neighbors is inaccurate, the rank of the erroneous elements in the retrieved set may still be close to that of the missed true nearest neighbors. Thus, these erroneous elements will have only little impacts on SV approximation errors. Simulation of the theoretical bound of LSH. According to Theorem 4, the complexity of the LSH-based approximation is dominated by the exponent g(CK\u2217), where K\u2217 = min{K, 1/ } and g(\u00b7) depends on the width r of the p-stable distribution used for LSH. We computed CK\u2217 and g(CK\u2217) for \u2208 {0.001, 0.01, 0.1, 1} and let K = 1 in this simulation. The orange line in Figure 10 (a) shows that a larger induces a larger value of relative contrast CK\u2217 , rendering the underlying nearest neighbor\nretrieval problem of the LSH-based approximation method easier. In particular, CK\u2217 is greater than 1 for all epsilons considered except for = 0.001. Recall that g(CK) = log fh(1/CK)/ log fh(1); thus, g(CK\u2217) will exhibit different trends for the epsilons with CK\u2217 > 1 and the ones with CK\u2217 < 1, as shown in Figure 10 (b). Moreover, Figure 10 (b) shows that the value of g(CK\u2217) is more or less insensitive to r after a certain point. For that is not too small, we can choose r to be the value at which g(CK\u2217) is minimized. It does not make sense to use the LSH-based approximation if the desired error is too small to have the corresponding g(CK\u2217) less than one, since its complexity is\ntheoretically higher than the exact algorithm. The blue line in Figure 10 (a) illustrates the exponent g(CK\u2217) as a function of when r is chosen to minimize g(CK\u2217). We observe that g(CK\u2217) is always below 1 except when = 0.001.\n6.2.2. Evaluation of Other Extensions. We introduced the extensions of the exact SV calculation algorithm to the settings beyond unweighted KNN classification. Some of these settings require polynomial time to compute the exact SV, which is impractical for large-scale datasets. For those settings, we need to resort to the MC approximation method. We first compare the sample complexity of different MC methods, including the baseline and our improved MC method (Section 5). Then, we demonstrate data values computed in various settings. Sample complexity for MC methods. The time complexity of the MC-based SV approximation algorithms is largely dependent on the number of permutations. Figure 11 compares the permutation sizes used in the following three methods against the actual permutation size needed to achieve a given approximation error (marked as \u201cground truth\u201d in the figure): (1) \u201cHoeffding\u201d, which is the baseline approach and uses the Hoeffding\u2019s inequality to decide the number of permutations; (2) \u201cBennett\u201d, which is our proposed approach and exploits Bennett\u2019s inequality to derive the permutation size; (3) \u201dHeuristic\u201d, which terminates MC simulations when the change of the SV estimates in the two consecutive iterations is below a certain value, which we set to /50 in this experiment. We notice that the ground truth requirement for the permutation size decreases at first and remains constant when the training data size is large enough. From Figure 11, the bound based on the Hoeffding\u2019s inequality is too loose to correctly predict the correct trend of the required permutation size. By contrast, our bound based on Bennett\u2019s inequality exhibits the correct trend of permutation size with respect to training data size. In terms of runtime, our improved MC method based on Bennett\u2019s inequality is more than 2\u00d7 faster than the baseline method when the training size is above 1 million. Moreover, using the aforementioned heuristic, we were able to terminate the\nMC approximation algorithm even earlier while satisfying the requirement of the approximation error. Performance. We conducted experiments on the dog-fish dataset to compare the runtime of the exact algorithm and our improved MC method. We took = 0.01 and \u03b4 = 0.01 in the approximation algorithm and used the heuristic to decide the stopping iteration. Figure 12 compares the runtime of the exact algorithm and our improved MC approximation for weighted KNN classification. In the first plot, we fixed K = 3 and varied the number of training points. In the second plot, we set the training size to be 100 and changed K. We can see that the runtime of the exact algorithm exhibits polynomial and exponential growth with respect to the training size and K, respectively. By contrast, the runtime of the approximation algorithm increases slightly with the number of training points and remains unchanged for different values of K. Figure 13 compares the runtime of the exact algorithm and the MC approximation for the unweighted KNN classification when each seller can own multiple data instances. To generate Figure 13 (a), we set K = 2 and varied the number of sellers. We kept the total number of training instances of all sellers constant and randomly assigned the same number of training instances to each seller. We can see that the exact calculation of the SV in the multi-data-per-seller case has polynomial time complexity, while the runtime of the approximation algorithm barely changes with the number of sellers. Since the training data in our approximation algorithm were sequentially inserted into a heap, the complexity of the approximation algorithm is mainly determined by the total number of training data held by all sellers. Moreover, as we kept the total number of training points constant, the approximation algorithm appears invariant over the number of sellers. Figure 13 (b) shows that the runtime of exact algorithm increases with K, while the approximation algorithm\u2019s\nruntime is not sensitive to K. To summarize, the approximation algorithm is preferable to the exact algorithm when the number of sellers and K are large. Unweighted vs. weighted KNN SV. We constructed an unweighted KNN classifier using the dog-fish. Figure 14 (a) illustrates the training points with top KNN SVs with respect to a specific test image. We see that the returned images are semantically correlated with the test one. We further trained a weighted KNN on the same training set using the weight function that weighs each nearest neighbor inversely proportional to the distance to a given test point; and compared the SV with the ones obtained from the unweighted KNN classifier. We computed the average SV across\nall test images for each training point and demonstrated the result in Figure 14 (b). Every point in the figure represents the SVs of a training point under the two classifiers. We can see that the unweighted KNN SV is close to the weighted one. This is because in the high-dimensional feature space, the distances from the retrieved nearest neighbors to the query point are large, in which case the weights tend to be small and uniform. Another observation from Figure 14 (b) is that the KNN SV assigns more values to dog images than fish images. Figure 14 (c) plots the distribution of the number test examples with regard to the number of their top-K neighbors in the training set are with a label inconsistent with the true label of the test example. We see that most of the nearest neighbors with inconsistent labels belong to the fish class. In other words, the fish training images are more close to the dog images in the test set than the dog training images to the test fish. Thus, the fish training images are more susceptible to mislead the predictions and should have lower values. This intuitively explains why the KNN SV places a higher importance on the dog images. Data-only vs. composite game. We introduced two game-theoretic models for distributing the gains from an ML model and would like to understand how the shares of the analyst and the data contributors differ in the two models. We constructed an unweighted KNN classifier with K = 10 on\nthe dog-fish dataset and compute the SV of each player in the data-only and the composite game. Recall that the total utility of both games is defined as the average test accuracy trained on the full set of training data. Figure 15 (a) shows that the SV for the analyst increases with the total utility. Therefore, under the composite game formulation, the analyst has huge incentive to train a good ML model as the values assigned to the analyst gets larger with a better ML model. In addition, in the composite game formulation, the analyst has exclusive control over the computational resources and the data only creates value when it is analyzed with computational modules, the analyst should take the greatest share of the utility extracted from the ML model. This intuition is reflected in Figure 15 (a). Figure 15 (b) demonstrates that the SV of the data contributors in the composite game is correlated with that in the data-only game, although the actual value is much smaller. Figure 15 (c) exhibits the trend of the SV of the analyst and data contributors as more data contributors participate in a data transaction. The SV of the analyst gets larger with more data contributors, while the average value obtained by each data contributor decreases in both composite and data-only games. Figure 15 (d) zooms into the change of the maximum and minimum value among all data contributors in the data-only game setting (the result in the composite game setting is similar). We can see that both the maximum and minimum value decreases at the beginning; as more data contributors are involved in a data transaction, the minimum value demonstrates a small increment. The points with lowest values tend to hurt the ML model performance when they are added into the training set. With more data contributors and more training points, the negative impacts of these \u201coutliers\u201d can get mitigated. Remarks. We summarize several takeaways from our experimental evaluation. (1) For unweighted KNN classifiers, the LSH-based approximation is more preferable than the exact algorithm when a moderate amount of approximation error can be tolerated and K is relatively small. Otherwise, it is recommended to use the exact algorithm as a default approach for data valuation. (2) For weighted KNN regressors or classifiers, computing the exact SV has O(NK) compleixty, thus not scalable for large datasets and large K. Hence, it is recommended to adopt the Monte Carlo method in Algorithm 2. Moreover, using the heuristic based on the change of SV estimates in two consecutive iterations to decide the termination point of the algorithm is much more efficient than using the theoretical bounds, such as Hoeffding or Bennett."
    },
    {
      "heading": "7. Discussion",
      "text": "From the KNN SV to Monetary Reward. Thus far, we have focused on the problem of attributing the KNN utility and its extensions to each data and computation contributor. In practice, the buyer pays a certain amount of money depending on the model utility and it is required to determine the share of each contributor in terms of monetary rewards. Thus, a remaining question is how to map the KNN SV, a share of the total model utility, to a share of the total revenue acquired from the buyer. A simple method for such mapping is to assume that the revenue is an affine function of the model utility, i.e., R(S) = a\u03bd(S) + b where a and b are some constants which can be determined via market research. Due to the additivity property, we have s(R, i) = as(\u03bd, i) + b. Thus, we can apply the same affine function to the KNN SV to obtain the the monetary reward for each contributor. Computing the SV for Models Beyond KNN. The efficient algorithms presented in this paper are possible only because of the \u201clocality\u201d property of KNN. However, given many previous empirical results showing that a KNN classifier can often achieve a classification accuracy that is comparable with classifiers such as SVMs and logistic regression given sufficient memory, we could use the KNN\nSV as a proxy for other classifiers. We compute the SV for a logistic regression classifier and a KNN classifier trained on the same dataset namely Iris, and the result shows that the SVs under these two classifiers are indeed correlated (see Figure 16). The only caveat is that KNN SV does not distinguish between neighboring data points that have the same label. If this caveat is acceptable, we believe that the KNN SV provides an efficient way to approximately assess the relative contribution of different data points for other classifiers as well. Moreover, for calculating the SV for general deep neural networks, we can take the deep features (i.e., the input to the last softmax layer) and corresponding labels, and train a KNN classifier on the deep features. We calibrate K such that the\nresulting KNN mimics the performance of the original deep net and then employ the techniques presented in this paper to calculate a surrogate for the SV under the deep net. Implications of Task-Specific Data Valuation. Since the SV depends on the utility function associated with the game, data dividends based on the SV are contingent on the definition of model usefulness in specific ML tasks. The task-specific nature of our data valuation framework offers clear advantages\u2014it allows to accommodate the variability of a data point\u2019s utility from one application to another and assess its worth accordingly. Moreover, it enables the data buyer to defend against data poisoning attacks, wherein the attacker intentionally contributes adversarial training data points crafted specifically to degrade the performance of the ML model. In our framework, the \u201cbad\u201d training points will naturally have low SVs because they contribute little to boosting the performance of the model. Having the data values dependent on the ML task, on the other hand, may raise some concerns about whether the data values may inherit the flaws of the ML models as to which the values are computed: if the ML model is biased towards a subpopulation with specific sensitive attributes (e.g., gender, race), will the data values reflect the same bias? Indeed, these concerns can be addressed by designing proper utility functions that devalue the unwanted properties of ML models. For instance, even if the ML model may be biased towards specific subpopulation, the buyer and data contributors can agree on a utility function that gives lower score to unfair models and compute the data values with respect to the concordant utility function. In this case, the training points will be appraised partially according to how much they contribute to improving the model fairness and the resulting data values would not be affected by the bias of the underlying model. Moreover, there is a venerable line of works studying algorithms to help improve fairness [ZWS+13,WGOS17,HPS+16]. These algorithms can also be applied to resolve the potential bias in value assignments. For instance, before providing the data to the data buyer, data contributors can preprocess the training data so that the \u201csanitized\u201d data removes the information correlated with sensitive attributes [ZWS+13]. However, to ensure that the data values are accurately computed according to an appropriate utility function that the buyer and the data contributors agree on or that the models are trained with\nproper fairness criteria, it is necessary to develop systems that can support transparent machine learning processes. Recent work has been studying training machine learning models on blockchains for removing the middleman to audit the model performance and enhancing transparency [blo]. We are currently implementing the data valuation framework on a blockchain-based data market, which can naturally resolve the problems of transparency and trust. Since the focus of this work is the algorithmic foundation of data valuation, we will leave the discussion of the combination of blockchains and data valuation for future work."
    },
    {
      "heading": "8. Related Work",
      "text": "The problem of data pricing has received a lot of attention recently. The pricing schemes deployed in the existing data marketplaces are simplistic, typically setting a fixed price for the whole or parts of the dataset. Before withdrawn by Microsoft, the Azure Data Marketplace adopted a subscription model that gave users access to a certain number of result pages per month [KUB+15]. Xignite [xig] sells financial datasets and prices data based on the data type, size, query frequency, etc. There is rich literature on query-based pricing [KUB+15,KUB+13,KUB+12,DKB17,LK14,LM12, UBS16], aimed at design pricing schemes for fine-grained queries over a dataset. In query-based pricing, a seller can assign prices to a few views and the price for any queries purchased by a buyer is automatically derived from the explicit prices over the views. Koutris et al. [KUB+15] identified two important properties that the pricing function must satisfy, namely, arbitrage-freeness and discount-freeness. The arbitrage-freeness indicates that whenever query Q1 discloses more information than query Q2, we want to ensure that the price of Q1 is higher than Q2; otherwise, the data buyer has an arbitrage opportunity to purchase the desired information at a lower price. The discount-freeness requires that the prices offer no additional discounts than the ones specified by the data seller. The authors further proved the uniqueness of the pricing function with the two properties, and established a dichotomy on the complexity of the query pricing problem when all views are selection queries. Li et al. [LM12] proposed additional criteria for data pricing, including non-disclosiveness (preventing the buyers from inferring unpaid query answers by analyzing the publicly available prices of queries) and regret-freeness (ensuring that the price of asking a sequence of queries in multiple interactions is not higher than asking them all-at-once), and investigated the class of pricing functions that meet these criteria. Zheng et al. [ZPW+19] studied how data uncertainty should affect the price of data, and proposed a data pricing framework for mobile crowdsensed data. Recent work on query-based pricing focuses on enabling efficient pricing over a wider range of queries, overcoming the issues such as double-charging arising from building practical data marketplaces [KUB+13, DKB17, UBS16], and compensating data owners for their privacy loss [LLMS17]. Due to the increasing pervasiveness of ML-based analytics, there is an emerging interest in studying the cost of acquiring data for ML. Chen et al. [CKK18,CKK17] proposed a formal framework to price ML model instances, wherein an optimization problem was formulated to find the arbitrage-free price that maximizes the revenue of a seller. The model price can be also used for pricing its training dataset. This paper is complementary to these works in that we consider the scenario where the training set is contributed by multiple sellers and focus on the revenue sharing problem thereof. While the interaction between data analytics and economics has been extensively studied in the context of both relational database queries and ML, few works have dived into the vital problem of\nallocating revenues among data owners. [KUB+12] presented a technique for fair revenue sharing when multiple sellers are involved in a relational query. By contrast, our paper focuses on the revenue allocation for nearest neighbor algorithms, which are widely adopted in the ML community. Moreover, our approach establishes a formal notion of fairness based on the SV. The use of the SV for pricing personal data can be traced back to [KPR01], which studied the SV in the context of marketing survey, collaborative filtering, and recommendation systems. [CL17] also applied the SV to quantify the value of personal information when the population of data contributors can be modeled as a network. [MAS+13] showed that for specific network games, the exact SV can be computed efficiently. There exist various methods to rank the importance of training data, which can also potentially be used for data valuation. For instance, influence functions [KL17] approximate the change of the model performance after removing a training point for smooth parametric ML models. Ogawa et al. [OST13] proposed rules to identify and remove the least influential data when training support vector machines (SVM) to reduce the computation cost. However, unlike the SV, these approaches do not satisfy the group rationality, fairness, and additivity properties simultaneously. Despite the desirable properties of the SV, computing the SV is known to be expensive. In its most general form, the SV can be #P-complete to compute [DP94]. For bounded utility functions, Maleki et al. [MTTH+13] described a sampling-based approach that requires O(N logN) samples to achieve a desired approximation error. By taking into account special properties of the utility function, one can derive more efficient approximation algorithms. For instance, Fatima et al. [FWJ08] proposed a probabilistic approximation algorithm with O(N) complexity for weighted voting games. Ghorbani et al. [GZ19] developed two heuristics to accelerate the estimation of the SV for complex learning algorithms, such as neural networks. One is to truncate the calculation of the marginal contributions as the change in performance by adding only one more training point becomes smaller and smaller. Another is to use one-step gradient to approximate the marginal contribution. The authors also demonstrate the use of the approixmate SV for outlier identification and informed acquisition of new training data. However, their algorithms do not provide any guarantees on the approximation error, thus limiting its viability for practical data valuation. Raskar et al [RVSS19] presented a taxonomy of data valuation problems for data markets and discussed challenges associated with data sharing."
    },
    {
      "heading": "9. Conclusion",
      "text": "The SV has been long advocated as a useful economic concept to measure data value but has not found its way into practice due to the issue of exponential computational complexity. This paper presents a step towards practical algorithms for data valuation based on the SV. We focus on the case where data are used for training a KNN classifier and develop algorithms that can calculate data values exactly in quasi-linear time and approximate them in sublinear time. We extend the algorithms to the case of KNN regression, the situations where a contributor can own multiple data points, and the task of valuing data contributions and analytics simultaneously. For future work, we will integrate our proposed data valuation algorithms into the clinical data market that we are currently building. We will also explore efficient algorithms to compute the data values for other popular ML algorithms such as gradient boosting, logistic regression, and deep neural networks."
    },
    {
      "heading": "Acknowledgement",
      "text": "This work is supported in part by the Republic of Singapores National Research Foundation through a grant to the Berkeley Education Alliance for Research in Singapore (BEARS) for the Singapore-Berkeley Building Efficiency and Sustainability in the Tropics (SinBerBEST) Program. This work is also supported in part by the CLTC (Center for Long-Term Cybersecurity); FORCES (Foundations Of Resilient CybEr-Physical Systems), which receives support from the National Science Foundation (NSF award numbers CNS-1238959, CNS-1238962, CNS-1239054, CNS1239166); the National Science Foundation under Grant No. TWC-1518899; and DARPA FA8650-18-2-7882. CZ and the DS3Lab gratefully acknowledge the support from the Swiss National Science Foundation (Project Number 200021 184628) and a Google Focused Research Award."
    },
    {
      "heading": "Appendix A. Additional Experiments",
      "text": "A.1. Runtime Comparision for Computing the Unweighted KNN SV. For each dataset, we randomly selected 100 test points, computed the SV of all training points with respect to each test point, and reported the average runtime across all test points. The results for K = 2, 5 are presented in Figure 17. We can see that the LSH-based method can bring a 3\u00d7-5\u00d7 speed-up compared with the exact algorithm.\nAppendix B. Proof of Lemma 1\nProof.\nsi \u2212 sj(35) = \u2211\nS\u2286I\\{i}\n|S|!(N\u2212 |S|\u2212 1)! N!\n[ \u03bd(S \u222a {i}) \u2212 \u03bd(S) ] \u2212 \u2211\nS\u2286I\\{j}\n|S|!(N\u2212 |S|\u2212 1)! N!\n[ \u03bd(S \u222a {j}) \u2212 \u03bd(S) ] (36)\n= \u2211\nS\u2286I\\{i,j}\n|S|!(N\u2212 |S|\u2212 1)! N!\n[ \u03bd(S \u222a {i}) \u2212 \u03bd(S \u222a {j}) ] +\n\u2211 S\u2208{T |T\u2286I,i/\u2208T,j\u2208T} |S|!(N\u2212 |S|\u2212 1)! N! [ \u03bd(S \u222a {i}) \u2212 \u03bd(S) ] \u2212\n\u2211 S\u2208{T |T\u2286I,i\u2208T,j/\u2208T} |S|!(N\u2212 |S|\u2212 1)! N! [ \u03bd(S \u222a {j}) \u2212 \u03bd(S) ] (37)\n= \u2211\nS\u2286I\\{i,j}\n|S|!(N\u2212 |S|\u2212 1)! N!\n[ \u03bd(S \u222a {i}) \u2212 \u03bd(S \u222a {j}) ] +\n\u2211 S \u2032\u2286I\\{i,j} (|S \u2032|+ 1)!(N\u2212 |S \u2032|\u2212 2)! N! [ \u03bd(S \u2032 \u222a {i}) \u2212 \u03bd(S \u2032 \u222a {j}) ] (38)\n= \u2211\nS\u2286I\\{i,j}\n( |S|!(N\u2212 |S|\u2212 1)! N! + (|S|+ 1)!(N\u2212 |S|\u2212 2)! N! )[ \u03bd(S \u222a {i}) \u2212 \u03bd(S \u222a {j}) ] (39)\n= 1\nN\u2212 1 \u2211 S\u2286I\\{i,j} 1 C |S| N\u22122 [ \u03bd(S \u222a {i}) \u2212 \u03bd(S \u222a {j}) ] .(40)"
    },
    {
      "heading": "Appendix C. Proof of Theorem 2",
      "text": "Proof. We first observe that if the true Shapley value |s\u03b1i | 6 min( 1 i , 1 K), then |si| 6 for\ni > i\u2217 = max(K, d1/ e). Hence, when i > i\u2217, the approximation error is given by\n|s\u0302\u03b1i \u2212 s\u03b1i | = |s\u03b1i | 6 .(41) When i 6 i\u2217 \u2212 1, s\u0302\u03b1i and s\u03b1i follow the same recursion, i.e.,\ns\u0302\u03b1i \u2212 s\u0302\u03b1i+1 = s\u03b1i \u2212 s\u03b1i+1 = 1[y\u03b1i = ytest] \u2212 1[y\u03b1i+1 = ytest]\nK\nmin(K\u2212 1, i\u2212 1) + 1 i .(42) As a result, we have\n|s\u0302\u03b1i \u2212 s\u03b1i | = |s\u0302\u03b1i+1 \u2212 s\u03b1i+1 | = \u00b7 \u00b7 \u00b7 = |s\u0302\u03b1i\u2217 \u2212 s\u03b1i\u2217 | 6 (43)\nTo sum up, |s\u0302\u03b1i \u2212 s\u03b1i | 6 for all i = 1, . . . ,N, provided that |s\u03b1i | 6 min( 1 i , 1 K). In the following, we will prove that the aforementioned condition is satisfied. We can convert the recursive expression of the KNN Shapley value in Theorem 1 to a nonrecursive one:\ns\u03b1N = 1[y\u03b1N = ytest]\nN (44)\ns\u03b1i = 1[y\u03b1i = ytest]\ni \u2212 N\u2211 j=i+1 1[y\u03b1j = ytest] j(j\u2212 1) for i > K(45)\ns\u03b1i = 1[y\u03b1i = ytest]\nK \u2212 N\u2211 j=K+1 1[y\u03b1j = ytest] j(j\u2212 1) for i 6 K\u2212 1(46)\nWe examine the bound on the absolute value of the Shapley value in three cases: (1) i = N, (2) i > K, and (3) i 6 K\u2212 1.\nCase (1). It is easy to verify that |s\u03b1N | 6 1 N . Case (2). We can bound the second term in (45) by\n0 6 N\u2211\nj=i+1\n1[y\u03b1j = ytest]\nj(j\u2212 1) 6 N\u2211 j=i+1\n1\nj(j\u2212 1) = N\u2211 j=i+1 ( 1 j\u2212 1 \u2212 1 j ) = 1 i \u2212 1 N (47)\nThus, s\u03b1i can be bounded by\n\u2212( 1 i \u2212 1 N ) 6 s\u03b1i 6 1 i ,(48) which yields the bound on the absolute value of s\u03b1i:\n|s\u03b1i | 6 1\ni .(49) Case (3). The absolute value of s\u03b1i for i 6 K\u2212 1 can be bounded using a similar technique as in Case (2). By (46), we have\n\u2212( 1 K \u2212 1 N ) 6 s\u03b1i 6 1 K (50) Therefore, |s\u03b1i | 6 1/K. Summarizing the results in Case (1), (2), and (3), we obtain |s\u03b1i | 6 min(1/i, 1/K) for i = 1, . . . ,N."
    },
    {
      "heading": "Appendix D. Proof of Theorem 3",
      "text": "Proof. For the hashing function h(x) = \u230a wTx+b t \u230b , [DIIM04] have shown that\nP(h(xi) = h(xtest)) = fh(\u2016xi \u2212 xtest\u2016p)(51) where the function fh(a) = \u222bt 0 1 afp( z a(1 \u2212 z t )dz is monotonically decreasing with a. fp is the probability density function of the absolute value of a p-stable random variable. Suppose the data are normalized by a factor such that Dmean = 1. Since such a normalization does not change the nearest neighbor search results, Dk = 1/Ck for k = 1, . . . , K. Denote the probability for one random test point xtest and a random training point to have the same code with one hash function by prand and the probability for xtest and its k-nearest neighbor to have the same code by pknn. According to (51),\nprand = fh(1)(52)\nand\npnn,k = fh(1/Ck)(53)\nbecause the expected distance between xtest and a random training point is Dmean = 1, and the expected distance between xtest and its k-nearest neighbor is 1/Ck. Let Ek denote the event that the k-nearest neighbor of xtest is included by one of the hash tables. Then, the probability of the inclusion of all K nearest neighbors is\nP(E1, . . . , EK) = 1\u2212 P(\u222aKk=1E\u0304k)(54)\n> 1\u2212 K\u2211 k=1 P(E\u0304k).(55)\nWe want to make sure that P(E1, . . . , EK) > 1\u2212 \u03b4, so it suffices to let P(E\u0304k) 6 \u03b4/K for all k = 1, . . . , K. Suppose there are m hash bits in one table and l hash tables in LSH. The probability that the true k-nearest neighbor has the same code as the query in one hash table is pmnn,k. Hence, the probability that the true k-nearest neighbor is missed by l hash tables is P(E\u0304k) = (1 \u2212 pmnn,k)\nl. In order to ensure P(E\u0304k) 6 \u03b4/K, we need\nl > log \u03b4K\nlog(1\u2212 pmnn,k) (56) The RHS is upper bounded by \u2212 log \u03b4 K\npmnn,k = p\u2212mnn,k log K \u03b4 . Therefore, it suffices to ensure\nl > p\u2212mnn,k log K\n\u03b4 (57) Note that pnn,k = p logpnn,k logprand rand and we can choose Np m rand = O(1), i.e., m = O(\nlogN logp\u22121rand ), as discussed\nin [GIM+99]. Hence,\npmnn,k = p m\nlogpnn,k logprand\nrand = O(( 1\nN )\nlogpnn,k logprand ) = O(N\u2212g(Ck))(58)\nwhere g(Ck) = logpnn,k logprand = log fh(1/Ck) log fh(1) . Plugging (58) into (56), we obtain\nl > O(Ng(Ck) log K\n\u03b4 )(59)\nIn order to guarantee P(E\u0304k) 6 \u03b4/K for all k = 1, \u00b7 \u00b7 \u00b7 , K, the number of hash tables needed is\nO(Ng(CK) log K\n\u03b4 )(60)"
    },
    {
      "heading": "Appendix E. Detailed Algorithms and Proofs for the Extensions",
      "text": "We first extend the algorithms to calculate the SV for unweighted KNN regression and weighted KNN. Further, we address the data valuation problem wherein a data curator contributes multiple data points. We then discuss how to valuate the parties offering computation in the data market.\nE.1. Unweighted KNN Regression. For regression tasks, we define the utility function by the negative mean square error of an unweighted KNN regressor:\n\u03bd(S) = \u2212\n( 1\nK min{K,|S|}\u2211 k=1 y\u03b1k(S) \u2212 ytest )2 (61)\nThe following theorem provides a simple iterative procedure to compute the SV for unweighted KNN regression. The derivation of the theorem requires to analyze the utility difference between two adjacent training points, similar to KNN classification.\nTHEOREM 6. Consider the KNN regression utility function in (61). Then, the SV of each training point can be calculated recursively as follows:\ns\u03b1N = \u2212 K\u2212 1\nNK y\u03b1N\n[ 1\nK y\u03b1N \u2212 2ytest +\n1\nN\u2212 1 \u2211 l\u2208I\\{N} y\u03b1l ] \u2212 1 N [ 1 K y\u03b1N \u2212 ytest ]2 (62)\ns\u03b1i = s\u03b1i+1 + 1\nK (y\u03b1i+1 \u2212 y\u03b1i) min{K, i} i ( 1 K N\u2211 l=1 A (l) i y\u03b1l \u2212 2ytest)(63)\nwhere\nA (l) i =  min{K\u22121,i\u22121} i\u22121 if 1 6 l 6 i\u2212 1 1 if l \u2208 {i, i+ 1} min{K,l\u22121}min{K\u22121,l\u22122}i\n(l\u22121)(l\u22122)min{K,i} if i+ 2 6 l 6 N (64)\nAccording to (63), two adjacent training points will have the same SV if they have the same label. Otherwise, their SV difference will depend on three terms: (1) their difference in the labels y\u03b1i+1 \u2212 y\u03b1i , (2) the rank of their distances to the test point min(K,i) i , and (3) the goodness of fit\nterm 1K \u2211N l=1A (l) i y\u03b1l \u2212 2ytest of a \u201cweighted\u201d KNN regression model in which A (l) i stands for the weight. By simple algebraic operations, it can be obtained that y\u03b1i and y\u03b1i+1 are weighted highest among all training points; therefore, the third term can be roughly thought of as how much error y\u03b1i and y\u03b1i+1 induce for predicting ytest. If the goodness of fit term represents a positive error and y\u03b1i > y\u03b1i+1 , then adding (x\u03b1i , y\u03b1i) into the training dataset will even enlarge the positive prediction error. Thus, (x\u03b1i , y\u03b1i) is less valuable than (x\u03b1i+1 , y\u03b1i+1) in terms of the SV. Similar intuition about the interaction between the first and third term can be established when y\u03b1i < y\u03b1i+1 . Moreover, the training points closer to the test point are more influential to the prediction result; this phenomenon is captured by the second term. In summary, the SV difference between two adjacent training points\nis large when their labels differ largely, their distances to the test point are small, and their presence in the training set leads to large prediction errors.\nProof of Theorem 6. W.l.o.g., we assume that x1, . . . , xn are sorted according to their similarity to xtest, that is, xi = x\u03b1i . We split a subset S \u2286 I \\ {i, i+ 1} into two disjoint sets S1 and S2 such that S = S1 \u222a S2 and S1 \u2229 S2 = \u2205. Given two neighboring points with indices i, i+ 1 \u2208 I, we constrain S1 and S2 to S1 \u2286 {1, . . . , i\u2212 1} and S2 \u2286 {i+ 2, . . . ,N}.\nWe analyze the difference between si and si+1 by considering the following cases: Case 1. Consider the case |S1| > K. We know that i > K and therefore \u03bd(S\u222a{i}) = \u03bd(S\u222a{i+1}) =\n\u03bd(S). From Lemma 1, it follows that\nsi \u2212 si+1 = 1\nN\u2212 1 N\u22122\u2211 k=0 1( N\u22122 k ) \u2211 S1\u2286{1,...,i\u22121}, S2\u2286{i+2,...,N}:\n|S1|+|S2|=k,|S1|>K\n[ \u03bd(S \u222a {i}) \u2212 \u03bd(S \u222a {i+ 1}) ] = 0.\nCase 2. Consider the case |S1| < K. The difference between \u03bd(S\u222a {i}) and \u03bd(S\u222a {i+ 1}) can be expressed as\n\u03bd(S \u222a {i}) \u2212 \u03bd(S \u222a {i+ 1})\n=( 1\nK K\u2211 j=1 y\u03b1j(S\u222a{i+1}) \u2212 ytest) 2 \u2212 ( 1 K K\u2211 j=1 y\u03b1j(S\u222a{i}) \u2212 ytest) 2\n= 1\nK (yi+1 \u2212 yi) \u00b7\n( 1\nK (yi+1 + yi) \u2212 2ytest +\n2\nK \u2211 j=1,...,K\u22121 y\u03b1j(S) ) By Lemma 1, the Shapley difference between i and i+ 1 is\nsi \u2212 si+1 = 1\nK (yi+1 \u2212 yi) \u00b7 ( 1\nN\u2212 1 N\u22122\u2211 k=0 1( N\u22122 k ) \u2211 S1\u2286{1,...,i\u22121}, S2\u2286{i+2,...,N}:\n|S1|+|S2|=k,|S1|6K\u22121\n( 1 K (yi+1 + yi) \u2212 2ytest ) \ufe38 \ufe37\ufe37 \ufe38\nU1\n+ 2\nK\n1\nN\u2212 1 N\u22122\u2211 k=0 1( N\u22122 k ) \u2211 S1\u2286{1,...,i\u22121}, S2\u2286{i+2,...,N}:\n|S1|+|S2|=k,|S1|6K\u22121\n\u2211 j=1,...,K\u22121 y\u03b1j(S)\n\ufe38 \ufe37\ufe37 \ufe38 U2\n)\nWe firstly simplify U1. Note that 1K(yi+1+yi)\u2212 2ytest does not depend on the summation; as a result, we have\nU1 = ( 1 K (yi+1 + yi) \u2212 2ytest ) 1 N\u2212 1 N\u22122\u2211 k=0 1( N\u22122 k )( \u2211 S1\u2286{1,...,i\u22121}, S2\u2286{i+2,...,N}:\n|S1|+|S2|=k,|S1|6K\u22121\n1\n)\n= ( 1 K (yi+1 + yi) \u2212 2ytest ) 1 N\u2212 1 N\u22122\u2211 k=0 1( N\u22122 k ) min(K\u22121,k)\u2211 m=0 ( i\u2212 1 m )( N\u2212 i\u2212 1 k\u2212m ) (65)\nThe sum of binomial coefficients in (65) can be further simplified as follows:\nN\u22122\u2211 k=0 1( N\u22122 k ) min(K\u22121,k)\u2211 m=0 ( i\u2212 1 m )( N\u2212 i\u2212 1 k\u2212m )\n= min(K\u22121,i\u22121)\u2211 m=0 N\u2212i\u22121\u2211 k=0 ( i\u22121 m )( N\u2212i\u22121 k )( N\u22122 m+k\n) =\nmin(K\u22121,i\u22121)\u2211 m=0 N\u2212 1 i\n= min(K, i) N\u2212 1\ni where the second equality follows from the binomial coefficient identity \u2211M j=0 (Ni )( M j )\n(N+Mi+j ) = M+N+1N+1 .\nHence,\nU1 = ( 1 K (yi+1 + yi) \u2212 2ytest )min(K, i) i\nThen, we analyze U2. We let\u2211 S1\u2286{1,...,i\u22121}, S2\u2286{i+2,...,N}:\n|S1|+|S2|=k,|S1|6K\u22121\n\u2211 j=1,...,K\u22121 y\u03b1j(S) = \u2211 l\u2208I\\{i,i+1} clyl(66)\nwhere cl counts the number of occurrences of yl in the left-hand side expression and\ncl =\n{ \u2211min(K\u22122,k\u22121) m=0 ( i\u22122 m )( N\u2212i\u22121 k\u2212m\u22121 ) if l \u2208 {1, . . . , i\u2212 1}\u2211min(K\u22122,k\u22121)\nm=0\n( l\u22123 m )( N\u2212l k\u2212m\u22121 ) if l \u2208 {i+ 2, . . . ,N} (67)\nPlugging in (66) and (67) into U2 yields\nU2 = 2\nK(N\u2212 1) N\u22122\u2211 k=0 1( N\u22122 k )[ \u2211 l\u2208{1,...,i\u22121} min(K\u22122,k\u22121)\u2211 m=0 ( i\u2212 2 m )( N\u2212 i\u2212 1 k\u2212m\u2212 1 ) yl\n+ \u2211\nl\u2208{i+2,...,N} min(K\u22122,k\u22121)\u2211 m=0 ( l\u2212 3 m )( N\u2212 l k\u2212m\u2212 1 ) yl ]\n= 2\nK(N\u2212 1) [ \u2211 l\u2208{1,...,i\u22121} yl ] \u00b7 [N\u22122\u2211 k=0 1( N\u22122 k ) min(K\u22122,k\u22121)\u2211 m=0 ( i\u2212 2 m )( N\u2212 i\u2212 1 k\u2212m\u2212 1 ) \ufe38 \ufe37\ufe37 \ufe38\nU21\n]\n+ 2\nK(N\u2212 1) [ \u2211 l\u2208{i+2,...,N} yl \u00b7 N\u22122\u2211 k=0 1( N\u22122 k ) min(K\u22122,k\u22121)\u2211 m=0 ( l\u2212 3 m )( N\u2212 l k\u2212m\u2212 1 ) \ufe38 \ufe37\ufe37 \ufe38\nU22\n] (68)\nUsing the binomial coefficient identity \u2211M j=0 (Ni )( M j )\n(N+M+1i+j+1 ) =\n(i+1)(M+N+2) (N+2)(N+1) , we obtain\nU21 = min(K\u22122,i\u22122)\u2211 m=0 N\u2212i\u22121\u2211 k=0 ( i\u22122 m )( N\u2212i\u22121 k )( N\u22122 k+m+1\n) =\nmin(K\u22122,i\u22122)\u2211 m=0 N\u2212 1 (i\u2212 1)i (m+ 1)\n= N\u2212 1\n(i\u2212 1)i min(K, i)min(K\u2212 1, i\u2212 1) 2 (69)\nand\nU22 = min(K\u22122,l\u22123)\u2211 m=0 N\u2212l\u2211 k=0 ( l\u22123 m )( N\u2212l k )( N\u22122 k+m+1\n) =\nmin(K\u22122,l\u22123)\u2211 m=0\nN\u2212 1\n(l\u2212 1)(l\u2212 2) (m+ 1)\n= N\u2212 1\n(l\u2212 1)(l\u2212 2) min(K, l\u2212 1)min(K\u2212 1, l\u2212 2) 2 (70)\nNow, we plug (69) and (70) into the expression of U2 in (68). Rearranging (68) gives us\nU2 = 1\nK \u2211 l\u2208{1,...,i\u22121} yl min(K, i)min(K\u2212 1, i\u2212 1) (i\u2212 1)i + 1 K \u2211 l\u2208{i+2,...,N} yl min(K, l\u2212 1)min(K\u2212 1, l\u2212 2) (l\u2212 1)(l\u2212 2)\nTherefore, we have\nsi \u2212 si+1\n= 1\nK (yi+1 \u2212 yi)(U1 +U2)\n= 1\nK (yi+1 \u2212 yi) \u00b7 [( 1 K (yi+1 + yi) \u2212 2ytest )min(K\u2212 1, i\u2212 1) + 1 i + 1 K \u2211 l\u2208{1,...,i\u22121} yl min(K, i)min(K\u2212 1, i\u2212 1) (i\u2212 1)i\n+ 1\nK \u2211 l\u2208{i+2,...,N} yl min(K, l\u2212 1)min(K\u2212 1, l\u2212 2) (l\u2212 1)(l\u2212 2) ] Now, we analyze the formula for sN, the starting point of the recursion. Since xN is farthest to xtest among all training points, xN results in non-zero marginal utility only when it is added to a set of size smaller than K. Hence, sN can be written as\nsN = 1\nN K\u22121\u2211 k=0 1( N\u22121 k ) \u2211 |S|=k,S\u2286I\\{N}, \u03bd(S \u222a {N}) \u2212 \u03bd(S)\n= 1\nN K\u22121\u2211 k=1 1( N\u22121 k ) \u2211 |S|=k,S\u2286I\\{N} [ ( 1 K \u2211 i\u2208S yi \u2212 ytest) 2 \u2212 ( 1 K \u2211 i\u2208S\u222a{N} yi \u2212 ytest) 2 ] + \u03bd({N}) N\n= 1\nN K\u22121\u2211 k=1 1( N\u22121 k ) \u2211 |S|=k,S\u2286I\\{N} [ (\u2212 1 K yN) \u00b7 ( 2 K \u2211 i\u2208S yi + 1 K yN \u2212 2ytest) ] + \u03bd({N}) N\n= \u2212 K\u2212 1\nNK yN(\n1 K yN \u2212 2ytest) \u2212 2 NK2 yN K\u22121\u2211 k=1\n( N\u22122 k\u22121 )( N\u22121 k ) \u2211 l\u2208I\\{N} yl + 1 N \u03bd({N})\n= \u2212 1\nN yN\n[ K\u2212 1\nK ( 1 K yN \u2212 2ytest) + 2 K2 ( \u2211\nl\u2208I\\{N}\nyl) K\u22121\u2211 k=1 k N\u2212 1 ] + \u03bd({N}) N\n= \u2212 K\u2212 1\nNK yN\n[ 1\nK yN \u2212 2ytest +\n1\nN\u2212 1 \u2211 l\u2208I\\{N} yl ] + \u03bd({N}) N\nE.2. Weighted KNN. A weighted KNN estimate produced by a training set S can be expressed as\ny\u0302(S) = min{K,|S|}\u2211 k=1 w\u03b1k(S)y\u03b1k(71)\nwhere w\u03b1k(S) is the weight associated with the kth nearest neighbor of the test point in S. The weight assigned to a neighbor in the weighted KNN estimate often varies with the neighbor-to-test distance so that the evidence from more nearby neighbors are weighted more heavily [Dud76]. Correspondingly, we define the utility function associated with weighted KNN classification and regression tasks as\n\u03bd(S) = min{K,|S|}\u2211 k=1 w\u03b1k(S)1[y\u03b1k(S) = ytest](72)\nand\n\u03bd(S) = \u2212 (min{K,|S|}\u2211 k=1 w\u03b1k(S)y\u03b1k(S) \u2212 ytest )2 .(73)\nFor weighted KNN classification and regression, the SV can no longer be computed exactly in O(N log(N)) time. The next theorem shows that it is however possible to compute the exact SV for weighted KNN in O(NK) time. The theorem applies the definition (2) to calculating the SV and relies on the following idea to circumvent the exponential complexity: when applying (2) to KNN, we only need to focus on the sets S whose utility might be affected by the addition of ith training instance. Moreover, since there are only NK possible distinctive combinations for K nearest neighbors, the number of distinct utility values for all S \u2286 I is upper bounded by NK, in contrast to 2N for general utility functions.\nTHEOREM 7. Consider the utility function in (72) or (73) with some weights w\u03b1k(S). Let Bk(i) = {S : |S| = k, i /\u2208 S, S \u2286 I}, for i = 1, . . . ,N and k = 0, . . . , K. Let r(\u00b7) be a function that maps the set of training data to their ranks of similarity to xtest. Then, the SV of each training point can be calculated recursively as follows:\ns\u03b1N = 1\nN K\u22121\u2211 k=0 1( N\u22121 k ) \u2211 S\u2208Bk(\u03b1N) [ \u03bd(S \u222a {\u03b1N}) \u2212 \u03bd(S) ] (74)\ns\u03b1i+1 = s\u03b1i + 1\nN\u2212 1 N\u22122\u2211 k=0 1( N\u22122 k ) \u2211 S\u2208Di,k Ai,k(75)\nwhere\nDi,k= { Bk(\u03b1i) \u2229 Bk(\u03b1i+1), 0 6 k 6 K\u2212 2 BK\u22121(\u03b1i) \u2229 BK\u22121(\u03b1i+1, K\u2212 1 6 k 6 N\u2212 2 (76)\nand\nAi,k= { 1, 0 6 k 6 K\u2212 2( N\u2212max r(S\u222a{\u03b1i,\u03b1i+1})\nk\u2212K+1\n) , K\u2212 1 6 k 6 N\u2212 2 (77)\nNote that |Bk(i)| 6 ( N\u22121 k ) . Thus, the complexity for computing the weighted KNN SV is at most\nN(N\u2212 1)\u00d7 ( N\u2212 1\nK\u2212 1 ) 6 ( e K\u2212 1 )K\u22121NK+1(78)\nProof of Theorem 7. Without loss of generality, we assume that the training points are sorted according to their distance to xtest, such that d(x1, xtest) 6 . . . 6 d(xN, xtest). We start by analyzing the SV for xN. Since the farthest training point does not affect the utility of S unless |S| 6 K\u2212 1, we have\nsN = 1\nN K\u22121\u2211 k=0 1( N\u22121 k ) \u2211 |S|=k,S\u2286I\\{N} [ \u03bd(S \u222a {N}) \u2212 \u03bd(S) ] For i 6 N\u2212 1, the application of Lemma 1 yields\nsi \u2212 si+1 = 1\nN\u2212 1 N\u22122\u2211 k=0 \u2211 |S|=k,S\u2286I\\{i,i+1} 1( N\u22122 k ) \u00b7 [\u03bd(S \u222a {i}) \u2212 \u03bd(S \u222a {i+ 1})](79) Recall that for KNN utility functions, \u03bd(S) only depends on the K training points closest to xtest. Therefore, we can also write si \u2212 si+1 as follows:\nsi \u2212 si+1 = 1\nN\u2212 1 K\u22121\u2211 k \u2032=0 \u2211 S \u2032\u2208Bk \u2032(i)\u2229Bk \u2032(i+1) Mk \u2032 i,i+1 [ \u03bd(S \u2032 \u222a {i}) \u2212 \u03bd(S \u2032 \u222a {i+ 1}) ] (80)\nwhich can be computed in at most \u2211K\u22121 k \u2032=0 ( N\u22122 k \u2032 ) \u223c O(NK), in contrast to O(2N\u22122) with (79). Our goal is thus to find Mk \u2032i,i+1 such that the right-hand sides of (80) and (79) are equal. More specifically, for each S \u2032 \u2208 Bk \u2032(i) \u2229 Bk \u2032(i + 1), we want to count the number of S \u2286 I \\ {i, i + 1} such that |S| = k, and \u03bd(S \u222a {i}) = \u03bd(S \u2032 \u222a {i}) and \u03bd(S \u222a {i+ 1}) = \u03bd(S \u2032 \u222a {i+ 1}); denoting the count by Ck,k \u2032i,i+1, we have\nMk \u2032 i,i+1 = N\u22122\u2211 k=0 Ck,k \u2032 i,i+1/ ( N\u2212 2 k ) .(81)\nWhen k \u2032 6 K\u2212 2, only S = S \u2032 satisfies \u03bd(S \u222a {i}) = \u03bd(S \u2032 \u222a {i}) and \u03bd(S \u222a {i+ 1}) = \u03bd(S \u2032 \u222a {i+ 1}). Therefore,\nCk,k \u2032\ni,i+1 =\n{ 1 if k \u2032 6 K\u2212 2 and k = k \u2032\n0 otherwise (82)\nWhen k \u2032 = K\u2212 1, there will be multiple subsets S of I \\ {i, i+ 1} that obey \u03bd(S \u222a {i}) = \u03bd(S \u2032 \u222a {i}) and \u03bd(S \u222a {i + 1}) = \u03bd(S \u2032 \u222a {i + 1}). Let r denote the index of the training point that is farthest to xtest\namong S\u222a {i, i+ 1}, i.e., r = maxS\u222a {i, i+ 1}. Note that adding any training points with indices larger than r into S \u2032 \u222a {i} or S \u2032 \u222a {i+ 1} would not affect their utility. Hence,\nCk,k \u2032\ni,i+1 =\n{ ( N\u2212r k\u2212K+1 ) if k \u2032 = K\u2212 1, k > k \u2032\n0 otherwise (83)\nCombining (80), (81), (82), and (83) yields the recursion in (74) and (75).\nE.3. Multiple Data Per Contributor. Now, we investigate the method to compute the SV when each seller provides more than one data instance. The goal is to fairly value individual sellers in lieu of individual training points. Following the previous notations, we still use I = {1, . . . ,N} to denote the set of all training instances and use Is to denote the set of all sellers, i.e., Is = {1, . . . ,M}. The number of training instances owned by jth seller is Nj. We denote the ith training point contributed by jth seller as x(i)j . Without loss of generality, we assume that every seller\u2019s data is sorted such that d(x (1) j , xtest) 6 . . . 6 d(x (Nj) j , xtest). Let h(i) denote the owner of ith training instance. With slight abuse of notations, we denote the owners of a set S of training instance as h(S), where S \u2286 I, and denote the training instances from the set of sellers S\u0303 \u2286 Is by h\u22121(S\u0303). Let N(S) = {\u03b11(S), . . . , \u03b1min{K,|S|}(S)} be a function that maps a set of training instances to its K-nearest neighbors. Let A = {S : S\u0303 \u2286 Is, |S\u0303| 6 K, S = N(h\u22121(S\u0303))} be the collection of all possible K-nearest neighbors formed by sellers; |S\u0303| 6 K because the top K instances cannot belong to more than K sellers. The next theorem shows that we can compute the SV of each seller with O(MK).\nTHEOREM 8. Consider the utility functions (5), (25), (26) or (27). Let A\\j = {S : S \u2208 A, j /\u2208 h(S)} be the set of top-K elements that do not contain sell j\u2019s data, D(S\u0303) = {S : S \u2208 A, h(S) = S\u0303} be the set of top-K elements of the data from the set S\u0303 of sellers, and G(S, j) = {j \u2032 : d(x(1)j \u2032 , xtest) > maxx\u2208S d(x, xtest), S \u2208 A\\j, j \u2032 \u2208 Is \\ {h(S), j}} be the set of sellers that do not affect the K-nearest neighbors when added into the sellers h(S) and S does not include seller j\u2019s data. Then, the SV of seller j can be represented as\nsj= 1\nM \u2211 S\u2208A\\j |G(S,j)|\u2211 k=0\n( |G(S,j)| k )( M\u22121\n|h(S)|+k )[\u03bd(D(h(S) \u222a {j}))\u2212\u03bd(S)](84) E.4. Valuing Computation. We show that one can compute the SV for both the sellers and the analyst with the same computational complexity as the one needed for the data-only game. The procedures to compute the SV for unweighted/weighted KNN classification/regression in the composite game setup are exhibited in the theorems below.\nE.4.1. Unweighted KNN classification.\nTHEOREM 9. Consider the utility function \u03bdc in (28), where \u03bd(\u00b7) is the KNN classification performance measure in (5). Then, the SV of each training point and the computation contributor can be calculated recursively as follows:\ns\u03b1N = K+ 1\n2(N+ 1)N 1[y\u03b1N = ytest](85)\ns\u03b1i = s\u03b1i+1 + 1[y\u03b1i = ytest] \u2212 1[y\u03b1i+1 = ytest] K \u00b7 min{i, K}(min{i, K}+ 1) 2i(i+ 1) (86)\nsC = \u03bd(I) \u2212 N\u2211 i=1 si(87)\nComparing s(\u03bd, i) in Theorem 1 and s(\u03bdc, i) in the above theorem, we have\ns(\u03bdc, \u03b1N)\ns(\u03bd, \u03b1N) = min{N,K}+ 1 2(N+ 1) (88)\ns(\u03bdc, \u03b1i) \u2212 s(\u03bdc, \u03b1i+1)\ns(\u03bd, \u03b1i) \u2212 s(\u03bd, \u03b1i+1) = min{i, K}+ 1 2(i+ 1) (89)\nNote that the right-hand side of (88) and (89) are at most 1/2 for all i = 1, . . . ,N \u2212 1; thus, each seller will receive a much smaller share of the total revenue in the composite game than that in the data-only game. Moreover, the analyst obtains a least one half of the total revenue in the composite game setup.\nE.4.2. Unweighted KNN Regression.\nTHEOREM 10. Consider the utility function in (28), where \u03bd(\u00b7) is the KNN regression performance measure in (25). Then, the SV of each training point and the computation contributor can be calculated recursively as follows:\ns\u03b1N = \u2212 1\nK(N+ 1) y\u03b1N\n[ (K+ 2)(K\u2212 1)\n2N ( 1 K y\u03b1N \u2212 2ytest) + 2(K\u2212 1)(K+ 1) 3N(N\u2212 1) \u2211 l\u2208I\\{\u03b1N} yl\n]\n\u2212 1\nN(N+ 1)\n[ 1\nK y\u03b1k(N) \u2212 ytest\n]2 (90)\ns\u03b1i = s\u03b1i+1 + 1\nK (y\u03b1i+1 \u2212 y\u03b1i) \u00b7 [( 1 K (y\u03b1i+1 + y\u03b1i) \u2212 2ytest ) \u00b7 min{K+ 1, i+ 1} \u00b7min{K, i} 2i(i+ 1)\n+ 1\nK \u2211 l\u2208{1,...,i\u22121} y\u03b1l \u00b7 2min(K+ 1, i+ 1)min(K, i)min(K\u2212 1, i\u2212 1) 3(i\u2212 1)i(i+ 1)\n+ 1\nK \u2211 l\u2208{i+2,...,N} y\u03b1l \u00b7 2min(K+ 1, l)min(K, l\u2212 1)min(K\u2212 1, l\u2212 2) 3l(l\u2212 1)(l\u2212 2) ] (91)\nsC = \u03bd(I) \u2212 N\u2211 i=1 si(92)\nE.4.3. Weighted KNN.\nTHEOREM 11. Consider the utility function in (28), where \u03bd(\u00b7) is the weighted KNN performance measure in (26) or (27) with some weights w\u03b1k(S). Let Bk(i) = {S : |S| = k, i /\u2208 S, S \u2286 I}, for i = 1, . . . ,N and k = 0, . . . , K. Let r(\u00b7) be a function that maps the set of training data to their ranks in terms of similarity to xtest. Then, the SV of each training point and the computation contributor can be calculated recursively as follows:\ns\u03b1N = 1\nN+ 1 K\u22121\u2211 k=0 1( N k+1 ) \u2211 S\u2208Bk(\u03b1N) \u03bd(S \u222a {\u03b1N}) \u2212 \u03bd(S)\n(93)\ns\u03b1i+1 = s\u03b1i + 1\nN K\u22122\u2211 k=0 1( N\u22121 k+1 ) \u2211 S\u2208Bk(\u03b1i)\u2229Bk(\u03b1i+1) \u03bd(S \u222a {\u03b1i}) \u2212 \u03bd(S \u222a {\u03b1i+1})\n+ 1\nN N\u22122\u2211 k=K\u22121 1( N\u22121 k+1 ) \u2211 S\u2208BK\u22121(\u03b1i)\u2229BK\u22121(\u03b1i+1) ( N\u2212 max r(S \u222a {\u03b1i, \u03b1i+1}) k\u2212 K+ 1 ) \u03bd(S \u222a {\u03b1i}) \u2212 \u03bd(S \u222a {\u03b1i+1}) (94)\nsC = \u03bd(I) \u2212 N\u2211 i=1 si\n(95)\nE.4.4. Multi-data-per-seller KNN.\nTHEOREM 12. Consider the utility functions (5), (25), (26) or (27). Let A\\j = {S : S \u2208 A, j /\u2208 h(S)} be the set of top-K elements that do not contain sell j\u2019s data,\nD(S\u0303) = {S : S \u2208 A, h(S) = S\u0303} be the set of top-K elements of the data from the set S\u0303 of sellers, and G(S, j) = {j \u2032 : d(x\n(1) j \u2032 , xtest) > maxx\u2208S d(x, xtest), S \u2208 A \\j, j \u2032 \u2208 Is \\ {h(S), j}} be the set of sellers that do not affect the K-nearest neighbors when added into the sellers h(S) and S does not include seller j\u2019s data. Then, the SV of seller j can be represented as\nsj = 1\nM+ 1 \u2211 S\u2208A\\j |G(S,j)|\u2211 k=0\n( |G(S,j)| k )( M\n|h(S)|+k+1 )[\u03bd(D(h(S) \u222a {j})) \u2212 \u03bd(S)](96) and the SV of the computation contributor is\nsC = \u03bd(I) \u2212 M\u2211 i=1 si(97)"
    },
    {
      "heading": "Appendix F. Generalization to Piecewise Utility Difference",
      "text": "A commonality of the utility functions between the unweighted KNN classifier and its extensions is that the difference in the marginal contribution of i and j to a set S \u2286 I \\ {i, j} has a \u201cpiecewise\u201d form:\n\u03bd(S \u222a i) \u2212 \u03bd(S \u222a j) = T\u2211 t=1 C (t) ij 1[S \u2208 St](98)\nwhere St \u2286 2I\\{i,j}. For instance, the utility difference for unweighted KNN classification obeys\n\u03bd(S \u222a {i}) \u2212 \u03bd(S \u222a {i+ 1}) = 1[yi = ytest] \u2212 1[yi+1 = ytest] K 1[S \u2208 S1](99)\nwhere we assume the training data is sorted according to their similarity to the test point and S1 = {S : \u2211 l\u2208S 1[d(xl, xtest) \u2212 d(xi, xtest) < 0] < K}(100)\nHence, we have T = 1 and C1ij = (1[yi = ytest] \u22121[yi+1 = ytest])/K for unweighted KNN classification utility function.\nThe utility difference for unweighted KNN regression can be expressed as\n\u03bd(S \u222a {i}) \u2212 \u03bd(S \u222a {i+ 1})\n= 1\nK (yi+1 \u2212 yi)(\n1 K (yi+1 + yi) \u2212 2ytest)1[S \u2208 S1] + 2 K2 (yi+1 \u2212 yi) \u2211 l\u2208I\\{i,i+1} yl1[S \u2208 S1, S 3 l](101)\nwhere S1 is defined in (100). Therefore, we can obtain the piecewise form of the utility difference in (98) by letting T = N\u2212 1, C(1)ij = 1 K(yi+1\u2212yi)( 1 K(yi+1+yi) \u2212 2ytest), {St} N\u22121 t=2 = {Sl}l\u2208I\\{i,i+1} where Sl = S1 \u2229 {S : l \u2208 S, S \u2286 I \\ {i, i+ 1}}, and the corresponding {C (t) ij } N\u22121 t=2 = { 2 K2\n(yi+1 \u2212 yi)yl}l\u2208I\\{i,i+1}. For weighted KNN utility functions, we can instantiate the utility difference (98) with T =\u2211K\nk=0 ( N\u22122 k ) adn St \u2286 2I\\{i,j} is a collection of sets that have the same top K elements.\nAn application of Lemma 1 to the utility functions with the piecewise utility difference form indicates that the difference in the SV between i and j can be represented as\nsi \u2212 sj = 1\nN\u2212 1 \u2211 S\u2286I\\{i,j} T\u2211 t=1 C (t) ij( N\u22122 |S|\n)1[S \u2208 St](102) = 1\nN\u2212 1 T\u2211 t=1 C (t) ij [N\u22122\u2211 k=0 |{S : S \u2286 I \\ {i, j}, S \u2208 St, |S| = k}|( N\u22122 k ) ](103) With the piecewise property (98), the SV calculation is reduced to a counting problem. As long as the quantity in the bracket of (103) can be efficiently evaluated, the SV can be obtained in O(NT)."
    },
    {
      "heading": "Appendix G. Proof of Theorem 5",
      "text": "Proof. We will use Bennett\u2019s inequality to derive the approximation error associated with the estimator in (4). Bennett\u2019s inequality provides an upper bound on the deviation of the empirical mean from the true mean in terms of the variance of the underlying random variable. Thus, we first provide an upper bound on the variance of \u03c6i for i = 1, . . . ,N. Let the range of \u03c6i for i = 1, . . . ,N be denoted by [\u2212r, r]. Further, let qi = P[\u03c6i = 0]. Let Wi be an indicator of whether or not \u03c6i = 0, i.e., Wi = 1[\u03c6i 6= 0]; thus P[Wi = 0] = qi and P[Wi = 1] = 1\u2212 qi.\nWe analyze the variance of \u03c6i. By the law of total variance,\nVar[\u03c6i] = E[Var[\u03c6i|Wi]] + Var[E[\u03c6i|Wi]](104)\nRecall \u03c6i \u2208 [\u2212r, r]. Then, the first term can be bounded by\nE[Var[\u03c6i|Wi]]\n= P[Wi = 0]Var[\u03c6i|Wi = 0] + P[Wi = 1]Var[\u03c6i|Wi = 1](105)\n= qiVar[\u03c6i|\u03c6i = 0] + (1\u2212 qi)Var[\u03c6i|\u03c6i 6= 0](106) = (1\u2212 qi)Var[\u03c6i|\u03c6i 6= 0](107)\n6 (1\u2212 qi)r 2(108)\nwhere the last inequality follows from the fact that if a random variable is in the range [m,M], then its variance is bounded by (M\u2212m) 2\n4 . The second term can be expressed as\nVar[E[\u03c6i|Wi]]\n= EWi [(E[\u03c6i|Wi] \u2212 E[\u03c6i]) 2](109) = P[Wi = 0](E[\u03c6i|Wi = 0] \u2212 E[\u03c6i])2 + P[Wi = 1](E[\u03c6i|Wi = 1] \u2212 E[\u03c6i])2(110)\n= qi(E[\u03c6i|\u03c6i = 0] \u2212 E[\u03c6i])2 + (1\u2212 qi)(E[\u03c6i|\u03c6i 6= 0] \u2212 E[\u03c6i])2(111)\n= qi(E[\u03c6i])2 + (1\u2212 qi)(E[\u03c6i|\u03c6i 6= 0] \u2212 E[\u03c6i])2(112)\nNote that\nE[\u03c6i] = P[Wi = 0]E[\u03c6i|\u03c6i = 0] + P[Wi = 1]E[\u03c6i|\u03c6i 6= 0](113) = (1\u2212 qi)E[\u03c6i|\u03c6i 6= 0](114)\nPlugging (114) into (109), we obtain\nVar[E[\u03c6i|W]] = (qi(1\u2212 qi)2 + q2i (1\u2212 qi))(E[\u03c6i|\u03c6i 6= 0])2(115)\nSince |\u03c6i| 6 r, (E[\u03c6i|\u03c6i 6= 0])2 6 r2. Therefore,\nVar[E[\u03c6i|W]] 6 qi(1\u2212 qi)r2(116)\nIt follows that\nVar[\u03c6i] 6 (1\u2212 q 2 i )r 2(117)\nTherefore, we can upper bound the variance of \u03c6i in terms of the probability that \u03c6=0. Now, let us compuate P[\u03c6i = 0] for i = 1, . . . ,N. Without loss of generality, we assume that xi are sorted according to their distance to the test point xtest in an ascending order. When i 6 K, then whatever place xi appears in the permutation \u03c0, adding xi to the set of points preceding i in the permutation will always potentially lead to a non-zero utility change. Therefore, we know that qi > 0 and\nVar[\u03c6i] 6 r 2 \u2261 \u03c32i for i = 1, . . . , K(118)\nWhen i > K+ 1, adding xi to P \u03c6 i may lead to zero utility change. More specifically, if there are\nno less than K elements in {x1, . . . , xi\u22121} appearing in P \u03c6 i , then adding i would not change the K nearest neighbors of P\u03c6i and thus \u03c6i. Let the position of xi in the permutation pi be denoted by k. Note that if there are at least K elements in {x1, . . . , xi\u22121} appearing before xi in the permutation, then xi must at least locate in order K+ 1 in the permutation, i.e., k > K+ 1. The number of permutations such that xi is in the kth slot and there are at least K elements appearing before xi is\nmin{i\u22121,k\u22121}\u2211 m=K ( k\u2212 1 m )( N\u2212 k i\u2212 1\u2212m ) (i\u2212 1)!(N\u2212 i)!(119)\nThus, the probability that \u03c6i is zero is lower bounded by\nq\u2217i =\n\u2211N k=K+1 \u2211min{i\u22121,k\u22121} m=K ( k\u22121 m )( N\u2212k i\u22121\u2212m ) (i\u2212 1)!(N\u2212 i)!\nN! (120)\n=\n\u2211N k=K+1 \u2211min{i\u22121,k\u22121} m=K ( k\u22121 m )( N\u2212k i\u22121\u2212m )( N\u22121 i\u22121 ) N (121)\n= i\u2212 K\ni (122) By (117), we have\nVar[\u03c6i] 6 (1\u2212 q \u22172 i )r 2 for i = K+ 1, . . . ,N(123)\nBy Bennett\u2019s inequality, we can bound the approximation error associated with s\u0302i by\nP[|s\u0302i \u2212 si| > ] 6 2 exp(\u2212 T\u03c32i r2 h( r\n\u03c32i ))(124)\nBy the union bound, if P[|s\u0302i \u2212 si| > ] 6 \u03b4i for all i = 1, . . . ,N and \u2211N i=1 \u03b4i = \u03b4, then we have\nP[max i |s\u0302i \u2212 si| > ] = P[\u222ai=1,...,N]{|s\u0302i\u2212si | > }] 6 N\u2211 i=1 P[|s\u0302i \u2212 si| > ] 6 N\u2211 i=1 \u03b4i = \u03b4(125)\nThus, to ensure that P[maxi |s\u0302i \u2212 si| > ] 6 \u03b4, we only need to choose T such that\n2 exp(\u2212 T\u03c32i r2 h( r\n\u03c32i )) 6 \u03b4i(126)\nwhich yields\nT > r2\n\u03c32ih( r \u03c32i\n) log\n2\n\u03b4i (127)\nSince\nr2\n\u03c32ih( r \u03c32i\n) 6\n1\n(1\u2212 q2i )h( (1\u2212q2i )r )\n(128)\nit suffices to let\nT > log 2\u03b4i\n(1\u2212 q2i )h( (1\u2212q2i )r )\n(129)\nfor all i = 1, . . . ,N. Therefore, we would like to choose {\u03b4i}Ni=1 such that maxi=1,...,N T \u2217 i is minimized. We can do this by letting\nlog 2\u03b4i (1\u2212 q2i )h(\n(1\u2212q2i )r\n) = T\u2217(130)\nwhich gives us\n\u03b4i = 2 exp(\u2212T\u2217(1\u2212 q2i )h(\n(1\u2212 q2i )r ))(131)\nSince \u2211N i=1 \u03b4i = \u03b4, we get\nN\u2211 i=1 exp(\u2212T\u2217(1\u2212 q2i )h( (1\u2212 q2i )r )) = \u03b4/2(132)\nand the value of T\u2217 can be solved numerically."
    },
    {
      "heading": "Appendix H. Derivation of the Approximate Lower Bound on Sample Complexity for the",
      "text": "Improved MC Approximation\nBecause log(1+u) > 2x2+x [Top06], we have h(u) > x2 2+x . Thus, (1\u2212q 2 i )h(\n(1\u2212q2i )r\n)) > 2\n2(1\u2212q2i )r+ r .\nFurthermore, by the definition of qi, (1\u2212 qi)2 = 1 for i = 1, . . . , K and decreases approximately with the speed 2K/i otherwise. Thus, the lower bound of (1 \u2212 q2i )h(\n(1\u2212q2i )r\n)) increases linearly with i\nwhen i > K+ 1. Letting x = exp(\u2212T\u2217), we can rewrite (32) as \u2211N i=1 x (1\u2212q2i )h( (1\u2212q2 i )r )) = \u03b4/2. In light of the above analysis, x (1\u2212q2i )h( (1\u2212q2 i )r ))\nwill have significant values when i > K and is comparatively negligible otherwise. Therefore, we can derive an approximate solution T\u0303 to T\u2217 by solving the following equation\nK exp(\u2212T\u0303h( r )) = \u03b4/2.(133) which gives us\nT\u0303 = 1\nh( /r) log\n2K\n\u03b4 (134)\nDue to the inequality h(u) 6 u2, we can obtain the following lower bound on T\u0303 :\nT\u0303 > r2\n2 log\n2K\n\u03b4 (135)"
    },
    {
      "heading": "UNIVERSITY OF CALIFORNIA, BERKELEY",
      "text": "E-mail : ruoxijia@berkeley.edu"
    },
    {
      "heading": "ETH ZURICH",
      "text": "E-mail : dwddao@gmail.com"
    },
    {
      "heading": "ZHEJIANG UNIVERSITY",
      "text": "E-mail : boxin.wang@outlook.com"
    },
    {
      "heading": "ETH ZURICH",
      "text": "E-mail : hubisf@student.ethz.ch"
    },
    {
      "heading": "ETH ZURICH",
      "text": "E-mail : nezihe.guerel@inf.ethz.ch"
    },
    {
      "heading": "UNIVERSITY OF ILLINOIS AT URBANACHAMPAIGN",
      "text": "E-mail : lxbosky@gmail.com"
    },
    {
      "heading": "ETH ZURICH",
      "text": "E-mail : ce.zhang@inf.ethz.ch"
    },
    {
      "heading": "UNIVERSITY OF CALIFORNIA, BERKELEY",
      "text": "E-mail : spanos@berkeley.edu"
    },
    {
      "heading": "UNIVERSITY OF CALIFORNIA, BERKELEY",
      "text": "E-mail : dawnsong@gmail.com"
    }
  ],
  "title": "Efficient Task-Specific Data Valuation for Nearest Neighbor Algorithms",
  "year": 2020
}
