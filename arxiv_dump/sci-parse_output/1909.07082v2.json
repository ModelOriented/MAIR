{"abstractText": "Explainable Artificial Intelligence (XAI) methods are typically deployed to explain and debug black-box machine learning models. However, most proposed XAI methods are black-boxes themselves and designed for images. Thus, they rely on visual interpretability to evaluate and prove explanations. In this work, we apply XAI methods previously used in the image and text-domain on time series. We present a methodology to test and evaluate various XAI methods on time series by introducing new verification techniques to incorporate the temporal dimension. We further conduct preliminary experiments to assess the quality of selected XAI method explanations with various verification methods on a range of datasets and inspecting quality metrics on it. We demonstrate that in our initial experiments, SHAP works robust for all models, but others like DeepLIFT, LRP, and Saliency Maps work better with specific architectures.", "authors": [{"affiliations": [], "name": "Udo Schlegel"}, {"affiliations": [], "name": "Hiba Arnout"}, {"affiliations": [], "name": "Daniela Oelke"}, {"affiliations": [], "name": "Daniel A. Keim"}], "id": "SP:6da93f07bb06fec9f3931cdcccec3586a751df1f", "references": [{"authors": ["L. Arras", "A. Osman", "K.-R. Mller", "W. Samek"], "title": "Evaluating Recurrent Neural Network Explanations", "venue": "8th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis", "year": 2019}, {"authors": ["S. Bach", "A. Binder", "G. Montavon", "F. Klauschen", "K.-R. Mller", "W. Samek"], "title": "On Pixel-Wise Explanations for Non-Linear Classifier Decisions by Layer-Wise Relevance Propagation", "venue": "PLOS ONE,", "year": 2015}, {"authors": ["M.C. Chuah", "F. Fu"], "title": "ECG anomaly detection via time series analysis", "venue": "International Symposium on Parallel and Distributed Processing and Applications, pages 123\u2013135. Springer", "year": 2007}, {"authors": ["G.E. Dahl", "D. Yu", "L. Deng", "A. Acero"], "title": "Contextdependent pre-trained deep neural networks for largevocabulary speech recognition", "venue": "IEEE Transactions on audio, speech, and language processing, 20(1):30\u201342", "year": 2011}, {"authors": ["H.A. Dau", "E. Keogh", "K. Kamgar", "C.-C.M. Yeh", "Y. Zhu", "S. Gharghabi", "C.A. Ratanamahatana", "Yanping", "B. Hu", "N. Begum", "A. Bagnall", "A. Mueen", "G. Batista"], "title": "The UCR Time Series Classification Archive", "year": 2018}, {"authors": ["F. Doshi-Velez", "B. Kim"], "title": "Towards A Rigorous Science of Interpretable Machine Learning", "venue": "A Roadmap for a Rigorous Science of Interpretability, (Ml):1\u201313", "year": 2017}, {"authors": ["A.H. Gee", "D. Garcia-Olano", "J. Ghosh", "D. Paydarfar"], "title": "Explaining Deep Classification of Time-Series Data with Learned Prototypes", "venue": "arXiv preprint arXiv:1904.08935, pages 1\u201316", "year": 2019}, {"authors": ["A.L. Goldberger", "L.A. Amaral", "L. Glass", "J.M. Hausdorff", "P.C. Ivanov", "R.G. Mark", "J.E. Mietus", "G.B. Moody", "C.K. Peng", "H.E. Stanley"], "title": "PhysioBank", "venue": "PhysioToolkit, and PhysioNet: components of a new research resource for complex physiologic signals. Circulation, 101 23:E215\u201320", "year": 2000}, {"authors": ["R. Guidotti", "A. Monreale", "S. Ruggieri", "F. Turini", "F. Giannotti", "D. Pedreschi"], "title": "A Survey Of Methods For Explaining Black Box Models", "venue": "ACM Computing Surveys, 51(5):93:1\u2013 93:42", "year": 2018}, {"authors": ["D. Gunning"], "title": "Explainable Artificial Intelligence (XAI) DARPA-BAA-16-53", "venue": "Technical report, Defense Advanced Research Projects Agency (DARPA),", "year": 2016}, {"authors": ["B. Hidasi", "C. Gspr-Papanek"], "title": "ShiftTree: An Interpretable Model-Based Approach for Time Series Classification", "venue": "D. Gunopulos, T. Hofmann, D. Malerba, and M. Vazirgiannis, editors, Machine Learning and Knowledge Discovery in Databases, pages 48\u201364, Berlin, Heidelberg", "year": 2011}, {"authors": ["E.-Y. Hsu", "C.-L. Liu", "V.S. Tseng"], "title": "Multivariate Time Series Early Classification with Interpretability Using Deep Learning and Attention Mechanism", "venue": "Q. Yang, Z.-H. Zhou, Z. Gong, M.-L. Zhang, and S.-J. Huang, editors, Advances in Knowledge Discovery and Data Mining, pages 541\u2013553, Cham", "year": 2019}, {"authors": ["B. Huval", "T. Wang", "S. Tandon", "J. Kiske", "W. Song", "J. Pazhayampallil", "M. Andriluka", "P. Rajpurkar", "T. Migimatsu", "R. Cheng-Yue"], "title": "and others", "venue": "An empirical evaluation of deep learning on highway driving. arXiv preprint arXiv:1504.01716", "year": 2015}, {"authors": ["K.-j. Kim"], "title": "Financial time series forecasting using support vector", "venue": "machines. Neurocomputing,", "year": 2003}, {"authors": ["S. Lundberg", "S.-I. Lee"], "title": "A Unified Approach to Interpreting Model Predictions", "venue": "In Advances in Neural Information Processing Systems,", "year": 2017}, {"authors": ["R.K. Mobley"], "title": "An Introduction to Predictive Maintenance", "venue": "Plant Engineering. Butterworth-Heinemann, Burlington, second edi edition", "year": 2002}, {"authors": ["S. Mohseni", "N. Zarei", "E.D. Ragan"], "title": "A Survey of Evaluation Methods and Measures for Interpretable Machine Learning", "venue": "arXiv preprint arXiv:1811.11839", "year": 2018}, {"authors": ["J. Redmon", "A. Farhadi"], "title": "YOLOv3: An Incremental Improvement", "venue": "ArXiv, abs/1804.02767", "year": 2018}, {"authors": ["M.T. Ribeiro", "S. Singh", "C. Guestrin"], "title": "Why Should I Trust You?", "venue": "International Conference on Knowledge Discovery and Data Mining, pages 1135\u20131144, New York, New York, USA", "year": 2016}, {"authors": ["C. Rudin"], "title": "Please Stop Explaining Black Box Models for High Stakes Decisions", "venue": "arXiv preprint arXiv:1811.10154,", "year": 2018}, {"authors": ["W. Samek", "A. Binder", "G. Montavon", "S. Lapuschkin", "K.R. Mller"], "title": "Evaluating the visualization of what a deep neural network has learned", "venue": "IEEE Transactions on Neural Networks and Learning Systems, 28(11):2660\u20132673", "year": 2017}, {"authors": ["W. Samek", "T. Wiegand", "K.-R. Mller"], "title": "Explainable Artificial Intelligence: Understanding", "venue": "Visualizing and Interpreting Deep Learning Models. arXiv preprint arXiv:1708.08296, abs/1708.08296", "year": 2017}, {"authors": ["A. Shrikumar", "P. Greenside", "A. Kundaje"], "title": "Learning Important Features Through Propagating Activation Differences", "venue": "International Conference on Machine Learning", "year": 2017}, {"authors": ["K. Simonyan", "A. Vedaldi", "A. Zisserman"], "title": "Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps", "venue": "arXiv preprint arXiv:1312.6034, pages 1\u20138", "year": 2013}, {"authors": ["M.D. Zeiler", "R. Fergus"], "title": "Visualizing and Understanding Convolutional Networks", "venue": "In Computer Vision,", "year": 2014}], "sections": [{"heading": "1. Introduction", "text": "Due to state-of-the-art performance of Deep Learning (DL) in many domains ranging from autonomous driving [14] to speech assistance [4] and the developing democratization of it, interpretability and explainability of such complex models captured more and more interest. Agencies such as DARPA introduced the explainable AI (XAI) initiative [11] to promote the research around interpretable Machine Learning (ML) to foster trust into models. Laws like the EU General Data Protection Regulation [7] got ratified to force companies to be able to explain the decisions of algorithms to support fairness and privacy and mitigate trust issues of users and costumers. The desiderata of ML systems (fairness, privacy, reliability, trust building [6]) led to a new selection process for models [21]. Depending\non the task either interpretable models, such as decision trees [12], or new XAI methods, e.g., local interpretable model-agnostic explanations (LIME) [20], on top of trained complex models, for instance, Convolutional Neural Networks (CNN) or Recurrent Neural Networks (RNN), are incorporated to guarantee the interpretability demands [10]. Due to these new methods for interpretability on a level above a model, we introduce a few new definitions. In the following, we refer, e.g., LIME as XAI method. Explainers are defined as an XAI method used on top of a model to get an XAI explanation of the decision making.\nMany prominent XAI methods are tailored onto certain input types such as images, e.g., Saliency Maps [25], or text, e.g., layer-wise relevance propagation (LRP) [2]. They often benefit of their domain to explain with certain aspects, such as a heatmap on the input [22], as they can be used as an overlay by building an abstract feature importance [10]. However, for instance, videos (sequences of images) and audio have another temporal dimension which is currently omitted by XAI methods. Only limited consideration is taken into account for sequence or temporal data, e.g., on XAI method evaluation on natural language processing [1]. There is currently only limited work about XAI on time series data such as interpretable decision tress [12], calculating prototypes [8] and using attention mechanisms [13]. Dividing the hard task of video classifier explanation into time series and image tasks is not possible as there is no good time series solution. However, due to sensors getting cheaper and cheaper, more time-oriented data besides video and audio is generated, and thus, it is important first to test already prominent XAI methods and discover new ones. Analyzing time series further enables to automate more actions, e.g., heartbeat anomaly detection [3], solve new tasks, e.g., predictive maintenance [17], and predict stock, e.g., stock market forecasting [15].\nar X\niv :1\n90 9.\n07 08\n2v 2\n[ cs\n.L G\n] 1\n7 Se\nTo debug and optimize time series prediction models in diverse tasks, not only understanding is essential but also that the XAI explanation is correct itself [18]. Evaluating and verifying these explanations is a difficult task due to raw time series being large and hardly interpretable even by domain experts themselves, and so an evaluation by raw data and explanation inspection is not feasible. Due to this lack of connectable domain knowledge, a quantifiable approach is necessary to verify explanations. Notably, in computer vision exists some work about the evaluation of explanations [23] (e.g., set relevant pixels to zero [26]), which is also possible to use on time series. However, these methods omit temporal dependencies by assuming feature independence or only local (short-term) dependency and thus are only limited verifiable on time-oriented data. Hence, adapted or novel variants of previous methods are needed to evaluate explanations on time series.\nIn this work, we show the practical use of various XAI methods on time series and present the first evaluation of selected methods on a variety of real-world benchmark datasets. Further, we introduce two sequence verification methods and a methodology to evaluate and check XAI explanations on time series automatically. In preliminary experiments, we show the results of our verification techniques for the selected XAI techniques and their results."}, {"heading": "2. Time Series Explanations", "text": "XAI methods have their main application field in computer vision due to the state-of-the-art success of black-box DL models in object recognition and detection [19] and the visual interpretability of the input [18]. However, a need for explainability is desired in other domains to either understand the decision making or to improve the models\u2019 performance by debugging failures. Thus, the domain of time series prediction has a high demand for XAI methods.\nA classification dataset with univariate time series data D consists of n samples with classes c1, c2, c3, ..., ck from a label (multiple classes) with k different classes. A sample t of D consists of m time points t = (t0, t1, t2, ..., tm). E.g., an anomaly detection dataset has only two classes (anomaly, e.g., c2, and normal, e.g., c1). In the following, the generally considered explanation of most XAI methods is the local feature importance. Time points get converted to features to introduce a workaround to use XAI methods on time series. The local feature importance produces a relevance ri for each time point ti. Afterward a tuple (ti, ri) can be build or more general for the time series vector t = (t0, t1, t2, ..., tm) a relevance vector can be generated as r = (r0, r1, r2, ..., rm).\nA model m trained on a subset X from D with labels Y can be formalized tom(x) = y with x \u2208 X and y \u2208 Y . The model m learns based on the provided data X , Y to predict an unseen subset Xnew. In the case of time series, x is a\nsample like t = (t0, t1, t2, ..., tm) with m time points. If then an XAI method xai is incorporated to explain the decisions of such a model, another layer on top of it is created. An explanation can then be formalized as xai(x,m) = exp with exp being the resulting explanation. With time series, the explanation exp is a relevance r = (r0, r1, r2, ..., rm) vector for m time points.\nSimilar to the saliency masks on images, a heatmap can be created based on the relevance produced by XAI methods. It is possible to create a visualization with this heatmap enriching a line plot of the original time series. Together with domain knowledge, an expert can inspect the produced explanation visualizations to verify the result qualitatively. Figure 1. shows an example of relevance heatmaps on time series. However, as these heatmaps are hard to interpret and a significant challenge to scale to large datasets or long time series, automated verification needs to be applied."}, {"heading": "3. Evaluating Time Series Explanations", "text": "There are various options on how to evaluate and verify XAI explanations automatically. In computer vision, a common method consists of a perturbation analysis [26]. This analysis method substitutes a few pixels (e.g., exchange to zero) of an image according to their importance (most or least relevant pixels). However, because, e.g., a zero could be an indicator for an anomaly in a time series task, the methodology of evaluation of XAI methods for time series needs specialized heuristics. We present two novel methods suited explicitly for time series by taking the sequence property of the time-oriented data into account."}, {"heading": "3.1. Perturbation on time series", "text": "At first, a perturbation analysis presents preliminary comparison baselines. The evaluation is based on the assumption that if relevant features (time points) get changed, the performance of an accurate model should decrease massively. If random time points of the data get changed, the performance should either stagnate or decrease. Perturbation Analysis \u2013 The assumption follows the time series t = (t0, t1, t2, ..., tm) and the relevance produced by the XAI method as r = (r0, r1, r2, ..., rm) to get a worse result of the quality metric qm for the classifier if combined. A time point ti gets gets changed if ri is larger than a certain threshold e, e.g. the 90th percentile of r. Due to XAI methods have problems with some time-series samples, the threshold leads to only changing a small number of time points. In the case of time series, the time point ti is set to zero or the inverse (maxti \u2212 ti) and leads to the new time series samples tzero and tinverse. Perturbation Verification \u2013 To verify the assumption, a random relevance rr = (r0, r1, r2, ..., rm) is used for the same procedure. The number of changed time points, amount of ri larger than the threshold e, is the same as in\nthe case before to set the same prerequisites for the classifier. This technique creates new time series like the perturbation analysis such as tzeror and t inverse r . The assumption to verify the model and the XAI method with the random relevance method follows the schema that the quality metric qm shows e.g. qm(t) \u2265 qm(tzeror ) > qm(tzero) for a model that maximizes qm."}, {"heading": "3.2. Sequence Evaluation", "text": "To verify that the model and the XAI method also includes time series features such as slopes or minima, we present two novel sequence-dependent methods. If the assumptions of the perturbation analysis hold, there is still a lack of evaluation of trends or patterns in the time series. E.g., for the classification, a decrease to zero could be significant, but the perturbation sets the zero to the max as it is essential for the model and so the classification should get worse. However, if a model learns the general pattern and generalizes good enough to overcome this change, the testing is useless. Thus to take the inter-dependency of time points into account, a closer look onto the time points itself is crucial. We propose two new techniques to test and evaluate XAI methods incorporating this hypothesis. Swap Time Points \u2013 The first additional method again takes the time series t = (t0, t1, t2, ..., tm) and the relevance for it r = (r0, r1, r2, ..., rm). However, it takes the time points with the relevance over the threshold as the starting point for further changes of the time series. So, ri > e describes the start point to extract the sub-sequence tsub = (ti, ti+1, ..., ti+ns) with length ns. The sub-sequence then gets reversed to tsub = (ti+ns , ..., ti+1, ti) and inserted back into the time series. Further, in another experiment, the sub-sequence gets set to zero to test the method. Also, like in the perturbation analysis, the same procedure is done with a random time point positions to verify the time points relevance again. Mean Time Points \u2013 Same as the first additional method,\nthe second one also takes into account the time series t = (t0, t1, t2, ..., tm) and the relevance for it r = (r0, r1, r2, ..., rm). Also, it takes the time points with the relevance over the threshold as the starting point for further changes of the time series. However, instead of swapping the time points, the mean \u00b5 of the sub-sequence tsub = (ti, ti+1, ..., ti+ns) is taken to exchange the whole sub-sequence to tsub = (\u00b5tsub , \u00b5tsub , ..., \u00b5tsub) and inserted back into the time series. Further, in another experiment, the sub-sequence gets set to zero to test the method. Also, like in the perturbation analysis, the same procedure is done with a random time point positions to verify the time points relevance again."}, {"heading": "3.3. Methodology", "text": "The methodology to verify an XAI method is conducted in three stages (model training and evaluation, model explanation creation, explanation evaluation, and verification).\n1. In the first step, a model learns the training data. Afterward, the trained model predicts the test data and a quality measure (e.g., accuracy) calculates the performance of the result.\n2. In the next step, a selected XAI method creates explanations for every sample of the test data. Based on the time point relevance by the explanations, the test data gets changed by the evaluation and verification methods mentioned before.\n3. Then, in the last step, each of these newly created test sets gets predicted by the model, and the quality measure is calculated for the comparison.\nIf the XAI method produces correct explanations, the assumptions qm(t) \u2265 qm(tcr) > qm(tc) with qm as the quality measure, t the original time series, tcr the random changed, and tc the relevant changed time series, holds."}, {"heading": "4. Discussion", "text": "The discussion divides into three parts. At first, the datasets and employed models are addressed to help to reproduce the experiments. Afterward, the selected XAI methods are introduced in short, giving an overview. Lastly, we discuss the preliminary evaluation results."}, {"heading": "4.1. Datasets & Models", "text": "Nine datasets of the UCR Time Series Classification Archive [5] and a ECG hearbeat dataset [9] are included in a real-world focused preliminary experiment. These ten datasets, namely FordA, FordB, ElectricDevices, MelbournePedestrian, ChlorineConcentration, Earthquakes, NonInvasiveFetalECGThorax1, NonInvasiveFetalECGThorax2, Strawberry [5], and Physionet\u2019s MITBIH Arrhythmia [9], consist of two different tasks, binary and multi-class prediction. Primarily, binary classification, e.g., for anomaly detection, is a critical use case for time-series predictions to tackle applications like predictive maintenance or heartbeat categorization.\nDuring the experiments, two different architectures (CNN and RNN) are used as baseline models. If available, the architecture provided by the dataset paper is also incorporated. The considered CNN consists of a 1D convolution layer with kernel and channel size of three. Afterward, a dense layer with 100 neurons learns the classification for a specific problem. The considered RNN consists of an LSTM layer with 100 neurons and again a dense layer with 100 neurons for the classifier. Both networks train each dataset individually for 50 epochs. The paper models consist of ResNet-based architectures and also 50 epochs."}, {"heading": "4.2. XAI methods", "text": "The experiment is conducted with the five most prominent XAI methods (LIME [20], LRP [2], DeepLIFT [24], Saliency Maps [25], SHAP [16]). LIME employs a socalled surrogate model to explain the decision of an ML model. Thru sampling data points around an example to be explained, it learns a linear model to extract local feature importance for the prediction of the more complex model. By propagating the gradients through the network, Saliency Maps and DeepLIFT build a heatmap as feature importance. LRP propagates a relevance score backward through the un-\nderlying model to specify feature importance. SHAP employs shapely values and game theory to find the best fitting feature to gain the most for the prediction."}, {"heading": "4.3. Results", "text": "Our preliminary results, see Table 1., show that DeepLIFT and LRP have the largest overall quality metric decrease in CNNS for the perturbation and sequence analysis, which shows the working local feature importance. Saliency Maps and SHAP outperform the others in RNNs by showing quality metric decreases, which is somewhat unexpected but shows a need for further exploration of RNNs with XAI methods. In more advanced ResNet architectures, SHAP produces the best results. However, also DeepLIFT and LRP show good results, which again shows the practical local feature importance. LIME shows terrible results in all cases, most likely due to large dimensionality and the employed linear classifier. Further, the results show the desiderata for the sequence verification methods as the random perturbation of time points has a significant quality metric decrease. Our proposed sequence verification methods present more clearly that the assumption qm(t) \u2265 qm(tmeanrandom) > qm(tmean) with qm as the quality measure, t the time series, tmeanrandom and t\nmean the changed time series holds."}, {"heading": "5. Conclusion and Future Work", "text": "Our methodology and verification methods show that XAI methods, proposed for images and text, work on time series data by specifying a relevance to time points. The methods also demonstrate that the models take the temporal aspect into account in some cases. In our experiment, we find that SHAP works robust for all models, but others like DeepLIFT, LRP, and Saliency Maps work better for specific architectures. LIME performs worst most likely because of the large dimensionality by converting time to features. However, we also conclude that a demand is given to introduce more suitable XAI methods on time series to guarantee a better human understanding in the process of XAI. As seen by the hard to interpret visual saliency masks (heatmaps) on time series, a need for a more abstract representation is necessary and increases the importance for more sophisticated visual XAI methods on time series."}], "title": "Towards a Rigorous Evaluation of XAI Methods on Time Series", "year": 2019}