{"abstractText": "Most of the work on interpretable machine learning has focused on designing either inherently interpretable models, which typically trade-off accuracy for interpretability, or post-hoc explanation systems, which lack guarantees about their explanation quality. We propose an alternative to these approaches by directly regularizing a black-box model for interpretability at training time. Our approach explicitly connects three key aspects of interpretable machine learning: (i) the model\u2019s innate explainability, (ii) the explanation system used at test time, and (iii) the metrics that measure explanation quality. Our regularization results in substantial improvement in terms of the explanation fidelity and stability metrics across a range of datasets and black-box explanation systems while slightly improving accuracy. Further, if the resulting model is still not sufficiently interpretable, the weight of the regularization term can be adjusted to achieve the desired trade-off between accuracy and interpretability. Finally, we justify theoretically that the benefits of explanation-based regularization generalize to unseen points.", "authors": [{"affiliations": [], "name": "Gregory Plumb"}, {"affiliations": [], "name": "Maruan Al-Shedivat"}, {"affiliations": [], "name": "Eric Xing"}, {"affiliations": [], "name": "Ameet Talwalkar"}], "id": "SP:18851b428d0fb0005b5486d93553da13ec0e7520", "references": [{"authors": ["Zachary C Lipton"], "title": "The mythos of model interpretability", "venue": "arXiv preprint arXiv:1606.03490,", "year": 2016}, {"authors": ["Marco Tulio Ribeiro", "Sameer Singh", "Carlos Guestrin"], "title": "Why should i trust you?: Explaining the predictions of any classifier", "venue": "In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining,", "year": 2016}, {"authors": ["Fulton Wang", "Cynthia Rudin"], "title": "Falling rule lists", "venue": "In Artificial Intelligence and Statistics,", "year": 2015}, {"authors": ["Rich Caruana"], "title": "Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission", "venue": "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "year": 2015}, {"authors": ["Tao Lei", "Regina Barzilay", "Tommi Jaakkola"], "title": "Rationalizing neural predictions", "venue": "arXiv preprint arXiv:1606.04155,", "year": 2016}, {"authors": ["Scott M Lundberg", "Su-In Lee"], "title": "A unified approach to interpreting model predictions", "venue": "In Advances in Neural Information Processing Systems,", "year": 2017}, {"authors": ["Been Kim", "Martin Wattenberg", "Justin Gilmer", "Carrie Cai", "James Wexler", "Fernanda Viegas"], "title": "Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav)", "venue": "In International Conference on Machine Learning,", "year": 2018}, {"authors": ["Maruan Al-Shedivat", "Avinava Dubey", "Eric P Xing"], "title": "Contextual explanation networks", "venue": "arXiv preprint arXiv:1705.10301,", "year": 2017}, {"authors": ["Gregory Plumb", "Denali Molitor", "Ameet S Talwalkar"], "title": "Model agnostic supervised local explanations", "venue": "In Advances in Neural Information Processing Systems,", "year": 2018}, {"authors": ["David Alvarez-Melis", "Tommi Jaakkola"], "title": "Towards robust interpretability with self-explaining neural networks", "venue": "In Advances in Neural Information Processing Systems,", "year": 2018}, {"authors": ["Karen Simonyan", "Andrea Vedaldi", "Andrew Zisserman"], "title": "Deep inside convolutional networks: Visualising image classification models and saliency maps", "venue": "arXiv preprint arXiv:1312.6034,", "year": 2013}, {"authors": ["Marco Tulio Ribeiro", "Sameer Singh", "Carlos Guestrin"], "title": "Anchors: High-precision model-agnostic explanations", "year": 2018}, {"authors": ["Luke de Oliveira", "Michael Kagan", "Lester Mackey", "Benjamin Nachman", "Ariel Schwartzman"], "title": "Jet-images\u2014deep learning edition", "venue": "Journal of High Energy Physics,", "year": 2016}, {"authors": ["Mukund Sundararajan", "Ankur Taly", "Qiqi Yan"], "title": "Axiomatic attribution for deep networks", "venue": "In Proceedings of the 34th International Conference on Machine LearningVolume", "year": 2017}, {"authors": ["Matthew D Zeiler", "Rob Fergus"], "title": "Visualizing and understanding convolutional networks", "venue": "In European conference on computer vision,", "year": 2014}, {"authors": ["Avanti Shrikumar", "Peyton Greenside", "Anna Shcherbina", "Anshul Kundaje"], "title": "Not just a black box: Learning important features through propagating activation differences", "venue": "arXiv preprint arXiv:1605.01713,", "year": 2016}, {"authors": ["Daniel Smilkov", "Nikhil Thorat", "Been Kim", "Fernanda Vi\u00e9gas", "Martin Wattenberg"], "title": "Smoothgrad: removing noise by adding noise", "venue": "arXiv preprint arXiv:1706.03825,", "year": 2017}, {"authors": ["Julius Adebayo", "Justin Gilmer", "Michael Muelly", "Ian Goodfellow", "Moritz Hardt", "Been Kim"], "title": "Sanity checks for saliency maps", "venue": "In Advances in Neural Information Processing Systems,", "year": 2018}, {"authors": ["Amirata Ghorbani", "Abubakar Abid", "James Zou"], "title": "Interpretation of neural networks is fragile", "venue": "arXiv preprint arXiv:1710.10547,", "year": 2017}, {"authors": ["David Alvarez-Melis", "Tommi S Jaakkola"], "title": "On the robustness of interpretability methods", "venue": "arXiv preprint arXiv:1806.08049,", "year": 2018}, {"authors": ["Andrew Slavin Ross", "Michael C Hughes", "Finale DoshiVelez"], "title": "Right for the right reasons: Training differentiable models by constraining their explanations", "venue": "arXiv preprint arXiv:1703.03717,", "year": 2017}, {"authors": ["Guang-He Lee", "Wengong Jin", "David Alvarez-Melis", "Tommi S Jaakkola"], "title": "Functional transparency for structured data: a game-theoretic approach", "year": 1902}, {"authors": ["Stephan Zheng", "Yang Song", "Thomas Leung", "Ian Goodfellow"], "title": "Improving the robustness of deep neural networks via stability training", "venue": "In Proceedings of the ieee conference on computer vision and pattern recognition,", "year": 2016}, {"authors": ["Yann LeCun"], "title": "The mnist database of handwritten digits. http://yann", "venue": "lecun. com/exdb/mnist/,", "year": 1998}, {"authors": ["Adam Bloniarz", "Ameet Talwalkar", "Bin Yu", "Christopher Wu"], "title": "Supervised neighborhoods for distributed nonparametric regression", "venue": "In Artificial Intelligence and Statistics,", "year": 2016}], "sections": [{"heading": "1. Introduction", "text": "Complex learning-based systems are increasingly shaping our daily lives, and, in order to monitor and understand these systems, we require clear explanations of model behavior. While model interpretability has many definitions and is often largely application specific (Lipton, 2016), local explanations are a popular and powerful tool (Ribeiro et al., 2016). Recent work on local interpretability in machine learning ranges from proposals of new models that are interpretable by-design (e.g., Wang and Rudin, 2015; Caruana et al., 2015) to model-agnostic, post-hoc algorithms for interpreting complex, black-box predictors such as ensem-\n1Machine Learning Department, Carnegie Mellon University, Pittsburgh, USA. Correspondence to: Gregory Plumb <gdplumb@andrew.cmu.edu>."}, {"heading": "2019 ICML Workshop on Human in the Loop Learning (HILL", "text": "2019), Long Beach, USA. Copyright by the author(s).\nbles and deep neural networks (e.g., Ribeiro et al., 2016; Lei et al., 2016; Lundberg and Lee, 2017; Selvaraju et al., 2017; Kim et al., 2018). Despite the variety of technical approaches, the underlying goal of all of these works is to develop an interpretable predictive system that produces two outputs: a prediction and its underlying explanation.\nBoth interpretability by-design and post-hoc explanation strategies have limitations. On the one hand, the by-design approaches are restricted to working with model families that provide inherent explainability, potentially at the cost of accuracy. On the other hand, by performing two disjointed steps, there is no guarantee that post-hoc explainers applied to an arbitrary model will produce explanations of suitable quality. Moreover, recent approaches that claim to overcome this apparent trade-off between prediction accuracy and explanation quality are in fact by-design proposals that impose certain constraints on the underlying model families they consider (e.g., Al-Shedivat et al., 2017; Plumb et al., 2018; Alvarez-Melis and Jaakkola, 2018a). In this work, we propose a novel alternative strategy called Explanationbased Optimization (ExpO) that aims to address both of these shortcomings by adding an interpretability regularizer to the loss function of an arbitrary predictive model. A small demo of how our regularizer can influence the explainability and accuracy of a model is in Figure 1a.\nIllustration. To motivate ExpO, consider a situation where Bob\u2019s loan application is denied by a machine learning system (see Figure 1b for a toy illustration). In this setting, a good local explanation can help Bob understand how to improve his application in order to get the loan. Unfortunately, as we see from Figure 1b, a standard model\u2014a multi-layer perceptron trained with SGD\u2014is difficult to explain well because it has many kinks and abrupt changes. Indeed, we can quantitatively measure the quality of local explanations using the standard fidelity (Ribeiro et al., 2016; Plumb et al., 2018) and stability (Alvarez-Melis and Jaakkola, 2018a) explanation metrics. To make the learned model more amenable to local explanation, ExpO augments the objective function with fidelity- or stability-based regularizers, effectively controlling the degree of local explainability. ar X iv :1\n90 6.\n01 43\n1v 1\n[ cs\n.L G\n] 3\n1 M\nay 2\n01 9\nThe specific contributions of our work are as follows:\n1. Interpretability Regularizers. We introduce two explanation regularizers associated with the fidelity and stability explanation metrics. The first, ExpO-Fidelity, is designed for semantic features and explainers that directly make predictions, such as (Ribeiro et al., 2016; Lundberg and Lee, 2017; Plumb et al., 2018). The second, ExpO-Stability, is tailored for non-semantic features (e.g., pixels) and explainers such as saliency maps (Simonyan et al., 2013), which identify features that are influential on a prediction. Both regularizers are differentiable and can be used to augment the objective function of an arbitrary model. In Section 3.1, we discuss how they differ from the classical approaches for local approximation and function smoothing.\n2. Generalizable Explanation Quality. We analyze the properties of the explanation quality metrics and show that the benefits of our regularization generalize to unseen points. Specifically, we derive a bound on the gap between the fidelity of explanations on training and held out points and connect it with the local variance of the learned model.\nEmpirical Results. We evaluate models trained with and without the proposed regularizers on a variety of regression and classification tasks with semantic and image features.1 We show experimentally that our regularizers slightly improve predictive performance across the nine datasets we consider (seven UCI regression tasks, a medical classification task, and MNIST). Moreover, from an interpretability perspective, our results demonstrate significant improvement in terms of explanation quality as measured by the fidelity and stability metrics. In particular, our regularization technique improved explanation fidelity by at least 25% on the UCI datasets and on the medical classification task; stability on MNIST was improved by orders of magnitude.\n1The code for our regularizers and all experiments is at:https://github.com/GDPlumb/ExpO"}, {"heading": "2. Background and Related Work", "text": "In this section, we introduce our notation, provide the necessary background on local explanations, and review the previous work that is most closely related to ExpO.\nConsider a supervised learning problem, where our goal is to estimate a model, f : X 7\u2192 Y , f \u2208 F , that maps input feature vectors, x \u2208 X , to targets, y \u2208 Y , and is trained using data, {xi, yi}Ni=1. If the class of functions used for modeling the data is complex, we can understand the behavior of f in some neighborhood, Nx \u2208 P[X ] (where P[X ] is the space of probability distributions over X ), by generating a local explanation. We denote algorithms that produce explanations (i.e., explainers) as e : X \u00d7 F 7\u2192 E , where E is the set of possible explanations. The choice of E generally depends on whether or not X consists of semantic features, and will be defined more precisely next."}, {"heading": "2.1. Semantic Features", "text": "We call features semantic if people can reason about them and understand what it means when their values change (e.g., a person\u2019s income, the concentration of a chemical, etc.). Consequently, local explanations try to predict how the model\u2019s output would change if the input was perturbed (Ribeiro et al., 2016; Lundberg and Lee, 2017; Plumb et al., 2018). Thus, we can define the output space of the explainer as Es := {g \u2208 G | g : X 7\u2192 Y}, where G is a class of interpretable (typically linear) functions.\nFidelity-Metric. When the explainer\u2019s output space is Es, the explanation is defined as a function g : X 7\u2192 Y , and it is natural to evaluate how accurately g models f in a neighborhood Nx (Ribeiro et al., 2016; Plumb et al., 2018):\nF (f, g,Nx) := Ex\u2032\u223cNx [(g(x\u2032)\u2212 f(x\u2032)) 2 ], (1)\nwhich we refer to as the neighborhood-fidelity (NF) metric. This metric is sometimes evaluated with Nx as a point mass on x and we call this version the point-fidelity (PF) metric. While Plumb et al. (2018) argued that point-fidelity can be misleading because it does not measure generalization of e(x, f) across Nx, it has been used for evaluation in the prior work (Ribeiro et al., 2016; Lundberg and Lee, 2017; Ribeiro et al., 2018) and we report it in our experiments along with the neighborhood-fidelity for completeness.\nBlack-box Explanation Systems. Various explainers have been proposed to generate local explanations of the form g : X 7\u2192 Y , typically assuming that g is linear. In particular, LIME (Ribeiro et al., 2016), one of the most popular black-box explanation systems2, solves the following\n2SHAP (Lundberg and Lee, 2017) is a popular variation of LIME that proposes a theoretically-motivated neighborhood sam-\noptimization problem:\ne(x, f) := arg min g\u2208Es F (f, g,Nx) + \u2126(g), (2)\nwhere \u2126(e) stands for an additive regularizer that encourages certain desirable properties of the explanations (e.g., sparsity). LIME\u2019s objective function is closely related to the fidelity metric and subsequently to our proposed ExpOFidelity regularizer. Consequently, we expect our regularizer to improve the quality of LIME-generated explanations and our experimental results in Section 4.1 corroborate this hypothesis.\nAlong with LIME, we consider another black-box explanation tool, called MAPLE (Plumb et al., 2018). It differs substantially from LIME in that its neighborhood function is learned from the data rather than specified as a parameter. In our experiments, we evaluate the quality of MAPLEgenerated local explanations for models regularized via ExpO-Fidelity, but do not use MAPLE\u2019s learned neighborhood function to define ExpO-Fidelity. We view this as a good test case to see how optimizing the fidelity metric for one neighborhood generalizes to another one (see Section 3 for a more detailed discussion of this point). In Section 4.1, we see that regularizing for LIME neighborhoods improves MAPLE\u2019s explanation quality."}, {"heading": "2.2. Non-Semantic Features", "text": "Non-semantic features lack an inherent interpretation, with images being a canonical example.3 When X consists of non-semantic inputs, we cannot assign meaning to the difference between x and x\u2032, hence it does not make sense to explain the difference between the predictions f(x) and f(x\u2032). As a result, fidelity is not an appropriate explanation metric. Instead, in this context, local explanations try to identify which parts of the input are particularly influential on a prediction (Sundararajan et al., 2017). Consequently, we consider explanations of the form Ens := Rd, where d is the number of features in X .\nStability Metric and Saliency Maps. When the explainer\u2019s output space is Ens, the explanation is a vector in Rd, and cannot be directly compared to the underlying model itself, as in the case of the fidelity metric. Instead, the focus in this setting is on the degree to which the explanation changes between points in a local neighborhood,\npling function, but requires explanations to be linear models that act on binary features. This requirement is too limiting in our case, hence SHAP is not used in our study.\n3In general, it is not clear how to interpret perturbations on the pixel level or whether such perturbations result in \u2018real\u2019 images. However, in certain cases such as scientific imaging (de Oliveira et al., 2016), each pixel value may have a precise meaning because of the way the images are processed.\nwhich we measure using the stability metric(Alvarez-Melis and Jaakkola, 2018a):\nS(f, e,Nx) := Ex\u2032\u223cNx [||e(x, f)\u2212 e(x\u2032, f)||22] (3)\nVarious explainers (Sundararajan et al., 2017; Zeiler and Fergus, 2014; Shrikumar et al., 2016; Smilkov et al., 2017; Adebayo et al., 2018) have been proposed to generate local explanations in Ens, with saliency maps (Simonyan et al., 2013) being a popular approach that we consider in this work. Saliency maps assign importance weights to image pixels based on the magnitude of the gradient of the predicted class with respect to the corresponding pixels.\nRecent work on model interpretability emphasizes that more stable explanations tend to be more trustworthy (AlvarezMelis and Jaakkola, 2018a; Ghorbani et al., 2017; AlvarezMelis and Jaakkola, 2018b). Note that the stability metric can also be considered in the context of semantic features in addition to the fidelity metric, and we consider both in our experiments."}, {"heading": "2.3. Related Methods", "text": "A few recently proposed approaches to model interpretability are closely related to our work. First, self-explaining neural networks (SENN) (Alvarez-Melis and Jaakkola, 2018a) (a variation of contextual explanation networks (CEN) (AlShedivat et al., 2017)) is an interpretable by-design approach that additionally (indirectly) optimizes their models to produce stable explanations. Second, \u201cRight For The Right Reasons\u201d (RTFR) (Ross et al., 2017) selectively penalizes gradients of the output with respect to certain input features at some points to discourage their use by the model. Finally, a work concurrent with ours (Lee et al., 2019) proposed to regularize models of structured data to encourage explainability in a way that is similar to ExpO.\nFrom a technical standpoint, SENN and RTFR both assume that the local explanation is close to the first order Taylor approximation of the model at that point. In Section 3.1, we demonstrate how Taylor approximations are often quite different from and more difficult to use than the neighborhood-based local explanations that we use in ExpO. Further, SENN\u2019s regularizer requires the neural network to have a very particular structure and, therefore, unlike ExpO, cannot by applied to an arbitrary model. While RTFR\u2019s regularization can be used with arbitrary models, it is not directly related to a measure of explanation quality; on the other hand, ExpO aims to directly improve quality of explanations with respect to a specific metric."}, {"heading": "3. Explanation Optimization", "text": "Running black-box explainers on arbitrary models does not guarantee the quality of the produced explanations. To\nAlgorithm 1. ExpO-Fidelity Regularizer input f\u03b8 , x, N regx , m\n1: Sample points: x\u20321, . . . , x \u2032 m \u223c N regx 2: Compute predictions:\ny\u0302j(\u03b8) = f\u03b8(x \u2032 j)\n3: Produce a local linear explanation:\n\u03b2x(\u03b8)=arg min\u03b2 \u2211m j=1(y\u0302j(\u03b8)\u2212\u03b2 >x\u2032j) 2\noutput 1 m \u2211m j=1(y\u0302j(\u03b8)\u2212\u03b2x(\u03b8) >x\u2032j) 2\nAlgorithm 2. ExpOStability Regularizer\ninput f\u03b8 , x, N regx , m 1: Sample points: x\u20321, . . . , x \u2032 m \u223c N regx\n2: Compute predictions:\ny\u0302j(\u03b8) = f\u03b8(x \u2032 j)\noutput 1 m \u2211m j=1(y\u0302j(\u03b8)\u2212f\u03b8(x)) 2\naddress this, we define a regularizer that can be added to the loss function and used to train an arbitrary model f . Specifically, we want to solve the following optimization problem:\nf\u0302 := argmin f\u2208F\n1\nN N\u2211 i=1 (L(f, xi, yi) + \u03b3R(f,N regxi )) (4)\nwhere L(f, xi, yi) is a standard predictive loss (e.g., squared error for regression or cross-entropy for classification), R(f,N regxi ) is a regularizer that encourages explainability of f in the neighborhood of xi, and \u03b3 > 0 controls the regularization strength.\nWe define R(f,N regx ) based on either the neighborhoodfidelity, Eq. (1), or the neighborhood-stability, Eq. (3). In order to compute these metrics exactly, we would need to run an explainer algorithm, e; this may be non-differentiable or too computationally expensive to use as a regularizer. Thus, for ExpO-Fidelity, we approximate e using a local linear model fit on points sampled fromNregx (Algorithm 1). For ExpO-Stability, we simply require that the model\u2019s output not change too much across Nregx (Algorithm 2). 4\nTo define a good regularization neighborhood, N regx , requires taking the following into consideration. On the one hand, we would like N regx to be similar to Nx, as used in Eq. 1 or Eq. 3, so that the neighborhoods used for regularization and for evaluation match. On the other hand, we also would like N regx to be consistent with the \u2018local neighborhood\u2019 defined by e internally, which may differ from Nx. For LIME, this is not a problem since the internal definition of the \u2018local neighborhood\u2019 is a hyperparameter that we can set. However for MAPLE, the \u2018local neighborhood\u2019 is learned from the data, and hence the regularization and explanation neighborhoods may differ. Ultimately, we left\n4We note that a similar procedure was explored previously in (Zheng et al., 2016) for adversarial robustness.\nresolving this tension to future work.\nComputational Cost. Algorithm 1 could be prohibitively expensive since the number of samples, m, from Nregx , has to be proportional to the dimension of x, resulting in O(d3) operations to compute the regularizer for a given point. In addition to running experiments with ExpO-Fidelity, we run experiments with a randomized version of the Algorithm 1 that randomly selects one dimension of x to perturb according to Nregx and penalizes the error of a local linear model along that dimension. This breaks the dependence of the computational cost of the objective function on the dimension of x (bringing it back to O(1)) and allows us to compute each gradient step with some constant increase in the number of function evaluations. We call this variation ExpO-1D-Fidelity."}, {"heading": "3.1. Understanding the Properties of ExpO", "text": "The goal of this section is to compare the behavior of local linear explanations and our regularizer to some existing theoretical function approximations and measures of variance to help develop an intuitive understanding of ExpO. First, we compare neighborhood-based local linear explanations to first order Taylor approximations to show that they can have fundamentally very different behaviors. Second, we compare ExpO-Fidelity to the Lipchitz Constant (LC) and Total Variation (TV) of the learned function.\nLocal Explanation vs. Taylor Approximations. A natural question to ask is, Why should we sample from Nx in order to locally approximate f when there are easier and theoretically motivated approximations? One possible way to do this is via the Taylor approximation (Alvarez-Melis and Jaakkola, 2018a). The downside of a Taylor approximationbased approach is that such approximation cannot readily be adjusted to different neighborhood scales and its fidelity and stability strictly depend on the learned function. This can be seen in Figure 2 where the Taylor approximations at two nearby points are both radically different and not faithful to the model outside of an infinitesimal neighborhood.\nFidelity-Regularization and the Model\u2019s LC or TV. From a theoretical perspective, our regularizer is similar to controlling the Lipschitz Constant or Total Variation of f across Nx after removing the part of f explained by e(x, f). From an interpretability perspective, there is nothing inherently wrong with having a large LC or TV, which is demonstrated in Figure 2. However, once we take into account what can be explained by e(x, f), then upper bounding any one of ExpO-Fidelity, the LC, or the TV will upper bound the remaining ones."}, {"heading": "3.2. Generalization of Local Linear Explanations", "text": "To conclude our analysis, we study the quality of local linear explanations in terms of generalization. Note that ExpO regularization encourages learning models that are explainable in the neighborhoods of each training point. However, how would this property generalize to unseen points?\nWe answer this question by providing a generalization bound in terms of neighborhood-fidelity metric. First, we assume that local linear explanations, \u03b2x, are obtained by solving the ordinary least squares regression problem (as given in Algorithm 1). The fidelity of the explanation in expectation over the neighborhood Nx can be computed analytically: r(f, x) = ENx [ f(x \u2032 ) 2 ] \u2212 ENx [ f(x \u2032 )x \u2032 ]> ENx [ [x \u2032 x \u2032> ] ]\u22121 ENx [ f(x \u2032 )x \u2032 ]\n(5)\nwhere expectation ENx [\u00b7] is taken with respect to x\u2032 over the neighborhood Nx. Note the equality in (5) is the expected value of the squared residual between f(x) and the optimal local linear explanation, which is upper-bounded by the variance of the model in the corresponding neighborhood.\nFor the explanations to generalize, we would like to make sure that the gap between the average fidelity on the training set and the expected fidelity is small with high probability. More formally, the following inequality should hold:\nP ( E [r(f, x)]\u2212 1\nn n\u2211 i=1 r(f, xi) > \u03b5\n) < \u03b4n(\u03b5) (6)\nUnder certain mild assumptions on the local behavior of f(x), the following proposition specifies a particular bound. Further, we show empirically that the benefits of our novel regularizers on explanation quality provably generalize to unseen test points.\nProposition 1 Let the neighborhood sampling function Nx be characterized by some parameter \u03c3 (e.g., the effective radius of a neighborhood) and the variance of the trained model f(x) across all such neighborhoods be bounded by some constant C(\u03c3) > 0. Then, the following bound holds\nwith at least 1\u2212 \u03b4 probability:\nE [r(f, x)] \u2264 1 n n\u2211 i=1 r(f, xi) +\n\u221a C2(\u03c3) log 1\u03b4\n2n (7)\nProof. (Sketch) By assumption, the variance of the model f(x) is bounded in each local neighborhood specified by Nx. Then (5) implies that each residual is bounded as 0 \u2264 r(f, x) \u2264 C(\u03c3). The result then follows by applying Hoeffding\u2019s inequality and rearranging the terms."}, {"heading": "4. Experimental Results", "text": "In our first set of experiments, we demonstrate the effectiveness of ExpO-Fidelity and ExpO-1D-Fidelity on datasets with semantic features using several regression problems from the UCI collection (Dheeru and\nKarra Taniskidou, 2017) as well as an in-hospital mortality classification problem.5 Our second experiment demonstrates the effectiveness of ExpO-Stability for creating saliency maps (Simonyan et al., 2013) on MNIST (LeCun, 1998). Dataset statistics are given in Table 1."}, {"heading": "4.1. Neighborhood-Fidelity Regularization", "text": "First, we compare models trained without our regularizers to models trained with them. We report accuracy and three interpretability metrics: (1) Point-Fidelity (PF), (2) Neighborhood-Fidelity (NF), (3) Stability (S) for explanations generated by LIME and MAPLE. For example, the \u201cMAPLE-PF\u201d label corresponds to the Point-Fidelity Metric for explanations produced by MAPLE.\nExperimental Setup. The network architectures and hyperparameters were chosen by a simple grid search. All inputs were standardized to have mean zero and variance one (including the response variable for regression problems). For the final set of experiments, we set Nx to be N (x, \u03c3) with \u03c3 = 0.1. Analysis of the effects of different neighborhood sizes is given in Figure 3a and shows that the size is not critical (the value of LIME-NF increase only slightly with \u03c3). For the UCI regression datasets and the in-hostpital mortality classification task, we set Nregx to be N (x, \u03c3) with \u03c3 = 0.5 as we found this to produce slightly more accurate and more interpretable models (Figure 3a).\n5http://biostat.mc.vanderbilt.edu/wiki/ Main/SupportDesc.\nEval Neighborhood: 0.10\nEval Neighborhood: 0.25 Eval Neighborhood: 0.50\nUCI Regression Experiments. The effects of ExpOFidelity and ExpO-1D-Fidelity on model accuracy and interpretability metrics are in Table 2. ExpO-Fidelity frequently improved the interpretability metrics by over 50%, with the smallest improvements being around 25%. In fact, our regularization lowered the prediction error on the \u2018communities\u2019, \u2018day\u2019, and \u2018YearPredictionMSD\u2019 datasets, which lets us conclude that it has a small positive effect on accuracy as well as a substantial benefit to the interpretability metrics. ExpO-1D-Fidelity, while generally having a similar effect, consistently improve interpretability of the models.\nWe also run experiments on the \u2018YearPredictionMSD\u2019 dataset6 to understand the scalability of ExpO to larger tasks. However, MAPLE-based evaluation was fairly slow on this dataset, and hence we only evaluate the interpretability metrics with respect to LIME on the first 1000 testing points. Both ExpO-Fidelity and ExpO-1D-Fidelity improved LIME\u2019s interpretability metrics at least 50% and both improved the model accuracy.\nMedical Classification Experiments. The SUPPORT2 dataset is used for in-hospital mortality prediction. The output layer of our models is the softmax over logits for two classes. Consequently, we run each explanation system on each of the individual logits. Table 3 presents the results. Again, we observe that ExpO-Fidelity did not affect the accuracy but did improve the interpretability metrics by 50% or more. ExpO-1D-Fidelity slightly decreased accuracy and did not improve the interpretability metrics by as much as ExpO-Fidelity, but it did improve them by at least 25%."}, {"heading": "4.2. Stability Regularization", "text": "In this final experiment, we fit a convolutional neural network to MNIST and then evaluate the stability of its\n6The task is to predict release year of song from a set of acoustic features, treated as a regression problem as in Bloniarz et al. (2016); the dataset is denoted MSD in Table 2.\n\u2020The relationship between inputs and targets in UCI Day dataset is very close to\nlinear and hence all errors are orders of magnitude smaller.\nAccuracy (%): None: 83.0\u00b1 0.3, Fidelity: 83.4\u00b1 0.4, 1D-Fidelity: 82.0\u00b1 0.3.\nsaliency maps to perturbations, where Nx \u2261 Nregx := Unif(x\u22120.05, x+0.05). Both an unregularized model and a model trained with ExpO-Stability achieved the accuracy of 99%. This demonstrates one of the practical differences between SENN (Alvarez-Melis and Jaakkola, 2018a) and our regularizers: SENN places strict structural constraints on the network and subsequently lowers the testing accuracy to roughly 97%; this is not the case for ExpO which can be applied to arbitrary networks. ExpO regularization decreased the average l2 distance between the explanation at x and some x\u2032 \u223c Nx from 6.94 to 0.0008. Finally, our regularization makes the resulting saliency maps look much better qualitatively by focusing on the presence or absence of certain pen strokes as seen in Figure 3b."}, {"heading": "5. Conclusion", "text": "In this work, we have introduced the novel idea of directly regularizing arbitrary models to be more interpretable. We contrasted our regularizers to classical approaches for function approximation and smoothing and provided a generalization bound for them. We demonstrated, across a variety of problem settings and explainers, that our regularizers slightly improve model accuracy and improve the interpretability metrics by somewhere from 25% to orders of magnitude."}], "title": "Regularizing Black-box Models for Improved Interpretability", "year": 2019}