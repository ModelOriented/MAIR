{
  "abstractText": "\u008ce emergence and growth of research on issues of ethics in Arti\u0080cial Intelligence, and in particular algorithmic fairness, has roots in an essential observation that structural inequalities in our society are re\u0083ected in the data used to train predictive models and in the design of objective functions. While research aiming to mitigate these issues is inherently interdisciplinary, the design of unbiased algorithms and fair socio-technical systems are key desired outcomes which depend on practitioners from the \u0080elds of data science and computing. However, these computing \u0080elds broadly also suffer from the same under-representation issues that are found in the datasets we analyze. \u008cis disconnect a\u0082ects the design of both the desired outcomes and metrics by which we measure success. If the ethical AI research community accepts this, we tacitly endorse the status quo and contradict the goals of non-discrimination and equity which work on algorithmic fairness, accountability, and transparency seeks to address. \u008cerefore, we advocate in this work for diversifying computing as a core priority of the \u0080eld and our efforts to achieve ethical AI practices. We draw connections between the lack of diversity within academic and professional computing \u0080elds and the type and breadth of the biases encountered in datasets, machine learning models, problem formulations, and the interpretation of results. Examining the current fairness/ethics in AI literature, we highlight cases where this lack of diverse perspectives has been foundational to the inequity in the treatments of underrepresented and protected group data. We also look to other professional communities, such as in the law and health domains, where disparities have been reduced both in the educational diversity of trainees and among their professional practices. We use these lessons to develop a set of recommendations that provide concrete steps for the computing community to increased diversity.",
  "authors": [
    {
      "affiliations": [],
      "name": "C. Kuhlman"
    },
    {
      "affiliations": [],
      "name": "L. Jackson"
    },
    {
      "affiliations": [],
      "name": "R. Chunara"
    }
  ],
  "id": "SP:8b4012bbc4aeff2b4b3bdbaea5281d5fd9e57c4c",
  "references": [
    {
      "authors": [
        "Aws Albarghouthi",
        "Samuel Vinitsky"
      ],
      "title": "Fairness-aware programming",
      "venue": "In Proceedings of the Conference on Fairness, Accountability, and Transparency",
      "year": 2019
    },
    {
      "authors": [
        "Junaid Ali",
        "Muhammad Bilal Zafar",
        "Adish Singla",
        "Krishna P Gummadi"
      ],
      "title": "Loss-Aversively Fair Classi\u0080cation",
      "venue": "In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",
      "year": 2019
    },
    {
      "authors": [
        "Marcella Alsan",
        "Owen Garrick",
        "Grant C Graziani"
      ],
      "title": "Does diversity ma\u0088er for health? Experimental evidence from Oakland",
      "venue": "Technical Report. National Bureau of Economic Research",
      "year": 2018
    },
    {
      "authors": [
        "Alexander Amini",
        "Ava Soleimany",
        "Wilko Schwarting",
        "Sangeeta Bhatia",
        "Daniela Rus"
      ],
      "title": "Uncovering and Mitigating Algorithmic Bias through Learned Latent Structure",
      "year": 2019
    },
    {
      "authors": [
        "Megan Bang",
        "Douglas Medin"
      ],
      "title": "Cultural processes in science education: Supporting the navigation of multiple epistemologies",
      "venue": "Science Education 94,",
      "year": 2010
    },
    {
      "authors": [
        "Solon Barocas",
        "Moritz Hardt",
        "Arvind Narayanan"
      ],
      "title": "Fairness andMachine Learning. fairmlbook.org. h\u008ap://www.fairmlbook.org",
      "year": 2019
    },
    {
      "authors": [
        "Solon Barocas",
        "Andrew D"
      ],
      "title": "Selbst. 2016",
      "venue": "Big data\u2019s disparate impact. Cal. L. Rev",
      "year": 2016
    },
    {
      "authors": [
        "Tolga Bolukbasi",
        "Kai-Wei Chang",
        "James Y Zou",
        "Venkatesh Saligrama",
        "Adam T Kalai"
      ],
      "title": "Man is to computer programmer as woman is to homemaker? debiasing word embeddings",
      "venue": "In Advances in neural information processing systems",
      "year": 2016
    },
    {
      "authors": [
        "Christian Borgs",
        "Jennifer Chayes",
        "Nika Haghtalab",
        "Adam Tauman Kalai",
        "Ellen Vitercik"
      ],
      "title": "Algorithmic greenlining: An approach to increase diversity",
      "venue": "In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",
      "year": 2019
    },
    {
      "authors": [
        "Hannah Riley Bowles",
        "Linda Babcock",
        "Lei Lai"
      ],
      "title": "It depends who is asking and who you ask: Social incentives for sex di\u0082erences in the propensity to initiate negotiation",
      "year": 2005
    },
    {
      "authors": [
        "Alison Wood Brooks",
        "Laura Huang",
        "Sarah Wood Kearney",
        "Fiona E Murray"
      ],
      "title": "Investors prefer entrepreneurial ventures pitched by a\u008aractive men",
      "venue": "Proceedings of the National Academy of Sciences 111,",
      "year": 2014
    },
    {
      "authors": [
        "Jeanne Brooks-Gunn",
        "Fong-ruey Liaw",
        "Pamela Kato Klebanov"
      ],
      "title": "E\u0082ects of early intervention on cognitive function of low birth weight preterm infants",
      "venue": "Pediatric Physical \u008aerapy 6,",
      "year": 1994
    },
    {
      "authors": [
        "Brown",
        "Danielle Brown",
        "Parker",
        "Melonie"
      ],
      "title": "Google diversity annual report",
      "venue": "h\u008aps://diversity.google/annual-report/. [Online; accessed",
      "year": 2019
    },
    {
      "authors": [
        "Michael Buckley",
        "John Nordlinger",
        "Devika Subramanian"
      ],
      "title": "Socially relevant computing",
      "venue": "In ACM SIGCSE Bulletin,",
      "year": 2008
    },
    {
      "authors": [
        "Joy Buolamwini",
        "Timnit Gebru"
      ],
      "title": "Gender shades: Intersectional accuracy disparities in commercial gender classi\u0080cation",
      "venue": "In Conference on fairness, accountability and transparency",
      "year": 2018
    },
    {
      "authors": [
        "Diana J Burgess"
      ],
      "title": "Addressing racial healthcare disparities: how can we shi\u0089 the focus from patients to providers",
      "venue": "Journal of general internal medicine 26,",
      "year": 2011
    },
    {
      "authors": [
        "Sheryl Burgstahler"
      ],
      "title": "Universal design: Implications for computing education",
      "venue": "ACM Transactions on Computing Education (TOCE) 11,",
      "year": 2011
    },
    {
      "authors": [
        "Toon Calders",
        "Sicco Verwer"
      ],
      "title": "\u008cree naive Bayes approaches for discrimination-free classi\u0080cation",
      "venue": "Data Mining and Knowledge Discovery 21,",
      "year": 2010
    },
    {
      "authors": [
        "Aylin Caliskan",
        "Joanna J Bryson",
        "Arvind Narayanan"
      ],
      "title": "Semantics derived automatically from language corpora contain human-like biases",
      "venue": "Science 356,",
      "year": 2017
    },
    {
      "authors": [
        "Ran Cane\u008ai",
        "Aloni Cohen",
        "Nishanth Dikkala",
        "Govind Ramnarayan",
        "Sarah Schef- \u0083er",
        "Adam Smith"
      ],
      "title": "From so\u0089 classi\u0080ers to hard decisions: How fair 8 can we be",
      "venue": "In Proceedings of the Conference on Fairness, Accountability, and Transparency",
      "year": 2019
    },
    {
      "authors": [
        "Rodrigo L Cardoso",
        "Wagner Meira Jr.",
        "Virgilio Almeida",
        "Mohammed J Zaki"
      ],
      "title": "A Framework for Benchmarking Discrimination-Aware Models in Machine Learning",
      "year": 2019
    },
    {
      "authors": [
        "Vicky Ca\u008aell"
      ],
      "title": "Poor people, poor places, and poor health: the mediating role of social networks and social capital",
      "venue": "Social Science & Medicine",
      "year": 2013
    },
    {
      "authors": [
        "L Elisa Celis",
        "Lingxiao Huang",
        "Vijay Keswani",
        "Nisheeth K Vishnoi"
      ],
      "title": "Classi\u0080cation with fairness constraints: A meta-algorithm with provable guarantees",
      "venue": "In Proceedings of the Conference on Fairness, Accountability, and Transparency",
      "year": 2019
    },
    {
      "authors": [
        "Irene Chen",
        "Fredrik D Johansson",
        "David Sontag"
      ],
      "title": "Why is my classi\u0080er discriminatory",
      "venue": "In Advances in Neural Information Processing Systems",
      "year": 2018
    },
    {
      "authors": [
        "Jiahao Chen",
        "Nathan Kallus",
        "Xiaojie Mao",
        "Geo\u0082ry Svacha",
        "Madeleine Udell"
      ],
      "title": "Fairness under unawareness: Assessing disparity when protected class is unobserved",
      "venue": "In Proceedings of the Conference on Fairness, Accountability, and Transparency",
      "year": 2019
    },
    {
      "authors": [
        "Jordan J Cohen",
        "Barbara A Gabriel",
        "Charles Terrell"
      ],
      "title": "\u008ce case for diversity in the health care workforce",
      "venue": "Health a\u0082airs 21,",
      "year": 2002
    },
    {
      "authors": [
        "Amanda Coston",
        "Karthikeyan Natesan Ramamurthy",
        "Dennis Wei",
        "Kush R Varshney",
        "Skyler Speakman",
        "Zairah Mustahsan",
        "Supriyo Chakraborty"
      ],
      "title": "Fair transfer learning with missing protected a\u008aributes",
      "venue": "In Proceedings of the AAAI/ACM Conference on Arti\u0080cial Intelligence,",
      "year": 2019
    },
    {
      "authors": [
        "Maria De-Arteaga",
        "Alexey Romanov",
        "Hanna Wallach",
        "Jennifer Chayes",
        "Christian Borgs",
        "Alexandra Chouldechova",
        "Sahin Geyik",
        "Krishnaram Kenthapadi",
        "Adam Tauman Kalai"
      ],
      "title": "Bias in bios: A case study of semantic representation bias in a high-stakes se\u008aing",
      "venue": "In Proceedings of the Conference on Fairness, Accountability, and Transparency",
      "year": 2019
    },
    {
      "authors": [
        "Cristian L Dezs\u00f6",
        "David Gaddis Ross"
      ],
      "title": "Does female representation in top management improve \u0080rm performance? A panel data investigation",
      "venue": "Strategic Management Journal 33,",
      "year": 2012
    },
    {
      "authors": [
        "Cynthia Dwork",
        "Moritz Hardt",
        "Toniann Pitassi",
        "Omer Reingold",
        "Richard Zemel"
      ],
      "title": "Fairness through awareness",
      "venue": "In Proceedings of the 3rd innovations in theoretical computer science conference",
      "year": 2012
    },
    {
      "authors": [
        "Michael Feldman",
        "Sorelle A Friedler",
        "John Moeller",
        "Carlos Scheidegger",
        "Suresh Venkatasubramanian"
      ],
      "title": "Certifying and removing disparate impact",
      "venue": "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
      "year": 2015
    },
    {
      "authors": [
        "Sorelle A Friedler",
        "Carlos Scheidegger",
        "Suresh Venkatasubramanian",
        "Sonam Choudhary",
        "Evan P Hamilton",
        "Derek Roth"
      ],
      "title": "A comparative study of fairness-enhancing interventions in machine learning",
      "venue": "In Proceedings of the Conference on Fairness, Accountability, and Transparency",
      "year": 2019
    },
    {
      "authors": [
        "Nikhil Garg",
        "Londa Schiebinger",
        "Dan Jurafsky",
        "James Zou"
      ],
      "title": "Word embeddings quantify 100 years of gender and ethnic stereotypes",
      "venue": "Proceedings of the National Academy of Sciences 115,",
      "year": 2018
    },
    {
      "authors": [
        "Naman Goel",
        "Boi Faltings"
      ],
      "title": "Crowdsourcing with Fairness, Diversity and Budget Constraints",
      "venue": "In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",
      "year": 2019
    },
    {
      "authors": [
        "Ben Green",
        "Yiling Chen"
      ],
      "title": "Disparate interactions: An algorithm-in-theloop analysis of fairness in risk assessments",
      "venue": "In Proceedings of the Conference on Fairness, Accountability, and Transparency",
      "year": 2019
    },
    {
      "authors": [
        "Moritz Hardt",
        "Eric Price",
        "Nati Srebro"
      ],
      "title": "Equality of opportunity in supervised learning",
      "venue": "In Advances in Neural Information Processing Systems",
      "year": 2016
    },
    {
      "authors": [
        "Hoda Heidari",
        "Michele Loi",
        "Krishna P Gummadi",
        "Andreas Krause"
      ],
      "title": "A moral framework for understanding of fair ml through economic models of equality of opportunity",
      "year": 2018
    },
    {
      "authors": [
        "Kenneth Holstein",
        "Jennifer Wortman Vaughan",
        "Hal Daum\u00e9 III",
        "Miro Dudik",
        "Hanna Wallach"
      ],
      "title": "Improving fairness in machine learning systems: What do industry practitioners need",
      "venue": "In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems",
      "year": 2019
    },
    {
      "authors": [
        "Herminia Ibarra"
      ],
      "title": "Race, opportunity, and diversity of social circles in managerial networks",
      "venue": "Academy of management journal 38,",
      "year": 1995
    },
    {
      "authors": [
        "Pablo Ibarrar\u00e1n",
        "Nadin Medell\u0131\u0301n",
        "Ferdinando Regalia",
        "Marco Stampini",
        "Sandro Parodi",
        "Luis Tejerina",
        "Pedro Cueva",
        "Madiery V\u00e1squez"
      ],
      "title": "How conditional cash transfers",
      "year": 2017
    },
    {
      "authors": [
        "Latifa Jackson",
        "Caitlin Kuhlman",
        "Fatimah Jackson",
        "P Keolu Fox"
      ],
      "title": "Including Vulnerable Populations in the Assessment of Data From Vulnerable Populations",
      "venue": "Frontiers in Big Data",
      "year": 2019
    },
    {
      "authors": [
        "Sampath Kannan",
        "Aaron Roth",
        "Juba Ziani"
      ],
      "title": "Downstream e\u0082ects of a\u0081rmative action",
      "venue": "In Proceedings of the Conference on Fairness, Accountability, and Transparency",
      "year": 2019
    },
    {
      "authors": [
        "Michael Kearns",
        "Seth Neel",
        "Aaron Roth",
        "Zhiwei Steven Wu"
      ],
      "title": "An empirical study of rich subgroup fairness for machine learning",
      "venue": "In Proceedings of the Conference on Fairness, Accountability, and Transparency",
      "year": 2019
    },
    {
      "authors": [
        "Michael P Kim",
        "Amirata Ghorbani",
        "James Zou"
      ],
      "title": "Multiaccuracy: Blackbox post-processing for fairness in classi\u0080cation",
      "venue": "In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",
      "year": 2019
    },
    {
      "authors": [
        "Michael Lachney"
      ],
      "title": "Computational communities: African-American cultural capital in computer science education",
      "venue": "Computer Science Education",
      "year": 2017
    },
    {
      "authors": [
        "Himabindu Lakkaraju",
        "Jon Kleinberg",
        "Jure Leskovec",
        "Jens Ludwig",
        "Sendhil Mullainathan"
      ],
      "title": "\u008ce selective labels problem: Evaluating algorithmic predictions in the presence of unobservables",
      "venue": "In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",
      "year": 2017
    },
    {
      "authors": [
        "Henry J Lowe",
        "Todd A Ferris",
        "Penni M Hernandez",
        "Susan C Weber"
      ],
      "title": "STRIDE\u2013An integrated standards-based translational research informatics platform",
      "venue": "In AMIA Annual Symposium Proceedings,",
      "year": 2009
    },
    {
      "authors": [
        "David Madras",
        "Elliot Creager",
        "Toniann Pitassi",
        "Richard Zemel"
      ],
      "title": "Fairness through causal awareness: Learning causal latent-variable models for biased data",
      "venue": "In Proceedings of the Conference on Fairness, Accountability, and Transparency",
      "year": 2019
    },
    {
      "authors": [
        "Daniel McNamara",
        "Cheng Soon Ong",
        "Robert C Williamson"
      ],
      "title": "Costs and bene\u0080ts of fair representation learning",
      "venue": "In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",
      "year": 2019
    },
    {
      "authors": [
        "Smitha Milli",
        "John Miller",
        "Anca D Dragan",
        "Moritz Hardt"
      ],
      "title": "\u008ce Social Cost of Strategic Classi\u0080cation",
      "venue": "In Proceedings of the Conference on Fairness, Accountability, and Transparency",
      "year": 2019
    },
    {
      "authors": [
        "Corinne A Moss-Racusin",
        "John F Dovidio",
        "Victoria L Brescoll",
        "Mark J Graham",
        "Jo Handelsman"
      ],
      "title": "Science faculty\u2019s subtle gender biases favor male students",
      "venue": "Proceedings of the National Academy of Sciences 109,",
      "year": 2012
    },
    {
      "authors": [
        "Sendhil Mullainathan",
        "Ziad Obermeyer"
      ],
      "title": "Who is Tested for Heart A\u008aack and Who Should Be: Predicting Patient Risk and Physician Error",
      "venue": "NBER Working Paper Series",
      "year": 2019
    },
    {
      "authors": [
        "Susan D Newman",
        "Jeanne\u008ae O Andrews",
        "Gayenell S Magwood",
        "Carolyn Jenkins",
        "Melissa J Cox",
        "Deborah C Williamson"
      ],
      "title": "Peer reviewed: community advisory boards in community-based participatory research: a synthesis of best processes. Preventing chronic disease",
      "year": 2011
    },
    {
      "authors": [
        "Mathias Wullum Nielsen",
        "Carter Walter Bloch",
        "Londa Schiebinger"
      ],
      "title": "Making gender diversity work for scienti\u0080c discovery and innovation",
      "venue": "Nature Human Behaviour (2018),",
      "year": 2018
    },
    {
      "authors": [
        "Marcus Noland",
        "Tyler Moran",
        "Barbara R Kotschwar"
      ],
      "title": "Is gender diversity pro\u0080table? Evidence from a global survey",
      "venue": "Peterson Institute for International Economics Working Paper",
      "year": 2016
    },
    {
      "authors": [
        "Alejandro Noriega-Campero",
        "Michiel Bakker",
        "Bernardo Garcia-Bulle",
        "Alex Pentland"
      ],
      "title": "Active Fairness in Algorithmic Decision Making",
      "venue": "arXiv preprint arXiv:1810.00031",
      "year": 2018
    },
    {
      "authors": [
        "Ziad Obermeyer",
        "Sendhil Mullainathan"
      ],
      "title": "Dissecting Racial Bias in an Algorithm that Guides Health Decisions for 70 Million People",
      "venue": "In Proceedings of the Conference on Fairness, Accountability, and Transparency",
      "year": 2019
    },
    {
      "authors": [
        "Luca Oneto",
        "Michele Doninini",
        "Amon Elders",
        "Massimiliano Pontil"
      ],
      "title": "Taking advantage of multitask learning for fair classi\u0080cation",
      "venue": "In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",
      "year": 2019
    },
    {
      "authors": [
        "Paul R. Hernandez",
        "Richard T. Serpe"
      ],
      "title": "Patching the Pipeline: Reducing Educational Disparities in the Sciences \u008crough Minority Training Programs",
      "venue": "Educ Eval Policy Anal 33,",
      "year": 2013
    },
    {
      "authors": [
        "Samir Passi",
        "Solon Barocas"
      ],
      "title": "Problem formulation and fairness",
      "venue": "In Proceedings of the Conference on Fairness, Accountability, and Transparency",
      "year": 2019
    },
    {
      "authors": [
        "Stephen Pfohl",
        "Ben Mara\u0080no",
        "Adrien Coulet",
        "Fatima Rodriguez",
        "Latha Palaniappan",
        "Nigam H Shah"
      ],
      "title": "Creating fair models of atherosclerotic cardiovascular disease risk",
      "venue": "In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society",
      "year": 2019
    },
    {
      "authors": [
        "Julia C. Prentice",
        "Steven D. Pizer"
      ],
      "title": "Delayed Access to Health Care and Mortality",
      "venue": "Health Services Research",
      "year": 2013
    },
    {
      "authors": [
        "Lincoln \u008billian",
        "Devah Pager",
        "Ole Hexel",
        "Arn\u0080nn H"
      ],
      "title": "Metaanalysis of \u0080eld experiments shows no change in racial discrimination in hiring over time",
      "venue": "Midtb\u00f8en",
      "year": 2017
    },
    {
      "authors": [
        "Inioluwa Deborah Raji",
        "Joy Buolamwini"
      ],
      "title": "Actionable auditing: Investigating the impact of publicly naming biased performance results of commercial ai products",
      "venue": "In AAAI/ACM Conf. on AI Ethics and Society,",
      "year": 2019
    },
    {
      "authors": [
        "Heraldo V Richards",
        "Ayanna F Brown",
        "Timothy B Forde"
      ],
      "title": "Addressing diversity in schools: Culturally responsive pedagogy",
      "venue": "Teaching Exceptional Children 39,",
      "year": 2007
    },
    {
      "authors": [
        "Edward Royce"
      ],
      "title": "Poverty and Power: \u008ae Problem of Structural Inequality (3rd Ed.)",
      "venue": "Rowman and Li\u008ale\u0080eld,",
      "year": 2019
    },
    {
      "authors": [
        "Chris Russell"
      ],
      "title": "E\u0081cient search for diverse coherent explanations",
      "venue": "arXiv preprint arXiv:1901.04909",
      "year": 2019
    },
    {
      "authors": [
        "Aaron Roth Sampath Kannan",
        "Juba Ziani"
      ],
      "title": "Downstream E\u0082ects of A\u0081rmative Action",
      "venue": "Educational Foundations (January",
      "year": 2019
    },
    {
      "authors": [
        "Kimberly A Sco",
        "Kimberly M Sheridan",
        "Kevin Clark"
      ],
      "title": "Culturally responsive computing: a theory revisited",
      "venue": "Learning, Media and Technology 40,",
      "year": 2015
    },
    {
      "authors": [
        "Preethi Shivayogi"
      ],
      "title": "Vulnerable population and methods for their safeguard",
      "venue": "Perspect Clin Res",
      "year": 2013
    },
    {
      "authors": [
        "Skyler Speakman",
        "Srihari Sridharan",
        "Isaac Markus"
      ],
      "title": "\u008cree population covariate shi\u0089 for mobile phone-based credit scoring",
      "venue": "In Proceedings of the 1st ACM SIGCAS Conference on Computing and Sustainable Societies",
      "year": 2018
    },
    {
      "authors": [
        "John Stolte",
        "Richard Emerson"
      ],
      "title": "Structural inequality: Position and power in network structures. Behavioral theory in sociology",
      "year": 1977
    },
    {
      "authors": [
        "Marta Tienda",
        "Teresa Sullivan"
      ],
      "title": "Texas Higher Education Opportunity Project",
      "venue": "Ann Arbor, MI: Inter-university Consortium for Political and Social Research [distributor]",
      "year": 2011
    },
    {
      "authors": [
        "Mariateresa Torchia",
        "Andrea Calabr\u00f2",
        "Morten Huse"
      ],
      "title": "Women directors on corporate boards: From tokenism to critical mass",
      "venue": "Journal of business ethics 102,",
      "year": 2011
    },
    {
      "authors": [
        "Eric Luis Uhlmann",
        "Geo\u0082rey L Cohen"
      ],
      "title": "Constructed criteria: Rede\u0080ning merit to justify discrimination",
      "venue": "Psychological Science 16,",
      "year": 2005
    },
    {
      "authors": [
        "Sepehr Vakil",
        "Rick Ayers"
      ],
      "title": "\u008ce racial politics of STEM education in the USA: interrogations and explorations",
      "year": 2019
    },
    {
      "authors": [
        "Sepehr Vakil",
        "Maxine McKinney de Royston",
        "Na\u2019ilah Suad Nasir",
        "Ben Kirshner"
      ],
      "title": "Rethinking race and power in design-based research: Re\u0083ections from the \u0080eld",
      "venue": "Cognition and Instruction 34,",
      "year": 2016
    },
    {
      "authors": [
        "Anniek van den Hurk"
      ],
      "title": "Interventions in education to prevent STEM pipeline leakage",
      "venue": "International Journal of Science Education",
      "year": 2019
    },
    {
      "authors": [
        "Sandra Wachter",
        "Brent Mi\u008aelstadt",
        "Chris Russell"
      ],
      "title": "Counterfactual Explanations without Opening the Black Box: Automated Decisions and the GPDR",
      "venue": "Harv. JL & Tech",
      "year": 2017
    },
    {
      "authors": [
        "Linda F Wightman"
      ],
      "title": "LSAC National Longitudinal Bar Passage Study",
      "venue": "LSAC Research Report Series",
      "year": 1998
    }
  ],
  "sections": [
    {
      "text": "CCS CONCEPTS \u2022Social and professional topics\u2192 Computing education;\nKEYWORDS diversity, fairness, ethics, structural inequality"
    },
    {
      "heading": "1 INTRODUCTION",
      "text": "e pervasive use of automated technologies in our society has prompted concerns regarding the fair and ethical use of large scale demographic data sets to make decisions that impact people\u2019s lives, particularly in legally regulated domains such as criminal justice, education, housing, and healthcare [6, 9]. Along with this new paradigm come opportunities to use data analysis that is accurate, reproducible, and transparent to address societal issues. Many sources of data that re ect disparities in social outcomes come from\npopulations that are identi ed as either vulnerable or underrepresented. Here, vulnerable populations are de ned as those lacking the social capital to represent themselves including children, incarcerated persons, students, and the economically alienated/poor [91]. Underrepresented groups, by contrast, are de ned as individuals derived from ethnic minority populations or gender groups that have undergone historical discrimination and, also, as we will highlight further in this paper, are underrepresented with respect to their participation in the technology workforce [54]. Data from these vulnerable and underrepresented groups show that they continue to be subject to systemic structural biases, o en manifesting at data collection, that can skew the outcome of automated decision making processes.\nIndeed, the interdisciplinary community that has recently arisen to address these biases in algorithmic design and deployment has made great strides in identifying unfairness and working to address it from a computational perspective. A current limitation of the community\u2019s work is that it has not yet succeeded in fully capturing the diverse perspectives of those populations most affected by potentially biased algorithmic systems. We argue this challenges the exact problems that much of the community targets in its research output. To address these gaps, some conversation in technology communities has centered the idea of educational pipeline development as the area in which the greatest strides in ameliorating the diversity de cit can be realized [79]. While we appreciate the role that recruitment of underrepresented groups plays in broadening the eld, we think that this approach critically under-utilizes potential diversity resources.\nus in this work we advocate for diversifying computing and the AI research community itself as a core priority of the eld and our research e orts. We limit our discussion mainly to applications of algorithmic fairness research within the social construct of North America. We acknowledge that the these challenges extend beyond the borders of the U.S., but given that us authors are based in the U.S., and the U.S. represents a relevant test case for diversity, this is most appropriate, and the main themes from this discussion are applicable and can be extended to other places. To encourage and facilitate discussion and innovation around this goal we make the following contributions:\n(1) We make clear the connection between the lack of diversity of communities represented in datasets and the type and breadth of the biases encountered in our data analysis, and the interpretation thereof. (2) We highlight recent research from the ethics in AI community which illustrates cases where a lack of diversity of perspectives may have been a critical factor in the design of models and methods which su er from unfair bias against protected groups.\nar X\niv :2\n00 2.\n11 83\n6v 1\n[ cs\n.C Y\n] 2\n6 Fe\nb 20\n20\n(3) We identify positive e orts of the ethics in AI community with respect to the diversity de cit in computer science, while making recommendations that are informed by the best practices of elds external to computing."
    },
    {
      "heading": "2 WHY IMPROVING DIVERSITY IS ESSENTIAL TO THE ETHICS IN AI COMMUNITY",
      "text": "Bias in data and algorithms are critical issues, and e orts to address these are essential as computing researchers and practitioners design models and algorithms that are being deployed in ever more real-world scenarios. Much scholarship within the ethics in AI community addresses unfair practices against members of vulnerable or underrepresented groups, including the explicit use of protected data a ributes such as age, gender, or race or ethnicity, as well as indirect discrimination that occurs when group status is exploited inadvertently [40].\nBias in data may occur when there is unequal representation of protected groups. Algorithms then trained on datasets encoding such biases can result in biased performance across groups [23, 28]. Additionally, even when datasets are equally representative of groups, biases in objective functions, for example optimizing for an outcome that can be driven by features of protected classes, can also result in unfair outcomes [73]. However, even if these two issues are addressed, there certainly are other systematic issues that can pervade. Here we formally articulate the reason that systematic issues critically impact the work of detecting and mitigating unfair bias in algorithmic systems.\nExisting research has proposed many statistical \u201cfairness\u201d criteria. To a rst approximation, most of these criteria fall into three di erent categories de ned along the lines of di erent (conditional) independence between the random variables of the sensitive attribute A, the target variable Y , and the classi er or score R; independence, separation and su ciency [8]. Accordingly, being based on A, Y and R, these notions do not incorporate any context that may result in or perpetuate such inequalities. To further illustrate this, we examine a popular notion of discrimination de ned as statistical parity [37], also referred to as disparate impact [40]. is notion requires that a certain group-conditional bene cial outcome rate should be the same for groups of interest. Formally, bias given as the following, should be minimized:\n| P(Y\u0302 = 1|A = 1) \u2212 P(Y\u0302 = 1|A = 0) | (1)\nwhere Y\u0302 is the predictor, representing Y\u0302 : X \u2192 Y a random variable that depends on A, X and U . Here A represents the group status associated with an individual, de ned by some protected attributes which must not be discriminated against. X represents other observable a ributes of any particular individual, U the set of a ributes which are not observed, and Y as above, the outcome to be predicted, e.g. by a machine learning algorithm. While the omission of context can be considered a limitation of the above notions, there is a possibility that by doing so, this may remove a ention from, or camou age the (broader/multilevel/structural) causes of such inequities which can hinder their pursuit and limit\nsustainable equity. Following this, we formally identify the challenge of structural inequality [93], and de ne it in line with this notation.\nDe nition. Structural inequality is a condition where one category of people are a ributed an unequal status in relation to other categories of people, and this relationship is perpetuated and reinforced by a con uence of unequal relations in roles, functions, decisions, rights, and opportunities. erefore, if a class A = 1 is subject to a structural inequality, that would mean that P(Y ) is confounded, and even if"
    },
    {
      "heading": "A is represented and statistical parity holds (equation 1 is equal to",
      "text": "zero), the measure of bias represented by this formulation may not be meaningful to the full extent."
    },
    {
      "heading": "2.1 Impact of Structural Inequality on Algorithmic Fairness Analysis",
      "text": "A structural challenge could occur in many real-world situations. Consider an example from the healthcare domain. Say ge ing e ective treatment for a particular condition is the positive outcome Y\u0302 = 1. Even if the probability that a patient gets treated for a particular condition is equal across all groups with a ribute A, and there is data for all groups A in the considered dataset, there could still exist an unobserved confounder, U , that impacts the outcomes for groups of patients. For instance, in this case the confounder could be access to treatment due to lower levels of healthcare provider trust in particular groups of patients\u2019 use of pain medication [20].\nFigure 1 uses a causal graph to illustrate such a scenario. e use of causal frameworks to understand the unfair impact of such confounders on potentially biased prediction has been proposed [59], along with methods to uncover the interactions between unobserved variables and outcomes. For example, Kusner et al. [59] use a single confounder in an accident prediction model, and Kannan et al. [56] evaluate a hiring problem. However we know that for\nsuch complex real world interactions, there are many confounders likely to have an impact on outcomes.\nWhat makes structural inequities more challenging than a typical confounder is found in the de nition above, wherein a structural inequality is \u201cperpetuated and reinforced by a con uence of unequal relations in roles, functions, decisions, rights, and opportunities\u201d indicating that the confounder could still a ect P(Y |X ) in an unknown/addressable way at any given point in time. In other words, by nature, the structural inequality is of an encompassing magnitude and di cult to quantify. Identifying a single variable U based on this situation is not straightforward and might not t into the simple (yet robust) paradigms o en considered.\nStructural inequality may in uence interactions throughout the causal graph. For instance in the hiring problem considered in [56] it is assumed that an employer at the end of a hiring pipeline is rational \u2013 that it computes and makes a decision based on a posterior distribution and all necessary data is available for this. However we know that such decisions come down to the judgments of human analysts, whose decision making is impacted by structural inequalities through their own implicit bias. Several studies have shown biases in hiring practices continue over time and have not shown any sign of decrease, despite the availability of information and policies which promote equal opportunity [84].\nTo continue our healthcare example, structural inequities may manifest through many di erent mechanisms. Studies have demonstrated the impact of social deprivation on health outcomes and have suggested multiple pathways that may contribute to adverse outcomes [26]. For example, patient-related health beliefs and behavior, as well as access to care through delayed presentation or\naccess to medical services [83]. Moreover, the decision to order tests can be a ected by human judgments (if doctors are biased againstA, they may be less likely to be treated (which can be referred to as the selective labels problem [61]). However the decision to treat conditional on test results has not been shown or suggested a er testing [68]. Each of these occurrences can generate adverse outcomes at the individual level and also lead to structural inequities over time if there is su cient penetrance of the described behaviors.\nIn sum, these examples serve to highlight how the speci c issues at hand, in our example here healthcare outcome disparities, are complex and may require further domain insight or awareness in order to fully develop a given problem statement and solution formulation. erefore in addition to the valuable approaches to fairness mitigation proposed in the recent literature, we feel it is important to consider such problems in the context of the conditions created by structural inequality."
    },
    {
      "heading": "2.2 Impact of Structural Inequality on the Computing Community",
      "text": "To gain insight into the current paradigm of research into fairness and bias mitigation strategies, we consider the recent literature which focuses on the treatment of historically disadvantaged groups. Such study typically de nes groups by sensitive data attributes protected by U.S. law in high impact domains [9], including the Fair Housing Act (FHA) [33] and Equal Credit Opportunity Act (ECOA) [32]. e data a ributes protected under these laws include age, disability, gender identity, marital status, national origin, race, recipient of public assistance, religion, and sex. To demonstrate the problem se ings covered, we present an (incomplete) survey\nof recent papers from exemplary leading conferences on fairness and AI Ethics for anecdotal consideration. Table 1 summarizes papers from the 2019 ACM Fairness, Accountability, and Transparency (FAT) Conference and 2019 AAAI/ACM conference on Arti cial Intelligence, Ethics and Society (AIES). 1 Papers included are those which experimentally evaluate bias-mitigation algorithms or fairness metrics. Examining the datasets used and the protected groups targeted, we can see that the majority of analysis focuses on a narrow set of a ributes, with race and gender the most prevalent sensitive a ributes targeted (in 44 out of 57 experiments).\nWe present this overview of focused a ention by the fairness and ethics in AI research community on the impact of structural inequality on women and racial or ethnic minorities in the United States to contrast with the inclusion of these groups in the eld of computing. Unfortunately, we see stark disparity in participation of these groups in tech jobs and computing education. is disparity can be seen across computer science educational programs, research institutions, and technical jobs in industry. For instance, we refer to the Taulbee survey [102], which has been conducted by the Computing Resource Association (CRA) annually since 1974. In the latest 2018 survey, across 169 PhD granting programs in the U.S. and Canada we see huge gender imbalances in computer science (77.7% male) and computer engineering (80.7% male) (Figure 2b). ere is also a troubling distribution across racial or ethnic groups, with white students making up 22.9% of enrolled students and black and latinx students making up only 2.0% and 1.7% respectively (Figure 2b). e survey also reveals the additional insight that 62.6% of students a ending these programs are from home countries di erent from where their institutions are located. is shows how stark the di erences in engagement with computing are particularly within the U.S. population. Similar disparity is present in industry as illustrated in Figure 2. We see the same gender imbalance exists worldwide in technical roles across top technology companies, and within the U.S. the same trend in racial disparities.\nWe explicitly demonstrate this troubling disconnect between the subjects of the research in fairness and ethical AI, and the body of researchers and practitioners here because the lack of needed domain insight and diverse perspectives has dire implications for the ability of the eld to build on this crucial research and to responsibly implement the proposed methods in production systems. Failing to improve diversity in the computing eld while advancing bias mitigation technologies is se ing up for failure, leaving researchers and practitioners under-resourced to preempt sources of unfair bias in the technologies they design and build.\nFor example, a recent study surveying machine learning practitioners was conducted to understand how tools enabled by machine learning can have a more positive impact on industry practice [50]. Several gaps were identi ed from themes of this discussion. Specifically, for the design of algorithmic systems, the crucial need to address biases in the humans embedded throughout the machine learning development pipeline was highlighted. Additionally, survey respondents noted their susceptibility to blind spots, in part due to the lack of diverse perspectives within their own teams as compared to the real-world users who interacted with their products\n1 e reader is referred to h ps://fatconference.org/network/ for a comprehensive listing of similar venues.\nMic ros\noft\nGo og\nle\nFac eb\noo k\nU.S . C\nom pu\nter an\nd M ath\nOc cup\nati on\ns\nCo mp\nute r a\nnd In\nfor ma\ntio n\nSy ste\nms Ma\nna ge\nrs\nEn rol\nlm en\nt in Ph\nD P rog\nram s\n0\n20\n40\n60\n80\nPe rc\nen t o\nf P eo\npl e\nin R\nol e Men\nWomen\n(a) Gender parity values for technical employees.\nMic ros\noft\nGo og\nle\nFac eb\noo k\nU.S . C\nom pu\nter an\nd M ath\nOc cup\nati on\ns\nCo mp\nute r a\nnd In\nfor ma\ntio n\nSy ste\nms Ma\nna ge\nrs\nEn rol\nlm en\nt in Ph\nD P rog\nram s\n0\n20\n40\n60\n80\nPe rc\nen t o\nf P eo\npl e\nin R\nol e White\nAsian Latinx Black Multiracial Native American*\n(b) Breakdown of racial and ethnic groups. *Native American includes Native Americans, Alaska Natives, Native Hawaiian and Paci c Islanders\nFigure 2: Demographic breakdown for technical employees at top technology companies, PhDgranting institutions, and computing elds in the U.S. Values for the companies are sourced from their most recent annual reports [15, 39, 65]. Educational values are from the Taulbee survey, [102], and U.S. occupational data is from the Bureau of labor statistics analysis of Computer and mathematical occupations from the January 2018 Current Population Survey [19].\nonce they were deployed. Improving the diversity of the workers and practitioners involved in this process could aid in ameliorating these issues, as it would directly provide an understanding of real world needs for appropriate product development.\nIt has been pointed out extensively how the lack of diversity leads to poor outcomes in many elds of endeavor. Examples range from evidence in hospitals that less diverse doctors and nurses leads to worse patient care [4, 30], to management in rms [35], scienti c discovery [70] and economic pro t [71]. e marked gender and racial disparity we see in computing no doubt similarly impacts innovation and value in the development of new technologies."
    },
    {
      "heading": "3 FAIRNESS IN THE LITERATURE AND POSSIBLE CONFOUNDING",
      "text": "To further elaborate on this premise that diverse representation could be a proactive approach to mitigating data and algorithm biases, we next identify speci c ways in which greater diversity among the designers and creators of algorithmic systems would have been integral in avoiding the cited scenarios studied in a number of recent ethics in AI papers."
    },
    {
      "heading": "3.1 Biases in Data",
      "text": "A highly cited example of data bias is the Gender Shades study by Buolamwini and Gebru which highlighted disparate performance in commercial facial recognition systems [17]. is well-discussed scenario highlights how designers of an image processing algorithm may not think of all its implications on di erent populations, namely a skin color that is not their own, or representative of the majority of the people around them. Due to a lack of representation of both female faces and dark skinned faces in the training datasets used, prediction rates by these commercial systems su ered greatly for these groups.\nAnother example of disparity due to training data is in natural language processing, where debiasing word embeddings has been a priority area of work [10, 44]. Historical stereotypes are re ected in corpora of text used to train these embedding models, which are then used widely as a pre-processing step for automated text analysis. ere may be both passive and active ways of pu ing together image or text datasets for algorithm development, and in both these cases, a proactive approach to sourcing such datasets could avoid wasting time and resources as well as potentially in icting unfair or harmful outcomes on underrepresented groups. Realizing that all sets of texts or images may not be free of bias and being in an anticipatory mode could help to address and resolve such issues."
    },
    {
      "heading": "3.2 Algorithmic Bias",
      "text": "Another recent paper by Obermeyer et al. identi es a \u2018problem formulation error\u2019, or in other words a mis-speci ed objective function as a source of unfair bias in an automated system. In this study, they examine a commercial algorithm that is deployed nationwide today and a ecting millions of people [73]. ey show that at the same health risk score, black patients are considerably sicker than whites due to the way the risk score is a ributed to di erent illnesses that occur disparately. Instead of optimizing over health-related variables, a proxy label (in this case, cost) was used. ough not discussed by the authors, a diverse team may have identi ed this issue during the design of the risk score, prior to it being deployed and a ecting the lives of millions of people.\nIn some ways this work connects to the broad idea of problem formulation, which has been discussed [80]. is study, combining ethnographic eldwork and ideas from sociology and history of science, as well as critical data studies, sought to describe the complex set of actors and activities involved in problem formulation. Broad conclusions demonstrated that problem speci cation and operationalization are always dynamic processes and normative considerations are rarely included. eir work thus also highlights the need for a broad range of perspectives and considerations at problem and algorithm formation time."
    },
    {
      "heading": "3.3 Missing Labels",
      "text": "Another major issue in the AI ethics literature that is being addressed via algorithmic solutions is that of missing data; speci - cally when membership labels for a protected class are unavailable [29]. Indeed, much work on algorithmic fairness must assume that protected a ributes are known [59, 61]. is is a reasonable assumption for work that builds on existing data, however the challenge of when a label needed to identify and ensure a class is represented/accounted for, reinforces the need for proactive recording of labels, which o en are missing. is challenge may also directly bene t from diverse groups of people involved in dataset creation and analysis, who may be able to identify such a ributes or recognize when they are not represented."
    },
    {
      "heading": "4 RECOMMENDATIONS FOR INCREASING DIVERSITY WITHIN COMPUTING AND THE ETHICS IN AI COMMUNITY",
      "text": "e recent literature has emphasized that increasing diversity is not simply a \u201cpipeline\u201d problem [99]. As such, we next discuss three areas in which we see potential to enhance diversity and inclusion in computing research and education by engaging the ethics in AI community: (1) connecting to a broader network of higher education institutions, (2) including stakeholders from diverse communities in the research process, and (3) creating opportunities within our own activities to support a diverse group of future leaders. We look to examples of successes from other disciplines where structural inequality has impacted the diversity of practitioners and therefore outcomes in those the elds. As well, we believe that these recommendations will also address challenges in creating sustainable diversity in computing and beyond, through impacting challenges including: societal norms [12, 96], limited access [52], heterogeneous sourcing [51], tokenism [95] and unfair treatment [13, 67], which have all been described in diversity and representation gaps. While increasing diversity at the table doesn\u2019t automatically x equity, it is thought to improve it at the individual and community level in part via exposure to di erent values and backgrounds [86].\nIn recommending these tangible steps we hope to build on the excellent work of the community to date. We feel the social relevance, interdisciplinary structure, and crucial importance of the emerging eld of AI Ethics make it rich with potential for broadening participation in computing by appealing to students\u2019 interests and values. Making a positive social impact has been demonstrated to motivate non-traditional students in computer science education [16, 46]. In addition, many research venues have already been evolving with diversity and inclusion as part of their core values, and this trend is encouraging. We note the success of a nity groups Black in AI, Women in Machine Learning, LatinX in AI, and eer in AI at the NeurIPS, a top venue for Machine Learning, and the addition of the \u201cCritiquing and Rethinking Accountability, Fairness and Transparency\u201d CRAFT call in the 2020 FAT conference as evidence of these communities\u2019 openness to innovative solutions to the lack of diversity in traditional computing conferences. Other considerations such as keeping cost of a ending low, providing scholarships for students, and making conference material available online through open access and livestreams also serves to keep\ncommunities open to a wide audience and promote engagement. We applaud these e orts."
    },
    {
      "heading": "4.1 Building Collaborations with Minority",
      "text": "Serving Institutions\ne United States continues to see signi cant barriers to the full inclusion of underrepresented groups in technology disciplines, as detailed in Section 2.2. A rmative action and other e orts to address pipeline problems have their limitations, as evidenced by analysis coming out of the fairness community itself [89]. One way to bolster the number of underrepresented perspectives in computing is for FAT to partner with minority serving institutions in the study of socio-technical algorithmic systems. Minority serving institutions in the U.S. include 108 Historically Black colleges and universities (HBCUs), 274 Hispanic Serving Institutions (HSIs), 35 Tribal Colleges and Universities (TCUs) and underrepresented Asian American and Paci c Islander Serving Institutions (AAPASIs). While HBCUs comprise only 3% of America\u2019s institutions of higher education, they produce 24% of all bachelors\u2019 degrees earned by African-Americans [75]. Within STEM disciplines they are responsible for graduating 40% of all African American STEM graduates [78]. Similarly, 40% of Hispanic-American students are awarded their bachelors\u2019 degrees from HSIs [75].\nWhen we broadly examine gender parity in science, technology, engineering and mathematics, it becomes clear that not every discipline has met with the same levels of gender success. When comparing gender parity in the workforce between medical schools, law schools and various tech industry staples, law school admission is one area where women have achieved parity in educational advancement since 2015 [82]. Interestingly, it has been minority serving/Historically Black Colleges and Universities (HBCUs) and non-traditional learner institutions that have been at the vanguard of this trend, due to their high enrollment numbers for female students. For example, in 2018 North Carolina Central University had a law school enrollment that was 66.85% female; Atlanta John Marshall Law School was 66.21 % female; Northeastern University was 65.76% female; and Howard University 65.70 % female enrollment. Female enrollment numbers at these institutions can be compared to the top ve U.S. News and World Reports ranked law schools in the country whose female enrollment does not exceed 49.6% [38].\nese statistics point to a potential solution to similar issues in computing education. While the tech industry and research institutions o en focus on recruitment from predominantly a small number of elite institutions, the lessons garnered from law school admissions suggests that partnering with non-traditional and minority serving institutions may be the way forward in addressing the lack of diversity in educational programs both in gender diversity and ethnic diversity. ese minority serving institutions (MSIs) are the location of nearly half of the underrepresented trainees in computing and represent a potentially untapped resource of diverse perspectives. Furthermore, of particular relevance to the study of fairness and ethics in AI is the fact that these institutions have a robust intellectual tradition of contextualizing the lives of marginalized populations.\nResearch partnerships can occur between individual researchers or as the result of organization to organization collaboration through\na memorandum of understanding outlining speci c exchanges and projects. We believe these e orts would represent a structural increase in the participation of underrepresented groups, bring a diversity of perspectives to bear on the design of algorithmic systems and thus directly address a goal of the ethics in AI community. Developing meaningful ongoing collaborations that can contextualize the implicit and sometimes explicit biases inherent in data is essential for this task."
    },
    {
      "heading": "4.2 Prioritizing Research Collaboration Between the Ethics in AI Community and Underrepresented/Interdisciplinary Groups",
      "text": "In addition to partnering with Minority Serving Educational Institutions, we see emphasizing collaborations with underrepresented and vulnerable groups themselves as a critical piece to broadening the scope of knowledge that the ethics in AI community has to draw upon. Recent scholarship in design-based research considers race and power dynamics between researchers and researched communities [97, 98], and can provide guidance on designing interventions which have meaningful impacts. For instance, it has been observed that traditional eurocentric epistemologies in research communities are o en disconnected from the cultural practices and ways of knowing of underrepresented and vulnerable communities [7]. Only working closely with these communities can we begin to incorporate this knowledge into our problem designs. Having a diversity of perspectives can support the development of culturally responsive computing technologies and educational pedagogy which take a proactive inclusive approach considering intersectionality, innovations, and technosocial activism, rather than one that requires accommodations a er the fact for communities le out at the development phase [21, 90]. Engagement can happen at all stages of the data analysis pipeline, and may be particularly important during the collection, analysis, and interpretation of data from their communities. For instance, recent work [55] illustrates three robust case studies for collaborations that data scientists can have with underrepresented communities, including biomedical applications for improving the health of under served populations. ese projects came out of proactive collaborations with members of these communities and suggest that underrepresented and indigenous communities are not only interested in being the subjects of research, or the passive recipients of derived knowledge about their own communities.\ne interdisciplinary nature of the ethics in AI community inherently facilitates collaboration between experts in di erent elds. In particular, collaboration with social scientists who have been assessing mechanisms for structural inequities has much to o er the eld. ough the work by social scientists may not be performed in a quantitative manner, this provides an opportunity for collaboration with quantitative communities interested in fairness. In general, as identifying and quantifying sources of Structural inequality is a complex task, it is potentially an area of interdisciplinary synergy, between quantitative scientists and the rich literature in the social sciences that already exists. is topic has been explored from multiple angles, including from the elds of education, political science, sociology, health and urban studies.\nFor example, a study on criminal justice algorithms might be greatly enriched by including social scientists from the communities most adversely a ected by biases in the analysis of judicial data. is recommendation also can help to complete the communion loop of data ndings back to those under served communities that are all to o en le disconnected from the analytic fruits of their data. Creating collaborations between researchers interested in AI ethics, underrepresented data scientists, and complimentary domain expert thought leaders in these communities can lead to more robust insights into how to prevent algorithmic inequalities. Ways of accomplishing such collaboration include the example of community advisory boards that many biomedical research institutions employ to formalize academic community partnerships [69], as well as research-based open houses that invite the community to learn about current research projects and provide opportunities to participate in research [60].\nA potential outcome of these strengthened ties of research collaboration could be to increase the number of interested trainees who enter the eld. is could also serve as a research idea generator where researchers use community input to de ne those research areas that represent their most immediate needs. rough a process of collaboration, community activists can amplify the algorithmic messages identi ed through careful ethics in AI research."
    },
    {
      "heading": "4.3 Providing Enhanced Mentorship to Trainees at Ethics in AI Research Conferences",
      "text": "A barrier to full and diverse participation in the computing research community can revolve around the lack of onsite mentorship from senior researchers in communal spaces. Many students from diverse communities have shallower professional networks than other students and this can inhibit their introduction and advancement in research. is has a potentially pervasive e ect on recruitment and retention e orts, in opposition to indications that black and latinx students show higher interest in learning computer science [54]. Networking not only serves as an information dissemination tool, but also as a critical skill for trainees to develop for later career advancement. is can be especially challenging for trainees from underrepresented groups during networking-intense events like conferences. One ongoing concern is that underrepresented students are o en less likely to have robust senior mentoring networks and strong ties to industry or research partnerships. is means that the ability to get recommendations to move forward, the prestige of those recommendations, and the ability to ask for introductions are truncated.\nOne way to increase and retain diverse participants within the computing research community is to use volunteer research mentors at conferences who can serve as a bridge between those underrepresented trainees and early career scientists who may not be well integrated into the community and more senior participants. Pairing these individuals with those more senior researchers can rapidly expand the networks of newcomers, thereby increasing the likelihood that they will be able to make long-term contributions to the eld.\nOne example of a robust mentoring network exists through the Society for Molecular Biology and Evolution (SMBE). SMBE has\ndeveloped a mentoring program to pair trainees and early career scientists with established researchers. is is particularly e ective because their society has an ethos of inclusiveness and contributing to the next generation of science and technologists. While their mentorship program is not limited to individuals from underrepresented groups, they make e orts to match trainees to mentors based on their merit, area of research interest, languages spoken, and geographical locations [41]. Trainees communicate with their conference mentors prior to the start of the meetings, giving both trainees and mentors a chance to learn about each other\u2019s research interests and career goals. During the conference, trainees and SMBE mentors are invited to have dinner together, meet up during the breaks and check in on research talks. is interaction creates an interpreted meeting that immediately connects neophytes with institutional knowledge. is link between well-networked researchers and those in need of connections can also facilitate retention of trainees and promote enthusiasm for the discipline amongst a broaden cohort of participants.\nAn important feature of the SMBE mentoring program is the inclusion of travel support for trainees whose characteristics will broaden the capacity of the organization to reach diversity goals [41]. Travel support for vulnerable and underrepresented trainees and early career scientists is a necessary investment in diversity and inclusion for computing communities. While merit only based awards are an excellent way to incentivize groundbreaking research, we observe that these programs o en play into the pre-existing resource imbalances that favor trainees at elite institutions where resources are less constrained than they are at most MSIs. is means that institutions that train the large portions of underrepresented graduates have the least likely path to participation in conferences and workshops, while having the greatest nancial barriers to entry into these intellectually rich spaces.\nIn Poverty and Power, Royce asserts that social networks are particularly important for underrepresented or marginalized populations because the ties binding people together and connecting individuals to organizations can:\n\u201d. . . channel information, convey cultural messages, create social solidarities, forge expectations and obligations, facilitate the enforcement of social norms, engender relations of mutual trust, serve as sources of social support, and operate as conduits of power and in uence. In the performance of these functions, furthermore, social networks shape the distribution of resources and opportunities, advantaging some and disadvantaging others.\u201d [87]\nis recitation of the comprehensive utility of social networks supports our assessment that mentoring, a modality to build robust social networks, can serve as a necessary tool to build ethics in AI community diversity amongst trainees and early career scientist from underrepresented populations.\ne use of a nity and mentoring workshop programs, such as those mentioned previously and Broadening Participation Workshops associated with conferences in a variety of computing subdisciplines, have the secondary e ect of building a new cadre of research leaders who can continue to invest their intellectual and\nservice e orts towards the be erment of the eld. Investment in trainees and early career researchers is an essential part of organizational capacity building. e ethics in AI community is a relatively new research community that could see substantial bene ts from the increased inclusion of trainees and early career scientists from underrepresented disciplines.\nOne such workshop program is the Broadening Participation in Data Mining (BPDM) Workshop. is workshop traditionally brings together underrepresented trainees and early career scientists to increase their exposure to academic, industry and federal careers in data science. In 2019, the 7th annual BPDM workshop brought together 55 trainees and 10 mentors to Howard University, a Historically Black College/University. In the past, BPDM was associated with national or international conferences such as ACM SIGKDD or SIAM CSE. is 3 day workshop allowed participants to network with peers and senior mentors within a community of other individuals from underrepresented communities. As part of a computing tutorial exercise during the workshop, participants spent time using the COMPAS dataset to identify key themes and possible solutions to structural inequality [55]. is tutorial session was important not only because it addressed fairness and ethics principles within a computer science context, but it also served to encourage participants to use computing to address issue that were of considerable concern to the social justice needs of their communities. Finally the workshop provided opportunities to create intellectual community for those participants whose experience in undergraduate, graduate and post-doctoral education is isolating by virtue of their identity."
    },
    {
      "heading": "5 CONCLUSION",
      "text": "Education e orts that grow representation in meaningful ways may obviate the need for much algorithmic manipulation, and also help to maintain retention e orts because no one member of an underrepresented community has to shoulder the burden of speaking for their group. is work recognizes opportunities for the ethics in AI community to increase the breadth of perspectives in computing in order to further develop our pursuit of algorithmic fairness. We have proposed three major areas within which the community can address structural inequalities: building educational collaborations with minority serving institutions, building capacity through research collaborations with community domain experts, and using educational mentoring to develop a cadre of diverse future leaders in the computing. is can be accomplished by meeting underrepresented and vulnerable communities \u2018where they are.\u2019 is idea refers to both identifying the educational institutions that produce the largest number of underrepresented trainees, and using mentoring approaches to increase opportunities for trainees to actively participate in communal spaces such as computing conferences.\nWe support an organic bo om up approach that both assists researchers in improving the fairness of algorithmic systems while empowering under served gender and ethnic communities to realize equity. is capacity building approach within research communities can also help to reinforce more equitable resources for research endeavours for collaborating partners, and increased communal activism to ameliorate structural inequalities re ected in data that the ethics in AI community works to account for. Without a empting\nto enact these educational initiatives, the ethics in AI community may miss out on a unique opportunity to build diverse perspective capacity, and to sustainably improve fairness, accountability and transparency in socio-technical systems through addressing structural inequalities."
    }
  ],
  "title": "No computation without representation: Avoiding data and algorithm biases through diversity",
  "year": 2020
}
