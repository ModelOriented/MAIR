{"abstractText": "Machine learning is used more and more often for sensitive applications, sometimes replacing humans in critical decision-making processes. As such, interpretability of these algorithms is a pressing need. One popular algorithm to provide interpretability is LIME (Local Interpretable Model-Agnostic Explanation). In this paper, we provide the first theoretical analysis of LIME. We derive closed-form expressions for the coefficients of the interpretable model when the function to explain is linear. The good news is that these coefficients are proportional to the gradient of the function to explain: LIME indeed discovers meaningful features. However, our analysis also reveals that poor choices of parameters can lead LIME to miss important features.", "authors": [{"affiliations": [], "name": "Damien Garreau"}, {"affiliations": [], "name": "Ulrike von Luxburg"}], "id": "SP:33be8825851dfba3dfd285323f18964cb1a42abd", "references": [{"authors": ["D. Baehrens", "T. Schroeter", "S. Harmeling", "M. Kawanabe", "K. Hansen", "K.-R. M\u00fcller"], "title": "How to explain individual classification decisions", "venue": "Journal of Machine Learning Research,", "year": 2010}, {"authors": ["S. Boucheron", "G. Lugosi", "P. Massart"], "title": "Concentration inequalities: A nonasymptotic theory of independence", "year": 2013}, {"authors": ["R. Guidotti", "A. Monreale", "S. Ruggieri", "F. Turini", "F. Giannotti", "D. Pedreschi"], "title": "A survey of methods for explaining black box models", "venue": "ACM Computing Surveys,", "year": 2019}, {"authors": ["D. Harrison Jr.", "D.L. Rubinfeld"], "title": "Hedonic housing prices and the demand for clean air", "venue": "Journal of environmental economics and management,", "year": 1978}, {"authors": ["S.M. Lundberg", "S.-I. Lee"], "title": "A unified approach to interpreting model predictions", "venue": "In NeurIPS,", "year": 2017}, {"authors": ["D.P. O\u2019Leary", "G.W. Stewart"], "title": "Computing the eigenvalues and eigenvectors of arrowhead matrices", "venue": "Journal of Computational Physics,", "year": 1996}, {"authors": ["X. Ren", "J. Malik"], "title": "Learning a classification model for segmentation", "venue": "In ICCV,", "year": 2003}, {"authors": ["M.T. Ribeiro", "S. Singh", "C. Guestrin"], "title": "Why should I trust you? explaining the predictions of any classifier", "venue": "In SIGKDD,", "year": 2016}, {"authors": ["L.S. Shapley"], "title": "A value for n-person games", "venue": "Contributions to the Theory of Games,", "year": 1953}, {"authors": ["A. Shrikumar", "P. Greenside", "A. Shcherbina", "A. Kundaje"], "title": "Not just a black box: Learning important features through propagating activation differences", "venue": "arXiv preprint arXiv:1605.01713,", "year": 2016}, {"authors": ["A. Shrikumar", "P. Greenside", "A. Kundaje"], "title": "Learning important features through propagating activation differences", "venue": "In ICML,", "year": 2017}, {"authors": ["C. Szegedy", "W. Liu", "Y. Jia", "P. Sermanet", "S. Reed", "D. Anguelov", "D. Erhan", "V. Vanhoucke", "A. Rabinovich"], "title": "Going deeper with convolutions", "venue": "In CVPR,", "year": 2015}, {"authors": ["A.W. Van der Vaart"], "title": "Asymptotic Statistics", "year": 2000}, {"authors": ["M.J. Wainwright"], "title": "High-dimensional statistics: a nonasymptotic viewpoint", "year": 2019}, {"authors": ["H. Weyl"], "title": "Das asymptotische Verteilungsgesetz der Eigenwerte linearer partieller Differentialgleichungen (mit einer Anwendung auf die Theorie der Hohlraumstrahlung)", "venue": "Mathematische Annalen,", "year": 1912}, {"authors": ["M.D. Zeiler", "R. Fergus"], "title": "Visualizing and understanding convolutional networks", "venue": "In ECCV,", "year": 2014}], "sections": [{"text": "Machine learning is used more and more often for sensitive applications, sometimes replacing humans in critical decision-making processes. As such, interpretability of these algorithms is a pressing need. One popular algorithm to provide interpretability is LIME (Local Interpretable Model-Agnostic Explanation). In this paper, we provide the first theoretical analysis of LIME. We derive closed-form expressions for the coefficients of the interpretable model when the function to explain is linear. The good news is that these coefficients are proportional to the gradient of the function to explain: LIME indeed discovers meaningful features. However, our analysis also reveals that poor choices of parameters can lead LIME to miss important features."}, {"heading": "1 Introduction", "text": ""}, {"heading": "1.1 Interpretability", "text": "The recent advance of machine learning methods is partly due to the widespread use of very complicated models, for instance deep neural networks. As an example, the Inception Network (Szegedy et al., 2015) depends on approximately 23 million parameters. While these models achieve and sometimes surpass humanlevel performance on certain tasks (image classification being one of the most famous), they are often perceived as black boxes, with little understanding of how they make individual predictions.\nThis lack of understanding is a problem for several\nProceedings of the 23rdInternational Conference on Artificial Intelligence and Statistics (AISTATS) 2020, Palermo, Italy. PMLR: Volume 108. Copyright 2020 by the author(s).\nreasons. First, it can be a source of catastrophic errors when these models are deployed in the wild. For instance, for any safety system recognizing cars in images, we want to be absolutely certain that the algorithm is using features related to cars, and not exploiting some artifacts of the images. Second, this opacity prevents these models from being socially accepted. It is important to get a basic understanding of the decision making process to accept it."}, {"heading": "1.2 Contributions", "text": "Our main goal in this paper is to provide theoretical guarantees for LIME. On the way, we shed light on\nar X\niv :2\n00 1.\n03 44\n7v 2\n[ cs\n.L G\n] 1\n3 Ja\nn 20\nsome interesting behavior of the algorithm in a simple setting. Our analysis is based on the Euclidean version of LIME, called \u201ctabular LIME.\u201d Our main results are the following:\n(i). When the model to explain is linear, we compute in closed-form the average coefficients of the surrogate linear model obtained by TabularLIME.\n(ii). In particular, these coefficients are proportional to the partial derivatives of the black-box model at the instance to explain. This implies that TabularLIME indeed highlights important features.\n(iii). On the negative side, using the closed-form expressions we show that it is possible to make some important features disappear in the interpretation, just by changing a parameter of the method. (iv). We also compute the local error of the surrogate model, and show that it is bounded away from 0 in general.\nWe explain how TabularLIME works in more details in Section 2. In Section 3, we state our main results. They are discussed in Section 4, and we provide an outline of the proof of our main result in Section 5. We conclude in Section 6."}, {"heading": "2 LIME: Outline and notation", "text": ""}, {"heading": "2.1 Intuition", "text": "From now on, we will consider a particular model encoded as a function f : Rd \u00d1 R and a particular instance \u03be P Rd to explain. We make no assumptions on this function, e.g., how it might have been learned. We simply consider f as a black-box model giving us predictions for all points of the input space. Our goal will be to explain the decision fp\u03beq that this model makes for one particular instance \u03be.\nAs soon as f is too complicated, it is hopeless to try and fit an interpretable model globally, since the interpretable model will be too simple to capture all the complexity of f . Thus a reasonable course of action is to consider a local point of view, and to explain f in the neighborhood of some fixed instance \u03be. This is the main idea behind LIME: To explain a decision for some fixed input \u03be, sample other examples around \u03be, use these samples to build a simple interpretable model in the neighborhood of \u03be, and use this surrogate model to explain the decision for \u03be.\nOne additional idea that makes a huge difference with other existing methods is to use discretized features of smaller dimension d1 to build the local model. These new categorical features are easier to interpret, since\nthey are categorical. In the case of images, they are built by using a split of the image \u03be into superpixels (Ren and Malik, 2003). See Figure 1 for an example of LIME output in the case of image classification. In this situation, the surrogate model highlights the superpixels of the image that are the most \u201cactive\u201d in predicting a given label.\nWhereas LIME is most famous for its results on images, it is easier to understand how it operates and to analyze theoretically on tabular data. In the case of tabular data, LIME works essentially in the same way, with a main difference: tabular LIME requires a train set, and each feature is discretized according to the empirical quantiles of this training set.\nWe now describe the general operation of LIME on Euclidean data, which we call TabularLIME. We provide synthetic description of TabularLIME in Algorithm 1, and we refer to Figure 2 for a depiction of our setting along a given coordinate. Suppose that we want to explain the prediction of the model f at the instance \u03be. TabularLIME has an intricate way to sample points in a local neighborhood of \u03be. First, TabularLIME constructs empirical quantiles of the train set on each dimension, for a given number p of bins. These quantile boxes are then used to construct a discrete representation of the data: if \u03bej falls between q\u0302k and q\u0302k`1, it receives the value k. We now have a discrete version of \u03be, say p2, 3, . . . qJ. The next step is to sample discrete examples in t1, . . . , pud uniformly at random: for instance, p1, 3, . . .qJ means that TabularLIME sampled an encoding such that the first coordinate falls into\nthe first quantile box, the second coordinate into the third, etc. TabularLIME subsequently un-discretizes these encodings by sampling from a normal distribution truncated to the corresponding quantile boxes, obtaining new examples x1, . . . , xn. For example, for sample p1, 3, . . .qJ we now sample the first coordinate from a normal distribution restricted to quantile box #1, the second coordinate from quantile box #3, etc. This sampling procedure ensures that we have samples in each part of the space. The next step is to convert these sampled points to binary features, indicating for each coordinate if the new example falls into the same quantile box as \u03be. Here, zi would be p1, 0, . . .qJ. Finally, an interpretable model (say linear) is learned using these binary features.\nAlgorithm 1 TabularLIME for regression Require: Model f , # of new samples n, instance \u03be,\nbandwidth \u03bd, # of bins p, mean \u00b5, variance \u03c32 1: q \u00d0 GetQuantiles(p,\u00b5,\u03c3) 2: t\u00d0 Discretize(\u03be,q) 3: for i \u201c 1 to n do 4: for j \u201c 1 to d do 5: yi,j \u00d0 SampleUniform(t1, . . . , pu) 6: pq`, quq \u00d0 pqj,yij , qj,yij`1q 7: xi,j \u00d0 SampleTruncGaussian(q`, qu, \u00b5, \u03c3) 8: zi,j \u00d0 1tj\u201cyi,j 9: end for\n10: \u03c0i \u00d0 exp \u00b4 \u00b4\u2016xi\u00b4\u03be\u20162 2\u03bd2 \u00af 11: end for 12: p\u03b2 \u00d0WeightedLeastSquares(z, fpxq, \u03c0) 13: return p\u03b2"}, {"heading": "2.2 Implementation choices and notation", "text": "LIME is a quite general framework and leaves some freedom to the user regarding each brick of the algorithm. We now discuss each step of TabularLIME in more detail, presenting our implementation choices and introducing our notation on the way.\nDiscretization. As said previously, the first step of TabularLIME is to create a partition of the input space using a train set. Intuitively, TabularLIME produces interpretable features by discretizing each dimension. Formally, given a fixed number of bins p, for each feature j, the empirical quantiles q\u0302j,0, . . . , q\u0302j,p are computed. Thus, along each dimension, there is a mapping \u03c6\u0302j : R\u00d1 t1, . . . , pu associating each real number to the index of the quantile box it belongs to. For any point x P Rd, the interpretable features are then defined as a 0 \u00b4 1 vector corresponding to the discretization of x being the same as the discretization of \u03be. Namely, zj \u201c 1\u03c6\u0302jpxq\u201c\u03c6\u0302jp\u03beq for all 1 \u010f j \u010f d. Intuitively, these\ncategorical features correspond to the absence or presence of interpretable components. The discretization process makes a huge difference with respect to other methods: we lose the obvious link with the gradient of the function, and it is much more complicated to see how the local properties of f influence the result of the LIME algorithm, even in a simple setting. In all our experiments, we took p \u201c 4 (quartile discretization, the default setting).\n\u22123 \u22122 \u22121 0 1 2 3\u22123\n\u22122\n\u22121\n0\n1\n2\n3\n\u03be\nq1\u2212 q1+\nq2\u2212\nq2+\nxi\nEmpirical vs. theoretical quantiles\nFigure 3: A visualization of the train set in dimension d \u201c 2 with \u00b5 \u201c p0, 0qJ, and \u03c32 \u201c 1. The empirical quantiles (dashed green lines) are already very close to the theoretical quantiles (green lines) for ntrain \u201c 500. The main difference in the procedure appears if \u03be (red cross) is chosen at the edge of a quantile box, changing the way all the new samples are encoded. But for a train set containing enough observations and a generic \u03be, there is virtually no difference between using the theoretical quantiles and the empirical quantiles.\nSampling strategy. Along with \u03c6\u0302, TabularLIME creates an un-discretization procedure \u03c8\u0302 : t1, . . . , pu \u00d1 R. Simply put, given a coordinate j and a bin index k, \u03c8\u0302jpkq samples a truncated Gaussian on the corresponding bin, with parameters computed from the training set. The TabularLIME sampling strategy for a new example amounts to (i) sample yi P t1, . . . , pud a random variable such that the yij are independent samples of the discrete uniform distribution on t1, . . . , pu, and (ii) apply the un-discretization step, that is, return \u03c8\u0302pyq. We will denote by x1, . . . , xn P Rd these new examples, and z1, . . . , zn P t0, 1ud their discretized counterparts. Note that it is possible to take other bin boxes than those given by the empirical quantiles, the yijs are then sampled according to the frequency observed in the dataset. The sampling step of TabularLIME helps to explore the values of the function in the neighborhood of the instance to explain. Thus it is not so important to sample according to the distribution of the data,\nand a Gaussian sampling that mimics it is enough.\nAssuming that we know the distribution of the train data, it is possible to use the theoretical quantiles instead of the empirical ones. For a large number of examples, they are arbitrary close (see, for instance, Lemma 21.2 in Van der Vaart (2000)). See Figure 3 for an illustration. It is this approach that we will take from now on: we denote the discretization step by \u03c6 and denote the quantiles by qjk for 1 \u010f j \u010f d and 0 \u010f k \u010f p to mark this slight difference. Also note that, for every 1 \u010f j \u010f d, we set qj\u02d8 the quantiles bounding \u03bej , that is, qj\u00b4 \u010f \u03bej \u0103 qj` (see Figure 2).\nTrain set. TabularLIME requires a train set, which is left free to the user. In spirit, one should sample according to the distribution of the train set used to fit the model f . Nevertheless, this train set is rarely available, and from now on, we choose to consider draws from a N ` \u00b5, \u03c32Id \u02d8\n. The parameters of this Gaussian can be estimated from the training data that was used for f if available. Thus, in our setting, along each dimension j, the pqjkq0\u010fk\u010fp are the (rescaled) quantiles of the normal distribution. In particular, they are identical for all features. A fundamental consequence is that sampling the new examples xis first and then discretizing has the same distribution as sampling first the bin indices yis and then un-discretizing.\nWeights. We choose to give each example the weight\n\u03c0i :\u201c exp \u02dc \u00b4 \u2016xi \u00b4 \u03be\u20162\n2\u03bd2\n\u00b8\n, (2.1)\nwhere \u2016\u00a8\u2016 is the Euclidean norm on Rd and \u03bd \u0105 0 is a bandwidth parameter. It should be clear that \u03bd is a hard parameter to tune:\n\u2013 if \u03bd is very large, then all the examples receive positive weights: we are trying to build a simple model that captures the complexity of f at a global scale. This cannot work if f is too complicated. \u2013 if \u03bd is too small, then only examples in the immediate neighborhood of \u03be receive positive weights. Given the discretization step, this amounts to choosing zi \u201c p1, . . . , 1qJ for all i. Thus the linear model built on top would just be a constant fit, missing all the relevant information.\nNote that other distances than the Euclidean distance can be used, for instance the cosine distance for text data. The default implementation of LIME uses \u2016zi \u00b4 t\u2016 instead of \u2016xi \u00b4 \u03be\u2016, with bandwidth set to 0.75d. We choose to use the true Euclidean distance between \u03be and the new examples as it can be seen as a smoothed version of the distance to zi and has the same behavior.\nInterpretable model. The final step in TabularLIME is to build a local interpretable model. Given a class of simple, interpretable models G, TabularLIME selects the best of these models by solving\narg min gPG\n\" Lnpf, g, \u03c0\u03beq ` \u2126 pgq * , (2.2)\nwhere Ln is a local loss function evaluated on the new examples x1, . . . , xn, and \u2126 : Rd \u00d1 R is a regularizer function. For instance, a natural choice for the local loss function is the weighted squared loss\nLnpf, g, \u03c0q :\u201c 1 n\nn \u00ff i\u201c1 \u03c0i pfpxiq \u00b4 gpziqq2 . (2.3)\nWe saw in Section 1.1 different possibilities for G. In this paper, we will focus exclusively on the linear models, in our opinion the easiest models to interpret. Namely, we set gpziq \u201c \u03b2Jzi ` \u03b20, with \u03b2 P Rd and \u03b20 P R. To get rid of the intercept \u03b20, we now use the standard approach to introduce a phantom coordinate 0, and z, \u03b2 P Rd`1 with z0 \u201c 1 and \u03b20 \u201c \u03b20. We also stack the zis together to obtain Z P t0, 1un\u02c6pd`1q.\nThe regularization term \u2126pgq is added to insure further interpretability of the model by reducing the number of non-zero coefficients in the linear model given by TabularLIME. Typically, one uses L2 regularization (ridge regression is the default setting of LIME) or L1 regularization (the Lasso). To simplify the analysis, we will set \u2126 \u201c 0 in the following. We believe that many of the results of Section 3 stay true in a regularized setting, especially the switch-off phenomenon that we are going to describe below: coefficients are even more likely to be set to zero when \u2126 \u2030 0.\nIn other words, in our case TabularLIME performs weighted linear regression on the interpretable features zis, and outputs a vector p\u03b2 P Rd`1 such that\np\u03b2 P arg min \u03b2PRd`1\n#\n1 n\nn \u00ff i\u201c1 \u03c0ipyi \u00b4 \u03b2Jziq2\n+\n. (2.4)\nNote that p\u03b2 is a random quantity, with randomness coming from the sampling of the new examples x1, . . . , xn. It is clear that from a theoretical point of view, a big hurdle for the theoretical analysis is the discretization process (going from the xis to the zis).\nRegression vs. classification. To conclude, let us note that TabularLIME can be used both for regression and classification. Here we focus on the regression mode: the outputs of the model are real numbers, and not discrete elements. In some sense, this is a more general setting than the classification case, since the classification mode operates as TabularLIME for\nregression, but with f chosen as the function that gives the likelihood of belonging to a certain class according to the model."}, {"heading": "2.3 Related work", "text": "Let us mention a few other model-agnostic methods that share some characteristics with LIME. We refer to Guidotti et al. (2019) for a thorough review.\nShapley values. Following Shapley (1953) the idea is to estimate for each subset of features S the expected prediction difference \u2206pSq when the value of these features are fixed to those of the example to explain. The contribution of the jth feature is then set to an average of the contribution of j over all possible coalitions (subgroups of features not containing j). They are used in some recent interpretability work, see Lundberg and Lee (2017) for instance. It is extremely costly to compute, and does not provide much information as soon as the number of features is high. Shapley values share with LIME the idea of quantifying how much a feature contributes to the prediction for a given example.\nGradient methods. Also related to LIME, gradient-based methods as in Baehrens et al. (2010) provide local explanations without knowledge of the model. Essentially, these methods compute the partial derivatives of f at a given example. For images, this can yield satisfying plots where, for instance, the contours of the object appear: a saliency map (Zeiler and Fergus, 2014). Shrikumar et al. (2016, 2017) propose to use the \u201cinput \u02c6 derivative\u201d product, showing advantages over gradient methods. But in any case, the output of these gradient based methods is not so interpretable since the number of features is so high. LIME gets around this problem by using a local dictionary with much smaller dimensionality than the input space."}, {"heading": "3 Theoretical value of the coefficients of the surrogate model", "text": "We are now ready to state our main result. Let us denote by p\u03b2 the coefficients of the linear surrogate model obtained by TabularLIME. In a nutshell, when the underlying model f is linear, we can derive the average value \u03b2 of the p\u03b2 coefficients. In particular, we will see that the \u03b2js are proportional to the partial derivatives Bjfp\u03beq. The exact form of the proportionality coefficients is given in the formal statement below, it essentially depends on the scaling parameters\n\u00b5\u0303 :\u201c \u03bd 2\u00b5` \u03c32\u03be \u03bd2 ` \u03c32 P R d and \u03c3\u0303 :\u201c \u03bd 2\u03c32 \u03bd2 ` \u03c32 \u0105 0 ,\nand the qj\u02d8s, the quantiles left and right of the \u03bejs. Theorem 3.1 (Coefficients of the surrogate model, theoretical values). Assume that f is of the form x \u00de\u00d1 aJx` b, and set\n\u03b2 :\u201c\n\u00a8\n\u02da\n\u02da\n\u02da\n\u02da\n\u02dd\nfp\u00b5\u0303q ` \u0159d j\u201c1 aj\u03b8j 1\u00b4\u03b1j\n\u00b4a1\u03b81 \u03b11p1\u00b4\u03b11q\n... \u00b4ad\u03b8d\n\u03b1dp1\u00b4\u03b1dq\n\u02db\n\u2039\n\u2039\n\u2039\n\u2039\n\u201a\nP Rd`1 , (3.1)\nwhere, for any 1 \u010f j \u010f d, we defined\n\u03b1j :\u201c \u201e 1 2erf \u02c6 x\u00b4 \u00b5\u0303j \u03c3\u0303 ? 2\n\u02d9qj`\nqj\u00b4\n,\nand \u03b8j :\u201c \u201e\n\u03c3\u0303? 2\u03c0 exp \u02c6 \u00b4px\u00b4 \u00b5\u0303jq2 2\u03c3\u03032 \u02d9qj`\nqj\u00b4\n.\nLet \u03b7 P p0, 1q. Then, with high probability greater than 1\u00b4 \u03b7, it holds that\u2225\u2225\u2225p\u03b2 \u00b4 \u03b2\u2225\u2225\u2225 \u00c0 maxp\u03c3 \u2016\u2207f\u2016 , fp\u00b5\u0303q ` \u03c3\u0303 \u2016\u2207f\u2016qc log 1{\u03b7\nn .\nA precise statement with the accurate dependencies in the dimension and the constants hidden in the result can be found in the Appendix (Theorem 10.1). Before discussing the consequences of Theorem 3.1 in the next section, remark that since \u03be is encoded by p1, 1, . . . , 1qJ, the prediction of the local model at \u03be, f\u0302p\u03beq, is just the sum of the p\u03b2js. According to Theorem 3.1, f\u0302p\u03beq will be close to this value, with high probability. Thus we also have a statement about the error made by the surrogate model in \u03be. Corollary 3.1 (Local error of the surrogate model). Let \u03b7 P p0, 1q. Then, under the assumptions of Theorem 3.1, with probability greater than 1\u00b4 \u03b7, it holds that\u2223\u2223\u2223\u2223\u2223f\u0302p\u03beq \u00b4 fp\u00b5\u0303q ` d\u00ff\nj\u201c1\naj\u03b8j \u03b1j \u2223\u2223\u2223\u2223\u2223 \u010f \u010f maxp\u03c3 \u2016\u2207f\u2016 , fp\u00b5\u0303q ` \u03c3\u0303 \u2016\u2207f\u2016q c\nlog 1{\u03b7 n ,\nwith hidden constants depending on d and the \u03b1js.\nObviously the goal of TabularLIME is not to produce a very accurate model, but to provide interpretability. The error of the local model can be seen as a hint about how reliable the interpretation might be."}, {"heading": "4 Consequences of our main results", "text": "We now discuss the consequences of Theorem 3.1 and Corollary 3.1.\nDependency in the partial derivatives. A first consequence of Theorem 3.1 is that the coefficients of the linear model given by TabularLIME are approximately proportional to the partial derivatives of f at \u03be, with constant depending on our assumptions. An interesting follow-up is that, if f depends only on a few features, then the partial derivatives in the other coordinates are zero, and the coefficients given by TabularLIME for these coordinates will be 0 as well. For instance, if fpxq \u201c 10x1 \u00b4 10x2 as in Figure 4, then \u03b21 \u00bb 11.4, \u03b22 \u00bb \u00b44.1, and \u03b2j \u201c 0 for all j \u011b 3. In a simple setting, we thus showed that TabularLIME does not produce interpretations with additional erroneous feature dependencies. Indeed, when the number of samples is high, the coordinates which do not influence the prediction will have a coefficient close to the theoretical value 0 in the surrogate linear model. For a bandwidth not too large, this dependency in the partial derivatives seems to hold to some extent for more general functions. See for instance Figure 6, where we demonstrate this phenomenon for a kernel regressor.\nRobustness of the explanations. Theorem 3.1 means that, for large n, TabularLIME outputs coefficients that are very close to \u03b2 with high probability, where \u03b2 is a vector that can be computed explicitly as\n\u22127.5\n\u22125.0\n\u22122.5\n0.0\n2.5\n5.0\n7.5\nCoeff c ents of the surrogate model for a l near model learned on Boston Hous ng\nper Eq. (3.1). Still without looking too closely at the values of \u03b2, this is already interesting and hints that there is some robustness in the interpretations provided by TabularLIME: given enough samples, the explanation will not jump from one feature to the other. This is a desirable property for any interpretable method, since the user does not want explanations to change randomly with different runs of the algorithm. We illustrate this phenomenon in Figure 5.\nInfluence of the bandwidth. Unfortunately, Theorem 3.1 does not provide directly a founded way to pick \u03bd, which would for instance minimize the variance for a given level of noise. The quest for a founded heuristic is still open. However, we gain some interesting insights on the role of \u03bd. Namely, for fixed \u03be, \u00b5, and \u03c3, the multiplicative constants \u03b8j{p\u03b1jp1\u00b4 \u03b1jqq appearing in Eq. (3.1) depend essentially on \u03bd.\nWithout looking too much into these constants, one can already see that they regulate the magnitude of the coefficients of the surrogate model in a non-trivial way. For instance, in the experiment depicted in Figure 4, the partial derivative of f along the two first coordinate has the same magnitude, whereas the interpretable coefficient is much larger for the first coordinate than the second. Thus we believe that the value of the coefficients in the obtained linear model should not be taken too much into account.\nMore disturbing, it is possible to artificially (or by\naccident) put \u03b8j to zero, therefore forgetting about feature j in the explanation, whereas it could play an important role in the prediction. To see why, we have to return to the definition of the \u03b8js: since qj\u00b4 \u0103 qj` by construction, to have \u03b8j \u201c 0 is possible only if\nVcrit :\u201c \u03c32 2\u03bej \u00b4 qj\u00b4 \u00b4 qj` \u00b42\u00b5j ` qj\u00b4 ` qj` \u0105 0 , (4.1)\nand \u03bd2 is set to Vcrit. We demonstrate this switchingoff phenomenon in Figure 7. An interesting take is that \u03bd not only decides at which scale the explanation is made, but also the magnitude of the coefficients in the interpretable model, even for small changes of \u03bd.\nError of the surrogate model. A simple consequence of Corollary 3.1 is that, unless some cancellation happens between in the term fp\u00b5\u0303q\u00b4 \u0159\nj aj\u03b8j \u03b1j , the local error of the surrogate model is bounded away from zero. For instance, as soon as \u00b5\u0303 \u2030 \u00b5, it is the general situation. Therefore, the surrogate model produced by TabularLIME is not accurate in general. We show some experimental results in Figure 8.\nFinally, we discuss briefly the limitations of Theorem 3.1.\nLinearity of f . The linearity of f is a quite restrictive assumption, but we think that it is useful to consider for two reasons.\nFirst, the weighted nature of the procedure means that TabularLIME is not considering examples that are too far away from \u03be with respect to the scaling parameter \u03bd. Thus it is truly a local assumption on f , that could be replaced by a boundedness assumption on the Hessian of f in the neighborhood of \u03be, at the price of more technicalities and assuming that \u03bd is not too large. See, in particular, Lemma 11.3 in the Appendix, after which we discuss an extension of the proof when f is linear with a second degree perturbative term. We show in Figure 6 how our theoretical predictions behave for a non-linear function (a kernel ridge regressor).\nSecond, our main concern is to know whether TabularLIME operates correctly in a simple setting, and not to provide bounds for the most general f possible. Indeed, if we can already show imperfect behavior for TabularLIME when f is linear as seen earlier, our guess is that such behavior will only worsen for more complicated f .\nSampling strategy. In our derivation, we use the theoretical quantiles of the Gaussian distribution along each axis, and not prescribed quantiles. We believe that the proof could eventually be adapted, but that the result would loose in clarity. Indeed, the computations for a truncated Gaussian distribution are far more convoluted than for a Gaussian distribution. For instance, in the proof of Lemma 8.1 in the Appendix, some complicated quantities depending on the prescribed\nquantiles would appear when computing E r\u03c0iziks."}, {"heading": "5 Proof of Theorem 3.1", "text": "In this section, we explain how Theorem 3.1 is obtained. All formal statements and proofs are in the Appendix.\nOutline. The main idea underlying the proof is to realize that p\u03b2 is the solution of a weighted least squares problem. Denote by \u03a0 P Rn\u02c6n the diagonal matrix such that \u03a0ii \u201c \u03c0i (the weight matrix), and set fpxq P Rd`1 the response vector. Then, taking the gradient of Eq. (5.1), one obtains the key equation\npZJ\u03a0Zqp\u03b2 \u201c ZJ\u03a0fpxq . (5.1)\nLet us define p\u03a3 :\u201c 1nZ J\u03a0Z and p\u0393 :\u201c 1nZ J\u03a0fpxq, as well as their population counterparts \u03a3 :\u201c Erp\u03a3s and \u0393 :\u201c Erp\u0393s. Intuitively, if we can show that p\u03a3 and p\u0393 are close to \u03a3 and \u0393, assuming that \u03a3 is invertible, then we can show that p\u03b2 is close to \u03b2 :\u201c \u03a3\u00b41\u0393.\nThe main difficulties in the proof come from the nonlinear nature of the new features zi, introducing tractable but challenging integrals. Fortunately, the Gaussian sampling of LIME allows us to overcome these challenges (at the price of heavy computations).\nCovariance matrix. The first part of our analysis is thus concerned with the study of the empirical covariance matrix p\u03a3. Perhaps surprisingly, it is possible\nto compute the population version of p\u03a3:\n\u03a3 \u201c Cd\n\u00a8\n\u02da\n\u02da\n\u02da\n\u02dd 1 \u03b11 \u00a8 \u00a8 \u00a8 \u03b1d \u03b11 \u03b11 \u03b1i\u03b1j ...\n. . . \u03b1d \u03b1i\u03b1j \u03b1d\n\u02db\n\u2039\n\u2039\n\u2039\n\u201a\n,\nwhere the \u03b1js were defined in Section 3, and Cd is a scaling constant that does not appear in the final result (see Lemma 8.1).\nSince the \u03b1js are always distinct from 0 and 1, the special structure of \u03a3 makes it possible to invert it in closed-form. We show in Lemma 8.2 that\nC\u00b41d\n\u00a8\n\u02da\n\u02da\n\u02da\n\u02da\n\u02dd\n1` \u0159d j\u201c1 \u03b1j 1\u00b4\u03b1j\n\u00b41 1\u00b4\u03b11 \u00a8 \u00a8 \u00a8\n\u00b41 1\u00b4\u03b1d\n\u00b41 1\u00b4\u03b11\n1 \u03b11p1\u00b4\u03b11q 0\n... . . .\n\u00b41 1\u00b4\u03b1d 0\n1 \u03b1dp1\u00b4\u03b1dq\n\u02db\n\u2039\n\u2039\n\u2039\n\u2039\n\u201a\n.\nWe then achieve control of \u2225\u2225\u2225p\u03a3\u00b41 \u00b4 \u03a3\u00b41\u2225\u2225\u2225\nop via standard\nconcentration inequalities, since the new samples are Gaussian and the binary features are bounded (see Proposition 8.1).\nRight-hand side of Eq. (5.1). Again, despite the non-linear nature of the new features, it is possible to compute the expected version of p\u0393 in our setting. In this case, we show in Lemma 9.1 that\n\u0393 \u201c Cd\n\u00a8\n\u02da\n\u02da\n\u02da\n\u02dd\nfp\u00b5\u0303q \u03b11fp\u00b5\u0303q \u00b4 a1\u03b81 ... \u03b1dfp\u00b5\u0303q \u00b4 ad\u03b8d\n\u02db\n\u2039\n\u2039\n\u2039\n\u201a\n,\nwhere the \u03b8js were defined in Section 3. They play an analogous role to the \u03b1js but, as noted before, they are signed quantities. As with the analysis of the covariance matrix, since the weights and the new features are bounded, it is possible to show a concentration result for p\u0393 (see Lemma 9.3).\nConcluding the proof. We can now conclude, first upper bounding \u2225\u2225\u2225p\u03b2 \u00b4 \u03a3\u00b41\u0393\u2225\u2225\u2225 by\u2225\u2225\u2225p\u03a3\u00b41\u2225\u2225\u2225 op \u2225\u2225\u2225p\u0393\u00b4 \u0393\u2225\u2225\u2225` \u2225\u2225\u2225p\u03a3\u00b41 \u00b4 \u03a3\u00b41\u2225\u2225\u2225 op \u2016\u0393\u2016 ,\nand then controlling each of these terms using the previous concentration results. The expression of \u03b2 is simply obtained by multiplying \u03a3\u00b41 and \u0393."}, {"heading": "6 Conclusion and future directions", "text": "In this paper we provide the first theoretical analysis of LIME, with some good news (LIME discovers interesting features) and bad news (LIME might forget\nsome important features and the surrogate model is not faithful). All our theoretical results are verified by simulations.\nFor future work, we would like to complement these results in various directions: Our main goal is to extend the current proof to any function by replacing f by its Taylor expansion at \u03be. On a more technical side, we would like to extend our proof to other distance functions (e.g., distances between the zis and \u03be, which is the default setting of LIME), to non-isotropic sampling of the xis (that is, \u03c3 not constant across the dimensions), and to ridge regression."}, {"heading": "Acknowledgements", "text": "The authors would like to thank Christophe Biernacki for getting them interested in the topic, as well as Leena Chennuru Vankadara for her careful proofreading. This work has been supported by the German Research Foundation through the Institutional Strategy of the University of T\u00fcbingen (DFG, ZUK 63), the Cluster of Excellence \u201cMachine Learning\u2014New Perspectives for Science\u201d (EXC 2064/1 number 390727645), and the BMBF Tuebingen AI Center (FKZ: 01IS18039A)."}, {"heading": "7 Setting", "text": "Let us recall briefly the main assumptions under which we prove Theorem 3.1. Recall that they are discussed in details in Section 2.2 of the main paper. H1 (Linear f). The black-box model can be written aJx` b, with a P Rd and b P R fixed. H2 (Gaussian sampling). The random variables x1, . . . , xn are i.i.d. N ` \u00b5, \u03c32Id \u02d8 .\nAlso recall that, for any 1 \u010f i \u010f n, we set the weights to\n\u03c0i :\u201c exp \u02dc \u00b4 \u2016xi \u00b4 \u03be\u20162\n2\u03bd2\n\u00b8\n. (7.1)\nWe will need the following scaling constant:\nCd :\u201c \u02c6 \u03bd2\n\u03bd2 ` \u03c32\n\u02d9d{2 \u00a8 exp \u02dc \u00b4 \u2016\u03be \u00b4 \u00b5\u20162\n2p\u03bd2 ` \u03c32q\n\u00b8\n, (7.2)\nwhich does not play any role in the final result. One can check that Cd \u00d1 1 when \u03bd \" \u03c3, regardless of the dimension.\nFinally, for any 1 \u010f j \u010f d, recall that we defined\n\u03b1j :\u201c \u201e 1 2erf \u02c6 x\u00b4 \u00b5\u0303j \u03c3\u0303 ? 2\n\u02d9qj`\nqj\u00b4\n, (7.3)\nand \u03b8j :\u201c \u201e\n\u03c3\u0303? 2\u03c0 exp \u02c6 \u00b4px\u00b4 \u00b5\u0303jq2 2\u03c3\u03032 \u02d9qj`\nqj\u00b4\n, (7.4)\nwhere qj\u02d8 are the quantile boundaries of \u03bej . These coefficients are discussed in Section 5 of the main paper. Note that all the expected values are taken with respect to the randomness on the x1, . . . , xn."}, {"heading": "8 Covariance matrix", "text": "In this section, we state and prove the intermediate results used to control the covariance matrix p\u03a3. The goal of this section is to obtain the control of \u2225\u2225\u2225p\u03a3\u00b41 \u00b4 \u03a3\u00b41\u2225\u2225\u2225 op in probability. Intuitively, if this quantity is small enough, then we can inverse Eq. (5.1) and make very precise statements about p\u03b2.\nWe first show that it is possible to compute the expected covariance matrix in closed form. Without this result, a concentration result would still hold, but it would be much harder to gain precise insights on the \u03b2js.\nLemma 8.1 (Expected covariance matrix). Under Assumption 2, the expected value of p\u03a3 is given by\n\u03a3 :\u201c Cd\n\u00a8\n\u02da\n\u02da\n\u02da\n\u02dd 1 \u03b11 \u00a8 \u00a8 \u00a8 \u03b1d \u03b11 \u03b11 \u03b1i\u03b1j ...\n. . . \u03b1d \u03b1i\u03b1j \u03b1d\n\u02db\n\u2039\n\u2039\n\u2039\n\u201a\n.\nProof. Elementary computations yield\np\u03a3 \u201c 1 n\n\u00a8\n\u02da\n\u02da\n\u02da\n\u02dd\n\u0159n i\u201c1 \u03c0i\n\u0159n i\u201c1 \u03c0izi1 \u00a8 \u00a8 \u00a8\n\u0159n i\u201c1 \u03c0izid\n\u0159n i\u201c1 \u03c0izi1\n\u0159n i\u201c1 \u03c0izi1\n\u0159n i\u201c1 \u03c0izikzi`\n... . . .\n\u0159n i\u201c1 \u03c0izid \u0159n i\u201c1 \u03c0izikzi`\n\u0159n i\u201c1 \u03c0izid\n\u02db\n\u2039\n\u2039\n\u2039\n\u201a\n.\nReading the coefficients of this matrix, we have essentially three computations to complete: E r\u03c0is, E r\u03c0iziks, and E r\u03c0izikzi`s.\nComputation of E r\u03c0is. Since the xis are Gaussian (Assumption 2) and using the definition of the weights (Eq. (7.1)), we can write\nE r\u03c0is \u201c \u017c\nRd exp\n\u02dc\n\u00b4 \u2016xi \u00b4 \u03be\u20162\n2\u03bd2\n\u00b8 exp \u02dc \u00b4 \u2016xi \u00b4 \u00b5\u20162\n2\u03c32\n\u00b8\ndxi1 \u00a8 \u00a8 \u00a8xid p2\u03c0\u03c32qd{2 .\nBy independence across coordinates, the last display amounts to\nd \u017a\nj\u201c1\n\u017c `8\n\u00b48 exp\n\u02c6 \u00b4px\u00b4 \u03bejq2 2\u03bd2 ` \u00b4px\u00b4 \u00b5jq2 2\u03c32 \u02d9 dx \u03c3 ? 2\u03c0 .\nWe then apply Lemma 11.1 to each of the integrals within the product to obtain\nd \u017a\nj\u201c1\n\u03bd? \u03bd2 ` \u03c32 \u00a8 exp \u02c6 \u00b4p\u03bej \u00b4 \u00b5jq2 2p\u03bd2 ` \u03c32q \u02d9 \u201c \u03bd d p\u03bd2 ` \u03c32qd{2 \u00a8 exp\n\u02dc\n\u00b4 \u2016\u03be \u00b4 \u00b5\u20162\n2p\u03bd2 ` \u03c32q\n\u00b8\n.\nWe recognize the definition of the scaling constant (Eq. (7.2)): we have proved that E r\u03c0is \u201c Cd.\nComputation of E r\u03c0iziks. Since the xis are Gaussian (Assumption 2) and using the definition of the weights (Eq. (7.1)),\nE r\u03c0is \u201c \u017c\nRd exp\n\u02dc\n\u00b4 \u2016xi \u00b4 \u03be\u20162\n2\u03bd2\n\u00b8 exp \u02dc \u00b4 \u2016xi \u00b4 \u00b5\u20162\n2\u03c32\n\u00b8\n1\u03c6pxiqk\u201c\u03c6p\u03beqk dxi1 \u00a8 \u00a8 \u00a8xid p2\u03c0\u03c32qd{2 .\nBy independence across coordinates, the last display amounts to\n\u017c qk`\nqk\u00b4\nexp \u02c6 \u00b4px\u00b4 \u03bekq2 2\u03bd2 ` \u00b4px\u00b4 \u00b5kq2 2\u03c32 \u02d9 dx \u03c3 ? 2\u03c0 \u00a8 d \u017a\nj\u201c1 j\u2030k\n\u017c `8\n\u00b48 exp\n\u02c6 \u00b4px\u00b4 \u03bejq2 2\u03bd2 ` \u00b4px\u00b4 \u00b5jq2 2\u03c32 \u02d9 dx \u03c3 ? 2\u03c0 .\nUsing Lemma 11.1, we obtain\n\u03bdd\np\u03bd2 ` \u03c32qd{2 \u00a8 exp\n\u02dc\n\u00b4 \u2016\u03be \u00b4 \u00b5\u20162\n2p\u03bd2 ` \u03c32q\n\u00b8 \u00a8 \u00ab\n1 2erf\n\u02dc\n\u03bd2px\u00b4 \u00b5kq ` \u03c32px\u00b4 \u03bekq \u03bd\u03c3 a 2p\u03bd2 ` \u03c32q\n\u00b8ffqk`\nqk\u00b4\n.\nWe recognize the definition of the scaling constant (Eq. (7.2)) and of the \u03b1k coefficient (Eq. (7.3)): we have proved that E r\u03c0iziks \u201c Cd\u03b1k.\nComputation of E r\u03c0izikzi`s. Since the xis are Gaussian (Assumption 2) and using the definition of the weights (Eq. (7.1)),\nE r\u03c0izikzi`s \u201c \u017c\nRd exp\n\u02dc\n\u00b4 \u2016xi \u00b4 \u03be\u20162\n2\u03bd2\n\u00b8 exp \u02dc \u00b4 \u2016xi \u00b4 \u00b5\u20162\n2\u03c32\n\u00b8\n1\u03c6pxiqk\u201c\u03c6p\u03beqk 1\u03c6pxiq`\u201c\u03c6p\u03beq` dxi1 \u00a8 \u00a8 \u00a8 dxid p2\u03c0\u03c32qd{2 .\nBy independence across coordinates, the last display amounts to d \u017a\nj\u201c1 j\u2030k,`\n\u017c `8\n\u00b48 exp\n\u02c6 \u00b4px\u00b4 \u03bejq2 2\u03bd2 ` \u00b4px\u00b4 \u00b5jq2 2\u03c32 \u02d9 dx \u03c3 ? 2\u03c0 \u00a8 \u017c qk`\nqk\u00b4\nexp \u02c6 \u00b4px\u00b4 \u03bekq2 2\u03bd2 ` \u00b4px\u00b4 \u00b5kq2 2\u03c32 \u02d9 dx \u03c3 ? 2\u03c0\n\u00a8 \u017c q``\nq`\u00b4\nexp \u02c6 \u00b4px\u00b4 \u03be`q2 2\u03bd2 ` \u00b4px\u00b4 \u00b5`q2 2\u03c32 \u02d9 dx \u03c3 ? 2\u03c0 .\nUsing Lemma 11.1, we obtain\n\u03bdd\np\u03bd2 ` \u03c32qd{2 \u00a8 exp\n\u02dc\n\u00b4 \u2016\u03be \u00b4 \u00b5\u20162\n2p\u03bd2 ` \u03c32q\n\u00b8 \u00a8 \u00ab\n1 2erf\n\u02dc\n\u03bd2px\u00b4 \u00b5kq ` \u03c32px\u00b4 \u03bekq \u03bd\u03c3 a 2p\u03bd2 ` \u03c32q\n\u00b8ffqk`\nqk\u00b4\n\u00a8 \u00ab\n1 2erf\n\u02dc\n\u03bd2px\u00b4 \u00b5`q ` \u03c32px\u00b4 \u03be`q \u03bd\u03c3 a 2p\u03bd2 ` \u03c32q\n\u00b8ffq``\nq`\u00b4\n.\nWe recognize the definition of the scaling constant (Eq. (7.2)) and of the alphas (Eq. (7.3)): we have proved that E r\u03c0izikzi`s \u201c Cd\u03b1k\u03b1`.\nAs it turns out, we show that it is possible to invert \u03a3 in closed-form, therefore simplifying tremendously our quest for control of \u2225\u2225\u2225p\u03a3\u00b41 \u00b4 \u03a3\u00b41\u2225\u2225\u2225 op . Indeed, in most cases, even if concentration could be shown, one would not have a precise idea of the coefficients of \u03a3\u00b41. Lemma 8.2 (Inverse of the covariance matrix). If \u03b1j \u2030 0, 1 for any j P t1, . . . , du, then \u03a3 is invertible, and\n\u03a3\u00b41 \u201c C\u00b41d\n\u00a8\n\u02da\n\u02da\n\u02da\n\u02da\n\u02dd\n1` \u0159d j\u201c1 \u03b1j 1\u00b4\u03b1j\n\u00b41 1\u00b4\u03b11 \u00a8 \u00a8 \u00a8\n\u00b41 1\u00b4\u03b1d\n\u00b41 1\u00b4\u03b11\n1 \u03b11p1\u00b4\u03b11q 0\n... . . .\n\u00b41 1\u00b4\u03b1d 0\n1 \u03b1dp1\u00b4\u03b1dq\n\u02db\n\u2039\n\u2039\n\u2039\n\u2039\n\u201a\n.\nProof. Define \u03b1 P Rd the vector of the \u03b1js. Set A :\u201c 1, B :\u201c \u03b1J, C :\u201c \u03b1, and\nD :\u201c\n\u00a8\n\u02da\n\u02dd \u03b11 \u03b1j\u03b1k . . .\n\u03b1j\u03b1k \u03b1d\n\u02db\n\u2039\n\u201a\n.\nThen \u03a3 is a block matrix that can be written \u03a3 \u201c Cd \u201e\nA B C D\n\n. We notice that\nD \u00b4 CA\u00b41B \u201c Diag p\u03b11p1\u00b4 \u03b11q, . . . , \u03b1dp1\u00b4 \u03b1dqq .\nNote that, since erf is an increasing function, the \u03b1js are always distinct from 0 and 1. Thus D \u00b4 CA\u00b41B is an invertible matrix, and we can use the block matrix inversion formula to obtain the claimed result.\nAs a direct consequence of the computation of \u03a3\u00b41, we can control its largest eigenvalue. Lemma 8.3 (Control of \u2225\u2225\u03a3\u00b41\u2225\u2225op). We have the following bound on the operator norm of the inverse covariance matrix: \u2225\u2225\u03a3\u00b41\u2225\u2225op \u010f 3dAdCd , where Ad :\u201c max1\u010fj\u010fd 1\u03b1jp1\u00b4\u03b1jq .\nProof. We control the operator norm of \u03a3\u00b41 by its Frobenius norm: Namely,\u2225\u2225\u03a3\u00b41\u2225\u22252op \u010f \u2225\u2225\u03a3\u00b41\u2225\u22252F \u201c C\u00b42d \u00ab \u02c6 1` \u00ff \u03b1j\n1\u00b4 \u03b1j \u02d92 ` \u00ff 1 p1\u00b4 \u03b1jq2 ` \u00ff 1 \u03b1jp1\u00b4 \u03b1jq\nff\n\u2225\u2225\u03a3\u00b41\u2225\u22252op \u010f 6C\u00b42d d2 \u02c6max 1\u03b1jp1\u00b4 \u03b1jq \u02d92 ,\nwhere we used \u03b1j P p0, 1q in the last step of the derivation.\nRemark 8.1. Better bounds can without doubt be obtained. A step in this direction is to notice that S :\u201c Cd\u03a3\u00b41 is an arrowhead matrix (O\u2019Leary and Stewart, 1996). Thus the eigenvalues of S are solutions of the secular equation\n1` d \u00ff\nj\u201c1\n\u03b1j 1\u00b4 \u03b1j\n\u00b4 \u03bb` d \u00ff\nj\u201c1\n\u03b1j p1\u00b4 \u03b1jqp1\u00b4 \u03bb\u03b1jp1\u00b4 \u03b1jqq \u201c 0 .\nFurther study of this equation could yield an improved statement for Lemma 8.3.\nWe now show that the empirical covariance matrix concentrates around \u03a3. It is interesting to see that the non-linear nature of the new coordinates (the zijs) calls for complicated computations but allows us to use simple concentration tools since they are, in essence, Bernoulli random variables. Lemma 8.4 (Concentration of the empirical covariance matrix). Let p\u03a3 and \u03a3 be defined as before. Then, for every t \u0105 0,\nP \u02c6\u2225\u2225\u2225p\u03a3\u00b4 \u03a3\u2225\u2225\u2225\nop \u011b t\n\u02d9\n\u010f 4d2 expp\u00b42nt2q .\nProof. Recall that \u2016\u00a8\u2016op \u010f \u2016\u00a8\u2016F: it suffices to show the result for the Frobenius norm. Next, we notice that the summands appearing in the entries of p\u03a3, Xp1qi :\u201c \u03c0i, X p2,kq i :\u201c \u03c0izik, and X p3,k,`q i :\u201c \u03c0izikzi`, are all bounded. Indeed, by the definition of the weights and the definition of the new features, they all take values in r0, 1s. Moreover, for given k, `, they are independent random variables. Thus we can apply Hoeffding\u2019s inequality (Theorem 11.1) to Xp1qi , X p2,kq i , and X p3,k,`q i . For any given t \u0105 0, we obtain\n$\n\u2019 &\n\u2019 %\nP ` \u2223\u2223 1 n \u0159n i\u201c1p\u03c0i \u00b4 E r\u03c0isq \u2223\u2223 \u011b t\u02d8 \u010f 2 expp\u00b42nt2q P ` \u2223\u2223 1 n \u0159n i\u201c1p\u03c0izik \u00b4 E r\u03c0isq\n\u2223\u2223 \u011b t\u02d8 \u010f 2 expp\u00b42nt2q P ` \u2223\u2223 1 n \u0159n i\u201c1p\u03c0izikzi` \u00b4 E r\u03c0isq\n\u2223\u2223 \u011b t\u02d8 \u010f 2 expp\u00b42nt2q We conclude by a union bound on the pd` 1q2 \u010f 2d2 entries of the matrix.\nAs a consequence of the two preceding lemmas, we can control the largest eigenvalue of \u03a3\u00b41. Lemma 8.5 (Control of \u2225\u2225\u2225p\u03a3\u00b41\u2225\u2225\u2225\nop ). For every t P\n\u00b4 0, Cd6dAd \u0131 , with probability greater than 1\u00b4 4d2 expp\u00b42nt2q,\n\u2225\u2225\u2225p\u03a3\u00b41\u2225\u2225\u2225 op \u010f 6dAd Cd .\nProof. Let t P p0, Cd{p6dAdqs. According to Lemma 8.3, \u03bbmaxp\u03a3\u00b41q \u010f 3dAd{Cd. We deduce that\n\u03bbminp\u03a3q \u011b Cd\n3dAd .\nNow let us use Lemma 8.4 with this t: there is an event \u2126, which has probability greater than 1\u00b4 4d2 expp\u00b42nt2q, such that \u2225\u2225\u2225p\u03a3\u00b4 \u03a3\u2225\u2225\u2225 op \u010f t. According to Weyl\u2019s inequality (Weyl, 1912), on this event,\u2223\u2223\u2223\u03bbminpp\u03a3q \u00b4 \u03bbminp\u03a3q\u2223\u2223\u2223 \u010f \u2225\u2225\u2225p\u03a3\u00b4 \u03a3\u2225\u2225\u2225\nop \u010f t .\nIn particular, \u03bbminpp\u03a3q \u011b \u03bbminp\u03a3q \u00b4 t \u011b\nCd 6dAd .\nFinally, we deduce that \u2225\u2225\u2225p\u03a3\u00b41\u2225\u2225\u2225 op \u010f 6dAd Cd .\nWe can now state and prove the main result of this section, controlling the operator norm of p\u03a3\u00b4\u03a3 with high probability.\nProposition 8.1 (Control of \u2225\u2225\u2225p\u03a3\u00b41 \u00b4 \u03a3\u00b41\u2225\u2225\u2225\nop ). For every t P\n\u00b4 0, 3dAdCd \u0131 , we have\nP \u02c6\u2225\u2225\u2225p\u03a3\u00b41 \u00b4 \u03a3\u00b41\u2225\u2225\u2225\nop \u011b t\n\u02d9 \u010f 8d2 exp \u02c6 \u00b4C4dnt2\n162d4A4d\n\u02d9\n.\nRemark 8.2. Proposition 8.1 is the key tool to invert Eq. (5.1) and gain precise control over p\u03b2. In the regime that we consider, the dimension d as well as the number of bins p are fixed, and d,Cd, and Ad are essentially numerical constants. We did not optimize these constant with respect to d, since the main message is to consider the behavior for a large number of new examples (n\u00d1 `8).\nProof. We notice that, assuming that p\u03a3 is invertible, p\u03a3\u00b41\u00b4\u03a3\u00b41 \u201c p\u03a3\u00b41p\u03a3\u00b4p\u03a3q\u03a3\u00b41. Since \u2016\u00a8\u2016op is sub-multiplicative, we just have to control each term individually. Lemma 8.3 gives us\u2225\u2225\u03a3\u00b41\u2225\u2225op \u010f 3dAdCd . Next, set t1 :\u201c C 2 dt\n18d2A2 d . According to Lemma 8.4, with probability greater than 1\u00b4 4d2 expp\u00b42nt21q,\u2225\u2225\u2225p\u03a3\u00b4 \u03a3\u2225\u2225\u2225 op \u010f t1 .\nFinally, set t2 :\u201c t1. It is easy to check that t2 \u010f Cd{p6dAdq. Thus we can use Lemma 8.5: with probability greater than 1\u00b4 4d2 expp\u00b42nt21q, \u2225\u2225\u2225p\u03a3\u00b41\u2225\u2225\u2225\nop \u010f 6dAd Cd .\nBy the union bound, with probability greater than 1\u00b4 8d2 exp \u00b4 \u00b4C4dnt 2\n162d4A4 d\n\u00af\n,\u2225\u2225\u2225p\u03a3\u00b41 \u00b4 p\u03a3\u2225\u2225\u2225 op \u010f \u2225\u2225\u03a3\u00b41\u2225\u2225op \u00a8 \u2225\u2225\u2225p\u03a3\u00b4 \u03a3\u2225\u2225\u2225op \u00a8 \u2225\u2225\u2225p\u03a3\u00b41\u2225\u2225\u2225op \u010f 3dAd\nCd \u00a8 t1 \u00a8 6dAd Cd \u201c t .\n9 Right-hand side of Eq. (5.1)\nIn this section, we state and prove the results in relation to p\u0393. We begin with the computation of \u0393, the expected value of p\u0393. Lemma 9.1 (Computation of \u0393). Under Assumption 2 and 1, the expected value of p\u0393 is given by\n\u0393 \u201c Cd\n\u00a8\n\u02da\n\u02da\n\u02da\n\u02dd\nfp\u00b5\u0303q \u03b11fp\u00b5\u0303q \u00b4 a1\u03b81 ... \u03b1dfp\u00b5\u0303q \u00b4 ad\u03b8d\n\u02db\n\u2039\n\u2039\n\u2039\n\u201a\n,\nwhere the \u03b8js are defined by\n\u03b8j :\u201c \u201e \u03c3\u0303? 2\u03c0 exp \u02c6 \u00b4px\u00b4 \u00b5\u0303jq2 2\u03c3\u03032 \u02d9qj`\nqj\u00b4\n.\nProof. Given the expression of p\u0393, we have essentially two computations to manage: E r\u03c0ifpxiqs and E r\u03c0izijfpxiqs.\nComputation of E r\u03c0ifpxiqs. Under Assumption 1, by linearity of the integral,\nE r\u03c0ifpxiqs \u201c E \u201c \u03c0ipaJ ` bq \u2030 \u201c bE r\u03c0is ` d \u00ff\nj\u201c1 ajE r\u03c0ixijs . (9.1)\nNow we have already seen in the proof of Lemma 8.1 that E r\u03c0is \u201c Cd. Thus we can focus on the computation of E r\u03c0ixijs for fixed i, j. Under Assumption 2, we have\nE r\u03c0ixijs \u201c \u017c\nRd xj \u00a8 exp\n\u02dc\n\u00b4 \u2016x\u00b4 \u03be\u20162 2\u03bd2 ` \u00b4 \u2016x\u00b4 \u00b5\u20162 2\u03c32\n\u00b8\ndx1 \u00a8 \u00a8 \u00a8 dxd p2\u03c0\u03c32qd{2 .\nBy independence, the last display amounts to \u017c `8\n\u00b48 x \u00a8 exp\n\u02c6 \u00b4px\u00b4 \u03bejq2 2\u03bd2 ` \u00b4px\u00b4 \u00b5jq2 2\u03c32 \u02d9 dx \u03c3 ? 2\u03c0 \u00a8 \u017a\nk\u2030j\n\u017c `8\n\u00b48 exp\n\u02c6 \u00b4px\u00b4 \u03bekq2 2\u03bd2 ` \u00b4px\u00b4 \u00b5kq2 2\u03c32 \u02d9 dx \u03c3 ? 2\u03c0 .\nA straightforward application of Lemmas 11.1 and 11.2 yields\nE r\u03c0ixijs \u201c Cd \u00a8 \u03bd2\u00b5j ` \u03c32\u03bej \u03bd2 ` \u03c32 .\nBack to Eq. (9.1), we have shown that\nE r\u03c0ifpxiqs \u201c Cdb` d \u00ff\nj\u201c1 aj \u00a8 Cd\n\u03bd2\u00b5j ` \u03c32\u03bej \u03bd2 ` \u03c32 \u201c Cdfp\u00b5\u0303q .\nComputation of E r\u03c0izijfpxisq. Under Assumption 1, by linearity of the integral,\nE r\u03c0izijfpxiqs \u201c bE r\u03c0izijs ` d \u00ff\nk\u201c1 ak \u00a8 E r\u03c0izijxiks . (9.2)\nWe have already computed E r\u03c0izijs in the proof of Lemma 8.1 and found that\nE r\u03c0izijs \u201c Cd\u03b1j .\nRegarding the computation of E r\u03c0izijxiks, there are essentially two cases to consider depending whether k \u201c ` or not. Let us first consider the case k \u201c j. Then we obtain\nE r\u03c0izijxiks \u201c \u017c\nRd xj exp\n\u02dc\n\u00b4 \u2016x\u00b4 \u03be\u20162 2\u03bd2 ` \u00b4 \u2016x\u00b4 \u00b5\u20162 2\u03c32\n\u00b8\n1\u03c6pxqj\u201c\u03c6p\u03beqj dx1 \u00a8 \u00a8 \u00a8 dxd p2\u03c0\u03c32qd{2 .\nBy independence, the last display amounts to \u017c qj`\nqj\u00b4\nx \u00a8 exp \u02c6 \u00b4px\u00b4 \u03bejq2 2\u03bd2 ` \u00b4px\u00b4 \u00b5jq2 2\u03c32 \u02d9 dx \u03c3 ? 2\u03c0 \u00a8 \u017a\nk\u2030j\n\u017c `8\n\u00b48 exp\n\u02c6 \u00b4px\u00b4 \u03bekq2 2\u03bd2 ` \u00b4px\u00b4 \u00b5kq2 2\u03c32 \u02d9 dx \u03c3 ? 2\u03c0 .\nAccording to Lemma 11.2 and the definition of \u03b1j and \u03b8j (Eqs. (7.3) and (7.3)), we have\nE r\u03c0izijxijs \u201c Cd \u03c32\u03bej ` \u03bd2\u00b5j \u03bd2 ` \u03c32 \u03b1j \u00b4 Cd\u03b8j .\nNow if k \u2030 j, by independence, E r\u03c0izijxiks splits in three parts:\nE r\u03c0izijxiks \u201c \u017c `8\n\u00b48 x \u00a8 exp\n\u02c6 \u00b4px\u00b4 \u03bekq2 2\u03bd2 ` \u00b4px\u00b4 \u00b5kq2 2\u03c32 \u02d9 dx \u03c3 ? 2\u03c0 \u00a8 \u017c qj`\nqj\u00b4\nexp \u02c6 \u00b4px\u00b4 \u03bejq2 2\u03bd2 ` \u00b4px\u00b4 \u00b5jq2 2\u03c32 \u02d9 dx \u03c3 ? 2\u03c0 \u00a8\n\u00a8 \u017a\n`\u2030j,k\n\u017c `8\n\u00b48 exp\n\u02c6 \u00b4px\u00b4 \u03bekq2 2\u03bd2 ` \u00b4px\u00b4 \u00b5kq2 2\u03c32 \u02d9 dx \u03c3 ? 2\u03c0 .\nLemma 11.1 and 11.2 yield\nE r\u03c0izijxiks \u201c Cd \u00a8 \u03c32\u03bek ` \u03bd2\u00b5k \u03bd2 ` \u03c32 \u00a8 \u03b1j .\nIn definitive, plugging these results into Eq. (9.2) gives\nE r\u03c0izijfpxiqs \u201c Cd\u03b1jb` aj \u02c6 Cd \u03c32\u03bej ` \u03bd2\u00b5j \u03bd2 ` \u03c32 \u03b1j \u00b4 Cd\u03b8j \u02d9 ` \u00ff\nk\u2030j ak \u00a8 Cd\n\u03c32\u03bek ` \u03bd2\u00b5k \u03bd2 ` \u03c32 \u03b1j\n\u201c Cd\u03b1jfp\u00b5\u0303q \u00b4 Cdaj\u03b8j .\nAs a consequence of Lemma 9.1, we can control \u2016\u0393\u2016. Lemma 9.2 (Control of \u2016\u0393\u2016). Under Assumptions 2 and 1, it holds that\n\u2016\u0393\u20162 \u010f C2d \u00b4 3dfp\u00b5\u0303q2 ` d\u03c3\u03032 \u2016\u2207f\u20162 \u00af .\nProof. According to Lemma 9.1, we have\n\u2016\u0393\u20162 \u201c C2d\n\u02dc\nfp\u00b5\u0303q2 ` d \u00ff\nj\u201c1 p\u03b1jfp\u00b5\u0303q \u00b4 aj\u03b8jq2\n\u00b8\n.\nSuccessively using px\u00b4 yq2 \u010f 2px2 ` y2q, \u03b1j P r0, 1s and \u03b8j P r\u00b4\u03c3\u0303{ ? 2\u03c0, \u03c3\u0303{ ? 2\u03c0s, we write\n\u2016\u0393\u20162 \u010f C2d\n\u02dc\nfp\u00b5\u0303q2 ` d \u00ff\nj\u201c1 2p\u03b12jfp\u00b5\u0303q2 ` a2j\u03b82j q\n\u00b8\n\u010f C2d \u00b4 3dfp\u00b5\u0303q2 ` d\u03c3\u03032 \u2016a\u20162 \u00af ,\nwhich concludes the proof.\nFinally, we conclude this section with a concentration result for p\u0393. Lemma 9.3 (Concentration of \u2225\u2225\u2225p\u0393\u2225\u2225\u2225). Under Assumptions 2 and 1, for any t \u0105 0, we have P \u00b4\u2225\u2225\u2225p\u0393\u00b4 \u0393\u2225\u2225\u2225 \u0105 t\u00af \u010f 4d exp\u02dc \u00b4nt2\n2 \u2016\u2207f\u20162 \u03c32\n\u00b8\n.\nProof. Since the xi are Gaussian with variance \u03c32 (Assumption 2), the random variable aJxi` b is Gaussian with variance \u2016a\u20162 \u03c32, and the Xp1qi :\u201c \u03c0ixi are sub-Gaussian with parameter \u2016a\u2016 2 \u03c32. They are also independent, thus we can apply Theorem 11.2 to the Xp1qi :\nP \u02dc \u2223\u2223\u2223\u2223\u2223 1n n \u00ff\ni\u201c1 \u03c0ifpxiq \u00b4 E r\u03c0ifpxiqs\n\u2223\u2223\u2223\u2223\u2223 \u0105 t \u00b8 \u010f 2 exp \u02dc \u00b4nt2 2 \u2016a\u20162 \u03c32 \u00b8 .\nFurthermore, the zij are t0, 1u-valued. Thus the random variables Xpjqi :\u201c \u03c0izijfpxiq are also sub-Gaussian with parameter \u2016a\u20162 \u03c32. We use Hoeffding\u2019s inequality (Theorem 11.2) again, to obtain, for any j,\nP \u02dc \u2223\u2223\u2223\u2223\u2223 1n n \u00ff\ni\u201c1 \u03c0izijfpxiq \u00b4 E r\u03c0izijfpxiqs\n\u2223\u2223\u2223\u2223\u2223 \u0105 t \u00b8 \u010f 2 exp \u02dc \u00b4nt2 2 \u2016a\u20162 \u03c32 \u00b8 .\nBy the union bound,\nP \u00b4 \u2225\u2225\u2225p\u0393\u00b4 \u0393\u2225\u2225\u2225 \u0105 t\u00af \u010f 2pd` 1q exp\u02dc \u00b4nt2 2 \u2016a\u20162 \u03c32 \u00b8 .\nWe deduce the result since d \u011b 1."}, {"heading": "10 Proof of the main result", "text": "In this section, we state and prove our main result, Theorem 10.1. It is a more precise version than Theorem 3.1 in the main paper. Theorem 10.1 (Concentration of p\u03b2). Let \u03b7 P p0, 1q and \u03b5 \u0105 0. Take\nn \u011b max \u02dc 288 \u2016\u2207f\u20162 \u03c32d2A2d \u03b52C2d log 12d \u03b7 , 18d2A2d C2d log 24d 2 \u03b7 , 648d5A4dp3fp\u00b5\u0303q2 ` \u03c3\u03032 \u2016\u2207f\u2016 2q C2d\u03b5 2 log 24d2 \u03b7 \u00b8 .\nThen, under assumptions 2 and 1, \u2225\u2225\u2225p\u03b2 \u00b4 \u03a3\u00b41\u0393\u2225\u2225\u2225 \u010f \u03b5 , with probability greater than 1\u00b4 \u03b7.\nProof. The main idea of the proof is to notice that\u2225\u2225\u2225p\u03b2 \u00b4 \u03a3\u00b41\u0393\u2225\u2225\u2225 \u201c \u2225\u2225\u2225p\u03a3\u00b41p\u0393\u00b4 \u03a3\u00b41\u0393\u2225\u2225\u2225 \u010f\n\u2225\u2225\u2225p\u03a3\u00b41pp\u0393\u00b4 \u0393q\u2225\u2225\u2225` \u2225\u2225\u2225pp\u03a3\u00b41 \u00b4 \u03a3\u00b41q\u0393\u2225\u2225\u2225 , and then to control these two terms using the results of Section 8 and 9. Control of \u2225\u2225\u2225p\u03a3\u00b41pp\u0393\u00b4 \u0393q\u2225\u2225\u2225. We use the upper bound \u2225\u2225\u2225p\u03a3\u00b41pp\u0393\u00b4 \u0393q\u2225\u2225\u2225 \u010f \u2225\u2225\u2225p\u03a3\u00b41\u2225\u2225\u2225 op \u00a8 \u2225\u2225\u2225p\u0393\u00b4 \u0393\u2225\u2225\u2225. We then achieve control of the operator norm of the empirical covariance matrix in probability with Lemma 8.5, and control of the norm of p\u0393\u00b4 \u0393 in probability with Lemma 9.3. Set\nt1 :\u201c Cd\n6dAd and n1 :\u201c\n18d2 C2d log 12d 2 \u03b7 .\nAccording to Lemma 8.5, for any n \u011b n1, there is an event \u2126n1 which has probability greater than 1\u00b44d2 expp\u00b42nt21q such that \u2225\u2225\u2225p\u03a3\u00b41\u2225\u2225\u2225\nop \u010f 6dAd\nCd\non this event. It is easy to check that 4d2 expp\u00b42n1t21q \u201c \u03b7{3, thus \u2126n1 has probability greater than 1\u00b4 \u03b7{3. Now set\nt2 :\u201c \u03b5Cd\n12dAd and n2 :\u201c 288 \u2016a\u20162 \u03c32d2A2d \u03b52C2d log 12d \u03b7 .\nAccording to Lemma 9.3, for any n \u011b n2, there exists an event \u2126n2 which has probability greater than 1 \u00b4 4d exp \u00b4\n\u00b4nt22 2\u2016a\u20162\u03c32 \u00af such that \u2225\u2225\u2225p\u0393\u00b4 \u0393\u2225\u2225\u2225 \u010f t2 on that event. One can check that\n4d exp \u02dc \u00b4n2t22 2 \u2016a\u20162 \u03c32 \u00b8 \u201c \u03b73 ,\nthus \u2126n2 has probability greater than 1\u00b4 \u03b7{3. On the event \u2126n1 X \u2126n2 , we have\u2225\u2225\u2225p\u03a3\u00b41pp\u0393\u00b4 \u0393q\u2225\u2225\u2225 \u010f \u2225\u2225\u2225p\u03a3\u00b41\u2225\u2225\u2225 op \u00a8 \u2225\u2225\u2225p\u0393\u00b4 \u0393\u2225\u2225\u2225 \u010f 6dAd Cd \u00a8 t2 \u010f \u03b5 2 ,\nby definition of t2.\nControl of \u2225\u2225\u2225pp\u03a3\u00b41 \u00b4 \u03a3\u00b41q\u0393\u2225\u2225\u2225. We use the upper bound \u2225\u2225\u2225pp\u03a3\u00b41 \u00b4 \u03a3\u00b41q\u0393\u2225\u2225\u2225 \u010f \u2225\u2225\u2225p\u03a3\u00b41 \u00b4 \u03a3\u00b41\u2225\u2225\u2225\nop \u00a8 \u2016\u0393\u2016. We then achieve control of \u2225\u2225\u2225p\u03a3\u00b41 \u00b4 \u03a3\u00b41\u2225\u2225\u2225\nop in probability with Proposition 8.1, whereas we can bound the norm of \u0393\nalmost surely with Lemma 9.2. If \u2016\u0393\u2016 \u201c 0, then there is nothing to prove. Otherwise, set\nt3 :\u201c min \u02c6 \u03b5 2 \u2016\u0393\u2016 , 3dAd Cd \u02d9 , n3 :\u201c 18d2A2d C2d log 24d 2 \u03b7 , and n4 :\u201c 648d5A4dp3fp\u00b5\u0303q2 ` \u03c3\u03032 \u2016a\u2016 2q C2d\u03b5 2 log 24d2 \u03b7 .\nAccording to Proposition 8.1, for any n \u011b maxpn3, n4q, there is an event \u2126n3 which has probability greater than 1\u00b4 8d2 exp \u00b4 \u00b4C3dnt 2 3\n162d2A4 d\n\u00af\nsuch that \u2225\u2225\u2225p\u03a3\u00b41 \u00b4 \u03a3\u00b41\u2225\u2225\u2225 op \u010f t3\non this event. With the help of Lemma 9.2, one can check that\nmax \u02c6 8d2 exp \u02c6 \u00b4C3dn3t23 162d2A4d \u02d9 , 8d2 exp \u02c6 \u00b4C3dn4t23 162d2A4d \u02d9\u02d9 \u010f \u03b73 .\nTherefore, \u2126n3 has probability greater than \u03b7{3 and, on this event,\u2225\u2225\u2225pp\u03a3\u00b41 \u00b4 \u03a3\u00b41q\u0393\u2225\u2225\u2225 \u010f \u2225\u2225\u2225p\u03a3\u00b41 \u00b4 \u03a3\u00b41\u2225\u2225\u2225 op \u00a8 \u2016\u0393\u2016 \u010f t3 \u00a8 \u2016\u0393\u2016 \u010f \u03b5 2 .\nConclusion. Set n \u011b maxpni, i \u201c 1 . . . 4q. Define \u2126n :\u201c \u2126n1 X \u2126n2 X \u2126n3 , where the \u2126ni are defined as before. According to the previous reasoning, on the event \u2126n,\u2225\u2225\u2225p\u03b2 \u00b4 \u03a3\u00b41\u0393\u2225\u2225\u2225 \u201c \u2225\u2225\u2225p\u03a3\u00b41p\u0393\u00b4 \u03a3\u00b41\u0393\u2225\u2225\u2225\n\u010f \u2225\u2225\u2225p\u03a3\u00b41pp\u0393\u00b4 \u0393q\u2225\u2225\u2225` \u2225\u2225\u2225pp\u03a3\u00b41 \u00b4 \u03a3\u00b41q\u0393\u2225\u2225\u2225\n\u010f \u03b52 ` \u03b5 2 \u201c \u03b5 .\nMoreover, the union bound gives P p\u2126nq \u011b 1 \u00b4 \u03b7. We conclude by noticing that n1 is always smaller than n3, thus we just have to require n \u011b maxpn2, n3, n4q, as in the statement of our result."}, {"heading": "11 Technical lemmas", "text": ""}, {"heading": "11.1 Gaussian integrals", "text": "In this section, we collect some Gaussian integral computations that are needed in our derivations. We provide succinct proof, since essentially any modern computer algebra system will provide these formulas. Our first result is for zero-th order Gaussian integral. Lemma 11.1 (Gaussian integral, 0-th order). Let \u03be, \u00b5 be real numbers, and \u03bd, \u03c3 be positive real numbers. Then, it holds that\n\u017c exp \u02c6 \u00b4px\u00b4 \u03beq2 2\u03bd2 ` \u00b4px\u00b4 \u00b5q2 2\u03c32 \u02d9 dx \u03c3 ? 2\u03c0 \u201c \u03bd? \u03bd2 ` \u03c32 \u00a8 exp \u02c6 \u00b4p\u03be \u00b4 \u00b5q2 2p\u03bd2 ` \u03c32q \u02d9 \u00a8 12erf \u02dc \u03bd2px\u00b4 \u00b5q ` \u03c32px\u00b4 \u03beq \u03bd\u03c3 a 2p\u03bd2 ` \u03c32q \u00b8 .\nIn particular, \u017c `8\n\u00b48 exp\n\u02c6 \u00b4px\u00b4 \u03beq2 2\u03bd2 ` \u00b4px\u00b4 \u00b5q2 2\u03c32 \u02d9 dx \u03c3 ? 2\u03c0 \u201c \u03bd? \u03bd2 ` \u03c32 \u00a8 exp \u02c6 \u00b4p\u03be \u00b4 \u00b5q2 2p\u03bd2 ` \u03c32q \u02d9 .\nProof. For any reals a, b, and c, it holds that \u017c\ne\u00b4ax 2`bx`c dx \u201c\nc\n\u03c0 a \u00a8 e b 2 4a`c \u00a8 12erf\n\u02c6\n2ax\u00b4 b 2 ? a\n\u02d9\n.\nWe apply this formula with a \u201c 12\u03bd2 ` 1 2\u03c32 , b \u201c \u03be \u03bd2 ` \u00b5 \u03c32 , and c \u201c \u00b4\n\u00b4\n\u03be2 2\u03bd2 ` \u00b52 \u03c32\n\u00af\n. We then notice that\nb2{p4aq ` c \u201c \u00b4p\u03be\u00b4\u00b5q 2\n2p\u03bd2`\u03c32q and 2ax\u00b4 b\n2 ? a \u201c \u03bd 2px\u00b4 \u00b5q ` \u03c32px\u00b4 \u03beq \u03bd\u03c3 a 2p\u03bd2 ` \u03c32q .\nRemark 11.1. We often replace \u03bd 2px\u00b4\u00b5q`\u03c32px\u00b4\u03beq \u03bd\u03c3 ? 2p\u03bd2`\u03c32q by the more readable px\u00b4 \u00b5\u0303q{p\u03c3\u0303\n? 2q in the main text of the\npaper.\nSince f is assumed to be linear in most of the paper, we need first order computations as well: Lemma 11.2 (Gaussian integral, 1st order). Let \u03be, \u00b5 be real numbers, and \u03bd, \u03c3 be positive numbers. Then it holds that\n\u017c x \u00a8 exp \u02c6 \u00b4px\u00b4 \u03beq2 2\u03bd2 ` \u00b4px\u00b4 \u00b5q2 2\u03c32 \u02d9 dx \u03c3 ? 2\u03c0 \u201c \u03bd? \u03bd2 ` \u03c32 \u00a8 exp \u02c6 \u00b4p\u03be \u00b4 \u00b5q2 2p\u03bd2 ` \u03c32q \u02d9\n\u00a8 \u00bb\n\u2013 \u03c32\u03be ` \u03bd2\u00b5 \u03bd2 ` \u03c32 \u00a8 1 2erf\n\u02dc\n\u03bd2px\u00b4 \u00b5q ` \u03c32px\u00b4 \u03beq \u03bd\u03c3 a 2p\u03bd2 ` \u03c32q\n\u00b8\n\u00b4 \u03bd\u03c3? 2\u03c0 ? \u03bd2 ` \u03c32 \u00a8 exp\n\u00a8 \u02dd\u00b4 \u02dc\n\u03bd2px\u00b4 \u00b5q ` \u03c32px\u00b4 \u03beq \u03bd\u03c3 a 2p\u03bd2 ` \u03c32q\n\u00b82 \u02db\n\u201a\nfi\nfl .\nIn particular, \u017c `8\n\u00b48 x \u00a8 exp\n\u02c6 \u00b4px\u00b4 \u03beq2 2\u03bd2 ` \u00b4px\u00b4 \u00b5q2 2\u03c32 \u02d9 dx \u03c3 ? 2\u03c0 \u201c \u03c3 2\u03be ` \u03bd2\u00b5 \u03bd2 ` \u03c32 \u00a8 \u03bd? \u03bd2 ` \u03c32 \u00a8 exp \u02c6 \u00b4p\u03be \u00b4 \u00b5q2 2p\u03bd2 ` \u03c32q \u02d9 .\nProof. For any a, b, c with a \u0105 0, it holds that \u017c\nx \u00a8 e\u00b4ax 2`bx`c dx \u201c\n? \u03c0b 4a3{2 eb 2{p4aq`cerf \u02c6 2ax\u00b4 b 2 ? a \u02d9 \u00b4 12ae \u00b4ax2`bx`c .\nFinally we want to mention the following result. Lemma 11.3 (Gaussian integral, 2nd order). Let \u03be, \u00b5 be real numbers, and \u03bd, \u03c3 be positive real numbers. Then, it holds that \u017c `8\n\u00b48 x2 \u00a8exp\n\u02c6 \u00b4px\u00b4 \u03beq2 2\u03bd2 ` \u00b4px\u00b4 \u00b5q2 2\u03c32 \u02d9 dx \u03c3 ? 2\u03c0 \u201c p\u03c3 2\u03be ` \u03bd2\u00b5q2 ` \u03bd2\u03c32p\u03bd2 ` \u03c32q p\u03bd2 ` \u03c32q2 \u00a8 \u03bd? \u03bd2 ` \u03c32 \u00a8exp \u02c6 \u00b4p\u03be \u00b4 \u00b5q2 2p\u03bd2 ` \u03c32q \u02d9 .\nRemark 11.2. As a consequence of Lemma 11.3, it would be possible to further our analysis by adding second degree terms to f . Indeed, quantities depending on \u2016xi \u00b4 \u03be\u2016, which would have to be computed to extend the proofs of Lemmas 9.1 and 9.3, can be computed with this lemma. For instance, one can show that\nE \u201d \u03c0i \u2016xi \u00b4 \u03be\u20162 \u0131 \u201c Cd \u00a8 \u201e\n\u03bd4\np\u03bd2 ` \u03c32q2 \u2016\u03be \u00b4 \u00b5\u2016 2 ` \u03bd\n2\u03c32d\n\u03bd2 ` \u03c32\n\n.\nProof. We use the fact that \u017c\nx2 \u00a8 e\u00b4ax 2`bx`c dx \u201c\n? \u03c0p2a` b2q\n8a5{2 e b\n2 4a`c \u00a8 erf\n\u02c6\n2ax\u00b4 b 2 ? a\n\u02d9\n\u00b4 ax` b4a2 \u00a8 e \u00b4ax2`bx`c ."}, {"heading": "11.2 Concentration results", "text": "In this section we collect some concentration results used throughout our proofs. Note that we use rather use the two-sided version of these results. Theorem 11.1 (Hoeffding\u2019s inequality). Let X1, . . . , Xn be independent random variables such that Xi takes its values in rai, bis almost surely for all i \u010f n. Then for every t \u0105 0,\nP\n\u02dc\n1 n\nn \u00ff i\u201c1 pXi \u00b4 E rXisq \u011b t\n\u00b8\n\u010f exp \u02c6 \u00b42t2n2 \u0159n i\u201c1pbi \u00b4 aiq2 \u02d9 .\nProof. This is Theorem 2.8 in Boucheron et al. (2013) in our notation.\nTheorem 11.2 (Hoeffding\u2019s inequality for sub-Gaussian random variables). Let X1, . . . , Xn be independent random variables such that Xi is sub-Gaussian with parameter s2 \u0105 0. Then, for every t \u0105 0,\nP\n\u02dc\n1 n\nn \u00ff i\u201c1 Xi \u00b4 E rXis \u0105 t\n\u00b8\n\u010f exp \u02c6 \u00b4nt2\n2s2\n\u02d9\n.\nProof. This is Proposition 2.1 in Wainwright (2019)."}], "title": "Explaining the Explainer: A First Theoretical Analysis of LIME", "year": 2020}