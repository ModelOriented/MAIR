{"abstractText": "The success of convolutional neural networks (CNNs) in various applications is accompanied by a significant increase in computation and parameter storage costs. Recent efforts to reduce these overheads involve pruning and compressing the weights of various layers while at the same time aiming to not sacrifice performance. In this paper, we propose a novel criterion for CNN pruning inspired by neural network interpretability: The most relevant elements, i.e. weights or filters, are automatically found using their relevance scores obtained from concepts of explainable AI (XAI). By exploring this idea, we connect the lines of interpretability and model compression research. We show that our proposed method can efficiently prune CNN models in transfer-learning setups in which networks pre-trained on large corpora are adapted to specialized tasks. The method is evaluated on a broad range of computer vision datasets. Notably, our novel criterion is not only competitive or better compared to state-ofthe-art pruning criteria when successive retraining is performed, but clearly outperforms these previous criteria in the resource-constrained application scenario in which the data of the task to be transferred to is very scarce and one chooses to refrain from fine-tuning. Our method is able to compress the model iteratively while maintaining or even improving accuracy. At the same time, it has a computational cost in the order of gradient computation and is comparatively simple to apply without the need for tuning hyperparameters for pruning.", "authors": [{"affiliations": [], "name": "Seul-Ki Yeoma"}, {"affiliations": [], "name": "Philipp Seegerera"}, {"affiliations": [], "name": "Sebastian Lapuschkinb"}, {"affiliations": [], "name": "Alexander Binderc"}, {"affiliations": [], "name": "Simon Wiedemannb"}, {"affiliations": [], "name": "Klaus-Robert M\u00fcllera"}, {"affiliations": [], "name": "Wojciech Samekb"}], "id": "SP:e0dc5e3a1c0e227a4eb15afcd589f6cc818f1655", "references": [{"authors": ["J. Gu", "Z. Wang", "J. Kuen", "L. Ma", "A. Shahroudy", "B. Shuai", "T. Liu", "X. Wang", "G. Wang", "J. Cai", "T. Chen"], "title": "Recent advances in convolutional neural networks, Pattern Recognition", "year": 2018}, {"authors": ["M. Denil", "B. Shakibi", "L. Dinh", "M. Ranzato", "N. de Freitas"], "title": "Predicting parameters in deep learning", "venue": "in: Advances in Neural Information Processing Systems (NIPS),", "year": 2013}, {"authors": ["V. Sze", "Y. Chen", "T. Yang", "J.S. Emer"], "title": "Efficient processing of deep neural networks: A tutorial and survey", "venue": "Proceedings of the IEEE", "year": 2017}, {"authors": ["Y. LeCun", "J.S. Denker", "S.A. Solla"], "title": "Optimal brain damage", "venue": "in: Advances in Neural Information Processing Systems (NIPS),", "year": 1989}, {"authors": ["Y. Tu", "Y. Lin"], "title": "Deep neural network compression technique towards efficient digital signal modulation recognition in edge device", "venue": "IEEE Access", "year": 2019}, {"authors": ["Y. Cheng", "D. Wang", "P. Zhou", "T. Zhang"], "title": "Model compression and acceleration for deep neural networks: The principles, progress, and challenges", "venue": "IEEE Signal Processing Magazine", "year": 2018}, {"authors": ["S. Bach", "A. Binder", "G. Montavon", "F. Klauschen", "K.-R. M\u00fcller", "W. Samek"], "title": "On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation", "venue": "PLoS ONE", "year": 2015}, {"authors": ["M. H\u00e4gele", "P. Seegerer", "S. Lapuschkin", "M. Bockmayr", "W. Samek", "F. Klauschen", "K.-R. M\u00fcller", "A. Binder"], "title": "Resolving challenges in deep learning-based analyses of histopathological images using explanation methods", "venue": "Scientific Reports", "year": 2020}, {"authors": ["P. Seegerer", "A. Binder", "R. Saitenmacher", "M. Bockmayr", "M. Alber", "P. Jurmeister", "F. Klauschen", "K.-R. M\u00fcller"], "title": "Interpretable deep neural network to predict estrogen receptor status from haematoxylin-eosin images", "year": 2020}, {"authors": ["G. Montavon", "W. Samek", "K.-R. M\u00fcller"], "title": "Methods for interpreting and understanding deep neural networks", "venue": "Digital Signal Processing", "year": 2018}, {"authors": ["M. Alber", "S. Lapuschkin", "P. Seegerer", "M. H\u00e4gele", "K.T. Sch\u00fctt", "G. Montavon", "W. Samek", "K.-R. M\u00fcller", "S. D\u00e4hne", "P.-J"], "title": "Kindermans, iNNvestigate neural networks", "venue": "Journal of Machine Learning Research", "year": 2019}, {"authors": ["S. Wiedemann", "K.-R. M\u00fcller", "W. Samek"], "title": "Compact and computationally efficient representation of deep neural networks", "venue": "IEEE Transactions on Neural Networks and Learning Systems", "year": 2020}, {"authors": ["F. Tung", "G. Mori"], "title": "Deep neural network compression by in-parallel pruning-quantization", "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": 2020}, {"authors": ["K. Guo", "X. Xie", "X. Xu", "X. Xing"], "title": "Compressing by learning in a low-rank and sparse decomposition form", "venue": "IEEE Access", "year": 2019}, {"authors": ["T. Xu", "P. Yang", "X. Zhang", "C. Liu"], "title": "LightweightNet: Toward fast and lightweight convolutional neural networks via architecture distillation", "venue": "Pattern Recognition", "year": 2019}, {"authors": ["X. Zhang", "X. Zhou", "M. Lin", "J. Sun"], "title": "Shufflenet: An extremely efficient convolutional neural network for mobile devices", "venue": "in: IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "year": 2018}, {"authors": ["P. Molchanov", "A. Mallya", "S. Tyree", "I. Frosio", "J. Kautz"], "title": "Importance estimation for neural network pruning", "venue": "in: IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "year": 2019}, {"authors": ["B. Hassibi", "D.G. Stork"], "title": "Second order derivatives for network pruning: Optimal brain", "venue": "surgeon, in: Advances in Neural Information Processing Systems (NIPS),", "year": 1992}, {"authors": ["P. Molchanov", "S. Tyree", "T. Karras", "T. Aila", "J. Kautz"], "title": "Pruning convolutional neural networks for resource efficient transfer learning", "venue": "in: Proceedings of the International Conference on Learning Representations (ICLR),", "year": 2017}, {"authors": ["C. Yu", "J. Wang", "Y. Chen", "X. Qin"], "title": "Transfer channel pruning for compressing deep domain adaptation models, International", "venue": "Journal of Machine Learning and Cybernetics", "year": 2019}, {"authors": ["C. Liu", "H. Wu"], "title": "Channel pruning based on mean gradient for accelerating convolutional neural networks, Signal Processing", "year": 2019}, {"authors": ["X. Sun", "X. Ren", "S. Ma", "H. Wang"], "title": "meprop: Sparsified back propagation for accelerated deep learning with reduced overfitting", "venue": "in: International Conference on Machine Learning (ICML),", "year": 2017}, {"authors": ["S. Han", "J. Pool", "J. Tran", "W.J. Dally"], "title": "Learning both weights and connections for efficient neural network, in: Advances in Neural Information", "venue": "Processing Systems (NIPS),", "year": 2015}, {"authors": ["S. Han", "X. Liu", "H. Mao", "J. Pu", "A. Pedram", "M.A. Horowitz", "W.J. Dally"], "title": "EIE: efficient inference engine on compressed deep neural network", "venue": "in: International Symposium on Computer Architecture (ISCA),", "year": 2016}, {"authors": ["W. Wen", "C. Wu", "Y. Wang", "Y. Chen", "H. Li"], "title": "Learning structured sparsity in deep neural networks, in: Advances in Neural Information", "venue": "Processing Systems (NIPS),", "year": 2016}, {"authors": ["H. Li", "A. Kadav", "I. Durdanovic", "H. Samet", "H.P. Graf"], "title": "Pruning filters for efficient convnets", "venue": "in: International Conference on Learning Representations,", "year": 2017}, {"authors": ["R. Yu", "A. Li", "C. Chen", "J. Lai", "V.I. Morariu", "X. Han", "M. Gao", "C. Lin", "L.S. Davis"], "title": "NISP: pruning networks using neuron importance score propagation", "venue": "in: IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "year": 2018}, {"authors": ["J.-H. Luo", "H. Zhang", "H.-Y. Zhou", "C.-W. Xie", "J. Wu", "W. Lin"], "title": "ThiNet: Pruning CNN filters for a thinner net", "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": 2019}, {"authors": ["J. Gan", "W. Wang", "K. Lu"], "title": "Compressing the CNN architecture for in-air handwritten Chinese character recognition", "venue": "Pattern Recognition Letters", "year": 2020}, {"authors": ["X. Dai", "H. Yin", "N.K. Jha"], "title": "Nest: A neural network synthesis tool based on a grow-andprune paradigm", "venue": "IEEE Transactions on Computers", "year": 2019}, {"authors": ["W. Samek", "G. Montavon", "A. Vedaldi", "L.K. Hansen", "K.-R"], "title": "M\u00fcller (Eds.), Explainable AI: Interpreting, Explaining and Visualizing Deep Learning, volume 11700 of Lecture Notes in Computer", "year": 2019}, {"authors": ["W. Samek", "A. Binder", "G. Montavon", "S. Lapuschkin", "K.-R. M\u00fcller"], "title": "Evaluating the visualization of what a deep neural network has learned", "venue": "IEEE Transactions on Neural Networks and Learning Systems", "year": 2017}, {"authors": ["S. Lazebnik", "C. Schmid", "J. Ponce"], "title": "Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories", "venue": "in: IEEE Conference on Computer Vision and Pattern Recognition (CVPR),", "year": 2006}, {"authors": ["L. Li", "F. Li"], "title": "What, where and who? Classifying events by scene and object recognition", "venue": "in: IEEE International Conference on Computer Vision (ICCV),", "year": 2007}, {"authors": ["J. Elson", "J.R. Douceur", "J. Howell", "J. Saul"], "title": "Asirra: a CAPTCHA that exploits interestaligned manual image categorization", "venue": "in: Proceedings of the 2007 ACM Conference on Computer and Communications Security (CCS),", "year": 2007}, {"authors": ["M. Nilsback", "A. Zisserman"], "title": "Automated flower classification over a large number of classes", "venue": "in: Sixth Indian Conference on Computer Vision, Graphics & Image Processing (ICVGIP),", "year": 2008}, {"authors": ["O. Russakovsky", "J. Deng", "H. Su", "J. Krause", "S. Satheesh", "S. Ma", "Z. Huang", "A. Karpathy", "A. Khosla", "M.S. Bernstein", "A.C. Berg", "F. Li"], "title": "Imagenet large scale visual recognition", "venue": "challenge, International Journal of Computer Vision", "year": 2015}, {"authors": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "title": "Deep residual learning for image recognition, in: 2016", "venue": "IEEE Conference on Computer Vision and Pattern Recognition,", "year": 2016}, {"authors": ["H. Wang", "Q. Zhang", "Y. Wang", "H. Hu"], "title": "Structured probabilistic pruning for convolutional neural network acceleration", "venue": "in: British Machine Vision Conference (BMVC),", "year": 2018}, {"authors": ["M. Guillemot", "C. Heusele", "R. Korichi", "S. Schnebert", "L. Chen"], "title": "Breaking batch normalization for better explainability of deep neural networks through layer-wise relevance propagation, CoRR abs/2002.11018 (2020)", "year": 2020}, {"authors": ["J. Liu", "Y. Wang", "Y. Qiao"], "title": "Sparse deep transfer learning for convolutional neural network", "venue": "in: AAAI Conference on Artificial Intelligence,", "year": 2017}], "sections": [{"text": "The success of convolutional neural networks (CNNs) in various applications is accompanied by a significant increase in computation and parameter storage costs. Recent efforts to reduce these overheads involve pruning and compressing the weights of various layers while at the same time aiming to not sacrifice performance. In this paper, we propose a novel criterion for CNN pruning inspired by neural network interpretability: The most relevant elements, i.e. weights or filters, are automatically found using their relevance scores obtained from concepts of explainable AI (XAI). By exploring this idea, we connect the lines of interpretability and model compression research. We show that our proposed method can efficiently prune CNN models in transfer-learning setups in which networks pre-trained on large corpora are adapted to specialized tasks. The method is evaluated on a broad range of computer vision datasets. Notably, our novel criterion is not only competitive or better compared to state-ofthe-art pruning criteria when successive retraining is performed, but clearly outperforms these previous criteria in the resource-constrained application scenario in which the data of the task to be transferred to is very scarce and one chooses to refrain from fine-tuning. Our method is able to compress the model iteratively while maintaining or even improving accuracy. At the same time, it has a computational cost in the order of gradient computation and is comparatively simple to apply without the need for tuning hyperparameters for pruning.\nKeywords: Pruning, Layer-wise Relevance Propagation (LRP), Convolutional Neural Network (CNN), Interpretation of Models, Explainable AI (XAI)\n\u2217Corresponding Authors Email addresses: yeom@tu-berlin.de (Seul-Ki Yeom), philipp.seegerer@tu-berlin.de (Philipp Seegerer), sebastian.lapuschkin@hhi.fraunhofer.de (Sebastian Lapuschkin), alexander_binder@sutd.edu.sg (Alexander Binder), simon.wiedemann@hhi.fraunhofer.de (Simon Wiedemann), klaus-robert.mueller@tu-berlin.de (Klaus-Robert M\u00fcller), wojciech.samek@hhi.fraunhofer.de (Wojciech Samek)\nPreprint submitted to arXiv.org (as update to 1912.08881v1) September 23, 2020\nar X\niv :1\n91 2.\n08 88\n1v 2\n[ cs\n.L G\n] 2"}, {"heading": "1. Introduction", "text": "Deep CNNs have become an indispensable tool for a wide range of applications [1], such as image classification, speech recognition, natural language processing, chemistry, neuroscience, medicine and even are applied for playing games such as Go, poker or Super Smash Bros. They have achieved high predictive performance, at times even outperforming humans. Furthermore, in specialized domains where limited training data is available, e.g., due to the cost and difficulty of data generation (medical imaging from fMRI, EEG, PET etc.), transfer learning can improve the CNN performance by extracting the knowledge from the source tasks and applying it to a target task which has limited training data.\nHowever, the high predictive performance of CNNs often comes at the expense of high storage and computational costs, which are related to the energy expenditure of the finetuned network. These deep architectures are composed of millions of parameters to be trained, leading to overparameterization (i.e. having more parameters than training samples) of the model [2]. The run-times are typically dominated by the evaluation of convolutional layers, while dense layers are cheap but memory-heavy [3]. For instance, the VGG-16 model has approximately 138 million parameters, taking up more than 500MB in storage space, and needs 15.5 billion floating-point operations (FLOPs) to classify a single image. ResNet50 has approx. 23 thousand parameters and needs 4.1 billion FLOPs. Note that overparametrization is helpful for an efficient and successful training of neural networks, however, once the trained and well generalizing network structure is established, pruning can help to reduce redundancy while still maintaining good performance [4].\nReducing a model\u2019s storage requirements and computational cost becomes critical for a broader applicability, e.g., in embedded systems, autonomous agents, mobile devices, or edge devices [5]. Neural network pruning has a decades long history with interest from both academia and industry [6] aiming to eliminate the subset of the network elements (i.e. weights or filters) which is the least important w.r.t. the network\u2019s intended task. For network pruning, it is crucial to decide how to identify the \u201cirrelevant\u201d subset of the parameters meant for deletion. To address this issue, previous researches have proposed specific criteria based on Taylor expansion, weight, gradient, and others, to reduce complexity and computation costs in the network . Related works are introduced in Section 2.\nFrom a practical point of view, the full capacity (in terms of weights and filters) of an overparameterized model may not be required, e.g., when (1) parts of the model lie dormant after training (i.e., are permanently \"switched off\"), (2) a user is not interested in the model\u2019s full array of possible outputs, which is a common scenario in transfer learning (e.g. the user only has use for 2 out of 10 available network outputs), or (3) a user lacks data and resources for fine-tuning and running the overparameterized model.\nIn these scenarios the redundant parts of the model will still occupy space in memory, and information will be propagated through those parts, consuming energy and increasing runtime. Thus, criteria able to stably and significantly reduce the computational complexity of deep neural networks across applications are relevant for practitioners.\nIn this paper, we propose a novel pruning framework based on Layer-wise Relevance Propagation (LRP) [7]. LRP was originally developed as an explanation method to assign importance scores, so called relevance, to the different input dimensions of a neural network that reflect the contribution of an input dimension to the models decision, and has been\n2\napplied to different fields of computer vision (e.g., [8, 9, 10]). The relevance is backpropagated from the output to the input and hereby assigned to each element of the deep model. Since relevance scores are computed for every layer and neuron from the model output to the input, these relevance scores essentially reflect the importance of every single element of a model and its contribution to the information flow through the network \u2014 a natural candidate to be used as pruning criterion. The LRP criterion can be motivated theoretically through the concept of Deep Taylor Decomposition (DTD) (c.f. [11, 12]). Moreover, LRP is scalable and easy to apply, and has been implemented in software frameworks such as iNNvestigate [13]. Furthermore, it has linear computational cost in terms of network inference cost, similar to backpropagation.\nWe systematically evaluate the compression efficacy of the LRP criterion compared to common pruning criteria for two different scenarios.\nScenario 1: We prune pre-trained CNNs followed by subsequent fine-tuning. This is the usual setting in CNN pruning and requires a sufficient amount of data and computational power. Scenario 2: In this scenario a pretrained model needs to be transferred to a related problem as well, but the data available for the new task is too scarce for a proper fine-tuning and/or the time consumption, computational power or energy consumption is constrained. Such transfer learning with restrictions is common in mobile or embedded applications. Our experimental results on various benchmark datasets and four different popular CNN architectures show that the LRP criterion for pruning is more scalable and efficient, and leads to better performance than existing criteria regardless of data types and model architectures if retraining is performed (Scenario 1). Especially, if retraining is prohibited due to external constraints after pruning, the LRP criterion clearly outperforms previous criteria on all datasets (Scenario 2). Finally, we would like to note that our proposed pruning framework is not limited to LRP and image data, but can be also used with other explanation techniques and data types.\nThe rest of this paper is organized as follows: Section 2 summarizes related works for network compression and introduces the typical criteria for network pruning. Section 3 describes the framework and details of our approach. The experimental results are illustrated and discussed in Section 4, while our approach is discussed in relation to previous studies in Section 5. Section 6 gives conclusions and an outlook to future work."}, {"heading": "2. Related Work", "text": "We start the discussion of related research in the field of network compression with network quantization methods which have been proposed for storage space compression by decreasing the number of possible and unique values for the parameters [14, 15]. Tensor decomposition approaches decompose network matrices into several smaller ones to estimate the informative parameters of the deep CNNs with low-rank approximation/factorization [16].\nMore recently, [17] also propose a framework of architecture distillation based on layer-wise replacement, called LightweightNet for memory and time saving. Algorithms for designing efficient models focus more on acceleration instead of compression by optimizing convolution operations or architectures directly (e.g. [18]).\n3\nNetwork pruning approaches remove redundant or irrelevant elements \u2014 i.e. nodes, filters, or layers \u2014 from the model which are not critical for performance [6, 19]. Network pruning is robust to various settings and gives reasonable compression rates while not (or minimally) hurting the model accuracy. Also it can support both training from scratch and transfer learning from pre-trained models. Early works have shown that network pruning is effective in reducing network complexity and simultaneously addressing over-fitting problems. Current network pruning techniques make weights or channels sparse by removing non-informative connections and require an appropriate criterion for identifying which elements of the model are not relevant for solving a problem. Thus, it is crucial to decide how to quantify the relevance of the parameters (i.e. weights or channels) in the current state of the learning process for deletion without sacrificing predictive performance. In previous studies, pruning criteria have been proposed based on the magnitude of their 1) weights, 2) gradients, 3) Taylor expansion/derivative, and 4) other criteria, as described in the following section.\nTaylor expansion: Early approaches towards neural network pruning \u2014 optimal brain damage [4] and optimal brain surgeon [20] \u2014 leveraged a second-order Taylor expansion based on the Hessian matrix of the loss function to select parameters for deletion. However, computing the inverse of Hessian is computationally expensive. The work of [21, 22] used a first-order Taylor expansion as a criterion to approximate the change of loss in the objective function as an effect of pruning away network elements. We contrast our novel criterion to the computationally more comparable first-order Taylor expansion from [21].\nGradient: Liu and Wu [23] proposed a hierarchical global pruning strategy by calculating the mean gradient of feature maps in each layer. They adopt a hierarchical global pruning strategy between the layers with similar sensitivity. Sun et al. [24] proposes a sparsified back-propagation approach for neural network training using the magnitude of the gradient to find essential and non-essential features in Multi-Layer Perceptron (MLP) and Long Short-Term Memory Network (LSTM) models, which can be used for pruning. We implement the gradient-based pruning criterion after [24].\nWeight: A recent trend is to prune redundant, non-informative weights in pre-trained CNN models, based on the magnitude of the weights themselves. Han et al. [25] and Han et al. [26] proposed the pruning of weights for which the magnitude is below a certain threshold, and to subsequently fine-tune with a lp-norm regularization. This pruning strategy has been used on fully-connected layers and introduced sparse connections with BLAS libraries, supporting specialized hardware to achieve its acceleration. In the same context, Structured Sparsity Learning (SSL) added group sparsity regularization to penalize unimportant parameters by removing some weights [27]. Li et al. [28], against which we compare in our experiments, proposed a one-shot channel pruning method using the lp norm of weights for filter selection, provided that those channels with smaller weights always produce weaker activations.\nOther criteria: [29] proposed the Neuron Importance Score Propagation (NISP) algorithm to propagate the importance scores of final responses before the softmax, classification layer in the network. The method is based on \u2014 in contrast to our proposed metric \u2014 a per-layer pruning process which does not consider global importance in the network. Luo et al. [30] proposed ThiNet, a data-driven statistical channel pruning technique based on the statistics computed from the next layer. Further hybrid approaches can be found in, e.g. [31], which suggests a fusion approach to combine with weight-based channel pruning and network quantization. More recently, Dai et al. [32] proposed an evolutionary paradigm\n4\nfor weight-based pruning and gradient-based growing to reduce the network heuristically."}, {"heading": "3. LRP-Based Network Pruning", "text": "A feedforward CNN consists of neurons established in a sequence of multiple layers of neurons, where each neuron receives the input data from one or more previous layers and propagates its output to every neuron in the succeeding layers, using a potentially nonlinear mapping. Network pruning aims to sparsify these elements by eliminating weights or filters that are non-informative (according to a certain criterion). We specifically focus our experiments on transfer learning, where the parameters of a network pre-trained on a source domain is subsequently fine-tuned on a target domain, i.e., the final data or prediction task. Here, the general pruning procedure is outlined in Algorithm 1.\nAlgorithm 1 Neural Network Pruning 1: Input: pre-trained model net, reference data xr, training data xt 2: pruning threshold t, pruning criterion c, pruning ratio r 3: while t not reached do 4: // Step 1: assess network substructure importance 5: for all layer in net do 6: for all element in layer do 7: B compute importance of element w.r.t. c (and xr) 8: end for 9: if required for c then 10: B globally regularize importance per element 11: end if 12: end for 13: // Step 2: identify and remove least important elements in groups of r 14: B remove r amount of element from net where importance is minimal 15: B remove orphaned connections of each removed element 16: if desired then 17: // Step 2.1: optional fine-tuning to recover performance 18: B fine-tune net on xt 19: end if 20: end while 21: // return the pruned network upon hitting threshold t (e.g. model performance or size) 22: return net\nEven though most approaches use an identical process, choosing a suitable pruning criterion to quantify the importance of model parameters for deletion while minimizing performance drop (Step 1) is of critical importance, governing the success of the approach."}, {"heading": "3.1. Layer-wise Relevance Propagation", "text": "In this paper, we propose a novel criterion for pruning neural network elements: the relevance quantity computed with LRP [7]. LRP decomposes a classification decision into proportionate contributions of each network element to the overall classification score, called\n5\n\u201crelevances\u201d. When computed for the input dimensions of a CNN and visualized as a heatmap, these relevances highlight parts of the input that are important for the classification decision. LRP thus originally served as a tool for interpreting non-linear learning machines and has been applied as such in various fields, amongst others for general image recognition, medical imaging and natural language processing, cf. [33]. The direct linkage of the relevances to the classifier output, as well as the conservativity constraint imposed on the propagation of relevance between layers, makes LRP not only attractive for model explaining, but can also naturally serve as pruning criterion (see Section 4.1).\nThe main characteristic of LRP is a backward pass through the network during which the network output is redistributed to all elements of the network in a layer-by-layer fashion. This backward pass is structurally similar to gradient backpropagation and has therefore a similar runtime. The redistribution is based on a conservation principle such that the relevances can immediately be interpreted as the contribution that an element makes to the network output, hence establishing a direct connection to the network output and thus its predictive performance. Therefore, as a pruning criterion, the method is efficient and easily scalable to generic network structures. Independent of the type of neural network layer \u2014 that is pooling, fully-connected, convolutional layers \u2014 LRP allows to quantify the importance of elements throughout the network, given a global prediction context.\n6"}, {"heading": "3.2. LRP-based Pruning", "text": "The procedure of LRP-based pruning is summarized in Figure 1. In the first phase, a standard forward pass is performed by the network and the activations at each layer are collected. In the second phase, the score f(x) obtained at the output of the network is propagated backwards through the network according to LRP propagation rules [7]. In the third phase, the current model is pruned by eliminating the irrelevant (w.r.t. the \u201crelevance\u201d quantity R obtained via LRP) neurons/filters and is (optionally) further fine-tuned.\nLRP is based on a layer-wise conservation principle that allows the propagated quantity (e.g. relevance for a predicted class) to be preserved between neurons of two adjacent layers. Let R(l)i be the relevance of neuron i at layer l and R (l+1) j be the relevance of neuron j at the next layer l + 1. Stricter definitions of conservation that involve only subsets of neurons can further impose that relevance is locally redistributed in the lower layers and we define R(l)i\u2190j as the share of R(l+1)j that is redistributed to neuron i in the lower layer. The conservation property always satisfies \u2211\ni\nR (l) i\u2190j = R (l+1) j , (1)\nwhere the sum runs over all neurons i of the (during inference) preceeding layer l. When using relevance as a pruning criterion, this property helps to preserve its quantity layer-by-layer, regardless of hidden layer size and the number of iteratively pruned neurons for each layer. At each layer l, we can extract node i\u2019s global importance as its attributed relevance R(l)i .\nIn this paper, we specifically adopt relevance quantities computed with the LRP-\u03b11\u03b20-rule as pruning criterion. The LRP-\u03b1\u03b2-rule was developed with feedforward-DNNs with ReLU activations in mind and assumes positive (pre-softmax) logit activations flogit(x) > 0 for decomposition. The rule has been shown to work well in practice in such a setting [34].\nThe propagation rule performs two separate relevance propagation steps per layer: one exclusively considering activatory parts of the forward propagated quantities (i.e. all a(l)i wij > 0) and another only processing the inhibitory parts (a(l)i wij < 0) which are subsequently merged in a sum with components weighted by \u03b1 and \u03b2 (s.t. \u03b1 + \u03b2 = 1) respectively.\nBy selecting \u03b1 = 1, the propagation rule simplifies to\nR (l) i = \u2211 j\n( a (l) i wij )+ \u2211\ni\u2032 (ai\u2032 (l)wi\u2032j)\n+R (l+1) j , (2)\nwhere R(l)i denotes relevance attributed to the ith neuron at layer l, as an aggregation of downward-propagated relevance messages R(l,l+1)i\u2190j . The terms (\u00b7)\n+ indicate the positive part of the forward propagated pre-activation from layer l, to layer (l + 1). The i\u2032 is a running index over all input activations a. Note that a choice of \u03b1 = 1 only decomposes w.r.t. the parts of the inference signal supporting the model decision for the class of interest.\nEquation (2) is locally conservative, i.e. no quantity of relevance gets lost or injected during the distribution of Rj where each term of the sum corresponds to a relevance message Rj\u2190k. For this reason, LRP has the following technical advantages over other pruning techniques such as gradient-based or activation-based methods: (1) Localized relevance conservation implicitly ensures layer-wise regularized global redistribution of importances from each network element.\n7\n(2) By summing relevance within each (convolutional) filter channel, the LRP-based criterion is directly applicable as a measure of total relevance per node/filter, without requiring a post-hoc layer-wise renormalization, e.g., via lp norm. (3) The use of relevance scores is not restricted to a global application of pruning but can be easily applied to locally and (neuron- or filter-)group-wise constrained pruning without regularization. Different strategies for selecting (sub-)parts of the model might still be considered, e.g., applying different weightings/priorities for pruning different parts of the model: Should the aim of pruning be the reduction of FLOPs required during inference, one would prefer to focus on primarily pruning elements of the convolutional layers. In case the aim is a reduction of the memory requirement, pruning should focus on the fully-connected layers instead.\nIn the context of Algorithm 1, Step 1 of the LRP-based assessment of neuron and filter importance is performed as a single LRP backward pass through the model, with an aggregation of relevance per filter channel as described above, for convolutional layers, and does not require additional normalization or regularization."}, {"heading": "4. Experiments", "text": "We start by an attempt to intuitively illuminate the properties of different pruning criteria, namely, weight magnitude, Taylor, gradient and LRP, via a series of toy datasets.\nWe then show the effectiveness of the LRP criterion for pruning on widely-used image recognition benchmark datasets \u2014 i.e. the Scene 15 [35], Event 8 [36], Cats & Dogs [37], Oxford Flower 102 [38], CIFAR-101, and ILSVRC 2012 [39] datasets \u2014 and four pre-trained feed-forward deep neural network architectures, AlexNet and VGG-16 with only a single sequence of layers, and ResNet-18 and ResNet-50 [40], which both contain multiple parallel branches of layers and skip connections.\nThe first scenario focuses specifically on pruning of pre-trained CNNs with subsequent fine-tuning, as it is common in pruning research [21]. We compare our method with several state-of-the-art criteria to demonstrate the effectiveness of LRP as a pruning criterion in CNNs. In the second scenario, we tested whether the proposed pruning criterion also works well if only a very limited number of samples is available for pruning the model. This is relevant in case of devices with limited computational power, energy and storage such as mobile devices or embedded applications."}, {"heading": "4.1. Pruning Toy Models", "text": "First, we systematically compare the properties and effectiveness of the different pruning criteria on several toy datasets in order to foster an intuition about the properties of all approaches, in a controllable and computationally inexpensive setting. To this end we evaluate all four criteria on different toy data distributions qualitatively and quantitatively. We generated three k-class toy datasets (\u201cmoon\u201d (k = 2), \u201ccircle\u201d (k = 2) and \u201cmulti\u201d (k = 4)), using respective generator functions2,3.\n1https://www.cs.toronto.edu/~kriz/cifar.html 2https://scikit-learn.org/stable/datasets 3https://github.com/seulkiyeom/LRP_Pruning_toy_example\n8\nEach generated 2D dataset consists of 1000 training samples per class. We constructed and trained the models as a sequence of three consecutive ReLU-activated dense layers with 1000 hidden neurons each. After the first linear layer, we have added a DropOut layer with a dropout probability of 50%. The model receives inputs from R2 and has \u2014 depending on the toy problem set \u2014 k \u2208 {2, 4} output neurons:\nDense(1000) -> ReLU -> DropOut(0.5) -> Dense(1000) -> -> ReLU -> Dense(1000) -> ReLU -> Dense(k)\nWe then sample a number of new datapoints (unseen during training) for the computation of the pruning criteria. During pruning, we removed a fixed number of 1000 of the 3000 hidden neurons that have the least relevance for prediction according to each criterion. This is equivalent to removing 1000 learned (yet insignificant, according to the criterion) filters from the model. After pruning, we observed the changes in the decision boundaries and re-evaluated for classification accuracy using the original training samples and re-sampled datapoints across criteria. This experiment is performed with n \u2208 [1, 5, 10, 20, 50, 100, 200] reference samples for testing and the computation of pruning criteria. Each setting is repeated 50 times, using the same set of random seeds (depending on the repetition index) for each n across all pruning criteria to uphold comparability.\nFigure 2 shows the data distributions of the generated toy datasets, an exemplary set of n = 5 samples generated for criteria computation, as well as the qualitative impact to the models\u2019 decision boundary when removing a fixed set of 1000 neurons as selected via the compared criteria. Figure 3 investigates how the pruning criteria preserve the models\u2019 problem solving capabilities as a function of the number of samples selected for computing the criteria. Table 1 then quantitatively summarizes the results for specific numbers of unseen samples (n \u2208 [1, 5, 20, 100]) for computing the criteria. Here we report the model accuracy on the training set in order to relate the preservation of the decision function as learned from data between unpruned (2nd column) to pruned models and pruning criteria (remaining columns).\nThe results in Table 1 show that, among all criteria based on reference sample for the computation of relevance, the LRP-based measure consistently outperforms all other criteria in all reference set sizes and datasets. Only in the case of n = 1 reference sample per class, the weight criterion preserves the model the best. Note that using the weight magnitude as a measure of network element importance is a static approach, independent from the choice of reference samples. Given n = 5 points of reference per class, the LRP-based criterion\n9\nalready outperforms also the weight magnitude as a criterion for pruning unimportant neural network structures, while successfully preserving the functional core of the predictor. Figure 2 demonstrates how the toy models\u2019 decision boundaries change under influence of pruning with all four criteria. We can observe that the weight criterion and LRP preserve the models\u2019 learned decision boundary well. Both the Taylor and gradient measures degrade the model significantly. Compared to weight- and LRP-based criteria, models pruned by gradient-based criteria misclassify a large part of samples.\nThe first row of Figure 3 shows that all (data dependent) measures benefit from increasing the number of reference points. LRP is able to find and preserve the functionally important network components with only very little data, while at the same time being considerably less sensitive to the choice of reference points than other metrics, visible in the measures\u2019 standard deviations. Both the gradient and Taylor-based measures do not reach the performance of LRP-based pruning, even with 200 reference samples for each class. The performance of pruning with the weight magnitude based measure is constant, as it does only depend on the learned weights itself.\nThe bottom row of Figure 3 shows the test performance of the pruned models as a function of the number of samples used for criteria computation. Here, we tested on 500 samples per class, drawn from the datasets\u2019 respective distributions, and perturbed with additional gaussian noise (N (0, 0.3)) added after data generation. Due to the large amounts of noise added to the data, we see the prediction performance of the pruned and unpruned models to decrease in all settings. Here we can observe that two out of three times the\n10\nLRP-pruned models outperforming all other criteria. Only once, on the \u201cmoon\u201d dataset, pruning based on the weight criterion yields a higher performance than the LRP-pruned model. Most remarkably though, only the models pruned with the LRP-based criterion exhibit prediction performance and behavior \u2014 measured in mean and standard deviation of accuracies measured over all 50 random seeds per n reference samples on the deliberatly heavily noisy data \u2014 highly similar to the original and unpruned model, from only n = 5 reference samples per class on. This yields another strong indicator that LRP is, among the compared criteria, most capable at preserving the relevant core of the learned network function, and to dismiss unimportant parts of the model during pruning.\nThe strong results of LRP, and the partial similarity between the results on the training datasets between LRP and weight raises the question where and how both metrics (and Taylor and gradient) deviate, as it can be expected that both metrics at least select highly overlapping sets of neurons and filters for pruning and preservation. We therefore investigate in all three toy settings \u2014 across the different number of reference samples and random seeds \u2014 the (dis)similarities and (in)consistencies in neuron selection and ranking by measuring the\n11\nset similaries (S1 \u2229 S2)/min(|S1|, |S2|) of the k neurons selected for pruning (ranked first) and preservation (ranked last) between and within criteria. Since the weight criterion is not influenced by the choice of reference samples for computation, it is expected that the resulting neuron order is perfectly consistent with itself in all settings (cf. Table 3). What is unexpected however, given the results in Table 1 and Figure 3 indicating similar model behavior after pruning to be expected between LRP- and weight-based criteria, at least on the training data, is the minimal set overlap between LRP and weight, given the higher set similarities between LRP and the gradient and Taylor criteria, as shown in Table 2. Overall, the set overlap between the neurons ranked in the extremes of the orderings show that LRP-derived pruning strategies have very little in common with the ones originating from the other criteria. This observation can also be made on more complex networks at hand of Figure 6, as shown and discussed later in this Section.\nTable 3 reports the self-similarity in neuron selection in the extremes of the ranking across random seeds (and thus sets of reference samples), for all criteria and toy settings. While LRP yields a high consistency in neuron selection for both the pruning (first-k) and the preservation (last-k) of neural network elements, both gradient and moreso Taylor exhibit lower self-similarities. The lower consistency of both latter criteria in the model components ranked last (i.e. preserved in the model the longest during pruning) yields an explanation for the large variation in results observed earlier: although gradient and Taylor are highly consistent in the removal of neurons rated as irrelevant, their volatility in the preservation of neurons which constitute the functional core of the network after pruning yields dissimilarities in the resulting predictor function. The high consistency reported for LRP in terms of neuron sets selected for pruning and preservation, given the relatively low Spearman correlation coefficient points out only minor local perturbations of the pruning order due to the selection of reference samples. We find a direct correspondence between the here reported (in)consistency of pruning behavior for the three data-dependent criteria, and the in [12] observed \u201cexplanation continuity\u201d observed for LRP (and discontinuity for gradient and Taylor) in neural networks containing the commonly used ReLU activation function, which provides an explanation for the high pruning consistency obtained with LRP, and the extreme volatility for gradient and Taylor.\nA supplementary analysis of the neuron selection consistency of LRP over different counts of reference samples n, demonstrating the requirement of only very few reference samples per class in order to obtain stable pruning results, can be found in Appendix B Supplementary Results 1.\n12\nTaken together, the results of Tables 2 to 3 and Supplementary Table B.5 elucidate that LRP constitutes \u2014 compared to the other methods \u2014 an orthogonal pruning criterion which is very consistent in its selection of (un)important neural network elements, while remaining adaptive to the selection of reference samples for criterion computation. Especially the similarity in post-pruning model performance to the static weight criterion indicates that both metrics are able to find valid, yet completely different pruning solutions. However, since LRP can still benefit from the influence of reference samples, we will show in Section 4.2.2 that our proposed criterion is able to outperform not only weight, but all other criteria in Scenario 2, where pruning is is used instead of fine-tuning as a means of domain adaptation. This will be discussed in the following sections."}, {"heading": "4.2. Pruning Deep Image Classifiers for Large-scale Benchmark Data", "text": "We now evaluate the performance of all pruning criteria on the CNNs, VGG-16, AlexNet as well as ResNet-18 and ResNet-50, \u2014 popular models in compression research [41] \u2014 all of which are pre-trained on ILSVRC 2012 (ImageNet). VGG-16 consists of 13 convolutional layers with 4224 filters and 3 fully-connected layers and AlexNet contains 5 convolutional layers with 1552 filters and 3 fully-connected layers. In dense layers, there exist 4,096+4,096+k neurons (i.e. filters), respectively, where k is the number of output classes. In terms of complexity of the model, the pre-trained VGG-16 and AlexNet on ImageNet originally consist of 138.36/60.97 million of parameters and 154.7/7.27 Giga Multiply-Accumulate Operations per Second (GMACS) (as a measure of FLOPs), respectively. ResNet-18 and ResNet-50 consist of 20/53 convolutional layers with 4,800/26,560 filters. In terms of complexity of the model, the pre-trained ResNet-18 and ResNet-50 on ImageNet originally consist of 11.18/23.51 million of parameters and 1.82/4.12 GMACS (as a measure of FLOPs), respectively.\nFurthermore, since the LRP scores are not implementation-invariant and depend on the LRP rules used for the batch normalization (BN) layers, we convert a trained ResNet into a canonized version, which yields the same predictions up to numerical errors. The canonization fuses a sequence of a convolution and a BN layer into a convolution layer with updated weights4 and resets the BN layer to be the identity function. This removes the BN layer effectively by rewriting a sequence of two affine mappings into one updated affine mapping [42]. The second change replaced calls to torch.nn.functional methods and the summation in\n4See bnafterconv_overwrite_intoconv(conv,bn) in the file lrp_general6.py in https://github. com/AlexBinder/LRP_Pytorch_Resnets_Densenet\n13\nthe residual connection by classes derived from torch.nn.Module which then were wrapped by calls to torch.autograd.function to enable custom backward computations suitable for LRP rule computations.\nExperiments are performed within the PyTorch and torchvision frameworks under Intel(R) Xeon(R) CPU E5-2660 2.20GHz and NVIDIA Tesla P100 with 12GB for GPU processing. We evaluated the criteria on six public datasets (Scene 15 [35], Event 8, Cats and Dogs [37], Oxford Flower 102 [38], CIFAR-10, and ILSVRC 2012 [39]). For more detail on the datasets and the preprocessing, see Appendix A. Our complete experimental setup will be made publicly available on https: // github. com/ seulkiyeom/ LRP_ pruning .\nIn order to prepare the models for evaluation, we first fine-tuned the models for 200 epochs with constant learning rate 0.001 and batch size of 20. We used the Stochastic Gradient Descent (SGD) optimizer with momentum of 0.9. In addition, we also apply dropout to the fully-connected layers with probability of 0.5. Fine-tuning and pruning are performed on the training set, while results are evaluated on each test dataset. Throughout the experiments, we iteratively prune 5% of all the filters in the network by eliminating neurons including their input and output connections. In Scenario 1, we subsequently fine-tune and re-evaluate the model to account for dependency across parameters and regain performance, as it is common."}, {"heading": "4.2.1. Scenario 1: Pruning with Fine-tuning", "text": "On the first scenario, we retrain the model after each iteration of pruning in order to regain lost performance. We then evaluate the performance of the different pruning criteria after each pruning-retraining-step.\nThat is, we quantify the importance of each filter by the magnitude of the respective criterion and iteratively prune 5% of all filters (w.r.t. the original number of filters in the model) rated least important in each pruning step. Then, we compute and record the training loss, test accuracy, number of remaining parameters and total estimated FLOPs. We assume\n14\nthat the least important filters should have only little influence on the prediction and thus incur the lowest performance drop if they are removed from the network.\n15"}, {"heading": "A. Cats and Dogs", "text": ""}, {"heading": "B. Oxford Flower 102", "text": "Figure 4 (and Supplementary Figure C.11) depict test accuracies with increasing pruning rate in VGG-16 and ResNet-50 (and AlexNet and ResNet-18, respectively) after fine-tuning for each dataset and each criterion. It is observed that LRP achieves higher test accuracies compared to other criteria in a large majority of cases (see Figure 5 and Supplementary Figure C.10). These results demonstrate that the performance of LRP-based pruning is stable and independent of the chosen dataset. Apart from performance, regularization by layer is a critical constraint which obstructs the expansion of some of the criteria toward several pruning strategies such as local pruning, global pruning, etc. Except for the LRP criterion, all criteria perform substantially worse without lp regularization compared to those with lp regularization and result in unexpected interruptions during the pruning process due to the biased redistribution of importance in the network (cf. top rows of Figure 4 and Supplementary Figure C.11).\nTable 4 shows the predictive performance of the different criteria in terms of training loss, test accuracy, number of remaining parameters and FLOPs, for the VGG-16 and ResNet-50 models. Similar results for AlexNet and ResNet-18 can be found in Supplementary Table C.6. Except for CIFAR-10, the highest compression rate (i.e. lowest number of parameters) could be achieved by the proposed LRP-based criterion (row \u201cParams\u201d) for VGG-16, but not for ResNet-50. However, in terms of FLOPs, the proposed criterion only outperformed the weight criterion, but not the Taylor and Gradient criteria (row\u201cFLOPs\u201d). This is due to the fact that a reduction in number of FLOPs depends on the location where pruning is applied within the network: Figure 6 shows that the LRP and weight criteria focus the pruning on upper layers closer to the model output, whereas the Taylor and Gradient criteria focus more on the lower layers.\nThroughout the pruning process usually a gradual decrease in performance can be observed. However, with the Event 8, Oxford Flower 102 and CIFAR-10 datasets, pruning leads to an initial performance increase, until a pruning rate of approx. 30% is reached. This\n16\nbehavior has been reported before in the literature and might stem from improvements of the model structure through elimination of filters related to classes in the source dataset (i.e., ILSVRC 2012) that are not present in the target dataset anymore [43].\nSupplementary Table C.6 and Supplementary Figure C.11 similarly show that LRP achieves the highest test accuracy in AlexNet and ResNet-18 for nearly all pruning ratios with almost every dataset.\nFigure 6 shows the number of the remaining convolutional filters for each iteration. We observe that, on the one hand, as pruning rate increases, the convolutional filters in earlier layers that are associated with very generic features, such as edge and blob detectors, tend to generally be preserved as opposed to those in latter layers which are associated with abstract, task-specific features. On the other hand, the LRP- and weight-criterion first keep the filters in early layers in the beginning, but later aggressively prune filters near the input which now have lost functionality as input to later layers, compared to the gradient-based criteria such as gradient and Taylor-based approaches. Although gradient-based criteria also adopt the greedy layer-by-layer approach, we can see that gradient-based criteria pruned the less important filters almost uniformly across all the layers due to re-normalization of the criterion in each iteration. However, this result contrasts with previous gradient-based works [21, 24] that have shown that neurons deemed unimportant in earlier layers, contribute significantly compared to neurons deemed important in latter layers. In contrast to this, LRP can efficiently preserve neurons in the early layers \u2014 as long as they serve a purpose \u2014 despite of iterative global pruning."}, {"heading": "4.2.2. Scenario 2: Pruning without Fine-tuning", "text": "In this section, we evaluate whether pruning works well if only a (very) limited number of samples is available for quantifying the pruning criteria. To the best of our knowledge, there are no previous studies that show the performance of pruning approaches when acting w.r.t. very small amounts of data. With large amounts of data available (and even though we can expect reasonable performance after pruning), an iterative pruning and fine-tuning procedure of the network can amount to a very time consuming and computationally heavy process. From a practical point of view, this issue becomes a significant problem, e.g. with limited computational resources (mobile devices or in general; consumer-level hardware) and reference data (e.g., private photo collections), where capable and effective one-shot pruning approaches are desired and only little leeway (or none at all) for fine-tuning strategies after pruning is available.\nTo investigate whether pruning is possible also in these scenarios, we performed experiments with a relatively small number of data on the 1) Cats & Dogs and 2) subsets from the ILSVRC 2012 classes. On the Cats & Dogs dataset, we only used 10 samples each from the \u201ccat\u201d and \u201cdog\u201d classes to prune the (on ImageNet) pre-trained AlexNet, VGG-16, ResNet-18 and ResNet-50 networks with the goal of domain/dataset adaption. The binary classification (i.e. \u201ccat\u201d vs. \u201cdog\u201d) is a subtask within the ImageNet taxonomy and corresponding output neurons can be identified by its WordNet5 associations. This experiment implements the task of domain adaptation.\n5http://www.image-net.org/archive/wordnet.is_a.txt\n17\nIn a second experiment on the ILSVRC 2012 dataset, we randomly chose k = 3 classes for the task of model specialization, selected only n = 10 images per class from the training set and used them to compare the different pruning criteria.\nFor each criterion, we used the same selection of classes and samples. In both experimental settings, we do not fine-tune the models after each pruning iteration, in contrast to Scenario 1 in Section 4.2.1. The obtained post-pruning model performance is averaged over 20 random selections of classes (ImageNet) and samples (Cats & Dogs) to account for randomness. Please note that before pruning, we first restructured the models\u2019 fully connected output layers to only preserve the task-relevant k network outputs by eliminating the 1000 \u2212 k redundant output neurons.\nFurthermore, as our target datasets are relatively small and only have an extremely reduced set of target classes, the pruned models could still be very heavy w.r.t. memory requirements if the pruning process would be limited to the convolutional layers, as in Section 4.2.1. More specifically, while convolutional layers dominantly constitute the source of computation cost (FLOPs), fully connected layers are proven to be more redundant [28]. In this respect, we applied pruning procedures in both fully connected layers and convolutional layers in combination for VGG-16.\nFor pruning, we iterate a sequence of first pruning filters from the convolutional layers, followed by a step of pruning filters/neurons from the model\u2019s fully connected layers. Note that both evaluated ResNet architectures mainly consist of convolutional- and pooling layers, and conclude in a single dense layer, of which the set of input neurons are only affected via their inputs by pruning the below convolutional stack. We therefore restrict the iterative pruning filters from the sequence of dense layers of the feed-forward architecture of the VGG-16.\nThe model performance after the application of each criterion for classifying a small number of classes (k = 3) from the ILSVRC 2012 dataset is indicated in Figure 7 for VGG16 and Figure 8 for ResNets (please note again that ResNets do not have fully-connected layers). During pruning at fully-connected layers, no significant difference across different pruning ratios can be observed. Without further fine-tuning, pruning weights/filters at the fully connected layers can retain performance efficiently.\nHowever, there is a certain difference between LRP and other criteria with increasing pruning ratio of convolutional layers for VGG-16/ResNet-18/ResNet-50, respectively: (LRP vs. Taylor with l2-norm; up to of 9.6/61.8/51.8%, LRP vs. gradient with l2-norm; up to 28.0/63.6/54.5 %, LRP vs. weight with l2-norm; up to 27.1/48.3/30.2 %).\nMoreover, pruning convolutional layers needs to be carefully managed compared to pruning fully connected layers. We can observe that LRP is applicable for pruning any layer type (i.e. fully connected, convolutional, pooling, etc.) efficiently. Additionally, as mentioned in Section 3.1, our method can be applied to general network architectures because it can automatically measure the importance of weights or filters in a global (network-wise) context without further normalization.\nFigure 9 shows the test accuracy as a function of the pruning ratio, in context a domain adaption task from ImageNet towards the Cats & Dogs dataset for all models. As the pruning ratio increases, we can see that even without fine-tuning, using LRP as pruning criterion can keep the test accuracy not only stable, but close to 100%, given the extreme scarcity of data in this experiment. In contrast, the performance decreases significantly when using the other\n18\n19\ncriteria requiring an application of the l2-norm. Initially, the performance is even slightly increasing when pruning with LRP. During iterative pruning, unexpected changes in accuracy with LRP (for 2 out of 20 repetitions of the experiment) have been shown around 50 - 55% pruning ratio, but accuracy is regained quickly again. However, only the VGG-16 model seems to be affected, and none other for this task. For both ResNet models, this phenomenon occurs for the other criteria instead. A series of in-depth investigations of this momentary decrease in performance did not lead to any insights and will be subject of future work6.\nBy pruning over 99% of convolutional filters in the networks using our proposed method, we can have 1) greatly reduced computational cost, 2) faster forward and backward processing (e.g. for the purpose of further training, inference or the computation of attribution maps), and 3) a lighter model even in the small sample case, all while adapting off-the-shelf pre-trained ImageNet models towards a dog-vs.-cat classification task."}, {"heading": "5. Discussion", "text": "Our experiments demonstrate that the novel LRP criterion consistently performed well compared to other criteria across various datasets, model architectures and experimental settings, and oftentimes outperformed the competing criteria. This is especially pronounced in our Scenario 2 (cf. Section 4.2.2), where only little resources are available for criterion computation, and no fine-tuning after pruning is allowed. Here, LRP considerably outperformed the other metrics on toy data (cf. Section 4.1) and image processing benchmark data (cf. Section 4.2.2). The strongly similar results between criteria observed in Scenario 1 (cf. Section 4.2.2) are also not surprising, as an additional file-tuning step after pruning may allow the pruned neural network model to recover its original performance, as long as the model has the capacity to do so [21].\n6We consequently have to assume that this phenomenon marks the downloaded pre-trained VGG-16 model as an outlier in this respect. A future line of research will dedicate inquiries about the circumstances leading to intermediate loss and later recovery of model performance during pruning.\n20\nFrom the results of Table 4 and Supplementary Table C.6 we can observe that with a fixed pruning target of n% filters removed, LRP might not always result in the cheapest sub-network after pruning in terms of parameter count and FLOPs per inference, however it consistently is able to identify the network components for removal and preservation leading to the best performing model after pruning. Latter results resonate also strongly in our experiments of Scenario 2 on both image and toy data, where, without the additional fine-tuning step, the LRP-pruned models vastly outperform their competitors. The results obtained in multiple toy settings verify that only the LRP-based pruning criterion is able to preserve the original structure of the prediction function (cf. Figures 2 and 3).\nUnlike the weight criterion, which is a static quantity once the network is not in training anymore, the criteria Taylor, gradient and LRP require reference samples for computation, which in turn may affect the estimation of neuron importance. From the latter three criteria, however, only LRP provides a continuous measure of network structure importance (cf. Sec 7.2 in [12]) which does not suffer from abrupt changes in the estimated importance measures with only marginal steps between reference samples. This quality of continuity is reflected in the stability and quality of LRP results reported in Section 4.1, compared to the high volatility in neuron selection for pruning and model performance after pruning observable for the gradient and Taylor criteria. From this observation it can also be deduced that LRP requires relatively few data points to converge to a pruning solution that possesses a similar prediction behavior as the original model. Hence, we conclude that LRP is a robust pruning criterion that is broadly applicable in practice. Especially in a scenario where no finetuning is applied after pruning (see Sec. 4.2.2), the LRP criterion allows for pruning of a large part of the model without significant accuracy drops.\nIn terms of computational cost, LRP is comparable to the Taylor and Gradient criteria because these criteria require both a forward and a backward pass for all reference samples. The weight criterion is substantially cheaper to compute since it does not require to evaluate any reference samples; however, its performance falls short in most of our experiments. Additionally, our experiments demonstrate that LRP requires less reference samples than the other criteria (cf. Figure 3 and Table 1), thus the required computational cost is lower in practical scenarios, and better performance can be expected if only low numbers of reference samples are available (cf. Figure 9).\nUnlike all other criteria, LRP does not require explicit regularization via `p-normalization, as it is naturally normalized via its enforced relevance conservation principle during relevance backpropagation, which leads to the preservation of important network substructures and bottlenecks in a global model context. In line with the findings by [21], our results in Figure 4 and Supplementary Figure C.11 show that additional normalization after criterion computation for weight, gradient and Taylor is not only vital to obtain good performance, but also to avoid disconnected model segments \u2014 something which is prevented out-of-the-box with LRP.\nHowever, our proposed criterion still provides several open questions that deserve a deeper investigation in future work. First of all, LRP is not implementation invariant, i.e., the structure and composition of the analyzed network might affect the computation of the LRP-criterion and \u201cnetwork canonization\u201d \u2014 a functionally equivalent restructuring of the model \u2014 might be required for optimal results, as discussed early in Section 4 and [42]. Furthermore, while our LRP-criterion does not require additional hyperparameters, e.g. for\n21\nnormalization, the pruning result might still depend on the chosen LRP variant. In this paper, we chose the \u03b11\u03b20-rule in all layers, because this particular parameterization identifies the network\u2019s neural pathways positively contributing to the selected output neurons for which reference samples are provided, is robust to the detrimental effects of shattered gradients affecting especially very deep CNNs [11], and has a mathematical well-motivated foundation in DTD [11, 12]. However, other work from literature provide [13] or suggest [9, 8] alternative parameterizations to optimize the method for explanatory purposes. It is an interesting direction for future work to examine whether these findings also apply to LRP as a pruning criterion."}, {"heading": "6. Conclusion", "text": "Modern CNNs typically have a high capacity with millions of parameters as this allows to obtain good optimization results in the training process. After training, however, high inference costs remain, despite the fact that the number of effective parameters in the deep model is actually significantly lower (see e.g. [44]). To alleviate this, pruning aims at compressing and accelerating the given models without sacrificing much predictive performance. In this paper, we have proposed a novel criterion for the iterative pruning of CNNs based on the explanation method LRP, linking for the first time two so far disconnected lines of research. LRP has a clearly defined meaning, namely the contribution of an individual network element, i.e. weight or filter, to the network output. Removing elements according to low LRP scores thus means discarding all aspects in the model that do not contribute relevance to its decision making. Hence, as a criterion, the computed relevance scores can easily and cheaply give efficient compression rates without further postprocessing, such as per-layer normalization. Besides, technically LRP is scalable to general network structures and its computational cost is similar to the one of a gradient backward pass.\nIn our experiments, the LRP criterion has shown favorable compression performance on a variety of datasets both with and without retraining after pruning. Especially when pruning without retraining, our results for small datasets suggest that the LRP criterion outperforms the state of the art and therefore, its application is especially recommended in transfer learning settings where only a small target dataset is available.\nIn addition to pruning, the same method can be used to visually interpret the model and explain individual decisions as intuitive relevance heatmaps. Therefore, in future work, we propose to use these heatmaps to elucidate and explain which image features are most strongly affected by pruning to additionally avoid that the pruning process leads to undesired Clever Hans phenomena [8]."}, {"heading": "Acknowledgements", "text": "This work was supported by the German Ministry for Education and Research (BMBF) through BIFOLD (refs. 01IS18025A and 01IS18037A), MALT III (ref. 01IS17058), Patho234 (ref. 031L0207D) and TraMeExCo (ref. 01IS18056A), as well as the Grants 01GQ1115 and 01GQ0850; and by Deutsche Forschungsgesellschaft (DFG) under Grant Math+, EXC 2046/1, Project ID 390685689; by the Institute of Information & Communications Technology Planning & Evaluation (IITP) grant funded by the Korea Government (No. 2019-0-00079, Artificial\n22\nIntelligence Graduate School Program, Korea University); and by STE-SUTD Cyber Security Corporate Laboratory; the AcRF Tier2 grant MOE2016-T2-2-154; the TL project Intent Inference; and the SUTD internal grant Fundamentals and Theory of AI Systems."}, {"heading": "Appendix A. Supplementary Methods 1: Data Preprocessing", "text": "During fine-tuning, images are resized to 256\u00d7256 and randomly cropped to 224\u00d7224 pixels, and then horizontally flipped with a random chance of 50% for data augmentation. For testing, images are resized to 224\u00d7224 pixels.\nScene 15: The Scene 15 dataset contains about 4,485 images and consists of 15 natural scene categories obtained from COREL collection, Google image search and personal photographs [35]. We fine-tuned four different models on 20% of the images from each class and achieved initial Top-1 accuracy of 88.59% for VGG-16, 85.48% for AlexNet, 83.96% for ResNet-18, and 88.28% for ResNet-50, respectively.\nEvent 8: Event-8 consists of 8 sports event categories by integrating scene and object recognition. We use 40% of the dataset\u2019s images for fine-tuning and the remaining 60% for testing. We adopted the common data augmentation method as in [25].\nCats and Dogs: This is the Asirra dataset provided by Microsoft Research (from Kaggle). The given dataset for the competition (KMLC-Challenge-1) [37]. Training dataset contains 4,000 colored images of dogs and 4,005 colored images of cats, while containing 2,023 test images. We reached initial accuracies of 99.36% for VGG-16, 96.84% for AlexNet, 97.97% for ResNet-18, and 98.42% for ResNet-50 based on transfer learning approach.\nOxford Flowers 102: The Oxford Flowers 102 dataset contains 102 species of flower categories found in the UK, which is a collection with over 2,000 training and 6,100 test images [38]. We fine-tuned models with pre-trained networks on ImageNet for transfer learning.\nCIFAR-10: This dataset contains 50,000 training images and 10,000 test images spanning 10 categories of objects. The resolution of each image is 32\u00d732 pixels and therefore we resize the images as 224\u00d7224 pixels.\nILSVRC 2012: In order to show the effectiveness of the pruning criteria in the small sample scenario, we pruned and tested all models on randomly selected k = 3 from 1000 classes and data from the ImageNet corpus [39]."}, {"heading": "Appendix B. Supplementary Results 1: Additional results on toy data", "text": "Here, we discuss with Table B.5 the consistency of LRP-based neuron selection across reference sample sizes. One can assume that the larger the choice of n the less volatile is the choice of (un)important neurons by the criterion, as the influence of individual reference samples is marginalized out. We therefore compare the first and last ranked sets of neurons selected for a low (yet due to our observations sufficient) number n = 10 of reference samples to all other reference sample set sizes m over all unique random seed combinations. For all comparisons of n\u00d7m (except for m = 1) we observe a remarkable consistency in the selection of (un)important network substructures. With an increasing m, we can see the consistency in neuron set selection gradually increase and then plateau for the \u201cmoon\u201d and \u201ccircle\u201d datasets, which means that the selected set of neurons remains consistent for larger sets of reference samples from that point on. For the \u201cmult\u201d toy dataset, we observe a gradual yet minimal decrease in the set similarity scores for m \u2265 10, which means that the results deviate from the selected neurons for n = 10, i.e. variability over the neuron sets selected for n = 10 are\ni\nthe source of the volatility between n-reference and m-reference selected neuron sets. In all cases, peak consistency is achieved at n \u2208 {5, 10} reference samples, identifying low numbers of n \u2208 {5, 10} as sufficient for consistently pruning our toy models."}, {"heading": "Appendix C. Supplementary Results 2: Additional results for image processing", "text": "Here, we provide results for AlexNet and ResNet-18 \u2014 in addition to the VGG16 and ResNet-50 results shown in the main paper \u2014 in Supplementary Figure C.10 (cf. Fig. 5), Supplementary Figure C.11 (cf. Fig. 4), and Supplementary Table C.6 (cf. Table 4). These results demonstrate that the favorable pruning performance of our LRP criterion is not limited to any specific network architecture.\nii"}], "title": "Pruning by Explaining: A Novel Criterion for Deep Neural Network Pruning", "year": 2020}