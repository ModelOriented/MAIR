{
  "abstractText": "Deep learning, a rebranding of deep neural network research works, has achieved a remarkable success in recent years. With multiple hidden layers, deep learning models aim at computing the hierarchical feature representations of the observational data. Meanwhile, due to its severe disadvantages in data consumption, computational resources, parameter tuning costs and the lack of result explainability, deep learning has also suffered from lots of criticism. In this paper, we will introduce a new representation learning model, namely \u201cSample-Ensemble Genetic Evolutionary Network\u201d (SEGEN), which can serve as an alternative approach to deep learning models. Instead of building one single deep model, based on a set of sampled sub-instances, SEGEN adopts a genetic-evolutionary learning strategy to build a group of unit models generations by generations. The unit models incorporated in SEGEN can be either traditional machine learning models or the recent deep learning models with a much \u201cnarrower\u201d and \u201cshallower\u201d architecture. The learning results of each instance at the final generation will be effectively combined from each unit model via diffusive propagation and ensemble learning strategies. From the computational perspective, SEGEN requires far less data, fewer computational resources and parameter tuning efforts, but has sound theoretic interpretability of the learning process and results. Extensive experiments have been done on several different real-world benchmark datasets, and the experimental results obtained by SEGEN have demonstrated its advantages over the state-of-the-art representation learning models.",
  "authors": [
    {
      "affiliations": [],
      "name": "Jiawei Zhang"
    },
    {
      "affiliations": [],
      "name": "Limeng Cui"
    }
  ],
  "id": "SP:748ac2b122032669ba8b17db9669cc561474076f",
  "references": [
    {
      "authors": [
        "E. Arisoy",
        "T. Sainath",
        "B. Kingsbury",
        "B. Ramabhadran"
      ],
      "title": "Deep neural network language models",
      "venue": "WLM",
      "year": 2012
    },
    {
      "authors": [
        "A. Bordes",
        "N. Usunier",
        "A. Garcia-Duran",
        "J. Weston",
        "O. Yakhnenko"
      ],
      "title": "Translating embeddings for modeling multi-relational data",
      "venue": "In NIPS",
      "year": 2013
    },
    {
      "authors": [
        "S. Chang",
        "W. Han",
        "J. Tang",
        "G. Qi",
        "C. Aggarwal",
        "T. Huang"
      ],
      "title": "Heterogeneous network embedding via deep architectures",
      "venue": "KDD",
      "year": 2015
    },
    {
      "authors": [
        "C. Chen",
        "M. Tsai",
        "Y. Lin",
        "Y. Yang"
      ],
      "title": "Query-based music recommendations via preference embedding",
      "venue": "RecSys",
      "year": 2016
    },
    {
      "authors": [
        "T. Chen",
        "Y. Sun"
      ],
      "title": "Task-guided and path-augmented heterogeneous network embedding for author identification",
      "venue": "CoRR, abs/1612.02814",
      "year": 2016
    },
    {
      "authors": [
        "J. Dean",
        "G. Corrado",
        "R. Monga",
        "K. Chen",
        "M. Devin",
        "Q. Le",
        "M. Mao",
        "M. Ranzato",
        "A. Senior",
        "P. Tucker",
        "K. Yang",
        "A. Ng"
      ],
      "title": "Large scale distributed deep networks",
      "venue": "NIPS",
      "year": 2012
    },
    {
      "authors": [
        "D. Decoste",
        "B. Sch\u00f6lkopf"
      ],
      "title": "Training invariant support vector machines",
      "venue": "Mach. Learn.",
      "year": 2002
    },
    {
      "authors": [
        "L. Deng",
        "G. Hinton",
        "B. Kingsbury"
      ],
      "title": "New types of deep neural network learning for speech recognition and related applications: An overview",
      "venue": "ICASSP",
      "year": 2013
    },
    {
      "authors": [
        "I. Goodfellow",
        "Y. Bengio",
        "A. Courville"
      ],
      "title": "Deep Learning",
      "venue": "MIT Press",
      "year": 2016
    },
    {
      "authors": [
        "A. Grover",
        "J. Leskovec"
      ],
      "title": "Node2vec: Scalable feature learning for networks",
      "venue": "KDD",
      "year": 2016
    },
    {
      "authors": [
        "S. Hill"
      ],
      "title": "Elite and upper-class families",
      "venue": "In Families: A Social Class Perspective",
      "year": 2012
    },
    {
      "authors": [
        "G. Hinton",
        "L. Deng",
        "D. Yu",
        "G. Dahl",
        "A. Mohamed",
        "N. Jaitly",
        "A. Senior",
        "V. Vanhoucke",
        "P. Nguyen",
        "T. Sainath",
        "B. Kingsbury"
      ],
      "title": "Deep neural networks for acoustic modeling in speech recognition",
      "venue": "IEEE Signal Processing Magazine",
      "year": 2012
    },
    {
      "authors": [
        "G. Hinton",
        "S. Osindero",
        "Y. Teh"
      ],
      "title": "A fast learning algorithm for deep belief nets",
      "venue": "Neural Comput.",
      "year": 2006
    },
    {
      "authors": [
        "H. Jaeger"
      ],
      "title": "Tutorial on training recurrent neural networks",
      "venue": "covering BPPT, RTRL, EKF and the \u201cecho state network\u201d approach. Technical report, Fraunhofer Institute for Autonomous Intelligent Systems (AIS)",
      "year": 2002
    },
    {
      "authors": [
        "A. Krizhevsky",
        "I. Sutskever",
        "G. Hinton"
      ],
      "title": "Imagenet classification with deep convolutional neural networks",
      "venue": "NIPS",
      "year": 2012
    },
    {
      "authors": [
        "Y. LeCun",
        "Y. Bengio",
        "G. Hinton"
      ],
      "title": "Deep learning",
      "venue": "Nature, 521",
      "year": 2015
    },
    {
      "authors": [
        "Y. Lecun",
        "L. Bottou",
        "Y. Bengio",
        "P. Haffner"
      ],
      "title": "Gradient-based learning applied to document recognition",
      "venue": "Proceedings of the IEEE",
      "year": 1998
    },
    {
      "authors": [
        "Y. Lin",
        "Z. Liu",
        "M. Sun",
        "Y. Liu",
        "X. Zhu"
      ],
      "title": "Learning entity and relation embeddings for knowledge graph completion",
      "venue": "AAAI",
      "year": 2015
    },
    {
      "authors": [
        "T. Mikolov",
        "I. Sutskever",
        "K. Chen",
        "G. Corrado",
        "J. Dean"
      ],
      "title": "Distributed representations of words and phrases and their compositionality",
      "venue": "NIPS",
      "year": 2013
    },
    {
      "authors": [
        "A. Mnih",
        "G. Hinton"
      ],
      "title": "A scalable hierarchical distributed language model",
      "venue": "In NIPS",
      "year": 2009
    },
    {
      "authors": [
        "B. Perozzi",
        "R. Al-Rfou",
        "S. Skiena"
      ],
      "title": "Deepwalk: Online learning of social representations",
      "venue": "KDD",
      "year": 2014
    },
    {
      "authors": [
        "G. Rudolph"
      ],
      "title": "Convergence analysis of canonical genetic algorithms",
      "venue": "IEEE Transactions on Neural Networks",
      "year": 1994
    },
    {
      "authors": [
        "R. Salakhutdinov",
        "G. Hinton"
      ],
      "title": "Semantic hashing",
      "venue": "International Journal of Approximate Reasoning",
      "year": 2009
    },
    {
      "authors": [
        "J. Tang",
        "M. Qu",
        "M. Wang",
        "M. Zhang",
        "J. Yan",
        "Q. Mei"
      ],
      "title": "Line: Largescale information network embedding",
      "venue": "WWW",
      "year": 2015
    },
    {
      "authors": [
        "P. Vincent",
        "H. Larochelle",
        "I. Lajoie",
        "Y. Bengio",
        "P. Manzagol"
      ],
      "title": "Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion",
      "venue": "Journal of Machine Learning Research",
      "year": 2010
    },
    {
      "authors": [
        "D. Wang",
        "P. Cui",
        "W. Zhu"
      ],
      "title": "Structural deep network embedding",
      "venue": "KDD",
      "year": 2016
    },
    {
      "authors": [
        "Z. Wang",
        "J. Zhang",
        "J. Feng",
        "Z. Chen"
      ],
      "title": "Knowledge graph embedding by translating on hyperplanes",
      "venue": "AAAI",
      "year": 2014
    },
    {
      "authors": [
        "J. Weston",
        "S. Bengio",
        "N. Usunier"
      ],
      "title": "Large scale image annotation: Learning to rank with joint word-image embeddings",
      "venue": "Journal of Machine Learning",
      "year": 2010
    },
    {
      "authors": [
        "J. Weston",
        "S. Bengio",
        "N. Usunier"
      ],
      "title": "Wsabie: Scaling up to large vocabulary image annotation",
      "venue": "IJCAI",
      "year": 2011
    },
    {
      "authors": [
        "Z. Zhou",
        "J. Feng"
      ],
      "title": "Deep forest: Towards an alternative to deep neural networks",
      "venue": "IJCAI",
      "year": 2017
    },
    {
      "authors": [
        "Z. Zhou",
        "J. Feng"
      ],
      "title": "Deep forest: Towards an alternative to deep neural networks",
      "venue": "IJCAI",
      "year": 2017
    },
    {
      "authors": [
        "Z. Zhou",
        "J. Wu",
        "W. Tang"
      ],
      "title": "Ensembling neural networks: Many could be better than all",
      "venue": "Artif. Intell.",
      "year": 2002
    }
  ],
  "sections": [
    {
      "text": "Index Terms\u2014Genetic Evolutionary Network; Deep Learning; Genetic Algorithm; Ensemble Learning; Representation Learning\nI. INTRODUCTION\nIn recent years, deep learning, a rebranding of deep neural network research works, has achieved a remarkable success. The essence of deep learning is to compute the hierarchical feature representations of the observational data [9], [16]. With multiple hidden layers, the deep learning models have the capacity to capture very good projections from the input data space to the objective output space, whose outstanding performance has been widely illustrated in various applications, including speech and audio processing [8], [12], language modeling and processing [1], [20], information retrieval [11], [23], objective recognition and computer vision [16], as well as multimodal and multi-task learning [28], [29]. By this context so far, various kinds of deep learning models have been proposed already, including deep belief network [13], deep Boltzmann machine [23], deep neural network [14], [15] and deep autoencoder model [25].\nMeanwhile, deep learning models also suffer from several serious criticism due to their several severe disadvantages [30]. Generally, learning and training deep learning models usually demands (1) a large amount of training data, (2) large and powerful computational facilities, (3) heavy parameter tuning costs, but lacks (4) theoretic explanation of the learning process and results. These disadvantages greatly hinder the application of deep learning models in many areas which cannot meet the requirements or requests a clear interpretability of the learning performance. Due to these reasons, by this context so far, deep learning research and application works are mostly carried out within/via the collaboration with several big technical companies, but the models proposed by them (involving hundreds of hidden layers, billions of parameters, and using a large cluster with thousands of server nodes [6]) can hardly be applied in other real-world applications.\nIn this paper, we propose a brand new model, namely SEGEN (Sample-Ensemble Genetic Evolutionary Network), which can work as an alternative approach to the deep learning models. Instead of building one single model with a deep architecture, SEGEN adopts a genetic-evolutionary learning strategy to train a group of unit models generations by generations. Here, the unit models can be either traditional machine learning models or deep learning models with a much \u201cnarrower\u201d and \u201cshallower\u201d structure. Each unit model will be trained with a batch of training instances sampled form the dataset. By selecting the good unit models from each generation (according to their performance on a validation set), SEGEN will evolve itself and create the next generation of unit modes with probabilistic genetic crossover and mutation, where the selection and crossover probabilities are highly dependent on their performance fitness evaluation. Finally, the learning results of the data instances will be effectively combined from each unit model via diffusive propagation and ensemble learning strategies. These terms and techniques mentioned here will be explained in great detail in Section III. Compared with the existing deep learning models, SEGEN have several great advantages, and we will illustrate them from both the bionics perspective and the computational perspective as follows.\nFrom the bionics perspective, SEGEN effectively models the evolution of creatures from generations to generations, where the creatures suitable for the environment will have a larger chance to survive and generate the offsprings. Mean-\nar X\niv :1\n80 3.\n08 63\n1v 2\n[ cs\n.N E\n] 5\nJ un\n2 01\n8\nwhile, the offsprings inheriting good genes from its parents will be likely to adapt to the environment as well. In the SEGEN model, each unit network model in generations can be treated as an independent creature, which will receive a different subsets of training instances and learn its own model variables. For the unit models suitable for the environment (i.e., achieving a good performance on a validation set), they will have a larger chance to generate their child models. The parent model achieving better performance will also have a greater chance to pass their variables to the child model.\nFrom the computational perspective, SEGEN requires far less data and resources, and also has a sound theoretic explanation of the learning process and results. The unit models in each generation of SEGEN are of a much simpler architecture, learning of which can be accomplished with much less training data, less computational resources and less hyper-parameter tuning efforts. In addition, the training dataset pool, model hyper-parameters are shared by the unit models, and the increase of generation size (i.e., unit model number in each generation) or generation number (i.e., how many generation rounds will be needed) will not increase the learning resources consumption. The relatively \u201cnarrower\u201d and \u201cshallower\u201d structure of unit models will also significantly enhance the interpretability of the unit models training process as well as the learning results, especially if the unit models are the traditional non-deep learning models. Furthermore, the sound theoretical foundations of genetic algorithm and ensemble learning will also help explain the information inheritance through generations and result ensemble in SEGEN.\nIn this paper, we will use network embedding problem [26], [3], [21] (applying autoencoder as the unit model) as an example to illustrate the SEGEN model. Meanwhile, applications of SEGEN on other data categories (e.g., images and raw feature inputs) with CNN and MLP as the unit model will also be provided in Section IV-D. The following parts of this paper are organized as follows. The problem formulation is provided in Section II. Model SEGEN will be introduced in Section III, whose performance will be evaluated in Section IV. Finally, Section V introduces the related works and we conclude this paper in Section VI."
    },
    {
      "heading": "II. PROBLEM FORMULATION",
      "text": "In this section, we will provide the definitions of several important terminologies, based on which we will define the network representation learning problem."
    },
    {
      "heading": "A. Terminology Definition",
      "text": "The SEGEN model will be illustrated based on the network representation learning problem in this paper, where the input is usually a large-sized network structured dataset.\nDEFINITION 1: (Network Data): Formally, a network structured dataset can be represented as a graph G = (V, E), where V denotes the node set and E contains the set of links among the nodes.\nIn the real-world applications, lots of data can be modeled as networks. For instance, online social media can be represented as a network involving users as the nodes and social\nconnections as the links; e-commerce website can be denoted as a network with customer and products as the nodes, and purchase relation as the links; academic bibliographical data can be modeled as a network containing papers, authors as the nodes, and write/cite relationships as the links. Given a largesized input network data G = (V, E), a group of sub-networks can be extracted from it, which can be formally represented as a sub-network set of G.\nDEFINITION 2: (Sub-network Set): Based on a certain sampling strategy, we can represent the set of sampled subnetworks from network G as set G = {g1, g2, \u00b7 \u00b7 \u00b7 , gm} of size m. Here, gi \u2208 G denotes a sub-network of G, and it can be represented as gi = (Vgi , Egi), where Vgi \u2286 V , Egi \u2286 E and G 6= gi.\nIn Section III, we will introduce several different sampling strategies, which will be applied to obtained several different sub-network pools for unit model building and validation."
    },
    {
      "heading": "B. Problem Formulation",
      "text": "Problem Statement: Based on the input network data G = (V, E), the network representation learning problem aims at learning a mapping f : V \u2192 Rd to project each node from the network to a low-dimensional feature space. There usually exist some requirements on mapping f(\u00b7), which should preserve the original network structure, i.e., closer nodes should have close representations; while disconnected nodes have different representations on the other hand."
    },
    {
      "heading": "III. PROPOSED METHODS",
      "text": "In this section, we will introduce the proposed framework SEGEN in detail. As shown in Figure 1, the proposed framework involves three steps: (1) network sampling, (2) sub-network representation learning, and (3) result ensemble. Given the large-scale input network data, framework SEGEN will sample a set of sub-networks, which will be used as the input to the genetic evolutionary network model for representation learning. Based on the learned results for the sub-networks, framework SEGEN will combine them together to obtain the final output result. In the following parts, we will introduce these three steps in great detail respectively."
    },
    {
      "heading": "A. Network Sampling",
      "text": "In framework SEGEN, instead of handling the input largescale network data directly, we propose to sample a subset (of set size s) of small-sized sub-networks (of a pre-specified subnetwork size k) instead and learn the representation feature vectors of nodes based on the sub-networks. To ensure the learned representations can effectively represent the characteristics of nodes, we need to ensure the sampled subnetworks share similar properties as the original large-sized input network. As shown in Figure 1, 5 different types of network sampling strategies (indicated in 5 different colors) are adopted in this paper, and each strategy will lead to a group of small-sized sub-networks, which can capture both the local and global structures of the original network.\n1) BFS based Network Sampling: Based on the input network G = (V, E), Breadth-First-Search (BFS) based network sampling strategy randomly picks a seed node from set V and performs BFS to expend to the unreached nodes. Formally, the neighbors of node v \u2208 V can be denoted as set \u0393(v; 1) = {u|u \u2208 V \u2227 (u, v) \u2208 E}. After picking v, the sampling strategy will continue to randomly add k \u2212 1 nodes from set \u0393(v; 1), if |\u0393(v; 1)| \u2265 k \u2212 1; otherwise, the sampling strategy will go to the 2-hop neighbors of v (i.e., \u0393(v; 2) = {u|\u2203w \u2208 V, (u,w) \u2208 E \u2227 (w, v) \u2208 E \u2227 (u, v) /\u2208 E}) and so forth until the remaining k \u2212 1 nodes are selected. In the case when the size of connected component that v involves in is smaller than k, the strategy will further pick another seed node to do BFS from that node to finish the sampling of k nodes. These sampled k nodes together with the edges among them will form a sampled sub-network g, and all the p sampled sub-networks will form the sub-network pool GBFS (parameter p denotes the pool size).\n2) DFS based Network Sampling: Depth-First-Search (DFS) based network sampling strategy works in a very similar way as the BFS based strategy, but it adopts DFS to expand to the unreached nodes instead. Similar to the BFS method, in the case when the node connected component has size less than k, DFS sampling strategy will also continue to pick another node as the seed node to continue the sampling process. The sampled nodes together with the links among them will form the sub-networks to be involved in the final sampled subnetwork pool GDFS (of size p).\nA remark to be added here: the sub-networks sampled via BFS can mainly capture the local network structure of nodes (i.e., the neighborhood), and in many of the cases they are star structured diagrams with the picked seed node at the center surrounded by its neighbors. Meanwhile, the sub-networks sampled with DFS are slightly different, which involve \u201cdeeper\u201d network connection patterns. In the extreme case, the sub-networks sampled via DFS can be a path from\nthe seed nodes to a node which is (k \u2212 1)-hop away. 3) HS based Network Sampling: To balance between those extreme cases aforementioned, we introduce a Hybrid-Search (HS) based network sampling strategy by combining BFS and DFS. HS randomly picks seed nodes from the network, and reaches other nodes based on either BFS or DFS strategies with probabilities p and (1 \u2212 p) respectively. For instance, in the sampling process, HS first picks node v \u2208 V as the seed node, and samples a random node u \u2208 \u0393(v; 1). To determine the next node to sample, HS will \u201ctoss a coin\u201d with p probability to sample nodes from \u0393(v; 1) \\ {u} (i.e., BFS) and 1\u2212p probability to sample nodes from \u0393(u; 1)\\{v} (i.e., DFS). Such a process continues until k nodes are selected, and the sampled nodes together with the links among them will form the sub-network. We can represent all the sampled sub-networks by the HS based network sampling strategy as pool GHS.\nThese three network sampling strategies are mainly based on the connections among the nodes, and nodes in the sampled sub-networks are mostly connected. However, in the realworld networks, the connections among nodes are usually very sparse, and most of the node pairs are not connected. In the following part, we will introduce two other sampling strategies to handle such a case.\n4) Biased Node Sampling: Instead of sampling subnetworks via the connections among them, the node sampling strategy picks the nodes at random from the network. Based on node sampling, the final sampled sub-network may not necessarily be connected and can involve many isolated nodes. Furthermore, uniform sampling of nodes will also deteriorate the network properties, since it treats all the nodes equally and fails to consider their differences. In this paper, we propose to adopt the biased node sampling strategy, where the nodes with more connections (i.e., larger degrees) will have larger probabilities to be sampled. Based on the connections among the nodes, we can represent the degree of node v \u2208 V as\nd(u) = |\u0393(u; 1)|, and the probabilities for u to be sampled can be denoted as p(u) = d(u)2|E| . Instead of focusing on the local structures of the network, the sub-networks sampled with the biased node sampling strategy can capture more \u201cglobal\u201d structures of the input network. Formally, all the sub-networks sampled via this strategy can be represented as pool GNS.\n5) Biased Edge Sampling: Another \u201cglobal\u201d sub-network sampling strategy is the edge based sampling strategy, which samples the edges instead of nodes. Here, uniform sampling of edges will be reduced to biased node selection, where highdegree nodes will have a larger probability to be involved in the sub-network. In this paper, we propose to adopt a biased edge sampling strategy instead. For each edge (u, v) \u2208 E , the probability for it to be sampled is actually proportional to d(u)+d(v)\n2|E| . The sampled edges together with the incident nodes will form a sub-network, and all the sampled sub-networks with biased edge sampling strategy can be denoted as pool GES.\nThese two network sampling strategies can select the substructures of the input network from a global perspective, which can effectively capture the sparsity property of the input network. In the experiments to be introduced in Section IV, we will evaluate these different sampling strategies in detail.\nB. GEN Model\nIn this part, we will focus on introducing the Genetic Evolutionary Network (GEN) model, which accepts each subnetwork pool as the input and learns the representation feature vectors of nodes as the output. We will use G to represent the sampled pool set, which can be GBFS, GDFS, GHS, GNS or GES respectively.\n1) Unit Model Population Initialization: In the GEN model, there exist multiple generations of unit models, where the earlier generations will evolve and generate the later generations. Each generation will also involve a group of unit models, namely the unit model population. Formally, the initial generation of the unit models (i.e., the 1st generation) can be represented as set M1 = {M11 ,M12 , \u00b7 \u00b7 \u00b7 ,M1m} (of size m), where M1i is a base unit model to be introduced in the following subsection. Formally, the variables involved in each unit model, e.g., M1i , can be denoted as vector \u03b8 1 i , which covers the weight and bias terms in the model (which will be treated as the model genes in the evolution to be introduced later). In the initialization step, the variables of each unit model are assigned with a random value generated from the standard normal distribution.\n2) Unit Model Description: In this paper, we will take network representation learning as an example, and propose to adopt the correlated autoencoder as the base model. We want to clarify again that the SEGEN framework is a general framework, and it works well for different types of data as well as different base models. For some other tasks or other learning settings, many other existing models, e.g., CNN and MLP to be introduced in Section IV-D, can be adopted as the base model as well.\nAutoencoder is an unsupervised neural network model, which projects data instances from the original feature space to a lower-dimensional feature space via a series of non-linear mappings. Autoencoder model involves two steps: encoder and decoder. The encoder part projects the original feature vectors to the objective feature space, while the decoder step recovers the latent feature representations to a reconstructed feature space.\nBased on each sampled sub-network g \u2208 T , where g = (Vg, Eg), we can represent the sub-network structure as an adjacency matrix Ag = {0, 1}|Vg|\u00d7|Vg|, where Ag(i, j) = 1 iff (vi, vj) \u2208 Eg . Formally, for each node vi \u2208 Vg , we can represent its raw feature as xi = Ag(i, :). Let y1i ,y 2 i , \u00b7 \u00b7 \u00b7 ,yoi be the corresponding latent feature representation of xi at hidden layers 1, 2, \u00b7 \u00b7 \u00b7 , o in the encoder step. The encoding result in the objective feature space can be denoted as zi \u2208 Rd of dimension d. In the decoder step, the input will be the latent feature vector zi, and the final output will be the reconstructed vector x\u0302i (of the same dimension as xi). The latent feature vectors at each hidden layers can be represented as y\u0302oi , y\u0302 o\u22121 i , \u00b7 \u00b7 \u00b7 , y\u03021i . As shown in the architecture in Figure 2, the relationships among these variables can be represented with the following equations:\n Encoder: y1i = \u03c3(W 1xi + b 1), yki = \u03c3(W kyk\u22121i + b k),\n\u2200k \u2208 {2, \u00b7 \u00b7 \u00b7 , o}, zi = \u03c3(W o+1yoi + b o+1).\n Decoder: y\u0302oi = \u03c3(W\u0302 o+1zi + b\u0302 o+1), y\u0302k\u22121i = \u03c3(W\u0302 ky\u0302ki + b\u0302 k),\n\u2200k \u2208 {2, \u00b7 \u00b7 \u00b7 , o}, x\u0302i = \u03c3(W\u0302 1y\u03021i + b\u0302 1).\nThe objective of traditional autoencoder model is to minimize the loss between the original feature vector xi and the reconstructed feature vector x\u0302i of data instances. Meanwhile, for the network representation learning task, the learning task of nodes in the sub-networks are not independent but highly correlated. For the connected nodes, they should have closer representation feature vectors in the latent feature space; while for those which are isolated, their latent representation feature vectors should be far away instead. What\u2019s more, since the input feature vectors are extremely sparse (lots of the entries are 0s), simply feeding them to the model may lead to some trivial solutions, like 0 vector for both zi and the decoded vector x\u0302i. Therefore, we propose to extend the Autoencoder\nmodel to the correlated scenario for networks, and define the objective of the correlated autoencoder model as follows:\nLe(g) = \u2211 vi\u2208Vg \u2016(xi \u2212 x\u0302i) bi\u201622 + \u03b1 \u2211 vi,vj\u2208Vg,vi 6=vj si,j \u2016zi \u2212 zj\u201622\n+ \u03b2 \u00b7 o\u2211 i=1 (\u2225\u2225Wi\u2225\u22252 F + \u2225\u2225\u2225W\u0302i\u2225\u2225\u22252 F ) ,\nwhere si,j = { +1, if Ag(i, j) = 1, \u22121, if Ag(i, j) = 0. and \u03b1, \u03b2 are the weights of the correlation and regularization terms respectively. Entries in weight vector bi have value 1 except the entries corresponding to non-zero element in xi, which will be assigned with value \u03b3 (\u03b3 > 1) to preserve these non-zero entries in the reconstructed vector x\u0302i.\n3) Generation Model Learning Setting: Instead of fitting each unit model with all the sub-networks in the pool G, in GEN, a set of sub-network training batches T1, T2, \u00b7 \u00b7 \u00b7 , Tm will be sampled for each unit model respectively in the learning process, where |Ti| = b,\u2200i \u2208 {1, 2, \u00b7 \u00b7 \u00b7 ,m} are of the pre-defined batch size b. These batches may share common sub-networks as well, i.e., Ti \u2229Tj may not necessary be \u2205. In the GEN model, the unit models learning process for each generation involves two steps: (1) generating the batches Ti from the pool set G for each unit model M1i \u2208 M1, and (2) learning the variables of the unit model M1i based on subnetworks in batch Ti. Considering that the unit models have a much smaller number of hidden layers, the learning time cost of each unit model will be much less than the deeper models on larger-sized networks. In Section IV, we will provide a more detailed analysis about the running time cost and space cost of SEGEN.\n4) Unit Model Fitness Evaluation and Selection: The unit models in the generation set M1 can have different performance, due to (1) different initial variable values, and (2) different training batches in the learning process. In framework SEGEN, instead of applying \u201cdeep\u201d models with multiple hidden layers, we propose to \u201cdeepen\u201d the models in another way: \u201cevolve the unit model into \u2018deeper\u2019 generations\u201d. A genetic algorithm style method is adopted here for evolving the unit models, in which the well-trained unit models will have a higher chance to survive and evolve to the next generation. To pick the well-trained unit models, we need to evaluate their performance, which is done with the validation set V sampled from the pool. For each unit model M1k \u2208 M1, based on the sub-networks in set V , we can represent the introduced loss of the model as\nLc(M1k ;V) = \u2211 g\u2208V \u2211 vi,vj\u2208Vg,vi 6=vj si,j \u2225\u2225z1k,i \u2212 z1k,j\u2225\u222522 ,\nwhere z1k,i and z 1 k,j denote the learned latent representation feature vectors of nodes vi, vj in the sampled sub-network g and si,j is defined based on g in the same way as introduced before.\nThe probability for each unit model to be picked as the parent model for the crossover and mutation operations can be represented as\np(M1k ) = exp\u2212L(M 1 k ;V)\u2211\nM1i \u2208M1 exp\u2212L(M 1 i ;V)\n.\nIn the real-world applications, a normalization of the loss terms among these unit models is necessary. For the unit model introducing a smaller loss, it will have a larger chance to be selected as the parent unit model. Considering that the crossover is usually done based a pair of parent models, we can represent the pairs of parent models selected from setM1 as P1 = {(M1i ,M1j )k}k\u2208{1,2,\u00b7\u00b7\u00b7 ,m}, based on which we will be able to generate the next generation of unit models, i.e., M2.\n5) Unit Model Crossover and Mutation: For the kth pair of parent unit model (M1i ,M 1 j )k \u2208 P1, we can denote their genes as their variables \u03b81i , \u03b8 1 j respectively (since the differences among the unit models mainly lie in their variables), which are actually their chromosomes for crossover and mutation. Crossover: In this paper, we propose to adopt the uniform crossover to get the chromosomes (i.e., the variables) of their child model. Considering that the parent models M1i and M1j can actually achieve different performance on the validation set V , in the crossover, the unit model achieving better performance should have a larger chance to pass its chromosomes to the child model.\nFormally, the chromosome inheritance probability for parent model M1i can be represented as\np(M1i ) = exp\u2212L(M 1 i ;V)\nexp\u2212L(M 1 i ;V) + exp\u2212L(M 1 j ;V)\nMeanwhile, the chromosome inheritance probability for model M1j can be denoted as p(M 1 j ) = 1\u2212 p(M1i ).\nIn the uniform crossover method, based on parent model pair (M1i ,M 1 j )k \u2208 P1, we can represent the obtained child model chromosome vector as \u03b82k \u2208 R|\u03b8 1| (the superscript denotes the 2nd generation and |\u03b81| denotes the variable length), which is generated from the chromosome vectors \u03b81i and \u03b81j of the parent models. Meanwhile, the crossover choice at each position of the chromosomes vector can be represented as a vector c \u2208 {i, j}|\u03b81|. The entries in vector c are randomly selected from values in {i, j} with a probability p(M1i ) to pick value i and a probability p(M1j ) to pick value j respectively. The lth entry of vector \u03b82k before mutation can be represented as\n\u03b8\u03022k(l) = 1 (c(l) = i) \u00b7 \u03b81i (l) + 1 (c(l) = j) \u00b7 \u03b81j (l),\nwhere indicator function 1(\u00b7) returns value 1 if the condition is True; otherwise, it returns value 0. Mutation: The variables in the chromosome vector \u03b8\u03022k(l) \u2208 R|\u03b81| are all real values, and some of them can be altered, which is also called mutation in traditional genetic algorithm. Mutation happens rarely, and the chromosome mutation probability is \u03b3 in the GEN model. Formally, we can represent the\nmutation indicator vector as m \u2208 {0, 1}d, and the lth entry of vector \u03b82k after mutation can be represented as\n\u03b82k(l) = 1 (m(l) = 0) \u00b7 \u03b8\u03022k(l) + 1 (c(l) = 1) \u00b7 rand(0, 1),\nwhere rand(0, 1) denotes a random value selected from range [0, 1]. Formally, the chromosome vector \u03b82k defines a new unit model with knowledge inherited form the parent models, which can be denoted as M2k . Based on the parent model set P1, we can represent all the newly generated models as M2 = {M2k}(M1i ,M1j )k\u2208P1 , which will form the 2nd generation of unit models."
    },
    {
      "heading": "C. Result Ensemble",
      "text": "Based on the models introduced in the previous subsection, in this part, we will introduce the hierarchical result ensemble method, which involves two steps: (1) local ensemble of results for the sub-networks on each sampling strategies, and (2) global ensemble of results obtained across different sampling strategies.\n1) Local Ensemble: Based on the sub-network pool G obtained via the sampling strategies introduced before, we have learned the Kth generation of the GEN model MK (or M for simplicity), which contains m unit models. In this part, we will introduce how to fuse the learned representations from each sub-networks with the unit models. Formally, given a sub-network g \u2208 G with node set Vg , by applying unit model Mj \u2208M to g, we can represent the learned representation for node vq \u2208 Vg as vector zj,q , where q denotes the unique node index in the original complete network G before sampling. For the nodes vp /\u2208 Vg , we can denote its representation vector zj,p = null, which denotes a dummy vector of length d. Formally, we will be able represent the learned representation feature vector for node vq as\nzq = \u2294\ng\u2208G,Mj\u2208M, zj,q, (1)\nwhere operator t denotes the concatenation operation of feature vectors.\nConsidering that in the network sampling step, not all nodes will be selected in sub-networks. For the nodes vp /\u2208 Vg,\u2200g \u2208 G, we will not be able to learn its representation feature vector (or its representation will be filled with a list of dummy empty vector). Formally, we can represent these non-appearing nodes as set Vn = V \\ \u22c3 g\u2208G Vg . In this paper, to compute the representation for these nodes, we propose to propagate the learned representation from their neighborhoods to them instead. Formally, given node vp \u2208 Vn and its neighbor set \u0393(vp) = {vo|vo \u2208 V \u2227 (u, vp) \u2208 E}, if there exists node in \u0393(vp) with non-empty representation feature vector, we can represent the propagated representation for vp as\nzp = 1\nN \u2211 vo\u2208\u0393(vp) 1(vo /\u2208 Vn) \u00b7 zo, (2)\nwhere N = \u2211 vo\u2208\u0393(vp) 1(vo /\u2208 Vn). In the case that \u0393(vp) \u2282 Vn, random padding will be applied to get the representation vector zp for node vp.\n2) Global Ensemble: Generally, these different network sampling strategies introduced at the beginning in Section III-A captures different local/global structures of the network, which will all be useful for the node representation learning. In the global result ensemble step, we propose to group these features together as the output.\nFormally, based on the BFS, DFS, HS, biased node and biased edge sampling strategies, to differentiate their learned representations for nodes (e.g., vq \u2208 V), we can denoted their representation feature vectors as zBFSq , z DFS q , z HS q , z NS q and z ES q respectively. In the case that node vq has never appeared in any sub-networks in any of the sampling strategies, its corresponding feature vector can be denoted as a dummy vector filled with 0s. In the global ensemble step, we propose to linearly sum the feature vectors to get the fuses representation z\u0304q as follows:\nz\u0304q = \u2211\ni\u2208{BFS,DFS,HS,NS,ES}\nwi \u00b7 ziq.\nLearning of the weight parameters wBFS, wDFS, wHS, wNS and wES is feasible with the complete network structure, but it may introduce extra time costs and greatly degrade the efficiency SEGEN. In this paper, we will simply assign them with equal value, i.e., z\u0304q is an average of zBFSq , z DFS q , z HS q , z NS q and z ES q learned with different sampling strategies."
    },
    {
      "heading": "D. Model Analysis",
      "text": "In this section, we will analyze the proposed model SEGEN regarding its performance, running time and space cost, which will also illustrate the advantages of SEGEN compared with the other existing deep learning models.\n1) Performance Analysis: Model SEGEN, in a certain sense, can also be called a \u201cdeep\u201d model. Instead of stacking multiple hidden layers inside one single model like existing deep learning models, SEGEN is deep since the unit models in the successive generations are generated by a namely \u201cevolutionary layer\u201d which performs the validation, selection, crossover, and mutation operations connecting these generations. Between the generations, these \u201cevolutionary operations\u201d mainly work on the unit model variables, which allows the immigration of learned knowledge from generation to generation. In addition, via these generations, the last generation in SEGEN can also capture the overall patterns of the dataset. Since the unit models in different generations are built with different sampled training batches, as more generations are involved, the dataset will be samples thoroughly for learning SEGEN. There have been lots of research works done on analyzing the convergence, performance bounds of genetic algorithms [22], which can provide the theoretic foundations for SEGEN.\nDue to the difference in parent model selection, crossover, mutation operations and different sampled training batches, the unit models in the generations of SEGEN may perform quite differently. In the last step, SEGEN will effectively combine the learning results from the multiple unit models together. With the diverse results combined from these different learning\nmodels, SEGEN is able to achieve better performance than each of the unit models, which have been effectively demonstrated in [32].\n2) Space and Time Complexity Analysis: According the the model descriptions provided in Section III, we summarize the key parameters used in SEGEN as follows, which will help analyze its space and time complexity. \u2022 Sampling: Original data size: n. Sub-instance size: n\u2032.\nPool size: p. \u2022 Learning: Generation number: K. Population size: m.\nFeature vector size: d. Training/Validation batch size: b. Here, we will use network structured data as an example to analyze the space and time complexity of the SEGEN model. Space Complexity: Given a large-scale network with n nodes, the space cost required for storing the whole network in a matrix representation is O(n2). Meanwhile, via network sampling, we can obtain a pool of sub-networks, and the space required for storing these sub-networks takes O ( p(n\u2032)2 ) . Generally, in application of SEGEN, n\u2032 can take very small number, e.g., 50, and p can take value p = c \u00b7 nn\u2032 (c is a constant) so as to cover all the nodes in the network. In such a case, the space cost of SEGEN will be linear to n, O(cn\u2032n), which is much smaller than O(n2). Time Complexity: Depending on the specific unit models used in composing SEGEN, we can represent the introduced time complexity of learn one unit model with the original network with n nodes as O(f(n)), where f(n) is usually a high-order function. Meanwhile, for learning SEGEN on the sampled sub-networks with n\u2032 nodes, all the introduced time cost will be O (Km(b \u00b7 f(n\u2032) + d \u00b7 n\u2032)), where term d \u00b7 n\u2032 (an approximation to variable size) represents the cost introduced in the unit model crossover and mutation about the model variables. Here, by assigning b with a fixed value b = c \u00b7 nn\u2032 , the time complexity of SEGEN will be reduced to O ( Kmc f(n\n\u2032) n\u2032 \u00b7 n+Kmdn\n\u2032 )\n, which is linear to n. 3) Advantages Over Deep Learning Models: Compared with existing deep learning models based on the whole dataset, the advantages of SEGEN are summarized below: \u2022 Less Data for Unit Model Learning: For each unit model,\nwhich are of a \u201cshallow\u201d and \u201cnarrow\u201d structure (shallow: less or even no hidden layers, narrow: based on sampled sub-instances with a much smaller size), which needs far less variables and less data for learning each unit model. \u2022 Less Computational Resources: Each unit model is of a much simpler structure, learning process of which consumes far less computational resources in both time and space costs. \u2022 Less Parameter Tuning: SEGEN can accept both deep (in a simpler version) and shallow learning models as the unit model, and the hyper-parameters can also be shared among the unit models, which will lead to far less hyperparameters to tune in the learning process. \u2022 Sound Theoretic Explanation: The unit learning model, genetic algorithm and ensemble learning (aforementioned) can all provide the theoretic foundation for\nSEGEN, which will lead to sound theoretic explanation of both the learning result and the SEGEN model itself."
    },
    {
      "heading": "IV. EXPERIMENTS",
      "text": "To test the effectiveness of the proposed model, extensive experiments will be done on several real-world network structured datasets, including social networks, images and raw feature representation datasets. In this section, we will first introduce the detailed experimental settings, covering experimental setups, comparison methods, evaluation tasks and metrics for the social network representation learning task. After that, we will show its convergence analysis, parameter analysis and the main experimental results of SEGEN on the social network datasets. Finally, we will provide the experiments SEGEN based on the image and raw feature representation datasets involving CNN and MLP as the unit models respectively."
    },
    {
      "heading": "A. Social Network Dataset Experimental Settings",
      "text": "1) Experimental Setup: The network datasets used in the experiments are crawled from two different online social networks, Twitter and Foursquare, respectively. The Twitter network dataset involves 5, 120 users and 130, 576 social connections among the user nodes. Meanwhile, the Foursquare network dataset contains 5, 392 users together with the 55, 926 social links connecting them. According to the descriptions of SEGEN, based on the complete input network datasets, a set of sub-networks are randomly sampled with network sampling strategies introduced in this paper, where the sub-network size is denoted as n\u2032, and the pool size is controlled by p. Based on the training/validation batches sampled sub-network pool, K generations of unit models will be built in SEGEN, where each generation involves m unit models (convergence analysis regarding parameter K is available in Section IV-B). Finally, the learning results at the ending generation will be effectively combined to generate the ensemble output. For the nodes which have never been sampled in any sub-networks, their representations can be learned with the diffusive propagation from their neighbor nodes introduced in this paper. The learned results by SEGEN will be evaluated with two application tasks, i.e., network recovery and community detection respectively. The detailed parameters sensitivity analysis is also available in Section IV-B.\n2) Comparison Methods: The network representation learning comparison models used in this paper are listed as follows\n\u2022 SEGEN: Model SEGEN proposed in this paper is based on the genetic algorithm and ensemble learning, which effectively combines the learned sub-network representation feature vectors from the unit models to generate the feature vectors of the whole network. \u2022 LINE: The LINE model is a scalable network embedding model proposed in [24], which optimizes an objective function that preserves both the local and global network structures. LINE uses a edge-sampling algorithm to addresses the limitation of the classical stochastic gradient descent. \u2022 DEEPWALK: The DEEPWALK model [21] extends the word2vec model [19] to the network embedding scenario. DEEPWALK uses local information obtained from trun-\ncated random walks to learn latent representations. \u2022 NODE2VEC: The NODE2VEC model [10] introduces a\nflexible notion of a node\u2019s network neighborhood and design a biased random walk procedure to sample the neighbors for node representation learning. \u2022 HPE: The HPE model [4] is originally proposed for learning user preference in recommendation problems, which can effectively project the information from heterogeneous networks to a low-dimensional space.\n3) Evaluation Tasks and Metrics: The network representation learning results can hardly be evaluated directly, whose evaluations are usually based on certain application tasks. In this paper, we propose to use application tasks, network recovery and clustering, to evaluate the learned representation features from the comparison methods. Furthermore, the network recovery results are evaluated by metrics, like AUC and\nPrecision@500. Meanwhile the clustering results are evaluated by Density and Silhouette.\n4) Default Parameter Setting: Without specific remarks, the default parameter setting for SEGEN in the experiments will be Parameter Setting 1 (PS1): sub-network size: 10, pool size: 200, batch size: 10, generation unit model number: 10, generation number: 30."
    },
    {
      "heading": "B. Social Network Dataset Experimental Analysis",
      "text": "In this part, we will provide experimental analysis about the convergence and parameters of SEGEN, including the subnetwork size, the pool size, batch size and generation size respectively.\n1) Convergence Analysis: The learning process of SEGEN involves multiple generations. Before showing the experimental results, we will analyze how many generations will be required for achieving stable results. In Figure 3, we provide the introduced loss by the SEGEN on both Foursquare and Twitter networks, where the x axis denotes the generations\nand y axis represents the sum of introduced Lc loss on the validation set based on all these 5 different sampling strategies. According to the results, model SEGEN can converge within less 30 generations for the network representation learning on both Foursquare and Twitter, which will be used as the maxgeneration number throughout the following experiments.\n2) Pool Sampling Parameter Analysis: In Figure 4, we show the sensitivity analysis about the network sampling parameters, i.e., sub-network size and the pool size, evaluated by AUC, Prec@500, Density and Silhouette respectively, where Figures 4(a)-4(d) are about the Foursquare and Figures 4(e)4(h) are about the Twitter network. The sub-network size parameter changes with values in {5, 10, 15, \u00b7 \u00b7 \u00b7 , 50} and pool size changes with values in range {100, 200, \u00b7 \u00b7 \u00b7 , 1000}.\nAccording to the plots, for the Foursquare network, larger sub-network size and larger pool size will lead to better performance in the network recovery task; meanwhile, smaller sub-network size will achiver better performance for the community detection task. For instance, SEGEN can achieve\nthe best performance with sub-network size 50 and pool size 600 for the network recovery task; and SEGEN obtain the best performance with sub-network size 25 and pool size 300 for the community detection. For the Twitter network, the performance of SEGEN is relatively stable for the parameters analyzed, which has some fluctuations for certain parameter values. According to the results, the optimal sub-network and pool sizes parameter values for the network recovery task are 50 and 700 for the network recovery task; meanwhile, for the community detection task, the optimal parameter values are 45 and 500 respectively.\n3) Model Learning Parameter Analysis: In Figure 5, we provide the parameter sensitivity analysis about the batch size and generation size (i.e., the number of unit models in each generation) on Foursquare and Twitter. We change the generation size and batch size both with values in {5, 10, 15, \u00b7 \u00b7 \u00b7 , 50}, and compute the AUC, Prec@500, Density and Silhouette scores obtained by SEGEN.\nAccording Figures 5(a)-5(d), batch size has no significant impact on the performance of SEGEN, and the generation size may affect SEGEN greatly, especially for the Prec@500 metric (the AUC obtained by SEGEN changes within range [0.81, 0.82] with actually minor fluctuation in terms of the values). The selected optimal parameter values selected for network recovery are 50 and 5 for generation and bath sizes. Meanwhile, for the community detection, SEGEN performs the best with smaller generation and batch size, whose optimal values are 5 and 35 respectively. For the Twitter network, the impact of the batch size and generation size is different from that on Foursquare: smaller generation size lead to better performance for SEGEN evaluated by Prec@500. The fluctuation in terms of AUC is also minor in terms of the values, and the optimal values of the generation size and batch size parameters for the network recovery task are 5 and 10 respectively. For the community detection task on Twitter, we select generation size 5 and batch size 40 as the optimal value."
    },
    {
      "heading": "C. Social Network Dataset Experimental Results",
      "text": "Based on the above parameter analysis, we provide the performance analysis of SEGEN and baseline methods in Tables I-II, where the parameter settings are specified next to the method name. We provide the rank of method performance among all the methods, which are denoted by the numbers in blue font, and the top 5 results are in a bolded font. As shown in the Tables, we have the network recovery and community detection results on the left and right sections respectively. For the network recovery task, we change the ratio of negative links compared with positive links with values {1, 5, 10}, which are evaluated by the metrics AUC and Prec@500. For the community detection task, we change the number of clusters with values {5, 25, 50}, and the results are evaluated by the metrics Density and Silhouette.\nBesides PS1 introduced at the beginning of Section IV-A, we have 4 other parameter settings selected based on the parameter analysis introduced before. PS2 for network recovery on Foursquare: sub-network size 50, pool size 600, batch\nsize 5, generation size 50. PS3 for community detection on Foursquare: sub-network size 25, pool size 300, batch size 35, generation size 5. PS4 for network recovery on Twitter: sub-network size 50, pool size 700, batch size 10, generation size 5. PS5 for community detection on Twitter: sub-network size 45, pool size 500, batch size 50, generation size 5.\nAccording to the results shown in Table I, method SEGEN with PS2 can obtain very good performance for both the network recovery task and the community detection task. For instance, for the network recovery task, method SEGEN with PS2 achieves 0.909 AUC score, which ranks the second and only lose to SEGEN-HS with PS2; meanwhile, SEGEN with PS2 also achieves the second highest Prec@500 score (i.e., 0.872 for np-ratio = 1) and the third highest Prec@500 score (i.e., 0.642 and 0.530 for np-ratios 5 and 10) among the comparison methods. On the other hand, for the community detection task, SEGEN with PS3 can generally rank the second/third among the comparison methods for both density and silhouette evaluation metrics. For instance, with the cluster number is 5, the density obtained by SEGEN ranks the second among the methods, which loses to SEGEN-LS only. Similar results can be observed for the Twitter network as shown in\nFigure II.\nBy comparing SEGEN with SEGEN merely based on HS, BFS, DFS, NS, LS, we observe that the variants based on one certain type of sampling strategies can obtain relatively biased performance, i.e., good performance for the network recovery task but bad performance for the community detection task or the reverse. For instance, as shown in Figure I, methods SEGEN with HS, BFS, DFS performs very good for the network recovery task, but its performance for the community detection ranks even after LINE, HPE and DEEPWALK. On the other hand, SEGEN with NS and LS is shown to perform well for the community detection task instead in Figure I, those performance ranks around 7 for the network recovery task. For the Twitter network, similar biased results can be observed but the results are not identically the same. Model SEGEN combining these different sampling strategies together achieves relatively balanced and stable performance for different tasks. Compared with the baseline methods LINE, HPE, DEEPWALK and NODE2VEC, model SEGEN can obtain much better performance, which also demonstrate the effectiveness of SEGEN as an alternative approach for deep learning models on network representation learning."
    },
    {
      "heading": "D. Experiments on Other Datasets and Unit Models",
      "text": "Besides the extended autoencoder model and the social network datasets, we have also tested the effectiveness of SEGEN on other datasets and with other unit models.\nIn Table III, we show the experimental results of SEGEN and other baseline methods on the MNIST hand-written image datasets. The dataset contains 60, 000 training instances and 10, 000 testing instances, where each instance is a 28 \u00d7 28 image with labels denoting their corresponding numbers. Convolutional Neural Network (CNN) is used as the unit model in SEGEN, which involves 2 convolutional layers, 2 max-\npooling layers, and two fully connection layers (with a 0.2 dropout rate). ReLU is used as the activation function in CNN, and we adopt Adam as the optimization algorithm. Here, the images are of a small size and no sampling is performed, while the learning results of the best unit model in the ending generation (based on a validation batch) will be outputted as the final results. In the experiments, SEGEN (CNN) is compared with several classic methods (e.g., LeNet5, SVM, Random Forest, Deep Belief Net) and state-of-the-art method (gcForest). According to the results, SEGEN (CNN) can outperform the baseline methods with great advantages. The Accuracy rate obtained by SEGEN is 99.37%, which is much higher than the other comparison methods.\nMeanwhile, in Table IV, we provide the learning results on three other benchmark datasets, including YEAST1, ADULT2 and LETTER3. These three datasets are in the traditional feature representations. Multi-Layer Perceptron (MLP) is used as the unit model in SEGEN for these three datasets. We cannot find one unified architecture of MLP, which works for all these three datasets. In the experiments, for the YEAST dataset, the MLP involves 1 input layer, 2 hidden layers and 1 output layers, whose neuron numbers are 8-64-16-10; for the ADULT, the MLP architecture contains the neurons 14- 70-50-2; for the LETTER dataset, the used MLP has 3 hidden layers with neurons 16-64-48-32-26 at each layer respectively. The Adam optimization algorithm with 0.001 learning rate is used to train the MLP model. For the ensemble strategy in these experiments, the best unit model is selected to generate the final prediction output. According to the results, compared with the baseline methods, SEGEN (MLP) can also perform very well with MLP on the raw feature representation datasets with great advantages, especially the YEAST and ADULT datasets. As to the LETTER dataset, SEGEN (MLP) only loses to gcForest, but can outperform the other methods consistently."
    },
    {
      "heading": "V. RELATED WORK",
      "text": "Deep Learning Research and Applications: The essence of deep learning is to compute hierarchical features or representations of the observational data [9], [16]. With the surge of deep learning research and applications in recent years, lots of research works have appeared to apply the deep learning methods, like deep belief network [13], deep\n1https://archive.ics.uci.edu/ml/datasets/Yeast 2https://archive.ics.uci.edu/ml/datasets/adult 3https://archive.ics.uci.edu/ml/datasets/letter+recognition\nBoltzmann machine [23], Deep neural network [14], [15] and Deep autoencoder model [25], in various applications, like speech and audio processing [8], [12], language modeling and processing [1], [20], information retrieval [11], [23], objective recognition and computer vision [16], as well as multimodal and multi-task learning [28], [29]. Network Embedding: Network embedding has become a very hot research problem recently, which can project a graphstructured data to the feature vector representations. In graphs, the relation can be treated as a translation of the entities, and many translation based embedding models have been proposed, like TransE [2], TransH [27] and TransR [18]. In recent years, many network embedding works based on random walk model and deep learning models have been introduced, like Deepwalk [21], LINE [24], node2vec [10], HNE [3] and DNE [26]. Perozzi et al. extends the word2vec model [19] to the network scenario and introduce the Deepwalk algorithm [21]. Tang et al. [24] propose to embed the networks with LINE algorithm, which can preserve both the local and global network structures. Grover et al. [10] introduce a flexible notion of a node\u2019s network neighborhood and design a biased random walk procedure to sample the neighbors. Chang et al. [3] learn the embedding of networks involving text and image information. Chen et al. [5] introduce a task guided embedding model to learn the representations for the author identification problem."
    },
    {
      "heading": "VI. CONCLUSION",
      "text": "In this paper, we have introduced an alternative approach to deep learning models, namely SEGEN. Significantly different from the existing deep learning models, SEGEN builds a group of unit models generations by generations, instead of building one single model with extremely deep architectures. The choice of unit models covered in SEGEN can be either traditional machine learning models or the latest deep learning models with a \u201csmaller\u201d and \u201cnarrower\u201d architecture. SEGEN has great advantages over deep learning models, since it requires much less training data, computational resources, parameter tuning efforts but provides more information about its learning and result integration process. The effectiveness of efficiency of SEGEN have been well demonstrated with the extensive experiments done on the real-world network structured datasets."
    }
  ],
  "title": "seGEN: Sample-Ensemble Genetic Evolutionary Network Model",
  "year": 2018
}
