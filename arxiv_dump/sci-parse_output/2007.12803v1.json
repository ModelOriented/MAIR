{"abstractText": "Human collaborators can effectively communicate with their partners to finish a common task by inferring each other\u2019s mental states (e.g., goals, beliefs, and desires). Such mind-aware communication minimizes the discrepancy among collaborators\u2019 mental states, and is crucial to the success in human ad-hoc teaming. We believe that robots collaborating with human users should demonstrate similar pedagogic behavior. Thus, in this paper, we propose a novel explainable AI (XAI) framework for achieving human-like communication in humanrobot collaborations, where the robot builds a hierarchical mind model of the human user and generates explanations of its own mind as a form of communications based on its online Bayesian inference of the user\u2019s mental state. To evaluate our framework, we conduct a user study on a real-time human-robot cooking task. Experimental results show that the generated explanations of our approach significantly improves the collaboration performance and user perception of the robot. Code and video demos are available on our project website: https://xfgao.github.io/xCookingWeb/.", "authors": [{"affiliations": [], "name": "Xiaofeng Gao"}, {"affiliations": [], "name": "Ran Gong"}, {"affiliations": [], "name": "Yizhou Zhao"}, {"affiliations": [], "name": "Shu Wang"}, {"affiliations": [], "name": "Tianmin Shu"}, {"affiliations": [], "name": "Song-Chun Zhu"}], "id": "SP:768ef11eed6bb2593f3ec0ad0be27e472836e27a", "references": [{"authors": ["S. Levine", "C. Finn", "T. Darrell", "P. Abbeel"], "title": "End-to-end training of deep visuomotor policies", "venue": "The Journal of Machine Learning Research, vol. 17, no. 1, pp. 1334\u20131373, 2016.", "year": 2016}, {"authors": ["T. Bansal", "J. Pachocki", "S. Sidor", "I. Sutskever", "I. Mordatch"], "title": "Emergent complexity via multi-agent competition", "venue": "arXiv preprint arXiv:1710.03748, 2017.", "year": 2017}, {"authors": ["D. Gunning"], "title": "Explainable artificial intelligence (xai)", "venue": "Defense Advanced Research Projects Agency (DARPA), nd Web, vol. 2, 2017.", "year": 2017}, {"authors": ["D. Premack", "G. Woodruff"], "title": "Does the chimpanzee have a theory of mind?", "venue": "Behavioral and brain sciences,", "year": 1978}, {"authors": ["C. Liu", "J.B. Hamrick", "J.F. Fisac", "A.D. Dragan", "J.K. Hedrick", "S.S. Sastry", "T.L. Griffiths"], "title": "Goal inference improves objective and perceived performance in human-robot collaboration", "venue": "Proceedings of the 2016 international conference on autonomous agents & multiagent systems. International Foundation for Autonomous Agents and Multiagent Systems, 2016, pp. 940\u2013948.", "year": 2016}, {"authors": ["D. Hadfield-Menell", "S.J. Russell", "P. Abbeel", "A. Dragan"], "title": "Cooperative inverse reinforcement learning", "venue": "Advances in neural information processing systems, 2016, pp. 3909\u20133917.", "year": 2016}, {"authors": ["K.P. Hawkins", "S. Bansal", "N.N. Vo", "A.F. Bobick"], "title": "Anticipating human actions for collaboration in the presence of task and sensor uncertainty", "venue": "2014 IEEE international conference on robotics and automation (ICRA). IEEE, 2014, pp. 2215\u20132222.", "year": 2014}, {"authors": ["S. Reddy", "A. Dragan", "S. Levine"], "title": "Where do you think you\u2019re going?: Inferring beliefs about dynamics from behavior", "venue": "Advances in Neural Information Processing Systems, 2018, pp. 1454\u20131465.", "year": 2018}, {"authors": ["P. Langley", "B. Meadows", "M. Sridharan", "D. Choi"], "title": "Explainable agency for intelligent autonomous systems", "venue": "Twenty-Ninth IAAI Conference, 2017.", "year": 2017}, {"authors": ["S. Anjomshoae", "A. Najjar", "D. Calvaresi", "K. Fr\u00e4mling"], "title": "Explainable agents and robots: Results from a systematic literature review", "venue": "Proceedings of the 18th International Conference on Autonomous Agents and MultiAgent Systems. International Foundation for Autonomous Agents and Multiagent Systems, 2019, pp. 1078\u20131088.", "year": 2019}, {"authors": ["T. Miller"], "title": "Explanation in artificial intelligence: Insights from the social sciences", "venue": "Artificial Intelligence, vol. 267, pp. 1\u201338, 2019.", "year": 2019}, {"authors": ["O. Struckmeier", "M. Racca", "V. Kyrki"], "title": "Autonomous generation of robust and focused explanations for robot policies", "venue": "2019 28th IEEE International Conference on Robot and Human Interactive Communication (RO-MAN). IEEE, 2019, pp. 1\u20138.", "year": 2019}, {"authors": ["N. Wang", "D.V. Pynadath", "S.G. Hill"], "title": "Trust calibration within a human-robot team: Comparing automatically generated explanations", "venue": "The Eleventh ACM/IEEE International Conference on Human Robot Interaction. IEEE Press, 2016, pp. 109\u2013116.", "year": 2016}, {"authors": ["A. Xu", "G. Dudek"], "title": "Optimo: Online probabilistic trust inference model for asymmetric human-robot collaborations", "venue": "2015 10th ACM/IEEE International Conference on Human-Robot Interaction (HRI). IEEE, 2015, pp. 221\u2013228.", "year": 2015}, {"authors": ["A. Dragan", "S. Srinivasa"], "title": "Generating legible motion", "venue": "2013.", "year": 2013}, {"authors": ["M. Kwon", "S.H. Huang", "A.D. Dragan"], "title": "Expressing robot incapability", "venue": "Proceedings of the 2018 ACM/IEEE International Conference on Human-Robot Interaction, 2018, pp. 87\u201395.", "year": 2018}, {"authors": ["Y. Zhang", "S. Sreedharan", "A. Kulkarni", "T. Chakraborti", "H.H. Zhuo", "S. Kambhampati"], "title": "Plan explicability and predictability for robot task planning", "venue": "2017 IEEE international conference on robotics and automation (ICRA). IEEE, 2017, pp. 1313\u20131320.", "year": 2017}, {"authors": ["A. Tabrez", "S. Agrawal", "B. Hayes"], "title": "Explanation-based reward coaching to improve human performance via reinforcement learning", "venue": "2019 14th ACM/IEEE International Conference on Human-Robot Interaction (HRI). IEEE, 2019, pp. 249\u2013257.", "year": 2019}, {"authors": ["Z. Gong", "Y. Zhang"], "title": "Behavior explanation as intention signaling in human-robot teaming", "venue": "2018 27th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN). IEEE, 2018, pp. 1005\u20131011.", "year": 2018}, {"authors": ["S. Sreedharan", "S. Srivastava", "S. Kambhampati"], "title": "Hierarchical expertise level modeling for user specific contrastive explanations.", "venue": "in IJCAI,", "year": 2018}, {"authors": ["S. Nikolaidis", "M. Kwon", "J. Forlizzi", "S. Srinivasa"], "title": "Planning with verbal communication for human-robot collaboration", "venue": "ACM Transactions on Human-Robot Interaction (THRI), vol. 7, no. 3, pp. 1\u201321, 2018.", "year": 2018}, {"authors": ["S. Devin", "R. Alami"], "title": "An implemented theory of mind to improve human-robot shared plans execution", "venue": "2016 11th ACM/IEEE International Conference on Human-Robot Interaction (HRI). IEEE, 2016, pp. 319\u2013326.", "year": 2016}, {"authors": ["P. Stone", "G.A. Kaminka", "S. Kraus", "J.S. Rosenschein"], "title": "Ad hoc autonomous agent teams: Collaboration without pre-coordination", "venue": "Twenty-Fourth AAAI Conference on Artificial Intelligence, 2010.", "year": 2010}, {"authors": ["C. Xiong", "N. Shukla", "W. Xiong", "S.-C. Zhu"], "title": "Robot learning with a spatial, temporal, and causal and-or graph", "venue": "2016 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2016, pp. 2144\u20132151.", "year": 2016}, {"authors": ["T. Shu", "X. Gao", "M.S. Ryoo", "S.-C. Zhu"], "title": "Learning social affordance grammar from videos: Transferring human interactions to human-robot interactions", "venue": "2017 IEEE international conference on robotics and automation (ICRA). IEEE, 2017, pp. 1669\u20131676.", "year": 2017}, {"authors": ["H. Liu", "Y. Zhang", "W. Si", "X. Xie", "Y. Zhu", "S.-C. Zhu"], "title": "Interactive robot knowledge patching using augmented reality", "venue": "2018 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2018, pp. 1947\u20131954.", "year": 2018}, {"authors": ["K. Tu", "M. Pavlovskaia", "S.-C. Zhu"], "title": "Unsupervised structure learning of stochastic and-or grammars", "venue": "Advances in neural information processing systems, 2013, pp. 1322\u20131330.", "year": 2013}, {"authors": ["T. Shu", "D. Xie", "B. Rothrock", "S. Todorovic", "S.-C. Zhu"], "title": "Joint inference of groups, events and human roles in aerial videos", "venue": "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 4576\u20134584.", "year": 2015}, {"authors": ["M.A. Carreira-Perpinan", "G.E. Hinton"], "title": "On contrastive divergence learning.", "venue": "in Aistats,", "year": 2005}], "sections": [{"text": "I. INTRODUCTION\nIn recent years, there has been a great amount of success on building powerful artificial intelligence (AI) systems to solve complex tasks [1], [2]. As highly autonomous robots are being developed, there is a growing need to make them quickly understood to avoid consequences caused by misunderstanding [3]. However, existing robot systems are often not human compatible \u2013 i) they do not understand humans\u2019 minds and ii) they are just black boxes to humans too. Such limits prevent the AI systems from working with humans effectively.\nInspired by studies on the Theory-of-Mind [4], [5], we believe that a crucial step towards building human compatible systems, particularly for human-robot collaborations, is to understand human activities and their underlying mental state. As a motivating example, consider a robot chef helping a human make salads in the kitchen shown in Figure 1. Even when the robot understands how to perform the task on its own, it would be challenging to finish the task efficiently without having a shared mental model with its human partner. For making the salad, the robot believes the plate should be picked up by the user while the human agent believes the other way. If the robot can identify such discrepancies between different agents\u2019 mental states, it can generate explanations to mitigate the differences and encourage the correction of sub-optimal human behavior.\n1 Center for Vision, Cognition, Learning, and Autonomy, UCLA. Emails: {xfgao, nikepupu, yizhouzhao, shuwang0712}@ucla.edu, sczhu@stat.ucla.edu.\n2 Massachusetts Institute of Technology. Email: tshu@mit.edu. * Equal contributions\nTo this end, we propose a framework that improves human-robot teaming performance through explanations. With a graph-based representation, the robot can maintain the mental states of both team members during a highlystructured collaborative task. The robot can then generate explanations when difference between mental states is detected, which implies sub-optimal user behaviors. In summary, the main contribution of this paper is three-fold: \u2022 We design a real-time collaborative cooking game as\nan online user study system and develop an evaluation protocol, which can be accessed from our website. \u2022 We propose to understand complex human activities using an action parsing algorithm based on an And-Or graph task representation, which allows the robot to infer human mental states in complex environments. \u2022 Based on the inferred human mental state, we propose an explanation generation framework. Experiments on a real-time cooking task show that our approach successfully improves user perception of the robot and leads to better human-robot collaborations."}, {"heading": "II. RELATED WORK", "text": "Human-aware planning. Designing robots that can work with humans has been widely studied by researchers. Most of the prior works hope to create robots to better understand and adapt to human collaborators. [6] evaluates a collaborative task allocation framework based on a Bayesian inference of human intention. [7] proposes a formulation of the value alignment problem assuming the robot learning\nar X\niv :2\n00 7.\n12 80\n3v 1\n[ cs\n.A I]\n2 4\nJu l 2\n02 0\nan unknown human reward function. Optimal solutions can be achieved when the human demonstrates active teaching behavior. To deal with sensor uncertainty and task ambiguity in a collaborative assembly task, [8] uses an And-Or tree structure as the task representation, which is similar to our approach. When sub-optimal user behavior are encountered, [9] proposes to learn the incorrect human internal dynamics model via inverse RL and then perform an internal-toreal dynamics transfer to assist users in shared-autonomy tasks. Our framework differs from this line of research in that we also aim at improving humans\u2019 understanding of robots\u2019 models using communicative actions. Such two-way understanding will further help human-robot collaborations.\nGoal-driven explainable AI. In contrast to data-driven XAI which improves understanding of \u201dblack-box\u201d machine learning algorithms given input data, goal-directed XAI typically explains the behavior of an agent or robot for a specific task [10], [11], [12], in order to increase model transparency [13], human\u2019s trust [14] or task performance [15]. Some of the works achieve this aim by enabling robots to directly generate easy-to-understand motions [16], [17] or task plans [18]. Other works, similar to ours, focus on using explicit communication to change user mental state, e.g., updating users\u2019 incorrect reward functions [19], correcting users\u2019 false belief or misunderstanding about the environment [20], [21], resolving the disagreement between collaborators\u2019 actions [22] or providing users with necessary knowledge about the current situation [23]. Compared to these work that often require offline training with humans or theoretical assumptions on the human models, this paper takes a direct approach to generate explanations solely based on an online estimation of human model and knowledge of the task structure. The experiment results show our approach is empirically effective in an ad-hoc human-robot teaming settings [24] where pre-coordination is not available."}, {"heading": "III. SINGLE AGENT MIND MODEL", "text": "And-Or graphs (AoGs) have been widely used for robot task planning [25], [26], [27] and human activity modeling [28], [29]. As a hierarchical representation, a spatialtemporal-causal And-Or graph (STC-AoG) encodes a joint task plan and corresponding spatial, temporal, and causal relations an agent could have about the task [25]. In this work, we propose to use a STC-AoG as a unified representation of a robot\u2019s knowledge and plan regarding the task as well as the inferred human\u2019s knowledge and plan. An example of a single-agent plan for making salad is in Figure 2."}, {"heading": "A. STC-AoG as a Hierarchical Mind Model", "text": "In general, an And-Or Graph consists of nodes and edges. The set of nodes includes Or node, And node, and Terminal node. Each Or node specifies the Or relation: only one of its children nodes would be performed at a given time. An And node represents the And relation and is composed of several children nodes. Each Terminal node represents a set of entities that cannot be further decomposed. The edge represents the top-down sampling process from a parent node to its children nodes. The root node of the And-Or tree is always an And node connected to a set of And/Or nodes. Each And-node represents a sub-task which can be further decomposed into a series of sub-tasks or atomic actions.\nIn this paper, the graph G =< A,F, T, V,R, P > is formally defined as the following: \u2022 A is a set of terminal nodes. Each node corresponds to an\natomic action a \u2208 A. \u2022 F is a set of object states essential to the task, including\npossible pre-conditions and post-effects of atomic actions. \u2022 T : F \u00d7 A \u2192 F is a set of transition rules that represent\nstate changes caused by atomic actions. \u2022 V is a set of non-terminal nodes, which can be further\ndecomposed into two sets: the And nodes S and the Or nodes O. Each sub-task corresponds to an And node s, which encodes a temporal relationship between its children. An Or node o forms a production rule with an associated probability, i.e. you may choose one of its children each weighted with a certain probability. \u2022 R is the set of production rules. \u2022 P is the set of probabilities on production rules. Causal relation. Causal knowledge represents the preconditions and the post-effects of atomic actions. We define it as a fluent change caused by an action. Fluent f \u2208 F can be viewed as some essential properties in a state x that can change over time, e.g., the temperature in a room and the status of a heater. For each atomic action, there are preconditions characterized by certain fluents of the states. E.g., an agent cannot successfully turn on the heater unless it is plugged in. As the effect of an action, certain fluents would be changed, and the state x would evolve to x\u2032. For example, if someone turns on a heater, the temperature of the room will be higher (and the heater would be on). It is formulated as one of the transition rules T . Temporal relation. Temporal knowledge encodes the schedule for an agent to finish each sub-task. It also contains the\ntemporal relations between atomic actions in a low level subtask. The sub-task preparing salad, for example, consists of taking salad, placing it onto the cutting board, and using the knife. Spatial relation. Spatial knowledge represents the physical configuration of the environment that is necessary to finish the task. In our case, to make the salad, an agent needs to know the locations of ingredients (e.g., lettuce), tool benches (e.g., basket, cutting board), delivery benches, etc."}, {"heading": "B. Parse Graphs as Mental State Representations", "text": "During the collaboration, an agent can use parse graphs to represent the mental states of itself or the other agent. A parse graph is an instance of an And-Or Graph, each of its Or nodes selects one child node. Figure 3 shows two parse graphs represent the robot and human\u2019s plan for the situation shown in 1. In our case, the parse graph pgt =< s h t , s r t , a h t , a r t , f h t , f r t > is one possible plan for both agents to finish the task. Particularly, the root node leads to a selection of individual sub-tasks (sht , s r t ) as sub-goals assigned to human and robot agent. To achieve these subgoals, agents perform atomic action (aht , a r t ) based on their belief of current fluent (fht , f r t )."}, {"heading": "C. Joint task planning by parsing STC-AoG", "text": "To construct the mental state representation for the robot, we design an algorithm based on STC-AoG parsing to select the optimal task plan for the team.\nGiven a set of sub-tasks S necessary to complete the joint task, the objective is to minimize the total task completion time by assigning a sub-task to either a robot or human agent, without violating any latent constraint:\nmin xvs ,\u03c4s max v\u2208{r,h} \u2211 s\u2208S xvs\u03b4 v s\ns.t. xvs \u2208 Xfeasible, \u03c4s \u2208 \u0393feasible. (1)\nwhere xvs is a binary variable indicating whether to assign sub-task s to agent v, and \u03c4s is a continuous variable representing the finishing time for the sub-task s. Constant \u03b4vs represents the amount of time for agent v to finish the sub-task s. Xfeasible and \u0393feasible represent the set of valid assignments that satisfies latent causal constraints, e.g., an\nagent cannot hold two objects at the same time; a subtask can be performed only if pre-conditions are met; after all assigned sub-tasks have been completed, the final state should satisfy the goal requirement.\nWe search for the optimal task plan via a dynamic programming algorithm. Starting from the initial state fb, we make valid sub-tasks assignments and simulate new intermediate state fe based on the state transition function T . By updating the current optimal consumed time and the corresponding sub-task assignment vectors for every intermediate state, our algorithm will finally reach the optimal plan for the entire task. During the updating process, we also record the sub-task assignment vectors for previous states, in order to generate the whole optimal assignment {xvs}s=1,...,|S| and completion time \u03c41, ..., \u03c4|S| for each subtask. After the task plan is computed, the robot\u2019s mental model is represented by a parse graph, as shown in the left part of 3: each sub-task in the task plan indicates a sub-goal that an agent needs to achieve at the time being. Sub-tasks are further connected with a sequence of corresponding atomic actions, which have certain pre-conditions and post-effects."}, {"heading": "IV. JOINT MIND MODELING FOR HUMAN-ROBOT COLLABORATIONS", "text": "Our goal is to enable efficient human-aware collaboration for a human-robot team. Specifically, robots need to understand human agents based on their actions and decide whether the team is moving in the right direction. We propose to model the robot mental state pgr and the human mental state pgh."}, {"heading": "A. Mind Models for Human and Robot", "text": "We treat the robot\u2019s mind as the oracle, i.e., it contains all necessary spatial, temporal, and causal information the team needs to finish the task. For example, at any given time t, the robot has a certain expectation of (i) current low level sub-goals (sht , s r t ) both agents should be pursuing; (ii) the actions (aht , a r t ) agents should perform; (iii) whether current object fluents satisfy pre-conditions of such actions, and what would be the post-effects.\nIt is also necessary to model the user\u2019s mind, which acts as a strong inductive bias in predicting user activities. As the\nuser\u2019s mental state pght is not directly available to the robot, we propose to infer it from user behavior and the history of communication."}, {"heading": "B. Human Mental State Inference", "text": "Based on the observed user behavior, we infer the most likely human mental state p\u0302gh, including the belief, goal and action plans. On a high level, this inference process uses observed user actions and communication history to infer human mental state. Specifically, given the And-Or graph G and human-robot interaction data DT = {dt}t=1,...,T , we infer the user mind p\u0302gh iteratively:\np\u0302gh = arg max pgh p(pgh|DT , G), (2)\np(pgh|DT , G) \u221d p(pgh|G,DT\u22121)p(dT |pgh, G). (3)\nHere the first term models the prior on the user mind given previous data DT\u22121 and AoG structure G. The second term models the likelihood for new data dT .\nTo model the likelihood function p(dT |pgh, G), we take a sampling-based approach. For each interaction data d, we consider user atomic action ahobs and communication between the two agents m. The idea is to model how likely the user performs action ahobs when receiving message from the robot mr, with current mental state pgh, as shown in Figure 4. Assuming ahobs and m\nr are conditional independent given pgh we have:\np(d|pgh, G) = p(ahobs|pgh, G)p(mr|pgh, G), (4)\np(ahobs|pgh, G) = \u2211 ahsamp p(ahsamp|pgh)p(ahobs|ahsamp), (5)\nwhere p(ahsamp|pgh) denotes the probability of sampled human action ahsamp given current estimation of human mental state pgh. p(ahobs|ahsamp) measures the similarity between observed human trajectory ahobs and sampled trajectory ahsamp.\nIn practice, we use rapid-exploring random tree (RRT*) for trajectory sampling and dynamic time warping (DTW)\nAlgorithm 1: Planning and explanation generation 1 while Task not finished do 2 if Replan needed then 3 Collect state information from the game; 4 Collect predicted human intentions from the last time step ; 5 Call DP planner ; 6 Obtain a new sequence of sub-tasks from planner and re-organize AoG based on it; 7 Parse AoG through checking pre-conditions and\npost-effects against the current environment state information ;\n8 Find out the next atomic action to execute based on parsing result ;\n9 Predict human intentions by equation (6) ; 10 Measure the difference between predicted intention and expected human actions; 11 Generate an explanation if the difference > \u03c4 ;\nbased approach to compare trajectories. DTW outputs a difference score diff . We use it in the energy function for the Boltzmann distribution. Then we update the human mental state in every time-step through the following equation:\nP (p\u0302ght+1|DT , G) = 1\nZ e\u2212 diff T \u03bbnP (p\u0302ght |DT\u22121, G), (6)\nwhere T is a constant temperature term, Z is a normalization constant, and \u03bb (> 1) is a constant that controls the importance of an explanation. It models how much information the user can retain for an explanation. n is the number of times an explanation about p\u0302gh is generated for the user in this task. Therefore, \u03bbn implicitly encodes the communication history m. Right now, we only consider communications from robot to human mr. Communication from human to robot mh can be considered in the future by adding corresponding energy terms. For now, some parameters (T and \u03bb) are set heuristically. These parameters can be learned from annotated user data [30]."}, {"heading": "C. Robot Mental State Update", "text": "Based on the observations in the environment, the robot can update its joint task plan. It is a two-step process. First, the robot collects all relevant information about the task and calls a DP planner described in Section III-C to obtain an optimal sequence of sub-tasks. Then the robot updates its mental state through re-organizing AoG (Delete finished nodes. Re-order unfinished nodes. If necessary, add back nodes deleted previously). Second, the robot uses causal knowledge (pre-conditions and post-effects of each atomic action) in the AoG terminal nodes to determine the next atomic action. If pre-conditions for the next atomic action are satisfied, the robot will execute it. Otherwise, the robot will be idle, waiting for the user to complete the other part of the job."}, {"heading": "V. EXPLANATION-BASED TASK COACHING", "text": "In this section, we propose a framework for explanation generation to enable efficient human-robot collaboration."}, {"heading": "A. Explanation framework", "text": "As shown in Algorithm 1, the framework includes an iterative process of online planning and explanation generation: 1) At a given time, the robot updates its mental state to\nrepresent the expected current goals of both agents and corresponding atomic actions; 2) The mental state of the human agent can be inferred, which would be further compared to the robot\u2019s mental state. Based on the result, the robot would decide whether explanations are necessary; 3) On the occasions where users perform an action other than that indicated in the explanation, the robot would update its task plan and mental model to reflect the best joint policy and expected mental models in the new state. Take the task making salad for example. At the beginning of the game, an optimal plan requires the user to first take the plate. A sub-optimal plan could be the user first taking the lettuce. If the user insists on taking the lettuce first regardless of whether explanations are given, the robot will update the task plan and expect the user to gather the plate afterwards."}, {"heading": "B. Explanation Timing", "text": "The explanation serves to provide users with the knowledge necessary to finish the task efficiently. This is achieved by inferring the user\u2019s mental model during the interaction and comparing it with the robot\u2019s. Whenever a disparity between these two models is detected, we can generate explanations to encourage correction of the user\u2019s mental state.\nDuring collaboration, we use temporal parsing to get robot mental state pgrt from its And-Or graph at time t. As in Section IV-B, user mental states p\u0302ght can be inferred based\non communication history and action sequences. The system generates explanations when there is a mismatch between the robot mental state and inferred human mental state: |pgrt \u2212 p\u0302ght | > . In practice, we measure P (p\u0302g h t |DT , G) for every sub-tasks at each time step based on equation (6). If the probability P (p\u0302ght = pg r t |DT , G) is lower than a threshold \u03c4 , we generate an explanation for the user. This process is shown in Figure 5."}, {"heading": "C. Explanation Content", "text": "We envision the disparity occurred between the user\u2019s mental state and robot\u2019s due to several reasons:\n1) The user wants to achieve goals that are different from the robot\u2019s expectation; 2) The user performs incorrect atomic actions to achieve a sub-goal; 3) The user is unaware of the pre-condition or effect of an atomic action.\nIn this paper, we do not distinguish between the possible causes of disparity when choosing the explanation timing, as they are too ambiguous. Instead, we propose to generate hierarchical explanation which consists of three components of the robot\u2019s mind representation:\n1) The robot would explain the current expected sub-goals of both agents (sht , s r t ) based on its mental state pg\nr, e.g., \u201dMy current goal is preparing the lettuce. Meanwhile, your expected goal is getting the plate.\u201d; 2) The robot communicates the expected atomic actions that both agents are supposed to perform (aht , a r t ), e.g.,\n\u201dCurrently, I\u2019m performing the action slicing the lettuce. You are supposed to perform the action taking the plate.\u201d; 3) In addition, by showing images of world states before and after an action (as shown in Figure 6b), the robot would also demonstrate the fluent change caused by an atomic action ft at\u2212\u2192 ft+1."}, {"heading": "VI. USER STUDY", "text": "We conducted a user study in a gaming environment to evaluate our algorithm, where participants can collaborate with agents on a virtual cooking task. The gaming environment and explanation interface are displayed in Figure 6."}, {"heading": "A. Experiment Domain", "text": "Our experiment domain is inspired by the video game Overcooked1, where multiple agents are supposed to make use of various tools and take different roles to prepare, cook, and serve various dishes. Particularly, we use Unreal Engine 4 (UE4) to create a real-time cooking task, namely making apple juice. To finish the task, teammates need to take apples from the box and slice them with a knife near the chopping board. Three apple slices should be put into the juicer before producing and delivering apple juice. Figure 6a shows a top-down view of the environment. The game interface is designed to be interactive (e.g., object appearance will change after taking valid actions) so that people can easily play through.\nTo finish the task, each user needs to complete a sequence of 62 atomic actions, if acting optimally, and observe 5 different object fluent changes with a total state space around 109. An example task schedule is shown in Figure 7.\n1http://www.ghosttowngames.com/overcooked/"}, {"heading": "B. Experiment Design", "text": "Hypotheses. The user study tests the following hypotheses with respect to our algorithm in the collaboration: \u2022 H1: Task completion time. Participants would collaborate\nwith the robot more efficiently if the robot generates explanations based on the human mental state modeling, compared to the other conditions. \u2022 H2: Perception of the robot. Participants would have higher perceived helpfulness and efficiency of the robot, as a result of receiving explanations based on the human mental state modeling, compared to the other conditions. Manipulated Variables. We use a between-subject design for our experiment. In particular, users are randomly assigned to one of three groups and receive different explanations from the robot: \u2022 Control: Users would not get any explanations from the\nrobot. As a result, they can learn to finish the task by interacting with the environment. \u2022 Heuristics: The robot gives explanations when there is no detected user action for a period of time. This serves as a simple heuristic for the robot to infer whether the user is having difficulties in finishing the task. The timing threshold is set to 9.3 seconds, based on the result of a pre-study in which users can actively ask for explanations when they get stuck.\n\u2022 Mind modeling: The robot gives explanations when there is a disparity between robot and human mental states.\nStudy Protocol. Before starting the experiment, each participant signs an informed consent form. An introduction is given afterward, including rules and basic controls of the game. As a part of the introduction, participants are given three chances to work on a simple single-agent training task, to verify their understanding. Those who fail to complete the training task in one minute would not continue the study. This is a comprehension test to exclude people who do not understand game control.\nParticipants who finish training get to see further instructions before starting to collaborate with the robot. They are first educated about the goal of a collaboration task (i.e., making apple juice) and what actions the team should perform to finish it. This is done to make sure every participant has sufficient knowledge to finish the task, so that the impact of user-specific prior knowledge can be minimized. To prepare users to interact and communicate with the robot agent, we would also show them a top-down view of the level map (as shown in Figure 6a), the appearance of the robot agent as well as an example of an explanation. During the task, the team is required to make and serve two orders of dishes in the virtual kitchen. At the end of the study, each participant is asked to complete a post-experiment survey to provide background information and evaluate the robot teammate. Measurement. In the background study, we have collected from users their basic demographic information, education, as well as experience with video games.\nOur objective measure is intended to evaluate the humanrobot teaming performance and subjective measure is designed for evaluating users\u2019 perception of the robot. Our dependent measures are listed below: \u2022 Teaming performance. We evaluate teaming performance\nby recording the time for the team to complete each order. \u2022 Perception of the robot. We measure user\u2019s perception\nabout the robot, in terms of its helpfulness and efficiency. Helpfulness is comprised of questions that measure users\u2019\nopinion on the robot\u2019s ability to provide necessary help. Efficiency is comprised of questions that measure users\u2019 opinion on how efficiently and fluently the team is able to finish the task."}, {"heading": "C. Results and Analysis", "text": "We recruited 29 subjects for our IRB-approved study from the university\u2019s subject pool. Most of the participants (69.3%) came from a non-STEM background. Their reported ages ranged from 17 to 36 (M=19.52, SD=2.89). All the participants have moderate experience with video games and have not played the video game Overcooked, which inspired our study design. Each participant got 1 course credit after completing the study. In addition, for ease of conducting the study, we discarded the data of 2 participants from the control group, as they got completely lost and failed to finish the designated task. As a result, there are 10 valid participants in the \u201dmind modeling\u201d and \u201dheuristics\u201d group, and 7 in the \u201dcontrol\u201d group.\nGenerally, we use ANOVA to test the effects of different experimental conditions on teaming performance and subjective perception of the robot. Tukey HSD tests are conducted on all possible pairs of experimental conditions.\nAs shown in Figure 8, we found marginally significant effects from \u201dmind modeling\u201d conditions on completion time of the first order (F (2, 24) = 2.038, p = .152). Post-hoc comparisons using the Tukey HSD tests revealed that teams could finish the first order significantly faster if users were under the \u201dmind modeling\u201d condition, compared to those under \u201dcontrol\u201d (p = .044). The result is marginally significant compared to those in \u201dheuristics\u201d (p = .120), confirming H1. However, for the completion time of the second order, we did not find any significant effect (F (2, 24) = 0.425, p = .658). This is not surprising since users were asked to finish the same task twice. They could take advantage of their previous experience working with the robot for the second order. Intuitively, the quantitative result showed that our explanation generation algorithm helped non-expert users to finish the task efficiently on their first run, while those in the control group needed to complete the task once to be able to finish it with the same efficiency.\nThe factorial ANOVA also revealed a significant effect of the explanation system on the perceived helpfulness (F (2, 24) = 4.663, p = .019) and efficiency (F (2, 24) = 4.136, p = .029) of the robot (Figure 9). In support of H2, post-hoc analysis with the Tukey HSD tests showed\nthat the robot\u2019s perceived helpfulness was significantly higher under the \u201dmind modeling\u201d condition, compared to \u201dcontrol\u201d (p = .023) and \u201dheuristics\u201d (p < .01). Users under the \u201dmind modeling\u201d were also more likely to believe the explanation system resulted in improved collaboration efficiency, compared to \u201dheuristics\u201d (p = .026) and \u201dcontrol\u201d (p < .01)."}, {"heading": "VII. CONCLUSION", "text": "In this paper, we propose a framework that allows a robot agent to improve teaming performance by communicating compelling explanations to its non-expert human teammate. By maintaining the mental state of both agents, the robot agent successfully generates explanations when the human behavior deviates from the optimal plan. By conducting a user study on a virtual collaborative cooking task, we demonstrate that the proposed algorithm can improve efficiency and quality of the interaction.\nFor simplicity of implementation, the current environment configuration prevents human and robot from having a shared workspace. For future work, we plan to study more cooking tasks in a diverse set of environments where multiple collaboration strategies can evolve. In addition, to make the robot\u2019s model more transparent, we consider to generate contrastive explanations with respect to identified incorrect user beliefs from the user\u2019s mental model in the future. Meanwhile, we plan to focus on a more balanced settings where both the human and robot agent have some information (e.g. ability, preference) to share with the teammates before a valid and efficient joint task plan can be formed."}, {"heading": "VIII. ACKNOWLEDGEMENTS", "text": "This work has been supported by DARPA XAI N6600117-2-4029."}], "title": "Joint Mind Modeling for Explanation Generation in Complex Human-Robot Collaborative Tasks", "year": 2020}