{
  "abstractText": "Abstract The ultimate goal of a supervised learning algorithm is to produce models constructed on the training data that can generalize well to new examples. In classification, functional margin maximization \u2013 correctly classifying as many training examples as possible with maximal confidence \u2013 has been known to construct models with good generalization guarantees. This work gives an information-theoretic interpretation of a margin maximizing model on a noiseless training dataset as one that achieves lossless maximal compression of said dataset \u2013 i.e. extracts from the features all the useful information for predicting the label and no more. The connection offers new insights on generalization in supervised machine learning, showing margin maximization as a special case (that of classification) of a more general principle and explains the success and potential limitations of popular learning algorithms like gradient boosting. We support our observations with theoretical arguments and empirical evidence and identify interesting directions for future work.",
  "authors": [
    {
      "affiliations": [],
      "name": "Nikolaos Nikolaou"
    },
    {
      "affiliations": [],
      "name": "Gavin Brown"
    }
  ],
  "id": "SP:e952953b9467df65d6fe71464da6687536d0f232",
  "references": [
    {
      "authors": [
        "A.A. Alemi",
        "I. Fischer",
        "J.V. Dillon",
        "K. Murphy"
      ],
      "title": "Deep variational information bottleneck",
      "venue": "arXiv preprint arXiv:1612.00410",
      "year": 2016
    },
    {
      "authors": [
        "A. Asadi",
        "E. Abbe",
        "S. Verd\u00fa"
      ],
      "title": "Chaining mutual information and tightening generalization bounds",
      "venue": "Advances in Neural Information Processing Systems, pp. 7234\u20137243",
      "year": 2018
    },
    {
      "authors": [
        "M. Belkin",
        "D. Hsu",
        "S. Ma",
        "S. Mandal"
      ],
      "title": "Reconciling modern machine learning and the bias-variance trade-off",
      "venue": "arXiv preprint arXiv:1812.11118",
      "year": 2018
    },
    {
      "authors": [
        "J. Bootkrajang",
        "A. Kab\u00e1n"
      ],
      "title": "Boosting in the presence of label noise",
      "venue": "Proceedings of the Twenty-Ninth Conference on Uncertainty in Artificial Intelligence, pp. 82\u201391. AUAI Press",
      "year": 2013
    },
    {
      "authors": [
        "P. B\u00fchlmann",
        "T. Hothorn"
      ],
      "title": "Boosting algorithms: Regularization, prediction and model fitting",
      "venue": "Statistical Science pp. 477\u2013505",
      "year": 2007
    },
    {
      "authors": [
        "C. Cortes",
        "V. Vapnik"
      ],
      "title": "Support-vector networks",
      "venue": "Mach. Learn. 20(3), 273\u2013297",
      "year": 1995
    },
    {
      "authors": [
        "G.K. Dziugaite",
        "D.M. Roy"
      ],
      "title": "Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data",
      "venue": "arXiv preprint arXiv:1703.11008",
      "year": 2017
    },
    {
      "authors": [
        "W. Gao",
        "Z.H. Zhou"
      ],
      "title": "On the doubt about margin explanation of boosting",
      "venue": "Artificial Intelligence 203, 1\u201318",
      "year": 2013
    },
    {
      "authors": [
        "T. Hastie",
        "A. Montanari",
        "S. Rosset",
        "R.J. Tibshirani"
      ],
      "title": "Surprises in high-dimensional ridgeless least squares interpolation",
      "venue": "arXiv preprint arXiv:1903.08560",
      "year": 2019
    },
    {
      "authors": [
        "A. Kalai",
        "R.A. Servedio"
      ],
      "title": "Boosting in the presence of noise",
      "venue": "Proceedings of the thirty-fifth annual ACM symposium on Theory of computing, pp. 195\u2013205. ACM",
      "year": 2003
    },
    {
      "authors": [
        "K. Kawaguchi",
        "L.P. Kaelbling",
        "Y. Bengio"
      ],
      "title": "Generalization in deep learning",
      "venue": "arXiv preprint arXiv:1710.05468",
      "year": 2017
    },
    {
      "authors": [
        "B. Neyshabur",
        "S. Bhojanapalli",
        "N. Srebro"
      ],
      "title": "Exploring generalization in deep learning",
      "venue": "Advances in Neural Information Processing Systems, pp. 5943\u20135952",
      "year": 2017
    },
    {
      "authors": [
        "R.E. Schapire",
        "Y. Freund",
        "P. Bartlett",
        "Lee",
        "W.S"
      ],
      "title": "Boosting the margin: A new explanation for the effectiveness of voting methods",
      "venue": "The annals of statistics 26(5), 1651\u20131686",
      "year": 1998
    },
    {
      "authors": [
        "R.A. Servedio"
      ],
      "title": "Smooth boosting and learning with malicious noise",
      "venue": "Journal of Machine Learning Research 4(Sep), 633\u2013648",
      "year": 2003
    },
    {
      "authors": [
        "O. Shamir",
        "S. Sabato",
        "N. Tishby"
      ],
      "title": "Learning and generalization with the information bottleneck",
      "venue": "Proc. of Advances in Learning Theory 8, 92\u2013107",
      "year": 2008
    },
    {
      "authors": [
        "C.E. Shannon"
      ],
      "title": "A mathematical theory of communication",
      "venue": "Bell system technical journal 27(3), 379\u2013423",
      "year": 1948
    },
    {
      "authors": [
        "C. Shen",
        "H. Li"
      ],
      "title": "On the dual formulation of boosting algorithms",
      "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence 32(12), 2216\u20132231",
      "year": 2010
    },
    {
      "authors": [
        "R. Shwartz-Ziv",
        "N. Tishby"
      ],
      "title": "Opening the black box of deep neural networks via information",
      "venue": "arXiv preprint arXiv:1703.00810",
      "year": 2017
    },
    {
      "authors": [
        "O Simeone"
      ],
      "title": "A brief introduction to machine learning for engineers",
      "venue": "Foundations and Trends R \u00a9 in Signal Processing 12(3-4), 200\u2013431",
      "year": 2018
    },
    {
      "authors": [
        "J. Sokoli\u0107",
        "R. Giryes",
        "G. Sapiro",
        "M.R. Rodrigues"
      ],
      "title": "Generalization error of deep neural networks: Role of classification margin and data structure",
      "venue": "Sampling Theory and Applications (SampTA), 2017 International Conference on, pp. 147\u2013151. IEEE",
      "year": 2017
    },
    {
      "authors": [
        "D. Soudry",
        "E. Hoffer",
        "N. Srebro"
      ],
      "title": "The implicit bias of gradient descent on separable data",
      "venue": "arXiv preprint arXiv:1710.10345",
      "year": 2017
    },
    {
      "authors": [
        "D. Strouse",
        "D.J. Schwab"
      ],
      "title": "The deterministic information bottleneck",
      "venue": "Neural computation 29(6), 1611\u20131630",
      "year": 2017
    },
    {
      "authors": [
        "N. Tishby",
        "F.C. Pereira",
        "W. Bialek"
      ],
      "title": "The information bottleneck method",
      "venue": "arXiv preprint physics/0004057",
      "year": 2000
    },
    {
      "authors": [
        "N. Tishby",
        "N. Zaslavsky"
      ],
      "title": "Deep learning and the information bottleneck principle",
      "venue": "Information Theory Workshop (ITW), 2015 IEEE, pp. 1\u20135. IEEE",
      "year": 2015
    },
    {
      "authors": [
        "V. Vapnik"
      ],
      "title": "Estimation of dependences based on empirical data",
      "venue": "Springer Series in Statistics",
      "year": 1982
    },
    {
      "authors": [
        "L. Wang",
        "M. Sugiyama",
        "Z. Jing",
        "C. Yang",
        "Z.H. Zhou",
        "J. Feng"
      ],
      "title": "A refined margin analysis for boosting algorithms via equilibrium margin",
      "venue": "Journal of Machine Learning Research 12(Jun), 1835\u20131863",
      "year": 2011
    },
    {
      "authors": [
        "C. Wei",
        "J.D. Lee",
        "Q. Liu",
        "T. Ma"
      ],
      "title": "On the margin theory of feedforward neural networks",
      "venue": "arXiv preprint arXiv:1810.05369",
      "year": 2018
    },
    {
      "authors": [
        "A.J. Wyner",
        "M. Olson",
        "J. Bleich",
        "D. Mease"
      ],
      "title": "Explaining the success of adaboost and random forests as interpolating classifiers",
      "venue": "arXiv preprint arXiv:1504.07676",
      "year": 2015
    },
    {
      "authors": [
        "A. Xu",
        "M. Raginsky"
      ],
      "title": "Information-theoretic analysis of generalization capability of learning algorithms",
      "venue": "Advances in Neural Information Processing Systems, pp. 2524\u20132533",
      "year": 2017
    },
    {
      "authors": [
        "Z.Q.J. Xu",
        "Y. Zhang",
        "Y. Xiao"
      ],
      "title": "Training behavior of deep neural network in frequency domain",
      "venue": "arXiv preprint arXiv:1807.01251",
      "year": 2018
    },
    {
      "authors": [
        "C. Zhang",
        "S. Bengio",
        "M. Hardt",
        "B. Recht",
        "O. Vinyals"
      ],
      "title": "Understanding deep learning requires rethinking generalization",
      "venue": "arXiv preprint arXiv:1611.03530",
      "year": 2016
    }
  ],
  "sections": [
    {
      "heading": "1 Introduction",
      "text": "The goal of a supervised learning algorithm is to construct a model on the training set that can generalize well on new data. Yet, generalization is an elusive property, involving intractable quantities to be approximated or bound \u2013like the generalization error\u2013 or notions with multiple definitions \u2013like that of model complexity. As a result\nN. Nikolaou Department of Physics & Astronomy, UCL Gower Street, London, WC1E 6BT, UK E-mail: n.nikolaou@ucl.ac.uk\nH. Reeve School of Computer Science, University of Birmingham Edgbaston, Birmingham, B15 2TT, UK E-mail: h.w.j.reeve@bham.ac.uk\nG. Brown School of Computer Science, University of Manchester Kilburn Building, Oxford Road, Manchester, M13 9PL, UK E-mail: gavin.brown@manchester.ac.uk\nar X\niv :2\n00 1.\n10 31\n8v 1\n[ cs\n.L G\n] 2\n8 Ja\nn 20\n20\nthere are many different theoretical routes to generalization, leading to often apparently \u2018contradictory\u2019 conclusions with one another or with empirical evidence [31]. For instance, why are certain learning algorithms that explore overparameterized or non-parametric model families so good at producing models that can generalize well, even without explicit regularization [5, 11, 31]? A unified language for comparing the complexity of models trained on a given dataset can help us identify good model selection and algorithmic practices that guide the learning algorithm towards models that are complex enough to not underfit yet also maximally resistant to overfitting.\nIn this paper we make a step towards this direction by bridging two \u2013until now disconnected\u2013 theoretical paths to generalization in the case of classification, namely information theory [16] inspired by recent advances on the information bottleneck principle [18, 23, 24] and margin theory [13, 25]. From an information-theoretic perspective, we would like our learning algorithm to learn a model that contains all the information from the features necessary for describing the target (we call this property losslessness) and no more information beyond that (we call this property maximal compression). Margin theory suggests constructing a model that can correctly classify as many training examples as possible with as high confidence as possible (i.e. one that maximizes the quantity known as the functional margin over the training set). We prove that in the case of classification of on noiseless (i.e. unambiguously labelled) datasets, functional margin maximization is equivalent to lossless maximal compression in the information-theoretic sense. The existence of margin-based bounds on the generalization error implies that margin maximization is beneficial for achieving good generalization and therefore so is lossless maximal compression.\nOur experiments on gradient boosting, a method that maximizes the training margins, show empirically that on noiseless data, margin maximization amounts to lossless maximal compression and that maximally compressed models on average exhibit the highest generalization capability (as estimated by the test error). We identify interesting similarities between the way training progresses in Deep Neural Networks (DNNs) and in gradient boosting and gain useful insights on the training of gradient boosting algorithms. All findings persist across a wide range of datasets & hyperparameter configurations.\nTo our knowledge, there is no prior work establishing the connection between functional margin maximization and lossless model compression in the informationtheoretic sense. Both margin theory and information theory have been individually connected to generalization and have been used to explain resistance to overfitting. The idea that functional margin maximization promotes good generalization can be traced back to [25]. It has been used in the theoretical analysis of Boosting algorithms [13], with recent work using it to explain good generalization in DNNs [7, 12, 20, 27]. The related notion of geometric margin maximization1 has been used to justify good generalization in Support Vector Machines (SVMs) [6]. The idea that a learned representation of a dataset that generalizes well is one that extracts from the features all the useful information for predicting the target and no more, is captured in information theoretic terms under the information bottleneck principle [23]. Recent work has offered insights into the good generalization capabilities of DNNs, utilizing these ideas [18,24]. More\n1 The geometric margin of a classifier is the distance of the closest example in the training set to the decision boundary. For linear models the geometric margin is a rescaling of the functional margin, therefore a model maximizing the one also maximizes the other.\nrecently, bounds on the generalization error of a learning algorithm in terms of the mutual information between its input and outputhave been established [2, 29].\n2 Background\n2.1 Binary Classification\nA classification algorithm, receives as input a training dataset S consisted of n pairs (xi, yi) of feature vectors xi \u2208 X and corresponding class labels yi \u2208 Y. The training set is drawn i.i.d. from some unknown probability measure P on X \u00d7 Y. We shall focus on binary classification, where Y = {\u22121, 1}. In this setting, we consider w.l.o.g. the output of the learning algorithm (model) as a function f : X \u2192 [\u22121, 1] that allows us to predict the label on unseen examples drawn from P with feature vector x as y\u0302 = sign(f(x)). Given any probability measure P on X \u00d7 Y and any function f : X \u2192 Y we let RP (f) denote the probability of making an error with respect to distribution P ,\nRP (f) = P [sign(f(X)) 6= Y ] . (1)\nIdeally the learning algorithm will output the model with the lowest possible risk w.r.t. the unknown distribution P, i.e. a function f that minimizes the true classification risk RP(f). However, since we do not have direct access to the unknown distribution P we must estimate P with the empirical measure P\u0302n defined for each set A \u2282 X \u00d7 Y in terms of the training data S = { (xi,yi) } i\u2208[n] by\nP\u0302n (A) = 1 n \u00b7 \u2211 i\u2208[n] 1 { (xi,yi) \u2208 A } . (2)\nThe empirical risk RS(f) := RP\u0302n(f) of a function f : X \u2192 [\u22121, 1] is given by\nRS(f) = P\u0302n [sign(f(X)) 6= Y ] = 1 n \u00b7 \u2211 i\u2208[n] 1 { sign ( f(xi) ) 6= yi } .\nIn what follows we shall refer to a general finitely supported measure P on X \u00d7 Y. The motivating example here is the empirical measure P\u0302n which is supported on the finite set S, the training dataset.\n2.2 Information theory\nWe now present some basic definitions and properties from information theory [16]. Let A & B be random variables (RVs), with alphabets A & B with probability distribution measure P . We shall assume that P is finitely supported and there exist finite subsets AP \u2282 A and BP \u2282 B such that P [A \u2208 AP ] = P [B \u2208 BP ] = 1.\nThe entropy of a RV A, measures the amount of uncertainty associated with its value when only its distribution is known. It is defined by\nHP (A) = \u2212 \u2211 a\u2208AP P (A = a) log (P (A = a)) . (3)\nThe amount of information shared by RVs A & B is their mutual information, defined as\nIP (A;B) = \u2211\na\u2208AP ,b\u2208BP\nP (A = a,B = b) log P (A = a,B = b)\nP (A = a)P (B = b) = IP (B;A). (4)\nIn terms of information theory, the chain rule of probability takes the form\nIP (A;B) = HP (A)\u2212HP (A|B) IP(A;B)=IP(B;A) = HP (B)\u2212HP (B|A), (5)\nwhere HP (A|B) is the conditional entropy of A given B, given by HP (A|B) = \u2212 \u2211\na\u2208AP ,b\u2208BP\nP (A = a,B = b) logP (A = a|B = b), (6)\nwhich measures the uncertainty of the value of RV A given the value of RV B. From Eq. (5), it is clear that IP (A;B) measures the decrease in uncertainty about either the value of RV A or the value of RV B, when the value of the other RV is known.\nIf B is a deterministic transformation of A then there is no uncertainty remaining about the value of B given the value of A, so we have HP (B|A) = 0\u21d0\u21d2 IP (A;B) = HP (B) \u2264 HP (A). Finally, if G(A) is an invertible transformation of RV A, we have IP (G(A);B) = IP (A;B) as the value of A grants us perfect knowledge of the value of G(A) and vice-versa.\nIn what follows we shall be particularly interested in the empirical entropy HS := HP\u0302n and the empirical mutual information IS := IP\u0302n where P\u0302n is the empirical measure of Eq. (2)."
    },
    {
      "heading": "2.2.1 The information bottleneck principle",
      "text": "Suppose we wish to learn a compressed representation F = f(X) from the original features X that is useful for predicting a target variable Y . Treating X, Y & F as RVs, the Information Bottleneck principle [23], offers a way to select a representation F , by trading-off the information the learned representation F captures from X regarding the target variable Y , i.e. IS(F ;Y ) \u2013the higher, the better for predicting Y \u2013 and the total information it captures from X, i.e. IS(F ;X) \u2013the lower, the higher the degree compression. We thus look for a representation F \u2217 such that\nF \u2217 = argmin F {IS(F ;X)\u2212 \u03b2IS(F ;Y )}, (7)\nwhere \u03b2 is a Lagrange multiplier that controls the aforementioned tradeoff. Recently, this principle has been used to explain good generalization in DNNs [18,24] and the use of training set estimates of Eq. (7) or variants of it as objective functions for training DNNs \u2013and other models\u2013 have been growing in popularity [1, 19, 22]. The reasoning is that compression controls for the complexity of the learned representation, thus promoting good generalization [15].\nIn this work we draw inspiration from the above line of research and our findings further reinforce the role of information compression in promoting generalization. We regard the trained model\u2019s outputs as the \u2018representation\u2019 F and explore the properties of a model F that minimizes IS(F ;X) subject to maximizing IS(F ;Y ) on a given training set S. We shall call such an intuitively \u2018ideal\u2019 model F a lossless maximal compressor of the training set S.\n2.3 Margin maximization\nThe (normalized) functional margin2 of a training example (xi, yi) under a model f is defined as\nmi = yif(xi) \u2208 [\u22121, 1].\nIt is a combined measure of confidence and correctness of the classification of the example under f . Its sign encodes whether the example is correctly classified (yif(xi) > 0) or misclassified (yif(xi) < 0), while the magnitude of the margin (i.e. the magnitude of the score f(xi)) measures the confidence of the model in its prediction (the higher, the more confident).\nMaximizing the margins over the training set has been connected to good generalization [13,25]. An upper bound to the generalization error PP(yf(x) \u2264 0) of an AdaBoost classifier f , based on its minimum margin over the training set, is proven in [13]. Tighter generalization bounds, dependent not only on the minimum margin but on the entire distribution of the training margins have been derived (e.g. Emargin bound [26], k-th margin bound [8]). Beyond boosting, such bounds hold for voting classifiers in general and recently similar bounds have been derived for DNNs [7,12,20,27].\nIn this work, we will establish an equivalence between models that maximize the margins on a noiseless training set S and lossless maximal compressors of S. We will verify our observations empirically, using boosting, a method that explicitly minimizes a monotonically decreasing loss function of the margin (i.e. maximizes training examples\u2019 margins)3. As we will see, boosting drives learning towards a lossless maximal compressor of the noiseless training dataset. It achieves the lowest generalization error (estimated by the average test set error) once lossless maximal compression has been achieved."
    },
    {
      "heading": "3 Lossless maximal compression & margin maximisation",
      "text": "3.1 An information-theoretic view of datasets & models\nWe will now define properties that capture the relationship between the information content of the model\u2019s output (F ) and the information present in the features & the target (X & Y , respectively) as measured on the training dataset S. In Figure 3.1 we provide a visual summary of these properties and their possible combinations. In\nTable 3.1 we summarize the information-theoretic equalities and inequalities that hold under each scenario. Proofs not directly following the statement of a lemma or theorem can be found in the Supplementary Material.\nAny function f : X 7\u2192 [\u22121, 1], can be considered as a model of the training dataset S. Typically, the model constructed by a learning algorithm is a member of some given model family. In this work we impose no restriction on the model space, i.e. f \u2208 \u03a6, where \u03a6 is the set of all models. Being a deterministic transformation of X, F = f(X) cannot contain more information than X. So, HP (F |X) = 0 \u21d0\u21d2 IP (X;F ) = HP (F ) \u2264 HP (X).\nDefinition 1 (Noiselessness) A probability distribution P is noiseless if and only if HP (Y |X) = 0.\n2 Also known as the hypothesis margin, or \u2013in the case of ensembles\u2013 the voting margin. 3 Adaboost approximately maximizes the average margin & actually minimizes the margins\u2019\nvariance [17].\nOtherwise, P is noisy and HP (Y |X) > 0. We shall say that a dataset S is noiseless (respectively, noisy) if the corresponding empirical measure P\u0302n is noiseless (respectively, noisy).\nUnder this information-theoretic perspective, a noiseless distribution P , hence a noiseless dataset S, is one in which the features X, contain all information to perfectly describe the target Y .\nGiven a distribution P on X \u00d7 Y we let f\u2217P : X \u2192 Y denote a minimizer of the risk i.e.\nf\u2217P \u2208 argmin f\u2208\u03a6 {RP (f)} .\nIn particular, when P is the underlying distribution P then sign \u25e6 f\u2217P is the Bayes classifier. When P is the empirical measure P\u0302n then sign \u25e6 f\u2217S is the empirical risk minimiser where f\u2217S := f \u2217 P\u0302n .\nLemma 1 A dataset S is noiseless if and only if RS(f\u2217S) = 0.\nIn other words, a noiseless training dataset S is one in which no datapoints with the same feature vector x have different labels y. For such a dataset4, there exists a model that can achieve zero empirical risk (training error), i.e. that can perfectly classify the training data. In other words, there exists some deterministic mapping from the features X to the target Y .\nWe shall now introduce properties that make a model f useful for the purposes of capturing relevant and ignoring redundant information from a training set S.\nDefinition 2 (Losslessness) A model f is lossless on the dataset S if and only if IS(F ;Y ) = IS(Y ;X). Otherwise, the model is lossy on S and IS(F ;Y ) < IS(Y ;X).\nA lossless5 model f on a dataset S is one that captures all the information in features X that is relevant for describing the target Y . We can equivalently state that the r.v. F is a sufficient statistic of the empirical distribution P\u0302n of the training data.\nLemma 2 Suppose dataset S is noiseless. A model f is lossless on S if and only if there exists an invertible transformation g : [\u22121, 1]\u2192 R such RS(g \u25e6 f) = RS(f\u2217S).\nLemma 2 means that if a model f is lossless on a training set S, its output can be used to describe the target Y with the only source of training error being the irreducible class overlap in the training set.\nDefinition 3 (Maximal Compression) A model f : X \u2192 [\u22121, 1] is a maximal compressor of the dataset S if and only if IS(F ;X) = IS(F ;Y ). Otherwise, the model is undercompressed on S and IS(F ;X) > IS(F ;Y ).\nA model f that is a maximal compressor of a training dataset S is one that only captures from the features X information relevant for describing the target Y . It does not necessarily capture all that information; this special case, merits a definition of its own given below.\n4 Also known as a unambiguously labelled or consistent training dataset in the literature. 5 Often the term \"lossless encoding\" of some r.v. X in the literature characterizes an encoding f(X) that allows us to recover the original value of X from it. In our case, because of the supervised nature of the learning task, it shall mean that f(X) allows us to recover the original value of the target Y from it. Not necessarily the value of the feature vector.\nDefinition 4 (Lossless Maximal Compression - (LMC)) A model f is a lossless maximal compressor (LMC) of a training dataset S if and only if it is lossless on S and a maximal compressor on S.\nProposition 1 A model f is an LMC of a training dataset S, if and only if it satisfies\nIS(F ;X) = IS(F ;Y ) = IS(Y ;X).\nProof. Follows straightforwardly from Definition 2 & Definition 3.\nA model f that is an LMC of a training dataset S is one that only captures from the features X all the information relevant for describing the target Y . From an information-theoretic perspective, an LMC of S is the optimal classification model that can be constructed from S.\nWe have defined the notion of a noiseless / noisy training dataset S and those of a lossy / lossless f on S and of an undercompressed / maximally compressed model f on S. In Figure 3.1 we provide a visual summary of these properties and their possible combinations in the form of entropy Venn diagrams. In Table 3.1 we summarize the relationships among the various information-theoretic quantities involved that hold under each scenario.\nIn the next subsections, we shall use the properties we defined here to obtain a better understanding of what types of models the information-theoretically optimal model, the LMC corresponds to for a noiseless dataset S and for a noisy dataset S.\n3.2 Noiseless data: Equivalence of lossless maximal compression & margin maximisation\nLet us first focus on the special case of a noiseless dataset S, i.e. a dataset that does not contain any datapoints with the same feature vector but different labels. We will then discuss the noisy case where ambiguously labelled datapoints can be present in the dataset.\nThe noiseless case merits a special discussion for several reasons: (i) It is the typical case studied in the literature and as such it allows us to connect our observations to existing work. (ii) It allows us to establish an equivalence between information theoretic lossless maximal compression and margin maximization. (iii) It is a very common case in practice since in large dimensional datasets, encountering datapoints that have the same feature vector but different class labels are typically expected to be rare6.\nWe will now show the equivalence between lossless maximal compression and margin maximisation on a noiseless dataset S.\nTheorem 1 Suppose S is noiseless and finitely supported. A model f is an LMC with respect to S if and only if there exists some invertible transformation g : [\u22121, 1] \u2192 R such that g \u25e6 f is a margin maximizer with respect to S.\nUnder Theorem 1 a classification model that maximizes the training margins on a given noiseless dataset7 is one that captures all the information present in the features of that dataset relevant for predicting the target label and no more. Conversely, since an LMC is a margin maximizer, it offers the same guarantees on the generalization error as the latter. Note that Theorem 1 captures a relationship between a noiseless dataset S and a model f , regardless of the underlying learning algorithm that produced it (i.e. the model family it explores or the optimization method used to explore it).\nFrom Lemma 1 & Lemma 2, we have that a lossless model f on a noiseless training dataset S is one whose output can be used to classify every training example to the correct class (i.e. S is separable by f). The success of algorithms that generate models that can interpolate8 the data, yet, despite exploring overparameterized model spaces, are resistant to overfitting (e.g. gradient boosting, random forests, SVMs and DNNs) has recently attracted considerable research interest [3, 9, 28].\nOur work connects these findings to information theory and margin theory: we posit that models generated by such methods are typically not simply lossless, but actually LMCs, hence margin maximizers and their good generalization follows via the marginbased generalization bounds. Algorithms such as the aforementioned, have mechanisms for promoting both losslessness (interpolation, in a noiseless dataset), guaranteeing the model produced will not underfit (afforded by overparameterizing the model space) and maximal compression (afforded via explicit or implicit margin maximization) which produces a model from that space that is maximally resistant to overfitting.\n6 This is because encountering datapoints that have the same feature vector in high dimensional feature spaces is typically expected to be rare in the first place.\n7 A margin maximizer on a noiseless dataset is a model that correctly classifies all training examples, with maximal confidence. Obviously, if yif(xi) = 1, \u2200(xi, yi) \u2208 S, then both the average and the minimal margin of f on S are equal to 1 (maximal) and the variance of the margin distribution of f on S is 0 (minimal).\n8 An interpolating classifier is one that can perfectly separate the data, i.e. achieve zero training error. In our terminology it is a lossless model on a noiseless dataset, as such it falls within the case examined here.\n3.3 Noisy data: The equivalence collapses\nLet us now discuss the case of a noisy dataset S and how it differs from the case of noiseless data.\nIn the noiseless case, any model that correctly classifies every training datapoint in S (i.e. achieves zero training error) is a lossless model on S. In the noisy case this observation is no longer relevant, as there exist at least 2 datapoints which are noisy, i.e. have the same feature vector x, but different labels y. It is no longer the case that there exists a model f that can perfectly separate the data.\nWe can also rephrase the observation stated above as follows: Any model f that yields the minimal achievable training error on a noiseless training dataset S is a lossless model on S. As we will see from Lemma 3, this condition is no longer sufficient for f to be lossless on a noisy training dataset S.\nA model that minimizes the training error on a noisy dataset S will be one that classifies all points in S with the same feature vector x to the majority class among them. Furthermore, a margin maximizer 9 f on S is a model that minimizes the training error while also assigning maximal absolute score to its predictions (i.e. |f(x)| = 1,\u2200x). It is easy to see that \u2013 unlike in the case of a noiseless training dataset S where a margin maximizer was an LMC\u2013 in the noisy case, a margin maximizer cannot even be a lossless model. This is a direct consequence of Lemma 3, the proof of which can be found in Section A of the Supplementary Material.\nLemma 3 Suppose that X and Y are discrete random variables taking values in X and Y = {\u22121,+1}, respectively. Suppose that f : X \u2192 Z and let F = f(X). Then I(X;Y ) = I(F ;Y ) if and only if the map x 7\u2192 P(Y = 1|X = x) is constant on all sets of the form f\u22121(z) \u2286 X for some z \u2208 Z.\nIn simpler terms, Lemma 3 tells us that if for two feature vectors x1 and x2 a model f satisfies f(x1) = f(x2), then it also has to be the case that P(Y = 1|X = x1) = P(Y = 1|X = x2) for f to be lossless (and inversely). Therefore, a margin maximizer, i.e. a model assigning maximal (i.e. the same) score both to noiseless positive examples (unambiguously labelled positive examples, for which P(Y = 1|X = x) = 1) and to noisy popsitive examples (ambiguously labelled examples, i.e. ones with 0.5 < P(Y = 1|X = x) < 1) violates the condition of losslessness.\nWe therefore see that a margin maximizing model f of a noisy training dataset S cannot be an LMC of S as it is not even lossless on S. Furthermore, as margin maximizers are themselves training error minimizers, this implies that not all training error minimizers of S are LMCs (or even lossless) on S either. These observations are summarized in Table 2.\nA lossless model (one satisfying IS(F ;Y ) = IS(X;Y )) is one that captures all the information present in the features X relevant for predicting the target Y . In the case of a noisy training dataset, this information includes the uncertainty introduced by the ambiguous labelling of a feature vector x, i.e. P(Y = 1|X = x). So a lossless model should assign different scores f(x) to feature vectors f(x) which have different values of P(Y = 1|X = x).\nMoreover, f is an LMC (has the minimum IS(F ;X) that allows IS(F ;Y ) = IS(X;Y )) iff it is lossless while using the fewest values f(x) possible to encode the\n9 We remind the reader that we refer to minimizers of the average (equivalently: total) margin over the training examples with this term.\nempirical P(Y = 1|X = x), i.e. have as many distinct values for f(x) as there are distinct values of P(Y = 1|X = x).\nThe above discussion provides an intuition into the limitations of margin maximization approaches in the presence of label noise. The sub-optimality of boosting (a margin maximization approach) in the presence of label noise has been observed in earlier studies [4, 10, 14] and here we provide an information-theoretic justification for this phenomenon. Simply put, the strategy of maximizing the margins on a noisy dataset is not producing a lossless maximally compressing model on that dataset. In fact, the resulting margin maximizing model, is not even going to be lossless on the training dataset as it will fail to capture the uncertainty over the labels. When the training data are noisy, we should instead aim to produce models whose scores f(x) capture the underlying empirical distribution P(Y = 1|X = x) (lossless). Ideally, we should aim for strategies producing models whose scores f(x) are in 1\u2212 1 correspondence to P(Y = 1|X = x) (LMCs).\n4 Empirical Evidence\n4.1 Experimental Setup\nBoosting, a method that explicitly maximizes the margins of the training examples10, can be shown empirically to also converge to LMC models on noiseless datasets. After lossless maximal compression is achieved, so is the minimal generalization error, as estimated by the error on the test set. To demonstrate this, we plot the trajectory of the boosting ensemble on the entropy-normalized information plane, IS(F ;Y )/HS(Y ) vs. IS(F ;X)/HS(X). For each boosting round t, F = Ft denotes the RV of which the ensemble\u2019s outputs are realizations.\n10 Gradient boosting is a family of ensemble learning methods that construct an additive model by adding on each round the component minimizing some monotonically decreasing loss function of the margin. It can be viewed as minimizing said loss by performing gradient descent on the space of components (base learners).\nThe experiments were carried out on binary classification tasks on both real-world UCI datasets and artificial data (dataset descriptions in the Supplementary Material). Qualitatively, the results are similar for all datasets (see Figure 2 as well as Section C of the Supplementary Material). The boosting ensemble consisted of a maximum of T = 100 decision trees (i.e. rounds of boosting) of maximal depth 6. No shrinkage of the updates or subsampling of the examples was performed (both are techniques to counter overfitting), and the exponential loss function was used (i.e. the loss minimized by AdaBoost). We performed no hyperparameter optimization. Plotting trajectories on the information plane follows [18,24]. All information-theoretic quantities were estimated on the training data by first discretizing the features & model outputs in b = 100 equal-sized bins11, then using maximum likelihood estimators. The joint RV X was then constructed by the discretized features X1, X2, . . . , Xd as X = \u2211d i=1Xib\ni\u22121. We plot average results across 100 runs with different train-test splits (50%\u201350%) on the same original data. We also visualize the trajectories obtained by some random individual runs to showcase that although they can vary significantly from one another, they all follow the same general pattern. All datasets & code used in the experiments can be downloaded at https://github.com/nnikolaou/margin_maximization_LMC.\n4.2 Results & Analysis\nLet us first introduce some characteristic points on the information plane: Lossless maximal compression (LMC) point: A red star on the information plane denotes the point of lossless maximal compression \u2013 the optimal feasible point a model f can occupy on the plane on a given dataset \u2013 on which IS(F ;Y ) is the maximal achievable while IS(F ;X) is minimal. On this point, IS(F ;Y ) = IS(F ;X) = IS(X;Y ) and for a noiseless dataset, IS(X;Y )/HS(Y ) = 1. Average margin maximization point:With a hollow green circle on the information plane, we denote the model (round of boosting) under which the average (equiv. total) margin is first minimized. Training error minimization point: With a full black dot on the information plane, we denote the model (round of boosting) under which the training error is first minimized (losslessness is achieved). At this point IS(F ;Y ) has reached its maximum, so for a noiseless dataset, IS(X;Y )/HS(Y ) = 1. Test error minimization point:With a magenta square on the information plane, we denote the model (round of boosting) under which the test error (proxy for generalization error) is first minimized.\nLet us now summarize our observations from Figure 2 & the figures of Section C of the Supplementary Material: Boosting leads to lossless maximal compression: In all datasets, the boosting ensemble traces a trajectory on the information plane that leads to the LMC point and once it reaches it in never escapes. Lossless maximal compression coincides with margin maximization: In all datasets the image on the information plane of the models that minimize the margin coincides with the LMC point. Lossless maximal compression coincides with maximal generalization: The\n11 Note that by discretizing the features we might convert an originally noiseless dataset into a noisy one. In the experiments included in this paper this did not happen for any dataset for the numbers of discretization bins chosen. So all results shown are on noiseless datasets.\npoint of the ensemble\u2019s trajectory corresponding to the minimal test error coincides \u2013on average\u2013 with the LMC point on the information plane (and so does the margin maximization point). In other words, LMCs correspond to the models exhibiting \u2013on average\u2013 the best generalization behaviour. Average trajectory shape: After the training error is minimized, the test error can be further decreased by training for more rounds. This is a known result in boosting, explained via margin theory. Here we give an information-theoretic interpretation. Training until training error minimization, amounts to achieving losslessness. Subsequent rounds result in travelling along the line of maximal IS(F ;X) on the information plane, towards the LMC point. This compresses the model f (relieves it of remaining information from X irrelevant for predicting Y ), decreasing its effective complexity12. Training in boosting consists of 2 (typically distinct) phases: The results suggest the presence of 2 distinct phases during training under gradient boosting. A similar behaviour was observed in [18] for the trajectories of the representations learned by DNNs. Following the terminology of [18], these are the empirical risk minimization (ERM) phase, when IS(F ;Y ) increases (the model better fits the training data) but typically so does IS(F ;X) (the model uses more information from X) and the compression phase, when IS(F ;X) decreases (the model uses increasingly less information from X, reducing its effective complexity), without decreasing IS(F ;Y ). We can view the ERM phase as decreasing the bias of the model while not decreasing its variance and\n12 Holds for average trajectories. Single runs include steps that both increase IS(F ;X) & decrease IS(F ;Y ).\nthe compression phase as reducing variance while not increasing bias. The ERM phase is usually much shorter than the compression phase, as is the case with DNNs [18]. Although typically we do observe these 2 phases as distinct in the average trajectories, they need not be, as was also observed in subsequent studies in DNNs [30]. Trajectories of individual runs, are not as smooth as the average trend; we can even observe steps that increase both bias & variance. However, the 2 phases still appear to be distinct: once losslessness is achieved (ERM phase terminates), it is maintained and pure compression begins. Early stopping does not improve generalization in gradient boosting: As long as losslessness can be achieved, additional boosting rounds do not hurt generalization. Once the model reaches the LMC point on the information plane, it never escapes it. Subsequent iterations neither increase the training nor the test error. This suggests that early stopping with boosting is unnecessary for improving generalization and agrees with recent observations [28]. General margin losses minimized via stochastic gradient descent (SGD) also exhibit similar behaviour [21]. Consistency across datasets, hyperparameter & discretization settings: The aforementioned observations hold across different datasets and hyperparameter settings. Section C of the Supplementary Material contains more results supporting this claim. They also hold if we change the number of bins used to discretize the features (provided the dataset remains noiseless) or the scores (provided they are\u2265 2). Margin maximization as a built-in regularization mechanism: Additional regularization techniques like subsampling or shrinkage are not the main reason why boosting regularizes. Their contribution is small compared to the algorithms\u2019 built-in regularization mechanism: margin maximization, which as we saw amounts to lossless maximal compression of the training dataset. This is another similarity shared with DNNs trained with SGD that achieve good generalization by tracing a similar trajectory on the information plane [18], and additional regularization control (e.g. dropout or batch normalization) is beneficial, but not the main contributor to their good generalization [11,18,31]."
    },
    {
      "heading": "5 Discussion",
      "text": "We characterized from an information theoretic perspective, models trained on a given training set w.r.t. the information they capture from it. Under this light, we identified an ideal model trained on a given dataset as its lossless maximal compressor (LMC): one capturing all the information from the features relevant for predicting the target and no more. We then established that an LMC is \u2013in the case of classification\u2013 equivalent to a margin maximizer of the dataset (provided it is noiseless, i.e. consistently labelled). The existence of margin-based bounds on the generalization error implies that margin maximization, hence lossless maximal compression, is beneficial to generalization.\nOur experiments on gradient boosting, demonstrate that indeed, margin maximization amounts to lossless maximal compression on noiseless data. The evolution of the model constructed by boosting, traces a trajectory on the information plane that leads to the point of lossless maximal compression which also coincides with the point of margin maximization and the point on average exhibiting the best generalization. In agreement with recent studies on boosting, we observe that early stopping is unnecessary for improving generalization [28] and identify interesting similarities between how training progresses in DNNs and in gradient boosting in terms of the trajectory\nthey trace on the information plane [18]. All observations persist across a wide range of datasets and hyperparameter configurations.\nThis work gives an information-theoretic interpretation of margin maximization and provides us with a principled way to define model complexity for the purposes of generalization, thus shedding more light on the success of methods like gradient boosting. It also opens various directions for future work. For instance, exploring how these concepts can be applied in model selection or to inform learning algorithm design to more efficiently traverse the information plane to reach the LMC point. It would also be of interest to identify the analogue of the LMC in learning tasks other than classification, like ranking or regression.\nAcknowledgements This project was partially supported by the EPSRC LAMBDA [EP/N035127/1] & Anyscale Apps [EP/L000725/1] project grants and the EU Horizon 2020 research & innovation programme [grant No 758892, ExoAI]. NN acknowledges the support of the EPSRC Doctoral Prize Fellowship at the University of Manchester and the NVIDIA Corporation\u2019s GPU grant. The authors thank Konstantinos Sechidis, Konstantinos Papangelou & Ingo Waldmann for their useful comments and suggestions."
    },
    {
      "heading": "6 Supplementary Material",
      "text": "A. Proofs\nIn this section we shall prove Lemma 1, Lemma 2 and Theorem 1. Rather than proving these results directly we shall instead prove generalisations to an arbitrary finitely supported probability measure P (Lemma 6, Lemma 5 and Theorem 2). The sample based (i.e. dataset based) results used in the main paper correspond to the special case in which the probability measure P is the empirical measure P\u0302n.\nDefinition 5 A finitely supported probability distribution P is noiseless if and only if HP (Y |X) = 0.\nDefinition 5 generalises Definition 1 which corresponds to the special case in which P is the empirical measure P\u0302n. The proofs of the following results require the following elementary lemma.\nLemma 4 Let the function \u03c6 : [0, 1]\u2192 R defined by \u03c6(0) = 0 and \u03c6(z) = z \u00b7 log(1/z) for z \u2208 (0, 1]. Then we have \u03c6(z) \u2265 0 for all z \u2208 [0, 1] with equality if and only if z \u2208 {0, 1}.\nWe shall now prove Lemma 5 which generalises Lemma 1.\nLemma 5 A finitely supported probability distribution P is noiseless if and only if RP (f\u2217P ) = 0.\nProof. We can write out the conditional entropy HP (Y |X) in terms of \u03c6 as follows,\nHP (Y |X) = \u2211 x\u2208XP P (X = x) \u2212\u2211 y\u2208Y P (Y = y|X = x) log (P (Y = y|X = x))  = \u2211 x\u2208XP P (X = x) \u2211 y\u2208Y \u03c6 (P (Y = y|X = x))\n . Since \u03c6 is non-negative it follows that HP (Y |X) = 0 if and only if for each x \u2208 XP we have \u03c6 (P (Y = y|X = x)) = 0 which is the case if and only if P (Y = y|X = x) \u2208 {0, 1}.\nNow suppose that HP (Y |X) = 0 so for each x \u2208 XP , y \u2208 Y, P (Y = y|X = x) \u2208 {0, 1}. Then we can define f\u2217P : X \u2192 [\u22121, 1] so that P (Y = f \u2217 P (x)|X = x) = 1, so P (sign(f\u2217P (x)) 6= Y |X = x) = 0. Thus for each x \u2208 XP if\nRP ( f\u2217P ) = P [ sign(f\u2217P (X)) 6= Y ] = \u2211 x\u2208XP P (X = x) \u00b7 P ( sign(f\u2217P (x)) 6= Y |X = x ) = 0.\nOne the other hand, if RP (f\u2217P ) = 0 then we must have P (Y = y|X = x) = 1 if y = sign (f\u2217P (x)) and P (Y = y|X = x) = 0 otherwise. Thus, P (Y = y|X = x) \u2208 {0, 1} for each x \u2208 XP , y \u2208 Y and so HP (Y |X) = 0.\nDefinition 6 generalises Definition 2.\nDefinition 6 A model f is lossless with respect to P if and only if IP (F ;Y ) = IP (Y ;X).\nLemma 6 generalises Lemma 2.\nLemma 6 Suppose P is noiseless. A model f is lossless with respect to P if and only if there exists an invertible transformation g : [\u22121, 1]\u2192 R such RP (g \u25e6 f) = RP (f\u2217P ).\nProof. The model f : X \u2192 [\u22121, 1] is lossless if and only if IP (F ;Y ) = IP (Y ;X), which is the case if and only if HP (Y |F ) = HP (Y |X) = 0, where we have used Eq. (5) and the assumption that P is noiseless. Moreover, as in the proof of Lemma 1 we have\nHP (Y |X) = \u2211\ns\u2208f(XP ) P (f(X) = s) \u2211 y\u2208Y \u03c6 (P (Y = y|f(X) = s))  . Using the fact that \u03c6(z) \u2265 0 on [0, 1] with equality only at {0, 1} we infer that HP (Y |X) = 0 if and only if for each y \u2208 Y, s \u2208 f(XP ) we have P (Y = y|f(X) = s) \u2208 {0, 1}.\nNow if RP (g \u25e6 f) = RP (f\u2217P ) for some invertible transformation g then\u2211 s\u2208f(XP ) P (f(X) = s) \u00b7 P (Y 6= sign(g(s))|f(X) = s) = P (sign(g \u25e6 f(X)) 6= Y )\n= RP (g \u25e6 f) = RP (f\u2217P ) = 0.\nThis implies that for each s \u2208 f(XP ) for y = sign(g(s)), P (Y = y|f(X) = s) = 1 and for y 6= sign(g(s)), P (Y = y|f(X) = s) = 0, so in general P (Y = y|f(X) = s) \u2208 {0, 1}, HP (Y |X) = 0 and f is lossless.\nConversely, if f is lossless then for each s \u2208 f(XP ) we can take g(s) = (2 \u00b7 P (Y = 1|f(X) = s)\u2212 1) ( s+ 2\n3\n) ,\nand extend g on [\u22121, 1]\\f(XP ) arbitrarily to form a bijection. Since f is lossless, for each s \u2208 f(XP ) and y \u2208 Y we have P (Y = y|f(X) = s) \u2208 {0, 1}. If for some s \u2208 f(XP ) we have P (Y = 1|f(X) = s) = 1 then g(s) > 0 and so P (Y 6= sign(g(s))|f(X) = s) = 0. Similarly, for s \u2208 f(XP ) with P (Y = \u22121|f(X) = s) = 1 we have g(s) < 0 and so again P (Y 6= sign(g(s))|f(X) = s) = 0. Hence, in general RP (g \u25e6 f) = 0 = RP (f\u2217P ).\nDefinitions 7 and 6 generalise Definitions 3 and 2, respectively.\nDefinition 7 A model f is a maximal compressor of a distribution P if and only if IP (F ;X) = IP (F ;Y ).\nDefinition 8 A model f is a lossless maximal compressor (LMC) of a training dataset S if and only if it is lossless on S and a maximal compressor on S.\nProposition 2 generalises Proposition 1.\nProposition 2 A model f is an LMC of a finitely supported probability distribution P , if and only if it satisfies\nIP (F ;X) = IP (F ;Y ) = IP (Y ;X).\nProof. Follows straightforwardly from Definition 6 & Definition 7.\nFinally we shall prove Theorem 2 which generalises Theorem 1.\nTheorem 2 Suppose P is noiseless and finitely supported. A model f is an LMC with respect to P if and only if there exists some invertible transformation g : [\u22121, 1] \u2192 R such that g \u25e6 f is a margin maximizer with respect to P .\nProof. As we saw in the proof of Lemma 1, the fact that P is noiseless implies that for each x \u2208 XP and y \u2208 Y we have P (Y = y|X = x) \u2208 {0, 1}. We form partition partition XP = X+ \u222a X\u2212 so that for x \u2208 X+, P (Y = 1|X = x) = 1 and for x \u2208 X\u2212, P (Y = \u22121|X = x) = 1.\nNow suppose that for some invertible transformation g : [\u22121, 1]\u2192 R, g\u25e6f is a margin maximizer with respect to P . Hence, if g(f(x)) = 1 for x \u2208 X+ and g(f(x)) = \u22121 for x \u2208 X\u2212. This implies that RP (g \u25e6 f) = RP (f\u2217P ) = 0. Thus, by Lemma 2, f is lossless. Moreover, g is invertible this is equivalent to f(x) = g\u22121(1) for x \u2208 X+ and f(x) = g\u22121(\u22121) for x \u2208 X\u2212. Hence, P (f(X) = s|Y = y) = 1 when s = g\u22121(y) and P (f(X) = s|Y = y) = 0 otherwise, so in general \u03c6(P (f(X) = s|Y = y)) = 0. Thus,\nHP (F |Y ) = \u2211 y\u2208Y P (Y = y) \u2211 s\u2208f(XP ) \u03c6(P (f(X) = s|Y = y)) = 0 = HP (F |X),\nwhere the final inequality follows from the fact that P (F = f(x)|X = x) = 1. Hence, we have IP (F ;Y ) = IP (F ;X). It follows that f is a maximal compressor and we have already shown that f is lossless.\nConversely, let\u2019s suppose that f is a lossless maximal compressor. Since f is lossless we infer from Lemma 2 that there is some transformation g\u0303 : [\u22121, 1] \u2192 R such RP (g\u0303 \u25e6 f) = RP (f\u2217P ) = 0, which in turn implies that if x+ \u2208 X+ and x\u2212 \u2208 X\u2212 then f(x+) 6= f(x\u2212). Moreover, since f is a maximal compressor we must have IP (F ;Y ) = IP (F ;X) which implies\u2211\ny\u2208Y P (Y = y) \u2211 s\u2208f(XP ) \u03c6(P (f(X) = s|Y = y)) = HP (F |Y ) = HP (F |X) = 0.\nThus, for each s \u2208 f(XP ) and y \u2208 Y we have P (f(X) = s|Y = y) \u2208 {0, 1}, where once again we use the fact that \u03c6 is non-negative with zero attained at {0, 1}. Hence, there exists some s+ \u2208 [\u22121, 1] such that for all x+ \u2208 X+, f(x+) = s+ and some s\u2212 \u2208 [\u22121, 1] with s\u2212 6= s+ such that for all x\u2212 \u2208 X\u2212, f(x\u2212) = s\u2212. Thus, if we choose g : [\u22121, 1]\u2192 R to be any invertible map with g(s+) = 1 and g(s\u2212) = \u22121 we see that g \u25e6 f is a margin maximiser. This completes the proof of the theorem.\nFinally, we prove Lemma 7 which generalises Lemma 3.\nLemma 7 Suppose that X and Y are discrete random variables taking values in X and Y = {\u22121,+1}, respectively. Suppose that f : X \u2192 Z and let F = f(X). Then I(X;Y ) = I(F ;Y ) if and only if the map x 7\u2192 P(Y = 1|X = x) is constant on all sets of the form f\u22121(z) \u2286 X for some z \u2208 Z.\nProof. The proof uses the entropy functional \u03c6 : [0, 1] \u2192 R by \u03c6(p) = \u2212p \u00b7 log(p) \u2212 (1\u2212 p) \u00b7 log(1\u2212 p). Note that \u03c6 is strictly concave. Now observe that\nH(Y |X) = \u2211 x\u2208X P(X = x) \u00b7 \u03c6 (P(Y = 1|X = x))\n= \u2211 z\u2208Z P(F = z) \u00b7 \u2211 x\u2208X P(X = x|F = z) \u00b7 \u03c6 (P(Y = 1|X = x)) ,\nwhere we have used the fact that P(X = x|F = z) = P(X = x)/P(F = z) if z = f(x) and P(X = x|F = z) = 0 otherwise. We also have\nH(Y |F ) = \u2211 z\u2208Z P(F = z) \u00b7 \u03c6 (P(Y = 1|F = z)) .\nBy the strict concavity of \u03c6 for each z \u2208 Z we have\u2211 x\u2208X P(X = x|F = z) \u00b7 \u03c6 (P(Y = 1|X = x))\n\u2264 \u03c6 (\u2211 x\u2208X P(X = x|F = z) \u00b7 P(Y = 1|X = x) ) = \u03c6 (P(Y = 1|F = z)) ,\nwith equality if and only if P(Y = 1|X = x) is constant for all x \u2208 X with P(X = x|F = z) > 0, so constant for all x \u2208 f\u22121(z). Hence, we have H(Y |X) \u2264 H(Y |F ) with equality if and only if for each z \u2208 Z, x 7\u2192 P(Y = 1|X = x) is constant on f\u22121(z) \u2286 X . To conclude note that I(Y ;X) = H(Y )\u2212H(Y |X) and I(Y ;F ) = H(Y )\u2212H(Y |F ), so I(Y ;X) \u2265 I(Y ;F ) with equality if and only H(Y |X) = H(Y |F ) which holds if and only if for each z \u2208 Z, x 7\u2192 P(Y = 1|X = x) is constant on f\u22121(z) \u2286 X .\nB. Details of datasets used"
    },
    {
      "heading": "B1. Artificial Data",
      "text": "The artificial dataset was generated by scikit-learn\u2019s make_classification() function. We generated 2000 examples, each consisting of 20 features, only 2 of which were relevant for predicting the class. The examples belonged to 2 different clusters for each of the 2 classes, each cluster\u2019s points normally distributed (with unit standard deviation) about vertices of a 2-sided hypercube. Some label noise was added by randomly flipping the label of each point with probability 0.01. For more information see the function\u2019s documentation at http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html."
    },
    {
      "heading": "B2. UCI Datasets",
      "text": "Table 3 shows the characteristics of the real-world datasets used in our experiments. The original datasets are all from the UCI repository. Examples with missing values were discarded. The multiclass datasets were converted to balanced binary ones by setting the minority class as the \u2018positive\u2019 one and uniformly sampling examples from the remaining classes to form the \u2018negative\u2019 class. A link to the final datasets will be provided along with the code used to generate the results.\n6.1 C. Additional experimental results\nThis section contains additional experimental results that further showcase the consistency of the trajectory of the boosting ensemble on the information plane towards the lossless maximally compressing (LMC) model point across different datasets and hyperparameter settings.\nFigure 6 shows only the average trajectories of the boosting ensemble trained on the mushroom dataset using base learners of varying capacity, demonstrating that the\ntrajectories are again qualitatively similar. Naturally, the lower the capacity of an individual learner, the more boosting rounds are required to reach the LMC point.\nFigure 7 shows the same results as Figures 2\u20135 on artificial data generated as described in Section B1 of this Supplementary Material. It also demonstrates that changing the loss function, using subsampling of the examples for the purposes of the updates or shrinkage of the updates does not change the trajectory qualitatively."
    }
  ],
  "title": "Margin Maximization as Lossless Maximal Compression",
  "year": 2020
}
