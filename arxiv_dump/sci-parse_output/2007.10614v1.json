{
  "abstractText": "With the increasing sophistication of machine learning models, there are growing trends of developing model explanation techniques that focus on only one instance (local explanation) to ensure faithfulness to the original model. While these techniques provide accurate model interpretability on various data primitive (e.g., tabular, image, or text), a holistic Explainable Artificial Intelligence (XAI) experience also requires a global explanation of the model and dataset to enable sensemaking in different granularity. Thus, there is a vast potential in synergizing the model explanation and visual analytics approaches. In this paper, we present MELODY, an interactive algorithm to construct an optimal global overview of the model and data behavior by summarizing the local explanations using information theory. The result (i.e., an explanation summary) does not require additional learning models, restrictions of data primitives, or the knowledge of machine learning from the users. We also design MELODY UI, an interactive visual analytics system to demonstrate how the explanation summary connects the dots in various XAI tasks from a global overview to local inspections. We present three usage scenarios regarding tabular, image, and text classifications to illustrate how to generalize model interpretability of different data. Our experiments show that our approaches: (1) provides a better explanation summary compared to a straightforward information-theoretic summarization and (2) achieves a significant speedup in the end-to-end data modeling pipeline.",
  "authors": [
    {
      "affiliations": [],
      "name": "Gromit Yeuk-Yin Chan"
    },
    {
      "affiliations": [],
      "name": "Luis Gustavo Nonato"
    },
    {
      "affiliations": [],
      "name": "Brian Barr"
    },
    {
      "affiliations": [],
      "name": "Enrico Bertini"
    },
    {
      "affiliations": [],
      "name": "Cl\u00e1udio T. Silva"
    }
  ],
  "id": "SP:f9eafc6a0df90beccb967cd8aee9dd34974a8a9a",
  "references": [
    {
      "authors": [
        "A. Adadi",
        "M. Berrada"
      ],
      "title": "Peeking inside the black-box: A survey on explainable artificial intelligence (xai)",
      "venue": "IEEE Access,",
      "year": 2018
    },
    {
      "authors": [
        "S. Amershi",
        "D. Weld",
        "M. Vorvoreanu",
        "A. Fourney",
        "B. Nushi",
        "P. Collisson",
        "J. Suh",
        "S. Iqbal",
        "P.N. Bennett",
        "K. Inkpen"
      ],
      "title": "Guidelines for humanai interaction",
      "venue": "In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems,",
      "year": 2019
    },
    {
      "authors": [
        "J. Bien",
        "R. Tibshirani"
      ],
      "title": "Prototype selection for interpretable classification",
      "venue": "The Annals of Applied Statistics,",
      "year": 2011
    },
    {
      "authors": [
        "A. Bilal",
        "A. Jourabloo",
        "M. Ye",
        "X. Liu",
        "L. Ren"
      ],
      "title": "Do convolutional neural networks learn class hierarchy",
      "venue": "IEEE transactions on visualization and computer graphics,",
      "year": 2017
    },
    {
      "authors": [
        "N. Boukhelifa",
        "M.-E. Perrin",
        "S. Huron",
        "J. Eagan"
      ],
      "title": "How data workers cope with uncertainty: A task characterisation study",
      "venue": "In Proceedings of the 2017 CHI Conference on Human Factors in Computing Systems,",
      "year": 2017
    },
    {
      "authors": [
        "D.V. Carvalho",
        "E.M. Pereira",
        "J.S. Cardoso"
      ],
      "title": "Machine learning interpretability",
      "venue": "A survey on methods and metrics. Electronics,",
      "year": 2019
    },
    {
      "authors": [
        "V. Chandola",
        "V. Kumar"
      ],
      "title": "Summarization\u2013compressing data into an informative representation",
      "venue": "Knowledge and Information Systems,",
      "year": 2007
    },
    {
      "authors": [
        "M.S. Charikar"
      ],
      "title": "Similarity estimation techniques from rounding algorithms",
      "venue": "In Proceedings of the thiry-fourth annual ACM symposium on Theory of computing,",
      "year": 2002
    },
    {
      "authors": [
        "C. Chen",
        "O. Li",
        "D. Tao",
        "A. Barnett",
        "C. Rudin",
        "J.K. Su"
      ],
      "title": "This looks like that: deep learning for interpretable image recognition",
      "venue": "In Advances in Neural Information Processing Systems,",
      "year": 2019
    },
    {
      "authors": [
        "M. Craven",
        "J.W. Shavlik"
      ],
      "title": "Extracting tree-structured representations of trained networks. In Advances in neural information processing",
      "year": 1996
    },
    {
      "authors": [
        "I.S. Dhillon"
      ],
      "title": "Co-clustering documents and words using bipartite spectral graph partitioning",
      "venue": "In Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining,",
      "year": 2001
    },
    {
      "authors": [
        "I.S. Dhillon",
        "S. Mallela",
        "D.S. Modha"
      ],
      "title": "Information-theoretic coclustering",
      "venue": "In Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining,",
      "year": 2003
    },
    {
      "authors": [
        "M. Ghoniem",
        "J.-D. Fekete",
        "P. Castagliola"
      ],
      "title": "On the readability of graphs using node-link and matrix-based representations: a controlled experiment and statistical analysis",
      "venue": "Information Visualization,",
      "year": 2005
    },
    {
      "authors": [
        "L.H. Gilpin",
        "D. Bau",
        "B.Z. Yuan",
        "A. Bajwa",
        "M. Specter",
        "L. Kagal"
      ],
      "title": "Explaining explanations: An overview of interpretability of machine learning",
      "venue": "IEEE 5th International Conference on data science and advanced analytics (DSAA),",
      "year": 2018
    },
    {
      "authors": [
        "A. Grover",
        "J. Leskovec"
      ],
      "title": "node2vec: Scalable feature learning for networks",
      "venue": "In Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining,",
      "year": 2016
    },
    {
      "authors": [
        "R. Guidotti",
        "A. Monreale",
        "S. Ruggieri",
        "F. Turini",
        "F. Giannotti",
        "D. Pedreschi"
      ],
      "title": "A survey of methods for explaining black box models",
      "venue": "ACM computing surveys (CSUR),",
      "year": 2018
    },
    {
      "authors": [
        "M. Hlawatsch",
        "M. Burch",
        "D. Weiskopf"
      ],
      "title": "Visual adjacency lists for dynamic graphs",
      "venue": "IEEE transactions on visualization and computer graphics,",
      "year": 2014
    },
    {
      "authors": [
        "F. Hohman",
        "A. Head",
        "R. Caruana",
        "R. DeLine",
        "S.M. Drucker"
      ],
      "title": "Gamut: A design probe to understand how data scientists understand machine learning models",
      "venue": "In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems,",
      "year": 2019
    },
    {
      "authors": [
        "F. Hohman",
        "M. Kahng",
        "R. Pienta",
        "D.H. Chau"
      ],
      "title": "Visual analytics in deep learning: An interrogative survey for the next frontiers",
      "venue": "IEEE transactions on visualization and computer graphics,",
      "year": 2018
    },
    {
      "authors": [
        "F. Hohman",
        "H. Park",
        "C. Robinson",
        "D.H.P. Chau"
      ],
      "title": "Summit: Scaling deep learning interpretability by visualizing activation and attribution summarizations",
      "venue": "IEEE transactions on visualization and computer graphics,",
      "year": 2019
    },
    {
      "authors": [
        "K. Holstein",
        "J. Wortman Vaughan",
        "H. Daum\u00e9 III",
        "M. Dudik",
        "H. Wallach"
      ],
      "title": "Improving fairness in machine learning systems: What do industry practitioners need",
      "venue": "In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems,",
      "year": 2019
    },
    {
      "authors": [
        "M. Kahng",
        "P.Y. Andrews",
        "A. Kalro",
        "D.H.P. Chau"
      ],
      "title": "Activis: Visual exploration of industry-scale deep neural network models",
      "venue": "IEEE transactions on visualization and computer graphics,",
      "year": 2017
    },
    {
      "authors": [
        "B. Kim",
        "C. Rudin",
        "J.A. Shah"
      ],
      "title": "The bayesian case model: A generative approach for case-based reasoning and prototype classification",
      "venue": "In Advances in Neural Information Processing Systems,",
      "year": 1952
    },
    {
      "authors": [
        "B. Kim",
        "M. Wattenberg",
        "J. Gilmer",
        "C. Cai",
        "J. Wexler",
        "F. Viegas"
      ],
      "title": "Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav)",
      "venue": "In International Conference on Machine Learning,",
      "year": 2018
    },
    {
      "authors": [
        "O. Li",
        "H. Liu",
        "C. Chen",
        "C. Rudin"
      ],
      "title": "Deep learning for case-based reasoning through prototypes: A neural network that explains its predictions",
      "venue": "In Thirty-Second AAAI Conference on Artificial Intelligence,",
      "year": 2018
    },
    {
      "authors": [
        "Q. Li",
        "K.S. Njotoprawiro",
        "H. Haleem",
        "Q. Chen",
        "C. Yi",
        "X. Ma"
      ],
      "title": "Embeddingvis: A visual analytics approach to comparative network embedding inspection",
      "venue": "IEEE Conference on Visual Analytics Science and Technology (VAST),",
      "year": 2018
    },
    {
      "authors": [
        "Q.V. Liao",
        "D. Gruen",
        "S. Miller"
      ],
      "title": "Questioning the ai: Informing design practices for explainable ai user experiences",
      "venue": "arXiv preprint arXiv:2001.02478,",
      "year": 2020
    },
    {
      "authors": [
        "M. Liu",
        "S. Liu",
        "H. Su",
        "K. Cao",
        "J. Zhu"
      ],
      "title": "Analyzing the noise robustness of deep neural networks",
      "venue": "IEEE Conference on Visual Analytics Science and Technology (VAST),",
      "year": 2018
    },
    {
      "authors": [
        "M. Liu",
        "J. Shi",
        "Z. Li",
        "C. Li",
        "J. Zhu",
        "S. Liu"
      ],
      "title": "Towards better analysis of deep convolutional neural networks",
      "venue": "IEEE transactions on visualization and computer graphics,",
      "year": 2016
    },
    {
      "authors": [
        "S. Liu",
        "P.-T. Bremer",
        "J.J. Thiagarajan",
        "V. Srikumar",
        "B. Wang",
        "Y. Livnat",
        "V. Pascucci"
      ],
      "title": "Visual exploration of semantic relationships in neural word embeddings",
      "venue": "IEEE transactions on visualization and computer graphics,",
      "year": 2017
    },
    {
      "authors": [
        "S.M. Lundberg",
        "S.-I. Lee"
      ],
      "title": "A unified approach to interpreting model predictions",
      "venue": "In Advances in neural information processing systems,",
      "year": 2017
    },
    {
      "authors": [
        "D. Martens",
        "B. Baesens",
        "T. Van Gestel"
      ],
      "title": "Decompositional rule extraction from support vector machines by active learning",
      "venue": "IEEE Transactions on Knowledge and Data Engineering,",
      "year": 2008
    },
    {
      "authors": [
        "Y. Ming",
        "S. Cao",
        "R. Zhang",
        "Z. Li",
        "Y. Chen",
        "Y. Song",
        "H. Qu"
      ],
      "title": "Understanding hidden memories of recurrent neural networks",
      "venue": "IEEE Conference on Visual Analytics Science and Technology (VAST),",
      "year": 2017
    },
    {
      "authors": [
        "Y. Ming",
        "H. Qu",
        "E. Bertini"
      ],
      "title": "Rulematrix: Visualizing and understanding classifiers with rules",
      "venue": "IEEE transactions on visualization and computer graphics,",
      "year": 2018
    },
    {
      "authors": [
        "Y. Ming",
        "P. Xu",
        "F. Cheng",
        "H. Qu",
        "L. Ren"
      ],
      "title": "Protosteer: Steering deep sequence model with prototypes",
      "venue": "IEEE Transactions on Visualization and Computer Graphics,",
      "year": 2019
    },
    {
      "authors": [
        "Y. Ming",
        "P. Xu",
        "H. Qu",
        "L. Ren"
      ],
      "title": "Interpretable and steerable sequence learning via prototypes",
      "venue": "In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining,",
      "year": 2019
    },
    {
      "authors": [
        "S. Mohseni",
        "N. Zarei",
        "E.D. Ragan"
      ],
      "title": "A survey of evaluation methods and measures for interpretable machine learning",
      "venue": "arXiv preprint arXiv:1811.11839,",
      "year": 2018
    },
    {
      "authors": [
        "T. M\u00fchlbacher",
        "L. Linhardt",
        "T. M\u00f6ller",
        "H. Piringer"
      ],
      "title": "Treepod: Sensitivity-aware selection of pareto-optimal decision trees",
      "venue": "IEEE transactions on visualization and computer graphics,",
      "year": 2017
    },
    {
      "authors": [
        "M. Muller",
        "I. Lange",
        "D. Wang",
        "D. Piorkowski",
        "J. Tsay",
        "Q.V. Liao",
        "C. Dugan",
        "T. Erickson"
      ],
      "title": "How data science workers work with data: Discovery, capture, curation, design, creation",
      "venue": "In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems,",
      "year": 2019
    },
    {
      "authors": [
        "S. Navlakha",
        "R. Rastogi",
        "N. Shrivastava"
      ],
      "title": "Graph summarization with bounded error",
      "venue": "In Proceedings of the 2008 ACM SIGMOD international conference on Management of data,",
      "year": 2008
    },
    {
      "authors": [
        "M. Okoe",
        "R. Jianu",
        "S. Kobourov"
      ],
      "title": "Node-link or adjacency matrices: Old question, new insights",
      "venue": "IEEE transactions on visualization and computer graphics,",
      "year": 2018
    },
    {
      "authors": [
        "N. Pezzotti",
        "T. H\u00f6llt",
        "J. Van Gemert",
        "B.P. Lelieveldt",
        "E. Eisemann",
        "A. Vilanova"
      ],
      "title": "Deepeyes: Progressive visual analytics for designing deep neural networks",
      "venue": "IEEE transactions on visualization and computer graphics,",
      "year": 2017
    },
    {
      "authors": [
        "N. Poerner",
        "B. Roth",
        "H. Sch\u00fctze"
      ],
      "title": "Evaluating neural network explanation methods using hybrid documents and morphological agreement",
      "venue": "arXiv preprint arXiv:1801.06422,",
      "year": 2018
    },
    {
      "authors": [
        "G. Ras",
        "M. van Gerven",
        "P. Haselager"
      ],
      "title": "Explanation methods in deep learning: Users, values, concerns and challenges",
      "venue": "In Explainable and Interpretable Models in Computer Vision and Machine Learning,",
      "year": 2018
    },
    {
      "authors": [
        "P.E. Rauber",
        "S.G. Fadel",
        "A.X. Falcao",
        "A.C. Telea"
      ],
      "title": "Visualizing the hidden activity of artificial neural networks",
      "venue": "IEEE transactions on visualization and computer graphics,",
      "year": 2016
    },
    {
      "authors": [
        "M.T. Ribeiro",
        "S. Singh",
        "C. Guestrin"
      ],
      "title": "why should i trust you?\u201d explaining the predictions of any classifier",
      "venue": "In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining,",
      "year": 2016
    },
    {
      "authors": [
        "A. Rule",
        "A. Tabard",
        "J.D. Hollan"
      ],
      "title": "Exploration and explanation in computational notebooks",
      "venue": "In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems,",
      "year": 2018
    },
    {
      "authors": [
        "V. Satopaa",
        "J. Albrecht",
        "D. Irwin",
        "B. Raghavan"
      ],
      "title": "Finding a\u201c kneedle\u201d in a haystack: Detecting knee points in system behavior",
      "venue": "In 2011 31st international conference on distributed computing systems workshops,",
      "year": 2011
    },
    {
      "authors": [
        "S. Sawada",
        "M. Toyoda"
      ],
      "title": "Model-agnostic visual explanation of machine learning models based on heat",
      "year": 2019
    },
    {
      "authors": [
        "A. Shrikumar",
        "P. Greenside",
        "A. Kundaje"
      ],
      "title": "Learning important features through propagating activation differences",
      "venue": "In Proceedings of the 34th International Conference on Machine Learning-Volume",
      "year": 2017
    },
    {
      "authors": [
        "K. Simonyan",
        "A. Vedaldi",
        "A. Zisserman"
      ],
      "title": "Deep inside convolutional networks: Visualising image classification models and saliency maps",
      "venue": "In Workshop at International Conference on Learning Representations,",
      "year": 2014
    },
    {
      "authors": [
        "H. Strobelt",
        "S. Gehrmann",
        "M. Behrisch",
        "A. Perer",
        "H. Pfister",
        "A.M. Rush"
      ],
      "title": "S eq 2s eq-v is: A visual debugging tool for sequence-to-sequence models",
      "venue": "IEEE transactions on visualization and computer graphics,",
      "year": 2018
    },
    {
      "authors": [
        "H. Strobelt",
        "S. Gehrmann",
        "H. Pfister",
        "A.M. Rush"
      ],
      "title": "Lstmvis: A tool for visual analysis of hidden state dynamics in recurrent neural networks",
      "venue": "IEEE transactions on visualization and computer graphics,",
      "year": 2017
    },
    {
      "authors": [
        "M. Sundararajan",
        "A. Taly",
        "Q. Yan"
      ],
      "title": "Axiomatic attribution for deep networks",
      "venue": "In Proceedings of the 34th International Conference on Machine Learning-Volume",
      "year": 2017
    },
    {
      "authors": [
        "F. . Tzeng",
        "K. . Ma"
      ],
      "title": "Opening the black box - data driven visualization of neural networks",
      "venue": "In VIS 05. IEEE Visualization,",
      "year": 2005
    },
    {
      "authors": [
        "S. Van Den Elzen",
        "J.J. van Wijk"
      ],
      "title": "Baobabview: Interactive construction and analysis of decision trees",
      "venue": "In 2011 IEEE conference on visual analytics science and technology (VAST),",
      "year": 2011
    },
    {
      "authors": [
        "P. Voigt",
        "A. Von dem Bussche"
      ],
      "title": "The eu general data protection regulation (gdpr)",
      "venue": "A Practical Guide,",
      "year": 2017
    },
    {
      "authors": [
        "J. Wexler",
        "M. Pushkarna",
        "T. Bolukbasi",
        "M. Wattenberg",
        "F. Vi\u00e9gas",
        "J. Wilson"
      ],
      "title": "The what-if tool: Interactive probing of machine learning models",
      "venue": "IEEE transactions on visualization and computer graphics,",
      "year": 2019
    },
    {
      "authors": [
        "K. Wongsuphasawat",
        "D. Smilkov",
        "J. Wexler",
        "J. Wilson",
        "D. Mane",
        "D. Fritz",
        "D. Krishnan",
        "F.B. Vi\u00e9gas",
        "M. Wattenberg"
      ],
      "title": "Visualizing dataflow graphs of deep learning models in tensorflow",
      "venue": "IEEE transactions on visualization and computer graphics,",
      "year": 2017
    },
    {
      "authors": [
        "T. Wu",
        "M.T. Ribeiro",
        "J. Heer",
        "D.S. Weld"
      ],
      "title": "Errudite: Scalable, reproducible, and testable error analysis",
      "venue": "In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,",
      "year": 2019
    },
    {
      "authors": [
        "S. Xiang",
        "X. Ye",
        "J. Xia",
        "J. Wu",
        "Y. Chen",
        "S. Liu"
      ],
      "title": "Interactive correction of mislabeled training data",
      "venue": "IEEE Conference on Visual Analytics Science and Technology (VAST),",
      "year": 2019
    },
    {
      "authors": [
        "H. Yang",
        "C. Rudin",
        "M. Seltzer"
      ],
      "title": "Scalable bayesian rule lists",
      "venue": "In Proceedings of the 34th International Conference on Machine Learning- Volume",
      "year": 2017
    },
    {
      "authors": [
        "M.D. Zeiler",
        "R. Fergus"
      ],
      "title": "Visualizing and understanding convolutional networks",
      "venue": "In European conference on computer vision,",
      "year": 2014
    },
    {
      "authors": [
        "X. Zhao",
        "Y. Wu",
        "D.L. Lee",
        "W. Cui"
      ],
      "title": "iforest: Interpreting random forests via visual analytics",
      "venue": "IEEE transactions on visualization and computer graphics,",
      "year": 2018
    }
  ],
  "sections": [
    {
      "text": "Index Terms\u2014explainable machine learning, information theory, graph visualization, visual analytics"
    },
    {
      "heading": "1 INTRODUCTION",
      "text": "\u201cAll models are wrong, but some are useful.\u201d The application of machine learning (ML) models, including deep learning neural networks, is prevalent in all aspects of human activities and nowadays the main driving force of technological advances such as self-driving cars, personal assistants and medical diagnoses. While there are more new\n\u2022 Gromit Yeuk-Yin Chan, Enrico Bertini and Cla\u0301udio T. Silva are with New York University. E-mail: gromit.chan,enrico.bertini,csilva@nyu.edu. \u2022 Luis Gustavo Nonato is with University of Sa\u0303o Paulo. E-mail: gnonato@icmc.usp.br. \u2022 Brian Barr is with Capital One. E-mail: brian.barr@capitalone.com.\nmodels and architectures proposed to improve the accuracies of different tasks, the reason for such popularity also implies that there is no silver bullet for creating the best model. Hence, the creation of an ML model is a human-centric activity that involves lots of reasoning, brainstorming, and most importantly \u2013 understanding processes. Understanding how a model works on one\u2019s own data improves the task performances in a holistic scope not only limited to the model design but also the data preprocessing and feature engineering steps. Yet, ML models nowadays introduce a challenging problem on their interpretability. It becomes so complex to understand what the models have learned that using them as a black box could result in adversely affecting people\u2019s safety, financial, or legal status [57].\nThus, explainable Artificial Intelligence (XAI) becomes an emerging research field where a lot of efforts have been devoted to extracting the\nar X\niv :2\n00 7.\n10 61\nlogic behind how the models think when making decisions. Overall, these logical models focus on the usage of decision trees, rules, and instance-level feature importance to mimic or customize the behavior of the ML models [16] so that people can understand how the model works through decision paths or scoring systems. In particular, instancelevel feature importance explanations become more popular to explain sophisticated models. It produces an accurate local explanation as they only focus on a single instance. Such customization can even allow the explainer to be embedded in the ML model [3, 9, 23, 25]. Thus, local explanations have been readily proposed not only in explaining tabular data classification [31, 46] but also complex deep learning tasks in natural language processing [43] and computer vision [9].\nOf course, the ability to customize does not come as a free lunch. As the explanation is tailored towards an individual instance, the explanation model loses the advantages of providing aggregated explanations to generalize on the whole dataset. This limits its usage on providing simple textual information or visualization to describe how the model works. Such a limit, however, is where visualization techniques come in handy. We observe that most feature importance based explanations in current literature can be summarized [7] into an explanation summary. The goal of summarization (Figure 2) is to find a compact description of the dataset with a minimum cost of information loss (i.e., information theoretic). In other words, it finds a explanation summary of the ML model, which is the groups of instances with similar explanations(colored regions) and the groups of features that are used to explain similar sets of instances (dashed lines). Thus, the summary is a compact global explanation that enables effective visualization to communicate a model\u2019s general behavior. To fill the gaps of local explanation techniques on XAI tasks, we propose a scalable data summarization technique that only takes the generic form of the explanation information into account so that we can leverage the existing explanation techniques on different domains to provide useful visual data summaries. In MELODY UI, we show that our implementations helps establish a holistic workflow for XAI experience concerning tabular data, texts or images. In short, our contributions are as follows: \u2022 MELODY, a scalable algorithm that generates a compact data\nsummary for an ML model and input data. It takes any generic feature importance based explanations from the model and works for both structured and unstructured data. The algorithm consists of (1) an information-theoretic model to determine the best data summary and (2) an efficient sketching technique to speed up the computation. In Section 7.2 we show that MELODY produces meaningful results and scales to large data. \u2022 MELODY UI, an interactive system for scalable interpretation and exploration of the input data and the ML model together. By leveraging our algorithm to group similar instances and explanations, we enable a seamless workflow that connects different needs regarding the global, local, and class explanations in the current XAI systems [27]. \u2022 Three use cases covering ML model interpretations on tabular data, image, and text. We demonstrate that our algorithm and system enhance the XAI user experiences on model interpretability to three mainstream data analysis."
    },
    {
      "heading": "2 RELATED WORK",
      "text": "To facilitate human understanding towards complex models through visualization, research mainly focus on visualization on three aspects: model internals, logics induced from the models, and instance level feature vectors that describe the behavior of the model."
    },
    {
      "heading": "2.1 Visualization of Model Internals",
      "text": "Visualization has been applied readily to understand and interact with deep learning neural networks. In fact, a survey about deep learning visual analytics by Hohman et al. [19] has listed more that 40 representative works in this area in the last 5 years. We encourage the readers to read the survey paper for a deeper investigation to the subject.\nThe simplest form of a neural network can be represented by a nodelink diagram in which each node represents a neuron and link presents a connection weight between two neurons [55]. As nowadays the ways neurons are connected become more sophisticated and opaque, various visual analytics approaches have been developed to understand different properties of the networks. RNNVis [33] and LSTMVis [53] address the understanding of recurrent neural network (RNN) by visualizing the bipartite relationship between hidden memories and input data and hidden memory dynamics with parallel coordinates respectively. Autoencoder is addressed by Seq2SeqVis which proposes a bipartite graph visualization to visualize the attention relationships between input and its possible translations to enable model debugging [52]. Another type of popular models for image classification is Convolutional Neural Networks (CNN). CNNVis [29], Blocks [4], AEVis [28] and Summit [20] are graph visualizations that aggregate similar neurons, connections, and similar activated image patches to convey learned visual representations from the model.\nBesides visualizing the structures of a neural network, there are visual analytics systems that assist the model development processes in the industry. ActiVis [22] is a visual analytics system used by Facebook to explore industrial deep learning models. Google has developed TensorFlow Graph [59] and what-if tool [58] to help developers understand and test the behavior of different ML models.\nThe work in these criteria mainly addresses the visual analysis for model developers who have sufficient knowledge of the methodologies of their models. However, a more general AI tool requires the assessment and involvement of end-users, decision-makers, and domain experts. Addressing the needs of border XAI user experience, our work focuses on providing general explanations of ML models to users without requiring them to know the architectures."
    },
    {
      "heading": "2.2 Visualization of Logical Models",
      "text": "Logical models like decision trees [10] or rules [32,62] can address the interpretation of complex models by using them to infer an approximated model from any ML models. Given a set of test data, the original model gives the predictions and the logical models use them as labels to train another classifier. The resulting classifier can be used to mimic the behavior of the original model while providing good interpretability to the users. Through visualizing logical models, users gain knowledge of the model\u2019s capability.\nRule Matrix [34] is proposed to build and visualize the surrogate rule list to understand the model\u2019s behavior by interacting with the rules. Gamut [18] uses generalized additive models to construct a set of linear functions for each feature in the dataset to understand models through line charts. TreePOD [38] and Baobabview [56] visualizes the decision trees with different metrics incorporated for model understanding. iForest [64] visualizes random forests with data flow diagrams to interact with multiple decision paths.\nFor complex model, using logical models to explain the complex model is the consideration of fidelity \u2013 the accuracy of the explanation on the original model. It creates an additional layer of performance concerns. Therefore, local explanation methods are proposed to explore the possibility to provide accurate explanations or even be embedded in the original model training process. Yet, they only return results for an instance and do not consider a global explanation to the whole dataset, our work addresses the challenges of visually constructing a global view for local explanations."
    },
    {
      "heading": "2.3 Feature Vector Visualization",
      "text": "Local explanation models give feature scores for each instance. The features can be the features from original data [31, 46, 50] or a set of external information like concepts [24] or training data [9, 25, 36]. Visual analysis can be directly applied to interact with the features [35] or the feature vectors can be visualized as a matrix where rows represent the instances and columns represent the features [49].\nBesides, the data comes out from a deep neural network can appear as embeddings such that the linear distances between vectors represent their similarities as the model\u2019s rationale. The main visualization technique to understand these feature vectors is projection [15,26,30,42,45,61]. Treating the embedding as high dimensional data projection techniques such as tSNE, MDS, or PCA are applied to discover semantic groups inside the dataset from the resulting scatterplot. Users can assess the ML model and refine the original data from the brushing the filtering interactions in the projection.\nOur technique identifies the scalability and usability challenges in the existing visualization. The projection technique mainly suffers from cluttering and the lack of feature information in the visualization which is crucial for a comprehensive explanation. MELODY aims at providing compact representations of both data and features so that visual information is more precise. Also, we address the needs of explanation exploration with different granularity by the proposed analytic workflow illustrated by MELODY UI, providing new ways to extend the powerful local explanations to scalable visual analytics."
    },
    {
      "heading": "3 TASKS ANALYSIS OF XAI SYSTEMS",
      "text": "Before we propose our methodology to generate an explanation summary for feature importance based explanations, we first review the taxonomy of XAI tasks to induce the reasons how can a visual explanation summary of ML model help. By understanding the tasks, we can consolidate the design considerations to expand our techniques into an effective user interface. To explain the tasks and the use of data summary systematically, we use a simple workflow of XAI (Figure 3) to connect the essential relationships among three main XAI tasks [27]: Global, Local, and Class explanations. T.1 Global Explanation. The goal is to understand the overall weights of features used by the model to explain how AI makes decisions on the dataset in general. For example, imagine we have a model that tells what animal does an image contains. To understand the model, the first question a user might ask is what features the model uses to make a prediction? An XAI technique might give us a sorted list of features based on their influences to the model (Figure 3.1) \u2013 it tells that \u201cskin color\u201d is the most important factor. T.2 Class Explanation. Understanding how and whether the model works in each class allows users to understand the decision boundaries in a smaller granularity to develop insights. For example, from a global explanation, \u201ceyes\u201d are used to predict many cats. \u201cEyes\u201d and \u201ccat\u201d are the key information to understand the model rationale in a local region. T.3 Local Explanation. For verification and inspections in full details, users need to inspect all explanatory features of a predicted instance (Figure 3.3). For example, why is there a cat predicted as \u201cdog\u201d? The\ndifference between instances\u2019 behavior can evaluate important decision boundaries. We can know that the image\u2019s cat has white skin, which is a \u201cdog\u201d feature by inspecting a single image. Usefulness of Explanation Summary. First, it can provide a global explanation with a better granularity (Figure 3.2). Instead of aggregating the whole dataset to rank the features, it tells directly that \u201ceyes\u201d are used on many cats while \u201cears\u201d are used on many dogs. This answer avoids the mirage of aggregated features over different subsets. Also, clustering instances allows users to go from local to global explanation. For example, by browsing a cat image and knowing it has a wrong prediction due to its white skin, we might want to know all the cat images with the \u201cwhite skin\u201d feature will be predicted as \u201cdog\u201d. By inspecting all the cat images or images that have \u201cwhite skins\u201d (Figure 3.4), we go back to the inspection of a group of images again.\nIn detail, the tasks in the above workflow generalize the studies that consolidate the key user requirements of model explainability [1, 6, 14, 16, 37, 44]. Furthermore, there is a plenty of empirical studies on the requirement of XAI from industry practitioners [2, 5, 18, 21, 27, 39, 47]. They provide good empirical evidence from real experts to outline the guidelines for designing XAI systems. The details of T.1-3 derived from these studies are provided in Appendix 9.2."
    },
    {
      "heading": "4 MACHINE LEARNING MODEL EXPLANATION SUMMARY",
      "text": "In this section, we describe the definition of a model explanation summary as well as the algorithms to compute it from the local explanations."
    },
    {
      "heading": "4.1 Generic Representation of Local Explanation",
      "text": "The most generic form of a local explanation ei is a feature vector with n total number of explanatory features used in explaining the whole dataset. Each value in the feature vector ei j is the explanation importance of feature j on the instance i (e.g. \u201cskin color\u201d on a cat image). For more details of local explanation techniques, we redirect readers to Appendix 9.1. All m instances\u2019 explanations from the whole dataset thus can be expressed as a real-value matrix E \u2208 Rm,n. One important property of the matrix is that it is sparse i.e. nm nnz(E) where nnz(E) is the number of nonzeros in E. It ensures the explanation to use a small number of features to explain the model behavior so that the decision logic would not overwhelm the user. Also, to simplify the discussion afterwards, we assume ei j \u2265 0 and \u2211i, j ei j = 1 1."
    },
    {
      "heading": "4.2 Explaining Tabular, Image, and Text Instances",
      "text": "Given a generic form of instance explanation, we now drill down to an in-depth discussion of how these feature vectors can be applied to explain tabular, image, and text instances. Although all of them result in explanation matrices, their intrinsic nature, shown in Table 1, affects how we modify our modeling pipelines to construct the features (i.e., columns) to acquire meaningful explanations. We provide end-to-end data modeling examples in Appendix 9.3.\n1All matrices can satisfy these properties with a min-max scaler (on its absolute values if the sign does not matter).\nTabular Data. Logical models like decision trees and random forests discretize the attributes in the dataset to a set of logics. Similarly, the explanatory features should be not only the original attributes but also the different ranges for better diversity. For example, all cities (instances) rain based on their precipitations (attribute), but how each city rain in different percentages of precipitation (ranges) reveals different climates. Image Data. Users normally classify images with the common visual features among the same entity (e.g., stripes in zebras). Similarly, an image in an ML model can be explained with representative image patches collected from the original data that unify the reasoning process with a limited set of features instead of pixels in each image. Text Data. Multiple documents are usually explained with common topics instead of single phrases because similarly meant data can be totally different words. For example, \u201cgood\u201d and \u201cgreat\u201d represent similar sentiments. Thus, the explanatory features should be a set of topics instead of words to avoid an overly sparse matrix."
    },
    {
      "heading": "4.3 Problem Definition",
      "text": "The information-theoretic goal for summarizing the explanation matrix is to group similar instances and explanatory features simultaneously that balance compactness and information loss. Let R and C be the set of row (instance) and column (feature) vectors in E respectively such that E is equivalent to a joint distribution between R and C (i.e. p(R,C)). Our goal is to find the optimal row and column clusters R\u0302 and C\u0302 so that it presents the explanation summary in Figure 2. Therefore, the first question is, how we should measure the information loss? For example, consider the following synthetic explanation matrix below:\np(R,C) = .1 .1 0 0.1 .1 0 00 0 .2 .2 0 0 0 .2  It is obvious to group the rows into two clusters: r\u03021 = {r1,r2}, r\u03022 = {r3,r4} and the columns into two clusters: c\u03021 = {c1,c2}, c\u03022 = {c3,c4}. The information theoretic definition of the resulting compression p(R\u0302,C\u0302) and the approximation matrix recovered from the compression q(R\u0302,C\u0302) are as follows [12]:\np(R\u0302,C\u0302) = [ .4 0 0 .6 ] , q(R\u0302,C\u0302) = .1 .1 0 0.1 .1 0 00 0 .133 .267 0 0 .067 .133  Each entry in the approximation matrix q(R\u0302,C\u0302) is calculated as follows:\nq(r,c) = p(r\u0302, c\u0302)\u00d7 pR(r) pR\u0302(r\u0302) \u00d7 pC(c) pC\u0302(c\u0302)\n(1)\nFor example, q(3,4) = .6\u00d7 (.4)/(.6)\u00d7 (.4)/(.6) = 0.267. Thus, the compression loss can be expressed with metrics such as KullbackLeibler (KL) divergence of p(R,C) from q(R\u0302,C\u0302):\nDKL(P,Q) = \u2211 x\u2208\u03c7 P(x)log( P(x) Q(x) ) (2)\nYet, we observe a shortcoming of directly using KL divergence on the whole matrix. The P(x) in Equation 2 tells us that each entry\u2019s contribution to the result is not independent to the clusters that it does not belong to. Therefore, we propose a loss function D(R\u0302,C\u0302) such that each entry\u2019s loss is marginal to its row and column cluster:\nD(R\u0302,C\u0302) = \u2211 r\u0302\u2208R\u0302 DKL(P(r \u2208 r\u0302,C),Q(r \u2208 r\u0302,C))\n+ \u2211 c\u0302\u2208C\u0302\nDKL(P(R,c \u2208 c\u0302),Q(R,c \u2208 c\u0302)) (3)\nSuch a marginalization prevents entries with high values dominating the calculation result, which we will demonstrate the subsequent improvement in Section 7.2.\nOnce we quantify the information loss, the next challenge is how should we choose the number of row and column clusters? If we do not cluster any rows and columns at all, D will equal to zero. Whereas if we only have one cluster, the loss will be huge. Yet, neither of them is a summary of the data as it either represent the original matrix or a summary with poor quality. To automatically determine the optimal partitions, the idea is to use Minimum Description Length Principle (MDL), which states that the best model is the one that minimizes the total description length of the expression: model (i.e., number of clusters\n\u2225\u2225R\u0302\u2225\u2225 and \u2225\u2225C\u0302\u2225\u2225) \u2212 correction (i.e., information loss D). Putting them all together, we can now write the total cost function T as:\nT (R\u0302;C\u0302) = \u03b2R \u2225\u2225R\u0302\u2225\u2225+\u03b2C \u2225\u2225C\u0302\u2225\u2225+D(R\u0302,C\u0302) (4)\nwhich we try to minimize it with the best rows and columns partitions. \u03b2R and \u03b2C are user defined parameters to penalize large number of clusters. Users can increase the values to produce fewer clusters."
    },
    {
      "heading": "4.4 The MELODY Algorithm",
      "text": "We now present our MELODY (MachinE Learning MODel SummarY) algorithm. In the previous section we have created our goal to find the row and column clusters that minimize the cost function in Equation 4 among all possible number of clusters and all possible rows and columns combinations. Yet, the equation itself does not tell us how to reach the solution efficiently. Since the matrix can be considered as a graph where each entry is a weighted edge between a row node and a column node, we can use graph summarization [40] approach to provide a baseline solution (Algorithm 1). The overall idea is as follows:\n1. Each row and column starts in its own cluster. Then, we put the row and column clusters into two separate lists (line 1-2).\n2. We first fix the column cluster assignment. For the row clusters in the list, we randomly select a row cluster (line 5).\n3. We compare the selected row cluster with the remaining row clusters in the list as merge candidates (line 7-12): we try merging the selected cluster with each remained cluster and calculate the cost reduction by Equation 4 (line 8). We choose the candidate that produces the least cost.\n4. If merging the selected cluster and its best candidate reduces the total cost, then we merge two clusters in the list (line 14-15). Otherwise, we remove the selected cluster in the list (line 17). Either way, the list will have one fewer item.\n5. We repeat steps 2-4, but we fix the row clusters and merge the column clusters instead. The whole algorithm stops until there are no clusters remained in both lists.\nOverall, in every iteration, a row (column) needs to measure the cost reductions with the remaining candidates in the list, which has the maximum size of ||R|| (||C||). Therefore, the time complexity of the basic algorithm is O(||R||2 + ||C||2). As a quadratic algorithm is infeasible for any moderately sized data for exploratory visual analysis, we now propose a speed-up strategy to make our algorithm suitable for interactive performance.\nALGORITHM 1: MELODY (MachinE Learning MODel SummarY) Input :R,C \u2013 instances and explanatory features\n\u03b2R, \u03b2C \u2013 regularization terms Output :R\u0302,C\u0302 \u2013 row and column clusters\n1 R\u2190 [{r1},{r2}, ...,{rm}], R\u0302 \u2190 {} /* intialize rows */ 2 C\u2190 [{c1},{c2}, ...,{cn}], C\u0302 \u2190 {} /* intialize columns */ 3 loss \u2190 D(R,C) /* initialize loss function */ 4 while size(R)> 0 and size(C)> 0 do 5 r0\u2190 random pop(R) /* randomly extract a cluster */ 6 \u2206Lmax \u2190 0, rmax \u2190 undefined 7 for r \u2190 R do 8 \u2206L \u2190 \u03b2R\u2212DKL({r\u222a r0},C\u222aC\u0302) 9 if \u2206L > \u2206Lmax then 10 \u2206Lmax \u2190 \u2206L, rmax \u2190 r 11 end 12 end 13 if \u2206Lmax > 0 then 14 rmax \u2190 {rmax \u222a r0} /* merge two clusters */ 15 else 16 R\u0302 .push(r0) /* push the cluster to final result */ 17 end\n/* same procedure as for C... */\n18 end"
    },
    {
      "heading": "4.4.1 Speed Up Strategies With Data Sketches",
      "text": "While a randomized bottom-up algorithm scales linearly, Algorithm 1 is time-consuming as it needs an extra loop to compare all possible row or column clusters (line 7) in every iteration. However, if we look at the example matrix in Section 4.3, it is obvious that the first two rows (columns) are completely different from the last two rows (columns). Comparing candidates that are different indeed is of no use since they are unlikely to reduce the total cost. Thus, to speed up the algorithm, we propose a k-nearest neighbor query strategy with a novel use of locality sensitive hashing (LSH) [8] scheme to encode a row of column clusters. LSH defines a family of hash functions (i.e., sketches) [h1(vi),h2(vi), ...,hn(vi)] for a vector vi so that the probability of hash collisions between two vectors is proportional to their euclidean distances (i.e., sim(vi,v j)\u223c Pr[hk(vi) = hk(v j)]). Vectors with similar values thus can be stored in the same buckets in an LSH table. Furthermore, we can extend this proportional to retrieve similar row (column) clusters. If two clusters have many similar vectors, then the number of hash collisions will be high. Therefore, the top-k clusters from the query will likely be similar neighbors.\nThe query algorithm is illustrated in Algorithm 2. First, an LSH table needs to be built for rows and columns, respectively. Then, when a neighbor query is performed, we can use the hash keys from the query\u2019s vectors to perform a table look-up to retrieve all the collided entries with the entries in the cluster ( subroutine query lsh table in line 2). We count the average number of collisions between the entries from the query cluster and the ones from the candidate clusters (line 5) and return the top k clusters with the highest number. This can drastically reduce the number of comparisons and the running time when the matrices are large (Section 7.2)."
    },
    {
      "heading": "4.4.2 Strategies Addressing Skewness and Sparsity",
      "text": "Empirically, we observe two challenges when computing the results from real datasets in which we provide the following heuristics to address the problems and demonstrate the effectiveness in Section 7.2: Smoothing the explanation values: When an explanation model assigns values to important features of an instance, the values can be very high (e.g., extremely sensitive features). It could affect the calculation of loss function (Equation 3) and prevent instances with similarly activated features from being grouped. Therefore, to have an even data distribution in the explanation matrix, we set the maximum value to the knee point of the overall value distributions in the matrix using a knee finding algorithm [48].\nALGORITHM 2: Top-k Nearest Neighbor Query /* Initialize TR \u2190 build lsh table(R) and TC \u2190\nbuild lsh table(C) after line 2 in Algo. 1 */ /* Replace R with query(r,R,TR,k) in line 7 of Algo. 1 */ Input :v \u2013 query cluster\nV, Tv \u2013 remaining clusters and LSH table k \u2013 number of neighbors\nOutput :knn\u2013 top k nearest neighbors 1 counter \u2190Counter() /* initialize counter */ 2 neighbors \u2190 query lsh table(v,Tv) /* get collided entries */ 3 for n \u2190 neighbors do 4 for v\u0304 in V do 5 if n in v\u0304 then /* collision between the clusters */ 6 counter[v\u0304] + = 1/|v\u0304| 7 break 8 end 9 end\n10 end 11 knn\u2190 counter.most common(k)\nPre-clustering for a cold start in a sparse environment: Given a sparse explanation matrix, the bottom-up approach might face difficulties in cluster entries when the cost function is stuck a local minimum. Also, as the matrix is sparse, it is hard for the algorithm to know whether there are cluster structures at the beginning. These adversely affect the formation of significant clusters. To address this cold start problem, we reference from spectral graph partitioning [11] to create relatively smaller partitions of rows and columns using their singular vectors from SVD decomposition. Then, we can use our information-theoretic objective function to compress the matrix further."
    },
    {
      "heading": "5 DESIGN CONSIDERATIONS",
      "text": "Based on Section 3 and Section 4, we distill the main design considerations for an interactive visualization interface for addressing a holistic XAI workflow and the summary\u2019s characteristics. Considerations in\nPINK address the tasks in Section 3 and those in BLUE address the data perspective in Section 4.\nC.1 VISUAL SUMMARY Synthesize instance and feature summary. Clusters of instances and clusters of features should be displayed together to understand the decision boundaries(knowledge of a model) on different subsets (the influenced population).\nC.2 SPARSE SUMMARY Scalable visualization for large sparse data. As the local explanations are highly customized and independent, the explanation summary will also be a sparse matrix. The visualization needs to highlight small but significant co-clusters.\nC.3 PREDICTION OUTCOME Display instances\u2019 outcomes from the model. Knowing when and where the model can fail is essential to understand its capability. Thus, the prediction outcome should be embedded in the visual summary and explanations.\nC.4 FILTERING Filtering data summary by classes or features. When a user collects insights from local or class explanations, the insights need to be verified on a larger population. Thus, filtering by classes or features act as a query from a local analysis to refine the explanation summary in a global view.\nC.5 LEVEL-OF-DETAIL Different level-of-detail presentations for tabular, image and text data. Level-of-details may come in different forms for different data primitives. Although the explanation summaries are the same, when users drill down on details, the presentations should be different.\nC.6 EXPLANATION IN A LOOP Connecting local, global, and class explanations as a loop. The main three themes of XAI should be connected for a complete ML model explanation (Figure 3). Different views related to different scopes of explanations should be tightly integrated."
    },
    {
      "heading": "6 MELODY UI",
      "text": "Based on our design considerations in Section 5 and our MELODY algorithm in Section 4, we present MELODY UI, an interactive system for helping users to understand an ML model\u2019s decisions on an input dataset 2. The interface consists of (A) a main explanation summary visualization, (B) an original data subset view from a selected summary, and (C) an instance view. In the following discussion, we will focus on the main summary visualization and how an XAI workflow in Figure 3 is established in a visual analytics fashion."
    },
    {
      "heading": "6.1 Explanation Summary Visualization Design",
      "text": "The explanation summary visualization (Figure 4) contains three visual components: the data flow, the adjacency list, and the legends. The dataflow shows how instances from different classes flow to different instance clusters through a Sankey diagram. The adjacency list displays the data summary from the local explanations. The legends display the features and their corresponding color encodings in the adjacency list."
    },
    {
      "heading": "6.1.1 Adjacency List",
      "text": "The main visual component of MELODY UI is the adjacency list of the explanation summary. The explanation summary is a matrix of two sets: instance clusters and feature clusters. The intersection between an instance cluster and a feature cluster is a real-valued submatrix of original explanations. Therefore, the simplest way to present the explanation summary is to directly show the original explanation matrix with rows and columns ordered according to their cluster memberships. However, we found the co-clusters hard to be observed when the matrix is sparse (C.2). Since an ML model\u2019s decisions are usually diverse on different subsets of the input data, the clustering will also result in many different row and column clusters. Thus, it becomes difficult for users to notice small clusters. Also, we found the information obtained from the matrix hard to memorize when users perform multiple visual inspections and interactions on different widgets at the same time. For example, when a feature cluster is selected, users inspect the features inside in a separate view. After the inspection, it becomes difficult to recall which feature cluster they have selected inside the matrix. These problems related to sparsity and stimulus have also been identified and thoroughly studied in previous literature [13, 17, 41].\nTo explore relevant instances and features in a large sparse matrix (C.1-2), we design an adjacency list visualization (inspired by [17]) to present the explanation summary (Figure 4B). Each row in the adjacency list represents an instance cluster, and each color texture represents a feature cluster. The size of an instance cluster is encoded\n2The system can be accessed at http://128.238.182.13:5004/\nwith text and height. For a feature cluster, the size is encoded with width. Thus, each intersection between the instance and feature cluster forms a block (i.e. a cell in p(R\u0302,C\u0302)). The blocks in each row are sorted by their values in p(R\u0302,C\u0302). In this arrangement, we fix the instances\u2019 positions for users to locate a subset of data easily. Also, the features are color encoded so that users can reference an explanatory feature easily by its color, which helps navigate the features across different widgets (C.6). Furthermore, as the column position restriction is removed, the adjacency list becomes more compact. We acknowledge that categorical color scheme might impose a scalability issue, thus we combine the colors with textures to increase the available selections. Visualizing Local Explanation Values Each block is a co-cluster between a group of instances and features. Thus it is also a sub-matrix of the original explanation matrix. As a sub-matrix contains a distribution of positive real numbers, we display such information as a histogram encoded by a color gradient (Figure 4 1\u00a9). The values in the sub-matrix are sorted from high to low and then encoded by a sequential color scheme. The sorting can provide better clarity on the quality of co-clusters under the sparse matrix clustering condition (C.2)."
    },
    {
      "heading": "6.1.2 Data Flow",
      "text": "To provide a picture of how data and predictions are arranged in the summary, a Sankey diagram is displayed (Figure4A) on the left of the adjacency list. A vertical rectangle is shown for each class with height encoded as the number of instances in the dataset. The amount of fill of each rectangle is proportional to the number of instances in the currently shown summary. The horizontal flows represent the portion of data falling into a designated instance cluster. Different colors in the flows represent the amount of data that is either correctly predicted (grey) or incorrectly predicted (red). It helps users assess the capability of the ML model: its performance on each class and the accuracy of each different decision boundaries (C.3)."
    },
    {
      "heading": "6.1.3 Legends",
      "text": "The legends (Figure 4C) show the color and texture encodings of the clusters of explanatory features. The features are sorted based on their existence in the current summary. When we click on a feature, its distribution of explanation values in the dataset is shown as a histogram (Figure 4 2\u00a9) to allow the inspection of its global importance (C.1)."
    },
    {
      "heading": "6.1.4 Interactions",
      "text": "The explanation summary can be filtered through various mechanisms. Besides explicitly selecting classes and explanatory features for filtering in the dropdown menus, the statistical properties such as the size of clusters and the average explanation values of a co-cluster allow the\nsummary to be filtered through sliding different thresholds. Also, when clusters are selected, the values in the subset are shown in a parallel coordinates to export the important instances from a sparse cluster (C.2) to the subset view through brushing the axes."
    },
    {
      "heading": "6.2 Visual Analytics Workflow of ML Model Explanation",
      "text": "We now describe how to leverage the explanation summary to complete a visual analytics workflow. The interactions between different explanations in Figure 3 are consistent with MELODY UI\u2019s views. While the adjacency list acts as a global overview of the ML model explanation, the components in the list can be selected and exported to a more focused class and instance inspection. In return, the adjacency list can use the findings from local explanations for verification or further insights. Thus, the workflow in the system forms a finite state transition among global, subset (class), and instance explanations. The discussion below mainly focuses on how the system helps circulate different XAI tasks."
    },
    {
      "heading": "6.2.1 Global \u2212\u2192 Subset (Class) Explanation",
      "text": "After exploring the adjacency list, users can proceed to a subset of the clusters by clicking on a row cluster or a co-cluster(zoom and filter). After selecting an explanation subset and extracting the instances with significant values, users can proceed to understand the local decision logics from the behavior of instances inside. To provide contextual explanations for tabular, image, and text data, we propose three different ways to visualize the subsets (C.5). Tabular. The system visualizes the tabular data in multiple sets of parallel coordinates (Figure 6D). Each set of parallel coordinates represents one class, and the lines inside represent the instances. The axes show the features in the original dataset, and the selected features are positioned at the front. The lines are colored based on whether their predictions. There are also two histograms on each axis that represent the distributions of the correctly and incorrectly predicted instances. Image. For image data, the system shows the similarly explained instance on one column and their corresponding common visual representations on another column (Figure 1B). The instances shown are also grouped by their classes and are surrounded by colored frames that indicate the predictions. All instances\u2019 and features\u2019 images are displayed to acquire a visual impression of similar images and explanations. Text. The system shows the number of selected instances as bar charts grouped by class and prediction outcome on the left column, and the topics and words that are used to explain the instances on the right column (Figure 7C). Users can understand what kinds of words are combined to make decisions on each prediction and further select individual words inside each topic to filter the bar charts. When a bar is clicked, the documents can be exported to the local explanation view."
    },
    {
      "heading": "6.2.2 Subset (Class) \u2212\u2192 Local Explanation",
      "text": "After a subset of instances and features are inspected, users can drill down to inspect an instance with full details for insights or hypotheses (detail on demand). Similarly, different arrangements are provided to inspect instances from tabular, image, and text data (C.5). Tabular Instances. The instances are selected by brushing the parallel coordinates in the subset view and rendered in the data table with original features to browse the exact numerical and categorical values. The color of each cell represents the prediction outcome. Single Image. An image and its top influencing features (image patches with the highest similarity scores) are displayed. The instance and features also have their bounding box of neuron activations to inspect the relationship between different patches.\nText Documents. The full documents selected from the bar charts are shown. The words that are explanatory features in the document are highlighted by a sequential color map with their explanation values."
    },
    {
      "heading": "6.2.3 Local \u2212\u2192 Global Explanation",
      "text": "Users might formulate insights and hypotheses throughout the topdown inspection. For testing the hypotheses, the explanatory features and instances in the local explanation panel are clicked, and their values become the conditions for filtering the adjacency list (Query) (C.4). For tabular data, when a cell in the data table is clicked, the logic that includes the cell\u2019s value will be included (e.g., when a categorical cell valued \u201ceducation\u201d in column \u201cpurpose\u201d is clicked, the explanatory feature \u201cpurpose = education\u201d will be selected). For image and text data, the user clicks on the image or document for class queries and the image patches or words for feature queries. Overall, users can filter the explanation summary by class, prediction outcome, and explanatory features. As a result, a new and refined overview summary is available to perform global explanation tasks again, which completes the loop."
    },
    {
      "heading": "7 EVALUATION",
      "text": "To evaluate the scalability and the quality of MELODY, we perform quantitative experiments and use case scenarios on a variety of datasets."
    },
    {
      "heading": "7.1 Experimental Setup",
      "text": "The implementations are written in NumPy, and the experiments are run in a MacBook Pro with 2.4 GHz 8-Core Intel Core i9 CPUs and 32GB RAM. We use the following real-world datasets and ML models to conduct our experiments and use cases: Caltech-UCSD Birds-200-2011 Images. The dataset includes 11,788 images with 200 species of birds. We use a Convolutional Neural Network (CNN) with a prototype layer [9] and achieves the highest test accuracy of 73.63%. The explanation matrix is extracted from the prototype layer, which has 1330 visual explanatory features. Home Equity Line of Credit (HELOC). It contains binary classifications of risk performance (i.e., good or bad) on 10,459 samples with even class distributions. We train a two-layer neural network and achieves the highest test accuracy of 72.59%. We extract 167 logics and use SHAP [31] to construct our explanation matrix. US Consumer Finance Complaints. The dataset contains 22,200 documents with ten classes (e.g., debt, credit card, and mortgage). We train an LSTM neural network model and achieve the highest test accuracy of 84.54%. We use IntGrad [54] to generate explanations for words in each document. We further combine the words by clustering their embeddings to generate 500 topics as the explanation features."
    },
    {
      "heading": "7.2 Quantitative Evaluation",
      "text": "End-to-end quality evaluation. To evaluate how each of our heuristics improves the quality of the summarization results, we report the quality (information loss) of the baseline implementation (i.e., straightforward minimization of Equation 2) as well as the effects of applying marginalization (Equation 3), smoothing, and pre-clustering from Section 4.4.2. Overall, the heuristics significantly improve the quality of the result (Figure 5). The final reductions of information\nloss range from 78% to 99%. To visually understand the quality of the summarization results, we provide visual outcomes of the explanation summaries in Figure 9-11 in the Appendix. Effect of data sketches on run time performance. We report the effect of the run time on the three datasets with the speedup strategies (Algorithm 2) in Table 2. The result clearly shows that by replacing the quadratic computation in the baseline approach (Algorithm 1), it becomes possible to produce results in interactive time. We also observe that the calculation of information loss is not linear in runtime since there are lots of data slicing operations to compute the approximation matrix (q(R\u0302,C\u0302)). The results highlight the importance of limiting the number of candidate comparisons in the bottom-up process."
    },
    {
      "heading": "7.3 Use Cases",
      "text": "We present a usage scenario of understanding deep neural networks related to image recognition and two use cases regarding tabular and text classifications of financial data. Our goal is to demonstrate that our technique generalizes different ML model interpretation challenges in understanding the models and datasets."
    },
    {
      "heading": "7.3.1 Usage Scenario: Understanding an Image Classifier",
      "text": "We first describe a hypothetical walkthrough of understanding what a deep learning model has learned from a set of images (Figure 1). We use images as examples because the visual presentations are intuitive to understand. Imagine Chris, an ornithologist, wants to study how birds\u2019 appearances distinguish their species. He downloads the data and runs the ML model to understand how the machine learns the visual features. Understand the summary. Chris uses MELODY to generate an explanation summary consisted of 37 instance clusters and 49 feature clusters. He imports the result to MELODY UI. After filtering small clusters and clusters with low explanation values, Chris discovers three broad groups of birds with similar prediction logics (Figure 1A). Each group contains different visual explanatory features (i.e., color blocks), so he decides to go through the instance groups one by one. Inspect an interesting subset. Chris clicks the text box on the row to select all the instances and features from the instance cluster for a detailed inspection. From the subset view (Figure 1B), he realizes that the neural network learns to group birds with similar colors (yellowish\nbirds) (Figure 1 B1\u00a9) for a coarse level of decision making. The images then are further classified by more detailed image patches such as the bird\u2019s head and belly. Chris notices some classes such as yellowthroated Vimeo have many wrong predictions (images with orange frames) in this subset. Therefore, he clicks on some of the images to examine an image and its classification logics in full detail. Develop hypotheses by inspecting an instance. Chris checks an image by clicking it in the subset view. The image and its top explanatory features thus are shown in the instance inspection view (Figure 1C). He sees a yellow-throated Vimeo is wrongly predicted as a blue-winged warbler because the furs on its neck look similar to the ones of a bluewinged warbler. Chris finds the whole process enjoyable since he quickly identifies the reasoning processes of the model on hundreds of images within a simple journey of visual analysis."
    },
    {
      "heading": "7.3.2 Tabular Use Case: Understanding the Data Capability",
      "text": "We now present a use case about approaching the limit of predictability in training a dataset. Understanding how the current features help to make predictions allows the financial worker to make improvements to the current credit system. Understand the summary. After filtering by value threshold and the number of instances in the clusters, the analyst obtains a visual explanation summary (Figure 6A). It shows that the blue-colored blocks occupy most of the rows. It consists of mainly items related to delinquency (Figure 6B). Then, he clicks and inspects the subsets and filters some low explanation values by brushing the parallel coordinates (Figure 6C), the subsets show very similar behaviors: for customers who have no history of delinquency, the model labels them as \u201cgood\u201d. Discovering more detailed logics in the model. The analyst sees that such logics provide an approximated accuracy of around 73% in more than half of the population. To understand how a \u201cbad\u201d decision is correctly predicted with a good delinquency record, he refines the summary by the classes. The summary shows another logic that influences the model outcome (Figure 6 A1\u00a9). The pink blocks represent the features related to a low external risk estimate, which means that the customer would still be graded as \u201cbad\u201d if the external risk estimate is low (Figure 6 D1\u00a9). Verifying insights. From the dataflow, the analyst sees that combining delinquency and risk estimate yields a good prediction result. By verify\nthis hypothesis, he filters the summary by showing only the wrong predictions under the same condition. By adjusting the value threshold to a low extent, the wrong predictions mainly attribute to the fact that they do not have a low risk-estimate (i.e., missing pink blocks related to risk estimate when blue blocks are presented) (Figure 6 A2\u00a9). Clicking the rows with missing pink blocks also reveals that the model fails to identify bad risk when the customer has a good delinquency record and high external risk estimates (Figure 6 D2\u00a9). Throughout the visual analysis in different granularity, the analyst acquires an overview of the model: the model mainly decides by the history of delinquency on the first level of reasoning, then further screen out the bad risks by low external risk estimates. The query panel shows that the rows explained by either these two logics cover more than 70% of the whole dataset."
    },
    {
      "heading": "7.3.3 Text Use Case: Predicting Customer Complaints",
      "text": "We present a use case of exploring a text classification model to understand different types of customer complaints. Understanding how customers complain can improve the call center\u2019s services. Our financial analyst first uses MELODY to acquire 28 instance clusters and 23 feature clusters. Then, as he is from the loan division in the company, he filters the explanation summary by \u201ccustomer loans\u201d class to explore the customers\u2019 inquiries related to loans (Figure 7(1)). Identify useful subsets. The analyst first discovers that the explanation summary is very sparse (Figure 7A). Therefore, he clicks on the block to examine a more detailed view of the explanation subset (Figure 7(1)). The details of value distributions in the selected block are shown in the explanation parallel coordinates (Figure 7B). The analyst discovers that the sparsity mainly comes from the low usage of many topics in the feature clusters. Thus, he brushes the axes of topics that have high values to acquire the subset of instances and topics that heavily correlate to each other. The result of the brushing is shown in the subset view (Figure 7C). Discover interesting topics in the subset. From the subset view, the analyst discovers that many complaints are related to words such as \u201cauto\u201d, \u201cbmw\u201d, and \u201cford\u201d. These words belong to automobiles and vehicles. Given the longer length of correctly labeled (blue) bars in the bar chart, these words contribute significantly to the correct prediction of customer loan complaints to the model. Therefore, by clicking the blue bar, the analyst inspects the raw documents of these automobilerelated instances (Figure 7D).\nInsights from instances. By browsing the documents, the analyst confirms that the loan payments complaints are related to vehicle purchases. By clicking the words like \u201cvehicles\u201d and \u201ccars\u201d, he queries the global explanation summary to verify his findings (Figure 7(5)). The queried results show that there are more than 120 complaints about customer loans that contain such phrases with the correctness of 93%. Thus, he concludes that automobile purchase is a popular topic when customers approach the financial institution. The company should these topics included during the call center training sessions."
    },
    {
      "heading": "8 CONCLUSION AND FUTURE WORK",
      "text": "In this work, we present MELODY, an interactive algorithm to construct an explanation summary of an ML model from local explanations of a dataset. The summary allows users to understand the decision rationale and data characteristics together for a holistics XAI experience. With the algorithm, we also present MELODY UI, an interactive visual analytics system to connect different granularity of XAI tasks. The versatility of our algorithm and system enable scalable visual explorations of generic ML model interpretations on tabular, image, and text data. Our future work includes: Embed summarization to model training processes. Instead of generating a summary after the training, we plan to integrate the summary as a layer in the deep neural network to increase the global explanation capability of the model. User study. Since many model developers use visualizations such as partial dependency plot or projections to understand the model, we plan to conduct a user study to see if providing explanatory features and similarity among data at the same time will improve any productivity in practice. Also, we plan to conduct a longitudinal evaluation of MELODY UI to ML researchers to investigate how the system affects model design, data engineering, and model debugging. Application domain. Apart from tabular, image, and text classifications, there are also other data primitives such as time series and graph classifications tasks. We plan to explore the visual analytics approach to apply our algorithm to explain ML tasks in these domains."
    },
    {
      "heading": "In Proceedings of the thiry-fourth annual ACM symposium on Theory of",
      "text": "computing, pp. 380\u2013388, 2002. [9] C. Chen, O. Li, D. Tao, A. Barnett, C. Rudin, and J. K. Su. This looks like that: deep learning for interpretable image recognition. In Advances in Neural Information Processing Systems, pp. 8928\u20138939, 2019. [10] M. Craven and J. W. Shavlik. Extracting tree-structured representations of trained networks. In Advances in neural information processing systems, pp. 24\u201330, 1996. [11] I. S. Dhillon. Co-clustering documents and words using bipartite spectral graph partitioning. In Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 269\u2013274, 2001. [12] I. S. Dhillon, S. Mallela, and D. S. Modha. Information-theoretic coclustering. In Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 89\u201398, 2003. [13] M. Ghoniem, J.-D. Fekete, and P. Castagliola. On the readability of graphs using node-link and matrix-based representations: a controlled experiment and statistical analysis. Information Visualization, 4(2):114\u2013135, 2005. [14] L. H. Gilpin, D. Bau, B. Z. Yuan, A. Bajwa, M. Specter, and L. Kagal. Explaining explanations: An overview of interpretability of machine learning. In 2018 IEEE 5th International Conference on data science and advanced analytics (DSAA), pp. 80\u201389. IEEE, 2018. [15] A. Grover and J. Leskovec. node2vec: Scalable feature learning for networks. In Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 855\u2013864, 2016. [16] R. Guidotti, A. Monreale, S. Ruggieri, F. Turini, F. Giannotti, and D. Pedreschi. A survey of methods for explaining black box models. ACM computing surveys (CSUR), 51(5):1\u201342, 2018. [17] M. Hlawatsch, M. Burch, and D. Weiskopf. Visual adjacency lists for dynamic graphs. IEEE transactions on visualization and computer graphics, 20(11):1590\u20131603, 2014. [18] F. Hohman, A. Head, R. Caruana, R. DeLine, and S. M. Drucker. Gamut: A design probe to understand how data scientists understand machine learning models. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, pp. 1\u201313, 2019. [19] F. Hohman, M. Kahng, R. Pienta, and D. H. Chau. Visual analytics in deep learning: An interrogative survey for the next frontiers. IEEE transactions on visualization and computer graphics, 25(8):2674\u20132693, 2018. [20] F. Hohman, H. Park, C. Robinson, and D. H. P. Chau. Summit: Scaling deep learning interpretability by visualizing activation and attribution summarizations. IEEE transactions on visualization and computer graphics, 26(1):1096\u20131106, 2019. [21] K. Holstein, J. Wortman Vaughan, H. Daume\u0301 III, M. Dudik, and H. Wallach. Improving fairness in machine learning systems: What do industry practitioners need? In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, pp. 1\u201316, 2019. [22] M. Kahng, P. Y. Andrews, A. Kalro, and D. H. P. Chau. Activis: Visual exploration of industry-scale deep neural network models. IEEE transactions\non visualization and computer graphics, 24(1):88\u201397, 2017. [23] B. Kim, C. Rudin, and J. A. Shah. The bayesian case model: A gen-\nerative approach for case-based reasoning and prototype classification. In Advances in Neural Information Processing Systems, pp. 1952\u20131960, 2014. [24] B. Kim, M. Wattenberg, J. Gilmer, C. Cai, J. Wexler, F. Viegas, et al. Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav). In International Conference on Machine Learning, pp. 2668\u20132677, 2018. [25] O. Li, H. Liu, C. Chen, and C. Rudin. Deep learning for case-based reasoning through prototypes: A neural network that explains its predictions. In Thirty-Second AAAI Conference on Artificial Intelligence, 2018. [26] Q. Li, K. S. Njotoprawiro, H. Haleem, Q. Chen, C. Yi, and X. Ma. Embeddingvis: A visual analytics approach to comparative network embedding inspection. In 2018 IEEE Conference on Visual Analytics Science and Technology (VAST), pp. 48\u201359. IEEE, 2018. [27] Q. V. Liao, D. Gruen, and S. Miller. Questioning the ai: Informing design practices for explainable ai user experiences. arXiv preprint arXiv:2001.02478, 2020. [28] M. Liu, S. Liu, H. Su, K. Cao, and J. Zhu. Analyzing the noise robustness of deep neural networks. In 2018 IEEE Conference on Visual Analytics Science and Technology (VAST), pp. 60\u201371. IEEE, 2018. [29] M. Liu, J. Shi, Z. Li, C. Li, J. Zhu, and S. Liu. Towards better analysis of deep convolutional neural networks. IEEE transactions on visualization and computer graphics, 23(1):91\u2013100, 2016. [30] S. Liu, P.-T. Bremer, J. J. Thiagarajan, V. Srikumar, B. Wang, Y. Livnat, and V. Pascucci. Visual exploration of semantic relationships in neural word embeddings. IEEE transactions on visualization and computer graphics, 24(1):553\u2013562, 2017. [31] S. M. Lundberg and S.-I. Lee. A unified approach to interpreting model predictions. In Advances in neural information processing systems, pp. 4765\u20134774, 2017. [32] D. Martens, B. Baesens, and T. Van Gestel. Decompositional rule extraction from support vector machines by active learning. IEEE Transactions on Knowledge and Data Engineering, 21(2):178\u2013191, 2008. [33] Y. Ming, S. Cao, R. Zhang, Z. Li, Y. Chen, Y. Song, and H. Qu. Understanding hidden memories of recurrent neural networks. In 2017 IEEE Conference on Visual Analytics Science and Technology (VAST), pp. 13\u201324. IEEE, 2017. [34] Y. Ming, H. Qu, and E. Bertini. Rulematrix: Visualizing and understanding classifiers with rules. IEEE transactions on visualization and computer graphics, 25(1):342\u2013352, 2018. [35] Y. Ming, P. Xu, F. Cheng, H. Qu, and L. Ren. Protosteer: Steering deep sequence model with prototypes. IEEE Transactions on Visualization and Computer Graphics, 26(1):238\u2013248, 2019. [36] Y. Ming, P. Xu, H. Qu, and L. Ren. Interpretable and steerable sequence learning via prototypes. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 903\u2013913, 2019. [37] S. Mohseni, N. Zarei, and E. D. Ragan. A survey of evaluation methods and measures for interpretable machine learning. arXiv preprint arXiv:1811.11839, 2018. [38] T. Mu\u0308hlbacher, L. Linhardt, T. Mo\u0308ller, and H. Piringer. Treepod: Sensitivity-aware selection of pareto-optimal decision trees. IEEE transactions on visualization and computer graphics, 24(1):174\u2013183, 2017. [39] M. Muller, I. Lange, D. Wang, D. Piorkowski, J. Tsay, Q. V. Liao, C. Dugan, and T. Erickson. How data science workers work with data: Discovery, capture, curation, design, creation. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, pp. 1\u201315, 2019. [40] S. Navlakha, R. Rastogi, and N. Shrivastava. Graph summarization with bounded error. In Proceedings of the 2008 ACM SIGMOD international conference on Management of data, pp. 419\u2013432, 2008. [41] M. Okoe, R. Jianu, and S. Kobourov. Node-link or adjacency matrices: Old question, new insights. IEEE transactions on visualization and computer graphics, 25(10):2940\u20132952, 2018. [42] N. Pezzotti, T. Ho\u0308llt, J. Van Gemert, B. P. Lelieveldt, E. Eisemann, and A. Vilanova. Deepeyes: Progressive visual analytics for designing deep neural networks. IEEE transactions on visualization and computer graphics, 24(1):98\u2013108, 2017. [43] N. Poerner, B. Roth, and H. Schu\u0308tze. Evaluating neural network explanation methods using hybrid documents and morphological agreement. arXiv preprint arXiv:1801.06422, 2018. [44] G. Ras, M. van Gerven, and P. Haselager. Explanation methods in deep\nlearning: Users, values, concerns and challenges. In Explainable and Interpretable Models in Computer Vision and Machine Learning, pp. 19\u2013 36. Springer, 2018. [45] P. E. Rauber, S. G. Fadel, A. X. Falcao, and A. C. Telea. Visualizing the hidden activity of artificial neural networks. IEEE transactions on visualization and computer graphics, 23(1):101\u2013110, 2016. [46] M. T. Ribeiro, S. Singh, and C. Guestrin. \u201c why should i trust you?\u201d explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, pp. 1135\u20131144, 2016. [47] A. Rule, A. Tabard, and J. D. Hollan. Exploration and explanation in computational notebooks. In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems, pp. 1\u201312, 2018. [48] V. Satopaa, J. Albrecht, D. Irwin, and B. Raghavan. Finding a\u201c kneedle\u201d in a haystack: Detecting knee points in system behavior. In 2011 31st international conference on distributed computing systems workshops, pp. 166\u2013171. IEEE, 2011. [49] S. Sawada and M. Toyoda. Model-agnostic visual explanation of machine learning models based on heat map. 2019. [50] A. Shrikumar, P. Greenside, and A. Kundaje. Learning important features through propagating activation differences. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 3145\u20133153. JMLR. org, 2017. [51] K. Simonyan, A. Vedaldi, and A. Zisserman. Deep inside convolutional networks: Visualising image classification models and saliency maps. In Workshop at International Conference on Learning Representations, 2014. [52] H. Strobelt, S. Gehrmann, M. Behrisch, A. Perer, H. Pfister, and A. M. Rush. S eq 2s eq-v is: A visual debugging tool for sequence-to-sequence models. IEEE transactions on visualization and computer graphics, 25(1):353\u2013363, 2018. [53] H. Strobelt, S. Gehrmann, H. Pfister, and A. M. Rush. Lstmvis: A tool for visual analysis of hidden state dynamics in recurrent neural networks. IEEE transactions on visualization and computer graphics, 24(1):667\u2013676, 2017. [54] M. Sundararajan, A. Taly, and Q. Yan. Axiomatic attribution for deep networks. In Proceedings of the 34th International Conference on Machine Learning-Volume 70, pp. 3319\u20133328. JMLR. org, 2017. [55] F. . Tzeng and K. . Ma. Opening the black box - data driven visualization of neural networks. In VIS 05. IEEE Visualization, 2005., pp. 383\u2013390, Oct 2005. doi: 10.1109/VISUAL.2005.1532820 [56] S. Van Den Elzen and J. J. van Wijk. Baobabview: Interactive construction and analysis of decision trees. In 2011 IEEE conference on visual analytics science and technology (VAST), pp. 151\u2013160. IEEE, 2011. [57] P. Voigt and A. Von dem Bussche. The eu general data protection regulation (gdpr). A Practical Guide, 1st Ed., Cham: Springer International Publishing, 2017. [58] J. Wexler, M. Pushkarna, T. Bolukbasi, M. Wattenberg, F. Vie\u0301gas, and J. Wilson. The what-if tool: Interactive probing of machine learning models. IEEE transactions on visualization and computer graphics, 26(1):56\u2013 65, 2019. [59] K. Wongsuphasawat, D. Smilkov, J. Wexler, J. Wilson, D. Mane, D. Fritz, D. Krishnan, F. B. Vie\u0301gas, and M. Wattenberg. Visualizing dataflow graphs of deep learning models in tensorflow. IEEE transactions on visualization and computer graphics, 24(1):1\u201312, 2017. [60] T. Wu, M. T. Ribeiro, J. Heer, and D. S. Weld. Errudite: Scalable, reproducible, and testable error analysis. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 747\u2013763, 2019. [61] S. Xiang, X. Ye, J. Xia, J. Wu, Y. Chen, and S. Liu. Interactive correction of mislabeled training data. In 2019 IEEE Conference on Visual Analytics Science and Technology (VAST), pp. 57\u201368, Oct 2019. doi: 10.1109/ VAST47406.2019.8986943 [62] H. Yang, C. Rudin, and M. Seltzer. Scalable bayesian rule lists. In Proceedings of the 34th International Conference on Machine LearningVolume 70, pp. 3921\u20133930. JMLR. org, 2017. [63] M. D. Zeiler and R. Fergus. Visualizing and understanding convolutional networks. In European conference on computer vision, pp. 818\u2013833. Springer, 2014. [64] X. Zhao, Y. Wu, D. L. Lee, and W. Cui. iforest: Interpreting random forests via visual analytics. IEEE transactions on visualization and computer graphics, 25(1):407\u2013416, 2018."
    },
    {
      "heading": "9 APPENDIX",
      "text": ""
    },
    {
      "heading": "9.1 Background of Local Explanation Models",
      "text": "We provide a background of the mainstream models that generate local explanations of a machine learning model\u2019s decisions to a dataset. The popularity of giving local explanations, except applying logical models such as decision trees or rules, is because these methods provide an independent and highly customized explanation for each instance. When explanations do not aggregate into general decisions or rules, they become more faithful to the original model.\nIn general, to generate a local explanation for an instance, explanation algorithms usually seek one of the following approaches:\n1. Local Linear Models: The algorithm searches the neighbors of an instance, then fits the subset to a linear model such that the higher the gradient of a feature in the linear model, the more important the feature is to the prediction of the selected instances. SHAP [31] and LIME [46] are the examples that use neighbors to evaluate an instance.\n2. Perturbation: Instead of using other instances to generate explanations, one can perturbate the values of its attributes and observe whether the output changes significantly after removing, masking or altering them. The sensitivity of each feature implies that its value lies in the decision boundary of the machine learning model. Thus, a sensitive feature from perturbation has a high influential power on the instance. This method has been applied to Convolutional Neural Networks (CNNs) for image classification [63].\n3. Prototype: The intuition is to use representative original training data (i.e. prototype) to explain a prediction, which can be selected by clustering the latent representations in the ML model. Given the class labels of each prototype and their similarities with the input data, the prediction and reasoning process becomes a scoring system where the class with the highest score (i.e. the sum of similarities of prototypes belonging to the class) is the returned result. This technique has been readily incorporated in deep neural networks for image, text and sequence predictions [9, 25, 36].\n4. Backpropagation: Since complex models like neural networks contain series of propagation of weights from the input to the output neurons to produce predictions, one can invert the process to backpropagate the neurons with great gradients from the output to the input data locate the portion of original data that causes the neuron activations in the output. Such a portion implies the meaningful features that explain the model\u2019s decision. Saliency Maps [51], DeepLIFT [50] and Intgrad [54] are the examples of such methods."
    },
    {
      "heading": "9.2 Tasks Breakdown of Explainable AI",
      "text": "In general, ML models explainability is achieved by three different types of tasks (T): Global (general behavior of a model), Local (behavior of a model to an instance), and Class (behavior of a model to a class) [16, 27]. Liao et.al. [27] in addition provides actionable suggestions (A) for each task. For each task and action, we identify different opportunities (O) that an explanation summary can help achieve the tasks.\nT.1 Global Explanation. The goal is to understand the overall weights of features used by the model to explain how AI makes decisions on the dataset in general. A.1 Users select the important features that affect the whole\ndataset\u2019s outcome to uncover data-related issues such as data collection, bias, and privacy. A.2 They may also evaluate the limit and capability of the model. By inspecting the main features, users can develop a mental model to interact with or improve the system.\nO.1 An explanation summary can define the appropriate level of details to explain the model without losing too many details nor overwhelming the users. Grouping the relevant features\nand instances allow interactions for users to prioritize different information shown at a time. O.2 Subsetting the information by similarity also decreases the complexity of the explanation since the instances and features shown have many common properties. This allows the global explanations visualized to be more representative.\nT.2 Local Explanation. The goal here is to inspect the model\u2019s behavior on a specific instance and understand how the instance\u2019s properties influence the outcome. A.3 A popular activity is to explore different what-if scenarios.\nUsers observe the outcome if some features become different which helps to explore more scenarios of applying the model and gain insights into the model\u2019s capability. A.4 Another action is to directly understand why does the instance belong to a prediction and why not does it result in other outcomes. This helps to discover the local decision boundaries of the model. A.5 Providing the original input/data provides a more holistic system capability to understand a particular decision and accommodate users\u2019 understandings and interactions.\nO.3 Grouping similar instances provide neighbors of the instance that are explained similarly by the model, which increases the number of instances to support users\u2019 insights and findings. Can we conclude that \u201cears\u201d are important in the prediction of \u201ccats\u201d from what we see on a single image? We also know that if there exists lots of cat images exhibiting similar characteristics. An explanation summary thus allows a large set of instances to be analyzed to avoid spurious conclusions [60]. O.4 Similar to grouping instances, grouping features allows users to prioritize important features that explain an instance and its neighbors, which reduce the cognitive workload when deriving understandings to the model\u2019s decision logic.\nT.3 Class Explanation (Counterfactual). How a prediction (class) works in the model is also an important emphasis. It is similar to a global explanation but with a smaller granularity on a specific class. Yet, the actions to understand a class are more similar to instance explanations, which focus on the sensitivity of features to each prediction. A.6 Testing the sensitivity of features towards a prediction is\nequivalent to the test of different what-if scenarios. By testing different ranges of features, users can understand the decision boundaries of a prediction class. A.7 Besides interaction, the exploration of the relevant features of a prediction also helps understand why and why not cases of a prediction to gain insights into the decision logic. O.5 With groups of similar instances and features, users can apply different levels of details to acquire more precise subsets. O.6 Extending the findings of an instance to its similar neighbors inside a class increases the confidences of the insights."
    },
    {
      "heading": "9.3 End-to-End Explanation Modeling Pipeline",
      "text": "In this section, we describe the example pipelines in handling tabular, image, and text data that result in explanation matrices with the explanatory features in Table 1. We explicitly categorize the pipeline with preprocessing, ML modeling, and explanation modeling stages. Notice that they are not the only ways to achieve the objectives of data engineering. The explanation models can be interchanged as well. In addition, we provide an synthetic example to illustrate how the whole explanation process works, as well as our goal to summarize the whole explanation data in Figure 8."
    },
    {
      "heading": "9.3.1 Tabular Data",
      "text": "Preprocessing. To enable logics as the explanatory features for tabular data, we need to preprocess the original data into one-hot encodings of logics under each attribute. For numerical and ordinal data, the attributes are first discretized into different quantiles. Then, the onehot encoding can be applied to transform the quantiles into separate columns, where 0 indicates the data does not fall into the ranges while\n1 indicates it does. The way of discretizing attributes can be as straightforward as choosing a fixed number of equal intervals or leveraging the statistical properties such as entropy. In our use case, we use Sturge\u2019s rule to determine the number of quantiles and the ranges of quantiles are determined by the training data. The one-hot encoding can also directly be applied to categorical attributes. ML modeling. Then, the transformed data is used to train a neural network so that the logics are the input features. This allows the logic to be evaluated in the explanation methods. Explanation Modeling. As the input features of the ML model are a set of logics, methods such as LIME and SHAP can be directly applied to the model and dataset to generate feature vectors composed of a set of logics."
    },
    {
      "heading": "9.3.2 Images",
      "text": "Preprocessing. For images, we do not need much feature engineering as the explanatory features are the pixels themselves. We only need to apply standard image augmenting techniques (i.e. replicating training images with scaling, rotating, and mirroring) to increase the training data size for a better model accuracy. ML modeling. We apply prototype learning inside a Convolutional Neural Network [9]. It adds a prototype layer on the last layer of the original neural network. The training process results in a selection of a fixed number of image patches from the training data as prototypes that are used to reason the prediction of new data. Explanation Modeling. As the explanation model is already incorporated as a layer in the ML model when new data comes in, an n\u00d7m explanation matrix can be constructed, where n is the number of tested data and m is the number of prototypes."
    },
    {
      "heading": "9.3.3 Text",
      "text": "Preprocessing. Similar to images, the explanation of text comes from the texts inside the documents as well. Thus, we only need to apply standard text preprocessing steps like removing stopwords and infrequent words to make sure the explanation models do not return explanations with meaningless topics. ML modeling. We can use common text models such as RNN and LSTM to generate predictions. Notice that usually the first layer of these models are the word embeddings of the whole dataset. We can leverage this word embeddings to find extract the topics in the dataset by clustering based on them. Explanation Modeling. For training the ML model, we can examine each word\u2019s importance to the prediction by gradient-based explanation models such as DeepLIFT and Intgrad. This results in an extremely sparse matrix where each feature is a word that appears in more or equal than one documents. Also, words with similar meanings such as \u201cgood\u201d and \u201cexcellent\u201d will be treated as different features. To densify the explanation matrix so that similar words are grouped and more significant hidden structures can be produced, we can transform the local explanation from a feature vector of words to a feature vector of\ntopics. The explanation importance of each topic to an instance can be determined by the maximum explanation importance among the words in the topic. Such allows words with similar semantics to be grouped before the matrix is summarized."
    }
  ],
  "title": "MELODY: Generating and Visualizing Machine Learning Model Summary to Understand Data and Classifiers Together",
  "year": 2020
}
