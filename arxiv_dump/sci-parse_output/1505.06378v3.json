{"abstractText": "Real-world machine learning applications may require functions to be fast-to-evaluate and interpretable. In particular, guaranteed monotonicity of the learned function with respect to some of the inputs can be critical to user confidence. We propose meeting these goals for low-dimensional machine learning problems by learning flexible, monotonic functions using calibrated interpolated look-up tables. We extend the structural risk minimization framework of lattice regression to train monotonic functions by adding linear inequality constraints. In addition, we propose jointly learning interpretable calibrations of each feature to normalize continuous features and handle categorical or missing data, at the cost of making the objective non-convex. We address large-scale learning through parallelization, mini-batching, and random sampling of additive regularizer terms. Case studies on real-world problems with up to sixteen features and up to hundreds of millions of training samples demonstrate the proposed monotonic functions can achieve state-of-the-art accuracy in practice while providing greater transparency to users.", "authors": [{"affiliations": [], "name": "Maya Gupta"}, {"affiliations": [], "name": "Andrew Cotter"}, {"affiliations": [], "name": "Jan Pfeifer"}, {"affiliations": [], "name": "Konstantin Voevodski"}, {"affiliations": [], "name": "Kevin Canini"}, {"affiliations": [], "name": "Wojciech Moczydlowski"}, {"affiliations": [], "name": "Alexander van Esbroeck"}], "id": "SP:2bd2b493b958ace2aa8a7a1daa1eb640ad5bc8ff", "references": [{"authors": ["Y.S. Abu-Mostafa"], "title": "A method for learning from hints", "venue": "In Advances in Neural Information Processing Systems,", "year": 1993}, {"authors": ["N.P. Archer", "S. Wang"], "title": "Application of the back propagation neural network algorithm with monotonicity constraints for two-group classification problems", "venue": "Decision Sciences,", "year": 1993}, {"authors": ["F. Bach"], "title": "Learning with submodular functions: A convex optimization perspective", "venue": "Foundations and Trends in Machine Learning,", "year": 2013}, {"authors": ["R.E. Barlow", "D.J. Bartholomew", "J.M. Bremner", "H.D. Brunk"], "title": "Statistical inference under order restrictions; the theory and application of isotonic regression", "year": 1972}, {"authors": ["A. Ben-David"], "title": "Automatic generation of symbolic multiattribute ordinal knowledge based DSS: methodology and applications", "venue": "Decision Sciences,", "year": 1992}, {"authors": ["A. Ben-David"], "title": "Monotonicity maintenance in information-theoretic machine learning algorithms", "venue": "Machine Learning,", "year": 1995}, {"authors": ["A. Ben-David", "L. Sterling", "Y.H. Pao"], "title": "Learning and classification of monotonic ordinal concepts", "venue": "Computational Intelligence,", "year": 1989}, {"authors": ["S. Boyd", "N. Parikh", "E. Chu", "B. Peleato", "J. Eckstein"], "title": "Distributed optimization and statistical learning via the alternating direction method of multipliers", "venue": "Foundations and Trends in Machine Learning,", "year": 2010}, {"authors": ["F. Brooks"], "title": "The Mythical Man-Month", "year": 1975}, {"authors": ["J. Casillas", "O. Cordon", "F. Herrera", "L. Magdalena (Eds"], "title": "Trade-off between accuracy and interpretability in fuzzy rule-based modelling", "venue": "Physica-Verlag,", "year": 2002}, {"authors": ["A. Cotter", "M.R. Gupta", "J. Pfeifer"], "title": "A Light Touch for Heavily Constrained SGD", "venue": "arXiv preprint,", "year": 2015}, {"authors": ["N. Dalvi", "M. Olteanu", "M. Raghavan", "P. Bohannon"], "title": "Deduplicating a places database", "venue": "Proc. ACM WWW Conf.,", "year": 2014}, {"authors": ["H. Daniels", "M. Velikova"], "title": "Monotone and partially monotone neural networks", "venue": "IEEE Trans. Neural Networks,", "year": 2010}, {"authors": ["O. Dekel", "R. Gilad-Bachrach", "O. Shamir", "L. Xiao"], "title": "Optimal distributed online prediction using mini-batches", "venue": "Journal Machine Learning Research,", "year": 2012}, {"authors": ["J. Duchi", "E. Hazan", "Y. Singer"], "title": "Adaptive subgradient methods for online learning and stochastic optimization", "venue": "Journal Machine Learning Research,", "year": 2011}, {"authors": ["C. Dugas", "Y. Bengio", "F. B\u00e9lisle", "C. Nadeau", "R. Garcia"], "title": "Incorporating second-order functional knowledge for better option pricing", "venue": "In Advances in Neural Information Processing Systems (NIPS),", "year": 2000}, {"authors": ["C. Dugas", "Y. Bengio", "F. B\u00e9lisle", "C. Nadeau", "R. Garcia"], "title": "Incorporating functional knowledge in neural networks", "venue": "Journal Machine Learning Research,", "year": 2009}, {"authors": ["W. Duivesteijn", "A. Feelders"], "title": "Nearest neighbour classification with monotonicity constraints", "venue": "Proc. European Conf. Machine Learning,", "year": 2008}, {"authors": ["A. Feelders"], "title": "Monotone relabeling in ordinal classification", "venue": "Proc. IEEE Conf. Data Mining, pages 803\u2013808,", "year": 2010}, {"authors": ["M. Fernandez-Delgado", "E. Cernadas", "S. Barro", "D. Amorim"], "title": "Do we need hundreds of classifiers to solve real world classification problems", "venue": "Journal Machine Learning Research,", "year": 2014}, {"authors": ["E.K. Garcia", "M.R. Gupta"], "title": "Lattice regression", "venue": "In Advances in Neural Information Processing Systems (NIPS),", "year": 2009}, {"authors": ["E.K. Garcia", "S. Feldman", "M.R. Gupta", "S. Srivastava"], "title": "Completely lazy learning", "venue": "IEEE Trans. Knowledge and Data Engineering,", "year": 2010}, {"authors": ["E.K. Garcia", "R. Arora", "M.R. Gupta"], "title": "Optimized regression for efficient function evaluation", "venue": "IEEE Trans. Image Processing,", "year": 2012}, {"authors": ["S. Garcia", "A. Fernandez", "J. Luengo", "F. Herrera"], "title": "A study of statistical techniques and performance measures for genetics-based machine learning: accuracy and interpretability", "venue": "Soft Computing,", "year": 2009}, {"authors": ["I.J. Good"], "title": "The Estimation of Probabilities: An Essay on Modern Bayesian Methods", "year": 1965}, {"authors": ["H. Gruber", "M. Holzer", "O. Ruepp"], "title": "Sorting the slow way: an analysis of perversely awful randomized sorting algorithms", "venue": "In Fun with Algorithms,", "year": 2007}, {"authors": ["M. Gupta", "S. Bengio", "J. Weston"], "title": "Training highly multiclass classifiers", "venue": "Journal Machine Learning Research,", "year": 2014}, {"authors": ["M.R. Gupta", "R.M. Gray", "R.A. Olshen"], "title": "Nonparametric supervised learning by linear interpolation with maximum entropy", "venue": "IEEE Trans. Pattern Analysis and Machine Intelligence,", "year": 2006}, {"authors": ["T. Hastie", "R. Tibshirani"], "title": "Generalized Additive Models", "year": 1990}, {"authors": ["T. Hastie", "R. Tibshirani", "J. Friedman"], "title": "The Elements of Statistical Learning", "year": 2001}, {"authors": ["C.C. Holmes", "N.A. Heard"], "title": "Generalized monotonic regression using random change points", "venue": "Statistics in Medicine,", "year": 2003}, {"authors": ["A. Howard", "T. Jebara"], "title": "Learning monotonic transformations for classification", "venue": "In Advances in Neural Information Processing Systems,", "year": 2007}, {"authors": ["H. Ishibuchi", "Y. Nojima"], "title": "Analysis of interpretability-accuracy tradeoff of fuzzy systems by multiobjective fuzzy genetics-based machine learning", "venue": "International Journal of Approximate Reasoning,", "year": 2007}, {"authors": ["T. Joachims", "L. Granka", "B. Pan", "H. Hembrooke", "G. Gay"], "title": "Accurately interpreting clickthrough data as implicit feedback", "venue": "Proc. SIGIR,", "year": 2005}, {"authors": ["H.R. Kang"], "title": "Comparison of three-dimensional interpolation techniques by simulations", "venue": "SPIE Vol. 2414,", "year": 1995}, {"authors": ["H.R. Kang"], "title": "Color Technology for Electronic Imaging Devices", "venue": "SPIE Press,", "year": 1997}, {"authors": ["J. Kasson", "W. Plouffe", "S. Nin"], "title": "A tetrahedral interpolation technique for color space conversion", "venue": "SPIE Vol. 1909,", "year": 1993}, {"authors": ["H. Kay", "L.H. Ungar"], "title": "Estimating monotonic functions and their bounds", "venue": "AIChE Journal,", "year": 2000}, {"authors": ["R.E. Knop"], "title": "A note on hypercube partitions", "venue": "Journal of Combinatorial Theory, Ser. A,", "year": 1973}, {"authors": ["W. Kotlowski", "R. Slowinski"], "title": "Rule learning with monotonicity constraints", "venue": "In Proceedings International Conference on Machine Learning,", "year": 2009}, {"authors": ["F. Lauer", "G. Bloch"], "title": "Incorporating prior knowledge in support vector regression", "venue": "Machine Learning,", "year": 2008}, {"authors": ["X. Liao", "H. Li", "L. Carin"], "title": "Quadratically gated mixture of experts for incomplete data classification", "venue": "Proc. ICML,", "year": 2007}, {"authors": ["T.-Y. Liu"], "title": "Learning to Rank for Information", "year": 2011}, {"authors": ["Malik Magdon-Ismail", "J. Sill"], "title": "A linear fit gets the correct monotonicity directions", "venue": "Machine Learning,", "year": 2008}, {"authors": ["G. Mann", "R. McDonald", "M. Mohri", "N. Silberman", "D.D. Walker"], "title": "Efficient largescale distributed training of conditional maximum entropy models", "venue": "Advances in Neural Information Processing Systems (NIPS),", "year": 2009}, {"authors": ["D.G. Mead"], "title": "Dissection of the hypercube into simplexes", "venue": "Proc. Amer. Math. Soc.,", "year": 1979}, {"authors": ["A. Minin", "M. Velikova", "B. Lang", "H. Daniels"], "title": "Comparison of universal approximators incorporating partial monotonicity by structure", "venue": "Neural Networks,", "year": 2010}, {"authors": ["H. Mukarjee", "S. Stern"], "title": "Feasible nonparametric estimation of multiargument monotone functions", "venue": "Journal of the American Statistical Association,", "year": 1994}, {"authors": ["B. Neelon", "D.B. Dunson"], "title": "Bayesian isotonic regression and trend analysis", "venue": "Biometrics, 60:398\u2013406,", "year": 2004}, {"authors": ["A. Nemirovski", "A. Juditsky", "G. Lan", "A. Shapiro"], "title": "Robust stochastic approximation approach to stochastic programming", "venue": "SIAM Journal on Optimization,", "year": 2009}, {"authors": ["K. Neumann", "M. Rolf", "J.J. Steil"], "title": "Reliable integration of continuous constraints into extreme learning machines", "venue": "International Journal of Uncertainty, Fuzziness and KnowledgeBased Systems,", "year": 2013}, {"authors": ["R. Nock"], "title": "Inducing interpretable voting classifiers without trading accuracy for simplicity: Theoretical results, approximation algorithms, and experiments", "venue": "Journal Artificial Intelligence Research,", "year": 2002}, {"authors": ["K.-M. Osei-Bryson"], "title": "Post-pruning in decision tree induction using multiple performance measures", "venue": "Computers and Operations Research,", "year": 2007}, {"authors": ["R. Potharst", "A.J. Feelders"], "title": "Classification trees for problems with monotonicity constraints", "venue": "ACM SIGKDD Explorations,", "year": 2002}, {"authors": ["R. Potharst", "A.J. Feelders"], "title": "Pruning for monotone classification", "venue": "trees. Springer Lecture Notes on Computer Science,", "year": 2002}, {"authors": ["Y.-J. Qu", "B.-G. Hu"], "title": "Generalized constraint neural network regression model subject to linear priors", "venue": "IEEE Trans. on Neural Networks,", "year": 2011}, {"authors": ["J.O. Ramsay"], "title": "Estimating smooth monotone functions", "venue": "Journal of the Royal Statistical Society, Series B,", "year": 1998}, {"authors": ["G. R\u00e4tsch", "S. Sonnenburg", "C. Sch\u00e4fer"], "title": "Learning interpretable SVMs for biological sequence classification", "venue": "BMC Bioinformatics,", "year": 2006}, {"authors": ["J. Riihim\u00e4ki", "A. Vehtari"], "title": "Gaussian processes with monotonicity information", "venue": "In International Conference on Artificial Intelligence and Statistics,", "year": 2010}, {"authors": ["R. Rovatti", "M. Borgatti", "R. Guerrieri"], "title": "A geometric approach to maximum-speed ndimensional continuous linear interpolation in rectangular grids", "venue": "IEEE Trans. on Computers,", "year": 1998}, {"authors": ["F. Schimdt", "R. Simon"], "title": "Some geometric probability problems involving the Eulerian numbers", "venue": "Electronic Journal of Combinatorics,", "year": 2007}, {"authors": ["G. Sharma", "R. Bala"], "title": "Digital Color Imaging Handbook", "year": 2002}, {"authors": ["T.S. Shively", "T.W. Sager", "S.G. Walker"], "title": "A Bayesian approach to non-parametric monotone function estimation", "venue": "Journal of the Royal Statistical Society, Series B,", "year": 2009}, {"authors": ["P.K. Shukla", "S.P. Tripathi"], "title": "A review on the interpretability-accuracy trade-off in evolutionary multi-objective fuzzy systems (EMOFS)", "year": 2012}, {"authors": ["J. Sill", "Y.S. Abu-Mostafa"], "title": "Monotonicity hints", "venue": "Advances in Neural Information Processing Systems (NIPS),", "year": 1997}, {"authors": ["D.J. Spiegelhalter", "R.P. Knill-Jones"], "title": "Statistical and knowledge-based approaches to clinical decision support systems, with an application in gastroenterology", "venue": "Journal of the Royal Statistical Society A,", "year": 1984}, {"authors": ["J. Spouge", "H. Wan", "W.J. Wilbur"], "title": "Least squares isotonic regression in two dimensions", "venue": "Journal of Optimization Theory and Applications,", "year": 2003}, {"authors": ["C. Strannegaard"], "title": "Transparent neural networks", "venue": "Proc. Artificial General Intelligence,", "year": 2012}, {"authors": ["B. Sun", "S. Zhou"], "title": "Study on the 3D interpolation models used in color conversion", "venue": "IACSIT Intl. Journal Engineering and Technology,", "year": 2012}, {"authors": ["A. van Esbroeck", "S. Singh", "I. Rubinfeld", "Z. Syed"], "title": "Evaluating trauma patients: Addressing missing covariates with joint optimization", "venue": "Proc. AAAI,", "year": 2014}, {"authors": ["M. Villalobos", "G. Wahba"], "title": "Inequality-constrained multivariate smoothing splines with application to the estimation of posterior probabilities", "venue": "Journal of the American Statistical Association,", "year": 1987}, {"authors": ["S. Wang"], "title": "A neural network method of density estimation for univariate unimodal data", "venue": "Neural Computing & Applications,", "year": 1994}, {"authors": ["L. Yu", "J. Xiao"], "title": "Trade-off between accuracy and interpretability: experience-oriented fuzzy modeling via reduced-set vectors", "venue": "Computers and Mathematics with Applications,", "year": 2012}], "sections": [{"text": "Keywords: interpretability, interpolation, look-up tables, monotonicity"}, {"heading": "1. Introduction", "text": "Many challenging issues arise when making machine learning useful in practice. Evaluation of the trained model may need to be fast. Features may be categorical, missing, or poorly calibrated. A blackbox model may be unacceptable: users may require guarantees that the function will behave sensibly for all samples, and prefer functions that are easier to understand and debug.\nWe have found that a key interpretability issue in practice is whether the learned model can be guaranteed to be monotonic with respect to some features. For example, suppose the goal is to estimate the value of a used car, and one of the features is the number of km it has been driven. If all the other feature values are held fixed, we expect the value of the\nar X\niv :1\n50 5.\n06 37\n8v 3\n[ cs\n.L G\nused car to never increase as the number of km driven increases. But a model learned from a small set of noisy samples may not, in fact, respect this prior knowledge.\nIn this paper, we propose learning monotonic, efficient, and flexible functions by constraining and calibrating interpolated look-up tables in a structural risk minimization framework. Learning monotonic functions is difficult, and previously published work has only been illustrated on small problems (see Table 1). Our experimental results demonstrate learning flexible, guaranteed monotonic functions on more features and data than prior work, and that these functions achieve state-of-the-art performance on real-world problems.\nThe parameters of an interpolated look-up table are simply values of the function, regularly spaced in the input space, and these values are interpolated to compute f(x) for any x. See Figures 1 and 2 for examples of 2\u00d7 2 and 2\u00d7 3 look-up tables and the functions produced by interpolating them. Each parameter has a clear meaning: it is the value of the function for a particular input, for a set of inputs on a regular grid. These parameters can be individually read and checked to understand the learned function\u2019s behavior.\nInterpolating look-up tables is a classic strategy for representing low-dimensional functions. For example, backs of old textbooks have pages of look-up tables for one-dimensional functions like sin(x), and interpolating look-up tables is standardized by the ICC Profile for the three and four dimensional nonlinear transformations needed to color manage printers\n(Sharma and Bala, 2002). Using the efficient linear interpolation method we refer to as simplex interpolation, we demonstrate that interpolating a look-up table with twenty features took 2 microseconds on a standard CPU. The practical limit to the number of features is more limited by the number of parameters, which scales as 2D for D features.\nEstimating the parameters of an interpolated look-up table using structural risk minimization was proposed by Garcia and Gupta (2009) and called lattice regression. Lattice regression can be viewed as a kernel method that uses the explicit nonlinear feature transformation formed by mapping an input x \u2208 [0, 1]D to a vector of linear interpolation weights \u03c6(x) \u2208 \u22062D over the 2D vertices of the look-up table cell that contains x, where \u2206 denotes the standard simplex. Then the function is linear in these transformed features: f(x) = \u03b8T\u03c6(x). We will refer to the look-up table parameters \u03b8 as the lattice, and to the interpolated look-up table f(x) as the lattice function. Earlier work in lattice regression focused on learning highly nonlinear functions over 2 to 4 features with fine-grained lattices, such as a 17 \u00d7 17 \u00d7 17 lattice for modeling a color printer or super-resolution of spherical images (Garcia et al., 2010, 2012). In this paper, we apply lattice regression to more generic machine learning problems with D = 5 to 16 features, and show that 2D lattices work well for many real-world classification and ranking problems, especially when paired with jointly trained one-dimensional pre-processing functions.\nWe begin with a survey of related work in machine learning of interpretable and monotonic functions. Then we review lattice regression in Section 3. The main contribution is learning monotonic lattices in Section 4. We discuss efficient linear interpolation in Section 5. We propose an interpretable torsion lattice regularizer in Section 6. We propose jointly\nlearning one-dimensional calibration functions in Section 7, and consider two strategies for supervised handling of missing data for lattice regression in Section 8. In Section 9, we consider strategies for speeding up training and handling large-scale problems and largescale constraint-handling. A series of case studies in Section 10 experimentally explore the paper\u2019s proposals, and demonstrate that monotonic lattice regression achieves similar accuracy as a random forest, and that monotonicity is a common issue that arises in many different applications. The paper ends with conclusions and open questions in Section 11."}, {"heading": "2. Related Work", "text": "We give a brief overview of related work in interpretable machine learning, then survey related work in learning monotonic functions."}, {"heading": "2.1 Related Work in Interpretable Machine Learning", "text": "Two key themes of the prior work on interpretable machine learning are (i) interpretable function classes, and (ii) preferring simpler functions within a function class."}, {"heading": "2.1.1 Interpretable Function Classes", "text": "The function classes of decision trees and rules are generally regarded as relatively interpretable. Na\u0308\u0131ve Bayes classifiers can be interpreted in terms of weights of evidence (Good, 1965; Spiegelhalter and Knill-Jones, 1984). Similarly, linear models form an interpretable function class in that the parameters dictate the relative importance of each feature. Linear approaches can be generalized to sum nonlinear components, as in generalized additive models (Hastie and Tibshirani, 1990) and some kernel methods, while still retaining some of their interpretable aspects.\nThe function class of interpolated look-up tables is interpretable in that the function\u2019s parameters are the look-up table values, and so are semantically meaningful: they are simply examples of the function\u2019s output, regularly spaced in the domain. Given two look-up tables with the same structure and the same features, one can analyze how their functions differ by analyzing how the look-up table parameters differ. Analyzing which parameters change by how much can help answer questions like \u201cIf I add training examples and re-train, what changes about the model?\u201d"}, {"heading": "2.1.2 Prefer Simpler Functions", "text": "Another body of work focuses on choosing simpler functions within a function class, optimizing an objective of the form: minimize empirical error and maximize simplicity, where simplicity is usually defined as some manifestation of Occam\u2019s Razor or variant of Kolmogorov complexity. For example, Ishibuchi and Nojima (2007) minimize the number of fuzzy rules in a rule set, Osei-Bryson (2007) prunes a decision tree for interpretability, Ra\u0308tsch et al. (2006) finds a sparse convex combination of kernels for a multi-kernel support vector machine, and Nock (2002) prefers smaller committees of ensemble classifiers. Similarly, Garcia et al. (2009) measure the interpretability of rule-based classifiers in terms of the number of rules and number of features used. More generally, this category of interpretability includes model selection criteria like the Bayesian information criterion and\nAkaike information criterion (Hastie et al., 2001), sparsity regularizers like sparse linear regression models, and feature selection methods. Other approaches to simplicity may include simplified structure in graphical models or neural nets, such as the structured neural nets of (Strannegaard, 2012).\nWhile sparsity-based approaches to interpretability can provide regularization that reduces over-fitting and hence increases accuracy, it has also been noted that such strategies may create a trade-off between interpretability and accuracy (Casillas et al., 2002; Nock, 2002; Yu and Xiao, 2012; Shukla and Tripathi, 2012). We hypothesize this occurs when the assumed simpler structure is a poor model of the true function.\nMonotonicity is another way to choose a semantically simpler function to increase interpretability (and regularize). Our case studies in Section 10 illustrate that when applied to problems where monotonicity is a good prior model, we do not see a trade-off with accuracy."}, {"heading": "2.2 Related Work in Monotonic Functions", "text": "A function f(x) is monotonically increasing with respect to feature d if f(xi) \u2265 f(xj) for any two feature vectors xi, xj \u2208 RD where xi[d] \u2265 xj [d] and xi[m] = xj [m] for m 6= d.\nA number of approaches have been proposed for enforcing and encouraging monotonicity in machine learning. The computational complexity of these algorthims tends to be high, and most methods scale poorly in the number of features D and samples n, as summarized in Table 1.\nWe detail the related work in the following sections organized by the type of machine learning, but these methods could instead be organized by strategy, which mostly falls into one of four categories:\n1. Constrain a more flexible function class to be monotonic, such as linear functions with positive coefficients, or a sigmoidal neural network with positive weights.\n2. Post-process by pruning or reducing monotonicity violations after training.\n3. Penalize monotonicity violations by pairs of samples or sample derivatives when training.\n4. Re-label samples to be monotonic before training."}, {"heading": "2.2.1 Monotonic Linear and Polynomial Functions", "text": "Linear functions can be easily constrained to be monotonic in certain inputs by requiring the corresponding slope coefficients to be non-negative, but linear functions are not sufficiently flexible for many problems. Polynomial functions (equivalently, linear functions with pre-defined crosses of features) can also be easily forced to be monotonic by requiring all coefficients to be positive. However, this is only a sufficient and not necessary condition: there are monotonic polynomials whose coefficients are not all positive. For example, consider the simple case of second degree multilinear polynomials defined over the unit square f : [0, 1]2 \u2192 R such that:\nf(x) = a0 + a1x[0] + a2x[1] + a3x[0]x[1]. (1)\nForcing the derivative to be positive on the domain x \u2208 [0, 1]2, one sees that the complete set of monotonic functions of the form (1) on the unit square is described by four linear inequalities:\na1 > 0\na2 > 0\na1 + a3 > 0\na2 + a3 > 0.\nThe general problem of checking whether a particular choice of polynomial coefficients produces a monotonic function requires checking whether the polynomial\u2019s derivative (also a polynomial) is positive everywhere, which is equivalent to checking if the derivative has any real roots, which can be computationally challenging (see, for example, Sturm\u2019s theorem for details).\nFunctions of the form (1) can be equivalently expressed as a 2 \u00d7 2 lattice interpolated with multilinear interpolation, but as we will show in Section 4, with this alternate parameterization it is easier to check and enforce the complete set of monotonic functions."}, {"heading": "2.2.2 Monotonic Splines", "text": "In this paper we extend lattice regression, which is a spline method with fixed knots on a regular grid and a linear kernel (Garcia et al., 2012), to be monotonic. There have been a number of proposals to learn monotonic one-dimensional splines. For example, building on Ramsay (1998), Shively et al. (2009) parameterize the set of all smooth and strictly monotonic one-dimensional functions using an integrated exponential form f(x) = a+ \u222b x 0 e\nb+u(t)dt, and showed better performance than the monotone functions estimators of Neelon and Dunson (2004) and Holmes and Heard (2003) for smooth functions. In other related spline work, Villalobos and Wahba (1987) considered smoothing splines with linear inequality constraints, but did not address monotonicity."}, {"heading": "2.2.3 Monotonic Decision Trees and Forests:", "text": "Stumps and forests of stumps are easily constrained to be monotonic. However, for deeper or broader trees, all pairs of leaves must be checked to verify monotonicity (Potharst and Feelders, 2002b). Non-monotonic trees can be pruned to be monotonic using various strategies that iteratively reduce the non-monotonic branches (Ben-David, 1992; Potharst and Feelders, 2002b). Monontonicity can also be encouraged during tree construction by penalizing the splitting criterion to reduce the number of non-monotonic leaves a split would create (Ben-David, 1995). Potharst and Feelders (2002a) achieved completely flexible monotonic trees using a strategy akin to bogosort (Gruber et al., 2007): train many trees on different random subsets of the training samples, then select one that is monotonic.\n7"}, {"heading": "2.2.4 Monotonic Support Vector Machines", "text": "With a linear kernel, it may be easy to check and enforce monotonicity of support vector machines, but for nonlinear kernels it will be more challenging. Lauer and Bloch (2008) encouraged support vector machines to be more monotonic by constraining the derivative of the function at the training samples. Riihima\u0308ki and Vehtari (2010) used the same strategy to encourage more monotonic Gaussian processes."}, {"heading": "2.2.5 Monotonic Neural Networks", "text": "In perhaps the earliest work on monotonic neural networks, Archer and Wang (1993) adaptively down-weighted samples during training whose gradient updates would violate monotonicity, to produce a positive weighted neural net. Other researchers explicitly proposed constraining the weights to be positive in a single hidden-layer neural network with the sigmoid or other monotonic nonlinear transformation (Wang, 1994; Kay and Ungar, 2000; Dugas et al., 2000, 2009; Minin et al., 2010). Dugas et al. (2009) showed with simulations of four features and 400 training samples that both bias and variance were reduced by enforcing monotonicity. However, Daniels and Velikova (2010) showed this approach requires D hidden layers to arbitrarily approximate any D-dimensional monotonic function. In addition to a general proof, they provide a simple and realistic example of a two-dimensional monotonic function that cannot be fit with one hidden layer and positive weights.\nAbu-Mostafa (1993) and Sill and Abu-Mostafa (1997) proposed regularizing a function to be more monotonic by penalizing squared deviations in monotonicity for pairs of the input samples. This strategy only works if all the features are constrained to be monotonic (otherwise it is not clear how to order a given pair of input samples). Unfortunately, it generally does not guarantee monotonicity everywhere, only with respect to those sampled pairs. (And in fact, to guarantee monotonicity for the sampled pairs, an exact penalty rather than squared error would be needed with a sufficiently large regularization parameter to ensure the regularization was equivalent to a constraint).\nLauer and Bloch (2008), Riihima\u0308ki and Vehtari (2010), and Neumann et al. (2013) encouraged extreme learning machines to be more monotonic by constraining the derivative of the function to be positive for a set of sampled points.\nQu and Hu (2011) did a small-scale comparison of encouraging monotonicity by constraining input pairs to be monotonic, encouraging monotonic neural nets by constraining the function\u2019s derivatives at a subset of samples (analogous to Lauer and Bloch (2008)), and using a sigmoidal function with positive weights. They concluded the positive-weight sigmoidal function is best.\nSill (1998) proposed a guaranteed monotonic neural network with two hidden layers by requiring the first linear layer\u2019s weights to be positive, using hidden nodes that take the maximum of groups of first layer variables, and a second hidden layer that takes the minimum of the maxima. The resulting surface is piecewise linear, and as such can represent any continuous differentiable function arbitrarily well. The resulting objective function is not strictly convex, but the authors propose training such monotonic networks using gradient descent where samples are associated with one active hyperplane at each iteration. Daniels and Velikova (2010) generalized this approach to handle the \u201cpartially monotonic\u201d case that the function is only monotonic with respect to some features."}, {"heading": "2.2.6 Isotonic Regression and Monotonic Nearest Neighbors", "text": "Isotonic regression re-labels the input samples with values that are monotonic and close to the original labels. These monotonically re-labeled samples can then be used, for example, to define a monotonic piecewise constant or piecewise linear surface. This is an old approach; see Barlow et al. (1972) for an early survey. Isotonic regression can be solved in O(n) time if monotonicity implies a total ordering of the n samples. But for usual multi-dimensional machine learning problems, monotonicity implies only a partial order, and solving the nparameter quadratic program is generally O(n4), and O(n3) for two-dimensional samples (Spouge et al., 2003). Also problematic for large n is the O(n) evaluation time for new samples.\nMukarjee and Stern (1994) proposed a suboptimal monotonic kernel regression that is computationally easier to train than isotonic regression. It computes a standard kernel estimate, then locally upper and lower bounds it to enforce monotonicity, for overall O(n) evaluation time.\nThe isotonic separation method of Chandrasekaran et al. (2005) is like the work of AbuMostafa (1993) in that it penalizes violations of monotonicity by pairs of training samples. Like isotonic regression, the output is a re-labeling of the original samples, the solution is at least O(n3) in the general case, and evaluation time is O(n).\nBen-David et al. (1989); Ben-David (1992) constructed a monotonic rule-based classifier by sequentially adding training examples (each of which defines a rule) that do not violate monotonicity restrictions.\nDuivesteijn and Feelders (2008) proposed re-labeling samples before applying nearest neighbors based on a monotonicity violation graph with the training examples at the vertices. Coupled with a proposed modified version of k-NN, they can enforce monotonic outputs. Similar pre-processing of samples can be used to encourage any subsequently trained classifier to be more monotonic (Feelders, 2010).\nSimilarly, Kotlowski and Slowinski (2009) try to solve the isotonic regression problem to re-label the dataset to be monotonic, then fit a monotonic ensemble of rules to the re-labeled data, requiring zero training error. They showed overall better performance than the ordinal learning model of Ben-David et al. (1989) and isotonic separation (Chandrasekaran et al., 2005)."}, {"heading": "3. Review of Lattice Regression", "text": "Before proposing monotonic lattice regression, we review lattice regression (Garcia and Gupta, 2009; Garcia et al., 2012). Key notation is listed in Table 2.\nLet Md \u2208 N be a hyperparameter specifying the number of vertices in the look-up table (that is, lattice) for the dth feature. Then the lattice is a regular grid of M 4 = M1 \u00d7 M2 \u00d7 . . .MD parameters (a look-up table) placed at natural numbers so that the lattice spans the hyper-rectangleM 4= [0,M1\u2212 1]\u00d7 [0,M2\u2212 1]\u00d7 . . . [0,MD\u2212 1]. See Figure 1 for examples of 2 \u00d7 2 lattices, and Figure 2 for an example 3 \u00d7 2 lattice. For machine learning problems we find Md = 2 for all d to be a good default, as detailed in the case studies in Section 10. For image processing applications with only two to four features, much larger values of Md were needed (Garcia et al., 2012).\nThe feature values are assumed to be bounded and linearly scaled to fit the lattice, so that the dth feature vector value x[d] lies in [0,Md \u2212 1]. (We propose learning non-linear scalings of features jointly with the lattice parameters in Section 7.)\nLattice regression is a kernel method that maps x \u2208M to a transformed feature vector \u03c6(x) \u2208 [0, 1]M . The values of \u03c6(x) are the interpolation weights for x for the 2D indices corresponding to the 2D vertices of the hypercube surrounding x; for all other indices, \u03c6(x) = 0.\nThe function f(x) is linear in \u03c6(x) such that f(x) = \u03b8T\u03c6(x). That is, the function parameters \u03b8 each correspond to a vertex in the lattice, and f(x) linearly interpolates the \u03b8 for the lattice cell containing x.\nBefore reviewing the lattice regression objective for learning the parameters \u03b8, we review standard multilinear interpolation to define \u03c6(x)."}, {"heading": "3.1 Multilinear Interpolation", "text": "The familiar bilinear interpolation commonly used to up-sample images is the D = 2 case of the multilinear interpolation that we review here. See Figure 2 for a pictorial explanation.\nFor notational simplicity, we assume a 2D lattice such that x \u2208 [0, 1]D. For multi-cell lattices, the same math and logic is applied to the lattice cell containing the x. Denote the kth component of \u03c6(x) as \u03c6k(x). Let vk \u2208 {0, 1}D be the kth vertex of the unit hypercube. The multilinear interpolation weight on the vertex vk is\n\u03c6k(x) = D\u22121\u220f d=0 x[d]vk[d](1\u2212 x[d])1\u2212vk[d]. (2)\nNote the exponents in (2) are vk and 1 \u2212 vk[d], which either equal 0 and 1, or equal 1 and 0, so these exponents act like selectors that multiply in either x[d] or 1\u2212 x[d] for each dimension d. Equivalently, one can write\n\u03c6k(x) = D\u22121\u220f i=0 ((1\u2212 bit[i, k]) (1\u2212 x[i]) + bit[i, k]x[i]) , (3)\nwhere bit[i, k] \u2208 {0, 1} denotes the ith bit of vertex vk, and can be computed bit[i, k] = (k i) &1 using bitwise arithmetic.\nThe resulting f(x) = \u03b8T\u03c6(x) is a multilinear polynomial over each lattice cell. For example, a 2\u00d72 lattice interpolated with multilinear interpolation (2) produces the function:\nf(x) = \u03b8[0](1\u2212 x[0])(1\u2212 x[1]) + \u03b8[1]x[0](1\u2212 x[1]) + \u03b8[2](1\u2212 x[0])x[1] + \u03b8[3]x[0]x[1]. (4)\nExpanding (4), one sees it is a different parameterization of the multilinear function given in (1), where the parameter vectors are related by a linear matrix transform: a = T\u03b8 for T \u2208 R4\u00d74. But parameterizing with \u03b8 makes the parameters easier to read, and as we show in Section 4, makes it easier to learn the complete set of monotonic functions.\nThe linear interpolation is applied per lattice cell. At lattice cell boundaries the resulting function is continuous, but not differentiable, and is piecewise polynomial, and hence a spline. It can be equivalently formulated using a linear basis function. Higher-order basis functions like the popular cubic spline will lead to smoother and potentially slightly more accurate functions (Garcia et al., 2012). However, higher-order basis functions destroy the interpretable localized effect of the parameters, and increase the computational complexity.\nThe multilinear interpolation weights are just one type of linear interpolation. In general, linear interpolation weights are defined as solutions to the system of D + 1 equations:\n2D\u2211 k=0 \u03c6k(x)vk = x and 2D\u2211 k=0 \u03c6k(x) = 1. (5)\nThis system of equations is under-determined and has many solutions for an x in the convex hull of a lattice cell. The multilinear interpolation weights given in (2) are the maximum entropy solution to (5) (Gupta et al., 2006), and thus have good noise averaging and smoothness properties compared to other solutions. We discuss a more efficient linear interpolation in Sec. 5.2."}, {"heading": "3.2 The Lattice Regression Objective", "text": "Consider the standard supervised machine learning set-up of a training set of randomly sampled pairs {(xi, yi)} pairs, where xi \u2208 M and yi \u2208 R, for i = 1, . . . , n. Historically, people created look-up tables by first fitting a function h(x) to the {xi, yi} using a regression algorithm such as a neural net or local linear regression, and then evaluating h(x) on a regular grid to produce the look-up table values (Sharma and Bala, 2002). However, even if they fit the function to minimize empirical risk on the training samples, they did not minimize the actual empirical risk because these approaches did not take into account that the produced look-up table would be interpolated at run-time, and this interpolation changes the error on the training samples.\nGarcia and Gupta (2009) proposed directly optimizing the look-up table parameters \u03b8 to minimize the empirical error between the training labels and the interpolated look-up table:\narg min \u03b8\u2208RM n\u2211 i=1 `(yi, \u03b8 T\u03c6(xi)) +R(\u03b8), (6)\nwhere ` is a loss function such as squared error, \u03c6(xi) \u2208 [0, 1]M is the vector of linear interpolation weights over the lattice for training sample xi (detailed in Section 3.1 and Sec. 5.2), f(xi) = \u03b8\nT\u03c6(xi) is the linear interpolation of xi from the look-up table parameters \u03b8, and R(\u03b8) is a regularizer on the lattice parameters. In general, we assume the loss ` and regularizer R are convex functions of \u03b8 so that solving (6) is a convex optimization. Prior work focused on squared error loss, and used graph regularizers R(\u03b8) of the form bTKb for some PSD matrix K, in which case (6) has a closed-form solution which can be computed with sparse matrix inversions (Garcia and Gupta, 2009; Garcia et al., 2010, 2012)."}, {"heading": "4. Monotonic Lattices", "text": "In this section we propose constraining lattice regression to learn monotonic functions."}, {"heading": "4.1 Monotonicity Constraints For a Lattice", "text": "In general, simply checking whether a nonlinear function is monotonic can be quite difficult (see the related work in Section 2.2). But for a linearly interpolated look-up table, checking for monotonicity is relatively easy: if the lattice values increase in a given direction, then the function increases in that direction. See Figure 1 for examples. Specifically, one must check that \u03b8s > \u03b8r for each pair of adjacent look-up table parameters \u03b8r and \u03b8s. If all features are specified to be monotonic for a 2D lattice, this results in D2D\u22121 pairwise linear inequality constraints to check.\nThese same pairwise linear inequality constraints can be imposed when learning the parameters \u03b8 to ensure a monotonic function is learned. The following result establishes these constraints are sufficient and necessary for a 2D lattice to be monotonically increasing in the dth feature (the result extends trivially to larger lattices):\nLemma 1 (Monotonicity Constraints) Let f(x) = \u03b8T\u03c6(x) for x \u2208 [0, 1]D and \u03c6(x) given in (2). The partial derivative \u2202f(x)/\u2202x[d] > 0 for fixed d and any x iff \u03b8k\u2032 > \u03b8k for all k, k\u2032 such that vk[d] = 0, vk\u2032 [d] = 1 and vk[m] = vk\u2032 [m] for all m 6= d.\nProof First we show the constraints are necessary to ensure monotonicity. Consider the function values f(vk) and f(vk\u2032) for some adjacent pair of vertices vk, vk\u2032 that differ only in the dth feature. For f(vk) and f(vk\u2032), all of the interpolation weight falls on \u03b8k or \u03b8k\u2032 respectively, such that f(vk) = \u03b8k and f(vk\u2032) = \u03b8k\u2032 . So \u03b8k\u2032 > \u03b8k is necessary for \u2202f(x)/\u2202x[d] > 0 everywhere.\nNext we show the constraints are sufficient to ensure monotonicity. Pair the terms in the interpolation f(x) = \u03b8T\u03c6(x) corresponding to adjacent parameters \u03b8k, \u03b8k\u2032 so that for\neach k, k\u2032 it holds that vk[d] = 0, vk\u2032 [d] = 1, vk[m] = vk\u2032 [m] for m 6= d:\nf(x) = \u2211 k,k\u2032 \u03b8k\u03c6k(x) + \u03b8k\u2032\u03c6k\u2032(x), then expand \u03c6k(x) and \u03c6k\u2032(x) using (2) :\n= \u2211 k,k\u2032 \u03b1k ( \u03b8kx[d] vk[d](1\u2212 x[d](1\u2212vk[d])) + \u03b8k\u2032x[d]vk\u2032 [d](1\u2212 x[d](1\u2212vk\u2032 [d])) ) , where \u03b1k is the product of the m 6= d terms in (2) that are the same for k and k\u2032,\n= \u2211 k,k\u2032 \u03b1k (\u03b8k(1\u2212 x[d]) + \u03b8k\u2032x[d]) by the definition of vk and vk\u2032 . (7)\nThe partial derivative of (7) is \u2202f(x)\u2202x[d] = \u2211\nk,k\u2032 \u03b1k(\u03b8k\u2032 \u2212 \u03b8k). Because each \u03b1k \u2208 [0, 1], it is sufficient that \u03b8k\u2032 > \u03b8k for each k, k \u2032 pair to guarantee this partial is positive for all x."}, {"heading": "4.2 Monotonic Lattice Regression Objective", "text": "We relax strict monotonicity to monotonicity by allowing equality in the adjacent parameter constraints (for an example, see the second function from the left in Figure 1). Then the set of pairwise constraints can be expressed as A\u03b8 \u2264 0 for the appropriate sparse matrix A with one 1 and \u22121 per row of A, and one row per constraint. Each feature can independently be left unconstrained, or constrained to be either monotonically increasing or decreasing by the specificiation of A.\nThus the proposed monotonic lattice regression objective is convex with linear inequality constraints:\narg min \u03b8 n\u2211 i=1 `(yi, \u03b8 T\u03c6(xi)) +R(\u03b8), s.t. A\u03b8 \u2264 b. (8)\nAdditional linear constraints can be included in A\u03b8 \u2264 b to also constrain the fitted function in other practical ways, such as f(x) \u2208 [0, 1] or f(x) \u2265 0.\nThe approach extends to the standard learning to rank from pairs problem (Liu, 2011), where the training data is pairs of samples x+i and x \u2212 i and the goal is to learn a function such that f(x+i ) \u2265 f(x \u2212 i ) for as many pairs as possible. For this case, the monotonic lattice regression objective is:\narg min \u03b8 n\u2211 i=1 `(1, \u03b8T\u03c6(x+i )\u2212 \u03b8 T\u03c6(x\u2212i )) +R(\u03b8), s.t. A\u03b8 \u2264 b. (9)\nThe loss functions in (6), (8) and (9) all have the same form, for example, squared loss `(y, z) = (y \u2212 z)2, hinge loss `(y, z) = max(0, 1 \u2212 yz), or logistic loss `(y, z) = log(1 + exp(y \u2212 z))."}, {"heading": "5. Faster Linear Interpolation", "text": "Interpolating a look-up table has long been considered an efficient way to specify and evaluate a low-dimensional non-linear function (Sharma and Bala, 2002; Garcia et al., 2012).\nBut computing linear interpolation weights with (3) requires O(D) operations for each of the 2D interpolation weights, for a total cost of O(D2D). In Section 5.1, we show the multilinear interpolation weights of (3) can be computed in O(2D) operations. Then, in Section 5.2, we review and analyze a different linear interpolation that we refer to as simplex interpolation that takes only O(D logD) operations."}, {"heading": "5.1 Fast Multilinear Interpolation", "text": "Much of the computation in (3) can be shared between the different weights. In Algorithm 1 we give a dynamic programming solution that loops D times, where the dth loop takes 2d time, so in total there are \u2211D\u22121 d=0 2 d = O(2D) operations.\nAlgorithm 1 Computes the multilinear interpolation weights and corresponding vertex indices for a unit lattice cell [0, 1]D and an x \u2208 [0, 1]D. Let the lattice parameters be indexed such that sd = 2\nd is the difference in the indices of the parameters corresponding to any two vertices that are adjacent in the dth dimension, for example, for the 2\u00d7 2 lattice, order the vertices [0 0], [1 0], [0 1], [1 1] and index the corresponding lattice parameters in that order.\nCalculateMultilinearInterpolationWeightsAndParameterIndices(x) 1 Initialize indices = [0], weights = [1] 2 For d = 0 to D \u2212 1: 3 For k = 0 to 2d \u2212 1: 4 Append sd + indices[k] to indices 5 Append x[d]\u00d7 weights[k] to weights 6 Update weights[k] = (1\u2212 (x[d]))\u00d7 weights[k] 7 Return indices and weights\nThe following lemma establishes the correctness of Algorithm 1.\nLemma 2 (Fast Multilinear Interpolation) Under its assumptions, Algorithm 1 returns the indices of the 2D parameters corresponding to the vertices of the lattice cell containing x:\nindices[k] = D\u22121\u2211 d=0 (bx[d]c+ biti(k)) sd, for k = 1, 2, . . . , 2D (10)\nand the corresponding 2D multilinear interpolation weights given by (3).\nProof At the end of the D\u2032th iteration over the dimension in Algorithm 1:\nsize (indices) = size (weights) = 2D \u2032+1\nindices[k] = D\u2032\u2211 d=0 (bx[d]c+ bitd(k)) sd\nweights[k] = D\u2032\u220f d=0 ((1\u2212 bitd(k)) (1\u2212 (x[d]\u2212 bxdc)) + bitd(k)(x[d]\u2212 bxdc)) .\nThe above holds for the D\u2032 = 1 case by the initialization and inspection of the loop. It is straightforward to verify that if the above hold for D\u2032, then they also hold for D\u2032+ 1. Then by induction it holds for D\u2032 = D \u2212 1, as claimed."}, {"heading": "5.2 Simplex Linear Interpolation", "text": "For speed, we propose using a more efficient linear interpolation for lattice regression that linearly interpolates each x from only D+ 1 of the 2D surrounding vertices. Many different linear interpolation strategies have been proposed to interpolate look-up tables using only a subset of the 2D vertices (for a review, see Kang (1997)). However, with most strategies it is too computationally expensive to determine exactly which of the vertices should be used to interpolate each x. The wonder of simplex interpolation is that it takes only O(D logD) operations to determine the D+1 vertices needed to interpolate any given x, and then only O(D) operations to interpolate the identified D + 1 vertices. A comparison of simplex and multilinear interpolation is given in Figure 3 for the same look-up table parameters.\nSimplex interpolation was proposed in the color management literature by Kasson et al. (1993), and independently later by Rovatti et al. (1998). Simplex interpolation is also known as the Lovasz extension in submodular optimization, where it is used to extend a function defined on the vertices of a unit hypercube to be defined on its interior (Bach, 2013).\nAfter reviewing how simplex interpolation works, we show in Section 5.2.3 that it requires the same constraints for monotonicity as multilinear interpolation, and then we discuss how its rotational dependence impacts machine learning in Section 5.2.4. We give example runtime comparisons in Section 10.7."}, {"heading": "5.2.1 Partitioning of the Unit Hypercube Into Simplices", "text": "Simplex interpolation implicitly partitions the hypercube into the set of D! congruent simplices that satisfy the following: each simplex includes the all 0\u2019s vertex, one vertex is all zeros but has a single 1, one vertex is all zeros but has two 1\u2019s, and so on, ending with one vertex that is all 1\u2019s, for a total of D + 1 vertices in each simplex. Figure 4 shows the partitioning for the D = 2 and D = 3 unit hypercubes.\nThis decomposition can also be described by the hyperplanes xk = xr for 1 \u2264 k \u2264 r \u2264 D (Schimdt and Simon, 2007). Knop (1973) discussed this decomposition as a special case of Eulerian partitioning of the hypercube, and Mead (1979) showed this is the smallest possible equivolume decomposition of the unit hypercube."}, {"heading": "5.2.2 Simplex Interpolation", "text": "Given x \u2208 [0, 1]D, the D + 1 vertices that specify the simplex that contains x can be computed in O(D logD) operations by sorting the D values of the feature vector x, and then the dth simplex vertex has ones in the first d sorted components of x. For example, if x =[.8 .2 .3], the D + 1 vertices of its simplex are [0 0 0], [1 0 0], [1 0 1], [1 1 1].\nLet V be the D+1 by D matrix whose dth row is the dth vertex of the simplex containing x. Then the simplex interpolation weights \u03c8(x) must satisfy the linear interpolation\nequations given in (5) such that\n[ V T\n1T\n] \u03c8(x) = [ x 1 ] . Thus \u03c8(x) = [ V T 1T ]\u22121 [ x 1 ] , where be-\ncause of the highly structured nature of the simplex decomposition the required inverse always exists, and has a simple form such that \u03c8(x) is the difference of sequential sorted components of x. For example, for a 2\u00d7 2 lattice, and an x such that x[0] > x[1], the simplex interpolation weights \u03c8(x) are 1\u2212x[0], x[0]\u2212x[1], x[1] on the vertices [0, 0], [1, 0], [1, 1], respectively. The general formula is detailed in Algorithm 2; for more mathematical details see Rovatti et al. (1998).\nAlgorithm 2 Computes the simplex interpolation weights and corresponding vertex indices for a unit lattice cell [0, 1]D and an x \u2208 [0, 1]D. Let the lattice parameters be indexed such that sd = 2\nd is the difference in the indices of the parameters corresponding to any two vertices that are adjacent in the dth dimension, for example, for the 2\u00d7 2 lattice, order the vertices [0 0], [1 0], [0 1], [1 1] and index the corresponding lattice parameters in that order.\nCalculateSimplexInterpolationWeightsAndParameterIndices(x) 1 Compute the sorted order \u03c0 of the components of x such that x[\u03c0[k]] is the kth largest value of x, 2 that is, x[\u03c0[1]] is the largest value of x, etc. 3 Initialize index = 0, indices[] = [index], weights[] = [1] 4 For d = 1 to D: 5 Update index = index + s\u03c0[d] 6 Append index to indices 7 Update weights[d] = weights[d]\u2212 x[\u03c0[d]] 8 Append x[\u03c0[d]] to weights 9 Return indices and weights"}, {"heading": "5.2.3 Simplex Interpolation and Monotonicity", "text": "We show that the same linear inequality constraints that guarantee monotonicity for multilinear interpolation also guarantee monotonicity with simplex interpolation:\nLemma 3 (Monotonic Constraints with Simplex Interpolation) Let f(x) = \u03b8T\u03c6(x) for \u03c6(x) given in Algorithm 2. The partial derivative \u2202f(x)/\u2202x[d] > 0 iff \u03b8k > \u03b8k\u2032 for all k, k\u2032 such that vk[d] = 0, vk\u2032 [d] = 1, and vk[m] = vk\u2032 [m] for all m 6= d.\nProof Note that Algorithm 2 linearly interpolates from D+ 1 vertices at a time, and thus the resulting function is linear over each simplex. Because the parameters are constrained to be increasing, each such linear function is monotonically increasing. Further, f(x) is continuous for all x, because any x on a boundary between simplices only has nonzero interpolation weight on the vertices defining that boundary. In conclusion, the function is piecewise monotonic and continuous, and thus monotonic everywhere."}, {"heading": "5.2.4 Using Simplex Interpolation for Machine Learning", "text": "Simplex interpolation produces a locally linear continuous function made-up of D! hyperplanes defined around the main diagonal axis of the unit hypercube. Compared to multilinear interpolation, simplex interpolation is not as smooth (though continuous), and it is rotationally-dependent.\nFor low-dimensional regression problems using a look-up table with many cells, performance of the two interpolation methods has been found to be similar, particularly if one is using a fine-grained lattice with many cells. For example, in a comparison by Sun and Zhou (2012) for the three-dimensional regression problem of color managing an LCD monitor, multilinear interpolation of a 9 \u00d7 9 \u00d7 9 look-up table (also called trilinear interpolation in the special case of three-dimensions) produced around 1% worse average error than simplex interpolation, but the maximum error with multilinear interpolation was only 60% of the\nmaximum simplex interpolation error. Another study by Kang (1995) using simulations concluded that the interpolation errors of these methods was \u201cabout the same.\u201d\nHowever, when using a coarser lattice like 2D, as we have found useful in practice for machine learning, the rotational dependence of simplex interpolation can cause problems because the flexibility of the interpolated function f(x) differs in different parts of the feature space. Figure 5 illustrates this for a binary classifier on two features.\nTo address the rotational dependence, we recommend using prior knowledge to define the features positively or negatively in a way that aligns the simplices\u2019 shared diagonal axis along the assumed slope of f(x). If there are monotonicity constraints, this is done by specifying each feature so that it is monotonically increasing, rather than monotonically decreasing. For binary classification, features should be specified so that the feature vector for the most prototypical example of the negative class is the all-zeros feature vector, and the feature vector for the most prototypical example of a positive class is the all-ones feature vector. This should put the decision boundary as orthogonal to the shared diagonal axis as possible, providing the interpolated function the most flexibility to model that decision boundary. In addition, for low-dimensional problems, using a finer-grained lattice will produce more flexibility overall, so that the flexibility within each lattice cell is less of an issue.\nFollowing these guidelines, we surprisingly and consistently find that simplex interpolation of 2D lattices is roughly as accurate as multilinear interpolation, and much faster for D \u2265 8. This is demonstrated in the case studies of Section 10 (runtime comparisons given in Section 10.7)."}, {"heading": "6. Regularizing the Lattice Regression To Be More Linear", "text": "We propose a new regularizer that takes advantage of the lattice structure and encourages the fitted function to be more linear by penalizing differences in parallel edges:\nRtorsion(\u03b8) = D\u2211 d=1 D\u2211 d\u0303=1 d\u0303 6=d\n\u2211 r,s,t,u such that\nvr and vs adjacent in dimension d, vt and vu adjacent in dimension d, vr and vt adjacent in dimension d\u0303\n((\u03b8r \u2212 \u03b8s)\u2212 (\u03b8t \u2212 \u03b8u))2. (11)\nThis regularizer penalizes how much the lattice function twists from side-to-side, and hence we refer to this as the torsion regularizer. The larger the weight on the torsion regularizer in the objective function, the more linear the lattice function will be over each 2D lattice cell.\nFigure 6 illustrates the torsion regularizer and compares it to previously proposed lattice regularizers, the standard graph Laplacian (Garcia and Gupta, 2009) and graph Hessian (Garcia et al., 2012). As shown in the figure, for multi-cell lattices, the torsion and graph Hessian regularizers make the function more linear in different ways, and may both be needed to closely approximate a linear function. Like the graph Laplacian and graph Hessian regularizers, the proposed torsion regularizer is convex but not strictly convex, and can be expressed in quadratic form as \u03b8TK\u03b8, where K is a positive semidefinite matrix."}, {"heading": "7. Jointly Learning Feature Calibrations", "text": "One can learn arbitrary bounded functions with a sufficiently fine-grained lattice, but increasing the number of lattice vertices Md for the dth feature multiplicatively grows the total number of parameters M = \u220f dMd. However, we find in practice that if the features are first each transformed appropriately, then many problems require only a 2D lattice to capture the feature interactions. For example, a feature that measures distance might be better specified as log of the distance. Instead of relying on a user to determine how to best transform each feature, we automate this feature pre-processing by augmenting our function class with D one-dimensional transformations cd(x[d]) that we learn jointly with the lattice, as shown in Figure 7."}, {"heading": "7.1 Calibrating Continuous Features", "text": "We calibrate each continuous feature with a one-dimensional monotonic piecewise linear function, as illustrated in Figure 8. Our approach is similar to the work of Howard and Jebara (2007), which jointly learns monotonic piecewise linear one-dimensional transformations and a linear function.\nThis joint estimation makes the objective non-convex, discussed further in Section 9.3. To simplify estimating the parameters, we treat the number of changepoints Cd for the dth feature as a hyperparameter, and fix the Cd changepoint locations (also called knots) at equally-spaced quantiles of the feature values. The changepoint values are then optimized jointly with the lattice parameters, detailed in Section 9.3.\nx 2 RD ...\nx(2) 2 R x(1) 2 R x(3) 2 R\nx(D ) 2 R\nc1(\u00a2)\n...\nc2(\u00a2) c3(\u00a2)\ncD (\u00a2)\nc1(x (1)) 2 R c2(x (2)) 2 R c3(x (3)) 2 R\ncD (x (D )) 2 R\nc(x) 2 RD f (\u00a2) f (c(x)) 2 R\nFigure 7: Block diagram showing one-dimensional calibration functions {cd(\u00b7)} applied to each feature before using a lattice f(\u00b7) to learn feature-interactions."}, {"heading": "7.2 Calibrating Categorical Features", "text": "If the dth feature is categorical, we propose using a calibration function cd(\u00b7) to map each category to a real value in [0,Md \u2212 1]. That is, let the set of possible categories for the dth feature be denoted Gd, then cd : Gd \u2192 [0,Md \u2212 1]. Figure 9 shows an example lattice with a categorical country feature that has been calibrated to lie on [0, 2]. If prior knowledge is given about the ordering of the original discrete values or categories, then partial or full pairwise constraints can be added on the mapped values to respect the known ordering information. These can be expressed as additional sparse linear constraints on pairs of parameters."}, {"heading": "8. Calibrating Missing Data and Using Missing Data Vertices", "text": "We propose two supervised approaches to handle missing values in the training or test set.\nFirst, one can do a supervised imputation of missing data values by calibrating a missing data value for each feature. This is the same approach proposed for calibrating categorical values in Section 7.2: learn the numeric value in [0,Md \u2212 1] to impute if the dth feature is missing that minimizes the structural risk minimization obejctive. In this approach, missing data is handled by a calibration function cd(\u00b7), and like the other calibration function parameters. Other researchers have also considered joint training of classifiers and imputations for missing data, for example van Esbroeck et al. (2014) and Liao et al. (2007).\nSecond, a more flexible option is to give missing data its own missing data vertices in the lattice, as shown in Figure 10. This is similar to a decision tree handling a missing data value by splitting a node on whether that feature is missing. For example, the nonmissing feature values can be scaled to [0,Md \u2212 2], and if the data is missing is it mapped to Md \u2212 1. This increases the number of parameters but gives the model the flexibility to handle missing data differently than non-missing data. For example, missing the street\nnumber in a business description may correlate with lower quality information for all the features.\nTo regularize the lattice parameters corresponding to missing data vertices, we apply the graph regularizers detailed in Section 6. These could be use to tie any of the parameters to the missing data parameters. In our experiments, for the purposes of graph regularization, we treat the missing data vertices as though they were adjacent to the minimum and maximum vertices of that feature in the lattice.\nWith either of these two proposed strategies, linear inequalities can be added on the appropriate parameters (the calibrator parameters in the first proposal, or the missing data\nvertex parameters in the second proposal) to ensure that the function value for missing data is bounded by the minimum and maximum function values, that is, that missing x[d] never produces a smaller f(x) than x[d] = 0, nor a larger f(x) than x[d] = Md."}, {"heading": "9. Large-Scale Training", "text": "For convex loss functions `(\u03b8) and convex regularizers R(\u03b8), any solver for convex problems with linear inequality constraints can be used to optimize the lattice parameters \u03b8 in (8). However, for large n and for even relatively small D, training the proposed calibrated monotonic lattices is challenging due to the number of constraints, the number of terms in the graph-regularizers, and the non-convexity created by using calibration functions.\nIn this section we discuss various standard and new strategies we found useful in practice: our use of stochastic gradient descent (SGD), stochastic handling of the regularizers, parallelizing-and-averaging for distributed training, handling the large number of constraints in the context of SGD, and finally some details on how we optimize the non-convex problem of training the calibrator functions and the lattice parameters. Throughout this section, we assume the standard setting of (8); the generalization to the pairwise ranking problem of (9) is straightforward."}, {"heading": "9.1 SGD and Reducing Variance of the Subgradients", "text": "To scale to a large number of samples n, we used SGD for all our experiments. For each SGD iteration t, a labeled training sample (xi, yi) is sampled uniformly from the set of training sample pairs. One finds the corresponding subgradient of (8), and takes a tiny step in its negative gradient direction. (The resulting parameters may then violate the constraints, which we discuss in Section 9.4.)\nA straightforward SGD implementation for (8) would use the subgradient: \u2206 = \u2207\u03b8` ( \u03b8T\u03c6 (xi) , yi ) +\u2207\u03b8R (\u03b8) , (12)\nwhere the \u2207\u03b8 operator finds an arbitrary subgradient of its argument w.r.t. \u03b8. Ideally, these subgradients should be cheap-to-compute, so each iteration is fast. The computational cost is dominated by computing the regularizer, if using any of the graph regularizers discussed in Section 6.\nBecause the training example (xi, yi) in (12) is randomly sampled, the above subgradient is a realization of a stochastic subgradient whose expectation is equal to the true gradient. The number of iterations needed for the SGD to converge depends on the squared Euclidean norms of the stochastic subgradients (Nemirovski et al., 2009), with larger norms resulting in slower convergence. The expected squared norm of the stochastic subgradient can be decomposed into the sum of two terms: the squared expected subgradient magnitude, and the variance. We can do little about the expected magnitude, but we can improve the tradeoff between the computational cost of each subgradient and the variance of the stochastic subgradients. In the next two sub-sections, we describe two such strategies."}, {"heading": "9.1.1 Mini-Batching", "text": "We reduce the variance of the stochastic subgradient\u2019s loss term by mini-batching over multiple random samples (Dekel et al., 2012). Let S` denote a set of k` training indices sampled uniformly with replacement from 1, . . . , n, then the mini-batched subgradient is:\n\u2206 = 1\nk` \u2211 i\u2208S` \u2207\u03b8` ( \u03b8T\u03c6 (xi) , yi ) +\u2207\u03b8R (\u03b8) . (13)\nThis simultaneously reduces the variance and increases the computational cost of the loss term by a factor of k`. For sufficiently small k`, this is a net win because differentiating the regularizer is the dominant computational term."}, {"heading": "9.1.2 Stochastic Subgradients for Regularizers", "text": "We propose to reduce the computational cost of each SGD iteration by randomly sampling the additive terms of the regularizer, for regularizers that can be expressed as a sum of terms: R(\u03b8) = \u2211m j=1 rj(\u03b8). For example, for a 2 D lattice, each calculation of the graph Laplacian regularizer subgradient sums over m = D2D\u22121 terms, and the graph torsion regularizer subgradient sums over m = D(D \u2212 1)2D\u22123 terms.\nLet SR denote a set of kR indices sampled uniformly with replacement from 1, l . . . ,m, then define the subgradient:\n\u2206 = 1\nk` \u2211 i\u2208S` \u2207\u03b8` ( \u03b8T\u03c6 (Xi) , Yi ) + m kR \u2211 j\u2208SR \u2207\u03b8rj (\u03b8) . (14)\nWhile this makes the subgradient\u2019s regularizer term stochastic, and hence increases the subgradient variance, we find that good choices of k` and kR in (14) can produce a useful tradeoff between the computational cost of computing each subgradient and the number of SGD iterations needed for acceptable converge. For example, in one real-world application using torsion regularization, the choice of kR = 1024 and k` = 1 led to a 150\u00d7 speed-up in training and produced statistically indistinguishable accuracy on a held-out test set."}, {"heading": "9.2 Parallelizing and Averaging", "text": "For a large number of training samples n, one can split the n training samples into K sets, then independently and in-parallel train a lattice on each of the K sets. Once trained, the vector lattice parameters for the K lattices can simply be averaged. This parallelize-andaverage approach was investigated for large-scale training of linear models by Mann et al. (2009). Their results showed similar accuracies to distributed gradient descent, but 1000\u00d7 less network traffic and reduced wall-clock time for large datasets. In our implementation of the parallelize-and-average approach we do multiple syncs: averaging the lattices, then sending out the averaged lattice to parallelized workers to keep improving with further training. We illustrate the performance and speed-up of this simple parallelize-and-average for learning monotonic lattices in Section 10.6 and Section 10.7. A more complicated implementation of this strategy would use the alternating direction method of multipliers with a consensus constraint (Boyd et al., 2010), but that requires an additional regularization towards a local copy of the most recent consensus parameters."}, {"heading": "9.3 Jointly Optimizing Lattice and Calibration Functions", "text": "To learn a calibrated monotonic lattice, we jointly optimize the calibration functions and the lattice parameters. Let x denote a feature vector with D components, each of which is either a continuous or categorical value (discrete features can be modeled either as continuous features or categorical as the user sees fit). Let cd(x[d];\u03b1\n(d)) be a calibration function that acts on the dth component of x and has parameters \u03b1(d).\nIf the dth feature is continuous, we assume it has a bounded domain such that x[d] \u2208 [ld, ud] for finite ld, ud \u2208 R. Then the dth calibration function cd(x[d];\u03b1(d)) is a monotonic piecewise linear transform with fixed knots at ld, ud, and the Cd\u22122 equally-spaced quantiles of dth feature over the training set. Let the first and last knots of the piecewise linear function map to the lattice bounds 0 and Md \u2212 1 respectively (as shown in Figure 8), that is, if Cd = 2 then cd(x[d];\u03b1\n(d)) simply linearly scales the raw range [ld, ud] to the lattice domain [0,Md \u2212 1] and there are no parameters \u03b1(d). For Cd > 2, the parameters \u03b1(d) \u2208 [0,Md \u2212 1]Cd\u22122 are the Cd \u2212 2 output values of the piecewise linear function for the middle Cd \u2212 2 knots.\nIf the dth feature is categorical with finite category set Gd such that x[d] \u2208 Gd, then the dth calibration function maps the categories to the lattice span such that cd(x[d];\u03b1\n(d)) : Gd \u2192 [0,Md\u22121] and the parameters are the |Gd| categorical mappings such that cd(x[d];\u03b1(d)) = \u03b1(d)[k] if x[d] belongs to category k and \u03b1(d) \u2208 [0,Md \u2212 1]|Gd|.\nLet c(x;\u03b1) denote the vector function with dth component function cd(x[d];\u03b1 (d)), and note c(x;\u03b1) maps a feature vector x to the domain M of the lattice function. Use ed to denote the standard unit basis vector that is one for the dth component and zero elsewhere with length D, then one can write:\nc(x;\u03b1) = D\u2211 d=1 edcd(e T d x;\u03b1 (d)), (15)\nThen the proposed calibrated monotonic lattice regression objective expands the monotonic lattice regression objective (8) to:\narg min \u03b8,\u03b1 n\u2211 i=1 `(yi, \u03b8 T\u03c6(c(xi, \u03b1)) +R(\u03b8) s.t. A\u03b8 \u2264 b and A\u0303\u03b1 \u2264 b\u0303,\nwhere each row of A specifies a monotonicity constraint for a pair of adjacent lattice parameters (as before), and each row of A\u0303 similarly specifies a monotonicity constraint for a pair of adjacent calibration parameters for one of the piecewise linear calibration functions.\nThis turns the convex optimization problem (8) into a non-convex problem that is marginally convex in the lattice parameters \u03b8 for fixed \u03b1, but not necessarily convex with respect to \u03b1 even if \u03b8 is fixed. Despite the non-convexity of the objective, in our experiments we found sensible and effective solutions by using projected SGD, updating \u03b8 and \u03b1 with the appropriate stochastic subgradient for each xi. Calculate the subgradient w.r.t. \u03b8 holding \u03b1 constant, essentially the same as before. Calculate the subgradient w.r.t \u03b1 by holding \u03b8 constant and using the chain rule:\n\u2202\u03b8T\u03c6(c(xi, \u03b1)) \u2202\u03b1(d) = \u2202\u03b8T\u03c6(c(xi, \u03b1)) \u2202c(xi, \u03b1) \u2202c(xi, \u03b1) \u2202\u03b1(d) . (16)\nIf the dth feature is categorical, the partial derivative is 1 for the calibration mapping parameter corresponding to the category of xi[d] and zero otherwise:\n\u2202c(xi, \u03b1)\n\u2202\u03b1(d)[k] = 1 if xi[d] is the kth category and 0 otherwise. (17)\nIf the dth feature is continuous, then the parameters \u03b1(j)[d] are the values of the calibration function at the knots of the piecewise linear function. If xi[d] lies between the kth and (k + 1)th knots at (fixed) positions \u03b2k and \u03b2k+1, then\n\u2202c(xi, \u03b1)\n\u2202\u03b1(d)[k] = (\u03b2k+1 \u2212 xi[d]) (\u03b2k+1 \u2212 \u03b2k)\n\u2202c(xi, \u03b1)\n\u2202\u03b1(d)[k + 1] = (xi[d]\u2212 \u03b2k) (\u03b2k+1 \u2212 \u03b2k) ,\nand the partial derviative is zero for all other components of \u03b1(d). After taking an SGD step that updates \u03b1(d)[k] and \u03b1(d)[k+ 1], the \u03b1(d) may violate the monotonicity constraints that ensure a monotonic calibration function, which can be fixed with a projection onto the constraints (see Section 9.4 for details).\nA standard strategy with nonconvex gradient descent is to try multiple random initializations of the parameters. We did not explore this avenue; instead we simply try to initialize sensibly. Each lattice parameter is initialized to be the sum of its monotonically increasing components (multiply by -1 for any monotonically decreasing components) so that the lattice initialization respects the monotonicity constraints and is a linear function. The piecewise linear calibration functions are initialized to scale linearly to [0,Md\u22121]. The categorical calibration parameters are ordered by their mean label, then spaced uniformly on [0,Md \u2212 1] in that order."}, {"heading": "9.4 Large-Scale Projection Handling", "text": "Standard projected stochastic gradient descent projects the parameters onto the constraints after each stochastic gradient update. Given the extremely large number of linear inequality constraints needed to enforce monotonicity for even small D, we found a full projection each iteration impractical and un-necessary. We avoid the full projection each iterate by using one of two strategies."}, {"heading": "9.4.1 Suboptimal Projections", "text": "We found that modifying the SGD update to approximate the projection worked well. Specifically, for each new stochastic subgradient \u03b7\u2206, we create a set of active constraints initialized to \u2205, and, starting from the last parameter values, move along the portion of \u03b7\u2206 that is orthogonal to the current active set until we encounter a constraint, add this constraint to the active set, and then continue until the update \u03b7\u2206 is exhausted or it is not possible to move orthogonal to the current active set. At all times, the parameters satisfy the constraints. It can be particularly fast because it is possible to exploit the sparsity of the monotonicity constraints (each of which depends on only two parameters) and the sparsity of \u2206 (when using simplex interpolation) to optimize the implementation.\nBut, this strategy is sub-optimal because we do not remove any constraints from the active set during each iteration, and thus parameters can \u201cget stuck\u201d at a corner of the feasible set, as illustrated in Figure 11. In practice, we found such problems resolve themselves because the stochasticity of the subsequent stochastic gradients eventually jiggles the parameters free. Experimentally, we found this suboptimal strategy to be very effective and to produce statistically similar objective function values and test accuracies more optimal approaches. All of the experimental results reported in this paper used this strategy. See Section 10.7 for example runtimes."}, {"heading": "9.4.2 Stochastic Constraints with LightTouch", "text": "An optimal approach we compared with for handling large-scale constraints is called LightTouch (Cotter et al., 2015). At each iteration, LightTouch does not project onto any constraints, but rather moves the constraints into the objective, and applies a random subset of constraints each iteration as stochastic gradient updates to the parameters, where the distribution over the constraints is learned as the optimization proceeds to focus on constraints that are more likely to be active. This replaces the per-iteration projections with cheap gradient updates. Intermediate solutions may not satisfy all the constraints, but one full projection is performed at the very end to ensure final satisfaction of the constraints. Experimentally, we found LightTouch to generally lead to faster convergence (see Cotter et al. (2015) for its theoretical convergence rate), while producing similar experimental results to the above approximate projected SGD. LightTouch does require a more complicated implementation to effectively learn the distribution over the constraints."}, {"heading": "9.4.3 Adapting Stepsizes with Adagrad", "text": "One can generally improve the speed of SGD with adagrad (Duchi et al., 2011), even for nonconvex problems (Gupta et al., 2014). Adagrad decays the step-size adaptively for each\nparameter, so that parameters updated more often or with larger magnitude gradients have a smaller step size. We found adagrad did speed up convergence slightly, but required more complicated implementation to correctly handle the constraints, as the projections must be with respect to the adagrad norm rather than the Euclidean norm. We experimented with\napproximating the adagrad norm projection with the Euclidean projection, but found this approximation resulted in poor convergence."}, {"heading": "10. Case Studies", "text": "We present a series of experimental case studies on real world problems to demonstrate different aspects of the proposed methods, followed by some example runtimes for interpolation and training in Section 10.7.\nPrevious datasets used to evaluate monotonic algorithms have been small, both in the number of samples and the number of dimensions, as detailed in Table 1. In order to produce statistically significant experimental results, and to better demonstrate the practical need for monotonicity constraints, we use real-world case studies with relatively large datasets, and for which the application engineers have confirmed that they expect or want the learned function to be monotonic with respect to some subset of features. The datasets used are detailed in Table 3, and include datasets with eight thousand to 400 million samples, and nine to sixteen features, most of which are constrained to be monotonic.\nThe case studies demonstrate that for problems where the monotonicity assumption is warranted, the proposed calibrated monotonic lattice regression produces similar accuracy to random forests. Random forests is an unconstrained method that consistently provides competitive results on benchmark datasets, compared to many other types of machine learning methods (Fernandez-Delgado et al., 2014)).\nBecause any bounded function can be expressed using a sufficiently fine-grained interpolation look-up table, we expect that with appropriate use of regularizers, monotonic lattice regression will perform similarly to other guaranteed monotonic methods that use a flexible function class and are appropriately regularized, such as monotonic neural nets (see 2.2.5). However, of guaranteed monotonic methods, the only monotonic strategy that has been demonstrated to scale to the number of training samples and the number of features treated in our case studies is linear regression with non-negative coefficients (see Table 1)."}, {"heading": "10.1 General Experimental Details", "text": "We used ten-fold cross-validation on each training set to choose hyperparameters, including: whether to use graph Laplacian regularization or torsion regularization, how much regularization (in powers of ten), whether to calibrate missing data or use a missing data vertex, the number of change-points if feature calibration was used from the choices: {2, 3, 5, 10, 20, 50}, and the number of vertices for each feature was started at 2 and increased by 1 as long as cross-validation accuracy increased. The step size was tuned using ten-fold cross-validation and choices were powers of 10; it was usually chosen to be one of {.01, .1, 1}. If calibration functions were used, a hyperparameter was used to scale the step size for the calibration function gradients compared to the lattice function gradients; this calibration step size scale was also chosen using ten-fold cross-validation and powers of 10, and was usually chosen to be one of {.01, .1, 1, 10}. Multilinear interpolation was used unless it is noted that simplex interpolation was used. The loss function was squared error, unless noted that logistic loss was used.\nComparisons were made to random forests (Breiman, 2001), and to linear models, with either the logistic loss (logistic regression) or squared error loss (linear regression), and a\nridge regularizer on the linear coefficients, with any categorical or missing features converted to Boolean features. All comparisons were trained on the same training set, hyperparameters were tuned using cross-validation, and tested on the same test set. Statistical significance was measured using a binomial statistical significance test with a p-value of .05 on the test samples rated differently by two models."}, {"heading": "10.2 Case Study: Business Entity Resolution", "text": "In this case study, we compare the relative impact of several of our proposed extensions to lattice regression. The business entity resolution problem is to determine if two business descriptions refer to the same real-world business. This problem is also treated by Dalvi et al. (2014), where they focus on defining a good title similarity. Here, we consider only the problem of fusing different similarities (such as a title similarity and phone similarity) into one score that predicts whether a pair of businesses are the same business. The learned function is required to be monotonically increasing in seven attribute similarities, such as the similarity between the two business titles and the similarity between the street names. There are two other features with no monotonicity constraints, such as the geographic region, which takes on one of 14 categorical values. Each sample is derived from a pair of business descriptions, and a label provided by an expert human rater indicating whether that pair of business descriptions describe the same real-world business. We measure accuracy in terms of whether a predicted label matches the ground truth label, but in actual usage, the learned function is also used to rank multiple matches that pass the decision threshold, and thus a strictly monotonic function is preferred to a piecewise constant function. The training and test sets, detailed in Table 3, were randomly split from the complete labeled set. Most of the samples were drawn using active sampling, so most of the samples are difficult to classify correctly.\nTable 4 reports results. The linear model performed poorly, because there are many important high-order interactions between the features. For example, the pair of businesses might describe two pizza places at the same location, one of which recently closed, and the other recently opened. In this case, location-based features will be strongly positive, but the classifier must be sensitive to low title similarity to determine the businesses are different. On the other hand, high title similarity is not sufficient to classify the pair as the same, for example, two Starbucks cafes across the street from each other in downtown London.\nThe lattice regression model was first optimized using cross-validation, and then we made the series of minor changes (with all else held constant) listed in Table 4 to illustrate the impact of these changes on accuracy. First, removing the monotonicity constraints resulted in a statistically significant drop in accuracy of half a percent. Thus it appears the monotonicity constraints are successfully regularizing given the small amount of training data and the known high Bayes error in some parts of the feature space. Lattice regression without the monotonicity constraints performed similarly to random forests (and not statistically significantly better), as expected due to the similar modeling abilities of the methods.\nThe cross-validated lattice was 3 \u00d7 3 \u00d7 3 \u00d7 26, where the first three features used a missing data vertex (so the non-missing data is interpolated from a 29 lattice). Calibrating the missing values for those three features instead of using missing data vertices statistically significantly dropped the accuracy from 81.9% to 80.7%. (However, if one subsamples the training set down to 3000 samples, then the less flexible option of calibrating the missing values works better than using missing data vertices.)\nThe cross-validated calibration used five changepoints for two of the four continuous features, and no calibration for the two other continuous features. Figure 8 shows the calibrations learned in the optimized lattice regression. Removing the continuous signal calibration resulted in a statistically significant drop in accuracy.\nAnother important proposal of this paper is calibrating categorical features to realvalued features. For this problem, this is applied to a feature specifying which of 14 possible geographical categories the businesses are in. Removing this geographic feature statistically significantly reduced the accuracy by half a percent.\nThe amount of torsion regularization was cross-validated to be 10\u22124. Changing to graph Laplacian and re-optimizing the amount of regularization decreased accuracy slightly, but not statistically significantly so. This is consistent with what we often find: torsion is often slightly better, but often not statistically significantly so, than the graph Laplacian regularizer.\nChanging the multilinear interpolation to simplex interpolation (see Section 5.2) dropped the accuracy slightly, but not statistically significantly. For some problems we even see simplex interpolation provide slightly better results, but generally the accuracy difference between simplex and multilinear interpolation is negligible."}, {"heading": "10.3 Case Study: Scoring Ad\u2013Query Pairs", "text": "In this case study, we demonstrate the potential of the calibration functions. The goal is to score how well an ad matches a web search query, based on five different features that each measure a different notion of a good match. The score is required to be monotonic with respect to all five features. The labels are binary, so this is trained and tested as a classification problem. The train and test sets were independently and identically distributed, and are detailed in Table 3.\nResults are shown in Table 5. The cross-validated lattice size was 2\u00d7 2\u00d7 2\u00d7 2\u00d7 2, and the calibration functions each used 5 changepoints. Removing the calibration functions and re-cross-validating the lattice size resulted in a larger lattice sized 4\u00d74\u00d74\u00d74\u00d74, and slightly worse (but not statistically significantly worse) accuracy. In total, the uncalibrated lattice\nmodel used 1024 parameters, whereas the calibrated lattice model used only 57 parameters. We hypothesize that the smaller calibrated lattice will be more robust to feature noise and drift in the test sample distribution than the larger uncalibrated lattice model. In general, we find that the one-dimensional calibration functions are a very efficient way to capture the flexibility needed, and that in conjunction with good one-dimensional calibrations, only coarse-grained (e.g. 2D) lattices are needed.\nBoth with and without calibration functions, the lattice regression models were statistically significantly better than the linear model. The random forest performed well, but was not statistically significantly better than the lattice regression.\nA boosted stumps model was also trained for this problem. See Fig. 12 for a comparison of two-dimensional slices of the boosted stumps and lattice functions. The boosted stumps\u2019 test set accuracy was relatively low at 75.4%. In practice, the goal of this problem is to have a score useful for ranking candidates as well as determining if they are a sufficiently good match. Even with many trees, this model produces many ties due its piecewise-constant surface. In addition, the live experiments with the boosted stumps showed that the output was problematically sensitive to feature noise, which would cause samples near the boundary of two piecewise constant surfaces to experience fluctuating scores."}, {"heading": "10.4 Case Study: Rendering Classifier", "text": "This case study demonstrates training a flexible function (using a lattice) that is monotonic with respect to fifteen features. The goal is to score whether a particular display element should be rendered on a webpage. The score is required to be monotonic in fifteen of the\nfeatures, and there is a sixteenth Boolean feature that is not constrained. The training and test sets (detailed in Table 3) consisted almost entirely of samples known to be difficult to correctly classify (hence the rather low accuracies).\nWe used a fixed 216 lattice size, a fixed 5 changepoints per feature for the six continuous signals (the other ten signals were Boolean), and no graph regularization, so no hyperparameters were optimized for this case study. Simplex interpolation was used for speed. A single training loop through the 20,000 training samples took around five minutes on a Xeon-type Intel desktop using a single-threaded C++ implementation with sparse vectors, with the training time dominated by the constraint handling. Training in total took around five hours.\nResults in Table 6 show substantial gains over the linear model, while still producing a monotonic, smooth function. The lattice regression was also statistically significantly better than random forests, we hypothesize due to the regularization provided by the monotonicity constraints which is important in this case due to the difficulty of the problem on the given examples and the relatively small number of training samples."}, {"heading": "10.5 Case Study: Fusing Pipelines", "text": "While this paper focuses on learning monotonic functions, we believe it is also the first paper to propose applying lattice regression to classification problems, rather than only regression problems. With that in mind, we include this case study demonstrating that lattice regression without constraints also performs similarly to random forests on a realworld large-scale multi-class problem.\nThe goal in this case study is to fuse the predictions from two pipelines, each of which makes a prediction about the likelihood of seven user categories based on a different set of high-dimensional features. Because each pipeline\u2019s probability estimates sum to one, only the first six probability estimates from each pipeline are needed as features to the fusion, for a total of twelve features. The training and test set were split by time, with the older 1.6 million samples used for training, and the newest 390,000 samples used as a test set.\nThe lattice was trained with a multi-class logistic loss, and used simplex interpolation for speed. The cross-validated model was a 212 lattice for six of the output classes (with the probability of the seventh class being subtracted from one) and no calibration functions, resulting in a total of 212 \u00d7 6 = 24, 576 parameters.\nThe results are reported in Table 7. Even though Pipeline 2 alone is 6.5% more accurate than Pipeline 1 alone, the test set accuracy can be increased by fusing the estimates from both pipelines, with a small improvement in accuracy by lattice regression over the random forest classifier, logistic regression, or simply averaging the two pipeline estimates."}, {"heading": "10.6 Case Study: Video Ranking and Large-Scale Learning", "text": "This case study demonstrates large-scale training of a large monotonic lattice and learning from ranked pairs. The goal is to learn a function to rank videos a user might like to watch, based on the video they have just watched. Experiments were performed on anonymized data from YouTube.\nEach feature vector xi is a vector of features about a pair of videos, xi = h(vj , vk), where vj is the watched video, vk is a candidate video to watch next, and h is a function that takes a pair of videos and outputs a twelve-dimensional feature vector xi. For example, a feature might be the number of times that video vj and video vk were watched in the same session.\nEach of the twelve features was specified to be positively correlated with users viewing preference, and thus we constrained the model to be monotonically increasing with respect\nto each. Of course, human preference is complicated and these monotonicity constraints cannot fully model human judgement. For example, knowing that a video that has been watched many times is generally a very good indicator that it is good to suggest, and yet a very popular video at some point will flare out and become less popular.\nMonotonicity constraints can also be useful to enforce secondary objectives. For example, all other features equal, one might prefer to serve fresher videos. While users in the long-run want to see fresh videos, they may preferentially click on familiar videos, thus click data may not capture this desire. This secondary goal can be enforced by constraining the learned function to be monotonic in a feature that measures video freshness. This achieves a multi-objective function without overly-complicating or distorting the training label definition.\nThere are billions of videos in YouTube, and thus many many pairs of watched-andcandidate videos to score and re-score as the underlying feature values change over time. Thus it is important the learned ranking functions to be cheap to evaluate, and so we use simplex interpolation for its evaluation speed; see Section 10.7 for comparison of evaluation speeds.\nWe trained to minimize the ranked pairs objective from (9), such that the learned function f is trained for the goal of minimizing pairwise ranking errors,\nf(h(vj , v + k )) > f(h(vj , v \u2212 k )),\nfor each training event consisting of a watched video vj , and a pair of candidate videos v + k and v\u2212k where there is information that a user who has just watched video vj prefers to watch v+k next over v \u2212 k ."}, {"heading": "10.6.1 Which Pairs of Candidate Videos?", "text": "A key question is which sample pairs of candidate videos v+k and v \u2212 k should be used as the preferred and unpreferred videos for a given watched video vj . We used anonymized click data from YouTube\u2019s current video-suggestion system. For each watched video vj , if a user clicked a suggested video in the second position or below, then we took the clicked video as the preferred video v+k , and the video suggested right above the clicked video as the unpreferred video v\u2212j . We call this choice of v + k and v \u2212 k a bottom-clicked pair. This choice is consistent with the findings of Joachims et al. (2005), whose eye-tracking experiments on webpage search results showed that users on average look at least at one result above the clicked result, and that these pairs of preferred/unpreferred samples correlated strongly with explicit relevance judgements. Also, using bottom-clicked pairs removes the trust bias that users know they are being presented with a ranked list and prefer samples that are rankedhigher (Joachims et al., 2005). In a set of preliminary experiments, we also tried training using either a randomly sampled video as v\u2212k , or the video just after the clicked video, and then tested on bottom-clicked pairs. Those results showed test accuracy on bottom-clicked pairs was up to 1% more accurate if the training set only included the bottom-clicked pairs, even though that meant fewer training pairs.\nAn additional goal (and one that is common in commercial large-scale machine learning systems for various practical reasons) is for the learned ranking function to be as similar to the current ranking function as possible. That is, we wish to minimize changes to the\ncurrent scoring if they do not improve accuracy; such accuracy-neutral changes are referred to as churn. To reduce churn, we added in additional pairs that reflect the decisions of the current ranking function. Each of these pairs also takes the clicked video as the preferred v+k , but sets the unpreferred video v \u2212 k to be the video that the current system ranked ten candidates lower than the clicked video. The dataset is a 50-50 mix of these churn-reducing pairs and bottom-clicked pairs."}, {"heading": "10.6.2 More Experimental Details", "text": "The dataset was randomly split into mutually exclusive training, test, and validation sets of size 400 million, 25 million, and 25 million pairs, respectively. To ensure privacy, the dataset only contained the feature vector, and no information identifying the video or user. The disadvantage of that is the train, test and validation sets are likely to have some samples from the same videos and same users. However, in total the datasets capture millions of unique users and unique watched videos.\nWe used a fixed 312 lattice, for a total of 531,441 parameters. The pre-processing functions were fixed in this case, so no calibration functions were learned. We compared training on increasingly-larger randomly-sampled subsets of the 400 million training set (see Figure 13 for training set sizes). We compared training on a single worker to the parallelizeand-average strategy explained in Section 9.2. Parallel results were parallelized over 100 workers. The stepsize was chosen independently for each training set based on accuracy on the validation set.\nWe report results with and without monotonicity constraints. For the unconstrained results, each training (single or parallel) touched each sample in the training set once. For the monotonic results (single or parallelized), each sample was touched ten times, and minibatching was used with a minibatch size of 32 stochastic gradients. Logistic loss was used."}, {"heading": "10.6.3 Results", "text": "Figure 13 compares test set accuracy for single and parallelized training for different amounts of training data, with and without monotonicity constraints. For each dataset, the single and parallel training saw the same total number of training samples and were allowed the same total number of stochastic gradient updates.\nOn the click data test set, not using monotonicity constraints (the dark lines) is about .5% better at pairwise accuracy than if we constrain the function to be monotonic. However, in live experiments that required ranking all videos (not only ones that had been top-ranked in the past, and hence possibly clicked on), models trained with monotonicity constraints showed better performance on the actual measures of user-engagement (as opposed to the training metric of pairwise accuracy). This discrepancy appears to be due to the biased sampling of the click data we train (and test on offline), as the click-data has a biased distribution over the feature space compared to the distribution of all videos which must get ranked in reality. The biased distribution of the click data appears to cause parameters in sparser regions of the feature space to be non-monotonic in an effort to increase the flexibility (and accuracy) of the function in the denser regions, thus increasing the accuracy on the click data. Enforcing monotonicity helps address this sampling bias problem by not\nallowing the training to ignore the accuracy in sparser regions that are important in practice to accurately rank all videos.\nEven though there are 500k parameters to train, the click-data accuracy is already very good with only 500k training samples, and test accuracy increases only slightly when trained on 400 million samples compared to 10 million samples. This is largely because the clickdata samples are densely clustered in the feature space, and with simplex interpolation, only a small fraction of the 500k parameters control the function over the dense part of the feature space.\nThe darker lines of Figure 13 show the parallelization versus single-machine results without monotonicity constraints. Unconstrained, the parallelized runs appear to perform slightly better to the single-machine training given the same number of training samples (and the same total number of gradient updates). We hypothesize this slight improvement is due to some noise-averaging across the 100 parallelized trained lattices.\nThe lighter lines of Figure 13 show the parallelization versus single-machine results with monotonicity constraints. Trained on 500k pairs, the parallelized training and singlemachine monotonic training produce the same test accuracy. However, as the training set size increases, the parallelized training takes more data to achieve the same accuracy as the single-machine training. We believe this is because averaging the 100 monotonic lattices is a convex combination of lattices likely on the edge of the monotonicity constraint set, producing an average lattice in the interior of the constraint set, that is, the averaged lattice is over-constrained."}, {"heading": "10.7 Run Times", "text": "We give some timing examples for the different interpolations and for training.\nFigure 14 shows average evaluation times for multilinear and simplex interpolation of one sample from a 2D lattice for D = 4 to D = 20 using a single-threaded 3.5GHz Intel Ivy Bridge processor. Note the multilinear evaluation times are reported on a log-scale, and on a log scale the evaluation time increases roughly linearly in D, matching the theoretical O(2D) complexity given in Section 5.1. The simplex evaluation times scale roughly linearly with D, consistent with the theoretical O(D logD) complexity. For D = 6 features, simplex interpolation is already three times faster than multilinear. With D = 20 features, the simplex interpolation is still only 750 nanoseconds, but the multilinear interpolation is about 15, 000 times slower, at around 12 milliseconds.\nTraining times are difficult to report in an accurate or meaningful way due to the high-variance of running on a large, shared, distributed cluster. Here is one example: with every feature constrained to be monotonic, a single worker training one loop of a 212 lattice on 4 million samples usually takes around 15 minutes, whereas with 100 parallelized workers one loop through 400 million samples (4 million samples for each worker) usually takes around 20 minutes. Large step-sizes can take much longer than smaller stepsizes, because larger updates tend to violate more monotonicity constraints and thus require more expensive projections. Minibatching is particularly effective at speeding up training because the averaged batch of stochastic gradients reduces the number of monotonicity violations and the need for projections. Without monotonicity constraints, training is generally 10\u00d7 to 1000\u00d7 faster, depending on how non-monotonic the data is."}, {"heading": "11. Discussion and Some Open Questions", "text": "We have proposed an approach to effectively learn flexible, monotonic functions for lowdimensional machine learning problems of classification, ranking, and regression. We ad-\ndressed a number of practical issues, including interpretability, evaluation speed, automated pre-processing of features, missing data, and categorical features. Experimental results show statistically significant state-of-the-art performance on the largest training sets and largest number of features published for monotonic methods.\nPractical experience has shown us that being able to check and ensure monotonicity helps users trust the model, and leads to models that generalize better. For us, the monotonicity constraints have come from engineers who believe the output should be monotonic in the feature. In the absence of clear prior information about monotonicity, it may be tempting to use the direction of a linear fit to specify a monotonic direction and then use monotonicity as a regularizer. Magdon-Ismail and Sill (2008) point out that using the linear regression coefficients for this purpose can be misleading if features are correlated and not jointly Gaussian.\nFor classifiers, requiring the function to be monotonic is a stronger requirement than needed to guarantee the decision boundary is monotonic. We have seen in practice that this can occur: one trains an unconstrained lattice, find that it is non-monotonic, but that the thresholded function g(x) = If(x)>0 is monotonic. It is an open question how this could be enforced and whether it would be useful.\nOne surprise was that for practical machine learning problems like those of Section 10, we find a simple 2D lattice is sufficient to capture the interactions of D features, especially if we jointly optimize D one-dimensional feature calibration functions. When we began this work, we expected to have to use much more fine-grained lattices with many vertices in each feature, or perhaps irregular lattices to achieve state-of-the-art accuracy. In fact, calibration functions can effectively linearize the data with respect to the label, making a 2D lattice sufficiently flexible for most of the problems we have encountered.\nFor some cases, a 2D lattice is too flexible. We reduced lattice flexibility with new regularizers: monotonicity, and the torsion regularizer that encourages a more linear model. While good for interpretability and accuracy, these regularization strategies do not reduce the model size.\nFor a large number of features D, the exponential model size of a 2D lattice is a memory issue. On a single machine, training and evaluating with a few million parameters is viable, but this still limits the approach to not much more than D = 20 features. An open question is how such large models could be sparsified, and if useful sparsification approaches could also provide additional useful regularization.\nA second surprise was that simplex interpolation provides similar accuracy to multilinear interpolation. The rotational dependence of simplex interpolation seemed at first troubling, but the proposed approach of aligning the shared axis of the simplices with the main increasing axis of the function appears to solve this problem in practice. The geometry of the simplices at first seemed odd in that it produces a locally linear surface over elongated simplices. However, this partitioning turns out to work well because it provides a very flexible piecewise linear decision boundary. Lastly, we found that the theoretical O(D logD) computational complexity does result in orders of magnitude faster interpolation than multilinear interpolation as D increases.\nA common practical issue in machine learning is handling categorical data. We proposed to learn a mapping from mutually exclusive categories to feature values, jointly with the other model parameters. We found categorical-mapping to be interpretable, flexible, and\naccurate. The proposed categorical mapping can be viewed as learning a one-dimensional embedding of the categories. Though we generally only needed two vertices in the lattice for continuous features, for categorical features we often find it helpful to use more vertices (a finer-grained lattice) for more flexibility. Some preliminary experiments learning twodimensional embeddings of categories (that is, mapping one category to [0, 1]2) showed promise, but we found this required more careful initialization and handling of the increased non-convexity.\nLearning the monotonic lattice is a convex problem, but composing the lattice and the one-dimensional calibration functions creates a non-convex objective. We used only one initialization of the lattice and calibrators for all our experiments, but tuned the stepsize of the stochastic gradient descent separately for the set of lattice parameters and the set of calibration parameters. In some cases we saw a substantial sensitivity of the accuracy to the initial SGD stepsizes. We hypothesize that this is caused by some interplay of the relative stepsizes and the relative size of the local optima.\nWe employed a number of strategies to speed up training. One of the biggest speed-ups comes from randomly sampling the additive terms of the graph regularizers, analogous to the random sampling of the additive terms of the empirical loss that SGD uses. We showed that a parallelize-and-average strategy works for training the lattices. The largest computational bottleneck remains the projections onto the monotonicity constraints. Mini-batching the samples reduces the number of projections and provides speed-ups, but a faster approach to optimization given possibly hundreds of thousands of constraints would be valuable.\nWhile we focused on constraints that impose monotonicity, imposing other constraints on the lattice may be useful. For example, one could learn a submodular function from noisy samples by imposing submodularity constraints on the parameters. Or, to guarantee that change from a prior model is not too large (for churn reduction, or consistency), one can constrain the lattice parameters to be within a fixed margin of the prior model\u2019s prediction for the corresponding vertex. If more than three (or more) vertices are used for a feature, then one can enforce Brooks\u2019 Law (Brooks, 1975), in which the function is constrained to first increase but then decrease as one input is increased, all other inputs held constant. Brooks\u2019 Law is most famous for modeling the productivity of software teams as programmers are added - Brooks argued that at some point adding more programmers can make the project slower not faster, due to increased communication overhead and difficulties of achieving consensus on decisions, a phenomenon referred to as the the mythical man-month. In fact, Brooks\u2019 Law may be useful for modeling a breadth of real-world relationships. For example, an extraordinarily large number of watches of a video suggests that everyone has already seen it, and that it may not be as good a recommendation as a video with slightly fewer watches. Or if predicting a used car\u2019s value, one expects the value to decrease with the car\u2019s age up to a certain point, after which it becomes a classic car and (all else held constant), its value increases with age. Combined with the proposed calibration functions that determine how to map raw values to the constrained lattice, it should be possible to learn effective functions with such complicated constraints for real-world problems."}, {"heading": "12. Acknowledgments", "text": "We thank Sugato Basu, David Cardoze, James Chen, Emmanuel Christophe, Brendan Collins, Mahdi Milani Fard, James Muller, Biswanath Panda, and Alex Vodomerov for help with experiments and helpful discussions."}], "title": "Monotonic Calibrated Interpolated Look-Up Tables", "year": 2020}