{"abstractText": "Black-box nature hinders the deployment of many high-accuracy models in medical diagnosis. It is risky to put one\u2019s life in the hands of models that medical researchers do not trust. However, to understand the mechanism of a new virus, such as COVID-19, machine learning models may catch important symptoms that medical practitioners do not notice due to the surge of infected patients during a pandemic. In this work, the interpretation of machine learning models reveals that a high C-reactive protein (CRP) corresponds to severe infection, and severe patients usually go through a cardiac injury, which is consistent with well-established medical knowledge. Additionally, through the interpretation of machine learning models, we find phlegm and diarrhea are two important symptoms, without which indicate a high risk of turning severe. These two symptoms are not recognized at the early stage of the outbreak, whereas our findings are corroborated by later autopsies of COVID-19 patients. We find patients with a high N-terminal pro B-type natriuretic peptide (NTproBNP) have a significantly increased risk of death which does not receive much attention initially but proves true by the following-up study. Thus, we suggest interpreting machine learning models can offer help to diagnosis at the early stage of an outbreak. Impact Statement\u2014We try to answer the question, how can medical practitioners cooperate with machine learning to win the race in this pandemic? Instead of targeting at a blackbox high-accuracy model that is hard to trust and deploy, we use a new way, model interpretation, that incorporates medical practitioners\u2019 prior knowledge to quickly find out the most important indicators in early diagnosis, and thus win the race in a pandemic. Besides, we find out that some evaluation metrics for interpretation methods are biased which could be an important research direction in the area of interpretable machine learning. This work was supported in part by HY Medical Technology, Scientific Research Department Beijing, CN Han Wu is with the University of Exeter, Stocker Rd, Exeter EX4 4PY UK (e-mail: hw@exeter.ac.uk). Wenjie Ruan is with the University of Exeter, Stocker Rd, Exeter EX4 4PY UK (e-mail: W.Ruan@exeter.ac.uk). Jiangtao Wang is with Coventry University, Priory St, Coventry CV1 5FB UK (e-mail: jiangtao.wang@coventry.ac.uk). Dingchang Zheng is with Coventry University, Priory St, Coventry CV1 5FB UK (e-mail: ad4291@coventry.ac.uk). Shaolin Li is with Fifth Affiliated Hospital of Sun Yat-sen University, Department of Radiology Zhuhai, CN, and Guangdong Provincial Key Laboratory of Biomedical Imaging Zhuhai, Guangdong, CN (e-mail: lishlin5@mail.sysu.edu.cn). Jia Chen is with Fifth Affiliated Hospital of Sun Yat-sen University, Department of Radiology Zhuhai, CN (e-mail: drchenj@126.com). Kunwei Li is with Fifth Affiliated Hospital of Sun Yat-sen University, Department of Radiology Zhuhai, CN (e-mail: likunwei@mail.sysu.edu.cn). Yayuan Geng is with HY Medical Technology, Scientific Research Department Beijing, CN (e-mail: gengyayuan@huiyihuiying.com). Xiangfei Chai is with HY Medical Technology, Scientific Research Department Beijing, CN (e-mail: chaixiangfei@huiyihuiying.com). Sumi Helal is with Lancaster University, Bailrigg, Lancaster LA1 4YW UK (e-mail: s.helal@lancaster.ac.uk).", "authors": [{"affiliations": [], "name": "Han Wu"}, {"affiliations": [], "name": "Wenjie Ruan"}, {"affiliations": [], "name": "Jiangtao Wang"}, {"affiliations": [], "name": "Dingchang Zheng"}, {"affiliations": [], "name": "Shaolin Li"}, {"affiliations": [], "name": "Jia Chen"}, {"affiliations": [], "name": "Kunwei Li"}, {"affiliations": [], "name": "Yayuan Geng"}, {"affiliations": [], "name": "Xiangfei Chai"}], "id": "SP:bf3e1ee0ac49279ae3271e8e0a4cd3c18f0789cb", "references": [{"authors": ["T. Singhal"], "title": "A review of coronavirus disease-2019 (covid-19)", "venue": "The Indian Journal of Pediatrics, vol. 87, no. 4, pp. 1\u20136, 2020.", "year": 2020}, {"authors": ["S. Basu", "S. Mitra", "N. Saha"], "title": "Deep learning for screening covid-19 using chest x-ray images", "venue": "medRxiv, 2020. [Online]. Available: https: //www.medrxiv.org/content/early/2020/05/08/2020.05.04.20090423", "year": 2020}, {"authors": ["R. Caruana", "Y. Lou", "J. Gehrke", "P. Koch", "M. Sturm", "N. Elhadad"], "title": "Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission", "venue": "KDD \u201915, 2015.", "year": 2015}, {"authors": ["A. Ramchandani", "C. Fan", "A. Mostafavi"], "title": "Deepcovidnet: An interpretable deep learning model for predictive surveillance of covid-19 using heterogeneous features and their interactions", "venue": "IEEE Access, vol. 8, pp. 159 915\u2013159 930, 2020.", "year": 2020}, {"authors": ["H. Guo", "R. Tang", "Y. Ye", "Z. Li", "X. He"], "title": "Deepfm: A factorizationmachine based neural network for ctr prediction", "venue": "International Joint Conference on Artificial Intelligence, 08 2017, pp. 1725\u20131731.", "year": 2017}, {"authors": ["F. Doshi-Velez", "B. Kim"], "title": "Towards a rigorous science of interpretable machine learning", "venue": "2017. [Online]. Available: https: //arxiv.org/abs/1702.08608", "year": 2017}, {"authors": ["I. Goodfellow", "J. Shlens", "C. Szegedy"], "title": "Explaining and harnessing adversarial examples", "venue": "International Conference on Learning Representations, 2015. [Online]. Available: http://arxiv.org/abs/1412. 6572", "year": 2015}, {"authors": ["L. Yan", "H.-T. Zhang", "J. Goncalves", "Y. Xiao", "M. Wang", "Y. Guo", "C. Sun", "X. Tang", "L. Jing", "M. Zhang", "X. Huang", "Y. Xiao", "H. Cao", "Y. Chen", "T. Ren", "F. Wang", "Y. Xiao", "S. Huang", "X. Tan", "N. Huang", "B. Jiao", "C. Cheng", "Y. Zhang", "A. Luo", "L. Mombaerts", "J. Jin", "Z. Cao", "S. Li", "H. Xu", "Y. Yuan"], "title": "An interpretable mortality prediction model for covid-19 patients", "venue": "Nature Machine Intelligence, vol. 2, no. 5, pp. 283\u2013288, May 2020.", "year": 2020}, {"authors": ["E. Matsuyama"], "title": "A deep learning interpretable model for novel coronavirus disease (covid-19) screening with chest ct images", "venue": "Journal of Biomedical Science and Engineering, vol. 13, no. 07, p. 140, 2020.", "year": 2020}, {"authors": ["J.H. Friedman"], "title": "Greedy function approximation: A gradient boosting machine.", "venue": "Ann. Statist., vol. 29,", "year": 2001}, {"authors": ["A. Goldstein", "A. Kapelner", "J. Bleich", "E. Pitkin"], "title": "Peeking inside the black box: Visualizing statistical learning with plots of individual conditional expectation", "venue": "Journal of Computational and Graphical Statistics, vol. 24, 09 2013.", "year": 2013}, {"authors": ["D.W. Apley", "J. Zhu"], "title": "Visualizing the effects of predictor variables in black box supervised learning models", "venue": "Journal of the Royal Statistical Society Series B, vol. 82, no. 4, pp. 1059\u20131086, September 2020.", "year": 2020}, {"authors": ["A. Fisher", "C. Rudin", "F. Dominici"], "title": "All models are wrong, but many are useful: Learning a variable\u2019s importance by studying an entire class of prediction models simultaneously", "venue": "Journal of Machine Learning Research, vol. 20, no. 177, pp. 1\u201381, 2019.", "year": 2019}, {"authors": ["M.T. Ribeiro", "S. Singh", "C. Guestrin"], "title": "why should i trust you?\u201d: Explaining the predictions of any classifier", "venue": "Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, ser. KDD \u201916. New York, NY, USA: Association for Computing Machinery, 2016, p. 1135\u20131144.", "year": 2016}, {"authors": ["S.M. Lundberg", "S.-I. Lee"], "title": "A unified approach to interpreting model predictions", "venue": "Advances in Neural Information Processing Systems 30, I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, Eds. Curran Associates, Inc., 2017, pp. 4765\u20134774.", "year": 2017}, {"authors": ["M.T. Ribeiro", "S. Singh", "C. Guestrin"], "title": "Anchors: High-precision model-agnostic explanations", "venue": "AAAI, 2018.", "year": 2018}, {"authors": ["D. Alvarez-Melis", "T.S. Jaakkola"], "title": "Towards robust interpretability with self-explaining neural networks", "venue": "2018. [Online]. Available: https://arxiv.org/abs/1806.07538", "year": 2018}, {"authors": ["R. Luss", "P.-Y. Chen", "A. Dhurandhar", "P. Sattigeri", "Y. Zhang", "K. Shanmugam", "C.-C. Tu"], "title": "Generating contrastive explanations with monotonic attribute functions", "venue": "2019. [Online]. Available: https://arxiv.org/abs/1905.12698", "year": 2019}, {"authors": ["M.T. Ribeiro", "S. Singh", "C. Guestrin"], "title": "Model-agnostic interpretability of machine learning", "venue": "2016. [Online]. Available: https://arxiv.org/abs/1606.05386", "year": 2016}, {"authors": ["T. Mikolov", "I. Sutskever", "K. Chen", "G. Corrado", "J. Dean"], "title": "Distributed representations of words and phrases and their compositionality", "venue": "Advances in Neural Information Processing Systems, vol. 26, 10 2013.", "year": 2013}, {"authors": ["C. Molnar"], "title": "Interpretable machine learning", "venue": "2019. [Online]. Available: https://christophm.github.io/interpretable-ml-book", "year": 2019}, {"authors": ["L.S. Shapley"], "title": "17. a value for n-person games", "venue": "Contributions to the Theory of Games (AM-28), Volume II, p. 307\u2013318, 1953.", "year": 1953}, {"authors": ["D.V. Carvalho", "E.M. Pereira", "J.S. Cardoso"], "title": "Machine learning interpretability: A survey on methods and metrics", "venue": "Electronics, vol. 8, no. 8, p. 832, Jul 2019.", "year": 2019}, {"authors": ["L. Breiman", "J. Friedman", "R. Olshen", "C. Stone"], "title": "Classification and regression trees. belmont, ca: Wadsworth international group.", "venue": "Encyclopedia of Ecology,", "year": 2015}, {"authors": ["L. Breiman"], "title": "Random forests", "venue": "Machine Learning, vol. 45, no. 1, pp. 5\u201332, 2001.", "year": 2001}, {"authors": ["S. Schaal", "C.C. Atkeson"], "title": "From isolation to cooperation: An alternative view of a system of experts", "venue": "Advances in Neural Information Processing Systems 8. MIT Press, 1996, pp. 605\u2013611.", "year": 1996}, {"authors": ["A. Maier", "C. Syben", "T. Lasser", "C. Riess"], "title": "A gentle introduction to deep learning in medical image processing", "venue": "Zeitschrift f\u00fcr Medizinische Physik, vol. 29, no. 2, pp. 86 \u2013 101, 2019, special Issue: Deep Learning in Medical Physics.", "year": 2019}, {"authors": ["G. Montavon", "W. Samek", "K.-R. M\u00fcller"], "title": "Methods for interpreting and understanding deep neural networks", "venue": "Digital Signal Processing, vol. 73, p. 1\u201315, Feb 2018.", "year": 2018}, {"authors": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg", "J. Vanderplas", "A. Passos", "D. Cournapeau", "M. Brucher", "M. Perrot", "E. Duchesnay"], "title": "Scikit-learn: Machine learning in Python", "venue": "Journal of Machine Learning Research, vol. 12, pp. 2825\u20132830, 2011.", "year": 2011}, {"authors": ["L. Wang"], "title": "C-reactive protein levels in the early stage of covid-19", "venue": "M\u00e9decine et Maladies Infectieuses, vol. 50, no. 4, pp. 332 \u2013 334, 2020. [Online]. Available: http://www.sciencedirect.com/science/article/ pii/S0399077X2030086X", "year": 2020}, {"authors": ["L. Gao", "D. Jiang", "X. Wen", "X. Cheng", "M. Sun", "B. He", "L.-n. You", "P. Lei", "X.-w. Tan", "S. Qin", "G. Cai", "D. Zhang"], "title": "Prognostic value of nt-probnp in patients with severe covid-19", "venue": "2020. [Online]. Available: https: //www.medrxiv.org/content/early/2020/03/10/2020.03.07.20031575", "year": 2020}, {"authors": ["Z. Xu", "L. Shi", "Y. Wang", "J. Zhang", "L. Huang", "C. Zhang", "S. Liu", "P. Zhao", "H. Liu", "L. Zhu", "Y. Tai", "C. Bai", "T. Gao", "J. Song", "P. Xia", "J. Dong", "J. Zhao", "F.S. Wang"], "title": "Pathological findings of COVID-19 associated with acute respiratory distress syndrome", "venue": "Lancet Respir Med, vol. 8, no. 4, pp. 420\u2013422, 04 2020.", "year": 2020}, {"authors": ["V. Arya", "R.K.E. Bellamy", "P.-Y. Chen", "A. Dhurandhar", "M. Hind", "S.C. Hoffman", "S. Houde", "Q.V. Liao", "R. Luss", "A. Mojsilovi\u0107", "S. Mourad", "P. Pedemonte", "R. Raghavendra", "J. Richards", "P. Sattigeri", "K. Shanmugam", "M. Singh", "K.R. Varshney", "D. Wei", "Y. Zhang"], "title": "One explanation does not fit all: A toolkit and taxonomy of ai explainability techniques", "venue": "Sep. 2019. [Online]. Available: https://arxiv.org/abs/1909.03012", "year": 2019}], "sections": [{"text": "Interpretable Machine Learning for COVID-19: An Empirical Study on Severity Prediction Task\nHan Wu, Wenjie Ruan, Jiangtao Wang, Dingchang Zheng, Shaolin Li, Jia Chen, Kunwei Li, Yayuan Geng, Xiangfei Chai and Sumi Helal, Fellow, IEEE\nAbstract\u2014Black-box nature hinders the deployment of many high-accuracy models in medical diagnosis. It is risky to put one\u2019s life in the hands of models that medical researchers do not trust. However, to understand the mechanism of a new virus, such as COVID-19, machine learning models may catch important symptoms that medical practitioners do not notice due to the surge of infected patients during a pandemic.\nIn this work, the interpretation of machine learning models reveals that a high C-reactive protein (CRP) corresponds to severe infection, and severe patients usually go through a cardiac injury, which is consistent with well-established medical knowledge. Additionally, through the interpretation of machine learning models, we find phlegm and diarrhea are two important symptoms, without which indicate a high risk of turning severe. These two symptoms are not recognized at the early stage of the outbreak, whereas our findings are corroborated by later autopsies of COVID-19 patients. We find patients with a high N-terminal pro B-type natriuretic peptide (NTproBNP) have a significantly increased risk of death which does not receive much attention initially but proves true by the following-up study. Thus, we suggest interpreting machine learning models can offer help to diagnosis at the early stage of an outbreak.\nImpact Statement\u2014We try to answer the question, how can medical practitioners cooperate with machine learning to win the race in this pandemic? Instead of targeting at a blackbox high-accuracy model that is hard to trust and deploy, we use a new way, model interpretation, that incorporates medical practitioners\u2019 prior knowledge to quickly find out the most important indicators in early diagnosis, and thus win the race in a pandemic. Besides, we find out that some evaluation metrics for interpretation methods are biased which could be an important research direction in the area of interpretable machine learning.\nThis work was supported in part by HY Medical Technology, Scientific Research Department Beijing, CN\nHan Wu is with the University of Exeter, Stocker Rd, Exeter EX4 4PY UK (e-mail: hw@exeter.ac.uk).\nWenjie Ruan is with the University of Exeter, Stocker Rd, Exeter EX4 4PY UK (e-mail: W.Ruan@exeter.ac.uk).\nJiangtao Wang is with Coventry University, Priory St, Coventry CV1 5FB UK (e-mail: jiangtao.wang@coventry.ac.uk).\nDingchang Zheng is with Coventry University, Priory St, Coventry CV1 5FB UK (e-mail: ad4291@coventry.ac.uk).\nShaolin Li is with Fifth Affiliated Hospital of Sun Yat-sen University, Department of Radiology Zhuhai, CN, and Guangdong Provincial Key Laboratory of Biomedical Imaging Zhuhai, Guangdong, CN (e-mail: lishlin5@mail.sysu.edu.cn).\nJia Chen is with Fifth Affiliated Hospital of Sun Yat-sen University, Department of Radiology Zhuhai, CN (e-mail: drchenj@126.com).\nKunwei Li is with Fifth Affiliated Hospital of Sun Yat-sen University, Department of Radiology Zhuhai, CN (e-mail: likunwei@mail.sysu.edu.cn).\nYayuan Geng is with HY Medical Technology, Scientific Research Department Beijing, CN (e-mail: gengyayuan@huiyihuiying.com).\nXiangfei Chai is with HY Medical Technology, Scientific Research Department Beijing, CN (e-mail: chaixiangfei@huiyihuiying.com).\nSumi Helal is with Lancaster University, Bailrigg, Lancaster LA1 4YW UK (e-mail: s.helal@lancaster.ac.uk).\nIndex Terms\u2014COVID-19, Interpretable, Machine Learning.\nI. INTRODUCTION\nTHe sudden outbreak of COVID-19, a communicabledisease with strong infectivity, brings an unprecedented impact worldwide. With more than 18 million confirmed cases as of mid-August, the pandemic is still accelerating globally. The disease is transmitted by inhalation or contact with infected droplets and the incubation period ranges from 2 to 14 days [1], making it more infectious and difficult to contain and mitigate.\nThe rapid transmission of COVID-19 causes strained medical resources in many countries. To help release the pressure of healthcare workers, different diagnostic and predictive models have been developed. For instance, a deep learning model for diagnosis using chest CT Images that detects abnormalities and extract key features of the altered lung parenchyma[2], and transfer learning is employed to train the model due to the inadequacy of COVID-19 datasets. However, deep neural networks are not interpretable due to their complexity which prevented many high-performance models from healthcare applications. Also, there are intelligible models for predicting readmission that use generalized additive models with pairwise interactions [3]. It is intelligible and modular, so patterns that do not obey medical knowledge can be recognized and removed. This method is able to scale to large datasets containing hundreds of thousands of patients and thousands of attributes while remaining intelligible and providing accuracy comparable to the best (unintelligible) machine learning methods. But this model is not suitable enough for more complex problems. To maintain both interpretability and complexity, DeepCOVIDNet is proposed for predictive surveillance that identifies the most influential features for prediction of the growth of the pandemic[4]. It is achieved through the combination of two modules, the embedding module, and the DeepFM [5] module. The embedding module takes as input various heterogeneous feature groups and outputs an equidimensional embedding corresponding to each feature group. These embeddings are then passed to the DeepFM module which computes second- and higher-order interactions between them.\nEmploying machine learning models achieves fast diagnosis and more reasonable distribution of medical resources according to the severity prediction of different areas. However, models with high accuracy may not provide explanations for their outputs due to the trade-off between accuracy and interpretability. More accurate models usually provide less\nar X\niv :2\n01 0.\n02 00\n6v 2\n[ cs\n.L G\n] 1\n7 O\nct 2\n02 0\ninterpretations[6]. For healthcare, being able to understand and validate its output is important for a model to be trusted to be safe and non-discriminative, and robust to adversarial attack [7]. To be applied in healthcare, the Multi-tree XGBoost algorithm is proposed that to identify most important indicators in COVID-19 diagnosis [8]. This method exploits the recursive tree-based decision system of the model to achieve great interpretability, and identifies LDH, hs-CRP and lymphocytes are three important indicators for COVID-19 prognostic prediction which is consistent with our research. There is another convolutional neural network (CNN)-based model, a ResNet50 based model, for discriminating coronavirus disease 2019 (COVID-19) from Non-COVID-19 using chest CT [9]. Its interpretability is achieved through implementing gradientweighted class activation mapping to produce a heat map for visually verifying where the CNN model is looking at the image, thereby, ensuring the model is performing correctly.\nBesides, many model-agnostic methods have been proposed to peek into black-box models, such as Partial Dependence Plot (PDP)[10], Individual Conditional Expectation (ICE)[11], Accumulated Local Effects (ALE)[12], Permutation Feature Importance [13], Local Interpretable Model-agnostic Explanations (LIME)[14], Shapley Additive Explanation (SHAP)[15], and Anchors [16]. Most model-agnostic methods are qualitatively reasoned through illustrative figures and human experiences. In order to justify different methods, several metrics for interpretability are proposed to quantatively measure their interpretations such as faithfulness [17] and monotonicity [18]. These interpretation methods and evaluation metrics will be introduced in detail in the next section.\nIn this paper, instead of targeting at a high-accuracy model, we use different methods to interpret models which are trained to predict the severity of COVID-19 using a dataset of 92 patient with 74 features. Besides checking whether or not their predictions are based on reasonable medical knowledge, we try to find clues that are neglected by medical practitioners in COVID-19 diagnosis. To quantitatively evaluate different interpretaion methods, we use faithfulness and monoticity to justify the interpretation of feature importance."}, {"heading": "II. PRELIMINARY OF AI INTERPRETABILITY", "text": "In this section, we summarize frequently used interpretation methods (PDP, ICE, ALE and Permutation Feature Importance), and evaluation metrics (Faithfullness and Monotonicity)."}, {"heading": "A. Model-Agnostic Methods", "text": "Restricting machine learning to interpretable models in healthcare is often a severe limitation. Separating explanations from the model has several flexibilities: model flexibility, explanation flexibility, and representation flexibility [19].\nModel flexibility: It is often necessary to train models that are accurate for most real-world applications. However, the behavior of high-accuracy models are usually too complex for humans to fully comprehend. Thus, the trade-off between accuracy and interpretability restricts the choice of models in many applications. While model-agnostic methods seperate\ninterpretability from the model thus frees up the model to be as flexible as necessary for the task, enabling the use of any machine learning approach.\nExplanation flexibility: Interpretable models are limited to certain forms of explanation which is not flexible. For different tasks, it might be useful to have a linear formula in some cases, while a graphic with feature importances can be more favorable under other scenarios. Being model-agnostic, the same model can be explained with different types of explanations, and different degrees of interpretability for each type of explanation [19].\nRepresentation flexibility: Many deep learning models use features that are not perceivable to human to represent input data, such as word embeddings in natural language processing (or NLP) [20]. As a result, the explanation system is unable to use a different feature representation as the model being explained. While model-agnostic approaches can generate explanations using different features than the one used by the the original model [19], thus more flexible.\nDue to these flexibilities, many model-agnostic methods are proposed to give explanations without knowing model details.\nPartial Dependence Plot: Partial Dependence Plots (PDP) reveal the dependence between the target function and several target features. The partial function \u02c6fxs(xs) is estimated by calculating averages in the training data, also known as Monte Carlo method. After setting up a grid for the features we are interested with (target features), we set all target features in our training set to be the value of grid points, then we make predictions and average them all at each grid. The drawback of PDP is that one feature produces 2D plots and two features produce 3D plots, as a result, it can be pretty hard to interpret more than two features, because it\u2019s not easy for human to understand plots in higher dimensions.\nf\u0302xs(xs) = 1\nn n\u2211 1 f\u0302(xs, x i c)\nIndividual Conditional Expectation: Individual Conditional Expectation (ICE) is similar with PDP, the difference is that PDP calculate the average over the marginal distribution while ICE keeps them all, which means one line in ICE represents the predictions for one instance, so that ICE draws one line for each individual. Without averaging, ICE uncovers heterogeneous relationships, but is limited to only one target feature because two features results in overlay surfaces in the plot which cannot be identified by human eyes [21].\nAccumulated Local Effects: Accumulated Local Effects (ALE) averages the changes in the predictions and accumulate them over the local grid. Its difference with PDP is that the value at each point of the ALE curve is the difference to the mean prediction, and the value is calculated in a small window rather than all of the grid, thus eliminates the effect of correlated features [21]. Calculating in a small window makes ALE more suitable in healthcare, because it\u2019s usually irrational to assume young people having physical characteristics within the range of the elderly.\nPermutation Feature Importance: The idea behind Permutation Feature Importance is intuitive. A feature is very\nimportant for the model, if there is a great increase in the model\u2019s prediction error after permutation. A feature is less important if the prediction error remains nearly unchanged after shuffling.\nLocal Interpretable Model-agnostic Explanations: Local Interpretable Model-agnostic Explanations (LIME) uses interpretable models to approximate the predictions of the original black-box model in specific regions for individual predictions. LIME works for tabular data, text and images, but the explanations may not be stable enough for medical applications.\nShapley Additive Explanation: SHapley Additive exPlanation (SHAP) borrows the idea of Shapley value from Game Theory [22], which represents contributions of each player in a game. Calculating Shapley values is computationally expensive when there are hundreds of features, thus Lundberg, Scott M., and Su-In Lee proposed fast implementation for treebased models to boost the calculation process [15]. SHAP has a solid theoretical foundation, but is still computationally slow for a lot of instances.\nTo summarize different interpretation methods, PDP, ICE and ALE only use graphs to visualize the impact of different features, while Permutation Feature Importance, LIME and SHAP provide numerical feature importance which means they quantatively rank the importance of different features.\nDifferent methods may understand the same model differently. In healthcare applications, sometimes we can use prior medical knowledge to distinguish reasonable ones, while sometimes it can be cumbersome to sort out most influential ones out of hundreds of features. Thus it is important to find quantitative metrics to evaluate different methods without human intervention."}, {"heading": "B. Metrics for Interpretability Evaluation", "text": "Different interpretation methods try to find out most important features to provide explanations for the output. But as Doshi-Velez and Kim questioned, \u201cAre all models in all defined-to-be-interpretable model classes equally interpretable [6]?\u201d And how can we measure the quality of different interpretation methods?\nTo figure out whether those features are correctly ranked, there are two types of indicators for assessment and comparison of explanations: qualitative and quantitative indicators [23]. Faithfulbess is a qualitative indicator, and monotonicity is a quantitative indicator.\nFaithfulness: Faithfulness incrementally removes each of the attributes deemed important by the interpretability metric, and evaluating the effect on the performance. Then it calculates the correlation between the weights (importance) of the attributes and corresponding model performance, and returns correlation between attribute importance weights and corresponding effect on classifier [17].\nMonotonicity: Monotonicity incrementally adds each attribute in order of increasing importance. As each feature is added, the performance of the model should correspondingly increase, thereby resulting in monotonically increasing model performance, and it returns True of False [18].\nIn our experiments, we use these two metrics to evaluate different interpretation methods. But it is important to notice here that evaluating metrics is still an area under active research. Evaluation metrics may be biased, which means the way the metric is calculated can be more friendly to some methods, and gives low score on interpretation methods that produce plausible explanations. As a result, different evaluation metrics can be used as references, but not the truth."}, {"heading": "III. EMPIRICAL STUDY ON COVID", "text": "In this section, we introduce our raw dataset and procedures of data preprocessing, then use it to train four different models, decision tree, random forest, gradient boosted trees and neural networks. Through interpretation of the four different models, we try to understand why different models give similar or different predictions, and whether or not their predictions are consistent with medical knowledge. Finally we focus on individual patient that our models fail to make the correct decision, and try to explain and evaluate the interpretation."}, {"heading": "A. Dataset and Perprocessing", "text": "The raw dataset comes from hospitals in China, including 92 patients contracted COVID-19.\nOur Research Ethics Committee waived written informed consent for this retrospective study that evaluated de-identified data and involved no potential risk to patients. All of the data of patients have been anonymized before analysis.\nThe 74 features consist of Body Mass Index (BMI), Complete Blood Count (CBC), Blood Biochemical Examination, inflammatory markers, symptoms, anamneses, and etc. While some tests are not compulsory for diagnosis of COVID-19, features such as LVEF remain unfilled for those who did not take Color Doppler Ultrasound Test.\nAfter pruning out features with less entries, and patients with incomplete records, there remains 86 records with 55 features. Among those, 77 records are used for training, cross validation, and 9 reserved for testing.\nThe table above lists all the features in our dataset, and more details can be found in the Appendix. The indicator used for severity classification is Severity01 which indicates normal with 0, and severe with 1.\nIt is important to perform feature engineering before training and interpreting our models, as some features may not provide valuable information or provide redundant information.\nFirst, we use some naive methods to remove unnecessary features. The two features, PCT2 and Stomachache, have the same value for all patients which means they do not provide valuable information in distinguishing normal and severe patients, thus we can remove them both. And all the included patients (n=1) with CAD have the comorbidity of arrythmia, so we can remove one of these two features.\nSecond, we try to remove correlated features. The table below lists all correlated features using Pearson\u2019s correlation coefficient. We try to understand the strong correlation between two features and remove one of them if necessary.\nThe strong correlation between cTnICKMBOrdinal1 and cTnICKMBOrdinal2 is because they are the same test among a short range of time, thus remain almost the same, so we can remove one of them, and is the same for LYM1 and LYM2. LDH and HBDH levels are significantly correlated with heart diseases, and the HBDH/LDH ratio can be calculated to differentiate between liver and heart diseases. As for the correlation between NEU1 and WBC1, NEU2 and WBC2, they are all correlated to the immune system, and in fact, most of the white blood cells that lead the immune system\u2019s response are neutrophils. Since there is no much information about N2L2, and is correlated with NTproBNP, we simply keep NTproBNP. Finally, the correlation between BMI and weight is straight forward because Body Mass Index (BMI) is a person\u2019s weight in kilograms divided by the square of height in meters.\nThird, several statistical methods are employed to remove features with redundant information.\nhow similar the joint distribution p(X,Y) is to the products of individual distributions p(X)p(Y). Univariate Test measures the dependence of 2 variables, and a high p value for this test means a less similar distribution among X and Y.\nI(X;Y ) = \u2211 x,y p(x, y)log p(x, y) p(x)p(y)\nFinally, there are still 36 features left for training and testing."}, {"heading": "B. Training Models", "text": "Machine learning models outperform human in many different areas in terms of accuracy. Interpretable models such as decision trees are easy to understand, but not suitable for large scale applications. Complex models achieve high accuracy while giving less explanation. We used 4 different models to extract information from our dataset, Decision Trees, Random Forests, Gradient Boosted Trees, Neural Networks.\nDecision Trees: Decision Tree (DT) is a widely adopted method for both classification and regression. It\u2019s a nonparametric supervised learning method which infers decision rules from data features. Decision trees try to find decision rules that make the best split measured by gini impurity or entropy. More importantly, the generated decision trees can be visualized, thus easy to understand and interpret [24]. .\nRandom Forest: Random Forests (RF) is a kind of ensemble learning method [25] that employs bagging strategy. Multiple decision trees are trained using the same learning algorithm, and then aggregate the predictions from the individual decision trees. Random forests produces great results most of the time even without much hyper-parameter tuning, thus it has been widely accepted for its simplicity and good performance. However, it is rather difficult for human to interpret hundreds of decision trees, thus the model itself is less interpretable than a single decision tree.\nGradient Boosted Trees: Gradient Boosted Trees is another ensemble learning method that employs boosting strategy [26]. Through sequentially adding one decision tree at one time, gradient boosted trees combines results along the way. With fine-tuned parameters, gradient boosting can result in better performance than random forests. Still, it is tough for human to interpret a sequence of decision trees, and thus comsidered as black-box models.\nNeural Networks: Neural Networks could be the most promising model as for achieving high accureacy, and even outperforms human in medical imaging [27]. Though the whole network is difficult to understand, deep neural networks are stacks of simple layers, thus can be partially understood through visualizing outputs of intermediate layers [28].\nFor healthcare, both accuracy and interpretability are required. Simple interpretable models cannot achieve satisfactory accuracy in applications of image, thus many modelagnostic methods are employed to interpret complex black-box models.\nAll these methods are implemented using scikit-learn [29], keras and python3.6.\nThere is no hyperparameter for decision tree, and we use 100 trees for random forest. The hypterparameters for gradient\nboosted trees are selected according to prior experience. The structure for neural networks is listed below.\nAfter training, gradient boosted trees and neural networks acheive the highest precision on the test set, while random forest gets the worst performance. Among the 9 patients in our test set, four of them are severe, which means Decision Tree fail to find two severe patients, and Random Forest loses three of them, while Gradient Boosted Trees and Neural Networks find all of severe patients.\nIn this paper, we do not try to get the most accurate model, instead we focus on understanding different models. Thus, in the next section, we\u2019ll interpret these models and try to understand why they make different decisions.\nC. Interpreting Models\nFirst, we use Permutation Feature Importance to find the most important features in different models.\nIt can be seen from the table that both CRP2 and NTproBNP are recognized as important features by all of the four models. From the perspective of medical research, CRP refers to CReactive Protein, which increases when there\u2019s inflammation or infection in ones body. C-reactive protein (CRP) levels are positively correlated with lung lesions and could reflect disease severity[30]. And NTproBNP refers to N-Terminal prohormone of Brain Natriuretic Peptide, which are released in response to changes in pressure inside the heart. Patients\ncontracted COVID19 will have a rise in CRP due to virus infection, and patients with higher NT-proBNP (above 88.64 pg/mL) level had more risks of in-hospital death [31]. The result implies that the two important features recognized by all of the four models does not obey medical knowlege.\nIn order to visualize the relationship between CRP and NTproBNP inside the model, we use PDP, ICE and ALE to see how they impact on prediction.\nIn the PDPs, all of the four models indicate a higher risk of severe with the increase of NTproBNP and CRP which it\u2019s consistent with retrospective study on COVID-19. Thus, both interpretable models and black-box models successfully catch features that are deemed important by medical researchers.\nThe difference is that different models have different tolerances and dependence on NTproBNP and CRP. Averagely, decision trees and gradient boosted trees give less tolerance on a high level of NTproBNP (\u00bf2000ng/ml), and gradient boosted trres give a much higher probability of death as CRP increases. Since PDPs calculate an average of all instances, we use ICEs to identify heterogeneity.\nICE reveals individual differences. Though all of the models give a prediction of higher risk of severe as NTproBNP and CRP increase, some patients have a much higher initial probabilities which indicates there are other features that have impact on overall predictions, for example, elderly people have higher NTproBNP than young people and have a higher risk of turing severe.\nIn the ALEs, all of the four models give a positive contribution of beging severe as NTproBNP and CRP get higher, which coincide with medical knowledge.\n1) Explaining Misclassified Instances: Even though the four models successfully find important features in the diagnosis of COVID-19, some models fail to recognize severe patients. Both Gradient Boosted Trees and Neural Networks successfully recognized all severe patients and yield a recall of 1.00, while Decision Trees misses two of them, and Random Forest fails to recognize three of them. To find out the reason, we use LIME and SHAP to explain misclassified instances.\nTake Random Forest as an example. The three severe patients it fails to predict are listed below.\nAmong three misclassified patients, patient No. 1 and No. 7 are predicted a probability of 0.52 of being severe which is around the boundary, while for patient No. 5, the model gives a high prediction of 0.7 which is a huge mistake. Thus we focus on patient No.5 and use LIME and SHAP to understand why there is such a mistake.\n2) Interpreting Random Forest: The table VIII shows medical conditions for patient No.5 who is a severe patient while random forest takes him as a normal one.\nFrom the explanation of LIME, we see that even though this patient has a high NTproBNP, CRP and LDH that reflect a severe infection and heart injury, the model takes him as normal because he has low cTnl and cTnlCKMBOrdinal, and does not have any critical symptom. Actually this explanation\nis reasonable to some extend, a person without any symptom cannot be a severe patient even if he\u2019s infected. But this is only the explanation from LIME, and it\u2019s not guaranteed to be a fully correct interpretation, so we use SHAP to make a comparison.\nFeatures pushing the prediction to be higher (normal) are shown in red, and those pushing the prediction to be lower (severe) are in blue. From the explanation of SHAP, we see that a high CRP, LDH and NtproBNP try to push the patient to be severe which are consistent with explanations from LIME, and\nwhile having no symptoms, and a low cTnl, LYM, temperature makes the model to take him as normal.\nIf we think different models are different doctors, then random forest, as a doctor, makes the wrong diagnosis. The reason human doctors take him as severe is that he actually needs a respirator to survive, so there are things random forest does not notice. But why gradient boosted trees and neural networks make the correct diagnosis?\n3) Interpreting Gradient Boosted Tree: Let\u2019s try to figure out why the doctor of gradient boosted tree make the correct decision. From the interpretation of both LIME and SHAP, we see that the main difference between gradient boosted tree and random forest is that gradient boosted tree gives a higher weight on NTproBNP and CRP, thus even though the patient has a low temperature without any critical symptoms, the overall diagnosis still remains severe.\n4) Interpreting Neural Networks: Similar with gradient boosted tree, Neural Networks takes it more important to be severe with a high NTproBNP. Besides, from the interpretation of SHAP, we notice that Neural Networks take Age as an\nimportant factor, the patient No.5 is diagnosed as severe is partly because the patient is old which is consistent with human doctors judgement.\nActually in our dataset, there is an extra feature that indicates how severe a patient is ranging from level 0 to level 3. If we calculate the average of different severity level, we notice that as people grow older, their situations are more likely to deteriorate.\nAs a result, taking the patient\u2019s old age into account, neural networks make a prediction of severe due to a high NTproBNP and CRP, even though without critical symptoms.\n5) Other Interpretations: There are other interesting features that models rely on to make predictions.\nIn the LIME explanation of Gradient Boosted Trees, Phlegm being False is considered an important symptom of turning severe, which means patients that do not produce phlegm is likely to turn severe. This seems to be odd at first glance, but it\u2019s corroborated by autopsies of COVID-19 patients[32]. Patients that don\u2019t spit out phlegm from the throat or lungs through coughing is more likely to undergo progressive respiratory failure due to sputum bolt.\nSimilarly, in the LIME explanation of Neural Networks, Diarrhea being False is considered an important symptom of turning severe, which means patients that have the symptom of diarrhea is more likely to recover. Initially, no one associates diarrhea with COVID-19 which usually causes alveolar damage, but as more and more people go to hospital due to diarrhea, but diagnosed as COIVD-19, gradually medial practitioners realize that diarrhea is the first sign of illness for some COVID-19 patients.\nThese two findings indicate that machine learning models are capable of catching importance clues for a new disease while human doctors may neglect. Thus even though it\u2019s inappropriate to deploy black-box models to clinical diagnosis, we may use it to unveil the mechanism behind a new virus through model interpretation."}, {"heading": "D. Evaluating Interpretation", "text": "Even though we do find some critical symptoms of COVID19 through model interpretation, they are confirmed credible because these interpretations are corroborated by later study. If we use interpretation to understand a new virus at a very early stage of an outbreak, there will be no enough evidence to support our interpretation initially. Thus we use Monoitinicity and Faithfulness to evaluate different interpretations using IBM AIX 360 toolbox [33].\nFaithfulness reveals the correlation between the importance assigned by the interpretability algorithm and the effect of each of the attributes on the performance of the predictive model. Though only SHAP receives a high faithfulness, this does not mean interpretations from LIME and Permutation is not reasonable because they are supported by medical knowledge in our experiment. The reason SHAP receives the highest faithfulness is that shapley value is calculated by removing the effect of specific features which similar to how faithfullness is computed, thus SHAP is more akin to faithfulness by native.\nAs for monotonicity, all three interpretation methods receive a False though we do find some valuable conclusions from these interpretations. The reason is that LIME trains a local surrogate model to imitate the behavior of the original one, but does not guarantee the same behavior globally. And\npermutation randomly shuffles the features, thus does not guarantee the monotonicity. As for SHAP, it is calculated by removing features rather than adding features.\nFrom these results, we can see that the scores different metrics give are heavily dependent on how similar they are calculated with the interpretation method, so these metrics can be biased. As a result, under current research, it may be more reliable to manually check the fidelity of different interpretations through comparing with prior medical knowledge.\nIn conclusion, the evaluation metrics is still under active research, it may offer some help in evaluating different interpretation methods, still it\u2019s hard to find a metric that is nonbiased for all different methods. And a non-biased classifieragnostic metrics In future research."}, {"heading": "IV. CONCLUSION", "text": "In this paper, we use Permutation Feature Importance to interpret four different models that predict COVID-19 severity, and use PDP, ALE, ICE to visualize the result. The interpretation reveals that all of the four models successfully find NTproBNP and CRP are two important indicators for CIVID19, which are consistent with medical research.\nBesides, high-accuracy models reveal that phlegm and diarrhea are two most indicative symptoms without which the patient is likely to turn severe. And these findings are consistent with autopsies of COVID-19 patients, and recognized as important signs of illness for some COVID-19 patients.\nFinally, we use monotonicity and faithfullness to evaluate different interpretation mehods, and find that faithfullness is more akin to SHAP due to the way it\u2019s calculated. Thus the evaluating metrics is still under active research, it\u2019s still hard to find a non-biased metric for different interpretation methods.\nIn conclusion, it\u2019s possible to use interpretable machine to reveal mechanisms of a new virus at the early stage of an outbreak.\nAPPENDIX"}], "title": "Interpretable Machine Learning for COVID-19: An Empirical Study on Severity Prediction Task", "year": 2020}