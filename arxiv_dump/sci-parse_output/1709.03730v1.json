{
  "abstractText": "Despite outperforming the human in many tasks, deep neural network models are also criticized for the lack of transparency and interpretability in decision making. The opaqueness results in uncertainty and low confidence when deploying such a model in model sharing scenarios, when the model is developed by a third party. For a supervised machine learning model, sharing training process including training data provides an effective way to gain trust and to better understand model predictions. However, it is not always possible to share all training data due to privacy and policy constraints. In this paper, we propose a method to disclose a small set of training data that is just sufficient for users to get the insight of a complicated model. The method constructs a boundary tree using selected training data and the tree is able to approximate the complicated model with high fidelity. We show that traversing data points in the tree gives users significantly better understanding of the model and paves the way for trustworthy model sharing.",
  "authors": [
    {
      "affiliations": [],
      "name": "Huijun Wu"
    },
    {
      "affiliations": [],
      "name": "Chen Wang"
    },
    {
      "affiliations": [],
      "name": "Jie Yin"
    },
    {
      "affiliations": [],
      "name": "Kai Lu"
    },
    {
      "affiliations": [],
      "name": "Liming Zhu"
    }
  ],
  "id": "SP:49f342dc5ab43123af9e30617a70e835c78acbb3",
  "references": [
    {
      "authors": [
        "Aamodt",
        "A. Plaza 1994] Aamodt",
        "E. Plaza"
      ],
      "title": "Case-based reasoning: Foundational issues, methodological variations, and system approaches. AI communications 7(1):39\u201359",
      "year": 1994
    },
    {
      "authors": [
        "Andoni",
        "A. Razenshteyn 2015] Andoni",
        "I. Razenshteyn"
      ],
      "title": "Optimal data-dependent hashing for approximate near neighbors",
      "venue": "In Proceedings of the forty-seventh annual ACM symposium on Theory of computing,",
      "year": 2015
    },
    {
      "authors": [
        "Bien",
        "J. Tibshirani 2011] Bien",
        "R. Tibshirani"
      ],
      "title": "Prototype selection for interpretable classification",
      "venue": "The Annals of Applied Statistics",
      "year": 2011
    },
    {
      "authors": [
        "Che"
      ],
      "title": "Interpretable deep models for icu outcome prediction",
      "venue": "In AMIA Annual Symposium Proceedings,",
      "year": 2016
    },
    {
      "authors": [
        "Craven",
        "M. Shavlik 1996] Craven",
        "J.W. Shavlik"
      ],
      "title": "Extracting tree-structured representations of trained networks. In Advances in neural information processing",
      "year": 1996
    },
    {
      "authors": [
        "Fong",
        "R. Vedaldi 2017] Fong",
        "A. Vedaldi"
      ],
      "title": "Interpretable explanations of black boxes by meaningful perturbation",
      "venue": "arXiv preprint arXiv:1704.03296",
      "year": 2017
    },
    {
      "authors": [
        "Gama"
      ],
      "title": "A survey on concept drift adaptation",
      "venue": "ACM Computing Surveys",
      "year": 2014
    },
    {
      "authors": [
        "Hinton"
      ],
      "title": "Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups",
      "venue": "IEEE Signal Process-",
      "year": 2012
    },
    {
      "authors": [
        "Jordaney"
      ],
      "title": "Transcend: Detecting concept drift in malware classification models",
      "venue": "In 26th USENIX Security Symposium (USENIX Security",
      "year": 2017
    },
    {
      "authors": [
        "Robie Kabra",
        "M. Branson 2015] Kabra",
        "A. Robie",
        "K. Branson"
      ],
      "title": "Understanding classifier errors by examining influential neighbors",
      "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "year": 2015
    },
    {
      "authors": [
        "Khanna Kim",
        "B. Koyejo 2016] Kim",
        "R. Khanna",
        "O.O. Koyejo"
      ],
      "title": "Examples are not enough, learn to criticize! criticism for interpretability",
      "venue": "In Advances in Neural Information Processing Systems,",
      "year": 2016
    },
    {
      "authors": [
        "Koh",
        "P.W. Liang 2017] Koh",
        "P. Liang"
      ],
      "title": "Understanding black-box predictions via influence functions",
      "venue": "arXiv preprint arXiv:1703.04730",
      "year": 2017
    },
    {
      "authors": [
        "Sutskever Krizhevsky",
        "A. Hinton 2017] Krizhevsky",
        "I. Sutskever",
        "G.E. Hinton"
      ],
      "title": "Imagenet classification with deep convolutional neural networks. Commun",
      "year": 2017
    },
    {
      "authors": [
        "LeCun",
        "Y others 2015] LeCun"
      ],
      "title": "Lenet5, convolutional neural networks. URL: http://yann",
      "venue": "lecun. com/exdb/lenet",
      "year": 2015
    },
    {
      "authors": [
        "Liang",
        "M. Hu 2015] Liang",
        "X. Hu"
      ],
      "title": "Recurrent convolutional neural network for object recognition",
      "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "year": 2015
    },
    {
      "authors": [
        "Mathy"
      ],
      "title": "The boundary forest algorithm for online supervised and unsupervised learning",
      "year": 2015
    },
    {
      "authors": [
        "Mu"
      ],
      "title": "Streaming classification with emerging new class by class matrix sketching",
      "year": 2017
    },
    {
      "authors": [
        "Singh Ribeiro",
        "M.T. Guestrin 2016] Ribeiro",
        "S. Singh",
        "C. Guestrin"
      ],
      "title": "Why should i trust you?: Explaining the predictions of any classifier",
      "venue": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
      "year": 2016
    },
    {
      "authors": [
        "Rifkin",
        "R. Klautau 2004] Rifkin",
        "A. Klautau"
      ],
      "title": "In defense of one-vs-all classification. Journal of machine learning research 5(Jan):101\u2013141",
      "year": 2004
    },
    {
      "authors": [
        "Aldrich Schmitz",
        "G.P. Gouws 1999] Schmitz",
        "C. Aldrich",
        "F.S. Gouws"
      ],
      "title": "Ann-dt: an algorithm for extraction of decision trees from artificial neural networks",
      "venue": "IEEE Transactions on Neural Networks",
      "year": 1999
    },
    {
      "authors": [
        "Silver"
      ],
      "title": "Mastering the game of go with deep neural networks and tree search. Nature 529(7587):484\u2013489",
      "year": 2016
    },
    {
      "authors": [
        "Tram\u00e8r"
      ],
      "title": "Stealing machine learning models via prediction apis",
      "year": 2016
    },
    {
      "authors": [
        "Yosinski"
      ],
      "title": "Understanding neural networks through deep visualization",
      "venue": "arXiv preprint arXiv:1506.06579",
      "year": 2015
    },
    {
      "authors": [
        "Zeiler",
        "M.D. Fergus 2014] Zeiler",
        "R. Fergus"
      ],
      "title": "Visualizing and understanding convolutional networks",
      "venue": "In European conference on computer vision,",
      "year": 2014
    },
    {
      "authors": [
        "Lakshminarayanan Zoran",
        "D. Blundell 2017] Zoran",
        "B. Lakshminarayanan",
        "C. Blundell"
      ],
      "title": "Learning deep nearest neighbor representations using differentiable boundary trees. arXiv preprint arXiv:1702.08833",
      "year": 2017
    }
  ],
  "sections": [
    {
      "heading": "Introduction",
      "text": "Deep neural networks (DNN) have achieved great success in image classification (Krizhevsky, Sutskever, and Hinton 2017), speech recognition (Hinton et al. 2012) and classic games such as Go (Silver et al. 2016) in recent years. The success inspires the use of DNN in a rapidly increasing number of applications. Training a DNN model, however, often requires large amounts of labeled data and non-trivial efforts of tuning. Sharing a trained model is cost-effective. Machine learning models are seen offered as a service in the cloud and charged in a Pay-As-You-Go model. However, a complicated DNN model remains a black-box and the model quality is difficult to assess, particularly for a different set of data. Model accuracy and confidence values are not sufficient to reveal model behaviors on data collected from different sources. A simple example is that the confidence value of classifying a data point belonging to an unseen class by the model can be high (as shown in Fig. 10 in Evaluation Section). It is challenging to make users trust a shared model.\nProviding means to interpret a model is effective to improve model transparency and gain trust from model users.\nCopyright c\u00a9 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\nThere has been a growing interest in exploring the interpretability for deep neural networks. Model interpretability has been tackled by different approaches in recent years. A common approach is to find examples or prototypes to guide decision making, which is along the line of the case based reasoning (Aamodt and Plaza 1994). It is formulated as a set cover optimization problem to identify training data points that are close to data points within their own classes and far away from those in different classes (Bien and Tibshirani 2011). The approach is complemented by (Kim, Khanna, and Koyejo 2016), which identifies data points that do not fit a model well to further explain the model trained on data without a \u201cclean\u201d distribution.\nAnother common approach is to mimic a complicated model using an interpretable simple model such as decision tree (Craven and Shavlik 1996; Schmitz, Aldrich, and Gouws 1999; Boz 2002; Che et al. 2016). The limitation of this approach is that it often requires structured data. When the structure is complicated, a decision tree itself can be very complex and difficult to interpret. Visualization is also used to examine hidden neurons in DNNs (Kabra, Robie, and Branson 2015; Yosinski et al. 2015; Zeiler and Fergus 2014). However, in model sharing scenarios, model users may not have the access to the internal structure of a model (Trame\u0300r et al. 2016).\nSample perturbations (Ribeiro, Singh, and Guestrin 2016; Fong and Vedaldi 2017) emerge as an important approach to understand features leading to certain predictions in image classification. These methods intend to learn an image perturbation mask that minimizes a class score. Their computational overhead is non-trivial. Moreover, they rely on the change of confidence values to infer the influence of a changing individual feature. The confidence value change can be inaccurate when there is a concept drift in data, which can be common in model sharing scenarios. Along the line, some recent research (Koh and Liang 2017) proposes to use influence functions to find the most important training data which leads to certain predictions.\nMost of the above-mentioned approaches assume the availability of the whole training data. However, sharing the whole training data is not realistic in most of the cases due to data privacy concerns or business competition reasons. In model sharing, there is a need for a model provider to disclose a small set of training data that can help model users\nar X\niv :1\n70 9.\n03 73\n0v 1\n[ cs\n.L G\n] 1\n2 Se\np 20\n17\nto gain an insight of a model to establish trust through interpretability. In this paper, we tackle this problem by providing users a small set of training data points that are enough to characterize the decision boundaries of a complicated DNN model. We employ a max-margin based approach to select the most representative training data that largely contributes to the forming of the decision boundary of the DNN model. These training data points are organized via an Explicable Boundary Tree (EB-tree) based on the distances in the DNN transformed space. The EB-tree is thus a compact structure that composes of a small set of training data but approximates the decision boundary of the DNN model with high fidelity.\nDifferent to the example or prototype selection based approach (Bien and Tibshirani 2011), EB-tree emphasizes revealing why a test data point near a boundary is classified to class A and not class B by presenting model users the traversal path of the data point among similar training data points across the DNN model decision boundary. The traversal path reveals the difference between a test data point to both sets of representative training data as well as the difference between the two sets of training data themselves, which gives a model user better interpretation of the DNN model than using examples and critics alone. Unlike perturbing based methods, EB-tree mimics the decision boundaries of the DNN classifiers by the confidence values of training points rather than perturbing the test points. This can also avoid the unexpected confidence change caused by concept drift or unseen classes (Gama et al. 2014).\nEB-tree is able to achieve a fidelity above 99.5% to the DNN model with less than 0.3% training data disclosure through a proposed boundary stitching algorithm. In addition, we show through a human pilot study that the traverse process of a test point in the tree clearly improves model users\u2019 understanding about how predictions are made by DNN models. We also demonstrate that in addition to giving model users insight of DNN model decision boundaries, EBtree can be used for identifying mislabeled training data and improving the efficiency of emerging new class detection."
    },
    {
      "heading": "Explicable Boundary Tree",
      "text": ""
    },
    {
      "heading": "Boundary Tree",
      "text": "The boundary tree (forest) algorithm (Mathy et al. 2015) is initially proposed for fast online learning. Each node of the boundary tree represents a training point. For a query of training sample y, the algorithm looks up the closest node x to y and use the label of x to predict the class of y. If x has the same label with y, y would be discarded. Otherwise, it is added into the tree. The process repeats for each training data point. A test data point is classified by the label of its closest node in the boundary tree.\nAs each edge in the tree crosses a decision boundary, all the nodes on the boundary tree, in essence, sketches the contours of the decision boundaries. The difference between two end nodes of an edge on the boundary tree can, therefore, serve as a local explainer of the decision boundary."
    },
    {
      "heading": "Explicable Boundary Tree",
      "text": "Even though its edges provide certain hints about the decision boundary between two classes, a basic boundary tree has three main limitations to support interpretability of a shared DNN model: firstly, the tree has relatively low fidelity to the DNN model in decision making; secondly, the number of training data points in the tree is not optimized and there is plenty of room to reduce the number for disclosing to model users; thirdly, the training data point selection does not characterize a boundary clearly as two data points far away from the boundary may be connected by an edge. Some recent work (Zoran, Lakshminarayanan, and Blundell 2017) intends to further learning a distance metric of boundary tree edges using DNN to improve its classification accuracy and boundary representation. However, the method is not scalable due to the computing complexity. We describe EB-tree in this section to address these limitations of the basic boundary tree. Our aim is to approximate the decision boundaries of a given DNN classifier accurately with a small number of training data points.\nFigure 1 shows the EB-tree classifier architecture. An EBtree consists of a DNN classifier f and an optimized boundary tree. The model translation module is responsible for constructing a boundary tree T to mimic the decision making of the DNN classifier. Each node of T is a training data point that is close to the decision boundary in the training dataset D. The DNN classifier transforms training data t \u2208 D to representation f(t). We use the output of softmax layer in the DNN as the transformed representation of a training data point. The distance between f(t1) and f(t2) is Euclidean distance.\nEB-tree selects a small portion of training data points that are close to the decision boundaries to approximate a DNN model. These data points are helpful for giving model users insight about the key differences between the two classes. For a test data point, traversing the tree to reach the closest training point provides an interpretation about the decision choice of the model.\nTo construct an EB-tree with good interpretability, it is important to ensure that the distance between two nodes connected by an edge is short. As an edge crosses the boundary, a short edge approximates the boundary with a narrow margin and gives a model user better visibility of the boundary. We give an ordering algorithm for training data points to achieve this boundary visibility in the following section. The algorithm also produces high accuracy and fidelity to the DNN model.\nOnce constructed, the EB-tree is able to answer queries of test data points from a model user as follows: A query sample y is firstly transformed by the DNN model to f(y); then a traversal process locates the closest node to f(y) in the tree, denoted by x; finally, the label of x is used to predict the class of y. The traversal path is used as an explainer of the prediction."
    },
    {
      "heading": "Model Translation",
      "text": "The model translation module in Figure 1 is responsible for identifying the most representative training data that char-\nacterize the decision boundaries of a DNN model and constructing an EB-tree to approximate the decision boundaries.\nThe basic boundary tree algorithm is not able to achieve sufficient fidelity as shown in our experiment ??, neither does it optimize the size of the tree, which may lead to unnecessary training data disclosure. We address this problem by train points sorting and boundary stitching described as below.\nTrain Points Selection and Sorting\nFigure 2 gives an example showing the training point selection problem. Node B may add either node A or A\u2032 as its child. The choice leads to different classification results for queryQ. Specifically, whenA \u2032 is selected, query of test data point Q is classified as class A because it is closer to node A (on the left of dash line l \u2032 ); otherwise, it is misclassified because Q is closer to node B than A. Selecting training data pairs close to the boundary from a different side and close to each other reduces the probability a test data point is misclassified, therefore enables the constructed tree to achieve high fidelity to the DNN model. In order to do this, we need to have a metric to characterize the distance between a data point and a boundary.\nEB-tree uses a max-margin based method to measure the distance. Let w denote a vector orthogonal to the decision\nboundary, b denote a scalar \u201coffset\u201d and {xi, yi} denote the DNN transformed representations of train points. We consider DNN transformed representations linear separable. The decision boundary of two classes can be represented as Equation 1, in which x is a set of training data points.\nwTx+ b = 0 (1)\nThe margin from the boundary to the nearest data points on each side of the boundary satisfies Equation 2.\n(wTxi + b) \u00b7 yi \u2265 1 (2)\nThe margin between two classes is therefore:\nd = 2\n||w|| (3)\nSimilar to support vector machine (SVM), we identify a boundary that maximizes the margin between two classes, which is equivalent to minimize ||w||.\nWe use the one-vs-all scheme (Rifkin and Klautau 2004) to obtain a boundary for each class based on max-margin. For each training data point (x, y) in the DNN transformed space, we can then compute its minimal distance to the decision boundary of its corresponding class. Training points are sorted in ascending order according to the distances to boundaries.\nThe EB-tree construction process picks sorted data points to add to the tree. The order has a significant impact on both node number and interpretability of the boundary tree constructed. Consider that training points are randomly included into the tree, a training data point that is far away from a decision boundary is likely to be added to the tree first. This may result in the discard of the subsequent adjacent data points that are closer to the boundary because they share the same label. Our experiments show that random order tends to include too many nodes in the tree while achieving suboptimal model mimicking performance.\nOn the interpretability aspect, randomly ordered data points are likely to include many long edges in the boundary tree, which make the feature difference between a parent and a child node difficult to infer. In contrast, an ascending order can keep the data points near a boundary in the constructed tree, meanwhile avoiding the inclusion of long\nedges. It is also likely to discard data points far away from boundaries. The classification of these data points is often consistent with human intuition and including them in the tree does not contribute to boundary characterization much. This approach helps to minimize the training data disclosure to model users."
    },
    {
      "heading": "Boundary Stitching Algorithm",
      "text": "Simply inserting training data points according to the ascending order of their distances to boundaries is not enough to construct a boundary tree with good interpretability. It is mainly due to that two training points with similar distances to the boundary may be located at different ends of the boundary and far away from each other. The features of such two points are unlikely to share sufficient commonality for a model user to understand the decision boundary. Figure 3a illustrates the case with an example, in which data points like node 3, 6 and 7 are not included in the tree because their corresponding closest nodes with the same label but closer to the boundary are already in the tree at the time they are being processed. In the following, we give a boundary stitching algorithm to address this problem and the algorithm intends to construct a tree well approximating the boundary as illustrated in Figure 3b.\nThe boundary stitching algorithm shown in Algorithm 1 takes into account the distances between current node in the tree and the candidate data points to insert. It prioritizes candidate data points with the different label based on their distances to the current node in the tree.\nThe algorithm first computes the DNN transformed representations of training data points and their distances to the boundaries via max-margin. It then sorts these data points according to the ascending order of their distances to the boundaries. The construction of the boundary tree starts from the node with the shortest distance to a boundary. The node is inserted into the tree as the root. A search for k Nearest Neighbors (kNNs) of the newly added node in the tree is then performed. If there is a data point among these neighbors falling into the second closest class of the new node, the data point is selected to insert into the tree. The insertion process traverses the selected data point to its closest node. If the data point has a different label with its closest node\nAlgorithm 1: Boundary Stitching Algorithm Input: R: A list of the DNN transformed representations of\ntraining points and corresponding DNN predictions. Output: The EB-tree T for the DNN model.\n1 Procedure EB-tree Construction() 2 //sort points according to distances to boundaries 3 Q = sortedByDistanceToBoundaries(R) 4 current = Q.removeFirst() 5 T.insert(null, current) // insert root 6 while Q is not empty do 7 child = getCandidate(current,Q,T, k) 8 // traverse T to find the closest node to child 9 parent = findParent(T, child)\n10 if parent.label 6= child.label then 11 T.insert(parent, child) 12 current = child"
    },
    {
      "heading": "13 else",
      "text": ""
    },
    {
      "heading": "14 end",
      "text": ""
    },
    {
      "heading": "15 end",
      "text": "16 Function getCandidate(current,Q, T, k) 17 currentIndex = Q.index(current) 18 //find k nearest candidates for the head of Q by LSH 19 kNearests = LSH[currentIndex/(N\nn )].query(current, k)\n20 if kNearests is not empty then 21 expectedLabel = current.secondClosestClass() 22 foreach n \u2208 kNearests do 23 if n.label == expectedLabel then 24 return n 25 end"
    },
    {
      "heading": "26 end",
      "text": "27 end 28 return Q.removeFirst()\nin the tree, it is inserted as a child of this node; otherwise, the data point is discarded as a similar node belonging to the same class has already been added to the tree. The process continues until all data points are processed.\nFinding k nearest neighbors incurs high computing cost. We use locality sensitive hashing (LSH) (Andoni and Razenshteyn 2015) to reduce the cost. The main idea of LSH is to hash the data points such that two data points close to each other have a higher probability of collision than those far away from each other. LSH can achieve a query time of O(d \u00b7 n\u03c1+o(1)), in which n is the number of data points, d is the data dimension and \u03c1 controls the approximation quality. By using LSH, the complexity of the EB-tree construction process is O(nlogkm + n\n\u03c1+o(1)), where m is the tree size. The cost is trivial compared to the cost of training a DNN model."
    },
    {
      "heading": "Evaluation",
      "text": "We evaluate the effectiveness of EB-tree on two image classification tasks. We use LeNet-5 model (LeCun and others 2015) for MNIST dataset and recurrent convolutional deep neural network (RCNN) (Liang and Hu 2015) as the DNN classifier for CIFAR-10 dataset. We set k in Algorithm 1 to 32 in our experiments. We evaluate our algorithm on both model mimicking performance and the interpretability of the\nconstructed EB-tree. The decision consistency between an EB-tree and its corresponding DNN classifier, or fidelity, is measured by the F-measure of the EB-tree predictions against the DNN model predictions. We measure the interpretability of EB-trees by conducting a human pilot study."
    },
    {
      "heading": "Model Accuracy and Fidelity",
      "text": "As shown by the error rate in Table ??, EB-tree can achieve an accuracy highly similar to that of the DNN classifiers. EB-tree also approximates the DNN classifiers with high fidelity (99.72% with MNIST-LeNet-5 and 99.56% with CIFAR-RCNN). EB-tree significantly outperforms the original boundary tree algorithm in accuracy, fidelity and node number in the tree. Note, for the MNIST dataset, the resulting EB-tree only needs to disclose 102 out of the 60,000 training data points (0.17%) to model users for them to understand the model. For CIFAR-10 dataset, the constructed EB-tree only needs to disclose 119 out of the 50,000 training data points (0.24%) to model users. Further tradeoffs can be made in the data point selection process if each data point is associated with different privacy information.\nEB-tree optimizes its edges to align better with the boundary than the basic boundary tree. The effect is shown in Figure 4. The nodes connected by the same edge in a basic boundary tree are visually more different than those in an EB-tree. The node selection process in EB-tree is able to identify training points with subtle difference to characterize the decision boundaries. When a test data point traverses the tree, these subtle differences give a model user insight about why the test point is classified to a specific class. As an example, Figure 4b shows the decision boundary between some 6/8 and 6/5 pairs which give a user hints about subtle features that differentiate the classes."
    },
    {
      "heading": "Use Cases of EB-tree",
      "text": "Case 1: Explaining Predictions of Individual Data Points.\nUnlike some existing work which tries to give the closest prototype as an explanation for a given test sample, EB-tree gives the traversal path on the tree as an explanation which assists the model users to understand the reasons behind predictions.\nTo evaluate how effective an EB-tree is on helping a user understanding the decision making of a model, we conduct a human pilot study. The study involves 10 users without machine learning research and development experience. We design tasks to measure whether a user gains insight of a model with the help of an EB-tree.\nWe present MNIST EB-tree and CIFAR-10 EB-tree to 10 users. We then randomly choose 100 images with 10 from each class from the test dataset of each model to show users. For each image, we provide 10 traversal paths and ask the participants to choose the one that gives them the most insight to understand the model decisions. The 10 traversal paths consist the following three categories:\n1. The path produced by the EB-tree algorithm; 2. Two paths modified slightly from the EB-tree path; 3. Seven paths generated by random walking on the EB-tree\nstarting from the root. The result is shown in Table 2. The agreement ratio between each user\u2019s choices and EB-tree paths is high, which confirms that EB-tree paths give users better understanding of both MNIST and CIFAR-10 DNN model.\nThe traversal paths and closest nodes given by the EBtree help the model users understand some predictions in depth. Particularly they help model users notice certain features that contribute to the classification of a test data point to a specific label. For example, Figure 5 shows six test images in CIFAR-10 dataset. Among the images, the first three are correctly classified while the last three are incorrectly classified by the DNN classifier. Figure 6 shows the traversal paths of these images on the EB-tree to interpret the predictions. For the correct predictions, the label of the final node on the traversal path is used to make the prediction. In other words, the final node is considered to be more similar to the test data compared to the node\u2019s parent and children. If this \u2019similar\u2019 relationship makes sense to human, the model decision is consistent with human intuition; otherwise, the model decision needs a careful check. We find the traversal paths of incorrect predictions unusual. The airplane in Figure 5d is classified as \u2019bird\u2019, the traversal path explains this by the final node in Figure 6d. In fact, the tail of the airplane makes it similar to a bird with the long tail in the training data. We identify that there lacks similar airplane images in the\ntraining data. The lack of similar training data contributes to the misclassification of the airplane to a bird. Similarly, Figure 5e is a deer while being classified as a \u2019dog\u2019 by the model. Our experiments on other test images also show a partial view of an object often leads to incorrect predictions on the CIFAR dataset. The green airplane in Figure 5f is classified as a \u2019frog\u2019 due to its visually similar color and shape with the frog, as shown in Figure 6f. Apparently, the lack of similar training airplane examples contributes to this misclassification.\nThe results indicate that EB-tree is able to give model users clear hints about why a data point is classified into a particular class. A model user can traverse their own data on the tree to gain insight about the model. For model providers, EB-tree is also helpful for debugging their models.\nCase 2: Visualizing Decision Boundaries. The decision boundaries learned by complex models like DNN classifiers are difficult to \u201csee\u201d by a human without examples. EB-tree\nprovides a means for model users to see a small number of training samples that characterize the boundaries. It is also helpful for improving the model training.\nWe implement an operation called boundary projection on the EB-tree to visualize boundaries. This operation traverses the EB-tree and finds all the edges with two end nodes from each pair of adjacent class. A sequence of these node pairs along a boundary produces a visualization of the boundary.\nBy utilizing the boundary projections, model users can find some hints about subtle differences between two classes. To quantitatively analyze the effect, we present boundaries visualized this way to the 10 human subjects and ask them whether they understand decision making of the DNN model better with these boundaries. Participants may describe their understanding about how a model differentiates two classes in plain text. By looking at the decision boundary between class 2 to class 7 in MNIST dataset, as shown in Figure 7b, 90% of the participants believe that they understand the decision making logic of the model.\nWe further evaluate the observation made by human subjects by examining the classification difference with examples in training data. This is done as follows: It is noticed that the horizontal line at the bottom of a digit should be long enough for the digit to be classified into 2. If the horizontal line of a digit is short at the bottom or is in the middle, the digit is often classified as 7. To verify this observation, we selected several test samples in MNIST dataset to check their predictions. We choose four samples in MNIST dataset (see Figure 8a to 8d). As expected, the first and fourth are classified as 7 while the second and the third are classified as 2. Similarly, Figure 7a shows the boundary between \u2019bird\u2019 and \u2019airplane\u2019 for the CIFAR-10 dataset. One may note that some birds in training data are at the airplane side of the boundary (e.g., the bird at rightmost). This is caused by training errors. Our participants reach a conclusion that\nthe birds with upwards wings are likely to be misclassified as airplanes. To verify, we choose test data with upwards wings in CIFAR-10 dataset (see Figure 8e and 8f) and check their predictions made by the DNN model. Not surprisingly, the DNN classifier classifies them into class \u2019airplane\u2019 incorrectly.\nThe boundary projection also enables identifying incorrectly labeled training data through unexplainable observations. If a training sample of class B is mislabeled as class A, it still shares significant similarity with many samples in class B. The mislabeled sample can be identified near the boundary. As shown in Figure 9a, the label for the 4th sample is \u20193\u2019. However, it is hard for one to guess why it is labeled as \u20195\u2019 rather than \u20193\u2019. Similarly, the 3rd sample at the class 3 side and the 5th sample at the class 5 side both look like a 9 while they are labeled by \u20193\u2019 and \u20195\u2019, respectively. Our investigation afterwards confirms these samples are indeed mislabeled 1. These two samples also appear in class 5 to class 3 boundary (see Figure 9b).\nCase 3: Detecting Emerging New Classes. Detecting emerging new classes is a typical problem in streaming classification as predicting a sample from a new class not seen in training may lead to unexpected results. The challenge is that it requires massive distance computation (Mu et al. 2017; Jordaney et al. 2017) to compare the incoming samples with the existing training samples. EB-tree is able to reduce the computing complexity by only computing distances between a test sample and those training data points that have the same traversal path (and final node) with the test sample on the EB-tree. Formally, for an incoming sample z that reaches node N in EB-tree traversal and its predicted class D, we only need to compare a subset DN that reach nodeN on the EB-tree through the same traversal path to detect new classes. As in (Jordaney et al. 2017), we use conformal evaluation to compute the similarity between z and data points in DN through p-values. The p-value pDNz of an incoming test data point z is calculated as below:\n1http://deepmachinelearning.blogspot.com.au/2016/07/mnistnaughties.html\n\u03b1z = A(DN , z) (4)\n\u2200i \u2208 DN .\u03b1i = A(DN\\zi, zi) (5)\npDNz = |{j : \u03b1j \u2265 \u03b1z}|\n|DN | (6)\nIn which, A is a distance function. The distance between two data points that end their EB-tree traversal at node N is defined as their probability distribution difference among node N , the parent and children of node N . The probability distribution is calculated in a similar way as in (Zoran, Lakshminarayanan, and Blundell 2017). The p-value pDNz for z indicates how different the new data point is from existing data points that share the same traversal path. A low p-value indicates that the prediction on the test data lacks of statistical support for fitting the prediction model.\nTo demonstrate the effectiveness of EB-tree on emerging new class detection, we simulated a data stream with the MNIST dataset. LeNet-5 is used as the DNN classification model. Initially, the model is only trained by data from class 0 to 8. The EB-tree constructed for the DNN model contains 52 nodes. We then mix samples from class 9 in the stream and see whether they can be accurately identified. For a test sample S (see Figure 10a) with true label 9, the sample is misclassified as a \u20194\u2019 because samples of class \u20199\u2019 have not been seen before. We compare all the nodes that reach the node that produces a prediction of \u20194\u2019 on the EB-tree and we get pS = 0.0069. This value indicates the sample is statistically different to previous data points being classified as 4 through the same node and is likely to be from a new class. By setting a threshold of 0.01 for p-value, we correctly identified 991 out of 1000 samples (99.1% accuracy) belonging to class \u20199\u2019 in the stream. Comparing to the existing method (Jordaney et al. 2017) that achieves an accuracy at 99.2%, EB-tree reduces the computation by around 97.1% through only computing the distances between an incoming sample and existing data points ending their EB-tree traversals on the same node. EB-tree provides a unique advantage for detecting emerging new classes and significantly reduces the detection cost."
    },
    {
      "heading": "Conclusion",
      "text": "We presented an Explicable Boundary Tree (EB-tree) to improve the trustworthiness of a shared complicated DNN model. EB-tree used a boundary stitching algorithm to equip the basic boundary tree with interpretability by disclosing a small set of representative training data. A model user gained insight of the decision making of a DNN model via tree traversal. We showed that an EB-tree approximated the corresponding DNN model with high fidelity and improved a model user\u2019s understanding of a complicated model. We also showed an EB-tree was effective on detecting mislabeled training data and emerging new classes in data."
    }
  ],
  "title": "Interpreting Shared Deep Learning Models via Explicable Boundary Trees",
  "year": 2018
}
