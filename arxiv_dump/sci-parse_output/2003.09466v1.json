{"abstractText": "In precision medicine, machine learning techniques have been commonly proposed to aid physicians in early screening of chronic diseases such as Parkinson\u2019s Disease. These automated screening procedures should be interpretable by a clinician who must explain the decision-making process to patients for informed consent. However, the methods which typically achieve the highest level of accuracy given early screening data are complex black box models. In this paper, we provide a novel approach for explaining black box model predictions of Parkinson\u2019s Disease progression that can give high fidelity explanations with lower model complexity. Specifically, we use the Parkinson\u2019s Progression Marker Initiative (PPMI) data set to cluster patients based on the trajectory of their disease progression. This can be used to predict how a patient\u2019s symptoms are likely to develop based on initial screening data. We then develop a black box (random forest) model for predicting which cluster a patient belongs in, along with a method for generating local explainers for these predictions. Our local explainer methodology uses a computationally efficient information filter to include only the most relevant features. We also develop a global explainer methodology and empirically validate its performance on the PPMI data set, showing that our approach may Pareto-dominate existing techniques on the trade-off between fidelity and coverage. Such tools should prove useful for implementing medical screening tools in practice by providing explainer models with high fidelity and significantly less functional complexity.", "authors": [{"affiliations": [], "name": "Qiaomei Li"}, {"affiliations": [], "name": "Rachel Cummings"}, {"affiliations": [], "name": "Yonatan Mintz"}], "id": "SP:a50515f9a96088c2dee935f98a12cbc0382fecaf", "references": [{"authors": ["A. Aswani", "P. Kaminsky", "Y. Mintz", "E. Flowers", "Y. Fukuoka"], "title": "Parkinson\u2019s disease from MRI data", "year": 2016}, {"authors": ["H. Bastani", "O. Bastani", "C. Kim"], "title": "Interpreting predictive models for human-in-the-loop analytics", "venue": "arXiv preprint 1705.08504,", "year": 2018}, {"authors": ["S. Bhat", "U.R. Acharya", "Y. Hagiwara", "N. Dadmehr", "H. Adeli"], "title": "Parkinson\u2019s disease: Cause factors, measurable indicators, and early diagnosis", "venue": "Computers in Biology and Medicine,", "year": 2018}, {"authors": ["L. Breiman"], "title": "Statistical modeling: The two cultures (with comments and a rejoinder by the author)", "venue": "Statistical Science,", "year": 2001}, {"authors": ["G. Brown", "A. Pocock", "M.-J. Zhao", "M. Luj\u00e1n"], "title": "Conditional likelihood maximisation: A unifying framework for information theoretic feature selection", "venue": "Journal of Machine Learning Research,", "year": 2012}, {"authors": ["D. Castillo-Barnes", "J. Ram\u0131\u0301rez", "F. Segovia", "F.J. Mart\u0301\u0131nez-Murcia", "D. Salas-Gonzalez", "J.M. Gorriz"], "title": "Robust ensemble classification methodology for I123-ioflupane SPECT images and multiple heterogeneous biomarkers in the diagnosis of Parkinson\u2019s disease", "venue": "Frontiers in Neuroinformatics,", "year": 2018}, {"authors": ["R. Erro", "C. Vitale", "M. Amboni", "M. Picillo", "M. Moccia", "K. Longo", "G. Santangelo", "A. De Rosa", "R. Allocca", "F. Giordano"], "title": "The heterogeneity of early Parkinson\u2019s disease: A cluster analysis on newly diagnosed untreated patients", "venue": "PLoS ONE,", "year": 2013}, {"authors": ["S.-M. Fereshtehnejad", "R.B. Postuma"], "title": "Subtypes of Parkinson\u2019s disease: What do they tell us about disease progression", "venue": "Current neurology and neuroscience reports,", "year": 2017}, {"authors": ["S.-M. Fereshtehnejad", "S.R. Romenets", "J.B. Anang", "V. Latreille", "J.-F. Gagnon", "R.B. Postuma"], "title": "New clinical subtypes of Parkinson disease and their longitudinal progression: A prospective cohort comparison with other phenotypes", "venue": "JAMA Neurology,", "year": 2015}, {"authors": ["S.-M. Fereshtehnejad", "Y. Zeighami", "A. Dagher", "R.B. Postuma"], "title": "Clinical criteria for subtyping Parkinson\u2019s disease: Biomarkers and longitudinal progression", "year": 1959}, {"authors": ["L.J. Findley"], "title": "The economic impact of Parkinson\u2019s disease", "venue": "Parkinsonism & Related Disorders,", "year": 2007}, {"authors": ["J. Friedman", "T. Hastie", "R. Tibshirani"], "title": "The Elements of Statistical Learning: Data Mining, Inference, and Prediction", "year": 2001}, {"authors": ["J.M. Graham", "H.J. Sagar"], "title": "A data-driven approach to the study of heterogeneity in idiopathic Parkinson\u2019s disease: Identification of three distinct subtypes", "venue": "Movement Disorders,", "year": 1999}, {"authors": ["T.J. Hirschauer", "H. Adeli", "J.A. Buford"], "title": "Computer-aided diagnosis of Parkinson\u2019s disease using enhanced probabilistic neural network", "venue": "Journal of Medical Systems,", "year": 2015}, {"authors": ["S.L. Kowal", "T.M. Dall", "R. Chakrabarti", "M.V. Storm", "A. Jain"], "title": "The current and projected economic burden of Parkinson\u2019s disease in the United States", "venue": "Movement Disorders,", "year": 2013}, {"authors": ["H. Lakkaraju", "S.H. Bach", "J. Leskovec"], "title": "Interpretable decision sets: A joint framework for description and prediction", "venue": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201916,", "year": 2016}, {"authors": ["J.C. Latourelle", "M.T. Beste", "T.C. Hadzi", "R.E. Miller", "J.N. Oppenheim", "M.P. Valko", "D.M. Wuest", "B.W. Church", "I.G. Khalil", "B. Hayete"], "title": "Large-scale identification of clinical and genetic predictors of motor progression in patients with newly diagnosed Parkinson\u2019s disease: A longitudinal cohort study and validation", "venue": "The Lancet Neurology,", "year": 2017}, {"authors": ["L. v. d. Maaten", "G. Hinton"], "title": "Visualizing data using t-SNE", "venue": "Journal of Machine Learning Research,", "year": 2008}, {"authors": ["P. Mart\u0301\u0131nez-Mart\u0301\u0131n", "A. Gil-Nagel", "L.M. Gracia", "J.B. G\u00f3mez", "J. Martinez-Sarries", "F. Bermejo", "C.M. Group"], "title": "Unified Parkinson\u2019s disease rating scale characteristics and structure", "venue": "Movement Disorders,", "year": 1994}, {"authors": ["P. Martinez-Martin", "C. Rodriguez-Blazquez", "M.J. Forjaz"], "title": "Rating scales in movement disorders", "year": 2017}, {"authors": ["Z.S. Nasreddine", "N.A. Phillips", "V. B\u00e9dirian", "S. Charbonneau", "V. Whitehead", "I. Collin", "J.L. Cummings", "H. Chertkow"], "title": "The montreal cognitive assessment, MoCA: A brief screening tool for mild cognitive impairment", "venue": "Journal of the American Geriatrics Society,", "year": 2005}, {"authors": ["F.L. Pagan"], "title": "Improving outcomes through early diagnosis of Parkinson\u2019s disease", "venue": "The American Journal of Managed Care,", "year": 2012}, {"authors": ["F. Pedregosa", "G. Varoquaux", "A. Gramfort", "V. Michel", "B. Thirion", "O. Grisel", "M. Blondel", "P. Prettenhofer", "R. Weiss", "V. Dubourg"], "title": "Scikit-learn: Machine learning in Python", "venue": "Journal of Machine Learning Research,", "year": 2011}, {"authors": ["B. Peng", "S. Wang", "Z. Zhou", "Y. Liu", "B. Tong", "T. Zhang", "Y. Dai"], "title": "A multilevel-ROI-features-based machine learning method for detection of morphometric biomarkers in Parkinson\u2019s disease", "venue": "Neuroscience Letters,", "year": 2017}, {"authors": ["R. Prashanth", "S.D. Roy", "P.K. Mandal", "S. Ghosh"], "title": "High-accuracy detection of early Parkinson\u2019s disease through multimodal features and machine learning", "venue": "International Journal of Medical Informatics,", "year": 2016}, {"authors": ["S.S. Rao", "L.A. Hofmann", "A. Shakil"], "title": "Parkinson\u2019s disease: Diagnosis and treatment", "venue": "American Family Physician,", "year": 2006}, {"authors": ["M.T. Ribeiro", "S. Singh", "C. Guestrin"], "title": "Why should I trust you?\u201d: Explaining the predictions of any classifier", "venue": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD \u201916,", "year": 2016}, {"authors": ["M.T. Ribeiro", "S. Singh", "C. Guestrin"], "title": "Anchors: High-precision model-agnostic explanations", "venue": "In Proceedings of the 32nd AAAI Conference on Artificial Intelligence,", "year": 2018}, {"authors": ["A. Rossi", "K. Berger", "H. Chen", "D. Leslie", "R.B. Mailman", "X. Huang"], "title": "Projection of the prevalence of Parkinson\u2019s disease in the coming decades: Revisited", "venue": "Movement Disorders,", "year": 2018}, {"authors": ["A. Siderowf"], "title": "Schwab and England activities of daily living scale, pages 99\u2013100", "year": 2010}, {"authors": ["G. Singh", "L. Samavedham", "E.C.-H. Lim", "A.D.N. Initiative", "P.P.M. Initiative"], "title": "Determination of imaging biomarkers to decipher disease trajectories and differential diagnosis of neurodegenerative diseases (DIsease TreND)", "venue": "Journal of Neuroscience Methods,", "year": 2018}, {"authors": ["B. Ustun", "C. Rudin"], "title": "Supersparse linear integer models for optimized medical scoring systems", "venue": "Machine Learning,", "year": 2016}, {"authors": ["F. Wang", "C. Rudin"], "title": "Falling rule lists", "venue": "In Proceedings of the 18th International Conference on Artificial Intelligence and Statistics,", "year": 2015}], "sections": [{"heading": "1 Introduction", "text": "In precision medicine, machine learning techniques have been commonly proposed to aid physicians in early screening of chronic diseases. Many of these diseases become more difficult to treat as they progress, so accurate early screening is critical to ensure resources are directed towards the most effective treatment plan [Pagan, 2012]. Since the final treatment decision must inevitably be made by a doctor, these screening procedures should be interpretable such that a clinician can explain the decision-making process to patients for informed consent. However, the types of models that achieve the highest level of accuracy given early screening data tend to be extremely complex, meaning that even machine learning experts have difficulties explaining why certain predictions are made, leading many to describe them as \u201cblack box\u201d [Breiman, 2001]. In this paper, we bridge this gap by providing a novel approach for explaining black box model predictions which can give high fidelity explanations with lower model complexity.\nIn particular we will focus on early screening of Parkinson\u2019s Disease (PD). PD is a complicated neurodegenerative disorder that affects the central nervous system and specifically the motor control of individuals [mjf, 2019]. This disorder is estimated to affect 930,000 individuals in the US by 2020, and is more prevalent in the geriatric population affecting more then 1% of the population over the age of 60 and 5% of the population over age 85 [Findley, 2007, Kowal et al., 2013, Rossi et al., 2018]. These statistics and other recent studies on Parkinson\u2019s epidemiology indicate that as the population ages, the prevalence of PD is expected to grow to over 1.2 million by 2030 in the US alone, increasing the total economic burden of the disorder to approximately $26 billion USD [Kowal et al., 2013, Rossi et al., 2018]. One of the most challenging\n\u2217H. Milton Stewart School of Industrial and Systems Engineering, Georgia Institute of Technology. Emails: {qli374, rachelc,ymintz}@gatech.edu. Part of this work was completed while R.C. was a Google Research Fellow at the Simons Institute for the Theory of Computing.\nar X\niv :2\n00 3.\n09 46\n6v 1\n[ cs\n.L G\n] 2\n0 M\nar 2\n02 0\naspects of PD research is that there is still no clear consensus on the root cause, and whether it is a single disease or a group of diseases characterized by similar symptoms known as Parkinsonism [Rao et al., 2006]. Since the disorder manifests differently between individuals (with different primary symptoms expressed across different patients) [Rao et al., 2006, Fereshtehnejad et al., 2017, Fereshtehnejad and Postuma, 2017], studying sub-categorization of PD disease progression has been of great interest in the medical community, particularly using novel advances in data-driven and statistical methods.\nIn this paper, we will use data from the Parkinson\u2019s Progression Marker Initiative (PPMI) [PPM] to develop and analyze a method for classifying patients based on their disease progression, and to provide data-driven PD sub-types. We develop a model that can be used with screening measurements to predict how a potential patient\u2019s symptoms are likely to develop over the course of the following two years. Our resulting five sub-types correlate well with known primary PD symptoms and have clear medical interpretations. We then develop a random forest model which can accurately predict which of these sub-types a patient should be classified into, given common screening data. Since this model is a black box, we additionally develop a method for generating local explainers for each prediction. Our local explainer methodology uses a computationally efficient information filter to include only the most relevant features to explain a given prediction, resulting in a methodology we believe useful for implementing such screening tools in practice by providing explainer models with high fidelity and significantly less functional complexity. We then develop a global explainer methodology by aggregating local explainers. We use an Integer Programming based approach to determine which local explainers to include in our global explainer. Our global explainer must trade off between coverage, fidelity of predictions, and interpretability. We show that it is on the Pareto frontier of this trade-off space, relative to existing methods. Additionally, many other global explainers are constrained to have perfect coverage, while our method has an additional degree of freedom, which allows for improvements in fidelity and interpretability.\nThe remainder of the paper will proceed as follows. A discussion of related literature and previous work connected to interpretable machine learning and PD diagnosis is given next in Section 1.1. We will discuss our data driven cluster analysis for determining PD progression sub-types in Section 2. Then in Section 3 we will discuss our local explainer methodology and provide with numerical validation of this methodology in Section 4. In Section 5 we show how to extend this local explainer framework into a global explainer using an Integer Programming (IP) framework, and in Section 6, we provide empirical validation and compare the performance of our IP-based approach with other local and global explainer methods."}, {"heading": "1.1 Related Work", "text": "Due to the prevalence and complexity of PD, there has been a significant amount of literature focused on using data-driven methods and machine learning to assist with diagnoses. Several diagnosis methods have been proposed including those that use classical ML models such as kernel SVMs [Prashanth et al., 2016], ensemble models [Latourelle et al., 2017, Castillo-Barnes et al., 2018], and both supervised and unsupervised deep learning methods [Hirschauer et al., 2015, Adeli et al., 2016, Peng et al., 2017, Singh et al., 2018]. The classical and ensemble methods have typically focused on lab tests and genetic markers, while the deep learning methods were developed to incorporate MRI imaging into these predictions. The majority of this work focuses on binary diagnoses, labeling individuals as either healthy or having PD, but do not give information on disease progression or disease sub-types. Also, most of the proposed methods\u2014particularly the ensemble and deep learning methods\u2014are difficult to interpret. For example, they may identify a region of interest in an MRI image or highlight certain genetic markers, but it is difficult to explain to clinicians or patients why these regions are important for the model\u2019s final decision. In contrast, our model is meant to predict the disease progression of individuals based on early screening data. To ensure interpretability, we introduce a local and global explainer techniques so that proper and clear rationale can be given to classifications.\nIn addition to the work on diagnoses, there has also been significant research into the use of data-driven methods for PD sub-type identification [Graham and Sagar, 1999, Erro et al., 2013, Fereshtehnejad et al., 2015, Fereshtehnejad and Postuma, 2017]. The majority of analyses that fall in this stream of literature focuses on using unsupervised methods such as k-means clustering to create patient sub-types based on\nscreening data, and then track the importance of the clustering based on longitudinal progression observations. In contrast, our model will first cluster patient types based on the dynamic behavior of disease progression, and then attempt to predict these clusters using screening data. We believe this approach will be useful in identifying the most effective course of treatment by directly treating the primary symptoms that develop in each progression cluster.\nOur paper also draws on previous work in the broader field of interpretable machine learning. The two primary types of interpretable learning include models that are interpretable by design [Aswani et al., 2019], and black box models that can be explained using global explainer [Wang and Rudin, 2015, Lakkaraju et al., 2016, Ustun and Rudin, 2016, Bastani et al., 2018] and local explainer [Ribeiro et al., 2016, 2018] methods. Models that are interpretable by design are perhaps the gold standard for interpretable ML; however, these models often require significant domain knowledge to formulate and train, and are therefore not suited for exploratory tasks such as PD diagnosis. Global explainer methodology attempts to train an explainable model (such as a decision tree with minimal branching) in order to match the predictions of a black box model across the entirety of its feature space. While these models may provide some understanding on the general behavior of the black box model, if the relationship between features and black-box predictions is too complex, then the global explainer may remove many subtleties that are vital for validation and explanation. In contrast, local explainer methods attempt to train simpler models centered around a the prediction of a single data point. The most commonly used local explainer methods are Local Interpretable Model-Agnostic Explanations (LIME) [Ribeiro et al., 2016] and anchors [Ribeiro et al., 2018]. While local methods cannot validate the full black box model, they are useful for understanding the subtleties and justification for particular predictions. The method we propose in this paper follows from the idea of local explanations. We then aggregate these local explainers into a global explainer, trading off between coverage of the global model and fidelity of the local explainers that comprise our global model. We believe this method is most appropriate for the problem of PD diagnosis, where the relationship between different screening measures and the diagnosis is quite complex, and the model should incorporate the richness of this relationship in its predictions."}, {"heading": "2 Clustering Methodology and PPMI Dataset", "text": "PD is a complex disorder, and is often expressed differently by different patients, which has motivated the need to create PD sub-types to better direct treatment. While many existing data-driven methods focus on clustering patients based on their baseline measurements [Fereshtehnejad and Postuma, 2017], we propose clustering patients using the trajectory of how their symptoms progress.\nWe will use data collected in the PPMI study [PPM], which is a long run observational clinical study designed to verify progression markers for PD. To achieve this aim, the study collected data from multiple sites and includes lab test data, imaging data, genetic data, among other potentially relevant features for tracking PD progression. The study includes measurements of all these various values for the participants across 8 years at regularly scheduled follow up appointments. The complete data set contains information on 779 patients, and included 548 patients diagnosed with PD or some other kind of Parkinsonism and 231 healthy individuals as a control group."}, {"heading": "2.1 Determination of Criterion and Cluster Analysis", "text": "Since there is significant heterogeneity in how PD symptoms are expressed, there also is no agreement on a single severity score or measurement that can be used as a surrogate for PD progression. Thus instead of considering a single score, we will model the severity of the disease as a multivariate vector, and the disease progression as the trajectory of this vector through a multidimensional space. Using the PPMI data [PPM] and other previous literature on PD progression [Rao et al., 2006, Martinez-Martin et al., 2017, Bhat et al., 2018], we considered the following measures of severity to model disease progression:\n\u2022 Unified Parkinson\u2019s Disease Rating Scale (UPDRS) II & III [Mart\u0301\u0131nez-Mart\u0301\u0131n et al., 1994]: The UPDRS is a questionnaire assessment that is commonly used to track symptoms of PD by an observer. It\nconsists of four major sections, each meant to measure a different aspect of the disease. These sections are: (I) Mentation Behavior and Mood, which includes questions related to depression and cognitive impairment; (II) Activities for Daily Living, which includes questions related to simple daily actions such as hygiene and using tools; (III) Motor Examination, which includes questions related to tremors and other physical ticks; and (IV) Complications of Therapy, which attempts to assess any adverse affects of receiving treatment. For our analysis we focused on the aggregate scores of sections II and III of the UPDRS to track physical symptoms of the disease.\n\u2022 Montreal Cognitive Assessment (MoCA) [Nasreddine et al., 2005]: Although not exclusively used for PD, the MoCA is a commonly used assessment for determining cognitive impairment and includes sections related to attention, executive functions, visual reasoning, and language. For our analysis, we used the MoCA scores of the individual patients as surrogates for their cognitive symptoms.\n\u2022 Modified Schwab and England Activities of Daily Living Scale (MSES) [Siderowf, 2010]: The MSES is a metric used to measure the difficulties that individuals face when trying to complete daily chores due to motor deficiencies. This assessment is generally administered at the same time as the UPDRS and is often appended as a section V or VI. We used this score as a measure of how much autonomy the patients experience based on their symptoms.\nWe formed the empirical trajectory of these scores for each patient using the values measured during the patients\u2019 participation in the PPMI study [PPM]. For our cluster analysis we used longitudinal measurements that were taken across the first seven visits of the study corresponding to a period of 21 months, where the first measurement formed the patient\u2019s baseline, and the next five measurements were taken at follow up visits at regular three month intervals; the final measurements were taken after six months. We chose this timeline for our analysis because participation was high among all participants in the study during this period, so we did not have to exclude any patients, and visits were more frequent to better capture disease progression over time. After these seven measurements, follow-up visits were scheduled too infrequently to provide useful trajectory modeling information.\nWe used these trajectories to cluster the patients together into progression sub-types. The main motivation for this approach is that if patients\u2019 severity scores progress in a similar way, then it may identify a useful sub-type for treatment design. Only patients diagnosed with PD were included in the cluster analysis, since we are interested in finding useful sub-types of disease progression. Each trajectory was then flattened out as a 28 dimensional vector, with the first four entries corresponding the measurements at baseline, the next four for the 3 month follow up, and so on. Using scikit-learn and Python 3.7, we performed k-means clustering on these trajectories to define our sub-types [Pedregosa et al., 2011, Friedman et al., 2001]. Using cross validation and the elbow method (as seen in Figure 8 in the appendix), we determined that there are four potential sub-types of disease progressions for the PPMI participants. We label these as: moderate physical symptoms cognitive decline cluster (Group 0), stagnant motor symptoms autonomy decline cluster (Group 1), motor symptom dominant cluster (Group 2), and moderate symptoms cluster (Group 3). The names we assigned to each individual cluster were given by the observed mean trajectories of the relevant scores for individuals that were classified into a particular cluster as shown in Figure 1.\nIn Figure 2 we show two 2-dimensional projections of the different cluster groups. Figure 2a shows the projection onto the first two principal components of the data using PCA [Friedman et al., 2001]; this projection method is meant to preserve linear relationships among data points as well as distances between data points that are far apart. The projection shown in Figure 2b corresponds to the tSNE projection of the data onto a two-dimensional space [Maaten and Hinton, 2008], this projection method was designed with manifolds in mind and is meant to preserve close distances (i.e., data points close in the tSNE projection should be also close in the higher dimensional space). Note that in both projections our resulting clusters are distinct and do not significantly overlap."}, {"heading": "2.2 Validation of Clusters", "text": "To test whether these clustered sub-types provide additional insight into the health of the patients, we performed several statistical comparisons of each patients\u2019 characteristics at baseline across all four subtypes plus healthy patients, to determine if there were any statistically significant differences. The results and values of these comparisons are presented in Table 1 below.\nAs seen in Table 1, many of the key screening measurements of the populations from the different clusters are significantly different, implying our clusters are informative about the health of individuals. In particular, we note that Group 0\u2014which corresponds to moderate physical symptoms with cognitive decline\u2014tends to be younger on average then the other groups, indicating this group may contain many more individuals with early onset PD. Moreover, the sub-types vary substantially in their sleep score and olfactory evaluation,\nwhich are both measures that have previously been shown to be strong indicators of PD [Rao et al., 2006] indicating that these progression sub-types are sensitive to these important predictors.\nOverall, the comparisons shown in Table 1 show that our data driven clusters are not only informative when comparing different forms of disease progression, but also correspond to variations in screening measurements. Based on this analysis, we believe that using screening data to predict these clusters could lead to clinically significant insights that can help with treatment."}, {"heading": "3 Local Explainer Algorithm", "text": "After identifying the four disease progression sub-types, we would like to predict which kind of disease progression an individual might experience, given measurements collected during a screening visit. As we will show in our experiments in Section 4, this task is best performed by complex black box models such as artificial neural networks (ANN) and bagged forests. This means that while the prediction may be accurate, it will not be easily explained, which make such models difficult to use for diagnosis recommendations. Our goal is to instead develop a method that trains simple auxiliary explainer models, and can still accurately describe the relationship between the data and the model output within a small region of a given prediction.\nThis methodology is known as training local explainer models and has been shown to be useful in understanding black box predictions [Ribeiro et al., 2016, 2018]. One of the key tradeoffs in generating model explanations is that of fidelity\u2014how well the explainer approximates the black box model\u2014and interpretability\u2014how easy it is for a practitioner to trace the predictions of the model. In contrast to previous literature which has proposed the use of regularization to achieve this goal, we propose directly computing locally significant features using an information filter. Generally, computing such filters can be computationally expensive and requires the use of numerical integration; however, one of our main contributions in this paper\nis to introduce an efficient algorithm for filtering out less significant features. This methodology will allow us to train local explainers that are significantly less complex then those that use regularization, with better fidelity."}, {"heading": "3.1 Notation", "text": "Before proceeding to our discussion on the local explainer method, we will first establish some technical notation. We assume that for each patient i = 1, ..., n we have an ordered pair (xi, yi), where xi \u2208 X \u2286 Rm are the features values of the patient and yi \u2208 L \u2286 Z is the corresponding class label generated by a black box model f . Through our analysis we will also refer to this set of points through matrix notation where X \u2208 Xn \u2286 Rm\u00d7n is the feature value matrix and y \u2208 Ln \u2286 Zn is the vector of class labels, where each row in these matrices corresponds to a single patient\u2019s data. For our analysis we assume that X is a compact set. Let \u03a6 = {1, ...,m} be the set of features, and it may also be used to denote the index set of the features. This set can be partitioned into two sets \u03a6c,\u03a6b \u2286 \u03a6 that represent the set of continuous and binary features respectively.\nFurthermore we define the set-valued function \u03a6\u2217 : X \u2192 \u03a6 as the function which extracts the minimum set of necessary features to accurately predict the class of a point x. Namely,\n\u03a6\u2217(x) = arg min \u03d5\u2286\u03a6 {|\u03d5| : p(y|x) = p(y|x[\u03d5])}, (1)\nwhere x[\u03d5] is an indexing operation that maintains the values of x but only for the features in \u03d5, and p is the conditional probability mass function of the labels y given the observation of some features. Specifically, if a feature index is not included \u03a6\u2217(x), then it is not required to understand the particular label of x. In addition, we will denote the ball around a point x of radius r with respect to a metric d as B(x, r, d).\nFinally, a key feature of the explainer training method we propose includes the use of mutual information. In information theory, mutual information is a quantity that measures how correlated two random variables are with one another. If X,Y are two random variables with joint density p and marginal densities px, py, then the mutual information between X and Y is denoted I(X;Y ) and calculated as:\nI(x; y) = E log p(X,Y )\npx(X), py(Y ) = \u222b x \u222b y p(x, y) log p(x, y) px(x), py(y) dxdy. (2)\nIf X and Y are independent then I(X;Y ) = 0; otherwise I(X;Y ) > 0, meaning that X contains some information about Y . A similar quantity can be computed using a conditional distribution on another random variable Z, known as the conditional mutual information and denoted I(X;Y |Z)."}, {"heading": "3.2 Local Explainer Algorithm Description", "text": "Our main local explainer algorithm extends previous local explainer methods such as LIME [Ribeiro et al., 2016] by restricting the sampling region around the prediction, and including an information filter to ensure that fewer features are included in the final explainer mode.\nOur general local explainer is formally presented in Algorithm 1, but we will give a brief overview of its operations here. The algorithm takes in hyper-parameters including number of points N to be sampled for training the explainer, a distance metric d, and a radius r around the point x\u0304 being explained. First the algorithm samples N points uniformly from within a r radius of x\u0304; we call this set of points T (x\u0304). Depending on the distance metric being used this can often be done quite efficiently, especially if the features are binary valued or an `p metric is used [Barthe et al., 2005]. Then using the sampled points, the algorithm uses the Fast Forward Feature Selection (FFFS) algorithm as a subroutine (formally presented in Section 3.3 and Appendix A), which uses an information filter to remove unnecessary features and reduce the complexity of the explainer model. The FFFS algorithm uses an estimate of the joint empirical distribution of (T (x\u0304), f(T (x\u0304)) to select the most important features for explaining the model\u2019s predictions in the given neighborhood. We denote this set of features \u03a6\u0302. Then, using these features and the selected points, the explainer model g is trained\nby minimizing an appropriate loss function that attempts to match its predictions to those of the black box model. In principle a regularization term can be added to the training loss of explainer g. However, through our empirical experiments in Section 4 we found that FFFS typically selected at most five features, so even the unregularized models where not overly complex.\nAlgorithm 1 Local Explainer Training Algorithm\nRequire: sampling radius r, number of sample points N , black box model f , data point to be explained x\u0304, and loss function L for the explainer model (x\u0304, y\u0304)\n1: Initialize T (x\u0304) = \u2205 2: for j = {1, ..., N} do 3: Sample x \u223c U(B(x\u0304, r, d)) 4: T (x\u0304)\u2190 T (x\u0304) \u222a x 5: end for 6: Obtain \u03a6\u0302(x\u0304) = FFFS(T (x\u0304),\u03a6, f) 7: Train g = arg ming\u0302\u2208G{ \u2211 x\u2208T (x\u0304) L(f(x)\u2212 g\u0302(x[\u03a6\u0302]))} 8: return g"}, {"heading": "3.3 Fast Forward Selection Information Filter", "text": "A key step in our algorithm is the use of a mutual information filter to reduce the number of features that will be included in the training of the local explainer. Mutual information filters are commonly used in various signal processing and machine learning applications to assist in feature selection [Brown et al., 2012]. However, these filters can be quite challenging to compute depending on the structure of the joint density function of the features and labels, and can require the use of (computationally expensive) numerical integration. We counteract this by considering an approximation of the density function, using histograms to calculate continuous features. When multiple combinations of features need to be considered as in our setting, the problem of finding the maximum-information minimum-sized feature set is known to be computationally infeasible [Brown et al., 2012]. As such, our proposed method for computing the filter includes a common heuristic known as forward selection, which essentially chooses the next best feature to be included in the selected feature pool in a greedy manner. Using this method alone would still require recomputing the conditional distribution of the data based on previously selected features, which can result in long run times for large N . However, using some prepossessing techniques, we can show that these quantities can be stored efficiently using a tree structure, which allows quick computation of the filter.\nThe general idea of the FFFS algorithm is to consider the feature selection process as a tree construction. Part of this construction relies on an estimate of the empirical density of the features as a histogram with at most B bins and preprocessed summary tensor M \u2208 {0, 1}B\u00d7|\u03a6|\u00d7N which indicates which bin of the histogram a feature value for a particular data point lays in. For each entry, M [b, \u03d5, x] = 1 if the value of feature \u03d5 at point x falls in the bin b. Otherwise, M [b, \u03d5, x] = 0. The depth of the tree represents the number of selected features and each node of the tree is a subset of T (x\u0304). For instance, at the beginning of the selection process, we have a tree with exactly one node R where R = T (x\u0304). Assume binary feature \u03d51 is selected in the first round. Then two nodes a, b are added under R, where a = {xj : M [1, \u03d51, j] = 1} and b = {xj : M(2, \u03d51, j) = 1}. In the second round, we use the partition sets a, b to compute the mutual information instead of the complete set R. The set a is used for computing p\u0302(\u03d5|\u03d51 = 1), p\u0302(y|\u03d51 = 1), and p\u0302(\u03d5; y|\u03d51 = 1), while b is used when the condition is \u03d51 = 2. In each round the leaves L of the current tree represent the set of partition sets corresponding to all random permutation of selected features information. Therefore, L provides us sufficient information for calculating the desired mutual information. As shown in Algorithm 4, the algorithm only outputs the leaves L, not the entire tree. The main algorithmic challenge is to efficiently calculate the marginal distributions (p\u0302(\u03d5|S), p\u0302(y|S) and joint distribution p\u0302(\u03d5; y|S), which we are able to do using the tree structure.\nThe detailed structure of the FFFS algorithm used to compute the filtered feature set \u03a6\u0302 requires several\nsubroutines, and the formal algorithmic construction for computing the filter is presented across Algorithms 2, 3, 4, and 5. The main FFFS algorithm is Algorithm 2, and it calls the subroutines for recursion (Algorithm 3), selecting features (Algorithm 4), and partitions (Algorithms 5). Formal presentation of these algorithms, as well as detailed descriptions, are given in Appendix A."}, {"heading": "4 Experimental Validation of Local Explainer", "text": "In this section we empirically evaluate the quality of our local explainer methodology by first showing that accurate sub-type predictions of our PD sub-type clusters (as described in Section 2) can be achieved using black-box methods applied to the data of individuals measured during the screening visit. We then apply our local explainer methodology developed in Section 3 to explain the predictions given by these black-box models.\nOur clusters were derived from longitudinal measurements of the four metrics of disease severity described in Section 2.1, measured across the first seven visits in the study over a period of 21 months. Treating these cluster (and the healthy patients) as our ground truth class labels, we first train black box machine learning models to predict which of these progression sub-types an individual will most likely experience given her screening data. This is meant to model the data available to a physician when she must make treatment decisions for a new patient. From screening data in the PPMI data set, we included the following 31 features: PTT, Lymphocytes, Hematocrit, Eyes, Psychiatric, Head-Neck-Lymphatic, Musculoskeletal, Sleep Score, Education Years, Geriatric Depression Score, Left Handed, Right Handed, Gender Male, Female Childbearing, Race White, Race Hispanic, Race American Indian, Race Asian, Race Black, Race PI, Anosmia, Hyponosmia, Normosmia, MRI Normal, MRI Abnormal Insignificant, MRI Abnormal Significant, BL/SC UPDRSII, BL/SC UPDRSIII, BL/SC MOCA, BL/SC MSES, and BL/SC Age. Among these 31 features, 20 features are binary variables and 11 features are continuous variables.\nFor accurate sub-type predictions using this data, in Section 4.1 we trained three machine learning prediction models: one interpretable model (logistic regression) and two complex black box models (a feed forward ANN and a bagged forest). Our results indicate that the black box models outperform the simpler model, which necessitates the use of a local explainer method for this application to achieve both accurate classification and explainability.\nIn Section 4.2 we computed local explanations based on the random forest model predictions (which was the model with the highest accuracy) using our proposed FFFS method with the information filter and a local explainer method. This is analogous to LIME [Ribeiro et al., 2016] which does not contain an information filter. Our results show that given a requirement of high explainer fidelity, the use of the information filter will result in less complex explainer models. All experiments described in this section were run on a laptop computer with a 1.2GHz Intel Core m3-7Y32 processor and MATLAB version R2019a with the machine learning and deep learning tool kits [MATLAB, 2010]."}, {"heading": "4.1 Machine Learning Models for Cluster Prediction", "text": "We considered three different kinds of machine learning models for the task of predicting the progression cluster: logistic regression, feed forward ANN, and a bagged forest model. The patient data was split into training, validation, and testing sets with 70% of the data used for training, 15% for validation, and 15% for testing. Among 779 patients, 545 patients were selected for training, and 117 patients were selected for validation and testing.\nSince bagged forests and ANNs are sensitive to hyperparamter settings, we used cross-validation to set their respective hyperparamters. Using cross validation and MATLAB\u2019s hyperparemeter optimization methods we found that the most effective ANN architecture for our task was with a single hidden layer containing one hundred hidden ReLu units. For the random forest model, we found that an ensemble of 50 bagged trees gave the best results compared to other forest sizes.\nFigures 3 and 4 show the performance of the models on the same training, validation, and testing sets. In both figures, the classes 1-4 correspond to Groups 0-3, and class 5 corresponds to healthy patients (which\nwe will also call Group 4). Figure 3 contains the confusion matrix for each model. The rows of the matrix are the output class, which represents the predicted class, and the columns of the matrix are the target class, which is the true class. The cells on the diagonal of the matrix count accurate predictions. Each cell in the rightmost column has two values: the top number is the percentage of patients that are correctly predicted to each class, and the bottom number is the percentage of patients that are incorrectly predicted to each class. For each cell on the bottom row, the top number is the percentage of patients that belong to each class and is correctly predicted, and the bottom number is the percentage of patients that are incorrectly predicted. For the rest of cells in the matrix, the number in each cell counts for the number of patients that fall in this observation. The cell at the bottom right corner of each matrix shows the total percentage of patients that were correctly and incorrectly predicted.\nAs shown in Figures 3 and 4, the logistic regression model under-performs relative to the ANN and bagged forest models. Even though the bagged forest model has a lower prediction rate for Group 0 compared to the ANN, it has equal or higher rates of accurate prediction for the other classes. Additionally, the bagged forest model consistently performed better than the ANN and logistic regression models in our experiments. We concluded from these results that the bagged forest classification model is the most effective for our prediction task, and we chose to consider its predictions when evaluating our local explainer method."}, {"heading": "4.2 Local Explainer Validation", "text": "Since the main difference between our local explainer training algorithm and those in the literature is our use of the FFFS information filter, our experiments on the local explainer are focused on validating the effectiveness of using this information filter. We compare the performance of our local explainer training algorithm to a similar algorithm without a filtering step. We then compare the performance of these methods in terms of explainer complexity and fidelity, across different sampling radii and across all patients.\nFor the sampling parameters of our algorithm, we sampled N = 10, 000 points centered around each patient within a radius r of either 3, 7, 11, or 15. The distance metric for computing this radius was a combination of the `\u221e norm for the continuous features and the `1 norm for the binary features. The continuous value feature of each of the points was sampled uniformly using standard techniques (Barthe et al. [2005]). For binary valued features, we randomly chose at most r binary features and flipped their values. We first randomly generated an integer k between 0 and r, and randomly selected k binary features which we then flipped from their current value (that is, values of 1 were set to 0 and vice versa). To compute probability density estimates, we found that the method performed well with histograms with only three bins for continuous features and two bins for binary features. Intuitively three bins allows us to categorize feature values as low, medium, or high relative to their range.\nFor both training methods, we chose to train decision trees as our the local explainer class because these have been shown to be ergonomically suitable for explaining black box models in healthcare contexts [Bastani et al., 2018]. Then we computed the corresponding fidelity score, defined as the percentage of data where the prediction of the decision tree matched the prediction of the random forest model. We used the number of leaves on the decision tree as a measure of the explainer complexity.\nIn Figure 5, we compare the explainer complexity and fidelity level of the explainers generated by the two different training methodology across the four different tested sampling radii. Unsurprisingly, when the sampling radius is small (i.e., r = 3), there is not much advantage to using the information filter in terms of reducing model complexity for a given fidelity level. Since all points are sampled so closely together, the relevant features are easily learned in explainer training. Conversely, when the sampling radius is large (r = 15), the addition of the information filter only helps slightly. With such a large radius, sampling feature values that are far from the point that is meant to be explained may not give useful information for that prediction. However, when considering the medium radius ranges, for high levels of fidelity, the inclusion of the information filter provides simpler models across the board. In particular, consider the plots corresponding for local explainer radius of r = 7 and r = 11 in Figure 5. Note that in both of these figures, when considering high fidelity explainers generated by both methods (fidelity \u2265 0.6), the explainers generated by the information filter method are less complex then those generated without the filter. This would indicate that using our information filter, we can obtain high fidelity local explainers that are on average less complex then those generated without this filter. When considering low fidelity explainers, the no filter method creates less complex models then the filter method. This is because our filter method is better equipped to find relevant features even in more complex regions of the black box model, while the no filter method is unable to learn these regions effectively with a fixed sample size. This is significant since this would indicate that our proposed methodology is able to explain a larger portion of the feature space using less complex models while still finding meaningful features for explanations, relative to existing methodologies.\nOverall, the plots in Figure 5 show that incorporating an information filter into local explainer training can be more effective in extracting relevant features then using regularization, and can generate less complex models with high fidelity. In addition, these results indicate that using an information filter allows for local explainers with information filters to obtain higher fidelity over a larger radius with relatively less complex models. This is particularly significant since less complex models can be me more easily interpreted by domain experts, making it easier for them to translate the clinical significance of the black box model outputs. while larger explanation radii are useful for model validation and generalization of explanations. Moreover, even in complex decision regions generated by the black box model, using an information filter in conjunction with local explainers is better at extracting relevant features for predictions which again can be useful for model validation and providing clinical insights."}, {"heading": "5 Global Explainer Methodology", "text": "While our proposed local explainer method is useful for providing insight into the behaviour of the black box model in a restricted region of the feature space it cannot be used for total model validation. For this task we require a global explainer that could provide insights into the behavior of the model across the entirety of the feature space. However, an explainer that is constrained to explain all of the feature space is likely have low fidelity since the explainer model is less complex then the black box. This introduces a trade-off between two qualities of an explainer model: its coverage and fidelity. One way to address this challenge is to create a global explainer model by aggregating several local explainer models. Several existing approaches use this idea [Ribeiro et al., 2016] by formulating the construction of the global explainer as an optimization problem. Generally, this problem is framed as maximizing the total coverage of the explainer subject to a constraint on the total number of local explainers included in the aggregate. Solving this problem is conjectured to computationally intractable [Ribeiro et al., 2016], and therefore in existing work it is solved using heuristics. In this section, we formulate the problem of constructing the aggregate explainer explicitly as an integer\nlinear program that allows us to directly trade off coverage and fidelity."}, {"heading": "5.1 Mathematical Programming Formulation of Aggregation Problem", "text": "In order to formulate the optimization problem for the global explainer, we first need to formally define the concepts of coverage and fidelity. Building upon the notation from Section 3.1, let gi,r : X \u2192 L be the local explainer generated by using Algorithm 1 on patient i with radius r. Furthermore let Xi,r \u2282 X be defined as the set of points explained by explainer gi,r; that is Xi,r := {x \u2208 X : \u2016x \u2212 xi\u2016 \u2264 r}. Define \u03b3 as the aggregate set of all explainers generated in this process: \u03b3 = {g1,r1 , g2,r2 , ..., gn,rn} for some potential local explainers g1,r1 , g2,r2 , ..., gn,rn . Using these quantities we define the coverage of aggregate exmplainer \u03b3 on the data set X as the total number of points that are covered by the explanation radius of at least one explainer contained in \u03b3. We denote the coverage as:\nCov(\u03b3,X ) = \u2211 x\u2208X max i\u2208{i:gi,r\u2208\u03b3} 1[x \u2208 Xi,r]. (3)\nNext recall that the fidelity of a single local explainer can be defined as the accuracy of that explainer using the predicted labels of the black box model as the ground truth. We will define the fidelity of aggregate explainer \u03b3 on the data set X as the minimum of the fidelity obtained by each individual local explainer. We denote this as:\nFidelity(\u03b3,X ) = min {i:gi,ri\u2208\u03b3}\n1 |Xi,r| \u2211 1[gi,r(x) = f(x)]1[x \u2208 Xi,r]. (4)\nAlthough the fidelity of \u03b3 can be defined as the average of the fidelities of its component explainers, we believe using the minimum fidelity gives a stricter measure on how well the global explainer captures the behavior of the black box model.\nLet K denote the budget of the maximum number of explainers that can be contained in \u03b3, and let \u03d5 be the minimum fidelity we would like the aggregate explainer to obtain. Then the optimization problem to be solved can be written as:\nmax \u03b3 {Cov(\u03b3,X ) : Fidelity(\u03b3,X ) \u2265 \u03d5, |\u03b3| \u2264 K}. (5)"}, {"heading": "5.2 Reformulation as Integer Program (IP)", "text": "Note that as written, optimization problem (5) is not trivial to solve, and in particular could require enumerating all possible subsets \u03b3 of local explainers. To address this challenge, we propose reformulating problem (5) as an Integer Program (IP) that can be solved using current commercial software. To do this, we first define the three different sets of binary variables that we will call wi, yj , zij . Let wi be a binary variable that is equal to 1 if explainer gi,ri \u2208 \u03b3. That is, wi = 1[gi,ri \u2208 \u03b3]. Let yj be defined as a binary variable that is equal to 1 if point j is covered by the global explainer \u03b3. That is yj = 1[xj \u2208 \u222a{i:gi,ri\u2208\u03b3}Xi,ri ]. Finally, let zij be a binary variable that is equal to 1 if explainer gi,ri \u2208 \u03b3 covers point xj . That is, zij = 1[xj \u2208 Xi,ri ]. We can now define the coverage and fidelity of aggregate explainer \u03b3 in terms of these three sets of variables.\nProposition 1. Cov(\u03b3,X ), the coverage of local explainer set \u03b3 on data set X , can be expressed with the following set of integer variables and constraints:\nCov(\u03b3,X ) = \u2211 j\u2208X yj ,\ns.t. zij \u2264 wi, i, j \u2208 X , yj \u2265 zij , i, j \u2208 X ,\nyj \u2264 \u2211 i\u2208X zij , j \u2208 X , \u2016xi \u2212 xj\u2016zij \u2264 ri, i, j \u2208 X .\n(6)\nProof. Recall the definition of Cov(\u03b3,X ) as given in Equation (3). We will directly reconstruct this definition using the binary variables defined above. First note that through a simple direct substitution we obtain Cov(\u03b3,X ) = \u2211 x\u2208X maxi\u2208{i:gi,r\u2208\u03b3} zij . Since taking the maximum of binary variables is equivalent to the Boolean OR operator, we see that yj = maxi\u2208{i:gi,r\u2208\u03b3} zij , which provides us with the first equality. The next two inequalities directly models that explainer gi,ri can only explain point xj if it is included in \u03b3, which is a standard way of modeling conditional logic in IP [Wolsey and Nemhauser, 1999]. The next two constraints come from modeling the Boolean OR operator using integer constraints [Wolsey and Nemhauser, 1999]. The final constraint ensures that a point xj can be covered by an explainer gi,ri if xj \u2208 Xi,ri , thus preserving the local region of the local explainer.\nNext we consider the average fidelity constraint.\nProposition 2. The constraint Fidelity(\u03b3,X ) \u2265 \u03d5 can be modeled using the following set of integer linear constraints:\n\u2016xi \u2212 xj\u2016zij \u2264 ri, i, j \u2208 X , zij \u2264 wi, i, j \u2208 X ,\u2211 j\u2208X ( 1{f(xj)=gi,ri (xj)} \u2212 \u03d5 ) zij \u2265 0, i \u2208 X .\n(7)\nProof. The first two constraints ensure proper local behavior of the local explainer as in Proposition 1. Thus we will focus the derivation of the final constraint. Using the definition of Fidelity(\u03b3,X ) in Equation (4) and directly substituting variables for indicators, we can express the lower bound constraint as,\nmin {i:gi,ri\u2208\u03b3}\n1 |Xi,r| \u2211 j\u2208X 1[gi,ri(xj) = f(xj)]zij \u2265 \u03d5.\nNote that if the minimum over all explainers gi,ri must have fidelity of at least \u03d5, then every local explainer must also have fidelity at least \u03d5. This allows us to disaggregate this constraint across all i \u2208 X . Let us consider the constraint for a single local explainer gi,ri . Using the definition of zij , we note that |Xi,ri | =\u2211 j\u2208X zij . Thus the new lower bound fidelity constraint for a single explainer can be written as:\u2211\nj\u2208X 1[gi,ri(xj) = f(xj)]zij\u2211 j\u2208X zij \u2265 \u03d5. (8)\nNote that the denominator of the left hand side can only be zero when the numerator is also zero because\u2211 j\u2208X zij \u2265 \u2211 j\u2208X 1[gi,ri(xj) = f(xj)]zij . This means that we can multiply both sides of the inequality by\nthe sum \u2211 j\u2208X zij while still maintining its validity. Distributing \u03d5 and combining like terms gives us with the form of the constraint presented in the proposition statement.\nWe can then use these expressions to for coverage and fidelity to re-write our optimization problem as an integer program that can then be solved using commercial solvers.\nProposition 3. The optimization problem in (5) can be written as the following integer program: max \u2211 j\u2208X yj ,\ns.t. zij \u2264 wi, i, j \u2208 X , yj \u2265 zij , i, j \u2208 X ,\nyj \u2264 \u2211 i\u2208X zij , j \u2208 X , \u2016xi \u2212 xj\u2016zij \u2264 ri, i, j \u2208 X ,\u2211 j\u2208X ( 1{f(xj)=gi,ri (xj)} \u2212 \u03d5 ) zij \u2265 0, i \u2208 X ,\u2211 i\u2208X wi \u2264 K, yj , wi, zij \u2208 {0, 1} i, j \u2208 X .\n(9)\nProof. The objective function and first five constraints come directly from Propositions 1 and 2. The next constraint comes using the definition of wi and direct substitution to obtain that |\u03b3| = \u2211 i\u2208X wi, which is then used to rewrite the budget constraint from (5). The final constraint ensures that our new variables are binary integers."}, {"heading": "6 Experimental Validation of Global Explainer", "text": "In this section we empirically validate the quality of our global explainer methodology using the PPMI dataset described in Sections 2 and 4. In our experiments, we first use the clustering algorithm of Section 2 to assign labels and then apply our local explainer as described in Section 3. We then use the IP-based global explainer described in Section 5 to choose the local explainers that will comprise our global explainer.\nWe ran the optimization algorithm for binary classification (2 class) between healthy individuals and patients diagnosed with PD, and for multi-class (5 class) classification among healthy individuals and the four PD sub-types. Figure 6 shows the coverage and average fidelity score of the global explainer generated by our IP-based approach, compared with other existing local and global explainers methods as a function of the budgeted number of local explainers that are allowed in the global explainer. We tested performance for budgets K = 5, 10, 15, 20, 30, 40, 50, 60, 70. The lines labeled lb=0.5,0.7 or 0.9 in Figure 6 represent the result of the IP with the lower bound of the fidelity score set to 50%, 70% or 90%, respectively. Based on testing different sampling radii (shown above in Figure 5), we found that when the sampling radius was in a medium range (i.e., r = 7 or 11) our method produced a simpler model with same precision than those produced by existing methodologies. Therefore we used sampling radius r = 11 in our experiments for all local explainers.\nWe compared our IP with the following prior methods: the submodular pick algorithm from [Ribeiro et al., 2016], a simple decision tree trained on the labels of the black box model Friedman et al. [2001], an extracted decision tree method method [Bastani et al., 2018], an interpretable decision set [Lakkaraju et al., 2016], and an anchor points method [Ribeiro et al., 2018]. The interpretable decision set, decision tree, and extracted approaches are all global explainer methods, so they are always guaranteed have perfect coverage. The submodular pick and anchor point methods, along with our IP method must trade off between coverage and fidelity.\nFor the simpler classification problem with only two classes, our IP approach, extracted decision tree, and regular decision tree methods all produced a model with significantly higher fidelity score and higher coverage than other methods. Since a large percentage of the patients are healthy individuals, it is reasonable that the decision tree and extracted tree methods achieve higher precision than our IP. However, our IP outperforms the other existing methodologies when the classification problem is more complex (5 class). Even though\nthe coverage of IP is lower than the other global explainer methods, the model produced by IP has much higher coverage and precision than the existing local explainers methods in both problems.\nFigure 7 shows the relationship between the coverage and fidelity of each method. The lines labeled MIP 5, MIP 10, and MIP 15 respectively show the smooth transition of coverage and fidelity score of our IP approach with fixed budgets of 5, 10, and 15 as the lower bound of the fidelity score varies from 0% to 90% in increments of 10%. Since our IP approach is the only one that has a lower bound constraint on fidelity, we are able to observe this trade-off between the coverage and fidelity for MIP, while the other existing methods lack such freedom."}, {"heading": "A FFFS Algorithmic Details", "text": "In this appendix, we present and discuss the FFFS algorithm used in our local explainer method. The main algorithm is presented in Algorithm 2, and the required subroutines are presented in Algorithms 3, 4, and 5.\nSince the main structure of the algorithm requires a recursive tree traversal, Algorithm 2 includes a general prepossessing wrapper algorithm that starts the recursion. In this part of the algorithm, the sampled data points are used to compute the empirical densities of their feature values. These densities are approximated using histograms which can vary in the number of bins. For simplicity of presentation, we assume each histogram has the same bin size, but of course this detail can be modified in implementation. The key addition here is the computation of tensor M , which tracks the inclusion of each data point\u2019s features into their respective histogram bin.\nAlgorithm 3 contains the main recursion of the filter computation, and it outputs the selected features when it terminates. The recursion of Algorithm 3 requires a set of selected features S, a set of unselected features U , the binary tensor M , the black box model predictions Y , and L, which is a set of partition sets of points in T (x\u0304). Since no features are selected prior to the first call to Algorithm 3, we initialize the inputs S = \u2205, U = \u03a6, Y = f(T (x\u0304)) and L = T (xi) when it is first called in Algorithm 2. The recursion terminates and outputs the current set of selected features when either all features are selected or L becomes empty. If the termination condition is not met, Algorithm 3 calls Algorithm 4, which updates S,U, and L using a bin expansion. Then Algorithm 3 makes a recursive call with updated inputs and repeat the previous steps.\nAlgorithm 4 is used to select one feature from the set of unselected features that maximizes the mutual information I(\u03d5;Y |U), and to update L given the current selected feature. We apply forward selection in Algorithm 4. In order to find \u03d5\u2217 = arg max\u03d5\u2208U I(\u03d5;Y |U), we compute I(\u03d5; y|S) for each unselected feature \u03d5. The approximated mutual information I(\u03d5; y|S) is computed using the following equation [Brown et al., 2012]:\nI(\u03d5; y|S) \u2248 I\u0302(\u03d5; y|S) = 1 |T (x\u0304)| N\u2211 i=1 log p\u0302(\u03d5; y|S) p\u0302(\u03d5|S)p\u0302(\u03d5|S) .\nIf I(\u03d5\u2217; y|S) is not positive, then we do not select any new features. If no new feature is selected, we terminate the process by setting U = \u2205, which satisfies the termination condition of Algorithm 3, and the feature selection process will be complete. If I(\u03d5\u2217; y|S) > 0, then we can obtain additional information on the prediction by adding \u03d5\u2217 to the set of selected features S and removing it from the set of unselected features U . Algorithm 4 then calls Algorithm 5 to update L to L\u2032. Algorithm 5 is used to partition each set in L given current selected feature \u03d5\u2217. Using the binary tensor M , we can collect the set of bins for \u03d5\u2217. As an illustrative example of this process, let B\u03d5\u2217 = {b1, b2} and L = T (x\u0304) = {x1, x2, ...., xp}. Assume x\u03d5 \u2217 i \u2208 b1 for i < 5 and x \u03d5\u2217\ni \u2208 b2 otherwise. Then we can partition the set {x1, x2, ...., xp} into 2 sets `1, `2 s.t. `1 = {x1, ..., x4} and `2 = {x5, ..., xp}. Next we add sets `1, `2 to L\u2032. Since L contains exactly one set, we finish the partition process, and Algorithm 5 outputs L\u2032 = {{x1, ..., x4}, {x5, ..., xp}}.\nAlgorithm 2 Fast Forward Feature Selection (FFFS)\nRequire: T (x\u0304),\u03a6, f 1: for \u03d5 \u2208 \u03a6c do 2: Form histogram with bin set B\u03d5 and frequencies p\u0302\u03d5 3: end for 4: set M \u2208 |B\u03d5| \u00d7 |\u03a6| \u00d7N as a zero tensor 5: for x \u2208 T (x\u0304) do 6: for \u03d5 \u2208 \u03a6 do 7: for b \u2208 B\u03d5 do 8: if x[\u03d5] \u2208 b then 9: Set M [b, \u03d5, x] = 1 10: end if 11: end for 12: end for 13: end for 14: return RecursionFFS(\u2205,\u03a6,M, f(T (x\u0304)), T (x\u0304))\nAlgorithm 3 Recursion FFS\nRequire: S,U,M, Y,L 1: if U = \u2205 or L = \u2205 then 2: return S 3: else 4: [S\u2032, U \u2032,L\u2032] = SelectFeature(S,U,M, Y,L) 5: return RecursionFFS(S\u2032, U \u2032,M, Y,L\u2032) 6: end if\nProposition 4. The time complexity of the FFFS algorithm for a fixed maximum discretization bin size is O(N |\u03a6|).\nProof. Note that the size of the generated points is given by the input parameter N , and the set of all features is denoted by \u03a6. First, since the bin sized is fixed as a constant, and the preprocessing step requires\nAlgorithm 4 Select Feature\nRequire: S,U,M, Y,L 1: f\u2217 = arg maxf\u2208U I(f ;Y |U) 2: if I(f\u2217;Y |U) > 0 then 3: U = U \\ f\u2217 4: S = S \u222a f\u2217 5: L\u2032= BinPartition (M,L, f\u2217) 6: return S,U,L\u2032 7: else 8: U = \u2205 9: return S,U,L 10: end if\nAlgorithm 5 Bin Partition Require: M,L, f\u2217 1: Use M to find Bf\u2217 s.t. Bf\u2217 = {b1, b2, ..., bk} is the set of bins for feature f\u2217 2: L\u2032 = \u2205 3: for ` \u2208 L do 4: Partition ` into smaller sets {`1, `2, ...`k} w.r.t Bf\u2217 : `i = {t \u2208 l : tf\n\u2217 \u2208 bi}\u2200i \u2208 {1, ..., k} 5: L\u2032 = L\u2032 \u222a {l1, ..., lk} 6: end for 7: return L\u2032\na nested for loop, the total time complexity of the preprocessing is O(N |\u03a6|). The FFFS algorithm operates as a tree traversal, where the depth of the tree at the final stage corresponds to the number of selected features. In each level of the tree, the mutual information of all points is evaluated using Algorithm 4 and the sets of generated points are partitioned into smaller sets using Algorithm 5, which combined require O(N) operations. Next, since in the worst case, all features contain positive mutual information on the prediction value of the black box model, the maximum possible tree depth is given by |\u03a6|. Combining these two facts gives the desired result.\nB Additional figures"}], "title": "Locally Interpretable Predictions of Parkinson\u2019s Disease Progression", "year": 2020}