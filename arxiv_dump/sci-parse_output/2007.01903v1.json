{"abstractText": "Data-driven pricing strategies are becoming increasingly common, where customers are offered a personalized price based on features that are predictive of their valuation of a product. It is desirable to have this pricing policy be simple and interpretable, so it can be verified, checked for fairness, and easily implemented. However, efforts to incorporate machine learning into a pricing framework often lead to complex pricing policies which are not interpretable, resulting in slow adoption in practice. We present a customized, prescriptive tree-based algorithm that distills knowledge from a complex black box machine learning algorithm, segments customers with similar valuations and prescribes prices in such a way that maximizes revenue while maintaining interpretability. We quantify the regret of a resulting policy and demonstrate its efficacy in applications with both synthetic and real-world datasets.", "authors": [{"affiliations": [], "name": "Max Biggs"}, {"affiliations": [], "name": "Wei Sun"}], "id": "SP:842d78c8deafb634b8551df3354ef7b0dcced8a7", "references": [{"authors": ["Ross Anderson", "Joey Huchette", "Will Ma", "Christian Tjandraatmadja", "Juan Pablo Vielma."], "title": "Strong mixed-integer programming formulations for trained neural networks", "venue": "Mathematical Programming (2020), 1\u201337.", "year": 2020}, {"authors": ["Susan Athey", "Guido Imbens."], "title": "Recursive partitioning for heterogeneous causal effects", "venue": "Proceedings of the National Academy of Sciences 113, 27 (2016), 7353\u20137360.", "year": 2016}, {"authors": ["Lennart Baardman", "Setareh Borjian Boroujeni", "Tamar Cohen-Hillel", "Kiran Panchamgam", "Georgia Perakis."], "title": "Detecting customer trends for optimal promotion targeting", "venue": "Available at SSRN 3242529 (2018).", "year": 2018}, {"authors": ["David Baehrens", "Timon Schroeter", "Stefan Harmeling", "Motoaki Kawanabe", "Katja Hansen", "KlausRobert M\u00c3\u017eller."], "title": "How to explain individual classification decisions", "venue": "Journal of Machine Learning Research 11, Jun (2010), 1803\u20131831.", "year": 2010}, {"authors": ["Gah-Yi Ban", "N Bora Keskin."], "title": "Personalized dynamic pricing with machine learning: High dimensional features and heterogeneous elasticity", "venue": "Available at SSRN 2972985 (2020).", "year": 2020}, {"authors": ["Gah-Yi Ban", "Cynthia Rudin."], "title": "The big data newsvendor: Practical insights from machine learning", "venue": "Operations Research 67, 1 (2019), 90\u2013108.", "year": 2019}, {"authors": ["Hamsa Bastani", "Osbert Bastani", "Carolyn Kim."], "title": "Interpreting predictive models for humanin-the-loop analytics", "venue": "arXiv preprint arXiv:1705.08504 (2018), 1\u201345.", "year": 2018}, {"authors": ["Dimitris Bertsimas", "Arthur Delarue", "Patrick Jaillet", "Sebastien Martin."], "title": "The Price of Interpretability", "venue": "arXiv preprint arXiv:1907.03419 (2019).", "year": 2019}, {"authors": ["Dimitris Bertsimas", "Jack Dunn", "Nishanth Mundru."], "title": "Optimal prescriptive trees", "venue": "INFORMS Journal on Optimization 1, 2 (2019), 164\u2013183.", "year": 2019}, {"authors": ["Dimitris Bertsimas", "Nathan Kallus."], "title": "The power and limits of predictive approaches to observational-data-driven optimization", "venue": "arXiv preprint arXiv:1605.02347 (2016).", "year": 2016}, {"authors": ["Dimitris Bertsimas", "Nathan Kallus."], "title": "From predictive to prescriptive analytics", "venue": "Management Science 66, 3 (2020), 1025\u20131044.", "year": 2020}, {"authors": ["Dimitris Bertsimas", "Christopher McCord."], "title": "Optimization over continuous and multidimensional decisions with observational data", "venue": "Advances in Neural Information Processing Systems. 2962\u20132970.", "year": 2018}, {"authors": ["Max Biggs", "Rim Hariss"], "title": "Optimizing objective functions determined from random forests", "venue": "Available at SSRN", "year": 2018}, {"authors": ["Fernanda Bravo", "Yaron Shaposhnik."], "title": "Mining optimal policies: A pattern recognition approach to model analysis", "venue": "Available at SSRN 3069690 (2018).", "year": 2018}, {"authors": ["Leo Breiman."], "title": "Classification and regression trees", "venue": "Routledge.", "year": 2017}, {"authors": ["Cristian Bucilu\u01ce", "Rich Caruana", "Alexandru Niculescu-Mizil."], "title": "Model compression", "venue": "Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining. 535\u2013541.", "year": 2006}, {"authors": ["Xi Chen", "Zachary Owen", "Clark Pixton", "David Simchi-Levi."], "title": "A statistical learning approach to personalization in revenue management", "venue": "Available at SSRN 2579462 (2015).", "year": 2015}, {"authors": ["Dragos Ciocan", "Velibor Mi\u0161i\u0107."], "title": "Interpretable optimal stopping", "venue": "(2018).", "year": 2018}, {"authors": ["Mark Craven", "Jude W Shavlik."], "title": "Extracting tree-structured representations of trained networks", "venue": "Advances in neural information processing systems. 24\u201330.", "year": 1996}, {"authors": ["Adam N. Elmachtoub", "Paul Grigas."], "title": "Smart \"Predict, then Optimize", "venue": "arXiv:math.OC/1710.08005", "year": 2017}, {"authors": ["Adam N Elmachtoub", "Vishal Gupta", "Michael Hamilton."], "title": "The value of personalized pricing", "venue": "Available at SSRN 3127719 (2018).", "year": 2018}, {"authors": ["Adam N. Elmachtoub", "Jason Cheuk Nam Liang", "Ryan McNellis."], "title": "Decision Trees for Decision-Making under the Predict-then-Optimize Framework", "venue": "arXiv:cs.LG/2003.00360", "year": 2020}, {"authors": ["Kris Johnson Ferreira", "Bin Hong Alex Lee", "David Simchi-Levi."], "title": "Analytics for an online retailer: Demand forecasting and price optimization", "venue": "Manufacturing & Service Operations Management 18, 1 (2016), 69\u201388.", "year": 2016}, {"authors": ["Jerome H Friedman", "Bogdan E Popescu"], "title": "Predictive learning via rule ensembles", "venue": "The Annals of Applied Statistics 2,", "year": 2008}, {"authors": ["Geoffrey Hinton", "Oriol Vinyals", "Jeff Dean."], "title": "Distilling the knowledge in a neural network", "venue": "arXiv preprint arXiv:1503.02531 (2015).", "year": 2015}, {"authors": ["Adel Javanmard", "Hamid Nazerzadeh."], "title": "Dynamic pricing in high-dimensions", "venue": "arXiv preprint arXiv:1609.07574 (2016).", "year": 2016}, {"authors": ["Nathan Kallus."], "title": "Recursive partitioning for personalization using observational data", "venue": "Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 1789\u20131798.", "year": 2017}, {"authors": ["Guolin Ke", "Qi Meng", "Thomas Finley", "Taifeng Wang", "Wei Chen", "Weidong Ma", "Qiwei Ye", "Tie-Yan Liu."], "title": "Lightgbm: A highly efficient gradient boosting decision tree", "venue": "Advances in neural information processing systems. 3146\u20133154.", "year": 2017}, {"authors": ["Himabindu Lakkaraju", "Stephen H Bach", "Jure Leskovec."], "title": "Interpretable decision sets: A joint framework for description and prediction", "venue": "Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining. 1675\u20131684.", "year": 2016}, {"authors": ["Benjamin Letham", "Cynthia Rudin", "Tyler H McCormick", "David Madigan"], "title": "Interpretable classifiers using rules and bayesian analysis: Building a better stroke prediction model", "venue": "The Annals of Applied Statistics 9,", "year": 2015}, {"authors": ["Velibor V Mi\u0161ic."], "title": "Optimization of tree ensembles", "venue": "arXiv preprint arXiv:1705.10883 (2017).", "year": 2017}, {"authors": ["Marco Tulio Ribeiro", "Sameer Singh", "Carlos Guestrin."], "title": " Why should i trust you?\" Explaining the predictions of any classifier", "venue": "Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining. 1135\u20131144.", "year": 2016}, {"authors": ["Marco Tulio Ribeiro", "Sameer Singh", "Carlos Guestrin"], "title": "Anchors: High-precision modelagnostic explanations", "venue": "In Thirty-Second AAAI Conference on Artificial Intelligence", "year": 2018}, {"authors": ["Ivan Sanchez", "Tim Rocktaschel", "Sebastian Riedel", "Sameer Singh."], "title": "Towards extracting faithful and descriptive representations of latent variable models", "venue": "AAAI Spring Syposium on Knowledge Representation and Reasoning (KRR): Integrating Symbolic and Neural Approaches, Vol. 1. 4\u20131.", "year": 2015}, {"authors": ["Berk Ustun", "Cynthia Rudin."], "title": "Supersparse linear integer models for optimized medical scoring systems", "venue": "Machine Learning 102, 3 (2016), 349\u2013391.", "year": 2016}, {"authors": ["Peng Ye", "Julian Qian", "Jieying Chen", "Chen-hung Wu", "Yitong Zhou", "Spencer De Mars", "Frank Yang", "Li Zhang."], "title": "Customized regression model for airbnb dynamic pricing", "venue": "Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 932\u2013940.", "year": 2018}, {"authors": ["Peng Ye", "Julian Qian", "Jieying Chen", "Chen-hung Wu", "Yitong Zhou", "Spencer De Mars", "Frank Yang", "Li Zhang."], "title": "Customized Regression Model for Airbnb Dynamic Pricing", "venue": "Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (London, United Kingdom) (KDD \u201918). Association for Computing Machinery, New York, NY, USA, 932\u2013940. https://doi.org/10.1145/3219819.3219830", "year": 2018}], "sections": [{"heading": "1 Introduction", "text": "To remain competitive, many firms are seeking to implement data-driven pricing policies. This is enabled by the large amounts of relevant data firms are collecting, and through recent advances in machine learning. This data rich environment has also enhanced personalized pricing, where customers are offered a personalized price based on features that are predictive of their valuation of a product. A common example of personalized pricing is targeted offers through loyalty programs. While personalized pricing may potentially lead to higher revenue, currently only a quarter of retail companies are supportive of ML adoption, the lowest acceptance rate across surveyed industries.1 One of the main concerns is that the most accurate prediction models are often non-parametric functions which are considered as a black-box. Their opaqueness makes it difficult for firms to understand and trust the underlying mechanics of how the prices are being formed, which may potentially lead to fairness and legal issues. Another challenge with a personalized pricing policy is that it might be very complex, making it difficult and cumbersome for a firm to implement and to verify that price is sensible.\nDespite the recent surge of interest in making machine learning prediction models more interpretable (Letham et al., 2015; Ustun and Rudin, 2016; Lakkaraju et al., 2016; Bertsimas et al., 2019a), there has been significantly less work on interpreting policies from these models when embedded in an\n1https://advisory.kpmg.us/content/dam/advisory/en/pdfs/2020/retail-living-in-an-ai-world.pdf\nPreprint. Under review.\nar X\niv :2\n00 7.\n01 90\n3v 1\n[ st\nat .M\nL ]\noperational decision-making context, which is the focus of this paper. It is crucial to note that it is not straightforward to adapt predictive models effectively to a prescriptive setting. Consider a treebased model, a partition of data which leads to good predictive accuracy (e.g., predicting purchase probability) does not necessarily translate to good prescriptive decisions (e.g., revenue-maximizing prices). An effective partition for prediction should have homogeneity in the labels within each set, whereas a prescriptive model should have homogeneity in what is judged to be the best course of action. This is challenging because the lack of counterfactual data in observational pricing data means that we do not observe what the outcome would have been with a different decision. Therefore, it can be difficult to estimate the optimal action. Pricing data also often has confounding effects, where past pricing decisions are made based on customer features and their perceived willingness to pay. Making decisions without properly controlling for confounding effects leads to sub-optimal revenue, as documented in Bertsimas and Kallus (2016).\nTo address these issues, we propose an interpretable method for optimizing personalized pricing policies, by adapting the student-teacher framework with a prescriptive student tree. To achieve this, we first train an accurate but opaque predictive teacher model, such as Boosted Trees or Neural Networks to predict the purchase probability. Using this model we are able to produce an optimal price for each item, but the pricing policy is often complex and not interpretable. The prescriptive student model seeks to distill this pricing policy into one which is simpler and more interpretable through a customized recursive partitioning algorithm. Each leaf in the prescriptive student tree defines a set of items which are given the same price. These items share similarities in their covariates, and are chosen in such a way that each item in a leaf has a similar optimal price, as evaluated by the teacher model. In other words, the teacher model is incorporated to estimate the counterfactual outcomes that are missing from the data, and control for observed confounding variables in the tree construction process.\nOur key contributions are as follows: we propose a method for prescribing interpretable policies, where we introduce a new impurity measure and algorithm for constructing a student prescriptive tree by incorporating a teacher model. We quantify the regret of the resulting policy and demonstrate its efficacy in applications with both synthetic and real-world datasets. While we focus on revenue optimization, this method can be easily extended to other prescriptive settings such as personalized medicine."}, {"heading": "2 Related literature", "text": "A popular approach to gain interpretability is through knowledge distillation, where a simpler model (student model) is trained on a transformed dataset. The observed outcomes are replaced by softlabels, the predictions of the complex model (teacher model). This approach was popularized in the deep-learning community by Hinton et al. (2015), but variations on this idea have been extensively studied in deep learning and for other non-parametric models, for example, SHAP: ?, LIME: Ribeiro et al. (2016), Anchors: Ribeiro et al. (2018), RuleFit: Friedman et al. (2008), also Baehrens et al. (2010); Sanchez et al. (2015); Bucilua\u030c et al. (2006). Of particular interest to this work is Craven and Shavlik (1996); Bastani et al. (2018), which use decision trees as the approximating model. Rather than using a teacher-student framework to approximate a prediction model, we adapt it to approximate a prescriptive policy.\nThere has been a recent focus within the operations research community on prescriptive analytics, i.e., how to use data and machine learning to make better operational decisions (Ban and Rudin, 2019; Bertsimas and Kallus, 2020; Biggs and Hariss, 2018; Mi\u0161ic, 2017; Bertsimas and McCord, 2018; Anderson et al., 2020). This includes personalized pricing models, where user and/or product data is used to customize prices for products (Chen et al., 2015; Ferreira et al., 2016; Javanmard and Nazerzadeh, 2016; Bertsimas and Kallus, 2016; Ye et al., 2018a; Baardman et al., 2018; Elmachtoub et al., 2018; Ban and Keskin, 2020). However, these approaches are not explicitly concerned with interpretability, and indeed may result in complex decision policies with no segmentation or limit on the number of prices offered.\nWe highlight some tree-based prescriptive approaches which are closest to the current work. Kallus (2017) introduce personalization trees, a customized tree algorithm which proposes an impurity measure based on prescribing the treatment which has the highest outcome on average within each set in the partition. Bertsimas et al. (2019b) extend this approach by incorporating predictive accuracy\ninto the tree construction algorithm, and exploring more sophisticated optimization techniques. Athey and Imbens (2016) introduce a causal tree algorithm for estimating heterogenous treatment effects for binary treatments. Ciocan and Mi\u0161ic\u0301 (2018) show a tree-based approach for solving optimal stopping problems. Elmachtoub et al. (2020) recently provides a tree-based implementation of the predict-then-optimize approach first introduced in Elmachtoub and Grigas (2017). While this a promising approach, it is not clear how to apply this to the pricing setting where the parameter being estimated is not just a function of exogenous covariates, but also price (which is the decision being made).\nA key difference between our approach and these methods is the use of a teacher model to guide the pricing policy. One implicit assumption behind these prescriptive tree-based approaches is that the leaves are essentially homogeneous, so the confounding effects of the other (observed) variables are controlled for. This assumption means that the only difference between outcomes in the same leaf is due to the treatment assigned. This might be a reasonable assumption when the trees are very deep (i.e., resulting in smaller leaves where the covariates are similar, risking overfitting), but is unlikely to hold in shallow trees. In many prescriptive settings, shallow trees with fewer segments which translate into fewer pricing policies are desirable. Incorporating a teacher model controls for the observed confounding variables at any depth, rather than assuming they are the same, therefore ensuring that confounding effects are minimized. We also note that these approaches only work for discrete treatments, whereas our approach can be applied to continuous treatments."}, {"heading": "3 Methodology", "text": ""}, {"heading": "3.1 Problem formulation", "text": "We assume we have n observational data samples {(xi, pi, yi)}ni=1, where xi \u2208 X d are features which describe the ith item, pi \u2208 R is the price assigned to the item, and yi \u2208 {0, 1} is whether the item sold (1) or not (0). For convenience of exposition, we may refer to \u201citem\u201d as a selling unit of a product, where xi only contains product features as in the airline pricing example in Section 4.4. It may also represent a (product, customer) pair when xi also includes customer features, as in Section 4.3.\nAs is common practice in the causal inference literature, we make the ignorability assumption of ?, that there were no unmeasured confounding variables that affected both the choice of decision and the outcome. Accordingly, we assume an underlying function f : X d \u00d7R\u2192 [0, 1] which maps the item features and price to a purchase probability f(x, p) = E[Y |X = x, P = p] = P[Y |X = x, P = p]. An optimal personalized pricing algorithm selects a price \u03c4\u2217(x) for each item based on its features to maximize the item\u2019s expected revenue:\n\u03c4\u2217(x) = arg max p\u2208R pf(x, p) (1)\nIn reality, the true response function f(x, p) is unknown, but can be estimated by solving an empirical risk minimization problem f\u0302 = arg minf\u2208F \u2211n i L(xi, pi, yi; f) over a class functions F and an appropriate loss function L for classification. We refer to f\u0302 as the teacher model.\nIf we substitute the proxy response function f\u0302 into the optimization in (1), the resulting personalized pricing policy may be too complex for a firm to verify and implement, with each feature vector potentially resulting in a different price. Furthermore, if the class of functions F is complex, there may not be any insight into how the predictions are made. Our goal is to find an interpretable personalized pricing policy. We achieve this by restricting the pricing policy to the class of decision tree functions of depth k, T (k), which we refer to as the prescriptive student model, and find the policy which achieves the highest revenue as predicted by the teacher model:\nmax \u03c4(x)\u2208T (k)\n1\nn n\u2211 i=1 \u03c4(xi)f\u0302(xi, \u03c4(xi)) (2)\nA decision tree (see ?) is a class of functions which recursively partitions the feature space into leaves, each of which is associated with a prediction. Contrary to the well-studied predictive setting where each leaf has a prediction on the outcome, the leaves of the tree prescribe a single price at which to\nsell the items. The binary tree structure is defined by internal nodes and leaves. At each internal node there is an axis-aligned split which checks if a feature is less than a threshold. If this condition is met when mapping a data point to a leaf then it proceeds to the left child node. Otherwise, it continues to the right child node until a leaf is reached.\nDecision trees result in interpretable pricing policies because specifying the depth of the tree restricts the number of unique prices in the policy. Furthermore, the set of items which are prescribed this price is defined by the single variable splits which lead to the leaf. Each of these rules can be easily verified by a practitioner. An example of a tree-based pricing policy is shown in Figure 3."}, {"heading": "3.2 Approximation guarantees", "text": "We show that the regret of a tree-based interpretable pricing policy optimized over the teacher model can be bounded relative to the optimal personalized pricing policy. Here the regret is defined as the difference in the expected revenue between the two policies. To do so, we assume uniform bounds on the error of the estimated teacher model and that the true expected revenue function is smooth. For convenience of exposition, we also assume x \u2208 [0, 1]d.\nAssumption 1 Estimation error uniformly bounded:|f(x, p)\u2212 f\u0302(x, p)| \u2264 K(n),\u2200x \u2208 [0, 1]d, p \u2208 R\nAssumption 2 pf(x, p) Lipshitz continuous in x: pf(x, p)\u2212pf(x\u2032, p) \u2264 L|x\u2212x\u2032| \u2200x, x\u2032 \u2208 [0, 1]d\nTheorem 1 Under the above assumptions, there exists a tree-based policy \u03c4\u0302(x) \u2208 T (k), trained on the proxy response function f\u0302 such that:\n\u03c4\u2217(x)f(x, \u03c4\u2217(x))\u2212 \u03c4\u0302(x)f(x, \u03c4\u0302(x)) \u2264 2L \u221a d\n2k + 2K(n) \u2200x \u2208 [0, 1]d (3)\nThe proof is given in the supplementary materials. An important example where assumption 1 is met is honest trees and forests created in ?. This theorem shows that the revenue gap between the tree-based pricing policy and optimal personalized policy can be decomposed into an approximation error and an estimation error. The estimation error is inherited from the estimation error of the teacher model. While the approximation error depends on the smoothness of the function and the dimension of the feature space, and decays rapidly with the depth of the tree."}, {"heading": "3.3 Recursive partitioning algorithm for student prescriptve trees", "text": "We propose a simple algorithm for solving (2). Considering the task of constructing an optimal classification tree is known to be NP-complete ?, we use a heuristic based on the recursive partitioning ?, whereby splits are chosen greedily. We achieve this by proposing a new splitting criterion, which utilizes the teacher model. Denote Sl \u2286 [n] as the subset of observations which belong to leaf l of the student tree. We define the revenue maximization criterion as follows:\nR(Sl) = max p \u2211 i\u2208Sl pf\u0302(xi, p) (4)\nAt each node in the tree, we consider a decision split partitioning the data into two sets S1(j, s) = {i \u2208 [n]|xi,j \u2264 s} and S2(j, s) = {i \u2208 [n]|xi,j > s}, where xi,j is the jth feature of observation i. We choose the split which results in maximum predicted revenue by giving each set formed a different price. We do this by optimizing over j, the feature chosen to split on, and s, the threshold, i.e., maxj,sR(S1(j, s)) +R(S2(j, s)). On the training data this is limited to searching the d dimensions of the feature vector, while there are at most n unique splits in the data, one for each data point. We continue recursively splitting until a specified criteria has been met. We use tree depth as termination criteria which corresponds to the desired complexity of the pricing policy, but can easily incorporate other commonly used approaches such as minimum leaf size criterion, or minimum improvement in impurity/revenue. In each leaf, the price assigned is the maximizer of (4).\nIn many pricing applications, there is a discrete set of prices that can be chosen from, p \u2208 P\u0304 = {p1, p2, ..., pm}. With this simplification, the estimated revenue for each item at each price ri,k =\npkf(xi, pk), k \u2208 [m] can be calculated prior to tree construction. The revenue for each segment can calculated and used instead of (4):\nRm(Sl) = max k\u2208[m] \u2211 i\u2208Sl ri,k (5)\nWe note that there are non-greedy approaches for constructing trees which could be adapted to solve (2), such as Bertsimas et al. (2019b). However, experiments in Bastani et al. (2018) have shown that greedy trees are more intuitive as the variables leading to the largest improvement are split on first, matching human reasoning and providing more interpretabilty. Furthermore greedy algorithms are known to scale better and solve more quickly for large datasets."}, {"heading": "4 Experimental results", "text": ""}, {"heading": "4.1 Benchmarks", "text": "We compare our approach with two other tree-based algorithms. Personalization trees (Kallus (2017)), prescribe the treatment which has the highest average outcome in each leaf. This algorithm can be adapted to a personalized pricing setting by using the observed revenue as the outcome for each item. More specifically, if there are m prices t \u2208 [m], we calculate the impurity for a set of items Sl as follows, and use this as the splitting criterion in a greedy algorithm:\nI(Sl) = max t\u2208[m] \u2211 i\u2208Sl piyi1{pi = t}\u2211 i\u2208Sl 1{pi = t}\n(6)\nWe also benchmark our algorithm against the causal trees algorithm from Athey and Imbens (2016). The approach estimates heterogeneous treatment effects, rather than prescribes the optimal treatment. In particular, it only considers estimation of CATE (conditional average treatment effect) for binary treatments \u03b4(x) = E[Y |X = x, T = 1] \u2212 E[Y |X = x, T = 0]. Following the testing procedure in Kallus (2017), we adapt the causal tree algorithm to prescribing the optimal treatment from a discrete set of alternatives using the \u201cone-vs.-all\" approach. In this approach, the treatment effect of each price is estimated by comparing the expected outcome of those items which received the price to those which received another price. Specifically, for each price t \u2208 [m] , the treatment effect \u03b4(x, t)1vall = E[Y |X = x, P = t] \u2212 E[Y |X = x, P 6= t] is estimated using causal trees and the prescribed treatment is the one which achieves the highest improvement, i.e., maxt\u2208[m] \u03b4(x, t)1vall. Note that this approach is no longer as interpretable as a single tree, given that there is no guarantee that the partitions induced by training for the different treatments will align.\nTo train the casual trees, we use the implementation from Athey and Imbens (2016), using default parameter values. Due to the lack of open-source code for Kallus (2017), we implement the impurity measure (6) in our own recursive partitioning procedure."}, {"heading": "4.2 Results on simulated datasets", "text": "Simulated datasets allow us to accurately evaluate the counterfactual outcomes associated with changing the price, and calculate the revenue from a policy, since the underlying probability model of simulated dataset is known. The datasets we examine come from the following generative model:\nY \u2217 = g(X) + h(X)P + , Y = { 1, if Y \u2217 > 0 0, if Y \u2217 \u2264 0 (7)\n\u2022 Dataset 1: linear probit model with no confounding: g(X) = X0, h(X) = \u22121 and X \u223c N(5, I2) and P \u223c N(5, 1).\n\u2022 Dataset 2: probit model with linear interaction: g(X) = X0, h(X) = \u2212X0. \u2022 Dataset 3: probit model with step interaction: g(X) = 5, h(X) = \u22121.21{X0 < \u22121} \u2212\n1.11{\u22121 \u2264 X0 < 0} \u2212 0.91{0 \u2264 X0 < 1} \u2212 0.81{1 \u2264 X0}. \u2022 Dataset 4: probit model with multi-dimensional step interaction: g(X) = 5, h(X) = \u22121.251{X0 < \u22121} \u2212 1.11{\u22121 \u2264 X0 < 0} \u2212 0.91{0 \u2264 X0 < 1} \u2212 0.751{1 \u2264 X0}+ 0.11{X1 < 0} \u2212 0.11{X1 \u2265 0}.\nWe set X = (X0, X1) \u223c N(0, I2), P \u223c N(X0 + 5, 1) , \u223c N(0, 1), i.i.d. unless otherwise mentioned. Dataset (1) and (6) are a common demand model used in the pricing literature, with and without confounding effects. Datasets (2)-(4) and (6), have heterogeneity in treatment effects making them suitable for testing personalized pricing. The visualization for all datasets can be found in the supplementary materials. All datasets except dataset (1) have confounding of observed, where the price observed is dependent on the features of the item. For the teacher model, we train a gradient boosted tree ensemble model using the lightGBM package Ke et al. (2017). We use default parameter values, with 50 boosting rounds. The discretized price set is set to 9 prices, ranging from the 10th to the 90th percentile of observed prices in 10% increments.\nWe compare the personalization tree (PT), causal tree (CT) and student prescriptive tree (SPT) in terms of their expected revenue, along with the fully personalized policy of the teacher model (lgbm). We also benchmark against the true optimum policy (optimal), which can be found by identifying the price which results in the highest revenue according to the underlying probability model for each observation, and as such may vary depending on the observations in the sample.\nWe explore how the expected revenue changes with the depth of a tree (k = {1, 2, 3, 4, 5}). The number of training samples is held constant at n = 5000. For each tree depth, we run 10 independent simulations for each dataset. Table 1 summarizes the best, the worst, and the average over the simulations of varying tree depth. We also compare their performance as the size of the dataset changes (n = {100, 300, 1000, 3000, 10000}), with the tree depth set to k = 3, again for 10 independent samples of each dataset. As an illustrative example, we show the performance on dataset (4) in Figure 1a and 1b, while detailed results for individual datasets can be found in the supplementary materials. We observe that SPT generally improves as the tree gets deeper, whereas PT and CT appear to perform worse. Moreover, while all methods improve as the training data size increases, the student tree model significantly outperforms other methods on small sample sizes. One explanation is that when the trees are deep or the training size is small, the number of observations\nper action is also small. This increases the variance of the estimation of the outcome of that action and makes it less likely that the action with the highest average revenue is the one with the strongest signal, rather than noise. Having a teacher model can help reduce some of this noise within the data for small leaves. Furthermore the student model (SPT) often outperforms the teacher model (lgbm), suggesting it is less prone to overfitting given inherently simpler functional form. We also observe that the SPT appears to perform relatively better than PT or CT in dataset (5) compared to (1), indicating that the teacher model also helps control for observed confounding effects as discussed previously, although it appears to outperform benchmarks in either setting."}, {"heading": "4.3 Dunnhumby grocery store case study", "text": "We benchmark the algorithms on a publicly available retail dataset collated by the analytics firm Dunnhumby2. \u201cThe complete journey\u201d dataset contains household level transactions over two years from a group of 2,500 households who are frequent shoppers at a grocery retailer, along with demographic information on each household. The customer features used in the personalized pricing algorithm are the discretized age of the shopper, discretized household income, whether they are a homeowner or renter, and the composition of the household (single male, single female, 2 adults no kids, 2 adults with kids, 1 adult with kids, or unknown). The data is processed into a format where each row corresponds to a shopping trip for a customer parameterized by their features, with a label to indicate whether the item was purchased at a listed price. More details on the data pre-processing procedure can be found in the supplementary materials. We focus on strawberries and milk (1 gallon) for the case study due to their relatively high sales and price variation which allows the models to accurately capture the price-demand relationship. Once processed, the strawberry dataset has 102080 shopping trips, with 3373 instances where strawberries are purchased, while milk has 89936 trips with 7688 purchases.\nDue to the lack of counterfactuals in the dataset, the efficacy of the pricing algorithms are evaluated using counterfactuals estimated from an independent lightGBM model. That is, for every price prescribed, the evaluator model predicts what the selling probability will be and calculates the expected revenue. We divide the dataset in half. The evaluator model is trained on a dataset which is independent from the dataset used for the teacher model and the tree-based algorithms. This approach for evaluating prescriptive algorithms is used in Biggs and Hariss (2018). Both the evaluator and teacher lightGBM model achieve an out-of-sample AUC of 0.79 for strawberries and 0.74 for milk with 50 boosting rounds and default classification parameters.\nFigure 2a and 2b show the performance of the pricing algorithms, the teacher model (lgbm) and the current pricing policy (no_change) in terms of predicted revenue over a range of different depth trees. SPT is able to outperform the other tree approaches and substantially improve upon the current pricing strategy with the predicted revenue per customer with a 67% increase for strawberries and a 22% improvement for milk, with a suitably chosen depth.\nAn example of the interpretable pricing policy for strawberries using SPT (k = 3) is shown in Figure 3. The model identifies that low income families are the most price sensitive and suggests a low price of $1.49. High income shoppers who are not renters are given a high price $2.99. The distribution of the prescribes prices corresponding to different pricing policies are shown in Figure 2c."}, {"heading": "4.4 Airline pricing case study", "text": "We worked with a large international airline and tested the prescriptive student tree method for pricing first class tickets. Interpretability is paramount as airlines need to file pricing rules with Airline Tariff Publishing Company to distribute and broadcast fares over the travel service network. A potential rule may look like the following: \u201cif the fare is between SFO and JFK, advance purchase window is less than 7 days and the flight departs on a weekday before 11am, prescribe a price of $400\". As regulation mandates that prices cannot differ based on customer features, the pricing algorithm works as an automated product differentiator by prescribing different prices to different tickets based on features such as advance booking window, time of the the flight, the origin and destination, distance of flight, the price of the main and basic cabin tickets, the inventory level of the first class, whether the flight is one-way or round trip, whether the round trip includes a Saturday night stay, etc.\n2https://www.dunnhumby.com/careers/engineering/sourcefiles\nFor commercial sensitivity reasons, some details of the study are omitted. The dataset used has 1,144,099 observations. A lightGBM model with 1000 rounds of boosting is used as the teacher model. The first class prices considered are split into $5.00 increments over the range of prices observed in the historical data. Figure 2d shows the predicted revenue of our proposed method to the other benchmarks, training on a 10% (n=114,400) sample and evaluated using a lightGBM model trained on the remaining 90% of the dataset. The student model is able to outperform the methods we benchmark against. Moreover, it is capable to achieving significant improvement over the current pricing with just a shallow tree."}, {"heading": "5 Conclusion", "text": "In this paper, we proposed a method which adapts the student-teacher framework for the prescriptive settings. The prescriptive student tree is capable of simultaneously creating segments of customers with similar valuations and determining the optimal revenue-maximizing price for a segment. We quantified the performance of our algorithm in terms of the regret. Experimental results on both synthetic and real datasets showed that our approach outperforms comparable methods. Moreover, the prescriptive student tree which produced interpretable rules is capable to approximate the opaque teacher model with relatively shallow trees. While we focus on personalized pricing, this method can be easily extended to other prescriptive settings such as personalized medicine."}, {"heading": "A Proof of Theorem 1", "text": "We begin by partitioning the feature space [0, 1]d into a set of hypercubes of width 1m , wherem \u2208 Z +. This results in md hypercubes. Note, the furthest distance between two points in each hypercube is\u221a d md .\nAn axis aligned binary tree of depth k \u2208 Z+ is able to partition the feature space into any set of 2k hyperrectangles. In particular find k\u0303 = min k \u2208 Z+ such that 2k \u2264 md. Then 1\nmd \u2264 1 2k\u0303 .\nDenote S(x) as subset of observations which are in the same hypercube as x. For notational convenience, let p\u2217 = \u03c4\u2217(x) and p\u0302 = arg maxp \u2211 i\u2208S(x) pf\u0302(xi, p) be an estimation of the revenue\nmaximizing price for hypercube which contains x. This is a feasible tree policy for a depth of k\u0303. The regret can be bounded as follows:\n\u03c4\u2217(x)f(x, \u03c4\u2217(x))\u2212 \u03c4\u0302(x)f(x, \u03c4\u0302(x)) = p\u2217f(x, p\u2217)\u2212 p\u0302f(x, p\u0302)\n= p\u2217f(x, p\u2217)\u2212 1 |S(x)| \u2211 i\u2208S(x) p\u0302f\u0302(xi, p\u0302) + 1 |S(x)| \u2211 i\u2208S(x) p\u0302f\u0302(xi, p\u0302)\u2212 p\u0302f(x, p\u0302)\n\u2264 p\u2217f(x, p\u2217)\u2212 1 |S(x)| \u2211 i\u2208S(x) p\u2217f\u0302(xi, p \u2217) + 1 |S(x)| \u2211 i\u2208S(x) p\u0302f\u0302(xi, p\u0302)\u2212 p\u0302f(x, p\u0302)\n\u2264 p\u2217f(x, p\u2217)\u2212 1 |S(x)| \u2211 i\u2208S(x) p\u2217f(xi, p \u2217) +K(n)\n+ 1 |S(x)| \u2211 i\u2208S(x) p\u0302f(xi, p\u0302)\u2212 p\u0302f(x, p\u0302) +K(n)\n= 1 |S(x)| \u2211 i\u2208S(x) [p\u2217f(x, p\u2217)\u2212 p\u2217f(xi, p\u2217)] +\n1 |S(x)| \u2211 i\u2208S(x) [p\u0302f(xi, p\u0302)\u2212 p\u0302f(x, p\u0302)] + 2K(n)\n\u2264 1 |S(x)| \u2211 i\u2208S(x) L|xi \u2212 x|+ 1 |S(x)| \u2211 i\u2208S(x) L|xi \u2212 x|+ 2K(n) \u2264 L \u221a d\nmd + L\n\u221a d md + 2K(n)\n\u2264 2L \u221a d\n2k\u0303 + 2K(n)\nThe third inequality follows from optimality of p\u0302 over the estimated revenue for the observations which fall in that leaf. The fourth inequality follows from the uniform bounds on the estimation error and triangle inequality. The fifth inequality follows from the Lipschitz continuity assumption and the sixth inequality follows from the maximum distance between two points in each hypercube. The final equality is due to the ability of a binary tree of depth k\u0303 to replicate the hypercubes of width 1m ."}, {"heading": "B Synthetic experiments", "text": "Datasets (1)-(6) used in section 4.2 are visualized in Figure 4. This provides insight into the shape of expected revenue, as a function of the treatment and X0. Note some figures have treatment on x-axis and X0 as the color, but in some figures this is reversed to give a more intuitive illustration of the function.\nFigure 5 shows how the expected revenue changes as the size of the training sets changes for the methods we benchmark against.\nFigure 6 shows how the expected revenue changes as the depth of the tree changes for the methods we benchmark against."}, {"heading": "C Preprocessing on Dunnhumby data", "text": "The original data is in a format where each row corresponds to an item purchased on a shopping trip. To make personalized prediction on whether an item would be purchased on a particular trip by a given shopper, the data was transformed to a format where each row corresponds a shopping trip, and a label was assigned to indicate whether the item was purchased. However, when an item is not purchased in a particular shopping trip, it\u2019s price is not recorded. Due to observed differences in the price trends over time, different imputation approaches are used for strawberries and milk. For strawberries, the price was assumed to be the mode of the price of the previous 3 sales. This is consistent with the slow moving price trends observed for strawberries where the price changed infrequently. There did not appear to be differences in pricing on a store to store level. This imputation approach has an accuracy of 83% and is marginally more accurate than just using the previous price 80%. Milk on the other hand, had significant variability in the price between stores. To impute the price for milk, we used the price of the last milk sale at that store. Only stores with at least 50 sales of milk were included in the data, to avoid long time lags between purchases. This imputation approach has an accuracy of 77% accuracy with last observation of the store, compared to 49% using the last purchased price without conditioning on the store.\nThe price for strawberries follows a discrete ladder of prices (from $1.99 to $4.99 per unit in $0.50 increments). The price for milk gallons follows a more irregular discrete ladder of prices, with 90% of prices falling in the set {$1.66, $1.99, $2.32, $2.49, $2.69, $2.79, $2.89}, with $2.32, $2.49, $2.69 being the most popular prices. The personalized prices in the algorithms are restricted to the same discretization. The processed data was also restricted to shoppers who purchased strawberries or milk at least once over the 2 years studied. For strawberries, this results in a dataset with 102080 shopping trips, with 3373 instances where strawberries are purchased, while milk has 89936 trips with 7688 purchases."}], "title": "Model Distillation for Revenue Optimization: Interpretable Personalized Pricing", "year": 2020}