{
  "abstractText": "Explainable artificial intelligence is gaining attention. However, most existing methods are based on gradients or intermediate features, which are not directly involved in the decision-making process of the classifier. In this paper, we propose a slot attention-based light-weighted classifier called SCOUTER for transparent yet accurate classification. Two major differences from other attention-based methods include: (a) SCOUTER\u2019s explanation involves the final confidence for each category, offering more intuitive interpretation, and (b) all the categories have their corresponding positive or negative explanation, which tells \u201cwhy the image is of a certain category\u201d or \u201cwhy the image is not of a certain category.\u201d We design a new loss tailored for SCOUTER that controls the model\u2019s behavior to switch between positive and negative explanations, as well as the size of explanatory regions. Experimental results show that SCOUTER can give better visual explanations while keeping good accuracy on a large dataset. Code is available.",
  "authors": [
    {
      "affiliations": [],
      "name": "Liangzhi Li"
    },
    {
      "affiliations": [],
      "name": "Bowen Wang"
    },
    {
      "affiliations": [],
      "name": "Manisha Verma"
    },
    {
      "affiliations": [],
      "name": "Yuta Nakashima"
    },
    {
      "affiliations": [],
      "name": "Ryo Kawasaki"
    },
    {
      "affiliations": [],
      "name": "Hajime Nagahara"
    }
  ],
  "id": "SP:3ac18ce02e28c1d45ffae5420b9ee52c70d3d5a8",
  "references": [
    {
      "authors": [
        "N. Carion",
        "F. Massa",
        "G. Synnaeve",
        "N. Usunier",
        "A. Kirillov",
        "S. Zagoruyko"
      ],
      "title": "End-to-End Object Detection with Transformers",
      "venue": "arXiv preprint arXiv:2005.12872 .",
      "year": 2020
    },
    {
      "authors": [
        "A. Chattopadhay",
        "A. Sarkar",
        "P. Howlader",
        "V.N. Balasubramanian"
      ],
      "title": "Grad-CAM++: Generalized gradientbased visual explanations for deep convolutional networks",
      "venue": "IEEE WACV, 839\u2013847.",
      "year": 2018
    },
    {
      "authors": [
        "J. Deng",
        "W. Dong",
        "R. Socher",
        "L. Li",
        "Kai Li",
        "Li Fei-Fei."
      ],
      "title": "ImageNet: A large-scale hierarchical image database",
      "venue": "IEEE CVPR, 248\u2013255.",
      "year": 2009
    },
    {
      "authors": [
        "S. Desai",
        "H.G. Ramaswamy"
      ],
      "title": "Ablation-CAM: Visual Explanations for Deep Convolutional Network via Gradient-free Localization",
      "venue": "IEEE WACV, 972\u2013980.",
      "year": 2020
    },
    {
      "authors": [
        "J. Devlin",
        "M.-W. Chang",
        "K. Lee",
        "K. Toutanova"
      ],
      "title": "BERT: Pre-training of deep bidirectional transformers for language understanding",
      "venue": "arXiv preprint arXiv:1810.04805 .",
      "year": 2018
    },
    {
      "authors": [
        "X. Du",
        "T.-Y. Lin",
        "P. Jin",
        "G. Ghiasi",
        "M. Tan",
        "Y. Cui",
        "Q.V. Le",
        "X. Song"
      ],
      "title": "SpineNet: Learning scalepermuted backbone for recognition and localization",
      "venue": "IEEE CVPR, 11592\u201311601.",
      "year": 2020
    },
    {
      "authors": [
        "Y. Goyal",
        "Z. Wu",
        "J. Ernst",
        "D. Batra",
        "D. Parikh",
        "S. Lee"
      ],
      "title": "Counterfactual Visual Explanations",
      "venue": "ICML.",
      "year": 2019
    },
    {
      "authors": [
        "K. He",
        "X. Zhang",
        "S. Ren",
        "J. Sun"
      ],
      "title": "Deep Residual Learning for Image Recognition",
      "venue": "IEEE CVPR, 770\u2013778.",
      "year": 2016
    },
    {
      "authors": [
        "L.A. Hendricks",
        "R. Hu",
        "T. Darrell",
        "Z. Akata"
      ],
      "title": "Generating Counterfactual Explanations with Natural Language",
      "venue": "ICML Workshop, 95\u201398.",
      "year": 2018
    },
    {
      "authors": [
        "G. Hinton",
        "O. Vinyals",
        "J. Dean"
      ],
      "title": "Distilling the knowledge in a neural network",
      "venue": "arXiv preprint arXiv:1503.02531 .",
      "year": 2015
    },
    {
      "authors": [
        "A.G. Howard",
        "M. Zhu",
        "B. Chen",
        "D. Kalenichenko",
        "W. Wang",
        "T. Weyand",
        "M. Andreetto",
        "H. Adam"
      ],
      "title": "Mobilenets: Efficient convolutional neural networks for mobile vision applications",
      "venue": "arXiv preprint arXiv:1704.04861 .",
      "year": 2017
    },
    {
      "authors": [
        "J. Hu",
        "L. Shen",
        "G. Sun"
      ],
      "title": "Squeeze-and-excitation networks",
      "venue": "IEEE CVPR, 7132\u20137141.",
      "year": 2018
    },
    {
      "authors": [
        "G. Huang",
        "Z. Liu",
        "L. Van Der Maaten",
        "K.Q. Weinberger"
      ],
      "title": "Densely connected convolutional networks",
      "venue": "IEEE CVPR, 4700\u20134708.",
      "year": 2017
    },
    {
      "authors": [
        "F. Locatello",
        "D. Weissenborn",
        "T. Unterthiner",
        "A. Mahendran",
        "G. Heigold",
        "J. Uszkoreit",
        "A. Dosovitskiy",
        "T. Kipf"
      ],
      "title": "Object-Centric Learning with Slot Attention",
      "venue": "arXiv preprint arXiv:2006.15055 .",
      "year": 2020
    },
    {
      "authors": [
        "I. Loshchilov",
        "F. Hutter"
      ],
      "title": "Decoupled Weight Decay Regularization",
      "venue": "ICLR.",
      "year": 2018
    },
    {
      "authors": [
        "D. Mascharka",
        "P. Tran",
        "R. Soklaski",
        "A. Majumdar"
      ],
      "title": "Transparency by design: Closing the gap between performance and interpretability in visual reasoning",
      "venue": "IEEE CVPR, 4942\u20134950.",
      "year": 2018
    },
    {
      "authors": [
        "R. Naidu",
        "J. Michael"
      ],
      "title": "SS-CAM: Smoothed Score-CAM for Sharper Visual Feature Localization",
      "venue": "arXiv preprint arXiv:2006.14255 .",
      "year": 2020
    },
    {
      "authors": [
        "D. Omeiza",
        "S. Speakman",
        "C. Cintas",
        "K. Weldermariam"
      ],
      "title": "Smooth Grad-CAM++: An enhanced inference level visualization technique for deep convolutional neural network models",
      "venue": "arXiv preprint arXiv:1908.01224 .",
      "year": 2019
    },
    {
      "authors": [
        "N. Parmar",
        "A. Vaswani",
        "J. Uszkoreit",
        "L. Kaiser",
        "N. Shazeer",
        "A. Ku",
        "D. Tran"
      ],
      "title": "Image Transformer",
      "venue": "ICML.",
      "year": 2018
    },
    {
      "authors": [
        "R.R. Selvaraju",
        "M. Cogswell",
        "A. Das",
        "R. Vedantam",
        "D. Parikh",
        "D. Batra"
      ],
      "title": "Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization",
      "venue": "IEEE ICCV, 618\u2013626.",
      "year": 2017
    },
    {
      "authors": [
        "A. Shrikumar",
        "P. Greenside",
        "A. Kundaje"
      ],
      "title": "Learning Important Features through Propagating Activation Differences",
      "venue": "ICML, 31453153.",
      "year": 2017
    },
    {
      "authors": [
        "K. Simonyan",
        "A. Zisserman"
      ],
      "title": "Very deep convolutional networks for large-scale image recognition",
      "venue": "arXiv preprint arXiv:1409.1556 .",
      "year": 2014
    },
    {
      "authors": [
        "C. Szegedy",
        "Wei Liu",
        "Yangqing Jia",
        "P. Sermanet",
        "S. Reed",
        "D. Anguelov",
        "D. Erhan",
        "V. Vanhoucke",
        "A. Rabinovich"
      ],
      "title": "Going deeper with convolutions",
      "venue": "IEEE CVPR, 1\u20139.",
      "year": 2015
    },
    {
      "authors": [
        "M. Tan",
        "Q.V. Le"
      ],
      "title": "Efficientnet: Rethinking model scaling for convolutional neural networks",
      "venue": "arXiv preprint arXiv:1905.11946 .",
      "year": 2019
    },
    {
      "authors": [
        "A. Vaswani",
        "N. Shazeer",
        "N. Parmar",
        "J. Uszkoreit",
        "L. Jones",
        "A.N. Gomez",
        "\u0141. Kaiser",
        "I. Polosukhin"
      ],
      "title": "Attention is all you need",
      "venue": "NeurIPS, 5998\u20136008.",
      "year": 2017
    },
    {
      "authors": [
        "H. Wang",
        "Z. Wang",
        "M. Du",
        "F. Yang",
        "Z. Zhang",
        "S. Ding",
        "P. Mardziel",
        "X. Hu"
      ],
      "title": "Score-CAM: Score-Weighted Visual Explanations for Convolutional Neural Networks",
      "venue": "IEEE CVPR Workshops, 24\u201325.",
      "year": 2020
    },
    {
      "authors": [
        "P. Wang",
        "N. Vasconcelos"
      ],
      "title": "SCOUT: Self-aware Discriminant Counterfactual Explanations",
      "venue": "In IEEE CVPR,",
      "year": 2020
    },
    {
      "authors": [
        "Z. Wu",
        "M. Palmer"
      ],
      "title": "Verbs Semantics and Lexical Selection",
      "venue": "ACL, 133138.",
      "year": 1994
    },
    {
      "authors": [
        "N. Xie",
        "F. Lai",
        "D. Doran",
        "A. Kadav"
      ],
      "title": "Visual entailment: A novel task for fine-grained image understanding",
      "venue": "arXiv preprint arXiv:1901.06706 .",
      "year": 2019
    },
    {
      "authors": [
        "N. Xie",
        "G. Ras",
        "M. van Gerven",
        "D. Doran"
      ],
      "title": "Explainable deep learning: A field guide for the uninitiated",
      "year": 2020
    },
    {
      "authors": [
        "S. Xie",
        "R. Girshick",
        "P. Doll\u00e1r",
        "Z. Tu",
        "K. He"
      ],
      "title": "Aggregated residual transformations for deep neural networks",
      "venue": "IEEE CVPR, 1492\u20131500.",
      "year": 2017
    },
    {
      "authors": [
        "H. Zhang",
        "C. Wu",
        "Z. Zhang",
        "Y. Zhu",
        "Z. Zhang",
        "H. Lin",
        "Y. Sun",
        "T. He",
        "J. Muller",
        "R. Manmatha",
        "M. Li",
        "A. Smola"
      ],
      "title": "ResNeSt: Split-Attention Networks",
      "venue": "arXiv preprint arXiv:2004.08955 .",
      "year": 2020
    },
    {
      "authors": [
        "Q. Zhang",
        "Y. Yang",
        "H. Ma",
        "Y.N. Wu"
      ],
      "title": "Interpreting CNNs via decision trees",
      "venue": "IEEE CVPR, 6261\u20136270.",
      "year": 2019
    },
    {
      "authors": [
        "B. Zhou",
        "A. Khosla",
        "A. Lapedriza",
        "A. Oliva",
        "A. Torralba"
      ],
      "title": "Learning deep features for discriminative localization",
      "venue": "IEEE CVPR, 2921\u20132929.",
      "year": 2016
    }
  ],
  "sections": [
    {
      "text": "large dataset. Code is available1."
    },
    {
      "heading": "1 Introduction",
      "text": "It is of great significance to know how deep learning models make predictions, especially for the fields like medical diagnosis, where potential risks exist when black-box models are adopted. Therefore, explainable artificial intelligence (XAI), which can give a close look into the inference process of deep learning models, has gained lots of attention.\nThe most popular paradigm in XAI is positive explanation, which is also called as attributive explanation. Positive explanation involves the contribution levels of pixels or regions to the final prediction (Selvaraju et al. 2017; Desai and Ramaswamy 2020). Therefore, it can be interpreted as positive supports of the model\u2019s prediction, answering the ques-\n1https://github.com/wbw520/scouter\nar X\niv :2\ntion \u201cwhy image x belongs to category t\u201d, or in short, \u201cwhy x is t\u201d, as shown in Fig. 1(left). There should also exist negative explanation to answer the negated question \u201cwhy image x does not belong to category t,\u201d or equivalently \u201cwhy x is not t\u201d, as shown in Fig. 1(right), providing negative supports on the prediction. Although both types of explanation are helpful to unveil what the model actually looks at, the negative explanation is underexplored.\nThe positive explanation is suitable for the classification among categories with substantial visual differences, where it is very useful to look for the unique patterns belonging to the target category. The negative explanation is more informative for fine-grained classifications, in which different categories mostly share the appearance but are differentiated by small and unique visual patterns. In this case, the patterns that deny a certain category may be beneficial. However, most XAI methods are designed only for positive explanation and cannot be easily extended to negative explanation.\nAnother important issue is explanatory regions\u2019 size. Typically, explanation for image recognition is provided as a heat map as in Fig. 1, highlighting explanatory regions. In fact, the existing XAI method, especially the gradient-based methods (Selvaraju et al. 2017), prefer giving large explanatory regions that cover a most part of the foreground objects (Wang and Vasconcelos 2020). These regions are usually consistent and may not fit the target objects well. Also, these approaches often fail in distinguishing small objects in their explanation.\nThese problems make XAI less useful because one prefers to know the precise regions that the model pays most attention to. In addition, preferences on explanatory regions may vary for different tasks or datasets. However, existing methods are not flexible enough to cope with such needs.\nTo address the aforementioned shortcomings, we propose a new XAI method named SCOUTER (Slot-based COnfigUrable and Transparent classifiER). SCOUTER serves as a classifier that directly converts the visual explanations into the final prediction results using a simple weighted sum function. This is different from the post-hoc approaches since explanation by SCOUTER is intrinsically involved in the forward process. Also, compared with other attentionbased methods, SCOUTER is more intuitive because the attention map directly contributes to the final classification result in a transparent way.\nSCOUTER is based on two major building blocks. The first one is the recently-emerged slot attention (Locatello et al. 2020). Slot attention is initially designed for extracting object-centric features on synthetic datasets. We modified several important components in slot attention and propose an explainable slot attention (xSlot Attention) module that can work on arbitrary types of images. The output from the xSlot Attention module is the confidence value for each category. Specifically, on top of the backbone network for feature extraction, the model has one slot dedicated for each category. Therefore, SCOUTER gives confidence values for all categories, as commonly used fully-connected (FC) layer-based classifiers do, and the softmax is adopted to find the category with the highest confidence. Another merit of this design is that each category can also have its own\nexplanation from the inherent attention map. The second building block is the SCOUTER loss, which makes the model pay more attention in finding humanunderstandable explanation while maintaining the prediction performance. In addition, the SCOUTER loss provides a way to switch between positive/negative explanations.\nSome visualization examples of SCOUTER are shown in\nFig. 1, with both configurations of the SCOUTER loss (positive or negative), the model can learn to find explanation to support the input image being or not being in the queried categories, respectively.\nThe main contributions of our work include: \u2022 a new classifier to give visual explanations, \u2022 a new loss to adjust the visualized area for different\ntasks/datasets, and \u2022 a way to switch the model\u2019s behavior to produce posi-\ntive/negative explanations."
    },
    {
      "heading": "2 Related Work",
      "text": ""
    },
    {
      "heading": "2.1 Image Classification",
      "text": "Images have been involved in various computer vision tasks, including object detection, semantic segmentation, etc. Most deep neural network-based models for these tasks have a common structure: as the first step, a backbone networkB is used to extract features F from the image x. Then a downstream network is tailored for a different task, produces a desired output from F , e.g. the region proposal network and pyramid pooling module. Among these tasks, image classification is one of the fundamental tasks and is used for pretraining B. Some popular B include VGG (Simonyan and Zisserman 2014), Inception (Szegedy et al. 2015), ResNet (He et al. 2016) and its variants (Xie et al. 2017; Zhang et al. 2020), as well as the recently introduced SpineNet (Du et al. 2020).\nNevertheless of the importance, researchers still prefer using a simple classifier for image classification, consisting of one or two FC layers and softmax. One major reason is that, despite its black-box nature, the FC classifier is the most general and expressive choice for image classification tasks. In this paper, we show the possibility to use a more sophisticated classifier in order to form a transparent decision-making process while keeping a similar classification performance as FC classifiers."
    },
    {
      "heading": "2.2 Explainable AI",
      "text": "There are mainly three kinds of methods for XAI (Xie et al. 2020), i.e. visualization, distillation, and intrinsic methods.\nVisualization methods are usually in the form of heat maps, which are defined as the level of responses occurred in the final output when some regions in the input or the intermediate features get modified. The most popular visualization methods are usually based on back-propagation, including GradCAM (Selvaraju et al. 2017) and DeepLIFT (Shrikumar, Greenside, and Kundaje 2017). One of the biggest advantages of these methods is that they produce visual explanation (i.e., heat maps) for each class. This kind of explanation is called attributive explanation. There is another kind of explanation named counterfactual explanation\n(Goyal et al. 2019), which represents \u201chow to change image x (belongs to ta) to make it look like images in tb\u201d. Recently, a new kind of explanation named discriminant explanation (Wang and Vasconcelos 2020) is proposed, which dot-multiplies the attributive explanation of the predicted category tp and the complement of the explanation of target category ti. Discriminant explanation can answer the question of \u201cwhy image x belongs to tp rather than ti\u201d, which is very similar to our proposed negative explanation. The major difference is, the discriminant explanation actually tries to identify the factors that differentiate ti from one single class tp, while the proposed negative explanation is designed to spot the factors to deny the target category ti itself.\nDistillation methods are built upon the concept of model distillation (Hinton, Vinyals, and Dean 2015). The basic idea is to use a new inherently transparent model to mimic the output and behaviors of a trained black-box deep neural network (Zhang et al. 2019).\nIntrinsic methods involve explanations as a part of their models, most popularly, as attentions (Mascharka et al. 2018; Xie et al. 2019). Thus, this kind of XAI methods may be more desirable as it does not need to seek explanations after it outputs the final prediction (Xie et al. 2020). The attention maps used in this kind of methods can also get visualized to give the reasons for choosing the predicted category, which is called positive explanation in this paper and is equal to the attributive explanation in the visualization methods. Besides that, we also propose an opposite explanation named negative explanation to give the reasons to deny categories. This is a new explanation and different from the existing ones.\nAccording to the above definitions, the proposed SCOUTER method belongs to the intrinsic methods because the explanations are used as part of the forward propagation. However, at the same time, SCOUTER adopts a design to couple explanation modules with respective confidence values computed at the classifier, which is completely different from the existing attention-based methods that generate attention maps within backbone networks. Also, SCOUTER is able to provide explanation for each category, which is a characteristic of the visualization methods."
    },
    {
      "heading": "2.3 Self-attention in Computer Vision",
      "text": "Self-attention is introduced in the Transformers (Vaswani et al. 2017) in which self-attention layers scan through the input elements one by one and update it using the aggregation over the whole input. Initially, self-attention, or the Transformer, is mainly used for replacing recurrent neural networks in tasks that involve sequential data, e.g., natural language processing (Devlin et al. 2018).\nImage Transformer (Parmar et al. 2018) is one of the first works that adopt the Transformer in the CV field. With the merits brought by self-attention, this method greatly increases the size of its processing images and can achieve good performance in the image generation task. DEtection TRansformer (DETR) (Carion et al. 2020) is for the object detection task, but it can also be applied to instance segmentation tasks.\nSelf-attention is also adopted to the slot attention module (Locatello et al. 2020) to extract object-centric features from synthetic images. SCOUTER is based on a variant of the slot attention; however, it is designed to get explanations for classification process and can be used on natural images."
    },
    {
      "heading": "3 The SCOUTER Classifier",
      "text": "Given an image x, the objective of classification network is to find its most probable category l in the category set \u2126 = {\u03c91, \u03c92, . . . , \u03c9n}. As mentioned in Section 2.1, this can be done by first extracting features F = B(x) \u2208 Rc\u00d7h\u00d7w using a backbone network B. F is then mapped into a score vector s \u2208 Rn using FC layers and get the category \u03c9 with the highest score using softmax. However, FC classifiers are more like a black box and lack in transparency.\nWe replace such FC classifier with our SCOUTER (Fig. 2(a)), consisting of xSlot Attention module, which produces the confidence for each category given features F . The whole network, including the backbone, is trained with SCOUTER loss, which is designed for controlling the size of explanatory regions and switching between positive and negative explanation."
    },
    {
      "heading": "3.1 xSlot Attention",
      "text": "In the original Slot Attention mechanism (Locatello et al. 2020), a slot is a representation of a local region aggregated based on the attention over the feature maps. A single slot attention module with L slots is attached on top of backbone network B, which produces L different features as output. This configuration is handy when there are multiple objects of interest. This idea can be easily transferred to spot the factors in the input image that support a certain prediction.\nThe main building block of SCOUTER is the xSlot Attention module, which is a variant of Slot Attention module tailored for explanation. Each slot of xSlot Attention mod-\nule is associated with a category and gives the confidence that the input image falls into the category. With the slot attention mechanism, this prediction is required to attend the essential factors in the image that are correlated to the category.\nGiven feature F , xSlot Attention module updates slotw(t)l for T times, where t represents the slot after t updates and l \u2208 \u2126 is the category associated to this slot. The slot is initialized with random weights, i.e.,\nw(0)c \u223c N (\u00b5, diag(\u03c3)) \u2208 R1\u00d7c \u2032 , (1)\nwhere \u00b5 and \u03c3 are the mean and variance of a Gaussian. We denote the slots for all categories by W (t) \u2208 Rn\u00d7c\u2032 .\nThe slot W (t+1) is updated using W (t) and feature F . Firstly, F goes through a 1 \u00d7 1 convolutional layer to reduce the number of channels and the ReLU nonlinearity as F \u2032 = ReLU(Conv(F )) \u2208 Rc \u2032\u00d7d + , with F \u2019s spatial dimensions being flattened (d = hw). F \u2032 is augmented by adding the position embedding to take the spatial information into account, following (Vaswani et al. 2017; Carion et al. 2020), i.e. F\u0303 = F \u2032 + PE, where PE is the position embedding. We then use two multilayer perceptrons (MLPs) Q and K, each of which has three FC layers and the ReLU nonlinearity between them, which gives more flexibility in query and key computation for self-attention. With\nQ(W (t)) \u2208 Rn\u00d7c \u2032 , K(F\u0303 ) \u2208 Rc \u2032\u00d7d, (2)\nwe obtain the dot-product attention A(t) using sigmoid \u03c3 as\nA(t) = \u03c3(Q(W (t))K(F\u0303 )) \u2208 (0, 1)n\u00d7d. (3) The attention is used to compute the weighted sum of fea-\ntures in the spatial dimensions by\nU (t) = A(t)F \u2032 > \u2208 Rn\u00d7c \u2032 , (4)\nand slot W (t) is eventually updated through a gated recurrent unit (GRU) as\nW (t+1) = GRU(U (t),W (t)), (5)\ntaking U (t) and W (t) as input and hidden state, respectively. Following the original Slot Attention module, we update the slot for T = 3 times.\nThe output of the xSlot Attention module is the sum of all elements for category l in U (T ), which is a function of F . Formally, the output of xSlot Attention module is:\nxSlot(F ) = U (T )1c\u2032 \u2208 Rn+, (6) where 1 is the column vector with all c\u2032 elements being 1.\nNote that in the original Slot Attention module, a linear transformation is applied to the features, i.e., V (F\u0303 ), which is then weighted using Eq. (4). However, the xSlot Attention module omits this transformation as it already has a sufficient number of learnable parameters in Q, K, GRU, etc. and thus the flexibility. Also, the confidences, given by Eq. (6), are typically computed by an FC layer, while SCOUTER just sums up all features for each category. This simplicity is essential for a transparent classifier as discussed in Section 3.3."
    },
    {
      "heading": "3.2 SCOUTER Loss",
      "text": "The whole model, including the backbone network, can be trained by simply applying softmax to xSlot(F ) and minimizing cross-entropy loss LCE. However, there is a phenomenon that, in some cases, the model prefers attending to a broad area. This phenomenon usually happens depending on the content of the image; however, if we have prior knowledge of the data, it can be beneficial to have control over the area of attended regions. Therefore, we design a new loss named SCOUTER loss to limit the area of attended regions. SCOUTER loss is defined by\nLSCOUTER = LCE + \u03bbLArea (7)\nwhere LArea is the area loss, \u03bb is a hyper-parameter to adjust the importance of the area loss. The area loss is defined by\nLArea = 1 > nA (T )1d, (8)\nwhich simply sums up all the elements in A(T ). With larger \u03bb, SCOUTER attends a smaller region. On the contrary, it prefers a larger attended area with smaller \u03bb. We will show in the experiment section the relationship between \u03bb and area sizes as well as the classification accuracy."
    },
    {
      "heading": "3.3 Switching Positive and Negative Explanation",
      "text": "The model with the SCOUTER loss in Eq. (7) can only provide positive explanation since larger elements in A(T ) means the prediction is made based on the corresponding features. We introduced a hyper-parameter e \u2208 {+1,\u22121} in Eq. (6), i.e.,\ns = xSlote(F ) = e \u00b7 U (T )1c\u2032 \u2208 Rn+. (9)\nThis hyper-parameter configures the xSlot Attention module to provide either positive or negative explanation.\nWith softmax and the cross-entropy loss, the model leans to give the largest confidence sl corresponding to groundtruth (GT) category l and a smaller value sl\u2032 to wrong category l\u2032 6= l. For e = +1, all elements given by xSlot is non-negative since both A(T ) and F \u2032 are non-negative and thus U (T ) is. For arbitrary non-negative F \u2032, thanks to simple reduction in Eq. (6), larger sl can be produced only when some elements in a(T )l , the row vector in A\n(T ) corresponding to l, is close to 1, whereas a smaller sl\u2032 is given when all elements in a(T )l are close to 0. Therefore, by setting e to +1, the model learns to find the common pattern among the images of the GT category, and the visualization of a(T )l serves as positive explanation, as shown in Fig. 1(left).\nOn the contrary, for e = \u22121, all elements in s are negative and thus the prediction by Eq. (9) gives sl close to 0 for correct category l and smaller sl\u2032 for non-GT category l\u2032. To make sl close to 0, all elements in a (T ) l must be close to 0, and a smaller sl\u2032 is given when a (T ) l\u2032 has some elements close to 1. For this, the model leans to find the patterns that do not exist in the images of GT category. As a result, a(T )l\u2032 can be used as negative explanation, as shown in Fig. 1(right)."
    },
    {
      "heading": "4 Experiments",
      "text": ""
    },
    {
      "heading": "4.1 Experimental Setup",
      "text": "We chose to use the ImageNet dataset (Deng et al. 2009), which is a large dataset with 1000 categories, to evaluate SCOUTER, because of three reasons: (i) It is commonly used in the evaluation of classification models. (ii) There are many classes with similar semantics and appearance, and the relationships among them are available in the synsets of the WordNet, which can be used to evaluate positive and negative explanation. (iii) Bounding boxes are available for foreground objects, which helps measure the accuracy of visual explanation. In some experiments, we use subsets of ImageNet by extracting the first n (0 < n \u2264 1, 000) categories in the ascending order of the category IDs.\nThe models were trained on the training set for 20 epochs and the performance scores are computed on the validation set with the trained models after the last epoch. All the quantitative results are obtained by averaging the scores from three independent runs.\nOur models are implemented with PyTorch. AdamW (Loshchilov and Hutter 2018) is adopted as the optimizer with the initial learning rate of 10\u22124. The featureF extracted by the backbone network is mapped into a new feature F \u2032 with a channel number c\u2032 = 64. All the experiments are conducted on three local GPU servers. Each server is equipped with two Intel(R) Xeon(R) Gold 5122 (@3.60GHz) CPUs, four Tesla V100-SXM2 (32GB) GPUs, as well as a 384GB memory. Every run of model training occupied one single GPU."
    },
    {
      "heading": "4.2 Explainability",
      "text": "Explainability is usually evaluated qualitatively (Goyal et al. 2019) or with some simple quantitative tests from machine teaching experiments (Hendricks et al. 2018), which are subjective and may not be so convincing (Wang and Vasconcelos 2020). Recently, a new evaluation metric using the intersection over union (IoU) between explanations and semantic masks is proposed (Wang and Vasconcelos 2020). However, this is designed for the counterfactual explanations, in which two categories are involved in one explanation. Also, our objective is to generate small yet accurate explanation, rather than having the same shape and size as the objects of interest. Therefore, the intersection between our explanation and the semantic mask can be always small and thus IoUs are biased toward 0.\nWe instead evaluate the accuracy of our visual explanation by the precision that measures how much regions spotted by our explanation are covered by the objects of interest. We use bounding boxes provided in ImageNet as a proxy of the object regions and compute the percentage of the pixels located inside the bounding box over the total pixel numbers in the whole explanation. Specifically, for set I of all pixels in the input image and set D of all pixels in the bounding box, our precision is defined as\nPrecisionl =\n\u2211 i\u2208D a\nl i\u2211\ni\u2208I a l i\n(10)\nwhere category l \u2208 \u2126 and ali is the value of pixel i in A\u0304l, which is attention map A resized to the same size as the input image by bilinear interpolation. This metric is associated with category l since SCOUTER produces different explanatory regions for different classes. For positive explanation, the GT category\u2019s precision must be close to 1, while the other categories\u2019 precision is not necessarily high. We compute this metric for the GT category and non-GT categories.\nIn addition, we calculate the overall size of the explanation areas by\nAreal = \u2211 i\u2208I ali, (11)\nwhere a smaller value is better to pinpoint the essential factor to differentiate one class from the others.\nThe results are shown in Table 1. We can see that SCOUTER can keep a small area size while achieving good precision scores. In addition, SCOUTER+ shows higher precision on non-GT classes than the GT class, while SCOUTER\u2212 performs oppositely. This is mainly because the background pixels get fewer inferences from the foreground pixels when the attentions on foreground pixels get weaker. Therefore, the overall precision becomes higher.\nTo further explore the explanation for non-GT categories, we define the semantic similarity between categories based on (Wu and Palmer 1994), which uses WordNet, as\nSimilarity = 2 d(LCS(l, l\u2032))\nd(l) + d(l\u2032) , (12)\nwhere d(\u00b7) gives the depth of category l in WordNet, and LCS(l, l\u2032) is to find the least common subsumer of two arbitrary categories l and l\u2032. We define the highly-similar categories as the category pairs with a similarity score no less than 0.9, similar categories as with a score in [0.7, 0.9), and the remaining categories are regarded as dissimilar categories.\nTable 2 summarizes the area sizes of the explanatory regions for the GT category, highly-similar categories, similar categories, and dissimilar categories. We can clearly see a\ntrend: SCOUTER+ decreases the area size when the intercategory similarity becomes smaller, while SCOUTER\u2212 gives larger explanatory regions for the dissimilar categories.\nSome visual results are given in Figs. 3 and 4. It can be seen that SCOUTER can give reasonable and accurate explanation. If we compare the positive explanation with the results from existing methods, i.e., GradCAM++ (Chattopadhay et al. 2018), and Smoothed Score-CAM (SS-CAM) (Wang et al. 2020), we find that SCOUTER+ can give explanations with more flexible shapes which fit the target objects better. For example, in the first row of Fig. 3, SCOUTER+ gives more accurate attention around the neck. In the second row, it can accurately find the individuals rather than covering all of them with connected regions. In addition, in the last row, SCOUTER+ spots the dorsal fin, which is also an important pattern to recognize sharks. In Fig. 4, SCOUTER\u2212 can also find the critical factors to deny categories, e.g., the wattle of the hen, and the hammerhead and the fin of the shark."
    },
    {
      "heading": "4.3 Classification Performance",
      "text": "SCOUTER is a classifier that replaces FC classifiers; therefore, the classification performance is the main criterion. We compare SCOUTER and FC classifiers with several commonly-used backbone networks with respect to computational costs (flops and parameter sizes) and classification accuracy. The inputs are with the size of 260 \u00d7 260 and the number of classes is set to 1, 000. The results in Table 3 show that, compared with FC classifiers, SCOUTER requires larger flops but fewer parameters. The increase of flops is because the xSlot module has some small FC layers (i.e., Q and K), GRU, and some matrix calculations. However, we can also see that this increase is not very significant and it increases linearly (Fig. 6). On the other hand, the\ndecrease of the parameter size is because there are only c\u2032 (c\u2032 = 64 in our implementation) learnable parameters for each category, which is much less than the parameter size of the FC layers.\nThe classification accuracies with different models are shown in Fig. 5. With the increase of the category number, both the FC classifier and SCOUTER show a performance drop. In total, they show similar trends with respect to the category number."
    },
    {
      "heading": "4.4 Choice of \u03bb",
      "text": "We show the relationship between the \u03bb value used in training process with the area size of explanation as well as the classification accuracy in Fig. 7 for n = 10. A clear pattern is that the area sizes of both SCOUTER+ and SCOUTER\u2212 drop quickly with the increase of \u03bb, as the model has larger attention on the area size factor of the SCOUTER loss. However, there are no significant changes in the classification accuracy for both SCOUTER+ and SCOUTER\u2212, which should be due to the cross entropy part of the SCOUTER loss.\nAlso, according to the visual results in Figs. 3 and 4, when giving a larger \u03bb, SCOUTER appears to not simply decrease its explanatory region size without changing the location. Instead, SCOUTER usually changes its focus from some obvious but large patterns to some small yet also decisive pat-\nterns with increment of \u03bb. For example, in the first row of Fig. 4, when \u03bb is small, SCOUTER\u2212 can easily make a decision that the input image is not a hen because of its heads and unique feathers on the neck. However, with increment of \u03bb, SCOUTER finds smaller patterns to support its decisions and thus the region of interest changes from head and neck, to the neck only, and ultimately, to the wattle only."
    },
    {
      "heading": "5 Conclusion",
      "text": "A new explanatory classifier is proposed in this paper. There are two variants, i.e., SCOUTER+ and SCOUTER\u2212, which can respectively give positive and negative explanation on the classification process. SCOUTER adopts an explanatory variant of the Slot Attention module, namely, xSlot Attention, which is also based on the dot-product attention and is light-weighted. Experimental results prove that the proposed method can give accurate explanation while keeping good classification performance.\nA problem of SCOUTER is that we found some cases in which training fails by chance when there are more than 100 categories. More exploration is needed to improve its stability when dealing with a large number of categories. Another limitation is that SCOUTER requires training to switch between positive and negative explanation, which may not be convenient for some applications."
    },
    {
      "heading": "6 Acknowledgements",
      "text": "This work was supported by Council for Science, Technology and Innovation (CSTI), cross-ministerial Strategic Innovation Promotion Program (SIP), \u201cInnovative AI Hospital System\u201d (Funding Agency: National Institute of Biomedical Innovation, Health and Nutrition (NIBIOHN)). This work was also supported by JSPS KAKENHI Grant Number 19K10662.\nEthical Impact Targeted research problem of XAI answers many questions in deep learning such as why and how in order to extend transparency. It can help in determining biasness of the model or data and it is highly informative and visual instead of a black box.\nA possible negative impact of this research is that the proposed classifier may introduce some bias based on the dataset (e.g., race, gender, ethnicity, etc. in human datasets) while it should be unbiased irrespective of data. However, bias can be visualized using proposed attention visualization and may be dealt with proper modification in data or model."
    }
  ],
  "title": "SCOUTER: Slot Attention-based Classifier for Explainable Image Recognition",
  "year": 2020
}
