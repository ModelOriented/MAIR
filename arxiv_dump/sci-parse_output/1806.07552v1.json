{"abstractText": "Several researchers have argued that a machine learning system\u2019s interpretability should be defined in relation to a specific agent or task: we should not ask if the system is interpretable, but to whom is it interpretable. We describe a model intended to help answer this question, by identifying different roles that agents can fulfill in relation to the machine learning system. We illustrate the use of our model in a variety of scenarios, exploring how an agent\u2019s role influences its goals, and the implications for defining interpretability. Finally, we make suggestions for how our model could be useful to interpretability researchers, system developers, and regulatory bodies auditing machine learning systems.", "authors": [{"affiliations": [], "name": "Richard Tomsett"}, {"affiliations": [], "name": "Dave Braines"}, {"affiliations": [], "name": "Dan Harborne"}, {"affiliations": [], "name": "Alun Preece"}, {"affiliations": [], "name": "Supriyo Chakraborty"}], "id": "SP:2f72e1c9ce526de4aaed2e5c10d4af99c1dccd47", "references": [{"authors": ["Chen", "Daizhuo", "Fraiberger", "Samuel P", "Moakler", "Robert", "Provost", "Foster"], "title": "Enhancing Transparency and Control when Drawing Data-Driven Inferences about Individuals", "year": 2016}, {"authors": ["Cowgill", "Bo", "Tucker", "Catherine E"], "title": "Algorithmic Bias: A Counterfactual Perspective", "venue": "Workshop on Trustworthy Algorithmic Decision-Making,", "year": 2017}, {"authors": ["Dhurandhar", "Amit", "Iyengar", "Vijay", "Luss", "Ronny", "Shanmugam", "Karthikeyan"], "title": "TIP: Typifying the Interpretability of Procedures", "year": 2017}, {"authors": ["Dhurandhar", "Amit", "Iyengar", "Vijay", "Luss", "Ronny", "Shanmugam", "Karthikeyan"], "title": "A Formal Framework to Characterize Interpretability of Procedures", "venue": "Proceedings of the ICML Workshop on Human Interpretability in Machine Learning,", "year": 2017}, {"authors": ["Doshi-Velez", "Finale", "Kim", "Been"], "title": "Towards a Rigorous Science of Interpretable", "venue": "Machine Learning", "year": 2017}, {"authors": ["Doshi-Velez", "Finale", "Kortz", "Mason", "Budish", "Ryan", "Bavitz", "Chris", "Gershman", "Sam", "O\u2019Brien", "David", "Schieber", "Stuart", "Waldo", "James", "Weinberger", "Wood", "Alexandra"], "title": "Accountability of AI Under the Law: the Role of Explanation", "year": 2017}, {"authors": ["Freitas", "Alex A"], "title": "Comprehensible Classification Models: A Position Paper", "venue": "SIGKDD Explor. Newsl.,", "year": 2014}, {"authors": ["Goodman", "Bryce", "Flaxman", "Seth"], "title": "European Union regulations on algorithmic decision-making and a \u201cright to explanation", "venue": "Proceedings of the ICML Workshop on Human Interpretability in Machine Learning,", "year": 2016}, {"authors": ["Guidotti", "Riccardo", "Monreale", "Anna", "Turini", "Franco", "Pedreschi", "Dino", "Giannotti", "Fosca"], "title": "A Survey of Methods for Explaining", "venue": "Black Box Models", "year": 2018}, {"authors": ["Harborne", "Dan", "Willis", "Chris", "Tomsett", "Richard", "Preece", "Alun"], "title": "Integrating learning and reasoning services for explainable information fusion", "venue": "In International Conference on Pattern Recognition and Artificial Intelligence,", "year": 2018}, {"authors": ["Hendricks", "Lisa Anne", "Akata", "Zeynep", "Rohrbach", "Marcus", "Donahue", "Jeff", "Schiele", "Bernt", "Darrell", "Trevor"], "title": "Generating visual explanations", "venue": "In European Conference on Computer Vision,", "year": 2016}, {"authors": ["Hirsch", "Tad", "Merced", "Kritzia", "Narayanan", "Shrikanth", "Imel", "Zac E", "Atkins", "David C"], "title": "Designing Contestability: Interaction Design, Machine Learning, and Mental Health", "venue": "In Proceedings of the 2017 Conference on Designing Interactive Systems,", "year": 2017}, {"authors": ["T. Kamishima", "S. Akaho", "J. Sakuma"], "title": "Fairness-aware Learning through Regularization Approach", "venue": "IEEE 11th International Conference on Data Mining Workshops,", "year": 2011}, {"authors": ["Kirsch", "Alexandra"], "title": "Explain to whom? Putting the User in the Center of Explainable AI", "venue": "In Proceedings of the First International Workshop on Comprehensibility and Explanation in AI and ML,", "year": 2017}, {"authors": ["Lipton", "Zachary Chase"], "title": "The Mythos of Model Interpretability", "venue": "Proceedings of the ICML Workshop on Human Interpretability in Machine Learning,", "year": 2016}, {"authors": ["Miller", "Tim"], "title": "Explanation in artificial intelligence: Insights from the social sciences", "year": 2017}, {"authors": ["Montavon", "Gr\u00e9goire", "Lapuschkin", "Sebastian", "Binder", "Alexander", "Samek", "Wojciech", "M\u00fcller", "Klaus-Robert"], "title": "Explaining nonlinear classification decisions with deep Taylor decomposition", "venue": "Pattern Recognition,", "year": 2017}, {"authors": ["Park", "Dong Huk", "Hendricks", "Lisa Anne", "Akata", "Zeynep", "Schiele", "Bernt", "Darrell", "Trevor", "Rohrbach", "Marcus"], "title": "Attentive Explanations: Justifying Decisions and Pointing to the Evidence", "year": 2016}, {"authors": ["Poursabzi-Sangdeh", "Forough", "Goldstein", "Daniel G", "Hofman", "Jake M", "Vaughan", "Jennifer Wortman", "Wallach", "Hanna"], "title": "Manipulating and Measuring Model Interpretability", "year": 2018}, {"authors": ["Ras", "Gabrielle", "van Gerven", "Marcel", "Haselager", "Pim"], "title": "Explanation Methods in Deep Learning: Users, Values", "venue": "Concerns and Challenges", "year": 2018}, {"authors": ["Ribeiro", "Marco Tulio", "Singh", "Sameer", "Guestrin", "Carlos"], "title": "Why should i trust you?: Explaining the predictions of any classifier", "venue": "In Proceedings of the 22 ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "year": 2016}, {"authors": ["Ross", "Andrew Slavin", "Hughes", "Michael C", "Doshi-Velez", "Finale"], "title": "Right for the Right Reasons: Training Differentiable Models by Constraining their Explanations", "year": 2017}, {"authors": ["Rudin", "Cynthia"], "title": "Algorithms for Interpretable Machine Learning", "venue": "In Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD", "year": 2014}, {"authors": ["Varshney", "Kush R", "Alemzadeh", "Homa"], "title": "On the Safety of Machine Learning: Cyber-Physical Systems, Decision Sciences, and Data Products", "venue": "Big Data,", "year": 2017}, {"authors": ["Weller", "Adrian"], "title": "Challenges for Transparency", "venue": "Proceedings of the ICML Workshop on Human Interpretability in Machine Learning,", "year": 2017}, {"authors": ["Zhang", "Quan-shi", "Zhu", "Song-chun"], "title": "Visual interpretability for deep learning: a survey", "venue": "Frontiers of Information Technology & Electronic Engineering,", "year": 2018}], "sections": [{"heading": "1. Introduction", "text": "\u201cInterpretability\u201d is a current hot topic in machine learning research. The increasing complexity of modern machine learning systems and the models they use, a wider adoption of machine learning in a variety of real-world systems, and new laws defining citizens\u2019 rights in relation to data processing systems (Doshi-Velez et al., 2017) all contribute to this heightened interest. Many groups have developed techniques intended to improve machine learning systems\u2019 interpretability (Zhang & Zhu, 2018; Guidotti et al., 2018; Chakraborty et al., 2017; Rudin, 2014), though the definitions of and motivations for interpretability (if specified) vary wildly between methods (Lipton, 2016).\nSeveral researchers are trying to address this problem by formalizing the study of machine learning interpretability. Freitas made an early contribution, analyzing five classification models and discussing possible measures for in-\n1IBM Research, Hursley, Hampshire, UK 2Crime and Security Research Institute, Cardiff University, Cardiff, UK 3IBM Research, Yorktown Heights, New York, USA. Correspondence to: Richard Tomsett <rtomsett@uk.ibm.com>.\n2018 ICML Workshop on Human Interpretability in Machine Learning (WHI 2018), Stockholm, Sweden. Copyright by the author(s).\nterpretability (Freitas, 2014). Lipton notes that a model requires better interpretability when its predictions, and the metrics calculated on those predictions, are insufficient for characterizing it. He provides a taxonomy for categorizing interpretability methods with different properties (Lipton, 2016). Doshi-Velez and Kim expand on this motivation: \u201cthe need for interpretability stems from an incompleteness in the problem formalization, creating a fundamental barrier to optimization and evaluation\u201d (Doshi-Velez & Kim, 2017), and provide a taxonomy for evaluating model interpretability. Miller reviews approaches to interpretability developed in philosophy and social science, discussing how artificial intelligence interpretability researchers could build on this existing literature (Miller, 2017). Poursabzi-Sangdeh et al. performed pre-registered experiments that measured the effect of different interpretability methods on user trust, ability to simulate models, and ability to detect mistakes (Poursabzi-Sangdeh et al., 2018). Dhurandhar et al. developed, to our knowledge, the first formal, quantifiable definition of interpretability (Dhurandhar et al., 2017b;a), which proposes measuring the interpretability of a procedure in relation to the performance of a target model (which could be a human or non-human agent) on a specific task. Most recently, Ras et al. analyzed what can be explained in relation to expert and lay-users of deep neural networks, providing a taxonomy for explanation methods (Ras et al., 2018).\nIn this paper, we explore to whom a machine learning system might be interpretable. While others previously identified that interpretability should be considered with reference to a specific user or user group (Kirsch, 2017), we develop this insight into a model for analyzing machine learning systems and the agents they interact with or affect. Agents have different beliefs and goals depending on their roles in relation to the machine learning system. Our model can be used to identify an agent\u2019s relationship to the system, and thus guide the analysis of what their relevant beliefs and goals might be for specifying suitable measures of interpretability for that agent."}, {"heading": "1.1. Definitions", "text": "Before outlining the model, we define how we intend to use some relevant terminology. Our model is built around a machine learning system, by which we mean a system that includes one or more machine learning models, the data used to train the model(s), any interface used to interact with the model(s), and any relevant documentation. The system (when unambiguous, we use just \u201csystem\u201d to mean \u201cmachine learning system\u201d) could be monolithic, or comprised of several different services owned by different entities, situated in different locations, and trained on data from many sources.\nThe system is situated in a machine learning ecosystem (or just \u201cecosystem\u201d), which includes the system and the agents that have interactions with, or are affected by, this system. An ecosystem always contains just one machine learning system and one or more agents (in the real-world, ecosystems will often overlap).\nWe define what we mean by an explanation and an interpretation:\n\u2022 Explanation: the information provided by a system to outline the cause and reason for a decision or output for a performed task \u2013 a \u201cpost-hoc explanation\u201d in Lipton\u2019s taxonomy (Lipton, 2016).\n\u2022 Interpretation: the understanding gained by an agent with regard to the cause for a system\u2019s decision when presented with an explanation.\nThus an agent forms an interpretation by examining one or more explanations from a system. Considering these definitions, we also define explainability and interpretability, as well as transparency as we use it in relation to interpretability:\n\u2022 Explainability: the level to which a system can provide clarification for the cause of its decisions/outputs.\n\u2022 Transparency: the level to which a system provides information about its internal workings or structure, and the data it has been trained with \u2013 this is similar to Lipton\u2019s definition of transparency (Lipton, 2016).\n\u2022 Interpretability: the level to which an agent gains, and can make use of, both the information embedded within explanations given by the system and the information provided by the system\u2019s transparency level.\nThese definitions hint that explainability, transparency and interpretability should be quantifiable. Indeed, our informal definition of interpretability is compatible with Dhurandhar et al.\u2019s quantitative -interpretability framework (Dhurandhar et al., 2017b;a) mentioned earlier.\nIn the next section we develop a conceptual model of an ecosystem and identify the different roles that agents can play within it. We then use this model to identify and classify the different agent groups in several example scenarios, and discuss how each kind of agent will have a different view of system interpretability. We also consider the constraints that need to be applied for each role, e.g., to protect sensitive data, or to preserve the privacy of the system\u2019s internal algorithms."}, {"heading": "2. Ecosystem model", "text": "We define six different roles for the agents in an ecosystem. These were identified through discussion of many different scenarios involving machine learning systems (a few of which we outline in the next section), and build on ideas from the literature (Weller, 2017; Doshi-Velez & Kim, 2017; Miller, 2017; Ras et al., 2018). The roles are not mutually exclusive: a single agent could occupy any combination of roles, though some combinations are more likely than others. Currently, we expect most of these roles to be fulfilled by humans. However, machines may increasingly occupy them in future, especially if artificial agents gain rights over their data.\n1. Creators: agents that create the machine learning system. Several teams of creators may work on different aspects of the same system e.g., architecture, design, implementation, training, documentation, deployment, and maintenance. Ecosystems always contain creators. When necessary, we further make the distinction between creator-owners and creator-implementers:\n\u2022 Owners: the agent(s) or organization(s) that own the intellectual property in the machine learning system.\n\u2022 Implementers: the agent(s) that directly implement the system, usually employees of or contractors for the owners\n2. Operators: agents that interact directly with the machine learning system. Operators provide the system with inputs, and directly receive the system\u2019s outputs. In some cases they may be able to interact directly with the system\u2019s creators. Ecosystems always contain operators.\n3. Executors: agents who make decisions that are informed by the machine learning system. Executors receive information from operators. Ecosystems always contain executors.\n4. Decision-subjects: agents who are affected by decision(s) made by the executor(s). Ecosystems usually contain decision-subjects.\n5. Data-subjects: agents whose personal data has been used to train the machine learning system. Ecosystems only contain data-subjects if the machine learning system has been trained on personal data.\n6. Examiners: agents auditing or investigating the machine learning system. Depending on the system, they may interact with one or more of the other roles and the machine learning system itself. Ecosystems only contain examiners when the system is being audited/inspected.\nWe developed this categorization to help inform our design of the requirements for interpretability in different scenarios. The role an agent fulfills will impact its goals within a particular scenario, and thus its conception of the machine learning system\u2019s interpretability. We illustrate this by outlining some example scenarios."}, {"heading": "3. Example scenarios", "text": "Scenario 1: web advertising\nMany web-sites offer paid advertising spaces that operate via auction. When a user visits such a site, advertisers bid for ad space based on their valuation of that user seeing their advertisement. They estimate this with models that take as inputs user-data sent by the host web-site. The highest bidder\u2019s advert is displayed on the web-site. The ecosystem in this scenario contains a machine learning system made up of models from many different advertisers, a host website that displays the advert depending on the bids from the system, and a user browsing the host web-site.\n\u2013 Creators: The advertising company and its employees, any third-party development companies and their employees\n\u2013 Operator: the host web-site\n\u2013 Executor: the host web-site\n\u2013 Decision-subject: the web-site user\n\u2013 Data-subjects: any internet denizen whose data has been obtained by the advertising companies\n\u2013 Examiners: relevant advertising standards body staff, \u201cdata commissioner\u201d style authority staff (e.g., the UK\u2019s Information Commissioner\u2019s Office); usually, such authorities will only become examiners if a complaint or information request is made\nScenario 2: route planning on a smartphone\nMost smartphones provide apps (often machine learning systems) for planning driving routes. The user enters a desired start and end location, and the app provides one or more possible routes for them to take. This ecosystem contains a user planning a route, and an app that generates possible routes.\n\u2013 Creators: the navigation app company and its employees\n\u2013 Operator: the app user\n\u2013 Executor: the app user\n\u2013 Decision-subject: the app user\n\u2013 Data-subjects: any road users whose location data has been obtained and used by the navigation app company\n\u2013 Examiners: \u201cdata commissioner\u201d style authority staff; usually, such authorities will only become examiners if a complaint or information request is made\nScenario 3: loan application\nWhen someone applies for a loan, the lender may use a machine learning system to determine the applicant\u2019s chance of defaulting, and adjust the the amount offered, rate of interest, and other terms accordingly (or simply refuse to lend). This ecosystem contains a lender, an applicant seeking a loan, and a machine learning system for assessing applicant risk.\n\u2013 Creators: The lender and its employees if they also developed the system, or any third-party development companies and their employees\n\u2013 Operators: the lender\u2019s (customer-facing) employees*\n\u2013 Executor: the lender\u2019s (higher-ranking) employees*\n\u2013 Decision-subject: the loan applicant\n\u2013 Data-subjects: prior loan applicants, any agent whose data has been obtained by the lender (likely most financial service\nusers)\n\u2013 Executors: financial regulation authority staff, financial ombudsman\n*The operators will likely be customer-facing agents who interact directly with applicants. They will make a decision based on the machine learning system\u2019s output, but the business logic for this decision will have been decided on by more senior employees at the lender. The senior employees would be seen as the executors in this case, as the customer-facing agent simply communicates the decision to the applicant.\nScenario 4: medical advice for clinicians\nSeveral machine learning systems have been developed to assist doctors with diagnosis and treatment planning. The doctor provides the system with a patient\u2019s data, and the system judges the most likely diagnosis, or recommends possible treatment options that the doctor can then discuss with the patient. This ecosystem contains a machine learning system for diagnosis and/or treatment recommendation, a doctor or team of medical professionals who operate the system, and a patient.\n\u2013 Creators: the medical software company and its employees, any collaborating medical professionals and researchers\n\u2013 Operators: medical professionals\n\u2013 Executors: the patient, medical professionals\u2020\n\u2013 Decision-subject: the patient\n\u2013 Data-subjects: other patients, researchers and study subjects (e.g., data loaded from publications)\n\u2013 Examiners: professional medical authority staff e.g., the UK\u2019s General Medical Council \u2020The role of executor in this scenario is debatable, and produced some discussion among the authors. The doctor makes decisions on treatment options based on the system\u2019s advice, so can be considered an executor. They would also be held responsible for treatment decisions from the point of view of an Examiner. However, the patient ultimately decides on their treatment, so could also be considered the executor. Additionally, in some cases a patient may be unable to make such decisions \u2014 they may be minors, or adults not in a sound frame of mind. In these cases, the patient\u2019s representative(s) (e.g., parent/guardian, attorneyin-fact) may be the executor(s).\nScenario 5: releasing defendants on bail\nMachine learning systems are used in some countries to predict the likelihood a defendant will be dangerous if released on bail. These predictions are used by the judge about whether to grant bail, and at what price. This ecosys-\ntem contains a judge, a defendant, and a machine learning system consulted by the judge.\n\u2013 Creators: the legal software company and its employees\n\u2013 Operators: the judge (or other court staff)\n\u2013 Executor: the judge\n\u2013 Decision-subject: the defendant\n\u2013 Data-subjects: previous defendants\n\u2013 Examiners: In the case of an appeal, the original decision may be scrutinized by, for example, the defendant\u2019s lawyers. In this scenario, these lawyers would become examiners.\nScenario 6: go no-go order in a military operation\nConsider a scenario involving a military operation to kill or capture a target. A machine learning system may be employed to find and help recognize this target, and the order to engage will be informed based on the system\u2019s recognition. After the event, this order may be scrutinized at a tribunal. This scenario includes the target, front-line personnel, analysts, a mission commander, tribunal jurors, and a machine learning system that may be distributed across several coalition partners.\n\u2013 Creators: employees of the various coalition partners\u2019 militaries, employees of any military contractors involved\n\u2013 Operators: military analysts\n\u2013 Executor: the mission commander\n\u2013 Decision-subject: the target, or agent identified as the target\n\u2013 Data-subjects: other individuals of interest, their known associates\n\u2013 Examiners: tribunal jurors"}, {"heading": "4. Role-based interpretability", "text": "Having defined the roles and provided example scenarios, we consider what interpretability means for agents fulfilling each role by considering their goals, noting that these goals are not always aligned (Weller, 2017)."}, {"heading": "4.1. Creators", "text": "In our above scenarios, the creator-implementers are the architects, designers, engineers, and technical writers responsible for constructing the system, and the creator-owner is the organization that owns the intellectual property in the system. Creator-implementers generally work for a single owner, but may create systems as collaborative efforts between several owners. Creator-implementers may also include subject matter experts from their own or other organizations, who provide knowledge to help train the system\n(e.g., medical professionals and researchers in scenario 4).\nCreators will want to improve system performance, where performance is a handily vague catch-all term for a variety of metrics to optimize for. These metrics depend on the particular scenario, and might include predictive accuracy, computational or data efficiency, bias minimization (Cowgill & Tucker, 2017), and/or safety (Varshney & Alemzadeh, 2017; Amodei et al., 2016). Their interpretability goal will therefore be to improve their understanding of the system such that they can better optimize it for their preferred metrics. A good example of creators engaging in such research in the domain of autonomous vehicles is given in (Bojarski et al., 2017).\nExplainability and transparency are both important for improving creator-interpretability."}, {"heading": "4.2. Operators", "text": "If the operator is not also the executor, then the operator must pass on information to the executor to inform their decision. They want to make sure the data they input to, or question they ask of the system is the right one for them to provide useful information to the executor. They may present all the available information to the executor, or a summary of it, depending on the scenario and executor\u2019s characteristics, and may need to obtain further explanations from the system in response to queries from the executor. Existing explanation methods that might improve operatorinterpretability include techniques to highlight relevant input features \u2014 e.g., LIME (Ribeiro et al., 2016), layer-wise relevance propagation (Montavon et al., 2017) \u2014 or those that generate text explanations for outputs (Hendricks et al., 2016; Park et al., 2016).\nExplainability is important for operator-interpretability. Transparency will be important if the operator requires some understanding of the system\u2019s internals to make good queries."}, {"heading": "4.3. Executors", "text": "Executors are responsible for decision making, so want to be sure that they make good decisions. What constitutes a \u201cgood\u201d decision varies depending on the executor\u2019s desires and goals. In scenario 2, a good decision might be to follow the shortest recommended route, or it might be to follow a longer route if that route is more scenic and if the executor values this feature for the journey in question. In scenario 4, the best treatment option for a terminal patient will depend on the patient\u2019s preferences around life-extension and quality-of-life. In scenario 6, the order to engage will be assessed based on a range of factors including mission success, casualties, and any collateral damage. In each case, suitable explanations for the system\u2019s outputs could affect decision\nmaking. We have previously developed an example system for providing some of the different levels of explanation that might be required by an executor (Harborne et al., 2018).\nExplainability is important for executor-interpretability. If the executor is not also the operator, it is unlikely that transparency will affect executor-interpretability significantly."}, {"heading": "4.4. Decision-subjects", "text": "If the decision-subject is not also the executor, then they will want to know why an executor made a particular decision, either out of plain curiosity, or to be able to challenge or change that decision \u2014 see (Hirsch et al., 2017) for an exploration of this idea of contestability. In scenario 1, the web-site user may want to know why they were shown a particular advert for interest, or so that they can remove or hide their personal data to prevent targeted advertising (or even provide more data to receive better targeted advertising). In scenario 3, the loan applicant may want to know how change their behaviour to make the system give them a lower risk score. In scenario 5, the defendant may want to challenge a decision, e.g., on grounds of discrimination. In scenario 6, the enemy may want to know how to avoid detection by the system to prevent being shot.\nThe goals of decision-subjects can clearly clash with those of executors and creators; this goal mismatch was previously noted in (Weller, 2017). In scenario 1, the web-site owner wants to maximize advertising revenue, but users who hide their data are less valuable than users with a data-rich profile. In scenario 3, the lender may be concerned that an applicant could game the system if it is highly interpretable to them \u2014 Akyol et al. formally analyzed this kind of strategic behaviour (Akyol et al., 2016). We leave identifying the goal mismatch in scenario 6 as an exercise for the reader.\nExplainability is important for executor-interpretability, while transparency may be important in some scenarios."}, {"heading": "4.5. Data-subjects", "text": "Data-subjects may not even be aware that a system has been trained on their data. However, in many jurisdictions, data subjects will have certain rights over their personal data. Given the growing awareness of data collection and sharing between companies, data-subjects may become more likely to exercise these rights and ask organizations to delete some or all of their data, with the goal of increasing their privacy \u2014 see, e.g., (Chen et al., 2016) for a study analyzing Facebook\u2019s predictions about users before and after data deletion.\nData-subjects may have moral concerns about how their data is being used to make decisions about other people, so may want to understand how their data affects a machine learning system\u2019s outputs. If they request that their data be deleted,\nthe system\u2019s behaviour may not change at all: for example, a neural network\u2019s weights will only update on re-training, but a k-nearest neighbour model uses the data directly so will be immediately affected. Data-subjects may want to know about these consequences before requesting data deletion, and indeed, may be given the right to request system retraining in the case that deletion does not immediately affect the system\u2019s behaviour.\nAs data-subjects do not directly see a system\u2019s outputs unless they are also an operator, only transparency affects data-subject-interpretability."}, {"heading": "4.6. Examiners", "text": "We added the examiner role to model agents tasked with compliance/safety-testing, auditing, or forensically investigating a system. Safety-testing hopefully occurs before deployment, so testers examine possible future outputs, while auditors and forensic investigators examine past system outputs. The latter requires an interpretable system to store both its decisions and their explanations, or to be able to generate explanations for stored past decisions. If explanations are generated, these should be identical to the explanation given at the original decision time. The examiner may want to interact with the system using new data to explore its outputs and their explanations (e.g., creating repeated \u201cwhat-if\u201d scenarios to establish decision sensitivity), which, if done forensically, also requires the system to respond as it would have at the time of the original outputs. This constraint might be very costly in, e.g., reinforcement learning, where models are continuously updated in response to environmental feedback.\nSome types of examiners may provide feedback to creators on how to improve the system. For example, they could suggest ways to retrain models using approaches designed to improve feature relevance (Ross et al., 2017), or to improve model fairness by regularization to ensure that certain sensitive attributes of the data-subjects are not used in model building, even if they are good discriminators (Kamishima et al., 2011).\nBoth explainability and transparency are important for examiner-interpretability."}, {"heading": "5. Discussion", "text": "Our model is a first draft intended to stimulate discussion and suggestions for improvements, but we anticipate that it could be useful to interpretability researchers, system developers (creator-owners), and regulatory bodies. As noted previously, interpretability is a woolly concept with inconsistently applied terminology. We hope to contribute to the formalization of some of these terms by building on existing definitions, using them consistently, and describing how\nthey relate to the roles in our ecosystem model. Such a formalization is crucial for progressing machine learning interpretability research as a rigorous science (Doshi-Velez & Kim, 2017), allowing appropriate, quantitative comparisons to be made between similar methods under well-defined circumstances.\nSystem creator-owners will ultimately decide on the extent to which they make their systems explainable and transparent to different roles. Our model could help creator-owners identify the interpretability needs of different agents in the ecosystem, allowing for a more systematic analysis and indicating where to focus their research and development efforts depending on their own goals for the ecosystem. The division of roles also helps to identify agents with conflicting goals, so that the owners can plan how best to manage this. They may develop a privacy model for the system around our ecosystem model, specifying different levels of access to various kinds of explanation depending on an agent\u2019s role (e.g., not allowing \u201cexplanations by analogy\u201d (Lipton, 2016) that would reveal personal data to particular roles, or limiting transparency to non-creators to protect intellectual property).\nFinally, regulatory bodies, filling the role of examiners, may use our model to aid in their auditing or forensic investigations of ecosystems. They may focus on decision-subjects and data-subjects, ensuring that the system is compliant with their rights \u2014 such as personal data privacy or the \u201cright to an explanation\u201d under GDPR (Goodman & Flaxman, 2016) \u2014 or examine why executors made specific decisions and the role of the machine learning system in influencing those decisions (e.g., identifying bias)."}, {"heading": "Acknowledgment", "text": "This research was sponsored by the U.S. Army Research Laboratory and the UK Ministry of Defence under Agreement Number W911NF\u201316\u20133\u20130001. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. Army Research Laboratory, the U.S. Government, the UK Ministry of Defence or the UK Government. The U.S. and UK Governments are authorized to reproduce and distribute reprints for Government purposes notwithstanding any copy-right notation hereon."}], "title": "Interpretable to Whom? A Role-based Model for Analyzing Interpretable Machine Learning Systems", "year": 2018}