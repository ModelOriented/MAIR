{"abstractText": "Understanding predictions made by Machine Learning models is critical in many applications. In this work, we investigate the performance of two methods for explaining tree-based models: \u2018Tree Interpreter (TI)\u2019 and \u2018SHapley Additive exPlanations TreeExplainer (SHAP-TE)\u2019. Using a case study on detecting anomalies in job runtimes of applications that utilize cloud-computing platforms, we compare these approaches using a variety of metrics, including computation time, significance of attribution value, and explanation accuracy. We find that, although the SHAP-TE offers consistency guarantees over TI, at the cost of increased computation, consistency does not necessarily improve the explanation performance in our case study.", "authors": [{"affiliations": [], "name": "Pulkit Sharma"}, {"affiliations": [], "name": "Shezan Rohinton Mirzan"}, {"affiliations": [], "name": "Apurva Bhandari"}, {"affiliations": [], "name": "Anish Pimpley"}, {"affiliations": [], "name": "Abhiram Eswaran"}, {"affiliations": [], "name": "Soundar Srinivasan"}, {"affiliations": [], "name": "Liqun Shao"}], "id": "SP:5d2e9db8caf546ce0232dba4a4759c062b6cbeec", "references": [{"authors": ["R. Caruana", "N. Karampatziakis", "A. Yessenalina"], "title": "An empirical evaluation of supervised learning in high dimensions", "venue": "In Proceedings of the 25th International Conference on Machine Learning", "year": 2008}, {"authors": ["V. Chandola", "A. Banerjee", "V. Kumar"], "title": "Anomaly detection: A survey", "venue": "ACM Comput. Surv", "year": 2009}, {"authors": ["A. Cuzzocrea", "E. Mumolo", "R. Cecolin"], "title": "Runtime anomaly detection in embedded systems by binary tracing and hidden markov models", "venue": "In 2015 IEEE 39th Annual Computer Software and Applications Conference (2015),", "year": 2015}, {"authors": ["A. Gentzel", "D. Garant", "D. Jensen"], "title": "The case for evaluating causal models using interventional measures and empirical data", "venue": "Advances in Neural Information Processing Systems 32,", "year": 2019}, {"authors": ["S. Lipovetsky", "M. Conklin"], "title": "Analysis of regression in game theory approach", "venue": "Applied Stochastic Models in Business and Industry", "year": 2001}, {"authors": ["S.M. Lundberg", "G. Erion", "H. Chen", "A. DeGrave", "J.M. Prutkin", "B. Nair", "R. Katz", "J. Himmelfarb", "N. Bansal", "Lee", "S.-I"], "title": "From local explanations to global understanding with explainable ai for trees", "venue": "Nature Machine Intelligence", "year": 2020}, {"authors": ["S.M. Lundberg", "Lee", "S.-I"], "title": "A unified approach to interpreting model predictions", "venue": "In Advances in Neural Information Processing Systems", "year": 2017}, {"authors": ["M. Peiris", "J.H. Hill", "J. Thelin", "S. Bykov", "G. Kliot", "C. Konig"], "title": "Pad: Performance anomaly detection in multi-server distributed systems", "venue": "IEEE 7th International Conference on Cloud Computing", "year": 2014}, {"authors": ["R. Primartha", "B.A. Tama"], "title": "Anomaly detection using random forest: A performance revisited", "venue": "In 2017 International Conference on Data and Software Engineering (ICoDSE)", "year": 2017}, {"authors": ["M.T. Ribeiro", "S. Singh", "C. Guestrin"], "title": "why should I trust you?\u201d: Explaining the predictions of any classifier", "venue": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San Francisco, CA,", "year": 2016}, {"authors": ["A. Shrikumar", "P. Greenside", "A. Kundaje"], "title": "Learning important features through propagating activation differences", "venue": "CoRR abs/1704.02685", "year": 2017}, {"authors": ["A. Shrikumar", "P. Greenside", "A. Shcherbina", "A. Kundaje"], "title": "Not just a black box: Learning important features through propagating activation differences", "year": 2016}, {"authors": ["E. \u0160trumbelj", "I. Kononenko"], "title": "Explaining prediction models and individual predictions with feature contributions", "venue": "Knowledge and Information Systems", "year": 2013}, {"authors": ["W. Webber", "A. Moffat", "J. Zobel"], "title": "A similarity measure for indefinite rankings", "venue": "ACM Trans. Inf. Syst. 28,", "year": 2010}, {"authors": ["D. Wulsin", "J. Blanco", "R. Mani", "B. Litt"], "title": "Semi-supervised anomaly detection for eeg waveforms using deep belief nets", "venue": "In 2010 Ninth International Conference on Machine Learning and Applications", "year": 2010}], "sections": [{"text": "Keywords: Explanation \u00b7 Feature Attribution \u00b7 Interventional Evaluation \u00b7 Tree Interpreter \u00b7 SHAP TreeExplainer"}, {"heading": "1 Introduction", "text": "Machine learning-based approaches have become popular in automatically detecting and predicting anomalies in a variety of applications. Anomaly Detection (AD) [2] has been applied to various domains, such as tracking anomalous events in traffic surveillance videos [18] and tracking irregular patterns in electrocardiographs of a patient [20]. AD has also become popular in the computing industry to detect system failures both in multi-server distributed systems [10] and in embedded systems[3]. A sister domain closely related to AD is Anomaly Reasoning (AR) that comprises delineating the causal factors associated with an anomaly. A robust system would not just predict the anomalous events, but also identify the root causes of a failure or an anomaly. In these applications, AR is crucial for an efficient analysis of faults, thereby reducing significantly, the time needed for manual investigation and the required computing resources.\nInterpreting machine learning models correctly has been a challenging task. Although linear models are easy to interpret, they fail to generalize in real-world\nar X\niv :2\n01 0.\n06 73\n4v 1\n[ cs\n.A I]\n1 3\nscenarios with predominantly non-linear behavior. This leads to the adoption of more accurate models at the cost of them being less interpretable. Hence, there has been a significant recent research emphasis on developing techniques that could add interpretability to these complex \u2018black-box models\u2019.\nThe LIME [12] algorithm interprets the predictions of any given model by utilizing explainable analogues that are valid in \u201clocal\u201d regions. Another popular apporach, DeepLIFT [15,16], is used for interpreting Neural Network-based models by assigning contribution scores to each neuron based on the difference in its activation to its \u201dreference\u201d activation. Other techniques [9,17,7] take a game-theoretic approach towards computing feature contributions to model explanations. Although, all three are primarily based on Shapley values [6], they calculate and further approximate the values differently to derive feature contributions.\nIn this paper, we compare the interpretation performance of two popular treeexplanation methods: the SHapley Additive exPlanation TreeExplainer (SHAPTE) [8] for model-agnostic interpretations and the TreeInterpreter (TI) [13]. Specifically, we conduct a case study on the task of reasoning about anomalies in computing jobs that run in cloud platforms. An example of a recent effort in this domain is Griffon [14] - Microsoft\u2019s Reasoning infrastructure deployed on Azure clusters. SHAP-TE averages out contributions of each possible feature set to obtain the final feature attribution values. This helps in reducing the bias added to the computation of feature attribution values when only a specific ordering is considered. However, TI only considers one ordering of features depending on how the tree was formed. This makes TreeInterpreter as essentially an approximation of SHAP-TE. Hence, we examine the performance of SHAP-TE using Griffon\u2019s AD algorithm and investigate if the TreeInterpreter\u2019s approximation-based approach used in Griffon could be generalized to other datasets or scenarios.\nWe conduct experiments under a variety of conditions on the recently introduced PostgreSQL dataset [5] and compare the above methods across a variety of metrics. Our major contributions are summarized as follows:\n\u2013 Scale Comparison. We compare, empirically, the scaling property of both methods, in terms of time-complexity, with respect to increasing data size and depth of trees. \u2013 Performance Comparison. We analyze their trade-offs and provide a novel analysis of the two methods in ranking features according to their contributions and attribution accuracy, and also experimentally evaluate the variance among the contribution values generated by the two methods to measure the significance of produced ordering of FAs3. \u2013 Critique of TI and SHAP-TE. We investigate whether the consistency 4\nproperty of SHAP-TE is crucial to making it a preferred method over TI in this domain on publicly available data, which would facilitate replication by the research community.\n3 Feature Attribution (FA) is defined as the contribution each independent variable or a \u201cfeature\u201d made to the final prediction of a model. 4 See Section 2 for the definition of consistency."}, {"heading": "2 Background Work", "text": "Unlike linear models, Decision Tree models cannot be represented as sum of linear contributions of each features for the whole model. Hence, they are difficult to interpret on the model level. However, individual predictions of a decision tree can be explained by decomposing the decision path into one component per feature. One can track a decision by traversing the tree and explain a prediction (y) by the additive contributions at each decision node as,\ny = bias + M\u2211 m=1 feature contribution{m,x} (1)\nwhere bias is the contribution of root node and feature contribution{m,x} is the contribution of feature m in predicting the outcome corresponding to an input x. Equation (1) forms the basis of TI Package [13], which is available for interpreting scikit-learn\u2019s decision tree and random forest predictions.\nSHAP-TE [8] is another tree-based FAM5 that uses Shapley values from game theory to make tree-based models interpretable. The feature values of an input data instance act as players in a coalition. These values essentially distribute the prediction result among different features. SHapley Additive exPlanation (SHAP) [9] value for a particular feature is the weighted average of all the marginal contributions to the prediction over all possible combinations of features. Using SHAP is inherently beneficial in terms of consistency. From [9], a FAM is consistent if a feature\u2019s attribution value does not decrease on increasing the true impact of that feature in the model.\nGriffon[14], introduced in Section 1, uses the notion of delta feature contribution between two different instances of a job type to predict features that impacted the most to the difference in their runtimes. FA values for both the jobs are simply subtracted to compute how much each feature contributes to the deviation in the predictions for both jobs.\nWork on the evaluation of Causal Models in [5] discusses the design of interventional and observational data to analyze the performance of model explanation paradigms. Motivated by their approach, we also adopt the postgreSQL data presented in their work in the experiments presented in this paper. This dataset is a collection of SQL queries submitted to stackoverflow server\u2019s database (largescale software system) that enables the experimenter to run the same experiment multiple times under different, controlled conditions. The conditions are determined by setting few key configuration parameters, called treatment variables. The effect of change in one configuration is observed in the output variables such as Runtime. Using background, domain knowledge, the dependence between the runtime and the \u201dtreatment\u201d variables is established. The features that describe a job are termed as covariates. Inspired by this work, we adopt the interventional data setting within the scope of Griffon; design of these experiments is discussed in Section 4. 5 Feature Attribution Method (FAM), referred to as the explanation method that\ncalculates FAs to interpret each prediction generated by a model."}, {"heading": "3 Evaluation Approach", "text": "In order to group similar data points together, we split the entire data set into smaller subgroups termed as templates. A template is a group of data points in which the covariate input features are kept nearly constant6 while the treatment feature variables are allowed to change for each data point. This approach allows us to easily establish a relationship between the prediction and the treatment feature variable for any two datapoints belonging to the same template. Hence, if we choose a pair of datapoints from a template such that both differ with respect to some m treatment feature variables, then the deviation in their predictions can be easily attributed to these m features.\nFor the case of anomaly detection, we consider pairs of data points from a particular template. FAM will calculate FA values for both these data points. These obtained FA values can then be used to reason about the output. In order to explain the difference in their predictions, we compute the difference in the computed FA values (See Figure 1). This is motivated from the delta feature contribution technique used in Griffon. Using the obtained delta feature contributions, we create an ordered list that ranks all the features from the most important to the least important feature. From this list of ranked features, we compute a Rank Biased overlap (RBO) metric [19] 7 to measure the similarity of ranked results produced by both the attribution methods. Motivated by the differences in RBO values observed 1, we further investigate the differences brought out by lower RBO values in terms of attribution accuracy. We define attribution accuracy as the fraction of times a FAM is able to correctly identify the cause of an anomaly. To assess the trade-off between using SHAP-TE and TI, we measure the attribution accuracy on interventional PostgreSQL data set using proposed Implicit and Explicit Interventional Measures."}, {"heading": "3.1 Implicit Interventional Measure", "text": "We use the \u201cInterventional Data\u201d setup proposed by [5] to establish the attribution accuracy of a particular method of explanation. We refer to this approach as an \u201cImplicit Interventional approach\u201d due to the presence of pre-existing treatment variables in our evaluation data and contrast it with a manually manipulated \u201cExplicit Interventional approach\u201d that we will introduce in Section 3.2.\nWe first divide the data set into a set of templates and then choose a pair of data points from within each template. In this pair, one point acts as an anomaly, while the other as the base line. We obtain a ranked feature list after processing this pair of points through an explanation method. The attribution is deemed to be correct, if the top attributed feature matches the treatment variable (see Section 4). This gives us the Top-1 measure of attribution accuracy. Similarly by considering the first k elements of the ranked list, we obtain the Top-k attribution accuracy.\n6 Some of the covariate variables in postgreSQL dataset are continuous, which when grouped reduces the number of data points per cluster. 7 RBO implementation : https://github.com/changyaochen/rbo"}, {"heading": "3.2 Explicit Interventional Measure", "text": "This measure helps us in evaluating correctness of each explanation method irrespective of deployment. We design this experiment by manually intervening in the data set to change the value of a treatment variable. Let d be the dimension of feature space in data set and i represent the index of treatment variable that was intervened. When we change the ith index for a data point x, we obtain a new intervened point, x\u2032 (Shown in Figure 1). Let e(\u00b7, F ) represent the operator to produce FA values for a data point under a trained Random Forest model F . The most important feature is then considered to be the one with highest difference in attribution value between the original and manually-intervened data point. Here, our assumption is that this feature should be the same as the manually- or explicitly-intervened feature i.\ni = argmax |e(x\u2032, F )\u2212 e(x, F )| (2)\nEvery data point that satisfies the above relation for a particular explanation method, contributes as a positive sample for that explanation method. For PostgreSQL data set used in our experiments described in Section 4, we apply the following two interventions on all the data points for all three treatment variables,\nxi \u2190 (xi \u00b1 1) mod 3 (3)\nSince the range of each treatment variable is 0-2, the above operation covers the domain of each variable. As discussed earlier, change in the values of treatment variables would affect the run-time of the job as these variables correspond to system settings while running a query. Results for this experiment are discussed in Section 4.5."}, {"heading": "4 Experiments and Results", "text": ""}, {"heading": "4.1 Experiment and Data Settings", "text": "We perform our experiments on the PostgreSQL8 data set that is a sample of the data from user generated queries on Stack Overflow9. It contains the execution information for 11, 252 queries run corresponding to 90, 016 different covariatetreatment combinations on Postgres. Treatment variables for this data set are three system parameters, namely (i) MemoryLevel: the amount of system memory available, (ii) IndexLevel: the type of indexing used for accessing a query in database and (iii) PageCost: the type of disk page access. Various outputs of each query are recorded. We choose query Runtime output to analyse our results.\nThe PostgreSQL dataset is a large dataset with nearly 1.5 million data points. Training a Random Forest model on the complete data generates a large model and this in turn increases the time for computing feature attributions, as explained in Section 4.2. To keep computation times tractable, we sample a subset from the entire data set ensuring that these samples preserve most of the unique samples in the original data set. After sampling, we have about 70, 000 data points, which are further sub-sampled into 60%\u201320%\u201320% train-val-test splits. We run our experiments on a cluster system with Xeon E5-2680 v4 @ 2.40GHz processor and 128GB of RAM.\nSHAP-TE and TI can be compared using any tree-based models including Decision Trees, GBMs, and Random Forest. However, among these options, Random Forest models have empirically been shown to have higher accuracy, especially for high-dimensional data in most real world scenarios [1]. Apart from Griffon, other anomaly detection research [11] ,[4] also use Random Forests as their base model. Hence, we choose to use Random Forest for comparing results."}, {"heading": "4.2 Runtime Comparison", "text": "We train a Random Forest Regression model to predict job runtime in milliseconds with a hyper-parameter setting of 200 estimators and a maximum depth 20. As running times are generally log-normally distributed, we use logarithm of Runtime as the output target.\nWith large number of jobs in a given computing cluster, the ability to detect and interpret anomalies in real time becomes crucial. SHAP-TE and TI both have an amortized computation-time complexity of O(TLD2) where T is the number of individual estimators, L is the maximum number of leaves in any tree and D is the maximum depth of any tree. Although the amortized time complexity is the same for both, their practical running times differ significantly.\n8 Dataset can be found at https://groups.cs.umass.edu/kdl/causal-eval-data 9 This data is collected in the work by [5].\nFirst, we compare the scaling of FAM computation time per data instance. We observe that TI scales at 0.058s per data instance while SHAPTE takes 3.44s per data instance which is significantly higher. Second, red lines in Figure 4.2 show the scaling of both methods with the depth of tree parameter of the Random Forest. We observe that both methods follow a square dependence, but SHAP-TE again scales worse that TI. The reason is that SHAP-TE averages out the contributions for each decision path while TI just considers the one deci-\nsion path resulting in shorter computation time. This indicates that in a practical deployment scenario with large number of data cases and larger tree depth, TI can be more efficient compared to the SHAP-TE, in terms of computation times."}, {"heading": "4.3 Rank List Similarity", "text": "We now report results using the rank biased overlap (RBO) measure to bring out the difference in the feature rankings between the two methods. RBO measures the set overlap of top selected elements in two lists. We run SHAP-TE and TI on the entire test set and obtain a list of FAs corresponding to each test point. We then compute the RBO corresponding to the ranked feature list of each test data point. Since the top ranked features are more relevant in explaining the output, we report RBO by considering different number of top elements from each ranked list. Table 1 shows the median RBO values for different values of k. As observed, SHAP-TE and TI begin to differ significantly in rankings if we consider Top-3 and Top-5 ranked features. A median value of 0.61 implies that for more than 50% of the data points, only 1 feature out of top 3 is the same for these methods. This motivates us to investigate the differences in their behaviour in more detail, as described below."}, {"heading": "4.4 Significance of Attribution Ranking", "text": "Since we are using the FA values to generate an ordered ranking of features, it is necessary to note the the amount of variation in attribution values to interpret the significance of obtained orderings. Higher variance in attribution values would imply a more significant ordering of features.10 For each data point, we measure the variance in the magnitude of Top k FA values and report the median for complete data set. From Table 2, we observe that TI captures nearly 50% more variance in the attribution values than SHAP-TE. This implies that rankings obtained from TI are more significant than from SHAP-TE."}, {"heading": "4.5 Attribution Accuracy: How correctly are the right features attributed", "text": "Figure 1 describes the outline of experiment to evaluate the correctness of feature attribution. To establish feature association, we apply SHAP-TE and TI to the trained Random Forest model to produce FA values of each feature for every test data point. These values represent the contribution of each feature in producing an output. If we have two data points which are almost same, except for changes across a few feature values, we expect to see change in the output to be attributed to these differing features. As proposed by Griffon [14], contribution of features for deviation in outputs can be computed by taking a difference of FA values.\nImplicit Interventional Measure In this scenario, we consider all the data points where only one of the treatment variable differs and all other variables including covariates remain the same. We believe this subset of data essentially emulates the real world scenario where one system parameter is controlled and remaining query variables are the same.\nFrom the results in Table 3, we see that TI outperforms SHAP-TE for 2 out of 3 Treatment variables for both Top-1 and Top-3 measures. It is worth noting that the performance of SHAP-TE improves significantly from Top-1 to Top-3. This is because SHAP-TE always prefers IndexLevel over the other two treatment variables However, PageCost or MemoryLevel do appear in the attributions at Rank 2 or 3.\nExplicit Interventional Measure This measure enables us to use the whole test set for evaluating performance for each treatment variable. We compute the explicit interventional attribution accuracy of both FAMs on the whole test set. Table 3 shows attribution accuracy measure when the true feature was among the top-1 or top-3 attributed features. We observe that average attribution accuracy of TI is better than that of SHAP-TE in both cases. Even for individual treatment variables, TI is significantly better in all cases except for Top-1 in IndexLevel.\n10 For eg, consider 2 lists of attribution values S1 = [1, 1.1, 1.3] and S2 = [1, 3, 5]. The ranking obtained from values in S2 is more reliable than S1."}, {"heading": "5 Conclusion", "text": "We evaluated two prominent feature attribution methods for explaining treebased models. The results show that the two methods differ in various aspects of efficacy and efficiency. In our case study, we observe that the amount of time that SHAP-TE takes to compute attribution values is nearly 60\u00d7 higher than that of TI. This could be a potential constraint in certain large-scale computing applications.\nWe also compared the performance accuracy of these methods using two different interventional approaches and observe that, on average, TI outperforms SHAP-TE. Based on these results, we conclude that despite the consistency guarantees, SHAP-TE does not provide benefits, in terms of attribution accuracy, in our case study of explaining job anomalies in cloud-computing applications. In addition, we have found that using TI provides high quality results at a lower computational footprint.\nWe invite the research community to build on our findings. The code used for obtaining these results is available publicly11."}, {"heading": "Acknowledgements", "text": "We thank our mentors, Javier Burroni and Prof. Andrew McCallum, for their guidance. We also thank Minsoo Thigpen for organizational support, as well as Scott Lundberg for providing insightful suggestions on a earlier draft. Finally, we thank anonymous reviewers for their feedback."}], "title": "Evaluating Tree Explanation Methods for Anomaly Reasoning: A Case Study of SHAP TreeExplainer and TreeInterpreter", "year": 2020}