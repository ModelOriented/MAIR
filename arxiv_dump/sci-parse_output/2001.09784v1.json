{"abstractText": "An increasing number of decisions regarding the daily lives of human beings are being controlled by artificial intelligence (AI) algorithms in spheres ranging from healthcare, transportation, and education to college admissions, recruitment, provision of loans and many more realms. Since they now touch on many aspects of our lives, it is crucial to develop AI algorithms that are not only accurate but also objective and fair. Recent studies have shown that algorithmic decision-making may be inherently prone to unfairness, even when there is no intention for it. This paper presents an overview of the main concepts of identifying, measuring and improving algorithmic fairness when using AI algorithms. The paper begins by discussing the causes of algorithmic bias and unfairness and the common definitions and measures for fairness. Fairness-enhancing mechanisms are then reviewed and divided into pre-process, in-process and post-process mechanisms. A comprehensive comparison of the mechanisms is then conducted, towards a better understanding of which mechanisms should be used in different scenarios. The paper then describes the most commonly used fairnessrelated datasets in this field. Finally, the paper ends by reviewing several emerging research sub-fields of algorithmic fairness.", "authors": [{"affiliations": [], "name": "DANA PESSACH"}], "id": "SP:87a41c0449c2453e6414a987fb7b8f016bac527e", "references": [{"authors": ["Himan Abdollahpouri", "Gediminas Adomavicius", "Robin Burke", "Ido Guy", "Dietmar Jannach", "Toshihiro Kamishima", "Jan Krasnodebski", "Luiz Pizzato"], "title": "Beyond Personalization: Research Directions in Multistakeholder Recommendation", "year": 2019}, {"authors": ["Adel Abusitta", "Esma A\u00efmeur", "Omar Abdel Wahab"], "title": "Generative Adversarial Networks for Mitigating Biases in Machine Learning Systems", "year": 2019}, {"authors": ["Alekh Agarwal", "Alina Beygelzimer", "Miroslav Dudik", "John Langford", "HannaWallach"], "title": "A Reductions Approach to Fair Classification", "venue": "In International Conference on Machine Learning", "year": 2018}, {"authors": ["Alekh Agarwal", "Miroslav Dudik", "Zhiwei StevenWu"], "title": "Fair Regression: Quantitative Definitions and Reduction- Based Algorithms", "venue": "In International Conference on Machine Learning", "year": 2019}, {"authors": ["Julia Angwin"], "title": "Machine Bias \u00e2\u0102\u0164 ProPublica", "venue": "https://www.propublica.org/article/machine-bias-riskassessments-in-criminal-sentencing", "year": 2016}, {"authors": ["Grigory Antipov", "Moez Baccouche", "Jean-Luc Dugelay"], "title": "Face aging with conditional generative adversarial networks", "venue": "IEEE International Conference on Image Processing (ICIP)", "year": 2017}, {"authors": ["Arturs Backurs", "Piotr Indyk", "Krzysztof Onak", "Baruch Schieber", "Ali Vakilian", "Tal Wagner"], "title": "Scalable Fair Clustering", "venue": "In International Conference on Machine Learning", "year": 2019}, {"authors": ["Jianmin Bao", "Dong Chen", "Fang Wen", "Houqiang Li", "Gang Hua"], "title": "CVAE-GAN: fine-grained image generation through asymmetric training", "venue": "In Proceedings of the IEEE International Conference on Computer", "year": 2017}, {"authors": ["Elias Bareinboim", "Judea Pearl"], "title": "Causal inference and the data-fusion problem", "venue": "Proceedings of the National Academy of Sciences 113,", "year": 2016}, {"authors": ["Solon Barocas", "Andrew D"], "title": "Selbst. 2016", "venue": "Big data\u2019s disparate impact. Calif. L. Rev", "year": 2016}, {"authors": ["Yahav Bechavod", "Katrina Ligett"], "title": "Learning fair classifiers: A regularization-inspired approach", "year": 2017}, {"authors": ["Yahav Bechavod", "Katrina Ligett"], "title": "Penalizing unfairness in binary classification", "year": 2017}, {"authors": ["Richard Berk", "Hoda Heidari", "Shahin Jabbari", "Matthew Joseph", "Michael Kearns", "Jamie Morgenstern", "Seth Neel", "Aaron Roth"], "title": "A convex framework for fair regression", "year": 2017}, {"authors": ["Richard Berk", "Hoda Heidari", "Shahin Jabbari", "Michael Kearns", "Aaron Roth"], "title": "Fairness in criminal justice risk assessments: The state of the art", "venue": "Sociological Methods & Research", "year": 2018}, {"authors": ["Alex Beutel", "Jilin Chen", "Tulsee Doshi", "Hai Qian", "Allison Woodruff", "Christine Luu", "Pierre Kreitmann", "Jonathan Bischof", "Ed H Chi"], "title": "Putting fairness principles into practice: Challenges, metrics, and improvements", "year": 2019}, {"authors": ["Alex Beutel", "Jilin Chen", "Zhe Zhao", "Ed H Chi"], "title": "Data decisions and theoretical implications when adversarially learning fair representations", "year": 2017}, {"authors": ["Tolga Bolukbasi", "Kai-Wei Chang", "James Y Zou", "Venkatesh Saligrama", "Adam T Kalai"], "title": "Man is to computer programmer as woman is to homemaker? debiasing word embeddings", "venue": "In Advances in neural information processing systems", "year": 2016}, {"authors": ["Shikha Bordia", "Samuel Bowman"], "title": "Identifying and Reducing Gender Bias in Word-Level Language Models", "venue": "In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Student Research Workshop", "year": 2019}, {"authors": ["Avishek Bose andWilliamHamilton"], "title": "Compositional Fairness Constraints for Graph Embeddings", "venue": "In International Conference on Machine Learning", "year": 2019}, {"authors": ["Robert Bredereck", "Piotr Faliszewski", "Ayumi Igarashi", "Martin Lackner", "Piotr Skowron"], "title": "Multiwinner elections with diversity constraints", "venue": "In Thirty-Second AAAI Conference on Artificial Intelligence", "year": 2018}, {"authors": ["Phil Brierley", "David Vogel", "Randy Axelrod"], "title": "Heritage Provider Network Health Prize Round 1 Milestone Prize: How we did it\u2013Team \u00e2\u0102\u0178Market Makers\u00e2\u0102\u0179", "year": 2011}, {"authors": ["Marc-Etienne Brunet", "Colleen Alkalay-Houlihan", "Ashton Anderson", "Richard Zemel"], "title": "Understanding the Origins of Bias in Word Embeddings", "venue": "In International Conference on Machine Learning", "year": 2019}, {"authors": ["Joy Buolamwini", "Timnit Gebru"], "title": "Gender shades: Intersectional accuracy disparities in commercial gender classification", "venue": "In Conference on fairness, accountability and transparency", "year": 2018}, {"authors": ["Robin Burke"], "title": "Multisided Fairness for Recommendation", "year": 2017}, {"authors": ["Robin Burke", "Nasim Sonboli", "Masoud Mansoury", "Aldo"], "title": "Balanced Neighborhoods for Fairness-aware Collaborative Recommendation", "venue": "Ordon\u0303ez-Gauger", "year": 2017}, {"authors": ["Toon Calders", "Sicco Verwer"], "title": "Three naive Bayes approaches for discrimination-free classification", "venue": "Data Mining and Knowledge Discovery 21,", "year": 2010}, {"authors": ["Aylin Caliskan", "Joanna J Bryson", "Arvind Narayanan"], "title": "Semantics derived automatically from language corpora necessarily contain human", "venue": "biases. CoRR,", "year": 2016}, {"authors": ["Flavio Calmon", "Dennis Wei", "Bhanukiran Vinzamuri", "Karthikeyan Natesan Ramamurthy", "Kush R Varshney"], "title": "Optimized pre-processing for discrimination prevention", "venue": "In Advances in Neural Information Processing Systems", "year": 2017}, {"authors": ["L Elisa Celis", "Lingxiao Huang", "Nisheeth K Vishnoi"], "title": "Multiwinner voting with fairness constraints", "venue": "In Proceedings of the 27th International Joint Conference on Artificial Intelligence", "year": 2018}, {"authors": ["L Elisa Celis", "Vijay Keswani"], "title": "Improved Adversarial Learning for Fair Classification", "year": 2019}, {"authors": ["L Elisa Celis"], "title": "Damian Straszak", "venue": "and Nisheeth K Vishnoi. 2018. Ranking with Fairness Constraints. In 45th International Colloquium on Automata, Languages, and Programming ", "year": 2018}, {"authors": ["Flavio Chierichetti", "Ravi Kumar", "Silvio Lattanzi", "Sergei Vassilvitskii"], "title": "Fair clustering through fairlets", "venue": "In Advances in Neural Information Processing Systems", "year": 2017}, {"authors": ["Alexandra Chouldechova"], "title": "Fair prediction with disparate impact: A study of bias in recidivism prediction instruments", "venue": "Big data 5,", "year": 2017}, {"authors": ["Alexandra Chouldechova", "Aaron Roth"], "title": "The frontiers of fairness in machine learning", "year": 2018}, {"authors": ["Sam Corbett-Davies", "Sharad Goel"], "title": "The measure and mismeasure of fairness: A critical review of fair machine learning", "year": 2018}, {"authors": ["Sam Corbett-Davies", "Emma Pierson", "Avi Feller", "Sharad Goel", "Aziz Huq"], "title": "Algorithmic decision making and the cost of fairness", "venue": "In Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining", "year": 2017}, {"authors": ["Jeffrey Dastin"], "title": "Amazon scraps secret AI recruiting tool that showed bias against women", "year": 2018}, {"authors": ["Amit Datta", "Michael Carl Tschantz", "Anupam Datta"], "title": "Automated experiments on ad privacy settings", "venue": "Proceedings on privacy enhancing technologies 2015,", "year": 2015}, {"authors": ["Jia Deng", "Wei Dong", "Richard Socher", "Li-Jia Li", "Kai Li", "Li Fei-Fei"], "title": "Imagenet: A large-scale hierarchical image database", "venue": "IEEE conference on computer vision and pattern recognition", "year": 2009}, {"authors": ["Cynthia Dwork", "Moritz Hardt", "Toniann Pitassi", "Omer Reingold", "Richard Zemel"], "title": "Fairness through awareness", "venue": "In Proceedings of the 3rd innovations in theoretical computer science conference", "year": 2012}, {"authors": ["Cynthia Dwork", "Christina Ilvento"], "title": "Fairness Under Composition", "venue": "Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik", "year": 2018}, {"authors": ["Cynthia Dwork", "Nicole Immorlica", "Adam Tauman Kalai", "Max Leiserson"], "title": "Decoupled classifiers for group-fair and efficient machine learning", "venue": "In Conference on Fairness, Accountability and Transparency", "year": 2018}, {"authors": ["Bora Edizel", "Francesco Bonchi", "Sara Hajian", "Andr\u00e9 Panisson", "Tamir Tassa"], "title": "FaiRecSys: mitigating algorithmic bias in recommender systems", "venue": "International Journal of Data Science and Analytics", "year": 2019}, {"authors": ["Harrison Edwards", "Amos Storkey"], "title": "Censoring representations with an adversary", "year": 2015}, {"authors": ["Michael D Ekstrand", "Maria Soledad Pera"], "title": "The demographics of cool", "venue": "Poster Proceedings at ACM RecSys. ACM,", "year": 2017}, {"authors": ["Michael D Ekstrand", "Mucun Tian", "Mohammed R Imran Kazi", "Hoda Mehrpouyan", "Daniel Kluver"], "title": "Exploring author gender in book rating and recommendation", "venue": "In Proceedings of the 12th ACM Conference on Recommender Systems", "year": 2018}, {"authors": ["Hadi Elzayn", "Shahin Jabbari", "Christopher Jung", "Michael Kearns", "Seth Neel", "Aaron Roth", "Zachary Schutzman"], "title": "Fair algorithms for learning in allocation problems", "venue": "In Proceedings of the Conference on Fairness, Accountability, and Transparency", "year": 2019}, {"authors": ["Vitalii Emelianov", "George Arvanitakis", "Nicolas Gast", "Krishna Gummadi", "Patrick Loiseau"], "title": "The Price of Local Fairness in Multistage Selection", "year": 2019}, {"authors": ["Danielle Ensign", "Sorelle A Friedler", "Scott Neville", "Carlos Scheidegger", "Suresh Venkatasubramanian"], "title": "Runaway Feedback Loops in Predictive Policing", "venue": "In Conference of Fairness, Accountability,", "year": 2018}, {"authors": ["Michael Feldman", "Sorelle A Friedler", "John Moeller", "Carlos Scheidegger", "Suresh Venkatasubramanian"], "title": "Certifying and removing disparate impact", "venue": "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining", "year": 2015}, {"authors": ["Yoav Freund", "Robert E Schapire"], "title": "Game theory, on-line prediction and boosting", "venue": "In COLT,", "year": 1996}, {"authors": ["Sorelle A Friedler", "Carlos Scheidegger", "Suresh Venkatasubramanian"], "title": "On the (im) possibility of fairness", "year": 2016}, {"authors": ["Sorelle A Friedler", "Carlos Scheidegger", "Suresh Venkatasubramanian", "Sonam Choudhary", "Evan P Hamilton", "Derek Roth"], "title": "A comparative study of fairness-enhancing interventions in machine learning", "venue": "In Proceedings of the Conference on Fairness, Accountability, and Transparency", "year": 2019}, {"authors": ["Robert Fullinwider"], "title": "Affirmative Action. In The Stanford Encyclopedia of Philosophy (summer 2018 ed.)", "venue": "Edward N. Zalta (Ed.). Metaphysics", "year": 2018}, {"authors": ["Gabriel Goh", "Andrew Cotter", "Maya Gupta", "Michael P Friedlander"], "title": "Satisfying real-world goals with dataset constraints", "venue": "In Advances in Neural Information Processing Systems", "year": 2016}, {"authors": ["Hila Gonen", "Yoav Goldberg"], "title": "Lipstick on a Pig: Debiasing Methods Cover up Systematic Gender Biases in Word Embeddings But do not Remove Them. In Proceedings of the 2019 Conference of the North American Chapter Algorithmic Fairness 23 of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)", "year": 2019}, {"authors": ["Ian Goodfellow", "Jean Pouget-Abadie", "Mehdi Mirza", "Bing Xu", "David Warde-Farley", "Sherjil Ozair", "Aaron Courville", "Yoshua Bengio"], "title": "Generative adversarial nets", "venue": "In Advances in neural information processing systems", "year": 2014}, {"authors": ["Arthur Gretton", "Karsten M Borgwardt", "Malte J Rasch", "Bernhard Sch\u00f6lkopf", "Alexander Smola"], "title": "A kernel two-sample test", "venue": "Journal of Machine Learning Research", "year": 2012}, {"authors": ["Nina Grgic-Hlaca", "Elissa M Redmiles", "Krishna P Gummadi", "Adrian Weller"], "title": "Human perceptions of fairness in algorithmic decision making: A case study of criminal risk prediction", "venue": "In Proceedings of the 2018 World Wide Web Conference. International World Wide Web Conferences Steering Committee,", "year": 2018}, {"authors": ["Evan Hamilton"], "title": "Benchmarking Four Approaches to Fairness-Aware Machine Learning", "venue": "Ph.D. Dissertation. Haverford College", "year": 2017}, {"authors": ["Moritz Hardt", "Eric Price", "Nati Srebro"], "title": "Equality of opportunity in supervised learning", "venue": "In Advances in neural information processing systems", "year": 2016}, {"authors": ["Hoda Heidari", "Andreas Krause"], "title": "Preventing Disparate Treatment in Sequential Decision Making", "venue": "In IJCAI", "year": 2018}, {"authors": ["Lisa Anne Hendricks", "Kaylee Burns", "Kate Saenko", "Trevor Darrell", "Anna Rohrbach"], "title": "Women also snowboard: Overcoming bias in captioning models", "venue": "In European Conference on Computer", "year": 2018}, {"authors": ["Kenneth Holstein", "Jennifer Wortman Vaughan", "Hal Daum\u00e9 III", "Miro Dudik", "Hanna Wallach"], "title": "Improving fairness in machine learning systems: What do industry practitioners need", "venue": "In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems", "year": 2019}, {"authors": ["Lily Hu", "Yiling Chen"], "title": "A short-term intervention for long-term fairness in the labor market", "venue": "In Proceedings of the 2018 World Wide Web Conference. International World Wide Web Conferences Steering Committee,", "year": 2018}, {"authors": ["Elle Hunt"], "title": "2016. Tay, Microsoft\u2019s AI chatbot, gets a crash course in racism from Twitter", "year": 2016}, {"authors": ["Pablo Ibarrar\u00e1n", "Nadin Medell\u00edn", "Ferdinando Regalia", "Marco Stampini", "Sandro Parodi", "Luis Tejerina", "Pedro Cueva", "Madiery V\u00e1squez"], "title": "How conditional cash transfers work", "venue": "IDB Publications (Books)", "year": 2017}, {"authors": ["Shahin Jabbari", "Matthew Joseph", "Michael Kearns", "Jamie Morgenstern", "Aaron Roth"], "title": "Fairness in reinforcement learning", "venue": "In Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org,", "year": 2017}, {"authors": ["Matthew Joseph", "Michael Kearns", "Jamie H Morgenstern", "Aaron Roth"], "title": "Fairness in learning: Classic and contextual bandits", "venue": "In Advances in Neural Information Processing Systems", "year": 2016}, {"authors": ["Nathan Kallus", "Xiaojie Mao", "Angela Zhou"], "title": "Assessing Algorithmic Fairness with Unobserved Protected Class Using Data Combination", "year": 2019}, {"authors": ["Faisal Kamiran", "Toon Calders"], "title": "Classifying without discriminating", "venue": "In 2009 2nd International Conference on Computer, Control and Communication", "year": 2009}, {"authors": ["Faisal Kamiran", "Toon Calders"], "title": "Data preprocessing techniques for classification without discrimination", "venue": "Knowledge and Information Systems 33,", "year": 2012}, {"authors": ["Faisal Kamiran", "Toon Calders", "Mykola Pechenizkiy"], "title": "Discrimination aware decision tree learning", "venue": "In 2010 IEEE International Conference on Data Mining", "year": 2010}, {"authors": ["Toshihiro Kamishima", "Shotaro Akaho", "Hideki Asoh", "Jun Sakuma"], "title": "Enhancement of the Neutrality in Recommendation", "venue": "In Decisions@", "year": 2012}, {"authors": ["Toshihiro Kamishima", "Shotaro Akaho", "Hideki Asoh", "Jun Sakuma"], "title": "Fairness-aware classifier with prejudice remover regularizer", "venue": "In Joint European Conference on Machine Learning and Knowledge Discovery in Databases", "year": 2012}, {"authors": ["Sampath Kannan", "Jamie H Morgenstern", "Aaron Roth", "Bo Waggoner", "Zhiwei Steven Wu"], "title": "A smoothed analysis of the greedy algorithm for the linear contextual bandit problem", "venue": "In Advances in Neural Information Processing Systems", "year": 2018}, {"authors": ["Kimmo K\u00e4rkk\u00e4inen", "Jungseock Joo"], "title": "FairFace: Face Attribute Dataset for Balanced Race, Gender, and Age", "year": 2019}, {"authors": ["Matthew Kay", "Cynthia Matuszek", "Sean A Munson"], "title": "Unequal representation and gender stereotypes in image search results for occupations", "venue": "In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems", "year": 2015}, {"authors": ["Ehsan Kazemi", "Morteza Zadimoghaddam", "Amin Karbasi"], "title": "Scalable deletion-robust submodular maximization: Data summarization with privacy and fairness constraints", "venue": "In International conference on machine learning", "year": 2018}, {"authors": ["Niki Kilbertus", "Mateo Rojas Carulla", "Giambattista Parascandolo", "Moritz Hardt", "Dominik Janzing", "Bernhard Sch\u00f6lkopf"], "title": "Avoiding discrimination through causal reasoning", "venue": "In Advances in Neural Information Processing Systems", "year": 2017}, {"authors": ["Jon Kleinberg", "Sendhil Mullainathan", "Manish Raghavan"], "title": "Inherent Trade-Offs in the Fair Determination of Risk Scores", "venue": "In 8th Innovations in Theoretical Computer Science Conference (ITCS 2017). Schloss Dagstuhl-Leibniz- Zentrum fuer Informatik", "year": 2017}, {"authors": ["Eric L Lee", "Jing-Kai Lou", "Wei-Ming Chen", "Yen-Chi Chen", "Shou-De Lin", "Yen-Sheng Chiang", "Kuan-Ta Chen"], "title": "Fairness-aware loan recommendation for microfinance services", "venue": "In Proceedings of the 2014 International Conference on Social Computing. ACM,", "year": 2014}, {"authors": ["Erich L Lehmann", "Joseph P Romano"], "title": "Testing statistical hypotheses", "year": 2006}, {"authors": ["Zachary C Lipton", "Alexandra Chouldechova", "Julian McAuley"], "title": "Does mitigating ML\u00e2\u0102\u0179s disparate impact require disparate treatment", "year": 2017}, {"authors": ["Weiwen Liu", "Robin Burke"], "title": "Personalizing fairness-aware re-ranking", "year": 2018}, {"authors": ["Joshua R Loftus", "Chris Russell", "Matt J Kusner", "Ricardo Silva"], "title": "Causal reasoning for algorithmic fairness", "year": 2018}, {"authors": ["Christos Louizos", "Kevin Swersky", "Yujia Li", "Max Welling", "Richard Zemel"], "title": "The variational fair autoencoder", "venue": "International Conference on Learning Representations (ICLR)", "year": 2016}, {"authors": ["Binh Thanh Luong", "Salvatore Ruggieri", "Franco Turini"], "title": "k-NN as an implementation of situation testing for discrimination discovery and prevention", "venue": "In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining", "year": 2011}, {"authors": ["David Madras", "Elliot Creager", "Toniann Pitassi", "Richard Zemel"], "title": "Learning adversarially fair and transferable representations", "year": 2018}, {"authors": ["David Madras", "Toniann Pitassi", "Richard Zemel"], "title": "Predict responsibly: Increasing fairness by learning to defer", "venue": "International Conference on Learning Representations (ICLR)", "year": 2018}, {"authors": ["Rowland Manthorpe"], "title": "Beauty.AI\u2019s \u2019robot beauty contest\u2019 is back \u00e2\u0102\u015e and this time it promises not to be racist", "year": 2017}, {"authors": ["Fernando Mart\u00ednez-Plumed", "C\u00e8sar Ferri", "David Nieves", "Jos\u00e9 Hern\u00e1ndez-Orallo"], "title": "Fairness and Missing Values", "year": 2019}, {"authors": ["Chandler May", "Alex Wang", "Shikha Bordia", "Samuel Bowman", "Rachel Rudinger"], "title": "On Measuring Social Biases in Sentence Encoders", "year": 2019}, {"authors": ["Aditya Krishna Menon", "Robert C Williamson"], "title": "The cost of fairness in binary classification", "venue": "In Conference on Fairness, Accountability and Transparency", "year": 2018}, {"authors": ["Weiwen Miao"], "title": "Did the results of promotion exams have a disparate impact on minorities? Using statistical evidence", "venue": "in Ricci v. DeStefano. J. of Stat. Ed 19,", "year": 2011}, {"authors": ["S\u00e9rgio Moro", "Paulo Cortez", "Paulo Rita"], "title": "A data-driven approach to predict the success of bank telemarketing", "venue": "Decision Support Systems", "year": 2014}, {"authors": ["Razieh Nabi", "Ilya Shpitser"], "title": "Fair inference on outcomes", "venue": "In Thirty-Second AAAI Conference on Artificial Intelligence", "year": 2018}, {"authors": ["Alejandro Noriega-Campero", "Michiel A Bakker", "Bernardo Garcia-Bulle", "Alex\u2019Sandy\u2019 Pentland"], "title": "Active Fairness in Algorithmic Decision Making", "venue": "In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society", "year": 2019}, {"authors": ["Pete Pachal"], "title": "Google Photos identified two black people as \u2019gorillas", "year": 2015}, {"authors": ["Sinno Jialin Pan", "Qiang Yang"], "title": "A survey on transfer learning", "venue": "IEEE Transactions on knowledge and data engineering 22,", "year": 2009}, {"authors": ["Jeffrey Pennington", "Richard Socher", "Christopher Manning"], "title": "Glove: Global vectors for word representation", "venue": "In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)", "year": 2014}, {"authors": ["D Pessach", "E Shmueli"], "title": "2020", "venue": "Improving Fairness in Semi-Supervised Problems with Privileged-Group Selection Bias. ", "year": 2020}, {"authors": ["Geoff Pleiss", "Manish Raghavan", "Felix Wu", "Jon Kleinberg", "Kilian Q Weinberger"], "title": "On fairness and calibration", "venue": "In Advances in Neural Information Processing Systems", "year": 2017}, {"authors": ["Novi Quadrianto", "Viktoriia Sharmanska"], "title": "Recycling privileged learning and distribution matching for fairness", "venue": "In Advances in Neural Information Processing Systems", "year": 2017}, {"authors": ["Novi Quadrianto", "Viktoriia Sharmanska", "Oliver Thomas"], "title": "Discovering fair representations in the data domain", "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition", "year": 2019}, {"authors": ["Michael Redmond", "Alok Baveja"], "title": "A data-driven software tool for enabling cooperative information sharing among police departments", "venue": "European Journal of Operational Research 141,", "year": 2002}, {"authors": ["Derek Roth"], "title": "A Comparison of Fairness-Aware Machine Learning Algorithms", "venue": "Ph.D. Dissertation. Haverford College", "year": 2018}, {"authors": ["Rachel Rudinger", "Jason Naradowsky", "Brian Leonard", "Benjamin Van Durme"], "title": "Gender Bias in Coreference Resolution", "year": 2018}, {"authors": ["Chris Russell", "Matt J Kusner", "Joshua Loftus", "Ricardo Silva"], "title": "When worlds collide: integrating different counterfactual assumptions in fairness", "venue": "In Advances in Neural Information Processing Systems", "year": 2017}, {"authors": ["George Rutherglen"], "title": "Disparate impact under title VII: an objective theory of discrimination", "venue": "Va. L. Rev", "year": 1987}, {"authors": ["George Rutherglen"], "title": "Ricci v DeStefano: Affirmative Action and the Lessons of Adversity", "venue": "The Supreme Court Review 2009,", "year": 2009}, {"authors": ["Hee Jung Ryu", "Hartwig Adam", "Margaret Mitchell"], "title": "Inclusivefacenet: Improving face attribute detection with race and gender diversity", "year": 2017}, {"authors": ["Samira Samadi", "Uthaipon Tantipongpipat", "Jamie H Morgenstern", "Mohit Singh", "Santosh Vempala"], "title": "The price of fair PCA: One extra dimension", "venue": "In Advances in Neural Information Processing Systems", "year": 2018}, {"authors": ["Richard H Sander"], "title": "A systemic analysis of affirmative action in American law schools", "venue": "Stan. L. Rev", "year": 2004}, {"authors": ["Prasanna Sattigeri", "Samuel C Hoffman", "Vijil Chenthamarakshan", "Kush Raj Varshney"], "title": "Fairness GAN: Generating datasets with fairness properties using a generative adversarial network", "venue": "IBM Journal of Research and Development", "year": 2019}, {"authors": ["Tom Simonite"], "title": "Probing the Dark Side of Google\u00e2\u0102\u0179s Ad-Targeting System. https://www.technologyreview", "year": 2015}, {"authors": ["Tom Simonite"], "title": "When It Comes to Gorillas, Google Photos Remains Blind", "year": 2018}, {"authors": ["Peter Spirtes", "Christopher Meek", "Thomas Richardson"], "title": "Causal inference in the presence of latent variables and selection bias", "venue": "In Proceedings of the Eleventh conference on Uncertainty in artificial intelligence", "year": 1995}, {"authors": ["Megha Srivastava", "Hoda Heidari", "Andreas Krause"], "title": "Mathematical Notions vs. Human Perception of Fairness: A Descriptive Approach to Fairness for Machine Learning", "year": 2019}, {"authors": ["Pierre Stock", "Moustapha Cisse"], "title": "Convnets and imagenet beyond accuracy: Explanations, bias detection, adversarial examples and model criticism", "year": 2017}, {"authors": ["\u00d6zge S\u00fcrer", "Robin Burke", "Edward CMalthouse"], "title": "Multistakeholder recommendation with provider constraints", "venue": "In Proceedings of the 12th ACM Conference on Recommender Systems", "year": 2018}, {"authors": ["N TISHBY"], "title": "1999", "venue": "The information bottleneck method. In Proc. 37th Annual Allerton Conference on Communications, Control and Computing", "year": 1999}, {"authors": ["Florian Tramer", "Vaggelis Atlidakis", "Roxana Geambasu", "Daniel Hsu", "Jean-Pierre Hubaux", "Mathias Humbert", "Ari Juels", "Huang Lin"], "title": "FairTest: Discovering unwarranted associations in data-driven applications", "venue": "IEEE European Symposium on Security and Privacy (EuroS&P)", "year": 2017}, {"authors": ["Isabel Valera", "Adish Singla", "Manuel Gomez Rodriguez"], "title": "Enhancing the accuracy and fairness of human decision making", "venue": "In Advances in Neural Information Processing Systems", "year": 2018}, {"authors": ["Emiel van Miltenburg"], "title": "Stereotyping and Bias in the Flickr30k Dataset", "year": 2016}, {"authors": ["Vladimir Vapnik", "Rauf Izmailov"], "title": "Learning using privileged information: similarity control and knowledge transfer", "venue": "Journal of machine learning research", "year": 2015}, {"authors": ["Sahil Verma", "Julia Rubin"], "title": "Fairness definitions explained", "venue": "IEEE/ACM International Workshop on Software Fairness (FairWare)", "year": 2018}, {"authors": ["Christina Wadsworth", "Francesca Vera", "Chris Piech"], "title": "Achieving fairness through adversarial learning: an application to recidivism prediction", "year": 2018}, {"authors": ["Blake Woodworth", "Suriya Gunasekar", "Mesrob I Ohannessian", "Nathan Srebro"], "title": "Learning Non-Discriminatory Predictors", "venue": "In Conference on Learning", "year": 2017}, {"authors": ["Depeng Xu", "Shuhan Yuan", "Lu Zhang", "Xintao Wu"], "title": "Fairgan: Fairness-aware generative adversarial networks", "venue": "IEEE International Conference on Big Data (Big Data)", "year": 2018}, {"authors": ["Sirui Yao", "Bert Huang"], "title": "New Fairness Metrics for Recommendation that Embrace Differences", "year": 2017}, {"authors": ["I-Cheng Yeh", "Che-hui Lien"], "title": "The comparisons of data mining techniques for the predictive accuracy of probability of default of credit card clients", "venue": "Expert Systems with Applications 36,", "year": 2009}, {"authors": ["Muhammad Bilal Zafar", "Isabel Valera", "Manuel Gomez Rodriguez", "Krishna P Gummadi"], "title": "Fairness beyond disparate treatment & disparate impact: Learning classification without disparate mistreatment", "venue": "In Proceedings of the 26th International Conference on World Wide Web. International World Wide Web Conferences Steering Committee,", "year": 2017}, {"authors": ["Muhammad Bilal Zafar", "Isabel Valera", "Manuel Gomez Rodriguez", "Krishna P Gummadi"], "title": "Fairness Constraints: Mechanisms for Fair Classification", "venue": "In Artificial Intelligence and Statistics", "year": 2017}, {"authors": ["Rich Zemel", "Yu Wu", "Kevin Swersky", "Toni Pitassi", "Cynthia Dwork"], "title": "Learning fair representations", "venue": "In International Conference on Machine Learning", "year": 2013}, {"authors": ["Brian Hu Zhang", "Blake Lemoine", "Margaret Mitchell"], "title": "Mitigating unwanted biases with adversarial learning", "venue": "In Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society", "year": 2018}, {"authors": ["Junzhe Zhang", "Elias Bareinboim"], "title": "Fairness in decision-making\u00e2\u0102\u0164the causal explanation formula", "venue": "In Thirty-Second AAAI Conference on Artificial Intelligence", "year": 2018}, {"authors": ["Jieyu Zhao", "Tianlu Wang", "Mark Yatskar", "Ryan Cotterell", "Vicente Ordonez", "Kai-Wei Chang"], "title": "Gender Bias in Contextualized Word Embeddings", "year": 2019}, {"authors": ["Jieyu Zhao", "TianluWang", "Mark Yatskar", "Vicente Ordonez", "Kai-Wei Chang"], "title": "MenAlso Like Shopping: Reducing Gender Bias Amplification using Corpus-level Constraints", "venue": "In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing", "year": 2017}, {"authors": ["Jieyu Zhao", "Tianlu Wang", "Mark Yatskar", "Vicente Ordonez", "Kai-Wei Chang"], "title": "Gender Bias in Coreference Resolution: Evaluation and Debiasing Methods. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)", "year": 2018}, {"authors": ["Jieyu Zhao", "Yichao Zhou", "Zeyu Li", "Wei Wang", "Kai-Wei Chang"], "title": "Learning Gender-Neutral Word Embeddings", "venue": "In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing", "year": 2018}, {"authors": ["Michael J Zimmer"], "title": "Emerging Uniform Structure of Disparate Treatment Discrimination Litigation", "venue": "Ga. L. Rev", "year": 1995}], "sections": [{"heading": "Algorithmic Fairness", "text": "DANA PESSACH\u2217, Department of Industrial Engineering, Tel-Aviv University, Israel EREZ SHMUELI, Department of Industrial Engineering, Tel-Aviv University, Israel\nAn increasing number of decisions regarding the daily lives of human beings are being controlled by artificial intelligence (AI) algorithms in spheres ranging from healthcare, transportation, and education to college admissions, recruitment, provision of loans and many more realms. Since they now touch on many aspects of our lives, it is crucial to develop AI algorithms that are not only accurate but also objective and fair. Recent studies have shown that algorithmic decision-making may be inherently prone to unfairness, even when there is no intention for it. This paper presents an overview of the main concepts of identifying, measuring and improving algorithmic fairness when using AI algorithms. The paper begins by discussing the causes of algorithmic bias and unfairness and the common definitions and measures for fairness. Fairness-enhancing mechanisms are then reviewed and divided into pre-process, in-process and post-process mechanisms. A comprehensive comparison of the mechanisms is then conducted, towards a better understanding of which mechanisms should be used in different scenarios. The paper then describes the most commonly used fairnessrelated datasets in this field. Finally, the paper ends by reviewing several emerging research sub-fields of algorithmic fairness.\nKeywords: Algorithmic Bias, Algorithmic Fairness, Fairness-Aware Machine Learning."}, {"heading": "1 INTRODUCTION", "text": "Nowadays, an increasing number of decisions are being controlled by artificial intelligence (AI) algorithms, with increased implementation of automated decision-making systems in business and government applications. The motivation for an automated learning model is clear \u2013 we expect algorithms to perform better than human beings for several reasons: First, algorithms may integrate much more data than a human may grasp and take many more considerations into account. Second, algorithms can perform complex computations much faster than human beings. Third, human decisions are subjective, and they often include biases.\nHence, it is a common belief that using an automated algorithm makes decisions more objective or fair. However, this is unfortunately not the case since AI algorithms are not always as objective as we would expect. The idea that AI algorithms are free from biases is wrong since the assumption that the data injected into the models are unbiased is wrong. More specifically, a prediction model may actually be inherently biased since it learns and preserves historical biases [85].\nSince many automated decisions (including which individuals will receive jobs, loans, medication, bail or parole) can significantly impact people\u2019s lives, there is great importance in assessing and improving the ethics of the decisions made by these automated systems. Indeed, in recent years, the concern for algorithm fairness has made headlines. One of the most common examples was in the field of criminal justice, where recent revelations have shown that an algorithm used by the United States criminal justice system had falsely predicted future criminality among African-Americans at twice the rate as it predicted for white people [6, 36]. In another case of a hiring application, it was recently exposed that Amazon discovered that their AI hiring system was discriminating against female candidates, particularly for software development and technical positions. One suspected reason for this is that most recorded historical data were for male software developers [40]. In a\n\u2217Corresponding author\nAuthors\u2019 addresses: Dana Pessach, Department of Industrial Engineering, Tel-Aviv University, P.O. Box 39040, 6997801, Tel-Aviv, Israel, danapessach@gmail.com; Erez Shmueli, Department of Industrial Engineering, Tel-Aviv University, P.O. Box 39040, 6997801, Tel-Aviv, Israel, shmueli@tau.ac.il.\nar X\niv :2\n00 1.\n09 78\n4v 1\n[ cs\n.C Y\n] 2\n1 Ja\nn 20\n20\ndifferent scenario in advertising, it was shown that Google\u2019s ad-targeting algorithm had proposed higher-paying executive jobs more for men than for women [41, 125].\nThese lines of evidence and concerns about algorithmic fairness have led to growing interest in the literature on defining, evaluating and improving fairness in AI algorithms (see, for example, [15, 37, 57, 68]). It is important to note, however, that the task of improving fairness of AI algorithms is not trivial since there exists an inherent trade-off between accuracy and fairness. That is, as we pursue a higher degree of fairness, we may compromise accuracy (see, for example, [85]).\nIn contrast to other recent surveys in this field [37, 57], our paper proposes a comprehensive and up-to-date overview of the field, ranging from definitions and measures of fairness to state-of-theart fairness-enhancing mechanisms. Our survey also attempts to cover the pros and cons of the various measures and mechanisms, and guide under which setting they should be used. Finally, a major goal of this survey is to highlight and discuss emerging areas of research that are expected to grow in the upcoming years. Overall, this survey provides the relevant knowledge to enable new researchers to enter the field, inform current researchers on rapidly evolving sub-fields, and provide practitioners the necessary tools to apply the results.\nThe rest of this paper is structured as follows: Section 2 discusses the potential causes of algorithmic unfairness; Section 3 presents definitions and measures of fairness and their trade-offs; Section 4 reviews fairness mechanisms and methods and a comparison of the mechanisms, focusing on the pros and cons of each mechanism; Section 5 outlines commonly used fairness-related datasets; Section 6 presents several emerging research sub-fields of algorithmic fairness; and Section 7 provides concluding remarks and sketches several open challenges for future research."}, {"heading": "2 POTENTIAL CAUSES OF UNFAIRNESS", "text": "The literature has indicated several causes that may lead to unfairness in machine learning [37, 99]:\n\u2022 Biases already included in the datasets used for learning, which are based on biased device measurements, historically biased human decisions, erroneous reports or other reasons. Machine learning algorithms are essentially designed to replicate these biases. \u2022 Biases caused by missing data, such as missing values or sample/selection biases, which result in datasets that are not representative of the target population. \u2022 Biases that stem from algorithmic objectives, which aim at minimizing overall aggregated prediction errors and therefore benefit majority groups over minorities. \u2022 Biases caused by \"proxy\" attributes for sensitive attributes. Sensitive attributes differentiate privileged and unprivileged groups, such as race, gender and age, and are typically not legitimate for use in decision making. Proxy attributes are non-sensitive attributes that can be exploited to derive sensitive attributes. In the case that the dataset contains proxy attributes, the machine learning algorithm can implicitly make decisions based on the sensitive attributes under the cover of using presumably legitimate attributes [11].\nTo illustrate the last cause mentioned above, consider the example depicted in Figure 1. The figure illustrates a case of SAT scores for two sub-populations: a privileged one and an unprivileged one.\nIn this illustration, SAT scores may be used to predict the probability of job success when hiring candidates since the higher the SAT score is, the higher the probability of success. However, unprivileged candidates with SAT scores of approximately 1100 perform just as well as privileged candidates with SAT scores of 1400 since they may have encountered more challenging pathways to achieve their scores. In other words, if the SAT scores were used for hiring, unprivileged candidates with high potential would be excluded, whereas lower potential candidates from the privileged group would be hired instead."}, {"heading": "3 FAIRNESS DEFINITIONS AND MEASURES", "text": "This section presents some general legal notions for discrimination followed by a survey of the most common measures for algorithmic fairness, and the inevitable trade-offs between them."}, {"heading": "3.1 Definitions of Discrimination in Legal Domains", "text": "The legal domain has introduced two main definitions of discrimination: i) disparate treatment [11, 151]: intentionally treating an individual differently based on his/her membership in a protected class (direct discrimination); ii) disparate impact [11, 119]: negatively affecting members of a protected class more than others even if by a seemingly neutral policy (indirect discrimination).\nPut in our context, it is important to note that algorithms trained with data that do not include sensitive attributes (i.e., attributes that explicitly identify the protected and unprotected groups) are unlikely to produce disparate treatment, but may still induce unintentional discrimination in the form of disparate impact [85]."}, {"heading": "3.2 Measures of Algorithmic Bias", "text": "This section presents the most prominent measures of algorithmic fairness in machine learning classification tasks. We refer the readers to the Appendix and specifically to Table 4 for a review of additional, less popular measures used in the literature. (1) Disparate impact [54] \u2013 This measure was designed to mathematically represent the legal\nnotion of disparate impact. It requires a high ratio between the positive prediction rates of both groups. This ensures that the proportion of the positive predictions is similar across groups. For example, if a positive prediction represents acceptance for a job, the condition\nrequires the proportion of accepted applicants to be similar across groups. Formally, this measure is computed as follows:\nP[Y\u0302 = 1|S , 1] P[Y\u0302 = 1|S = 1] \u2265 1 \u2212 \u03b5 (1)\nwhere S represents the protected attribute (e.g., race or gender), S = 1 is the privileged group, and S , 1 is the unprivileged group. Y\u0302 = 1 means that the prediction is positive. Let us note that if Y\u0302 = 1 represents acceptance (e.g., for a job), then the condition requires the acceptance rates to be similar across groups. A higher value of this measure represents more similar rates across groups and therefore more fairness. Note that this notion relates to the \"80 percent rule\" in disparate impact law [54], which requires that the acceptance rate for any race, sex, or ethnic group be at least 80% of the rate for the group with the highest rate.\n(2) Demographic parity \u2013 This measure is similar to disparate impact, but the difference is taken instead of the ratio [28, 44]. This measure is also commonly referred to as statistical parity. Formally, this measure is computed as follows: P[Y\u0302 = 1|S = 1] \u2212 P[Y\u0302 = 1|S , 1] \u2264 \u03b5 (2) A lower value of this measure indicates more similar acceptance rates and therefore better fairness. Demographic parity (and disparate impact) ensure that the positive prediction is assigned to the two groups at a similar rate.\nOne disadvantage of these two measures is that a fully accurate classifier may be considered unfair, when the base rates (i.e., the proportion of actual positive outcomes) of the various groups are significantly different. Moreover, in order to satisfy demographic parity, two similar individuals may be treated differently since they belong to two different groups \u2013 such treatment is prohibited by law in some cases (note that this notion also corresponds to the practice of affirmative action [58]).\n(3) Equalized odds \u2013 This measure was designed by [65] to overcome the disadvantages of measures such as disparate impact and demographic parity. The measure computes the difference between the false positive rates (FPR), and the difference between the true positive rates (TPR) of the two groups. Formally, this measure is computed as follows: P[Y\u0302 = 1|S = 1,Y = 0] \u2212 P[Y\u0302 = 1|S , 1,Y = 0] \u2264 \u03b5 (3) P[Y\u0302 = 1|S = 1,Y = 1] \u2212 P[Y\u0302 = 1|S , 1,Y = 1] \u2264 \u03b5 (4) Where the upper formula requires the absolute difference in the FPR of the two groups to be bounded by \u03b5 , and the lower formula requires the absolute difference in the TPR of the two groups to be bounded \u03b5 . Smaller differences between groups indicate better fairness. In contrast to demographic parity and disparate impact measures, a fully accurate classifier will necessarily satisfy the two equalized odds constraints. Nevertheless, since equalized odds relies on the actual ground truth (i.e., Y ), it assumes that the base rates of the two groups are representative and were not obtained in a biased manner.\nOne use case that demonstrates the effectiveness of this measure investigated the COMPAS [1] algorithm used in the United States criminal justice system. For predicting recidivism,\nalthough its accuracy was similar for both groups (African-Americans and Caucasians), it was discovered that the odds were different. It was discovered that the system had falsely predicted future criminality (FPR) among African-Americans at twice the rate predicted for white people [6]; importantly, the algorithm also induced the opposite error, significantly underestimating future crimes among Caucasians (FNR).\n(4) Equal opportunity \u2013 This requires true positive rates (TPRs) to be similar across groups (meaning the probability of an individual with a positive outcome to have a positive prediction) [65]. This measure is similar to equalized odds but focuses on the true positive rates only. This measure is mathematically formulated as follows: P[Y\u0302 = 1|S , 1,Y = 1] \u2212 P[Y\u0302 = 1|S = 1,Y = 1] \u2264 \u03b5 (5) Let us note that following the equality in terms of only one type of error (e.g., true positives) will increase the disparity in terms of the other error [112]. Moreover, according to [38], this measure may be problematic when base rates differ between groups.\nThus far, we have mapped the most common group notions of fairness, which require parity of some statistical measure across groups. The literature has additionally indicated individual notions of fairness. It is alternatively possible to match other measures such as accuracy, error rates or calibration values between groups (see the Appendix and specifically Table 4). Group definitions of fairness, such as demographic parity, disparate impact, equalized odds and equalized opportunity, consider fairness with respect to the whole group, as opposed to individual notions of fairness.\n(5) Individual fairness \u2013 This requires that similar individuals will be treated similarly. Similarity may be defined with respect to a particular task [44, 73]. Individual fairness may be described as follows: P(Y\u0302 (i) = y |X (i), S (i)) \u2212 P(Y\u0302 (j) = y |X (j), S (j)) \u2264 \u03b5 ; i f d(i, j) \u2248 0 (6) where i and j denote two individuals, S (\u00b7) refers to the individuals\u2019 sensitive attributes andX (\u00b7) refers to their associated features. d(i, j) is a distance metric between individuals that can be defined depending on the domain such that similarity is measured according to an intended task. This measure considers other individual attributes for defining fairness, rather than just the sensitive attributes. However, note that in order to define similarity between individuals, a similarity metric needs to be defined, which is not trivial. This measure, in addition to assuming a similarity metric, also requires some assumptions regarding the relationship between features and labels (see, for example, [37])."}, {"heading": "3.3 Trade-offs", "text": "Determining the right measure to be used must take into account the proper legal, ethical, and social context. As demonstrated above, different measures exhibit different advantages and disadvantages. Next, we highlight the main trade-offs that exist between different notions of fairness, and the inherent trade-off between fairness and accuracy."}, {"heading": "Fairness measures trade-offs", "text": "Interestingly, several recent studies have shown that it is not possible to satisfy multiple notions of fairness simultaneously [15, 36, 38, 39, 56, 85, 112]. For example, when base rates differ between\ngroups, it is not possible to have a classifier that equalizes both calibration and odds (except for trivial cases such as a classifier that assigns all examples to a single class). Additionally, there is also evidence for incompatibility between equalized accuracy and equalized odds, as in the COMPAS criminal justice use case [6, 15]. [112] recommends that in light of the inherent incompatibility between equalized calibration and equalized odds, practical implications requires choosing only one of these goals according to the specific application\u2019s requirements. We recommend that any selected measure of algorithmic fairness be considered in the appropriate legal, social and ethical contexts."}, {"heading": "Fairness through", "text": ""}, {"heading": "Equalized", "text": ""}, {"heading": "Demographic", "text": ""}, {"heading": "Disparate", "text": ""}, {"heading": "Fairness-accuracy trade-off", "text": "The literature extensively discusses the inherent trade-off between accuracy and fairness - as we pursue a higher degree of fairness, we may compromise accuracy (see for example [85]). A theoretical analysis of the trade-off between fairness and accuracy was studies in [39] and [91]. Since then, many papers have empirically supported the existence of this trade-off (for example, [12, 57, 101]). Generally, the aspiration of a fairness-aware algorithm is to achieve a model that allows for higher fairness without significantly compromising the accuracy or other alternative notions of utility.\nTable 1 presents a summary of the measures presented in this section. For further reading about algorithmic fairness measures, we refer the reader to [38], [85], and [136]."}, {"heading": "4 FAIRNESS-ENHANCING MECHANISMS", "text": "Numerous recent papers have proposed mechanisms to enhance fairness in machine learning algorithms. These mechanisms are typically categorized into three types: pre-process, in-process, and post-process. The following three subsections review studies in each one of these categories. The fourth subsection is devoted for comparing the three mechanism types and providing guidelines on when each type should be used."}, {"heading": "4.1 Pre-Process Mechanisms", "text": "Mechanisms in this category involve changing the training data before feeding it into a machine learning algorithm. Preliminary mechanisms, such as the ones proposed by [76] and [95] proposed changing the labels of some instances or reweighing them before training to make the classification fairer. Typically, the labels that are changed are related to samples that are closer to the decision boundary since these are the ones that are most likely to be discriminated. More recent mechanisms suggest modifying feature representations, so that a subsequent classifier will be fairer [30, 54, 94, 122, 144].\nFor example, [54] suggest modifying the features in the dataset so that the distributions for both privileged and unprivileged groups become similar, and therefore, making it more difficult for the algorithm to differentiate between the two groups. A tuning parameter \u03bb was provided for controlling the trade-off between fairness and accuracy (\u03bb=0 indicates no fairness considerations, while \u03bb=1 maximizes fairness). [8, 35] use the same notion of fair representation learning and applies it for fair clustering, and [122] applies it for fair dimensionality reduction (PCA). For more fair representation learning using adversarial learning, see section 6.2.\nNote that this approach to achieving fairness is somewhat related to the field of data compression [131, 144]. It is also very closely related to privacy research since both fairness and privacy can be enhanced by removing or obfuscating the sensitive information, with the adversary goal of minimal data distortion [47, 83]."}, {"heading": "4.2 In-Process Mechanisms", "text": "These mechanisms involve modifying the machine learning algorithms to account for fairness during the training time [4, 12, 13, 28, 59, 79, 138, 142, 143].\nFor example, [79] suggest adding a regularization term to the objective function that penalizes the mutual information between the sensitive feature and the classifier predictions. A tuning parameter \u03b7 was provided to modulate the trade-off between fairness and accuracy.\n[142], [143] and [138] suggest adding constraints to the classificationmodel that require satisfying a proxy for equalized odds [138, 142] or disparate impact [143]. [138] also show that there exist difficult computational challenges in learning a fair classifier based on equalized odds.\n[12] and [13] suggest incorporating penalty terms into the objective function that enforce matching proxies of FPR and FNR. [77] suggest adjusting a decision tree split criterion to maximize information gain between the split attribute and the class label while minimizing information gain with respect to the sensitive attribute. [144] combine fair representation learning with an in-process model by applying a multi-objective loss function based on logistic regression, and [94] apply this notion using a variational autoencoder.\n[113] suggest using the notion of privileged learning1 for improving performance in cases where the sensitive information is available at training time but not at testing time. They add constraints and regularization components to the privileged learning support vector machine (SVM) model proposed by [135]. They combine the sensitive attributes as privileged information that is known only at training time, and they additionally use a maximum mean discrepancy (MMD) criterion [62] to encourage the distributions to be similar across privileged and unprivileged groups. [14] propose a convex in-process fairness mechanism for regression tasks and use three regularization terms that include variations of individual fairness, group fairness and a combined hybrid fairness penalty term. [5] propose an in-process minimax optimization formulation for enhancing fairness in regression tasks based on the suggested design of [4] for classification tasks. They use two fairness metrics adjusted for regression tasks. One is an adjusted demographic parity measure, which requires the predictor to be independent of the sensitive attribute as measured by the cumulative distribution function (CDF) of the protected group compared to the CDF of the general population [5] using the Kolmogorov-Smirnov statistic [90]. The second measure is the bounded group loss (BGL), which requires that the prediction error of all groups remain below a predefined level [5]."}, {"heading": "4.3 Post-Process Mechanisms", "text": "These mechanisms perform post-processing of the output scores of the classifier to make decisions fairer [39, 46, 65, 101]. For example, [65] propose a technique for flipping some decisions of a classifier to enhance equalized odds or equalized opportunity. [39] and [101] similarly suggest selecting separate thresholds for each group separately, in a manner that maximizes accuracy and minimizes demographic parity. [46] propose a decoupling technique to learn a different classifier for each group. They additionally combine a transfer learning technique with their procedure to learn from out-of-group samples (to read more about transfer learning, see [108]).\nTable 2 presents a summary of the pre-process, in-process and post-process mechanisms for algorithmic fairness discussed in this section. These methods were designed for the task of classification. Fairness mechanisms for other learning tasks are discussed in section 6.\n1Privileged learning is designed to improve performance by using additional information, denoted as the \u00e2\u0102\u0132privileged information,\u00e2\u0102\u0130 which is present only in the training stage and not in the testing stage [135]."}, {"heading": "Paper Mechanism", "text": ""}, {"heading": "4.4 Which Mechanism to Use?", "text": "The different mechanism types present respective advantages and disadvantages. Pre-process mechanisms can be advantageous since they can be used with any classification algorithm. However, they may harm the explainability of the results. Moreover, since they are not tailored for a specific classification algorithm, there is high uncertainty with regard to the level of accuracy obtained at the end of the process.\nSimilar to pre-process mechanisms, post-process mechanisms may be used with any classification algorithm. However, due to the relatively late stage in the learning process in which they are applied, post-process mechanisms typically obtain inferior results [138]. In a post-process mechanism, it may be easier to fully remove bias types such as disparate impact; however, this is not always the desired measure, and it could be considered as discriminatory since it deliberately damages accuracy for some individuals in order to compensate others (this is also related to the controversies in the legal and economical field of affirmative action, see [58]). Specifically, post-process mechanisms may treat differently two individuals who are similar across all features except for the group to which they belong. This approach requires the decision maker at the end of the loop to possess the information of the group to which individuals belong (this information may be unavailable due to legal or privacy reasons). In-process mechanisms are beneficial since they can explicitly impose the required trade-off between accuracy and fairness in the objective function [138]. However, such mechanisms are tightly coupled with the machine algorithm itself.\nHence, we see that the selection of method depends on the availability of the ground truth, the availability of the sensitive attributes at test time, and on the desired definition of fairness, which can also vary from one application to another.\nSeveral preliminary attempts were made in order to understand which methods are best for use. The study in [64] was a first effort in comparing several fairness mechanisms previously proposed in the literature [28, 54, 79, 143]. The analysis focuses on binary classification with binary sensitive attributes. The authors have demonstrated that the performances of the methods vary across datasets, and there was no conclusively dominating method.\nAnother study by [116] has shown as a preliminary benchmark that in several cases, in-process mechanisms perform better than pre-process mechanisms, and for other cases, they do not, leading to the conclusion that there is a need for much more extensive experiments. A recent empirical study [57] has provided a benchmark analysis of several fairness-aware methods and compared the fairness-accuracy trade-offs obtained by these methods. The authors have tested the performances of these methods across different measures of fairness and across different datasets. They have concluded that there was no single method that outperformed the others in all cases and that the results depend on the fairness measure, on the dataset, and on changes in the train-test splits.\nMore research is required for developing robust fairness mechanisms and metrics or, alternatively, for finding the adequate mechanism and metric for each scenario. For instance, the conclusions reached when considering missing data might be very different than those reached when all information is available [74, 99]. [74] explore the limitations of measuring fairness when the membership in a protected group is not available in the data. [99] have tested imputation strategies to deal with the fairness of partially missing examples in the dataset. They have shown that rows containing missing values may be more fair than the rest and therefore suggest imputation rather than deletion of these data. [110] find that when there is an evident selection bias in the\ndata, meaning that there is an extreme under-representation of unprivileged groups, pre-process mechanisms can outperform in-process mechanisms."}, {"heading": "5 FAIRNESS-RELATED DATASETS", "text": "In this section, we review the most commonly used datasets in the literature of algorithmic fairness.\nProPublica risk assessment dataset The ProPublica dataset includes data from the COMPAS risk assessment system (see [1, 6, 88]).\nThis dataset was previously extensively used for fairness analysis in the field of criminal justice risk [15]. The dataset includes 6,167 individuals, and the features in the dataset include number of previous felonies, charge degree, age, race and gender. The target variable indicates whether an inmate recidivated (was arrested again) within two years after release from prison. As for the sensitive variable, this dataset was previously used with two variations \u2013 the first when race was considered as the sensitive attribute and the second when gender was considered as the sensitive attribute [13, 30, 52, 57, 99]."}, {"heading": "Adult income dataset", "text": "The Adult dataset is a publicly available dataset in the UCI repository [43] based on 1994 US census data. The goal of this dataset is to successfully predict whether an individual earns more or less than 50,000$ per year based on features such as occupation, marital status, and education. The sensitive attributes in this dataset includes age [94], gender [144] and race [57, 99, 143].\nThis dataset is used with several different preprocessing procedures. For example, the dataset of [143] includes 45,222 individuals after preprocessing (48,842 before preprocessing)."}, {"heading": "German credit dataset", "text": "The German dataset is a publicly available dataset in the UCI repository [43] that includes information of individuals from a German bank in 1994. The goal of this dataset is to predict whether an individual should receive a good or bad credit risk score based on features such as employment, housing, savings, and age. The sensitive attributes in this dataset include gender [57, 94] and age [75, 144]. This dataset is significantly smaller, with only 1,000 individuals with 20 attributes."}, {"heading": "Ricci promotion dataset", "text": "The Ricci dataset includes the results of an exam administered to 118 individuals to determine which of them would receive a promotion. The dataset originated from a case that was brought to the United States Supreme Court [102, 120]. The goal of this dataset is to successfully predict whether an individual receives a promotion based on features that were tested in the exam, as well as the current position of each individual. The sensitive attribute in this dataset is race."}, {"heading": "Mexican poverty dataset", "text": "The Mexican poverty dataset includes poverty estimation for determining whether to match households with social programs. The data originated from a survey of 70,305 households in 2016 [71]. The target feature is poverty level, and there are 183 features. This dataset was studied, for example, in [106]. The authors studied two sensitive features: young and old families; urban and rural areas."}, {"heading": "Diabetes dataset", "text": "The Diabetes dataset includes hospital data for the task of predicting whether a patient will be readmitted. It is publicly available in the UCI repository [43]. The data contain approximately 100,000 instances and 235 attributes. This dataset was studied, for example, in [48], where it was studied with race as the sensitive feature."}, {"heading": "Heritage health dataset", "text": "The Heritage health dataset originated from a competition conducted by the United States as a competition to improve healthcare through early prediction. It includes data of 147,473 patients with 139 features. The goal of this dataset is to predict whether an individual will spend any days in the hospital during the next year [23]. This dataset was studied, for example, in [144], [94] and [132], where age was the sensitive feature."}, {"heading": "The College Admissions dataset", "text": "The College Admissions dataset was collected by the UCLA law school [123]. It includes data of over 20,000 records of law school students who took the bar exam. The goal of this dataset is to predict whether a student will pass the exam based on factors such as LSAT score, undergraduate GPA and family income. This dataset was used, for example, by [14], where gender was studied as the sensitive feature, and [13], where race was studied as the sensitive feature."}, {"heading": "The Bank Marketing dataset", "text": "The Bank Marketing dataset is a publicly available dataset in the UCI repository [43, 103], and it includes 41,188 individuals with 20 attributes. The task is to predict whether the client has subscribed to a term deposit service based on features such as marital status and age. It was previously investigated by [143], where age was studied as the sensitive attribute."}, {"heading": "The Loans Default dataset", "text": "The Loans Default dataset includes 30,000 instances and 24 attributes of credit card users. It is publicly available in the UCI repository [43, 141]. The goal is to predict whether a customer will default on payments. The features include age, gender, marital status, past payments, credit limit and education.\nThis dataset was used, for example, by [13] and [141], where gender was studied as the sensitive feature."}, {"heading": "The Dutch Census dataset", "text": "The Dutch Census dataset includes 189,725 instances and 13 attributes of individuals. It is publicly available in the IPUMS repository [34]. [76] and [4] use this dataset with only the 60,420 individuals who are not underaged. Their goal is to predict whether an individual holds a highly prestigious occupation by using features such as gender, age, household details, location, citizenship, birth country, education, economic status, and marital status. The sensitive feature utilized is gender."}, {"heading": "The Communities and Crimes dataset", "text": "The Communities and Crimes dataset includes 1,994 instances and 128 attributes of communities in the United States. It is publicly available in the UCI repository [43, 115]. The goal is to predict the number of violent crimes per 100,000 individuals based on features such as percentage of population by age, by marital status, by number of children, by race, and more. [76] add a new\nsensitive attribute that represents whether the percentage of the African-American population in the community is greater than 0.06.\nTable 3 presents a summary of the benchmark datasets for algorithmic fairness that were discussed in this section."}, {"heading": "Dataset", "text": ""}, {"heading": "College", "text": ""}, {"heading": "6 EMERGING RESEARCH ON ALGORITHMIC FAIRNESS", "text": "In this section, we review selected emerging sub-fields of algorithmic fairness."}, {"heading": "6.1 Fair Sequential Learning", "text": "Most existing research on algorithmic fairness considers batch classification, where the complete data are available in advance. However, many applications require investigation of online and\nreinforcement learning, where the data are collected over time. In online learning, in contrast to batch learning, the system includes feedback loops so that the decision at each step may influence the state and future decisions. This imposes challenges in both defining and making fair algorithmic decisions, as fairness should now be considered at each step, and short-term actions may affect long-term results. In these cases, there is a need to balance exploitation of existing knowledge (such as hiring an already known population) and exploration of sub-optimal solutions to gather more data (such as hiring populations of different backgrounds that differ from current employees).\nSeveral studies have investigated fairness in sequential learning [66, 72, 73, 80, 133]. For example, [72] study fairness in reinforcement learning and model the environment as a Markov decision process. In their model, fairness is defined such that one action will never be preferred over another if its long-term discounted reward is lower. [66] define fairness as time-dependent individual fairness and require that algorithmic decisions be consistent over time. They propose a post-process mechanism that imposes these time-dependent constraints such that two individuals that arrive during the same period and are similar in their feature dimension must be assigned similar labels. Open challenges in this domain include the limitation of any specific time-dependent fairness definitions to the selected period of time and the effect of different discount factors. Moreover, it is worth noting that an exploration process may be unethical on its own and impose a new type of unfairness. In a similar line of work, researchers have investigated scenarios where feedback loops have the potential to cause amplification of bias. In these scenarios, the decisions based on the machine learning models then affect the future collected data. The risk of feedback loops is that they can introduce self-fulfilling predictions, where acting on a prediction can change the outcomes. For example, sending more police officers to an area that was predicted to be at high risk for crime will inevitably cause the arrest of more individuals in this area, and then, the prediction model will eventually further increase the risk prediction for the area [53]. Note that fair sequential learning is somewhat different from another researched domain in algorithmic fairness that concerns a selection process that consists of multiple stages, such as screening candidates first by their resumes, then by their test scores and finally by interviews, where more information is gained about individuals during each stage. This field is sometimes referred to as fair pipelines [21, 45, 52, 69, 97]. In these studies, fairness is revised to be considered in each stage, not only in the final stage."}, {"heading": "6.2 Fair Adversarial Learning", "text": "Today, adversarial learning is highly popular in the use of generative adversarial networks (GANs) [61]. GANs are commonly used for the generation of simulated representative samples based on a training set. In this field of research, input data can be of various domains such as images or tabular data. In computer vision (CV), for instance, adversarial learning is used for tasks such as image generation (such as in [9]) or modification (such as in [7]), along with other CV tasks. Nowadays, fair adversarial learning is attracting increasing attention with respect to both fair classification and the generation of fair representations. In one distressing incident, a face modifying application was exposed as racist when the app\u2019s \"image filter\" that was intended to change face images to more \"attractive\" made skin lighter [111]. GANs are generally constructed as a feedback loop model, starting from a generator G that generates \"fake\" simulated samples and a discriminator D (the \"adversary\") that determines whether the generated samples are real or fake and returns the decisions as feedback to the generator G to improve its model. Improving G means enhancing its ability to generate samples that are increasingly similar to real samples in a manner that \"fools\" the discriminator D, thus minimizing\nits ability to differentiate between real and fake samples. Typically, both G and D are multi-layer neural networks. To use GANs for fair learning, previous studies have developed different approaches. These models are often constructed as minimax optimization problems that aim at maximizing the predictor\u2019s capability to accurately predict the outcomes while minimizing the adversary\u2019s capability to predict the sensitive feature. In one approach, it was suggested to use the feedback structure to check whether a trained classifier is fair or not and then update the model accordingly [32, 137, 145]. A different approach encourages the use of GANs to additionally learn fair representations or embedding from the training data so that it is more difficult for a subsequent classifier to distinguish which samples belong to a privileged group and which belong to an unprivileged group [17, 48, 96].\nAnother approach [3, 139] is using GANs for generating fair synthetic data from the initial input data and then using them to train any classifier. [139], for example, use a GAN framework with one generator and two discriminators. One discriminator is trained to distinguish whether a generated sample is real or fake (as in conventional GANs), and the second identifies whether the sample belongs to the privileged or unprivileged group. Some of these methods make efforts to also preserve semantic information of the data while learning fair representations [114, 124]. This is essential since the interpretability and transparency of how fairness is approached in algorithmic decision-making are crucial to enhancing the understanding of decisions and trust in algorithms, and it is a paramount challenge that future research should face. Some other papers have studied fair adversarial learning. [17] investigate the effect of input sample selection on the resulting fairness in adversarial fair learning models and show that a small balanced dataset can be effective for achieving fair representations. [20] extend fair adversarial learning to improve fairness in graph embedding. [16] have raised some concerns about fair adversarial learning that should be further investigated. They argue that these models may sometimes be unstable and therefore may present some risks when using them in a production environment. Note that other closely related problems are aimed at finding equilibrium (minimax) points in the field of game theory, and therefore, game-theoretic schemes may also be used for solutions [4, 5, 32, 55]. From the literature on adversarial learning, we can also note that learning fairly to predict the outcome of an unprivileged group can be thought of as learning to predict in a different domain [48, 96], and therefore, notions from the field of domain adaptation can be adopted to enhance the study of the field of algorithmic fairness, and vice versa."}, {"heading": "6.3 Fair Word Embedding", "text": "Word embedding models construct representations of words and map them to vectors (also commonly referred to asword2vecmodels). Training of word embeddings is performed using raw textual data with an extremely large number of text documents and is based on the assumption that words that occur in the same contexts tend to have similar meanings. These models are primarily designed such that the embedding vectors will indicate something about the meanings and relationships between words (i.e., words with similar meanings have vectors that are close in the vector space). As such, they are broadly used in many natural language processing (NLP) applications, such as in search engines, machine translation, resume filtering, job recommendation systems, online reviews and more. However, previous studies have shown that there are inherent biases in word embeddings (for example, [18, 24, 29, 150]). These studies showed that word embedding models have exhibited social biases and gender stereotypes. For instance, it has been shown that the embedding of the\nword \"computer programmer\" is closer to \"male\" than it is to \"female\". The implications of this are disturbing since these biases may affect people\u2019s lives and cause discrimination in social applications such as in job recruitment or school admissions. Another example is Microsoft\u2019s AI chat bot, named Tay, which learned abusive language from Twitter data after only the first day of release (the bot was eventually shelved, see [70]).\nA common definition of gender bias in word embedding is the measure of cosine similarity from a selected word to the words \"he\" and \"she\" (or any other two pronouns that indicate gender, such as \"him\"/\"her\" or \"Mr.\"/\"Mrs.\"). The difference between these two similarities may indicate the extent of bias in the embedding model. For example, consider the sentence \"The CEO raised the salary of the receptionist because he is generous.\" In this sentence, \"he\" refers to \"CEO,\" and further similar references in many additional sentences and texts may create a large contextual connection between these two words or other occupational nouns [150]. Hence, even if the algorithms themselves are not biased, historical biases and norms may be embedded into the algorithm results. To mitigate these types of biases, several studies have developed methods for debiasing the results. For example, [18] suggest a post-process mechanism for removing gender bias, referred to as hard-debiasing. Their method first identifies a gender dimension, which is determined by a set of words that indicate gender definitions (e.g., \"he\"/\"she\"). Second, it inherently defines neutral words (such as occupations) and then negates the projection of all of these neutral words with respect to the gender direction (so that the bias of neutral words is now zero by definition) by re-embedding the wordw :\n\u00aew := ( \u00aew \u2212 \u00aewB )/ \u00aew \u2212 \u00aewB (7)\nwhere \u00aew is the embedding of the selected word and \u00aewB is the projection ofw with respect to the gender direction.\nThe same paper also suggests an additional soft-debiasing mechanism that reduces bias while still maintaining some similarity to the original embedding, thus providing a parameter that controls the trade-off between debiasing and the need to preserve information.\n[150] suggest an in-process mechanism, referred to as Gender-Neutral Global Vectors (GN-GloVe), to reduce bias. Their method trains word embedding using GloVe [109] with an altered loss function that constructs an embedding such that the protected attribute (e.g., gender) is represented in a certain dimension (a sub-vector of the embedding), which can then be ignored for debiasing. This is done by encouraging pairs of words with gender indication (e.g., \"mother\" and \"father\") to have larger distance in their gender dimension and gender-neutral words to be orthogonal to the gender direction.\n[24] propose a pre-process mechanism for reducing bias by perturbing or removing documents during the training stage that are traced as the origin for the word embedding bias. One challenge of these fairness mechanisms is the need for lists of words that indicate the sensitive attribute dimension and words that should be considered as neutral. [18], for example, tackle this challenge by using an initial set of gender-definitional words and train a support vector machine (SVM) to expand the list.\nLet us note that the challenges of fair word embedding also affect many other downstream applications that use these word representations, for example, in coreference resolution [117, 147, 149], in sentence encoding [100], inmachine translation citepvanmassenhove2019getting,font2019equalizing, in language models [19], and in semantic role labeling [148].\nCuriously, a recent study has argued that a major concern is that some of the proposed methods for removing biases in word embeddings are actually not able to remove biases but rather just \"hide\" them [60]. They show, by a clustering illustration, that gender biases are still reflected\nin the embedding even after applying these methods. They additionally show that by using a support vector machine (SVM) classifier, most of the gender information can be recovered from the embedding. Hence, it seems that existing methods as well as definitions for fair embeddings might be insufficient, and these challenges require more extensive research."}, {"heading": "6.4 Fair Visual Description", "text": "The study of fairness in computer vision has recently gained extensive interest since computer vision models have been shown to produce disturbingly biased results with respect to several tasks. For example, [25] have found that facial analysis models were negatively affected towards discriminating results by the under-representation of female dark-skinned faces in datasets. [82] show that image searches of occupations in Google\u2019s engine resulted in gender-biased results. Google\u2019s labeling application has recklessly identified black Americans as \"gorillas\" [107, 126]. Furthermore, an app that classified the attractiveness of individuals from photos turned out to be discriminative against dark skin [98]. There are several previous papers in the domain of fair image classification [25, 46, 114, 124]. However, the task becomes much more complex when a fair description of images is required, such as in multi-label classification tasks [82, 129, 134, 148], in face attribute detection [81, 121], or in the task of image captioning [67]. The last task is even more complicated than the others because of the unstructured character of the problem. Mitigating bias in these types of problems is challenging for several reasons: First, the multimodal nature of the task requires handling fairness at both levels of natural language processing (NLP) models and computer vision (CV) models. As mentioned in a previous section, the challenges of fair word embedding also affect many other downstream applications that use these word representations (see, for example image captioning in [67]). Second, the labels of the data usually depend on annotators and are not always accurate [129, 134]. For example, [129] show that human annotators fail to capture parts of the visual concepts in images and [134] show that the crowdsourced descriptions of the images in the Flickr30K dataset are often based on stereotypes and prejudices of the annotators; another challenge is that the datasets are inherently unbalanced since in order to have balance - all possible co-occurrences must be balanced [67, 134] [148] consider multi-label role classification, show that some tasks display severe gender bias, and suggest a method to re-balance the resulting predictions by solving a constrained optimization problem using Lagrange relaxation. Additional constraints require the model predictions to have a similar distribution as the training set in terms of co-occurrences of gender indication and the predicted target value (this is applied to the entire corpus level since the entire corpus is needed in order to assess the frequency of occurrences). [67] consider the problem of fair image captioning. They propose a method to reduce gender bias by directly using a person\u2019s appearance information in the image. The method is designed to be more cautious when there is no gender information in the image. Moreover, it is constructed to function even when the distributions of genders differ between training and test sets."}, {"heading": "6.5 Fair Recommender Systems", "text": "Recommender systems are prevalent in many automated and online systems and are designed to analyze users\u2019 data to provide them with personalized propositions that correspond to each user\u2019s tastes and interests. An inherent concept in recommendations is that the best items for one user may be different than those for another. Some examples of recommended items are movies, news articles, products, jobs, loans, etc. These systems have the potential to facilitate activities for both providers and consumers; however, they have also been found to exhibit fairness issues\n[26, 27, 49, 50]. For instance, it was shown that Google\u2019s ad-targeting algorithm had proposed higher-paying executive jobs more commonly for men than for women [41, 125].\nMost recommender systems employ user and item similarities and are therefore prone to result in homogeneous selections that may not provide sufficient opportunities for minority populations.\nA recent paper, [26], notes that extending the notion of fairness from general classification tasks to recommender systems should take personalization into account. Several studies are investigating the user\u2019s perspective [33, 47, 49, 140], where fairness is considered as recommending items equitably to different user groups (referred to as C-fairness), such as recommending high-paying jobs to both men and women. Other studies refer to the provider\u2019s perspective [50, 89], where items from different providers should be recommended equally (referred to as P-fairness), for instance, when trying to avoid market monopolistic domination. [26] notes that many recommender system applications involve multiple stakeholders and may therefore give rise to fairness issues for more than one group of participants simultaneously, as well as achieving fairness at a regulatory level or the level of the entire system (referred to as multisided fairness).\nIt is interesting to note that P-fairness is somewhat related to the categorical diversity in recommender systems, requiring that recommendation lists are diverse [86, 92]. For example, consider a hiring recommender system \u2013 we may observe all male candidates as items of one provider and all female candidates as items of another provider. We may then ensure the equal recommendation of men and women to each of the positions. In diversity-enhancing methods, some common measures of equality may be considered, such as the Gini coefficient that is also used in economic contexts [86]. Moreover, note that an individual definition of P-fairness, rather than group-fairness, may be somewhat similar to the definition of coverage in recommender systems, requiring that each item be recommended fairly [26]. [130] propose enhancing multi-stakeholder fairness using a constraint-based integer programming optimizationmodel. The problem is computationally difficult, and hence, a following relaxation heuristic is proposed in order to solve it. [47] suggest a post-processing mechanism that alters a fraction of the entries in the recommendation matrix so that it would be more difficult to predict the sensitive attributes from the matrix while preserving the high utility of the matrix. The \u03b5-fairness of a recommendation matrix with respect to a certain sensitive attribute is defined as the error level when predicting the sensitive attribute using the recommendation matrix. The price of achieving \u03b5-fairness is measured by the distance between the two matrices \u2013 the original one obtained from the prediction algorithm and the altered one obtained after the post-processing mechanism is performed.\n[140] and [78] suggest an in-process mechanism by including additional regularization factors in the objective functions. [78] introduce the notion of recommendation independence to fairness-aware recommender systems. This notion requires statistical independence between recommendation results and the sensitive attribute. This means that the sensitive information will not affect the results. [140] define several notions of fairness in recommender systems. Value unfairness measures the inconsistency in the signed estimation error across user groups. Absolute unfairness is similar to the previous measure, but with absolute estimation. Underestimation unfairness represents the extent to which predictions underestimate the true ratings. Overestimation unfairness is similar to the previous measure, but with overestimation. [31] and [22] use similar notions for fairness in multiwinner voting.\nOne challenge in fairness-aware recommender systems is the possibility of discriminated groups based onmore than one sensitive attribute [47]. It is required to first devise a definition for fairness in such cases in order to address the problem. In multi-stakeholder fairness optimization, the problems are computationally difficult [130], so there is a need for more computational enhancements in order to work with large datasets. Future work may also focus on fairness in systems where there\nare network structures that define relationships between providers and between users (such as in [20]). Another possible research direction may be the incorporation of sequential notions of fairness into recommender systems through the introduction of additional time-dependent constraints. Note that the domain of recommender systems is also closely related to other common multistakeholder environments [2], like resource allocation problems such as police distribution to districts [51] or the allocation of aid in disaster response, where fairness is also a major concern."}, {"heading": "6.6 Fair Causal Learning", "text": "Observational data collected from real world systems can mostly provide associations and correlations, rather than causal structure understandings. In contrast, causal learning relies on additional knowledge structured as a model of causes and effects.\nCausal approaches may assist in enhancing fairness in several manners. For instance, by understanding causes and effects in the data, the model may assist in tackling the challenges of fairness definitions by analyzing which types of discrimination should be allowed and which should not [87, 118].\nAnother approach to improve fairness using causal reasoning is to provide an understanding of how to perform imputation of missing values or how to repair a dataset that contains sample or selection bias [10, 127].\nMoreover, understanding a causal model may help with other ethical issues, such as defining liability and responsibility by understanding the sources of biases. This may increase the transparency and explainability of the fair models, which is also crucial for trust. [87] have suggested a measure of causal fairness called counterfactual fairness. It measures the extent to which it is possible to build two identical predictions Y\u0302 \u2013 one trained on the privileged group and the second trained on the unprivileged group \u2013 using any combination of variables in the system that are not caused by the sensitive attribute. The intuition is that, in a fair model, the prediction would not change if only the sensitive attribute (and its affected variables) is changed. More precisely, a causal graph satisfies counterfactual fairness when the predicted label is not dependent on any descendant of the sensitive attribute. Several studies have proposed alternative notions to counterfactual fairness, which relax the rigid restriction on any descendant to less strict limitations. For example, the graph does not suffer from proxy discrimination [84] if the predicted label is not dependent on any proxy of the sensitive attribute (a proxy feature is a feature that can be exploited to derive the sensitive feature). Moreover, the graph does not suffer from unresolved discrimination if the predicted label is not dependent on any resolving variable (a resolving variable is influenced by the sensitive feature but is accepted by practitioners as non-discriminatory). [93] suggest categorizing causal fairness methods according to three dimensions: individual vs. group level [84] causal effects; explicit vs. implicit [105] structural equations; and creating fair predictors vs. explaining and quantifying discrimination [146]. For example, [93] classify counterfactual fairness as individual, explicit and used for prediction tasks. For a broader review on causal fairness, see [93]. It is important to note that causal fairness models can indeed help us overcome many of the challenges encountered with respect to fair prediction tasks; however, in practice, it is difficult to obtain the correct causal model. Moreover, removing all correlated features found through a causal model may significantly compromise accuracy."}, {"heading": "7 DISCUSSION AND CONCLUSION", "text": "In this paper, we presented a comprehensive and up-to-date overview of the algorithmic fairness research field. We started by describing the main causes of unfairness, followed by common definitions and measures of fairness, and the inevitable trade-offs between them. We then presented fairness-enhancing mechanisms, focusing on their pros and cons, aiming at better understanding which mechanisms should be used in different scenarios. Commonly used fairness-related datasets were then reviewed. Lastly, we listed several emerging research sub-fields of algorithmic fairness including fair sequential learning, fair adversarial learning, fair word embedding, fair visual description, fair recommender systems and fair causal learning. In addition to the already studied problems and the emerging ones, we identify several open challenges that should be further investigated in future research. One major challenge stems from biases inherent in the dataset. Such biases may arise for example, when the labeling process was performed in an already unfair manner, or if there are under-represented populations in the dataset, or in the case of systematic lack of data and in particular labels. Representative datasets are very difficult to achieve, and therefore, it is crucial to devise methods to overcome these issues. Another challenge is the proliferation of definitions and measures, fairness-related datasets, and fairness-enhancing mechanisms. It is not clear how newly proposed mechanisms should be evaluated, and in particular which measures should be considered? which datasets should be used? and which mechanisms should be used for comparison? A closely related challenge is the difficulty in determining the balance between fairness and accuracy. That is, what are the costs that should be assigned to each of these measures for evaluation purposes. Future efforts should be invested in generating a benchmarking framework that will allow a more unified and standard evaluation process for fairness mechanisms.\nThe interpretability and transparency of how fairness is addressed by AI algorithms are another important challenge. Such transparency is crucial to increase the understanding and trust of users in these algorithms, and in many domains, is even required by law. This need is further supported by several recent studies that have addressed the question of how devised mathematical notions of fairness are perceived by users [63, 128]. It turns out that users tend to prefer the simpler notion of demographic parity, probably due to the difficulty of grasping more complex definitions of fairness.\nTo conclude, since the use of algorithms is expanding to all aspects of our lives, demanding that automated decisions be more ethical and fair is inevitable. We should aspire to not only develop fairer algorithms, but also to design procedures to reduce biases in the data. Such procedures may rely for example on integrating both humans and algorithms in the decision pipeline. However, thus far, it seems that biased algorithms are easier to fix than biased humans or procedures [104]."}, {"heading": "ACKNOWLEDGMENTS", "text": "This paper was partially supported by the Koret foundation grant for Smart Cities and Digital Living 2030."}, {"heading": "A ADDITIONAL MEASURES FOR ALGORITHMIC FAIRNESS", "text": "Section 3 discussed the most prominent definitions and measures of algorithmic fairness. This appendix reviews additional measures used in the literature. (1) Overall accuracy equality \u2013 This requires similar accuracy across groups [15]. Thismeasure\nis mathematically formulated as follows: P[Y = Y\u0302 |S = 1] \u2212 P[Y = Y\u0302 |S , 1] \u2264 \u03b5 (8) where S represents the sensitive attribute (e.g., race and gender), S = 1 is the privileged group and S , 1 is the unprivileged group. Y\u0302 = Y means that the prediction was correct. A lower value indicates better fairness. It should be noted that this measure does not guarantee equalized odds or fair decisions (see [88]).\n(2) Predictive parity \u2013 This requires that the positive predictive values (PPVs) are similar across groups (meaning the probability of an individual with a positive prediction actually experiencing a positive outcome) [36]. This measure is mathematically formulated as follows: P[Y = 1|S = 1, Y\u0302 = 1] \u2212 P[Y = 1|S , 1, Y\u0302 = 1] \u2264 \u03b5 (9) Note that a lower value indicates better fairness. This measure uses the ground truth of the outcome, assuming that the outcome was achieved fairly. However, it has been shown to be incompatible with equalized odds and equal opportunity when prevalence differs across groups [36, 38, 39].\n(3) Equal calibration \u2013 This requires that, for any predicted probability value, both groups will have similar positive predictive values (PPV represents the probability of an individual with a positive prediction actually experiencing a positive outcome) [36, 85]. Note that this measure is similar to predictive parity when the score value is binary (but does not guarantee predictive parity when the score is not binary) [36]. This measure is mathematically formulated as follows:\n|P[Y = 1|S = 1,V = v] \u2212 P[Y = 1|S , 1,V = v]| \u2264 \u03b5 (10) where V is the predicted probability value. Note that in some studies the definition of calibration requires that the PPV also be equal to V [38, 85]. A lower value indicates better fairness. Although in some cases equal calibration may be the desired measure, it has been shown that it is incompatible with equalized odds [112] and is insufficient to ensure accuracy or equitable decisions [38]. Moreover, it conflicts with balance for the positive class and balance for the negative class [36, 39].\n(4) Conditional statistical parity \u2013 Controlling for a limited set of \"legitimate\" features, an equal proportion of individuals is selected from each group [39]. This measure is mathematically formulated as follows: P[Y\u0302 = 1|S = 1,L = l] \u2212 P[Y\u0302 = 1|S , 1,L = l] \u2264 \u03b5 . (11) L is a set of legitimate factors. A lower value indicates better fairness. Note that using this measure requires defining which features are legitimate, which is not a trivial task. It is not practical to find features that are entirely independent of the sensitive attributes.\n(5) Predictive equality \u2013 This requires false positive rates (FPRs; meaning the probability of an individual with a negative outcome to have a positive prediction) to be similar across groups [39]. This measure is mathematically formulated as follows: P[Y\u0302 = 1|S = 1,Y = 0] \u2212 P[Y\u0302 = 1|S , 1,Y = 0] \u2264 \u03b5 (12) This measure requires the ground truth of the outcome, assuming that the outcome was achieved fairly. A lower value indicates better fairness. However, it considers only one type of error (as opposed to equalized odds, for example, which require the equality of both FPRs and FNRs). As mentioned, following equality in terms of only one type of error will increase the disparity in terms of the other error [112].\n(6) Conditional use accuracy equality \u2013 This requires positive predictive values (PPVs) and negative predictive values (NPVs) to be similar across groups [15]. NPV represents the probability of an individual with a negative prediction actually experiencing a negative outcome. PPV represents the probability of an individual with a positive prediction actually experiencing a positive outcome. This measure is mathematically formulated as follows: P[Y = 1|S = 1, Y\u0302 = 1] \u2212 P[Y = 1|S , 1, Y\u0302 = 1] \u2264 \u03b5\n\u2227 P[Y = 0|S = 1, Y\u0302 = 0] \u2212 P[Y = 0|S , 1, Y\u0302 = 0] \u2264 \u03b5 (13) This measure requires the ground truth of the outcome, assuming that the outcome was achieved fairly. A lower value indicates better fairness. This measure considers more than one type of error. According to [15], following this measure does not guarantee equalized odds.\n(7) Treatment equality \u2013 This requires an equal ratio of false negatives (FNs) and false positives (FPs) [15]. The false negative cases are all of the cases that were predicted to be in the negative class when the actual outcome belongs to the positive class. The false positive cases are all of the cases that were predicted to be in the positive class when the actual outcome belongs to the negative class. This measure is mathematically formulated as follows:\n|FNS=1/FPS=1 \u2212 FNS,1/FPS,1 | \u2264 \u03b5 (14)\nThis measure requires the ground truth of the outcome, assuming that the outcome was achieved fairly. A lower value indicates better fairness. Note that according to [15], following this measure may harm the conditional use accuracy equality.\n(8) Balance for the positive class \u2013 This requires an equal mean of predicted probabilities for individuals that experience a positive outcome [85]. This measure is mathematically formulated as follows:\n|E[V |Y = 1, S = 1] \u2212 E[V |Y = 1, S , 1]| \u2264 \u03b5 (15)\nwhere V is the predicted probability value. A lower value indicates better fairness. This measure was proven to be incompatible with equal calibration [85].\n(9) Balance for the negative class \u2013 This requires an equal mean of predicted probabilities for individuals that experience a negative outcome [85]. This measure is mathematically formulated as follows:\n|E[V |Y = 0, S = 1] \u2212 E[V |Y = 0, S , 1]| \u2264 \u03b5 (16) where V is the predicted probability value. A lower value indicates better fairness. This measure was proven to be incompatible with equal calibration [85].\n(10) Fairness through unawareness \u2013 This requires that no sensitive attributes are explicitly used in the algorithm. The predicted outcomes are the same for candidates with the same attributes [87]. This measure is mathematically formulated as follows:\nXi = X j \u2192 Y\u0302i = Y\u0302j (17) where i and j denote two individuals, and X are the attributes describing an individual except for the sensitive attributes. This measure requires predictions to be the same for candidates with the same attributes. However, note that even when not considering the sensitive attributes, the model could still be biased through \"proxies\" or other causes such as sample or selection bias. Moreover, explicitly considering sensitive attributes is sometimes required, and excluding information can lead to discriminatory decisions [38].\n(11) Mutual information \u2013 This measures the mutual dependence between the sensitive feature and the predicted outcome [78]. This measure is mathematically formulated as follows:\u2211(\nP(y\u0302, s)lo\u0434( P(y\u0302, s) P(y\u0302)P(s) )\n) \u2264 \u03b5 (18)\nNote that this measure does not consider the actual outcomes. The lower the measure is, the lower the dependence between the sensitive attribute and the predictions; thus, lower values represent better fairness. One advantage of this measure is that it can consider binary, categorical or numerical predictions.\n(12) Mean difference \u2013 This measures the difference between the means of the predictions across groups [152]. For example, [94] use a variation of this measure that computes the difference between the means of the predicted probabilities across groups. E[Y\u0302 |S = 1] \u2212 E[Y\u0302 |S , 1] \u2264 \u03b5 (19) Note that this measure does not consider the actual outcomes and that lower values indicate better fairness. One advantage of this measure is that it can consider binary, categorical or numerical predictions.\nTable 4 presents a summary of the measures described in this appendix."}, {"heading": "Treatment", "text": ""}, {"heading": "Conditional Use Accuracy", "text": ""}, {"heading": "Predictive", "text": ""}, {"heading": "Conditional Statistical", "text": ""}, {"heading": "Predictive", "text": ""}, {"heading": "Balance for the Positive", "text": "Class\n[85] Requires equal mean\npredicted probabilities\nfor individuals that\nexperience a positive\noutcome\nGroup \u2713 \u2713 Binary Binary"}, {"heading": "Balance for the Negative", "text": "Class\n[85] Requires equal mean\npredicted probabilities\nfor individuals that\nexperience a negative\noutcome\nGroup \u2713 \u2713 Binary Binary"}, {"heading": "Fairness through", "text": "Unawareness\n[87] Requires that no sensi-\ntive attributes are explic-\nitly used in the algo-\nrithm\nIndividual \u2717 \u2717 - - \u2022 Fairness through blind-\nness [39]\n\u2022 Anti-classification [38]\nMutual Infor-\nmation\n[78] Measures mutual depen-\ndence between the sensi-\ntive feature and the pre-\ndicted outcome\nGroup \u2717 \u2713 - Binary,\nNumerical,\nCategorical\nRelated to prejudice index\n[78]\nMean Differ-\nence\n[152] Measures the difference\nbetween the means\nof the targets across\ngroups\nGroup \u2717 \u2713 - Binary,\nNumerical\n\u2022 Discrimination probabil-\nity [94];\n\u2022 Similar to demographic\nparity when the target is\nbinary"}], "title": "Algorithmic Fairness", "year": 2020}