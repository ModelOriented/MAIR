{"abstractText": "The increasing adoption of machine learning tools has led to calls for accountability via model interpretability. But what does it mean for a machine learning model to be interpretable by humans, and how can this be assessed? We focus on two definitions of interpretability that have been introduced in the machine learning literature: simulatability (a user\u2019s ability to run a model on a given input) and \u201cwhat if\u201d local explainability (a user\u2019s ability to correctly determine a model\u2019s prediction under local changes to the input, given knowledge of the model\u2019s original prediction). Through a user study with 1000 participants, we test whether humans perform well on tasks that mimic the definitions of simulatability and \u201cwhat if\u201d local explainability on models that are typically considered locally interpretable. To track the relative interpretability of models, we employ a simple metric, the runtime operation count on the simulatability task. We find evidence that as the number of operations increases, participant accuracy on the local interpretability tasks decreases. In addition, this evidence is consistent with the common intuition that decision trees and logistic regression models are interpretable and are more interpretable than neural networks.", "authors": [{"affiliations": [], "name": "Dylan Slack"}, {"affiliations": [], "name": "Sorelle A. Friedler"}, {"affiliations": [], "name": "Carlos Scheidegger"}, {"affiliations": [], "name": "Chitradeep Dutta Roy"}], "id": "SP:a6e33eb5dd1d24deecc21403a0fb18f46cdef11c", "references": [{"authors": ["P. Adler", "C. Falk", "Friedler"], "title": "S", "venue": "A.; Nix, T.; Rybeck, G.; Scheidegger, C.; Smith, B.; and Venkatasubramanian, S.", "year": 2018}, {"authors": ["H. Allahyari"], "title": "and Lavesson", "venue": "N.", "year": 2011}, {"authors": ["Buitinck"], "title": "API design for machine learning software: experiences from the scikit", "year": 2013}, {"authors": ["Sen Datta", "A. Zick 2016] Datta", "S. Sen", "Y. Zick"], "title": "Algorithmic transparency via quantitative input influence: Theory and experiments with learning systems", "venue": "In Security and Privacy (SP),", "year": 2016}, {"authors": ["F. Doshi-Velez"], "title": "and Kim", "venue": "B.", "year": 2017}, {"authors": ["B. Goodman"], "title": "and Flaxman", "venue": "S.", "year": 2016}, {"authors": ["Guidotti"], "title": "A survey of methods for explaining black box models", "year": 2018}, {"authors": ["Henelius"], "title": "A peek into the black box: exploring classifiers by randomization. Data mining and knowledge discovery 28(5-6):1503\u20131529", "year": 2014}, {"authors": ["Lage"], "title": "2018a. An evaluation of the human-interpretability of explanation", "venue": "In Conference on Neural Information Processing Systems (NeurIPS) Workshop on Correcting and Critiquing Trends", "year": 2018}, {"authors": ["Lage"], "title": "2018b. Human-in-the-loop interpretability prior", "venue": "In Conference on Neural Information Processing Systems (NeurIPS)", "year": 2018}, {"authors": ["Lakkaraju, H.", "Bach"], "title": "S", "venue": "H.; and Leskovec, J.", "year": 2016}, {"authors": ["Lipton"], "title": "Z", "venue": "C.", "year": 2018}, {"authors": ["McCabe"], "title": "T", "venue": "J.", "year": 1976}, {"authors": ["Olah"], "title": "The building blocks of interpretability. Distill. https://distill.pub/2018/building-blocks", "year": 2018}, {"authors": ["F. Poursabzi-Sangdeh", "D.G. Goldstein", "J.M. Hofman", "Vaughan"], "title": "J", "venue": "W.; and Wallach, H.", "year": 2017}, {"authors": ["Prolific"], "title": "2014", "venue": "https://prolific.ac/, last accessed on June 5th,", "year": 2019}, {"authors": ["Ribeiro"], "title": "M", "venue": "T.; Singh, S.; and Guestrin, C.", "year": 2016}, {"authors": ["A.D. Selbst"], "title": "and Barocas", "venue": "S.", "year": 2018}, {"authors": ["A.D. Selbst"], "title": "and Powles", "venue": "J.", "year": 2017}, {"authors": ["B. Ustun"], "title": "and Rudin", "venue": "C.", "year": 2016}, {"authors": ["Mittelstadt Wachter", "S. Floridi 2017] Wachter", "B. Mittelstadt", "L. Floridi"], "title": "Why a right to explanation of automated decision-making does not exist in the general data protection regulation. International Data Privacy Law 7(2):76\u201399", "year": 2017}], "sections": [{"heading": "Introduction", "text": "Recently, there has been growing interest in interpreting machine learning models. The goal of interpretable machine learning is to allow oversight and understanding of machinelearned decisions. Much of the work in interpretable machine learning has come in the form of devising methods to better explain the predictions of machine learning models. However, such work usually leaves a noticeable gap in understanding interpretability (Lipton 2018; Doshi-Velez and Kim 2017). The field currently stands on shaky foundations: papers mean different things when they use the word \u201cinterpretability\u201d, and interpretability claims are typically not validated by measuring human performance on a controlled task. However, there is growing recognition in the merit of such human validated assessments (Lage et al. 2018b; Lage et al. 2018a; Lakkaraju, Bach, and Leskovec 2016). In line with this goal, we seek concrete, falsifiable notions of interpretability.\nCopyright c\u00a9 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.\n\u201cInterpretability\u201d can be broadly divided into global interpretability, meaning understanding the entirety of a trained model including all decision paths, and local interpretability, the goal of understanding the results of a trained model on a specific input and small deviations from that input. We focus on local interpretability, and on two specific definitions. We assess simulatability (Lipton 2018), the ability of a person to\u2014independently of a computer\u2014run a model and get the correct output for a given input, and \u201cwhat if\u201d local explainability (Ribeiro, Singh, and Guestrin 2016; Lipton 2018): the ability of a person to correctly determine how small changes to a given input affect the model output. We will refer to a model as locally interpretable if users are able to correctly perform both of these tasks when given a model and input. The experiments we present here are necessarily artificial and limited in scope. We see these as lower bounds on the local interpretability of a model; if people cannot perform these interpretability tasks, these models should not be deemed locally interpretable.\nIn addition to considering the successful completion of these tasks a lower bound on the local interpretability of a model, we might reasonably ask whether these are valuable interpretability tasks at all. Though purposefully limited in scope, we argue that these tasks are still valuable in real-world settings. Consider a defense attorney faced with a client\u2019s resulting score generated by a machine learned risk assessment. In order to properly defend their client, the attorney may want to verify that the risk score was correctly calculated (simulatability) and argue about the extent to which small changes in features about their client could change the calculated score (local explainability). Despite being simple interpretability tasks, successfully completing them is important to the attorney\u2019s ability to defend their client from potential errors or issues with the risk assessment.\nWe assessed the simulatability and \u201cwhat if\u201d local explainability of decision trees, logistic regressions, and neural networks through a crowdsourced user study using Prolific (Prolific 2014). We asked 1,000 participants to simulate the model on a given input and anticipate the outcome on a slightly modified version of the input. We measured user accuracy and completion time over varied datasets, inputs, and model types (described in detail in the User Study De-\nar X\niv :1\n90 2.\n03 50\n1v 2\n[ cs\n.L G\n] 2\nA ug\n2 01\nsign section). The results are consistent with the folk hypotheses (Lipton 2018) that decision trees and logistic regression models are locally interpretable and are more locally interpretable than neural networks given the particular model representations, datasets, and user inputs used in the study.\nAs has been previously observed (Lipton 2018), it may be the case that a small neural network is more interpretable than a very large decision tree. To begin to answer questions surrounding cross-model comparisons and generalizations of these results to models not studied here, we investigated a measure for its suitability as a proxy for the users\u2019 ability to correctly perform both the simulation and \u201cwhat if\u201d local explainability tasks. We hypothesized that the number of program operations performed by an execution trace of the model on a given input would be a good proxy for the time and accuracy of users\u2019 attempts to locally interpret the model under both definitions; specifically, that as the total number of operations increased, the time taken would increase and the accuracy on the combined task would decrease.\nAnalyzing the results of this study, we find evidence that as the number of total operations performed by the model increases, the time taken by the user increases and their accuracy on the combined local interpretability task decreases. We anticipated that as the number of operations increases, the model would become uninterpretable because all users are eventually expected to make a mistake simulating a very large model. The operation count at which the users cannot locally interpret a model can be considered an upper bound limit to the interpretability of the model. Users reached this upper bound when simulating the largest neural network sizes we considered. We see this work as a first step in a more nuanced understanding of the users\u2019 experience of interpretable machine learning."}, {"heading": "Related Work", "text": "Work on the human interpretability of machine learning models began as early as Breiman\u2019s study of random forests (Breiman 2001). Since then, many approaches to the interpretability of machine learning models have been considered, including the development of new globally interpretable models (Ustun and Rudin 2016), post-hoc local explanations (Ribeiro, Singh, and Guestrin 2016) and visualizations (Olah et al. 2018), and post-hoc measurement of the global importance of different features (Henelius et al. 2014; Datta, Sen, and Zick 2016; Adler et al. 2018). We refer the interested read to Molnar and Guidotti et al. for a more detailed discussion of these methods (Molnar 2018; Guidotti et al. 2018).\nSome of the recent activity on interpretability has been prompted by Europe\u2019s General Data Protection Regulation (GDPR). A legal discussion of the meaning of the regulation with respect to interpretability is ongoing. Initially, the GDPR regulations were described as providing a \u201cright to an explanation\u201d (Goodman and Flaxman 2016), although subsequent work challenges that claim (Wachter, Mittelstadt, and Floridi 2017), supporting a more nuanced right to \u201cmeaningful information\u201d about any automated decision impacting a user (Selbst and Powles 2017). Exactly what\nis meant by interpretability to support the GDPR and in a broader legal context remains in active discussion (Selbst and Barocas 2018).\nThe uncertainty around the meaning of \u201cinterpretability\u201d has prompted calls for more precise definitions and carefully delineated goals (Lipton 2018). One thought-provoking paper makes the case for a research agenda in interpretability driven by user studies and formalized metrics that can serve as validated proxies for user understanding (DoshiVelez and Kim 2017). Doshi-Velez and Kim argue that human evaluation of the interpretability of a method in its specific application context is the pinnacle of an interpretability research hierarchy followed by human evaluation of interpretability on a simplified or synthetic task and analysis of proxy tasks without associated user studies. In order to perform interpretability analysis without user studies, they argue, it is necessary to first assess proxies for user behavior. Here, we propose one such metric and assess its suitability as a proxy for the local interpretability of a model.\nAlthough we are unaware of existing metrics for the local interpretability of a general model, many measures developed by the program analysis community aim at assessing the understandability of a general program, which could be seen as metrics for global interpretability. For example, the cyclomatic complexity counts the number of independent paths through a program using its control flow graph (McCabe 1976). Metrics for specific model types have also been developed. Lage et al. (Lage et al. 2018a) investigate how different measures of complexity in decision sets affect accuracy and response time on tasks consisting of simulatability, verification, and counterfactual-reasoning. Via six different user studies of 150 people (for a total of 900 participants) they find that increased complexity in decision set logic results in increased response time but do not find a significant connection with accuracy. They measure decision set complexity as a combination between the explanation size, clauses in the disjunctive normal form of the input (called cognitive chunks), and number of repeated input conditions to the decision set. Their work is specific to decision sets and does not generalize to other model types.\nThere have also been experimentally grounded assessments of model properties related to (but different from) interpretability. Poursabzi-Sangdeh et al. (Poursabzi-Sangdeh et al. 2017) consider the impact of model attributes (e.g. black-box vs. clear) on user trust, simulatability, and mistake detection using randomized user studies on a similar scale to what we will consider here. They find that clear models (models where the inner calculations are displayed to the user) are best simulated. Allahyari et al. (Allahyari and Lavesson 2011) measure the perceived relative understandability of decision trees and rule-based models and find decision trees are seen as more understandable than rule-based models.\nOther methods are concerned with human in the loop optimization of the interpretability of machine learning models. Lage et. al. (Lage et al. 2018b) develop a method that optimizes models for both interpretability and accuracy by including user studies in the optimization loop. Their method minimizes the number of user studies needed to generate\nmodels that are both interpretable and accurate. They perform experiments on optimizing decision trees and find that the proxy interpretability metric optimized by the model (e.g. number of nodes, mean path length) varies based on dataset."}, {"heading": "A Metric for Local Interpretability", "text": "Motivated by the previous literature and its calls for uservalidated metrics that capture aspects of interpretability, we wish to assess whether a candidate metric captures a user\u2019s ability to simulate and \u201cwhat if\u201d locally explain a model. The candidate metric we consider here is the total number of runtime operation counts performed by the model when run on a given input. We consider two basic variants of operations, arithmetic and boolean, and track their totals separately. Effectively, we seek a proxy for the work that a user must do (in their head or via a calculator) in order to simulate a model on a given input, and will claim that the total number of operations also impacts a user\u2019s ability to perform a \u201cwhat if\u201d local explanation of a model."}, {"heading": "An Example", "text": "As an example of how this metric would work, consider the visualization of a decision tree in Figure 1. The result of run-\nning the model on the input (a = \u221280, b = 200) is shown circled in blue and the result of running the same model on the input (a = \u221264, b = 115) is shown circled in red. The red answer is at a depth of 10 in the decision tree while the blue answer is at a depth of 5. Counting the operations that the model takes to run on the input (including each boolean comparison operation or memory access required, which we count as an arithmetic operation) gives the total number of runtime operations - our candidate metric. Using the below methodology to count these operations, to determine the number of runtime operations executed when evaluating the decision tree model on the inputs from the example above, (blue: a = \u221280, b = 200 and red: a = \u221264, b = 115), the blue input is found to require 17 total operations (6 operations are arithmetic and 11 are boolean) while the red input requires 32 total operations (11 arithmetic and 21 boolean). Essentially, at each branch point one arithmetic operation is performed to do a memory access, one boolean operation is performed to check if the node is a leaf node, and one more boolean operation is performed for the branching operation."}, {"heading": "Calculating Runtime Operation Counts", "text": "In order to calculate the number of runtime operations for a given input, we instrumented the prediction operation for existing trained models in python\u2019s scikit-learn package (Buitinck et al. 2013). The source code for the technique is available at URL removed for anonymization. Since most machine learning models in scikit-learn use (indirectly, via other dependencies) cython, Fortran, and C for speed and memory efficiency, we implemented a pure Python version of the predict method for the classifiers, and instrumented the Python bytecode directly. We created purePython versions of the decision tree, logistic regression, and neural network classifiers in scikit-learn.1\nOnce working only with pure Python code, we used the tracing feature of python\u2019s sys module and a custom tracer function to count the number of boolean and arithmetic operations. The default behavior of tracer in python is line based, meaning the trace handler method is called for each line of the source code. We used the dis module to modify the compiled bytecode objects of useful modules stored in their respective .pyc files. In particular, we modified the line numbering metadata so that every bytecode is given a new line number, ensuring that our tracer function is called for every bytecode instruction (Ned 2008b; Ned 2008a; Ike-Nwosu 2018). Inside the tracer function we use the dis module to determine when a byte corresponds to a valid operation and count them accordingly for our simplified predict method implementations when run on a given input."}, {"heading": "User Study Design", "text": "We have two overall goals in this project: to assess the simulatability and \u201cwhat if\u201d local explainability of machine learning models, and to study the extent to which the proposed metric works as proxy for local interpretability. To\n1Specifically, sklearn.tree.DecisionTreeClassifier, sklearn.linear model. LogisticRegression, and sklearn.neural network.MLPClassifier.\nthose ends, we designed a crowdsourced experiment that was given to 1000 participants. Participants were asked to run a model on a given input and then evaluate the same model on a locally changed version of the input. We start by describing the many potentially interacting factors that required a careful experimental design."}, {"heading": "Models and Representations", "text": "For this study we consider the local interpretability of three models: decision trees, logistic regression, and neural networks. We chose decision trees and logistic regression because they are commonly considered to be interpretable (Lipton 2018). In contrast, we picked neural networks because they are commonly considered uninterpretable. The models were trained using the standard package scikit-learn.2\nOur decision tree representation is a standard node-link diagram representation for a decision tree or flow chart. In order to allow users to simulate the logistic regression and neural network classifiers we needed a representation that would walk the users through the calculations without previous training in using the model or any assumed mathematical knowledge beyond arithmetic. The resulting representation for logistic regression is shown in Figure 2. The neural network representation used the same representation as the logistic regression for each node and one page per layer.\nThe representations described so far are for the first question a user will be asked about a model - the request to simulate it on a given input. In order to allow users to assess the \u201cwhat if\u201d local explainability of the model, we also asked them to determine the output of the model for a perturbed version of the initial input they were shown. The representations used here are the same as the ones described, but a snapshot of the participants\u2019 previously filled in answers are shown for the logistic regression and neural network representations (see Figure 3) and users are not given blank entries to allow the re-simulation of the model."}, {"heading": "Data and Inputs", "text": "In order to avoid effects from study participants with domain knowledge, we created synthetic datasets to train the models. We created four synthetic datasets simple enough so that each model could achieve 100% test accuracy. These datasets consisted of a 2 dimensional dataset with rotation around an axis applied, 2 dimensional without rotation around an axis, 3 dimensional with rotation around an axis,\n2Decision trees were trained using sklearn.tree.DecisionTreeClassifier without any depth restrictions and with default parameters. Logistic regression was trained using sklearn.linear model.LogisticRegression with the multi class argument set to \u2019multinomial\u2019 and \u2019sag\u2019(Stochastic average gradient descent) as the solver. The neural network was implemented using sklearn.neural network.MLPClassifier. The neural network used is a fully connected network with 1 input layer, 1 hidden layer with 3 nodes, and 1 output layer. The relu (rectified linear unit) activation function was used for the hidden layer.\nand 5 dimensional with rotation around an axis. As the number of dimensions increases, so does the operation count. These four datasets were used to train the three considered models via an 80/20 train-test split. We generated user inputs using the test data. For each test data point, we changed one dimension incrementally in order to create a perturbed input.\nFrom this set of input and perturbed input pairs, we then chose a set of eight pairs for each trained model (i.e., for each model type and dataset combination) to show to the participants. The set was chosen to fit the following conditions: 50% of the classifications of the original inputs are True, 50% of the classifications on the perturbed input are True, and 50% of the time the classification between input and its perturbed input changes. We used this criteria in order to distribute classification patterns evenly across users so that a distribution of random guesses by the participants\nwould lead to 50% correctness on each task, and guessing that the perturbed input had the same outcome as the original input would also be correct 50% of the time."}, {"heading": "Pilot Studies", "text": "In order to assess the length of the study and work out any problems with instructions, we conducted three pilot studies. In the first informal study, one of us watched and took notes while a student attempted to simulate an input on each of the three types of models and determine the outcome for a perturbed input for each of those three models. In the second two pilots we recruited about 40 participants through Prolific and gave the study for a few fixed models and inputs with the same setup as we would be using for the full study. The main takeaways from these pilot studies were that we estimated it would take users 20-30 minutes to complete the survey, but that some users would take much longer. We had originally planned to include a dataset with 10 dimensions, and based on the time taken by users in the pilot survey decreased our largest dataset to 5 dimensions and added the 2-dimensional dataset with no rotation."}, {"heading": "Experimental Setup", "text": "We used Prolific to distribute the survey to 1000 users each of whom was paid $3.50 for completing it. Participants were restricted to those with at least a high school education (due to the mathematical nature of the task) and a Prolific rating greater than 75 out of 100. The full survey information (hosted through Qualtrics) and resulting data is available online.3\nEach participant was asked to calculate the output of a machine learning model for a given input, and then to determine the output of a perturbed input applied to the same model. We showed each participant three trained models: a logistic regression, a decision tree, and a neural network in a random order. Each participant was shown a model trained on a specific dataset (chosen from the four described earlier) at most once to avoid memory effects across models. Each question began with the initial input and a brief description of the task. As an attention check, we included a question in the survey that asked users to do some basic addition. Lastly, we asked each user at the end of the study to indicate whether they fully attempted to determine correct answers and that they would still be compensated in case they selected no. We considered only the data of the 930 users, who we will refer to as confident respondents, that selected they fully tried to determine correct answers and who correctly answered the basic addition problem.\nPreregistered Hypotheses We preregistered two experimental hypotheses. Namely, that time to complete will be positively related to operation count and that accuracy will be negatively related to operation count. We also preregistered two exploratory hypotheses. These were that we would explore the specific relationship between time and accuracy versus operation count and that we would explore how the\n3URL removed for anonymization\nperturbed input is related to time and operation count. These hypotheses can be found at the Open Science Framework at: url removed for anonymization\nStudy Setup Issues After running the user study, we found that an error in the survey setup meant that the survey exited prematurely for users given two of the eight inputs on the decision tree models for one dataset. Since we did not receive data from these participants, Prolific recruited other participants who were allocated to other inputs and datasets, so the analyzed dataset does not include data for these two inputs. Users who contacted us to let us know about the problem were still paid.\nMultiple Comparison Corrections In order to mitigate the problem of multiple comparisons, all p-values and confidence intervals we report in the next section include a Bonferroni correction factor of 28. While we include 15 statistical tests in this paper, we considered a total of 28. Reported p-values greater than one arise from these corrections."}, {"heading": "User Study Results", "text": "Based on the results from the described user study, we now examine folk hypotheses regarding the local interpretability of different model types, consider the relative local interpretability of these models, and assess our proposed metric."}, {"heading": "Assessing the Local Interpretability of Models", "text": "In order to assess the local interpretability of different model types, we first separately consider the user success on the task for simulatability (the original input) and the task for \u201cwhat if\u201d local explainability (the perturbed input). Since inputs were chosen so that 50% of the correct model outputs were \u201cyes\u201d and 50% were \u201cno\u201d, we compare the resulting participant correctness rates to the null hypothesis that respondents are correct 50% of the time. The resulting pvalues and confidence intervals are shown in Table 1.\nThe results indicate strong support for the simulatability of decision trees, logistic regression, and neural networks based on the representations the users were given. The results also indicate strong support for the \u201cwhat if\u201d local explainability of decision trees and logistic regression models, but neural networks were not found to be \u201cwhat if\u201d locally explainable.\nRecall that we consider models to be locally interpretable if they are both simulatable and \u201cwhat if\u201d locally explainable. Based on the results in Table 1, we thus have evidence that decision trees and logistic regression models are locally interpretable and neural networks are not, partially validating the folk hypotheses about the interpretability of these models. Next, we\u2019ll consider the relative local interpretability of these models."}, {"heading": "Assessing Relative Local Interpretability", "text": "In order to assess the relative local interpretability of models \u2014 to evaluate the folk hypothesis that decision trees and logistic regression models are more interpretable than neural networks \u2014 we compared the distributions of correct and incorrect answers on both tasks across pairs of model types. We applied one-sided Fisher exact tests with the null hypothesis that the models were equally simulatable, \u201cwhat if\u201d locally explainable, or locally interpretable. The alternative hypotheses were that decision trees and logistic regression models were more interpretable (had a greater number of correct responses) than neural networks and that decision trees were more interpretable than logistic regression.\nThe results (see Table 2) give strong evidence that decision trees are more locally interpretable than logistic regression or neural network models on both the simulatability and \u201cwhat if\u201d local explainability tasks. While there was strong evidence that logistic regression is more \u201cwhat if\u201d locally explainable and more locally interpretable than neural networks, there is not evidence that logistic regression is more simulatable than neural networks using the given representations. This may be because the logistic regression and neural network representations were very similar. An analysis of the users who got both tasks right, i.e., were able to locally interpret the model, shows that the alternative hypothesis was strongly supported in all three cases, thus supporting the folk hypotheses that decision trees and logistic regression models are more interpretable than neural networks."}, {"heading": "Assessing Runtime Operations as a Metric for", "text": "Local Interpretability In order to evaluate our preregistered hypotheses, we considered the relationship between total operation counts, time, and accuracy on the simulatability, \u201cwhat if\u201d local explainability, and combined local interpretability tasks. The graphs showing these relationships, including ellipses that depict the degree to which the different measurements are linearly related to each other, are shown in Figure 4. The time and accuracy given for the simulatability and \u201cwhat if\u201d local explainability tasks are separated individually for those tasks in the first two columns of the figure, while the final local interpretability column includes the sum of the time taken\nby the user on both tasks and credits the user with an accurate answer only if both the simulatability and \u201cwhat if\u201d local explainability tasks were correctly answered. The accuracies as displayed in the figure are averaged over all users given the same input into the trained model. All total operation counts given are for the simulation task on the specific input. In the case of the \u201cwhat if\u201d local explainability task for decision trees, this operation count is for the simulatability task on the perturbed input; the logistic regression and neural network simulatability operation counts do not vary based on input. The local interpretability total operation count is the sum of the counts on the simulatability and \u201cwhat if\u201d local explainability tasks. Additionally, we considered the effect on time and accuracy of just the arithmetic operation counts. The overall trends are discussed below."}, {"heading": "Assessing the Relationship Between Runtime Operations and Time", "text": "The number of operations has a positive relationship with the time taken. Across all three interpretability tasks it appears clear that as the number of operations increases, the total time taken by the user also increases (see the first row of Figure 4). This trend is especially clear for the simulatability task, validating Hypothesis 1. This effect is perhaps not surprising, since the operation count considered is for the simulatability task and the representations given focus on performing each operation.\nUsers were locally interpreting the \u201cwhat if\u201d local explainability task. Users spent much less time on the local explainability task than the simulatability task across all models. The difference suggests that users were actually locally interpreting the model on the \u201cwhat if\u201d local explainability task as opposed to re-simulating the whole model.\nThe time taken to simulate neural networks might not be feasible in practice. The neural network simulation time was noticeably greater than that of the decision tree and logistic regression. In some cases, the time expended was greater than 30 minutes. A user attempting the simulate the results of a model might give up or be unable to dedicate that much time to the task. The study takers likely feared lack of compensation if they gave up. This result suggests that in time constrained situations, neural networks are not simulatable."}, {"heading": "Assessing the Relationship Between Runtime Operations and Accuracy", "text": "The relationship between accuracy and operation count is clear for decision trees but not the other model types. As the total number of runtime operations increases, we hypothesized that the accuracy would decrease. In the second row of Figure 4 we can see that this trend appears to hold clearly for all three interpretability tasks for the decision tree models, but there is no clear trend for the logistic regression and neural network models. This lack of effect may be due to the comparatively smaller range of operation counts examined for these two model types, or it may be that the local interpretability of these model types is not as related to operation count as it is for decision trees. The lack of overlap in the ranges for the operation counts of logistic regression and neural networks also makes it hard to separate the effects of the model type on the results.\nSome users might not have understood the logistic regression and neural network tasks. Because the logistic regression and neural network tasks could be considered more challenging than the decision tree task, there may have been noise introduced by the variability in user ability to per-\nform the task. While operation counts might influence the accuracy for users who are able to understand the base task, this trend may be hidden by the fact that some users who were confident did not understand the task."}, {"heading": "Discussion and Conclusion", "text": "We investigated the local interpretability of three common model types: decision trees, logistic regression, and neural networks, and our user study provides evidence for the folk hypotheses that decision trees and logistic regression models are locally interpretable, while neural networks are not. We also found that decision trees are more locally interpretable than logistic regression or neural network models. We also showed that as the number of runtime operations increase, participants take longer to locally interpret a model, and they become less accurate on local interpretation tasks. This runtime operations metric provides some insight into the local interpretability of the discussed models and representations, and could indicate to practitioners the extent to which their models fulfill a lower bound requirement of interpretability. Further work is needed to consider the extent to which the metric generalizes to other model types. In addition, we found that users were consistently unable to locally inter-\npret the largest operation count neural networks shown to them, and their inability to simulate such neural networks could suggest that users struggle to locally interpret models more than 100 operations. Because we did not give users other models of similar operation count due to their potential display size to the user, further work is needed to verify if users inability to locally interpret large neural networks was caused by the number of operation counts or neural networks themselves.\nFurther, there are many caveats and limitations to the reach of this work. The domain-agnostic nature of our synthetic dataset has transferability advantages, but also has disadvantages in that it does not study interpretability within its target domain. The definitions of local interpretability that we assess here \u2014 simulatability and \u201cwhat if\u201d local explainability\u2014 are limited in their reach and the specific user study setup that we introduce may be limited in capturing the nuance of these definitions. Still, this work provides a starting point for designing user studies to validate notions of interpretability in machine learning. Such controlled studies are delicate and time-consuming, but are ultimately necessary in order for the field to make progress."}, {"heading": "SIGKDD Conference on Knowledge Discovery and Data", "text": "Mining (KDD).\n[Lipton 2018] Lipton, Z. C. 2018. The mythos of model interpretability. Queue 16(3):30.\n[McCabe 1976] McCabe, T. J. 1976. A complexity measure. IEEE Transactions on software Engineering (4):308\u2013320.\n[Molnar 2018] Molnar, C. 2018. Interpretable Machine Learning. https://christophm.github.io/interpretable-mlbook/. https://christophm.github.io/interpretable-ml-book/.\n[Ned 2008a] Ned, B. 2008a. The structure of .pyc files. Blog. https://nedbatchelder.com/blog/200804/the structure of pyc files.html.\n[Ned 2008b] Ned, B. 2008b. Wicked hack: Python bytecode tracing. Blog. https://nedbatchelder.com/blog/200804/ wicked hack python bytecode tracing.html.\n[Olah et al. 2018] Olah, C.; Satyanarayan, A.; Johnson, I.; Carter, S.; Schubert, L.; Ye, K.; and Mordvintsev, A. 2018. The building blocks of interpretability. Distill. https://distill.pub/2018/building-blocks.\n[Poursabzi-Sangdeh et al. 2017] Poursabzi-Sangdeh, F.; Goldstein, D. G.; Hofman, J. M.; Vaughan, J. W.; and Wallach, H. 2017. Manipulating and measuring model interpretability. Transparent and Interpretable Machine Learning in Safety Critical Environments Workshop at NIPS.\n[Prolific 2014] Prolific. 2014. https://prolific.ac/, last accessed on June 5th, 2019.\n[Ribeiro, Singh, and Guestrin 2016] Ribeiro, M. T.; Singh, S.; and Guestrin, C. 2016. Why should i trust you?: Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining, 1135\u20131144. ACM.\n[Selbst and Barocas 2018] Selbst, A. D., and Barocas, S. 2018. The intuitive appeal of explainable machines. Fordham Law Review. Forthcoming. Available at SSRN: https: //ssrn.com/abstract=3126971.\n[Selbst and Powles 2017] Selbst, A. D., and Powles, J. 2017. Meaningful information and the right to explanation. International Data Privacy Law 7(4):233\u2013242.\n[Ustun and Rudin 2016] Ustun, B., and Rudin, C. 2016. Supersparse linear integer models for optimized medical scoring systems. Machine Learning 102(3):349\u2013391.\n[Wachter, Mittelstadt, and Floridi 2017] Wachter, S.; Mittelstadt, B.; and Floridi, L. 2017. Why a right to explanation of automated decision-making does not exist in the general data protection regulation. International Data Privacy Law 7(2):76\u201399."}], "title": "Assessing the Local Interpretability of Machine Learning Models", "year": 2019}