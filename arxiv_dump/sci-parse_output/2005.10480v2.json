{"abstractText": "Traditionally, abnormal heart sound classification is framed as a three-stage process. The first stage involves segmenting the phonocardiogram to detect fundamental heart sounds; after which features are extracted and classification is performed. Some researchers in the field argue the segmentation step is an unwanted computational burden, whereas others embrace it as a prior step to feature extraction. When comparing accuracies achieved by studies that have segmented heart sounds before analysis with those who have overlooked that step, the question of whether to segment heart sounds before feature extraction is still open. In this study, we explicitly examine the importance of heart sound segmentation as a prior step for heart sound classification, and then seek to apply the obtained insights to propose a robust classifier for abnormal heart sound detection. Furthermore, recognizing the pressing need for explainable Artificial Intelligence (AI) models in the medical domain, we also unveil hidden representations learned by the classifier using model interpretation techniques. Experimental results demonstrate that the segmentation plays an essential role in abnormal heart sound classification. Our new classifier is also shown to be robust, stable and most importantly, explainable, with an accuracy of almost 100% on the widely used PhysioNet dataset.", "authors": [{"affiliations": [], "name": "Theekshana Dissanayake"}, {"affiliations": [], "name": "Tharindu Fernando"}, {"affiliations": [], "name": "Sridha Sridharan"}, {"affiliations": [], "name": "Houman Ghaemmaghami"}, {"affiliations": [], "name": "Clinton Fookes"}, {"affiliations": [], "name": "Senior"}], "id": "SP:aa4fa97a03285ce397410bb05cce59ad951973e8", "references": [{"authors": ["M.A. Santos", "R. Munoz", "R. Olivares", "P.P.R. Filho", "J.D. Ser", "V.H.C. d. Albuquerque"], "title": "Online heart monitoring systems on the internet of health things environments: A survey, a reference model and an outlook", "venue": "Information Fusion, vol. 53, pp. 222\u2013239, 1 2020.", "year": 2020}, {"authors": ["S. Patidar", "R.B. Pachori"], "title": "Classification of cardiac sound signals using constrained tunable-Q wavelet transform", "venue": "Expert Systems with Applications, vol. 41, pp. 7161\u20137170, 11 2014. 9", "year": 2014}, {"authors": ["H. Ghaemmaghami", "N. Hussain", "K. Tran", "A. Carey", "S. Hussain", "F. Syed", "A.J. Sinskey", "K. O\u2019Hashi", "J. Sperling"], "title": "Automatic segmentation and classification of cardiac cycles using deep learning and a wireless electronic stethoscope", "venue": "pp. 210\u2013213, Dec 2017.", "year": 2017}, {"authors": ["G.D. Clifford", "C. Liu", "B. Moody", "J. Millet", "S. Schmidt", "Q. Li", "I. Silva", "R.G. Mark"], "title": "Recent advances in heart sound analysis", "venue": "2017.", "year": 2017}, {"authors": ["W. Zhang", "J. Han", "S. Deng"], "title": "Abnormal heart sound detection using temporal quasi-periodic features and long short-term memory without segmentation", "venue": "Biomedical Signal Processing and Control, vol. 53, 8 2019.", "year": 2019}, {"authors": ["V. Arora", "R. Leekha", "R. Singh", "I. Chana"], "title": "Heart sound classification using machine learning and phonocardiogram", "venue": "Modern Physics Letters B, vol. 33, p. 1950321, 08 2019.", "year": 1950}, {"authors": ["V.G. Sujadevi", "K.P. Soman", "R. Vinayakumar", "A.U. Prem Sankar"], "title": "Anomaly Detection in Phonocardiogram Employing Deep Learning", "venue": "Advances in Intelligent Systems and Computing, vol. 711, pp. 525\u2013534, Springer Verlag, 2019.", "year": 2019}, {"authors": ["V. Maknickas", "A. Maknickas"], "title": "Recognition of normalabnormal phonocardiographic signals using deep convolutional neural networks and mel-frequency spectral coefficients", "venue": "Physiological Measurement, vol. 38, pp. 1671\u20131684, 7 2017.", "year": 2017}, {"authors": ["A.L. Goldberger", "L.A.N. Amaral", "L. Glass", "J.M. Hausdorff", "P.C. Ivanov", "R.G. Mark", "J.E. Mietus", "G.B. Moody", "C.-K. Peng", "H.E. Stanley"], "title": "PhysioBank, PhysioToolkit, and PhysioNet: Components of a new research resource for complex physiologic signals", "venue": "Circulation, vol. 101, no. 23, pp. e215\u2013e220, 2000 (June 13). Circulation Electronic Pages: http://circ.ahajournals.org/content/101/23/e215.full PMID:1085218; doi: 10.1161/01.CIR.101.23.e215.", "year": 2000}, {"authors": ["F. Numan", "S. Salleh", "C.-M. Ting", "S. Samdin", "H. Ombao", "H. Hussain"], "title": "A markov-switching model approach to heart sound segmentation and classification", "venue": "IEEE Journal of Biomedical and Health Informatics, 06 2019.", "year": 2019}, {"authors": ["M.E. Chowdhury", "A. Khandakar", "K. Alzoubi", "S. Mansoor", "A. M Tahir", "M.B.I. Reaz", "N. Al-Emadi"], "title": "Real-Time Smart-Digital Stethoscope System for Heart Diseases Monitoring", "venue": "Sensors (Basel, Switzerland), vol. 19, 6 2019.", "year": 2019}, {"authors": ["H. Alaskar", "N. Alzhrani", "A. Hussain", "F. Almarshed"], "title": "The Implementation of Pretrained AlexNet on PCG Classification", "venue": "pp. 784\u2013794, 2019.", "year": 2019}, {"authors": ["E. Kay", "A. Agarwal"], "title": "DropConnected neural networks trained on time-frequency and inter-beat features for classifying heart sounds", "venue": "Physiological Measurement, vol. 38, pp. 1645\u20131657, 7 2017.", "year": 2017}, {"authors": ["B.M. Whitaker", "P.B. Suresha", "C. Liu", "G.D. Clifford", "D.V. Anderson"], "title": "Combining sparse coding and time-domain features for heart sound classification", "venue": "Physiological Measurement, vol. 38, pp. 1701\u2013 1713, 7 2017.", "year": 2017}, {"authors": ["C. Potes", "S. Parvaneh", "A. Rahman", "B. Conroy"], "title": "Ensemble of featurebased and deep learning-based classifiers for detection of abnormal heart sounds", "venue": "2016 Computing in Cardiology Conference (CinC), pp. 621\u2013 624, Sep. 2016.", "year": 2016}, {"authors": ["S.M. Lundberg", "P.G. Allen", "S.-I. Lee"], "title": "A Unified Approach to Interpreting Model Predictions", "venue": "tech. rep."}, {"authors": ["A. Holzinger", "C. Biemann", "C.S. Pattichis", "D.B. Kell"], "title": "What do we need to build explainable AI systems for the medical domain", "venue": "12 2017.", "year": 2017}, {"authors": ["A. Adadi", "M. Berrada"], "title": "Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI)", "venue": "IEEE Access, vol. 6, pp. 52138\u201352160, 9 2018.", "year": 2018}, {"authors": ["M.T. Ribeiro", "S. Singh", "C. Guestrin"], "title": "Why should i trust you?\u201d Explaining the predictions of any classifier", "venue": "Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, vol. 13-17-August-2016, pp. 1135\u20131144, Association for Computing Machinery, 8 2016.", "year": 2016}, {"authors": ["A. Shrikumar", "P. Greenside", "A. Kundaje"], "title": "Learning Important Features Through Propagating Activation Differences", "venue": "tech. rep."}, {"authors": ["C. Molnar"], "title": "Interpretable Machine Learning", "year": 2019}, {"authors": ["L.S. Shapley"], "title": "A value for n-person games", "venue": "1952.", "year": 1952}, {"authors": ["M.D. Zeiler", "R. Fergus"], "title": "Visualizing and understanding convolutional networks", "venue": "Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), vol. 8689 LNCS, pp. 818\u2013833, Springer Verlag, 2014.", "year": 2014}, {"authors": ["M. Zabihi", "A. Bahrami Rad", "S. Kiranyaz", "M. Gabbouj", "A. Katsaggelos"], "title": "Heart sound anomaly and quality detection using ensemble of neural networks without segmentation", "venue": "09 2016.", "year": 2016}, {"authors": ["T. Fernando", "H. Ghaemmaghami", "S. Denman", "S. Sridharan", "N. Hussain", "C. Fookes"], "title": "Heart Sound Segmentation using Bidirectional LSTMs with Attention", "venue": "IEEE Journal of Biomedical and Health Informatics, pp. 1\u20131, 2019.", "year": 2019}, {"authors": ["F. Mendona", "S. Mostafa", "A. Ravelo-Garcia", "F. Morgado-Dias", "T. Penzel"], "title": "A review of obstructive sleep apnea detection approaches", "venue": "IEEE Journal of Biomedical and Health Informatics, vol. PP, pp. 1\u20131, 04 2018.", "year": 2018}, {"authors": ["A. Haridas", "R. Marimuthu", "V. Sivakumar"], "title": "A critical review and analysis on techniques of speech recognition: The road ahead", "venue": "International Journal of Knowledge-based and Intelligent Engineering Systems, vol. 22, pp. 39\u201357, 03 2018.", "year": 2018}, {"authors": ["M. Swain", "A. Routray", "P. Kabisatpathy"], "title": "Databases, features and classifiers for speech emotion recognition: a review", "venue": "International Journal of Speech Technology, vol. 21, pp. 93\u2013120, 3 2018.", "year": 2018}, {"authors": ["F. Chollet"], "title": "Keras", "venue": "2015.", "year": 2015}, {"authors": ["Yaseen", "G.-Y. Son", "S. Kwon"], "title": "Classification of Heart Sound Signal Using Multiple Features", "venue": "Applied Sciences, vol. 8, p. 2344, 11 2018.", "year": 2018}, {"authors": ["M. Brusco", "H. Nazeran"], "title": "Digital phonocardiography : A pdabased approach", "venue": "Conference proceedings : ... Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Conference, vol. 3, pp. 2299\u2013302, 02 2004.", "year": 2004}], "sections": [{"text": "1 Understanding the Importance of Heart Sound Segmentation for Heart Anomaly Detection\nTheekshana Dissanayake, Tharindu Fernando Member, IEEE, Simon Denman, Member, IEEE, Sridha Sridharan, Life Senior Member, IEEE, Houman Ghaemmaghami, Clinton Fookes,Senior Member, IEEE\nAbstract\u2014Traditionally, abnormal heart sound classification is framed as a three-stage process. The first stage involves segmenting the phonocardiogram to detect fundamental heart sounds; after which features are extracted and classification is performed. Some researchers in the field argue the segmentation step is an unwanted computational burden, whereas others embrace it as a prior step to feature extraction. When comparing accuracies achieved by studies that have segmented heart sounds before analysis with those who have overlooked that step, the question of whether to segment heart sounds before feature extraction is still open. In this study, we explicitly examine the importance of heart sound segmentation as a prior step for heart sound classification, and then seek to apply the obtained insights to propose a robust classifier for abnormal heart sound detection. Furthermore, recognizing the pressing need for explainable Artificial Intelligence (AI) models in the medical domain, we also unveil hidden representations learned by the classifier using model interpretation techniques. Experimental results demonstrate that the segmentation plays an essential role in abnormal heart sound classification. Our new classifier is also shown to be robust, stable and most importantly, explainable, with an accuracy of almost 100% on the widely used PhysioNet dataset.\nIndex Terms\u2014Heart sound segmentation, Biomedical Signal Processing, Phonocardiogram, Neural Networks.\nI. INTRODUCTION\nCardiovascular diseases have become one of the leading causes of death, and often lead to other medical conditions such as strokes, hypertension, heart failure and arrhythmia [1]\u2013 [3]. In the field of biomedical engineering, automatic abnormal heart sound detection can be considered a major prior step to cardiovascular disease diagnosis. The process of identifying whether a given heart sound is normal or abnormal can be divided into three major steps: segmentation, feature extraction, and classification [4]. Firstly, the segmentation technique locates the fundamental heart sounds of the Phonocardiogram (PCG) signal: S1 (first heart sound) and S2 (second heart sound); see Figure 1. However, detecting the fundamental heart sounds is itself a complex task, and can be affected by other internal sounds such as murmurs, the presence of third (S3) and fourth (S4) heart sounds and noise [5]. After segmenting the heart sound, various feature extraction techniques are used to extract features from the signal for training a classifier. These extracted features can generally be categorised as the time domain, frequency domain or time-frequency domain\nT. Dissanayake, T. Fernando, S. Denman, S. Sridharan and C. Fookes are with the Speech Audio Image and Video Technologies (SAIVT) Research Lab, Queensland University of Technology, Australia. H. Ghaemmaghami is with the M3DICINE Pty Ltd.\nfeatures of the PCG wave. As the final step, using the extracted features, a classifier is developed to identify abnormal heart sounds.\nStudying these fundamental steps followed for normalabnormal heart sound classification, the segmentation step can be considered an essential step to localize the signal before extracting various kinds of features. However several sophisticated studies in the literature, which have achieved superior performance for abnormal heart sound classification, have eschewed this step prior to feature extraction [5]\u2013[8]. Therefore, whether segmentation is required prior to developing a classifier is still an open question that should be answered to determine if the additional computational burden of this process is beneficial.\nIn summary, the principal objective of our research is to understand the importance of heart sound segmentation for the normal-abnormal heart sound classification task via a series of experiments involving empirical evaluations and model interpretation. Ultimately, the goal of this study is to develop a robust machine learning model for normal-abnormal heart sound classification while being able to explain the hidden representations learned by the model, and provide insight into the importance of segmentation for the overall task. The following list highlights the main contributions of our research:\n1) We empirically evaluate the importance of heart sound segmentation as a prior step to heart sound classification, and then, propose a novel deep learning model which achieves an accuracy of 98.71% for heart sound classification on the PhysioNet [9] dataset. 2) Going beyond the quantitative results, we interpret the developed deep learning model using the SHAP (SHapley Additive exPlanations [16]) algorithm to reveal the hidden representations learned by the model. 3) Based on insights obtained from the first experiment regarding segmentation, a second architecture (a slight\nar X\niv :2\n00 5.\n10 48\n0v 2\n[ cs\n.S D\n] 2\n9 Se\np 20\n20\n2 variant) which achieves close to 100% accuracy is proposed.\n4) We propose a procedure for interpreting models with temporal data, especially signals, using the SHAP algorithm and Occlusion maps; allowing us to critically evaluate the role of segmentation."}, {"heading": "II. RELATED WORK", "text": ""}, {"heading": "A. Heart Sound Anomaly Detection", "text": "Table I outlines recent studies on abnormal heart sound detection summarising the employed methods, features and prediction accuracies gained by the respective techniques. Along with these measures and design methodologies, this table shows whether the considered study has used segmentation prior to feature extraction or not. It should be noted that this table only represents the most recent studies on normalabnormal heart signal classification which demonstrate state of art performance.\nReviewing the accuracies gained for heart anomaly detection, both Ensemble Learners and various classes of Neural Networks demonstrate better performance compared to traditional machine learning algorithms. However, it should be noted that most best-performing methods in the literature have employed pre-extracted features from the PCG wave, and the results achieved so far show promising evidence for PCG signal based medical diagnosis.\nFrom consideration of the studies in Table I, the answer to the question of whether to segment heart-sounds before performing classification remains unresolved. This concern was also raised by Clifford et al. [4] in their extensive review of heart sound analysis research from the years 2015 to 2017. Moreover, according to the recent study by Zhang et al. [5], which introduces the best-performing model to date, segmentation is not a mandatory step for heart anomaly detection because the primary focus of the algorithm should be detecting the presence of an anomaly, which can be achieved without locating it. However, examining the recent literature, studies that considered segmentation as a prior step have also achieved competitive performance [11], [15]. Therefore further investigations are required.\nIn addition to understanding the importance of segmentation for PCG signal-based classification tasks, another primary concern when applying machine learning to the medical domain is\nthe trust associated with predictions made by a model. Even when a model performs with excellent accuracy, unless its behavior and predictions can be explained, a medical expert or a patient may not trust the validity of the system [17], [18]. Hence, there is an essential need to advance machine learning-based medical diagnosis research to a state where these concerns can be effectively addressed.\nIt is well established that, unlike traditional machine learning algorithms, deep learning models are better able to model complex relations in the data and thus achieve state-of-art results. However, these models have a black-box nature by virtue of their large number of parameters and complex architecture. Even if the model shows good performance for a particular task, understanding what leads to the achieved performance can be complex [19]. Fortunately, there are novel techniques that can be used to understand the nature of the model up to some extent (for eg: LIME [19], DeepLIFT [20] and SHAP [16]).\nDespite these advances in model understanding approaches, they have not been applied to bio-signal models, even though there is a pressing need for explainable AI within the medical domain. Motivated by this need, we interpret the designed models to discover which segments in the signal leads to the achieved performance. More importantly, this can be used to determine which segments (or segment combinations) in the heart sound indicate the presence of an anomaly."}, {"heading": "B. Model Interpretation", "text": "1) Shapley Values: One way of understanding the hidden representations learned by a black-box model is examining how a particular feature in the input contributes to a decision made by the model. In the context of a simple linear classifier, f(x), a prediction f\u0302(x) made for the n dimensional instance x can be expressed as,\nf\u0302(x) = \u03b20 + \u03b21x1 + \u03b22x2 + \u00b7 \u00b7 \u00b7+ \u03b2nxn. (1)\nHere, each xj is the value of each feature and \u03b2j is the weight associated with each feature.\nThe contribution factor, or in this case learned weights of a particular feature, might have a positive value or a negative value indicating the overall influence on the final prediction. Accordingly, if the contribution value is near zero\n3 or low compared to other absolute weights, it implies that the considered feature does not have a substantial contribution to the final prediction made by the model. Although this simple analogy explains a prediction made by a model with respect to the contributions made by the input features, this approach can not be adopted to explain the performance of the entire machine learning algorithm when applied to a particular application [21].\nFortunately, the SHapley Additive exPlanations (SHAP) [16] algorithm, designed by adopting Shapley values introduced in Game Theory [22], has the ability to provide such instance-level explanations for any machine learning model. According to this strategy, if a feature in the input is treated as a player, and the final prediction made by the model is the playout, then the computed Shapley values indicate how the playout is distributed among players. More precisely, the Shapley value of a particular feature is the contribution made by that feature to the final decision made by the model.\nThe SHAP algorithm, an advanced version of Shapley values, is able to produce local and global explanations for a model. Local explanations provided by the SHAP algorithm imply how much a particular feature contributes (or how important it is) to the decision made by the model; and global explanations present insights into the overall importance of a particular feature for the predictions made by the model. It should be noted that, as with weights in a simple linear classifier, Shapley values can have both negative and positive signs.\n2) Occlusion Maps: Another way of understanding how input features or, feature regions impact predictions is by visualizing Occlusion Maps [23]. These maps provide insights into important feature regions that the model concentrates on while making a correct prediction. Simply put, this map is generated by masking the input feature map using a kernel and visualizing the activations (or probabilities) for the class of interest. For instance, if the masked region contains features that are important for making a correct prediction, predictions made by the model for the masked feature map will have high probabilities for the correct class. Similarly, if the masked region is not important, or in other words the model does not focus on that region to make a correct prediction, then, the map will have lower probabilities for the class of interest.\nLike Shapley values, this method can be seen as another way of understanding what regions in the input feature map contribute to a correct prediction. Unlike Shapley values, this technique can be seen as a local explanation (or an instancelevel) method. Furthermore, local Shapley values provide insights into the contribution of a particular feature for the prediction made. Sometimes, highly contributing features (or important features) are not present in the input, yet the model might be able to produce a correct prediction using the available features. Therefore, visualizing Occlusion maps provides additional insights into the importance of having a particular feature region in the input feature map. An occlusion map will also show whether the model is focusing on the expected region or is focusing on seemingly unimportant sections in the input (i.e background textures, noise).\nThe rest of the paper is organized as follows. The next sec-\ntion of the study explains the data preparation and evaluation method adopted from [5]. Then, the next two sections explain the two experiments conducted, where the first experiment (Section II) aims to understand the importance of segmentation; and the second experiment (Section V) proposes a robust model by considering the observations made. Finally, Section VI critically analyzes the results and implications."}, {"heading": "III. DATA PREPARATION AND EVALUATION", "text": "The PhysioNet [9] database was selected as the main data source for investigation. This database contains an imbalanced dataset from clinical and non-clinical experiments and contains numerous variations including differences in device type, age and gender, variations in the physical conditions of the subject while acquiring the data, and different device placement locations. In our evaluation we adopt the 10 Fold Cross-Validation procedure proposed by Zhang et al. [5] (i.e the best performing model in the literature).\nSince in PhysioNet files are of different lengths, a windowing algorithm was employed to extract 1s signals from each wave file with a 0.1s shift (i.e 0.9s overlap). Recognizing the imbalanced nature of the PhysioNet database, studies in the literature have adopted various techniques to improve the model training process with imbalanced data. Unlike those, our investigation trains the model on a balanced database (bal db) which is not augmented or up-sampled. To create such database, all abnormal signals in the sample were selected. To ensure the the balanced database captured all normal wave files in the PhysioNet database, a sampling algorithm was employed. This algorithm selects windowed-signals from all normal wave files while keeping the balanced nature of the resulting data. Ultimately this process yields two databases, the balanced database (bal db) which is used to train/validate the model and the rest of the remaining (dropped) normal data (rest).\nbal db = [ [traini, vali] for i \u2208 [1, 2, . . . , 10] ]. (2)\nAs shown in Equation 2, for each fold i, the model will be trained on the traini database, and the model will be evaluated on the vali database and the rest. Then, the ground truth signal level classification for each fold will be performed by majority voting. Ultimately, for each fold, the entire PhysioNet database will be employed, and the test set is used for each fold to compute accuracy, sensitivity and specificity as same as in previous studies [5], [11], [24]."}, {"heading": "IV. HYBRID MODELS WITH SEGMENTATION", "text": "This section describes the first experiment where we aim to determine the importance of segmentation using three deep learning architectures. We start the experiment with a quantitative evaluation concerning model accuracies to evaluate the performance which can be achieved by adopting segmentation. Following this, we use model interpretation to understand the hidden representations learned by the model.\n4"}, {"heading": "A. Quantitative Evaluation", "text": "Prior to investigating the importance of segmentation, a robust segmentation model that can be used for the experiments is needed. Hence, we use the segmentation model proposed by Fernando et al. [25] which has an accuracy of 97% using an Attention-based Long-Short Term Memory (LSTM) network. Since the considered architecture employs Mel-Frequency Cepstral Coefficients (MFCCs), and MFCCs have been extensively used in medical [26], speech [27] and emotion [28] domains, we also adopt MFCCs as the primary feature.\nSimilar to [25], we use six Mel-Frequency filter banks within the range of 30Hz-300Hz and corresponding Delta (\u2206) and Delta-Delta (\u22062) features of the MFCC spectrum, resulting in a feature map shaped [6\u00d7 99\u00d7 3] for a 1s signal.\nFigure 2 presents the three main components employed in the experiment (top row) and the three proposed hybrid architectures derived by combining those components (bottom three). The first component is the pre-trained LSTM neural network adopted from [25] with the final layer removed.\nThe second component implements a Convolution Neural Network (CNN) encoder to perform spatial feature learning on the supplied feature maps. The third component acts as the final function of the hybrid model. This is a simple Multilayer Perseptron Network (MLP) with three hidden layers. This network accepts the learned representations from the previous feature extractors. Additionally, each layer of the MLP network has a 0.5 Dropout [29] rate and a value of 3.0 for the MaxNorm Kernel Normalizer [29]. Similarly, all convolution layers of the CNN network have values of 0.2 and 2.7 respectively for these hyper parameters.\nThree architectures are defined based on these components: \u2022 Model 1: This architecture connects the segmentation\nmodel and the CNN encoder to the MLP network. The MLP network accepts a combined learned feature map of shape [460 \u00d7 1] by concatenating the features extracted by both models. \u2022 Model 2: This model combines the CNN encoder and the MLP network while accepting the extracted segmentation-related feature map as an additional input channel. This feature map is generated by staking the transpose of the segmentor\u2019s output (shaped [99 \u00d7 1]) vertically desired shape of [6 \u00d7 99], at which point the feature map can be used as an additional channel alongside the MFCC, \u2206 and \u22062 features; resulting in a final input shape of [6\u00d7 99\u00d7 4] (*It should be noted that the transpose operation preserves the temporal relationships among the MFCCs and LSTM\u2019s outputs.) \u2022 Model 3: A CNN+MLP model without the feed from the segmentation model outputs (i.e the model without segmentation).\nTable II presents the 10 Fold Cross-Validation results of the proposed model architectures. Additionally, this table also shows three variants of the model (Model i\u2217) that only use the MFCC feature map as the input, though they do provide the full feature map to the segmentation model (i.e MFCC, \u2206 and \u22062) if that component is used. By examining the results, models accepting MFCC feature maps tend to perform with similar accuracy to models recieving all three feature maps. Another interesting fact that can be observed is the model combined with the intermediate segmentation feature map (Model 1) has similar accuracy compared to the model without segmentation (Model 3). However, the model which accepts the segmentation feature map as an input to the CNN encoder (Model 2) has a significantly lower prediction accuracy. Collectively, the presented results introduce a set of robust and stable classifiers that outperform the best performing model\n5 in the literature, Zhang et al. [5] by almost 4%. According to the results, feeding the learned representation from the segmentor seems to introduce some additional complexity or noise to the input feature map. Furthermore, models receiving an MFCC feature map produce similar results to the models which receive multiple feature maps. This might be explained by \u2206 and \u22062 features being slightly different versions of the MFCC map, which could be derived through a simple convolution kernel (if necessary).\nExamining all results, it is apparent that the segmentation step does not significantly improve the model prediction accuracy, however it is important to try to understand why segmentation appears to offer limited or no benefit."}, {"heading": "B. Model Interpretation", "text": "We first analyse the intermediate layer of Model 1 where the segmentation information is supplied to the MLP network. The dimension of this layer is [460 \u00d7 1] where the first [100 \u00d7 1] features are from the segmentation model. Figure 3 shows absolute Shapley values computed for 15 normal and 15 abnormal testing instances (signal feature maps), while the model holding different validation accuracies (i.e as the model trains). These absolute Shapley values represent the contribution of a particular intermediate feature to the final output. If the individual contribution is high, they are clearly visible as white contours in the feature maps, and low contributing features are dark green. The first 100 values are\nthe features from the segmenter. Examining the heat maps, it is apparent that the model is learning to focus on the CNN encoder features instead of segmentation-based features, leading CNN-based features to have a higher contribution to the final prediction. This factor seems to be valid for all considered testing instances. Therefore, according to this insight, even with the segmentation feature map present, the model is focusing on CNN features. However, according to Figure 4, this conclusion might not be entirely valid regarding the importance of segmentation to the final result.\nFigure 4 is generated from absolute Shapley values of four normal signal feature maps from the dataset. The figure only demonstrates Shapley value maps of the first feature dimension (i.e the MFCC map [6 \u00d7 99 \u00d7 1]). Higher values within the Shapley maps can be seen in the S1 and S2 locations, suggesting that this area of the signal has a greater contribution when making a decision. This is visible in almost all generated Shapley value explanations from a randomly picked sample of size 60. It should be noted that, unlike \u2206 and \u22062 maps, MFCCs have a strong temporal relationship with the ground truth signal, and therefore, aligning them with the temporal axis of the ground truth signal helps to understand the model.\nBy examining the Shapley value maps of the sample, S1 location-based MFCCs seem to be more important for prediction than S2-based MFCCs. Therefore, seemingly, the model has the ability to focus on S1 or S2 locations of the PCG wave without using the additional knowledge provided by the segmentation model (at the beginning or intermediate\n6\nlevel). Unfortunately, as shown in Figure 5, the same algorithm applied to abnormal signals does not provide such interpretable results. By considering these observations, it is apparent that without utilizing the knowledge provided by the segmentation model, the CNN+MLP network is capable of learning a complex function that can distinguish between normal-abnormal sounds while being able to localize the signal if needed.\nIn summary, these results highlight three main points regarding the segmentation of heart sounds and normal-abnormal heart sound classification.\n\u2022 Considering the classification accuracy of three models and their variants, these models perform well even with a single MFCC feature map. In fact, the accuracy difference between having MFCCs only compared to MFCCs, \u2206 and \u22062 is negligible. \u2022 Regarding the importance of segmentation, if the model is powerful enough, it will be able to learn to segment the data. Furthermore, Shapley values indicate that the MFCC features near the S1 and S2 locations have a higher contribution to prediction. Therefore, segmentation can be seen as an essential requirement for the classifier to make accurate predictions. \u2022 Feeding the knowledge separately may not generate the features that the model desires. One reason for this may be the model is learning a pattern that has a strong association with the S1 and S2 locations, which can only be extracted by the model itself.\nExamining these conclusions, it is apparent that the MFCCs and the structure of the proposed architecture plays an important role in the performance of the classifier. Given the above findings, in the next experiment, we propose a variant of the same model architecture with an enhanced MFCC feature map."}, {"heading": "V. A ROBUST MODEL WITHOUT SEGMENTATION", "text": ""}, {"heading": "A. Model Architecture", "text": "Figure 6 shows a detailed diagram of the proposed architecture. The new architecture has the same organization as the Model 3 proposed in the Section II. However, the MFCC feature map used in the proposed model is computed from 26 Mel-frequency filter banks within an extended frequency range of 0-500Hz. In contrast with the previous structure, this model\nuses wider convolution filters due to the new expanded feature map. Furthermore, since the resulting feature map from the CNN encoder holds a higher number of intermediate features (shaped [3 \u00d7 6 \u00d7 60]), the MLP network of this architecture has been also modified.\nThe proposed model achieves 99.78 (\u00b10.22)% accuracy for abnormal heart sound classification. Furthermore, the model achieves 99.77 (\u00b10.12)% sensitivity and 99.72 (\u00b10.18)% specificity for detecting abnormal heart sounds. The designed model outperforms the best performing model in the literature, Zhang et al. [5], with an accuracy gain of almost 5%. Furthermore, the proposed classifier outperforms models by Chowdhury et al. [11] and Zabihi et al. [24] which adopts the same analysis strategy.\nAs an additional step, we also evaluate the model on the murmur database created by Yaseen et al. [30]. This\n7 database contains 200 normal heart sounds and 800 abnormal murmur sounds (200 each for Aortic Stenosis, Mitral Regurgitation, Mitral Stenosis and Mitral Valve Prolapse). Following the evaluation protocol of [5], we achieve performance of 99.97 (\u00b10.23)% accuracy, 99.98 (\u00b10.31)% sensitivity and 99.96 (\u00b10.26)% specificity."}, {"heading": "B. Computational Efficiency and Stability", "text": "The standard deviation of the 10-Fold Cross-Validation result shows that the designed model is consistent and converges to almost the same classification accuracy for every fold. The model only takes 3s to make 20000 predictions on a computer with 12GB memory, six Intel E5-2680 2.50 GHz CPUs and one Nvidia K40 GPU. Hence, the proposed model represents a simple, stable and efficient architecture for abnormal heart sound classification."}, {"heading": "C. Model Interpretation", "text": "The next step of the experiment strives to find the hidden representations learned by the classifier using Shapley values. As observed in the previous analysis, the most imporant feature for the predictions are near to the fundamental heart sound locations. Since Shapley values only provide insights related to the contribution of a particular feature for the final prediction, this research also employs Occlusion maps [23] (computed using [3\u00d7 3] mask), which can be used to find the importance of a particular feature region by masking it from the input. To be more precise, the generated output presents the variation of the prediction if the considered feature region is not visible at the input.\nA set of randomly picked normal and abnormal signals were used to compute Shapley value maps and Occlusion maps for model interpretation (each sample had 60 signal files for the interpretation). As mentioned, this model uses 26 filter banks to generate the MFCC feature map resulting in an input shape\nof [26\u00d799\u00d71]. Figure 7 and 8 show Shapley values (middle) and Occlusion maps (bottom) for four ground truth normal and abnormal signals (top). The Shapley value graphs also illustrate whether the considered feature contributes negatively (pink) or positively (green) to the prediction made. The color map of the Occlusion map shows the output of the classifier if the considered region is masked (the decision boundary considered for the abnormal classification is 0.9).\nConsidering the patterns observed in the Shapley values and ground truth signals, almost all normal signal SHAP explanations show a pattern where the classifier is learned to focus on S1 and S2 locations within the PCG wave while making correct predictions. As mentioned, the previous Shapley computations were unable to provide such insights into what sort of signal locations contribute to correct abnormal signal prediction. However, Figure 8 clearly shows that the model is concentrating on either S1 or S2 regions in the MFCC map.\nAs discussed, unlike Shapley values, Occlusion maps provide insights about the impact on the presence of a feature region in the input. By observing consecutive Shapley values and Occlusion maps, the features which are contributing most can be seen as the most important in the input. However, even without those features, the model might be able make correct predictions. This is because the other features that are aligned to S1 or S2 locations contribute to the decision (i.e Shapley values that have medium contribution factors). Furthermore, the same phenomenon is also visible in Figure 8 where the Occlusion maps are computed for abnormal heart sounds. When examining the Occlusion map values for normal prediction, even the S1 location features are masked, the model is able to make correct predictions. Fortunately, this factor also seem to be valid for abnormal cases (i.e the computed probabilities/predictions are greater than 0.9, see Figure 8). Therefore, the model is robust enough to make correct predictions even if the part of the desired feature region is masked.\n8\nCollectively it is apparent that compared to the previous models, the model designed in Experiment II has the ability to locate S1 and S2 locations accurately. This is likely the reason for the model having a prediction accuracy of almost 100%. Therefore, this result further enhances the outcomes gained in the previous experiment, and the assumptions made from those interpretations are confirmed."}, {"heading": "VI. CONCLUSION", "text": "The main objective of this research was to discover the advantages of heart sound segmentation for abnormal heart sound classification, and derive a robust explainable model using those insights.\nConsidering the models proposed in this study, Model 3 from Experiment I and the model described in Experiment II, have superior and stable performance for normal-abnormal heart sound classification compared to the state-of-the-art. The final model, which is designed by considering the conclusions made in the first experiment, outperforms the best performing classifier in the literature with an accuracy by 5%. Furthermore, the architecture of the classifier only has three convolution layers and three fully connected layers, making it a simple but efficient model.\nRegarding the importance of segmentation, if the model has the capacity to learn the segmentation-function while extracting associated features from the S1 and S2 locations, then the segmentation is not needed as a prior step. This factor entirely depends on the robustness of the model, though as demonstrated in this work MFCC features do allow the identification of S1 and S2 locations. Ultimately, in conclusion, segmentation plays an essential role in abnormal heart sound detection. Another significant insight is, models from the investigations that overlooked segmentation [5], [6] might possess such capability within, but those insights were not evident or have not been investigated. Therefore, being the first investigation that uses model interpretation to understand abnormal heart sound classification, our study provide valuable\ninsights into the compelling need of explainable AI in the medical domain.\nAs discussed, one of the primary concerns in applying machine learning to the medial domain is the trust associated with the predictions made by the model. However, completely understanding the function of a classifier, especially a deep learning classifier, is a complex task. Algorithms such as SHAP and Occlusion maps provide insights into how different inputs impact predictions. By analyzing the results, both Shapley values and Occlusion maps reveal that the model is concentrating on the fundamental heart sounds of the PCG wave. Patterns associated with the correct classification of these signals are evident at those locations or in between them. In fact, this learned insight presents substantial evidence on the medical nature of the PCG signal, and these representations learned by the model seems to have a direct relation to medical procedure followed in digital phonocardiography [31]. As a result, the behavior of the model appears to be human-like.\nThe SHAP algorithm can be applied to interpret a model by making global explanations or instance level explanations. However, when the algorithm is applied to the biosignalbased medical domain, the temporal nature of the signal should also be considered to ensure interperetability of the results. Furthermore, in the medical domain, pre-extracted feature representations such as MFCCs or spectrograms are highly utilized [4]. As these have a direct relation to the signal (temporal), interpreting them alongside the input signal provides valuable insights into the predictions made by the model."}], "title": "Understanding the Importance of Heart Sound Segmentation for Heart Anomaly Detection", "year": 2020}