{
  "abstractText": "Applications using Artificial Intelligence techniques demand a thorough assessment of different aspects of trust, namely, data and model privacy, reliability, robustness against adversarial attacks, fairness, and interpretability. While each of these aspects has been extensively studied in isolation, an understanding of the trade-offs between different aspects of trust is lacking. In this work, the tradeoff between fault tolerance, privacy, and adversarial robustness is evaluated for Deep Neural Networks, by considering two adversarial settings under security and a privacy threat model. Specifically, this work studies the impact of training the model with input noise (Adversarial Robustness) and gradient noise (Differential Privacy) on Neural Network\u2019s fault tolerance. While adding noise to inputs, gradients or weights enhances fault tolerance, it is observed that adversarial robustness lowers fault tolerance due to increased overfitting. On the other hand, (\u03b5dp , \u03b4dp )-Differentially Private models enhance the fault tolerance, measured using generalisation error, which theoretically has an upper bound of edp \u22121+\u03b4dp . This novel study of the trade-offs between different aspects of trust is pivotal for training trustworthy Machine Learning models.",
  "authors": [
    {
      "affiliations": [],
      "name": "Vasisht Duddu"
    },
    {
      "affiliations": [],
      "name": "N. Rajesh Pillai"
    },
    {
      "affiliations": [],
      "name": "D. Vijay Rao"
    },
    {
      "affiliations": [],
      "name": "Valentina E. Balas"
    }
  ],
  "id": "SP:2c32e5c6d821c4b7cf58265f6728f219d36ec507",
  "references": [
    {
      "authors": [
        "Martin Abadi",
        "Andy Chu",
        "Ian Goodfellow",
        "H. Brendan McMahan",
        "Ilya Mironov",
        "Kunal Talwar",
        "Li Zhang"
      ],
      "title": "Deep Learning with Differential Privacy",
      "venue": "In Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security (CCS \u201916)",
      "year": 2016
    },
    {
      "authors": [
        "V. Beiu",
        "N.C. Rohatinovici",
        "L. D\u0103u\u015f",
        "V.E. Balas"
      ],
      "title": "Transport reliability on axonal cytoskeleton",
      "venue": "In 2017 14th International Conference on Engineering of Modern Electric Systems (EMES)",
      "year": 2017
    },
    {
      "authors": [
        "Jose Bernier",
        "Julio Ortega",
        "Eduardo Vidal",
        "Ignacio Rojas",
        "Alberto Prieto"
      ],
      "title": "A Quantitative Study of Fault Tolerance, Noise Immunity, and Generalization Ability of MLPs",
      "venue": "Neural Computation",
      "year": 2001
    },
    {
      "authors": [
        "Chris M. Bishop"
      ],
      "title": "Training with Noise is Equivalent to Tikhonov Regularization",
      "venue": "Neural Comput",
      "year": 1995
    },
    {
      "authors": [
        "Nicholas Carlini",
        "Chang Liu",
        "\u00dalfar Erlingsson",
        "Jernej Kos",
        "Dawn Song"
      ],
      "title": "The Secret Sharer: Evaluating and Testing Unintended 7 Memorization in Neural Networks",
      "venue": "In 28th USENIX Security Symposium (USENIX Security 19)",
      "year": 2019
    },
    {
      "authors": [
        "S.R. Cowell",
        "V. Beiu",
        "L. D\u0103u\u015f",
        "P. Poulin"
      ],
      "title": "On the Exact Reliability Enhancements of Small Hammock Networks",
      "venue": "IEEE Access",
      "year": 2018
    },
    {
      "authors": [
        "D. Deodhare",
        "M. Vidyasagar",
        "S. Sathiya Keethi"
      ],
      "title": "Synthesis of fault-tolerant feedforward neural networks using minimax optimization",
      "venue": "IEEE Transactions on Neural Networks 9,",
      "year": 1998
    },
    {
      "authors": [
        "P. Dey",
        "K. Nag",
        "T. Pal",
        "N.R. Pal"
      ],
      "title": "Regularizing Multilayer Perceptron for Robustness",
      "venue": "IEEE Transactions on Systems, Man, and Cybernetics: Systems 48,",
      "year": 2018
    },
    {
      "authors": [
        "Vasisht Duddu"
      ],
      "title": "A Survey of Adversarial Machine Learning in Cyber Warfare",
      "venue": "Defence Science Journal 68,",
      "year": 2018
    },
    {
      "authors": [
        "Vasisht Duddu",
        "D Vijay Rao",
        "Valentina E Balas"
      ],
      "title": "Adversarial Fault Tolerant Training for Deep Neural Networks",
      "year": 2019
    },
    {
      "authors": [
        "Cynthia Dwork"
      ],
      "title": "Differential Privacy: A Survey of Results",
      "venue": "In Theory and Applications of Models of Computation,",
      "year": 2008
    },
    {
      "authors": [
        "Christian Etmann",
        "Sebastian Lunz",
        "Peter Maass",
        "Carola Schoenlieb"
      ],
      "title": "On the Connection Between Adversarial Robustness and Saliency Map Interpretability",
      "venue": "In Proceedings of the 36th International Conference on Machine Learning (Proceedings of Machine Learning Research),",
      "year": 2019
    },
    {
      "authors": [
        "Matt Fredrikson",
        "Somesh Jha",
        "Thomas Ristenpart"
      ],
      "title": "Model Inversion Attacks That Exploit Confidence Information and Basic Countermeasures",
      "venue": "In Proceedings of the 22Nd ACM SIGSAC Conference on Computer and Communications Security (CCS \u201915)",
      "year": 2015
    },
    {
      "authors": [
        "Karan Ganju",
        "Qi Wang",
        "Wei Yang",
        "Carl A. Gunter",
        "Nikita Borisov"
      ],
      "title": "Property Inference Attacks on Fully Connected Neural Networks Using Permutation Invariant Representations",
      "venue": "In Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security (CCS \u201918)",
      "year": 2018
    },
    {
      "authors": [
        "K.I. . Ho",
        "C. Leung",
        "J. Sum"
      ],
      "title": "Convergence and Objective Functions of Some Fault/Noise-Injection-Based Online Learning Algorithms for RBF Networks",
      "venue": "IEEE Transactions on Neural Networks 21,",
      "year": 2010
    },
    {
      "authors": [
        "L. Holmstrom",
        "P. Koistinen"
      ],
      "title": "Using additive noise in back-propagation training",
      "venue": "IEEE Transactions on Neural Networks",
      "year": 1992
    },
    {
      "authors": [
        "Matthew Jagielski",
        "Michael Kearns",
        "Jieming Mao",
        "Alina Oprea",
        "Aaron Roth",
        "Saeed Sharifi Malvajerdi",
        "Jonathan Ullman"
      ],
      "title": "Differentially Private Fair Learning",
      "venue": "In Proceedings of the 36th International Conference on Machine Learning (Proceedings of Machine Learning Research),",
      "year": 2019
    },
    {
      "authors": [
        "Bargav Jayaraman",
        "David Evans"
      ],
      "title": "Evaluating Differentially Private Machine Learning in Practice",
      "venue": "In 28th USENIX Security Symposium (USENIX Security 19)",
      "year": 2019
    },
    {
      "authors": [
        "Jinyuan Jia",
        "Ahmed Salem",
        "Michael Backes",
        "Yang Zhang",
        "Neil Zhenqiang Gong"
      ],
      "title": "MemGuard: Defending against Black-Box Membership Inference Attacks via Adversarial Examples",
      "year": 2019
    },
    {
      "authors": [
        "Cong Jin",
        "Shu-Wei Jin"
      ],
      "title": "Applications of Fuzzy Integrals for Predicting Software Fault-prone",
      "venue": "J. Intell. Fuzzy Syst. 26,",
      "year": 2014
    },
    {
      "authors": [
        "Aleksander Madry",
        "Aleksandar Makelov",
        "Ludwig Schmidt",
        "Dimitris Tsipras",
        "Adrian Vladu"
      ],
      "title": "Towards Deep Learning Models Resistant to Adversarial Attacks",
      "venue": "In International Conference on Learning Representations. https://openreview.net/forum?id=rJzIBfZAb",
      "year": 2018
    },
    {
      "authors": [
        "K. Matsuoka"
      ],
      "title": "Noise injection into inputs in back-propagation learning",
      "venue": "IEEE Transactions on Systems, Man, and Cybernetics 22,",
      "year": 1992
    },
    {
      "authors": [
        "Matthew Mirman",
        "Timon Gehr",
        "Martin Vechev"
      ],
      "title": "Differentiable Abstract Interpretation for Provably Robust Neural Networks",
      "venue": "In Proceedings of the 35th International Conference on Machine Learning (Proceedings of Machine Learning Research), Jennifer Dy and Andreas Krause (Eds.),",
      "year": 2018
    },
    {
      "authors": [
        "I. Mironov"
      ],
      "title": "R\u00e9nyi Differential Privacy",
      "venue": "IEEE 30th Computer Security Foundations Symposium (CSF)",
      "year": 2017
    },
    {
      "authors": [
        "Milad Nasr",
        "Reza Shokri",
        "Amir Houmansadr"
      ],
      "title": "Machine Learning with Membership Privacy Using Adversarial Regularization",
      "venue": "In Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security (CCS \u201918)",
      "year": 2018
    },
    {
      "authors": [
        "C. Neti",
        "M.H. Schneider",
        "E.D. Young"
      ],
      "title": "Maximally fault tolerant neural networks",
      "venue": "IEEE Transactions on Neural Networks",
      "year": 1992
    },
    {
      "authors": [
        "Nicolas Papernot",
        "Shuang Song",
        "Ilya Mironov",
        "Ananth Raghunathan",
        "Kunal Talwar",
        "Ulfar Erlingsson"
      ],
      "title": "Scalable Private Learning with PATE",
      "venue": "In International Conference on Learning Representations",
      "year": 2018
    },
    {
      "authors": [
        "D.S. Phatak",
        "I. Koren"
      ],
      "title": "Complete and partial fault tolerance of feedforward neural nets",
      "venue": "IEEE Transactions on Neural Networks 6,",
      "year": 1995
    },
    {
      "authors": [
        "Ahmed Salem",
        "Apratim Bhattacharyya",
        "Michael Backes",
        "Mario Fritz",
        "Yang Zhang"
      ],
      "title": "Updates-Leak: Data Set Inference and Reconstruction Attacks in Online Learning",
      "venue": "CoRR abs/1904.01067 (2019)",
      "year": 2019
    },
    {
      "authors": [
        "Ahmed Salem",
        "Yang Zhang",
        "Mathias Humbert",
        "Mario Fritz",
        "Michael Backes"
      ],
      "title": "ML-Leaks: Model and Data Independent Membership Inference Attacks and Defenses on Machine Learning Models",
      "venue": "In Annual Network and Distributed System Security Symposium",
      "year": 2019
    },
    {
      "authors": [
        "Reza Shokri",
        "Martin Strobel",
        "Yair Zick"
      ],
      "title": "Privacy Risks of Explaining Machine Learning Models",
      "venue": "arXiv preprint arXiv:1907.00164",
      "year": 2019
    },
    {
      "authors": [
        "R. Shokri",
        "M. Stronati",
        "C. Song",
        "V. Shmatikov"
      ],
      "title": "Membership Inference Attacks Against Machine Learning Models",
      "venue": "IEEE Symposium on Security and Privacy (SP)",
      "year": 2017
    },
    {
      "authors": [
        "Aman Sinha",
        "Hongseok Namkoong",
        "John Duchi"
      ],
      "title": "Certifiable Distributional Robustness with Principled Adversarial Training",
      "venue": "In International Conference on Learning Representations",
      "year": 2018
    },
    {
      "authors": [
        "Liwei Song",
        "Reza Shokri",
        "Prateek Mittal"
      ],
      "title": "Privacy Risks of Securing Machine Learning Models against Adversarial Examples",
      "year": 2019
    },
    {
      "authors": [
        "J. Wang",
        "Q. Chang",
        "Y. Liu",
        "N.R. Pal"
      ],
      "title": "Weight Noise Injection-Based MLPs With Group Lasso Penalty: Asymptotic Convergence and Application to Node Pruning",
      "venue": "IEEE Transactions on Cybernetics (2018),",
      "year": 2018
    },
    {
      "authors": [
        "Yu-Xiang Wang",
        "Jing Lei",
        "Stephen E. Fienberg"
      ],
      "title": "Learning with Differential Privacy: Stability, Learnability and the Sufficiency and Necessity of ERM Principle",
      "venue": "Journal of Machine Learning Research 17,",
      "year": 2016
    },
    {
      "authors": [
        "Hongyang Zhang",
        "Yaodong Yu",
        "Jiantao Jiao",
        "Eric Xing",
        "Laurent El Ghaoui",
        "Michael Jordan"
      ],
      "title": "Theoretically Principled Trade-off between Robustness and Accuracy",
      "venue": "In Proceedings of the 36th International Conference on Machine Learning (Proceedings of Machine Learning Research),",
      "year": 2019
    }
  ],
  "sections": [
    {
      "text": "ar X\niv :1\n91 0.\n13 87\n5v 2\n[ cs\n.C R\n] 7\nM ar\n2 02\n0\nApplications using Artificial Intelligence techniques demand a thorough assessment of different aspects of trust, namely, data and model privacy, reliability, robustness against adversarial attacks, fairness, and interpretability. While each of these aspects has been extensively studied in isolation, an understanding of the trade-offs between different aspects of trust is lacking. In this work, the tradeoff between fault tolerance, privacy, and adversarial robustness is evaluated for Deep Neural Networks, by considering two adversarial settings under security and a privacy threat model. Specifically, this work studies the impact of training the model with input noise (Adversarial Robustness) and gradient noise (Differential Privacy) on Neural Network\u2019s fault tolerance. While adding noise to inputs, gradients or weights enhances fault tolerance, it is observed that adversarial robustness lowers fault tolerance due to increased overfitting. On the other hand, (\u03f5dp , \u03b4dp )-Differentially Private models enhance the fault tolerance, measured using generalisation error, which theoretically has an upper bound of e\u03f5dp \u22121+\u03b4dp . This novel study of the trade-offs between different aspects of trust is pivotal for training trustworthy Machine Learning models.\nKEYWORDS\nTrustworthy Machine Learning, Differential Privacy, Fault Tolerance, Adversarial Robustness, Deep Learning."
    },
    {
      "heading": "1 INTRODUCTION",
      "text": "There is a growing reliance on Artificial Intelligence (AI) techniques in safety-critical real-time applications with high-stake decision-making such as autonomous vehicles, criminal justice, and healthcare. These applications demand satisfying different aspects of trust: fairness among disparate groups, the privacy of individuals in the training data, robustness against adversarially perturbed inputs, fault tolerance for safety and model interpretability. Training the models to incorporate and optimize for all the aspects of trust is difficult and hence, it is crucial to understand the trade-offs between different aspects of trust for designing efficient Pareto-optimal solutions for trustworthy AI systems. Prior research has indicated that privacy and explainability [32], adversarial robustness and membership privacy [35] are at odds. On the other hand, other aspects of trust go hand in hand: fairness and differential privacy [18], adversarial robustness and explainability [13]. However, the impact on fault tolerance due to robust and private models has not been explored yet. To address this requirement, this research analyses the impact of training Machine Learning models, specifically Deep Neural Networks, for adversarial robustness (security) and differential privacy on the model\u2019s fault tolerance (reliability).\nFault Tolerance is a crucial property for Neural Networks to ensure reliable computation for a long duration with graceful degradation over time. Typically, well generalized models have the parameters with low variance ensuring equal computational weight to all nodes in the network [10]. Hence, the loss of some of the nodes can be compensated by other nodes without a significant loss in performance [8][7][27]. Practically, this is achieved by adding noise during training to the inputs, gradients or the weights. The noise added to the inputs can be modeled as Tikhonov regularization which enhances the generalization [4].\nHowever, in an adversarial setting under a security threat model, carefully crafted imperceptible noise can be added to input images by an adversary to force the model to misclassify the image, violating the integrity of the model prediction. The design of Neural Networks within such an adversarial setting requires training on inputs with adversarial noise to ensure robustness against adversary\u2019s worst-case perturbation. The goal of this work, in this setting, is to address the following research question,\nWhat is the impact of adversarially robust (input\nnoise) training on fault tolerance?\nAlternatively, in an adversarial setting under a privacy threat model, an adversary performs inference attacks to identify training data attributes or membership details for sampled data point [15][33][31]. This poses a serious privacy risk for sensitive training data such as financial and medical records, personal photos, location history, and user preferences. Differential Privacy provides a provable guarantee on the maximum privacy leakage by making the data points indistinguishable using gradient noise during training [12][1]. In such a setting, this work addresses the following research question,\nWhat is the impact of training models with Dif-\nferential Privacy (gradient noise) on fault toler-\nance?\nMain Contributions. This work makes the following novel contributions:\n\u2022 Evaluate the fault tolerance of provably robust Neural Networks and compare them with the theoretical equivalent Tikhonov regularized model. \u2022 Evaluate the fault tolerance of Differentially Private mod-\nels under a privacy threat model and compare it with regularised and naturally (without noise) trained models. \u2022 Prove theoretically that generalization error, used to measure fault tolerance, has an upper bound of e\u03f5dp \u2212 1 + \u03b4 on training the model using (\u03f5dp , \u03b4dp )-Differential Privacy, thus giving provable guarantees on fault tolerance metric.\nThe performance of the training algorithms is evaluated using two common benchmarking datasets: CIFAR10 and FashionMNIST,\nwith different Neural Network architectures. It is observed that adversarial robustness and fault tolerance are at odds with each other, i.e, training a model with adversarial input noise results in overfitting which lowers fault tolerance. On the other hand, noise added for (\u03f5dp , \u03b4dp )-Differential Privacy is an alternate approach for enhancing fault tolerance, while guaranteeing privacy, with a theoretical bound on the generalization error in terms of the privacy parameters: \u03f5dp , \u03b4dp . To the best of our knowledge, this is the first work that evaluates the fault tolerance of adversarially robust and differentially private Deep Neural Networks. Such an analysis is crucial for a unified framework for trustworthy Machine Learning combining security, privacy, and reliability for real-world deployment."
    },
    {
      "heading": "2 BACKGROUND",
      "text": ""
    },
    {
      "heading": "2.1 Fault Tolerance in Neural Networks",
      "text": "DEFINITION 1. A Neural Network N performing computations HN is said to be fault tolerant if the computation performed by a faulty network HNf ault is close to HN . Formally, a Neural Network is \u03f5 fault tolerant if, HN(X) \u2212 HNf ault (X) \u2264 \u03f5f t (1)\nfor \u03f5f t > 0 and X \u2208 D.\nFault Tolerance Metric. Improving the generalization results in enhancing the fault tolerance and vice versa [3]. Hence, capturing the overfitting of the model provides a way to compare the relative fault tolerance of multiple models. This has been extensively used in literature to measure fault tolerance of Neural Networks [36] [10].\nFormally, given a dataset Dtest such thatDtr ain \u2229Dtest = \u03d5, the accuracy of the model is estimated on the training set (Rtr ain) and on the testing set (Rtest ). The generalization error is given by the difference between training accuracy and the testing accuracy,\nGer r = Rtr ain \u2212 Rtest (2)\nThis gives the estimate of overfitting in the Neural Network (Rtr ain > Rtest ), i.e, higher the generalization error more the overfitting. This estimate of fault tolerance is used for the evaluation of different Neural Networks throughout the paper.\nFault Model. In this work, the faults occurring in the hardware are simulated in the form of stuck at \u201c0\" errors during the Neural Network computation. Further, multiple faults can occur simultaneously for which the performance degradation is measured using test accuracy. The faults are simulated in two ways: firstly, these faults can manifest in the form of random node crashes in the Neural Network due to which the output of the node is forced to zero. Secondly, the parameters of the Neural Networks can be stuck at \u201c0\" which includes weights in the case of Multilayer Perceptron, and filter and kernel values in case of Convolutional Neural Networks. This is a common fault model frequently used in evaluating the reliability of systems [10].\nRelated Work. A widely used approach for enhancing fault tolerance is to penalize large values of the parameters using a regularization function [8][36]. Alternatively, constraint optimisation approaches using minimax constraint [27] as well as quadratic programming [7] can be used for small networks. Unlike simple regularization functions, unsupervised pre-training of the initial network layers followed by supervised fine-tuning can significantly enhance the fault tolerance [10]. Traditional techniques to enhance reliability such as additional redundancy by adding nodes and synapses provides partial fault tolerance [29]. Further, reliability of axonal transport has been explored using Hammock Networks [2][6]. Detection of faults and enhancing tolerance in software has been explored for fuzzy control systems [21]."
    },
    {
      "heading": "2.2 Adversarial Robustness",
      "text": "Within the adversarial setting with a security adversary, the problem of adversarial robustness is modeled as a game between the attacker and defender with conflicting interests. Here, the adversary wants to force the target model to misclassify by adding carefully calculated noise to input, while the defender wants to train the model to defend against such inputs with adversarial noise [9].\nAttacker Knowledge. In this work, the adversary has no knowledge about the target model. In other words, the adversary has remote access to the target black-box model and can query the model and receive corresponding predictions through an API. This is typically the black box setting seen in Machine Learning as a Service.\nAttacker Goal. The goal of the adversary is to find the noise to maximize the loss of the target model (F\u03b8 ()) and force the model to misclassify the perturbed input. Formally, given an input sampled from the underlying data distribution x \u223c P(X ,Y ), the adversary computes the worst-case adversarial noise \u03b4adv to maximize the loss (l) between predicted output and true output y,\n\u03b4\u22c6(x)adv = argmax \u2016\u03b4adv \u2016\u2264\u03f5adv \u2113(F\u03b8 (x + \u03b4adv ),y) (3)\nsubject to a bound on the perturbation computed using parameter \u03f5adv ,\n\u2206 = {\u03b4adv : | |\u03b4adv | |p \u2264 \u03f5adv } (4)\nThe optimization is subjected to the condition that the noise is imperceptible by restricting \u03b4 within a perturbation region \u2206 defined by a lp norm, more commonly l\u221e [22].\nDefender Strategy. To defend against the worst attack possible, the model is trained using adversarial inputs as part of the training data. Formally, this empirical defense can be modeled as a minimax optimization problem given below,\nminimize \u03b8 \u00a9\u00ab 1 |D | \u2211 x,y\u2208D max \u2016\u03b4 \u2016\u2264\u03f5 \u2113(F\u03b8 (x + \u03b4 ),y) \u00aa\u00ae \u00ac\n(5)\nHere, instead of minimising the expected loss over the data points sampled from the distribution, the optimisation minimises the worst case loss over the data with adversarial noise. In other words, the defender minimises the loss corresponding to the worst case adversarial attack.\nIn this work, TRADES algorithm is considered as a defense since it provides provable bounds against adversarial examples [38]. TRADES algorithm decomposes the prediction error for adversarial example as the sum of natural classification error and the boundary error to provide a tight differentiable upper bound. This defense minimizes the maximum Kullback-Leibler (KL) Divergence between the output prediction corresponding to a benign sample (X ) and adversarial sample (Xadv \u2190 X + \u03b4adv ). This is used to generate adversarial examples within the inner maximization.\nmin \u03b8 (E[l(\u03b8, F (X ),Y )] + max Xadv \u2208\u00b7(X ,\u03f5 ) dkl (F (X ), F (Xadv ))) (6)\nAlgorithm 1 Adversarial Training by adding noise to inputs\nInput: Dtr ain = {(xi ,yi ), \u00b7 \u00b7 \u00b7 , (xN ,yN )} and Loss: L(\u03b8t ,xi ) Input: A: Algorithm to generate adversarial noise\n1: for each epoch do 2: Sample a random batch B \u2208 Dtr ain 3: for each (xi ,yi ) \u2208 B do 4: Compute Adversarial Noise:\nx\u2217 adv \u2190 A(xi ,yi )\n5: Compute gradient on adversarial inputs: \u2202Li \u2202\u03b8t \u2190 \u2207\u03b8tL(\u03b8t ,x \u2217 adv ) 6: Update \u03b8 using Gradient Descent:\n\u03b8t+1 \u2190 \u03b8t \u2212 \u03b1 \u2202Li \u2202\u03b8t\n7: end for 8: Output: Parameters \u03b8 of trained model with adversarial robustness\n9: end for\nRelated Work. While only the defense algorithm with tight upper bound and provable robustness guarantees is considered in this work (TRADES), other empirical approaches use Projected Gradient Descent [22] and Wasserstein norm [34] to ensure robustness. Alternatively, verification based defenses use function transformations to compute the worst-case loss to express the adversarial perturbations [24]."
    },
    {
      "heading": "2.3 Differential Privacy",
      "text": "Differential Privacy is the de facto privacy standard that provides a strong privacy definition with provable bounds on information leaked by a mechanism in terms of the privacy budget \u03f5dp [12]. The output of a randomized mechanism should not allow the adversary to learn any more about an individual in the dataset than that could be learned via the same analysis without the individual in the dataset. In this sense, this definition of privacy captures the individual\u2019s membership in the dataset.\nDEFINITION 2. For a randomized mechanism M : X \u2192 Y is (\u03f5dp , \u03b4dp ) differentially private on two neighbouring datasets D andD\u2032 differing by an individual data point, then for all outputs O\n\u2286 Y,\nP[M(D) \u2208 O] \u2264 e\u03f5dpP[M(D\u2032) \u2208 O] + \u03b4dp (7)\nHere, the parameters \u03f5dp is considered as the privacy budget and \u03b4dp is considered as the failure probability [11].\nA tighter and accurate estimation of the privacy loss can be computed using the Renyi Differential Privacy [25] which uses the Renyi divergence metric D\u03b1 which applies to any moment of the privacy loss random variable.\nDEFINITION 3. For a randomized mechanism M : X \u2192 Y is \u03f5dp -Renyi differentially private of the order \u03b1 , on two neighbouring datasetsD and D\u2032 differing by a individual data point, then for all\noutputs O \u2286 Y,\nD\u03b1 (M(D) || M(D \u2032)) \u2264 \u03f5 (8)\nAttacker Strategy. The goal of the attacker within the privacy setting is to use inference attacks to leak training data details resulting in privacy violations where the training data is sensitive. Membership inference attacks infer whether a given data point was used in the training data or not based on the difference in model performance on training data and testing data [33]. On the other hand, attribute inference attacks extract particular features of the training data [14] or reconstruct the entire training data [30]. Another class of attacks exploits the memorization capacity of the model to infer the sensitive attributes in the data by querying the model [5].\nDefender Strategy. The defender, in order, to reduce the success of the inference attacks utilizes the notion of differential privacy to train the model with provable privacy leakage guarantees. To this extent, the defender samples a noise from either a Laplace or Gaussian distribution proportional to the sensitivity of the model (S).\nM(D) \u2243 HN(X) +N(0, S 2 .\u03c32) (9)\nIn case of Deep Neural Networks, this noise is added to the gradients of the model and mitigates several of the attacker\u2019s inference strategies, however, at the cost of utility (performance) [19].\nRelated Work. To preserve privacy against membership inference attacks, several empirical defenses exist. For instance, the adversary\u2019s inference attack can be modeled as a minimax optimization problem, where the target model is trained to minimize the adversary\u2019s best attack [26]. Another line of defense is to add noise to the output of the model, force the inference attack machine learning model to misclassify while ensuring the utility does not degrade\n[20]. While these approaches are empirical, all of them face utility privacy trade-off and do not provide a theoretical guarantee on the maximum leakage of the model. Hence, in this work, Differential Privacy based private training is used since it provides provable guarantees on the leakage of the model about the training data [1]. An alternative Differential Privacy based framework uses a teacherstudent ensemble approach [28].\nAlgorithm 2 Differentially Private Training by adding noise to gradients during Backpropagation\nInput: Dtr ain = {(xi ,yi ), \u00b7 \u00b7 \u00b7 , (xN ,yN )} and Loss function: L(\u03b8t , xi )\n1: for each epoch do 2: Sample a random batch B \u2208 Dtr ain 3: for each (xi ,yi ) \u2208 B do 4: Compute gradient:\n\u2202Li \u2202\u03b8t \u2190 \u2207\u03b8tL(\u03b8t ,xi )\n5: Gradient Clipping:\n\u0434t (xi ) \u2190 \u2202Li \u2202\u03b8t /max\n( 1, \u2016 \u2202Li \u2202\u03b8t \u20162\nC ) 6: Add Noise:\n\u0434\u2032t \u2190 1 L (\u2211 i \u0434t (xi ) +N(0,\u03c3 2C2I ) )\n7: Update \u03b8 using Gradient Descent:\n\u03b8t+1 \u2190 \u03b8t \u2212 \u03b1\u0434 \u2032 t\n8: end for 9: Output: Parameters \u03b8 of model trained using (\u03f5, \u03b4 )- Differential Privacy.\n10: end for"
    },
    {
      "heading": "3 EXPERIMENT SETUP",
      "text": "The code for training using TRADES adversarial training algorithm is based on the official source code from the authors1. The official code from Tensorflow Privacy Library2 for Differentially Private training is adapted to different architectures and datasets."
    },
    {
      "heading": "3.1 Datasets",
      "text": "The evaluation and training of adversarially robust and differentially private models are done on two major benchmarking datasets, namely, FashionMNIST and CIFAR10.\nFashionMNIST. The dataset is similar to the MNIST dataset and consists of a training set of 60,000 examples and a test set of 10,000 examples. Each data sample is a 28\u00d728 grayscale image associated with a label from 10 classes such as boots, shirt, bag and so on.\nCIFAR10. The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images."
    },
    {
      "heading": "3.2 Architectures",
      "text": "For the TRADES robust training algorithm on CIFAR10 dataset, an Neural Network architecture with nine convolutional layers followed by fully connected layers is used. In the case of FashionMNIST dataset with adversarial robust training, the FMNIST CNN 2\n1https://github.com/yaodongyu/TRADES 2https://github.com/tensorflow/privacy\narchitecture based on LeNet architecture is used. For Differential Private training, FMNIST CNN 1 architecture is used with minor differences in hyperparameters to the FMNIST CNN 2 architecture. Further, for evaluating on a simple Multilayer Perceptron Network, a Neural Network with two hidden layers of sizes 512 nodes each is used. The details of the exact architectures used in the experiments are given in Table 2."
    },
    {
      "heading": "4 FAULT TOLERANCE AND ADVERSARIAL ROBUSTNESS",
      "text": ""
    },
    {
      "heading": "4.1 Input Noise as a Regularizer",
      "text": "Adding noise to inputs has been shown to provide a regularization effect that theoretically is equivalent to Tikhonov regularization [4]. To understand the effect on input noise to generalization, a simple architecture of 500 hidden layers is considered for a binary classification problem and differentiating two types of circles3. Here, two types of noise are considered, namely, additive Gaussian noise and the multiplicative Gaussian noise which are commonly used for enhancing the fault tolerance of Neural Networks by improving the generalization [16] [17] [23]. This indicates that adding noise enhances for fault tolerance for small noise values beyond which the model overfits and the performance degrades."
    },
    {
      "heading": "No Noise",
      "text": "In the case of additive noise, on adding a small noise of a standard deviation of 0.01, the generalization performance on the binary classification problem improves (Table 3). However, on increasing the values of the standard deviation, the model starts to overfit the noisy training data and the performance starts to decline. A similar phenomenon is observed on training models in the presence of worst-case adversarial noise as shown in the subsequent sections."
    },
    {
      "heading": "4.2 Adversarial Noise",
      "text": "On adding adversarial noise, the goal is to estimate the extent of overfitting for different training algorithms which will indicate the impact on fault tolerance. As shown in Figure 1, the model on training using adversarially robust algorithm overfits significantly compared to model trained using natural Stochastic Gradient Descent. In this particular architecture, the generalization error of robust models is about 17% compared to 9% error of naturally trained model.\nFor CIFAR10 dataset, the performance for robust training using TRADES is evaluated and compared with the baseline of Tikhonov regularization. Tikhonov regularized models are used as a baseline since, training with input noise is shown as theoretically equivalent to Tikhonov function in the objective function. As shown in Table 4, adding adversarial noise results in increase in the overall generalization error (extent of model overfitting).\nFor the FashionMNIST dataset, the generalization error for the TRADES adversarial training algorithm is evaluated. As shown in Table 5, the generalization error for the models trained using adversarial noise is significantly higher compared to the generalization\nof the Tikhonov regularization and naturally trained model without any additional optimizations. This indicates that adversarially computed noise, in fact, has a negative impact on the fault tolerance. On evaluating the generalization error on Multilayer Perceptron, the error increases from 3.29% for regularized model to 8.83% for robust models. A similar pattern is observed for a more complex Convolutional Neural Network, with the generalization error increases from 4.90% for a regularized model to 8.01% for robust models."
    },
    {
      "heading": "4.3 Comparing Fault Tolerance through Parameter Distribution",
      "text": "Alternatively, another approach to estimate the fault tolerance is by evaluating the standard deviation of the trained model\u2019s parameter distribution [10]. The standard deviation of the parameters \u03b8 of the model is written as,\n\u03c3 =\n\u221a\u2211 i |\u03b8i \u2212 \u03b8\u0304 |\nN (10)\nwhere \u03b8\u0304 is the average of all the parameter values and N is the total number of parameters in the model. Higher the standard deviation of the parameter distribution, more varied are the parameter values due to which some nodes are given more importance over the others. Here, the loss of those important nodes in case of random faults results in a significant drop in accuracy. For low \u03c3 , the parameters give equal weightage to all the nodes and hence, the loss of a few nodes does not impact the overall model performance.\nIn Figure 2 (left) for the CIFAR10 dataset, the standard deviation of the adversarially robust model is 0.125378 compared to 0.032451 of the regularized model parameter distribution. For FashionMNIST dataset (Figure 2 (right)), the standard deviation of the parameter distribution for robust model is 0.17901 while the deviation for regularized model is 0.023772. This indicates that the fault tolerance of models trained using adversarial noise is significantly less than Tikhonov regularized model or naturally trained model."
    },
    {
      "heading": "4.4 Impact of Varying \u03f5adv",
      "text": "An important study is to evaluate the impact of increasing the overall range of perturbation added to the inputs, i.e, \u03f5adv . Increasing \u03f5adv , results in increasing the overall noise region from which the noise can be sampled. Thus, this results in increasing the strength of the noise added to the input.\nAs seen in Table 6, the overall fault tolerance measured as the difference in training and testing accuracy increases with an increase in the noise budget \u03f5adv . Specifically, for the case of Convolutional Neural Network trained on CIFAR10 dataset, increasing \u03f5adv from 2/255 to 8/255, the generalization error increases (equivalently fault tolerance decreases) from 8.19% to 17.22%."
    },
    {
      "heading": "4.5 Simulation of Faults for Adversarially Robust Models",
      "text": "Since the fault model considered in the paper is random faults resulting in stuck at \"0\" values, these faults are simulated for adversarially robust models and compared with Tikhonov regularized models. As seen in Figure 3, on increasing the faults into the parameters from 50% to 90%, the accuracy drop in the case of the regularized model is 1.14% compared to 26.93% of the robust model. This confirms the above analysis, indicating that adversarially robust models are less fault-tolerant to regularized models."
    },
    {
      "heading": "5 DIFFERENTIAL PRIVACY AND FAULT TOLERANCE",
      "text": "In this section, the impact of adding noise to the gradients, to mitigate inference attacks via Differential Privacy, on fault tolerance is considered.\nFor the Convolutional Neural Network (Table 7), a clear tradeoff between the generalization error (fault tolerance) and accuracy\ncan be seen. Higher fault tolerance can be achieved at the cost of low test accuracy. This trade-off is also observed by other standard functions such as L1 and L2 regularizers, however, they do not provide privacy guarantees. As the privacy leakage bound \u03f5dp is increased from 0.49 to 106, the generalization error increases from 0.75% to 4.40%. An increase of \u03f5dp indicates more information leakage. This indicates that fault tolerance and privacy are highly correlated with each other, i.e, increasing the privacy (lowering \u03f5dp ) will also increase the overall fault tolerance at the cost of test accuracy.\nAs seen in Table 8 for MLP based model, a similar pattern is observed where the generalization error increases from 1.01% to 8.29% as the values of \u03f5dp increases.\nTHEOREM 1. Given a Machine Learning Model trained using (\u03f5dp , \u03b4dp )-Differential Privacy, the model\u2019s fault tolerance metric, given by the generalization error, is bounded by e\u03f5dp \u2212 1 + \u03b4dp .\nProof Sketch. Differential Privacy is a strong notion of stability where the change in the data point in the training data should not change the final output. Further, fault tolerance is also a notion of stability where a change in the model architecture should not change the final output. A Differentially private mechanism is also uniform RO stable and the generalization error of the mechanism can be bounded by e\u03f5dp \u2212 1+\u03b4dp [37]. Since generalization error is used to measure the relative fault tolerance between different models, the corresponding fault tolerance is bounded by e\u03f5dp \u2212 1 + \u03b4dp .\nProof. Given the data population P of all possible input and output pairs, the model is trained on a subset of data Dtr ain sampled from P by minimising the training error,\nEtr ain = 1\nntr ain \u2211 i l(f (\u03b8, xi ),yi ) (11)\nIn order to evaluate the performance on any possible sample that the model might encounter, we evaluate the error on the testing dataset Dtest sampled from P , where Dtest \u2229 Dtr ain = \u03d5.\nEtest = 1\nntest \u2211 i \u2032 l(f (\u03b8, xi \u2032),yi \u2032) (12)\nThe generalization error is given by the difference between the testing (Etest ) and training error (Etr ain).\nA mechanism which satisfies (\u03f5dp ,\u03b4dp )-Differential Privacy also satisfies uniform RO stability [37]. Hence, for datasets D and D\u2019 differing by a single data point,\n|ED \u2212 ED\u2032 | \u2264 e \u03f5dp \u2212 1 + \u03b4dp (13)\nFurther, generalizing this result for the training dataset and testing dataset,\n|Etr ain \u2212 Etest | \u2264 e \u03f5dp \u2212 1 + \u03b4dp (14)\nSince the fault tolerance is measured as the difference in the training and testing error, we can see that this is bounded by e\u03f5dp\u22121+\u03b4dp on training the model with (\u03f5dp ,\u03b4dp )-Differential Privacy. This result on provable bound on generalization error is based on the folklore theorem by Frank McSherry. For small values of \u03f5dp , e\u03f5dp \u2248 1+\u03f5dp and hence, e\n\u03f5dp \u22121+\u03b4dp can be written as \u03f5dp +\u03b4dp which is agreement with folklore theorem. Hence, training for privacy objective using Differential privacy provides an alternate approach for enhancing fault tolerance with a provable bound on the generalization error."
    },
    {
      "heading": "6 CONCLUSIONS",
      "text": "Designing a trustworthy Machine Learning system requires to understand the trade-offs between different aspects of trust. This work highlights the trade-offs between three such aspects of trust in Machine Learning, namely, reliability, privacy, and adversarial robustness. This work considers two adversarial settings, with a security threat model where the adversary aims to force the model to misclassify by adding adversarial noise to the input, and a privacy threat model where the adversary aims to infer whether a data point was part of the sensitive training data or not. Under the security threat model, the impact of fault tolerance on adversarially robust Neural Networks is evaluated and robust Neural Networks are observed to have lower the fault tolerance due to overfitting. Under the privacy threat model, it is shown that Differentially Private models exhibit fault tolerance for a careful choice of privacy parameters (\u03f5dp ,\u03b4dp ). Hence, fault tolerance can be achieved by training models with privacy objective. Theoretically, the bound on the model\u2019s generalization error is shown in terms of the parameters for Differential Privacy. This study is a crucial step towards understanding the design of trustworthy Machine Learning systems."
    },
    {
      "heading": "ACKNOWLEDGEMENT",
      "text": "Valentina E. Balas would like to thank the European Research Development Fund under the Competitiveness Operational Program (BioCell-NanoART = Novel Bio-inspired Cellular Nano-architectures, POC-A1-A1.1.4-E nr. 30/2016) for supporting the research."
    }
  ],
  "title": "Fault Tolerance of Neural Networks in Adversarial Settings",
  "year": 2020
}
