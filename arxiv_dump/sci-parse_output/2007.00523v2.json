{"abstractText": "Deep learning bears promise for drug discovery, including advanced image analysis, prediction of molecular structure and function, and automated generation of innovative chemical entities with bespoke properties. Despite the growing number of successful prospective applications, the underlying mathematical models often remain elusive to interpretation by the human mind. There is a demand for \u2018explainable\u2019 deep learning methods to address the need for a new narrative of the machine language of the molecular sciences. This review summarizes the most prominent algorithmic concepts of explainable artificial intelligence, and dares a forecast of the future opportunities, potential applications, and remaining challenges.", "authors": [{"affiliations": [], "name": "Jos\u00e9 Jim\u00e9nez-Luna"}, {"affiliations": [], "name": "Francesca Grisoni"}, {"affiliations": [], "name": "Gisbert Schneider"}], "id": "SP:6ac1863d9cec316639668a99e4a383cc6a5fd0c6", "references": [{"authors": ["E. Gawehn", "J.A. Hiss", "G. Schneider"], "title": "Deep learning in drug discovery", "venue": "Molecular Informatics 35,", "year": 2016}, {"authors": ["L. Zhang", "J. Tan", "D. Han", "H. Zhu"], "title": "From machine learning to deep learning: progress in machine intelligence for rational drug discovery", "venue": "Drug Discovery Today", "year": 2017}, {"authors": ["H. Chen", "O. Engkvist", "Y. Wang", "M. Olivecrona", "T. Blaschke"], "title": "The rise of deep learning in drug discovery", "venue": "Drug Discovery Today", "year": 2018}, {"authors": ["W. Tang", "J. Chen", "Z. Wang", "H. Xie", "H. Hong"], "title": "Deep learning for predicting toxicity of chemicals: A mini review", "venue": "Journal of Environmental Science and Health, Part C", "year": 2018}, {"authors": ["X. Yang", "Y. Wang", "R. Byrne", "G. Schneider", "S. Yang"], "title": "Concepts of artificial intelligence for computer-assisted drug discovery", "venue": "Chemical Reviews 119,", "year": 2019}, {"authors": ["Muratov", "E. N"], "title": "QSAR without borders", "venue": "Chemical Society Reviews", "year": 2020}, {"authors": ["J. Hemmerich", "G.F. Ecker"], "title": "In silico toxicology: From structure-activity relationships towards deep learning and adverse outcome pathways", "venue": "WIREs Computational Molecular Science 10,", "year": 2020}, {"authors": ["J. Schmidhuber"], "title": "Deep learning in neural networks: An overview", "venue": "Neural Networks", "year": 2015}, {"authors": ["Lenselink", "E. B"], "title": "Beyond the hype: Deep neural networks outperform established methods using a chembl bioactivity benchmark set", "venue": "Journal of Cheminformatics", "year": 2017}, {"authors": ["G.B. Goh", "C. Siegel", "A. Vishnu", "N.O. Hodas", "N. Baker"], "title": "Chemception: a deep neural network with minimal chemistry knowledge matches the performance of expert-developed QSAR/QSPR models", "year": 2017}, {"authors": ["T Unterthiner"], "title": "Deep learning as an opportunity in virtual screening", "venue": "In Proceedings of the Deep Learning Workshop at NIPS,", "year": 2014}, {"authors": ["I. Wallach", "M. Dzamba", "A. Heifets"], "title": "Atomnet: a deep convolutional neural network for bioactivity prediction in structurebased drug discovery", "year": 2015}, {"authors": ["O. M\u00e9ndez-Lucio", "B. Baillif", "Clevert", "D.-A", "D. Rouqui\u00e9", "J. Wichard"], "title": "De novo generation of hit-like molecules from gene expression signatures using artificial intelligence", "venue": "Nature Communications", "year": 2020}, {"authors": ["D. Merk", "L. Friedrich", "F. Grisoni", "G. Schneider"], "title": "De novo design of bioactive small molecules by artificial intelligence", "venue": "Molecular Informatics", "year": 2018}, {"authors": ["A Zhavoronkov"], "title": "Deep learning enables rapid identification of potent DDR1 kinase inhibitors", "venue": "Nature Biotechnology", "year": 2019}, {"authors": ["P. Schwaller", "T. Gaudin", "D. Lanyi", "C. Bekas", "T. Laino"], "title": "Found in translation\u2019: Predicting outcomes of complex organic chemistry reactions using neural sequence-to-sequence models", "venue": "Chemical Science", "year": 2018}, {"authors": ["C.W. Coley", "W.H. Green", "K.F. Jensen"], "title": "Machine learning in computer-aided synthesis planning", "venue": "Accounts of Chemical Research", "year": 2018}, {"authors": ["Coley", "C. W"], "title": "A graph-convolutional neural network model for the prediction of chemical reactivity", "venue": "Chemical Science", "year": 2019}, {"authors": ["Senior", "A. W"], "title": "Improved protein structure prediction using potentials from deep learning", "venue": "Nature 577,", "year": 2020}, {"authors": ["J Yang"], "title": "Improved protein structure prediction using predicted interresidue orientations", "venue": "Proceedings of the National Academy of Sciences U.S.A", "year": 2020}, {"authors": ["H. \u00d6zt\u00fcrk", "A. \u00d6zg\u00fcr", "E. Ozkirimli"], "title": "DeepDTA: Deep drug\u2013 target binding affinity prediction", "venue": "Bioinformatics 34,", "year": 2018}, {"authors": ["X Zeng"], "title": "Target identification among known drugs by deep learning from heterogeneous networks", "venue": "Chemical Science", "year": 2020}, {"authors": ["J Jimenez"], "title": "Pathwaymap: Molecular pathway association with self-normalizing neural networks", "venue": "Journal of Chemical Information and Modeling", "year": 2018}, {"authors": ["R.L. Marchese Robinson", "A. Palczewska", "J. Palczewski", "N. Kidley"], "title": "Comparison of the predictive performance and interpretability of random forest and linear models on benchmark data sets", "venue": "Journal of Chemical Information and Modeling", "year": 2017}, {"authors": ["S.J. Webb", "T. Hanser", "B. Howlin", "P. Krause", "J.D. Vessey"], "title": "Feature combination networks for the interpretation of statistical machine learning models: Application to Ames mutagenicity", "venue": "Journal of Cheminformatics", "year": 2014}, {"authors": ["F. Grisoni", "V. Consonni", "D. Ballabio"], "title": "Machine learning consensus to predict the binding to the Androgen receptor within the CoMPARA project", "venue": "Journal of Chemical Information and Modeling", "year": 2019}, {"authors": ["P Polishchuk"], "title": "Structural and physico-chemical interpretation (SPCI) of QSAR models and its comparison with matched molecular pair analysis", "venue": "Journal of Chemical Information and Modeling", "year": 2016}, {"authors": ["V.E. Kuz\u2019min", "P.G. Polishchuk", "A.G. Artemenko", "S.A. Andronati"], "title": "Interpretation of QSAR models based on random forest methods", "venue": "Molecular Informatics", "year": 2011}, {"authors": ["Y. Chen", "C. Stork", "S. Hirte", "J. Kirchmair"], "title": "NP-scout: Machine learning approach for the quantification and visualization of the natural product-likeness of small molecules", "venue": "Biomolecules 9,", "year": 2019}, {"authors": ["L. Rosenbaum", "G. Hinselmann", "A. Jahn", "A. Zell"], "title": "Interpreting linear support vector machine models with heat map molecule coloring", "venue": "Journal of Cheminformatics 3,", "year": 2011}, {"authors": ["S. Riniker", "G.A. Landrum"], "title": "Similarity maps-a visualization strategy for molecular fingerprints and machine-learning methods", "venue": "Journal of Cheminformatics 5,", "year": 2013}, {"authors": ["F. Grisoni", "V. Consonni", "M. Vighi", "S. Villa", "R. Todeschini"], "title": "Investigating the mechanisms of bioconcentration through QSAR classification trees", "venue": "Environment International 88,", "year": 2016}, {"authors": ["R. Todeschini", "M. Lasagni", "E. Marengo"], "title": "New molecular descriptors for 2D and 3D structures", "venue": "Theory. Journal of Chemometrics", "year": 1994}, {"authors": ["C. Rudin"], "title": "Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead", "venue": "Nature Machine Intelligence", "year": 2019}, {"authors": ["G. Schneider"], "title": "Mind and machine in drug design", "venue": "Nature Machine Intelligence", "year": 2019}, {"authors": ["Z.C. Lipton"], "title": "The mythos of model interpretability", "venue": "Queue 16,", "year": 2018}, {"authors": ["W.J. Murdoch", "C. Singh", "K. Kumbier", "R. Abbasi-Asl", "B. Yu"], "title": "Interpretable machine learning: definitions, methods, and applications", "year": 1901}, {"authors": ["F. Doshi-Velez", "B. Kim"], "title": "Towards a rigorous science of interpretable machine learning", "venue": "arXiv preprint arXiv:1702.08608", "year": 2017}, {"authors": ["S Lapuschkin"], "title": "Unmasking clever Hans predictors and assessing what machines really learn", "venue": "Nature Communications 10,", "year": 2019}, {"authors": ["T. Miller"], "title": "Explanation in artificial intelligence: Insights from the social sciences", "venue": "Artificial Intelligence 267,", "year": 2019}, {"authors": ["Y. Xu", "J. Pei", "L. Lai"], "title": "Deep learning based regression and multiclass models for acute oral toxicity prediction with automatic chemical feature extraction", "venue": "Journal of Chemical Information and Modeling", "year": 2017}, {"authors": ["H.L. Ciallella", "H. Zhu"], "title": "Advancing computational toxicology in the big data era by artificial intelligence: Data-driven and mechanism-driven modeling for chemical toxicity", "venue": "Chemical Research in Toxicology", "year": 2019}, {"authors": ["S. Dey", "H. Luo", "A. Fokoue", "J. Hu", "P. Zhang"], "title": "Predicting adverse drug reactions through interpretable deep learning framework", "venue": "BMC Bioinformatics", "year": 2018}, {"authors": ["C Yan"], "title": "Interpretable retrosynthesis prediction in two steps. ChemRxiv preprint (2020)", "year": 2020}, {"authors": ["R.P. Sheridan"], "title": "Interpretation of QSAR models by coloring atoms according to changes in predicted activity: How robust is it", "venue": "Journal of Chemical Information and Modeling", "year": 2019}, {"authors": ["M Manica"], "title": "Toward explainable anticancer compound sensitivity prediction via multimodal attention-based convolutional encoders", "venue": "Molecular Pharmaceutics", "year": 2019}, {"authors": ["P. \u017duvela", "J. David", "M.W. Wong"], "title": "Interpretation of annbased QSAR models for prediction of antioxidant activity of flavonoids", "venue": "Journal of Computational Chemistry", "year": 2018}, {"authors": ["K. Preuer", "G. Klambauer", "F. Rippmann", "S. Hochreiter", "T. Unterthiner"], "title": "Interpretable deep learning in drug discovery, 331\u2013345 (Springer, 2019)", "year": 2019}, {"authors": ["M. Gupta", "H.J. Lee", "C.J. Barden", "D.F. Weaver"], "title": "The blood\u2013brain barrier (BBB) score", "venue": "Journal of Medicinal Chemistry", "year": 2019}, {"authors": ["Z. Rankovic"], "title": "CNS physicochemical property space shaped by a diverse set of molecules with experimentally determined exposure in the mouse brain: Miniperspective", "venue": "Journal of Medicinal Chemistry", "year": 2017}, {"authors": ["T.T. Wager", "X. Hou", "P.R. Verhoest", "A. Villalobos"], "title": "Central nervous system multiparameter optimization desirability: Application in drug discovery", "venue": "ACS Chemical Neuroscience", "year": 2016}, {"authors": ["T.J. Ritchie", "S.J. Macdonald", "S. Peace", "S.D. Pickett", "C.N. Luscombe"], "title": "Increasing small molecule drug developability in sub-optimal chemical space", "venue": "MedChemComm 4,", "year": 2013}, {"authors": ["P.D. Leeson", "R.J. Young"], "title": "Molecular property design: Does everyone get it", "venue": "ACS Medicinal Chemistry Letters", "year": 2015}, {"authors": ["T. Fujita", "D.A. Winkler"], "title": "Understanding the roles of the \u00e2\u0102IJtwo QSARs\u00e2\u0102\u0130", "venue": "Journal of Chemical Information and Modeling", "year": 2016}, {"authors": ["P Schneider"], "title": "Rethinking drug design in the artificial intelligence era", "venue": "Nature Reviews Drug Discovery 19,", "year": 2020}, {"authors": ["R Guidotti"], "title": "A survey of methods for explaining black box models", "venue": "ACM Computing Surveys (CSUR) 51,", "year": 2018}, {"authors": ["Lundberg", "S. M"], "title": "From local explanations to global understanding with explainable AI for trees", "venue": "Nature Machine Intelligence", "year": 2020}, {"authors": ["K. McCloskey", "A. Taly", "F. Monti", "M.P. Brenner", "L.J. Colwell"], "title": "Using attribution to decode binding mechanism in neural network models for chemistry", "venue": "Proceedings of the National Academy of Sciences U.S.A", "year": 2019}, {"authors": ["R. Rodr\u00edguez-P\u00e9rez", "J. Bajorath"], "title": "Interpretation of compound activity predictions from complex machine learning models using local approximations and Shapley values", "venue": "Journal of Medicinal Chemistry", "year": 2019}, {"authors": ["R. Rodr\u00edguez-P\u00e9rez", "J. Bajorath"], "title": "Interpretation of machine learning models using Shapley values: application to compound potency and multi-target activity predictions", "venue": "Journal of Computer-Aided Molecular Design", "year": 2020}, {"authors": ["P.E. Pope", "S. Kolouri", "M. Rostami", "C.E. Martin", "H. Hoffmann"], "title": "Explainability methods for graph convolutional neural networks", "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "year": 2019}, {"authors": ["J. Hochuli", "A. Helbling", "T. Skaist", "M. Ragoza", "D.R. Koes"], "title": "Visualizing convolutional neural network protein-ligand scoring", "venue": "Journal of Molecular Graphics and Modelling", "year": 2018}, {"authors": ["S. Ishida", "K. Terayama", "R. Kojima", "K. Takasu", "Y. Okuno"], "title": "Prediction and interpretable visualization of retrosynthetic reactions using graph convolutional networks", "venue": "Journal of Chemical Information and Modeling", "year": 2019}, {"authors": ["C Shang"], "title": "Edge attention-based multi-relational graph convolutional networks", "venue": "arXiv preprint arXiv:1802.04944", "year": 2018}, {"authors": ["S. Ryu", "J. Lim", "S.H. Hong", "W.Y. Kim"], "title": "Deeply learning molecular structure-property relationships using attention-and gate-augmented graph convolutional network", "year": 2018}, {"authors": ["P Schwaller"], "title": "Molecular transformer: A model for uncertainty-calibrated chemical reaction prediction", "venue": "ACS Central Science", "year": 2019}, {"authors": ["Y. Zhang", "A.A. Lee"], "title": "Bayesian semi-supervised learning for uncertainty-calibrated prediction of molecular properties and active learning", "venue": "Chemical Science", "year": 2019}, {"authors": ["R. Liu", "H. Wang", "K.P. Glover", "M.G. Feasel", "A. Wallqvist"], "title": "Dissecting machine-learning prediction of molecular activity: Is an applicability domain needed for quantitative structure\u2013 activity relationship models based on deep neural networks", "venue": "Journal of Chemical Information and Modeling", "year": 2019}, {"authors": ["M. Sundararajan", "A. Taly", "Q. Yan"], "title": "Axiomatic attribution for deep networks", "venue": "In Proceedings of the 34th International Conference on Machine Learning-Volume 70,", "year": 2017}, {"authors": ["D. Smilkov", "N. Thorat", "B. Kim", "F. Vi\u00e9gas", "M. Wattenberg"], "title": "Smoothgrad: Removing noise by adding noise", "venue": "arXiv preprint arXiv:1706.03825", "year": 2017}, {"authors": ["J. Adebayo", "J. Gilmer", "I. Goodfellow", "B. Kim"], "title": "Local explanation methods for deep neural networks lack sensitivity to parameter values", "year": 2018}, {"authors": ["D.E. Rumelhart", "G.E. Hinton", "R.J. Williams"], "title": "Learning representations by back-propagating errors", "venue": "Nature 323,", "year": 1986}, {"authors": ["J Adebayo"], "title": "Sanity checks for saliency maps", "venue": "In Advances in Neural Information Processing Systems,", "year": 2018}, {"authors": ["S. Lipovetsky", "M. Conklin"], "title": "Analysis of regression in game theory approach", "venue": "Applied Stochastic Models in Business and Industry", "year": 2001}, {"authors": ["S.M. Lundberg", "Lee", "S.-I"], "title": "A unified approach to interpreting model predictions", "venue": "In Advances in Neural Information Processing Systems,", "year": 2017}, {"authors": ["M.T. Ribeiro", "S. Singh", "C. Guestrin"], "title": "why should i trust you?\" explaining the predictions of any classifier", "venue": "In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "year": 2016}, {"authors": ["A. Shrikumar", "P. Greenside", "A. Kundaje"], "title": "Learning important features through propagating activation differences", "venue": "In Proceedings of the 34th International Conference on Machine Learning-Volume", "year": 2017}, {"authors": ["S Bach"], "title": "On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation", "venue": "PLOS ONE 10,", "year": 2015}, {"authors": ["H. Lakkaraju", "E. Kamar", "R. Caruana", "J. Leskovec"], "title": "Interpretable & explorable approximations of black box models", "venue": "arXiv preprint arXiv:1707.01154", "year": 2017}, {"authors": ["H. Deng"], "title": "Interpreting tree ensembles with intrees", "venue": "International Journal of Data Science and Analytics", "year": 2019}, {"authors": ["O. Bastani", "C. Kim", "H. Bastani"], "title": "Interpreting blackbox models via model extraction", "venue": "arXiv preprint arXiv:1705.08504", "year": 2017}, {"authors": ["H.R. Maier", "G.C. Dandy"], "title": "The use of artificial neural networks for the prediction of water quality parameters", "venue": "Water Resources Research", "year": 1996}, {"authors": ["G. Balls", "D. Palmer-Brown", "G. Sanders"], "title": "Investigating microclimatic influences on ozone injury in clover (Trifolium subterraneum) using artificial neural networks", "venue": "New Phytologist 132,", "year": 1996}, {"authors": ["A. Sung"], "title": "Ranking importance of input parameters of neural networks", "venue": "Expert Systems with Applications 15,", "year": 1998}, {"authors": ["E. \u0160trumbelj", "I. Kononenko", "M.R. \u0160ikonja"], "title": "Explaining instance classifications with interactions of subsets of feature values", "venue": "Data & Knowledge Engineering", "year": 2009}, {"authors": ["R.C. Fong", "A. Vedaldi"], "title": "Interpretable explanations of black boxes by meaningful perturbation", "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "year": 2017}, {"authors": ["J.D. Olden", "D.A. Jackson"], "title": "Illuminating the \u00e2\u0102IJblack box\u00e2\u0102\u0130: A randomization approach for understanding variable contributions in artificial neural networks", "venue": "Ecological Modelling 154,", "year": 2002}, {"authors": ["L.M. Zintgraf", "T.S. Cohen", "T. Adel", "M. Welling"], "title": "Visualizing deep neural network decisions: Prediction difference analysis", "year": 2017}, {"authors": ["M. Ancona", "E. Ceolini", "C. \u00d6ztireli", "M. Gross"], "title": "Towards better understanding of gradient-based attribution methods for deep neural networks", "year": 2017}, {"authors": ["Selvaraju", "R. R"], "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization", "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "year": 2017}, {"authors": ["J Zhang"], "title": "Top-down neural attention by excitation backprop", "venue": "International Journal of Computer Vision 126,", "year": 2018}, {"authors": ["R.R. Tice", "C.P. Austin", "R.J. Kavlock", "J.R. Bucher"], "title": "Improving the human hazard characterization of chemicals: A Tox21 update", "venue": "Environmental Health Perspectives 121,", "year": 2013}, {"authors": ["J. Jim\u00e9nez-Luna", "M. Skalic", "G. Martinez-Rosell", "G. De Fabritiis"], "title": "KDEEP: Protein\u2013ligand absolute binding affinity prediction via 3D-convolutional neural networks", "venue": "Journal of Chemical Information and Modeling", "year": 2018}, {"authors": ["J Jim\u00e9nez-Luna"], "title": "DeltaDelta neural networks for lead optimization of small molecule potency", "venue": "Chemical Science", "year": 2019}, {"authors": ["F Doshi-Velez"], "title": "Accountability of AI under the law: The role of explanation", "venue": "arXiv preprint arXiv:1711.01134", "year": 2017}, {"authors": ["M.T. Ribeiro", "S. Singh", "C. Guestrin"], "title": "Anchors: Highprecision model-agnostic explanations", "venue": "In Thirty-Second AAAI Conference on Artificial Intelligence", "year": 2018}, {"authors": ["S. Wachter", "B. Mittelstadt", "C. Russell"], "title": "Counterfactual explanations without opening the black box: Automated decisions and the GDPR", "venue": "Harv. JL & Tech. 31,", "year": 2017}, {"authors": ["A. Van Looveren", "J. Klaise"], "title": "Interpretable counterfactual explanations guided by prototypes", "venue": "arXiv preprint arXiv:1907.02584", "year": 2019}, {"authors": ["M.A. Kramer"], "title": "Nonlinear principal component analysis using autoassociative neural networks", "venue": "AIChE Journal 37,", "year": 1991}, {"authors": ["A Dhurandhar"], "title": "Explanations based on the missing: Towards contrastive explanations with pertinent negatives", "venue": "In Advances in Neural Information Processing Systems,", "year": 2018}, {"authors": ["A. Herman"], "title": "Are you visually intelligent? What you don\u2019t see is as important as what you do see", "venue": "Medical Daily", "year": 2016}, {"authors": ["H. Zou", "T. Hastie"], "title": "Regularization and variable selection via the elastic net", "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology)", "year": 2005}, {"authors": ["A. Mousavi", "G. Dasarathy", "R.G. Baraniuk"], "title": "Deepcodec: Adaptive sensing and recovery via deep convolutional neural networks", "year": 2017}, {"authors": ["D. Stumpfe", "J. Bajorath"], "title": "Exploring activity cliffs in medicinal chemistry: Miniperspective", "venue": "Journal of Medicinal Chemistry", "year": 2012}, {"authors": ["M Cruz-Monteagudo"], "title": "Activity cliffs in drug discovery: Dr", "venue": "Jekyll or Mr. Hyde? Drug Discovery Today", "year": 2014}, {"authors": ["D.A. Erlanson"], "title": "Introduction to fragment-based drug discovery", "venue": "(Springer, Berlin,", "year": 2011}, {"authors": ["R. Todeschini", "V. Consonni"], "title": "Molecular descriptors for chemoinformatics: volume I: alphabetical listing/volume II: appendices", "venue": "references, vol. 41 (Wileya\u0302A\u0306R\u030cVCH, Weinheim,", "year": 2009}, {"authors": ["M. Randi\u0107", "G.M. Brissey", "R.B. Spencer", "C.L. Wilkins"], "title": "Search for all self-avoiding paths for molecular graphs", "venue": "Computers & Chemistry", "year": 1979}, {"authors": ["L.B. Kier"], "title": "A shape index from molecular graphs", "venue": "Quantitative Structure-Activity Relationships", "year": 1985}, {"authors": ["M. Randi\u0107"], "title": "On unique numbering of atoms and unique codes for molecular graphs", "venue": "Journal of Chemical Information and Computer Sciences", "year": 1975}, {"authors": ["D. Bonchev", "N. Trinajsti\u0107"], "title": "Information theory, distance matrix, and molecular branching", "venue": "The Journal of Chemical Physics 67,", "year": 1977}, {"authors": ["Duvenaud", "D. K"], "title": "Convolutional networks on graphs for learning molecular fingerprints", "venue": "In Advances in Neural Information Processing Systems,", "year": 2015}, {"authors": ["J. Gilmer", "S.S. Schoenholz", "P.F. Riley", "O. Vinyals", "G.E. Dahl"], "title": "Neural message passing for quantum chemistry", "venue": "In Proceedings of the 34th International Conference on Machine Learning-Volume", "year": 2017}, {"authors": ["T.N. Kipf", "M. Welling"], "title": "Semi-supervised classification with graph convolutional networks", "venue": "arXiv preprint arXiv:1609.02907", "year": 2016}, {"authors": ["H. Nguyen", "Maeda", "S.-i", "K. Oono"], "title": "Semi-supervised learning of hierarchical representations of molecules using neural message passing", "year": 2017}, {"authors": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "title": "ImageNet classification with deep convolutional neural networks", "venue": "In Advances in Neural Information Processing Systems,", "year": 2012}, {"authors": ["Y. Kim"], "title": "Convolutional neural networks for sentence classification", "venue": "arXiv preprint arXiv:1408.5882", "year": 2014}, {"authors": ["S. Kearnes", "K. McCloskey", "M. Berndl", "V. Pande", "P. Riley"], "title": "Molecular graph convolutions: Moving beyond fingerprints", "venue": "Journal of Computer-Aided Molecular Design", "year": 2016}, {"authors": ["Z Wu"], "title": "Moleculenet: A benchmark for molecular machine learning", "venue": "Chemical Science", "year": 2018}, {"authors": ["W. Jin", "R. Barzilay", "T. Jaakkola"], "title": "Junction tree variational autoencoder for molecular graph generation", "venue": "arXiv preprint arXiv:1802.04364", "year": 2018}, {"authors": ["Z. Ying", "D. Bourgeois", "J. You", "M. Zitnik", "J. Leskovec"], "title": "GNNExplainer: Generating explanations for graph neural networks", "venue": "In Advances in Neural Information Processing Systems,", "year": 2019}, {"authors": ["A.K. Debnath", "R.L. Lopez de Compadre", "G. Debnath", "A.J. Shusterman", "C. Hansch"], "title": "Structure-activity relationship of mutagenic aromatic and heteroaromatic nitro compounds. correlation with molecular orbital energies and hydrophobicity", "venue": "Journal of Medicinal Chemistry", "year": 1991}, {"authors": ["P Veli\u010dkovi\u0107"], "title": "Graph attention networks", "venue": "arXiv preprint arXiv:1710.10903", "year": 2017}, {"authors": ["D. Bahdanau", "K. Cho", "Y. Bengio"], "title": "Neural machine translation by jointly learning to align and translate", "venue": "arXiv preprint arXiv:1409.0473", "year": 2014}, {"authors": ["C Limban"], "title": "The use of structural alerts to avoid the toxicity of pharmaceuticals", "venue": "Toxicology Reports", "year": 2018}, {"authors": ["T.B. Hughes", "G.P. Miller", "S.J. Swamidass"], "title": "Site of reactivity models predict molecular reactivity of diverse chemicals with glutathione", "venue": "Chemical Research in Toxicology", "year": 2015}, {"authors": ["J Kirchmair"], "title": "Computational prediction of metabolism: sites, products, SAR, P450 enzyme dynamics, and mechanisms", "venue": "Journal of Chemical Information and Modeling", "year": 2012}, {"authors": ["C.G. Wermuth"], "title": "Selective optimization of side activities: The SOSA approach", "venue": "Drug Discovery Today 11,", "year": 2006}, {"authors": ["T. Laugel", "Lesot", "M.-J", "C. Marsala", "X. Renard", "M. Detyniecki"], "title": "The dangers of post-hoc interpretability: Unjustified counterfactual explanations", "year": 1907}, {"authors": ["D.A. Melis", "T. Jaakkola"], "title": "Towards robust interpretability with self-explaining neural networks", "venue": "In Advances in Neural Information Processing Systems,", "year": 2018}, {"authors": ["D.B. Leake"], "title": "Case-based reasoning: Experiences, lessons and future directions (MIT press", "year": 1996}, {"authors": ["B. Kim", "C. Rudin", "J.A. Shah"], "title": "The bayesian case model: A generative approach for case-based reasoning and prototype classification", "venue": "In Advances in Neural Information Processing Systems,", "year": 2014}, {"authors": ["O. Li", "H. Liu", "C. Chen", "C. Rudin"], "title": "Deep learning for casebased reasoning through prototypes: A neural network that explains its predictions", "venue": "In Thirty-Second AAAI Conference on Artificial Intelligence", "year": 2018}, {"authors": ["C Chen"], "title": "This looks like that: Deep learning for interpretable image recognition", "venue": "In Advances in Neural Information Processing Systems,", "year": 2019}, {"authors": ["N.D. Goodman", "J.B. Tenenbaum", "T. Gerstenberg"], "title": "Concepts in a probabilistic language of thought", "venue": "Tech. Rep., Center for Brains, Minds and Machines (CBMM)", "year": 2014}, {"authors": ["B.M. Lake", "R. Salakhutdinov", "J.B. Tenenbaum"], "title": "Humanlevel concept learning through probabilistic program induction", "venue": "Science 350,", "year": 2015}, {"authors": ["Z. Ghahramani"], "title": "Probabilistic machine learning and artificial intelligence", "venue": "Nature 521,", "year": 2015}, {"authors": ["O. Vinyals", "C. Blundell", "T. Lillicrap", "K. Kavukcuoglu", "D. Wierstra"], "title": "Matching networks for one shot learning", "venue": "In Advances in Neural Information Processing Systems,", "year": 2016}, {"authors": ["H. Altae-Tran", "B. Ramsundar", "A.S. Pappu", "V. Pande"], "title": "Low-data drug discovery with one-shot learning", "venue": "ACS Central Science", "year": 2017}, {"authors": ["B Kim"], "title": "Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (TCAV)", "venue": "arXiv preprint arXiv:1711.11279", "year": 2017}, {"authors": ["Gilpin", "L. H"], "title": "Explaining explanations: An overview of interpretability of machine learning", "venue": "IEEE 5th International Conference on Data Science and Advanced Analytics (DSAA),", "year": 2018}, {"authors": ["Hendricks", "L. A"], "title": "Generating visual explanations", "venue": "In European Conference on Computer Vision,", "year": 2016}, {"authors": ["S Antol"], "title": "VQA: Visual question answering", "venue": "In Proceedings of the IEEE International Conference on Computer Vision,", "year": 2015}, {"authors": ["C.E. Rasmussen"], "title": "Gaussian processes in machine learning", "venue": "In Summer School on Machine Learning,", "year": 2003}, {"authors": ["A. Nguyen", "J. Yosinski", "J. Clune"], "title": "Deep neural networks are easily fooled: High confidence predictions for unrecognizable images", "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,", "year": 2015}, {"authors": ["L.K. Hansen", "P. Salamon"], "title": "Neural network ensembles", "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence", "year": 1990}, {"authors": ["B. Lakshminarayanan", "A. Pritzel", "C. Blundell"], "title": "Simple and scalable predictive uncertainty estimation using deep ensembles", "venue": "In Advances in Neural Information Processing Systems,", "year": 2017}, {"authors": ["D.A. Freedman"], "title": "Bootstrapping regression models", "venue": "The Annals of Statistics 9,", "year": 1981}, {"authors": ["G Huang"], "title": "Snapshot ensembles: Train one, get m for free", "venue": "arXiv preprint arXiv:1704.00109", "year": 2017}, {"authors": ["R. Zhang", "C. Li", "J. Zhang", "C. Chen", "A.G. Wilson"], "title": "Cyclical stochastic gradient MCMC for bayesian deep learning", "year": 1902}, {"authors": ["A. Graves"], "title": "Practical variational inference for neural networks", "venue": "In Advances in Neural Information Processing Systems,", "year": 2011}, {"authors": ["S. Sun", "G. Zhang", "J. Shi", "R. Grosse"], "title": "Functional variational bayesian neural networks", "venue": "arXiv preprint arXiv:1903.05779", "year": 2019}, {"authors": ["Y. Gal", "Z. Ghahramani"], "title": "Dropout as a bayesian approximation: Representing model uncertainty in deep learning", "venue": "In International Conference on Machine Learning,", "year": 2016}, {"authors": ["A. Kendall", "Y. Gal"], "title": "What uncertainties do we need in bayesian deep learning for computer vision", "venue": "In Advances in Neural Information Processing Systems,", "year": 2017}, {"authors": ["M. Teye", "H. Azizpour", "K. Smith"], "title": "Bayesian uncertainty estimation for batch normalized deep networks", "venue": "arXiv preprint arXiv:1802.06455", "year": 2018}, {"authors": ["D.A. Nix", "A.S. Weigend"], "title": "Estimating the mean and variance of the target probability distribution", "venue": "In Proceedings of 1994 IEEE International Conference on Neural Networks (ICNN\u201994),", "year": 1994}, {"authors": ["G. Chryssolouris", "M. Lee", "A. Ramsey"], "title": "Confidence interval prediction for neural network models", "venue": "IEEE Transactions on Neural Networks", "year": 1996}, {"authors": ["J.G. Hwang", "A.A. Ding"], "title": "Prediction intervals for artificial neural networks", "venue": "Journal of the American Statistical Association", "year": 1997}, {"authors": ["A. Khosravi", "S. Nahavandi", "D. Creighton", "A.F. Atiya"], "title": "Lower upper bound estimation method for construction of neural network-based prediction intervals", "venue": "IEEE Transactions on Neural Networks", "year": 2010}, {"authors": ["R. Ak", "V. Vitelli", "E. Zio"], "title": "An interval-valued neural network approach for uncertainty quantification in short-term wind speed prediction", "venue": "IEEE Transactions on Neural Networks and Learning Systems", "year": 2015}, {"authors": ["H. Jiang", "B. Kim", "M. Guan", "M. Gupta"], "title": "To trust or not to trust a classifier", "venue": "In Advances in Neural Information Processing Systems,", "year": 2018}, {"authors": ["W. Huang", "D. Zhao", "F. Sun", "H. Liu", "E. Chang"], "title": "Scalable gaussian process regression using deep neural networks", "venue": "In Twenty-Fourth International Joint Conference on Artificial Intelligence", "year": 2015}, {"authors": ["R.P. Sheridan", "B.P. Feuston", "V.N. Maiorov", "S.K. Kearsley"], "title": "Similarity to molecules in the training set is a good discriminator for prediction accuracy in QSAR", "venue": "Journal of Chemical Information and Computer Sciences", "year": 2004}, {"authors": ["R. Liu", "A. Wallqvist"], "title": "Molecular similarity-based domain applicability metric efficiently identifies out-of-domain compounds", "venue": "Journal of Chemical Information and Modeling", "year": 2018}, {"authors": ["J.P. Janet", "C. Duan", "T. Yang", "A. Nandy", "H.J. Kulik"], "title": "A quantitative uncertainty metric controls error in neural network-driven chemical discovery", "venue": "Chemical Science", "year": 2019}, {"authors": ["G. Scalia", "C.A. Grambow", "B. Pernici", "Li", "Y.-P", "W.H. Green"], "title": "Evaluating scalable uncertainty estimation methods for deep learning-based molecular property prediction", "venue": "Journal of Chemical Information and Modeling", "year": 2020}, {"authors": ["O. Obrezanova", "G. Cs\u00e1nyi", "J.M. Gola", "M.D. Segall"], "title": "Gaussian processes: a method for automatic QSAR modeling of ADME properties", "venue": "Journal of Chemical Information and Modeling", "year": 2007}, {"authors": ["Schroeter", "T. S"], "title": "Estimating the domain of applicability for machine learning QSAR models: a study on aqueous solubility of drug discovery molecules", "venue": "Journal of Computer-Aided Molecular Design", "year": 2007}, {"authors": ["Clark", "R. D"], "title": "Using beta binomials to estimate classification uncertainty for ensemble models", "venue": "Journal of Cheminformatics 6,", "year": 2014}, {"authors": ["N Bosc"], "title": "Large scale comparison of QSAR and conformal prediction methods and their applications in drug discovery", "venue": "Journal of Cheminformatics 11,", "year": 2019}, {"authors": ["I. Cort\u00e9s-Ciriano", "A. Bender"], "title": "Deep confidence: A computationally efficient framework for calculating reliable prediction errors for deep neural networks", "venue": "Journal of Chemical Information and Modeling", "year": 2018}, {"authors": ["A Vaswani"], "title": "Attention is all you need", "venue": "In Advances in Neural Information Processing Systems,", "year": 2017}, {"authors": ["L. Hirschfeld", "K. Swanson", "K. Yang", "R. Barzilay", "C.W. Coley"], "title": "Uncertainty quantification using neural networks for molecular property prediction", "year": 2005}, {"authors": ["N Kokhlikyan"], "title": "Pytorch Captum. https://github.com/ pytorch/captum (2019)", "year": 2019}, {"authors": ["A Paszke"], "title": "Pytorch: An imperative style, highperformance deep learning library", "venue": "In Advances in Neural Information Processing Systems", "year": 2019}, {"authors": ["J. Klaise", "A. Van Looveren", "G. Vacanti", "A. Coca"], "title": "Alibi: Algorithms for monitoring and explaining machine learning models", "venue": "https://github.com/SeldonIO/alibi", "year": 2020}, {"authors": ["F Pedregosa"], "title": "Scikit-learn: Machine learning in Python", "venue": "Journal of Machine Learning Research", "year": 2011}, {"authors": ["M Abadi"], "title": "Tensorflow: A system for large-scale machine learning", "venue": "In 12th USENIX Symposium on Operating Systems Design and Implementation", "year": 2016}, {"authors": ["J.D. Hirst", "R.D. King", "M.J. Sternberg"], "title": "Quantitative structure-activity relationships by neural networks and inductive logic programming. i. the inhibition of dihydrofolate reductase by pyrimidines", "venue": "Journal of Computer-Aided Molecular Design", "year": 1994}, {"authors": ["M. Fiore", "F. Sicurello", "G. Indorato"], "title": "An integrated system to represent and manage medical knowledge", "venue": "Medinfo. MED- INFO", "year": 1995}, {"authors": ["Kutchukian", "P. S"], "title": "Inside the mind of a medicinal chemist: The role of human bias in compound prioritization during drug discovery", "venue": "PLOS ONE 7,", "year": 2012}, {"authors": ["S. Boobier", "A. Osbourn", "J.B. Mitchell"], "title": "Can human experts predict solubility better than computers", "venue": "Journal of Cheminformatics 9,", "year": 2017}, {"authors": ["P. Schneider", "G. Schneider"], "title": "De novo design at the edge of chaos: Miniperspective", "venue": "Journal of Medicinal Chemistry", "year": 2016}, {"authors": ["Z.C. Lipton"], "title": "The doctor just won\u2019t accept that", "venue": "arXiv preprint arXiv:1711.08037", "year": 2017}, {"authors": ["B. Goodman", "S. Flaxman"], "title": "European Union regulations on algorithmic decision-making and a \u2018right to explanation", "venue": "AI Magazine 38,", "year": 2017}, {"authors": ["P.F. Bendassolli"], "title": "Theory building in qualitative research: reconsidering the problem of induction", "venue": "Forum Qualitative Social Research 14,", "year": 2013}, {"authors": ["H. Ikebata", "K. Hongo", "T. Isomura", "R. Maezono", "R. Yoshida"], "title": "Bayesian molecular design with a chemical language model", "venue": "Journal of Computer-Aided Molecular Design", "year": 2017}, {"authors": ["M.H. Segler", "T. Kogej", "C. Tyrchan", "M.P. Waller"], "title": "Generating focused molecule libraries for drug discovery with recurrent neural networks", "venue": "ACS Central Science", "year": 2018}, {"authors": ["D Nagarajan"], "title": "Computational antimicrobial peptide design and evaluation against multidrug-resistant clinical isolates of bacteria", "venue": "Journal of Biological Chemistry", "year": 2018}, {"authors": ["A.T. M\u00fcller", "J.A. Hiss", "G. Schneider"], "title": "Recurrent neural network model for constructive peptide design", "venue": "Journal of Chemical Information and Modeling", "year": 2018}, {"authors": ["J. Jim\u00e9nez-Luna", "A. Cuzzolin", "G. Bolcato", "M. Sturlese", "S. Moro"], "title": "A deep-learning approach toward rational molecular docking protocol selection", "venue": "Molecules 25,", "year": 2020}, {"authors": ["D. Rogers", "M. Hahn"], "title": "Extended-connectivity fingerprints", "venue": "Journal of Chemical Information and Modeling", "year": 2010}, {"authors": ["M. Awale", "Reymond", "J.-L"], "title": "Atom pair 2D-fingerprints perceive 3D-molecular shape and pharmacophores for very fast virtual screening of ZINC and GDB-17", "venue": "Journal of Chemical Information and Modeling", "year": 2014}, {"authors": ["R. Todeschini", "V. Consonni"], "title": "New local vertex invariants and molecular descriptors based on functions of the vertex degrees", "venue": "MATCH Commun. Math. Comput. Chem", "year": 2010}, {"authors": ["A.R. Katritzky", "E.V. Gordeeva"], "title": "Traditional topological indexes vs electronic, geometrical, and combined molecular descriptors in QSAR/QSPR research", "venue": "Journal of Chemical Information and Computer Sciences", "year": 1993}, {"authors": ["M. Vighi", "A. Barsi", "A. Focks", "F. Grisoni"], "title": "Predictive models in ecotoxicology: Bridging the gap between scientific progress and regulatory applicability\u00e2\u0102\u0164remarks and research needs", "venue": "Integrated Environmental Assessment and Management", "year": 2019}, {"authors": ["F Sahigara"], "title": "Comparison of different approaches to define the applicability domain of qsar models", "venue": "Molecules 17,", "year": 2012}, {"authors": ["M. Mathea", "W. Klingspohn", "K. Baumann"], "title": "Chemoinformatic classification methods and their applicability domain", "venue": "Molecular Informatics", "year": 2016}, {"authors": ["C Szegedy"], "title": "Intriguing properties of neural networks", "venue": "arXiv preprint arXiv:1312.6199", "year": 2013}, {"authors": ["D. Hendrycks", "K. Gimpel"], "title": "A baseline for detecting misclassified and out-of-distribution examples in neural networks", "venue": "arXiv preprint arXiv:1610.02136", "year": 2016}, {"authors": ["C. Hale"], "title": "The MELLODDY project", "venue": "Fierce Biotech", "year": 2019}, {"authors": ["S. Nembri", "F. Grisoni", "V. Consonni", "R. Todeschini"], "title": "In silico prediction of cytochrome P450-drug interaction: QSARs for CYP3A4 and CYP2C9", "venue": "International Journal of Molecular Sciences", "year": 2016}, {"authors": ["M Hiratsuka"], "title": "Characterization of human cytochrome p450 enzymes involved in the metabolism of cilostazol", "venue": "Drug Metabolism and Disposition", "year": 2007}, {"authors": ["R.S. Foti", "D.A. Rock", "L.C. Wienkers", "J.L. Wahlstrom"], "title": "Selection of alternative CYP3A4 probe substrates for clinical drug interaction studies using in vitro data and in vivo simulation", "venue": "Drug Metabolism and Disposition", "year": 2010}, {"authors": ["D. Waller", "A. Renwick", "B. Gruchy", "C. George"], "title": "The first pass metabolism of nifedipine in man", "venue": "British Journal of Clinical Pharmacology", "year": 1984}, {"authors": ["K.D. Raemsch", "J. Sommer"], "title": "Pharmacokinetics and metabolism of nifedipine", "venue": "Hypertension 5,", "year": 1983}, {"authors": ["Wishart", "D. S"], "title": "DrugBank 5.0: A major update to the drugbank database for 2018", "venue": "Nucleic Acids Research 46,", "year": 2018}], "sections": [{"text": "Deep learning bears promise for drug discovery, including advanced image analysis, prediction of molecular structure and function, and automated generation of innovative chemical entities with bespoke properties. Despite the growing number of successful prospective applications, the underlying mathematical models often remain elusive to interpretation by the human mind. There is a demand for \u2018explainable\u2019 deep learning methods to address the need for a new narrative of the machine language of the molecular sciences. This review summarizes the most prominent algorithmic concepts of explainable artificial intelligence, and dares a forecast of the future opportunities, potential applications, and remaining challenges."}, {"heading": "1 Introduction", "text": "Several concepts of \u2018artificial intelligence\u2019 (AI) have been adopted for computer-assisted drug discovery.1\u20137 Deep learning algorithms, i.e., artificial neural networks with multiple processing layers,8 are currently receiving particular attention, owing to their capacity to model complex nonlinear input-output relationships, and perform pattern recognition and feature extraction from low-level data representations.8,9\nCertain deep learning models have been shown to match or even exceed the performance of the existing machinelearning and quantitative structure-activity relationship (QSAR) methods for drug discovery.6,10\u201313 Moreover, deep learning has boosted the potential and broadened the applicability of computer-assisted discovery, e.g., for molecular de novo design,14\u201316 chemical synthesis planning,17\u201319 protein structure prediction,20,21 and macromolecular target identification.22\u201324 The ability to capture intricate nonlinear relationships between input data (e.g., chemical structure representations) and the associated output (e.g., assay readout) often comes at the price of limited comprehensibility of the resulting model. While there have been efforts to explain QSARs in terms of algorithmic insights and molecular descriptor analysis,25\u201334 deep neural network models notoriously elude immediate accessibility by the human mind.35,36\nIn an effort to mitigate the lack of interpretability of deep learning models, attention has been drawn to explainable artificial intelligence (XAI).37,38 Providing informative explanations of AI models aims to (i) render the underlying decision-making process transparent (\u2018under-\nstandable\u2019),39 (ii) avoid correct predictions for the wrong reasons (the so-called \u2018clever Hans effect\u201940), (iii) avert unfair biases or unethical discrimination,41 and (iv) bridge the gap between the machine learning community and other scientific disciplines. In medicinal chemistry, XAI already enables the mechanistic interpretation of drug action,42,43 and contributes to drug safety enhancement44 and organic synthesis planning.17,45 Intensified efforts towards interpreting deep learning models will help increase their reliability and foster their acceptance and usage in drug discovery and medicinal chemistry projects.46\u201349 The availability of \u2018rule of thumb\u2019 scores correlating biological effects with physicochemical properties50\u201354 underscores the willingness, in certain situations, to sacrifice accuracy in favour of models that better fit human intuition. Thus, blurring the lines between the \u2018two QSARs\u201955 (i.e., mechanistically interpretable vs. highly accurate models) can be the key for accelerated drug discovery.56\nWhile the exact definition of XAI is still under debate,41,57 several aspects of XAI are certainly desirable in drug design applications:37 (i) transparency, i.e., knowing how the system reached a particular answer, (ii) justification, i.e., elucidating why the answer provided by the model is acceptable, (iii) informativeness, i.e., providing new information to human decision-makers, and (iv) uncertainty estimation, i.e., the quantification of how reliable a prediction is. Moreover, AI model explanations can be global (i.e., summarizing the relevance of input features in the model) or local (i.e. on individual predictions).58 Furthermore, XAI can be model-dependent or agnostic, which in turn affects the potential applicability of each method.\nThe field of XAI is still in its infancy but moving forward at a fast pace, and we expect an increase of its relevance in the years to come. In this review, we aim to provide a comprehensive overview of recent XAI research, including its benefits, limitations and future opportunities for drug discovery. In what follows, after providing an introduction to the most relevant XAI methods structured into conceptual categories, the existing and some of the potential applications to drug discovery are highlighted. Finally, we discuss the limitations of contemporary XAI and point to the potential methodological improvements needed to foster practical applicability of these techniques to pharmaceutical research."}, {"heading": "2 State of the art", "text": "This section aims to provide a concise overview of modern XAI approaches, and exemplify their use in computer\nar X\niv :2\n00 7.\n00 52\n3v 2\n[ cs\n.A I]\n2 J\nul 2\n02 0\nFeature attribution Determine local feature importance towards a prediction.\n- Gradient-based Ligand pharmacophore identification,59\u201361 structural alerts for adverse effect,62 protein-ligand interaction profiling.63 - Surrogate models\n- Perturbation-based\nInstance-based Compute a subset of features that need to be present or absent to guarantee or change a prediction. - Anchors Not reported\n- Counterfactual instances\n- Contrastive explanations\nGraph-convolution-based Interpret models within the message-passing framework.\n- Sub-graph approaches Retrosynthesis elucidation,64 toxicophore and pharmacophore identification,49 ADMET,65,66 and reactivity prediction.19 - Attention-based\nSelf-explaining Develop models that are explainable by design.\n- Prototype-based Not reported\n- Self-explaining neural networks\n- Concept learning\n- Natural language explanations\nUncertainty estimation Quantify the reliability of a prediction.\n- Ensemble-based Reaction prediction,67 active learning,68 molecular activity prediction.69\n- Probabilistic\n- Other approaches\nvision, natural-language processing, and discrete mathematics. We then highlight selected case studies in drug discovery and propose potential future areas of XAI research. A summary of the methodologies and their goals, along with reported applications is provided in Table 1. In what follows, without loss of generality, f will denote a model (in most cases a neural network); x \u2208 X will be used to denote the set of features describing a given instance, which are used by f to make a prediction y \u2208 Y."}, {"heading": "2.1 Feature attribution methods", "text": "Given a regression or classification model f : x \u2208 RK \u2192 R, a feature attribution method is a function E : x \u2208 RK \u2192 RK that takes the model input and produces an output whose values denote the relevance of every input feature for the final prediction computed with f . Feature attribution methods can be grouped into the following three categories (Figure 1):\n\u2022 Gradient-based feature attribution. These approaches measure how much a change around a local neighborhood of the input x corresponds to a change in the output f(x). A common approach among deeplearning practitioners relies on the use of the derivative of the output of the neural network with re-\nspect to the input (i.e., \u03b4f\n\u03b4x ) to determine feature\nimportance.70\u201372 Its popularity arises partially from\nthe fact that this computation can be performed via backpropagation,73 the main way of computing partial first-order derivatives in neural network models. While the use of gradient-based feature attribution may seem straightforward, several methods relying on this principle have been shown to lead to only partial reconstruction of the original features,74 which is prone to misinterpretation.\n\u2022 Surrogate-model feature attribution. Given a model f , these methods aim to develop a surrogate explanatory model g, which is constructed in such a way that: (i) g is interpretable, and (ii) g approximates the original function f . A prominent example of this concept is the family of additive feature attribution methods, where the approximation is achieved through a linear combination of binary variables zi:\ng ( z \u2032\ni\n) = \u03c60 + M\u2211 i=1 \u03c6izi, (1)\nwhere zi \u2208 {0, 1}M , M is the number of original input features, \u03c6i \u2208 R are coefficients representing the importance assigned to each i -th binary variable and \u03c60 is an intercept. Several notable feature attribution methods belong to this family,75,76 such as Local Interpretable Model-Agnostic Explanations (LIME),77 Deep Learning Important FeaTures (DeepLIFT),78\nShapley Additive Explanations (SHAP)76 and LayerWise Relevance Propagation.79 Both gradient-based methods and the additive subfamily of surrogate attribution methods provide local explanations (i.e., each prediction needs to be examined individually), but they do not offer a general understanding of the underlying model f . Global surrogate explanation models aim to fill this gap by generically describing f via a decision tree or decision set80 model. If such an approximation is precise enough, these aim to to mirror the computation logic of the original model. While early attempts limited f to the family of tree-based ensemble methods (e.g., random forests81), more recent approaches are readily applicable to arbitrary deep learning models.82\n\u2022 Perturbation-based methods modify or remove parts of the input aiming to measure its corresponding change in the model output; this information is then used to assess the feature importance. Alongside the wellestablished step-wise approaches,83\u201385 methods such as feature masking,86 perturbation analysis,87 response randomization,88 and conditional multivariate models89 belong to this category. While perturbationbased methods have the advantage of directly estimating feature importance, they are computationally slow when the number of input features increases,89 and the final result tends to be strongly influenced by the number of features that are perturbed altogether.90\nFeature attribution methods have been used for ligandand structure-based drug-discovery. For instance, McCloskey et al.59 employed gradient-based attribution70 to detect ligand pharmacophores relevant for binding. The study showed that, despite good performance of the models on held-out data, these still can learn spurious correlations.59 Pope et al.62 adapted gradient-based feature attribution91,92 for the identification of relevant functional groups in adverse effect prediction.93 Recently, SHAP76\nwas used to interpret relevant features for compound potency and multi\u00e2\u0102\u015atarget activity prediction.60,61 Hochuli et al.63 compared several feature attribution methodologies, showing how the visualization of attributions assists in parsing and interpreting of protein-ligand scoring with 3D convolutional neural networks.94,95\nIt should be noted that the interpretability of feature attribution methods is limited by the original set of features (model input). In drug discovery the interpretability of machine learning methods is often hampered by the use of complex input molecular descriptors. When making use of feature attribution approaches, it is advisable to choose molecular descriptors or representations for model construction that one considers to bear \u2018interpretable meaning\u2019 (Box 1)."}, {"heading": "2.2 Instance-based approaches", "text": "Instance-based approaches compute a subset of relevant features (instances) that must be present to retain (or absent to change) the prediction of a given model (Figure 2). An instance can be real (i.e., drawn from the set of data) or generated for the purposes of the method. Instancebased approaches have been argued to provide \u2018natural\u2019 model interpretations for humans, because they resemble counterfactual reasoning (i.e. producing alternative sets of action to achieve a similar or different result).96\n\u2022 Anchor algorithms97 offer model-agnostic interpretable explanations of classifier models. They compute a subset of if -then rules based on one or more features that represent conditions to sufficiently guarantee a certain class prediction. In contrast to many other local explanation methods,77 anchors therefore explicitly model the \u2018coverage\u2019 of an explanation. Formally, an anchor A is defined as a set of rules such that, given a set of features x from a sample, they return A(x) = 1 if said rules are met, while guaranteeing the desired predicted class from f with a certain probability \u03c4 :\nED(z|A) [ 1f(x)=f(z) ] \u2265 \u03c4, (2)\nwhere D(z|A) is defined as the conditional distribution on samples where anchor A applies. This methodology has successfully been applied in several tasks such as image recognition, text classification, and visual question answering.97\n\u2022 Counterfactual instance search. Given a classifier model f and an original data point x, counterfactual instance search98 aims to find examples x\u2032 that (i) are as close to x as possible, and (ii) for which the classifier produces a different class label from the label assigned to x. In other words, a counterfactual describes small feature changes in sample x such that it is classified differently by f . The search for the set of instances x\u2032 may be cast into an optimization problem:\nmin x\u2032 max \u03bb\n(ft \u2212 pt)2 + \u03bbL1 (x\u2032, x) , (3)\nwhere ft is the prediction of the model for the t-th class, pt a user-defined target probability for the same\nx c Input Predicted class f\nx cf\nx' c'f\nModel\nAnchor\nCounterfactual\ninput (i.e., those features that guarantee a prediction with a high degree of certainty). The proposed approach uses an elastic net regularizer,103 and optionally a conditional autoencoder model104 so that the found explanations are more likely to lie closer to the original data manifold.\nTo the best of our knowledge, instance-based approaches have yet to be applied to drug-discovery. In our opinion, they bear promise in several areas of de novo molecular design, such as (i) activity cliff prediction,105,106 as they can help identify small structural variations in molecules that cause large bioactivity changes, (ii) fragment-based virtual screening,107 by highlighting a minimal subset of atoms responsible for a given observed activity), and (iii) hit-to-lead optimization, by helping identify the minimal set of structural changes required to improve one or more biological or physicochemical properties."}, {"heading": "2.3 Graph-convolution-based methods", "text": "Molecular graphs are a natural mathematical representation of molecular topology, with nodes and edges representing atoms and chemical bonds, respectively108 (Figure 3a). Furthermore, their usage has been commonplace in chemoinformatics and mathematical chemistry since the late 1970s.109\u2013112 Thus, it does not come as a surprise in these fields to witness the increasing application of novel graph-convolution neural networks,113 which formally fall under the umbrella of neural-message passing algorithms.114\u2013116 Generally speaking, convolution refers to a mathematical operation on two functions that produces a third function expressing how the shape of one is modified by the other. This concept is widely used in convolutional neural networks for image analysis. Graph convolutions naturally extend the convolution operation typically used in computer vision117 or in natural language processing118 applications to arbitrarily-sized graphs. In the context of drug discovery, graph convolutions have been applied to molecular property prediction119,120 and in generative models for de novo drug design.121\nExploring the interpretability of models trained with graph-convolution architectures is currently a particularly active research topic. For the purpose of this review, XAI methods based on graph-convolution are grouped into the following two categories:\n\u2022 Sub-graph identification approaches aim to identify one or more parts of a graph that are responsible for a given prediction (Figure 3b). GNNExplainer122 is a model-agnostic example of such category, and provides explanations for any graph-based machine learning task. Given an individual input graph, GNNExplainer identifies a connected sub-graph structure, as well a set of node-level features that are relevant for a particular prediction. The method can also provide such explanations for a group of data points belonging to the same class. GNNExplainer is formulated as an optimization problem, where a mutual information objective between the prediction of a graph neural network and the distribution of feasible subgraphs is maximized. Mathematically, given a node v, the goal is to identify a sub-graph GS \u2286 G with\nassociated features XS = {xj |vj \u2208 GS} that are relevant in explaining a target prediction y\u0302 \u2208 Y via a mutual information measure MI:\nmax GS\nMI (Y, (GS , XS)) = H(Y )\u2212\nH(Y |G = GS , X = XS). (4)\nIn practice, however, this objective is not mathematically tractable, and several continuity and convexity assumptions have to be made. GNNExplainer was tested on a set of molecules labeled for their mutagenic effect on Salmonella typhimurium,123 and identified several known mutagenic functional groups (i.e., certain aromatic and heteroaromatic rings and amino/nitro groups123) as relevant.\n\u2022 Attention-based approaches. The interpretation of graph-convolutional neural networks can benefit from attention mechanisms,124 which borrow from the natural language processing field, where their usage has become standard.125 The idea is to stack several message-passing layers to obtain hidden node-level representations, by first computing attention coefficients associated with each of the edges connected to the neighbors of a particular node in the graph (Figure 3c). Mathematically, for a given node, an attention-based graph convolution operation obtains its updated hidden representation via a normalized sum of the node-level hidden features of the topological neighbours:\nh (l+1) i = \u03c3  \u2211 j\u2208N (i) \u03b1lijW (l)h (l) j  , (5) where N (i) is the set of topological neighbours of node i with a one-edge distance, \u03b1lij are learned attention coefficients over those neighbours, \u03c3 is a nonlinear activation function, and W (l) is a learnable feature matrix for layer l. The main difference between this approach and a standard graph convolution update is that, in the latter, attention coefficients are replaced by a fixed normalization constant cij = \u221a |N (i)| \u221a |N (j)|.\nA recent study49 describes how the interpretation of filters in message-passing networks can lead to the identification of relevant pharmacophore- and toxicophore-like substructures, showing consistent findings with literature reports. Gradient-based feature attribution techniques, such as Integrated Gradients,70 were used in conjunction with graph-convolutional networks to analyze retrosynthetic reaction predictions and highlight the atoms involved in each reaction step.64 Attention-based graph convolutional neural networks have also been used for the prediction of solubility, polarity, synthetic accessibility, and photovoltaic efficiency, among other properties,65,66 leading to the identification of relevant molecular substructures for the target properties. Finally, attention-based graph architectures have also been used in chemical reactivity prediction,19 pointing to structural motifs that are consistent\n\u03b1 (l) ij for the l-th layer, which assign \u2019importance\u2019 to the set of neighbours N (i) (e.g., adjacent atoms) of a node i. These coefficients are an explicit component in the computation of new hidden-node representations h(l+1)i (Eq. (5)) in attention-based graph-convolutional architectures. Such learned attention coefficients can be then used to highlight the predictive relevance of certain edges and nodes.\nwith a chemist\u2019s intuition in the identification of suitable reaction partners and activating reagents.\nDue to their intuitive connection with the twodimensional representation of molecules, graphconvolution-based XAI bears the potential of being applicable in many common modeling tasks in drug discovery. Some of the applications that might benefit most from the usage of these approaches are those aiming to identify relevant structural molecular motifs, e.g., for structural alert identification,126 site of reactivity127 or metabolism128 prediction, and the selective optimization of side activities.129"}, {"heading": "2.4 Self-explaining approaches", "text": "The XAI methods introduced so far produce a posteriori explanations of deep learning models. Although such post-\nhoc interpretations have been shown to be useful, some argue that, ideally, XAI methods, should automatically offer human-interpretable explanation alongside their predictions.130 Such approaches (herein referred to as \u2018selfexplaining \u2019) would promote verification and error analysis, and be directly linkable with domain knowledge.131 While the term self-explaining has been coined to refer to a specific neural network architecture \u2013 self-explaining neural networks,131 described below \u2013 in this review, the term is used in a broader sense, so as to identify methods that feature interpretability as a central part of their design. Self-explaining XAI approaches can be grouped into the following categories:\n\u2022 Prototype-based reasoning refers to the task of forecasting future events (i.e., novel samples) based on particularly informative known data points. Usually, this is done by identifying prototypes, i.e., representative samples, which are adapted (or used directly) to make a prediction. These methods are motivated by the fact that predictions based on individual, previously seen examples mimic human decision making.132 The Bayesian case model,133 is a pre-deeplearning approach that constitutes a general framework for such prototype-based reasoning. A Bayesian case model learns to identify observations that best represent clusters in a dataset (i.e., prototypes), along with a set of defining features for that cluster. Joint inference is performed on cluster labels, prototypes, and extracted relevant features, thereby providing interpretability without sacrificing classification accuracy.133 Recently, Li et al.134 developed a neural network architecture composed of an autoencoder and a therein named \u2018prototype layer\u2019, whose units store a learnable weight vector representing an encoded training input. Distances between the encoded latent space of new inputs and the learned prototypes are then used as part of the prediction process. This approach was later expanded by Chen et al.135 to convolutional neural networks for computer vision tasks.\n\u2022 Self-explaining neural networks131 aim to associate input or latent features with semantic concepts. They jointly learn a class prediction and generate explanations using a feature-to-concept mapping. Such a network model consists of (i) a sub-network that maps raw inputs into a predefined set of explainable concepts, (ii) a parameterizer that obtains coefficients for each individual explainable concept, and (iii) an aggregation function that combines the output of the previous two components to produce the final class prediction.\n\u2022 Human-interpretable concept learning refers to the task of learning a class of concepts, i.e. high-level combinations of knowledge elements136) from data, aiming to achieve human-like generalization ability. The Bayesian Program Learning approach137 was proposed with the goal of learning visual concepts in computer vision tasks. Such concepts were represented as probabilistic programs expressed as structured procedures in an abstract description language.138 The model then composes more complex programs using the elements of previously learned ones using\na Bayesian criterion. This approach was shown to reach human-like performance in one-shot learning tasks.139,140\n\u2022 Testing with concept activation vectors141 computes the directional derivatives of the activations of a layer w.r.t. its input, towards the direction of a concept. Such derivatives quantify the degree to which the latter is relevant for a particular classification (e.g., how important the concept \u2018stripes\u2019 is for the prediction of the class \u2018zebra\u2019). It does so by considering the mathematical association between the internal state of a machine learning model \u2013 seen as a vector space Em spanned by basis vectors em that correspond to neural activations \u2013 and human-interpretable activations residing in a different vector space Eh spanned by basis vectors eh. A linear function is computed that translates between these vector spaces (g : Em \u2192 Eh). The association is achieved by defining a vector in the direction of the values of a concept\u2019s set of examples, and then training a linear classifier between those and random counterexamples, to finally take the vector orthogonal to the decision boundary.\n\u2022 Natural language explanation generation. Deep networks can be designed to generate humanunderstandable explanations in a supervised manner.142 In addition to minimizing the loss of the main modeling task, several approaches synthesize a sentence using natural language that explains the decision performed by the model, by simultaneously training generators on large data sets of human-written explanations. This approach has been applied to generate explanations that are both image- and classrelevant.143 Another prominent application is visual question answering.144 To obtain meaningful explanations, however, this approach requires a significant amount of human-curated explanations for training, and might, thus, find limited applicability in drug discovery tasks.\nTo the best of our knowledge, self-explaining deep learning has not been applied to chemistry or drug design yet. Including interpretability by design could help bridge the gap between machine representation and the human understanding of many types of problems in drug discovery. For instance, prototype-reasoning bears promise in the modeling of heterogeneous sets of chemicals with different modes of action, allowing to preserve both mechanistic interpretability and predictive accuracy. Explanation generation (either concept- or text-based) is another potential solution to include human-like reasoning and domain knowledge in the model building task. In particular, explanation generation approaches might be applicable to certain decision-making processes, such as the replacement of animal testing and in-vitro to in-vivo extrapolation, where human-understandable generated explanations constitute a crucial element."}, {"heading": "2.5 Uncertainty estimation", "text": "Uncertainty estimation, i.e. the quantification of epistemic error in a prediction, constitutes another approach to\nmodel interpretation. While some machine learning algorithms, such as Gaussian processes,145 provide built-in uncertainty estimation, deep neural networks are known for being poor at quantifying uncertainty.146 This is one of the reasons why several efforts have been devoted to specifically quantify uncertainty in neural network-based predictions. Uncertainty estimation methods can be grouped into the following categories:\n\u2022 Ensemble approaches. Model ensembles improve the overall prediction quality and have become a standard for uncertainty estimates.147 Deep ensemble averaging148 is based on m identical neural network models that are trained on the same data and with a different initialization. The final prediction is obtained by aggregating the predictions of all models (e.g., by averaging), while an uncertainty estimate can be obtained from the respective variance (Figure 4a). Similarly, the sets of data on which these models are trained can be generated via bootstrap re-sampling.149 A disadvantage of this approach is its computational demand, as the underlying methods build on m independently trained models. Snapshot ensembling150 aims to overcome this limitation by periodically storing model states (i.e. model parameters) along the the training optimization path. These model \u2018snapshots\u2019 can be then used for constructing the ensemble.\n\u2022 Probabilistic approaches aim to estimate the posterior probability of a certain model output or to perform post-hoc calibration. Many of these methods treat neural networks as Bayesian models, by considering a prior distribution over its learnable weights, and then performing inference over their posterior distribution with various methods (Figure 4b), e.g., Markov Chain Monte Carlo151 or variational inference.152,153 Gal et al.,154 suggested the usage of dropout regularization to perform approximate Bayesian inference, which was later extended155 to compute epistemic (i.e., caused by model mis-specification) and aleatory uncertainty (inherent to the noise in the data). Similar approximations can also be made via batch normalization.156 Mean variance estimation157 considers a neural network designed to output both a mean and variance value, to then train the model using a negative Gaussian log-likelihood loss function. Another subcategory of approaches consider asymptotic approximations of a prediction by making Gaussian distributional assumptions of its error, such as the delta technique.158,159\n\u2022 Other approaches. The LUBE (lower upper bound estimation)160 approach trains a neural network with two outputs, corresponding to the upper and lower bounds of the prediction. Instead of quantifying the error of single predictions, LUBE uses simulated annealing and optimizes the model coefficients to achieve (a) maximum coverage (probability that the real value of the i-th sample will fall between the upper and the lower bound) of training measurements and (b) minimum prediction interval width. Ak et al. suggested to quantify the uncertainty in neural network models by directly modelling interval-valued data.161 Trust scores162 measure the agreement between a neural network and a k-nearest neighbour\nclassifier that is trained on a filtered subset of the original data. The trust score considers both the distance between the instance of interest to the nearest class that is different from the original predicted one and its distance towards the predicted class. Unionbased methods163 first train a neural network model and then feed its embeddings to a second model that handles uncertainty, such as a Gaussian process or a random forest. Distance-based approaches164 aim to estimate the prediction uncertainty of a new sample x\u2032 by measuring the distance to the closest sample in the training set, either using input features165 or an embedding produced by the model.166\nMany uncertainty estimation approaches have been successfully implemented in drug discovery applications,167 mostly in traditional QSAR modeling, either by the use of models that naturally handle uncertainty168 or posthoc methods.169\u2013171 Attention has recently been drawn toward the development of uncertainty-aware deep-learning applications in the field. \u2019Snapshot ensembling\u2019 was applied to model 24 bioactivity datasets,172 showing that it performs on par with random forest and neural network ensembles, and also leads to narrower confidence intervals. Schwaller et al.67 proposed a transformer model173 for the task of forward chemical reaction prediction. This approach implements uncertainty estimation by computing the product of the probabilities of all predicted tokens in a SMILES sequence representing a molecule. Zhang et al.68 have recently proposed a Bayesian treatment of a semi-supervised graph-neural network for uncertaintycalibrated predictions of molecular properties, such as the\nmelting point and aqueous solubility. Their results suggest that this approach can efficiently drive an active learning cycle, particularly in the low-data regime \u2013 by choosing those molecules with the largest estimated epistemic uncertainty.\nImportantly, a recent comparison of several uncertainty estimation methods for physicochemical property prediction showed that none of the methods systematically outperformed all others.174"}, {"heading": "3 Available software", "text": "Given the attention deep learning applications are currently receiving, several software tools have been developed to facilitate model interpretation. A prominent example is Captum,175 an extension of the PyTorch176 deeplearning and automatic differentiation package that provides support for most of the feature attribution techniques described in this work. Another popular package is Alibi,177 which provides instance-specific explanations for certain models trained with the scikit-learn178 or TensorFlow179 packages. Some of the explanation methods implemented include anchors, contrastive explanations, and counterfactual instances."}, {"heading": "4 Conclusions and outlook", "text": "Automated analysis of medical and chemical knowledge to extract and present features in a human-readable format dates back to the 1990s,180,181 but it\u2019s receiving increasing attention in the last years due to the renaissance of neural networks in chemistry and healthcare. Given the current pace of AI in drug discovery and related fields, there will be an increased demand for methods that help us understand and interpret deep learning models.\nFor drug discovery and design in particular, XAI will foster collaborations between medicinal chemists, chemoinformaticians and data scientists. If successful, XAI bears the potential to provide fundamental support in the analysis and interpretation of increasingly more complex chemical data, as well as in the formulation of new pharmacological hypotheses, while avoiding human biases.182,183 At the same time, novel drug discovery challenges might boost the development of application-tailored XAI approaches, to promptly respond to specific scientific questions related to human biology and pathophysiology.\nIt will be important to explore the opportunities and limitations of the established chemical language for representing the decision space of these models. Additionally, we have to concede our incomplete understanding of the human pathology at the molecular level, with all its individual idiosyncrasies. In this context, full comprehensibility may be hard to achieve, given, for instance, the often nonlinear relationships between chemical structure and pharmacological activity, the presence of error, and limited predictability.184\nXAI also poses technical challenges, given the multiplicity of possible explanations and methods applicable to a given task.185 Most approaches do not come as readily usable out-of-the-box solutions, but need to be tailored to each individual application. Additionally, profound knowledge of the problem domain is crucial to identify which\nmodel decisions demand further explanations, which type of answers are meaningful to the user and which are instead trivial or expected.186 For human decision making, the explanations generated with XAI have to be nontrivial, non-artificial, and sufficiently informative for the respective scientific community. At least for the time being, finding such solutions will require the collaborative efforts of deep-learning experts, chemoinformaticians and data scientists, chemists, biologists and other domain experts, to ensure that XAI methods serve their intended purpose and deliver reliable answers.\nA crucial challenge for future AI-assisted drug discovery is the data representation used for machine and deep learning. In contrast to many other areas in which deep learning has been shown to excel, such as natural language processing and image recognition, there is no naturally applicable, complete, \u2018raw\u2019 molecular representation. After all, molecules \u2013 as scientists conceive them \u2013 are models themselves. Such \u2018induction-based\u2019 approach, which builds higher-order (e.g., deep learning) models from lower-order ones (e.g., molecular representations or descriptors based on observational statements) is therefore philosophically debatable.187 The question as to how to represent molecules for deep learning thus constitutes still one of the fundamental challenges of XAI in drug discovery and design.\nOne step forward towards explainable AI is to build on interpretable \u2018low-level\u2019 molecular representations that have direct meaning for chemists (e.g. SMILES strings,188,189 amino acid sequences,190,191 and spatial 3D-voxelized representations94,192). Many recent studies also rely on well-established molecular descriptors, such as hashed binary fingerprints193,194 and topochemical and geometrical descriptors,195,196 which capture structural features defined a priori. Often, molecular descriptors, while being relevant for subsequent modeling, capture complex chemical information in a non readilyinterpretable way. Consequently, when striving for XAI, there is an understandable tendency to employ molecular representations that are more easily interpretable in terms of the known language of chemistry.197 It therefore goes without saying that the development of novel interpretable molecular representations for deep learning will constitute a critical area of research for the years to come, including the development of self-explaining approaches to overcome the hurdles of non-interpretable but information-rich descriptors, by providing human-like explanations alongside sufficiently accurate predictions.\nMost of the deep learning models in drug discovery do not consider applicability domain issues,198 i.e., the region of chemical space where statistical learning assumptions are met.199 Inclusion of applicability domain restrictions \u2013 in an interpretable way, whenever possible \u2013 should be considered as an integral element of explainable AI. Knowing when to apply which particular model will, in fact, help address the problem of high confidence of deep learning models on wrong predictions,146,200,201 avoiding unnecessary extrapolations at the same time. Assessing a model\u2019s applicability domain and rigorous determination of model accuracy might arguably be more important for decision making than the particular modeling approach chosen.69\nAt present, XAI in drug discovery is lacking opencommunity platforms, where software code, model inter-\npretations, and the respective training data can be shared and improved by synergistic efforts of researchers with different scientific backgrounds. Initiatives like MELLODDY (Machine Learning Ledger Orchestration for Drug Discovery) for decentralized, federated model development and secure data handling across pharmaceutical companies constitute a first step in the right direction.202 Such kinds of collaboration will hopefully foster the validation and acceptance of XAI approaches and the associated explanations they provide.\nEspecially in time- and cost-sensitive scenarios like drug discovery, the users of deep learning methods have the responsibility to cautiously inspect and interpret the predictions made by such a computational model. Keeping in mind the possibilities and limitations of drug discovery with XAI, it is reasonable to assume that the continued development of alternative models that are more easily comprehensible and computationally affordable will not lose its importance.\nConflict of interest G.S. declares a potential financial conflict of interest in his role as a co-founder of inSili.com GmbH, Zurich, and consultant to the pharmaceutical industry.\nAuthor contributions All authors contributed equally to this manuscript."}, {"heading": "Acknowledgements", "text": "Dr. Nils Weskamp is thanked for helpful feedback on the manuscript. This work was financially supported by the ETH RETHINK initiative, the Swiss National Science Foundation (grant no. 205321_182176), and Boehringer Ingelheim Pharma GmbH & Co. KG.\nRelated links \u2022 PyTorch Captum: captum.ai\n\u2022 Alibi: github.com/SeldonIO/alibi\n\u2022 MELLODDY Consortium: melloddy.eu"}], "title": "Drug discovery with explainable artificial intelligence", "year": 2020}