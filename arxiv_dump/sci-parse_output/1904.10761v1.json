{"abstractText": "The wide use of machine learning is fundamentally changing the software development paradigm (a.k.a. Software 2.0) where data becomes a first-class citizen, on par with code. As machine learning is used in sensitive applications, it becomes imperative that the trained model is accurate, fair, and robust to attacks. While many techniques have been proposed to improve the model training process (in-processing approach) or the trained model itself (post-processing), we argue that the most effective method is to clean the root cause of error: the data the model is trained on (pre-processing). Historically, there are at least three research communities that have been separately studying this problem: data management, machine learning (model fairness), and security. Although a significant amount of research has been done by each community, ultimately the same datasets must be preprocessed, and there is little understanding how the techniques relate to each other and can possibly be integrated. We contend that it is time to extend the notion of data cleaning for modern machine learning needs. We identify dependencies among the data preprocessing techniques and propose MLClean, a unified data cleaning framework that integrates the techniques and helps train accurate and fair models. This work is part of a broader trend of Big data \u2013 Artificial Intelligence (AI) integration. ACM Reference Format: Ki Hyun Tae, Yuji Roh, Young Hun Oh, Hyunsu Kim, Steven Euijong Whang. 2019. Data Cleaning for Accurate, Fair, and Robust Models: A Big Data AI Integration Approach. In DEEM workshop. ACM, New York, NY, USA, 4 pages. \u2217Corresponding author Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. DEEM \u201919, June 30, 2019, Amsterdam, NL \u00a9 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.", "authors": [{"affiliations": [], "name": "Ki Hyun Tae"}, {"affiliations": [], "name": "Yuji Roh"}, {"affiliations": [], "name": "Young Hun Oh"}, {"affiliations": [], "name": "Hyunsu Kim"}, {"affiliations": [], "name": "Steven Euijong Whang"}, {"affiliations": [], "name": "Steven Euijong"}], "id": "SP:dbd64c741c972b2a11fdefe7d538b2acac80233f", "references": [{"authors": ["n. d"], "title": "Software 2.0. https://medium.com/@karpathy/ software-2-0-a64152b37c35", "venue": "Accessed Mar", "year": 2019}, {"authors": ["Rachel K.E. Bellamy", "Kuntal Dey", "Michael Hind"], "title": "2018. AI Fairness 360: An Extensible Toolkit for Detecting, Understanding, and Mitigating Unwanted Algorithmic Bias", "venue": "CoRR abs/1810.01943", "year": 2018}, {"authors": ["Alexandra Chouldechova", "Aaron Roth"], "title": "The Frontiers of Fairness in Machine Learning", "venue": "CoRR abs/1810.08810", "year": 2018}, {"authors": ["Xu Chu", "Ihab F. Ilyas", "Sanjay Krishnan", "Jiannan Wang"], "title": "Data Cleaning: Overview and Emerging Challenges", "venue": "In SIGMOD", "year": 2016}, {"authors": ["Xin Luna Dong", "Theodoros Rekatsinas"], "title": "Data Integration and Machine Learning: A Natural Synergy", "venue": "In SIGMOD", "year": 2018}, {"authors": ["Denis Baylor"], "title": "TFX: A TensorFlow-Based Production-Scale Machine Learning Platform", "venue": "In KDD", "year": 2017}, {"authors": ["Andr\u00e1s Gy\u00f6rgye", "Luis Mu\u00f1oz-Gonz\u00e1lez", "Andras Gyorgy", "Emil C. Lupu"], "title": "Detection of Adversarial Training Examples in Poisoning Attacks through Anomaly Detection", "year": 2018}, {"authors": ["Pang Wei Koh", "Percy Liang"], "title": "Understanding Black-box Predictions via Influence Functions", "venue": "In ICML", "year": 2017}, {"authors": ["Pang Wei Koh", "Jacob Steinhardt", "Percy Liang"], "title": "Stronger Data Poisoning Attacks Break Data Sanitization Defenses", "venue": "CoRR abs/1811.00741", "year": 2018}, {"authors": ["Neoklis Polyzotis", "Sudip Roy", "Steven Euijong Whang", "Martin Zinkevich"], "title": "Data Management Challenges in Production Machine Learning", "venue": "In SIGMOD", "year": 2017}, {"authors": ["Theodoros Rekatsinas", "Xu Chu", "Ihab F. Ilyas", "Christopher R\u00e9"], "title": "HoloClean: Holistic Data Repairs with Probabilistic Inference", "venue": "PVLDB 10,", "year": 2017}], "sections": [{"text": "ACM Reference Format: Ki Hyun Tae, Yuji Roh, Young Hun Oh, Hyunsu Kim, Steven Euijong Whang. 2019. Data Cleaning for Accurate, Fair, and Robust Models: A Big Data - AI Integration Approach. In DEEM workshop. ACM, New York, NY, USA, 4 pages.\n\u2217Corresponding author\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. DEEM \u201919, June 30, 2019, Amsterdam, NL \u00a9 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM."}, {"heading": "1 INTRODUCTION", "text": "Machine learning is becoming a new paradigm of programming (called Software 2.0 [1]) where data becomes a firstclass citizen, on par with code. While existing development involved implementing certain functionalities and testing code, the development process of machine learning applications involves data collection, data analysis and validation, model training, and model validation where the entire process can repeat [10]. As an example, Google has developed and is opensourcing its end-to-end machine learning platform TensorFlow Extended (TFX) [6], in order to democratize AI and make machine learning development easy for anyone.\nAs machine learning becomes widely used even in sensitive applications like self-driving cars and medical applications, training a model needs to be more \u201cbullet proof\u201d as well. In addition to being accurate, machine learning models should not discriminate certain demographics of people or be sensitive to adversarial data. However, machine learning is only as good as its data, and no matter how good the training algorithm, the ultimate problem may lie in the data itself.\nInterestingly, multiple communities have been investigating how to prepare data for machine learning (see Section 2 for details). Unfortunately, little is known how the different data preprocessing approaches depend on each other and can be used together when a dataset is dirty, biased, and adversarial. In addition, we cannot assume that solving one problem will automatically solve the others. From a data management standpoint, we contend that it is a good time to extend the data cleaning problem for the pressing needs of modern machine learning for accurate, fair, and robust model training.\nIn this paper, our novel contributions aremaking a comparison of three data preprocessing techniques and proposing MLClean, a data cleaning framework that performs traditional data cleaning, unfairness mitigation, and data sanitization together.MLClean views machine learning models as black boxes, which means it can support any model, but cannot exploit the internals of it. MLClean is an example of the larger trend of Big data and AI integration where data management and machine learning techniques are converging and opens up many new research opportunities.\nar X\niv :1\n90 4.\n10 76\n1v 1\n[ cs\n.D B\n] 2\n2 A\npr 2\n01 9"}, {"heading": "In addition, e6 has an anomalous age.", "text": ""}, {"heading": "2 EXTENDING DATA CLEANING", "text": "We compare and identify dependencies among the three data preprocessing techniques and discuss how data cleaning can possibly be extended to the other preprocessing techniques."}, {"heading": "2.1 Traditional Data Cleaning", "text": "Data cleaning [4] originates from the data management community and has been studied for decades. Traditionally, there is a focus on cleaning structured data with schema at scale where integrity constraints, denial constraints, and functional dependencies need to be satisfied. In addition, duplicates must be removed, and values need to be corrected to be within certain ranges or to exist in external data sources. More recently, there are efforts to improve machine learning accuracy [5] and data validation techniques for machine learning pipelines [10]. However, these techniques do not resolve the pressing issues of model fairness or model robustness against adversarial data. As a running example, suppose we are cleaning a set of training examples in Table 1 (small for illustration purposes). This data is not clean in the sense that e2 and e3 refer to the same person because their ages are the same and Joe is an abbreviation of Joseph. (In comparison, e4 and e5 are not the same person because they have very different ages.) In addition, e6 has an unusually-high age, which can be viewed as an incorrect value. Hence, cleaning this data may involve merging e2 and e3 to a single example e23 and fixing or removing e6\u2019s age. However, there are also potential fairness and security issues as we will see in the next sections."}, {"heading": "2.2 Unfairness Mitigation", "text": "The machine learning community has been exploring the issue of model fairness where a model may inappropriately discriminate certain demographics. Many definitions of fairness exist (see a recent survey [3]) and can be largely classified into two categories: local and global. Local measures apply on individuals and make sure the model predictions on\nthem are similar to the predictions on close neighbors. Global measures compare sensitive groups of individuals (e.g., men versus women) and makes sure they have similar statistics. For example, the demographic parity measure dictates that the ratios of positive predictions per group should be the same. Model unfairness is largely due to the inherent bias in the data, which can reflect the opinions of the people who made it and how the data was collected. More recently, there are mitigation techniques for fixing unfairness, which can be done before (pre-processing), during (in-processing), or after (post-processing) model training [2]. These techniques typically tradeoff some model accuracy in order to improve model fairness. Among them, we focus on the pre-processing approach where the example weights are adjusted (i.e., reweighed [3]) to maximize fairness. For example, a simplified reweighing technique for demographic parity is to increase the weights of positivelylabeled examples in sensitive groups whose ratio of weighted positive labels is lower than other groups. In Table 1, the sensitive groups Gender = M and Gender = F have ratios of 1.01.0+1.0+1.0 = 0.33 and 1.0+1.0 1.0+1.0+1.0 = 0.67, so we can increase the weight of e1 (the only example that has a positive label in Gender = M) from 1.0 to 4.0 so that the ratio is\n4.0 4.0+1.0+1.0 = 0.67. To extend data cleaning to unfairness mitigation, we must understand their possible interactions. In Table 1, suppose that e2 and e3 are merged into e23, which has a total weight of 1.0 and a label of 0. Then the ratio of positive examples in Gender = M increases to 1.01.0+1.0 = 0.5, so e1\u2019s weight now needs to be changed from 1.0 to 2.0 (notice this value is smaller than the reweighed result of the previous paragraph) so that 2.02.0+1.0 = 0.67."}, {"heading": "2.3 Data Sanitization", "text": "The machine learning and security communities are actively studying the problem of robust machine learning against adversarial data in critical applications including spam filtering, autonomous driving, and cybersecurity. A major problem is that the training data is often collected from external data sources, which are vulnerable to attacks by malicious actors [9]. A popular solution is to make the model training more robust. Another approach that is gaining interest is sanitizing the poisoned data before it is used in training. Data poisoning attacks have recently become more sophisticated [9], and there is an arms race on developing better defenses to stop them as well. Data poisoning can also be done on the test data where the same sanitization techniques can apply.\nData sanitization may conflict with data cleaning. For example, consider e6 in Table 1, which shows an unusually-high age for Sally. This example can be a real attack to confuse\nthe model training, or just an honest typo. Depending on the action taken (e.g., remove e6 or fix its age to a reasonable value), the model training is affected differently as well. In general, using data cleaning and sanitization together can be tricky because incorrect data and poisoned values may be hard to distinguish. While obviously anomalous values may indicate an attack, more recent attacks can generate data closer to the data\u2019s distribution and are thus indistinguishable. However, data poisoning is clearly designed to reduce model accuracy while dirty data may have mixed results. Hence, one way to tell the two apart is to see how drastically the model\u2019s accuracy changes."}, {"heading": "3 MLCLEAN", "text": "Since data cleaning, unfairness mitigation, and data sanitization are ultimately preprocessing the same dataset, it makes sense to unify them. The na\u00efve approach of applying each technique independently in any sequence can be problematic for several reasons. Simply ignoring the dependencies between preprocessing techniques may result in incorrect results. For example, if we reweigh examples and then attempt to remove duplicates, then the reweighing may need to be done again to ensure fairness. Moreover, running one operation at a time may have efficiency issues due to redundant operations on the data."}, {"heading": "3.1 Basic Architecture", "text": "We present MLClean, an extended data cleaning framework that takes into account the dependencies of the three preprocessing techniques and integrates them to produce clean, unbiased, and sanitized data (see architecture in Figure 1). Data sanitization can be viewed as an extreme version of data cleaning and thus be executed together in one component. The unfairness mitigation component comes afterwards because, while data sanitization and cleaning may affect the bias of data, reweighing examples only changes the example weights and does not effect the correctness of sanitization and cleaning on the other features.\nExample. Suppose the data sanitization is anomaly detection based on clustering and data cleaning is entity resolution [4]. We can tightly couple the two techniques by clustering the examples and performing anomaly detection, then running entity resolution on each cluster, assuming they contain the candidate duplicates. This approach is common in entity resolution where candidate duplicates are narrowed down to reduce expensive comparisons. Say we use demographic parity as the fairness measure.\nFigure 2 shows how the examples in Table 1 can be preprocessed by MLClean. First, anomaly detection is used to form the clusters {e1, e2, e3} and {e4, e5} where e6 is considered an outlier and removed. Suppose an entity resolution process merges e2 and e3 into e23 with a summed weight of 2 and a label of 0. Finally, unfairness mitigation adjusts e23\u2019s weight from 2 to 1 so that the ratio of positive examples for {e1, e23} (men) and {e4, e5} (women) are the same 1.01.0+1.0 = 0.5."}, {"heading": "3.2 Extensions", "text": "TheMLClean framework can be extended to more general combinations of data sanitization, data cleaning, and unfairness mitigation. Beyond removing duplicates, data cleaning can be any general process like HoloClean [11]. Data sanitization can also employ more sophisticated defenses [9]. Of course, one should carefully analyze the possible interactions between each cleaning and sanitization combination. Some preprocessing techniques may use a previouslytrained model as shown in Figure 1. For example, certain model fairness measures are computed using model predictions. In addition, recall the discussion that data sanitization and cleaning are sometimes hard to distinguish because an example may be adversary or contain an honest typo. In order to distinguish the two cases, we may want to use a previous model and techniques such as influence functions [8] to estimate whether the model\u2019s accuracy drastically changes when the example is removed. We believeMLClean opens up many avenues of research on how to tightly integrate different preprocessing techniques."}, {"heading": "4 PRELIMINARY RESULTS", "text": "We evaluateMLClean and compare it with other baseline approaches. We use the Census Income (C) and German Credit\n(G) datasets [2], which are widely used in the machine learning community. We train a linear model, although MLClean supports any type of model. To measure model accuracy, we compute the portion of examples that were correctly classified. This metric can be used both for evaluating data cleaning and data sanitization. To measure fairness, we use demographic parity and compute the ratio of the probabilities of positive predictions between two sensitive groups (male/female for C and old/young for G) as in [2]. The closer the ratio is to 1, the fairer the model.\nFor data sanitization, we add adversarial examples to the datasets by using Lasso classifier poisoning [7] and then defend the attack with clustering-based anomaly detection using the k-means algorithm. Here the scenario is that a malicious actor is intentionally poisoning the training data at some external data source. For data cleaning, we introduce duplicates in the datasets by replicating the examples where the degree of replication follows a Zipfian distribution. We then run entity resolution using pairwise comparisons. For unfairness mitigation, we use example reweighing for improving demographic parity as in [2].\nTable 2 compares the model accuracy and fairness results of different scenarios. The baseline is when none of the techniques are used (denoted as None). When using the preprocessing techniques individually, S and C improve accuracy while M improves fairness. When running all three techniques in a sequence one by one, the accuracy and fairness results are better overall, although they are not the highest compared to the individual results. In addition, the ordering of the sequence matters. In case of C, running the sequence \u27e8M, S,C\u27e9 results in lower fairness than \u27e8S,C,M\u27e9 because there is a dependency from S and C to M . For G, there is less of a difference because G was already producing fair models, so there was little unfairness to mitigate in the first place. Finally, we see thatMLClean has similar accuracy and fairness results as \u27e8S,C,M\u27e9, but runs significantly faster (by 2.2-2.5x) because it tightly couples S and C where C only runs on clusters identified by S . Hence, MLClean has the best balance of accuracy, fairness, and runtime."}, {"heading": "5 DISCUSSION", "text": "In this paper, we contend that data cleaningmust be extended to incorporate unfairness mitigation and data sanitization to address the pressing needs of machine learning. We proposedMLClean, the first framework to tightly integrate data cleaning, unfairness mitigation, and data sanitization. We argue that fixing the source of error (i.e., the data itself) is the best solution for ensuring model accuracy and fairness and demonstrated how the three preprocessing techniques can be used together effectively.\nThis work is part of a larger trend of Big data - AI integration and opens up many new data management challenges. In particular, we are interested in exploring other combinations of data preprocessing techniques, exploiting white-box models, evaluating MLClean on general benchmarks, and addressing scalability issues on larger datasets with tighter integrations of preprocessing techniques."}, {"heading": "ACKNOWLEDGMENT", "text": "This research was supported by a Google AI Focused Research Award and the Engineering Research Center Program through the National Research Foundation of Korea funded by the Korean Government MSIT (NRF-2018R1A5A1059921)."}], "title": "Data Cleaning for Accurate, Fair, and Robust Models: A Big Data - AI Integration Approach", "year": 2019}