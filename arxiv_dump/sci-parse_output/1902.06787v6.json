{
  "abstractText": "Most of the work on interpretable machine learning has focused on designing either inherently interpretable models, which typically trade-off accuracy for interpretability, or post-hoc explanation systems, whose explanation quality can be unpredictable. Our method, EXPO, is a hybridization of these approaches that regularizes a model for explanation quality at training time. Importantly, these regularizers are differentiable, model agnostic, and require no domain knowledge to define. We demonstrate that post-hoc explanations for EXPO-regularized models have better explanation quality, as measured by the common fidelity and stability metrics. We verify that improving these metrics leads to significantly more useful explanations with a user study on a realistic task.",
  "authors": [
    {
      "affiliations": [],
      "name": "Gregory Plumb"
    },
    {
      "affiliations": [],
      "name": "Maruan Al-Shedivat"
    }
  ],
  "id": "SP:da2ef100947f21a97529d555b109ea9a24a3376b",
  "references": [
    {
      "authors": [
        "Julius Adebayo",
        "Justin Gilmer",
        "Michael Muelly",
        "Ian Goodfellow",
        "Moritz Hardt",
        "Been Kim"
      ],
      "title": "Sanity checks for saliency maps",
      "venue": "In Advances in Neural Information Processing Systems,",
      "year": 2018
    },
    {
      "authors": [
        "Maruan Al-Shedivat",
        "Avinava Dubey",
        "Eric P Xing"
      ],
      "title": "Contextual explanation networks",
      "venue": "arXiv preprint arXiv:1705.10301,",
      "year": 2017
    },
    {
      "authors": [
        "David Alvarez-Melis",
        "Tommi Jaakkola"
      ],
      "title": "Towards robust interpretability with self-explaining neural networks",
      "venue": "In Advances in Neural Information Processing Systems,",
      "year": 2018
    },
    {
      "authors": [
        "David Alvarez-Melis",
        "Tommi S Jaakkola"
      ],
      "title": "On the robustness of interpretability methods",
      "venue": "arXiv preprint arXiv:1806.08049,",
      "year": 2018
    },
    {
      "authors": [
        "Adam Bloniarz",
        "Ameet Talwalkar",
        "Bin Yu",
        "Christopher Wu"
      ],
      "title": "Supervised neighborhoods for distributed nonparametric regression",
      "venue": "In Artificial Intelligence and Statistics,",
      "year": 2016
    },
    {
      "authors": [
        "Rich Caruana"
      ],
      "title": "Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission",
      "venue": "In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,",
      "year": 2015
    },
    {
      "authors": [
        "Finale Doshi-Velez",
        "Been Kim"
      ],
      "title": "Towards a rigorous science of interpretable machine learning, 2017",
      "year": 2017
    },
    {
      "authors": [
        "Mengnan Du",
        "Ninghao Liu",
        "Fan Yang",
        "Xia Hu"
      ],
      "title": "Learning credible deep neural networks with rationale regularization",
      "venue": "IEEE International Conference on Data Mining (ICDM),",
      "year": 2019
    },
    {
      "authors": [
        "Amirata Ghorbani",
        "Abubakar Abid",
        "James Zou"
      ],
      "title": "Interpretation of neural networks is fragile",
      "venue": "arXiv preprint arXiv:1710.10547,",
      "year": 2017
    },
    {
      "authors": [
        "Been Kim",
        "Martin Wattenberg",
        "Justin Gilmer",
        "Carrie Cai",
        "James Wexler",
        "Fernanda Viegas"
      ],
      "title": "Interpretability beyond feature attribution: Quantitative testing with concept activation vectors (tcav)",
      "venue": "In International Conference on Machine Learning,",
      "year": 2018
    },
    {
      "authors": [
        "Diederik P Kingma",
        "Jimmy Ba"
      ],
      "title": "Adam: A method for stochastic optimization",
      "venue": "arXiv preprint arXiv:1412.6980,",
      "year": 2014
    },
    {
      "authors": [
        "Himabindu Lakkaraju",
        "Osbert Bastani"
      ],
      "title": " how do i fool you?\" manipulating user trust via misleading black box explanations",
      "venue": "In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society,",
      "year": 2020
    },
    {
      "authors": [
        "Yann LeCun"
      ],
      "title": "The mnist database of handwritten digits. http://yann",
      "venue": "lecun. com/exdb/mnist/,",
      "year": 1998
    },
    {
      "authors": [
        "Guang-He Lee",
        "David Alvarez-Melis",
        "Tommi S Jaakkola"
      ],
      "title": "Game-theoretic interpretability for temporal modeling",
      "venue": "arXiv preprint arXiv:1807.00130,",
      "year": 2018
    },
    {
      "authors": [
        "Guang-He Lee",
        "Wengong Jin",
        "David Alvarez-Melis",
        "Tommi Jaakkola"
      ],
      "title": "Functional transparency for structured data: a game-theoretic approach",
      "venue": "In International Conference on Machine Learning,",
      "year": 2019
    },
    {
      "authors": [
        "Tao Lei",
        "Regina Barzilay",
        "Tommi Jaakkola"
      ],
      "title": "Rationalizing neural predictions",
      "venue": "arXiv preprint arXiv:1606.04155,",
      "year": 2016
    },
    {
      "authors": [
        "Zachary C Lipton"
      ],
      "title": "The mythos of model interpretability",
      "venue": "arXiv preprint arXiv:1606.03490,",
      "year": 2016
    },
    {
      "authors": [
        "Scott M Lundberg",
        "Su-In Lee"
      ],
      "title": "A unified approach to interpreting model predictions",
      "venue": "In Advances in Neural Information Processing Systems,",
      "year": 2017
    },
    {
      "authors": [
        "Gregory Plumb",
        "Denali Molitor",
        "Ameet S Talwalkar"
      ],
      "title": "Model agnostic supervised local explanations",
      "venue": "In Advances in Neural Information Processing Systems,",
      "year": 2018
    },
    {
      "authors": [
        "Chongli Qin",
        "James Martens",
        "Sven Gowal",
        "Dilip Krishnan",
        "Krishnamurthy Dvijotham",
        "Alhussein Fawzi",
        "Soham De",
        "Robert Stanforth",
        "Pushmeet Kohli"
      ],
      "title": "Adversarial robustness through local linearization",
      "venue": "In Advances in Neural Information Processing Systems,",
      "year": 2019
    },
    {
      "authors": [
        "Marco Tulio Ribeiro",
        "Sameer Singh",
        "Carlos Guestrin"
      ],
      "title": "Why should i trust you?: Explaining the predictions of any classifier",
      "venue": "In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining,",
      "year": 2016
    },
    {
      "authors": [
        "Marco Tulio Ribeiro",
        "Sameer Singh",
        "Carlos Guestrin"
      ],
      "title": "Anchors: High-precision model-agnostic explanations",
      "year": 2018
    },
    {
      "authors": [
        "Laura Rieger",
        "Chandan Singh",
        "W James Murdoch",
        "Bin Yu"
      ],
      "title": "Interpretations are useful: penalizing explanations to align neural networks with prior knowledge",
      "year": 1909
    },
    {
      "authors": [
        "Andrew Slavin Ross",
        "Michael C Hughes",
        "Finale Doshi-Velez"
      ],
      "title": "Right for the right reasons: Training differentiable models by constraining their explanations",
      "venue": "arXiv preprint arXiv:1703.03717,",
      "year": 2017
    },
    {
      "authors": [
        "Ramprasaath R Selvaraju",
        "Michael Cogswell",
        "Abhishek Das",
        "Ramakrishna Vedantam",
        "Devi Parikh",
        "Dhruv Batra"
      ],
      "title": "Grad-cam: Visual explanations from deep networks via gradient-based localization",
      "venue": "In Proceedings of the IEEE International Conference on Computer Vision,",
      "year": 2017
    },
    {
      "authors": [
        "Avanti Shrikumar",
        "Peyton Greenside",
        "Anna Shcherbina",
        "Anshul Kundaje"
      ],
      "title": "Not just a black box: Learning important features through propagating activation differences",
      "venue": "arXiv preprint arXiv:1605.01713,",
      "year": 2016
    },
    {
      "authors": [
        "Karen Simonyan",
        "Andrea Vedaldi",
        "Andrew Zisserman"
      ],
      "title": "Deep inside convolutional networks: Visualising image classification models and saliency maps",
      "venue": "arXiv preprint arXiv:1312.6034,",
      "year": 2013
    },
    {
      "authors": [
        "Daniel Smilkov",
        "Nikhil Thorat",
        "Been Kim",
        "Fernanda Vi\u00e9gas",
        "Martin Wattenberg"
      ],
      "title": "Smoothgrad: removing noise by adding noise",
      "venue": "arXiv preprint arXiv:1706.03825,",
      "year": 2017
    },
    {
      "authors": [
        "Mukund Sundararajan",
        "Ankur Taly",
        "Qiqi Yan"
      ],
      "title": "Axiomatic attribution for deep networks",
      "venue": "arXiv preprint arXiv:1703.01365,",
      "year": 2017
    },
    {
      "authors": [
        "Richard Tomsett",
        "Dan Harborne",
        "Supriyo Chakraborty",
        "Prudhvi Gurram",
        "Alun Preece"
      ],
      "title": "Sanity checks for saliency metrics",
      "venue": "arXiv preprint arXiv:1912.01451,",
      "year": 2019
    },
    {
      "authors": [
        "Fulton Wang",
        "Cynthia Rudin"
      ],
      "title": "Falling rule lists",
      "venue": "In Artificial Intelligence and Statistics,",
      "year": 2015
    },
    {
      "authors": [
        "Ethan Weinberger",
        "Joseph Janizek",
        "Su-In Lee"
      ],
      "title": "Learning deep attribution priors based on prior knowledge",
      "year": 1912
    },
    {
      "authors": [
        "Mike Wu",
        "Michael C Hughes",
        "Sonali Parbhoo",
        "Maurizio Zazzi",
        "Volker Roth",
        "Finale Doshi-Velez"
      ],
      "title": "Beyond sparsity: Tree regularization of deep models for interpretability",
      "year": 2018
    },
    {
      "authors": [
        "Mike Wu",
        "Sonali Parbhoo",
        "Michael Hughes",
        "Ryan Kindle",
        "Leo Celi",
        "Maurizio Zazzi",
        "Volker Roth",
        "Finale Doshi-Velez"
      ],
      "title": "Regional tree regularization for interpretability in black box models",
      "year": 1908
    },
    {
      "authors": [
        "Matthew D Zeiler",
        "Rob Fergus"
      ],
      "title": "Visualizing and understanding convolutional networks",
      "venue": "In European Conference on Computer Vision,",
      "year": 2014
    },
    {
      "authors": [
        "Stephan Zheng",
        "Yang Song",
        "Thomas Leung",
        "Ian Goodfellow"
      ],
      "title": "Improving the robustness of deep neural networks via stability training",
      "venue": "In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,",
      "year": 2016
    }
  ],
  "sections": [
    {
      "heading": "1 Introduction",
      "text": "Complex learning-based systems are increasingly shaping our daily lives. To monitor and understand these systems, we require clear explanations of model behavior. Although model interpretability has many definitions and is often application specific [Lipton, 2016], local explanations are a popular and powerful tool [Ribeiro et al., 2016] and will be the focus of this work.\nRecent techniques in interpretable machine learning range from models that are interpretable bydesign [e.g., Wang and Rudin, 2015, Caruana et al., 2015] to model-agnostic post-hoc systems for explaining black-box models such as ensembles and deep neural networks [e.g., Ribeiro et al., 2016, Lei et al., 2016, Lundberg and Lee, 2017, Selvaraju et al., 2017, Kim et al., 2018]. Despite the variety of technical approaches, the underlying goal of these methods is to develop an interpretable predictive system that produces two outputs: a prediction and its explanation.\nBoth by-design and post-hoc approaches have limitations. On the one hand, by-design approaches are restricted to working with model families that are inherently interpretable, potentially at the cost of accuracy. On the other hand, post-hoc approaches applied to an arbitrary model usually offer no recourse if their explanations are not of suitable quality. Moreover, recent methods that claim to overcome this apparent trade-off between prediction accuracy and explanation quality are in fact by-design approaches that impose constraints on the model families they consider [e.g., Al-Shedivat et al., 2017, Plumb et al., 2018, Alvarez-Melis and Jaakkola, 2018a].\nIn this work, we propose a strategy called Explanation-based Optimization (EXPO) that allows us to interpolate between these two paradigms by adding an interpretability regularizer to the loss function used to train the model. EXPO uses regularizers based on the fidelity [Ribeiro et al., 2016, Plumb et al., 2018] or stability [Alvarez-Melis and Jaakkola, 2018a] metrics. See Section 2 for definitions.\n34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.\nar X\niv :1\n90 2.\n06 78\n7v 6\n[ cs\n.L G\n] 8\nUnlike by-design approaches, EXPO places no explicit constraints on the model family because its regularizers are differentiable and model agnostic. Unlike post-hoc approaches, EXPO allows us to control the relative importance of predictive accuracy and explanation quality. In Figure 1, we see an example of how EXPO allows us to interpolate between these paradigms and overcome their respective weaknesses.\nAlthough fidelity and stability are standard proxy metrics, they are only indirect measurements of the usefulness of an explanation. To more rigorously test the usefulness of EXPO, we additionally devise a more realistic evaluation task where humans are asked to use explanations to change a model\u2019s prediction. Notably, our user study falls under the category of Human-Grounded Metric evaluations as defined by Doshi-Velez and Kim [2017].\nThe main contributions of our work are as follows:\n1. Interpretability regularizer. We introduce, EXPO-FIDELITY, a differentiable and model agnostic regularizer that requires no domain knowledge to define. It approximates the fidelity metric on the training points in order to improve the quality of post-hoc explanations of the model.\n2. Empirical results. We compare models trained with and without EXPO on a variety of regression and classification tasks.1 Empirically, EXPO slightly improves test accuracy and significantly improves explanation quality on test points, producing at least a 25% improvement in terms of explanation fidelity. This separates it from many other methods which trade-off between predictive accuracy and explanation quality. These results also demonstrate that EXPO\u2019s effects generalize from the training data to unseen points.\n3. User study. To more directly test the usefulness of EXPO, we run a user study where participants complete a simplified version of a realistic task. Quantitatively, EXPO makes it easier for users to complete the task and, qualitatively, they prefer using the EXPO-regularized model. This is additional validation that the fidelity and stability metrics are useful proxies for interpretability."
    },
    {
      "heading": "2 Background and Related Work",
      "text": "Consider a supervised learning problem where the goal is to learn a model, f : X 7\u2192 Y , f \u2208 F , that maps input feature vectors, x \u2208 X , to targets, y \u2208 Y , trained with data, {xi, yi}Ni=1. If F is complex, we can understand the behavior of f in some neighborhood, Nx \u2208 P[X ] where P[X ] is the space of probability distributions over X , by generating a local explanation. We denote systems that produce local explanations (i.e., explainers) as e : X \u00d7 F 7\u2192 E , where E is the set of possible explanations. The choice of E generally depends on whether or not X consists of semantic features. We call features semantic if users can reason about them and understand what changes in their values mean (e.g., a person\u2019s income or the concentration of a chemical). Non-semantic features lack an inherent interpretation, with images as a canonical example. We primarily focus on semantic features but we briefly consider non-semantic features in Appendix A.8.\n1https://github.com/GDPlumb/ExpO\nWe next state the goal of local explanations for semantic features, define fidelity and stability (the metrics most commonly used to quantitatively evaluate the quality of these explanations), and briefly summarize the post-hoc explainers whose explanations we will use for evaluation.\nGoal of Approximation-based Local Explanations. For semantic features, we focus on local explanations that try to predict how the model\u2019s output would change if the input were perturbed such as LIME [Ribeiro et al., 2016] and MAPLE [Plumb et al., 2018]. Thus, we can define the output space of the explainer as Es := {g \u2208 G | g : X 7\u2192 Y}, where G is a class of interpretable functions. As is common, we assume that G is the set of linear functions. Fidelity metric. For semantic features, a natural choice for evaluation is to measure how accurately g models f in a neighborhood Nx [Ribeiro et al., 2016, Plumb et al., 2018]:\nF (f, g,Nx) := Ex\u2032\u223cNx [(g(x\u2032)\u2212 f(x\u2032)) 2 ], (1)\nwhich we refer to as the neighborhood-fidelity (NF) metric. This metric is sometimes evaluated with Nx as a point mass on x and we call this version the point-fidelity (PF) metric.2 Intuitively, an explanation with good fidelity (lower is better) accurately conveys which patterns the model used to make this prediction (i.e., how each feature influences the model\u2019s prediction around this point).\nStability metric. In addition to fidelity, we are interested in the degree to which the explanation changes between points in Nx, which we measure using the stability metric [Alvarez-Melis and Jaakkola, 2018a]: S(f, e,Nx) := Ex\u2032\u223cNx [||e(x, f)\u2212 e(x\u2032, f)||22] (2) Intuitively, more stable explanations (lower is better) tend to be more trustworthy [Alvarez-Melis and Jaakkola, 2018a,b, Ghorbani et al., 2017].\nPost-hoc explainers. Various explainers have been proposed to generate local explanations of the form g : X 7\u2192 Y . In particular, LIME [Ribeiro et al., 2016], one of the most popular post-hoc explanation systems, solves the following optimization problem:\ne(x, f) := arg min g\u2208Es F (f, g,Nx) + \u2126(g), (3)\nwhere \u2126(g) stands for an additive regularizer that encourages certain desirable properties of the explanations (e.g., sparsity). Along with LIME, we consider another explanation system, MAPLE [Plumb et al., 2018]. Unlike LIME, its neighborhoods are learned from the data using a tree ensemble rather than specified as a parameter."
    },
    {
      "heading": "2.1 Related Methods",
      "text": "There are three methods that consider problems conceptually similar to EXPO: Functional Transparency for Structured Data (FTSD) [Lee et al., 2018, 2019], Self-Explaining Neural Networks (SENN) [Alvarez-Melis and Jaakkola, 2018a], and Right for the Right Reasons (RRR) [Ross et al., 2017]. In this section, we contrast EXPO to these methods along several dimensions: whether the proposed regularizers are (i) differentiable, (ii) model agnostic, and (iii) require domain knowledge; whether the (iv) goal is to change an explanation\u2019s quality (e.g., fidelity or stability) or its content (e.g., how each feature is used); and whether the expected explainer is (v) neighborhood-based (e.g., LIME or MAPLE) or gradient-based (e.g., Saliency Maps [Simonyan et al., 2013]). These comparisons are summarized in Table 1, and we elaborate further on them in the following paragraphs.\nFTSD has a very similar high-level objective to EXPO: it regularizes black-box models to be more locally interpretable. However, it focuses on graph and time-series data and is not well-defined for general tabular data. Consequently, our technical approaches are distinct. First, FTSD\u2019s local neighborhood and regularizer definitions are different from ours. For graph data, FTSD aims to understand what the model would predict if the graph itself were modified. Although this is the same type of local interpretability considered by EXPO, FTSD requires domain knowledge to define Nx in order to consider plausible variations of the input graph. These definitions do not apply to general tabular data. For time-series data, FTSD aims to understand what the model will predict for the next\n2Although Plumb et al. [2018] argued that point-fidelity can be misleading because it does not measure generalization of e(x, f) across Nx, it has been used for evaluation in prior work [Ribeiro et al., 2016, 2018]. We report it in our experiments along with the neighborhood-fidelity for completeness.\npoint in the series and defines Nx as a windowed-slice of the series to do so. This has no analogue for general tabular data and is thus entirely distinct from EXPO. Second, FTSD\u2019s regularizers are non-differentiable, and thus it requires a more complex, less efficient bi-level optimization scheme to train the model.\nSENN is a by-design approach that optimizes the model to produce stable explanations. For both its regularizer and its explainer, it assumes that the model has a specific structure. In Appendix A.1, we show empirically that EXPO is a more flexible solution than SENN via two results. First, we show that we can train a significantly more accurate model with EXPO-FIDELITY than with SENN; although the EXPO-regularized model is slightly less interpretable. Second, if we increase the weight of the EXPO-FIDELITY regularizer so that the resulting model is as accurate as SENN, we show that the EXPO-regularized model is much more interpretable.\nRRR also regularizes a black-box model with a regularizer that involves a model\u2019s explanations. However, it is motivated by a fundamentally different goal and necessarily relies on extensive domain knowledge. Instead of focusing on explanation quality, RRR aims to restrict what features are used by the model itself, which will be reflected in the model\u2019s explanations. This relies on a user\u2019s domain knowledge to specify sets of good or bad features. In a similar vein to RRR, there are a variety of methods that aim to change the model in order to align the content of its explanations with some kind of domain knowledge [Du et al., 2019, Weinberger et al., 2019, Rieger et al., 2019]. As a result, these works are orthogonal approaches to EXPO.\nFinally, we briefly mention two additional lines of work that are also in some sense related to EXPO. First, Qin et al. [2019] proposed a method for local linearization in the context of adversarial robustness. Because its regularizer is based on the model\u2019s gradient, it will have the same issues with flexibility, fidelity, and stability discussed in Appendix A.2. Second, there is a line of work that regularizes black-box models to be easier to approximate by decision trees. Wu et al. [2018] does this from a global perspective while Wu et al. [2019] uses domain knowledge to divide the input space into several regions. However, small decision trees are difficult to explain locally by explainer\u2019s such as LIME (as seen in Figure 1) and so these methods do not solve the same problem as EXPO."
    },
    {
      "heading": "2.2 Connections to Function Approximations and Complexity",
      "text": "The goal of this section is to intuitively connect local linear explanations and neighborhood fidelity with classical notions of function approximation and complexity/smoothness, while also highlighting key differences in the context of local interpretability. First, neighborhood-based local linear explanations and first-order Taylor approximations both aim to use linear functions to locally approximate f . However, the Taylor approximation is strictly a function of f and x and cannot be adjusted to different neighborhood scales for Nx, which can lead to poor fidelity and stability. Second, Neighborhood Fidelity (NF), the Lipschitz Constant (LC), and Total Variation (TV) all approximately measure the smoothness of f across Nx. However, a large LC or TV does not necessarily indicate that f is difficult to explain across Nx (e.g., consider a linear model with large coefficients which has a near zero NF but has a large LC/TV). Instead, local interpretability is more closely related to the LC or TV of the part of f that cannot be explained by e(x, f) across Nx. Additionally, we empirically show that standard l1 or l2 regularization techniques do not influence model interpretability. Examples and details for all of these observations are in Appendix A.2.\nAlgorithm 1 Neighborhood-fidelity regularizer\ninput f\u03b8 , x, N regx , m 1: Sample points: x\u20321, . . . , x\u2032m \u223c N regx 2: Compute predictions: y\u0302j(\u03b8) = f\u03b8(x\u2032j) for j = 1, . . . ,m 3: Produce a local linear explanation: \u03b2x(\u03b8) = argmin\u03b2 \u2211m j=1(y\u0302j(\u03b8)\u2212 \u03b2 >x\u2032j) 2 output 1 m \u2211m j=1(y\u0302j(\u03b8)\u2212 \u03b2x(\u03b8) >x\u2032j) 2"
    },
    {
      "heading": "3 Explanation-based Optimization",
      "text": "Recall that the main limitation of using post-hoc explainers on arbitrary models is that their explanation quality can be unpredictable. To address this limitation, we define regularizers that can be added to the loss function and used to train an arbitrary model. This allows us to control for explanation quality without making explicit constraints on the model family in the way that by-design approaches do. Specifically, we want to solve the following optimization problem:\nf\u0302 := argmin f\u2208F\n1\nN N\u2211 i=1 (L(f, xi, yi) + \u03b3R(f,N regxi )) (4)\nwhere L(f, xi, yi) is a standard predictive loss (e.g., squared error for regression or cross-entropy for classification),R(f,N regxi ) is a regularizer that encourages f to be interpretable in the neighborhood of xi, and \u03b3 > 0 controls the regularization strength. Because our regularizers are differentiable, we can solve Equation 4 using any standard gradient-based algorithm; we use SGD with Adam [Kingma and Ba, 2014].\nWe defineR(f,N regx ) based on either neighborhood-fidelity, Eq. (1), or stability, Eq. (2). In order to compute these metrics exactly, we would need to run e; this may be non-differentiable or too computationally expensive to use as a regularizer. As a result, EXPO consists of two main approximations to these metrics: EXPO-FIDELITY and EXPO-STABILITY. EXPO-FIDELITY approximates e using a local linear model fit on points sampled from Nregx (Algorithm 1). Note that it is simple to modify this algorithm to regularize for the fidelity of a sparse explanation. EXPO-STABILITY encourages the model to not vary too much across Nregx and is detailed in Appendix A.8.\nComputational cost. The overhead of using EXPO-FIDELITY comes from using Algorithm 1 to calculate the additional loss term and then differentiating through it at each iteration. If x is ddimensional and we sample m points from Nregx , this has a complexity of O(d\n3 + d2m) plus the cost to evaluate f on m points. Note that m must be at least d in order for this loss to be non-zero, thus making the complexity \u2126(d3). Consequently, we introduce a randomized version of Algorithm 1, EXPO-1D-FIDELITY, that randomly selects one dimension of x to perturb according to Nregx and penalizes the error of a local linear model along that dimension. This variation has a complexity of O(m) plus the cost to evaluate f on m points, and allows us to use a smaller m.3"
    },
    {
      "heading": "4 Experimental Results",
      "text": "In our main experiments, we demonstrate the effectiveness of EXPO-FIDELITY and EXPO-1DFIDELITY on datasets with semantic features using seven regression problems from the UCI collection [Dheeru and Karra Taniskidou, 2017], the \u2018MSD\u2019 dataset4, and \u2018Support2\u2019 which is an in-hospital mortality classification problem5. Dataset statistics are in Table 2.\nWe found that EXPO-regularized models are more interpretable than normally trained models because post-hoc explainers produce quantitatively better explanations for them; further, they are often more accurate. Additionally, we qualitatively demonstrate that post-hoc explanations of EXPO-regularized\n3Each model takes less than a few minutes to train on an Intel 8700k CPU, so computational cost was not a limiting factor in our experiments. That being said, we observe a 2x speedup per iteration when using EXPO-1D-FIDELITY compared to EXPO-FIDELITY on the \u2018MSD\u2019 dataset and expect greater speedups on higher dimensional datasets.\n4As in [Bloniarz et al., 2016], we treat the \u2018MSD\u2019 dataset as a regression problem with the goal of predicting the release year of a song.\n5http://biostat.mc.vanderbilt.edu/wiki/Main/SupportDesc.\nmodels tend to be simpler. In Appendix A.8, we demonstrate the effectiveness of EXPO-STABILITY for creating Saliency Maps [Simonyan et al., 2013] on MNIST [LeCun, 1998].\nExperimental setup. We compare EXPO-regularized models to normally trained models (labeled \u201cNone\u201d). We report model accuracy and three interpretability metrics: Point-Fidelity (PF), Neighborhood-Fidelity (NF), and Stability (S). The interpretability metrics are evaluated for two black-box explanation systems: LIME and MAPLE. For example, the \u201cMAPLE-PF\u201d label corresponds to the Point-Fidelity Metric for explanations produced by MAPLE. All of these metrics are calculated on test data, which enables us to evaluate whether optimizing for explanation fidelity on the training data generalizes to unseen points.\nAll of the inputs to the model are standardized to have mean zero and variance one (including the response variable for regression problems). The network architectures and hyper-parameters are chosen using a grid search; for more details see Appendix A.3. For the final results, we set Nx to be N (x, \u03c3) with \u03c3 = 0.1 and Nregx to be N (x, \u03c3) with \u03c3 = 0.5. In Appendix A.4, we discuss how we chose those distributions.\nRegression experiments. Table 3 shows the effects of EXPO-FIDELITY and EXPO-1D-FIDELITY on model accuracy and interpretability. EXPO-FIDELITY frequently improves the interpretability metrics by over 50%; the smallest improvements are around 25%. Further, it lowers the prediction error on the \u2018communities\u2019, \u2018day\u2019, and \u2018MSD\u2019 datasets, while achieving similar accuracy on the rest. EXPO-1D-FIDELITY also significantly improves the interpretability metrics, although on average to a lesser extent than EXPO-FIDELITY does, and it has no significant effect on accuracy on average.\nA qualitative example on the UCI \u2018housing\u2019 dataset. After sampling a random point x, we use LIME to generate an explanation at x for a normally trained model and an EXPO-regularized model. Table 2 shows the example we discuss next. Quantitatively, training the model with EXPO-1DFIDELITY decreases the LIME-NF metric from 1.15 to 0.02 (i.e., EXPO produces a model that is more accurately approximated by the explanation around x). Further, the resulting explanation also has fewer non-zero coefficients (after rounding), and hence it is simpler because the effect is attributed to fewer features. More examples, that show similar patterns, are in Appendix A.5.\nMedical classification experiment. We use the \u2018support2\u2019 dataset to predict in-hospital mortality. Since the output layer of our models is the softmax over logits for two classes, we run each explainer on each of the logits. We observe that EXPO-FIDELITY had no effect on accuracy and improved the interpretability metrics by 50% or more, while EXPO-1D-FIDELITY slightly decreased accuracy and improved the interpretability metrics by at least 25%. See Table 9 in Appendix A.6 for details."
    },
    {
      "heading": "5 User Study",
      "text": "The previous section compared EXPO-regularized models to normally trained models through quantitative metrics such as model accuracy and post-hoc explanation fidelity and stability on held-out test data. Doshi-Velez and Kim [2017] describe these metrics as Functionally-Grounded Evaluations, which are useful proxies for more direct applications of interpretability. To more directly\nmeasure the usefulness of EXPO, we conduct a user study to obtain Human-Grounded Metrics [Doshi-Velez and Kim, 2017], where real people solve a simplified task.\nIn summary, the results of our user study show that the participants had an easier time completing this task with the EXPO-regularized model and found the explanations for that model more useful. See Table 4 and Figure 8 in Appendix A.7 for details. Not only is this additional evidence that the fidelity and stability metrics are good proxies for interpretability, but it also shows that they remain so after we directly optimize for them. Next, we describe the high-level task, explain the design choices of our study, and present its quantitative and qualitative results.\nDefining the task. One of the common proposed use cases for local explanations is as follows. A user is dissatisfied with the prediction that a model has made about them, so they request an explanation for that prediction. Then, they use that explanation to determine what changes they should make in order to receive the desired outcome in the future. We propose a similar task on the UCI \u2018housing\u2019 regression dataset where the goal is to increase the model\u2019s prediction by a fixed amount.\nWe simplify the task in three ways. First, we assume that all changes are equally practical to make; this eliminates the need for any prior domain knowledge. Second, we restrict participants to changing a single feature at a time by a fixed amount; this reduces the complexity of the required mental math. Third, we allow participants to iteratively modify the features while getting new explanations at each point; this provides a natural quantitative measure of explanation usefulness, via the number of changes required to complete the task.\nDesign Decisions. Figure 2 shows a snapshot of the interface we provide to participants. Additionally, we provide a demo video of the user study in the Github repository. Next, we describe several key design aspects of our user study, all motivated by the underlying goal of isolating the effect of EXPO.\n1. Side-by-side conditions. We present the two conditions side-by-side with the same initial point. This design choice allows the participants to directly compare the two conditions and allows us to gather their preferences between the conditions. It also controls for the fact that a model may be more difficult to explain for some x than for others. Notably, while both conditions have the same initial point, each condition is modified independently. With the conditions shown side-by-side, it may be possible for a participant to use the information gained by solving one condition first to help solve the other condition. To prevent this from biasing our aggregated results, we randomize, on a per-participant basis, which model is shown as Condition A.\n2. Abstracted feature names and magnitudes. In the explanations shown to users, we abstract feature names and only show the magnitude of each feature\u2019s expected impact. Feature names are abstracted in order to prevent participants from using prior knowledge to inform their decisions.\nCondition Steps Usefulness Expectation None 11.45 11 11 ExpO 8.00 28 26 No Preference N.A. 15 17\n3. Learning Effects. To minimize long-term learning (e.g., to avoid learning general patterns such as \u2018Item 7\u2019s explanation is generally unreliable.\u2019), participants are limited to completing a single experiment consisting of five recorded rounds. In Figure 7 from Appendix A.7, we show that the distribution of the number of steps it takes to complete each round across participants does not change substantially. This result indicates that learning effects were not significant.\n4. Algorithmic Agent. Although the study is designed to isolate the effect EXPO has on the usefulness of the explanations, entirely isolating its effect is impossible with human participants. Consequently, we also evaluate the performance of an algorithmic agent that uses a simple heuristic that relies only on the explanations. See Appendix A.7 for details.\nCollection Procedure. We collect the following information using Amazon Mechanical Turk:\n1. Quantitative. We measure how many steps (i.e., feature changes) it takes each participant to reach the target price range for each round and each condition.\n2. Qualitative Preferences. We ask which condition\u2019s explanations are more useful for completing the task and better match their expectation of how the price should change.6\n3. Free Response Feedback. We ask why participants preferred one condition over the other.\nData Cleaning. Most participants complete each round in between 5 and 20 steps. However, there are a small number of rounds that take 100\u2019s of steps to complete, which we hypothesize to be random clicking. See Figure 6 in Appendix A.7 for the exact distribution. As a result, we remove any participant who has a round that is in the top 1% for number of steps taken (60 participants with 5 rounds per participant and 2 conditions per round gives us 600 observed rounds). This leaves us with a total of 54 of the original 60 participants.\nResults. Table 4 shows that the EXPO-regularized model has both quantitatively and qualitatively more useful explanations. Quantitatively, participants take 8.00 steps on average with the EXPOregularized model, compared to 11.45 steps for the normally trained model (p = 0.001, t-test). The participants report that the explanations for the EXPO-regularized model are both more useful for completing the task (p = 0.012, chi-squared test) and better aligned with their expectation of how the model would change (p = 0.042, chi-squared test). Figure 8 in Appendix A.7 shows that the\n6Note that we do not ask participants to rate their trust in the model because of issues such as those raised in Lakkaraju and Bastani [2020].\nalgorithmic agent also finds the task easier to complete with the EXPO-regularized model. This agent relies solely on the information in the explanations and thus is additional validation of these results.\nParticipant Feedback. Most participants who prefer the EXPO-regularized model focus on how well the explanation\u2019s predicted change matches the actual change. For example, one participant says \u201cIt [EXPO ] seemed to do what I expected more often\u201d and another notes that \u201cIn Condition A [None] the predictions seemed completely unrelated to how the price actually changed.\u201d Although some participants who prefer the normally trained model cite similar reasons, most focus on how quickly they can reach the goal rather than the quality of the explanation. For example, one participant plainly states that they prefer the normally trained model because \u201cThe higher the value the easier to hit [the] goal\u201d; another participant similarly explains that \u201cIt made the task easier to achieve.\u201d These participants likely benefited from the randomness of the low-fidelity explanations of the normally trained model, which can jump unexpectedly into the target range."
    },
    {
      "heading": "6 Conclusion",
      "text": "In this work, we regularize black-box models to be more interpretable with respect to the fidelity and stability metrics for local explanations. We compare EXPO-FIDELITY, a model agnostic and differentiable regularizer that requires no domain knowledge to define, to classical approaches for function approximation and smoothing. Next, we demonstrate that EXPO-FIDELITY slightly improves model accuracy and significantly improves the interpretability metrics across a variety of problem settings and explainers on unseen test data. Finally, we run a user study demonstrating that an improvement in fidelity and stability improves the usefulness of the model\u2019s explanations."
    },
    {
      "heading": "7 Broader Impact",
      "text": "Our user study plan has been approved by the IRB to minimize any potential risk to the participants, and the datasets used in this work are unlikely to contain sensitive information because they are public and well-studied. Within the Machine Learning community, we hope that EXPO will help encourage Interpretable Machine Learning research to adopt a more quantitative approach, both in the form of proxy evaluations and user studies. For broader societal impact, the increased interpretability of models trained with EXPO should be a significant benefit. However, EXPO does not address some issues with local explanations such as their susceptibility to adversarial attack or their potential to artificially inflate people\u2019s trust in the model."
    },
    {
      "heading": "Acknowledgments",
      "text": "This work was supported in part by DARPA FA875017C0141, the National Science Foundation grants IIS1705121 and IIS1838017, an Okawa Grant, a Google Faculty Award, an Amazon Web Services Award, a JP Morgan A.I. Research Faculty Award, and a Carnegie Bosch Institute Research Award. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of DARPA, the National Science Foundation, or any other funding agency. We would also like to thank Liam Li, Misha Khodak, Joon Kim, Jeremy Cohen, Jeffrey Li, Lucio Dery, Nick Roberts, and Valerie Chen for their helpful feedback."
    },
    {
      "heading": "A Appendix",
      "text": ""
    },
    {
      "heading": "A.1 Comparison of EXPO to SENN",
      "text": "We compare against SENN [Alvarez-Melis and Jaakkola, 2018a] on the UCI \u2018breast cancer\u2019 dataset which is a binary classification problem. Because SENN\u2019s implementation outputs class probabilities, we run the post-hoc explainers on the probability output from the EXPO-regularized model as well (this differs from the \u2018support2\u2019 binary classification problem where we explain each logit individually). The results are in Table 5.\nBy comparing the first row, which shows the results for an EXPO-FIDELITY-regularized model whose regularization weight is tuned for accuracy, to the third row, which shows SENN\u2019s results, we can see that SENN\u2019s by-design approach to model interpretability seriously impaired its accuracy. However, SENN did produce a more interpretable model. From these results alone, there is no objective way to decide if the EXPO or SENN model is better. But, looking at the MAPLE-NF metric, we can see that its explanations have a standard error of around 4% relative to the model\u2019s predicted probability. This is reasonably small and probably acceptable for a model that makes a fraction as many mistakes.\nLooking at the second row, which shows the results for a EXPO-FIDELITY-regularized model whose regularization weight has been increased to produce a model that is approximately accurate as SENN, we can see that the EXPO-regularized model is more interpretable than SENN.\nConsidering both of these results, we conclude that EXPO is more effective than SENN at improving the quality of post-hoc explanations.\nHowever, this is a slightly unusual comparison because SENN is designed to produce its own explanations but we are using LIME/MAPLE to explain it. When we let SENN explain itself, it has a NF of 3.1e-5 and a Stability of 2.1e-3. These numbers are generally comparable to those of LIME explaining EXPO-Over Regularized. This further demonstrates EXPO\u2019s flexibility."
    },
    {
      "heading": "A.2 Expanded Version of Section 2.2",
      "text": "This section provides the details for the results outlined in Section 2.2.\nLocal explanations vs. Taylor approximations. A natural question to ask is, Why should we sample from Nx in order to locally approximate f when we could use the Taylor approximation as done in [Ross et al., 2017, Alvarez-Melis and Jaakkola, 2018a]? The downside of a Taylor approximationbased approach is that such an approximation cannot readily be adjusted to different neighborhood scales and its fidelity and stability strictly depend on the learned function. Figure 3 shows that the Taylor approximations for two close points can be radically different from each other and are not necessarily faithful to the model outside of a small neighborhood.\nFidelity regularization and the model\u2019s LC or TV. From a theoretical perspective, EXPOFIDELITY is similar to controlling the Lipschitz Constant or Total Variation of f across Nx after removing the part of f explained by e(x, f). From an interpretability perspective, having a large LC or TV does not necessarily lead to poor explanation quality, which is demonstrated in Figure 4.\nStandard Regularization. We also consider two standard regularization techniques: l1 and l2 regularization. These regularizers may make the network simpler (due to sparser weights) or smoother, which may make it more amenable to local explanation. The results of this experiment are in Table 6; notice that neither of these regularizers had a significant effect on the interpretability metrics."
    },
    {
      "heading": "A.3 Model Details and Selection",
      "text": "The models we train are Multi-Layer Perceptrons with leaky-ReLU activations. The model architectures are chosen by a grid search over the possible widths (100, 200, 300, 400, or 500 units) and depths (1, 2, 3, 4, or 5 layers). The weights are initialized with the Xavier initialization and the biases are initialized to zero. The models are trained with SGD with the Adam optimizer and a learning rate of 0.001. For each dataset, the architecture with the best validation loss is chosen for final evaluation as the \u201cNone\u201d model. Then, we use that same architecture and add the EXPO regularizer with weights chosen from 0.1, 0.05, 0.025, 0.01, 0.005, or 0.001. Note that these are not absolute weights and are instead relative weights: so picking 0.1 means that the absolute regularization weight is set such that the regularizer has 1/10th the weight of the main loss function (estimated using a single mini-batch at the start of training and then never changed). This makes this hyper-parameter less sensitive to the model architecture, initialization, and dataset. We then pick the best regularization weight for each dataset using the validation loss and use that for the final evaluation as the EXPO model. Final evaluation is done by retraining the models using their chosen configurations and evaluating them on the test data."
    },
    {
      "heading": "A.4 Defining the Local Neighborhood",
      "text": "Choosing the neighborhood shape. Defining a good regularization neighborhood, requires considering the following. On the one hand, we would like N regx to be similar to Nx, as used in Eq. 1 or Eq. 2, so that the neighborhoods used for regularization and for evaluation match. On the other hand, we would also like N regx to be consistent with the local neighborhood defined internally by e, which may differ from Nx. LIME can avoid this problem since the internal definition of the local neighborhood is a hyperparameter that we can set. However, for our experiments, we do not do this and, instead, use the default implementation. MAPLE cannot easily avoid this problem because the local neighborhood is learned from the data, and hence the regularization and explanation neighborhoods probably differ.\nChoosing \u03c3 for Nx and Nregx . In Figure 5, we see that the choice of \u03c3 for Nx was not critical (the value of LIME-NF only increased slightly with \u03c3) and that this choice of \u03c3 for Nregx produced slightly more interpretable models."
    },
    {
      "heading": "A.5 More Examples of EXPO\u2019s Effects",
      "text": "Here, we demonstrate the effects of EXPO-FIDELITY on more examples from the UCI \u2018housing\u2019 dataset (Table 7). Observe that the same general trends hold true:\n\u2022 The explanation for the EXPO-regularized model more accurately reflects the model (LIMENF metric)\n\u2022 The explanation for the EXPO-regularized model generally considers fewer features to be relevant. We consider a feature to be \u2018significant\u2019 if its absolute value is 0.1 or greater.\n\u2022 Neither model appears to be heavily influenced by CRIM or INDUS. The EXPO-regularized model generally relies more on LSTAT and less on DIS, RAD, and TAX to make its predictions.\nThe same comparison for examples from the UCI \u2018winequality-red\u2019 are in Table 8. We can see that the EXPO-regularized model depends more on \u2018volatile acidity\u2019 and less on \u2018sulphates\u2019 while usually agreeing about the effect of \u2018alcohol\u2019. Further, it is better explained by those explanations than the normally trained model."
    },
    {
      "heading": "A.6 Quantitative Results on the \u2018support2\u2019 Dataset",
      "text": "In Table 9, we compare EXPO-regularized models to normally trained models on the \u2018support2\u2019 dataset."
    },
    {
      "heading": "Output Regularizer LIME-PF LIME-NF LIME-S MAPLE-PF MAPLE-NF MAPLE-S",
      "text": ""
    },
    {
      "heading": "A.7 User Study: Additional Details",
      "text": "Data Details. Figure 6 shows the histogram of the number of steps participants take to complete each round. We use 150 as our cut-off value for removing participant\u2019s data from the final evaluation. Figure 7 shows a histogram of the number of steps participants take to complete each round. There is no evidence to suggest that the earlier rounds took a different amount of time than the later rounds. So learning effects were not significant in this data.\nAlgorithmic Agent. In addition to measuring humans\u2019 performance on this task (see Section 5 for details), we are also interested in measuring a simple algorithmic agent\u2019s performance on it. The benefit of this evaluation is that the agent relies solely on the information given in the explanations and, as a result, does not experience any learning affects that could confound our results.\nIntuitively, we could define a simple greedy agent by having it change the feature whose estimated effect is closest to the target change. However, this heuristic can lead to loops that the agent will never escape. As a result, we consider a randomized version of this greedy agent.\nLet \u03bb the degree of randomization for the agent, y denote the model\u2019s current prediction, t denote the target value, and ci denote the explanation coefficient of feature i. Then the score of feature i, which measures how close this features estimated effect is to the target change, is: si = \u2212\u03bb \u2217 ||ci| \u2212 |y\u2212 t||. The agent then chooses to use feature i with probability e\nsi\u2211 j esj .\nLooking at this distribution, we see that it is uniform (i.e., does not use the explanation at all) when \u03bb = 0 and that it approaches the greedy agent as \u03bb approaches infinity.\nIn Figure 8, we run a search across the value of \u03bb to find a rough trade-off between more frequently using the information in the explanation and avoiding loops. Note that the agent performs better for the EXPO-regularized model."
    },
    {
      "heading": "A.8 Non-Semantic Features",
      "text": "When X consists of non-semantic features, we cannot assign meaning to the difference between x and x\u2032 as we could with semantic features. Hence it does not make sense to explain the difference between the predictions f(x) and f(x\u2032) and fidelity is not an appropriate metric.\nInstead, for non-semantic features, local explanations try to identify which parts of the input are particularly influential on a prediction [Lundberg and Lee, 2017, Sundararajan et al., 2017]. Consequently, we consider explanations of the form Ens := Rd, where d is the number of features in X , and our primary explanation metric is stability. Note that we could consider these types of local explanations for semantic features as well, but that they answer a different question than the approximation-based local explanations we consider.\nPost-hoc explainers. Various explainers [Sundararajan et al., 2017, Zeiler and Fergus, 2014, Shrikumar et al., 2016, Smilkov et al., 2017] have been proposed to generate local explanations in Ens for images. However, it should be noted that the usefulness and evaluation of these methods is uncertain [Adebayo et al., 2018, Tomsett et al., 2019]. For our experiment, we will consider saliency maps [Simonyan et al., 2013] which assign importance weights to image pixels based on the magnitude of the gradient of the predicted class with respect to the corresponding pixels.\nStability Regularizer. For EXPO-STABILITY, we simply require that the model\u2019s output not change too much across Nregx (Algorithm 2). A similar procedure was explored previously in [Zheng et al., 2016] for adversarial robustness.\nExperimental setup. For this experiment, we compared a normally trained convolutional neural network on MNIST to one trained using EXPO-STABILITY. Then, we evaluated the quality of saliency map explanations for these models. Both Nx and Nregx where defined as Unif(x\u2212 0.05, x+ 0.05). Both the normally trained model and model trained with EXPO-STABILITY achieved the same accuracy of 99%. Quantitatively, training the model with EXPO-STABILITY decreased the stability metric from 6.94 to 0.0008. Qualitatively, training the model with EXPO-STABILITY made the resulting saliency maps look much better by focusing them on the presence or absence of certain pen strokes (Figure 9).\nAlgorithm 2 Neighborhood-stability regularizer input f\u03b8 , x, N regx , m\n1: Sample points: x\u20321, . . . , x\u2032m \u223c N regx 2: Compute predictions:\ny\u0302j(\u03b8) = f\u03b8(x \u2032 j), for j = 1, . . . ,m\noutput 1 m \u2211m j=1(y\u0302j(\u03b8)\u2212 f(x)) 2"
    }
  ],
  "title": "Regularizing Black-box Models for Improved Interpretability",
  "year": 2020
}
